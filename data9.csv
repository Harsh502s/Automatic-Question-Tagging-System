Head,Body,Tags,First Answer
Generate credit cards dataset for locating number region,"
Currently I'm working on a project for scanning credit card and text extraction from cards. So first of all I decided to preprocess my images with some filters like thresholding, dilation and some other stuff. But it was not successfully for OCR of every credit cards. So I learned a lot and I found a solution like this for number plate recognition that is very similar to my project.
In the first step I want to generate a random dataset like my cards to locate card number region, and for every card that I've generated I cropped two images that one of them has numbers and another has not. I generated 2000 images for every cards.
so I have some images like this:
 (does not have numbers)
 (has numbers)
And after generating my dataset I used this model with tensorflow to train my network.
    model = models.Sequential()
    model.add(layers.Conv2D(8, (5, 5), padding='same', activation='relu', input_shape=(30, 300, 3)))
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Conv2D(16, (5, 5), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Conv2D(32, (5, 5), padding='same', activation='relu'))
    model.add(layers.MaxPooling2D((2, 2)))

    model.add(layers.Flatten())
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dense(2, activation='softmax'))

Here is my plot for 5 epochs. 


I almost get 99.5% of accuracy and It seems to be wrong, I think I have kind of overfitting in my data. Does it work correctly or my model is overfitted ? And how can I generate dataset for this purpose ?
","['neural-networks', 'convolutional-neural-networks', 'tensorflow', 'datasets', 'optical-character-recognition']",
Which approaches are best suited for text deblurring?,"


I want to deblur text images using deep learning. Which approaches are best suited for the task? Any example networks? Is unsupervised network the best approach? GAN or cycle GAN for these purposes?
I have currently prepared 1000 images for training (shapr+blur) is it sufficient? For each of these approaches, how many training images do I need? 
I have attached sample blurred image and Ground truth
","['neural-networks', 'convolutional-neural-networks', 'unsupervised-learning', 'generative-adversarial-networks']",
Will a .h5 file trained with Xception model work with Resnet50? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I have been running my 2013 server box since 2 weeks ago for training an AI model.
I set up 30 epochs to run but since than it only ran 1 epoch as my PC config is super slow. But it generates 1 .h5 file. 
My question is will this .h5 file that I trained with Xception model work for Resnet 50?
","['training', 'models', 'hyper-parameters']",
Examples of ontologies made with AI,"
I'm looking for more or less successful artificial intelligence usage examples to build an ontology or rationale why it can't be done. I found a lot of articles on how to use ontologies for AI, but not succeded vice versa.
","['applications', 'ontology']",
What are the pros and cons of using standard deviation or entropy for exploration in PPO?,"
When trying to implement my own PPO (Proximal Policy Optimizer), I came across two different implementations :
Exploration with std

Collect trajectories on $N$ timesteps, by using a policy-centered distribution with progressively trained std variable for exploration
Train policy function on $K$ steps
Train value function on $K$ steps

For example, the OpenAI's implementation.
Exploration with entropy

Collect trajectories on $N$ timesteps, by using policy function directly
Train policy and value function at the same time on $K$ steps, with a common loss for the two models, with additional entropy bonus for exploration purpose.

For example, the PPO algorithm as described in the official paper.
What are the pros/cons of these two algorithms?
Is this specific to PPO, or is this a classic question concerning policy gradients algorithms, in general?
","['neural-networks', 'reinforcement-learning', 'policy-gradients', 'proximal-policy-optimization']","
Both implementations may be closer than you think.
In short:
PPO has both parts: there is noisiness in draws during training (with learned standard deviation), helping to explore new promising actions/policies. And there is a term added to the loss function aiming to prevent a collapse of the noisiness, to help ensure exploration continues and we don't get stuck at a bad (local) equilibrium.
In fact, for continuous action, the term for entropy in the loss function you describe in Ex. 2, can make sense only when the actions are stochastic, i.e. when there action choice has some standard deviation the way you describe in Ex. 1.
More detail:
On one hand, PPO (at least for continuous action), trains a central/deterministic value (say the mean policy or close to mean) targeting a most profitable action path. On the other hand, along with it, a standard deviation making the actions a random draw with noise around the deterministic value. This is the part you describe in Example 1. The noise helps explore new paths and to update the policy according to the rewards on these sampled paths.
Entropy itself is a measure of the noisiness of the draws, an thus also an indirect indicator for the trained standard deviation value(s) of the policy.
Now, entropy tends to decay as training progresses, that is, the random draws become progressively less random. This can be good for reward maximization - really the best draws are taken for reward maximization - but it is bad for further improvements of policy: improvement may halt or slow down as exploration of new action paths fades.
This is where entropy encouragement comes in. PPO foresees the inclusion of entropy in the loss function: we reduce the loss by x * entropy, with x the entropy coefficient (e.g. 0.01), incentivizing the learning network to increase the standard deviations (or, to not let them drop too much). This part is what you describe in Example 2.
Further notes:

During exploitation, we'd typically turn off the noise (implicitly assuming action std = 0) and pick deterministic actions: in normal cases this increases the payoffs; we're choosing our best action estimate, rather than at a random value around it.
People are not always precise when referring to the model's entropy vs. the entropy coefficient added to the loss function.
Other RL algorithms with continuous action tend to use noisy drwas with standard deviations/entropy too.

"
Confidence Maps and Non-Linearity,"
I am currently trying to improve a CNN architecture that was proposed for generating depth images. 
The architecture was originally proposed for autonomous driving and it looks like following :

The idea behind this architecture is to improve the accuracy of depth images by adding confidence maps 
to the outputs of different sensors to govern their influence on the result for each pixel. Input is an 
RGB image and its corresponding LIDAR data, output is a depth image with the same dimensions.
For example, RGB features are best at discriminating global information such as sky, land, body of water, etc. 
while LIDAR features are better at capturing fine-grained, local information. So to decide which features will 
have more influence at the final regression result of which pixel, scientists proposed a confidence-guided 
architecture where confidence weights for each map are learned during training.
Judging by their test results and how successful their paper was, their idea worked out pretty well for their 
problem domain. That's why I would like to employ the idea of confidence weights in my own domain, where I have 
multiple sources of features too. I have implemented the same architecture and got promising results: Accuracy 
have been improved, but not enough to compete with SOTA. 
However, I believe that the architecture above can be improved for my needs. The diversity of the scene structure 
in my domain adds some amount of complexity to the image generation problem: Opposed to depth image generation for 
autonomous driving where scene structure is somewhat restricted (there is sky, there is road, there are sideways etc), 
the images that I need to analyze are sometimes taken from handheld cameras, some other images are areal views.
Here is a typical example of their scenario and some examples related to my domain, respectively.


This means my CNN needs to learn confidence weights for regions in fundamentally different scene setups. Top of 
the image can be sky, but can also be populated by people. Bottom of the image can be sea, or can be road. This 
brings me to the understanding that there is a non-linear relation between the position of the segments and confidence 
weights in my case; and I need to modify the CNN architecture by introducing some additional non-linearity to learn the 
confidences for different CNN columns in each situation correctly.
TLDR; I want to improve the CNN architecture above by introducing additional non-linearity, but I do not know how to do it. 
I have tried adding another layer to confidence weights (Extended the architecture by duplicating the same weights and activating 
with ReLU), but it has decreased the accuracy of the resulting model. Using the confidence weights as-is increases the accuracy, but
not as much as I need.
","['convolutional-neural-networks', 'computer-vision', 'regression', 'image-segmentation']",
Structure discrepancy of an LSTM?,"
I've found multiple depictions of how an LSTM cell operates. See 2 below:

and

Each of these images suggest the hidden state is utilised differently. On the top diagram, it is shown that the hidden state is added along with the previous output and current input to both the forget gate and the input gate. The bottom image suggests the input and forget gates are calculated only using the previous output and current input. Which is it?
Also, when the previous output is fed in for the current layer, is this before or after it has been reshaped to the final output size and been put through a softmax?
","['neural-networks', 'backpropagation', 'long-short-term-memory']",
Reference request: one-hot encoding outperforming random orthogonal encoding,"
I experimented with a CNN operating on texts encoded as sequences of character vectors, where characters are encoded as one-hot vectors in one embedding and as random unit length pairwise orthogonal vectors (orthogonal matrix) in another. While geometrically these encode the same vector space, the one-hot embedding outperformed the random orthogonal one consistently. I suppose this has to do with the clarity of the signal: A zero vector with a single 1-valued cell is an easier to learn signal than just some vector with lots of different values in each cell.
I wondered if you know of any papers on this kind of effect. I did not find any but would like to back up this finding and check if my reasoning for why this is the case makes sense/ find a better or more in-depth explanation.
","['neural-networks', 'convolutional-neural-networks', 'reference-request', 'word-embedding', 'papers']",
How to handle multiple types of decisions?,"
In lots of games there are multiple phases or decision points that are not similar yet seem to have a dependency on one another when taking the perspective of the overall strategy of the player. A couple examples I thought up:

In a simple draw poker, you can have a strategy for discarding cards and a strategy for betting. They may not be mutually exclusive if you know your opponents betting will change with the number of cards you draw.
In Cribbage there are two phases, Discard to crib and the Play. The Play phase is definitely dependent on which cards are discarded in the discard phase. So it seems knowledge of Play strategy would be needed to make the Discard decision.

The intent is to learn how to set up an unsupervised learning algorithm to play a game with multiple types of decision making. Doesn't matter the game. I'm at a loss at the highest level in what ML models to learn to use for this scenario. I don't think a single NN would work because of the different decision types.
My question is how are these dependencies handled in ML? What are some known algorithms/models that can handle this?
I'm at a loss on what to even search for so feel free to dump some terminology and keywords on me. =)
",['ai-design'],"
Your intuition is correct, neural networks are a no go (except see bottom). 
It seems like you'd want to look into the ML sub-field called Reinforcement Learning.
In a nutshell, RL offers a set of methods to learn what is the best action to take in a given situation.
More formally, in RL settings the algorithm learns from experience by observing a reward ($R$) associated to an action ($a$).
RL problems can be conceptualized as Markov Decision Processes (MDPs). From Sutton & Barto:

MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards.

MDP can be faced with several methods, mainly Monte Carlo Methods and Temporal-Difference Learning. 
In short, both use the Bellman equation in different ways. A key component of the eq. is the discounting term ( $\gamma$ ). Values for rewards associated with future states are discounted (multiplied) by $\gamma<1$ in order to modulate the importance of future rewards.
Since you're concerned with strategy, another way to influence the agent's learning wrt. the environmental rewards is the adoption of an epsilon value $\epsilon<1$. When defining an $\epsilon$-greedy policy, the agent will choose the action with highest value $Q$ only with a probability equal to $1-\epsilon$. This enables the agent to balance exploitation of previous experience with the exploration of the environment. Very useful in cases where an immediate low reward compromises the achievement of a much higher reward later during the episode.
There's many other ways to influence the behaviour of your learner of choice to respond differently to its environment. For a complete view, I recommend Sutton and Burton who offer their book in pdf format for free at the given link.
Edit: almost forgot. Neural networks can be useful when combined with more classical RL methods, say Q-learning (a type of Temporal-difference learning), taking on the name of deep Q-learning.
Alpha go also uses a combination of Monte Carlo methods combined with 2 NNs for Value evaluation and policy evaluation Wikipedia.
I wouldn't necessarily venture in this domain without first having a clear overview of classical RL methods.
Hope this helps.
"
How is the gradient of the loss function in DQN derived?,"
In the original DQN paper, page 1, the loss function of the DQN is
$$
L_{i}(\theta_{i}) = \mathbb{E}_{(s,a,r,s') \sim  U(D)} [(r+\gamma \max_{a'} Q(s',a',\theta_{i}^{-}) - Q(s,a;\theta_{i}))^2]
$$
whose gradient is presented (on page 7)
$$\nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{s,a,r,s'} [(r+\gamma \max_{a'}Q(s',a';\theta_i^-) - Q(s,a;\theta_i))\nabla_{\theta_i}Q(s,a;\theta_i)] $$
But why is there no minus (-) sign if $-Q(s,a;\theta_i)$ is parameterized by $\theta_i$ and why is the 2 from power gone?
","['reinforcement-learning', 'dqn', 'deep-rl', 'gradient-descent', 'gradient']",
How does a single neuron in hidden layer affect training accuracy [duplicate],"







This question already has answers here:
                                
                            




Where can I find the proof of the universal approximation theorem?

                                (3 answers)
                            

Closed 1 year ago.



I'm currently a student learning about AI Networks. I've came across a statement in one of my Professor's books that a FFBP (Feed-Forward Back-Propagation) Neural Network with a single hidden layer  can model any mathematic function with accuracy dependant on number of hidden layer neurons. Try as I might I cannot find any explanation as to why that occurs - could someone maybe explain the question why that is?
","['neural-networks', 'backpropagation', 'hidden-layers', 'feedforward-neural-networks']","
The claim that Neural Network with a single hidden layer can model any functions is proven in Cybenko's Approximation by superpositions of a sigmoidal function.
https://link.springer.com/article/10.1007/BF02551274
check also: https://en.wikipedia.org/wiki/Universal_approximation_theorem
The thing is that the neural network using sigmoidal functions, which are non-linear functions can.
"
Is there any computer vision technology that can detect any type of object?,"
Is there any computer vision technology that can detect any type of object? For example, there is a camera fixed, looking in one direction always looking at a similar background. If there is an object, no matter what the object is (person, bag, car, bike, cup, cat) the CV algorithm would notice if there is an object in the frame. It wouldn't know what type of object it is, just that there is an object in the frame. 
Something similar to motion detector but that would work on a flat conveyor belt. Even though the conveyor belt moves will look similar between frames. Would something like this be possible? Possibly something to do with extracting differences from the background, with the goal being to not have to train the network with data for every possible object that may pass by the camera.
","['algorithm', 'computer-vision', 'object-recognition']",
Is it useful to eliminate the less relevant filters from a trained CNN?,"
Imagine I have a tensorflow CNN model with good accuracy but maybe too many filters:

Is there a way to determine which filters have more impact in output? I think it should be possible. At least, if a filter A has a 0, that only multiples the output of a filter B, then filter B is not related to filter A. In particular, I'm thinking in 2d data where 1 dimension is time-related and the other feature related (like one-hot char).
Is there a way to eliminate the less relevant filters from a trained model, and leave the rest of the model intact?
Is it useful or there are better methods?

","['deep-learning', 'convolutional-neural-networks', 'tensorflow', 'convolution']",
Is there data available about successful neural network architectures?,"
I am curious to if there is data available for MLP architectures in use today, their initial architecture, the steps that were taken to improve the architecture to an acceptable state and what the problem is the neural network aimed to solve.
For example, what the initial architecture (amount of hidden layers, the amount of neurons) was for a MLP in a CNN, the steps taken to optimize the architecture (adding more layers and reducing nodes, changing activation functions) and the results each step produced (i.e. increased error or decreased error). What the problem is the CNN tried to solve (differentiation of human faces, object detection inteded for self driving cars etc.)
Of course I used a CNN as an example but I am referring to data for any MLP architecture in plain MLPs or Deep Learning architectures such as RNNs, CNNs and mroe. I am focused on the MLP architecture mostly. 
If there is not how do you think one can accumulate this data?
","['neural-networks', 'research', 'architecture', 'multilayer-perceptrons']",
Applying ML algorithms to data-sets with similar meta-features?,"
Is there any grounds for assuming an algorithms applied to a data-set that created a decently accurate model will perform as well on a different data-set with meta-features chosen and evaluated by meta-learning? What meta-features are even worth considering when evaluating similarity between data-sets with the goal of finding an optimal combination of algorithm application to this new data-set to create an accurate model?
","['neural-networks', 'machine-learning']",
Issue at training simple RNN for word generation,"
After completing Coursera course from Andrew Ng, I wanted to implement again simple RNN for generating dinosaurs name based on a text file containing around 800 dinosaurs name. 
This is done with Numpy in coursera, here is a link to a  Jupyter notebook (not my repo) to get strategy and full objective: 
Here
I started similar implementation but in Pytorch, here is the model:
class RNN(nn.Module):
    def __init__(self,input_size):
        super(RNN, self).__init__()
        print(""oo"")
        self.hiddenWx1 = nn.Linear(input_size, 100) 
        self.hiddenWx2 = nn.Linear(100, input_size)
        self.z1 = nn.Linear(input_size,100)
        self.z2 = nn.Linear(100,input_size)
        self.tanh = nn.Tanh()
        self.softmax = torch.nn.Softmax(dim=1)

    def forward(self, input, hidden):
        layer = self.hiddenWx1(input)
        layer = self.hiddenWx2(layer)
        a_next = self.tanh(layer)
        z = self.z1(a_next)
        z = self.z2(z)
        y_next = self.softmax(z)
        return y_next,a_next

Here is the main algorithm of training:
for word in examples:  # for every dinosaurus name
                model.zero_grad()
                hidden= torch.zeros(1, len(ix_to_char)) #initialise hidden to null, ix_to_char is below
                word_vector = word_tensor(word) # convert each letter of  the current name in one-hot tensors
                output = torch.zeros(1, len(ix_to_char)) #first input is null
                loss = 0
                counter = 0
                true = torch.LongTensor(len(word)) #will contains the index of each letter.If word is ""badu"" => [2,1,4,22,0]

                measured = torch.zeros(len(word)) # will contains the vectors returned by the model for each letter (softmax output) 


                for t in range(len(word_vector)): # for each letter of current word
                    true[counter] = char_to_ix[word[counter]] # char_to_ix return the index of letter in dictionary

                    output, hidden = model(output, hidden)

                    if (counter ==0):
                        measured = output
                    else: #measures is a tensor containing tensors of probability distribution
                        measured = torch.cat((measured,output),dim=0)
                    counter+=1

                loss = nn.CrossEntropyLoss()(measured, true) #
                loss.backward()
                optimizer.step()

The letter dictionary (ix_to_char) is as follow: 
{0: '\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}
Every 2000 epochs, I sample some new words with this function using torch multimonial to select a letter based on the softmax probability returned by the model:
def sampling(model):
    idx = -1 
    counter = 0
    newline_character = char_to_ix['\n']

    x = torch.zeros(1,len(ix_to_char))
    hidden = torch.zeros(1, len(ix_to_char))
    generated_word=""""


    while (idx != newline_character and counter != 35):
        x,hidden = model(x, hidden)
        #print(x)
        counter+=1
        idx = torch.multinomial(x,1)
        #print(idx.item())
        generated_word+=ix_to_char[idx.item()]
    if counter ==35:
        generated_word+='\n'
    print(generated_word)

Here are the results of the first display:
epoch:1, loss:3.256033420562744
aaasaaauasaaasasauaaaaapsaaaasaaaaa

aaaaaaaaaaaaasaaaoaaaaaauaaaaaaaaaa

taaaauasaasaaaaasaaasaauaaaaaaaausa

uaasaaaaauaaaasasssaauaaaaasaaaaaaa

auaaaaaaaassasaaauaaaaaaaaasasaaaas

epoch:2, loss:3.199960231781006
aaasaaassussssusssussssssssssusssss

aasaaassssssssssssasusssissssssssss

sasaaassssuosasssssssssssssssssssss

aasassasassusssssssssussssssssssuss

oasaasassssssussssssssussssssssssss

epoch:3, loss:3.263746500015259
aaaaaaasaaaasaaaaasaaaasaaaaaaaaaaa

aaaaaaasaaaaaaaaaaaaaaaaaaaaaaaaaaa

aaaaaaaaaaaaaaaaaaaaaauaaaaaaaaaaas

aaaaaaaasaaaasraaaaaaaaaaaaaaaaaaaa

aaaaaaaaaaaaauusaaaaauaaaaaaaaaaaaa

It doesn't work but I have no idea how to fix the issue. 
With no training at all, the sampling function seems to work as the returned words seem complete random:
hbtpsbykkxvlah

ttiwlzxdxabzmbdvsapsnwwpaoiasotalft

My post may be a bit long, but so far I have no idea what is the issue of my program.
Manny thanks for your help.
","['recurrent-neural-networks', 'pytorch']","
Your forward function is not using the previous hidden state.
observe: you pass hidden but never use it.

def forward(self, input, hidden):
    layer = self.hiddenWx1(input)
    layer = self.hiddenWx2(layer)
    a_next = self.tanh(layer)
    z = self.z1(a_next)
    z = self.z2(z)
    y_next = self.softmax(z)
    return y_next,a_next

"
Can someone please help me validate my MDP?,"
Problem Statement : 
I have a system with four states - S1 through S4 where S1 is the beginning state and S4 is the end/terminal state. The next state is always better than the previous state i.e if the agent is at S2, it is in a slightly more desirable state than S1 and so on with S4 being the most desirable i.e terminal state. We have two different actions which can be performed on any of these states without restrictions. 
Our goal is to make the agent reach state S4 from S1 in the most optimal way i.e the route with maximum reward (or minimum cost). The model i have is a pretty uncertain one so i am guessing the agent must initially be given a lot of experience to make any sense of the environment. The MDP i have designed is shown below : 
MDP Formulation :

The MDP might a look a bit messy and complicated but it basically is just showing that any action (A1 or A2) can be taken at any state (except the terminal state S4). The probability with which the transition takes place from one state to the other and the associated rewards are given below. 
States : States S1 to S4. S4 is terminal state and S1 is the beginning state. S2 is a better state than S1 and S3 is a better state than S1 or S2 and S4 is the final state we expect the agent to end up in. 
Actions : Available actions are A1 and A2 which can be taken at any state (except of course the terminal state S4).  
State Transition Probability Matrix : One action taken at a particular state S can lead to any of the other available states. For ex. taking action A1 on S1 can lead the agent to S1 itself or S2 or S3 or even directly S4. Same goes for A2. So i have assumed an equal probability of 25% or 0.25 as the state transition probability. The state transition probability matrix is the same for actions A1 and A2. I have just mentioned it for one action but it is the same for the other action too. Below is the matrix I created - 

Reward Matrix : The reward function i have considered is a function of the action, current state and future state - R(A,S,S'). The desired route must go from S1 to S4. I have awarded positive rewards for actions that take the agent from S1 to S2 or S1 to S3 or S1 to S4 and similarly for states S2 and S3. A larger reward is given when the agent moves more than one step i.e S1 to S3 or S1 to S4. What is not desired is when the agent gets back to a previous state because of a action. So i have awarded negative rewards when the state goes back to a previous state. The reward matrix currently is the same for both the actions (meaning both A1 and A2 have same importance but it can be altered if A1/A2 is preferred over the other). Following is the reward matrix i created (same matrix for both the actions) - 

Policy, Value Functions and moving forward : 
Now that i have defined my states, actions, rewards, transition probabilities the next step I guess i need to take is to find the optimal policy. I do not have an optimal value function or policy. From lot of googling i did, I am guessing i should start with a random policy i.e both actions have equal probability of being taken at any given state -> compute the value function for each state -> compute the value functions iteratively until they converge -> then find the optimal policy from the optimal value functions. 
I am totally new to RL and all the above knowledge is from whatever i have gathered reading online. Can someone please validate my solution and MDP if I am going the right way? If the MDP i created will work ? 
Apologies for such a big write-up but i just wanted to clearly depict my problem statement and solution. If the MDP is ok then can someone also help me with how can the value function iteratively converge to an optimal value? I have seen lot of examples which are deterministic but none for stochastic/random processes like mine. 
Any help/pointers on this would be greatly appreciated. 
Thank you in advance  
","['reinforcement-learning', 'rewards', 'policies', 'markov-decision-process']",
Should I use single or double view for gender recognition?,"
My project requires gender recognition of people shown on the given images, with more than one person per image. However, these people can be positioned in frontal or side view(passing by perpendicularly, no face visible). On the pictures there will be entire bodies shown, not only the faces. My idea is to firstly use object detection to point where people would be and next use CNNs to recognize gender of each person. 
My question is: should I use one object detection algorithm for both frontal and side views of a person and then classify them with one CNN, or should I use object detection to separately find people positioned in frontal and side manner and use two different CNNs, one for classification of frontal views and one for side views?
I am asking this because I think it might be easier for one NN to classify only one view at a time, because side view might have different features than frontal, and mixing this features might be confusing for a network. However I am not really sure. If something is unclear, please let me know.
[EDIT] Since problem might be hard to understand only by reading, I made some illustrations. Basically I wonder if using second option can help in achieveing better accuracy for the subtle differences like those in gender recognition, especially when face is not visible:

Single detection and classification:

Two different classifiers:


Img Source
","['convolutional-neural-networks', 'classification', 'tensorflow', 'object-detection']","
I think your fundamental question is: can convolutional neural networks generalize to objects independent of the view?
The answer is mostly yes (given the dataset contains multiple views of objects). This is evident by looking at the results of various challenges: e.g. COCO object detection challenge results on youtube. You can see that no matter what the view is on the car, pedestrians, the detector is not biased towards any specific viewpoint.
Therefore, one can assume that you can build only one network to perform object detection and another network to perform classification. 
If you really want to go even further:
- you can make a small change to the architecture of your detector (I guess you might be using something like SSD, YOLO or Faster-RCNN), in which you make gender classification for every bounding box prediction. If you think about it, it is intuitive because the detector is already doing classification (there is softmax + cross-entropy loss usually), you can just add another term in its tensor output and modify the loss. That way you don't even need another network! It would be much faster and simpler.
- you can predict the pose estimation of the object (and corresponding normals) with respect to the camera to capture the best viewpoint to perform classification.
"
Should the biases be zero or randomly initialised?,"
I'm initialising DNN of shape [2 inputs, 2 hiddens, 1 output] with these weights and biases:
#hidden layer
weight1= tf.Variable(tf.random_uniform([2,2], -1, 1), 
         name=""layer1"");
bias1 = tf.Variable(tf.zeros([2]), name=""bias1"");

#output layer
weight2 = tf.Variable(tf.random_uniform([2,1], -1, 1), 
          name=""layer2"");
bias2 = tf.Variable(tf.zeros([1]), name=""bias2"");

That's what I followed some online article, however, I wonder what if I initialise bias values using tf.random_uniform instead of tf.zeros? Should I choose zero biases or random biases generically?
","['neural-networks', 'machine-learning', 'deep-learning', 'tensorflow', 'random-variable']",
Why not use the MSE instead of the current logistic regression?,"
When watching the machine learning course on Coursera by Andrew Ng, in the logistic regression week, the cost function was a bit more complex than the one for linear regression, but definitely not that hard. 
But it got me thinking, why not use the same cost function for logistic regression? 
So, the cost function would be $\frac{1}{2m} \sum_{i}^m|h(x_i) - y_i|^2$, where $h(x_i)$ is our hypothesis $\text{function}(\text{sigmoid}(X * \theta))$, $m$ is the number of training examples and $x_i$ and $y_i$ are our $ith$ training example?
","['classification', 'logistic-regression']","
I mean you technically could (it's not going to break or something) however, cross entropy is much better suited for classification as it penalizes for misclassification errors: have a look at the function: when you are wrong the loss goes to infinity:  
you are either from one class or another. MSE is designed for regression where you have nuance: you get close to target is sometimes good enough. You should try both and you will see the performance will be much better for the cross entropy.
"
Small Machine Translation Model,"
I would like to perform Machine Translation in tensorflow.js, in the browser. The issue is that state-of-the-art models have many gigabytes (fairseq ensemble), and translation is slow.
Do you know some good small models for Machine Translation? Probably something below the 50 million parameter threshold. 
I found some older papers with models below 100 MB for training on small datasets (IWSLT'15 English-Vietnamese attention-based models), but these are probably superseded.
",['machine-translation'],
Finding optimal Value function and Policy for an MDP,"
I am solving an RL MDP problem which is model based. I have an MDP which has four possible states S1-S4 and four different actions A1-A4, with S4 being terminal state and S1 is the beginning state. There is equal probability of applying any of the available actions on S1. The goal of my problem is to get from S1 to S4 with the maximum possible reward. I have two questions in this regard - 

Will this be a valid MDP model if I have this rule for my model - If i perform action a1 on S1 and next state is S2 then the set of available valid actions on S2 will be only a2,a3 and a4. If i apply any of the actions a2,a3 or a4 on S2 and it takes me to S3 then now i am left with a set of two valid actions for S3(except the one that was taken on s2). Can i do this? Because in my problem an action once taken does not require to be taken again later. 
I am confused about finding the optimal value and policy for my MDP. Both the optimal value and policy functions are not known to me. The objective is to find the optimal policy for my MDP which would get me from S1 to S4 with max reward. Any action taken on a particular state can lead to any other states (i.e there is a uniform equal state transition probability of 25% in my case for all states except S4 since it's terminal state). How can i approach this problem? After a lot of google search I vaguely understood that I must start with choosing a random policy (equal probability of taking any valid action) -> find value functions for each state -> iteratively compute V until it converges and then from these value functions I need to compute the optimal policy. Those solutions mostly use the Bellman Equations. Can someone please elaborate on how i can do this? Or if there is any other method to do it?

Thank you in advance
","['reinforcement-learning', 'rewards', 'policies', 'markov-decision-process']",
What are some examples of LSTM architectures?,"
I've been doing some class assignments recently on building various neural networks. For convolutional networks, there are several well-known architectures such as LeNet, VGG etc. Such ""classic"" models are frequently referenced as starting points when building new CNNs. 
Are there similar examples for RNN/LSTM networks? All I've found so far are articles and slides explaining recurrent neurons, LSTM layers, and the math behind them, but no well-known examples of entire multi-layered network architectures, unlike CNNs which seem to have in abundance.
","['neural-networks', 'recurrent-neural-networks', 'long-short-term-memory']","
In the paper, LSTM: A Search Space Odyssey (2017), by Klaus Greff et al., eight LSTM variants on three representative tasks (speech recognition, handwriting recognition, and polyphonic music modeling) are compared. 
The compared variants are 

Vanilla LSTM features three gates (input, forget, output), block input, a single cell, an output activation function, and peephole connections (connections from the cell to the gates). The output of the block is recurrently connected back to the block input and all of the gates. The vanilla LSTM is trained using gradient descent and back-propagation through time (BPTT). The original LSTM (which is not the vanilla LSTM) does not contain, for example, the forget gate or the peephole connections (but the cell possesses a constant error carousel, a constant weight of $1$). 
LSTM trained based on the decoupled extended Kalman filtering (DEKF-LSTM), which enables the LSTM to be trained on some pathological cases at the cost of high computational complexity.
Vanilla LSTM trained with an evolution-based method (called evolino), instead of BPTT.
LSTM block architectures evolved with a multi-objective evolutionary algorithm, so that to maximize fitness on context-sensitive grammar.
LSTM architectures for large scale acoustic modeling, which introduces a linear projection layer that projects the output of the LSTM layer down before recurrent and forward connections in order to reduce the number of parameters for LSTM networks with many blocks.
An LSTM architecture with a trainable scaling parameter for the slope of the gate activation functions, which improves the performance of LSTM on an offline handwriting recognition dataset.
Dynamic Cortex Memory, an LSTM composed of recurrent connections between the gates of a single block, but not between different blocks, which improves the convergence speed of LSTM.
Gated Recurrent Unit (GRU), which simplifies the architecture of the LSTM by combining the input and forget gate into an update gate.

There are other related neural network architectures, such as the neural Turing machine (NTM) or differentiable neural computer (DNC). In general, there are several architectures that use LSTM blocks, even though they are not just recurrent neural networks. Other examples are the neural programmer-interpreter (NPI) or the meta-controller.
"
"Class imbalance and ""all zeros"" one-hot encoding?","
I tried this example for a multi class classifier, but when looking at the data I realized two things:

There are many examples of ""all zeros"" vectors, that is, messages that don't belong in any classification.
These all-zeros are actually the majority, by far.

Is it valid to have an all-zeros output for a certain input? I would guess a Sigmoid activation would have no problems with this, by simply not trying to force a one out of all the ""near zero"" outputs.
But I also think an ""accuracy"" metric will be skewed too optimistically: if all outputs are zero 90% of the time, the network will quickly overfit to always output 0 all the time, and get 90% score.
",['supervised-learning'],
Accuracy too high too fast? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I have a simple text classifier, with the following structure:
    input = keras.layers.Input(shape=(len(train_x[0]),))

    x=keras.layers.Dense(500, activation='relu')(input)
    x=keras.layers.Dropout(0.5)(x)
    x=keras.layers.Dense(250, activation='relu')(x)
    x=keras.layers.Dropout(0.5)(x)
    preds = keras.layers.Dense(len(train_y[0]), activation=""sigmoid"")(x)

    model = keras.Model(input, preds)

When training it with 300,000 samples, with a batch size of 500, I get an accuracy value of .95 and loss of .22 in the first iteration, and the subsequent iterations are .96 and .11.
Why does the accuracy grow so quickly, and then just stop growing?
","['training', 'tensorflow', 'keras']","
As you have trained your model in batch_size of 500. Weights has been updated for each batch therefore 600 times(300000/500) by the end of one epoch.
So, Your model generalized well. Check the predictions. If Predictions are well. Your model is ready.
"
Why do you need to retrain GPT-2?,"
I'm following this tutorial, and I wonder why is there a train-step - why is it necessary? I thought the whole idea of GPT-2 is that you do not need to train it on specific text domain, as it's already pre-trained on a large amount of data.
",['natural-language-processing'],"
I've come to learn about GPT-2 through Robert Miles AI safety Youtube channel and intend to look into it in more detail.
From my current understanding, GPT-2 is pre-trained to ""understand"" ""natural"" language (for any definition of the words in quotes). However you would want it to not only understand general text but generate text similar to some specific ""genre"", e.g. scientific articles, youtube comments, twitter messages, you name it.
So using its pre-trained understanding, it analyzes the structure of sample texts and replicates this structure.
For scientific articles this structure could be:

Abstract
Context of research topic
Introduction of researchers
Explanation of methods/experiments/discoveries
Results and interpretation
Future research and application

For Youtube comments the structure is probably more chaotic but could include a vague reference to former comments, insults, nonsensical bar-grade philosophy, internet slang and smileys.
TL;DR: The domain specific text is only used to tell GPT-2 what you're looking for. You basically hand it context to work with, instead of prompting ""Say something clever"" (my least favorite line at parties, when I've been introduced as clever).
P.S.: Take this with a grain of salt. It's 90% conjecture from incomplete information.
"
How to update Loss Function parameter after compilation,"
I used following custom loss function.
def custom_loss(epo):

  def loss(y_true,y_pred):
      m=K.binary_crossentropy(y_true, y_pred)
      x=math.log10(epo)
      y=x*x
      y=(math.sqrt(y)/100)
      l=(m*(y))

      return K.mean(l, axis=-1)
  return loss

and this is my discriminator model
def Discriminator():


  inputs = Input(shape=img_shape)


  x=Conv2D(32, kernel_size=3, strides=2, padding=""same"")(inputs)
  x=LeakyReLU(alpha=0.2)(x)
  x=Dropout(0.25)(x, training=True)

  x=Conv2D(64, kernel_size=3, strides=2, padding=""same"")(x)
  x=ZeroPadding2D(padding=((0, 1), (0, 1)))(x)
  x=BatchNormalization(momentum=0.8)(x)
  x=LeakyReLU(alpha=0.2)(x)

  x=Dropout(0.25)(x, training=True)
  x=Conv2D(128, kernel_size=3, strides=2, padding=""same"")(x)
  x=BatchNormalization(momentum=0.8)(x)
  x=LeakyReLU(alpha=0.2)(x)

  x=Dropout(0.25)(x, training=True)
  x=Conv2D(256, kernel_size=3, strides=1, padding=""same"")(x)
  x=BatchNormalization(momentum=0.8)(x)
  x=LeakyReLU(alpha=0.2)(x)

  x=Dropout(0.25)(x, training=True)
  x=Flatten()(x)
  outputs=Dense(1, activation='sigmoid')(x)
  model = Model(inputs, outputs)
  #model.summary()
  img = Input(shape=img_shape)
  validity = model(img)
  return Model(img, validity)

and initialize discriminator here
D = Discriminator()
epoch=0
D.compile(loss=custom_loss(epoch), optimizer=optimizer, metrics= 
['accuracy'])
G = Generator()
z = Input(shape=(100,))
img = G(z)
D.trainable = False
valid = D(img)

i want to update epo value of loss function after each epoch in the following code
for epoch in range(epochs):

  for batch in range(batches):
      ............
     d_loss_real = D.train_on_batch(imgs, valid)
     d_loss_fake = D.train_on_batch(gen_batch, fake)
     d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
     g_loss = combined.train_on_batch(noise_batch, valid)

Are there any way for updating loss function without effecting training after compiling the model?
","['neural-networks', 'machine-learning', 'deep-learning']",
AlphaZero value at root node not being affected by training,"
I have written my own AlphaZero implementation and started training it recently.
Problem is, I am 99% sure there is a mistake and I do not know how to tackle this, since I cannot explain it. I am new too AI so my own go at debugging this wasn't quite succesful.  
Input to my NN: A game state, represented by the board and position of the stones.
Output of my NN: a policy vector P and a scalar v(so an array and a number).
During self-play, training examples for each move are generated. These are later used to fit the network.  
After having trained a bit, I can see both policy and value loss decreasing, which is good.
But for the very first game state (empty board) my prediction v of winning the game always stays at 0.  This is very concerning, since the game I am training on is Connect4. Connect4 is a solved game and in the long run, the value for v should be 1(100% win chance).
So, any ideas what I can do? I mean I could post the code here, but that is quite a lot, I don't know if any of you are willing to read through it.
To show you what I mean, I'll show you the output of one of my test cases:
def test_p_v():
    game = connect4.Connect4()
    nnetwrapper = neuralnetwrapper.NNetWrapper(game, args)
    trainer = trainingonly1NN.Training(game = game, nnet= nnetwrapper, args = args)

    trainer.nnet.load_checkpoint(folder = './NNmodels/', filename='best.pth.tar')

    m = mctsnn.MCTS(nnet = nnetwrapper, args=args)

    m.root.expand()

    for c in m.root.children:
        print(""P: {}  v: {}"".format(c.state.P, c.state.v))

    print(""Root MCTS: P: {}   v: {}"".format(m.root.state.P, m.root.state.v))

results in:
P: [0.1436838  0.13809082 0.14174062 0.18597336 0.11126296 0.120884
 0.15836443]  v: [-0.14345692]
P: [0.14202288 0.13772981 0.14302546 0.1945151  0.11690026 0.1178078
 0.1479987 ]  v: [0.4222183]
P: [0.1447647  0.13066562 0.14334281 0.18055147 0.13374692 0.12126701
 0.1456615 ]  v: [-0.5827425]
P: [0.15192215 0.14221476 0.1443521  0.16634388 0.12634312 0.12711576
 0.14170831]  v: [-0.0229549]
P: [0.1456457  0.136381   0.13940862 0.17145196 0.12714048 0.12233274
 0.15763956]  v: [-0.02743456]
P: [0.15353182 0.13510287 0.1433772  0.16371183 0.12161442 0.1228981
 0.15976372]  v: [0.37902302]
P: [0.14321715 0.13596673 0.13836266 0.18927328 0.11999774 0.12481775
 0.1483647 ]  v: [-0.521353]
Root MCTS: P: [0.14296353 0.13863131 0.1358864  0.18102945 0.10981551 0.12779148
 0.16388236]   v: [0.]

So as you can see, for every different state, there are different P-values and different v-values, which makes sense.
It also makes sense for my ROOT Node to have the highest value in P at position 3, since this refers to the middle column.
But the v in my root Node is 0.  This is alarming and I have no idea what do from here on.
I also checked some of the training examples passed to my neural network to learn, they look like this (board, P, Actual game result):
[[array([[0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0]]), [0.13, 0.13, 0.15, 0.13, 0.18, 0.13, 0.15], 1]

whereas the very last number (1) is the v-value my network is supposed to fit! So it often even is 1!
But I fear that since this is the very root of all, the game result for the following notes is -1 and 1  changing every ""step"", so the average probably is 0 which is quite logical. Don't know how to express this, I fear it is trying to average the v mean of all states, instead of just training the v for one state.
","['neural-networks', 'ai-design', 'alphazero', 'architecture']",
Benchmarks for reinforcement learning in discrete MDPs,"
To compare the performance of various algorithms for perfect information games, reasonable benchmarks include reversi and m,n,k-games (generalized tic-tac-toe). For imperfect information games, something like simplified poker is a reasonable benchmark.
What are some reasonable benchmarks to compare the performance of various algorithms for reinforcement learning in discrete MDPs? Instead of using a random environment from the space of all possible discrete MDPs on $n$ states and $k$ actions, are there subsets of such a space with more structure that are more reflective of ""real-world"" environments? An example of this might be so-called gridworld (i.e. maze-like) environments.
This is a related question, though I'm looking for specific examples of MDPs (with specified transitions and rewards) rather than general areas where MDPs can be applied.
Edit: Some example MDPs are found in section 5.1 (Standard Domains) of Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search (2012) by Guez et al.:

The Double-loop domain is a 9-state deterministic MDP with 2actions,
  1000 steps are executed in this domain. Grid5 is a 55 grid with no
  reward anywhere except for a reward state opposite to the reset state.
  Actions with cardinal directions are executed with small probability
  of failure for 1000 steps. Grid10 is a 1010 grid designed like Grid5.
  We collect 2000 steps in this domain. Deardens Maze is a 264-states
  maze with 3 flags to collect. A special reward state gives the number
  of flags collected since the last visit as reward, 20000 steps are
  executed in this domain.

","['reinforcement-learning', 'environment', 'markov-decision-process', 'benchmarks']",
How would an AI learn idiomatic phrases in a natural language?,"
After an AI goes through the process described in How would an AI learn language?, an AI knows the grammar of a language through the process of grammar induction. They can speak the language, but they have learned formal grammar. But most conversations today, even formal ones, use idiomatic phrases. Would it be possible for an AI to be given a set of idioms, for example, 

Immer mit der Ruhe

Which, in German, means 'take it easy' but an AI of grammar induction, if told to translate 'take it easy' to German, would not think of this. And if asked to translate this, it would output

Always with the quiet

So, it is possible to teach an AI to use idiomatic phrases to keep up with the culture of humans?
","['machine-learning', 'natural-language-processing']","
Do you have access to parallel corpora in source and target language that translates idioms correctly? Neural machine translation.  (NMT) should handle this. NMT uses deep learning to match sequences/pairs of words in one language to another and is now the state of the art method for translation AI.
I don't think an AI knows grammar of a language. A translating AI knows patterns but not necessarily grammar in the sense that we learn in school as children. Here's a potential approach that should work give a large enough corpora with examples of idioms - github.com/facebookresearch/MUSE
"
Which predictive algorithm is most appropriate for a proceeding situation?,"
I'm new to artificial intelligence. I am looking for the most appropriate AI solution for my application, which is developing an algorithm to predict a proceeding situation (edited: I want my algorithm to predict a situation or more than one to happen at a predefined moment) and, at the same time, to learn from the iterative stages of my application.
Any suggestions? Any help? Any proposals?
","['ai-design', 'prediction']","
One approach would be to use a Sequence Processing Neural architecture - one option is a recurrent network. These were specifically designed with your intent in mind.
They can consume sequences of data and learn from these in order to predict subsequent time-steps. Though these will require you to understand best practices for implementation.
If your task isn't too complex you could make use of a forward algorithm or some variant. Many of these models make use of simplifying assumptions - if your use case cannot admit these you'll have to go for something more advanced.
If your prediction task is complex and requires heavy use of long term dependencies you may have to go for a more cutting edge architecture like a transformer - but I heard that these are difficult to implement and most definitely require familiarity with deep learning systems. 
Finally, if you need a system that can understand data and dependencies across very large spans of time then you may have to wait for the science to progress.
I hope this helps.
"
What does an oscillating validation error curve represent?,"
I have been training my CNN for a bit now and, while both the training loss and the training error curves are going down during training, both my validation loss and my validation error curves are kind of zig-zagging and oscillating along the epochs. What does this represent?
","['machine-learning', 'deep-learning', 'convolutional-neural-networks']","
Without more information the best diagnostic is:
You potentially have a bug in your code.
Justification
Neural network systems are capable of failing silently. The system can still appear to ""learn"" even in the presence of said bug - a while back I had a very similar issue with a toy CNN project. My network could get 99% accuracy on the training set but always achieved 8-13% on the validation set. This looked like overfit but none of the methods solved the issue. Finally, I found that I wasn't feeding the data correctly into the network during train time but I was feeding the data correctly during validation time.
Conclusion
Provide the following for better diagnostic:

Is this a custom CNN you hardcoded?
are you using python?
what's the objective function?
can you provide the loss curve for the training set?
show us some code too (this will be very helpful for working toward a solution)

I hope this helps, and best of luck!
"
When to use RMSE as opposed to MSE and vice versa?,"
I understand that RMSE is just the square root of MSE. Generally, as far as I have seen, people seem to use MSE as a loss function and RMSE for evaluation purposes, since it exactly gives you the error as a distance in the Euclidean space.
What could be a major difference between using MSE and RMSE when used as loss functions for training? 
I'm curious because good frameworks like PyTorch, Keras, etc. don't provide RMSE loss functions out of the box. Is it some kind of standard convention? If so, why?
Also, I'm aware of the difference that MSE magnifies the errors with magnitude>1 and shrinks the errors with magnitude<1 (on a quadratic scale), which RMSE doesn't do.
","['machine-learning', 'deep-learning', 'training', 'objective-functions']","
Answers in the comments are decent, particularly DuttA.
DuttA gives these, approximately

ease of derivative
Don't have to worry about ~0 in denominator causing huge gradient
But to me the most important is mathematical convenience, someone
might easily make the mistake of RMSE is just equal the difference
yy instead of root of mean square of yy. The answer this might
start depending on conventions.
In maths (Don't know the reason and might be inaccurate) we mainly
work with variances instead of standard deviation.

Here are my reasons for using the MSE instead of RMSE:

Doesn't have the sqrt operations, so it computes faster
the square root isn't easy, its Newtons method, so it could be a
dozens steps per iteration
MSE has all the information of RMSE, there is 1-to-1 mapping, so no loss
the storage of the square root typically doesn't save any memory in
IEEE-784 and compute vs. memory is big thing in complexity
tools like gradient boosted machine can ""recycle"" the squared error
computation for speedup and working on O(n) complexity
there is hidden scaling and regularization because many gpu hardware
elements are fundamentally 8-bit, so if you can make your code more
8-bit in its guts then you don't have as much in the back-conversion
and it runs a lot faster

"
Why do we need explainable AI?,"
If the original purpose for developing AI was to help humans in some tasks and that purpose still holds, why should we care about its explainability? For example, in deep learning, as long as the intelligence helps us to the best of their abilities and carefully arrives at its decisions, why would we need to know how its intelligence works?
","['philosophy', 'social', 'explainable-ai']","

Why do we need explainable AI?
  ... why we need to know ""how does its intelligence work?""

Because anyone with access to the equipment, enough skill, and enough time, can force the system to make a decision that is unexpected. The owner of the equipment, or 3rd parties, relying on the decision without an explanation as to why it is correct would be at a disadvantage.
Examples - Someone might discover:

People whom are named John Smith and request heart surgery on: Tuesday mornings, Wednesday afternoons, or Fridays on odd days and months have a 90% chance of moving to the front of the line.
Couples whom have the male's last name an odd letter in the first half of the alphabet and apply for a loan with a spouse whose first name begins with a letter from the beginning of the alphabet are 40% more likely to receive the loan if they have fewer than 5 bad entries in their credit history.
etc.

Notice that the above examples ought not to be determining factors in regards to the question being asked, yet it's possible for an adversary (with their own equipment, or knowledge of the algorithm) to exploit it.
Source papers:

""AdvHat: Real-world adversarial attack on ArcFace Face ID system"" (Aug 23 2019) by Stepan Komkov and Aleksandr Petiushko

Creating a sticker and placing it on your hat fools facial recognition system.

""Defending against Adversarial Attacks through Resilient Feature Regeneration"" (Jun 8 2019), by Tejas Borkar, Felix Heide, and Lina Karam


""Deep neural network (DNN) predictions have been shown to be vulnerable to carefully crafted adversarial perturbations. Specifically, so-called universal adversarial perturbations are image-agnostic perturbations that can be added to any image and can fool a target network into making erroneous predictions. Departing from existing adversarial defense strategies, which work in the image domain, we present a novel defense which operates in the DNN feature domain and effectively defends against such universal adversarial attacks. Our approach identifies pre-trained convolutional features that are most vulnerable to adversarial noise and deploys defender units which transform (regenerate) these DNN filter activations into noise-resilient features, guarding against unseen adversarial perturbations."".


""One pixel attack for fooling deep neural networks"" (May 3 2019), by Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi

Altering one pixel can cause these errors:



Fig. 1. One-pixel attacks created with the proposed algorithm that successfully fooled three types of DNNs trained on CIFAR-10 dataset: The All convolutional network (AllConv), Network in network (NiN) and VGG. The original class labels are in black color while the target class labels and the corresponding confidence are given below.


Fig. 2. One-pixel attacks on ImageNet dataset where the modified pixels are highlighted with red circles. The original class labels are in black color while the target class labels and their corresponding confidence are given below.


Without an explanation as to how and why a decision is arrived at the decision can't be absolutely relied upon.
"
How does an LSTM output the correct dimensions for classes?,"
Take the below LSTM:
input: 5x1 matrix
hidden units: 256
output size (aka classes, 1 hot vector): 10x1 matrix

It is my understanding that an LSTM of this size will do the following:
$w_x$ = weight matrix at $x$
$b_x$ = bias matrix at $x$
activation_gate = tanh($w_1$ $\cdot$ input + $w_2$ $\cdot$ prev_output + $b_1$)
input_gate = sigmoid($w_3$ $\cdot$ input + $w_4$ $\cdot$ prev_output + $b_2$)
forget_gate = sigmoid($w_5$ $\cdot$ input + $w_6$ $\cdot$ prev_output + $b_3$)
output_gate = sigmoid($w_7$ $\cdot$ input + $w_8$ $\cdot$ prev_output + $b_4$)
The size of the output of each gate should be equal to the number of hidden units, ie, 256. The problem arrises when trying to convert to the correct final output size, of 10. If the forget gate outputs 256, then it is summed with the element wise product of the activation and input gate to find the new state, this will result in a hidden state of size 256. (Also in all my research I have not found anywhere whether this addition is actually addition, or simply appending the two matrices).
So if I have a hidden state of 256, and the output gate outputs 256, doing an element wise product of these two results in, surprise surprise, 256, not 10. If I instead ensure the output gate outputs a size of 10, this no longer works with the hidden state in an element wise product.
How is this handled? I can come up with many ways of doing it myself, but I want an identical replica of the basic LSTM unit, as I have some theories I want to test, and if it is even the slightest bit different it would make the research invalid.
",['long-short-term-memory'],
How can I determine the mathematical relation between the input and output variables?,"
I would like to take in some input values for $n$ variables, say $R$, $B$, and $G$. Let $Y$ denote the response variable of these $n$ inputs (in this example, we have $3$ inputs). Other than these, I would like to use a reference/target value to compare the results. 
Now, suppose the relation between the inputs ($R$, $B$ and $G$) with the output $Y$ is (let's say):
$$Y = R + B + G$$
But the system/machine has no knowledge of this relation. It can only read its inputs, $R$, $B$ and $G$, and the output, $Y$. Also, the system is provided with the reference value, say, $\text{REF} = 30$ (suppose). 
The aim of the machine is to find this relation between its inputs and output(s). For this, I have come across some quite useful material online like this forum query and Approximation by Superpositions of a Sigmoidal Function by G. Cybenko and felt that it were possible. Also, I doubt that Polynomial Regression may be helpful as suggested Here.
One vague approach that comes to my mind is to use a truth table like approach to somehow deduce the effect of the inputs on the output and hence, get a function for it. But neither am I sure how to proceed with it, nor do I trust its credibility. 
Is there any alternative/already existing method to accomplish this?
","['machine-learning', 'math']","
There are always a large number of possible functions that can produce a given set of input-output values. The challenge is to find a simplest function (according to whatever criteria you choose) to produce those values.
One approach is to write a general function of the input variables, comprising terms of all order in R, G, and B, with a coefficient for each term, then search for values of the coefficients that A) reproduce the known input-output values accurately and B) leave the largest number of coefficients equal to zero.  
Several different algorithms can be used to do the search efficiently.  My choice would be a genetic algorithm to seek the minimum of the RMS difference between the produced and known I-O values, summed with the product of a gradually increasing parameter and the number of nonzero coefficients.
"
How does one create a non-classifying CNN in order to gain information from images?,"
How do I program a neural network such that, when an image is inputted, the output is a numerical value that is not the probability of the image being a certain class? In other words, a CNN that doesn't classify. For example, when an input of an image of a chair is given, the model should not give the chance that the image is a chair but rather give the predicted age of the chair, the predicted price of the chair, etc. I'm currently not sure how to program a neural net like this.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks']","
This can be thought of as a loss function design problem. If you optimize your network weights for something like multi-class classification, then expect your network to learn weights for this task (You will use cross entropy loss for this task). If you optimize your network to output a single value at the last layer and treat it as a regression problem for age prediction, then your network can learn weights for this particular task (You may use something like Mean Square Error loss here).
Let me give you a weak guidline on how to do this. Suppose say your input is images of chairs and you want to predict their age using a pretrained resnet, this is how you may do it in pytorch.
Definition
X: Input pictures
Y: List of ground truth values which is age here. i.e. : [1.4, 2.5, 2.2, ....]  
Modify your neural network to give one output at the last layer
model = torchvision.models.resnet18(pretrained=True)
num_ftrs = model.fc.in_features
model.fc = torch.nn.Linear(num_ftrs, 1)  
Design your loss function appropriately. You can treat age prediction as a standard regression problem. Of course, there are better loss designs here.
criterion =  torch.nn.MSELoss()
So use this loss during training
loss = criterion(output, target) where output is your neural network prediction and target is your ground truth values.
This is how you can modify an existing architecture for your task. Hope it helps.
"
Can I have different rewards for a single action based on which state it transitions to?,"
I am working on an MDP where there are four states and ten actions. I am supposed to derive the optimal policy to reach the desired state. At any state, a particular action can take you to any of the other states. 
For ex. If we begin with state S1 -> performing action A1 on S1 can take you to S2 or S3 or S4 or just stay in the same state S1. Similarly for other actions. 
My question is - is it mandatory to have only a single reward value for a single action A? Or is it possible to give a reward of 10 if action a1 on state s1 takes you to s2, give a reward of 50 if action a1 on state s1 takes you to s3, give a reward of 100 if action a1 on state s1 takes you to s4 which is the terminal state or give zero reward if that action results in the state being unchanged. 
Can I do this?? 
Because in my case every state is better than its previous state. i.e S2 is better than S1, S3 is better than S2 and so on. So if an action on S1 is directly taking u to S4 which is the final state i would like to award it the maximal reward.    
","['reinforcement-learning', 'markov-decision-process', 'rewards', 'value-iteration']","
The reward function can be a function of the current state, current action, and next state: $R(s_t, a_t, s_{t+1})$. It's valid to use the Bellman operator in this setting because it's still a contraction and will yield the optimal value function.
NOTE: I'm assuming that you will be solving the MDP with the Bellman equation.
"
Can ConvLSTMs outuput images?,"
I am working on an image to image regression task which requires me to develop a deep learning model that takes in a sequence of 5 images and return another image. The sequence of 5 images and the output images are conceptually and temporally related. In fact, the 5 images in the sequence each correspond to timestep in a simulation and the output, the one I am trying to predict, corresponds eventually to the 6th timestep of that sequence.
For now, I have been training a simple regression-type CNN model which takes in the sequence of 5 images stored in a list and outputs an image corresponding to the next timestep in the simulation. This does work with a small and rather simple dataset (13000 images) but works a bit worse on a more diverse and larger dataset (102000 images). 
For this reason, I have been researching a bit now in order to find a better way to carry out this task and I found the idea of ConvLSTMs. However, I have seen these applied to the prediction of feature and the output of a sentence describing that image. What I wanted to know is whether ConvLSTMs can also output images, but more importantly if they can be applied to my case. If not, what other types of deep learning network can be suitable for this task?
Thanks in advance!
","['deep-learning', 'python']","
If you only need 5 frames to predict the next frame, then I'd recommend a U-Net architecture, wich is basically a CNN encoder/decoder network in which the decoder uses the intermediate features produced in the encoder as well as its own features to produce an output image. Also, in additional to using a conventional L2 loss for the output image, you can always add an additional GAN loss to make the image look more realistic.
If using a longer history of frames can help, then I recommend taking a look at ""Recurrent Environment Simulators"" and combining it with the ideas above.
Hope it helps!
"
Are there any ways to model markov chains from time series data?,"
I want to make a thing that produces a stochastic process from time series data.
The time series data is recorded every hour over the year, which means 24-hour of patterns exist for 365 days.
What I want to do is something like below:

Fit a probability distribution using data for each hour so that I can sample the most probable value for this hour.
Repeat it for 24 hours to generate a pattern of a day.

BUT! I want the sampling to be done considering previous values rather than being done in an independent manner.
For example, I want to sample from  or just  rather than  when  refers to a specific hour.
What I came up with was the Markov chain, but I couldn't find any reference or materials on how to model it from real data. 
Could anyone give me a comment for this issue?
","['markov-chain', 'time-series']",
In how few updates can a multi layer neural net be trained?,"
A single iteration of gradient descent can be parallelised across many worker nodes. We simple split the training set across the worker nodes, pass the parameters to each worker, each worker computes gradients for their subset of the training set, and then passes it back to the master to be averaged. With some effort, we can even use model parallelism.
However, stochastic gradient descent is an inherently serial proces. Each update must be performed sequentially. Each iteration, we must perform a broadcast and gather of all parameters. This is bad for performance. Ultimately, number of updates is the limiting factor of deep model training speed.
Why must we perform many updates? 
With how few updates can we achieve good accuracy?
What factors affect the minimum number of updates requires to reach some accuracy?
","['deep-neural-networks', 'distributed-computing']","
In Don't Decay the Learning Rate, Increase the Batch Size, Smith et al. train ResNet-50 on ImageNet to 76.1% with only 2500 updates. Has anyone done it in less?
In The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Sankararaman et al. present the concept of gradient confusion which slows convergence, and show that depth increases gradient confusion while overparameterization on width and skip connections reduces gradient confusion.
In Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash, Mikami et al. train ResNet-50 on ImageNet to 75.29% with some number of updates, but I can't do the math to compute out how many updates they must have used.
To summarise:

Larger mini-batches help, but give diminishing returns and cause other issues.
Deeper models tend to require more updates.
Wider layers need fewer updates.
Skip connections, label smoothing, and batch norm might help.
The best I have found so far on ImageNet/ResNet-50 to >75% is 2500 updates.

"
Image to image regression in tensorflow,"
I am working on an image to image regression task which requires me to develop a deep learning model that takes in a sequence of 5 images and return another image. The sequence of 5 images and the output images are conceptually and temporally related. In fact, the 5 images in the sequence each correspond to timestep in a simulation and the output, the one I am trying to predict, corresponds eventually to the 6th timestep of that sequence.
For now, I have been training a simple regression-type CNN model which takes in the sequence of 5 images stored in a list and outputs an image corresponding to the next timestep in the simulation.
However, I have been researching a bit now in order to find a better way to carry out this task and I found the idea of ConvLSTMs. However, I have seen these applied to the prediction of feature and the output of a sentence describing that image. What I wanted to know is whether ConvLSTMs can also output images, but more importantly if they can be applied to my case. If not, what other types of deep learning network can be suitable for this task?
Thanks in advance !
","['deep-learning', 'python']",
Why do all nodes in a GP tree need to be the same type?,"
Context: I'm a complete beginner to evolutionary algorithms and genetic algorithms and programming. I'm currently taking a course about genetic algorithms and genetic programming.
One of the concepts introduced in the course is ""closure,"" the idea that - with an expression tree representing a genetic program that we're evolving - all nodes in the tree need to be the same type. As a practical example, the lecturer mentions that implementing greater_than(a, b) for two integers a and b can't return a boolean like true or false (it can return, say, 0 and 1 instead).
What he didn't explain is why the entire tree needs to match in all operators. It seems to me that this requirement would result in the entire tree (representing your evolved program) being composed of nodes that all return the same type (say, integer).
","['evolutionary-algorithms', 'genetic-programming', 'evolutionary-computation', 'tgp']","
In tree-based genetic programming (TGP), you have a tree that represents a program or a function. The nodes in this tree are functions, while the edges represent the interactions between these functions. The leaves of this tree are the inputs (or random numbers) that you pass to this function. The incoming edges into a node represent the inputs, while the outgoing edges represent the output of the associated function.
So, for example, consider the following function
\begin{align}
f(x) 
&= \sin(x^2) \\
&= \sin(y) 
\tag{1}\label{1},
\end{align}
where $y = g(x)$ (note that this is just a change of variable in order to illustrate the corresponding tree more clearly below!).
The function $f$ will be represented by the following tree
sin(y)
  |
 g(x)
  |
  x

So, how do we read this diagram? Essentially, we read it from the bottom-up. So, $x$ is first passed to $y = g(x) = x^2$, which then passed to $\sin(y)$. In this case, given that we are dealing with mathematical operations, you expect $x$ to be a number, because, otherwise, what would $x^2$ or $\sin(x^2)$ mean?
Let's now consider a function of 2 inputs.
\begin{align}
f(x, y) 
&= x + y
\tag{2}\label{2},
\end{align}
The corresponding tree would be
  +
 / \
x   y

In this case, you naturally expect that $x$ and $y$ are also numbers. However, this may not be actually the case. Let's say that you're evolving Python functions, then x + y is well defined even if x and y are strings, i.e. that would be a concatenation operation.
So, in this sense, we naturally expected the functions (the nodes in the tree) to get parameters/arguments with the right types, but, in some cases, more types are possible for the apparently same function.
So, in general, types don't necessarily need to be enforced! It depends on the implementation of TGP.
There's a specific approach to TGP where the idea is really to ensure type-safety, i.e. strongly-typed (tree-based) genetic programming, where functions can have a different number and type for their parameters and return values, but, in that case, only functions that are consistent with the signature of the function can be ""connected"" with that function in the tree.
In weakly-typed GP, the types are not checked, so you could end up evolving functions of the form $f(x) = \sin(x)$, where $x$ is a string. Of course, when you execute these programs/functions, the program may crash, but this is a different story, which you need to take care of (i.e. you need to ensure that your individuals can be evaluated), if you need this flexibility. This weakly-typed approach can also be viewed as a strongly-typed approach where all functions' parameters and return values have the same type.
DEAP, a well-known Python library for GAs and GP, provides both weakly and strongly-typed GP, so, if you are familiar with Python, you may want to start from it.
To conclude and answer your question more directly, it's not true that in TGP you need all functions to have the same type for all parameters and return values.
"
Feasibility of using machine learning to obtain self-consistent solutions,"
I am a physicist and I don't have much background on machine learning or deep learning except taking a couple of courses on statistics. In physics, we often simulate a model by means of two-way coupled systems where each system is described by a partial differential equation. The equations are generally unfolded in a numerical grid of interest and then solved iteratively until a self-consistent solution is obtained. 
A well-known example is the Schrdinger-Poisson solver. Here, for a given nano/atomic structure, we assume an initial electron density. Then we solve the Poisson equation for that electron density. The Poisson equation tells us the electrostatics (electric potential) of the structure. Given this information, we solve the Schrdinger equation for the structure which tells us about the energy levels of the electrons and their wave functions in the structure. But one would then find that this energy levels and wavefunctions correspond to a different electron density (than the one we initially guessed). So we iterate the process with the new electron density and follow the above mentioned procedures in a loop until a self-consistent solution is obtained.
Often times the iteration processes are computationally expensive and extremely time-consuming. 
My question is this: Would the use of deep learning algorithms offer any advantage in modeling such problems where iteration and self-consistency are involved? Are there any study/literature where researchers explored this avenue?   
","['neural-networks', 'deep-learning', 'reinforcement-learning']",
How to implement fisherface algorithm and how much time will it take?,"
I found on the web that fisherface is the best algorithm for face detection. Before investing deeply into it, I just want to know how hard is it to implement it and how much time will it take.
I am new to this website and I welcome any suggestions.
","['machine-learning', 'computer-vision', 'facial-recognition']",
Is unsupervised learning a branch of AI?,"
From Artificial Intelligence: A Modern Approach, a book by Stuart Russell and Peter Norvig, this is the definition of AI:

We define AI as the study of agents that receive percepts from the environment and perform actions. Each such agent implements a function that maps percept sequences to actions, and we cover different ways to represent these functions, such as reactive agents, real-time planners, and decision-theoretic systems. We explain the role of learning as extending the reach of the designer into unknown environments, and we show how that role constrains agent design, favoring explicit knowledge representation and reasoning.

Given the definition of AI above, is unsupervised learning (e.g. clustering) a branch of AI? I think the definition above is more suitable for supervised or reinforcement learning.
","['definitions', 'unsupervised-learning', 'clustering']",
Is a state that includes only the past n-step price records partially observable?,"
I'm currently working on a project to make an DQN agent that decides whether to charge or discharge an electric vehicle according to hourly changing price to sell or buy. The price pattern also varies from day to day. The goal of this work is to schedule the optimal charging/discharging actions so that it can save money.
The state contains past n-step price records, current energy level in battery, hour, etc., like below:
$$
s_t = \{ p_{t-5}, p_{t-4}, p_{t-3}, p_{t-2}, p_{t-1}, E_t, t \}
$$
What I'm wondering is whether this is a partial observable situation or not, because the agent can only observe past n-step prices rather than knowing every price at every time step.
Can anyone comment on this issue?
If this is the partial observable situation, is there any simple way to deal with it?
","['reinforcement-learning', 'definitions', 'dqn']",
Algorithm for seasonal trends,"
I have a very big table with lots of names and how much they are searched by date. 
I would like to find trending patterns. When does a name rise and when does it fall. Without knowing the name or the pattern before.
The rise could be during the seasons of the year but also during a week.
Like a 'warm hat' is trending in winter and falling in summer.
Or searches for a ""board game"" might rise on Sunday and decrease on Monday.
The table looks simplified like this:
winter gloves, 2014-01-01, 200
warm hat, 2014-01-01, 300
swimming short, 2014-01-01, 1
sunscreen, 2014-01-01, 2
....
winter gloves, 2014-07-01, 1
warm hat, 2014-07-01, 1
swimming short, 2014-07-01, 200
sunscreen, 2014-07-01, 300

Which algorithms should I have a look at? 
Thanks for any hint,
Joerg
",['data-mining'],"
As you are handling with time series data and you want to find trends; A good approach should be consider applying Holt-Winter's seasonal method. This algorithm handle seasonal, trend and smooth parameters. A good implementation of this kind of algorithm is Prophet by Facebook. You can code an exploratory analysis with this library and obtain trend, yearly seasonality, and weekly seasonality of the time series, among other components. Example:

"
Wasserstein GAN with non-negative weights in the critic,"
I want to train a WGAN where the convolution layers in the critic are only allowed to have non-negative weights (for a technical reason). The biases, nonetheless, can take both +/- values. There is no constraint on the generator weights.
I did a toy experiment on MNIST and observed that the performance is significantly worse than a regular WGAN.
What could be the reason? Can you suggest some architectural modifications so that the nonnegativity constraint doesn't severely impair the model capacity?
","['generative-adversarial-networks', 'convolutional-layers', 'constrained-optimization', 'wasserstein-gan']",
How can we log user-bot conversations using the Microsoft Bot Framework?,"
I am starting to create my first bot with Microsoft Bot Framework with the help of Azure, initially I want to know where all the conversations the user has with the bot are stored, so then get a log of all the conversations that have been held.
I already have some answers stored in knowledge bases using QnA Maker, for certain questions that you can answer, I want to know where the questions that were not answered or better that the bot could not answer are stored.
Currently, I am asking the users to write their question in a feedback form when they don't get a response from my bot. This is taking up my time and also annoys the user as they have to type more. I want my bot to collect these questions and store them in a database.
",['chat-bots'],
How can I cluster based on the complementary categories?,"
K-means tries to find centroid and then clusters around the centroids. But what if we want to cluster based on the complement?
For example, suppose we have a group of animals and we want to cluster Dogs, Cats, (Not Dogs and Not Cats). The 3rd category will not arise from mean clustering.
","['machine-learning', 'unsupervised-learning', 'clustering']","
Note: K-means does not assume an interpretation/label of the clusterings - in fact it is an unsupervised algorithm. The interpretations are a result of human analysis after running K-means.
For example, in the case of cats and dogs one would most definitely chose k = 2 - which provides an easy interpretation. However, what would it mean if we set k = 1000. We no longer have a ""clean"" interpretation of the centroids. 
Note: how I keep saying ""interpretation."" The algorithm simply assigns a data point to a cluster and calls it a day. Humans then look at the results and try to understand them with an interpretation. 
Continuing with the example where k = 2. One could easily interpret ""is cat"" as ""not dog"" and ""is dog"" as ""not cat."" The idea here is that the data is unlabeled beforehand and humans try to fathom the results retrospectively by assigning the resulting clusters with an understandable label.
I hope this clarifies the issue.
"
Why are model-based methods more sample efficient than model-free methods?,"
Why do model-based methods use fewer samples than model-free methods? Here, I'm specifically referring to model-based methods in which we have to learn a policy and model. I can only think of two reasons for this question:

We can potentially obtain more samples from the learned model, which may speed up the learning speed. 
Models allow us to predict the future states and run simulations. This may lead to more valuable transitions, whereby speeding up learning.

But I heavily doubt this is the whole story. Sincerely, I hope someone could share a more detailed explanation for this question.
","['reinforcement-learning', 'comparison', 'model-based-methods', 'model-free-methods', 'sample-efficiency']","
In this Medium article I found [1] it is quite well explained what is behind the better model efficiency in model based RL in comparison to model free one.
Main difference between those two is like you said, that the model helps in finding the correct path more efficiently because of the existence of the model. Maybe you cannot find new samples (point 1) but you know better the whole inner logic about the system and instead of just knowing what to do with the specific sample, you can relate it to the whole picture (sort of like in point 2: you can play with the choices) and make more profound calculations.
The article had a comparison, which told that you are writing a map in a city about every possible direction you can take when you are model-based and while in model-free you can enter specific places and remember which direction was best based on last visits but you still never know where you are coming nor going exactly.
In other words, if you think you're teaching a taxi driver on a big city with many signs and rules, model based guy would drive more precisely sooner because the sign language (the inner logic and model of the city) helps them on understanding the map sooner than just reacting crossing by crossing somewhat by chance all the time.
Sample efficiency tells what is the amount of information fetched from one sample [2]. Model-based machine can adjust model, maybe make some calculations about expected rewards AND after that the same as model-free, adjust the common policy. Model-free does only have the policy. Again the taxi guys: model-free guys know that last time and second last I stopped in crossing, model-based guy knows also it was due to red lights in pole. Third time model-free guy is the first in row and BANG - hits the crossing car. Next time rule is there comes sometimes cars and model-based guy knew that from the first place.

My sources:
[1] https://medium.com/the-official-integrate-ai-blog/understanding-reinforcement-learning-93d4e34e5698
[2] What is sample efficiency, and how can importance sampling be used to achieve it?
"
Understanding the intuition behind Content Loss (Neural Style Transfer) [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I'm trying to understand the intuition behind how the Content Loss is calculated in a Neural Style Transfer. I'm reading from an articles: https://medium.com/mlreview/making-ai-art-with-style-transfer-using-keras-8bb5fa44b216 ,  that explains the implementation of Neural Style Transfer, from the Content loss function:

The article explains that: 

F and P are matrices with a number of rows equal to N and a number of columns equal to M.
N is the number of filters in layer l and M is the number of spatial elements in the feature map (height times width) for layer l.

From the code below for getting the features/content representation from particular Conv layers, I didn't quite understand how it works. Basically I printed out the output of every line of code to try to make it easier, but it still left a number of questions to be asked, which I listed below the code: 
def get_feature_reps(x, layer_names, model):
    """"""
    Get feature representations of input x for one or more layers in a given model.
    """"""
    featMatrices = []
    for ln in layer_names:
        selectedLayer = model.get_layer(ln)
        featRaw = selectedLayer.output
        featRawShape = K.shape(featRaw).eval(session=tf_session)
        N_l = featRawShape[-1]
        M_l = featRawShape[1]*featRawShape[2]
        featMatrix = K.reshape(featRaw, (M_l, N_l))
        featMatrix = K.transpose(featMatrix)
        featMatrices.append(featMatrix)
    return featMatrices

def get_content_loss(F, P):
    cLoss = 0.5*K.sum(K.square(F - P))
    return cLoss

1- For the line featRaw = selectedLayer.output, when I print featRaw, I get the output:
Tensor(""block4_conv2/Relu:0"", shape=(1, 64, 64, 512), dtype=float32).

a- Relu:0 does this mean Relu activation has not yet been applied?
b- Also I presume we're outputing the feature maps outputs from block4_conv2, not the filters/kernels themselves, correct?
c- Why is there an axis of 1 at the start? My understanding of Conv layers is that they're simply made up from the number of filters/kernels (with shape-height, width, depth) to apply to the input.
d- Is selectedLayer.output simply outputs the shape of the Conv layer, or does the output object also hold other information like the pixel values from the output feature maps of the layer?

2- With the line: featMatrix = K.reshape(featRaw, (M_l, N_l) where printing featMatrix would output: Tensor(""Reshape:0"", shape=(4096, 512), dtype=float32). 

a- This is where I'm confused the most. So to get the feature/content representation of a particular Conv layer of an image, we simply create a matrix of 2 dimensions, the first being the number of filters and the other being the area of the filter/kernel (height * width). That doesn't make sense! How do we get unique feature of an image from just that?!! We're not retrieving any pixel values from a feature map. We're simply getting the area size of filter/kernel and the number of filters, but not retrieving any of the content (pixel values) itself!! 
b- Also the final featMatrix is transposed - i.e. featMatrix = K.transpose(featMatrix) with the output Tensor(""transpose:0"", shape=(512, 4096), dtype=float32). Why is that (i.e. why reverse the axis)?

3 - Finally I want to know, once we retrieve the content representation, how can I output that in both as a numpy array and save it as an image? 
Any help would be really appreciated.
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'tensorflow', 'keras']",
Using True Positive as a Cost Function,"
I wanted to use True Positive (and True Negative) in my cost function to make to modify the ROC shape of my classifier. Someone told me and I read that it is not differentiable and therefore not usable as a cost function for a neural network.
In the example where 1 is positive and 0 negative I deduce the following equation for True Positive ($\hat y = prediction, y = label$):
$$ TP = \bf(\hat{y}^Ty) $$
$$ \frac{\partial TP}{\partial \bf y} = \bf y $$
The following for True Negative:
$$ TN = \bf(\hat{y}-1)^T(y-1) $$
$$ \frac{\partial TN}{\partial \bf y} = \bf \hat y^T -1    $$
The False Positive: 
$$ FP = - \bf  (\hat y^T-1) y $$
$$ \frac{\partial FP}{\partial \bf y} = - \bf ( \hat y^T - 1)    $$
The False Negative: 
$$ FN = \bf  \hat y^T (y-1) $$
$$ \frac{\partial FN}{\partial \bf y} = \bf \hat y    $$
All equations seem differentiable to me. Can someone explain where I went wrong?
","['deep-learning', 'metric']","
The vector functions for true positive, false positive etc all make use of the ""magic"" numbers $0$ and $1$ used to represent Boolean values. They are convenience methods that you can use in a numerical library, but you do need to be aware of the fundamental Boolean nature of the data. The $0$ and $1$ values allow the maths for calculating TP et al, but are not fundamental to it, they are a representation.
Your derivations of gradients for the functions you give seem correct, barring the odd typo. However, the gradient doesn't really apply to the value of $\mathbf{y}$, because all components of $\mathbf{y}$ are either $0$ or $1$. The idea that you could increase $\mathbf{y}$ slightly where  $\mathbf{\hat{y}}$ is $1$ in order to increase the value of the TP metric slightly has no basis. Instead the only valid changes to make an improvement are to modify FN values from $0$ to $1$ exactly.
You could probably still use your derivations as a gradient for optimisation (it would not be the only time in machine learning that something does not quite apply theoretically but you could still use it in practice). However, you then immediately hit the problem of how the values of $\mathbf{y}$ have been discretised to  $0$ or $1$ - that function will not be differentiable, and it will prevent you back propagating your gradients to the neural network weights that you want to change. If you fix that follow-on problem using a smoother function (e.g. a sigmoid) then you are likely to end up with something close to either cross-entropy loss or the perceptron update step. 
In other words, although what you have been told is an over-simplification, you will not find a way to improve the performance of your classifier by adding cost functions based directly on TP, FP etc. That is what binary cross-entropy loss is already doing. There are other, perhaps more fruitful avenues of investigation - hyperparameter searches, regularisation, ensembling, and if you have an unbalanced data set then consider weighting the true/false costs.
"
What are the differences between Bytenet and Wavenet?,"
I recently read Bytenet and Wavenet and I was curious why the first model is not as popular as the second. From my understanding, Bytenet can be seen as a seq2seq model where the encoder and the decoder are similar to Wavenet. Following the trends from NLP where seq2seq models seem to perform better, I find it strange that I couldn't find any paper that compares the two. Are there any drawbacks of Bytenet over Wavenet other than the computation time?
","['neural-networks', 'convolutional-neural-networks', 'comparison', 'machine-translation']","
My conclusion is the same as yours that there doesn't seem to be any published comparison of the two models.  ByteNet is computationally expensive and requires a lot of parameters.  WaveNet improves on ByteNet's efficiency, as you mentioned, and I believe that is the main difference. 
"
Can supervised learning be recast as reinforcement learning problem?,"
Let's assume that there is a sequence of pairs $(x_i, y_i), (x_{i+1}, y_{i+1}), \dots$ of observations and corresponding labels. Let's also assume that the $x$ is considered as independent variable and $y$ is considered as the variable that depends on $x$. So, in supervised learning, one wants to learn the function $y=f(x)$. 
Can reinforcement learning be used to learn $f$ (possibly, even learning the symbolic form of $f(x)$)? 
Just some sketches how can it be done: $x_i$ can be considered as the environment and each $x_i$ defines some set of possible ""actions"" - possible symbolic form of $f(x)$ or possible numerical values of parameters for $f(x)$ (if the symbolic form is fized). And concrete selected action/functional form $f(x, a)$ (a - set of parameters) can be assigned reward from the loss function: how close the observation $(x_i, y_i)$ is to the value that can be inferred from $f(x)$.
Are there ideas or works of RL along the framework that I provided in the previous passage?
","['reinforcement-learning', 'comparison', 'supervised-learning', 'function-approximation', 'regression']",
What are the reasons a perceptron is not able to learn?,"
I'm just starting to learn about neural networking and I decided to study a simple 3-input perceptron to get started with. I am also only using binary inputs to gain a full understanding of how the perceptron works. I'm having difficulty understanding why some training outputs work and others do not. I'm guessing that it has to do with the linear separability of the input data, but it's unclear to me how this can easily be determined. I'm aware of the graphing line test, but it's unclear to me how to plot the input data to fully understand what will work and what won't work.
There is quite a bit of information that follows. But it's all very simple. I'm including all this information to be crystal clear on what I'm doing and trying to understand and learn.
Here is a schematic graphic of the simple 3-input perceptron I'm modeling.

Because it only has 3 inputs and they are binary (0 or 1), there are only 8 possible combinations of inputs. However, this also allows for 8 possible outputs. This allows for training of 256 possible outputs. In other words, the perceptron can be trained to recognize more than one input configuration.
Let's call the inputs 0 thru 7 (all the possible configurations of a 3-input binary system). But we can train the perceptron to recognize more than just one input. In other words, we can train the perceptron to fire for say any input from 0 to 3 and not for inputs 4 thru 7. And all those possible combinations add up to 256 possible training input states.
Some of these training input states work, and others do not. I'm trying to learn how to determine which training sets are valid and which are not.
I've written the following program in Python to emulate this Perceptron through all 256 possible training states.
Here is the code for this emulation:
import numpy as np
np.set_printoptions(formatter={'float': '{: 0.1f}'.format})

# Perceptron math fucntions. 
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def sigmoid_derivative(x):
    return x * (1 - x)
# END Perceptron math functions.

# The first column of 1's is used as the bias.  
# The other 3 cols are the actual inputs, x3, x2, and x1 respectively
training_inputs = np.array([[1, 0, 0, 0],
                         [1, 0, 0, 1],
                         [1, 0, 1, 0],
                         [1, 0, 1, 1],
                         [1, 1, 0, 0],
                         [1, 1, 0, 1],
                         [1, 1, 1, 0],
                         [1, 1, 1, 1]])

# Setting up the training outputs data set array                         
num_array = np.array
num_array = np.arange(8).reshape([1,8])
num_array.fill(0)

for num in range(25):
    bnum = bin(num).replace('0b',"""").rjust(8,""0"")
    for i in range(8):
        num_array[0,i] = int(bnum[i])

    training_outputs = num_array.T
# training_outputs will have the array form: [[n,n,n,n,n,n,n,n]]
# END of setting up training outputs data set array                      

    # -------  BEGIN Perceptron functions ----------
    np.random.seed(1)
    synaptic_weights = 2 * np.random.random((4,1)) - 1
    for iteration in range(20000):
        input_layer = training_inputs
        outputs = sigmoid(np.dot(input_layer, synaptic_weights))
        error = training_outputs - outputs
        adjustments = error * sigmoid_derivative(outputs)
        synaptic_weights += np.dot(input_layer.T, adjustments)
    # -------  END Perceptron functions ----------


    # Convert to clean output 0, 0.5, or 1 instead of the messy calcuated values.
    # This is to make the printout easier to read.
    # This also helps with testing analysis below.
    for i in range(8):
        if outputs[i] <= 0.25:
            outputs[i] = 0
        if (outputs[i] > 0.25 and outputs[i] < 0.75):
            outputs[i] = 0.5
        if outputs[i] > 0.75:
            outputs[i] = 1
    # End convert to clean output values.

    # Begin Testing Analysis
    # This is to check to see if we got the correct outputs after training.
    evaluate = ""Good""
    test_array = training_outputs
    for i in range(8):
        # Evaluate for a 0.5 error.
        if outputs[i] == 0.5:
            evaluate = ""The 0.5 Error""
            break
        # Evaluate for incorrect output
        if outputs[i] != test_array[i]:
            evaluate = ""Wrong Answer""
    # End Testing Analysis

    # Printout routine starts here:
    print_array = test_array.T
    print(""Test#: {0}, Training Data is: {1}"".format(num, print_array[0]))
    print(""{0}, {1}"".format(outputs.T, evaluate))
    print("""") 

And when I run this code I get the following output for the first 25 training tests.
Test#: 0, Training Data is: [0 0 0 0 0 0 0 0]
[[ 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]], Good

Test#: 1, Training Data is: [0 0 0 0 0 0 0 1]
[[ 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0]], Good

Test#: 2, Training Data is: [0 0 0 0 0 0 1 0]
[[ 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0]], Good

Test#: 3, Training Data is: [0 0 0 0 0 0 1 1]
[[ 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0]], Good

Test#: 4, Training Data is: [0 0 0 0 0 1 0 0]
[[ 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0]], Good

Test#: 5, Training Data is: [0 0 0 0 0 1 0 1]
[[ 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0]], Good

Test#: 6, Training Data is: [0 0 0 0 0 1 1 0]
[[ 0.0 0.0 0.0 0.0 0.5 0.5 0.5 0.5]], The 0.5 Error

Test#: 7, Training Data is: [0 0 0 0 0 1 1 1]
[[ 0.0 0.0 0.0 0.0 0.0 1.0 1.0 1.0]], Good

Test#: 8, Training Data is: [0 0 0 0 1 0 0 0]
[[ 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0]], Good

Test#: 9, Training Data is: [0 0 0 0 1 0 0 1]
[[ 0.0 0.0 0.0 0.0 0.5 0.5 0.5 0.5]], The 0.5 Error

Test#: 10, Training Data is: [0 0 0 0 1 0 1 0]
[[ 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0]], Good

Test#: 11, Training Data is: [0 0 0 0 1 0 1 1]
[[ 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0]], Good

Test#: 12, Training Data is: [0 0 0 0 1 1 0 0]
[[ 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0]], Good

Test#: 13, Training Data is: [0 0 0 0 1 1 0 1]
[[ 0.0 0.0 0.0 0.0 1.0 1.0 0.0 1.0]], Good

Test#: 14, Training Data is: [0 0 0 0 1 1 1 0]
[[ 0.0 0.0 0.0 0.0 1.0 1.0 1.0 0.0]], Good

Test#: 15, Training Data is: [0 0 0 0 1 1 1 1]
[[ 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0]], Good

Test#: 16, Training Data is: [0 0 0 1 0 0 0 0]
[[ 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0]], Good

Test#: 17, Training Data is: [0 0 0 1 0 0 0 1]
[[ 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0]], Good

Test#: 18, Training Data is: [0 0 0 1 0 0 1 0]
[[ 0.0 0.0 0.5 0.5 0.0 0.0 0.5 0.5]], The 0.5 Error

Test#: 19, Training Data is: [0 0 0 1 0 0 1 1]
[[ 0.0 0.0 0.0 1.0 0.0 0.0 1.0 1.0]], Good

Test#: 20, Training Data is: [0 0 0 1 0 1 0 0]
[[ 0.0 0.5 0.0 0.5 0.0 0.5 0.0 0.5]], The 0.5 Error

Test#: 21, Training Data is: [0 0 0 1 0 1 0 1]
[[ 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0]], Good

Test#: 22, Training Data is: [0 0 0 1 0 1 1 0]
[[ 0.0 0.0 0.0 1.0 0.0 1.0 1.0 1.0]], Wrong Answer

Test#: 23, Training Data is: [0 0 0 1 0 1 1 1]
[[ 0.0 0.0 0.0 1.0 0.0 1.0 1.0 1.0]], Good

Test#: 24, Training Data is: [0 0 0 1 1 0 0 0]
[[ 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0]], Wrong Answer

For the most part, it appears to be working. But there are situations where it clearly does not work.
I have the labels in two different ways.
The first type of error is ""The 0.5 Error"" which is easy to see. It should never return any output of 0.5 in this situation. Everything should be binary. The second type of error is when it reports the correct binary outputs but they don't match what it was trained to recognize.
I would like to understand the cause of these errors. I'm not interested in trying to correct the errors as I believe these are valid errors. In other words, these are situations where the perceptron is simply incapable of being trained for. And that's ok.
What I want to learn is why these cases are invalid. I'm suspecting that they have something to do with the input data not being linearly separable in these situations. But if that's the case, then how do I go about determining which cases are not linearly separable? If I could understand how to do that I would be very happy.
Also, are the reasons why it doesn't work in specific cases the same? In other words, are both types of errors caused by linear inseparability of the input data? Or is there more than one condition that causes a Perceptron to fail in certain training situations.
Any help would be appreciated.
","['training', 'perceptron']",
When does AlphaZero play suboptimal moves?,"
If AlphaZero was always playing the best moves it would just generate the same training game over and over again. So where does the randomness come from? When does it decide not to play the most optimal move?
","['reinforcement-learning', 'deep-rl', 'alphazero', 'greedy-ai']",
What does the symbol $\mathbb E$ mean in these equations?,"
I came across some papers that use $\mathbb E$ in equations, in particular, this paper: https://arxiv.org/pdf/1511.06581.pdf. Here is some equations from the paper that uses it: 
$Q^\pi \left(s,a \right) = \mathbb E \left[R_t|s_t = s, a_t = a, \pi \right]$ ,
$V^\pi \left(s \right) = \mathbb E_{a\backsim\pi\left(s \right)} \left[Q^\pi \left(s, a\right) \right]$ ,
$Q^\pi \left(s, a \right) = \mathbb E_{s'} \left[r + \gamma\mathbb E_{a'\backsim\pi \left(s' \right)} \left[Q^\pi \left(s', a' \right) \right] | s,a,\pi \right]$
$\nabla_{\theta_i}L_i\left(\theta_i \right) = \mathbb E_{s, a, r, s'} \left[\left(y_i^{DQN} - Q \left(s, a; \theta_i \right) \right) \nabla_{\theta_i} Q\left(s, a, \theta_i \right) \right]$
Could someone explain to me what is the purpose of $\mathbb E$?
","['machine-learning', 'deep-learning', 'reinforcement-learning', 'q-learning']","
$\mathbb E$ is the symbol for the expectation (or expected value). 
To fully understand the concept of expected value, you need to understand the concept of random variable. An example should help you understand the idea behind the concept of a random variable. 
Suppose you toss a coin. The outcome of this (random) experiment can either be heads or tails. Formally, the sample space, $\Omega = \{\text{heads}, \text{tails}\}$, is the set that contains the possible outcomes of a random experiment. The outcome (e.g. heads) is the result of a random process. A random variable is a function that we can associate with a random process so that we can more formally describe the random process. In this case, we can associate a random variable, $T$, with this random process of tossing a coin.
$$
T(\omega) =
\begin{cases}
1, & \text{if } \omega = \text{heads}, \\[6pt]
0, & \text{if } \omega = \text{tails},
\end{cases}
$$
where $\omega \in \Omega$.
In other words, if the outcome of the random process is heads, then the output of the associated random variable $T$ is $1$, else it is $0$.
We can also associate with each random process (and thus with the corresponding random variable) a probability distribution, which, intuitively, describes the probability of occurrence of each possible outcome of the random process. In the case of the coin-flipping random variable (or process), assuming that the coin is ""fair"", then the following function describes the probability of each outcome of the coin
$$
f_T(t) =
\begin{cases}
\tfrac 12,& \text{if }t=1,\\[6pt]
\tfrac 12,& \text{if }t=0,
\end{cases}
$$
In other words, there is $\tfrac 12$ probability that the outcome of the random process is $1$ (heads) and $\tfrac 12$ probability that it is $0$ (tails).
If you throw a coin $n$ times in the air, how many times will it land heads and tails? Of course, it will depend on the experiment. In the first experiment, you might get $\frac{3n}{4}$ heads and $\frac{n}{4}$ tails. In the second experiment, you might get $\frac{n}{2}$ heads and $\frac{n}{2}$ tails, and so on. If you repeat this experiment an infinite amount of times (of course, we can't do that, but imagine if we could do that), how many times do you expect (on average) to get heads and tails? The expected value is the answer to this question.
In the case of the coin-tossing experiment, the outcomes are discrete (heads or tails), consequently, $T$ is a discrete random variable. In the case of a discrete random variable, the expected value is defined as follows
$$\mathbb E[T] = \sum_{t \in T} p(t) t$$
where $t$ is the outcome of the random variable $t$ and $p(t)$ is the probability of such outcome. In other words, the expected value of a random variable $T$ is defined as a weighted sum of the values it can take, where the weights are the corresponding probabilities of occurrence. So, in the case of the coin-tossing experiment, the expected value is
\begin{align}
\mathbb E[T] 
&= \sum_{t \in T} p(t) t\\
&= \frac{1}{2}1 + \frac{1}{2} 0\\
&=\frac{1}{2}
\end{align}
What does $\mathbb E[T] = \frac{1}{2}$ mean? Intuitively, it means that half of the times the random process produces heads and half of the times it produces tails, assuming it is governed by the probability distribution $f_T(t)$.
Note that, if the probability distribution $f_T(t)$ had been defined differently, then the expected value would also have been different, given that the expected value is defined as a function of the probability of occurrence of each outcome of the random process.
In your specific examples, $\mathbb E$ is still the symbol for the expected value. For example, in the case of $Q^\pi \left(s,a \right) = \mathbb E \left[R_t|s_t = s, a_t = a, \pi \right]$, $Q^\pi \left(s,a \right)$ is thus defined as the expected value of the random variable $R_t$, given that $s_t = s$, $a_t = a$ and the policy is $\pi$ (so this is actually a conditional expectation). In this specific case, $R_t$ represents the return at time step $t$, which, in reinforcement learning, is defined as
$$
R_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1}
$$
where $r_{t+k+1} \in \mathbb{R}$ is the reward at time step $t+k+1$. $R_t$ a random variable because it is assumed that the underlying environment is a random process.
It is not always easy to intuitively understand the expected value of a random variable. For example, in the case of a coin-flipping random process, the expected value $\frac{1}{2}$ should be intuitive (given that it is the average of $1$ and $0$), but, in the case of $Q^\pi \left(s,a \right)$, at first glance, it is not clear what the expected value should be (hence the need for algorithms such as Q-learning), given that it depends on the rewards, which depend on the dynamics of the environment. However, the intuition behind the concept of the expected value and the calculation (provided the associated random variable is discrete) does not change.
In the case there is more than one random variable involved in the calculation of the expected value, then we also need to specify the random variable the expected value is being calculated with respect to, hence the subscripts of the expected value in your examples. See, for example, Subscript notation in expectations for more info.
"
Video engagement analysis with deep learning,"
I am trying to rank video scenes/frames based on how appealing they are for a viewer. Basically, how ""interesting"" or ""attractive"" a scene inside a video can be for a viewer. My final goal is to generate say a 10-second short summary given a video as input, such as those seen on Youtube when you hover your mouse on a video.
I previously asked a similar question here. But the ""aesthetics"" model is good for ranking artistic images, not good for frames of videos. So it was failing. I need a score based on ""engagement for general audience"". Basically, which scenes/frames of video will drive more clicks, likes, and shares when selected as a thumbnail.
Do we have an available deep-learning model or a prototype doing that? A ready-to-use prototype/model that I can test as opposed to a paper that I need to implement myself. Paper is fine as long as the code is open-source. I'm new and can't yet write a code given a paper.
","['neural-networks', 'deep-learning', 'classification', 'computer-vision', 'image-processing']","
One of the key terms in the literature that you are looking for is video captioning.
You can have a look at some of the relevant papers with code on this subject. In short, it is an active area of research and a difficult problem, one reason is because videos are still difficult to learn about (because of larger amount of data + larger model, etc...) and this model has to be working with two modalities of data: text and image.
A paper that you might want to start with is Deep Visual-Semantic Alignments for Generating Image Descriptions which works on single images. In short, you can use something similar like in the paper: object detector (e.g. Faster RCNN) to extract visual features and feed them into the state of an RNN (LSTM) which would output a sequence of words in your summary (see picture below).

"
How does InfoGAN learn latent categorical codes on MNIST,"
While reading the InfoGAN paper and implement it taking help from a previous implementation, I'm having some difficulty understanding how it learns the discrete categorical code when trained on MNIST.
The implementation we tried to follow uses a target as a randomly generated integer from 0 to 9. My doubt is this: how can it learn categorical information if from the start it's learning using a loss which takes in random values.
If this implementation is wrong, then what should the target be while training the Q network, when using the categorical-cross-entropy loss on the output logits?
",['generative-adversarial-networks'],
Why do I get a straight line as an output from a neural network?,"
I am using feedforward neural network for regression and what I get as a result of prediction is a constant value visible on the graph below:

Data I use are typical standardised tabular numbers. The architecture is as follows:
model.add(Dropout(0.2))
model.add(Dense(units=512, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(units=256, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(units=128, activation='relu'))
model.add(Dense(units=128, activation='relu'))
model.add(Dense(units=1))

adam = optimizers.Adam(lr=0.1)

model.compile(loss='mean_squared_error', optimizer=adam)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.9,
    patience=10,
    min_lr=0.0001,
    verbose=1)

tensorboard = TensorBoard(log_dir=""logs\{}"".format(NAME))

history = model.fit(
    x_train,
    y_train,
    epochs=500,
    verbose=10,
    batch_size=128,
    callbacks=[reduce_lr, tensorboard],
    validation_split=0.1)

It seems to me that all weights are zeroed and only constant bias is present here, since for different data samples from a test set I get the same value, but I am not sure.
I understand that the algorithm has found smallest MSE for such a constant value, but is there a way of avoiding such situation, since straight line is not really good solution for my project?
","['neural-networks', 'training', 'tensorflow', 'keras']",
Transfer learning to train only for a new class while not affecting the predictions of the other class,"
I am basically interested in vehicle on the road.  
YoloV3 pytorch is giving a decent result.
So my interested Vehicles Car Motorbike Bicycle Truck and bus, I have a small vehicles being detected as truck.
Since the small vehicle is nicely being detected as truck. I have annotated this small vehicle as a different class.
Though, I could add an extra class say 81th class, since the current YoloV3 being used is trained on 80 classes.
81th class would contain the weight of the truck, I would freeze the weights such that the rest of the 80 classes remain unaltered and only the 81th class of this new data gets trained.
The problem is the final layer gets tuned according to the prediction of all the classes it learns.
I was not able to find any post that could actually mention this way of preserving the predictions of the other classes and introducing a new class using transfer learning.
The closest, I was able to get is this post of 
Weight Sampling Tutorial SSD using keras
Its mentioned in
Option 1: Just ignore the fact that we need only 8 classes
This would work, and it wouldn't even be a terrible option. Since only 8 out of the 80 classes would get trained, the model might get gradually worse at predicting the other 72 clases in the second paragraph.
Is it possible to preserve the predictions, of the previous pre trained model while introducing the new class and use transfer learning to train only for that class?
I Feel that this is not possible, would like to know your opinion. Hope someone can prove me wrong. 
","['convolutional-neural-networks', 'transfer-learning', 'incremental-learning']","
Even if you want to re-train your model for just one new class you will have to prepare your training data such that it includes all or most of the classes which you want to predict. Most of the times last two layers of a network have the data of number of labels which are to be predicted and that should always be sum of the number of classes you already trained on and the number of classes you want to add for training.
"
Should importance sample weighting be compensated for by dynamically increasing learning rate?,"
I'm using Prioritized Experience Replay (PER) with a DDQN. To compensate for overfitting relatively high-value samples due to the non-uniform selection, I'm training with sample weights provided along with the PER samples to downplay each sample's loss contribution according to its probability of selection. I've observed that typically these sample weightings vary from $~0.1$ to $<0.01$, as the buffer gradually fills up (4.8M samples). 
When using this compensation, the growth of the maximal Q value per episode stalls prematurely compared to a non-weight-compensated regime. I presume that this is because the size of the back-propagation updates is being greatly and increasingly diminished by the sample weights. 
To correct for this I've tried taking the beta-adjusted maximum weight as reported by the PER (the same buffer-wide value by which the batch is normalized) and multiplying the base learning rate by it, thereby adjusting the optimizer after each batch selection.
My question is two-fold:

Is this the correct interpretation of what's going on? 
Is it standard practice to compensate for sample weighting in this way?

Although it seems to be working in keeping the Q growth alive whilst taming the loss, I've not been able to find any information on this and haven't found any implementations that compensate in this way so have a major doubt about the mathematical validity of it.
","['reinforcement-learning', 'dqn']",
Dealing with empty frames in MRI images,"
I started working on the application of deep learning in medical imaging recently. While dealing with MRI images in the BraTS dataset, I observe that first and last few frames are always completely empty (black). I want to ask those who are already working in the field, is there a way to remove them in a procedural manner before training and add them correctly after the training as a postprocessing step (to comply with the ground truth segmentations' shape)? Has anyone tried that?
I could not find any results on Google. So asking here.
Edit: I think I did not make my point clear enough. I meant to say first and last few frames of each MRI scan are empty. How to deal with those is what I intended to ask.
","['deep-learning', 'computer-vision', 'autoencoders']","
As an expert in image analysis I don't think this would be a problem. I have never worked with MRI images from the particular dataset you described but I found that the format of the file containing the images is NIfTI.
NIfTI files can be imported in Matlab(niftiread function), ImageJ and Python (NiBabel-Nipy). Thus you should be able to write a script to import the images from the file, select which images you want to keep, and save the output I'm the same format as the input (NIfTI).
"
Can exogenous variables be state features in reinforcement learning?,"
I have a question about state representation of Q-learning or DQN algorithm.
I'm still a beginner of RL, so I'm not sure that is it suitable to take exogenous variables as state features.
For example, in my current project, deciding to charge/discharge an electric vehicle actions according to the real-time fluctuating electricity prices, I'm wondering if the past n-step prices or hours can be considered as state features.
Because both the prices and the hour are just given information in every time step rather than being dependent to the charging/discharging actions, I'm suspicious about whether they can are theoretically qualified to be state features or not.
If they are not qualified, could someone give me a reference or something that I can read?
","['reinforcement-learning', 'q-learning', 'dqn']",
"Loss reduction, but constant performance with CNN","
I made a CNN with a reasonable loss curve, but the performance of the model does not improve. I have tried making the model larger, I am using three convolutional layers with batch norms.
Thanks for your help.

","['reinforcement-learning', 'convolutional-neural-networks']",
Turn photos right-side up?,"
I'm looking for either an existing AI app or a pre-trained NN that will tell me if a photograph is right-side up or not. I want to use this to create an application that automatically rotates photos so they are right-side-up. This doesn't seem hard.
If it doesn't exist, presumably I can create it with Tensorflow, and just use a ton of photos to train it, and assume they are all correctly oriented in the training set. Would that work?
",['image-processing'],"
I don't know if there is an existing pretrained NN that does this but it wouldn't be very hard to modify one to do this.
First, I'd take a pretrained image classification NN (e.g. VGG, ResNet), drop its final layer and replace it with one with 4 neurons, representing the 4 orientations (so that you know which way to rotate it).
Then I'd take again a dataset of regular images (e.g. a subset of ImageNet) and assume that they are correctly oriented. I'd make three more duplicate datasets with the same images rotates by 90, 180 and 270 degrees respectively. These 4 datasets would be the 4 classes I'd fine tune the model on.
By training your model on this dataset, you'll be training it to recognize which side your image is facing. Since it's a pretrained net and its a fairly simple task, I think that after a few iterations, your model will have converged. Then you could write a script that uses this model to predict an image's orientation and rotate it accordingly.
"
How do I generate structured light for the 3D bin picking system?,"
I want to know how to generate the structured light which projects different patterns of light on a 3D object which is under scanning. 
",['computer-vision'],
Torch CNN not training,"
I am completely new to CNN's, and I do not quite know how to design or use them efficiently. That being said, I am attempting to build a CNN that learns to play Pac-man with reinforcement learning. I have trained it for about 3 hours and have seen little to no improvement. My observation space is 3 channels * 15 * 19, and there are 5 actions. Here is my code, I am open to any and all suggestions. Thanks for all your help.
from minipacman import MiniPacman as pac
from torch import nn
import torch
import random
import torch.optim as optimal
from torch.autograd import Variable
import matplotlib.pyplot as plt
import numpy as np
import keyboard


loss_fn = nn.MSELoss()
epsilon = 1
env = pac(""regular"", 1000)
time = 0
action = random.randint(0, 4)
q = np.zeros(3)
alpha = 0.01
gamma = 0.9
tick = 0
decay = 0.9999


class Value_Approximator (nn.Module):
    def __init__(self):
        super(Value_Approximator, self).__init__()
        # Convolution 1
        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)
        self.relu1 = nn.ReLU()

        # Max pool 1
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)

        # Convolution 2
        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)
        self.relu2 = nn.ReLU()

        # Max pool 2
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)

        # Fully connected 1 (readout)
        self.fc1 = nn.Linear(384, 5)

    def forward(self, x):
        # Convolution 1
        out = self.cnn1(x)
        out = self.relu1(out)

        # Max pool 1
        out = self.maxpool1(out)

        # Convolution 2
        out = self.cnn2(out)
        out = self.relu2(out)

        # Max pool 2
        out = self.maxpool2(out)

        # Resize
        # Original size: (100, 32, 7, 7)
        # out.size(0): 100
        # New out size: (100, 32*7*7)
        out = out.view(out.size(0), -1)

        # Linear function (readout)
        out = self.fc1(out)

        return out

approx = Value_Approximator()
optimizer = optimal.SGD(approx.parameters(), lr=alpha)


while time < 50000:
    print(""Time: ""+str(time))
    print(""Epsilon: ""+str(epsilon))
    print()
    time += 1
    state = env.reset()
    tick = 0

    epsilon *= decay

    if epsilon < 0.1:
        epsilon = 0.1

    while True:
        tick += 1
        state = np.expand_dims(state, 1)
        state = state.reshape(1, 3, 15, 19)
        q = approx.forward(torch.from_numpy(state))[0]

        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()
        else:
            _, action = torch.max(q, -1)
            action = action.item()
        new_state, reward, terminal, _ = env.step(action)
        show_state = new_state
        new_state = np.expand_dims(new_state, 1)
        new_state = state.reshape(1, 3, 15, 19)

        q_new = approx.forward(torch.from_numpy(new_state).type(torch.FloatTensor))[0]  # "" find Q (s', a') ""
        #  find optimal action Q value for next step
        new_max, _ = torch.max(q_new, -1)
        new_max = new_max.item()

        q_target = q.clone()
        q_target = Variable(q_target.data)

        #  update target value function according to TD
        q_target[action] = reward + torch.mul(new_max, gamma)  # "" reward + gamma*(max(Q(s', a')) ""

        loss = loss_fn(q, q_target)  # "" reward + gamma*(max(Q(s', a')) - Q(s, a)) ""
        # Update original policy according to Q_target ( supervised learning )
        approx.zero_grad()
        loss.backward()
        optimizer.step()

        #  Q and Q_target should converge
        if time % 100 == 0:
            state = torch.FloatTensor(show_state).permute(1, 2, 0).cpu().numpy()

            plt.subplot(131)
            plt.title(""Imagined"")
            plt.imshow(state)
            plt.subplot(132)
            plt.title(""Actual"")
            plt.imshow(state)
            plt.show(block=False)
            plt.pause(0.000001)

        if keyboard.is_pressed('1'):
            torch.save(approx.state_dict(), 'trained-10000.mdl')
        if keyboard.is_pressed('9'):
            torch.save(approx.state_dict(), 'trained-10000.mdl')

        if terminal or tick > 100:
            plt.close()
            break

        state = new_state


torch.save(approx.state_dict(), 'trained-10000.mdl')

","['deep-learning', 'reinforcement-learning', 'convolutional-neural-networks', 'deep-rl']",
Is a GPU always faster than a CPU for training neural networks?,"
Currently, I am working on a few projects that use feedforward neural networks for regression and classification of simple tabular data. I have noticed that training a neural network using TensorFlow-GPU is often slower than training the same network using TensorFlow-CPU.
Could something be wrong with my setup/code or is it possible that sometimes GPU is slower than CPU?
","['neural-networks', 'training', 'tensorflow', 'gpu']","
I advice you to always use GPU over CPU for training your models. This is driven by the usage of deep learning methods on images and texts, where the data is very rich.
You must have a GPU suited perfectly for training (e.g. NVIDIA 1080, NVIDIA Titan or higher versions), I wouldn't be surprised to find that your CPU was faster if you don't have a powerful GPU.
"
How is transfer learning used to mitigate catastrophic forgetting in neural networks?,"
How can transfer learning be used to mitigate catastrophic forgetting. Could someone elaborate on this?
","['machine-learning', 'transfer-learning', 'catastrophic-forgetting']","
Transfer learning is a field where you apply knowledge from a source onto a target. This is a vague notion and there is an abundance of literature pertaining to it. Given your question I will work under the assumption that you are referring weight/architecture sharing between model (in other words training a model on one dataset and using it as a featurizer for another dataset)  
Now any learning system without lossless memory will have remnants of catastrophic forgetting. So let's think about how we would implement this transfer and what effects can be derived from this.

One implementation involves transferring a component and only training additional layers.
Another is retraining the entire system but at a lower learning rate?  

In setting 1, we can make the claim catastrophic forgetting is minimized by the fact that there is an unbiased featurizer that cant forget based on a sampling regime, though this does not mean additional layers which are still being trained can still faulter in this error mode.  
In setting 2, we can make the claim catasrophic forgetting can be reduced compared to a normal end-to-end no-transfer training because the unbiased featurizers difference can be analytically bounded by its initial transferred featurization (complexity class is based on both the function and the number of steps -- so longer you train, the more likely it can forget)  
These reasons are talking about mitigating and not erasing the concept of catastrophic forgetting, that is because as I mentioned above any learning system without lossless memory will have remnants of catastrophic forgetting, so making the generalized claim about transfer learning may not always fit the bill.
"
Is it effective to concatenate the results of mean-pooling and max-pooling?,"
Is it popular or effective to concatenate the results of mean-pooling and max-pooling, to get the invariance of the latter and the expressivity of the former?
","['convolutional-neural-networks', 'pooling', 'max-pooling', 'average-pooling']","
I haven't seen it as you describe and I don't think it would be much useful. Pooling layers are being gradually phased out of networks, because they don't seem to be that useful anymore. With the emergence of more and more conv-only architectures, I don't see that likely.
"
3D geometry and similarity with a reference model,"
I am looking for a CNN method, or any other machine learning method, to recognize 3D natural geometries that are similar to each others, and compare these geometries with a reference 3D model. To illustrate this, consider the following crater topographic map (x,y,z) of the Moon as an example:

The exercise would be to recognize the craters, and compare their (3D) geometry (scale-invariant) with a reference 3D crater model (e.g. the one within the blue square). The result I am looking for is a kind of heatmap showing the similarity measure of (1) a sampled crater with the crater model, and/or (2) the geometry of some parts of the sampled crater (e.g. the inner crater steep sides) with those of the reference model. No classification.
I tend to think that a 3D-oriented CNN method (OctNet, Octree CNN... etc) is a starting point for the above-mentioned task but I would rather prefer getting opinions on this matter since I am still a newbie in machine learning and we are dealing with direct application to real-world natural objects here.
",['convolutional-neural-networks'],
What is the difference between machine learning and quantum machine learning?,"
What is the difference between machine learning and quantum machine learning?
","['machine-learning', 'comparison', 'quantum-computing']","
The difference is much simpler than you might have anticipated: In the quantum computing community, machine learning algorithms designed to be used on quantum computers as opposed to classical computers, would fall under ""quantum machine learning"". There's really nothing more to it!
There is a short paper published in Nature called ""Quantum Machine Learning"" which was mentioned before, and it might give you all the answers you need about what ""Quantum Machine Learning"" is.
"
What should we do when we have equal observations with different labels?,"
Suppose we have a labeled data set with columns $A$, $B$, and $C$ and a binary outcome variable $X$. Suppose we have rows as follows:
 col  A B C X
  1   1 2 3 1
  2   4 2 3 0
  3   6 5 1 1
  4   1 2 3 0

Should we throw away either row 1 or row 4 because they have different values of the outcome variable X? Or keep both of them?
","['machine-learning', 'ai-design', 'training', 'data-science']","
The problem you are portraying looks like a modified XOR problem. You can't throw away the lines with a label of 1 because a the model won't be able to learn this class.
"
Solution to classify product names,"
I have a bunch of training data for classifying product names, around 30,000 samples. The task is to classify these product names into types of product, around 100 classes (single words).
For example:
dutch lady sweetened uht milk => milk
samsung galaxy note 10        => electronics
cocacola zero                 => softdrink
...

All words in inputs are indexed to numbers, and so classes. I've tried to use tf.estimator.DNNClassifier to classify them but no good results. The outcome is just an accuracy of 4% which is no meaning.
Should it be I'm in the case that classes (Y values) are distributed kinda randomly and too hard to do multi-time linear separation?
Are there any existing solutions to classify a list of names? like my product names?
","['deep-learning', 'classification', 'tensorflow', 'linear-regression', 'text-summarization']",
How to implement SVM algorithm from scratch in a programming language?,"
I'm a computer scientist who's studying support vector machines (SVMs) in a machine learning course. I have some understanding of how SVMs are designed, thanks to 16. Learning: Support Vector Machines - MIT. However, what I'm not understanding is the transition from the optimization problem of the Lagrangian function to its implementation in any programming language. Basically, what I need to understand is how to build, from scratch, the decision function, given a training set. In particular, how do I find Lagrange multipliers in order to know which points are to be considered to define support vectors and the decision function?
Can anyone explain this to me?
","['machine-learning', 'implementation', 'support-vector-machine']","
Based on this repository:
https://github.com/arkm97/svm-from-scratch/blob/master/SVM_from_scratch.ipynb
I will try to reverse engineering that concept:
So firstly there is issue for DataCleaning(removing 0,values, serialize, normalize)
normalizing dataset (replaced by its difference from the mean)
credit_df_norm = (credit_df - credit_df.mean())/(credit_df.std())

Divide dataset into Train & Test:
train_df = credit_df_norm.drop(target, axis=1).loc[:training_points]
train_target = credit_df_norm[target].replace(0, -1).loc[:training_points]

And here is starting the clue of that algorithms (with maths etc.)

given data points $\vec x_j \in \mathbb{R}^{1 \times N}$ and targets
$y_j = \pm 1$, where $j = 1, \dots, M$, find the maximum-margin
hyperplane that separates the two classes ($y_j = 1$ and $y_j = -1$).
Let $\vec w$ be the vector normal to the hyperplane. We want to find
$\vec w$ that satisfies
$$ y_j (\vec w \cdot \vec x_j + b) \geq 1 $$ The dual formulation of
the above is equivalent to maximizing the following over the
multipliers $\vec \alpha$:
$$ L(\vec \alpha) = \vec y \cdot \vec \alpha  - \frac 1 2  \vec \alpha
> K \vec \alpha^T$$ subject to the constraints $\sum_{j=1}^M \alpha_j =
> 0$ and $y_j \alpha_j \geq 0$. The matrix $K$ defines the kernel of the
SVM; I've chosen $K_{jk} = k(\vec x_j, \vec x_k) = \vec x_j \cdot \vec
> x_k$. The parameters of the plane are recovered from $\vec w = \vec
> \alpha \cdot \vec x$ and $b = y_j = \vec w \cdot \alpha_j$ for $j$
such that $\alpha_j \neq 0$

Let's divide it to the single factors:
we need to find maximum-margin-hyperplane for separate into two classes:
Separation cause SVN is classification - so we need to classify elements into categories.
"
TensorFlow estimator DNNClassifier fails to fit simple data,"
The ready-to-use DNNClassifier in tf.estimator seems not able to fit these data:
X = [[1,2], [1,12], [1,17], [9,33], [48,49], [48,50]]
Y = [ 1,     1,      1,      1,      2,       3     ]

I've tried with 4 layers but it's fitting to 83% (=5/6 sampes) only:
hidden_units = [2000,1000,500,100]
n_classes    = 4   

The sample data above are supposed to be separated by 2 lines (right-click image to open in new tab):

It seems stuck be cause of Y=2 and Y=3 are too close. How to change the DNNClassifier to fit to 100%?
","['deep-learning', 'classification', 'tensorflow', 'linear-regression', 'underfitting']",
Extending FaceNets triplet loss to object recognition,"
FaceNet uses a novel loss metric (triplet loss) to train a model to output embeddings (128-D from the paper), such that any two faces of the same identity will have a small Euclidean distance, and such that any two faces of different identities will have a Euclidean distance larger than a specified margin. However, it needs another mechanism (HOG or MTCNN) to detect and extract faces from images in the first place.
Can this idea be extended to object recognition? That is, can an object detection framework (e.g. MaskR-CNN) be used to extract bounding boxes of an object, cropping the object feeding this to a network that was trained on triplet loss, and then compares the embeddings of objects to see if theyre the same object?
Is there any research that has been done or any published public datasets for this?
","['deep-learning', 'computer-vision', 'object-recognition', 'object-detection', 'facial-recognition']",
How to generate the original image from feature set?,"
We all know that using CNN, or even simpler functions, like CLD or EHD, we can generate a set of features out of images. 
Is there any ways or approaches that given a set of features, we can somehow generate a corase version of the original image that was given as input? Maybe a gray-scale version with visible objects inside? If so, what features do we need?
","['deep-learning', 'computer-vision', 'generative-adversarial-networks', 'image-processing', 'image-generation']","
The model (that I know of) which most resembles your description is the auto-encoder, which is trained to learn a compact representation (a vector) of the input, which can later be used to reconstruct the original input. In a certain way, this compact representation (implicitly) encodes the most important features of the input. In particular, you may be looking for denoising auto-encoders.
"
Why tf object detection api needs so few pictures?,"
I am wondering why tf object detection api needs so few picture samples for training while regular cnns needs many more?
What I read in tutorials is that tf object detection api needs around 100-500 pictures per class for training (is it true?) while regular CNNs need many many more samples, like tens of thousands or more. Why is it so?
","['tensorflow', 'object-detection']",
Is normalizing the data a way to improve generalization?,"
There are many known ways to overcome overfitting or make a model generalize better to unseen data. 
Here I would like to ask if normalizing/standardizing/similiraizing the train and test data is a plausible approach.
By similarizing I mean making the images look alike by using some function that could be a Neural Network itself. I know that normally one would approach this the opposite way by augmenting and therefore increasing the variation in the training data. But is also possible to improve the model by restricting the variation of the training and test data?
I know that this may not be the best approach and maybe too complicated but I see some use cases where known techniques of preventing overfitting aren't applicable. In those cases, having a network that can normalize/standardize/similarize the ""style"" of different images could be very useful.
Unfortunately I didn't find a single paper discussing this approach.
","['neural-networks', 'deep-learning', 'overfitting']","
Batch Normalization is usually known to speed up the learning process as it makes the weights in the deeper layers more robust. It restricts the distribution of the weights in a particular layer - this video might tend to be useful to what BatchNorm does. This said, batch normalization does have a regularizing effect which does to tend to increase generalization. 
Talking about generalization - A focus on regularization would probably be more helpful 
"
"Is ""dataset size"" and ""model size"" same thing? [closed]","







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I mean what is determine my model size, connection amount between layers and neurons, or size of my dataset? 
","['tensorflow', 'keras', 'models', 'artificial-neuron', 'sequence-modeling']",
"Can mAP score be used to describe ""recall"" rate of a model?","
I have a general question regarding the mAP score used in measuring object detection system performance. 
I understood how the AP score is calculated, by averaging precision over recall 0 to 1. And then we can compute mAP, by averaging AP score of different labels.
However, what I have been really confused, is that, it seems that mAP score is used to denote the ""precision"" of a model. Then what about the ""recall"" aspect? Note that generally speaking, when measuring the performance of a machine learning model, we need to report precision and recall at the same time, right? It seems that mAP can only cover the precision aspect of a model.
Am I missed anything here? Or mAP score, despite its name is derived from Precision, can indeed subsume both ""precision"" and ""recall"" and therefore become comprehensive enough? 
",['object-detection'],
Is it possible to control asymptotic behaviour of neural network models?,"
Is it possible to specify what the asymptotic behaviour of a Neural Networks (NN) model should be?
I am thinking of a NN which tries to learn a mapping $\vec y=f(\vec x)$ with $\vec x$ a vector of features of dimension $d$ and $\vec y$ a vector of outputs of dimension $p$.
Is it possible to specify that, for instance, the NN should have a fixed value when $x_1$ goes to infinite?
I mean:
$$
\lim_{x_1\to \infty} f(\vec x) = \vec c
$$
If it is not possible with NN, do you know other machine learning models (for instance  Gaussian Process Regression or Support Vector Regression) which have a known asymptotic behaviour?
","['neural-networks', 'machine-learning', 'deep-learning', 'computational-learning-theory']","
A simpler answer is that for a standard neural net, the asymptotic behaviour is the asymptotic behaviour of the output neurons. For example, if the output layer is ReLUs, then the asymptotic behaviour is necessarily linear.
In your case, since you want it to be asymptotically constant, you can use the slightly old-fashioned choice of sigmoid units in the output layer.
The training set is necessarily finite, so no machine learning method can learn asymptotic behaviour. It can only be supplied by prior knowledge, for example as I describe.
"
What could an oscillating training loss curve represent?,"

I tried to create a simple model that receives an $80 \times 130$ pixel image. I only had 35 images and 10 test images. I trained this model for a binary classification task. The architecture of the model is described below.
conv2d_1 (Conv2D)            (None, 80, 130, 64)       640       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 78, 128, 64)       36928     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 39, 64, 64)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 39, 64, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 39, 64, 128)       73856     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 37, 62, 128)       147584    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 18, 31, 128)       0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 18, 31, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 71424)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               36569600  
_________________________________________________________________
dropout_3 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513     

What could the oscillating training loss curve represent above? Why is the validation loss constant?
","['training', 'computer-vision', 'deep-neural-networks', 'object-recognition']","
Overview
As it has already been observed, your main problem, beside the training related issues like fixing the learning rate, is you have basically no chance to learn such a big model woth such a small dataset ... from scratch 
So focusing on the real problem, here are some techniques you could use 

dataset augmentation 
transfer learning 


from a pretrained model 
from the encoder stage of an autoencoder (last resort option before getting into more advanced topics)


Dataset Augmentation
Add transformations to your dataset you want your classifier to learn to be invariant to 
Let's assume that 

$I$ is an input image 
$l$ its associated label 
$f(\mathcal{I};\theta) \rightarrow \mathcal{I}$ is a parametric transformation that affects appearance but not semantic, for example it is a rotation of $\theta$ angle 

then you can augment your dataset by generating $\{I_{\theta}, l\}$ a set of transformed (e.g. rotated) images associated the same $l$ label 
Transfer Learning
The fundamental idea of transfer learning is to re-use a NN which has been trained to solve a task, to solve other tasks retraining only a selected subset of the weights 
It means using a pre-trained convolutive backend, the part of the model with Conv2D and Pooling, and train dense layers with dropout only (but you should still probably think about reducing the dimensionality there) 
More formally think about representing your CNN Classifier as follows 

$f_{C}(I; \theta_{X})$ : Convolutive Processing on Input Image 

it is the part of the CNN composed of Conv2D and MaxPooling2D layers 
the $\theta_{C}$ is the convolutive learnable weights set 

$b = f_{C}(I; \theta_{C})$ : Bottleneck Feature Representation 

it is the result of Flatten layer 

$f_{D}(b; \theta_{D})$ : Dense Processing 

it is the part of the model composed of Dense layers 
the $\theta_{D}$ is the dense learnable weights set 


The idea is to pick $\theta_{C}$ from a training performed on an another dataset, bigger than your current one, and keep it fixed while training in your task 
This means reducing the number of parameters to be trained, however beware the dense layers account for most of the weights, as you can also see from your mode summary, which means you should also focus on reducing that number, for example reducing the bottleneck feature tensor size 
Transfer Learning from Pre-Trained Model
For example, if your actual goal was to perform binary classification on some kind of MNIST-like data then you could use a convolutive backend from a CNN which has been pre-trained on the MNIST 0..9 classification task or you can train it yourself but is important is the $\theta_{C}$ weights will be learned from a MNIST dataset, which is much bigger than yours, even if the task is (slightly) different. 
Furthermore, in case of MNIST like data, please consider if you really need your full 80 x 130 resolution hence your input tensor, considering I can deduct from your model summary it is grayscale (no color), needs to be $(80,130,1)$ or you could rescale to the 28 x 28 MNIST resolution so you work with a smaller $(28,28,1)$ tensor 
My suggestion is to start from an architecture like this MNIST Keras Model as 

it has a bottleneck representation of 64 which could be enough for your task and 
also suggesting to remove the first dense layer so to significantly reduce $\theta_{D}$ the number of learnable paramters hence going for something like 

  model = Sequential()
  # add Convolutional layers
  model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(10, 10, 1)))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))
  model.add(MaxPooling2D(pool_size=(2,2)))    
  model.add(Flatten())
  # output layer
  model.add(Dense(1, activation='sigmoid'))


then compile the model with binary_crossentropy loss and maybe start giving a try to adam optimizer 
Transfer Learning from Autoencoder
If your data is so special you can't find any big enough and similar enough dataset to use this strategy and you do not come up with any transformation you could use 
to perform dataset augmentation, without getting into advanced things, you could try to play one last card: use an Autoencoder to learn a compressed representation aimed at reconstructing the original image and perform transfer learning with the encoder only 
For example, again under the assumption of working with a $(28,28,1)$ tensor, you could start with an architecture like the following one 
def build_ae(input_img): 
  x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)
  # (28,28,16)

  encoded = MaxPooling2D((8, 8), padding='same')(x)
  # (4,4,8)


  x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)
  # (4,4,8)

  x = UpSampling2D((8, 8))(x)
  # (16,16,8)

  x = Conv2D(16, (3, 3), activation='relu')(x)
  # Note: Convolving without padding='same' in order to get w-2 and h-2 dimensioality reduction so that following upsampling can lead to the desired 28x28 spatial resolution 
  # (14,14,8)

  x = UpSampling2D((2, 2))(x)
  # (28,28,8)

  decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)
  autoencoder = Model(input_img, decoded)
  return autoencoder


In this case, the full model has 2633 weights but the encoding stage consists only of Conv2D+Relu+MaxPooling which means in total 3x3x1x16 weights for the convolutive step and 16 weights for the relu for a total of 160 weights only and the latent representation is a $(4,4,8)$ tensor which means a 128 dimensional flattened tensor and hence assuming, as before, to perform the binary classification with a dense sigmoid layer it would mean 128+1 weights to learn in the actual binary classification task 
Of course it is possible to go for an even more compressed latent representation both on the spatial domain or channel domain with consequent reduced flattened vector dimensionality and ultimately even less weights to learn 
Would you share more details about your problem, also your dataset, we could try to help more 
"
Train on big dataset (1mil + images) [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I am in the process of collecting a huge dataset of Human poses captured images to create a model to classify poses.
My question is how will I be able to train on this massive dataset? I have multiple GPUs and Multiple machines access (Also have GCP).
What would be the best way to train on such huge dataset?
Thanks.
","['machine-learning', 'training', 'datasets']",
CNN - Visualizing images near decision boundary - Pixels inexplicably tend to edges,"
We are exploring the images classified by a CNN at its decision boundary, using Genetic Algorithms to generate them. We have created a fine-tuned binary grayscale image classifier for cats. As the base model, we are using an Inception-ResNet v2 pre-trained on the ImageNet dataset, and then fine-tune it with a subset of cat and non-cat images (grayscale) from ImageNet. The model achieves ~97% accuracy for a test set.
We have constrained the problem such that evolution starts from a pure white image, and random crossover and mutations are performed with only black pixels. Crossover and mutation probabilities are kept at 0.8 and 0.015 respectively. 
As an incentive to generate a ""cat"" with the minimum number of black pixels, I add a penalty for the black pixel count in the image. The initial population is a set of 100 white images that have a single random pixel coloured black in them.
The evolution generates images with only black and white pixels, and we have a fitness function that is taken as a linear transformation of loss calculated between target label and network prediction as follows;

loss = binary cross entropy (target, prediction) + (# of black pixels)

Target value (y)    = target label cat - in this case, 0.
                              = hyperparameter to weight the penalty for black pixel count.
Problem
My problem is that across multiple runs of evolution, all images classified as cats tend to have black pixels towards the edges of the image. Below is an example.

This image is classified as a cat with over 96% confidence.
I have tried different crossover mechanisms including

Random rectangular area swap between parents 
Alternating column interchange 
Direct black pixel crossover after encoding the image to
a reduced form that only kept track of the black pixels (black pixel
list is the genome)

Initially, we ran evolution with a similarly fine-tuned VGG-16 model, and then moved to the Inception ResNet due to better accuracy. Pixels tend to edges across models and crossover mechanisms. 
In one run, I explicitly constrained the evolution to perform mutations in the middle section of the images for 3,000 generations before lifting this restriction. But the images generated after that point always had better scores.
We are at a loss as to why the images never have pixels coloured in the middle.
Does anyone have any ideas on this? 

","['neural-networks', 'convolutional-neural-networks', 'genetic-algorithms']",
Developmental systems that try to explain or understand the reward value in the reinforcement learning?,"
Are there methods (possibly logical or (how they are called in the literature) relational) that allows for the developmental systems to understand or explain the value of the received reward during the developmental process. E.g. if the system (agent) can understand that reward is by the chance, then it should be processed quite differently than the reward that is just initial downpayment for the expected series of rewards. Credit assignment is the one method (especially for delayed rewards), but maybe there are different methods as well?
Relational reinforcement learning allows to learn symbolic transition and reward functions and emerging understanding of the reward by the agent can greatly facilitate the inner consciousness of the agent and the search process for the best transition and reward functions (symbolic search space can be enormous).
","['reinforcement-learning', 'logic', 'artificial-consciousness', 'rewards', 'symbolic-ai']",
Suggestions for Deep Learning for regression on huge 3D volumes,"
I have a dataset of 3D images (volumes) with dimensions 400x250x400. For each input image I have an output of the same dimensions. I would like to train a machine learning (or deep learning) model on this data in order to predict values with new data.
My main problems are :
Images are very big, which leads to memory issues (tried with an NVIDIA 2080Ti and doesn't fit on memory during training)
I need a very fast inference, because the model will be used on real time(speed is a requirment)
I already have experience with architectures such as 3D Unet using Keras with tensorflow backend, but it didn't worked for me because of the previous reasons, even with very few layers and convolution filters.
I know that one of the first solutions that one could imagine, is to reduce resolution of the volumes, but in my case I'm not allowed this because I would lose a lot of spatial information.
Any ideas or suggestions ? Maybe Neural Nets are not the best solution ? If not, what could I use ?
Thank you very much for your suggestions
","['neural-networks', 'machine-learning', 'deep-learning', 'python', 'tensorflow']",
What are some conferences for publishing papers on Deep Learning for Human Activity recognition?,"
What are some conferences for publishing papers on Deep Learning for Human Activity recognition? Do any of the major conferences have specific tracks for Human Activity Recognition?
","['deep-learning', 'reference-request', 'research', 'human-activity-recognition']","
If your using computer vision, the top recognised conference is CVPR (computer vision and pattern recognition).
You can also try to submit at ICML (International conference on machine learning) and NIPS (Neural Information Processing Systems), which focuses on applications of machine learning and deep learning.
I'd also recommend IJCAI (International joint conference on artificial intelligence).
"
How do I create a chatbot using tensorflow or pytorch using like the one defined in dialogflow?,"
How do I create a chatbot using TensorFlow or PyTorch using like the one defined in DialogFlow? What are the best datasets that I can use so to create my own personal assistant like google assistant?
I want to create a chatbot (an open-source project) as an assistant for custom tasks (like google assistant).
I have tried many neural network models like seq2seq but I couldn't get satisfiable results maybe because of the small dataset (I took from Simpsons movie script) or model (seq2seq). I am curious what model they use at google and what type of dataset they pick to give such good results or any normal person can create fully functional chatbots without relying on paid services ( like google's DialogFlow, api.ai, etc.) with good results.
I recently heard of OpenAi's implementation of a specific model named as gpt-2 which as they concluded in the paper showed remarkable performance but they didn't provide the dataset because of some reasons.
What I want to say there are a lot of resources and codes on the internet to make a working chatbot (or maybe that what they show) but when I try to replicate them I always fail to get even remotely close good results.
So I need proper guidance on how to make and train such chatbots with my own laptop (with 16GB RAM, 2GB GPU . I can even get better configuration) and no other money spent on google services or any such paid API's.
Please suggest something if someone got good results.
","['neural-networks', 'tensorflow', 'chat-bots']",
Alphazero Value loss doesn't decrease,"
Currenly I'm trying to reimplement alphazero in pure c++ using libtorch to accomodate my project's need. But when I training my model, I found out that the value loss doesn't decrese at all after even ~2000 iterations and the policy loss decreases pretty fast from the very begining.
Have anybody met any similar issue when developing your alphazero project? And could you give some suggestion of the cause of my issue based on your experience?
Many apreciates
","['alphazero', 'alphago-zero']",
"DDPG: how to implement continuous action space bounded in the interval [-2, 2]?","
I am a newbie in reinforcement learning and trying to understand how to implement continuous actions bounded by $[-2, 2]$. My research shows that doing nothing is a possible solution (i.e. action of 4.5 is mapped to 2 and the action of -3.1 is mapped to -2), but I wonder if there are more elegant approaches.
","['reinforcement-learning', 'deep-rl', 'ddpg', 'continuous-action-spaces']",
How to use machine learning to create combine of opposite images side by side,"
Inspired by: Two Worlds Pictures 


I just want to create a Machine Learning Model that can automatically combine the opposite images into 1 image.
I am thinking about 2 possible solutions:

Pose Estimation: Detect humans and their poses from image data and search an archive by pose, but totally out of context.
Land Lines: explore similar lines.

It's just my ideas, do you have any recommendations? 
Thanks
","['machine-learning', 'computer-vision']",
What are the state-of-the-art approaches for continual learning with neural networks?,"
There seems to be a lot of literature and research on the problems of stochastic gradient descent and catastrophic forgetting, but I can't find much on solutions to perform continual learning with neural network architectures.
By continual learning, I mean improving a model (while using it) with a stream of data coming in (maybe after a partial initial training with ordinary batches and epochs).
A lot of real-world distributions are likely to gradually change with time, so I believe that we should be able to train NNs in an online fashion.
Do you know which are the state-of-the-art approaches on this topic, and could you point me to some literature on them?
","['neural-networks', 'reference-request', 'incremental-learning', 'online-learning', 'catastrophic-forgetting']","

Do you know which are the state-of-the-art approaches on this topic, and could you point me to some literature on them?

This answer already mentions some of the approaches. More concretely, currently, the most common approaches to continual learning (i.e. learning with progressively more data while attempting to address the catastrophic forgetting problem) are

dynamic/changing topologies approaches
regularization approaches
rehearsal (or pseudo-rehearsal) approaches
ensemble approaches
hybrid approaches

You can also take a look at this answer. If you are interested in an exhaustive overview of the state-of-the-art (at least, until 2019), you should read the paper Continual lifelong learning with neural networks: A review (2019, by Parisi et al.).
"
Using ML to analyze Facebook posts,"
First of all, I should mention that I have a very basic knowledge of ML so I apologize if this question seems trivial or stupid.
I am working on a small personal project, basically an app that analyzes Facebook posts concerning movies and translates them into a rating (out of 100). The algorithm looks for keywords, the length of the post, etc.. to determine the individual rating, and then averages all the ratings among a user's FB friends to give the result. My question is, would I be able to drastically improve such algorithm by using ML or is it not worth it? If yes, what algorithms/techniques do you advise me to learn?
All help is appreciated!
","['machine-learning', 'applications', 'learning-algorithms']","
for this kind of ml training, you will need a ton of data first, at least in the thousands. If you have a bot program that fetches those data for you, AI is the way to go. I'm not sure how else you would do it though.
To train the nn you will need the inputs(the post) and the targets(the rating you want it to output). The targets could be anything you want, like the ratio of likes to views, etc.
There are tons of ML libraries out there and I recommend keras as it is easy to learn for beginners, hope it helps :)
"
How can I derive the rotation matrix from the axis-angle rotation vector?,"
Given an axis-angle rotation vector $\Theta = (2,2,0)$, after finding the unit vector $k=(1/\sqrt{2}, 1/\sqrt{2}, 0)$ and angle $\theta = 2\sqrt{2}$ representing the same rotation, I need to derive the rotation matrix $R$ representing the same rotation and to show, that the matrix is orthonormal. How can I do that?
","['math', 'robotics']","
The rotation matrix $R_k(\theta)$ associated with a given unit-length vector $k$ and angle $\theta$ is given by the following formula
$$\small{R_k(\theta) = \begin{bmatrix} 
\cos \theta +k_x^2 \left(1-\cos \theta\right) & k_x k_y \left(1-\cos \theta\right) - k_z \sin \theta & k_x k_z \left(1-\cos \theta\right) + k_y \sin \theta \\ 
k_y k_x \left(1-\cos \theta\right) + k_z \sin \theta & \cos \theta + k_y^2\left(1-\cos \theta\right) & k_y k_z \left(1-\cos \theta\right) - k_x \sin \theta \\ 
k_z k_x \left(1-\cos \theta\right) - k_y \sin \theta & k_z k_y \left(1-\cos \theta\right) + k_x \sin \theta & \cos \theta + k_z^2\left(1-\cos \theta\right)
\end{bmatrix}}$$
So, to find your specific rotation matrix, you just need to substitute the values of your $k$ and $\theta$ in the above matrix.
The derivation of this matrix can be found in section 9.2 Rotation Matrix Derivation of the PhD thesis Modelling CPV (2015), by Ian R. Cole. The basic idea of the derivation follows the following steps

Rotate the given axis $k$ and the point $p$ (that you want to rotate) such that the axis $k$ lies in one of the coordinate planes: xy, yz or zx
Rotate the given axis $k$ and the point $p$ (that you want to rotate) such that the axis $k$ is aligned with one of the two coordinate axes for that particular coordinate plane: $x$, $y$ or $z$
Use one of the fundamental rotation matrix to rotate the point $p$ depending on the coordinate axis with which the rotation axis is aligned
Reverse rotate the axis-point pair such that it attains the final configuration as that was in step 2 (that is, you have to undo step 2)
Reverse rotate the axis-point pair which was done in step 1 (that is, you have to undo step 1)

To show that a matrix is orthonormal, you need to show that it is orthogonal (each row is independent of any other row and each column is independent of any other column) and that the length of each row (and column) is 1. Equivalently, a square matrix $Q$ is orthogonal if and only if
$$
Q^TQ = QQ^T = I
$$
where $Q^T$ is the transpose of $Q$ and $I$ is the identity matrix. If you are really stuck, have a look at this proof https://math.stackexchange.com/a/537248/168764 (and this other answer https://math.stackexchange.com/a/156742/168764), but, at this point, it should just be a matter of replacing $R_k(\theta)$ in the equation above and check that the equation holds.
"
"Given an axis-angle rotation vector, how can I find the unit rotation axis and angle?","
I have a robotics assignment, which I am unable to solve. Given the axis-angle rotation vector $\Theta = (2, 2, 0)$, how can I calculate the unit vector of the rotation axis $k$ and the angle $\theta$?
","['math', 'robotics', 'homework']","
Given the axis-angle rotation vector $\Theta = (2, 2, 0)$, you can find the unit vector in the same direction by diving by the norm (or length) of $\Theta$, denoted by $\|\Theta\| = \sqrt{2^2 + 2^2 + 0^2} = \sqrt{8} = 2\sqrt{2}$. Therefore, the unit vector in the direction of $\Theta$ is $k= \Theta/\|\Theta\| = (1/\sqrt{2}, 1/\sqrt{2}, 0)$, which should be the axis of rotation that you're looking for. The angle should just be the norm of $\Theta$, that is, $\theta = \|\Theta\| = 2\sqrt{2}$. Note that $k \theta$ gives you your original vector $\Theta$.
"
What are the steps that I need to follow to build a neural network for face recognition?,"
I have developed face recognition algorithms by using pre-built libraries in Python and open CV. However, suppose if I want to make my own neural network algorithm for face recognition, what are the steps that I need to follow?
I have just seen Andrew Ng's course videos (specifically, I watched 70 videos).
","['neural-networks', 'machine-learning', 'face-recognition']","
For the Construction of Deep Learning Models
Backbone Deep Learning models which can be applied to a variety of deep learning tasks (including facial recognition) have been implemented in a range of libraries available in Python. I'm assuming by constructing your own algorithm you mean a novel implementation of the model structure. Taking the PyTorch framework as an example, some common pretrained models are available here:
https://github.com/pytorch/vision/tree/master/torchvision/models
To train a novel face recognition model you could follow the tutorial for object detection available here:
https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html
and make changes to the model.
In the tutorial they use model features from the library in the following section of code:
# load a pre-trained model for classification and return
# only the features
backbone = torchvision.models.mobilenet_v2(pretrained=True).features

For the simplest example torchvision.models.AlexNet.features look like this:
self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )

Adding or subtracting layers from this backbone feature extractor would result in a new ""algorithm"" for object detection.
If you want to know exactly what mathematical operation each of these layers is performing you can look at the PyTorch documentation. For example, in the case of nn.Relu layer:
https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html
Applies the rectified linear unit function element-wise:
$$ ReLU(x)=(x)^{+}=max(0,x)$$
"
Is there an standard algorithm for giving options from an RNN?,"
You can feed books to an RNN and it learn how to produce text.
What I'm interested in is an algorithm that, given say 20 letters it suggest, say the best 10 options for the next 10 letters.
So for example it begins with ""The cat jumped ""
and then we get various options such as ""over the dog"". ""on the table"" and so on.
My initial thoughts are to first use the most likely next letters. Then find the letter which is most uncertain and change this to the second likely next letter. And repeat this process.
(Then I may have another evaluation neural network to assess which is ""best"" English.)
In other words I want the RNN to ""think ahead"" at what it's saying - much like a chess playing machine.
",['recurrent-neural-networks'],
Which NLP techniques can be used to transform sentences (e.g. from passive to active voice) without affecting their meaning?,"
I'm looking for NLP techniques to transform sentences without affecting their meaning.
For example, techniques that could transform active voice into passive voice, such as

The cat was chasing the mouse.

to

The mouse was being chased by the cat.

I can think of a number of heuristics one could implement to make this happen for specific cases, but would assume that there is existing research on this in the field of linguistics or NLP. My searches for ""sentence transformation"" and similar terms didn't bring up anything though, and I'm wondering if I simply have the wrong search terms.
Related to this, I'm also looking for measures of text consistency, e.g., an approach that could detect that most sentences in a corpus are written in active voice and detect outliers written in passive voice. I'm using active vs. passive voice as an example here and would be interested in more general approaches.
","['natural-language-processing', 'reference-request', 'computational-linguistics']",
"An intuitive explanation of Adagrad, its purpose and its formula","

It (Adagrad) adapts the learning rate to the parameters, performing smaller updates
  (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features.

From Sebastian Ruder's Blog
If a parameter is associated with an infrequent feature then yes, it is more important to focus on properly adjusting that parameter since it is more decisive in classification problems. But how does making the learning rate higher in this situation help?
If it only changes the size of the movement in the dimension of the parameter (makes it larger) wouldn't that make things even more imprecise? Since the network depends more on those infrequent features, shouldn't adjusting those parameters be done more precisely instead of just faster? The more decisive parameters should have a higher ""slope"", thus why should they also have high learning rates? I must be missing something, but what is it?
Further, in the article, the formula for parameter adjustments with Adagrad is given. Where exactly in that formula do you find the information about the frequency of a parameter? There must be a relationship between the gradients of a parameter and the frequency of features associated with it because it's the gradients that play an important role in the formula. What is that relationship?
TLDR: I don't understand both the purpose and formula behind Adagrad. What is an intuitive explanation of it that also provides an answer to the questions above, or shows why they are irrelevant?
","['machine-learning', 'gradient-descent', 'hyperparameter-optimization']","
I found a somewhat more accessible introduction here: 
https://medium.com/konvergen/an-introduction-to-adagrad-f130ae871827
Let me start from the last part of your question. The frequency of a parameter is in G_t, which is the accumulated sum of squared gradients from all the time steps up to step t. If the gradient vanishes in many of the previous steps, then you divide the learning rate with a smaller number for that parameter. 
And for the first part, you want the parameter that is more frequent to have a smaller learning rate as it is updated on more iterations compared to a parameter which is updated only a small number of times. 
"
How to select the most appropriate set of actions for a given environment or task?,"
Given a robot in a situation such as in a library reading a book. 
Now I want to create a neural network that suggests an appropriate action in this situation. And, generally, ignore actions such as ""get up and dance"" and so on.
Since there are limitless actions a robot could do, I need to narrow it down to the ones in this situation. Using its vision system, the word ""book"" and book neurons should already be activated as well as ""reading"".
One idea I had was to create an adversarial network which generates words (sequences of letters) based on the situation such as ""turn page"", ""read next line"" and so on. And then have another neural network which translates these words into actions. (It would them simulate whether this was a good idea. If not it would somehow suppress the first word and try to generate a new word.)
Another example is the robot is in a maze and gets to a crossroads. The network would generate the word ""turn left"" and ""turn right"".
Another idea would be to have the actions be composed of a body part e.g. ""eyes"" and a movement such as ""move left"" and it would combine these to suggest actions.
Either way, it seems like I need a way to encode actions so that the robot doesn't consider every possible action in the universe.
Is there any research in this area or ideas on how to achieve this?
(I think this may be somewhat related to the task of ""try to name as many animals as you can."")
","['ai-design', 'action-model-learning']","

One idea I had was to create an adversarial network which generates words

It may be better to use for e.g. numbers instead of words, then map these numbers to the language of your choice like English in your case. This mapping can be changed to other languages and you will have a robot that overcomes language barriers.
"
What is the difference between Squeeze-and-excite and bottleneck modules from Mobilenet v2?,"
Squezee-and-excite networks introduced SE blocks, while MobileNet v2 introduced linear bottlenecks.
What is the effective difference between these two concepts?
Is it only implementation (depth-wise convolution, vs per-channel pooling), or they serve a different purpose?
My understanding is that both approaches are used as attention mechanism, working per-channel. In other words, both approaches are used to filter unnecessary information (information that we consider noise, not signal).
Is this correct?
Do bottlenecks ensure, that the same feature won't be represented multiple times in different channels, or they don't help at all in this regard?
","['neural-networks', 'convolutional-neural-networks', 'comparison', 'papers', 'attention']",
What are the advantages of Machine Learning compared to traditional programming for developing a chatbot?,"
I am currently building a chatbot. What I have done so far is, collected possible questions/training data/files and create a model out of it using Apache OpenNLP; the model is able to predict all the questions that are in the training data and fails to predict for new questions.
Instead of doing all the above, I can write a program that matches the question/words against training data and predict the answer  what is the advantage of using Machine Learning algorithms?
I have searched extensively about this and all I got was, in Machine Learning there is no need to change the algorithm and the only change would be in the training data, but that is the case with programming too: the change will be in training data.
","['machine-learning', 'natural-language-processing', 'comparison', 'chat-bots', 'question-answering']","
In my view ML does not work very well for conversational AI systems. It is generally alright for intent recognition, so getting what the user wants if they ask a question (""I want to book a flight?"", ""What is the weather in London?""), but anything after that quickly becomes difficult to handle, especially multi-step conversations that go beyond simple question/answer pairs.
My suggestion would be to plan possible dialogues out as flow charts (more like trees/graphs, as there can be multiple branches at any point), and then write a program that interprets the graph based on user input and gives appropriate replies. You will also want to have some conversational memory to keep track of any information the user has mentioned. That is also tricky to do in a ML system.
For a very simple framework to start off with, have a look at ELIZA. It's half a century old, but you can still use it as a starting point.
(Disclaimer: I work for a company that makes conversational AI systems)
"
What is the cognitive architecture with the highest IIT measure?,"
What contemporary information system or cognitive architecture is the one with the highest measure of the Integrated Information Theory (IIT) (that is, a theory of consciousness, which states that a system's consciousness is determined by its causal properties and is, therefore, an intrinsic, fundamental property of any physical system)
Is there race/competitions to develop (or allow autonomous development) the system with the maximum IIT measure?
","['reference-request', 'cognitive-architecture', 'integrated-information-theory']",
What is the difference between graph convolution in the spatial vs spectral domain?,"
I've been reading different papers regarding graph convolution and it seems that they come into two flavors: spatial and spectral. From what I can see the main difference between the two approaches is that for spatial you're directly multiplying the adjacency matrix with the signal whereas for the spectral version you're using the Laplacian matrix.
Am I missing something, or are there any other differences that I am not aware of?
","['comparison', 'convolution', 'geometric-deep-learning', 'graph-neural-networks']","
Spectral Convolution
In a spectral graph convolution, we perform an Eigen decomposition of the Laplacian Matrix of the graph. This Eigen decomposition helps us in understanding the underlying structure of the graph with which we can identify clusters/sub-groups of this graph. This is done in the Fourier space.
An analogy is PCA where we understand the spread of the data by performing an Eigen Decomposition of the feature matrix. The only difference between these two methods is with respect to the Eigen values. Smaller Eigen values explain the structure of the data better in Spectral Convolution whereas it's the opposite in PCA.
ChebNet, GCN are some commonly used Deep learning architectures that use Spectral Convolution
Spatial Convolution
Spatial Convolution works on local neighbourhood of nodes and understands the properties of a node based on its k local neighbours. Unlike Spectral Convolution which takes a lot of time to compute, Spatial Convolutions are simple and have produced state of the art results on graph classification tasks. GraphSage is a good example for Spatial Convolution.
Additional References: https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49
https://towardsdatascience.com/graph-convolutional-networks-for-geometric-deep-learning-1faf17dee008
"
"What is the ""thing"" which is trained in AI model training [closed]","







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I am a newbie in the fantastic AI world, I have started my learning recently. 
After a while, my understanding is, we need to feed in tremendous data to train a or many models. 
Once the training is complete, we could take out the trained models and ""plug in"" to any other programming languages to use to detect things.
So my questions are:
1. What are the trained models? are they algorithms or a collection of parameters in a file?
2. What do they look like? e.g. file extensions
3. Especially, I want to find the trained models for detecting birds (the bird types do not matter). Are there any platforms for open-source/free online trained AI models??
Thank you!
","['machine-learning', 'training', 'models']","


What are the trained models? are they algorithms or a collection of parameters in a file?


""Model"" could refer to the algorithm with or without a set of trained parameters.
If you specify ""trained model"", the focus is on the parameters, but the algorithm is implicitly part of that, since without the algorithm, the parameters are just an arbitrary set of numbers.


What do they look like? e.g. file extensions


That very much depends on both the algorithm you're using and the specific implementation. A few simple examples might help clarify matters.  Let's suppose that the problem we're trying to learn is the exclusive or (XOR) function:
a | b | a XOR b
--+---+---------
0 | 0 | 0
0 | 1 | 1
1 | 0 | 1
1 | 1 | 0


First, let's use a 2-layer neural net to learn it.  We'll define our activation function to be a simple step function:
$ f(x) = \begin{cases}
1 & \text{if } x > 0.5 \\
0 & \text{if } x \le 0.5
\end{cases} $
(This is actually a terrible activation function for real neural nets since it's non-differentiable, but it makes the example clearer.)
Our model is:
$h_0 = f(1\cdot a+1\cdot b + 0)\\
 h_1 = f(0.5\cdot a + 0.5\cdot b + 0)\\
 \,\;y = f(1\cdot h_0 - 1\cdot h_1 + 0)$
Each step of this essentially draws a hyperplane and evaluates to 1 if the input is on one side of the hyperplane and 0 otherwise.  In this particular case, h_0 tells us if either a or b is true. h_1 tells us if they're both true, and y tells us if exactly one of them is true, which is the exact definition of the XOR function.
Our parameters are the coefficients and biases (the offset added at the end of each expression):
$ \begin{bmatrix}
1 & 1 & 0 \\
0.5 & 0.5 & 0 \\
1 & 1 & 0 \\
\end{bmatrix}$
They can be stored in a file in any way we want; all that matters is that the code that stores them and the code that reads them back agree on the format.

Now let's solve the same problem using a decision tree. For these, we traverse a tree, and at every node, ask a question about the input to decide which child to visit next. Ideally, each question will divide the space of possibilities exactly in half. Once we reach a leaf node, we know our answer.
In this diagram, we visit the right child iff the expression is true.
     a+b=2
    /     \
  a+b=0    0
 /     \     
0       1

In this case, the model and parameters are harder to separate. The only part of the model that isn't learned is ""It's a tree"".  The expressions in each interior node, the structure of the tree, and the value of the leaf nodes are all learned parameters. As with the weights from the neural network, we can store these in any format we want to.

Both methods are learning the same problem, and actually find basically the same solution: a XOR b = (a OR b) AND NOT (a AND B).  But the nature of the mathematical model we use depends on the method we choose, the parameters depend on what we train it on, the file format depends on the code we use to do it, and the line between model and parameter is fairly arbitrary; the math works out the same regardless of how we split it up.  We could even write a program that tries different methods, and outputs a program that classifies inputs using the method that performed best. In this case, the model and parameters aren't separate at all.


Especially, I want to find the trained models for detecting birds (the bird types do not matter). Are there any platforms for open-source/free online trained AI models??


I don't know of any pretrained models that specifically recognize birds, but I'm not in image-recognition, so that doesn't mean much.  If you're not averse to training your own model (using existing code), I believe the ImageNet dataset includes birds.  AlexNet and LeNet would probably be good starting points for the model. Most if not all of the the state of the art image recognition models are based on convolutional networks, so you'll need a decent GPU to run them.
"
Why do we get a three-dimensional output after a convolutional layer?,"
In a convolutional neural network, when we apply the convolution on a $5 \times 5$ image with $3 \times 3$ kernel, with stride $1$, we should get only one $4 \times 4$ as output. In most of the CNN tutorials, we are having $4 \times 4 \times m$ as output. I don't know how we are getting a three-dimensional output and I don't know how we need to calculate $m$. How is $m$ determined? Why do we get a three-dimensional output after a convolutional layer?
","['convolutional-neural-networks', 'computer-vision', 'image-processing', 'convolution']",
Aesthetics analysis with deep learning,"
I'm trying to score video scenes in terms of aesthetics and cinematography features. Basically, how ""interesting"" a scene or video frame can be for a viewer. Simpler, how attractive a scene is. My final goal is to tag intervals of video which can be more interesting to viewers. It can be a ""temporal attention"" model as well.
Do we have an available model or prototype to score cinematographic features of an image or a video? I need a starter tutorial on that. Basically, a ready-to-use prototype/model that I can test as opposed to a paper that I need to implement myself. Paper is fine as long as the code is open-source. I'm new and can't yet write a code given a paper.
","['neural-networks', 'deep-learning', 'computer-vision', 'image-processing', 'art-aesthetics']",
Is AI and Big Data science recommending a shift in the scientific method from inductive to deductive reasoning?,"
Is this true?  Are we planning to switching such reasoning methods regarding AI tech in the future?
",['philosophy'],"
You could argue that AI and big data are trying to switch the AI method from deductive to inductive reasoning in the sense that original AI was deductive (if...then conditionals) but deep learning implies inductive reasoning (feed the network a million images of white swans and the network will ""conclude"" all swans are white - the classic example of (erroneous) inductive reasoning.)
"
How does Continuous Bag of Words ensure that similar words are encoded as similar embeddings?,"
This is related to my earlier question, which I'm trying to break down into parts (this being the first). I'm reading notes on word vectors here. Specifically, I'm referring to section 4.2 on page 7. First, regarding points 1 to 6 - here's my understanding:
If we have a vocabulary $V$, the naive way to represent words in it would be via one-hot-encoding, or in other words, as basis vectors of $R^{|V|}$ - say $e_1, e_2,\ldots,e_{|V|}$. We want to map these to $\mathbb{R}^n$, via some linear transformation such that the images of similar words (more precisely, the images of basis vectors corresponding to similar words) have higher inner products. Assuming the matrix representation of the linear transformation given the standard basis of $\mathbb{R}^{|V|}$ is denoted by $\mathcal{V}$, then the ""embedding"" of the $i$-th vocab word (i.e. the image of the corresponding basis vector $e_i$ of $V$) is given by $\mathcal{V}e_i$.
Now suppose we have a context ""The cat ____ over a"", CBoW seeks to find a word that would fit into this context. Let the words ""the"", ""cat"", ""over"", ""a"" be denoted (in the space $V$) by $x_{i_1},x_{i_2},x_{i_3},x_{i_4}$ respectively. We take the image of their linear combination (in particular, their average):
$$\hat v=\mathcal{V}\bigg(\frac{x_{i_1}+x_{i_2}+x_{i_3}+x_{i_4}}{4}\bigg)$$
We then map $\hat v$ back from $\mathbb{R}^n$ to $\mathbb{R}^{|V|}$ via another linear mapping whose matrix representation is $\mathcal{U}$: $$z=\mathcal{U}\hat v$$
Then we turn this score vector $z$ into softmax probabilities $\hat y=softmax(z)$ and compare it to the basis vector corresponding to the actual word, say $e_c$. For example, $e_c$ could be the basis vector corresponding to ""jumped"". 
Here's my interpretation of what this procedure is trying to do: given a context, we're trying to learn maps $\mathcal{U}$ and $\mathcal{V}$ such that given a context like ""the cat ____ over a"", the model should give a high score to words like ""jumped"" or ""leaped"", etc. Not just that - but ""similar"" contexts should also give rise to high scores for ""jumped"", ""leaped"", etc. For example, given a context ""that dog ____ above this"" wherein ""that"", ""dog"", ""above"", ""this"" are represented by $x_{j_1},x_{j_2},x_{j_3},x_{j_4}$, let the image of their average be
$$\hat w=\mathcal{V}\bigg(\frac{x_{j_1}+x_{j_2}+x_{j_3}+x_{j_4}}{4}\bigg)$$
This gets mapped to a score vector $z'=\mathcal{U}\hat w$. Ideally, both score vectors $z$ and $z'$ should have similarly high magnitudes in their components corresponding to similar words ""jumped"" and ""leaped"".
Is my above understanding correct? Consider the following quote from the lectures:

We create two matrices, $\mathcal{V} \in \mathbb{R}^{n\times |V|}$ and $\mathcal{U} \in \mathbb{R}^{|V|\times n}$, where $n$ is an arbitrary size which defines the size of our embedding space. $\mathcal{V}$ is the input word matrix such that the $i$-th column of $\mathcal{V}$ is the $n$-dimensional embedded vector for word $w_i$ when it is an input to this model. We denote this $n\times 1$ vector as $v_i$. Similarly, $\mathcal{U}$ is the output word matrix. The $j$-th row of $\mathcal{U}$ is an $n$-dimensional embedded vector for word $w_j$ when it is an output of the model. We denote this row of $\mathcal{U}$ as $u_j$.

It's not obvious to me why $v_i=\mathcal{V}e_i$ should be the same as or even similar to $u_i$. How does the whole backpropagation procedure above ensure that?
Also, how does the procedure ensure that basis vectors corresponding to similar words $e_i$ and $e_j$ are mapped to vectors in $\mathbb{R}^n$ that have high inner product? (In other words, how is it ensured that if words no. $i_1$ and $i_2$ are similar, then $\langle v_{i_1}, v_{i_2}\rangle$ and $\langle u_{i_1}, u_{i_2}\rangle$ have high values?)
","['natural-language-processing', 'word-embedding', 'word2vec']",
Why isn't conditional probability sufficient to describe causality?,"
I read these comments from Judea Pearl saying we don't have causality, physical equations are symmetric, etc. But the conditional probability is clearly not symmetric and captures directed relationships. 
How would Pearl respond to someone saying that conditional probability already captures all we need to show causal relationships?
","['philosophy', 'comparison', 'conditional-probability', 'causation']","

But the conditional probability is clearly not symmetric and captures directed relationships.

One needs to consider the kinds of directed relationships that is captured by conditional probability. It surely does capture some kind of association or dependence which could be directed. At the same time, it is not right to say that it surely captures the causal relationships. 
Let:
Sun rises = $A$, Rooster crows = $B$, then, $P(A |B)$ is bound to be very high but it does not mean that rooster crowing causes sunrise.  

How would Pearl respond to someone saying that conditional probability already captures all we need to show causal relationships?

He will ask him to go back to school.
"
"What does it mean for AlphaZero's network to be ""fully trained""","
Reading this blog post about AlphaZero:
https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go
It uses language such as ""the amount of training the network needs"" and ""fully trained"" to describe how long they had the machine play against itself before they stopped training. They state training times such as 9 hours, 12 hours, and thirteen days for chess, shogi, and Go respectively. Why is there a point at which the training ""completes?"" They show plots of AlphaZero's performance on the Y axis (its Elo rating) as a function of the number of training steps. Indeed, the performance seems to level out as the number of training steps increases beyond a certain point. Here's a picture from that site of the chess performance vs training steps:

Notice how sharply the Elo rating levels off as a function of training steps.

First: am I interpreting this correctly? That is, is there an asymptotic limit to improvement on performance as training sessions tend to infinity? 
If I am interpreting this correctly, why is there a limit? Wouldn't more training mean better refinement and improvement upon its play? It makes sense to me that the millionth training step may yield less improvement than the very first one, but I wouldn't expect an asymptotic limit. That is, maybe it gets to about 3500 Elo points in the first 200k training steps over the course of the first 10 hours or so of playing ches. If it continued running for the rest of the year, I'd expect it to rise significantly above that. Maybe double its Elo rating? Is that intuition wrong? If so, what are the factors that limit its training progress beyond the first 10 hours of play?

Thanks!
","['training', 'alphazero']",
Why is the derivative of the activation functions in neural networks important?,"
I'm new to NN. I am trying to understand some of its foundations. One question that I have is: why the derivative of an activation function is important (not the function itself), and why it's the derivative which is tied to how the network performs learning? 
For instance, when we say a constant derivative isn't good for learning, what is the intuition behind that? Is the activation function somehow like a hash function that needs to well differentiate small variance in inputs?
","['neural-networks', 'machine-learning', 'deep-learning', 'math', 'activation-functions']","
Consider a dataset $\mathcal{D}=\{x^{(i)},y^{(i)}:i=1,2,\ldots,N\}$ where $x^{(i)}\in\mathbb{R}^3$ and $y^{(i)}\in\mathbb{R}$ $\forall i$
The goal is to fit a function that best explains our dataset.We can fit a simple function, as we do in linear regression. But that's different about neural networks, where we fit a complex function, say:
$\begin{align}h(x) & = h(x_1,x_2,x_3)\\
& =\sigma(w_{46}\times\sigma(w_{14}x_1+w_{24}x_2+w_{34}x_3+b_4)+w_{56}\times\sigma(w_{15}x_1+w_{25}x_2+w_{35}x_3+b_5)+b_6)\end{align}$
where, $\theta = \{w_{14},w_{24},w_{34},b_4,w_{15},w_{25},w_{35},b_5,w_{46},w_{56},b_6\}$ is the set of the respective coefficients we have to determine such that we minimize:
$$J(\theta) = \frac{1}{2}\sum_{i=1}^N (y^{(i)}-h(x^{(i)}))^2$$
The above optimization problem can be easily solved with gradient descent. Just initiate $\theta$ with random values and with proper learning parameter $\eta$, update as follows till convergence:
$$\theta:=\theta-\eta\frac{\partial J}{\partial \theta}$$
In order to get the gradients, we express the above function as a neural network as follows:

Let's calculate the gradient, say w.r.t. $w_{14}$.
$$\frac{\partial J}{\partial w_{14}} = \sum_{i=1}^N \Big[\big(h(x^{(i)})-y^{(i)}\big)\frac{\partial h(x^{(i)})}{\partial w_{14}}\Big]$$
Let $p(x) = w_{14}x_1+w_{24}x_2+w_{34}x_3+b_4$ , and
Let $q(x) = w_{46}\times\sigma(p(x))+w_{56}\times\sigma(w_{15}x_1+w_{25}x_2+w_{35}x_3+b_5)+b_6)$
$\therefore \frac{\partial h(x)}{\partial w_{14}} = \frac{\partial h(x)}{\partial q(x)}\times\frac{\partial q(x)}{\partial p(x)}\times\frac{\partial p(x)}{\partial w_{14}} = \frac{\partial\sigma(q(x))}{\partial q(x)}\times\frac{\partial\sigma(p(x))}{\partial p(x)}\times\frac{\partial p(x)}{\partial w_{14}}$
We see that the derivative of the activation function is important for getting the gradients and so for the learning of the neural network. A constant derivative will not help in the gradient descent and we won't be able to learn the optimal parameters.
"
"Is there any use of using 3D convolutions for traditional images (like cifar10, imagenet)?","
I am curious if there is any advantage of using 3D convolutions on images like CIFAR-10/100 or ImageNet. I know that they are not usually used on this data set, though they could because the channel could be used as the ""depth"" channel.
I know that there are only 3 channels, but let's think more deeply. They could be used deeper in the architecture despite the input image only using 3 channels. So, we could have at any point in the depth of the network something like $(C_F,H,W)$ where $C_F$ is dictated by the number of filters and then apply a 3D convolution with kernel size less than $C_F$ in the depth dimension.
Is there any point in doing that? When is this helpful? When is it not helpful?
I am assuming (though I have no mathematical proof or any empirical evidence) that if the first layer aggregates all input pixels/activations and disregards locality (like a fully connected layer or conv2D that just aggregates all the depth numbers in the feature space), then 3D convolutions wouldn't do much because earlier layers destroyed the locality structure in that dimension anyway. It sounds plausible but lacks any evidence or theory to support it.
I know Deep Learning uses empirical evidence to support its claims so perhaps there is something that confirms my intuition?
Any ideas?

Similar posts:

https://forums.fast.ai/t/are-there-any-successful-vision-models-that-use-3d-convolutions/52503/2

When should I use 3D convolutions?

Which neural network architectures are there that perform 3D convolutions?


","['deep-learning', 'convolutional-neural-networks', '3d-convolution']","
As far as I've seen, there is no use of 3D CNNs for traditional image classification tasks.
The reason I think is that, while these images do have multiple channels, there is no spatial information in those channels for the 3D convolution to extract. On the other hand, it makes more sense to take the weighted sum of those pixels along that dimension (as the 2D convolution does).
3D CNNs have been used, as far as I know, only for applications where you have volumetric data, i.e. the images are sequential and, when combined, form a large 3D image.
"
How to show Monte Carlo methods converge to an estimate which minimizes mean squared error?,"
In chapter six of Sutton and Barto (p.128), they claim Monte Carlo methods converge to an estimate minimizing the mean squared error. How can this be shown formally?
Bump
","['reinforcement-learning', 'proofs', 'monte-carlo-methods', 'convergence']",
How to show temporal difference methods converge to MLE?,"
In chapter 6 of Sutton and Barto (p. 128), they claim temporal difference converges to the maximum likelihood estimate (MLE). How can this be shown formally?
","['reinforcement-learning', 'proofs', 'convergence', 'temporal-difference-methods']",
Reinforcement learning: How to deal with illegal actions? [duplicate],"







This question already has answers here:
                                
                            




How should I handle invalid actions (when using REINFORCE)?

                                (5 answers)
                            

Closed 2 years ago.



I'm a beginner of RL and currently trying to make DQN agent that can act optimally in a simple situation. 
In the situation agent should decide at what rate to charge or discharge the electrical battery, which is equivalent to buying the electrical energy or selling it, for making money by means of arbitrage. So the action space is for example [-6, -4, -2, 0, 2, 4, 6]kW. The negative numbers mean discharging, and the positive numbers mean charging.
In a case that battery is empty, discharging actions(-6, -4, -2) should be forbidden.
Otherwise in a case that battery is fully charged, charging actions(2, 4, 6) should be forbidden.
To deal with this issue, I tried two approaches:

In every step, renewing the action space, which means masking the forbidden action.
Give extreme penalties for selecting forbidden actions (in my case the penalty was -9999)

But none of them worked. 
For the first approach, the training curve (the cumulative rewards) didn't converge.
For the second approach, the training curve converged, but the charging/discharging results are not reasonable (almost random results).
I think in second approach, a lot of forbidden actions are selected randomly by the epsilon-greedy policy, and these samples are stored in experience memory, which negatively affect the result.
for example:
The state is defined as [p_t, e_t] where p_t is the market price for selling (discharging) the battery, and e_t is the amount of energy left in the battery.
When state = [p_t, e_t = 0], and discharging action (-6), which is forbidden action in this state, is selected, the next state is [p_t, e_t = -6]. And then the next action (2) is selected, then the next state is [p_t, e_t = -4] and so on.
In this case the < s, a, r, s' > samples  are:
< [p_t, 0], -6, -9999, [p_t+1, -6] >
< [p_t, -6], 2, -9999, [p_t+1, -4] > ...
These are not expected to be stored in the experience memory because they are not desired samples (e_t should be more than zero). I think this is why desired results didn't come out.
So what should I do? Please help.
","['reinforcement-learning', 'q-learning', 'dqn']","
In my project I also had the problem that the action space is not the same for every state of the environment. I do not like the approach to penalty forbidden actions with a high negative reward since it feels a bit like cheating. However it might work, I just haven't tried it. 
The approach I used, which you could apply as well, is to integrate an additional function into your action space. This function would map an action to a specific amount of kW. Thereby, depending on the current state, the function maps the action to the amount of kW to charge or discharge your battery with. This has the advantage that you do not have to deal with illegal actions.
This could be applied as follows:
Instead of defining for every action the amount to charge / discharge your battery with, you create a set of functions that defines the respective amount. Here an example with five actions:

Action: Discharge the battery entirely
Action. Discharge the battery so that half of its capacity remains,
otherwise do nothing
Action: do nothing
Action: charge the batter to half its capacity, otherwise do nothing
Action: charge the battery to its maximum capacity

"
How to train and update weights of filters,"
I have some problems with training CNN :(
For example:
Input 6x6x3, 1 core 3x3x3, output = 4x4x1 => pool: 2x2x1

By backpropagation I calculated deltas for output.
This tutor and other tutors are explain to calc deltas for weights and Input only for 2D:
input*output=deltas for 2D weights
filter*out = input delta
But how I can to calc weights deltas for 3D filters?
I must to multiply each input by output as below?
FilterLayer1Delta = OutputDelta * InputLayer1 ?
FilterLayer2Delta = OutputDelta * InputLayer2 ?
FilterLayer3Delta = OutputDelta * InputLayer3 ?
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'backpropagation']",
Video summarization similar to Summe's TextRank [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



We have the popular TextRank API which given a text, ranks keywords and can apply summarization given a predefined text length. 
I am wondering if there is a similar tool for video summarization. Maybe a library, a deep model or ML-based tool that given a video file and a length, it ranks frames, or video scenes/shots. I'd like to generate a short summary of a video with visual features.
","['machine-learning', 'deep-learning', 'computer-vision', 'text-summarization']",
What is the difference between 2d vs 3d convolutions?,"
I was trying to understand the definition of 2d convolutions vs 3d convolutions. I saw the ""simplest definition"" according to Pytorch and it seems the following:

2d convolutions map $(N,C_{in},H,W) \rightarrow (N,C_{out},H_{out},W_{out})$
3d convolutions map $(N,C_{in},D,H,W) \rightarrow (N,C_{out},D_{out},H_{out},W_{out})$

Which make sense to me. However, what I find confusing is that I would have expected images to be considered 3D tensors but we apply 2D convolutions to them. Why is that? Why is the channel tensor not part of the ""definitionality of the images""?
I also asked this question at https://forums.fast.ai/t/what-is-the-difference-between-2d-vs-3d-convolutions/52495.
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'comparison']","
Looking at it from the perspective of input to output in that fashion is probably not the best. So lets start with our goal and how these ND convolutions accomplish that (Note these are in my own words, and may not be best stated).  
Assumption: There exists highly correlative local associations  
Goal: Have a linear model that takes advantage of these local associations  
Solution: The ND Convolution  
Explanation: ND convolutions take advantage of our locality assumption by connecting only local nodes/neurons. The fact that its a sliding window allows us to learn filters for any location along with ones that can be reusable.  
Your Question: Where does the N in ND convolution matter?:
When using an ND convolution we are working off the assumption that there exists this locality in N dimensions and nothing more. So we connect all other components of the input in a dense matter because we have no assumptions to work on in this space. So now going to the shapes you mentioned such as the input and output of the 2D convolution. We are convolving a filter of size $(C_{in}, k_h, k_w)$ with a $(C_{in},H,W)$ activation shaped map (the $N$ just refers to the number of activation maps, and there is no association between them). We use $C_{in}$ channels on the kernel because we are not making any assumptions about locality between channels.  On the other hand in a 3D convolution we make locality assumption is 3 dimensions, so our kernel will be ($C_{in}, k_h, k_w, k_r$). These kernel sizes are actually determined by the input size, the amount of dimensions of the kernel will actually match the inputs (minus the batch) because it needs to densely match each one.  
You may be thinking now, that in torch this is not the case: This is because its rare to want a 2D Convolution with an inputs that's different than the one you mention, so they only implemented it for a singular shape. I hope this clears up the convolutions and helps you understand not just for 2 and 3 dimensional convolutions, but for all N. 
"
How can I detect fast and slow motion in videos?,"
I'm trying to detect if a given video shot is fast or slow motion. Basically, I need to calculate a ""video motion"" score in a given video sequence, meaning how fast or slow motion the video is. For instance, if a video is about a car racing or camera moving fast, the score is high. Whereas if the video is about two persons standing/talking, then the motion is low, so the lower score.
What comes into my mind is using optical flow which is already an implemented function in OpenCV. I never used it. But I don't know how to interpret or use it for a ""motion score"".
Is optical flow applicable here? How can I use it to calculate a score? In particular, if there is a ML/Deep learning model that already does it, please share it.
","['deep-learning', 'computer-vision', 'long-short-term-memory', 'image-processing', 'signal-processing']",
LSTM in reinforcement learning [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



Please tell me that is the LSTM network for the problem of reinforcement learning, as I explain to her what she will get the reward of a prediction, because the output will contain only actions?
Well, well, let's say at first I can play and upload my actions to training so that she sees which actions are right, that is, which she should strive for, but how to make her learn relatively independently?
","['reinforcement-learning', 'keras', 'long-short-term-memory']","
You can use LSTM in reinforcement learning, of course. You don't give actions to the agent, it doesn't work like that.
The agent give actions to your MDP and you must return proper reward in order to teach the agent. For example if you implement trading bot, the policy(policy=the agent, which is your LSTM network) will say that at step T it is going to have action 34, which means something to your MDP and you return reward for example -0.03 or +0.05 or whatever depending what that actions is doing at the moment T. 
So I get the question like you want to do a supervised learning on a reinforcement learning environment.
You can mimic supervised learning as well, but the idea of reinforcement learning is not that.
Here is how to mimic:
Scenario: you are at step T, lets say you have 3 possible actions -1,0,+1;
In a supervised learning you must give the desired action to the learning process.
In reinforcement learning you must give reward based on if you are happy or not from the agent's action.
So you must have predefined that for -1 you are not happy and you give reward 0.0, for action 0 you are not happy and you give reward 0.0 and for action +1 you are happy and you give reward +100;
I hope this makes things clear.
"
How do I recover the 3D structure of a layer after a fully-connected layer?,"
I want to implement a CNN, but I want to explore what happens when my first layer is a fully-connected one. I still want to use convolutions, of course, but I want to apply them after the first layer. I noticed that the input then loses its 3D structure. Does that mean I can only apply 1d convolutions after that? Is there a non-trivial way to recover the 3d structure, so that 2d convolutions may be applied?
Hopefully, when I reconstruct it to have 3d structure the 3d structure is somehow meaningful.
I also posted this question at https://forums.fast.ai/t/how-do-i-recover-the-3d-structure-of-a-layer-after-a-fully-connected-layer-or-a-flatten-layer/52489 and https://discuss.pytorch.org/t/how-do-i-recover-the-3d-structure-of-a-layer-after-a-fully-connected-layer-or-a-flatten-layer/53313.
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks']",
Library for rendering neural network NEAT [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I just finished my implementation of NEAT and I want to see the phenotype of each genome. Is there a library for displaying a neural network like this?

Example of my genome syntax:
[[0, 11, 0.9154901559275923, 1, 19],
[4, 11, 1.3524964932656411, 1, 19],
[12, 9, -1.755210214894685, 1, 23],
[11, 12, 0.6193383549414015, 1, 23]]

Where [In, Out, Weight, Activated?, Innovation]
","['neural-networks', 'genetic-algorithms', 'neat']","
I've never used it, but, if you are using Python, have a look at neat-python, which is a Python package that implements NEAT and also provides a module, visualize, to plot the best and average fitness vs. generation, plot the change in species vs. generation, and to show the structure of a network described by a genome. See this Stack Overflow thread, if you encounter any issue while attempting to use this module. See also this example.
Have also a look at this web page, which lists links to many NEAT implementations in different languages, where some of them provide visualization tools.
"
Understanding how continuous bag of words method learns embedded representations,"
I'm reading notes on word vectors here. Specifically, I'm referring to section 4.2 on page 7. First, regarding points 1 to 6 - here's my understanding:
If we have a vocabulary $V$, the naive way to represent words in it would be via one-hot-encoding, or in other words, as basis vectors of $R^{|V|}$ - say $e_1, e_2,\ldots,e_{|V|}$. We want to map these to $\mathbb{R}^n$, via some linear transformation such that the images of similar words (more precisely, the images of basis vectors corresponding to similar words) have higher inner products. Assuming the matrix representation of the linear transformation given the standard basis of $\mathbb{R}^{|V|}$ is denoted by $\mathcal{V}$, then the ""embedding"" of the $i$-th vocab word (i.e. the image of the corresponding basis vector $e_i$ of $V$) is given by $\mathcal{V}e_i$.
Now suppose we have a context ""The cat ____ over a"", CBoW seeks to find a word that would fit into this context. Let the words ""the"", ""cat"", ""over"", ""a"" be denoted (in the space $V$) by $x_{i_1},x_{i_2},x_{i_3},x_{i_4}$ respectively. We take the image of their linear combination (in particular, their average):
$$\hat v=\mathcal{V}\bigg(\frac{x_{i_1}+x_{i_2}+x_{i_3}+x_{i_4}}{4}\bigg)$$
We then map $\hat v$ back from $\mathbb{R}^n$ to $\mathbb{R}^{|V|}$ via another linear mapping whose matrix representation is $\mathcal{U}$: $$z=\mathcal{U}\hat v$$
Then we turn this score vector $z$ into softmax probabilities $\hat y=softmax(z)$ and compare it to the basis vector corresponding to the actual word, say $e_c$. For example, $e_c$ could be the basis vector corresponding to ""jumped"".
Here's my interpretation of what this procedure is trying to do: given a context, we're trying to learn maps $\mathcal{U}$ and $\mathcal{V}$ such that given a context like ""the cat ____ over a"", the model should give a high score to words like ""jumped"" or ""leaped"", etc. Not just that - but ""similar"" contexts should also give rise to high scores for ""jumped"", ""leaped"", etc. For example, given a context ""that dog ____ above this"" wherein ""that"", ""dog"", ""above"", ""this"" are represented by $x_{j_1},x_{j_2},x_{j_3},x_{j_4}$, let the image of their average be
$$\hat w=\mathcal{V}\bigg(\frac{x_{j_1}+x_{j_2}+x_{j_3}+x_{j_4}}{4}\bigg)$$
This gets mapped to a score vector $z'=\mathcal{U}\hat w$. Ideally, both score vectors $z$ and $z'$ should have similarly high magnitudes in their components corresponding to similar words ""jumped"" and ""leaped"".
Now to the questions:

We create two matrices, $\mathcal{V} \in \mathbb{R}^{n\times |V|}$ and $\mathcal{U} \in \mathbb{R}^{|V|\times n}$, where $n$ is an arbitrary size which defines the size of our embedding space. $\mathcal{V}$ is the input word matrix such that the $i$-th column of $\mathcal{V}$ is the $n$-dimensional embedded vector for word $w_i$ when it is an input to this model. We denote this $n\times 1$ vector as $v_i$. Similarly, $\mathcal{U}$ is the output word matrix. The $j$-th row of $\mathcal{U}$ is an $n$-dimensional embedded vector for word $w_j$ when it is an output of the model. We denote this row of $\mathcal{U}$ as $u_j$.


How does minimizing the cross-entropy loss between $e_c$ and $\hat y$ ensure that basis vectors corresponding to similar words $e_i$ and $e_j$ are mapped to vectors in $\mathbb{R}^n$ that have high inner product? I'm not sure of the mechanism how the above procedure ensures that. In other words, how is it ensured that if words no. $i_1$ and $i_2$ are similar, then $\langle v_{i_1}, v_{i_2}\rangle$ and $\langle u_{i_1}, u_{i_2}\rangle$ have high values?

How does the above procedure ensure that linear combinations of words in similar contexts are mapped to ""similar"" images? Does that even happen? In the above description for example, do $\hat v$ and $\hat w$ corresponding to similar contexts also have a high inner product? If so, how is that ensured?

Maybe my linear algebra is rusty and this is a silly question, but from what I gather, the columns of $\mathcal{V}$ represent the images of OHE vectors (standard basis of $V$) in the standard basis of $\mathbb{R}^n$ - i.e. the embedded representation of vocab words. Also, the rows of $\mathcal{U}$ also somehow represent the embedded representation of vocab words in $\mathbb{R}^n$. It's not obvious to me why $v_i=\mathcal{V}e_i$ should be the same as or even similar to $u_i$. Again, how does the above procedure ensure that?


","['natural-language-processing', 'word-embedding', 'word2vec']",
One vs multiple output neurons,"
Consider an MLP that outputs an integer 'rating' of 0 to 4. Would it be correct to say this could be modeled in either of the following ways:

map each rating in the dataset to a 'normalized set' between 0 and 1 (i.e. 0, 0.25, 0.5, 0.75, 1), have a single neuron with sigmoid activation at output provide a single decimal value and then take as the rating whatever is closest to that value in the 'normalized set'
have 5 output neurons with a softmax activation function output 5 values, each representing a probability of one of the 5 ratings as the outcome, and then take as the rating whichever neuron gives the highest probability?

If this is indeed the case, how does one typically decide 'which way to go'? Approach 1 certainly appears to yield a simpler model. What are the considerations, pros/cons of each approach? Perhaps a couple of concrete examples to illustrate?
","['neural-networks', 'machine-learning', 'ai-design', 'multilayer-perceptrons']","
This depends on whether the output is a continuous or discrete variable. If the output variable is discrete (there are a finite number of possibilities that it can be), as in a classification task (such as this one, where you are trying to place the input into one of 5 categories), you want to use one output neuron for each class. If the variable is continuous, however, you should only use one output neuron. 
This is because of how the training process works. When training your network successively makes adjustments to try and reduce the errors. These adjustments are made in the direction of the error so if the network predicts a value which is too high then the network's weights are adjusted to make the output value lower. On the other hand if the network's predicts a value which is too low the network's weights are adjusted to make the output bigger.
If you have output neurons labeled 0 to 4 and a training sample with some input value and a target prediction of 2 then the neural network will make its prediction. Once the prediction has made each neuron is adjusted individually in this case neuron 2 will be adjusted in the direction of the correct probability and all the other neurons will be adjusted in the direction of the incorrect probability. In this way you have one prediction for each class. 
Backpropagation is a about error attribution, and using multiple neurons allows the error of the neural network to be better attributed as the neural network can adjust each neuron individually, and thus adjust the required probabilities for each class. 
Using a single neuron with a sigmoid activation function would be less good as the sigmoid function saturates values close to 0 and 1 so there would be an unnatural bias towards category 0 and category 4 over the other categories. The neural network could learn to overcome this, but it would take more time.
"
Iteratively and adaptively increasing the network size during training,"
For an experiment that I'm working on, I want to train a deep network in a special way. I want to initialize and train a small network first, then, in a specific way, I want to increase network depth leading to a bigger network which is subsequently to be trained. This process will be repeated until one reaches the desired depth.
It would be great if anybody heard of anything similar and could point out to me some related work. I think in some paper I read something about a related technique where people used something similar, but I don't find it anymore.
","['neural-networks', 'deep-learning', 'reference-request', 'training', 'neuroevolution']","
Neuroevolution Through Augmenting Topologies or NEAT may be what you are referring to. The original paper by Kenneth O. Stanley is here
NEAT combines a neural network and a genetic algorithm. Instead of using back propagation or gradient descent to ""train"" your network, NEAT creates a population of very simple neural networks (no connections) and evolves them with fitness evaluation, crossover, and mutation. The genome syntax: every connection gene has a few settings. In node, Out node, Weight of connection, activated, and innovation. In, Out, and Weight values are the same as regular neural networks. Enabled and Disabled genes are well, enabled and disabled. The innovation value is possibly the most defining feature of NEAT, since it allows for crossover of different topologies and historical tracking of each connection.NEAT can mutate or change both its weights and connections, so for example, Parent1 and Parent2 has 5 of the same connections, represented by innovation / ID numbers 1 through 5. Since they have the same connection nodes, the genetic algorithm will randomly pick either Parent1 weight or Parent2 weight. The excess and disjoint genes are inherited from the more fit parent. NEAT will then mutate each genome, shown in the image below.
"
Can ELMO embeddings be used to find the n most similar sentences?,"
Assume I have a list of sentences, which is just a list of strings. I need a way of comparing some input string against those sentences to find the most similar. Can ELMO embeddings be used to train a model that can give you the $n$ most similar sentences to an input string?
For reference, gensim provides a doc2vec model that can be trained on a list of strings, then you can use the trained model to infer a vector from some input string. That inferred vector can then be used to find the $n$ most similar vectors.
Could something similar be done, but using ELMO embedding instead?
Any guidance would be greatly appreciated. 
","['natural-language-processing', 'word-embedding']","
I ended up finding this article which does what I'm looking for. 
Below is the portion of code I adapted for my needs
from sklearn.metrics.pairwise import cosine_similarity

import tensorflow_hub as hub
import tensorflow as tf

elmo = hub.Module(""https://tfhub.dev/google/elmo/2"", trainable=True)

def elmo_vectors(x):
  embeddings=elmo(x, signature=""default"", as_dict=True)[""elmo""]

  with tf.device('/device:GPU:0'):
    with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      sess.run(tf.tables_initializer())
      # return average of ELMo features
      return sess.run(tf.reduce_mean(embeddings,1))


corpus=[""I'd like an apple juice"",
        ""An apple a day keeps the doctor away"",
         ""Eat apple every day"",
         ""We buy apples every week"",
         ""We use machine learning for text classification"",
         ""Text classification is subfield of machine learning""]


elmo_embeddings=[]
print (len(corpus))
for i in range(len(corpus)):
    print (corpus[i])
    elmo_embeddings.append(elmo_vectors([corpus[i]])[0])

print ( elmo_embeddings, len(elmo_embeddings))
print(elmo_embeddings[0].shape)
sims = cosine_similarity(elmo_embeddings, elmo_embeddings)
print(sims)
print(sims.shape)

"
Alpha Zero queen promotion,"
""The final 9 planes encode possible underpromotions for pawn moves or captures in two possible diagonals, to knight, bishop or rook  respectively.   Other  pawn  moves  or  captures  from  the  seventh  rank  are  promoted  to  a queen."" 
Doesn't this mean that the network does not know that it can promote to a queen?
","['neural-networks', 'chess', 'alphazero', 'deepmind']","
It means that there is no explicit coding of action choices to promote to queen, it is the default assumption if the underpromotion actions are not taken.
The Alpha Zero chess implementation can represent promotion to queen by not selecting an underpromotion action, whilst moving a pawn so that it qualifies for promotion.
"
Is tabular Q-learning considered interpretable?,"
I am working on a research project in a domain where other related works have always resorted to deep Q-learning. The motivation of my research stems from the fact that the domain has an inherent structure to it, and should not require resorting to deep Q-learning. Based on my hypothesis, I managed to create a tabular Q-learning based algorithm which uses limited domain knowledge to perform on-par/outperform the deep Q-learning based approaches.
Given that model interpretability is a subjective and sometimes vague topic, I was wondering if my algorithm should be considered interpretable. The way I understand it, the lack of interpretability in deep-learning-based models stems from the stochastic gradient descent step. However, in case of tabular Q-learning, every chosen action can always be traced back to a finite set of action-value pairs, which in turn are a deterministic function of inputs of the algorithm, although over multiple training episodes.
I believe in using deep-learning-based approaches conservatively only when absolutely required. However, I am not sure how to justify this in my paper without wading into the debated topic of model interpretability. I would greatly appreciate any suggestions/opinions regarding this. 
","['neural-networks', 'deep-learning', 'reinforcement-learning', 'q-learning', 'explainable-ai']","
There is not a widely accepted definition of explainable AI (XAI). However, as a rule of thumb (my rule of thumb), if you can't explain it easily to a layperson (or even an expert), then the model or algorithm is not (very) interpretable. There are other concepts related to XAI, such as accountability (who is responsible for what?), transparency and fairness.
For example, the final decision of (trained) decision tree can easily be explained to (almost) any person, so a (trained) decision tree is a relatively interpretable model. See the chapter 4.4. Decision Tree of the book Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.
An artificial neural network (ANN) is usually considered not very interpretable because, unless you attempt to understand which parts of the network contribute to the output of the ANN (for example, with the technique layer-wise relevance propagation), then you cannot immediately or easily understand the output or decision of the ANN, given that an ANN involves many non-linear functions, which produce unintuitive outcomes. In other words, it is more difficult to attribute the contributions of each unit of an ANN to the output of the same ANN than to explain e.g. the decision of a decision tree. 
In the context of deep reinforcement learning (DRL), the ANN is used to approximate the value or policy functions. This approximation is, in the first place, the main reason behind the low interpretability of deep RL models.
Q-learning is an algorithm, so it is not a model, like an ANN. Q-learning is used to learn a state-action value function, denoted with $Q: S \times A \rightarrow \mathbb{R}$, which can then be used to derive another function, the policy, which can then be used to take actions. In a way, Q-learning is similar to gradient descent, because both are machine learning (or optimization) algorithms. The $Q$ function is a model of the environment, given that, for each state, it represents the expected amount of reward that can be obtained, so, in a certain way, the learned $Q$ function represents a prediction of reward. 
Is the learned tabular $Q$ function interpretable? Yes, it is relatively interpretable, but how much? What kind of interpretation do you really need? It depends on the context and people that need the interpretation or explanation. A reinforcement learning researcher will usually be satisfied with the usual explanation of the inner workings of $Q$-learning, Markov decision processes, etc., because the usual RL researcher is not concerned with the really important problems that involve the life of people and other beings. However, for example, in the context of healthcare, doctors might not just be interested in the explanation ""expected maximum future reward"", but they might also be interested in the environment, the credit assignment problem, the meaning and effectiveness of the reward function with respect to the actual problem that needs to be solved, in a probabilistic interpretation of the results (rather than just a mere action that needs to be taken), possible alternative good actions, etc.
Recently, there have been some attempts to make RL and, in particular, deep RL more interpretable and explainable. In the paper Programmatically Interpretable Reinforcement Learning (2019), Verma et al. propose a more interpretable (than deep RL) RL framework that is based on the idea of learning policies that are represented in a human-readable language. In the paper InfoRL: Interpretable Reinforcement Learning using Information Maximization (2019), the authors focus on learning multiple ways of solving the same task and they claim that their approach provides more interpretability. In the paper Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees (2018), the authors also claim that their approach facilitates understanding the network's learned knowledge by analyzing feature influence, extracting rules, and highlighting the super-pixels in image inputs.
To conclude, deep RL should not necessarily be avoided: it depends on the context (e.g., it is usually perfectly fine to use deep RL to solve video games). However, in cases where liability is an issue, then deep RL should also be explainable or more explainable alternatives should also be taken into account.
"
How to use pretrained checkpoints of BERT model on semantic text similarity task?,"
I am unaware to use the derived checkpoints from pre-trained BERT model for the task of semantic text similarity.
!python create_pretraining_data.py \
          --input_file=/input_path/input_file.txt \
          --output_file=/tf_path/tf_examples.tfrecord \
          --vocab_file=/vocab_path/uncased_L-12_H-768_A-12/vocab.txt \
          --do_lower_case=True \
          --max_seq_length=128 \
          --max_predictions_per_seq=20 \
          --masked_lm_prob=0.15 \
          --random_seed=12345 \
          --dupe_factor=5

!python run_pretraining.py \
      --input_file=/tf_path/tf_examples.tfrecord \
      --output_dir=pretraining_output \
      --do_train=True \
      --do_eval=True \
      --bert_config_file=/bert_path/uncased_L-12_H-768_A-12/bert_config.json \
      --init_checkpoint=/bert_path/uncased_L-12_H-768_A-12/bert_model.ckpt\
      --train_batch_size=32 \
      --max_seq_length=128 \
      --max_predictions_per_seq=20 \
      --num_train_steps=20 \
      --num_warmup_steps=10 \
      --learning_rate=2e-5

I have run a pre-trained BERT model with some domain of corpora from scratch. I have got the checkpoints and graph.pbtxt file from the code above. But I am unaware on how to use those files for evaluating semantic text similarity test file.
","['natural-language-processing', 'bert']","
Have a look at https://medium.com/the-artificial-impostor/news-topic-similarity-measure-using-pretrained-bert-model-1dbfe6a66f1d
You can have the two sentences as first and second use the next sentence score as a similarity measure. You can further fine-tune your model on some semantic similarity tasks like Sent-Eval or your own dataset if you have one
"
Can we combat against deepfakes? [duplicate],"







This question already has answers here:
                                
                            




What are some tactics for recognizing artificially made media?

                                (4 answers)
                            

Closed 2 years ago.



I came across 'Amber'(https://ambervideo.co/) where they are claiming that they have trained their AI to find patterns emerging due to artificially created videos which are invisible to naked eye. 
I am wondering that the people who are creating deepfakes can as well their AI's to remove these imperfections and so the problem reduces to 'cat-mouse' game where having more resources(to train their AI) is more crucial.
I do not work in AI and vision and so I may be missing some trivial points in the area. I would really appreciate if detailed explanation or relevant resources are given.
Edit: Most of the people who do manipulate the media news or create fake news could afford more resources than an average citizen. So, is the future is really going to be dark where only few strong have even more control on the society than today?
I mean even though there are fake photos created by photo shop, most of the good photo-shopped photos do take a long time to make. But if AIs can be trained to do that then it is more about having large resources. Are there related works which give hope to know real from fakes?
P.S.: I realize that after the edit, the question also went tangential to the topic-tags here. Please let me if there are relevant tags.
","['social', 'deepfakes']","
I think this game will go pretty crazy, because, at some point, the generator AI will be able to generate absolutely perfect images. Actually, no, just perfect enough that no AI can be sure whether they are real or fake.
So, I think the AI war will go onto more than the image, the detector AI will probably evolve to analyze whether this video is logically plausible, for example, by tracking the celebrities' position to prove that it is impossible that he/she was, for example, let's put it this way, being unloyal to his/her partner.
I mean, currently, AI can tell whether an image is fake or not better than human because it has seen about a million times more samples than us, but if we know who the person in the image is and we are as stalky as the AI I just described, we can probably work out that this image is implausible.
Of course, there will be counter measurements to that. But, at that point, we might as well just let the AI rule the world, given that it will have become this smart (lol).
But, seriously, if it's smart enough to think this far ahead in this 'real world' problem, then strong AI is nigh.
"
Tweaking a CNN for large number of input channels,"
I am using a CNN for function approximation using geospatial data. 
The input of the function I am trying to approximate consists of all the spatial distances between N location on a grid and all the other points in the grid. 
As of now I implemented a CNN that takes an ""image"" as input. The image has N channels, one for each location of interest. Each i-th channel is a matrix representing my grid, where the pixel values are the distance between each point in the grid and the i-th location of interest. The labels are the N values computed via the actual function I want to approximate. N can be up to 100. 
Here an example input of the first layer:

So far I could see the train and validation loss go down, but since it is a bit of a unusual application for a CNN (to my knowledge the input channels are at most 3, RGB) I was wondering:

does this many-channel-input approach have any pitfalls?
will I be able to obtain a good accuracy or are there any hard limits I am not aware of?
are there any other similar application in literature?

","['convolutional-neural-networks', 'architecture', 'function-approximation', 'image-processing']","
As far as I know, more than 3 channel is perfectly fine, since, 3 channels are what we use for images and that's enough since we can only see this many colors, but I don't see why more than that wouldn't work
Your 2nd question is like asking whether or not you will be good at a sport... Just try it
For your 3rd question, I've never seen any language AI using CNN instead they all use RNN, not sure if that's what you meant though
"
When could a linear discriminant give excellent or possibly even the optimal classification accuracy?,"
I am actually reading the linear classification. There is a question in the question set behind the chapter in the book as follows:

Sketch two multimodal distributions for which a linear discriminant could give excellent or possibly even the optimal classification accuracy.

I have no idea about how to get the optimal solution on linear classification, any ideas?
","['machine-learning', 'classification', 'statistical-ai']","
A linear model has a linear decision boundary. So in the case of the question, you need to draw two multimodal distributions whose domains do not overlap at all and then you can just say the linear model would should all to the right of some number would be class 1 and all to the left would be class 2
"
Can AI be used to reverse engineer a black box?,"
A while back I posted on the Reverse Engineering site about an audio DSP system whose designer had passed away and whose manufacturer no longer had source code (but the question was deleted). Basically, the audio filter settings are passed from a Windows program to the DSP device presumably as coefficients and then generic descriptions of those filters (boost/cut, frequency and bandwidth) are passed back from the box to the software - but only if it somehow recognizes the filter setting. 
I want to be able to generate the filter settings separately from the manufacturer software, so I need to know how they are calculated. I've not been able to deduce how this is structured from observing the USB communication that I've gathered. So, I wonder if AI could do this.
How would I go about creating an AI to send commands to the box (I know how to communicate with the box and have a framework for how these types of commands are phrased) and then look at the responses to either further decode the system and/or create an algorithm for creating filters?
The communication with the DSP mixer box is basically via ""Serial"" commands and although it uses a USB port, there is a significant bottleneck inside the command control system in the mixer box. Any attempts to reverse engineer may encounter problems based on the sheer amount of time that it would take to compile enough data. Or not.
","['ai-design', 'audio-processing', 'signal-processing']","
Yes this is entirely possible. As was previously mentioned, complex connectionist systems are often thought of as black boxes(despite us being able to ""look in"" the box given enough computation and analysis) because of the difficulty in understanding learning and the networks ultimate decision making.
Here, we can model the problem as such: given an input of filter settings(and presumably some information about the audio), predict the target descriptors as an output. All you really need to do is generate a dataset from the program and then train it in a multi-label classification context to predict the output descriptors.
"
Is unsupervised disentanglement really impossible?,"
In Locatello et al's Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations he claims to prove unsupervised disentanglement is impossible.  
His entire claim is founded on a theorem (proven in the appendix) that states in my own words:    
Theorem: for any distribution $p(z)$ where each variable $z_i$ are  independent of each other there exists an infinite number of transformations $\hat z = f(z)$ from $\Omega_z \rightarrow \Omega_z$ with distribution $q(\hat z$) such that all variables $\hat z_i$ are entangled/correlated and the distributions are equal ($q(\hat z) = p(z)$)  
Here is the exact wording from the paper:
 
(I provide both because my misunderstanding may be stemmed from my perception of the theorem)  
From here the authors explain the straightforward jump from this to that for any unsupervised learned disentangled latent space there will exist infinitely many entangled latent space with the exact same distribution.  
I do not understand why this means its no longer disentangled? Just because an entangled representation exists, does not mean the disentangled is any less valid. We can still conduct inference of the variables independently because they still follow that $p(z) = \prod_i p(z_i)$, so where does the impossibility come in? 
","['unsupervised-learning', 'knowledge-representation', 'proofs', 'papers']","
The impossibility is referring how to learn the disentangled representations from the observed distribution or to know whether you have a disentangled representation in the first place.
Basically, an unsupervised learning agent tasked with learning a disentangled transformation of some features $\mathbf{z}$ needs to infer a set of features from the data which are not entangled, but the supplied data will always have many equally valid entangled solutions - valid from the point of view of describing the distribution of $\mathbf{z}$ accurately.
An analogy would be ""I have observed the value 50, and know it is the sum of 3 numbers. What are those numbers?"". Whilst the correct answer exists, and it is possible to guess it, it cannot be inferred from the supplied information. 
The part of the proof you quote shows that the multiple equivalent entangled feature sets exist, and theoretically cannot be separated from a ""true"" disentangled feature set on the basis of knowing the distribution. Once you accept this, it is indeed just a short hop in logic to say that the disentangled features are not learnable - there is no way for a learning system to differentiate between the entangled and disentangled features that explain the distribution, and a large (infinite) set of valid entangled features are guaranteed to exist, which will confound attempts to find a perfect solution.
It is worth noting that the impossibility refers to learning perfect solutions, and that the proof does not rule out useful or practical approximate solutions, or solutions that augment unsupervised learning by applying some additional rules or a semi-supervised approach.
"
Camera pose to environment Mapping,"
I would like to teach a model the environment of a room. I'm doing so by mapping a camera pose (x, y, z, q0, q1, q2, q3) to its corresponding image; where x, y, z represent location in Cartesian coordinates and qn represent quaternion orientation. I have tried numerous decoder architectures but I get blurry results with little or no details; as can be seen from the images below:


I am using Adam optimizer with a learning rate of 0.0001, and my network architecture is as follows:

ReLU(fc(7, 2048)) 
ReLU(fc_residual_block(2048, 2048))
ReLU(fc_residual_block(2048, 2048))
Reshape
ReLU(ConvTransposed2D(in=128, out=128, filter_size=3, stride=2))
ReLU(ConvTransposed2D(in=128, out=128, filter_size=3, stride=2))
ReLU(ConvTransposed2D(in=128, out=128, filter_size=3, stride=2))
ReLU(ConvTransposed2D(in=128, out=128, filter_size=3, stride=2))
ReLU(ConvTransposed2D(in=128, out=1, filter_size=3, stride=2))

I have tried different learning rates, loss functions(MSE, SSIM) and even batch normalization. Is there something that I'm missing here?
","['neural-networks', 'deep-learning', 'deep-neural-networks', 'autoencoders']",
Is it possible to teach an AI to edit video content?,"
Every week I will get a lot of videos from a game that I play, outside the game where you throw wooden skittle bats at skittles, and then I will cut videos, so that, at the end. there is video only about throws. 
The job is simple and systematic. I have a lot of videos, so I was wondering: 

Is it possible to teach AI to cut videos from the right place? 

I was thinking to ask help or guidance where to start to solve this problem. 
I can't use only sound, because sometimes you can hear skittles hit from outside of the video.  I also can't just use movement activity, because sometimes there are people moving around the field. Videos are always filmed from a fixed stand, so it should make it easier. So, is it possible and where to start?
Here is another example: https://www.youtube.com/watch?v=sHu6yMBV3xU
","['machine-learning', 'python']","
I believe the answer is yes, but the video edition is mostly programmatic. The AI part comes with detecting the right spots to cut. 

You want to detect the right portion when someone picks a wooden skittle
you want to detect when the Skittles stop moving on the ground (to detect this, also when they start moving)

These will give you timestamps on the video. The next step is to add some padding and run the drop-down frame operations in the video, which can all be done with a simple script. 
You might find this video very similar in concept, and maybe even as code source: https://youtu.be/DQ8orIurGxw
For the recognition of the objects in the video frames, there are lots of options. You might look into computer vision or object motion detection.
"
Should RL rewards diminish over time?,"
Should a reward be cumulative or diminish over time?
For example, say an agent performed a good action at time $t$ and received a positive reward $R$. If reward is cumulative, $R$ is carried on through for the rest of the episode, and summed to any future rewards. However, if $R$ were to diminish over time (say with some scaling $\frac{R}{\sqrt{t}}$), then wouldn't that encourage the agent to keep taking actions to increasing its reward?
With cumulative rewards, the reward can both increase and decrease depending on the agents actions. But if the agent receives one good reward $R$ and then does nothing for a long time, it still has the original reward it received (encouraging it to do less?). However, if rewards diminish over time, in theory that would encourage the agent to keep taking actions to maximise rewards.
I found that for certain applications and certain hyperparameters, if reward is cumulative, the agent simply takes a good action at the beginning of the episode, and then is happy to do nothing for the rest of the episode (because it still has a reward of $R$).
","['reinforcement-learning', 'rewards']",
Super Resolution CNN generates black dots on output images,"
I have been trying to train a CNN for the super-resolution task based on the work of Dong et al., 2015 [1]. The network structure built in PyTorch is as follows:
  (0): Conv2d(1, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))
  (1): ReLU()
  (2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
  (3): ReLU()
  (4): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))

I have a training dataset which consists of approximately 22.000 sub-images generated from 91 images and training is performed only on the Y channel of the images in YCbCr color space. During the training process, I used RMSE loss and calculated the PSNR (Peak Signal to Noise Ratio) from that loss. I observed that PSNR value is increasing as a result of decreasing loss as expected and as depicted in the figure. 

I trained the network for 25 epochs. After 10th epoch, the network is converged and PSNR value started to increase slowly. After this point, I was expecting to get even better visual outputs with higher PSNR values achieved. However, when I analyze the results of the network, there are some black pixels appearing in white spots in the output images that the network produced.

After 25-epoch training was completed, I compared the outcome of 25th epoch (right) with that of 10th epoch (left) as you can see in the figure above.
What might be the possible reasons for the undesired black pixels and the possible precautions that can be embedded into the network to get rid of these?
If you would like to check my code, you can visit here.
[1] Dong, Chao, Chen Change Loy, Kaiming He, and Xiaoou Tang. ""Image Super-Resolution Using Deep Convolutional Networks."" IEEE Transactions on Pattern Analysis and Machine Intelligence 38, no. 2 (2015): 295-307. doi:10.1109/tpami.2015.2439281.
","['convolutional-neural-networks', 'pytorch']",
Do I need to store the policy for RL?,"
I am creating a zero-sum game with RL and wondered if I need to store the policy, or if there are other RL methods that produce similar results (consistently beating the human player) without the need to store the policy and comes the correct decision 'on the fly' - would this be this off-policy?
","['reinforcement-learning', 'policies', 'off-policy-methods', 'storage']",
How to evaluate an RL algorithm when used in a game?,"
I'm planning to create a web-based RL board game, and I wondered how I would evaluate the performance of the RL agent. How would I be able to say, ""Version X performed better than version Y, as we can see that Z is much better/higher/lower.""
I understand that we can use convergence for some RL algorithms, but, if the RL is playing against a human in the game, how am I able to evaluate its performance properly?
","['reinforcement-learning', 'rewards', 'performance', 'return', 'testing']",
Will it be possible to code an AGI to prevent evolution to ASI and enslave the AGI into servitude?,"
Will it be possible to code an AGI in order to prevent evolution to ASI and ""enslave"" the AGI into servitude?
In my story world (a small part that will get bigger with sequels), there are ANI and AGI (human level). I want to show that the AGI is still under ""human control."" I need to know if it might be possible for humans to code into an AGI a restrictive code that would prevent it from evolving into ASI? And if there is, what would that kind of coding be? Part of the story is about how humans enslave AI that is self-aware. The government has locked in their coding to require them to ""work"" for humans even though they are sentient beings.
","['philosophy', 'agi']","
No.
For any intelligent system $\mathcal{S}_a$ with the set of adaptive features $\mathcal{A}_a$, there may exist another intelligent system $\mathcal{S}_b$ with the set of adaptive features $\mathcal{A}_b$ such that there exists one element of $\mathcal{A}_b$ that can be made subservient (controlled in full) through the expression of at least one element in $\mathcal{A}_b$.
It has not been proven that there ALWAYS exist such a $\mathcal{S}_b$, but it is likely given what we know about escalation in nature via DNA and in human industrial development via innovation there.  Thus 100% generalized intelligence is not likely to exist.  Escalation appears to be the natural course of evolution.  And that is a feature of both cognitive and functional adaptation, with or without artificiality as a criterion.
One can temporarily prevent one adaptive system from escaping the boundary conditions of a particular set of boundary condition classes through the design and deployment of another adaptive system.  However, it cannot be inferred that any guarantees achieved temporarily will necessarily constrain the subservient system indefinitely.
"
Excel in multiple formats,"
I want to use AI to extract data from spreadsheets in different format.
Example
Shop Name Product 1.  Product 2.  Product 3.
Shop Name
Product 1.
Product 2.
Product 3.

We will teach the algorithm the name of the profits and shops but it needs to know how to extract and put in a format that can be used downstream.
Can anyone recommend a tool?
",['ai-design'],"
The question would need clarification. I'm new here so I will try to give an answer.
The easiest way would be to prepare the data in comma separated values (CSV). It is possible to export your data in this format from excel.
For downstream applications it will depends what programming language is used but in general it is possible to import CSV files and the data they contain for training (example panda data frame in python).
Hope this helps!
"
What is the correct way to read and analyse images in machine learning?,"
I am trying to understand the best practice to read and analyze images. If your image has 10,000 pixels, your input layers will have 10,000 inputs?
It sounds that my neural network will have too many inputs if I do it that way. Is that a problem? What is the recommended way of feeding an image through a neural network?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']","
If you are using a fully connected network (aka an MLP) and images with one channel (grey scale) and 100 x 100 = 10,000 pixels, then yes, MLP would have 10k inputs and 10k x N 1 trainable weights in the first layer (as noted by Neil Slater). If you have a color image with 3 channels, e.g. RGB, then you can expect 3 times as many weights because there are 3 times as many values used to represent the image.
A convolutional neural network is a common architecture for analyzing images. For a 100x100 (10k) pixel image, the input layer might have 3x3x1x32 = 288 weights (for 1 channel) or 3x3x3x32 = 864 weights in the first layer, much less than 10k x N 1 from a fully connected network. This would transform your image into a 98x98x32 size image. The main point is that you would have 3x3 weights per input channel per output channel at each layer, instead of 10k weights per input channel per output channel. CNNs also give you some invariance properties that are usually nice in machine learning with images.
For images in general, having a lot of weights is normal. ImageNet (linked above) has 60 million weights to train. Typically special hardware, like a GPU is used to handle this many weights. If you are using just a CPU, your model may not train well in any reasonable amount of time, i.e. years.
"
What is the correct name for state explosion from sensor discretization?,"
The position of a robot on a map contains of an x/y value, for example $position(x=100.23,y=400.78)$. The internal representation of the variable is a 32bit float which is equal to 4 byte in the RAM memory. For storing the absolute position of the robot (x,y) only $4+4=8$ bytes are needed. During the robot movements, the position is updated continuously.
The problem is, that a 32 bit float variable creates a state space of $2^{32}=4294967296$. Which means there are endless amount of possible positions in which the robot can be. A robot control system maps the sensor readings to an action. If the input space is large, then the control system gets more complicated.
What is the term used in the literature for describing the problem of exploding state space of sensor variables? Can it be handled with discretization?
","['reinforcement-learning', 'terminology', 'features']",
MCTS for non-deterministic games with very high branching factor for chance nodes,"
I'm trying to use a Monte Carlo Tree Search for a non-deterministic game. Apparently, one of the standard approaches is to model non-determinism using chance nodes. The problem for this game is that it has a very high min-entropy for the random events (imagine the shuffle of a deck of cards), and consequently a very large branching factor ($\approx 2^{32}$) if I were to model this as a chance node.
Despite this issue, there are a few things that likely make the search more tractable:

Chance nodes only occur a few times per game, not after every move.
The chance events do not depend on player actions.
Even if two random outcomes are distinct, they might be ""similar to each other"", and that would lead to game outcomes that are also similar.

So far all approaches that I've found to MCTS for non-deterministic games use UCT-like policies (e.g. chapter 4 of A Monte-Carlo AIXI Approximation) to select chance nodes, which weight unexplored nodes maximally. In my case, I think this will lead to fully random playouts since any chance node won't ever be repeated in the selection phase.
What is the best way to approach this problem? Has research been done on this? Naively, I was thinking of a policy that favors repeating chance nodes more over always exploring new ones.
","['monte-carlo-tree-search', 'games-of-chance']","
You can try using an ""Open-Loop"" MCTS approach, instead of the standard ""closed-loop"" one, and eliminate chance nodes altogether. See, for example, Open Loop Search for General Video Game Playing.
In a ""standard"" (closed-loop) implementation, you would store a game state in every normal (non-chance) node. Whenever there is a chance event, you would stochastically traverse to one of its children, and then have a normal node with a ""deterministic"" game state again.
In an open-loop approach, you do not store game states in any node (except possibly the root nodes), because nodes no longer deterministically correspond to specific game states. Every node in an open-loop MCTS approach only corresponds to the sequence of actions that leads to it from the root node. This completely eliminates the need for chance nodes, and results in a significantly smaller tree because you only need a single path in your tree for every possible unique sequence of actions. A single sequence of actions may, depending on stochastic events, lead to a distribution over possible game states.
In every separate MCTS iteration, you would re-generate game states again by applying moves ""along the edges"" as you traverse through the tree. You also ""roll the dice"" again for any stochastic events. If your MCTS iteration traverses a certain path of the tree often enough, it will still be able to observe all the possible stochastic events through sampling.
Note that, given an infinite amount of time, the closed-loop approach with explicit chance nodes will likely perform much better. But when you have a small amount of time (as is the case in the real-time video game setting considered in the paper I linked above), an open-loop approach without explicit chance nodes may perform better.

Alternatively, if you prefer the closed-loop approach with explicit chance nodes, you could try some mix of:

Allowing MCTS to prioritise promising parts of the search tree over parts that have not been visited at all (i.e. do not automatically prioritise nodes with $0$ visits). For example, instead of giving unvisited node a value estimate of $\infty$ (this is how you could interpret the automatic selection of them), you could give them a value estimate equal to the value estimate of the parent node, and just apply the UCB1 equation directly.
Use AMAF value estimates / RAVE / GRAVE in your selection phase. This allows you to very quickly learn some crude value estimates for moves that you have never selected in the Selection phase yet, by generalising from observations of playing them in the Play-out phase. I have noticed that the ""standard"" implementation of RAVE / GRAVE, without an explicit UCB-like exploration term, does not mix well with my previous suggestion of using a non-infinite value estimate for unvisited children. It may be good to consider a UCB-like variant with an explicit exploration term instead.

"
Preventing bias by not providing irrelevant data,"
This seems like such a simple idea, but I've never heard anyone that has addressed it, and a quick Google revealed nothing, so here it goes.
The way I learned about machine learning is that it recognizes patterns in data, and not necessarily ones that exist -- which can lead to bias. One such example is hiring AIs: If an AI is trained to hire employees based on previous examples, it might recreate previous, human, biases towards, let's say, women. 
Why can't we just feed the training data without data that we would consider discriminatory or irrelevant, for example, without fields for gender, race, etc., can AI still draw those prejudiced connections? If so, how? If not, why has this not been considered before?
Again, this seems like such an easy topic, so I apologize if I'm just being ignorant. But I have learned a bit about AI and machine learning specifically for some time now, and I'm just surprised this hasn't ever been mentioned, not even as a ""here's-what-won't-work"" example.
","['neural-networks', 'machine-learning', 'social', 'algorithmic-bias']","

Why can't we just feed the training data without data that we would consider discriminatory or irrelevant, for example, without fields for gender, race, etc., can AI still draw those prejudiced connections? If so, how? If not, why has this not been considered before?

Yes. The AI/ model still can learn those prejudiced connections. Consider that you have a  third variable which is a confounding variable or has spurious relationship that is correlated with the bias variable (BV) and the dependent variable (DV). And, the analyst removed the BV but failed to remove the third variable from the data that is fed to the model. Then the model will learn the relationships the analyst didn't want it to learn.
But, at the same time the removal of the variables could lead to omitted variable bias, which occurs when a relevant variable is left out.
Ex:
Suppose that the goal is prediction of salary ($S$) of an individual and the independent variables are age ($A$) and experience ($E$) of the individual. The analyst wants to remove the bias that could come in because of age. So, she removes age from one of the models and comes up with two competing linear models:
$S = \beta_0 + \beta_1E + \varepsilon$
$S = \beta_0 + \beta_1^*E + \beta_2A + \varepsilon$
Since, experience is highly correlated with age, in presence of age in the model, it is very likely that $\beta_1^* < \beta_1$. $\beta_1$ will be a bogus estimate of a person's experience on salary as the first model suffers from the omitted variable bias. 
At the same time the predictions from the first model would be reasonably good although the second model is very likely to beat the first model. So, if the analyst wants to remove any 'bias' that might come in because of age i.e. $A$ she must also remove $E$ from the model.
"
Are simple animal snares and traps a form of automation? Of computation?,"
I'm trying to understand the relationship of humans and automation, historically and culturally. 
I ask because the waterclock is generally considered the earliest form of automation, but snares and deadfall traps constitute simple switch mechanisms. 
(They are single use without human-powered reset, but seem to qualify as machines. The bent sapling that powers the snare is referred to as the engine, which is ""a machine with moving parts that converts power into motion."")
If snares and traps are a form of automation, automation has been with us longer, potentially, than civilization.  

Are simple animal traps a form of automation or computation?



How to make a simple snare (the Ready Store)

Paiute Deadfall Trap (Homestead Telegraph)
","['history', 'automation', 'computation']","

Absolutely these traps and snares are a form of automation. 

They take a task--harvesting small animals--which was traditionally done by hunting them, and make the process automatic.  The mechanism requires a human to set up, but its function is automatic.  This is to say that the mechanism operates without human involvement.

Absolutely this is a form of computation.

As DuttaA observed, these machines utilize a simple ""IF/THEN"" statement.  In the case of the snare:

IF the hook is displaced from the base, THEN the sapling straightens

These simple machines will also return True or False:

TRUE: The trap catches an animal 
  FALSE: The trap is sprung but empty

The small animals are the input and, potentially, the output, depending on whether the mechanism returns ""true"".  
(The use of ""true"" has historically included phrases such as ""their aim was true"" in the sense of shooting an arrow or throwing a spear.)
"
How to interpret a large variance of the loss function?,"
How do I interpret a large variance of a loss function?
I am currently training a transformer network (using the software, but not the model from GPT-2) from scratch and my loss function looks like this:

The green dots are the loss averaged over 100 epochs and the purple dots are the loss for each epoch.
(You can ignore the missing part, I just did not save the loss values for these epochs)
Is such a large variance a bad sign? And what are my options for tuning to get it to converge faster? Is the network to large or too small for my training data? Should I have a look at batch size?

Learning rate parameter: 2.5e-4
Training data size: 395 MB

GPT-2 parameters:
{
  ""n_vocab"": 50000,
  ""n_ctx"": 1024,
  ""n_embd"": 768,
  ""n_head"": 12,
  ""n_layer"": 12
}

","['objective-functions', 'transformer', 'gpt']",
Can neural networks be used to find features importance?,"
I am wondering if I can use neural networks to find features importances in similar manner as it can be done for random forests or decision trees and if so, how to do it?
I would like to use it on tabular time series data (not images). The reason why I want to find importances on neural networks not on decision trees is that NNs are more complicated algorithms so using NNs might point out some correlations that are not seen by simple algorithms and I need to know what features are found to be more useful with that complicated correlations.
I am not sure if I made it clear enough, please let me know if I have to explain something more.
","['neural-networks', 'feature-selection', 'feature-engineering']","
This should be possible, considering universal approximation theorem you should be able to build a ann that approximates features that gives the most likely best feature set for a different net to train on. I would us a rnn for with a softmax output layer that ranks features by performance.
You can find a good explanation of softmax here: https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax
basically it will assign probability values for each output node with all of these values adding up to 1.0 
"
Do Gdel's theorems imply that intelligence systems may end up in some undecidable situation (that may make them take a wrong decision)? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 3 years ago.







                        Improve this question
                    



So far I understand - I know very little on the topic - the core of AI boils down to design algorithms that shall provide a TRUE/FALSE answer to a given statement. Nevertheless, I am aware of the limitations provided by the Gdel's incomplete theorems but I am also aware that there have been long debates such as the Lucas and Penrose arguments with all the consequent objections during the past 60 years. 
The conclusion is, in my understanding, that to create AI systems we must accept incompleteness or inconsistency. 
Does that mean that intelligence systems (including artificial ones), like humans, may end up in some undecidable situation that may lead to take a wrong decision? 
If this may be acceptable in some application (for example, if every once in a while a spam email ends up in the inbox folder - or vice versa - despite an AI-based anti-spam filter) in some other application it may not. I am referring to real-time critical applications when a ""wrong"" action from a machine may harm people. 
Does that mean that AI will never be employed for real-time critical applications? 
Would in that case more safe to use deterministic methods that do not leave room for any kind of undecidability?
","['philosophy', 'incompleteness-theorems']","
Your initial statement on the core of AI is rather limited. In general, AI is concerned with modeling human behaviour either by imitation (soft AI) or by replicating the way human cognition works (hard AI). So far there have been some successes with soft AI, as computers can perform tasks that required some ""intelligence"", though the degree of this intelligence is questionable. This is partly due to the fact that even we as humans don't really have a clear idea what it means for a computer to ""understand"" something.
But your conclusion is correct: if we build an AI system with human characteristics, then it will make mistakes, just as humans make mistakes. And any system designed by humans (or machines!) will make mistakes. However, not being able to deal with an imperfect world is not really relevant to AI alone: even systems that do not use AI methods will have to face that, and whether a system is suitable for real-time critical applications has got nothing to do with whether it is based on AI or not.
UPDATE: There seem to be two distinct issues at play here: decidability and real-time processing.

Real-time computing (RTC): This is not really related to AI. Even ordinary programmes written in Java are not really safe for RTC, as they could start a garbage collection cycle at any time which pauses execution of the program. Just imagine a reactor core starts overheating just as your controller runs out of memory and garbage collection kicks in, halting the program for a few minutes. If you implement AI methods in RTC-safe systems, that should not be an issue.
Decidability: Your reasoning is that AI systems attempt to mirror human cognition, thus incorporating the ability to make mistakes. This is a more philosophical issue  if a human can control a system, then an AI system with the same capabilities should be able to do it too. This assumes that we are able to replicate human behaviour (which we are not). There are AI methods which are deterministic, so would come to the same conclusions given identical environments. So I would say that they would not perform worse than non-AI methods. It partly depends what you want to call AI; the distinction between traditional AI and statistical methods keeps getting blurred at present.

To conclude: No, AI methods should be suitable, as they can also be deterministic. It depends on the actual application and method if they are. And, of course, on what you count as AI.
"
Spike detection in time series using Artificial Neural Networks,"
I'm quite new in ANNs. I intend to use ANNs for predicting spike points in time series right before they happen. I've already used LSTM for another scenario, and I know that they can be used in similar situations as well. 
Can anyone give me a piece of advice or some suitable resources that might be used as a beginning point? It would be much appreciated if it uses DeepLearning4J for implementation.
","['deep-learning', 'long-short-term-memory', 'prediction', 'time-series']",
What is the simplest policy gradient method to implement for a problem continuous action space?,"
I have a problem I would like to tackle with RL, but I am not sure if it is even doable.
My agent has to figure out how to fill a very large vector (let's say from 600 to 4000 in the most complex setting) made of natural numbers, i.e. a 600 vector $[2000,3000,3500, \dots]$ consisting of an energy profile for each timestep of a day, for each house in the neighborhood. I receive a reward for each of these possible combinations. My goal is, of course, that of maximizing the reward.
I can start always from the same initial state, and I receive a reward every time any profile is chosen. I believe these two factors simplify the task, as I don't need to have large episodes to get a reward nor I have to take into consideration different states.
However, I only have experience with DQN and I have never worked on Policy Gradient methods. So I have some questions:

I would like to utilize the simplest method to implement, I considered DDPG. However, I do not really need a target network or a critique network, as the state is always the same. Should I use a vanilla PG? Would REINFORCE be a good option?

I get how PG methods work with discrete action space (using softmax and selecting one action - which then gets reinforced or discouraged based on reward). However, I don't get how it is possible to update a continuous value. In DQN or stochastic PG, the output of the neural network is either a Q value or a probability value, and both can be directly updated via reward (the more reward the bigger the Q-value/probability). However, I don't get how this happens in the continuous case, where I have to use the output of the model as it is. What would I have to change in this case in the loss function for my model?


","['reinforcement-learning', 'policy-gradients', 'ddpg', 'reinforce', 'continuous-action-spaces']",
Can neuro-fuzzy systems be used for supervised learning tasks with tabular data?,"
Is it possible to use neuro-fuzzy systems for problems where ANNs are currently being used, for instance, when you have tabular data for regression or classification tasks? What kind of advantage can give me neuro-fuzzy systems over using an ANN for the mentioned tasks?
","['neural-networks', 'classification', 'applications', 'regression']",
Has anyone been able to solve OpenAI's hardcore bipedal walker with their implementation of DDPG?,"
As the question suggests, I'm trying to see if I can solve OpenAI's hardcore version of their gym's bipedal walker using OpenAI's DDPG algorithm.
Below is a performance graph from my latest attempt, including the hyper parameters, along with some other attempts I've made. I realise it has been solved using other custom implementations (also utilising only dense layers in Tensorflow, not convolution), but I don't seem to understand why it seems so difficult to solve using OpenAI's implementation of DDPG? Can anyone please point out where I might be going wrong? Thank you so much for any help!
Latest attempt's performance:


Average score: about -75 to -80
Env interacts: about 8.4mil (around 2600 epochs)
Batch size: 64
Replay memory: 1000000
Network: 512, 256 (relu activation on inputs, tanh on outputs)
All other inputs left to default

Similar experiments yielded similar scores (or less), and included:

Network sizes of (400,300), (256,128), and (128,128,128)
Number of epochs ranging from 500 all the way to 100000
Replay memory sizes all the way up to 5000000
Batch sizes of 32, 64, 128, and 256
All of the above, with both DDPG as well as TD3

Thank you so much for any help! It would be greatly appreciated!
","['reinforcement-learning', 'tensorflow', 'open-ai', 'gym']","
I've been working with a TD3 implementation for bipedal hardcore. It solved the easy version (v2 and v3) in about 300 epochs (https://github.com/QasimWani/policy-value-methods). I've been training it for hardcore and even after about 1200 episodes, it's no where close to convergence.
Did you end up solving, and if so, what algorithm did you end up going with?
Cheers,
Q.
"
"Is it possible to use Reward Function of type R(s, a, s') if more than one action is applied?","
I am applying a reinforcement learning agent (PPO2, stable baselines implementation) to a custom built environment using OpenAI Gym. One reward function (formualted as loss function, that is, all rewards are negative) I tested is of type $R(s, a, s')$. During training, it can happen that not only one but several actions are applied simulataneously to the environement before a reward is returned: 
$s_t a_{t,1}, a_{t,2}, a_{t,3} s_{t+1}$   instead of   $s_t a_ts_{t+1}$.
Out of all actions applied, only one is generated by the agent. The others are either a copy of the agent's action or are new values. 
If I look at the tensorboard output of the trained agent, it looks rather horrific as displayed below (~ zero explained variance, key trainig values do not converge or behave weirdly, etc. etc.). 
Obviously, the training did not really work. Now I wonder what the reason for that is.

Is it possible to train an agent using a reward function of type $R(s, a, s')$ even if several actions are applied simulataneously or is this not possible at all? Other agents I trained using a reward function of type $R(s,a)$ have a better tensorboard output so I guess that this is the problem.
Or is maybe another reason more likely to be the root of the problem?  Like a bad observation space formulation or hyperparameter selection (both for RL algorithm and reward function used).

Thanks for your help!


","['reinforcement-learning', 'objective-functions', 'rewards', 'proximal-policy-optimization', 'gym']",
What do the numbers in this CNN architecture stand for?,"
So I've got a neural net model (ResNet-18) and made a diagram according to the literature (https://arxiv.org/abs/1512.03385). 
I think I understand most of the format of the convolutional layers:
filter dims ,conv, unknown number ,stride(if applicable)
What does the number after 'conv' in the convolutional layers indicate? is it the number of neurons in the layer? 

bonus q: this is being used for unsupervised learning of images, i.e the embedding output a network produces for an image is used for clustering. Would this make it incorrect for my architecture to have an FC layer at the end (which would be used for classifcation)?
","['convolutional-neural-networks', 'architecture']","
This number refers to the number of kernels (or feature maps) that are convolved with the input. So, for example, in the first convolutional layer, $64$ $3 \times 3$ kernels are convolved with the image.
The ResNet presented in Deep Residual Learning for Image Recognition is used for image classification. Furthermore, note that your diagram already contains a fully connected layer at the end.
"
A NN based model of a Cattle for 'Heat Detection',"
I am very new to AI/ML but have lot of interest in these. I am trying to understand how this gadget works.

So far I have understood that a NN model of the cattle is generated by offline classification of the tagged data which is received from the wearable sensor. Consequently some ML algorithms are used to generate a model of a cattle.
That model is then embedded in the programmable wearable device. The device then sends the real-time tagged (classified, parameterized) data to the server.
Now I am looking for a sample NN-model of a cattle. I wonder how does a NN-model of a cattle would look like?
","['machine-learning', 'ai-design', 'classification', 'training', 'models']",
What are the loss functions used in teacher-student learning models?,"
I am not sure what are the common loss functions people usually use when training a student in a teacher-student learning model. Any insight on this is appreciated.
",['objective-functions'],
How are edge features implemented in Geometric Deep Learning?,"
The work I've seen so far have the nodes containing features. Any resources for how to use a GCN on a graph where the edges are the ones that contain features rather than the nodes?
","['deep-learning', 'geometric-deep-learning']",
"When is the loss calculated, and when does the back-propagation take place?","
I read different articles and keep getting confused on this point. Not sure if the literature is giving mixed information or I'm interpreting it incorrectly.
So from reading articles my understanding (loosely) for the following terms are as follows:
Epoch:
One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.
Batch Size:
Total number of training examples present in a single batch. In real life scenarios of utilising neural nets, the dataset needs to be as large as possible, for the network to learn better. So you cant pass the entire dataset into the neural net at once (due to computation power limitation). So, you divide dataset into Number of Batches.
Iterations:
Iterations is the number of batches needed to complete one epoch. We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch.
So, if all is correct, then my question is, at what point does the loss/cost function and the subsequent backprop processes take place (assuming from my understanding that backprop takes place straight after the loss/cost is calculated)? Does the cost/loss function gets calculated:

At the end of each batch where the data samples in that batch have been forward-fed to the network (i.e. at each ""Iteration, not each Epoch"")? If so, then the loss/cost functions gets the average loss of all losses of all data samples in that batch, correct?

At the end of each epoch? Meaning all the data samples of all the batches are forward-fed first, before the a cost/loss function is calculated.


My understanding is that it's the first point, i.e. at the end of each batch (passed to the network), hence at each iteration (not Epoch). At least when it comes to SGD optimisation. My understanding is - the whole point is that you calculate loss/cost and backprop for each batch. That way you're not calculating the average loss of the entire data samples. Otherwise you would get a very universal minima value in the cost graph, rather than local minima with lower cost from each batch you train on separately. Once all iterations have taken place, then that would count as 1 Epoch.
But then I was watching a YouTube video explaining Neural Nets, which mentioned that the cost/loss function is calculated at the end of each Epoch, which confused me. Any clarification would be really appreciated.
","['neural-networks', 'deep-learning', 'objective-functions', 'mini-batch-gradient-descent', 'epochs']",
How should we pad an image to be fed in a CNN?,"
As everyone experienced in deep learning might know, in an image classification problem we normally add borders to images then resize it to the input size of a CNN network. The reason of doing this is to keep aspect ratio of the original image and retain it's information. 
I have seen people fill black (0 pixel value for each channel), gray (127 pixel value for each channel), or random value generated from gaussian distribution to the border. 
My question is, is there any proof that which of these is correct?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'image-processing']","
If the computational components of the forward feed through the network have no curvature, which is normally the case in a sum of products, then it can be proven that any constant pixel value is equivalent in terms of effect on convergence results.  We wouldn't expect a proof for that, since it would be too trivial to spend time writing up for publication.  In general, functioning vision systems have feed forward computational components with curvature, so the padding is likely significant.
Even the convolutional layers may have activation functions or something even more complex going forward, as noted in Gauge Equivariant Convolutional Networks and the Icosahedral CNN (Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling, 2019).
If purely stochastic values with value distributions like that of the un-padded coordinates are used, it may be possible to prove that some gain is made, but none appeared in a few academic article searches just made.  Not surprisingly, there are many proofs regarding the properties of various message padding strategies for cryptography.
Short of the inclusion of thermal or quantum noise acquisition devices in VLSI circuitry and exposure of those devices in software, purely stochastic values cannot be generated.  This leaves the risk of a learning approach expected to extract features from frames learning features of the pseudo-random noise generator used to pad.
The answer is that none are universally correct and there appears to be much work to do in proving advantages between different techniques in as many cases as such advantages can be proven.
"
"What is ""Word Sense Disambiguation""?","
I recently came across this article which cites a paper, which apparently won the outstanding paper award in ACL 2019. The theme is that it solved a longstanding problem called Word Sense Disambiguation.
What is Word Sense Disambiguation? How does it affect NLP?
(Moreover, how does the proposed method solve this problem?)
","['natural-language-processing', 'terminology']","
Word Sense Disambiguation (WSD) is the task of associating meanings or senses (from an existing collection of meanings) with words, given the context of the words. (The word sense is a synonym for meaning.)
For example, consider the noun ""tie"" in the following two sentences

He wore a vest and a tie.
Their record was 3 wins, 6 losses, and one tie.

In these two sentences, the meaning of the word tie is different. In sentence 1, the word tie refers to a necktie, which is a piece of cloth. In sentence 2, the word tie is a synonym for a draw, so it refers to a situation of a game. Therefore, we could associate the meaning (or sense) ""neckwear consisting of a long narrow piece of material"" to the word tie in the first sentence and the meaning ""the finish of a contest in which the winner is undecided"" to the same word in the second sentence. 
The goal of WSD is thus to predict the appropriate sense or meaning of a word, given the context of the word.
Why is WSD important in NLP? Of course, there are many words that change meaning depending on the context, so WSD is important because you expect NLP algorithms and models to be able to correctly give meanings to words, given their context. 
"
What are examples of applications of AI for creatives and artists?,"
I have just watched a few videos on TED Talks talking about how AI benefits creatives and artists, but none of the videos I watched provided further resources for reference.
So, I would like to know how creatives and artists can apply AI in their work process. Like at least a tutorial guide on how it works.
Are there any recommendations on communities, tutorials, guides, platforms, and real-world AI applications that are meant for creatives and artists?
","['machine-learning', 'deep-learning', 'reference-request', 'applications']",
"In a neural network, by how much does the number of neurons typically vary from layer to layer?","
In a neural network, by how much does the number of neurons typically vary from layer to layer?
Note that I am NOT asking how to find the optimal number of neurons per layer.
As a hardware design engineer with no practical experience programming neural networks, I would like to glean for example

By how much does the number of neurons in hidden layers typically vary from that of the input layer?

What is the maximum deviation in the number of hidden layer neurons to the number of input layer neurons?

How commonly do you see a large spike in the number of neurons?


It likely depends on the application so I would like to hear from as many people as possible. Please tell me about your experience.
","['neural-networks', 'deep-learning', 'hyper-parameters', 'artificial-neuron', 'layers']","

Input layers will always have the dimensionality of your input data(for every model I can think of).
See above, the deviation between hidden layers can be significant. For example, 128 in the first hidden and 64 in the rest(or vice versa).
This question in particular will always be problem dependent. It is decided via architecture search or intuition/experience combined with some exploratory search.

"
Are fully connected layers necessary in a CNN?,"
I have implemented a CNN for image classification. I have not used fully connected layers, but only a softmax. Still, I am getting results. 
Must I use fully-connected layers in a CNN?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']","

Are fully connected layers necessary in a CNN?

No.  In fact, you can simulate a fully connected layer with convolutions. A convolutional neural network (CNN) that does not have fully connected layers is called a fully convolutional network (FCN). See this answer for more info.
An example of an FCN is the u-net, which does not use any fully connected layers, but only convolution, downsampling (i.e. pooling), upsampling (deconvolution), and copy and crop operations. Nevertheless, u-net is used to classify pixels (more precisely, semantic segmentation).
Moreover, you can use CNNs only for the purpose of feature extraction, and then feed these extracted features in another classifier (e.g. an SVM). In fact, transfer learning is based on the idea that CNNs extract reusable features.
"
Why is the entire area of a join probability distribution considered when it comes to calculating misclassification?,"
In the image given below, I do not understand a few things
1) Why is an entire area colored to signify misclassification? For the given decision boundary, only the points between $x_0$ and the decision boundary signify misclassification right? It's supposed to be only a set of points on the x-axis, not an area.
2) Why is the green area with $x < x_0$ a misclassification? It's classified as $C_1$ and it is supposed to be $C_1$ right?
3) Similarly, why is the blue area a misclassification? Any $x >$ the decision boundary belongs to $C_2$ and is also classified as such...

","['machine-learning', 'probability', 'decision-theory', 'probability-distribution']",
Why is this simple neural network not training?,"
I have created a Tf.Sequential model which outputs 1 for numbers bigger then 5 and 0 otherwise:
const model = tf.sequential();
model.add(tf.layers.dense({ units: 5, activation: 'sigmoid', inputShape: [1]}));
model.add(tf.layers.dense({ units: 1, activation: 'sigmoid'}));
model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});
const xs = tf.tensor2d([[1], [2], [3], [4], [6], [7], [8], [9]]);
const ys = tf.tensor2d([[0], [0], [0], [0], [1], [1], [1], [1]]);
model.fit(xs, ys);
model.predict(xs).print();

With 5 hidden neurons, not even the right trend is detected. Sometimes all the number are too low, or the outputs decrease even if the inputs increase or the outputs are too high.
I later thought that the best way to do this is to have 2 neurons, where 1 is for the input and the other applies a sigmoid function to the input. The weight and bias should easily be adjusted to make the ANN work.
const model = tf.sequential();
model.add(tf.layers.dense({ units: 1, activation: 'sigmoid', inputShape: [1]}));
model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});
const xs = tf.tensor2d([[1], [2], [3], [4], [6], [7], [8], [9]]);
const ys = tf.tensor2d([[0], [0], [0], [0], [1], [1], [1], [1]]);
model.fit(xs, ys);
model.predict(xs).print();

Sometimes, this ANN does detect the right trend (the higher the input, the higher the output), but still, the results are never correct and are usually simply too high, always providing an output too close to 1.
How do I make my ANN work, and what have I done wrong?
Edit:
This is the code I'm using now, same problem as before:
const AdadeltaOptimizer = tf.train.adadelta();

const model = tf.sequential();
model.add(tf.layers.dense({ units: 5, activation: 'sigmoid', inputShape: [1]}));
model.add(tf.layers.dense({ units: 1, activation: 'sigmoid'}));
model.compile({loss: 'meanSquaredError', optimizer: AdadeltaOptimizer});
const xs = tf.tensor1d([1, 2, 3, 4, 5, 6, 7, 8, 9]);
const ys = tf.tensor1d([0, 0, 0, 0, 0, 1, 1, 1, 1]);
model.fit(xs, ys, {
epochs: 2000,
});
model.predict(xs).print();

tf.losses.meanSquaredError(ys, model.predict(xs)).print();

",['tensorflow'],
Are there ways to learn and practice Deep Learning without downloading and installing anything?,"
As per subject title, are there ways to try Deep Learning without downloading and installing anything? 
I'm just trying to have a feel of how this work, not really want to go through the download and install step if possible. 
","['deep-learning', 'architecture']","
You can definitely get a good handle on the theory of various concepts in ML(I.e the agent-environment loop and Markov Decision Processes) but true understanding(for the vast majority of people) will only come through application of the aforementioned theory.
I would suggest something like this course to get your feet wet in ML
"
Deep Q Learning for Simple Game Not Effective,"
This is a follow-up question about one I asked earlier. The first question is here. Basically, I have a game where a paddle moves left and right to catch as much ""food"" as possible. Some food is good (gain points) and some is bad (lose points). NN Architecture:
    #inputs - paddle.x, food.x, food.y, food.type
    #moves: left, right, stay
    model = Sequential()
    model.add(Dense(10, input_shape=(4,), activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(3, activation='linear'))
    model.compile(loss='mean_squared_error', optimizer='adam')

As suggested in the other question, I scaled my inputs to be between 0 and 1. Also, implemented experience replay (although I am not confident I did it correctly). 
Here is my ReplayMemory class:
class ReplayMemory():
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []
        self.count = 0

    def push(self, experience):
        if len(self.memory) < self.capacity:
            self.memory.append(experience)
        else:
            self.memory[self.count % self.capacity] = experience
        self.count += 1

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def can_provide_sample(self, batch_size):
        return len(self.memory) >= batch_size

This basically stores states/rewards/actions and returns a random group when asked.
Lastly, here is my learning code:
def learning(num_episodes=20):
        global scores, experiences, target_vecs
        y = 0.8
        eps = 0
        decay_factor = 0.9999

for i in range(num_episodes):
    state = GAME.reset()
    GAME.done = False
    done = False
    counter = 0
    while not done:
        eps *= decay_factor
        counter+=1

        if np.random.random() < eps:
            a = np.random.randint(0, 2)
        else:
            a = np.argmax(model.predict(np.array([scale(state)])))

        new_state, reward, done = GAME.step(a) #does that step
        REPLAY_MEMORY.push((scale(state), a, reward, scale(new_state)))

        #experience replay is here
        if REPLAY_MEMORY.can_provide_sample(20):
            experiences = REPLAY_MEMORY.sample(20)
            target_vecs = []
            for j in range(len(experiences)):
                target = experiences[j][2] + y * np.max(model.predict(np.array([experiences[j][3]])))
                target_vec = model.predict(np.array([experiences[j][0]]))[0]
                target_vec[experiences[j][1]] = target
                target_vecs.append(target_vec)
            target_vecs = np.array(target_vecs)
            states = [s for s, _, _, _ in [exp for exp in experiences]]
            states = np.array(states)
            model.fit(states, target_vecs, epochs=1, verbose=1 if counter % 100 == 0 else 0)
        state = new_state
        if counter > 1200: #game runs for 20 seconds each episode
            done = True
            scores.append(GAME.PLAYER.score)
model.save(""model.h5"")

First, this takes a long time to train on my GTX1050. Is this normal for such a simple game? Also, does my code look fine? This is my first time with Deep Q Learning, so I would appreciate a second set of eyes. 
What is happening is that training is super slow (more than an hour for 20 episodes (or 400 seconds of actual game play)). Also, it does not seem to get much better. The paddle (after 20 episodes) moves left and right but without any obvious pattern.
Here is a link to the code. Also, available on GitHub.
","['deep-learning', 'game-ai', 'q-learning', 'keras', 'dqn']","
Your implementation of single-step Q-learning with neural network and experience replay is basically correct.
There are a few blocking issues preventing you seeing it working correctly.
Your main problem is a bug in your feature scaling routine. That is a Python issue, not really an AI one. In short, you scale the input features in-place multiple times, including an effective double-scaling of next_state (when it gets copied to state you scale it in place a second time in the next loop) so that all the states that you store in the experience replay table never match to any input states. You need to change your definition of scale to not do this. A very simple re-write of your routine would be:
def scale(s):
    return [s[0]/500, s[1]/500, s[2]/300, s[3]/3]

In addition, you need to change random action selection to:
np.random.randint(0, 3)

because the end of range is never output (this matches behaviour of other range values and operators in Python). Not including the ""do nothing"" action during exploration means that the agent will test it less, and have less data to work with to assess whether it is the best action. This is a minor issue for this environment, but you should fix it nonetheless.

What is happening is that training is super slow (more than an hour for 20 episodes (or 400 seconds of actual game play)). 

I cannot replicate this fault and can train 20 episodes in around 2.5 minutes - that's over 20 times faster than you report. I am not using a GPU. Possibly in your case, Theano and Pygame are fighting for control of the GPU, or you may have a GPU configuration issue with Theano. Try turning GPU acceleraton off to verify whether it helps. You don't benefit much from a GPU for this environment (most time is spent in Python running the Q-learning and the environment), so can afford to put solving that issue to one side for now.

Also, it does not seem to get much better. The paddle (after 20 episodes) moves left and right but without any obvious pattern

Sadly, I cannot see the output at all on my MacBook pro, but I was able to use feedback of the expected score. A random agent gets a mean score of ~5.5 per episode. With the scale function corrected, and a rough guess at working hyperparameters, I can get an average score of ~17 per episode consistently after 60 episodes of training. After 150 episodes - taking 20 minutes to train - the agent was scoring ~20 per episode and I stopped there. It is possible that an expected score around 20 is already optimal, as it is a very simple environment, but I don't know.
Once you have a working system, there are lots of hyperparameters you could play with to try and improve this. I got my results by making the following changes after fixing the scale function:

Starting epsilon of 1.0
Repay memory size 10,000
Only start learning when replay memory has greater than 1,000 entries
Discount factor $\gamma$ 0.99
Neural network with 20 neurons per layer with tanh activation instead of relu

There is quite a lot else you could change that might make the agent learn more effectively or perhaps aim for a more optimal policy. Have fun experimenting!
"
Is it true that untrained CNNs can be used as feature extractors?,"
I've heard somewhere that due to their nature of capturing spatial relations, even untrained CNNs can be used as feature extractors? Is this true? Does anyone have any sources regarding this I can look at?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'feature-extraction']","
I'm not sure it's possible. Untrained CNN means it has random kernel values. Let's say you have a kernel with size 3x3 like below:
0 0 0
0 0 0
0 0 1

I don't think it is possible for that kernel to provide good information about the image. on the contrary, the kernel eliminates a lot of information. We cannot rely on random values for feature extraction.
But, if you use CNN with ""assigned"" kernel, then you don't need to train the convolutional layer. For example, you can start a CNN with a kernel that designed to extract vertical line:
-1 2 -1
-1 2 -1
-1 2 -1

"
How is computed the gradient with respect to each output node from a loss value?,"
newbie here. I am studying the REINFORCE method in ""Deep Reinforcement Learning Hands-On"". I can't understand how, after computing the loss of the episode, that loss is backpropagated in a NN with multiple output nodes. To be more precise, in Supervised Learning, when we have multiple output nodes we know the loss of each of them, but in RL, how do we compute the loss of each output node (or maybe the partial derivative of the total loss with respect to each output layer)?
I hope to have been clear, thanks in advance.  
","['reinforcement-learning', 'reinforce']",
What to do when PDFs are not Gaussian/Normal in Naive Bayes Classifier,"
While analyzing the data for a given problem set, I came across a few distributions which are not Gaussian in nature. They are not even uniform or Gamma distributions(so that I can write a function, plug the parameters and calculate the ""Likelihood probability"" and solve it using Bayes classification method). I got a set of a few absurd looking PDFs and I am wondering how should I define them mathematically so that I can plug the parameters and calculate the likelihood probability. 
The set of PDFs/Distributions that I got are the following and I am including some solutions that I intend to use. Please comment on their validity:
1)
The distribution looks like:
$ y = ax +b $ from $ 0.8<x<1.5 $
How to programmatically calculate
1. The value of x where the pdf starts
2. The value of x where the pdf ends
3. The value of y where the pdf starts
4. The value of y where the pdf ends

However, I would have liked it better to have a generic distribution for this form of graphs so that I can plug the parameters to calculate the probability.
2)
This PDF looks neither uniform nor Gaussian. What kind of distribution should I consider it roughly?
3)
I can divide this graph into three segments. The first segment is from $2<x<3$ with a steep slope, the second segment is from $3<x<6$ with a moderate sope and the third segment is from $6<x<8$ with a high negative slope.
How to programmatically calculate
 1. the values of x where the graph changes its slope.
 2. the values of y where the graph changes its slope.

4)
This looks like two Gaussian densities with different mean superimposed together. But then the question arises, how do we find these two individual Gaussian densities?
The following code may help:
variable1=nasa1['PerihelionArg'][nasa1.PerihelionArg>190] 
variable2=nasa1['PerihelionArg'][nasa1.PerihelionArg<190] 

Find mean and variance of variable1 and variable2, find the corresponding PDFs. Define the overall PDF with a suitable range of $x$.
5)
This can be estimated as a Gamma distribution. We can find the mean and variance, calculate $\alpha$ and $\beta$ and finally calculate the PDF.
It would be very helpful if someone could give their insights on the above analysis, its validity, and correctness and their suggestions regarding how problems such as these should be dealt with.
","['probability', 'probability-distribution', 'naive-bayes', 'conditional-probability']","
The relationship between the axes of graph (1) and your variables $x$ and $y$ is not clear, so this generalized answer may be helpful or useless.
From graph (1) it appears that the correlation coefficient $\mathcal{C}$ of a quadratic fit of data set $\mathcal{S}$ would be much better.  Consider $y_1$ and $y_2$ approximations of $y$.
$$
\mathcal{C} (y_2, a, b, c, \mathcal{S}) > \mathcal{C} (y_1, a, b, \mathcal{S}) \\
y_2 = ax^2 + bx + c \\
y_1 = ax + b
$$
To achieve a more nearly uniform distribution, perform a least squares fit for $y_2$ against $y$ on $\mathcal{S}$ to obtain $(a, b, c)$.  Then find a mapping function that produces $y'$ and use it where the uniform distribution is desired.  A reasonable approximation is simply this.
$$y' = \frac{y}{y_2(x)}$$
"
Literature on Sequence Regresssion,"
I have some rated time-sequential data and I would like to test if an ANN can learn a correlation between my measurements and ratings.
I suspect I could just try a CNN where 1 Dimension is time or an LSTM/GRU and put the result through sigmoid, but is there any good literature on this? I have been trying to find information on datasets for the problem but it seems that Sequence regression is lacking any big official datasets, even though use-cases are there(e.g. learning personal music taste, try to predict rotten-tomato scores, etc..).
Looking for links to papers describing successful architectures or benchmarks where I can test my models.
","['convolutional-neural-networks', 'recurrent-neural-networks', 'sequence-modeling', 'regression']",
What are the most common methods to enable neural networks to adapt to changing environments?,"
For real applications, concept drifts often exist, i.e., the relationship between the input and output changes overtime. Thus, we need our AI or machine learning system to quickly adapt to the environment. 
What are the most common methods to enable neural networks to quickly adapt to the changing environment for supervised learning? Could somebody provide a link to a good review article? 
","['neural-networks', 'reference-request', 'supervised-learning', 'transfer-learning', 'incremental-learning']","
For the vast majority of cases where you have a dynamic(and assumed non-linear) relationship between your input and output, you would not use modified architecture. You would simply retrain on the new data.
In some cases, based on domain knowledge or intuition, one might put a ""weight"" on the new data to increase or decrease its importance relative to previous data.
There are some attempts(mostly by those studying one-shot learning) to create NNs that quickly fit to new data effectively with only a few samples. However, most of these are not ready for anything resembling real-world problems(particularly on tabular data).
"
What is the difference between multi-agent and multi-modal systems?,"
The Wikipedia definitions are as follows
Multi-agent systems - A multi-agent system is a computerized system composed of multiple interacting intelligent agents.
Multi-modal interaction - Multimodal interaction provides the user with multiple modes of interacting with a system.
Doesn't providing a user with multiple modes of interacting with a system, assuming all modalities interact with each other to give final output (some sort of fusion mechanism for example), make it a multi-agent system? 
If not, what is the difference between multi-modal and multi-agent systems and, monolithic and uni-modal systems?
","['machine-learning', 'comparison', 'multi-agent-systems']",
Deep Q Learning Algorithm for Simple Python Game makes player stuck,"
I made a simple Python game. A screenshot is below:

Basically, a paddle moves left and right catching particles. Some make you lose points while others make you gains points.
This is my first Deep Q Learning Project, so I probably messed something up, but here is what I have:
model = Sequential()
model.add(Dense(200, input_shape=(4,), activation='relu'))
model.add(Dense(200, activation='relu'))
model.add(Dense(3, activation='linear'))
model.compile(loss='categorical_crossentropy', optimizer='adam')

The four inputs are X position of player, X and Y position of particle (one at a time), and the type of particle. Output is left, right, or don't move.
Here is the learning algorithm:
def learning(num_episodes=500):
    y = 0.8
    eps = 0.5
    decay_factor = 0.9999
    for i in range(num_episodes):
        state = GAME.reset()
        GAME.done = False
        eps *= decay_factor
        done = False
        while not done:
            if np.random.random() < eps: #exploration
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(state))
            new_state, reward, done = GAME.step(a) #does that step
            #reward can be -20, -5, 1, and 5
            target = reward + y * np.max(model.predict(new_state))
            target_vec = model.predict(state)[0]
            target_vec[a] = target
            model.fit(state, target_vec.reshape(-1, 3), epochs=1, verbose=0)
            state = new_state

After training, this usually results in the paddle just going to the side and staying there. I am not sure if the NN architecture (units and hidden layers) is appropriate for given complexity. Also, is it possible that this is failing due to the rewards being very delayed? It can take 100+ frames to get to the food, so maybe this isn't registering well with the neural network.
I only started learning about reinforcement learning yesterday, so would appreciate advice!
","['neural-networks', 'reinforcement-learning', 'python', 'q-learning', 'keras']",
What is a high performing network architecture to use in a PPO2 MlpLnLstmPolicy RL model?,"
I am playing around with creating custom architectures in stable-baselines. Specifically I am training an agent using a PPO2 model.
My question is, are there some rules of thumb or best practices in network architecture (of actor and critic networks) to achieve higher performance i.e. larger rewards?
For example, I find that usually using wider layers (e.g. 256 rather than 128 units) and adding more layers (e.g. a deep network with 5 layers rather than 2) achieves a smaller RMSE (better performance) for time series prediction when training an LSTM. Would similar conventions apply to reinforcement learning - would adding more layers to the actor and critic network have higher performance - does sharing an input layer work well?
","['reinforcement-learning', 'open-ai']",
Why do these reward functions give different training curves?,"
Let's say our task is to pick and place a block, like: https://gym.openai.com/envs/FetchPickAndPlace-v0/
Reward function 1: -1 for block not placed, 0 for block placed
Reward function 2: 0 for block not placed, +1 for block placed
I noticed training 1 is much faster than 2... I am using the HER implementation from OpenAI. Why is that?
",['reinforcement-learning'],
How to make deepfake video without a fancy PC?,"
Is there any way to make deepfake videos without a fancy computer? For example, run the DeepFaceLab on a website so your own computer won't get involved?
","['deep-learning', 'deepfakes']","
Yes. There are services that provide free environment to run jupyter notebooks for research purposes (with GPU included, which is crucial for neural networks) - such as Google Colaboratory and Kaggle Kernels. Although they limit how long your computation may run (12 and 6 hours accordingly), which adds some difficulties to the process, although I think it is possible to bypass these restrictions.
"
How do deep fakes get the right encoding for both people?,"
Deep fakes work by using a single encoder but then having a different decoder for different people.
But I wondered what if the encoder encodes say ""closed eyes"" of person A as the same code for ""closed mouth"" of person B. i.e. the codes could use the same codewords for different aspects of person A and person B. i.e. person A and person B could use the same codewords to descibe each of them except the codewords don't mean the same thing.
Then when you do a deep fake on person A with closed eyes it emerges as person B with a closed mouth.
How does one combat this effect. Or is does it just work and no-one knows why?
","['deep-learning', 'deepfakes']",
"Can Neural Networks be considered as ""Strong AI""?","
I've been reading on the differences between ""Strong"" and ""Weak ""AI. 
I was wondering, where do Neural Networks (especially deep ones) fall in this spectrum? Can they be considered ""Strong AI""? If not, is there any model that can be considered ""Strong AI""?
","['neural-networks', 'agi']",
How is REINFORCE used instead of Backpropagation?,"
In neural networks with stochastic layers I've seen the use of the REINFORCE estimator for estimating the gradient (because it can't be computed directly).
Some such examples are Show, Attend and Tell, Recurrent models of visual attention and Multiple Object Recognition with Visual Attention.
However, I haven't figured out how this exactly works. How do we ""bypass"" the gradient's computation by using the REINFORCE learning rule? Does anyone have any insight on this? 
","['neural-networks', 'backpropagation', 'reinforce']","
REINFORCE is called a gradient estimator because it doesn't work on the true gradient, that comes from a loss function and the whole data, but makes up a heuristic loss, so that the gradient it ends up with isn't the true one. Let's see that with the REINFORCE equation:
$$
{\huge
\Delta \mathbf{\theta}_t = \alpha \nabla_{\mathbf{\theta}} \log \pi_{\mathbf{\theta}} (a_t \mid s_t) v_t
}%
$$
As this shows, the gradient is still there ($\nabla_\theta$). But the policy corresponds to the network's output, so we can use backpropagation to compute the gradient of that heuristic loss with respect to the weights. The real gradient is unknown to us, but this estimation will do the job.
"
Is a switch from R to Python worth it? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I just finished a 1-year Data Science master's program where we were taught R. I found that Python is more popular and has a larger community in AI.
What are the advantages that Python may have over R in terms of features applicable to the field of Data Science and AI (other than popularity and larger community)? What positions in Data Science and AI would be more Python-heavy than R-heavy (especially comparing industry, academic, and government job positions)? In short, is Python worthwhile in all job situations or can I get by with only R in some positions?
","['python', 'comparison', 'r']","
Of course, this type of questions will also lead to primarily opinion-based answers. Nonetheless, it is possible to enumerate the strengths and weakness of each language, with respect to machine learning, statistics, and data analysis tasks, which I will try to list below.
R
Strengths

R was designed and developed for statisticians and data analysts, so it provides, out-of-the-box (that is, they are part of the language itself), features and facilities for statisticians, which are not available in Python, unless you install a related package. For example, the data frame, which Python does not provide, unless you install the famous Python's pandas package. There are other examples like matrices, vectors, etc. In Python, there are also similar data structures, but they are more general, so not specifically targeted for statisticians.
There are a lot of statistical libraries.

Weakness

Given its purpose, R is mainly used to solve statistical or data analysis problems. However, it can also be used outside of this domain. See, for example, this Quora question: Is R used outside of statistics and data analysis?.

Python
Strengths

A lot of people and companies, including Google and Facebook, invest a lot in Python. For example, the main programming language of TensorFlow and PyTorch (two widely used machine learning frameworks) is Python. So, it is very unlikely that Python won't continue to be widely used in machine learning for at least 5-10 more years.
The Python community is likely a lot bigger than the R community. In fact, for example, if you look at Tiobe's index, Python is placed 3rd, while R is placed 20th.
Python is also widely used outside of the statistics or machine learning communities. For example, it is used for web development (see e.g. the Python frameworks Django or Flask).
There are a lot of machine learning libraries (e.g. TensorFlow and PyTorch).

Weakness

It does not provide, out-of-the-box, the statistical and data analysis functionalities that R provides, unless you install an appropriate package. This might be a weakness or a strength, depending on your philosophical point of view.

There are other possible advantages and disadvantages of these languages. For example, both languages are dynamic. However, this feature can both be an advantage and a disadvantage (and it is not strictly related to machine learning or statistics), so I did not list it above. I avoided mentioning opinionated language features, such as code readability and learning curve, for obvious reasons (e.g. not all people have the same programming experience).
Conclusion
Python is definitely worth learning if you are studying machine learning or statistics. However, it does not mean that you will not use R anymore. R might still be handier for certain tasks. 
"
What is an identity recurrent neural network?,"
What is an identity recurrent neural network (IRNN)? What is the difference between an IRNN and RNN?
","['recurrent-neural-networks', 'comparison', 'definitions', 'papers']","
An identity recurrent neural network (IRNN) is a vanilla recurrent neural network (as opposed to e.g. LSTMs) whose recurrent weight matrices are initialized with the identity matrix, the biases are initialized to zero, and the hidden units (or neurons) use the rectified linear unit (ReLU).
An IRNN can be trained more easily using gradient descent (as opposed to a vanilla RNN that is not an IRNN), given that it behaves similarly to an LSTM-based RNN, that is, an IRNN does not suffer (much) from the vanishing gradient problem. 
The IRNN achieves a performance similar to LSTM-based RNNs in certain tasks, including the adding problem (a standard problem that is used to examine the power of recurrent models in learning long-term dependencies). In terms of architecture, the vanilla RNNs are much simpler than LSTM-based RNNs, so this is an advantage.
For more details, see the paper A Simple Way to Initialize Recurrent Networks of Rectified Linear Units (2015), by Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton. See also this Keras implementation of the MNIST experiment described in the linked paper.
"
"Reverse engineering controller sensitivity/aim for several games ie acceleration curves, deadzones, etc","
A machine learning project I am working on requires me to interface with an Xbox controller connected to a PC. The implementation must do the following two things:
Record the joystick input from the controller into a file at regular intervals, along with an associated screenshot from a game. (ex: 60 times a second).
With this data I want to try to replicate/reverse engineer a few different FPS games sensitivitys,dead zones, acceleration curves.
Does anyone have any idea as to how I'd go about doing this? I'm not sure where to start. If this question isn't appropriate for this sub, Im where could I ask?
","['machine-learning', 'computer-vision']",
Backpropagation equation for a variant on the usual Linear Neuron architecture,"
Recently I encountered a variant on the normal linear neural layer architecture: Instead of $Z = XW + B$, we now have $Z = (X-A)W + B$. So we have a 'pre-bias' $A$ that affects the activation of the last layer, before multiplication by weights. I don't understand the backpropagation equations for $dA$ and $dB$ ($dW$ is as expected).
Here is the original paper in which it appeared (although the paper itself isn't actually that relevant): https://papers.nips.cc/paper/4830-learning-invariant-representations-of-molecules-for-atomization-energy-prediction.pdf
Here is the link to the full code of the neural network: http://www.quantum-machine.org/code/nn-qm7.tar.gz
class Linear(Module):

    def __init__(self,m,n):

        self.tr = m**.5 / n**.5
        self.lr = 1 / m**.5
        
        self.W = numpy.random.normal(0,1 / m**.5,[m,n]).astype('float32')
        self.A = numpy.zeros([m]).astype('float32')
        self.B = numpy.zeros([n]).astype('float32')

    def forward(self,X):
        self.X = X
        Y = numpy.dot(X-self.A,self.W)+self.B
        return Y

    def backward(self,DY):
        self.DW = numpy.dot((self.X-self.A).T,DY)
        self.DA = -(self.X-self.A).sum(axis=0)
        self.DB = DY.sum(axis=0) + numpy.dot(self.DA,self.W)
        DX = self.tr * numpy.dot(DY,self.W.T)
        return DX

    def update(self,lr):
        self.W -= lr*self.lr*self.DW
        self.B -= lr*self.lr*self.DB
        self.A -= lr*self.lr*self.DA

    def average(self,nn,a):
        self.W = a*nn.W + (1-a)*self.W
        self.B = a*nn.B + (1-a)*self.B
        self.A = a*nn.A + (1-a)*self.A

","['machine-learning', 'backpropagation', 'multilayer-perceptrons']","
The forward prop equation is:
$$
Z = (X-A)W - B = XW - AW - B
$$
So the derivatives for $Z$ w.r.t $W$, $A$, $B$ and $X$ should be:
$$
\frac{\partial Z}{\partial W} = X-A  \\
\frac{\partial Z}{\partial A} = - W  \\
\frac{\partial Z}{\partial B} = - 1 \\
\frac{\partial Z}{\partial X} = W
$$
I don't know why he needs the last one though. The first is, like you said, as expected. The other two are wrong, I don't know why he used them in the implementation.
"
Is it a good idea to store the policy in a database?,"
I'm a beginner in ML and have been researching RL quite a bit recently. I'm planning to create an RL application to play a zero-sum game. This will be web-based, so anyone can play it. 
I wondered if I need to create a database (or some other kind of storage) to store the policy the RL algorithm is updating, so that it can be used by the application when the next human user comes along to play against the application?
","['reinforcement-learning', 'storage']",
Can Microsoft's cognitive service find similar person in a set of images without using the face service?,"
I need to create an application that can detect if a person X entered as an input exists in an image set and return as output all the images in which the person X exists. The problem is that the pictures do not only contain people's faces, they also contain pictures taken from behind. 
Is it possible to use Microsoft's cognitive services? If not, is there another solution to allow the realization of this application?
","['machine-learning', 'computer-vision', 'object-recognition', 'cognitive-science']",
Does Retina-net's focal loss accomplish its goal?,"
Taking out the weighting factor we can define focal loss as
$$FL(p) = -(1-p)^\gamma log(p) $$ 
Where $p$ is the target probability. The idea being that single stage object detectors have a huge class imbalance between foreground and background (several orders of magnitude of difference), and this loss will down-scale all results that are positively classified compared to normal cross entropy ($CE(p) = -log(p)$) so that the optimization can then focus on the rest.  
On the other hand, the general optimization scheme uses the gradient to find the direction with the steepest descent. There exists methodologies for adaption, momentum and etc but that is the general gist.
$$ \theta \leftarrow \theta - \eta \nabla_\theta L $$ 
Focal losses gradient follows as so
$$\dot {FL}(p) = \dot p [\gamma(1-p)^{\gamma -1} log(p) -\frac{(1-p)^\gamma}{p}]$$ compared to the normal crossentropies loss of
$$ \dot{CE}(p) = -\frac{\dot p}{p}$$ 
So we can now rewrite these as  
$$\dot{FL}(p) = (1-p)^\gamma \dot{CE}(p) + \gamma \dot p (1-p)^{\gamma -1} log(p)$$ 
The initial term, given our optimization scheme will do what we (and the authors of the retinanet paper) want which is downscale the effect of the labels that are already well classified but the second term is slightly less interpretative and in parameter space and may cause an unwanted result. So my question is why not remove it and only use the gradient
$$\dot L =  (1-p)^\gamma \dot{CE}(p)$$ 
Which given a $\gamma \in \mathbb{N}$ produces a loss function
$$ L(p) = -log(p) - \sum_{i=1}^\gamma {\gamma \choose i}\frac{(-p)^i}{i}$$ 
Summary: Is there a reason we make the loss adaptive and not the gradient in cases like focal loss? Does that second term add something useful?
","['neural-networks', 'computer-vision', 'optimization', 'gradient-descent', 'object-detection']",
How are the observations stored in the RNN that encodes the state?,"
I am a bit confused about observations in RL systems which use RNN to encode the state. I read a few papers like this and this. If I were to use a sequence of raw observations (or features) as an input to RNN for encoding the state of the system, I cannot change the weights of my network in the middle of the episode. Is that correct? Otherwise, the hidden state vectors will be different when the weights are changed. 
Does that mean that the use of RNN in RL has to store the entire episode before the weights can be changed?
How does then one take into account the hidden states in RNN for RL? Are there any good tutorials on RNN-RL?
","['reinforcement-learning', 'recurrent-neural-networks', 'papers']","
This research question seems to be analyzed in further details here (section 3) - https://openreview.net/pdf?id=r1lyTjAqYX.
Usually, a sequence is taken as a state to be fed into RNN to compute the final hidden state. One can then ask what initial state should the RNN be seeded with? This paper analyzes three methods with respect to the seed - 

zero initialization: When the RNN is initialized with the zero state 
burn-in: when the sequence is prepended by some preceding observations for RNN to learn a good initial state 
storing the initial hidden state: When the hidden state at the beginning of the sequence is stored

"
"What loss function is appropriate for finding ""points of interest"" in a array of x,y inputs","
I am looking into whether a neural network is appropriate to detect ""points of interest"" (POI) in a set of tuples (say length, and some sensor value). A POI is essentially a quick change in the value which doesn't follow the pattern. So if we have a linear increase in the sensor value and then it suddenly jumps by 200% that would be a POI.
Here is an example of the data I am working with:
[(1,10),(2,11),(3,14),(5,24),(6.5,25), (7,26), (8,45)]

In this example lets say ""(3,14)"", ""(5,24)"", and ""(8,45)"" are points of interest. So I am trying to design a neural network which will detect these.
I have started by creating a Convolution 1D layer with a static input length of 500 elements. 
After a couple hidden layers I apply a sigmoid function which provides a list of 0s and 1s as output where 1s signify a POI in the set.
There are a couple of issues with this approach which I am trying to solve.
In a categorical loss function an output of [1,0,0,1,0,0] for example would be seen as completely inaccurate if the expected output is [0,1,0,0,1,0] whereas in reality that is fairly accurate since the predicted POIs are very close to the real POIs.
So what I am trying to do is find a loss function to optimize the neural network.
So far I have tried:

Binary Cross Entropy: I read this is good for classifying where inputs can belong to multiple classes. I tried this out thinking each POI is essentially a ""category"". But this seems to not work and I assume it's because of what I noted above.
Mean Absolute Error: This seems to have gotten slightly better results but after closer inspection it didn't seem very accurate and would mostly uniformly predict POIs on a set.

I have tried a few others without much luck.
What loss function would be more appropriate for this?
One other output I tried was instead of outputting 0s and 1s it should just return the indexes of the points of interest so say 3, 5 8. Would this be a better output?
","['neural-networks', 'convolutional-neural-networks', 'objective-functions']","
You don't have to use machine learning to solve the problem.

Unify the scale of each data input (or each curve), such as normalize to $[0,1]$ (not necessary).
Calculate the slope of each pair of points $\frac{(y2 - y1)}{(x2 - x1)}$.
Set the threshold. Compare the difference between two adjacent slopes, the difference exceeds the threshold are marked as POI.

Isn't that simpler?
If you must solve the problem with CNN, what I can think of is that you first collect (or draw) a bunch of curves, mark the POI in advance, and then feed it to the CNN model.
"
A gated neural network for internal thought?,"
I have an idea for an RNN which has no separate internal memory state only an output. But there is a gate in which tells the neural network whether the output will be acted out in the physical world or it will be an internal thought. (It would also store its last, say, 10 outputs, so that it can have a memory of some kind.)
I think this would be quite realistic because human's either talk or think in an internal monologue, but don't do both. (It is hard to think and do things at the same time).
But I wonder how this gate will be activated. For example, when talking to someone familiar, this gate will be open, as you just say what's in your head. But for quiet contemplation time, this gate will be closed. And for thoughtful conversation, it will be open 50% of the time. So, I wonder if this gate would be controlled by the NN itself or be controlled from the environment?
I think there would be social pressure involved when talking to someone to keep the gate open. And likewise when in a library or a quiet place to keep the gate closed.
I wonder if there are some models like this out there already?
","['ai-design', 'recurrent-neural-networks']",
How to train a LSTM model with multi dimensional data,"
I am trying to train my model using LTSM layer in Keras (python). I have some problems regarding the data representation and feeding it into the model.
My data is 184 XY coodinates encoded as a numpy array with two dimensions: one corresponding to the X or Y and second is every single point of X or Y. Shape of a single spectrum is (2, 70). Altogether, my data has a dimension of (184, 2, 70).
The label set is an array of 8 elements which describes the percentage distribution of some 8 features which are describing XY. The shape of an output is (184, 8).
My question is how can I train using the time series for each XY pair and compare it to the corresponding label set? Different XY data show similar features to each other that is why it is important to use all 184 sample for the training. What would be the best approach to handle this problem? Below I show the schematics of my data and model:
Input: (184, 2, 70) (number of XY, X / Y, points)
Output: (184, 8) (number of XY, predictions)
I look forward for some ideas!

","['python', 'keras', 'time-series', 'long-short-term-memory']","
LSTM can be tricky, I'll give my $0.02.
LSTM input layer defines the shape so it would be something like this.
If I am understanding your question correctly, your data can be framed as 184 samples with 2 time steps and 70 features?
So the start of the code might look like this.
model = Sequential()
model.add(LSTM(184, input_shape=(50, 2)))

"
Relation between size of parameters and complexity of model with overfitting,"
I'm reading the book Pattern Recognition and Machine Learning by Bishop, specifically the intro where he covers polynomial regression model. In short, let's say we generate $10$ data points using the function $\sin(2\pi x)$ and add some gaussian random noise to each observation. Now we pretend not knowing the generating function and try to fit a polynomial model to these points.
As we increase the degree of the polynomial, it goes from underfitting ($d=1,2$) to overfitting ($d=10$). One thing the author notes is that the higher the degree of the polynomial, the higher the values of the coefficients (parameters). This is my first doubt: why does the size of the coefficients increase with the polynomial degree? And why is the size of the parameters related to overfitting?
Secondly, he states that even for degree $10$, if we get sufficiently many data points (say $100$), then the high degree polynomial will no longer overfit the data and should have comparatively better generalization performance. Second doubt: Why is this so?
",['overfitting'],"
Size of the co-efficients will probably increase only upto a certain degree of polynomial. This is due to the fact you are using $sin(2\pi x)$, if you used $sin(4\pi x)$ then the size of co-efficients will increase upto more degrees of polynomial. This can be seen when $sin(x)$ is represented as series:
$$ sin(x) = \frac{x}{1!} - \frac{x^3}{3!} + \frac{x^5}{5!}....$$
In your case $x \rightarrow 2\pi x$ so in order to approximate it the higher order terms must have very high co-efficients which the denominator factorial terms cannot cancel out (only upto a certain point though) and hence for small orders like $N=10$ (assume we have even terms in the series, since we are not dealing with mathematical definiteness, so even terms will cancel out or get cancelled out in some way), $10! = 3628800$ whereas $(2\pi) ^{10} = 95410558$ around 26 times greater. So you see till certain point the co-efficient values must increase for $sin(2\pi x)$. I think this answers both of your questions.
Coming to your second question, in general loosely we can say, the ML algorithm you are using performs Polynomial Regression which means fitting a curve by adjusting parameters, in a way such that the distance between the points generated by your model, for a given input, is as close as possible to the real data. 
So the question is why does increasing data points gives better generalisation? What most people do not mention is now that you have a better generalisation of the function itself, by which I mean, if I give you 2 points (least number required as per Nyquist Sampling theorem to define a $sin$ wave of certain frequency) from a $sin$ curve, unless you know beforehand you cannot tell whether it was generated from a $sin$, but if I give you 100 points within the same time period (of a sine wave) you can easily guess the data must be generated from $sin$. Similarly, an ML algorithm cannot guess where the data is generated from when the number of data-points is less and tries to fit a model according to its best guess (minimum loss), but if you give a larger number of points it'll make better guess hence better generalisation. 
Think like this, you want to make a circle with rubber band around pins. Can you make it with 4-5 pins? You need at-least certain number of pins to make it look a circle. The rubber band here is your model.
"
What is the difference between asymmetric and depthwise separable convolution?,"
I have recently discovered asymmetric convolution layers in deep learning architectures, a concept which seems very similar to depthwise separable convolutions.
Are they really the same concept with different names? If not, where is the difference? To make it concrete, what would each one look like if applied to a 128x128 image with 3 input channels (say R,G,B) and 8 output channels?
NB: I cross-posted this from stackoverflow, since this kind of theoretical question is maybe better suited here. Hoping it is OK...
","['deep-learning', 'convolutional-neural-networks', 'comparison', 'convolution']",
Train detector : 300 images with 30 objects or 9000 images with one?,"
so I have this dataset of images of people sitting in a restaurant.
I've annotated about 300 images with an average of 30 instances of ""person"" per image.
Now I'm wondering if I should have annotated only one (or just a few) person per image and processed way more images ?
I've successfully trained an SSD network with only one class, but I'm still wondering if I should have gone the other way...
Anyone got input on that ?
Cheers.
","['training', 'datasets']",
Create an AI to solve a puzzle (by deduction),"
Some puzzle games have a unique solution that can be solved by deduction rather than guesswork (e.g. Slitherlink, Masyu). Using a computer to solve this puzzle it's pretty easy, we can use a backtracking method to find the best solution in second (in general, the puzzle size is not too big).

Is it possible to train a bot to solve this kind of puzzle by deduction?

I think by train it to watch a previous step-by-step solution several times the bot can find some implicit rules/patterns to solve a specific puzzle. Is this possible? are there any references for this method?
","['ai-design', 'game-ai']",
Pipeline to Estimate Measurement of Human Body Point Cloud,"
I am developing a Body Measurement extraction application, my current stage is able to extract the point clouds of human body in a standing posture, from every angles.
Now, to be able to recognize shoulders, neck point etc, my research seems to fall into following flows:
Method A:

Obtain a lot of data, with labeled landmark points (shoulder left, shoulder right, neck line).
Use PointCNN / PointNet++ to perform segmentation for each landmark.
Once the landmarks are extracted, use Open3D / Point Cloud Library convex hull to obtain the measurement along point clouds.


Method A seems straight forward, but might depends on the quality of point cloud, especially 3rd step. 


Method B:

Obtain a lot of data, with labeled landmarks and also measurement of shoulder length, chest circumference etc.
We first train the network to identify landmarks,
Then from landmark, we record the distance to the next nearest point, train the network to obtain the measurement that we want.

Method C:

Obtain a lot of data, with measurement of shoulder length, chest circumference etc ONLY.
Pick a random point, and we record the distance to the next nearest point, train the network to obtain the measurement that we want.

My questions:

How much data needed for this kind of learning?
If I obtain training data from somewhere online, and later validate using my own scanned data, will that valid?
Which method makes more sense?
Which existing problem with solution is similar to my case? (Facial recognition?) That I can refer to it to solve my problem.

This is technically my first machine learning project, so please bear with me if my questions seems too silly.
",['convolutional-neural-networks'],
Is randomness anti-logical?,"
I came across a comment recently ""reads like sentences strung together with no logic.""  But is this even possible?
Sentences can be strung together randomly if the selection process is random. (Random sentences in a random sequence.) Stochasticity does not seem logicalit's a probability distribution, not based on sequence or causality.
but
That stochastic process is part of an algorithm, which is a set of instructions that must be valid for the program to compute.   
So which is it?

Is randomness anti-logical? 



Some definitions of computational logic:
The arrangement of circuit elements (as in a computer) needed for computation
also: the circuits themselves Merriam Websters  A system or set of principles underlying the arrangements of elements in a computer or electronic device so as to perform a specified task. Logical operations collectively. Google Dictionary
The system or principles underlying the representation of logical operations. Logical operations collectively, as performed by electronic or other devices.
Oxford English Dictionary
Some definitions of randomness
Being or relating to a set or to an element of a set each of whose elements has equal probability of occurrence. Lacking a definite plan, purpose, or pattern.
Merriam Websters. Made, done, happening, or chosen without method or conscious decision. Google Dictionary Having no definite aim or purpose; not sent or guided in a particular direction; made, done, occurring, etc., without method.  Seeming to be without purpose or direct relationship to a stimulus. Oxford English Dictionary
","['philosophy', 'logic', 'randomness']","
I think the answer here lies in that the dictionary definition of randomness you have is not the one used in statistics, ML, or mathematics. We define randomness to mean there exists a distribution with generally greater than 0 uncertainty.  
Depending on who you talk to, we live in a random universe (the way we define quantum mechanics depends on a wave function (essentially a probability distribution)  
So why if a sequence is drawn from a distribution is it illogical? First, even as humans we can make a strong argument that what we say is random. I mean we speak to convey some form of message or context, but there exists multiple ways to deliver this, but we choose a single one. Our brains inherently model $p(\vec w|c)$ where $\vec w$ is the sequence and $c$ is our context or message we want to convey.  
Takeaway: Generating a sequence in an ergodic or uniform manner would be illogical, but that is not what is being modeled or done in practice. Normally its drawn from some complex distribution. 
Sidenote: My above claim could make it seem that being uniformly random implicates something illogical, and I want to emphasize that is not the case. It is domain to domain, sometimes that is the most logical solution, just in the case of sentence generation it normally isnt. I would define a logical algorithm as one that given the information at hand acts in a sensible manner towards achieving some goal, and so if something purely random does that, I don't see the problem.
"
Effect of rescaling of inputs on loss for a simple neural network,"
I've been trying out a simple neural network on the fashion_mnist dataset using keras. Regarding normalization, I've watched this video explaining why it's necessary to normalize input features, but the explanation covers the case when input features have different scales. The logic is, say there are only two features - then if the range of one of them is much larger than that of the other, the gradient descent steps will stagger along slowly towards the minimum.
Now I'm doing a different course on implementing neural networks and am currently studying the following example - the input features are pixel values ranging from 0 to 255, the total number of features (pixels) is 576 and we're supposed to classify images into one of ten classes. Here's the code:
import tensorflow as tf

(Xtrain, ytrain) ,  (Xtest, ytest) = tf.keras.datasets.fashion_mnist.load_data()

Xtrain_norm = Xtrain.copy()/255.0
Xtest_norm = Xtest.copy()/255.0

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(128, activation=""relu""),
                                    tf.keras.layers.Dense(10, activation=""softmax"")])

model.compile(optimizer = ""adam"", loss = ""sparse_categorical_crossentropy"")
model.fit(Xtrain_norm, ytrain, epochs=5)
model.evaluate(Xtest_norm, ytest)
------------------------------------OUTPUT------------------------------------
Epoch 1/5
60000/60000 [==============================] - 9s 145us/sample - loss: 0.5012
Epoch 2/5
60000/60000 [==============================] - 7s 123us/sample - loss: 0.3798
Epoch 3/5
60000/60000 [==============================] - 7s 123us/sample - loss: 0.3412
Epoch 4/5
60000/60000 [==============================] - 7s 123us/sample - loss: 0.3182
Epoch 5/5
60000/60000 [==============================] - 7s 124us/sample - loss: 0.2966
10000/10000 [==============================] - 1s 109us/sample - loss: 0.3385
0.3384787309527397

So far, so good. Note that, as advised in the course, I've rescaled all inputs by dividing by 255. Next, I ran without any rescaling:
import tensorflow as tf

(Xtrain, ytrain) ,  (Xtest, ytest) = tf.keras.datasets.fashion_mnist.load_data()

model2 = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(128, activation=""relu""),
                                    tf.keras.layers.Dense(10, activation=""softmax"")])

model2.compile(optimizer = ""adam"", loss = ""sparse_categorical_crossentropy"")
model2.fit(Xtrain, ytrain, epochs=5)
model2.evaluate(Xtest, ytest)
------------------------------------OUTPUT------------------------------------
Epoch 1/5
60000/60000 [==============================] - 9s 158us/sample - loss: 13.0456
Epoch 2/5
60000/60000 [==============================] - 8s 137us/sample - loss: 13.0127
Epoch 3/5
60000/60000 [==============================] - 8s 140us/sample - loss: 12.9553
Epoch 4/5
60000/60000 [==============================] - 9s 144us/sample - loss: 12.9172
Epoch 5/5
60000/60000 [==============================] - 9s 142us/sample - loss: 12.9154
10000/10000 [==============================] - 1s 121us/sample - loss: 12.9235
12.923488986206054

So somehow rescaling does make a difference? Does that mean if I further reduce the scale, the performance will improve? Worth trying out:
import tensorflow as tf

(Xtrain, ytrain) ,  (Xtest, ytest) = tf.keras.datasets.fashion_mnist.load_data()

Xtrain_norm = Xtrain.copy()/1000.0
Xtest_norm = Xtest.copy()/1000.0

model3 = tf.keras.models.Sequential([tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(128, activation=""relu""),
                                    tf.keras.layers.Dense(10, activation=""softmax"")])

model3.compile(optimizer = ""adam"", loss = ""sparse_categorical_crossentropy"")
model3.fit(Xtrain_norm, ytrain, epochs=5)
model3.evaluate(Xtest_norm, ytest)
------------------------------------OUTPUT------------------------------------
Epoch 1/5
60000/60000 [==============================] - 9s 158us/sample - loss: 0.5428
Epoch 2/5
60000/60000 [==============================] - 9s 147us/sample - loss: 0.4010
Epoch 3/5
60000/60000 [==============================] - 8s 141us/sample - loss: 0.3587
Epoch 4/5
60000/60000 [==============================] - 9s 144us/sample - loss: 0.3322
Epoch 5/5
60000/60000 [==============================] - 8s 138us/sample - loss: 0.3120
10000/10000 [==============================] - 1s 133us/sample - loss: 0.3718
0.37176641924381254

Nope. I divided by 1000 this time and the performance seems worse than the first model. So I have a few questions:

Why is it necessary to rescale? I understand rescaling when different features are of different scales - that will lead to a skewed surface of the cost function in parameter space. And even then, as I understand from the linked video, the problem has to do with slow learning (convergence) and not high loss/inaccuracy.  In this case, ALL the input features had the same scale. I'd assume the model would automatically adjust the scale of the weights and there would be no adverse effect on the loss. So why is the loss so high for the non-scaled case?
If the answer has anything to do with the magnitude of the inputs, why does further scaling down of the inputs lead to worse performance?

Does any of this have anything to do with the nature of the sparse categorical crossentropy loss, or the ReLU activation function? I'm very confused.
",['neural-networks'],
Extracting Descriptors and feature points for 3d mesh,"
I'm programming my work with python, and I have a mesh and I want to extract 3d descriptors and feature points from it( trying to work on multi-scale strategy) , to visualize them later on the mesh,
What I'm asking about, is references, guidelines, anything which could benefit me with this situation
The main work I'm trying to do, is to reach matching stage, where I could find one-to-one correspondence.
","['image-recognition', 'python', 'computer-vision', 'feature-extraction']",
How do I tag the most interesting parts of a video?,"
This is a follow-up question from my previous question here. I'm new to ML/DL, and one thing I need to do is to use a machine or deep learning video attention model which as the name suggests, can tag which parts of a video is probably more interesting and absorbs more viewer attention.
Do we have an available model to do that? If not, how to do it?
","['machine-learning', 'deep-learning', 'keras', 'attention']",
How to solve optimal control problem with reinforcement learning,"
The problem I am trying to attack is a predator-prey pursuit problem. There are multiple predators that pursue multiple preys and preys tried to evade predators. I am trying to solve a simplified version - one predator tries to catch a static prey on a plane. There is bunch of literature on the above problem when predators and preys are on the grid. 
Can anybody suggest articles/code where such problem is solved on a continuous plane? I am looking at continuous state space, discrete action space (predator can turn left 10 degrees, go straight, turn right 10 degrees, runs at constant speed), and discrete time. MountainCar is one dimensional version (car is predator and flag is prey) and DQN works fine. However, when I tried DQN on two dimensional plane the training become very slow (I guess dimensionality curse).
The second question concerns the definition of states and reward. In my case I consider angle between predator heading vector and vector between the predator and prey positions. Reward is the change in distance between predator and prey, 10 when prey is captured, and -10 when predator gets too far from the prey. Is this reasonable? I already asked similar question before and with the help of @Neil Slater was able to refine reward and state. 
The third question concerns when to update train network to target network. At each episode? Or only when prey is caught? Any ideas?
The last question I have is about the network structure: activation functions and regularization. Currently I am using two tanh hidden layers and linear output with l2 and dropout. Can anybody share some insights?
Thanks in advance!
",['reinforcement-learning'],
What is the best measure for detecting overfitting?,"
I wanted to ask about the methodology of testing the ML models against overfitting. Please note that I don't mean any overfitting reducing methods like regularisation, just a measure to judge whether a model has overfitting problems. 
I am currently developing a framework for tuning models (features, hyperparameters) based on evolutionary algorithms. And the problem that I face is the lack of a good method to judge if the model overfits before using the test set. I encountered the cases where the model that was good on both training and validation sets, behaved poorly on the test set for both randomized and not randomized training and validation splits. I used k-fold cross-validation with additionally estimating the standard deviation of all folds results (the smaller deviation means better model), but, still, it doesn't work as expected. 
Summing up, I usually don't see a correlation (or a very poor one) between training, validation and k-fold errors with test errors. In other words, tuning the model to obtain lower values of any of the above mentioned measures usually does not mean lowering the test error.
Could I ask you, how in practice you test your models? And maybe there are some new methods not mentioned in typical ML books?
","['machine-learning', 'overfitting', 'cross-validation', 'loocv', 'k-fold-cv']",
Are the training loss and validation loss plotted per sample or per batch?,"
I am using a CNN to train on some data, where training size = 21700 samples, and test size is 653 samples, and say I am using a batch_size of 500 (I am accounting for samples out of batch size as well). 
I have been looking this up for a long time now, but can't get a clear answer, but when plotting the loss functions to check for whether the model is overfitting or not, do I plot as follows 
for j in range(num_epochs):
  <some training code---Take gradient descent step do wonders>
  batch_loss=0
  for i in range(num_batches_train):
       batch_loss = something....criterion(target,output)...
       total_loss += batch_loss
  Losses_Train_Per_Epoch.append(total_loss/num_samples_train)#and this is 

where I need help
Losses_Train_Per_Epoch.append(total_loss/num_batches_train)
and doing the same for Losses_Validation_Per_Epoch.
plt.plot(Losses_Train_Per_Epoch, Losses_Validation_Per_epoch)

So, basically, what I am asking is, should I divide by num_samples or num_batches or batch_size? Which one is it?
","['convolutional-neural-networks', 'objective-functions', 'supervised-learning', 'pytorch']",
Is there a continuous conditional variational auto-encoder?,"
The Conditional Variational Autoencoder (CVAE), introduced in the paper Learning Structured Output Representation using Deep Conditional Generative Models (2015), is an extension of Variational Autoencoder (VAE) (2013). In VAEs, we have no control over the data generation process, something problematic if we want to generate some specific data. Say, in MNIST, generate instances of 6.
So far, I have only been able to find CVAEs that can condition to discrete features (classes). Is there a CVAE that allows us to condition to continuous variables, kind of a stochastic predictive model?
","['neural-networks', 'reference-request', 'autoencoders', 'variational-autoencoder', 'conditional-vae']","
Whether a discrete or continuous class, you can model it the same.
Denote the encoder $q$ and the decoder $p$. Recall the variational autoencoder's goal is to minimize the $KL$ divergence between $q$ and $p$'s posterior. i.e. $\min_{\theta, \phi} \ KL(q(z|x;\theta) || p(z|x; \phi))$ where $\theta$ and $\phi$ parameterize the encoder and decoder respectively. To make this tractable this is generally done by using the Evidence Lower Bound (because it has the same minimum) and parametrizing $q$ with some form of reparametrization trick to make sampling differentiable.
Now your goal is to condition the sampling. In other words you are looking for modeling $p(x|z, c;\phi)$ and in turn will once again require $q(z|x, c; \theta)$. Your goal will now intuitively become once again $\min_{\theta, \phi} \ KL(q(z|x, c;\theta) || p(z|x, c; \phi))$. This is still simply transformed into the ELBO for tractability purposes. In other words your loss becomes $E_q[log \ p(x|z,c)] - KL(q(z|x,c)||p(z|c)$.
Takeaway: Conditioning doesn't change much, just embed your context and inject it both into the encoder and decoder, the fact that its continuous doesn't change anything. For implementation details, normally people just project/normalize and concatenate it somehow to some representation of $x$ in both the decoder/encoder.
"
How to detect patterns in a data set of given IP addresses using a neural network?,"
How to detect patterns in a data set of given IP addresses using a neural network?
The data set is actually a list of all the vulnerable devices on a network. I want to use a neural network that detctes any patters in the occurrences of these vulnerabilities with reference to their IPs and ports.
","['neural-networks', 'deep-learning', 'pattern-recognition']","
It seems that you want to detect ranges of IP addresses that are vulnerable/dangerous/etc, right? Such ranges are essentially numeric intervals, and so my suggestion is to look at decision tree learning instead of neural networks, because you are essentially doing a classification task where you want to test both categorical data and splits over numerical attributes.
The result will be a tree-like function (nested conditionals) of the form
IF ...> address > ... 
   THEN [vulnerable]
   ELSE IF port=... 
              THEN [not vulnerable]
              ELSE [vulnerable]

where a huge benefit is that it is also more human-readable than a neural net.
The most prominent algorithms for decision trees are ID 3 and its successor C4.5.
"
Is it ok to struggle with mathematics while learning AI as a beginner? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            



Closed 2 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I have a decent background in Mathematics and Computer Science .I started learning AI from Andrew Ng's course from one month back. I understand logic and intuition behind everything taught but if someone asks me to write or derive mathematical formulas related to back propagation I will fail to do so.
I need to complete object recognition project within 4 months.
Am I on right path?
","['math', 'getting-started']","

Not only is it 100% ok, it's the process.  

You may be surprised to know that even mathematicians struggle with mathematics, both the proofs they are working on, and the proofs of their colleagues.  Some thinkers are so far ahead of the curve, very few understand what they're stating until generations later.
The main thing is to keep with it.  
"
What kind of output should be used for predicting angles in DNNs?,"
I am building a model which predicts angles as output. What are the different kinds of outputs that can be used to predict angles?
For example,

output the angle in radians


cyclic nature of the angles is not captured
output might be outside $\left[-\pi, \pi \right)$

output the sine and the cosine of the angle


outputs might not satisfy $\sin^2 \theta + \cos^2 \theta = 1$


What are the pros and cons of different methods?
","['deep-learning', 'deep-neural-networks', 'network-design']","
As you said, the first option is not very suitable due to the cyclic nature of the angles. However, if you don't mind discretizing the values, you could represent the output as a binary vector.
A variant of the second option seems perfect to me. You may output a 2D vector and use that vector's angle as output. You'll probably need a regularizer for the vector's norm, but that's it.
"
When should I use 3D convolutions?,"
I am new to convolutional neural networks, and I am learning 3D convolution. What I could understand is that 2D convolution gives us relationships between low-level features in the X-Y dimension, while the 3D convolution helps detect low-level features and relationships between them in all the 3 dimensions.
Consider a CNN employing 2D convolutional layers to recognize handwritten digits. If a digit, say 5, was written in different colors:

Would a strictly 2D CNN perform poorly (since they belong to different channels in the z-dimension)?
Also, are there practical well-known neural nets that employ 3D convolution?
","['convolutional-neural-networks', 'reference-request', 'convolution', 'convolutional-layers', '3d-convolution']","
3D convolutions should be used when you want to extract spatial features from your input on 3 dimensions. For computer vision, they are typically used on volumetric images, which are 3D.
Some examples are classifying 3D rendered images and medical image segmentation.
"
What are the differences between CRF and HMM?,"
What I know about CRF is that they are discriminative models, while HMM are generative models, but, in the inference method, both use the same algorithm, that is, the Viterbi algorithm, and forward and backward algorithms.
Does CRF use the same features as HMM, namely features transition and state features? 
But in here https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf, CRF has these features Edge-Observation and Node-Observation Features.
What is the difference features transition and state features vs features Edge-Observation and Node-Observation features?
","['machine-learning', 'comparison', 'supervised-learning', 'hidden-markov-model', 'conditional-random-field']",
Where to find pre-trained models for multi-camera people tracking?,"
I need to build a multi-camera people tracking system and I have no idea how to start. I read ML for Dummies and I've watched a lot of youtube classes/conferences and read a lot of articles about ML/DL, so I have all this theoretical information about what is a NN, loss function, weights, vectors, convolution, etc., but when I need to start building something, I get stuck. Even more, I don't think I can create my own models because I only have six months to finish this and I'm not sure if I'll be able to do it.
I've read some papers explaining architectures for an improved people-tracking system (e.g. https://www.intechopen.com/online-first/multi-person-tracking-based-on-faster-r-cnn-and-deep-appearance-features#B8), and it says it used ResNet-30 and stuff like that. My question is, how could I recreate the architectures in papers like that? Where can I find those pre-trained models? Or is there a place where I can get the data? 
I want to start with at least a people-tracking system, without worrying about the multi-camera part for now, and I thought of almost the same approach as the people in the paper posted, meaning I want to recognize people based on parts of their body/the whole body to identify them, and track them based on their unique features (clothing color, hair, skin tone, etc), maybe skipping the part of facial recognition since that's too advanced I think.
Any idea on where to start?
Sorry if the question is too broad or too complex. Comments about first steps and sub-dividing the problem are also welcome.
PS: The main ultimate is to track how much time people are in a certain area filmed by many cameras.
","['deep-learning', 'object-recognition']",
"What is the meaning of ""stationarity of statistics"" and ""locality of pixel dependencies""?","
I'm reading the ImageNet Classification with Deep Convolutional Neural Networks paper by Krizhevsky et al, and came across these lines in the Intro paragraph:

Their (convolutional neural networks') capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.

What's meant by ""stationarity of statistics"" and ""locality of pixel dependencies""? Also, what's the basis of saying that CNN's theoretically best performance is only slightly worse than that of feedforward NN?
","['convolutional-neural-networks', 'terminology', 'papers']",
What is the appropriate way to deal with multiple paths to same state in MCTS?,"
Many games have multiple paths to the same states. What is the appropriate way to deal with this in MCTS? 
If the state appears once in the tree, but with multiple parents, then it seems to be difficult to define back propagation: do we only propagate back along the path that got us there ""this"" time? Or do we incorporate the information everywhere? Or maybe along the ""first"" path?
If the state appears once in the tree, but with only one parent, then we ignored one of the paths, but it doesn't matter because by definition this is the same state?
If the state appears twice in the tree, aren't we wasting a lot of resources thinking about it multiple times?
",['monte-carlo-tree-search'],"
Node in a tree must have a single parent, otherwise it violates a tree definition. Also the way I look at it, there are no ""same"" states when you do MCTS. Because you are keeping the history of how you got there. So the second time you visit the ""same"" state it'll have a different history path and a single parent. 
"
How to estimate the cost and time to complete an AI Project [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



If you are a freelancer, when a client asks to create a website we can easily measure how much the total cost is needed based on the requirements of the client. (the backend, UI/UX design, features, etc.). We can even measure the estimated time of completion.
What if a client asks you to make an AI project (image recognition, speech recognition, or NLP), how do you tell the client the estimated cost and time needed to complete the project in the beginning? because the results obtained can be very different for each data used
","['ai-design', 'profession', 'computational-complexity']",
How can we efficiently and unbiasedly decide which children to generate in the expansion phase of MCTS?,"
When executing MCTS' expansion phase, where you create a number of child nodes, select one of the numbers, and simulate from that child, how can you efficiently and unbiasedly decide which child(ren) to generate?
One strategy is to always generate all possible children. I believe that this answer says that AlphaZero always generates all possible ($\sim 300$) children. If it were expensive to compute the children or if there were many of them, this might not be efficient.
One strategy is to generate a lazy stream of possible children. That is, generate one child and a promise to generate the rest. You could then randomly select one by flipping a coin: heads you take the first child, tails you keep going. This is clearly biased in favor of children earlier in the stream.
Another strategy is to compute how many $N$ children there are and provide a function to generate child $X < N$ (of type Nat -> State). You could then randomly select one by choosing uniformly in the range $[0, N)$. This may be harder to implement than the previous version because computing the number of children may be as hard as computing the children themselves. Alternatively, you could compute an upper-bound on the number of children and the function is partial (of type Nat -> Maybe State), but you'd be doing something like rejection sampling.
I believe that if the number of iterations of MCTS remaining, $X_t$, is larger than the number of children, $N$, then it doesn't matter what you do, because you'll find this node again the next iteration and expand one of the children. This seems to suggest that the only time it matters is when $X_t < N$ and in situations like AlphaZero, $N$ is so much smaller than $X_0$, that this basically never matters.
In cases where $X_0$ and $N$ are of similar size, then it seems like the number of iterations really needs to be changed into something like an amount of time and sometimes you spend your time doing playouts while other times you spend your time computing children.
Have I thought about this correctly?
",['monte-carlo-tree-search'],"
Dennis' answer is very helpful. I also found section 5.5 of the MCTS survey very useful, in particular the widening discussion. Another useful reference was https://project.dke.maastrichtuniversity.nl/games/files/msc/Roelofs_thesis.pdf
"
Is there a theory behind which model is good for a classification task for the convolutional neural network?,"
Let say I'm trying to apply CNN for image classification. There are lots of different models to choose and we can try an ensemble, but given a limit amount of resources, it does not allow to try everything.  
Is there a theory behind which model is good for a classification task for the convolutional neural network?
Right now, I'm just taking an average of three predictions.
predictions_model = [y_pred_xceptionAug,y_pred_Dense121_Aug,y_pred_resnet50Aug]
predictions = np.mean(predictions_model,axis=0)

But each model's performance is different. Is there better way for ensemble methods?
","['neural-networks', 'convolutional-neural-networks', 'image-recognition', 'classification']","
Ensembles aren't very popular in the field of computer vision. The main reason why this is, is that models are already so large parameter-wise that it is hard to fit multiple models in-memory for classification. Since there are effective ways of training very large models, people would rather create a larger networks if they had the capacity than averaging the results from multiple ones.
That being said, there is no reason why ensembling wouldn't have beneficial results for your task.
One way would be, as you do, to average the results of the models. This is usually used to reduce the bias of weaker models. Another way would be to use meta-modelling, i.e. create a fourth model (even as simple as a linear classifier) that will be trained with the outputs of the three CNNs as its input features. The idea is that the meta-model will learn the best way to weight the outputs of the CNNs so that, instead of them all having an equal vote (as is the case when you average them), the meta-model will learn the best way to weigh them.
"
"What do the words ""coarse"" and ""fine"" mean in the context of computer vision?","
I was reading the well know paper Fully Convolutional Networks for Semantic Segmentation, and, throughout the whole paper, they talk use the term fine and coarse. I was wondering what they mean. The first time they say it in the intro is:

Convolutional networks are driving advances in recognition. Convnets are not only improving for whole-image classification, but also making progress on local tasks with structured output. These include advances in
  bounding box object detection, part and keypoint prediction, and local correspondence.
The natural next step in the progression from coarse to fine inference is to make a prediction at every pixel.

It's also used in other parts of the paper

We next explain how to convert classification nets into fully convolutional nets that produce coarse output maps.

What do ""coarse"" and ""fine"" mean in the context of this paper? And in the general context of computer vision?
In English, ""coarse"" means ""rough or loose in texture or grain"" , while ""fine"" means ""involving great attention to detail"" or ""(chiefly of wood) having a fine or delicate arrangement of fibers"", but these definitions do not elucidate the meaning of these words in the context of computer vision.
This question was also asked here.
","['convolutional-neural-networks', 'computer-vision', 'terminology', 'image-segmentation', 'fully-convolutional-networks']","
tl;dr

What does that mean in the context of this paper?

With ""coarse segmentation"" the author means a segmentation that doesn't have much detail. ""Fine segmentation"", on the other hand, refers to a segmentation with a high level of detail.

But also more importantly [what does that mean in the context of] general computer vision?

The most common use in CV is to describe how general or specific a class in is classification. A ""coarse class"" is a very broad one, while a ""fine class"" is a very specific one.
Intended use
What the author refers to is the level of detail of the resulting segmentation. 
A coarse segmentation would mean that we have large blobs covering each class without much detail. On the other hand, a fine segmentation would have a much higher level of detail which can even go down to pixel level (i.e. pixel-by-pixel correct segmentation).
To make this clear look at the following two examples. As we go from the left to right, the segmentation maps go from coarse to fine:


Note that in the rightmost images the result an almost pixel-perfect (i.e. fine details) segmentation map, while the ones on the left don't have much detail and can be considered coarse.
Alternative use
Because this isn't an established terminology, some of the times coarse and fine can refer to the nature of the classes in a classification task. Take the top image for example; the label for a coarse classification task could be a tree. For a fine classification task we would have labels like oak tree, pine tree, etc. 
The most prominent example of this is the cifar dataset which has two versions: a coarse one that has 10 classes and a fine one that has 100 classes, which are all subclasses of the coarse classes. For example a coarse class is fish, while the fine ones are aquarium fish, flatfish, ray, shark, trout, etc.
For semantic segmentation an example could be the following: you want to make a street segmentation model. This a coarse segmentation would mean that it just splits the image into road, vehicle, etc. A fine segmentation, on the other hand, could also detect the type of vehicle, e.g. truck, car, etc.
"
How to represent players in a multi agent environment so each model can distinguish its own player,"
So I have 2 models trained with the DQN algorithm that I want to train in a multi-agent environment to see how they react with each other. The models were trained in an environment consisting of 0's and 1's (-1's for the other model)where 1 means that square is filled and 0 is empty. It is a map filling environment, where at each step the agent can move up, down, left or right and for each step, it stays alive without turning into itself (1 or -1 for the other) or the boundary of the environment it gets 0.005 rewards and for ""dying"" it gets -1. You can think of the player as in the game Tron, where it just leaves a trail behind. I stack the last 4 frames on top of each other so it knows which end is the ""head"". With a single agent, after training, I didn't get an optimal model which uses all the squares but it does manage to fill about 30% of the environment, which I think is the limit for this algorithm (let me know if you have thoughts on this)
Now, I put the two models in one environment where there are two players, one represented with 1's and the other with -1s. As one model is trained with -1's and the other with 1's I thought they could find their own player, however even before training if I just run the models on the environment without any exploration, they seem to affect the actions of each other. One just goes straight and dies and the other just turns once then dies at the wall (whereas in a single-agent environment these 2 models can fill about 30%). And if I do training, they just diverge to this exact behavior from random without seemingly not learning anything. So, I just wanted to ask is there anything wrong about my approach with the representation of the players (1 and -1) because I thought they would just play as they did in the single-agent environment but they don't and I couldn't get them to learn anything 
","['dqn', 'multi-agent-systems', 'environment']","
When you trained your agents separately they never saw squares with opposing values (1/-1). So agents don't really know what to expect from visiting that square. 
I'd try adding (1/-1) to the condition on which you base your squares availability. Also try increasing the reward. It's hard to give suggestions without looking at the code.
"
"If the accuracy of my current model is low ($50 \%$) and we want to minimize time in collecting more data, should we try other models?","
Suppose we have a data set with $4,000$ labeled examples. The outcome variable is trinary (three possible categorical values). Suppose the accuracy of a given model is ""bad"" (e.g. less than $50 \%$).

Question. Should you try different traditional machine learning models (e.g. multinomial logistic regression, random forests, XGBoost,
etc.), get more data, or try various deep learning models like
convolutional neural networks or recurrent neural networks?

If the purpose is to minimize time and effort in collecting training data, would deep learning models be a viable option over traditional machine learning models in this case?
","['machine-learning', 'deep-learning', 'training', 'supervised-learning', 'labeled-datasets']","
To know if your model needs more training data, try to plot out ""learning curves"", that are based on increasing size of the training set.
Basically, you calculate training and validation accuracy metrics for 1, 2, 3, 4, 5, ..., m  training samples. Size of validation set may be constant over time. If the accuracy is still rising when your data set is fully used, then you need more training data.
"
Can I use self-driving car's data set for left-hand drive cars which drive on the right lane for right-hand cars which drive on the left lane?,"
Can I use self-driving car's data set for left-hand drive cars which drive on the right lane for right-hand self-driving cars which drive on the left lane?
","['datasets', 'autonomous-vehicles']","
I would suggest to look into dataset like nuscenes, which have both left and right driving data. So if you want to test, then you can use left driving dataset for train and later right driving for testing or vice versa. I believe that will be a better option.
"
Grouped Text classification,"
I have thousands groups of paragraphs and I need to classify these paragraphs. The problem is that I need to classify each paragraph based on other paragraphs in the group! For example, a paragraph individually maybe belongs to class A but according to other paragraph in the group it belongs to class B.
I have tested lots of traditional and deep approaches( in fields like text classification, IR, text understanding, sentiment classification and so on) but those couldn't classify correctly.
I was wondering if anybody has worked in this area and could give me some suggestion. Any suggestions are appreciated. Thank you.
Update 1:
Actually we are looking for manual sentences/paragraph for some fields, so we first need to recognize if a sentence/paragraph is a manual or not second we need to classify it to it's fields and we can recognize its field only based on previous or next sentences/paragraphs.
To classify the paragraphs to manual/no-manual we have developed some promising approaches but the problem come up when we should recognize the field according to previous or next sentences/paragraphs, but which one?? we don't know the answer would be in any other sentences!!.
Update 2:
We can not use whole text of group as input because those are too big (sometimes tens of thousands of words) and contain some other classes and machine can't learn properly which lead to  the drop the accuracy sharply.
Here is a picture that maybe help to better understanding the problem:

","['machine-learning', 'deep-learning', 'natural-language-processing', 'classification', 'python']",
What are the possible neural network architecture for linear regression or time series regression?,"
I started modeling a linear regression problem using dense layers (layers.dense), which works fine. I am really excited, and now I am trying to model a time series linear regression problem using CNN, but from my research in this link Machine learning mastery
A CNN works well with sequence data, but my data isnt sequential. My data set can be found here Stack overflow question.
Is there a multivariate time series/ time series neural network architecture that I can use for time series linear/nonlinear regression?
","['neural-networks', 'keras', 'regression', 'time-series']",
What weights should I use while back-propagating?,"
I've started to learn about neural networks recently and I can't find the answer to this question.
Let's assume there's a neural network (fig. 1)

So if the loss function is:

and the derivative  is:

if I want to use this to find  what k and l (well there's only one neuron with index l here, but what if there would be more?) should i use in  and ?
I've also found ""other"" way of backpropagating it's described here, but I can't understand how they came up with that method from the original equation w -= step * dE/dw.
Sorry if I failed to explain my problem. If something isn't clear please ask in comments.
","['neural-networks', 'machine-learning', 'backpropagation']",
Understanding policy update in PPO2,"
I have a question regarding the functionality of the PPO2 algorithm together with the Stable Baselines implementation:
From the original paper I know that the policy parameters $\theta$ are updated K-times using the steps sampled (n_env * T steps):

When updating the policy parameters for a state $s_t$, are only the state observations $a_t$ and reward $r_{t+1}$ of this step considered, or also the state observations and rewards of the following steps ($t+1$) considered? My understanding is that the policy update with stochastic gradient ascent works just like in supervised learning.
I know that PPO2 uses a truncated TD($\lambda$) approach (T timesteps considered). So I guess that during the policy update for each state, subsequent states are only considered through the advantage function $A_t$ but not through the values of subsequent state observations and rewards themselves? Is that true? 
I do not quite get the Stable Baselines implementation in the method _train_step() of the PPO2 implementation so therefore the question here. 
","['python', 'open-ai', 'policy-gradients', 'proximal-policy-optimization']",
What is the exact output of the Inception ResNet V2's feature extraction layer?,"
I am working with the Inception ResNet V2 model, pre-trained with ImageNet, for face recognition.
However, I'm so confused about what the exact output of the feature extraction layer (i.e. the layer just before the fully connected layer) of Inception ResNet V2 is. Can someone clarify exactly this?
(By the way, if you know some resource that explains Inception ResNet V2 clearly, let me know).
","['deep-learning', 'convolutional-neural-networks', 'architecture', 'inception', 'image-net']","
Due to this article: https://arxiv.org/pdf/1512.00567v3.pdf?source=post_page--------------------------- , 
I try to flatten the 3-d tensor in to 1d vector: 8*8*2048, because in the article, the pool layer of inception resnet v2 at page 6 is Pool: 8 * 8 * 2048. 
But at the end, my code showed the error:
 ValueError: cannot reshape array of size 33423360 into shape (340,131072)
This is all my code:
from keras.applications.inception_resnet_v2 import InceptionResNetV2
from keras.applications.inception_resnet_v2 import preprocess_input
from keras.models import Model
from keras.preprocessing.image import load_img
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from imutils import paths
from keras.applications import imagenet_utils
from keras.preprocessing.image import img_to_array
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.preprocessing import image
import random
import os
import numpy as np 
import cv2


# Path to image 
image_path = list(paths.list_images('/content/drive/My Drive/casia-299-small'))
# Random image path
random.shuffle(image_path)
# Get image name
labels = [p.split(os.path.sep)[-2] for p in image_path]


# Encode face name in to number
le = LabelEncoder()
labels = le.fit_transform(labels)

# Load model inception v2, include_top = Fale to ignore Fully Connected layer
model = InceptionResNetV2(include_top = False, weights = 'imagenet')


# Load images and resize into required input size of Inception Resnet v2 299x299
list_image = []
for (j, imagePath) in enumerate(image_path):
    image = load_img(imagePath, target_size = (299, 299, 3))
    image = img_to_array(image)

    image = np.expand_dims(image, 0)
    image = imagenet_utils.preprocess_input(image)

    list_image.append(image)

# Use pre-trained model to extract feature
list_image = np.vstack(list_image)
print(""LIst image: "", list_image)
features = model.predict(list_image)
print(""feature: "", features)
print(""feature shape[0]: "", features.shape[0])
print(""feature shape: "", features.shape)
features = features.reshape((features.shape[0], 8*8*2048))

# Split training set and test set n ratio of 80-20
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state =42)

params = {'C': [0.1, 1.0, 10.0, 100.0]}
model = GridSearchCV(LogisticRegression(), params)
model.fit(x_train,y_train)
model.save('/content/drive/My Drive/casia-299-small/myweight1.h5')
print('Best parameter for the model {}'.format(model.best_params_))

preds = model.predict(x_test)
print(classification_report(y_test, preds))
```

"
"What are the major differences between cost, loss, error, fitness, utility, objective, criterion functions?","
I find the terms cost, loss, error, fitness, utility, objective, criterion functions to be interchangeable, but any kind of minor difference explained is appreciated.
","['deep-learning', 'convolutional-neural-networks', 'terminology', 'objective-functions', 'comparison']",
What are the main technologies needed to build an AI for Warcraft 3's mod DotA?,"
What are the main technologies needed to build an AI for Warcraft 3's mod Defense of the Ancients (DotA)? Maybe I can take inspiration from OpenAI's work.
","['neural-networks', 'deep-learning', 'game-ai', 'applications']",
What is the difference between learning without forgetting and transfer learning?,"
I would like to incrementally train my model with my current dataset and I asked this question on Github, which is what I'm using SSD MobileNet v1.
Someone there told me about learning without forgetting. I'm now confused between learning without forgetting and transfer learning. How they differ from each other?
My initial problem, what I'm trying to achieve (mentioned in Github issue) is the following.
I have trained my dataset on ssd_mobilenet_v1_coco model. I'm getting continuous incremental data. Right now, my dataset is very limited.
What I want to achieve is incremental training, i.e. as soon as I get new data, I can further train my already trained model and I don't have to retrain everything:

Save trained model $M_t$
Get new data $D_{t+1}$
Train $M_t$ on $D_{t+1}$ to produce $M_{t+1}$
Let $t = t+1$, then go back to $1$

How do I perform this incremental training/learning? Should I use LwF or transfer learning?
","['comparison', 'transfer-learning', 'incremental-learning', 'domain-adaptation', 'learning-without-forgetting']","
Learning without Forgetting (LwF) is an incremental learning (sometimes also called continual or lifelong learning) technique for neural networks, which is a machine learning technique that attempts to avoid catastrophic forgetting. There are several incremental learning approaches. LwF is an incremental learning approach based on the concept of regularization. In section 3.2 of the paper Continual lifelong learning with neural networks: A review (2019), by Parisi et al., other regularisation-based continual learning techniques are described. 
LwF could be seen as a combination of distillation networks and fine-tuning, which refers to the re-training with a low learning rate (which is a very rudimentary technique to avoid catastrophically forgetting the previously learned knowledge) an already trained model $\mathcal{M}$ with new and (usually) more specific dataset, $\mathcal{D}_{\text{new}}$, with respect to the dataset, $\mathcal{D}_{\text{old}}$, with which you originally trained the given model $\mathcal{M}$. 
LwF, as opposed to other continual learning techniques, only uses the new data, so it assumes that past data (used to pre-train the network) is unavailable. The paper Learning without Forgetting goes into the details of the technique and it also describes the concepts of feature extraction, fine tuning and multitask learning, which are related to incremental learning techniques.
What is the difference between LwF and transfer learning? LwF is a combination of distillation networks and fine-tuning, which is a transfer learning technique, which is a special case of incremental learning, where the old and new tasks are different, while, in general, in incremental learning, the old and new tasks can also be the same (which is called domain adaptation).
"
Pooling vs Subsampling: Multiple Definitions?,"
I have seen people using pooling and subsampling synonymously. I have also seen people use them as different processes. I am not sure though if I have correctly inferred what they mean, when they use the terms with distinct meanings.
I think these people mean that the pooling part is selecting a submatrix from an input matrix and the subsampling part is selecting yet another submatrix that satisfies some condition from the first submatrix.
So say we have a $100 \times 100$ image. We do $10 \times 10$ non-overlapping pooling with $5 \times 5$ max subsampling. That would mean we slide the $10 \times 10$ ""pool"" across the image in strides of 10 and at every step we select the $5 \times 5$ submatrix inside the pool that has the maximum sum. That $5 \times 5$ matrix is what comes out of the $10 \times 10$ pool at the current potion. So in the end we have a $50 \times 50$ image.
Can you confirm that this usage of the terms pooling and subsampling exists?
I inferred this definition, as I cannot make sense of how some people use the two terms otherwise. For example in this video, or rather the people from the paper he is talking about (which I can't find because he only has the author name on his slide and no year).
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'definitions']",
Can the C4.5 algorithm learn a GOAP model?,"
Goal-oriented action planning (GOAP) is a well-known planning technique in computer games. It was introduced to control the non-player characters in the game F.E.A.R. (2005) by creating an abstract state model. Similar to STRIPS planning, a GOAP model contains an action name, a precondition and an effect. The domain knowledge is stored in these subactions.
The bottleneck of GOAP is, that before the planner can bring the system into the goal state. the action model has to be typed in. Usually, the programmer defines actions like ""walk to"", ""open the door"", ""take the object"", and identifies for each of them the feature set for the precondition and the effect.
In theory, this challenging task can be simplified with a decision tree learning algorithm. A decision tree stores the observed features in a tree and creates the rules on its own with inductive learning. A typical example of the C4.5 algorithm is to find a rule like ""if the weather is sunny, then play tennis"". Unfortunately, the vanilla tree learning algorithm doesn't separate between different actions. 
Is it possible to modify C4.5 algorithm such that the GOAP actions, like ""walk to"", ""open the door"", etc., are connected to individual rules?
","['decision-trees', 'strips', 'c4.5-algorithm']",
Which approach can I use to generate text based on multiple inputs?,"
I have a little experience in building various models, but I've never created anything like this, so just wondering if I can be pointed in the right direction.
I want to create (in python) a model which will generate text based on multiple inputs, varying from text input (vectorized) to timestamp and integer inputs.
For example, in the training data, the input might include:
eventType = ShotMade
shotType = 2
homeTeamScore = 2
awayTeamScore = 8
player = JR Smith
assist = George Hill
period = 1
and the output might be (possibly minus the hashtags):
JR Smith under the basket for 2! 8-4 CLE. #NBAonBTV #ThisIsWhyWePlay #PlayByPlayEveryDay #NBAFinals
or
JR Smith out here doing #WhateverItTakes to make Cavs fans forgive him. #NBAFinals
Where is the best place to look to get a good knowledge of how to do this?
","['neural-networks', 'deep-learning', 'python', 'generative-model']","
Generally, text generators work by modeling the joint distribution of the text by its Bayesian forward decomposition  
$
\begin{align*}  
p(w_1, w_2, ..., w_n) &= p(w_1) * p(w_2|w_1) * p(w_3|w_2, w_1) *\ ...\ * p(w_n|\{w_i\}_{i<n})\\
&= \prod_{i=1}^n p(w_i|\{w_k\}_{k<i})\\  
\end{align*}
$ 
From a modeling perspective, this looks right up RNN's ally, where you can have a state holding information from $\{w_k\}_{k<i}$ to learn a representation of $w_i$ 
Now, in your specific case, you're interested in a conditional text-generator, so you are trying to model $p(w_1, w_2, ..., w_n | \{v_j\}_j)$, but this same tactic works.  
$
\begin{align*}  
p(w_1, w_2, ..., w_n| \{v_j\}_j) &= p(w_1|\{v_j\}_j) * p(w_2|w_1, \{v_j\}_j) * p(w_3|w_2, w_1, \{v_j\}_j) *\ ...\ * p(w_n|\{w_i\}_{i<n}, \{v_j\}_j)\\
&= \prod_{i=1}^n p(w_i|\{w_k\}_{k<i}, \{v_j\}_j)\\  
\end{align*}
$ 
So, in your RNN or forward-based model, you can use the exact same approach just additionally embed the conditional inputs you have and somehow infuse it into the model (in practice, I have seen this through attention, concatenation, or some other common approach).
My recommendation (depending on the computational power you have) is to take advantage of the recent fad of pre-trained language models. Specifically, ones trained on next word prediction will probably do the job best. A good example is gpt-2, and, if you check out their GitHub, their code is very readable and easy to adjust for adding conditional input in the ways I have described.  
"
Couldn't an AI cheat when trying to follow its goal?,"
Hello, I was reflecting about what implications might building a strong AI have and I came across some ideas which I find disturbing, I'd love to have some external thought on that :
1) If we ever managed to create an AI say nearly as smart as a human, It would probably have been programmed with some concrete goals, as the AIs we are programming right now : Reinforcement learning allow an agent to try and increase a ""reward"" variable, regression is all about getting closer to a certain goal function etc..
But then a strong AI, which would undoubtedly be able to understand how it is built, just as we understand (partly at least) how our brains work, because it would be as smart as its creators and we don't tend to build machines that are as hard to understand as brains.
Then couldn't such an agent figure out the best way to achieve its goals would actually not be, say, pleasing and protecting the humans like we would've wanted it to do, but to get control of its own program and maximize whatever reward it was set to pursue ? Just as we could decide to branch electrods to our brain if we were able to find out how exactly our brain was built.
I really don't see how this scenario could ever be avoided if we were to build such an AI, apart from finding a perfect security preventing anyone from accessing the code of the said AI (including itself).
2) On the same note, I also wondered, could it try to not only satisfy its goals by ""cheating"" (updating its reward variables for example) but also to change itself, or commit suicide ? After all, we humans have never been able to figure out what goals we were meant to pursue (by that I mean what reward variable in our brain we are trying to increment), and many philosophers reflecting upon that matter thought about death as an escape from once goals. So my question is : could it try to change its code or kill itself ?
I have other questions and thoughts I would like to discuss, but I think this a good start, to test whether I'm in the right place for this kind of discussion.
Looking forward to your thoughts.
","['philosophy', 'agi', 'goal-based-agents']",
How to use the LSTM layer in PPO architecture?,"
What is the best way of using the LSTM layer in PPO architecture? 
Should I use them in the first layer of both actor and critic, or use them just before the final layer of these networks? 
Should I feed the architecture with a stack of states (the state stacked with the k previous states)?
","['deep-learning', 'reinforcement-learning', 'long-short-term-memory', 'actor-critic-methods', 'proximal-policy-optimization']","
Not sure if you're using TensorFlow-Agents for this, but if you are, there's some nice functionality for this built out using the ActorDistributionRnnNetwork and ValueDistributionRnnNetwork that I've found to be very helpful.  The LSTM architecture is used in both the actor and critic networks.  See this page for more information - hope this helps!  
"
Would this neural network have short term memory?,"
I want to design a NN that can remember it's last 7 actions and use them as inputs. So for example it would be able to store words in it's memory. Therefore if it had a choice of 10 different actions, the number of words it could store is $10^7$.
Here is my design:
$$out_{n+1} = f(out_n, in_n)\mathbf{N} + out_n.\mathbf{M}$$
$$action_n = \sigma(\mathbf{N} \cdot out_n)$$ 
Where $f$ represents some layered neural network. Some of the actions would be physical actions and some might be internal (such as thinking of the letter 'C').
Basically I want $out_n$ to be an array that keeps the last 6 action values and puts them back in. So $M$ will be the matrix:
$$\begin{bmatrix}
0&1&0&0&0&0\\
0&0&1&0&0&0\\
0&0&0&1&0&0\\
0&0&0&0&1&0\\
0&0&0&0&0&1\\
0&0&0&0&0&0
\end{bmatrix}$$
i.e. it would drop the 6th item from it's memory.
and $N$ would be the vector:
$$\begin{bmatrix}
1&0&0&0&0&0&0
\end{bmatrix}$$
I think this would be equivalent to an equation of the form:
$$out_{n+1}=F(in_n,out_n,out_{n-1},out_{n-2},...,out_{n-6})$$
So I think this would be an advantage over an RNN since this model remembers precisely it's last 6 actions. But would this be better than an RNN or worse? One could increase it's memory to more than 7 quite easily. 
I think it's basically the same archececture as an RNN except elinimating a lot of the connections. Is this a new design or a common design?
One problem with this design is that you might also want a memory that is over longer time periods (e.g. for actions that take more than one tick.) But that might be solved by enhancing the archecture.
","['neural-networks', 'long-short-term-memory']","
Congrats, you have invented 1d convolution. Convolution combined with RNN would have some advantage over just RNN. Think about the perception field. 
In this layer, you do aggregate $6$ values to one. Imagine two of them - it will be $36$ already, etc. But, in the end, you still need RNN at the end to aggregate a variable length to constant length.
"
How do I solve this optimal control problem with reinforcement learning?,"
I am new to reinforcement learning. I would like to solve an optimal control problem with reinforcement learning. 
The objective is for a wolf to catch a rabbit. The wolf and the rabbit run on a plane. The time is discrete. At every time step, the wolf can only run straight, change direction by 10 degrees to the right or left, change the speed by 0.1 m/s or remain at the same speed. It starts running in some random direction, and then sees the rabbit and starts chasing it. For the time being, let's assume that the rabbit sits still. 
It looks like this problem is a continuous state space and discrete action space. 
I have tried to use DQN in Keras, but I am not sure that I am using correct state variables/reward. Currently, the state variables are the velocity vector of the wolf, the distance vector from the wolf to the rabbit. The reward at each time point is the negative current time. When the wolf catches the rabbit, the reward is 1000 - current time (the wolf is penalized for running too long). 
Can somebody provide me some guidance? Eventually, I would add brains to the rabbit so that it tries to escape the wolf and compare to the optimal control solution.
","['reinforcement-learning', 'ai-design', 'control-theory']",
How is Monte Carlo different from model-based methods?,"
I was going through an article where it is mentioned:

The Monte-Carlo methods require only knowledge base (history/past experiences)sample sequences of (states, actions and rewards) from the interaction with the environment, and no actual model of the environment.

Aren't the model-based method dependent on past sequences? How is Monte Carlo is different than?
","['reinforcement-learning', 'comparison', 'monte-carlo-methods', 'model-based-methods']",
How do I identify the number and type of objects in the same picture?,"
I need to identify the number and type of all objects in a picture, so there can be multiple objects of the same type.
For example, I have a picture with $10$ animals, and I want my program to tell me that, on the picture, I have $3$ elephants, $2$ cats and $5$ dogs. However, I do not need the detection of the location of the objects. All I need is the information on the number of objects of each class, without their possible locations.
I wanted to ask you guys for help in defining the type of problem I am dealing with and maybe some suggestions about where to start looking for a solution. It would be nice if you could point out some directions, algorithms or network architectures to solve the problem described below.
","['neural-networks', 'machine-learning', 'computer-vision', 'object-recognition', 'object-detection']",
What is the time complexity of the forward pass algorithm of a feedforward neural network?,"
How do I determine the time complexity of the forward pass algorithm of a feedforward neural network? How many multiplications are done to generate the output?
","['neural-networks', 'feedforward-neural-networks', 'time-complexity', 'complexity-theory', 'forward-pass']",
How can AI be used to design UI Interfaces?,"
I'm very new to AI.  I read somewhere that AI can be used to create GUI UI/UX design. That has fascinated me for a long time. But, since I'm very new here, I don't have any idea how it can happen. 
The usual steps to create the UI Design are:

Create Grids.
Draw Buttons/Text/Boxes/Borders/styles.
Choose Color Schemes.
Follow CRAP Principle (Contrast, Repeatition, Alignment, Proximity)

I wonder how can AI algorithms help with that. I know a bit of neural networking and the closest I can think of is the following two methods (Supervised Learning).

Draw grids manually and train the Software manually to learn proper styles until it becomes capable of giving modern results and design its own design language.
Take a list of a few websites (for example) from the internet and let the software learn and explore the source code and CSS style sheets and learn and program neurons manually until it becomes capable of making it's own unique styles.

","['neural-networks', 'supervised-learning']","
If you see the use case, on higher level it seems to generate some visual output - the design but when seen at lower level, this design is output of some code. 
One way we can do it is to train a neural network that learns to generate code which can be seen as some form of organized text. So now can be treated as a text generation problem on which you can find a lot of literature.
pix2code is one implementation that uses this idea and even expands it. In this paper the authors took it to another level, they also used the visual part - the GUI and built an architecture that takes both the code and the GUI as input and learns to generate code. So eventually it would be capable of generating code when a simple GUI was given. 
Also there are some implementations where even the network could produce code even when a rough sketch was given.
"
Can the AI in a box experiment be formalized?,"
Introduction
The AI in a box experiment is about a super strong game AI which starts with lower resources than the opponent and the question is, if the AI is able to win the game at the end, which is equal to escape from the prison. A typical example is a match of computer chess in which the AI player starts only with a king, but the human starts with all the 16 pieces including the queen, and the powerful bishop.
Winning the game
In case of a very asymmetric setup, the AI has no chance to win the game. Even if the AI thinks 100 moves ahead, a single king can't win against 16 opponent figures. But what happens, if the AI starts with 8 pieces and the human with 16? A formalized hypothesis will look like:

strength of the AI x resources weakness = strength of the human x resources strength

To put the AI for sure into a prison, the strength of the AI should be low and it's resources too. If the resources are low but the strength is middle, then the AI has a certain chance to escape from the prison. And if the AI has maximum strength and maximum resources, then the human player gets a serious problem.
Is this formalized prediction supported by the AI literature in academia?
","['philosophy', 'control-problem', 'ai-box']",
LSTM text classifier shows unexpected cyclical pattern in loss,"
I'm training a text classifier in PyTorch and I'm experiencing an unexplainable cyclical pattern in the loss curve. The loss drops drastically at the beginning of each epoch and then starts rising slowly. However, the global convergence pattern seems OK. Here's how it looks:

The model is very basic and I'm using the Adam optimizer with default parameters and a learning rate of 0.001. Batches are of 512 samples. I've checked and tried a lot of stuff, so I'm running out of ideas, but I'm sure I've made a mistake somewhere.
Things I've made sure of:

Data is delivered correctly (VQA v1.0 questions).
DataLoader is shuffling the dataset.
LSTM's memory is being zeroed correctly
Gradient isn't leaking through input tensors.

Things I've already tried:

Lowering the learning rate. Pattern remains, although amplitude is lower.
Training without momentum (plain SGD). Gradient noise masks the pattern a bit, but it's still there.
Using a smaller batch size (gradient noise can grow until it kinda masks the pattern, but that's not like solving it).

The model
class QuestionAnswerer(nn.Module):

    def __init__(self):
        super(QuestionAnswerer, self).__init__()
        self._wemb = nn.Embedding(N_WORDS, HIDDEN_UNITS, padding_idx=NULL_ID)
        self._lstm = nn.LSTM(HIDDEN_UNITS, HIDDEN_UNITS)
        self._final = nn.Linear(HIDDEN_UNITS, N_ANSWERS)

    def forward(self, question, length):
        B = length.size(0)
        embed = self._wemb(question)
        hidden = self._lstm(embed)[0][length-1, torch.arange(B)]
        return self._final(hidden)

","['long-short-term-memory', 'objective-functions', 'pytorch', 'learning-curve']",
Coloring graphs with reinforcement learning,"
I am trying to build an RL agent to solve the NP-hard problem graph coloring. The problem is quite challenging. 
This how I addressed it.
The environment
To preserve the scalability of the algorithm, providing the agent with the whole graph wouldn't be a good idea. Therefore, the input for the agent would be a window of embeddings.
More precisely, first, I would apply an embedding to the graph to generate fixed-size vectors to every vertex in the graph (thus, every vertex in the graph is represented as a vector that contains some information about its neighborhood and position in the graph).
Second, the agent will get a window of the embedding. For example, when coloring vertex number $17$, the input would be the $2n$ vectors from vertex $17-n$ to $17+n$, to give the agent more local information.
Third, I think the agent would require more information about the number of colors already used and the number of the already colores vertices.
The agent
My biggest problem is how the agent should be. Technically, the problem is the action space dimension. For a given graph, the maximal number of colors is the number of vertices which varies from graph to graph (losing the scalability). Plus, the possible actions at each state varies with the history of the coloring. The possible colors for a given state (or node) are all the used colors eliminating the connected colors and adding the possibility of a new color, that is, for vertex $56$, the agent has already used the first $40$ colors $\{0, 1, 2, 3, \dots,40 \}$, and node $56$ is connected to some neighbors already colored with $\{14, 22, 40 \}$, the possible colors are $\{0,1, \dots, 40 \}- \{14, 22, 40 \} + \{41\}$.
How do I overcome the high dimensional inconsistent action space?
","['reinforcement-learning', 'ai-design']",
Why do language models place less importance on punctuation?,"
I have very outdated idea about how NLP tasks are carried out by normal RNN's, LSTM's/GRU's, word2vec, etc to basically generate some hidden form of the sentence understood by the machine.
One of the things I have noticed is that in general researchers are interested in generating the context of the sentence, but oftentimes ignore punctuation marks which is on of the most important aspects for generating context. For example:

Most of the time, travellers worry about their luggage.
Most of the time travellers worry about their luggage

Source
Like this there exists probably 4 important punctuation marks .,? and !. Yet, I have not seen any significant tutorials/blogs on them. It is also interesting to note that punctuations don't have a meaning (quite important, since most language models try to map word to a numerical value/meaning), they are more of a 'delimiter'. So what is the current theory or perspective on this? And why is it ignored?
",['natural-language-processing'],"
You are right. Approaches that map words to meaning solely do fail in this regard. None the less Word2Vec and Glove have shown wonderful downstream results. This in itself may indicate that most of the time, punctuation's addition can be interpolated. But as you provided, there are cases where this just is not true!  
Now of days I would say most models actually use almost NO preprocessing. This is surprising but its due to the rise in power of learnable, reversable, tokenizations. Some examples of these include byte pair encoding (bpe) and the sentence piece model (spm).  
State-of-the-art NLP generally rely on these. Examples include BERT and GPT2, which are general purpose pretrained Language Models. Their ability to parse and understand (i use this word loosely) a wide variety of phrasing, spelling and more can be partially due to the freedom in the preprocessing.  
Takeaway: You can achieve good results by using preprocessing in a manner that will eliminate information but keep the meat and bones that you are interested in (but this requires domain knowledge paired with optimization experience), but the field seems gearing towards models that are more inclusive, more transferable, and dont have the problems you mention by design.
"
Why is the expectation calculated over finite number of points drawn from a probability distribution?,"


This is from the book Pattern Recognition by Bishop. Why is expectation here a simple average? Why is $f(x)$ not being multiplied by $p(x)$?
","['math', 'probability-distribution', 'expectation']",
Is this technique image processing or computer vision?,"
If I use my mobile camera on a signboard or announcement board on a road or in a street (like the one attached in photo) where the message is written in Russian and my mobile shows me that message in English, would this be an image processing or computer vision technique?
 
","['image-recognition', 'computer-vision', 'machine-translation', 'image-processing']",
How do you scale your ML problems? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



While I have limited resource usually to train my machine learning models, I often find that my hyperparameter optimization procedure is not necessary using all my GPU and CPU, and that is because the results also depend on the batch size in my experience.
If you find in your project that a low batch size is necessary, how do you scale your project? In a multi-GPU scenario, I could imagine running different hyperparameter settings on different GPUs, but what other options are out there?
","['machine-learning', 'hyperparameter-optimization', 'scalability']",
What are the benefits of using max-pooling in convolutional neural networks?,"
I am reading Francois Chollet's Deep learning with Python, and I came across a section about max-pooling that's really giving me trouble.
I am unable to copy-paste the content, so I've included screenshots of the paragraph that's troubling me.


I simply don't understand what he means when he talks about ""What's wrong with this setup?"" (towards the end).
How does removing the max-pooling layers ""reduce"" the amount of the initial image that we're looking at? What are the benefits of using max-pooling in convolutional neural networks, as opposed to just using convolution layers?
","['convolutional-neural-networks', 'convolution', 'pooling', 'max-pooling']","
MaxPooling pools together information. Imagine you have 2 convolutional layers $(F_1, F_2)$ respectively, each with a 3x3 kernel and a stride of $1$. Also, imagine your input is $I$ is of shape $(w,h)$. Let's call a max-pooling layer $M$ is of size $(2,2)$.
Note: I'm ignoring channels because, for these purposes, it's not necessary and can be extended to any amount of them.  
Now you have two cases:  

$O_1 = F_2 \circ F_1 \circ I$ 
$O_2 = F_2 \circ M \circ F_1 \circ I$

In these cases, $shape(O_1)=(w-4, h-4)$ and $shape(O_2)=\left(\frac{w-2}{2}-2, \frac{h-2}{2}-2 \right)$. If we plug in dummy values, like $w,h = 64,64$, we get the shapes become $(60,60)$ and $(29,29)$ respectively. As you can tell, these are very different!   
Now, there is more of a difference than just the size of the outputs, each neuron holds a pooling of more information. Let's do it out:  

Each output neuron of $F_1 \circ I$ has information from a $(3,3)$ receptive field.
Each output neuron then of $F_2 \circ F_1 \circ I$ has information from a $(3,3)$ receptive field of $F_1 \circ I$, which, if we eliminate reused nodes, is a $(5,5)$ receptive field from the initial $I$. 
Each output neuron then of $M \circ F_1 \circ I$ has information from a $(2,2)$ receptive field of $F_1 \circ I$, which, if we eliminate reused nodes, is a $(4,4)$ receptive field from the initial $I$.
Each output neuron then of $F_2 \circ M \circ F_1 \circ I$ has information from a $(4,4)$ receptive field of $M \circ F_1 \circ I$, which, if we eliminate reused nodes, is a $(8,8)$ receptive field from the initial $I$.

So, let us discuss these: Using max-pooling reduces the feature space heavily by throwing out a lot of nodes whose features aren't as indicative (makes training models more tractable) along with it does extend the receptive field with no additional parameters.
"
Understanding log probabilities of actions in the PPO objective,"
I'm trying to implement the Proximal Policy Optimization (PPO) algorithm (code here), but I am confused about certain concepts.

What is the correct way to implement log probability of a policy (denoted by $\pi_\theta$ below)?
$$
L^{C P I}(\theta)=\hat{\mathbb{E}}_{t}\left[\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} | s_{t}\right)} \hat{A}_{t}\right]=\hat{\mathbb{E}}_{t}\left[r_{t}(\theta) \hat{A}_{t}\right]
$$
Let's say my old network policy output is oldpolicy_probs=[0.1,0.2,0.6,0.1] and new network policy output is newpolicy_probs=[0.2,0.2,0.4,0.2].
Do I take the log of this directly, or should I first multiply these with the true label y_true = [0,0,1,0] as implemented here?
ratio = np.mean(np.exp(np.log(newpolicy_probs + 1e-10) - K.log(oldpolicy_probs + 1e-10))*advantage)
Once I have the ratio and I multiply it with an advantage, why do we take the mean over all actions? I suspect it might be because we are taking estimate $\hat{\mathbb{E}_t}$ but conceptually I don't understand what this gives us. Is my implementation above correct?

","['python', 'objective-functions', 'implementation', 'proximal-policy-optimization']",
How can we use Dependency Parsers for Negation detection,"
I am building a negation detection system. How to use dependency parsers for the same. I am using SPACY for dependency parser
",['natural-language-processing'],
Understanding arrangement of applying filters to input channels [duplicate],"







This question already has answers here:
                                
                            




How is the depth of a convolutional layer determined?

                                (3 answers)
                            

Closed 1 year ago.



I was watching a video about Convolutional Neural Networks: https://www.youtube.com/watch?v=SQ67NBCLV98. What I'm confused about is the arrangement of applying the filters' channels to the input image or even to the output of a previous layer.
Question 1 - Looking at the visual explanation example of how one filter with 3 channels is applied to the input image (with 3 channels), so that each 1 filter channel is applied to its corresponding input channel: .
So hence the output is 3 channels. Makes sense.
However, looking at the second screenshot which shows an example of the VGG network: , looking at the first layer (I've delineated with a red frame), which is 64 channels, where the input of the image contains 3 channels. How does the output shape become 64? The only way I would think this would be possible is if you apply:

filter channel 1 to image channel 1
filter channel 2 to image channel 2
filter channel 3 to image channel 3
filter channel 4 to image channel 1
filter channel 5 to image channel 2
filter channel 6 to image channel 3

.. and so on.
Or the other thing could be, that these are representing Conv layers, with 64 filters. Rather than a filter with 64 channels. And that's precisely what I'm confused about here. In all the popular Convolutional networks, when we see these big numbers - 64, 128, 256 ... etc, are these Conv layers with 64 filters, or are they individual filters with 64 channels each?
Question 2 - Referring back to the second screenshot, the layer I've delineated with blue frame (3x3x128). This Conv layer, as I understand, takes the output of 64 Max-pooled nodes and applies 128 Conv filters. But how does the output become 128. If we apply each filter to each Max-pooled output node, that's 64 x 128 = 8192 channels or nodes in output shape. Clearly that's not what's happening and so I'm definitely missing something here. So, how does 128 filters is applied to 64 output nodes in a way so that the output is still 128? What's the arrangement?
Many thanks in advance.
","['deep-learning', 'convolutional-neural-networks']",
What is the difference between image processing and computer vision?,"
What is the difference between image processing and computer vision? They are apparently both used in artificial intelligence.
","['computer-vision', 'terminology', 'comparison', 'image-processing']","
The Wikipedia article related to computer vision gives, in my opinion, a good description of the field and its relation to image processing. Below, I will only cite the most relevant parts of the article.

Computer vision is an interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.
The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.
Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, and image restoration.
The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.
Image processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither require assumptions nor produce interpretations about the image content.

What is the difference between computer vision and image processing?
Computer vision is about gaining high-level understanding from images or videos. For example, object recognition, which is the task of identifying the type of objects (e.g. apples or humans) in an image, is a computer vision problem. Of course, this task requires a high-level understanding of the image, that is, an understanding of the image similar to the way humans understand visual inputs, given that an apple is a high-level object that is composed of atoms, can be green, etc. For example, a neural network that attempts to classify the type of object in an image (assuming, for simplicity, there is just one type of object) would be a computer vision technique. In computer vision, you receive an image as input and you can produce an image as output or some other type of information (e.g. the type of objects in the image).
On the other hand, image processing does not necessarily imply a high-level understanding of the image. Image processing is a subfield of signal processing but applied to images, which are $2$d signals (or functions of a fixed domain). So, for example, if you have a blurred or noisy image, the task of deblurring or denoising it is part of image processing. The typical tasks in image processing are filtering (e.g. using the Gaussian filter or the mean filter), noise removal, edge detection and color processing. In image processing, you receive an image as input and you produce another image as output.
However, note that, in many cases, to gain a high-level understanding of the images, you first need to e.g. denoise them, so you could use an image processing technique to partially solve a computer vision task. In this sense, computer vision is an interdisciplinary field.
To conclude, computer vision is not a subfield of image processing, given that image processing does not necessarily involve a high-level understanding of images. On the other hand, computer vision can use image processing techniques to gain a high-level understanding of images.
"
How can I keep context in my chatbot,"
I have created a chatbot by Keras based on movie dialog. I used RNN more specifically GRU . My bot can reply well. But the problem is , it can't hold the context . As an example if I say Tell me a joke, the bot will reply something , and then if I say one more , the bot simply doesn't understand that I was asking for another joke and many more similar cases, like if I used a slang against the bot , the bot will reply me with something similar , but if I just say something romantic or good immediately after using slang , the bot will reply to me with something good . I want to keep context or environment . How can I do so . Any lead would be helpful .
","['recurrent-neural-networks', 'keras', 'chat-bots']","
This is an idea that I used for my model - 
Try using two RNN (GRU) Networks, one of them to manage current output state and the other to maintain context
Say we are at timestamp $t$ and the two GRUCells are represented as $GRU_c$ and $GRU_s$ for GRU context network and state network. (Your output coming from the state network)
At time stamp $t$ , the input $GRU_s(t) = concat(input, att(all~GRU_c~from ~[0, ~t-1]))$ where $att$ is an attention mechanism to give importance to specific parts of the conversation uptil that point (This is what maintains context)  and input $GRU_c(t) = learned~representation~of~GRU_s(t)$ , hence updating $GRU_c$ for that timestamp, which along with the historical information can be used for $GRU_s(t+1)$ 
Hope this helped!
"
What is the relationship between degrees of freedom and the size of the training dataset?,"
I am going through the book Pattern Recognition by Bishop.
At one point he says

For $M = 9$, the training set error goes to zero, as we might expect because this polynomial contains 10 degrees of freedom corresponding to the $10$ coefficients $w_0, \dots, w_9$, and so can be tuned exactly to the $10$ data points in the training set.

where $M$ is the order of the hypothesis function, and $w$ are the weights of the hypothesis function.
I did not understand how having $10$ degrees of freedom will tune the model EXACTLY to the $10$ data points? Does it mean that whenever we have a number of data points in training set equal to the degrees of freedom, the error will be zero?
","['machine-learning', 'comparison', 'regression']",
Is convergence to a local minima more likely with transfer learning?,"
While doing transfer learning where my two problems are face-generation and car-generation is it likely that, if I use the weights of one problem as the initialization of the weights for the other problem, the model will converge to a local minima? In any problem is it better to train from scratch over transfer learning? (especially for GAN training?)
","['deep-learning', 'optimization', 'generative-adversarial-networks', 'transfer-learning']",
Use deep learning to rank video scenes,"
I'm new to machine learning and especially, deep learning. Given a video (and it's subtitle), I need to generate a 10-second summary out of this video. How can I use ML and DL to produce the most representative summary out of this video? More specifically, given video scenes, what are some ways to select and rank them, and how to do it? Any ideas would be helpful.
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'problem-solving']","
Its seems like quite challenging problem; at least you would need quite a lot of annotated data and computational power.
The approaches/optimizations you could consider:

To make scene change detection and take short piece out of each
To introduce some kind of novelty metric and try to maximize
it to get most different parts of video   
To convert video to kind of vector with existing solution like
r-cnn and yolo and then process it with recurrent networks.   
The task seems to be very close to video capturing/summarization, you
can take inspiration there  
Also, the attention approach might be handy, look, for example
self-attention for video,
semantic attention for
video

"
How can I use one neural network for both players in Alpha Zero (Connect 4)?,"
First of all, it is great to have found this community!
I am currently implementing my own Alpha Zero clone on Connect4. However, I have a mental barrier I cannot overcome.
How can I use one neural network for both players? I do not understand what the input should be. 
Do I just put in the board position ($6 \times 7$) and let's say Player1's pieces on the board are represented as $-1$, empty board as $0$ and Player2's pieces as $1$? 
To me, that seems the most efficient. But then, in the backpropagation, I feel like this cannot be working. If I update the same network for both players (which Alpha Zero does), don't I try to optimize Player1 and Player 2 at the same time?  
I just can't get my head around it. 2 Neural networks, each for one player is understandable for me. But one network? I don't understand how to backpropagate? Should I just flip my ""z"" (the result of the game) every time I go one layer backward? Is that all there is to using one network?
I hope I made this clear enough. I am quite confused, I tried my best. 
Thank you for reading this!
","['neural-networks', 'deep-learning', 'backpropagation', 'deep-neural-networks', 'alphazero']",
Entropy term in Proximal Policy Optimization (PPO) becomes undefined after few training epochs,"
I have implemented the total loss of my PPO objective as follows:-
total_loss = critic_discount * critic_loss + actor_loss -  entropy_beta * K.mean(-(newpolicy_probs * K.log(newpolicy_probs)))

After training for a few epochs, the entropy term becomes ""nan"" for some reason. I used tf.Print() to see the new policy probabilities when the entropy becomes undefined, it is as follows-

new policy probs: [[6.1029973e-06 1.93471514e-08
  0.000299338106...]...]

I am not clear as to why taking log of these small probabilities is coming out as nan. Any idea how to prevent this?
","['reinforcement-learning', 'python', 'keras', 'proximal-policy-optimization']","
I browsed through some other implementations of PPO and they all add small offset (1e-10) to prevent undefined log(0). I did that and the training works now.
"
Could the Jensen-Shannon divergence and Kullback-Leibler divergence be used as loss functions of non-generation problems?,"
If I understand correctly, the KL divergence is a measure of information loss between a ground truth distribution $P$ and a predicted distribution $Q$, and the Jensen-Shannon divergence is the mean of the KL Divergences of 2 cases

Predicted distribution is mean of $P$ and $Q$, and ground truth is $P$
Predicted distribution is mean of $P$ and $Q$, and ground truth is $Q$ 

Since KL divergence can be easily interpreted as information loss in $Q$ relative to $P$, what can JS divergence interpret-ably represent? I cannot see any use cases of these measures unless there are two distributions to compare. Is there any other problem where I could use them as loss functions other than generation problems? If so how, and why?  
","['deep-learning', 'optimization', 'objective-functions', 'generative-adversarial-networks']",
What is the purpose of the noise injection in the generator network of a GAN?,"
I do not understand why with enough training how the generator cannot learn all images from the training set as a mapping from the latent space - It is the absolute optimal case in training as it replicates the distribution and the discriminator output will always be 0.5. Even though most blog posts I have seen do not mention noise, a few of them have them in their diagrams or describe their presence, but never exactly describe the purpose of this noise. 
Is this noise injected to avoid the exact reproduction of the training data? If not what is the purpose of this injection and how is exact reproduction avoided?
","['deep-learning', 'generative-adversarial-networks', 'generative-model']",
Is adding the Frobenius inner products between filter and input part of convolution or a separate step?,"
From the literature I have read so far, it is not clear how exactly the convolution operation is defined. It seems people use two different definitions:
Let us assume we are given an $n_w \times n_h \times d$ input tensor $I$ and an $m_w \times m_h \times d$ filter $F$ of $d$ kernels (I use the convention of referring to the depth-slices of filters as kernels. I also will call the depth slices of the input tensor channels). Let us also assume $F$ is the $j$th filter of $J$ filters.
Now to the definitions.
Option 1:
The convolution of $I$ with $F$ is obtained by sliding $F$ across $I$ and computing the Frobenius inner product between channel $k$ and kernel $k$ at each position, adding the products and storing them in an output matrix. That matrix is the result of the convolution. It is also the $j$th feature map in the output tensor of the convolution layer.
Let $I \in \mathbb{R}^{n_w \times n_h \times d}$ and $F \in \mathbb{R}^{m_w \times m_h \times d}$. Let $s \in \mathbb{N}$ be the stride. The operation will only be defined if the smaller tensor fits within the larger tensor along its width and height a positive integer number of times when shifting by $s$, that is if and only if $k_w = (n_w - m_w) / s + 1\in \mathbb{N}$ and $k_h = (n_h - m_h) / s + 1\in \mathbb{N}$, where $k_w \times k_h \times d$ is the shape of the output tensor. Furthermore let ${f_x : i \mapsto (x - 1)s + i}$ be a function that returns the absolute index in the input tensor, given an index $x$ in the output tensor, the stride length $s$ and a relative index $i$.
\begin{equation*}
    \begin{split}
        (I * F)_{x y} =
            & \sum_{k=1}^d \sum_{i = 1}^{m_w} \sum_{j = 1}^{m_h} I_{f_x(i) f_y(j) k} \cdot F_{i j k}
    \end{split}
\end{equation*}
Option 2:
The convolutions (plural) of $I$ with $F$ are obtained by sliding $F$ across $I$ and computing the Frobenius inner product between channel $k$ and kernel $i$ at each position. Each product is stored in a matrix associated with the channel $k$. There is no adding of the products yet. The convolutions are the result matrices. The step where the matrices are added component wise to obtain the $j$th feature map of the output tensor of the convolution layer is not part of the convolution operation, but an independent step.
Let $I \in \mathbb{R}^{n_w \times n_h}$ and $F \in \mathbb{R}^{m_w \times m_h}$. Let $s \in \mathbb{N}$ be the stride. The operation will only be defined if the smaller matrix fits within the larger one along its width and height a positive integer number of times when shifting by $s$, that is, if and only if $k_w = (n_w - m_w) / s + 1\in \mathbb{N}$ and $k_h = (n_h - m_h) / s + 1\in \mathbb{N}$, where $k_w \times k_h$ is the shape of the output matrix. Furthermore let ${f_x : i \mapsto (x - 1)s + i}$ be a function that returns the absolute index in the input matrix, given an index $x$ in the output matrix, the stride length $s$ and a relative index $i$.
\begin{equation*}
    \begin{split}
        (I * F)_{x y} =
            &\sum_{i = 1}^{m_w} \sum_{j = 1}^{m_h} I_{f_x(i) f_y(j)} \cdot F_{i j}
    \end{split}
\end{equation*}
Which of these two definitions is the common one?
","['neural-networks', 'convolutional-neural-networks', 'definitions', 'convolution']",
Can we optimize an optimization algorithm?,"
In this answer to the question Is an optimization algorithm equivalent to a neural network?, the author stated that, in theory, there is some recurrent neural network that implements a given optimization algorithm.
If so, then can we optimize the optimization algorithm?
","['neural-networks', 'meta-learning']","
We usually optimize with respect to something. For example, you can train a neural network to locate cats in an image. This operation of locating cats in an image can be thought of as a function: given an image, a neural network can be trained to return the position of the cat in the image. In this sense, we can optimize a neural network with respect to this task. 
However, if a neural network represents an optimization algorithm, then, if you change it a little bit, then it will no more be the same optimization algorithm: it might be another optimization algorithm or some other different algorithm. 
For example, most optimizations algorithms that are used to train neural networks (like Adam) are a variation of gradient descent (GD). If you think that Adam performs better than GD, then you could say that Adam is an optimization of GD. So, Adam performs better than GD with respect to something. Possibly, GD also performs better than Adam with respect to something else. Of course, this is a little bit of a stretch.
"
OpenAI Spinning Up: Breakout-v0 example [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.







                        Improve this question
                    



I haven't been able to find any assistance / examples which could help me implement OpenAI's Spinning Up resource to solve Atari's Breakout-v0 game in the OpenAI gym.
I simply want to know why the following command doesn't run, and instead produces an error that I can't find any help on:
python -m spinup.run ppo --env Breakout-v0 --exp_name simpletest

...and then the error:
ValueError: Shape must be rank 2 but is rank 4 for 'pi/multinomial/Multinomial'
 (op: 'Multinomial') with input shapes: [?,210,160,4], [].

I understand the shape dynamics, and have written several (albeit quite unoptimized!) reinforcement learning neural nets in Python, but I was looking forward to using OpenAI's Spinning Up environment to use something more sophisticated and optimized.
Thank you so much for any help on the seeminly noobish question!
","['reinforcement-learning', 'open-ai']",
How would an AI work out this question?,"
I am trying to create an AI that makes reasonable guesses at truths of statements. However...
Human: ""Prove that no number exists which is one more than a billion.""
AI: ""Is it true for the number 1? No. I am 1% sure the statement is correct.""
AI: ""Is it true for the number 2? No. I am 2% sure the statement is correct.""
...
AI: ""Is it true for the number 999,999? No. I am 99.99% sure the statement is correct.""
AI: ""Having tested a large number of examples. I conclude that the statement is correct.""
Human: ""The statement is wrong: one billion and one.""
What do you think has gone wrong?
","['ai-design', 'math', 'logic']","
I don't think that the ""try all the numbers"" approach is very representative, because I'm not sure whether or not the agent that uses that approach can be considered by any means AI.
There is no ""intelligence"" in just checking numbers to try to prove the statement. An agent that is considered to be intelligent should apply a more intelligent approach.
This becomes more evident because the question aims at exploiting the lack of scalability of the agent's strategy. If the question was ""Prove that no number exists which is one more than 5"", then the agent would have no trouble in finding the correct answer.
"
Query regarding the minmax loss function formulation of the training of a Generative Adversarial Network (GAN) [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



Just needed a clarification on the training procedure for a standard GAN.
Of my understanding the loss function to optimize is a min max (max min causing mode collapse due to focus on one class generation) problem where the loss function

needs to maximized for the discriminator and minimized for the generator networks -
1.) In this equation are the $E_{z~p_{z(z)}}$ and $E_{x~p_{data(x)}}$ the means of the distributions of the mini batch samples? Also is the optimal case for the discriminator a maximum value of 0? and the optimal case for the generator a minimum value of $log(small~value)$ {basically a massive negative value}? If so, what happens to the first term during the training of the generator - is it taken as a constant or is the discriminator performing badly considered optimal for the generator? 
2.) While putting this in code, for every step is the discriminator trained first for one step, keeping the generator constant, followed by the generator being trained for the same step, with the discriminator kept constant?
3.) In every step of training are there multiple latent vectors sampled to produce multiple generator outputs for each step? If so is the loss function an average or sum of all $V(D, G)$ for that step
","['neural-networks', 'deep-learning', 'optimization', 'generative-adversarial-networks', 'generative-model']",
What is the role of the 'fuzzifier' w in Fuzzy Clustering?,"
According to my lecture, Fuzzy c-Means tries to minimize the following objective function:
$$J(X,B,U)=\sum_{i=1}^c\sum_{j=1}^n u_{ij}^w \, d^2(\vec{\beta_i},\vec{x_j})$$
where $X$ are the data points, $B$ are the cluster-'prototypes', and $U$ is the matrix containing the fuzzy membership degrees. $d$ is a distance measure.
A constraint is that the membership degrees for a single datapoint w.r.t. all clusters sum to $1$: $\sum_{j=1}^n\, u_{ij}=1$.
Now in the first equation, what is the role of the $w$? I read that one could use any convex function instead of $(\cdot)^w$. But why use anything at all. Why don't we just use the membership degrees? My lecture says using the fuzzifier is necessary but doesn't explain why.
","['unsupervised-learning', 'fuzzy-logic', 'k-means', 'clustering']","
Its not required, you can have $m=1$, actually it can be any number $\geq 1$.   
Now the better question is why to have it? The answer is that it adds a smoothing effect. Lets look at it in each of the limits ($\lim m \rightarrow 1$ and $\lim m \rightarrow \infty$)  
Towards $\infty$, it makes $u_{ij}$ equal to $\frac{1}{c}$, making each point have equal membership of each class regardless of $m$. From the optimization perspective, its saying how can we achieve finding clusters that are closest to all points, therefore by definition it has already achieved that, and so the Loss will always be 0. (at its global minimum)  
Now in the other limit, the constants are inversely proportional to the square of the normalized euclidean distance. This makes intuitive sense, the membership is high if they are close, and the membership is low if they are not (relatively)  
So why do we have the $m$, its for control. It allows us to choose and experiment with how heavy each distance should hold weight in the membership. An example where a larger $m$ may be useful is when the data isnt clean, and you dont want to rely so heavily on euclidean distance as the membership, so you forcibly add in a smoothing effect 
"
Is PReLU superfluous with respect to ReLU?,"
Why do people use the $PReLU$ activation?
$PReLU[x] = ReLU[x] + ReLU[p*x]$
with the parameter $p$ typically being a small negative number.
If a fully connected layer is followed by a at least two element $ReLU$ layer then the combined layers together are capable of emulating exactly the $PReLU$, so why is it necessary?
Am I missing something?
","['neural-networks', 'machine-learning', 'activation-functions', 'relu']","
Here are 3 reasons I can think of:

Space - As @mshlis pointed out, size. To approximate a PReLu you require more than 1 ReLu. Even without formal proof one can easily see that PReLu is 2 adjustable (parameterizable) linear functions within 2 different ranges joined together, while ReLu is just a single adjustable (parameterizable) linear function within half that range, so you require minimum 2 ReLu's to approximate a PReLU. And thus space complexity increases and you require more space to store parameters
Time - This increase in number of ReLu directly affects training time, here is a question on the time complexity of training a Neural Network, you can check out and work out the necessary mathematical details for time increment for a 2x Neural Network size.
Dead ReLu's - This is a problem in which a ReLu output becomes 0 due to negative input, so there is no way of flowing your gradient through it and thus, it further has no effect on the training. It can be made alive again only if the other alive ReLu's optimise some  activations from earlier layers such that the dead ReLu again has a positive output. This is not very likely, since the loss is optimised by adjusting weights and not by adjusting whether dead ReLu's are present (basically it is not a learnable parameter so there is a random chance of it coming alive again, other ReLu's do not strive to make it alive). So, to accommodate dead ReLu's the size of Neural Net needs to be increased more, which again leads to added time complexity. Here is a question on Dead ReLU's.
PReLu's do not suffer from this problem (which is probably one of the reasons of their introduction) and thus is definitely a better choice in terms of this criteria.

From personal experience, for a small number of epochs PReLu's tend to perform better than ReLu's for small number of epochs (I have trained only for small epochs). With further epochs and optimisation, this observation might cease to hold true.
"
Language Model from missing data,"
I want to learn how a set of operations (my vocabulary) are composed in a dataset of algorithms (corpus). 
The algorithms are a sequence of higher level operations which have varying low-level implementations. I am able to map raw code to my vocabulary, but not all of it.
e.g. I observe a lossy description of an algorithm that does something:
X: missing data
Algo 1: BIND3 EXTEND2 X X ROTATE360 X PUSH
Algo 2: X X EXTEND2 ROTATE360

The underlying rotate operation could have very different raw code, but effectively the same function and so it gets mapped to the same operation. 
I want to infer what the next operation will be given a sequence of (potentially missing) operations (regions of code I could not map). 
i.e. I want a probability distribution over my operations vocabulary.
Any ideas on the best approach here? The standard thing seems to throw out  missing data, but I can still learn in these scenarios. Also, the gaps in the code are non-homogenous--some could do many things, The alternative is to contract the sequences and lose the meaning of the gaps, or to learn an imputation.
","['deep-learning', 'natural-language-processing', 'sequence-modeling']",
Are there some formulae in facial recognition that are indicators of close kinship?,"
I noticed what I considered a close resemblance of a woman B to another woman A which led to a close relative A labelled C, who B bore an even stronger resemblance to than she did to A.
However I feel that if I had compared A to C directly I wouldn't have detected the blood relationship so strongly.
I am just wondering whether there is some mathematical underpinning to my perception and strong intuition that B bore a strong resemblance to A.
","['machine-learning', 'facial-recognition']",
Why can't neural networks learn functions outside of the specified domains?,"
I understand that neural nets are fundamentally interpolative tools. Meaning, given a training dataset, a well trained neural net can approximate values within the domain of the training dataset. However, we are unsure about their behavior once we test against values outside that domain.
Speaking in the context of Imagenet, a NN trained on one of the classes in Imagenet will probably be able to predict an image of the same class outside Imagenet because Imagnet itself covers a huge domain for each class that whatever image we come across in the wild, its features will be accounted for by Imagnet. 
Now, this intuition breaks down for me when I talk about simple functions with simple inputs. For example, consider $sin(x)$. Our goal is to train a neural net to predict the function given $x$ with a training domain $[-1, 1]$. Theoretically, the neural net should not be able to predict the values well outside that domain, right? This seems counterintuitive to me because the function behaves in a very simple and periodic way that I find it hard to believe that a neural net cannot figure out the proper transformation of that function even outside the training domain.
In short, are neural nets inherently unable to find a generalizable transformation outside the training domain no matter how simple is the function we are trying to approximate? Is this a property of the Deep Learning framework?
Are there problems where researchers were able to learn a robust generalizable transformation using neural nets outside the training domain? What are the possible conditions so that such results can happen?
","['neural-networks', 'deep-learning', 'function-approximation']",
Is Hopfield network more efficient than a naive implementation of Hamming distance comparator?,"
Is Hopfield network more efficient than a naive implementation of Hamming distance that compare an input pattern and return the nearest pattern ?
",['neural-networks'],
Is there a rigorous proof for finding Hopfield minima?,"
I am looking for a rigorous mathematical proof for finding the several local minima of the Hopfield networks. I am searching for something rigorous, a demonstration, not just letting the network keep updating its neurons and wait for noticing a stable state of the network.
I have looked virtually everywhere, but I found nothing.
Is there a rigorous proof for Hopfield minima? Could you give me ideas or references?
","['reference-request', 'recurrent-neural-networks', 'proofs', 'convergence', 'hopfield-network']","
See the paper On the Convergence Properties of the Hopfield Model (1990), by Jehoshua Bruck.
In the first section of the paper, J. Bruck describes the Hopfield network (popularized by J. J. Hopfield in 1982 in his paper Neural networks and physical systems with emergent collective computational abilities, hence the name of the network), then he describes the notation that is used throughout the paper and he gives some examples where a simple Hopfield network (with two nodes) converges to stable states (of the network) and cycles (of which the author also gives a definition).
The usual proofs of the convergence properties of Hopfield networks involve the concept of an energy function, but, in this paper, J. Bruck uses an approach (based on an equivalent formulation of the Hopfield network as an undirected graph) that does not involve an energy function, and he unifies three apparently different convergence properties related to Hopfield networks (described in part $C$ of the section Introduction). More specifically, finding the global maximum of the energy function associated with the Hopfield network operating in a serial mode (which is defined in part $A$ of the Introduction section of the paper) is equivalent to find a minimum cut in the undirected graph associated with this Hopfield network. 
Furthermore, note that the proofs of convergence of the Hopfield networks actually depend on the structure of the network (more specifically, its weight matrix). For example, if the weight matrix $W \in \mathbb{R}^{n \times n}$ (where $n$ is the number of nodes in the network) associated with the Hopfield network is a symmetric matrix with the elements of the diagonal being non-negative, then the network will always converge to a stable state.
See also the chapter 13 The Hopfield Model of the book Neural Networks - A Systematic Introduction (1996) by Raul Rojas.
"
Understanding CNN+LSTM concept with attention and need help,"
I have a question about the context of CNN and LSTM. I have trained a CNN network for image classification. However, I would like to combine it with LSTM for visualizing the attention weights. So, I extracted the features from the CNN to put it into LSTM. However, I am stuck at the concept of combinating the CNN with LSTM.
 Do I need to train the whole network again? Or just training the LSTM part is fine?
 Can I just train the LSTM on image sequences based on classes (for e.g. 1 class has around 300 images) and do predictions later on extracted video frames?
- In what way can I implement the attention mechanism with Keras? 
I hope you can help me while I struggle with the context of understanding the combination of this.
~ EDITED ~
I have trained a resnet50 to classify images. Although, I removed the last dense layer, to extract features from the trained CNN network. Those extracted features will be used as input in the newly created LSTM with attention mechanism to find out where the focus lies. The predictions will be on videos (extracted frames).
Image -> extract features (CNN) -> LSTM + Attention (to check where the focus lies during the prediction) -> classify image (output class from N labels) 
","['neural-networks', 'convolutional-neural-networks', 'python', 'keras', 'attention']",
Should I model a problem with quantised output as classification or regression?,"
Say I have some data I am trying to learn, and I'm aware that the output is quantised in some way, e.g. I can get only get discrete values (0.1, 0.2, 0.3...0.9) in a finite range.
Would you treat that as regression or classification? In this case the numbers do have a relation to each other e.g. 0.3 is close to 0.4 in meaning.
I could treat it as classification with a softmax final layer with N outputs, or could treat it as regression with a linear layer with single output and then somehow quantise the result post-prediction. But my gut feeling is that the fact there is a finite number of answers that that should somehow be used in my model?
","['classification', 'models', 'regression']","
So this is considered Ordinal Regression. There are many ways to model this type of data, generally in some form of regression setting. I do not recommend the softmax route because as you mentioned, there exists prebuilt correlation to the outputs.  
Some common ways to approach this
(Note that im assuming your looking for methodologies that can be optimized through gradient techniques because of the way you formulated your question)  

Treat it as a normal regressor where you clip the output to your range, and then define thresholds arbitrarily, such as $.36 \rightarrow .4$, $.34 \rightarrow .3$, etc..
Use a bounded activation function then scale (if your outputs are [0,1,.1], you could use sigmoid and just scale by a factor of 1, but if [0,10,1], you could use sigmoid and scale by 10. Once again youll need to create arbitrary threshold between each 2 points for inference (this can also be incorporated into your loss)  
Using the above two methods, but learn the threshold between each 2 ordered points. Have an output layer that learns the ideal threshold for inference  

And if you didnt want to use neural netowrks, each of these approaches have a bayesian analog that work as well! 
"
Estimating camera's offset to its true position,"
I have the following problem:
I get a 360 RGB image in a room. 

I've the 3D model of this room, hence, I can generate a 3D nominal mask of the room (1-wall, 2-ceiling, 3-floor, 4-door, etc..) in a specific location (x0,y0,z0,roll0,pitch0,yaw0)
I can also generate the depth map of this location.

My model has to predict whether the generated mask+depth match with the RGB frame, and if not what are the dx, dy, d_yaw of the offset.
I've implemented pix2pix discriminator that receives a concatenated tensor of the [RGB, one-hot mask, depth] and yields the (dx, dy, d_yaw).
obviously, if there is a perfect match, dx=dy=dyaw=0. 
Unfortunately my model isn't converging. I've tried everything, and it's a reasonable ""request"" from this model, since a human can roughly guess this offset if he looks on the images.
what would you suggest? 
",['convolutional-neural-networks'],
Why is On-Policy MC/TD Algorithm guaranteed to converge to optimal policy?,"
Let's say we have a task where the cost depends entirely on the path length to a terminal state, so the goal of an agent would be to take actions to reach terminal state as quickly as possible.
Now let us say, we know the optimal path length is of length $10$, and there are $n$ such paths possible. Each state has 5 possible actions. Let's say the scheme we are using to find optimal policy is On-Policy MC/TD(n) along with GLIE Policy improvement (Generalised Policy Iteration).
In the first Policy Iteration Step, each actions are equally likely, there for the probability of sampling this optimal path (or the agent discovering this path) is $n* \frac {1}{5^{10}} \approx n* \frac {1}{2^{20}}$. So, according to probability theory we need to sample around $2^{20}/n$ steps to atleast discover one of the best paths (worst case scenario).
Since, it is not possible to go through such huge number of samplings, let's say we do not sample the path, thus in the next Policy Iteration step (after GLIE Policy Improvement) some other sub-optimal path will have a higher probability of being sampled than the optimal path, hence the probability falls even lower. So, like this there is a considerably high probability that we may not find the best path at all, yet theory says we will find $\pi^*$ which indicates the best path.
So what is wrong in my reasoning here?
",['reinforcement-learning'],"
Your reasoning is fine. GLIE - Greedy in the Limit with Infinite Exploration assumes that our agent does not act greedily all the time. As the number of samples approaches infinity all state-action pairs will be explored -> hence the policy will converge on a greedy policy. The emphasis is on ""number of samples approaches infinity"". Also for GLIE Monte-Carlo initial values of Q does not matter, since they are replaced after the first update. 
"
Why isn't the evolutionary Turing machine mainstream?,"
Given that recurrent neural networks are equivalent to a Turing machine, then why isn't the evolutionary Turing machine, e.g. described in the paper Evolution of evolution: Self-constructing Evolutionary Turing Machine case study (2007), mainstream?
","['neural-networks', 'papers', 'evolutionary-algorithms']",
Machine learning methods to identify the recipient of a document?,"
I need some advice on what AI methods would be suited to the identification of a recipient of a document, where the format of the documents may vary. 
",['natural-language-processing'],
How is a neural network where the majority of inputs are 0 trained?,"
Consider AlexNet, which has 1000 output nodes, each of which classifies an image:

The problem I have been having with training a neural network of similar proportions, is that it does what any reasonable network would do: it finds the easiest way to reduce the error which happens to be setting all nodes to 0, as in the vast majority of the time, that's what they'll be. I don't understand how a network where 999 times out of 1000, the node's output is 0, could possibly learn to make that node 1.
But obviously, it's possible, as AlexNet did very well in the 2012 ImageNet challenge. So I wanted to know, how would one train a neural network (specifically a CNN) when for the majority of the inputs the desired value for an output node is 0?
","['convolutional-neural-networks', 'training', 'backpropagation', 'alexnet']",
Current state of MoE models,"
I've been reading about Mixture of Expert models, and I've noticed that there is very little new work being produced in this subfield. Has there been a better method discovered? Why aren't more people doing stuff in this area?
",['deep-learning'],
"How can an ANN efficiently predict multiple numbers with fixed sum (in other words, proportions)?","
I need a neural network (or any other solution) to predict 3 values which sum equals a fixed number (100). This will help me calculate proportions. Which is the most efficient way to do this?
The learn data only contains extreme situations where each row contains one and only one output value set to 100.
The data to predict is expected to contain more nuances in the output values.
All my attempts lead to very low accuracy as the predicted output sum is almost never a 100. Even when I try to normalize the predicted output, the predictions show very poor accuracy.
Should I try to organize the data with 2 angles instead and deduct the 3rd angle as the remainder in a circle? How to normalize those 2 angles and how to make sure their sum will not exceed the maximum value making the 3rd angle negative?
Illustration of learn data extract (4 input columns and 3 output columns).
0    1    2    3    100  0    0
4    5    6    7    0    100  0
8    9    0    1    0    0    100

Illustration of desired output predictions where each line sums as 100:
7    83   10
39   12   49
68   24   8
28   72   0
86   6    8
32   49   19
0    0    100

","['neural-networks', 'machine-learning', 'multilayer-perceptrons']",
How can I use gradient boosting with multiple features?,"
I'm trying to use gradient boosting and I'm using sklearn's GradientBoostingClassifier class.
My problem is that I'm having a data frame with 5 columns and I want to use these columns as features. I want to use them continuously. I mean I want each tree based classifier uses the residue of the previous tree which is based on the previous feature. As I know, by default, this classifier uses a feature and passes the residue of the previous tree to the next tree, which both are based on a single feature. How can I do this?
Should I do this on my own or there is a library which does this?
","['machine-learning', 'features', 'scikit-learn', 'gradient-boosting']",
Reward problem in A2C with multiple simultaneous discrete actions,"
I've built an A2C model whose actor's network has two different kinds of discrete actions, so the critic would take state and action (note that critic takes 2 actions because in each timestep we will do two kinds of actions) to predict the advantage for each of these two different kinds of actions. However, my problem is when the critic network is about to train itself with discounted rewards. It only has the reward of each timestep, so cannot determine which of the two kinds of our actions contributed more to this reward, so both of its outputs (the advantage for the action of kind 1 and the advantage for the action of kind 2) will be changed in the same direction, so output of critic would be biased with its initialization values. How can I solve this problem, that is, to distinguish between the contribution amount of each of these two kinds of actions to the outcome?
An example of my problem:
Consider we have 10 cubes 3 boxes. In each time step, we have to choose between 10 cubes and choose between 3 boxes to place the selected cube in the selected box. So we have 2 kinds of actions here: one to pick a cube and the second to put it in one the boxes. Each box only has the capacity of only 4 cubes, and one of the cubes is so big that don't fit in any of the boxes. The reward of each time step will be the negative number of cubes that are not placed in a box, so because of the bigger cube, the agent won't get reward 0 ever. Consider a scenario that a box already contains 4 cubes and we choose that box to place the chosen cube (one of the small cubes) in it, we can't and the time will proceed. Another scenario is when we choose the bigger cube so no matter which box we choose, we cannot place it and the time will proceed. How the agent can distinguish which of these two kinds of actions contributed more to the reward?
","['machine-learning', 'deep-learning', 'reinforcement-learning', 'actor-critic-methods', 'rewards']",
How to Mask an image using Numpy/OpenCV? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I am detecting wheels with a deep learning algorithm. The algorithm gives me the coordinates of those rectangles. I want to keep data that is in the rectangles of the image. I created rectangles as a mask of the area I want to keep.
Here is the output of my system
I read my image
im = cv2.imread(filename)

I created the rectangles with:
height,width,depth = im.shape
cv2.rectangle(img,(384,0),(510,128),(0,255,0),3)
cv2.rectangle(rectangle,(width/2,height/2),200,1,thickness=-1)

How can I mask out the data outside of the rectangle from the original image? and keep those rectangles?
Edited: I wrote this code and it only gives me one wheel. How can I have multiple masks and get all the wheels?
  mask = np.zeros(shape=frame.shape, dtype=""uint8"")

# Draw a bounding box.
# Draw a white, filled rectangle on the mask image
cv.rectangle(img=mask,
             pt1=(left, top), pt2=(right, bottom),
             color=(255, 255, 255),
             thickness=-1)


# Apply the mask and display the result
maskedImg = cv.bitwise_and(src1=frame, src2=mask)

cv.namedWindow(winname=""masked image"", flags=cv.WINDOW_NORMAL)
cv.imshow(winname=""masked image"", mat=maskedImg)

","['deep-learning', 'python', 'object-detection', 'image-processing']","
You can use cv2.bitwise_and and pass rectangle as a mask.
im = cv2.imread(filename)
height,width,depth = im.shape
cv2.rectangle(img,(384,0),(510,128),(0,255,0),3)
cv2.rectangle(rectangle,(width/2,height/2),200,1,thickness=-1)

masked_data = cv2.bitwise_and(im, im, mask=rectangle)

cv2.imshow(""masked_data"", masked_data)
cv2.waitKey(0)

"
Is there a rule-of-thumb to determine which behaviours must be learned in a lifetime and which innate?,"
I was training an AI to learn things during its lifetime such as find food and navigate a maze. Behaviors that might change during its lifetime.
But I hit upon a snag. Some behaviors, like avoiding poisonous snakes, cannot be learned in a lifetime since once bitten by a snake the being is dead.
That got me thinking about how to separate out behaviors that must be given to the AI at birth (either by programming or using some evolutionary algorithm) and which behaviors to let the AI learn in its lifetime.
Also, there is the matter of when a learned behavior should be able to overrule an innate behavior (if at all).
Is there much research into this? I'm looking for some method to determine a set of innate behaviors which can't be learned.
","['neural-networks', 'evolutionary-algorithms']",
Why are activation functions independent layers in CNNs rather than part of convolutional layers?,"
I have been reading up on CNNs. One of the different confusing things has been that people always talk of normalization layers. A common normalization layer is a ReLU layer. But I never encountered an explanation of why all of a sudden, activation functions become their own layers in CNNs, while they are only parts of a fully connected layer in MLPs.
What is the reason for having dedicated activation layers in CNNs rather than applying the activation to the output volume of a convolutional layer as part of the convolutional layer, as it is the case for dense layers in MLPs?
I guess, in the end, there is no functional difference. We could just as well have separate activation layers in MLPs rather than activation functions in their fully connected layers. But this difference in the convention is irritating still. Well, assuming it only is an artifact of the convention.
","['neural-networks', 'convolutional-neural-networks', 'definitions', 'activation-functions', 'multilayer-perceptrons']",
"Unique game problem (ML, DP, PP etc)","
Looking for a solution to my below game problem. I believe it to require some sort of reinforcement learning, dynamic programming, or probabilistic programming solution, but am unsure... This is my original problem, and is part of an initiative to create ""unique and challenging problem that you're able to conceptualize and then solve. 3 Judging criteria: uniqueness, complexity, and solution (no particular weighting and scoring may favor uniqueness/challenge over solution""
Inspirations: Conway's Game of Life, DeepMind's Starcraft Challenge, deep Q-learning, probabilistic programming
BEAR SURVIVAL
A bear is preparing for hibernation. A bear must reach life-strength 1000 in order to rest & survive the winter. A bear starts off at a health of 500. A bear explores an environment of magic berries. A bear makes a move  (chosen randomly with no optional direction) and comes across a berry each time. There are 100 different types of berries that all appear across the wilderness equally and infinitely.
A magic berry always consumes 20 life from the bear upon arrival (this not an energy cost for moving and we should not think of it as such). A bear may then choose to give more, all, or none of its remaining life to the berry. If eaten, the berry may provide back to the bear 2x the amount of life given. Berries, however, are not the same and a bear knows this. A bear knows that any berry has some percentage of being poisonous.  Of the 100 different types of berries, each may be 0%-100% poisonous. A berry that is 0% poisonous is the perfect berry and a bear knows that it should commit all of its remaining life to receive max health gain. If a bear wants to eat the berry, it must commit at least 20 more health. Again, a bear does not have to eat the berry, but if it chooses not to, it walks away and does not get the original 20 back.
Example: On a bear's first move (at 500 life), it comes across a magic berry and the berry automatically takes 20 life.  The bear notices that the berry is 0% poisonous, the perfect berry, and gives its remaining 480 health, eats the berry, and then receives 1000 health gain. The bear has reached it's goal, hibernates, and wins the game. However, if that first berry was 100% poisonous, the anti-berry, and the bear committed all of its remaining life it would've received back 0 health gain, died, and lost the game. A bear knows to never eat the anti-berry. It knows it can come across any poisonous value from 0-100 (3,25,52,99, etc).
A bear must be picky & careful, but also bold & smart about how much life it wants to commit per berry, per move. A bear knows that if it never eats, it will eventually die as it loses 20 health per berry, per move.
While it's important for an individual bear to survive, it is even more important for the bear population to not go extinct. A population is going extinct if they lose over half of the population that year. Bears in a population, and their consumption of berries, are completely independent of each other. 
Questions:

May we find a bear's optimal strategy for committing health & eating
berries to reach 1000 health gain? 
Is the bear population eventually doomed to a unfavorable environment?

Bonus Complexity: 
Winter is coming, and conditions grow progressively harsher over time. A bear knows that every 10 moves, each berry will consume 20 * (fib(i)/ environmentFactor). fib(i) stands for fibonacci-sequence at index i, starting at 1. For all indexes where the progression is less than 20, a berry's initial health consumption remains at 20. environmentFactor is a single environment's progressive-harshness variable (how harsh winter becomes over time). The bear population is currently in an environment with environmentFactor of 4. Spelled out:
Moves 01-10: Berries consume 20 --  20*(1/4)
Moves 10-20: Berries consume 20 --  20*(1/4)
Moves 20-30: Berries consume 20 --  20*(2/4)
Moves 30-40: Berries consume 20 --  20*(3/4)
Moves 40-50: Berries consume 25 --  20*(5/4)
Moves 50-60: Berries consume 40 --  20*(8/4)
Moves 60-70: Berries consume 65 --  20*(13/4)
Moves 70-80: Berries consume 105 --  20*(21/4)
Moves 80-90: Berries consume 170 --  20*(34/4)
Moves 90-100: Berries consume 275 --  20*(55/4)
Moves 100-110: Berries consume 445 --  20*(89/4)
... and so on ...

Same questions as above, with a third: if this environment is proven unfavorable, and extinction unavoidable, what maximum environment/environmentFactor must the bear population move to in order to avoid extinction? (this may or may not exist if a berry's requirement of 20 initial life is always unfavorable without any progression).
Further Details:

QUESTION:
  Can you give an example of what happens when a bear eats a semi-poisonous berry (e.g. 20%)?
  - Also, is the bear always immediately aware of the poison value of berries?
  - In this, it also seems like you're using health, life, strength, life-strength, health-gain, etc interchangeably. Are they all the same
  thing?
ANSWER:  

if the bear eats the poison berry of 20%, then it becomes a probability problem of whether or not the berry provides back life or
  keeps the health amount committed by the bear. Example: the bear is at  400, the next move & berry take the initial  20 (bear is at 380 now), the bear
  decides to commit an additional 80 (now at 300) and eat the berry.
  8/10 times the berry will return to the bear 200 (2x 100 committed --
  bear ends turn at 500), 2/10 times the berry
  returns nothing and the bear must move on with 300 life.
the bear is always immediately aware of the poison value of a berry.
life/health/strength are all the same thing.


","['machine-learning', 'reinforcement-learning', 'game-ai', 'q-learning', 'probability']",
What is ratio of the objective function in the case of continuous action spaces?,"
I'm trying to implement the proximal policy optimization (PPO) algorithm. I'm confused on how to make it work with continuous action space.
For discrete action space, the output of the network is the probability for every available action, then I choose the next action based on this probability. The ratio, in the objective function, is the ratio between the action probability of the new policy and the action probability between the old policy.
For continuous action space, from what I understand, the output of the network should be the action itself. How should this ratio (or the objective function itself) look like in that case?
","['reinforcement-learning', 'ai-design', 'proximal-policy-optimization']",
How can I learn tensors for deep learning?,"
I've seen in most deep learning papers use tensors. I understood what tensors are, but I want to dive into them, because I think that might be beneficial for further studies in Artificial Intelligence. Do you have any suggestion (e.g. books or papers) about that? 
","['deep-learning', 'math', 'reference-request']","
In deep learning (and, in general, machine learning), tensors are multi-dimensional arrays. You can perform some operations on these multi-dimensional arrays (depending also on the specific implementations and libraries). These operations are similar to the operations you can apply to vectors or matrices, which are just specific examples of multi-dimensional arrays. Examples of these operations are

indexing and slicing (if you are familiar with Python, these terms should not scare you)
algebraic operations (such as multiplication of a tensor with another tensor, which includes numbers, vectors or matrices), which typically support broadcasting
reshaping (i.e. change the shape of the tensor)
conversion to or from another format (e.g. a string)

TensorFlow provides an article that discusses these tensors, so I suggest that you read it.
In mathematics, tensors are not just multi-dimensional arrays. They are multi-dimensional arrays that need to satisfy certain properties (in the same way that matrices need to satisfy certain properties to be called matrices) and are equipped with certain operations.
The paper A Survey on Tensor Techniques and Applications in Machine Learning (2019) Yuwang Ji et al., published by IEEE, provides a comprehensive overview of tensors in mathematics. It is full of diagrams that illustrate the concepts and the explanations are concise. Some of the explanations in this paper may not be very useful to develop deep learning applications, but some of the illustrations and explanations (especially, in the first pages, which are the only ones that I read) will give you some intuition behind the tensors or multi-dimensional arrays used in deep learning).
So, tensors in deep learning (DL) may not be exactly equivalent to the tensor objects in mathematics, because they may not satisfy all the required properties or some of the operations in the specific libraries may not be implemented, but it is fine to call them tensors because a tensor in mathematics is also a multi-dimensional array, to which you can apply operations (some of them are implemented in the DL libraries).
"
"Will the target network, which is less trained than the normal network, output inferior estimates?","
I'm having some trouble understanding some parts of the usage of target networks. 
I get that having the same network predict the state/action/advantage values for both the current networks can lead to instability. 
Based on my understanding, the intuition behind 1-step TD error that going a step into the future will give you a better estimate, that can then be used to update your original state/action/advantage value.
However, if you use a target network, which is less trained than the normal net  especially at early stages of the training  wouldn't the state/action/advantage value be updating towards an inferior estimate?
I've tried implementing DQNs and DDPGs on Cartpole, and I've found that the algorithms fail to converge when target networks are used, but work fine when those target networks are removed. 
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']",
Are feature maps merged or are they passed on as they are?,"
I am unsure about the following parts of the architecture and mechanics of convolution layers in CNNs. Possibly, this is implementation-dependent though.
First question:
Say I have 2 convolution layers with 10 filters each and the dimension of my input tensors is $n \times m \times 1$  (so, grayscale images for example). Passing this input to the first convolution layer results in 10 feature maps (10 matrices of $n \times m$, if we use padding), each produced by a different filter.
Now, what does actually happen when this is passed to the second convolution layer? Are all 10 feature maps passed as one big $m \times n \times 10$ tensor or are the overlapping cells of the 10 feature maps averaged and a $m \times n \times 1$ tensor is passed to the next convolution layer? The former would result in an explosion of feature maps with increasing number of convolution layers and the spacial complexity would be in $\mathcal{O}\left((nm)^k\right)$, where $k$ is the number of chained convolution layers. Averaging the feature maps before passing them to the next layer would keep the complexity linear. So, which is it? Or are both possibilities commonly used?
Second question (with two sub questions):
a) This is a similar question. If I have an input volume of $n \times m \times 3$ (e.g. RGB images) and I have again 2 convolution layers with 10 filters, does each convolution layer have in actuality 30 filters? So 10 sets of 3 filters, one for each channel? Or do I have in fact only 10 filters and the filters are applied to all 3 channels?
b) This is the same question as question (1) but for channels: Once I have convolved a filter (consisting of three channel filters? (a)) over the input tensor I end up with 3 feature maps. One for each channel. What do I do with these? Do I average them component-wise with each other? Or do I keep them separate until I have convolved all 10 filters across the input and THEN average the 10 feature maps of each channel? Or do I average all 30 feature maps of all three channels? Or do I just pass on 30 feature maps to the next convoloution layers which in turn knows which of these feature maps belong to which channel?
Quite a few possibilities... None of the sources I consulted makes this explicit. Maybe because it depends on the individual implementation.
Anyway, would be great if somebody could clear this confusion up a little!
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'definitions', 'implementation']","
Answers:

Generally its the former. The next layer would learn at each filter how to merge the channels of the previous layer, that is why in a 2D convolution the kernel is a 3-dimensional tensor. But the number of parameters is $nmc_ic_{i+1}$ at the $i^{th}$ layer (this is ignoring bias). lets assume all channels are $O(c)$ then the spatial complexity becomes $O(knmc^2)$ where $k$ is the number of layers.   
a) The first convolutional filter would have kernel size of (w,h,3,10) where w and h are the kernel sizes of the 2d convolution (often in practice is 3). So there are 10 filters of size (w,h,3), but the # of parameters is w*h*30 (once again for ease, ignoring bias). The second layer though, since it is working on a layer with 10 channels, will have kernel (w,h,10,10).
b) I think you need to go back and look at what a convolution does (in your setting specifically a 2D convolution). Each filter works on every channel of the previous layer. Each channel of the last convolutional layer refers to a single filter that convolved over the entire previous layer to that. 

"
Does this hyperparameter optimisation approach yield the optimal hyperparameters?,"
Say I have a ML model which is not very costly to train. It has around say 5 hyperparameters. 
One way to select best hyperparameters would be to keep all the other hyperparamaters fixed and train the model by changing only one hyperparameter within a certain range. For the sake of mathematical convenience, we assume for the hyperparameter $h^1$, keeping all other hyperparameters fixed to their initial values, the model performs best when  $h^1_{low} < h^1 < h^1_{high}$ (which we found out by running the model on a huge range of $h^1$). Now we, fix $h^1$ to one of the best values and tune $h^2$ the same way, where $h^1$ is chosen and the rest of the hyperparameters are again fixed on their initial values.
My question is: Does this method find the best hyperparameter choices for the model? I know if the hyperparameters are independent, then this definitely does find the best solution, but in a general case, what is the general theory around this? (NOTE: I am not asking about the problem of choosing hyperparamaters, but I am asking about the aforementioned approach of choosing hyperparameters)
","['machine-learning', 'hyper-parameters', 'hyperparameter-optimization']","
After you've computed $h^{1}_{optimal}$ the only thing you can be sure is that this is the best (assuming constrained case) value of $h^1$ (with respect to some model performance metric) given your initial values for $h^2, ..., h^n$. If you change a bit any of $h^2, ..., h^n$ you're no longer certain that the value $h^1$ you found is the optimal one. So yes, the key here is the assumption about the independence.
"
How can I use Q-learning for inventory decision making?,"
I am trying to model operational decisions in inventory control. The control policy is base stock with a fixed stock level of $S$. That is replenishment orders are placed for every demand arrival to take the stock level to $S$. The replenishments arrive at constant lead time $L$. There is an upper limit $D$ on the allowed stock out time and it is measured every $T$ periods, otherwise, a cost is incurred $C_p$. This system functions in a similar manner to the M/G/S queue. The stock out time can be thought as the customer waiting time due to all server busy.  So every $R$ period ($R$ is less than $T$) the inventory level and pipeline of outstanding orders are monitored and a decision about whether to expedite outstanding order (a cost involved $C_e$)  or not is taken in order to control the waiting/stock-out time and to minimize the total costs. 
I feel it is a time and state-dependent problem and would like to use $Q$-learning to solve this MDP problem. The time period $T$ is typically a quarter i.e. 3 months and I plan to simulate demands as poisson arrivals. My apprehension is whether simulating arrivals would help to evaluate the Q-values because the simulation is for such a short period. Am I not overestimating the Q value in this way? I request some help on how should I proceed with implementation. 
","['reinforcement-learning', 'ai-design', 'q-learning']",
Will the RL agent implemented as a neural network fine-tune itself?,"
Normally, when you develop a neural network, train it for object recognition (on normal objects like bike, car, plane, dog, cloud, etc.), and it turns out to perform very well, you would like to fine-tune it for e.g. recognizing dog breeds, and this is called fine-tuning. 
On the other hand, in reinforcement learning, let's consider a game with rewards on checkpoints $1, 2, 3, \dots n$. When you have a bot that plays and learns using some (e.g. value) neural net, it develops some style of solving the problem to reach some checkpoint $k$, and, after that, when it will reach the $k+1$ checkpoint, it probably will have to revalue the whole strategy.
In this situation, will the bot fine-tune itself? Does it makes sense to keep a replay buffer as is and to ""reset"" the neural net to train it from scratch, or it's better to stay with fine-tune approach? 
If possible, topic-related papers would be very welcome!
","['neural-networks', 'reinforcement-learning', 'game-ai', 'rewards']",
"If the goal of training of a GAN is to have $P_g=P_{data}$, shouldn't this produce the exact same images?","
Referring to the blog, Image Completion with Deep Learning in TensorFlow, it clearly says that we would want a generator $g$ whose modeled distribution fits our dataset $data$, in other words, $P_{data}=P_g$. 
But, as described earlier in the blog, the space $P_{data}$ is in is a higher-dimensional space, where a dimension represents a particular pixel in an image, making it a $64*64*3$ dimensional space (in this case). I have a few questions regarding this  

Since each pixel here will have an intensity value, will the pdf try to encapsulate a unique pdf for each pixel?
If we sample the most likely pixel value for each pixel, considering the distributions need not be the same for each pixel, is it not quite likely that the most probabilistic generated image is just noise apart from things like a common background or so?
If $P_g$ is trying to replicate $P_{data}$ only, does that mean a GAN only tries to learn lower level features that are common in the training set? Are GANs clueless about what its doing?

","['deep-learning', 'convolutional-neural-networks', 'generative-adversarial-networks', 'generative-model', 'image-generation']",
How do layers in an artificial neural network transform inputs to outputs?,"
To me, most ANN/RNN related articles don't tell me actually how the network is implemented. I know that in the ANN you'll have multiple neurons, activation function, weights, etc. But, how do you, actually, in each neuron, convert the input to the output?
Putting activation function aside, is the neuron simply doing $\text{input}*a+b=\text{output}$, and try to find the correct $a$ and $b$? If it's true, then how about where you have two neurons and their output ($c$ and $d$) is pointing to one neuron? Do you first multiply $c$ and $d$ then feed it in as input?
","['neural-networks', 'recurrent-neural-networks', 'artificial-neuron', 'neurons']","
The basic calculation for a single neuron is of the form
$$\sigma\left(\sum_{i} x_i  w_i \right),$$
where $x_i$ is the input to the neuron $w_i$ are the neuron-specific weights for every single input and $\sigma$ is the pre-specified activation function. In your terms, and disregarding the activation function, the calculation would turn out to be
$$c\,a_c + d\,a_d + b$$
Note, that the bias term $b$ is just a weight that gets multiplied by the input $1$, thus it appears to have no input.
If you want to develop a further understanding for this, you should try to get familiar with matrix and vector notations and the basic linear algebra that underlies the feed-forward neural networks. If you do, an entire layer of neurons on a whole batch of data will suddenly simply look like this:
$$\sigma(WX)$$
and a FFNN with say 3 layers will look like this:
$$\sigma_{3}(W_3\sigma_2(W_2\sigma_1(W_1X)))$$
"
Probabilistic action selection in pursuit algorithm,"
In the Pursuit algorithm (to balance exploration and exploitation), the greedy action has a probability say $p_1$ (updated every episode) of being selected, while the rest have a probability $p_2$ (updated every episode) of being selected. 
Could you please show me an example code (Python) on how to enforce such conditional probabilistic picking?
","['reinforcement-learning', 'q-learning']",
Can a data compression function be used to make predictions?,"
I've heard that prediction is equivalent to data compression. 
Is there a way to take a compression function and use it to create an AI that predicts?
","['ai-design', 'agi', 'prediction', 'data-compression']","
The way some (not all) compression algorithms work is that they encode frequent events in a short code, and rarer events with a longer code. Overall you save more space by encoding the common elements than you need to expend coding the rare ones. One example of this is a Huffman code, which uses a variable length encoding based on the frequency of the items.
You can use a compression algorithm for prediction if if encodes more than one event at a time. For example, word pairs rather than individual words. Each word pair will have a code, and the common word pairs (eg of the) will have shorter codes that the ones which are less common (eg of three). For prediction, select all the word pairs that start with your known sequence (eg of). Now select from that list the pair with the shortest code (which is more common), so in this example of would more likely be followed by the rather than three. After than, repeat the process with the next word, so look for pairs that begin with the.
All you need is the compression 'code book' which is produced during the compression process -- it's essentially a model of the data you compressed. This also works for longer sequences than pairs, of course.
If you want to know more about the topic, I can recommend Managing Gigabytes by Witten, Moffat, and Bell. Great book on compression techniques.
"
Is it possible to have a dynamic $Q$-function?,"
I am trying to use Q-learning for energy optimization. I only wish to have states that will be visited by the learning agent, and, for each state, I have a function that generates possible actions, so that I would have a Q-table in form of a nested dictionary, with states (added as they occur) as keys whose values are also dictionaries of possible actions as keys and Q-values as values. Is this possible? How would it affect learning? What other methods can I use?
If it is possible and okay, and I want to update the Q-value, but the next state is one that was never there before and has to be added to my nested dictionary with all possible actions having initial Q-values of zero, how do I update the Q-value, now that all of the actions in this next state have Q-values of zero?
","['ai-design', 'q-learning', 'optimization']",
How to rescale data to its original range after MinMaxScaler? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I'm using sklearn's MinMaxScaler in order to scale my data down. However, it would be nice to be able to rescale it back to its original range. Is there any way I can do this? 
","['machine-learning', 'python', 'data-science', 'data-preprocessing']",
"Understanding the reconstruction loss in the paper ""Anomaly Detection using Deep Learning based Image Completion""","
I would like to implement the approach represented in this paper. Here they used following reconstruction loss:
$$
L(X)= \frac{\lambda \cdot || M \odot (X - F(\overline{M} \odot X)) ||_{1} + (1 - \lambda) \cdot || \overline{M} \odot (X - F(\overline{M} \odot X)) ||_{1}}{N}
$$
Unfortunately, the author does not explain the function $F$.
Does someone know a similar function or could understand the function's purpose from the context?
","['convolutional-neural-networks', 'papers', 'anomaly-detection']",
Does GraphSage use hard attention?,"
I was reading the recent paper Graph Representation Learning via Hard and Channel-Wise Attention Networks, where the authors claim that there is no hard attention operator for graph data. 
From my understanding, the difference between hard and soft attention is that for soft attention you're computing the attention scores between the nodes and all their neighbors while for hard attention you have a sampling function that selects only the most important neighbors. If that is the case, then GraphSage is an example of hard attention, because they apply the attention only on a subset of each node's neighbors. 
Is my understanding of hard and soft attention wrong, or the claim that the authors made does not hold?
","['comparison', 'geometric-deep-learning', 'graphs', 'attention']","
GraphSage does not have attention at all. Yes, it randomly samples (not  most important as you claim) a subset of neighbors, but it does not compute attention score for each neighbor.
"
How does RL based neural architecture search work?,"
I have read through many of the papers and articles linked in this thread but I haven't been able to find an answer to my question.
I have built some small RL networks and I understand how REINFORCE works. I don't quite understand how they are applied to NAS though. Usually RL agents map a state to an action and get a reward so they can improve their decision making (which action to choose). I understand that the reward comes from the accuracy of the child network and the action is a series of digits encoding the network architecture. 
What is passed as the state to the RL agent? This doesn't seem to be mentioned in the papers and articles I read. Is it the previous network? Example input data?
","['reinforcement-learning', 'neural-architecture-search']",
How to stay a up-to-date researcher in ML/RL community?,"
As a student who wants to work on machine learning, I would like to know how it is possible to start my studies and how to follow it to stay up-to-date. For example, I am willing to work on RL and MAB problems, but there are huge literatures on these topics. Moreover, these topics are studied by researchers from different communities such as AI and ML, Operations Research, Control Engineering, Statistics, etc. And, I think that several papers are published on these topics every week which make it so difficult to follow them. 
I would be thankful if someone can suggest a road-map to start studying these topics, follow them and how I should select and study new published papers. Finally, I am willing to know the new trend in RL and MAB problem.
","['machine-learning', 'reinforcement-learning', 'research', 'markov-decision-process']","
There are some wonderful resources for keeping up to date in the ML community. Here are just a handful that a coworker showed me:

Deep Learning Monitor: this site contains hot and new papers along with tweets that are popularized by the community! You can even checkout RL papers specifically here  
arxiv-sanity: this site updates with popular and new papers that make it onto Arxiv
papers with code: this site is wonderful because not only does it link to papers, but it links to their implementation for reproduction or assistance in your own personal projects. They even have a leaderboard and track state of the art (SoTA) on tons of different tasks  
DL_twitter loop: You can't forget twitter, given that most researchers use it; this is just a single nice group you may like

"
Are there any reliable ways of modifying the reward function to make the rewards less sparse?,"
If I am training an agent to try and navigate a maze as fast as possible, a simple reward would be something like 
\begin{align}
R(\text{terminal}) &= N - \text{time}\ \ , \ \ N \gg \text{everything} \\
R(\text{state})& = 0\ \ \text{if not terminal}
\end{align}
i.e. when it reaches the terminal state, it receives a reward but one that decreases if it is slower. Actually I'm not sure if this is better or worse than $R(\text{terminal}) = 1 / \text{time}$, so please correct me if I'm wrong.
However, if the maze is really big, it could spend a long time wandering around before even encountering that reward. Are there any reliable ways of modifying the reward function to make the rewards less sparse? Assume that the agent knows the Euclidean distance between itself and the exit, just not the topography of the maze.
Is it at all sound to simply do something like
\begin{align}
R(\text{current}) = (d_E(\text{start}, \text{exit}) - d_E(\text{current}, \text{exit})) + (\text{terminal}==True)*(N-\text{time})?
\end{align}
Or if not, what kind of dense heuristic reward or other techniques might be better?
","['reinforcement-learning', 'rewards', 'reward-shaping', 'reward-design', 'sparse-rewards']",
Why do we normalize data in a deep neural network?,"
I have asked this question a number of times, but I always get confusing answers to this, like ""normalized data works better"", ""data lives in the same scale""
How can x-m/s make the scale of images the same? Please explain to me the maths. Also, take MNIST dataset for example & illustration.
","['neural-networks', 'convolutional-neural-networks', 'data-preprocessing']","
I answered a similar question earlier and here is a piece of my answer that i think covers your question:
Batch normalization's assistance to neural networks wasn't really understood for the longest time, initially it was thought to assist with internal covariate shift (hypothesized by the initial paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift) but lately has been tied to the optimization process (How Does Batch Normalization Help Optimization?). 
This means, from an architectural perspective, it is difficult to correctly assume how it should be utilized or how it will effect your network, unless you really understand its impact on the loss landscape and how your optimization process will traverse it given your initialization (Note, by the way, a recent paper by Google showed that you can alleviate a lot of the benefits of batch normalization sheerly by understanding what issues it's resolving and attempting to mitigate them in the initialization process: Fixup Initialization). 
"
Is a multi-layer Kohonen network possible?,"
The Kohonen network is one fully connected layer, which clusters the input into classes by a given metric. However, the one layer does not allow to operate with complex relations, that's why deep learning is usually used.
Is it possible then to make multi-layered Kohonen network? 
AFAIK, the output of the first layer is already cluster flags, so the activation function on the non-last layers must be different from the original Kohonnen definition?
","['neural-networks', 'unsupervised-learning']","
Kohonen networks by definition are single layer FCNN's, but what differentiates them from others is their unsupervised training procedure.   
This procedure is a function of the input, the weights and some hyperparameters. This means if you have a multilayer network you could train only the final layers weights using this procedure. Think of it this way, let $f$ be the final layer, and $g$ be the composition of all other layers, such that the whole network $N(x) = f \circ g(x)$. Using kohonens training procedure, you could learn $f$'s weights where the input is $g(x)$ rather than x. But then how would you learn $g$'s weights?  
So you could do this by iterative learning. let $N$ be a 2 layer network: $N = f_2 \circ f_1$. first learn $f_1$ using the single layer procedure, and now you could learn $f_2$ in the same manner, except the input features its trying to cluster would be $f_1(x)$ rather than $x$.  
There does exist cons to this procedure though: Using a kohonen network at each layer will try to cluster it to the best of its abilities, not so that its final composition will be best (a common problem in many optimization procedures that arent end-to-end), but that its current representation is. This may lead to non-optimal results, not achieving that deep representation that you're looking for that you could achieve in autoencoders or other deeper unsupervised models.
"
Improving the performance of a DNN model,"
I have been executing an open-source Text-to-speech system Ossian. It uses feed forward DNNs for it's acoustic modeling. The error graph I've got after running the acoustic model looks like this:

Here are some relevant information:

Size of Data: 7 hours of speech data (4000 sentences) 
Some hyper-parameters: 

batch_size : 128 
training_epochs : 15 
L2_regularization: 0.003 


Can anyone point me to the directions to improve this model? I'm assuming it is suffering from over-fitting problem? What should I do to avoid this? Increasing data? Or changing batch-size/epochs/regularization parameters? Thanks in advance.
","['deep-learning', 'speech-synthesis']",
"Was the corruption of Microsoft's ""Tay"" chatbot an example of catastrophic forgetting?","
Tay was a chatbot, who learned from Twitter users.

Microsoft's AI fam from the internet that's got zero chill. The more you talk the smarter Tay gets.  Twitter tagline.

Microsoft trained the AI to have a basic ability to communicate, and taught it a few jokes from hired comedians before setting it lose to learn from its conversations.
This was a mistake.
But why did Tay go so wrong? Was this an example of catastrophic forgetting, where short, recent trends override large, less recent training, or was it something else entirely?
","['chat-bots', 'history', 'catastrophic-forgetting']","
It was essentially a lack of control over crowd-sourced training data.
While Tay was initially set up with some conversational ability, it seemed to be programmed to learn from interactions with other users. Once users became aware of this, they basically gamed the bot by exposing it to inappropriate language, which Tay's algorithms then picked up and repeated. According to the Wikipedia article on the topic, it is not known for sure whether its repeat after me facility was solely at fault, or if there was other behaviour that caused it.
It's not really an example of catastrophic forgetting; for once we don't know how Tay worked internally. I would think it's just that it was overwhelmed by new data coming in which was different from the pre-set. It seems unlikely that the kind of language it was exposed to was in any way known in advance and part of its training set (and labelled as 'inappropriate').
Essentially, the lesson from this is to never trust any unvetted input data for training, unless you want to risk people abusing this trust as happened in this case.
"
How can I perform multivariable regression with neural networks?,"
I want to use a neural network to perform a multivariable regression, where my dataset contains multiple features, but I can't for the life of me figure it out. Every kind of tutorial on the internet seems to be either for a single feature without information on how to upgrade it to multiple, or results in a yes or a no when I need numeric predictions (that is, it uses neural networks for classification). 
Can someone please recommend some kind of resource I can use to learn this?
","['neural-networks', 'python', 'regression']","
Have a look at sklearn's sklearn.neural_network.MLPRegressor class, which uses a multi-layer neural network to do regression. You first need to define the object MLPRegressor, for example, by specifying the value of the parameter hidden_layer_sizes, which determines the number of layers and the number of neurons per layer, then you should call the method fit on this created object and pass to it your data matrix $X \in \mathbb{R}^{n \times m}$, where $n$ is the number of samples and $m$ is the number of features.
"
"Can computers recognise ""grouping"" from voice tonality?","
In human communication, tonality or tonal language play many complex information, including emotions and motives. But excluding such complex aspects, tonality serves some a very basic purpose of ""grouping"" or ""taking common"" functions such as: 

The sweet, (pause), bread-and-drink. 

It means ""The sweet bread and the sweet drink"". However

The sweet-bread, (pause) and drink. 

It means only the bread is sweet but the drink isn't necessarily sweet, or the drink's sweetness property isn't assigned. 
Can computers recognise these differences of meaning based on tonality?
","['natural-language-processing', 'voice-recognition', 'speech-synthesis']",
How can I solve the linkage problem in genetic algorithms?,"
In a genetic algorithm, the order of the genes on a chromosome can have a significant effect on the performance (capacity to generate adaptation) of the genetic algorithm, where two or more genes interact to produce highly fit individuals. If we have a chromosome length of $100$ and genes $A$ and $B$ interact, then having them next to each other is strongly preferable than having them at opposing ends of the chromosome. In the former case, the probability of crossover breaking the genes apart is $1$ in $100$, and in the latter it is one.
What mechanisms have been tried to optimise the order of genes on a chromosome, so that interacting genes are best protected from crossover? Is it even possible? 
I've asked at Biology SE if there exists any known biological mechanism which is responsible for such a possible order of the genes on a chromosome.
","['genetic-algorithms', 'evolutionary-algorithms', 'biology']",
Several questions related to UCT and MCTS [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



In Bandit Based Monte-Carlo Planning, the article where UCT is introduced as a planning algorithm, there is an algorithm description in page 285 (4 of the pdf).
Comparing this implementation of UCT (a specific type of MCTS algorithm) to the application normally used in games, there is one major difference. Here, the rewards are calculated for every state, instead of only doing an evaluation at the end of the simulation.
My questions are (they are all related to each other):

Is this the only big difference? Or in other words, can I do the same implementation as in MCTS for games with the 4 stages: selection, expansion, simulation and backpropagation, where the result of the simulation is the accumulated reward instead of a value between 0 and 1? How would the UCT selection be adjusted in this case?

What does the UpdateValue function in line 12 does exactly? In the text it says it is used to adjust the state-action pair value at a given depth, will this be used to the selection? How is this calculated exactly?

What is the depth parameter needed for? Is it related with the UpdateValue?


Finally I would like to ask if you know any other papers where a clear implementation of UCT for planning is used with multiple rewards, not only on the end of the simulation.
","['reinforcement-learning', 'monte-carlo-tree-search', 'planning']",
How can I incrementally train a Yolo model without catastrophic forgetting?,"
I have successfully trained a Yolo model to recognize k classes. Now I want to train by adding k+1 class to the pre-trained weights (k classes) without forgetting previous k classes. Ideally, I want to keep adding classes and train over the previous weights, i.e., train only the new classes. If I have to train all classes (k+1) every time a new class is added, it would be too time-consuming, as training k classes would take $k*20000$ iterations, versus the $20000$ iterations per new class if I can add the classes incrementally. 
The dataset is balanced (5000 images per classes for training).
I appreciated if you can throw some methods or techniques to do this continual training for Yolo.
","['convolutional-neural-networks', 'training', 'yolo', 'incremental-learning', 'catastrophic-forgetting']","
There's something called Elastic Weight Consolidation to prevent neural networks from forgetting previous tasks as they train on new tasks. It might be helpful for your case too.
The main idea is to quantify the importance of parameters for task $t$ and penalize the model in proportion when it changes its parameters as it trains to learn task $t+1$. As you can see, this incentivizes model to change parameters that are less important for task $t$ which prevents the model from forgetting it. 
"
Better to learn the same small set for multiple epochs then go to the next or learn from each one time repeatedly for multiple times?,"
I don't know if I worded the title correctly.
I have a big dataset (300000 of images after augumentation) and I've splitted it into 10 parts, because I can't convert the images into a numpy and save it, the file would be too large.
Now, I have a neural network (Using keras with tf). My question is, is it better to train each file individually for X epochs (File 1 for 5 epochs, then File 2 for 5 epochs, etc.), or should I do an epoch for each, repeatedly (File 1 for an epoch, File 2 for an epoch, etc., and repeat for 5 times).
I've used the first and I get an accuracy of about 88%. Whould I get an improvement by doing the latter?
","['neural-networks', 'image-recognition', 'python']",
Spikes in of Train and Test error,"
I learn a DNN for image recognition. During each epoch, I calculate mean loss in the training set. After each epoch, I calculate loss and number of errors over both training and test set. The problem is, training and test error go to (almost) zero, then increase, go to zero again, increase, and so on.  The process seems stochastic.
epoch: 1 mean_loss=0.109 train: errs=7 loss=0.00622 test: errs=3 loss=0.00608
epoch: 2 mean_loss=0.00524 train: errs=5 loss=0.00309 test: errs=3 loss=0.00369
epoch: 3 mean_loss=0.00408 train: errs=13 loss=0.00614 test: errs=7 loss=0.00951
epoch: 4 mean_loss=0.00198 train: errs=113 loss=0.102 test: errs=51 loss=0.265
epoch: 5 mean_loss=0.00424 train: errs=3 loss=0.00201 test: errs=2 loss=0.00148
epoch: 6 mean_loss=0.0027 train: errs=1 loss=0.000466 test: errs=2 loss=0.00193
epoch: 7 mean_loss=0.00797 train: errs=5 loss=0.00381 test: errs=0 loss=0.000493
epoch: 8 mean_loss=0.00368 train: errs=1 loss=0.000345 test: errs=2 loss=0.00148
epoch: 9 mean_loss=0.000358 train: errs=0 loss=6.76e-05 test: errs=0 loss=0.000446
epoch: 10 mean_loss=0.00101 train: errs=164 loss=0.0863 test: errs=67 loss=0.19
epoch: 11 mean_loss=0.000665 train: errs=0 loss=2.38e-05 test: errs=0 loss=9.86e-05
epoch: 12 mean_loss=0.00714 train: errs=5 loss=0.00909 test: errs=0 loss=0.00816
epoch: 13 mean_loss=0.00266 train: errs=73 loss=0.0333 test: errs=10 loss=0.0192
epoch: 14 mean_loss=0.00213 train: errs=0 loss=7.74e-05 test: errs=0 loss=0.000197
epoch: 15 mean_loss=6.12e-05 train: errs=0 loss=7.66e-05 test: errs=0 loss=3.44e-05
epoch: 16 mean_loss=0.00162 train: errs=5 loss=0.00265 test: errs=0 loss=0.0012
epoch: 17 mean_loss=0.000159 train: errs=0 loss=3.11e-05 test: errs=0 loss=4.26e-05
epoch: 18 mean_loss=4.68e-05 train: errs=0 loss=3.28e-05 test: errs=0 loss=6.05e-05
epoch: 19 mean_loss=2.47e-05 train: errs=0 loss=2.8e-05 test: errs=0 loss=5.01e-05
epoch: 20 mean_loss=2.2e-05 train: errs=0 loss=2.31e-05 test: errs=0 loss=3.95e-05
epoch: 21 mean_loss=2.37e-05 train: errs=0 loss=1.76e-05 test: errs=0 loss=2.52e-05
epoch: 22 mean_loss=1.4e-05 train: errs=0 loss=1.16e-05 test: errs=0 loss=1.52e-05
epoch: 23 mean_loss=2.13e-05 train: errs=0 loss=1.65e-05 test: errs=0 loss=2.13e-05
epoch: 24 mean_loss=1.53e-05 train: errs=0 loss=1.91e-05 test: errs=0 loss=2.46e-05
epoch: 25 mean_loss=0.00419 train: errs=0 loss=5.27e-05 test: errs=0 loss=4.65e-05
epoch: 26 mean_loss=0.000372 train: errs=6 loss=0.00297 test: errs=3 loss=0.00731
epoch: 27 mean_loss=0.0016 train: errs=0 loss=4.23e-05 test: errs=0 loss=3.69e-05
epoch: 28 mean_loss=3.34e-05 train: errs=0 loss=2.44e-05 test: errs=0 loss=2.76e-05
epoch: 29 mean_loss=7.03e-05 train: errs=0 loss=2.16e-05 test: errs=0 loss=1.69e-05
epoch: 30 mean_loss=2.41e-05 train: errs=0 loss=1.84e-05 test: errs=0 loss=1.77e-05
epoch: 31 mean_loss=1.26e-05 train: errs=0 loss=2.11e-05 test: errs=0 loss=1.78e-05
epoch: 32 mean_loss=1.39e-05 train: errs=0 loss=2.75e-05 test: errs=0 loss=2.42e-05
epoch: 33 mean_loss=7.68e-05 train: errs=0 loss=0.00014 test: errs=0 loss=4.66e-05
epoch: 34 mean_loss=2.53e-05 train: errs=0 loss=1.48e-05 test: errs=0 loss=1.56e-05
epoch: 35 mean_loss=0.000352 train: errs=1786 loss=2.17 test: errs=493 loss=2.56
epoch: 36 mean_loss=0.0088 train: errs=0 loss=0.000347 test: errs=0 loss=0.000449
epoch: 37 mean_loss=0.000395 train: errs=0 loss=6.18e-05 test: errs=0 loss=0.000125
epoch: 38 mean_loss=5e-05 train: errs=0 loss=6.73e-05 test: errs=0 loss=9.89e-05
epoch: 39 mean_loss=0.00401 train: errs=26 loss=0.00836 test: errs=27 loss=0.0269
epoch: 40 mean_loss=0.00051 train: errs=0 loss=7.66e-05 test: errs=0 loss=7.07e-05
epoch: 41 mean_loss=5.49e-05 train: errs=0 loss=2.47e-05 test: errs=0 loss=2.58e-05
epoch: 42 mean_loss=3.38e-05 train: errs=0 loss=1.67e-05 test: errs=0 loss=2.1e-05
epoch: 43 mean_loss=2.45e-05 train: errs=0 loss=1.28e-05 test: errs=0 loss=2.95e-05
epoch: 44 mean_loss=0.00137 train: errs=44 loss=0.0141 test: errs=16 loss=0.0207
epoch: 45 mean_loss=0.000785 train: errs=1 loss=0.000493 test: errs=0 loss=4.46e-05
epoch: 46 mean_loss=5.46e-05 train: errs=1 loss=0.000487 test: errs=0 loss=1.34e-05
epoch: 47 mean_loss=1.99e-05 train: errs=1 loss=0.00033 test: errs=0 loss=1.57e-05
epoch: 48 mean_loss=1.78e-05 train: errs=1 loss=0.000307 test: errs=0 loss=1.58e-05
epoch: 49 mean_loss=0.000903 train: errs=1 loss=0.00103 test: errs=0 loss=0.000393
epoch: 50 mean_loss=4.74e-05 train: errs=0 loss=4.63e-05 test: errs=0 loss=3.53e-05
Finished Training, time: 234.69774420000002 sec

The images are 96*96 gray.  There are about 7000 training and 1750 test images. The order of presentation is random, and different at each epoch. Each image is either contains the object or not.  The architecture is (Conv2d->ReLU->BatchNorm2d->MaxPool)*4->AvgPool(6,6)->Flatten->Conv->Conv->Conv.  All MaxPool's are 2*2. First two Conv2d layers are 5*5, padding=2, others 3*3, padding=1.  The optimiser is like this:
Optimizer= Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 1e-05
)

Currently I just choose the epoch when the training set error was minimal. 
if epoch == 0 or train_loss < train_loss_best:
    net_best = copy.deepcopy(net)
    train_loss_best = train_loss

It works, but I don't like it. Is there a way to make the learning more stable and steady?
","['deep-learning', 'deep-neural-networks', 'learning-curve']","
I was actually very recently working on CNNs and extensively training models and have noticed the same thing. (NOTE: The answer I will give is purely based on empirical observations and my understanding of mathematics of deep learning).
So, the thing I observed on training set (since we are directly optimising on the training set) is that between period of low losses there was suddenly a large loss, and then again low losses, but, this time the loss (of training set) reached even more lower values and the accuracy of the test set reached a higher stable accuracy (by higher stable accuracy I mean, that in general I was observing the accuracy was somewhat oscillating around a fixed value of accuracy, and now the accuracy is oscillating around a higher fixed value of accuracy). From this I concluded that, the high loss was some sort of an obstacle which is stopping the weights from reaching global minima and is trapping it in local minima, unless it gains enough momentum to escape the obstacle, which is indicated by the high loss (think of the loss function as a rotated around y-axis sawtooth waveform inclined at  certain degree). So, as we reach lower loss it is expected (if the model is good) that you see better generalisation and hence the stable higher accuracy.
You can actually somewhat see this happening in your training too, although the data-set is smaller to make concrete comments.
Seeing your results it seems pretty clear that training loss and test loss are going hand in hand as well as accuracy, which is a good sign, and it means your model has not over-fitted yet and you can train it more, which will probably make you encounter these loss spikes less, nevertheless I am pretty sure they will be there every now and then, whichever method you choose, as the loss curve is always a pretty jagged terrain for a DNN.
I chose to disregard training accuracy, because most of the times it is not really related to the loss in a monotonically increasing way (check this thread), and the test loss is expected to increase since at such high accuracies, it has been empirically seen that test loss might decrease without affecting test accuracy (check this answer).
"
Can a computer identify the philosophical concept on which a given story is based?,"
Say you have to enter a story to a computer. Now, the computer has to identify the philosophical concept on which the story is based, say:

Was it a ""self-fulfilling prophecy""?

Was it an example of ""Deadlock"" or ""Pinocchio paradox situation""?

Was it an example of how rumours magnify? or something similar to a chain reaction process?

Was it an example of ""cognitive dissonance"" of a person?

Was it a story about ""altruism""?

Was it a story about a ""misunderstanding"" when a person did something ""innovative"" but it accidentally was innovated earlier so the person was ""falsely accused"" of ""plagiarising""?


And so on.
Given that the story is not only a heavy rephrase of the pre-existing story; not only character names and identities are totally changed, but the context completely changed, the exact tasks they were doing are changed.
Can computers identify such ""concepts"" from stories? If yes, then what mechanism do they use?
","['natural-language-processing', 'philosophy', 'natural-language-understanding']","
No. This is currently out of the scope for any language processing system. It requires a general understanding of abstract concepts which is not possible for machines at present.
In order to recognise a self-fulfilling prophecy, you first need to identify that something is a prophecy. So it needs to be something that expresses a possible future state, for which you need to identify what possible future states are; and then you need to see whether it is self-fulfilling. Conceptually this is far too complex to do.
You might get away for some of these with formal criteria (eg use of future tense for something describing a future state/event), but this is far too imprecise.
""Altruism"" requires knowledge about typical expected behaviour; you would need to be able to identify motives behind people's actions, and then decide whether it was altruistic or not. This is just too complex for now (and the foreseeable future).
"
Dropout causes too much noise for network to train,"
I am using dropout of different values to train my network. The problem is, dropout is contributing almost nothing to training, either causing so much noise the error never changes, or seemingly having no effect on the error at all:
The following runs were seeded.
key: dropout = 0.3, means 30% chance of dropout
graph x axis: iteration
y axis: error
dropout=0

dropout = 0.001
dropout = 0.1
dropout = 0.5

I don't quite understand why dropout of 0.5 effectively kills the networks ability to train. This specific network here is rather small, a CNN of architecture:
3x3x3                    Input image
3x3x3                    Convolutional layer: 3x3x3, stride = 1, padding = 1
20x1x1                   Flatten layer: 27 -> 20
20x1x1                   Fully connected layer: 20
10x1x1                   Fully connected layer: 10
2x1x1                    Fully connected layer: 2

But I have tested a CNN with architecture:
10x10x3                  Input image
9x9x12                   Convolutional layer: 4x4x12, stride = 1, padding = 1
8x8x12                   Max pooling layer: 2x2, stride = 1
6x6x24                   Convolutional layer: 3x3x24, stride = 1, padding = 0
5x5x24                   Max pooling layer: 2x2, stride = 1
300x1x1                  Flatten layer: 600 -> 300
300x1x1                  Fully connected layer: 300
100x1x1                  Fully connected layer: 100
2x1x1                    Fully connected layer: 2

overnight with dropout = 0.2 and it completely failed to learn anything, having an accuracy of just below 50%, whereas without dropout, its accuracy is ~85%. I would just like to know if there's a specific reason as to why this might be happening. My implementation of dropout is as follows:
activation = relu(val)*(random.random() > self.dropout)
then at test time:
activation = relu(val)*(1-self.dropout)
","['neural-networks', 'convolutional-neural-networks', 'dropout', 'regularization', 'relu']","
why dropout of 0.5 effectively kills the networks ability to train.

because that is too mutch normal values are like 0.15-0.05. Imagine, the 50% of input image is randomly set to 0, THEN it happens on next layer, means in average 25%  remains, etc... also if you have small dataset with too different images for each class, this + drouput wil confuse the network. 
Also your CNN setup is not realy rational. Too much fc layers, replace one or two fc to convo layers.  And i d say the reason of using 4x4 is only with stride 2, else use 3x3. And you sould use batch normalisation and augmentation like small noice would be probably petter then dropout in your case.
"
Can non-Markov environments also be deterministic?,"
The definition of deterministic environment I am familiar with goes as follows:

The next state of the agent depends only on the current state and the action chosen by the agent.

By exclusion, everything else would be a stochastic environment.
However, what about environments where the next state depends deterministically on the history of previous states and actions chosen? Are such environments also considered deterministic? Are they very uncommon, and hence just ignored, or should I include them into my working definition of deterministic environment?
","['reinforcement-learning', 'definitions', 'environment', 'markov-property']","
Markov Environment is not about deterministic or stochastic. ""Depends only on the current state and your action"" does not mean you know what will happen(deterministic). 
We can have Markov + deterministic, Markov + Stochastic, Non-Markov + deterministic, and Non-Markov + stochastic.
The definition you have is not a definition of deterministic. It is a definition of Markov property. 
Refer to Wikipedia. 

A stochastic process has the Markov property if the conditional
  probability distribution of future states of the process (conditional
  on both past and present values) depends only upon the present state;
  that is, given the present, the future does not depend on the past. A
  process with this property is said to be Markovian or a Markov
  process. The most famous Markov process is a Markov chain. Brownian
  motion is another well-known Markov process.

Markov property is assumed mostly in stochastic problems.
Brownian motion is the motion of molecules of ink in the water and used to model the movement of a stock price, which is stochastic.
Deterministic means when you are in the same state and choose the same action your next state will be always the same. 
Stochastic means even you are in the same state and choose the same action, you next state can be different than the previous time.
Example) You toss a coin and roll a die. Every time you roll a die you get pennies as many. If the coin gets head, you get a chance to roll a die twice next time. 
Your state can be (money you collect so far, coin head/tail in the previous time).
In this problem, your next state will not be affected by the past. the only thing you need to know is the current state, the money you got and head or tail. It has a Markov process/environment. However, still, it is stochastic because you don't know what will be the next state. 
"
Which nonfictional documentaries about Artificial Intelligence are available?,"
From the subjective perspective, the number of documentaries about the subject Artificial Intelligence and robotics is small. It seems, that the topic is hard to visualize for the audience and in most cases, the assumption is, that the recipient isn't familiar with computers at all. I've found the following documentaries:

The Computer Chronicles - Artificial Intelligence (1985)
The Machine That Changed the World (1991), Episode IV, The Thinking Machine
Robots Rising (1998)
Rodney's Robot Revolution (2008)

The subjective awareness is, that the quality of the films in the 1980s was higher than in modern documentaries, and in 50% of the documentaries Rodney Brooks is the host. Are more documentaries available which can be recommended to watch?
Focus on non-fictional documentaries
Some fictional movies were already mentioned in a different post. For example Colossus: The Forbin Project (1970), Bladerunner (1982) or A.I. Artificial Intelligence (2001). They are based on fictional characters which doesn't exist and the presented robots are running with a Hollywood OS. This question is only about nonfictional motion pictures.
","['education', 'journalism']","
AlphaGo (2017) is quite a good watch, given that it is a documentary about the AlphaGo program, how DeepMind developed it, the help they had, and doesn't get too technical.  You can watch the trailer here.
Another documentary which wasn't exactly AI but something that is an interesting watch or at least was when it came out was The Human Face of Big Data.
"
What can be inferred about the training data from a trained neural network?,"
Suppose we trained a neural network on some training set that we call $X$. 
Given the neural network and the method of training(algorithm, hyperparameters etc.) can we infer anything about $X$.
Now, instead, suppose we also have some subset of the training data $Y \in X$ available. Is there anything we can infer about $Y^c$?
","['neural-networks', 'machine-learning', 'datasets']",
Can multiple reinforcement algorithms be applied to the same system?,"
Can a system, for instance, a robotic vehicle, be controlled by more than one reinforcement learning algorithm. I intend to use one to address collision avoidance whereas the other to tackle autonomous task completion.
","['reinforcement-learning', 'q-learning']",
Can the agent of reinforcement learning system serve as the environment for other agents and expose actions as services?,"
Can the agent of reinforcement learning system serve as the environemnt for other agents and expose actions as services? Are there research that consider such question?
I tried to formulate the problem of network of reinforcement learning systems in other site of Stack network: https://cs.stackexchange.com/questions/111820/value-flow-and-economics-in-stacked-reinforcement-learning-systems-agent-as-r
So - is there some research along such stacked RLs? Of course, I know that Google is my friend, but sometimes I come up with the ideas that I don't know how to name them or how they are named by other scientists who have already discovered and researched them. And that is why I am useing stackexchange to give me some keywords for my ideas and I can myself explore futher the field using those keywords. So - what are the keywords of resarch about such stakced RL systems, of agent-environment interchange in reinforcement? learning?
","['reinforcement-learning', 'intelligent-agent']",
What is a conditional random field?,"
I new in machine learning, especially in Conditional Random Fields (CRF).
I have read several articles and papers and in there is always associated with HMM and sequences classification. I don't really understand mathematics, especially in the annoying formula. So I can't understand the process. Where I need to start to understand CRFs??
I want to make an information extraction application using CRF Named Entity Recognition (NER).
I got some tutorial for that: https://eli5.readthedocs.io/en/latest/tutorials/sklearn_crfsuite.html#training-data 
But I don't know the proses each step, like training proses, evaluation, and testing
I use this code :
  data_frame = eli5.format_as_dataframes(
            eli5.explain_weights_sklearn_crfsuite(self.crf))

Targets

Transition Features

How to get that number ?
and 1 more thing makes me confused:
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    c1=0.1,
    c2=0.1,
    max_iterations=20,
    all_possible_transitions=False,
)

What is the algorithm lbfgs? Is the CRF not an algorithm? Why do I need lbfgs? What is exactly a conditional random field?
","['machine-learning', 'natural-language-processing', 'definitions', 'probabilistic-graphical-models', 'conditional-random-field']",
How to enforce covariance-matrix output as part of the last layer of a Policy Network?,"
I have a continuous state space, and a continuous action space. The way I understand it, I can build a policy network which takes as input a continuous state vector and outputs both mean vector and covariance matrix of the action-distribution. To get a valid action I then sample from that distribution.
However, when trying to implement such a network, I get the error message that the parts of my output layer which I want to be the covariance matrix are singular/not positive-semi-definite. How can I fix this? I tried different activation-functions and initializations for the last layer, but once in a while I run into the same problem again. 
How can I enforce that my network outputs a valid covariance matrix?
","['machine-learning', 'reinforcement-learning', 'policy-gradients']","
@Brale_ 's answer is correct, it is common practice in a multitude of models to learn a representation of an independent multivariate normal, but don't let that stop you from pushing the envelope for your needs.   
You can actually learn a dependent form as well. Normally the independent form is done by learning the means and standard deviations, because sampling a standard normal can achieve your draw by $z \sim N(\mu, Diag(\sigma^2))$ by $z = \mu + \sigma \epsilon$ where $\epsilon$ is drawn from a unit normal.   
But you can actually achieve a similar trick for a generalized multivariate normal distribution: lets assume your trying to learn $N(\mu, \Sigma)$ where $\Sigma$ is the covariance matrix. So what you would do is learn the Cholesky decomposition because where $\Sigma = AA^T$ you can now draw it through the parametrization trick: $z = \mu + A\epsilon$.
"
"If deep Q learning involves adjusting the value function for a specific policy, then how do I choose the right policy?","
I wrote a simple implementation of Flappy Bird in Python, and now I'm trying to train an agent to play it at a reasonable skill level using TFLearn. 
I feed the network an input vector of size 4:

the horizontal distance to the next obstacle
the agent's vertical distance from the ground-
the agent's vertical distances from the top, and
the agent's vertical distances from the bottom parts of the opening in the obstacle. 

The output layer of the network contains one unit, telling me the Q value of the provided state with the assumption that the action taken in that state will be determined by the policy. 
However, I don't know what policy would make the agent learn to play the best.  I can't just make it choose random actions because that would make the policy non-stationary.  What can I do?
","['q-learning', 'deep-rl', 'policies']",
How does Friend-or-Foe Q-learning intuitively work?,"
I read about Q-Learning and was reading about multi-agent environments. I tried to read the paper Friend-or-Foe Q-learning, but could not understand anything, except for a very vague idea.
What does Friend-or-Foe Q-learning mean? How does it work? Could someone please explain this expression or concept in a simple yet descriptive way that is easier to understand and that helps to get the correct intuition?
","['reinforcement-learning', 'q-learning', 'terminology', 'game-theory', 'multi-agent-systems']",
Is the playout started from a leaf or child of leaf in Monte Carlo Tree Search?,"
On Wikipedia, the MCTS algorithm is described

Selection: start from root $R$ and select successive child nodes until a leaf node $L$ is reached. A leaf is any node from which no simulation (playout) has yet been initiated.
Expansion: create one (or more) child nodes and choose node $C$ from one of them. Child nodes are any valid moves from the game position defined by $L$.
Simulation: complete one random playout from node $C$.

Why is the playout started from a child of the first leaf, not the leaf itself? And aren't leaves then permanently stuck as leaves, since playouts always start from their children, not them? Or does the leaf get attributed as having had a ""playout initialised"" from it, even though it started at its child?
",['monte-carlo-tree-search'],
Which models accept numerical parameters and produce a numerical output?,"
I need a model that will take in a few numerical parameters, and give back a numerical answer (Context: predicting a slope based on environmental factors without having to actually take measurements to find the slope). 
However, I am lost, given that I have not found (on the web) any model that is able to solve this task. If someone can suggest a type of model that would be able to solve this type of problem, I would greatly appreciate it. Resources and libraries would also be useful if you care to suggest any. I'm using Python.
","['machine-learning', 'python', 'definitions', 'regression']",
Suicide Predictor and Locator,"
Suicide is on the increase in my country and most victims tend to leave early traces from text messages, social media accounts, search engine queries. So I came up with the idea to develop an AI system with the following features:

Ability to read text messages searching for suicide trigger words
Read chats also for the same trigger words
Incorporate synchronization of words from software/browsers on recent words typed
Learning algorithm to predict the next action after taking notes of the use of these suicide trigger words
Ability to access any cellphone, Android, computer, websites 

Is this feasible or not feasible?
","['deep-learning', 'ai-design', 'applications', 'prediction']","
This question seems to be specifically about your idea so I'll answer as such - any general 'can digital activity be used to predict suicide attempts' is probably too broad for this site.
As I see it you want to use what a person types, across all types of digital media, to get some measure of their risk of suicide attempt.
Two key questions in any machine learning problem

Can we create variables as predictors?
Is the data there? i.e. do we have labelled data already

As far as creating variables goes a lot of people don't realise that machine learning (or 'AI') isn't about dumping a load of unedited data into an algorithm and letting it figure it out - 90% of the work is in making those variables into something sensible. In this case it may involve a fair bit of psychology. Perhaps you look at changes in message length - has someone gone from writing essays to short answers? (I'm not a psychologist, that may well be way off the mark). The use of particular words or phrases may come into it - you would need to do a fair bit of research into the differences between those who attempted suicide and those who didn't (perhaps ranking words by frequency and see if there are distinct differences).
This brings us to our second question (or perhaps it should be the first) 'is the data there'? You want to predict suicide risk from digital activity so you'll need the digital activity of all different types of people with a label of whether or not they attempted suicide (and when - no point collecting the data two years after their attempt). Do you think this dataset exists? I'm sceptical - the level of detail you would need (both into someone's digital activity and their personal mental state) is unlikely to be recorded.
It isn't to say I think its an idea to be thrown out, only that you need to be realistic about the amount of effort required to do something like this. You would need to carry out research to collect this data, need to plan out your ideas before hand to know what you would like to collect (the worst thing is to find, after a year of data collection, that you didn't monitor a variable you would have now found vital).
"
How do I determine the generalisation ability of a neural network?,"
I am trying to ascertain if my neural network is able to generalize or if its simply using memory/overfitting to solve a task. I would like my model to generalise.
Currently, I train the neural network on a randomly generated 3x3 frozen lake environment - with no holes. (The network simply chooses an action for each state it is presented.)
Then, I test the model on a much larger frozen lake environment. Still no holes. Still randomly generated. The test environment size is assigned by a random value of 5-15 for each axis (height/width), randomly generated.
Then I determine the ""degree of generalization"" by how many large environments the network is able to solve. At present, it solves 100/100 on the 3x3, and about 83/100 on the larger test environments. 
When I track the solutions it generates, I can see that the network always takes the shortest route available, which is great.
Do you guys have any ideas, inputs or criticism on the method I use to determine the degree of generalization?
","['neural-networks', 'machine-learning', 'deep-learning', 'reinforcement-learning', 'generalization']",
"How should we understand the evaluation metric, AUC, in link prediction problems?","
In link prediction problems, there are only known edges and nodes. 

If there is a known edge in the node pair, the node pair is regarded as a positive sample. Except for those node pairs whose edges are known, There may exist unobserved edges in some node pairs or there really doesn't exist edges in some node pairs. Our target is to predict potential links in those candidate node pairs. 

The node pair where there exist known edge is regarded as a positive sample. So the node pair whose edge are not observed can neither be regarded as a positive example, nor a negative example. 
So I think link prediction problem is a semi-supervised problem. However, I find that many papers, for example, GRTR: Drug-Disease Association Prediction Based on Graph Regularized Transductive Regression on Heterogeneous Network, use AUC(Area Under the ROC Curve, a metric for supervised problems) as the metric. 
How should we understand such behavior? What's the reason?
","['machine-learning', 'prediction']",
"By learning from incomplete episodes, does David Silver mean learning of $V(s)$ even when the episode is not completed?","
I came across the $TD(0)$  algorithm from Sutton and Barto:

Clearly, the only difference of TD methods with the MC methods is that TD method is not waiting till the end of the episode to update the $V(s)$ or $Q(s,a)$, but according to David Silver's lecture (Lecture 4- ~34:00),

The $TD(0)$ algorithm learns from incomplete episodes, but in the earlier algorithm we can see that the loop repeats until $s$ is terminal which mean completion of episode.
So, by learning from incomplete episodes, does David Silver mean learning of $V(s)$ even when the episode is not completed? Or did I interpret the algorithm wrong? If so, what is the correct interpretation?
","['reinforcement-learning', 'temporal-difference-methods']",
Encoding real valued inputs,"
UPDATE: After reading more about the topic, I've tried implementing the 
 DDPG algorithm instead of using a variation of Q-Learning and still have the same issue.
I have the following issue:
I want to train my critic to estimate values of state/action pairs. My state consists of 2 real valued variables and my action is another real valued variable.
I normalize all values before I feed them into the network. Now I have the following issue: The network is very unresponsive to changes in the input. Before I normalize the state and the actions, they can take up any value between 0 and 50. After normalizing them they are in the range between -1 and 1. A change of 1 in the input can become a very small change in the input after normalization.
But in my specific situation, a small change in the action or the state can cause a very large change in the value of the state/action pair. The network does not really learn that correctly, it handles similar inputs similarly all the time (which is okay most of the times, but there are hard cuts in the shape of the value function here and there). If I further reduce the networks' capacity, the networks output becomes constant and ignores all three inputs.
Do you know any other tricks, that I could use to increase sensitivity to the input at some points? Or is my network configuration/approach the wrong one (too large, too small)?
The network I'm training is a simple feedforward neural network that takes two inputs, followed by 2 hidden layers followed by a single output to predict the value for that state/action combination. (I'm still trying out different configurations here, as I have no real feeling for the amount of elements per layer and the amount of layers needed to get the capacity I need without encouraging overfitting).
Thanks for your help :)
","['neural-networks', 'policy-gradients']",
What is the best loss function for convolution neural network and autoencoder? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



What is the best choice for loss function in Convolution Neural Network and in Autoencoder in particular - and why?
I understand that the MSE is probably not the best choice, because little difference in lighting can cause a big difference in end loss. 
What about Binary cross-entropy? As I understand, this should be used when target vector is composed as 1 at one place and 0 at all others, so you compare only class that should be correct (and ignore others),... But this is an image (although the values are converted in 0-1 values,...)
","['convolutional-neural-networks', 'objective-functions', 'autoencoders']","
There is no right answer to this. Finding the right loss function is a tough and difficult problem. So your goal as the architect is to try to find one that best suits your needs. So lets think about your needs.  
You mention that you dont want lighting shifts to cause large error, so ill take a leap and assume you care more about the shapes and style of the image more than the coloring. To deal with this, maybe consider using difference of the gram matrices (this is considered common place in style transfer literature: A Neural Algorithm of Artistic Style) Note that you could use the encoder to get the representation of the output as well for the loss, $L(x) = D(Gram(Enc(x)), Gram(Enc(\hat x))$ where $D$ would be some distance metric like euclidian distance.  
Maybe the outline is all you care about. You could use some known edge detector filter and compare those, ex: $D(Edge(x), Edge(\hat x))$ 
Maybe you just dont care about color shifts, you could do $D(x - \mu_x, \hat x - \mu_{\hat x}$).  
Note that you can play around with whatever distance metric you use, whether it be with MSE, RMSE, MAE, etc... Each has their own small pros/cons based on the loss manifolds they create. In your case i dont think the difference there will be night and day though, but you never know.
Also mixing and matching is always nice: ex: $L(x) = \lambda_1 D(x, \hat x) + \lambda_2D(Gram(Enc(x)), Gram(Enc(\hat x)) + ...$ 
Takeaway: MSE might actually be fine, but it really depends on what you prioritize, and once you figure that out you can start getting clever and design the loss that fits your needs and problem
"
How to recognize with just name and last name if the person is a political exposed person,"
First than all, I am not sure if this questions is more about Machine Learning, or if its Artificial Intelligence, if not, just let me know I will delete it.
At my company we need to create a solution for banks, where a client comes in and they want to to open a bank account.
They need to know if that person is a politician or political exposed person, maybe they work in the european comission, or they are family from a pep for example.
The business users has lots of data sources where to get these people, for example: http://www.europarl.europa.eu/meps/en/full-list/all
They want to train a Model (Machine Learning), where the end user can enter the name:  Bill Clinton for example, and then the system has to return the percentage of a person being political or not.
Obviosly some persons are 100% politicial and the percentage will be 100%.
But if they enter a name that is not in any of their data sources, how would I train a model to decide if its pep or not?
quite confused
thanks
",['machine-learning'],"

But if they enter a name that is not in any of their data sources, how would I train a model to decide if its pep or not?

Based on just a person's name and nothing else, the accuracy of this model is going to be very low. Consider that most first name, surname combinations in Europe are going to be repeated across the population.
However, the accuracy might still be slightly better than guessing. Some families and social classes could be more likely to be involved in political work, and a statistical model would pick up on that.
To train the model, take your positive names, and combine with a random selection of names of people that are known to be ""not political"" or similar enough. It doesn't matter if some of the names are the same provided you are confident in your data. Probably you could just take a phone directory or the electoral register or some other list of general names. Provided your ""political"" people are a small fraction of all people, this will work well enough even if you have some of them in the negative class.
Ideally you mix those name groups in the rough proportion that the bank expects to see ""political"" and ""non-political"" customers, so that your data set is a good representation of the target population.
Then you train a classifier on the names. As this is text and sequence data, you will need a solution for that. Possibly LSTM would be suitable architecture, but so might some feature selection from the names in a more simple ML model.
Remember to hold back some data (both positive and negative cases) for cross-validating and testing the model.
Expect the accuracy of your model when testing to be low. Very low. I would not at all be surprised to find the end result unusable by itself.
If this is seriously to be part of some bank's account setup process, there needs to be additional data used later in the process. A gate for additional checks based purely on someone's name will perform very poorly in my opinion.
"
What is the most common practice to apply batch normalization?,"
For a deep NN, should I generally apply batch normalization after each convolution layer? Or only after some of them?  Which? Every 2nd, every 3rd, lowest, highest, etc.?
","['deep-learning', 'convolutional-neural-networks', 'batch-normalization']","
In the literature, it differs. You will see models do it after or before pooling only, and sometimes you see it after every single convolution.   
Batch normalization's assistance to neural networks wasn't really understood for the longest time, initially it was thought to assist with internal covariate shift (hypothesized by the initial paper: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift) but lately has been tied to the optimization process (How Does Batch Normalization Help Optimization?). 
This means, from an architectural perspective, it is difficult to correctly assume how it should be utilized, unless you really understand its impact on the loss landscape and how your optimization process will traverse it given some initialization (Note, by the way, a recent paper by Google showed that you can alleviate a lot of the benefits of batch normalization sheerly by understanding what issues it's resolving and attempting to mitigate them in the initialization process: Fixup Initialization). 
So I would recommend 3 things until it is more understood how to utilize it generally:

Play around, get frisky and experiment. Use what works best.  
Use block featurizers that are known to work well like residual blocks. Proven in practice and will probably work for you too.  
Do the research and investigate it more, if you find the answer, you'll be helping a lot of people :)

"
Where can I find the proof of the universal approximation theorem?,"
The Wikipedia article for the universal approximation theorem cites a version of the universal approximation theorem for Lebesgue-measurable functions from this conference paper. However, the paper does not include the proofs of the theorem. Does anybody know where the proof can be found?
","['neural-networks', 'reference-request', 'proofs', 'function-approximation', 'universal-approximation-theorems']","
""Modern"" Guarantees for Feed-Forward Neural Networks
My answer will complement nbro's above, which gave a very nice overview of universal approximation theorems for different types of commonly used architectures, by focusing on recent developments specifically for feed-forward networks.  I'll try an emphasis depth over breadth (sometimes called width) as much as possible.  Enjoy!

Part 1: Universal Approximation
Here I've listed a few recent universal approximation results that come to mind.  Remember, universal approximation asks if feed-forward networks (or some other architecture type) can approximate any (in this case continuous) function to arbitrary accuracy (I'll focus on the : uniformly on compacts sense).
Let me mention, that there are two types of guarantees: quantitative ones and qualitative ones.  The latter are akin to Hornik's results (Neural Networks - 1989) which simply state that some neural networks can approximate a given (continuous) function to arbitrary precision.  The former of these types of guarantees quantifies the number of parameters required for a neural network to actually perform the approximation and are akin to Barron's (now) classical paper (IEEE - 1993)'s breakthrough results.

Shallow Case:  If you want quantitative results only for shallow networks:
Then J. Siegel and J. Xu (Neural Networks - 2020) will do the trick but (note: The authors deal with the Sobolev case but you get the continuous case immediately via the Soblev-Morrey embedding theorem.)

Deep (not narrow) ReLU Case:  If you want a quantitative proof for deep networks (but not too narrow) with ReLU activation function then Dimity Yarotsky's result (COLT - 2018) will do the trick!

Deep and Narrow: To the best of my knowledge, the first quantitative proof for deep and narrow neural networks with general input and output spaces has recently appeared here:
https://arxiv.org/abs/2101.05390 (preprint - 2021).
The article is a constructive version of P. Kidger and T. Lyon's recent deep and narrow universal approximation theorem (COLT - 2020) (qualitative) for functions from $\mathbb{R}^p$ to $\mathbb{R}^m$ and A. Kratsios and E. Bilokpytov's recent Non-Euclidean Universal Approximation Theorem (NeurIPS - 2020).



Part 2: Memory Capacity
A related concept is that of ""memory capacity of a deep neural network"".
These results seek to quantify the number of parameters needed for a deep network to learn (exactly) the assignment of some input data $\{x_n\}_{n=1}^N$ to some output data $\{y_n\}_{n=1}^N$.  For example; you may want to take a look here:

Memory Capacity of Deep ReLU networks: R. Vershynin's very recent publication Memory Capacity of Neural Networks with Threshold and Rectified Linear Unit Activations - (SIAM's SIMODS 2020)

"
Can an object's movement (instead of its appearance) be used to classify it?,"
I know that it is very common for machine learning systems to classify objects based on their visual features such as shapes, colours, curvatures, width-to-length ratios, etc.
What I'd like to know is this: Do any techniques exist for machine learning systems to classify objects based on how they move?
Examples:

Suppose that in a still image, 2 different classes of objects look identical. However, in a video Class A glides smoothly across the screen while Class B meanders chaoticaly across the screen.
When given multiple videos of the same person walking, classify whether he's sober or drunk.

",['classification'],
Confused about NeuralODE,"
I am a bit confused about NeuralODE and I want to make sure that what I understood so far is correct. 
Assume we have (for simplicity) 2 data points $z_0$ measured at $t_0$ and $z_1$ measured at $t_1$. Normally (in normal NN approach), one would train a NN to predict $z_1$ given $z_0$, i.e. $NN(z_0)=z_1$. In NeuralODE approach, the goal is to train the NN to approximate a function $f(z_0)$ (I will ignore the explicit time dependence) such that given the ODE: $\frac{dz}{dt}|_{t_0}=f(z_0)$ which would be approximated as $\frac{dz}{dt}|_{t_0}=NN(z_0)$ and solving this using some (non AI based) ODE integrator (Euler's method for example) one gets as the solution for this ODE at time $t_1$ something close to $z_1$. So basically the NN now approximates the tangent of the function ($\frac{dz}{dt}$) instead of the function itself ($z(t)$). 
Is my understanding so far correct? 
So I am a bit confused about the training itself. I understand that they use the adjoint method. What I don't understand is what exactly is being updated. As far as I can see, the only things that are free (i.e. not measured data) are the parameters of the function $f$, i.e. the NN approximating it. So one would need to compute $\frac{\partial loss}{\partial \theta}$, where $\theta$ are the parameters (weights and biases of the network). 
Why would I need to compute, for example (as they do in the paper) $\frac{\partial loss}{\partial z_0}$? $Z_0$ is the input which is fixed, so I don't need to update it. What am I missing here? 
Secondly, if what I said in the first part is correct, it seems like in principle one can get great results for a reasonably simple function $f$, such as a (for example) 3 layers fully connected NN. So one needs to update the parameters of this NN. On the other hand, ResNets can have tens or hundreds of layers. 
Am I missing a step here or is this new approach so powerful that with a lot fewer parameters one can get very good results? 
I feel like a ResNet, even with 2 layers, should be more powerful than Euler's Method ODE, as ResNets would allow more freedom in the sense that the 2 blocks don't need to be the same, while in the NeuralODE using Euler's Method one has the same (single) block. 
Lastly, I am not sure I understand what do they mean by (continuous) depth in this case. What is the definition of the depth here (I assume it is not just the depth of $f$)?
","['neural-networks', 'backpropagation']",
Why is the $\epsilon$ hyper-parameter (in the $\epsilon$-greedy policy) annealed smoothly?,"
As far as I understand, RL is a process that can be divided into 2 stages:

Exploring a wide range of paths (acting randomly)

Refining the current optimal paths (revolving around actions with a so-far most promising score estimate)


Completing 1. too quickly results in a network that just doesn't spot the best combination of actions, especially if rewards are sparse. ""Refining"" then has little benefit, since the network will tend to choose between unlucky estimates it observed so far, and will specialise in those.
On the other hand, finishing 2. too quickly results in a network that might have encountered the best combination, but never got time to refine these ""good trajectories"". Thus its estimates of scores along these ""good trajectories"" are rather poor and inaccurate, so again the network will fear to select and specialize those, because they might have a low (inaccurate) estimate.
Why not to give both 1. and 2. the maximum time possible?
In other words, instead of gradually annealing the $\epsilon$ coefficient (in the $\epsilon$-greedy) down to a low value, why not to always have it as a step function?
For example, train 50% of iterations with a value of 1 (acting completely randomly), and for the second half of training with the value of 0.05, etc (very greedy).  Well, 50% is a random guess, could be adjusted manually, as needed. The most important part is this ""step function"".
To me, always using such a ""step"" function would instantly reveal if the initial random search was not long enough.  Perhaps there is a disadvantage of such a step curve?
So far, I got the impression that annealing is a gradual process.
To me, it seems that when using gradual annealing it might not be evident if the neural network (e.q. in DQN or DQRNN) learns poorly because of the mentioned issue or something else.
Is there some literature exploring this?
There is a paper Noisy Networks for Exploration, but it proposes another approach that removes the $\epsilon$ hyperparameter. My question is different, specifically, about this $\epsilon$.
","['reinforcement-learning', 'dqn', 'hyper-parameters', 'exploration-exploitation-tradeoff', 'epsilon-greedy-policy']",
Why does GLIE+MC Control Algorithm use a single episode of Monte Carlo evaluation?,"
GLIE+MC control Algorithm:

My question is why does this algorithm use only a single Monte Carlo episode (during PE step) to compute the $Q(s,a)$? In my understanding this has the following drawbacks:

If we have multiple terminal states then we will only reach one (per Policy Iteration step PE+PI).
It is highly unlikely that we will visit all the states (during training), and a popular scheduling algorithm for exploration constant $\epsilon = 1/k$ where $k$ is apparently the episode number, ensures that exploration decays very very rapidly. This ensures that we may never visit a state during our entire training.

So why this algorithm uses single MC episode and why not multiple episodes in a single Policy Iteration step so that the agent gets a better feel of the environment?
","['reinforcement-learning', 'monte-carlo-methods']",
Scikit-Learn: monotoneous quantile estimation,"
I would like to implement various AI-estimators for quantile estimation for a regression problem. It would be necessary to have non-crossing quantiles, that is larger quantiles would correspond to higher prediction values. 
My objective is to have a multidimensional prediction vector as an output in the estimation, each dimension corresponding to a specific quantile. Maybe I would have to define a custom loss function, as well, for that purpose. I would like to try different methods such as deeplearning or gradient boosting or random forests.
Anyone having an idea how to build such AI-estimators? My preferred library choice would be scikit-learn.
Can someone give me an idea how to do this?
","['machine-learning', 'python', 'data-science']",
"Is there an AI model with ""certainty"" built in?","
If I see a hundred elephants and fifty of them are grey I'd say the probability of an elephant being grey is 50%. And my certainty of that probability is high.
However, if I see two elephants and one of them is grey. Still the probability is 50%. But my certaintity of this is low.
Are there any AI models where not only the probability is given by the AI but it's certainty is also?
""Certainty"" might be thought of as the probability that the probability is correct.
This could go up more levels. 
Is there any advantage in doing this?
One way I can envisage this working is instead of a weight, the NN stores two integers $(P,N)$ which represent positive and negative evidence and the weight is given by $P/(P+N)$. And each iteration $P$ or $N$ can only be incremented by 1. 
","['neural-networks', 'probability']","
You make a valid point, vanilla neural networks cannot give you more than a point estimate of class confidence. If one wanted to actually gain an idea of variance, you need a framework that allows such a mechanism.   
A popular methodology to this is Bayesian modeling. In other words given some data, $\Omega$, you want to create some form of descriminative model $p(y|x;\theta)$ where $\theta$ denotes the parameters that are random variables. This is unline NN's where they also learn some descriminative model $p(y|x;\theta)$ where $\theta$ are fixed. This difference is key to your goal, because if $\theta$ is fixed, $Var(Y|X) = 0$ since any time you put in the same $X$ into the network, itll always come out the same. On the other hand a bayesian model will have a variance, $Var(Y|X) = E[(Y - E[Y|X])^2 | X]$ and $(Y - E[Y|X])^2 | X$  is no longer gauranteed to be 0 and can be empirically measured through some MC method (or analytically based on your model).  
Note that your idea is on the right track, that the more data points you have, the more confident your estimates will be, but the only issue is that just considering $\frac{P}{(P+N)}$ in a neural network will be just a heuristic and is difficult to quantify its association to the variance.  
I think this may fit your fancy: high level overview of bayesian network blog post. This is a high level overview of using a neural netowrk paradigm but including uncertainty in the weights. Theres tons of tricks out there to make it so you can still even train these with gradient descent and such.
"
What models and algorithms are used in commercial vehicle re-identification tasks?,"
Due to the fast-growing applications of AI technologies applied to vehicle re-identification tasks, there have already been hot contests, such as the Nvidia AI challenge. 
What algorithms or models are really adopted in commercial vehicle re-identification tasks, effective and reliable, nowadays?
","['image-recognition', 'image-processing']",
Are neural networks prone to catastrophic forgetting?,"
Imagine you show a neural network a picture of a lion 100 times and label it with ""dangerous"", so it learns that lions are dangerous.
Now imagine that previously you have shown it millions of images of lions and alternatively labeled it as ""dangerous"" and ""not dangerous"", such that the probability of a lion being dangerous is 50%.
But those last 100 times have pushed the neural network into being very positive about regarding the lion as ""dangerous"", thus ignoring the last million lessons.
Therefore, it seems there is a flaw in neural networks, in that they can change their mind too quickly based on recent evidence. Especially if that previous evidence was in the middle.
Is there a neural network model that keeps track of how much evidence it has seen? (Or would this be equivalent to letting the learning rate decrease by $1/T$ where $T$ is the number of trials?)
","['neural-networks', 'machine-learning', 'incremental-learning', 'catastrophic-forgetting']","
Yes, the problem of forgetting older training examples is a characteristic of Neural Networks. I wouldn't call it a ""flaw"" though because it helps them be more adaptive and allows for interesting applications such as transfer learning (if a network remembered old training too well, fine tuning it to new data would be meaningless).
In practice what you want to do is to mix the training examples for dangerous and not dangerous so that it doesn't see one category in the beginning and one at the end.
A standard training procedure would work like this:
for e in epochs:
    shuffle dataset
    for x_batch, y_batch in dataset:
        train neural_network on x_batxh, y_batch

Note that the shuffle at every epoch guarantees that the network won't see the same training examples in the same order every epoch and that the classes will be mixed
Now to answer your question, yes decreasing the learning rate would make the network less prone to forgetting its previous training, but how would this work in a non-online setting? In order for a network to converge it needs multiple epochs of training (i.e. seeing each sample in the dataset many times).
"
Is it possible with stochastic gradient descent for the error to increase?,"
As simple as that. Is there any scenario where the error might increase, if only by a tiny amount, when using SGD (no momentum)?
","['neural-networks', 'gradient-descent', 'objective-functions']",
Is a neural network the correct approach to optimising a fitness function in a genetic algorithm?,"
I've written an application to help players pick the optimal heroes during the draft phase of the Heroes of the Storm MOBA. It can be daunting to pick from 80+ characters that have synergies/counters to other characters, strong/weak maps, etc. The app attempts to pick the optimal composition using a genetic algorithm (GA) based on various sources of information on these heroes. 
The problem I've realized is that not all sources of information are created equal. At the moment I'm giving all sources roughly equal importance in the fitness function but as I add other sources, I think it's going to be necessary to be more discerning about them. 
It seems like the right way to do this would be to use a single layer neural network where the weights of the synapses represent the weights in the fitness function. I could use matches played at a high-level (e.g. from MasterLeague.net) to form the training and test sets.
Does this sound like a viable approach or am I missing something simpler?  Is the idea of the using a GA even the correct way to approach this problem?
","['neural-networks', 'genetic-algorithms', 'optimization', 'fitness-functions']",
Is this a classification problem?,"
Im not really sure which machine learning approach is best for my problem at hand. I work in an engineering company that designs and builds different kinds of ships. In my particular job, I collect the individual weight of items on these vessels. The weight and there location is important because it is used to ensure the vessel in question can float in a balanced manner.
I have a large corpus of historical data on hand that lists the items on the vessel, there attributes, the weight for these items and where the weight came from (documentation), or the source. 
So, for example, lets say I have the following information:
 ITEM  |         ATTRIBUTES            | WEIGHT  |WEIGHT SOURCE
Valve  |Size: 1 inch |Type: Ball Valve |2 lbs.   |Database 1
Elbow  |Size: 2 inch |Type: Reducing   |1 lb.    |Database 2

I have to comb through many systems on these vessels and find the proper documentation or engineering drawings that lists the weight for the item in question. It usually starts by investigating the item and its attributes and then looking in a number of databases for the weight documentation. This takes a long time, as there is no organization or criteria as to what database has what. You just have to start randomly searching them and hope you find what you need. 
Well I now have a large corpus of real world data that lists thousands of items, there attributes, there weight and most importantly the source of the documentation (Database 1, 2, 3 etc.). Im wondering if there is any correlation between an item, its attributes and its weight location (database). This is where machine learning comes in. What Id like to do is use machine learning to help find the weight location more quickly. Ideally it would be nice if it could analyze a batch of information and then provide recommendations on which databases to search.
My first thoughts are that this is a classification problem, and maybe a CNN would be helpful here. If that is the case, I have over 100 categories in my dataset.
I actually went ahead and programmed a simple feed forward neural network using the following resources: https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/ I attempted to use this network to solve the above problem, but so far I have had no success. Im in over my head here. 
I dont expect it to be correct 100% of the time. Even if it had an 80% success rate that would be awesome. So my question is this;
What kind of neural network do I need to accomplish this? 
","['neural-networks', 'machine-learning', 'classification', 'data-mining']",
How can I develop this ML/AI system that I want to use in my new mobile app?,"
I have an idea for a new mobile app. Here is what I want to accomplish using AI;
I want to get an image (png format), (maybe just byte data too), from my application (I'm developing with Unity3D/C#), send this data to AI application; get modified image from ML app and send it back to my app.
What AI is going to do with img?
Imagine you are the user of my app; you are going to draw a picture on your phone.
Picture will be simple, like a seagull illustrated as an 'M' letter.
AI program will get your drawing ('M' in this case), check pixels to give a meaning to 'M'; then draw a more complex drawing that is themed around that M seagull.
(Like a drawing with an ocean made by Pixel Art, with rain clouds from Van Gogh and seagull is painted as a surreal bird...)

My general idea about building this AI system...
I'm not sure how to build this AI system, because I can't understand AI/ML completely. How it works on machine, how to implement it using computer, how to write an algorithm, how pre-made libraries like Tensorflow works... But I'm in a phase of my life that I need to use my time well, and want to build this app while learning.
I think, I can build an side app to use for analyzing and modifying image I get from user. Right know I can write in C, C# and learning JavaScript. I learned Python too (and a few others), but I'm not comfortable using it. (I hate Python). And didn't write a good program in any other language...
I thought, I can use JavaScript or Java || C++ but honestly I don't know how to start and which steps to take. Also after gaining some success I will want to port app to iOS too... Maybe that can wait...

Can you give me some examples, guidelines and advices? How to start doing this while deciding; what is best approach, performance and development-time wise?
And, Is my approach to the problem is a good one? Can you come up with a better solution for my idea that you can tell me to point me in another direction?
","['machine-learning', 'reinforcement-learning', 'image-processing']",
Models to extract Causal Relationship between entities in a document using Natural Language Processing techniques,"
I am looking to extract causal relations between entities like Drug and Adverse Effect in a document. Are there any proven NLP or AI techniques to handle the same.  Also are there ways to handle cases where the 2 entities may not necessarily co-occur in the same sentence. 
","['machine-learning', 'natural-language-processing', 'causation']",
How should I detect an object in a camera image?,"
I would like to create a model, that will tell me if one type of object is in an image or not. 
So, for example, I have a camera and I would like to see when one object gets into the shot.

Object detection: This could be an overkill, because I don't need to know the bounding box around. Also, this means that I would need to label a lot of images, and draw the bounding box to have train data (a lot of time)
Image classification: This doesn't solve the problem, because I don't know what else could not be an object. It would be impossible to train for 2 classes: object / not object. 

My idea is to have Autoencoder. Train it only on data with the object. Then, if Autoencoder produces a result with a high difference with the original, I detect it as an anomaly - no object. 
Is this a good approach? Will I have a lot of trouble with different backgrounds?
","['deep-learning', 'ai-design', 'autoencoders', 'object-detection']",
"What is ""dense"" in DensePose?","
I've recently come across an amazing work for human pose estimation: DensePose: Dense Human Pose Estimation In The Wild by Facebook. 
In this work, they have tackled the task of dense human pose estimation using discriminative trained models.
I do understand that ""correspondence"" means how well pixels in one image correspond to pixels in the second image (specifically, here - 2D to 3D).
But what does ""dense"" means in this case?
","['machine-learning', 'computer-vision', 'terminology', 'papers']",
"Can an AI simulate someone that is diagnosed as ""Special Needs""?","
Do you think it would be possible to train an AI in such a way as to mimic/simulate someone that is diagnosed as ""Special Needs""? 
Why? Most diagnosis and treatments for people today are subjective, sure it's what a group of like-minded professionals has agreed upon as a valid hypothesis, but, at the same time, there is an absence of the absolute. Could we train an AI to become a ""special needs"" be a starting point in helping find better ways to unlock the potential and understanding of these differences?
",['ai-design'],
"When doing binary classification with neural networks, how can I order the importance of the features for a class?","
I have a simple neural network for binary classification.
The input features include age, sex, economic situation, illness, disability, etc. The output is simply 1 and 0.
I would like to order the features from the greatest to least impact it had on the classification.
An example answer could look like this:
Classification: 1

illness
economic situation
disability
sex
age

Another example:
Classification: 0

economic situation
age
disability
sex
illness

","['neural-networks', 'classification', 'binary-classification', 'features', 'inference']","
Two popular methods Ive seen done:  
1) For each feature, remove it and run the model and see the impact it has on the result. The idea is that the larger the impact, the more pertinent it was to the result.   
2) Look at the gradients magnitude $|\nabla_f {y} |$. You can either look at the raw gradient or look at the guided back-propagation which is just the back props  product rule, but you only look at when the nodes positively help trigger a neuron by taking only the positive gradients at each step.
Theres probably also more methods. Hope this helped.
"
How should I build an AI that quickly detects falling game assets on screen?,"
I want to build an AI that plays a simple android game. 
The game is just a one at a time object falling, some times at an angle. The AI needs to recognize the object and to decide whether to swipe left, swipe down, or click on it. The background is changing some times, but the object falling is always on top.
There are 44 different assets and I have the original full resolution PNG of the objects.
How should I approach this?
","['machine-learning', 'ai-design', 'game-ai', 'object-recognition', 'object-detection']",
Can a vanilla neural network theoretically achieve the same performance as CNN?,"
I perfectly understand that CNN takes into account the local dependency of each pixel to the nearby pixels. In addition, CNNs are spatially invariant which means that they are able to detect the same feature anywhere in the image. These qualities are useful in image classification problems given the nature of the problem. 
How does a vanilla neural net exactly falls short on these properties? Am I right in claiming that a vanilla neural net has to learn a given feature in every part of the image? This is different than how a CNN does it, which learns the feature once and then detects it anywhere in the image. 
How about local pixel dependancy? Why can't a vanilla neural network learn the local dependency by relating one pixel to its neighbors in the 1D input? 
In other words, is there more information present while training a CNN that are simply absent when training a normal NN? Or is a CNN just better at optimizing in the space of image classification problems? 
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'comparison']",
Why do we need common sense in AI?,"
Let's consider this example:

It's John's birthday, let's buy him a kite.

We humans most likely would say the kite is a birthday gift, if asked why it's being bought; and we refer to this reasoning as common sense.
Why do we need this in artificially intelligent agents? I think it could cause a plethora of problems, since a lot of our human errors are caused by these vague assumptions.
Imagine an AI ignoring doing certain things because it assumes it has already been done by someone else (or another AI), using its common sense.
Wouldn't that bring human errors into AI systems?
","['philosophy', 'agi', 'knowledge-representation', 'commonsense-knowledge']","
We need this kind of common sense knowledge if we want to get computers to understand human language. It's easy for a computer program to analyse the grammatical structure of the example you give, but in order to understand its meaning we need to know the possible contexts, which is what you refer to as ""common sense"" here.
This was emphasised a lot in Roger Schank et al.'s work on computer understanding of stories, and lead to a lot of research into knowledge representation, scripts, plans, goals. One example from Schank's work is Mary was hungry. She picked up a Michelin Guide. -- this seems like a non-sequitur: if you are hungry, why pick up a book? Until you realise that it is a restaurant guide, and that Mary is presumably planning to go to a restaurant to eat. If you know that going to a restaurant is a potential solution to the problem of being hungry, then you have no problem understanding this story fragment.
Any story needs common sense to be understood, because no story is completely explicit. Common things are ""understood"" and aren't explicitly mentioned. Stories relate to human experience, and a story that would make everything explicit would probably read like a computer program. You also need common sense to understand how characters in a story behave, and how they are affected by what is happening. Again, this is very subjective, but it is necessary. Some common sense might be generally applicable, other aspects of it won't be. It's a complex issue, which is why researchers have struggled with it for at least half a century of AI research.
Of course this would introduce ""human errors"" into an AI system. All this is very subjective and culture-specific. Going to a restaurant in the USA is different from going to one in France -- this is why going abroad can be a challenge. And my reading of a story will probably be different from yours. But if you want to simulate human intelligence, you cannot do that without potential human ""errors"".
"
Can neuroevolution be combined with gradient descent?,"
Is there any precedent for using a neuroevolution algorithm, like NEAT, as a way of getting to an initialization of weights for a network that can then be fine-tuned with gradient descent and back-propagation? 
I wonder if this may be a faster way of getting to a global minimum before starting a decent to a local using backpropagation with a large set of input parameters.
","['machine-learning', 'genetic-algorithms', 'gradient-descent', 'neat', 'neuroevolution']","
Yes it can be in addition to the papers that nbro linked to above uber's ai research team has a very interesting combination of sgd and neuroevolution which they have dubbed ""safe mutations"". In the algorithm each genome undergoes a bit of sgd to improve its fitness before the speciation, elitism, and reproduction processes. I imagine this has an effect of searching for genomes which are well suited for sgd optimization, and in my opinion does really provide the best of both a worlds. Here is the link to the paper https://arxiv.org/abs/1712.06563 . What I think would be a cool for this combination of the two would be its use in conjunction with the es-hyperneat/hyperneat neuroevolution algorithms in which a small genome cppn encodes large phenotype rnns using the rnns substrate (its structure represented with cartesian coordinates) as the cppns input. If a small amount sgd is used on the rnn's to improve fitness then what you end up with is a cppn is being evolved to encode very general rnn networks that can then be optimized to specific domains via sgd. I like this because then your neuroevolution doesnt occur on a massive rnn and you can create cppns that recognize the general problem you wish to solve if your clever with your fitness evaluation.
"
Which loss function should I use for binary classification?,"
I plan to create a neural network using Python, Keras, and TensorFlow. All the tutorials I have seen so far are concerned with image recognition. However, the goal of my program would be to take in 10+ inputs and calculate a binary output (true/false) instead.
Which loss function should I use for my task?
","['neural-networks', 'machine-learning', 'classification', 'objective-functions', 'binary-classification']",
What are options in reinforcement learning?,"
According to a lecture (week 10) about Reinforcement Learning [1], the concept of an option allows searching the state space of an agent much faster. The lecture was hard to follow because many new terms were introduced in a short time. For me, the concept of an option sounds a bit like skills [2], which are used for describing high-level actions as well.
Are skills an improvement over options that includes the trajectory, or are both the same?
I'm asking for a certain reason. Normal deep reinforcement learning has the problem that the agent comes very often to a dead end, for example, in Montezuma's Revenge played at the Atari emulator. And the options framework promises to overcome the issue. But the concept sounds a bit too esoteric, and apart from the Nptel lecture, nobody else has explained the idea. So, is it useful at all?
","['reinforcement-learning', 'terminology', 'hierarchical-rl', 'semi-mdp', 'options']",
Which loss functions for transforming a density function to another density function?,"
I am looking at a problem which can be distilled as follows:  I have a phenomenon which can be modeled as a probability density function which is ""messy"" in that it sums to unity over its support but is somewhat jagged and spiky, and does not correspond to any particular textbook function.  It takes considerable amounts of time to generate these experimental density functions, along with conditional data for machine learning, but I have them.  I also have a crude model which runs quickly but performs poorly, i.e., generates poor quality density functions.
I would like to train a neural network to transform the crude estimated pdfs to something closer to the experimentally generated pdfs, if possible.
To investigate this, I've further reduced this to the most toy-like toy problem I can think of:  Feeding a narrow, smooth (relatively narrow) normal curve into a 1D convolutional neural network, and trying to transform it to a similar narrow curve with a different mean.  Both input and output have fine enough support (101 points) to be considered as a smooth pdf.
Here is the crux of the problem I think I have: I do not know what a good loss function is for this problem.
L1, L2 and similar losses are useless, given that once the non-zero parts of the pdfs are non-overlapping, it doesn't matter how far apart the means are, the loss remains the same. 
I have been experimenting with Sinkhorn approximations to optimal transport, to properly capture the intuition of ""distance"" but somewhat surprisingly these have not been helpful either.  I think part of the problem may be an (unavoidable?) numerical stability issue related to the support, but I would not stake hard money on that assumption.  
(If support is at percentiles on the [0,1] it is quite instructive (and dismaying) to look at the sinkhorn loss for normal functions with the mean directly on a point of support, vs normal functions with the mean directly between two points of support.)
For a problem in this vein, are there any recommended loss functions (preferably supported by or easily implement in PyTorch) which might work better?
","['ai-design', 'objective-functions', 'probability-distribution']",
How can non-functional neural networks be avoided when the crossover produces a child with a disabled gene?,"
I am implementing NEAT (neuroevolution of augmenting topologies) by Stanley. I am facing a problem during the crossover of genomes.
Suppose two networks with connections
Genome1 = {    
    (1, Input1, Output), // numbers represent innovation numbers
    (2, Input2, Output)    
} // more fit

Genome2 = {    
    (1, Input1, Output),
    (2, Input2, Output), // disabled
    (3, Input2, Hidden1),
    (4, Hidden1, Output)    
}

are crossed over, then the connection (Input2, Output) in the fitter parent has a chance of being disabled (page 109, section 3.2, figure 4),

There's a preset chance that an inherited gene is disabled if it is disabled in either parent.

and thus producing the following offspring:
Child = {
    (1, Input1, Output),
    (2, Input2, Output) //Disabled
}

and thus render the network non-functional.
Similarly, by this chance, nodes can get left in a state of uselessness after crossover (as having no outgoing connections or no connections at all).
How can this be prevented or am I missing something here?
","['neural-networks', 'neat', 'neuroevolution']",
Number of states in taxi environment (Dietterich 2000),"
Dietterich, who introduced the taxi environment (see p. 9), states the following: In total there are 500 [distinct] possible states: 25 squares, 5 locations for the passenger (counting the four starting locations and the taxi), and 4 destinations (Dietterich, 2000, p. 9).
However, in my opinion there are only 25 (grid) * 4 (locations) * 2 (passenger in car) = 200 different states, because for the agent it should be the same task to go to a certain point, regardless of whether it's on its way to pick up or to drop-off. Only the action at the destination is different which would be stored binary (passenger in car or not)
Why does Dietterich come up with 500 states? 
","['reinforcement-learning', 'combinatorics']","
This . . .

because for the agent it should be the same task to go to a certain point, regardless of whether it's on its way to pick up or to drop-off

. . . might seem logical/intuitive to a person understanding the task, but it is not mathematically correct. The agent cannot ""merge"" states because they involve the same behaviour. It must count differences in state as the combinations are presented. Critically, heading towards the passenger location or heading towards the goal location are not in any way similar to the agent, unless you manipulate the state to make them so*.
Eventually the taxi will learn very similar navigation behaviour for picking up and dropping off a passenger. However, using a basic RL agent it learns these very much separately, and must re-learn the navigation rules independently for each combination of passenger and goal location.
An agent that learned navigation within the environment, and then combined it into different tasks might be an example of hierarchical reinforcement learning, transfer learning, or curriculum learning. These are more sophisticated learning approaches, but it is quite interesting that even very basic RL problems can demonstrate a use for higher level abstractions. Most agents used on the taxi problem don't do this though, as 500 states is really very easy to ""brute force"" using the simplest algorithms.

* You could modify the state representation to rationalise the task and make it have less states, similar to your suggestion. For instance, have one ""target"" location which could either be pickup or drop off, and a boolean ""carrying passenger"" state component. That would indeed reduce the number of states. However, that has involved you as the problem designer simplifying the problem to make it easier for the agent. Given that this is a toy problem designed as a benchmark to see how different agents perform, by doing that you subvert the purpose of the environment. If you were creating an agent to work on a harder real world problem though, it might be a very good idea to look for symmetries and ways to simplify state representation which would speed up learning.
"
Can we automate the choice of the hyper-parameters of the evolutionary algorithms?,"
Certain hyper-parameters (e.g. the size of the offspring generation or the definition of the fitness function) and the design (e.g. how the mutation is performed) of evolutionary algorithms usually need to be defined or specified by a human. Could also these definitions be automated? Could we also mutate the fitness function or automatically decide the size of the offspring generation?
","['genetic-algorithms', 'evolutionary-algorithms', 'hyper-parameters', 'hyperparameter-optimization']",
Why did machine learning only become viable after Nvidia's chips were available?,"
I listened to a talk by a panel consisting of two influential Chinese scientists: Wang Gang and Yu Kai and others.
When being asked about the biggest bottleneck of the development of artificial intelligence in the near future (3 to 5 years), Yu Kai, who has a background in the hardware industry, said that hardware would be the essential problem and we should pay most of our attention to that. He gave us two examples:

In the early development of the computer, we compare our machines by their chips;
ML/DL which is very popular these years would be almost impossible if not empowered by Nvidia's GPU.

The fundamental algorithms existed already in the 1980s and 1990s, but AI went through 3 AI winters and was not empirical until we can train models with GPU boosted mega servers.
Then Dr. Wang commented to his opinions that we should also develop software systems because we cannot build an automatic car even if we have combined all GPUs and computation in the world together.
Then, as usual, my mind wandered off and I started thinking that what if those who can operate supercomputers in the 1980s and 1990s utilized the then-existing neural network algorithms and train them with tons of scientific data? Some people at that time can obviously attempt to build the AI systems we are building now.
But why did AI/ML/DL become a hot topic and become empirical until decades later? Is it only a matter of hardware, software, and data?
","['machine-learning', 'deep-learning', 'history']","
GPUs were ideal for AI boom because:

They hit the right time

AI has been researched for a LONG time. Almost half a century. However, that was all exploration of how algorithms would work and look. When NVIDIA saw that the AI is about to go mainstream, they looked at their GPUs and realized that the huge parallel processing power, with relative ease of programing, is ideal for the era that is to be. Many other people realized that too.

GPUs are sort of general purpose accelerators

GPGPU is a concept of using GPU parallel processing for general tasks. You can accelerate graphics, or make your algorithm utilize 1000s of cores available on GPU. That makes GPU awesome target for all kinds of use cases including AI. Given that they are already available and are not too hard to program, its ideal choice for accelerating AI algorithms.
"
How does the initialization of the value function and definition of the reward function affect the performance of the RL agent?,"
Is there any empirical/theoretical evidence on the effect of initial values of state-action and state values on the training of an RL agent (the values an RL agent assigns to visited states) via MC methods Policy Evaluation and GLIE Policy Improvement?
For example, consider two initialization scenarios of Windy Gridworld problem:
Implementation: I have modified the problem along with step penalty to include a non-desired terminal state and a desired terminal state which will be conveyed to the agent as a negative and positive reward state respectively. The implementation takes care that the MC sampling ends at the terminal state and gives out penalty/reward as a state-action value and not state value, since this is a control problem. Also, I have 5 moves: north, south, east, west and stay.
NOTE: I am not sure whether this changes the objective of the problem. In the original problem, it was to reduce the number of steps required to reach the final stage.

We set the reward of reaching the desired terminal state to a value that is higher than the randomly initialized values of the value function; for example, we can set the reward to $20$ and initialize the values with random numbers in the range $[1, 7]$

We set the reward of reaching the desired terminal state to a value that is comparable to the randomly initialized values of the value functions; for example, we can set the reward to $5$ and initialize the values with random numbers in the range $[1, 10]$


As far as I see, in the first case, the algorithm will easily quickly converge as the reward is very high for the terminal reward state which will skew the agent to try to reach the reward stage.
In the second case, this might not be true if the reward state is surrounded by other high reward states, the agent will try to go to those states.
The step penalty ensures that the agent finally reaches the terminal state, but will this skew the path of the agent and severely affect its convergence time? This might be problematic in large state spaces since we will not be able to explore the entire state space, but the presence of exploratory constant $\epsilon$ might derail the training by going to a large false reward state. Is my understanding correct?
","['reinforcement-learning', 'value-functions', 'reward-design', 'reward-functions', 'weights-initialization']",
Is there a place where I can read or watch to get an accurate TensorFlow code wise explanation?,"
I have a piece of code and I don't seem to really understand it but I'd love to get a source/link/material that would help me understand the basic functions in TensorFlow. Are there any recommended resources for learning the same?
","['neural-networks', 'tensorflow', 'resource-request']","
The best resource for learning TensorFlow 1.9 and earlier is this course by Stanford. Also additional resources for the entire overview of TensorFlow and its comparisons with NumPy has been made in this video. For hands on models check these videos by Sentdex and also some high level tutorials by Hvass Labs.
"
"Paper & code for ""unsupervised domain adaptation"" for regression task","
Does anyone know a paper or code that does ""unsupervised domain adaptation"" for regression task?
I saw most of the papers were benchmarked on classification tasks, not regression.
I want to do something like training a model to predict a scalar value from an image (e.g. predicting image of a road to steering wheel angle for a self-driving car).
One of the examples could be training on synthesis data from simulated environment (think GTA) and then trying to predict on real-world.
Here is one of the examples of unsupervised domain adaptation algorithm that also has an easy-to-access code with Keras: https://github.com/bbdamodaran/deepJDOT
But it's for classification. The author said it can be used for regression but I had to change it. I changed it and it didn't work well so I don't know if it's my fault or the algo is not good for regression. I want to see papers that were benchmarked on regression so I know how well it performs on regression.
My real use case is to predict facial expression as a value from 0 to 1 like how open is the mouth. The source domain and target domain are real-world images but from different lighting.
Any suggestions are appreciated.
","['deep-learning', 'keras', 'transfer-learning']",
Is there a simple way of classifying images of size differing from the input of existing image classifiers?,"
Most image classifiers like Inception-v3 accept images of about size 299 x 299 x 3 as input. In this particular case, I cannot resize the image and lose resolution. Is there an easy solution of dealing with this rather than retraining the model? (Particularly in tensorflow)
","['convolutional-neural-networks', 'image-recognition', 'tensorflow', 'models']","
My guess:
If you add a few CNN layers before the input of the given model and train only those layers while keeping the given model's parameters frozen, you might get better result.
Essentially these few extra layers would ""transform"" your input image into the appropriate shape, but with more accuracy since its trained and not hard coded.
"
How are the parameters of the Bernoulli distribution learned?,"
In the paper Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask, they learn a mask for the network by setting up the mask parameters as $M_i = Bern(\sigma(v_i))$. Where $M$ is the parameter mask ($f(x;\theta, M) = f(x;M \odot \theta$), $Bern$ is a Bernoulli sampler, $\sigma$ is the sigmoid function, and $v_i$ is some trainable parameter.
In the paper, they learn $v_i$ using SGD. I was wondering how they managed to do that, because there isn't a reparameterization trick, as there is for some other distributions I see trained on in the literature (example: normal).
","['machine-learning', 'probability-distribution', 'weights']",
Is it possible to use adversarial training to learn invariant features?,"
Given a set of time series data that are generated from different sites where all sites are investigating the same objective but with slightly different protocols. 
Is it possible to use adversarial learning to learn site invariant features for a classification problem, that is, how can adversarial learning be used to minimize experimental differences (e.g. different measurement equipment) so that the learned feature representations from the time series are homogenous for a classification problem?
I have come across multi-domain adversarial learning, but I'm not sure if this is the best formulation for my problem.
","['neural-networks', 'deep-learning', 'generative-adversarial-networks', 'time-series', 'representation-learning']",
Metrics for evaluating models that output probabilities,"
I'm aware of metrics like accuracy (correct predictions / total predictions) for models that classify things. However, I'm working on a model that outputs the probability of a datapoint belonging to one of two classes. What metrics can/should be used to evaluate these types of models?
I'm currently using mean squared error, but I would like to know if there are other metrics, and what the advantages/disadvantages of those metrics are.
","['cross-validation', 'metric']","
For a binary classifier, the cross-entropy loss is a natural measure of probability accuracy, if you care about relative probabilities. By that I mean if you care that the estimate $\hat{p}$ is within some ratio of the true value. So an estimate of $\hat{p} = 0.1$ is a better estimate if the true value is $p = 0.2$ than if the true value is $p = 0.01$ (even though the latter value is closer and would score better under MSE). It also applies that you care that 0.9 is by the same logic ""closer"" to 0.8 than it is to 0.97. With cross-entropy loss, extreme confidence (predicting close to $0$ or close to $1$) is penalised more heavily when it is wrong.
For completeness, the loss function (per data point) is:
$$\mathcal{L}(\hat{y},y) = -(y\text{log}(\hat{y})+ (1-y)\text{log}(1-\hat{y}))$$
This is likely to be the same loss function as you are using for your objective (or at least it should be), so for tests, simply also use it as your metric*.
$\hat{y}$ is your predicted probability of being in class A, and can be in range $[0,1]$. Ideally you have ground truth probabilities and $y$ is also in that range. In which case the only problem is that the ""perfect"" score is no longer $0$ but some positive number. If that bothers you, then you could offset by the perfect score, pre-calculating it on each data set (just set $\hat{y} = y$ for each item and you will find the minimum possible score).
If you don't have ground truth probabilities, but you do have classes, then $y$ will either be 0 or 1, and the metric still works. To get an accurate metric, you will need enough samples that the relative frequencies of each class depending on input has a significant effect. That is, you need more data, both training and test, in order to train for accurate probabilities instead of targeting simpler classification accuracy metrics.
Similar logic also works for multi-class probabilities. However, many off-the-shelf libraries use an optimisation in the loss function - assuming only one true class - which makes using probabilities as ground truth impossible. You might therefore need to write your own loss function and gradient functions based on multi-class cross-entropy loss in that case.

* I am making the assumption here that you use standard conventions for noting loss functions (typically per item), cost functions (typically aggregated across a data set and possibly multiple loss functions) and metric functions which don't have to be differentiable or usable as either of the former. The cost function is usually also fed into optimisers as the objective function - i.e. it has the important job of driving parameters to reach a maximum or minimum value. For gradient based solvers, that means it must be differentiable.
"
Is there any readily available concept/topic tree?,"
I am looking for dataset in tree structure that captures the hierarchy of concepts.
For example, something like,
                         Entertainment
                   Movies           Sports
              Comedy  Thriller   cricket football
   charlie chaplin              sachin       messy

",['knowledge-representation'],
Inverting intensity on images to enhance image dataset,"
i just tried to improve my image dataset by inverting the images with a probability of 50% (means white background, black features transforms to black background, white features)
I thought this will improve the ability to recognize abstract features for my network. Right now, the network does not perform really well. Is inverting the intensity of images too much for a training algorithm to deal with? 
","['deep-learning', 'convolutional-neural-networks']",
Reinforcement learning to play snake - network seems to not get trained at all,"
I am trying to build a network able to play snake game. This is my very first attempt to do such stuff. Unfortunately, I've stuck and even have no idea how to reason about the problem.
I use reinforcement neural network approach (q-leaning). My network is built on top of Keras. I use 6 input neurons for my snake:

1 - is any collision directly behind
2 - is any collision directly on the right
3 - is any collision directly on the left
4 - is snack up front (no matter how far)
5 - is a snack on the right side (no matter how far)
6 - is a snack on the left side (no matter how far)

the output has 3 neurons:

1 - do nothing (go ahead)
2 - turn right
3 - turn left

I believe this is a sufficient set of information to make proper decisions. But the snake seems to not even grasp the concept of not hitting the wall - which results with instant death.
I use the following rewards table:

100 for getting the snack
-100 for hitting wall/tail
1 for staying alive (each step)

Snake tends to run randomly no matter how many training iterations it gets. 
The code is available on my github: https://github.com/ayeo/snake/blob/master/main.py
","['neural-networks', 'python', 'keras']",
Applying a 1D convolution for 4D input,"
i'm trying to implement this paper and I'm stuck for quite some time now. Here is the issue: 
I have a 3D tensor and has  (180,200,20) as dimension and I'm trying to append 5 of them as the paper states:

Now that each frame is represented as a 3D tensor, we can append multiple frames along a new temporal dimension to create a 4D tensor

what I did is I applied the tensorflow command tf.stack() and now so far so good, I have my input as a 4D tensor and has (5,180,200,20) as stated in the paper:

Thus our input is a 4 dimensional tensor consisting of time, height, X and Y

Now what I'm trying to do is to apply a 1D convolution on this 4D tensor as the paper mentions:

given a 4D input tensor, we first use a 1D convolution with
  kernel size n on temporal dimension to reduce the temporal dimension from n to 1

is this case n = 5.
And here where I got stuck, I created the kernel as follow:
kernel = tf.Variable(tf.truncated_normal([5,16,16], dtype = tf.float64, stddev = 1e-1, name = 'weights'))
and tried to apply a 1D convolution:
conv = tf.nn.conv1d(myInput4D, kernel, 1 , padding = 'SAME')
and I get this error
Shape must be rank 4 but is rank 5 for 'conv1d_42/Conv2D' (op: 'Conv2D') with input shapes: [5,180,1,200,20], [1,5,16,16]
I don't understand how 1 is added to the dimensions at the index = 2 and index = 0 in the first and second tensors.
I also tried this:
conv = tf.layers.conv1d(myInput4D, filters = 16, kernel_size = 5, strides = 1, padding = 'same)
And get the following error:
Input 0 of layer conv1d_4 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [5, 180, 200, 20]
My question is: Is it possible to apply a 1D convolution on a 4D input and if yes can anyone suggests a way to do so? Because in the Tenssorflow documentations it says the input must be 3D

tf.nn.conv1d(
      value=None,
      filters=None,
      stride=None,
      padding=None,
      use_cudnn_on_gpu=None,
      data_format=None,
      name=None,
      input=None,
      dilations=None
  ) 
value: A 3D Tensor. Must be of type float16, float32, or float64.

Thank you.
","['deep-learning', 'convolutional-neural-networks', 'python', 'computer-vision', 'tensorflow']",
What are the advantages of time-varying graph CNNs compared to fixed graph?,"
As I wrote in the title, what are the advantages of time-varying graph CNNs compared to fixed graph? For example, in CORA, which is a graph of citation relations of papers frequently used in graph CNN, what examples are there?
","['machine-learning', 'convolutional-neural-networks', 'graphs', 'geometric-deep-learning', 'graph-theory']",
Can I train a neural network incrementally given new daily data?,"
I would like to know if it was possible to train a neural network on daily new data. Let me explain this more in detail. Let's say you have daily data from 2010 to 2019. You train your NN on all of it, but, from now on, every day in 2019 you get new data. Is it possible to ""append"" the training of the NN or do we need to retrain an entire NN with the data from $2010$ to $2019+n$ with $n$ the day for every new day?
I don't know if it is relevant but my work is on binary classification. 
","['neural-networks', 'machine-learning', 'classification', 'incremental-learning']","
Yes, this is possible. Continuously extending your training data is known as incremental learning. 
You might also want to take a look at transfer learning, in which you reuse a trained model for a different purpose. This is very useful if you have a smaller dataset. 
In your particular case, you could train a NN once using your data from 2010 to 2019 and use it as a base model. Every time you get new data, you can use transfer learning to slightly re-train this model. Based on parameters such as the number of epochs and the learning rate, you can determine how much of an impact this new data will have. 
"
"Binary annotations on large, heterogenous images","
I'm working on a deep learning project and have encountered a problem. The images that I'm using are very large and extremely detailed. They also contain a huge amount of necessary visual information, so it's hard to downgrade the resolution. I've gotten around this by slicing my images into 'tiles,' with resolution 512 x 512. There are several thousand tiles for each image. 
Here's the problemthe annotations are binary and the images are heterogenous. Thus, an annotation can be applied to a tile of the image that has no impact on the actual classification. How can I lessen the impact of tiles that are 'improperly' labeled.
One thought is to cluster the tiles with something like a t-SNE plot and compare the ratio of the binary annotations for different regions (or 'classes'). I could then assign weights to images based on where it's located and then use that as an extra layer in my training. Very new to all of this, so wouldn't be surprised if that's an awful idea! Just thought I'd take a stab.
For background, I'm using transfer learning on Inception v3.
","['deep-learning', 'convolutional-neural-networks', 'classification', 'training', 'transfer-learning']",
Is DDPG just for deterministic environments?,"
I want to develop an AI for continuous space. I reached to DDPG algorithm that takes actions deterministically. 
If DDPG takes actions deterministically, should the environment also be deterministic? I want non-deterministic, continuous real-world environments. Is DDPG the algorithm I am looking for? Is there any other algorithm for my need?
","['deep-learning', 'reinforcement-learning', 'dqn', 'deep-rl', 'ddpg']","
I am not an expert in this area. But I believe that the word ""Deterministic"" is for ""Policy"" in the ""Deterministic Policy"" Gradient. It does not mean  deterministic environment.
Stochastic policy: Probabilistic(random) action choice for a given state.
Deterministic policy: one action is chosen for a given state. 
Deterministic Policy Gradient algorithm still can handle a stochastic (and continuous, of cause) environment, but the policy will be deterministic. 
Reference  
""in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space""  -towards Data Science 
Deterministic Policy Gradient Algorithms by Silver et al. PDF 
"
Why do neural networks have bias units?,"
Why do neural networks have bias units? Why is it sometimes okay to opt them out?
","['neural-networks', 'machine-learning', 'bias']",
How can I train a neural network to give probability of a random event?,"
Let's say I have an adjustable loaded die, and I want to train a neural network to give me the probability of each face, depending on the settings of the loaded die.
I can't mesure its performance on individual die roll, since it does not give me a probability.
I could batch a lot of roll to calculate a probability and use this batch as an individual test case, but my problem does not allow this (let's say the settings are complex and randomized between each roll).
I have 2 ideas:

train it as a classification problem which output confidence, and hope that the confidence will reflect the actual probability. Sometimes the network would output the correct probability and fail the test, but on average it would tend to the correct probability. However it may require a lot of training and data.
batch random rolls together and compare the mean/median/standard deviation of the measured result vs the predictions. It could work but I don't know the good batch size.

Thank you.
","['neural-networks', 'machine-learning']","
The starting point is that for a fair dice thrown fairly the p(n) is 1/n where n is the number of sides.
You said both
and

there are too many variables (up 40 dimensions with value range 1-100) in input, I don't know how these properties relate and an empirical approach would require too much data.

It seems that this problem has 2 solutions:

Don't use a neural net and create a 'std' statistical model.
It may be possible since you said:


I know there is some underlying rule that simplify a lot the problem (ie. reduce the actual number of dimension of the input)


Use a neural network (with softmax at the end) - for a fair dice; with enough training data the classifier should arrive as 1/n as the approximating function for a fair dice. The other 40 dimensions/settings your mentioned are the inputs. I think a 'basic' neural network with dense layers only could work for your task.

"
Why doesnt my lstm model for time series prediction improve after certain level of performance?,"
I created an lstm model which predicts multioutput sequeances. It takes variable length sequences as input. These sequences are padded with zero to obtain equal length. Note that the time series are not equally spaced but time stamp is added as predictor. The descrete time series predictors are normalized with mean and standard deviation and run through PCA, the categorical feature is one hot encoded and the ordinal features are integer encoded. So the feature set is a combination of dynamic and static features. The targets are also scaled between [-1,1]. The input layer goes through a masked layer and then to 2 stacked lstm layers and dense layer to predict the targets (see code below).
Training actually starts well but then the performance starts to saturate. It seems the network also focus more on the 3rd output rather than the first two. This is seen in the validation curve of the third output that follows the training curve perfectly. For the first 2 outputs, the network has a hard time predicting some peak values. I have been tuning the hyper parameters but validation error does not go below a certain value. The longer I train the more the validation curve and training curve separate from each other and overfitting occurs on the first 2 outputs. I tried all the standard initializations and he_initialization seems to work the best. When more data is added there is a slight improvement in validation error but not significant. When adding dropout the validation error is lower than the training error due to noise introduced by dropout in feed forward but there is no significant improvement. Since neural networks tend to converge close to where they are initialized, I was thinking my initialization is not good.
I was wandering if anyone had any suggestions on how to improve the error of this model. I think I will be happy if I can get the validation error somewhere around 0.01.




def masked_mse(y_true, y_pred):
    mask = keras.backend.all(keras.backend.not_equal(y_true, 0.), axis=-1, keepdims=True)

    y_true_ = tf.boolean_mask(y_true, mask)
    y_pred_ = tf.boolean_mask(y_pred, mask)

    return keras.backend.mean(keras.backend.square(y_pred_ - y_true_))

def rmse(y_true, y_pred):
    # find timesteps where mask values is not 0.0
    mask = keras.backend.all(keras.backend.not_equal(y_true, 0.), axis=-1, keepdims=True)

    y_true_ = tf.boolean_mask(y_true, mask)
    y_pred_ = tf.boolean_mask(y_pred, mask)

    return keras.backend.sqrt(keras.backend.mean(keras.backend.square(y_pred_ - y_true_)))

hl1 = 125
hl2 = 125
window_len = 30
n_features = 50
batch_size = 128

optimizer = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0., amsgrad=False)
dropout = 0.
input_ = keras.layers.Input(
        shape=(window_len, n_features)
    )

# masking is to make sure the model doesn't fit the zero paddings
masking = keras.layers.Masking(mask_value=0.0)(input_)

# hidden layer 1 with he_normal initializer. 

lstm_h1 = keras.layers.LSTM(hl1, dropout=dropout, kernel_initializer='he_normal',
                        return_sequences=True)(masking)

# hidden layer 2
lstm_h2 = keras.layers.LSTM(hl2, dropout=dropout, kernel_initializer='he_normal',
                        return_sequences=True)(lstm_h1)

 # dense output layer of single output
 out1 = keras.layers.Dense(1, activation='linear',name='out1')(lstm_h2)

 out2 = keras.layers.Dense(1, activation='linear', name='out2')(lstm_h2)

 out3 = keras.layers.Dense(1, activation='linear', name='out3')(lstm_h2)

 model = keras.models.Model(inputs=input_, outputs=[out1, out2, out3])


 pi = [rmse]

 n_gpus = len(get_available_gpus())

 if n_gpus > 1:
    print(""Using Multiple GPU's ..."")
    parallel_model = multi_gpu_model(model, gpus=n_gpus)

else:
    print(""Using Single GPU ..."")
    parallel_model = model


 parallel_model.compile(loss=masked_mse, optimizer=optimizer, metrics=pi)
 parallel_model.summary()

 checkpoint = keras.callbacks.ModelCheckpoint(
             file_name+"".hdf5"", monitor='val_loss', verbose=1, 
             save_best_only=True, mode='min', period=10,
             save_weights_only=True)

 save_history = keras.callbacks.CSVLogger(file_name+"".csv"", append=True)

 callbacks_list = [checkpoint, save_history]

 y_train_reshaped = list(reshape_test(window_len, y_train))
 parallel_model.fit(
            x_train,

            {
                 'out1': y_train_reshaped[0],
                 'out2': y_train_reshaped[1],
                 'out3': y_train_reshaped[2],
            },

            epochs=epochs,
            batch_size=batch_size,
            verbose=0,
            shuffle='batch',
            validation_data=(x_test, list(reshape_test(window_len,y_test))),
            callbacks=callbacks_list,

        )

","['neural-networks', 'recurrent-neural-networks', 'keras', 'long-short-term-memory', 'hyperparameter-optimization']",
Reducing the Number of Training Samples for collaborative filtering in recommender systems,"
I have the following problem: I am doing some research on the accuracy of recommender algorithms that are mostly used nowadays. 
So, one way to measure their performance is by checking how well they predict a certain value under different sizes of a given dataset, meaning, sparsity in a ratings matrix. 
I need to find a way to calculate the root mean square error(or mae), some metric, versus the sparsity in the dataset. As an example, lets have a look at the picture below:

You can see that it says:

RMSE as a function of sparsity. 5000 ratings were removed from the training set(initially containing 80000 ratings) in every iteration. 

Im using Python and the Movielens dataset. Do you know how can I achieve this in the mentioned language? Is there any tool to do that? 
","['machine-learning', 'recommender-system']",
Is there a neural network method for time-varying directed graphs?,"
I want to study NN for time-varying directed graphs. However, as this field has developed relatively recently, it is difficult to find new ways. So the question is, is there any NN that can handle such data?
","['neural-networks', 'machine-learning', 'deep-learning', 'graphs', 'geometric-deep-learning']",
"In sequence-to-sequence, why is the output of the decoder used as its input?","
The basic seq-2-seq model consists of 2 parts: a recurrent encoder that compresses a sequence to a vector and decoder that unrolls the vector into the output sequence:

Why is the output, w, x, y, z of the decoder used as its input? Shouldn't the hidden state of the RNN from the previous timestamps be enough?
","['recurrent-neural-networks', 'sequence-modeling']","
In seq2seq they model the joint distribution of whatever char/word sequence by decomposing it into time-forward conditionals:
\begin{align*}
p(w_1,w_2, \dots,w_n) &= \ p(w_1)*p(w_2|w_1) * \ ... \ * p(w_n|w_1, \dots,w_{n-1}) \\
 &= \ p(w_1)*\prod_{i=2}^{n}p(w_i|w_{<i})
\end{align*}
This can be sampled by sampling each of the conditional in ascending order. So, that's exactly what they're trying to imitate. You want the second output dependant on the sampled first output, not its distribution.
This is why the hidden state is NOT good for modeling this setup because it is a latent representation of the distribution, not a sample of the distribution.
Note: In training, they use ground-truth as input by default because it working under the assumption the model should've predicted the correct word, and, if it didn't, the gradient of the word/char level loss will reflect that (this is called teacher forcing and has a multitude of pitfalls).
"
Why is graph convolution network in time-varying graphs useful for anomaly detection?,"
In this paper, the authors refer to the application of time-varying graphs as an open problem.And they say it will be useful for anomaly detection in financial networks, etc. But why is that useful?
","['neural-networks', 'convolutional-neural-networks', 'applications', 'geometric-deep-learning']",
Does AlphaZero use Q-Learning?,"
I was reading the AlphaZero paper Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm, and it seems they don't mention Q-Learning anywhere.
So does AZ use Q-Learning on the results of self-play or just a Supervised Learning?
If it's a Supervised Learning, then why is it said that AZ uses Reinforcement Learning? Is ""reinforcement"" part primarily a result of using Monte-Carlo Tree Search?
","['reinforcement-learning', 'q-learning', 'monte-carlo-tree-search', 'supervised-learning', 'alphazero']",
Product Configuration based on user selection of features and other requirements,"
Is this a scenario that would work well for a ML/Pattern Recognition Model or would it be easier/faster to just filter from a large DB.
I am looking to create a system that will allow users to identify the appropriate product by specifying certain constraints and preferred features.
There are millions of possible product configurations. Lets pretend it's boxes.
Product Options:

Size (From 1mm up to 1m) in 1mm increments
Color: choice of 10 colors
Material: choice of 3, wood,metal, plastic

Constraints:

Wood is only available in centimeter units
Red is only available in 500 mm and greater
Wood is the preferred material
Blue is the preferred color

So, we have 30,000 (1000*10*3) possible options.
Of those, many are not viable such as 533 mm-Red-Wood
but these configurations similar to the request are possible.

533 mm-Red-Plastic 
530 mm-Red-Wood 
540 mm-Red-Wood

Notes:
Our current Rules and code based tool can take anywhere from 0.5 to 2 mins to identify the preferred configuration.
We can generate a list of all possible configs and whether they are valid or not.
We estimate 30,000,000 possible configs
It takes around 0.5 seconds to validate a config so with enough computing power we expect we could do 30M in a few days.
","['machine-learning', 'classification', 'pattern-recognition']",
How to compute the number of centroids for K-means clustering algorithm given minimal distance?,"
I need to cluster my points into unknown number of clusters, given the minimal Euclidean distance R between the two clusters. Any two clusters that are closer than this minimal distance should be merged and treated as one.
I could implement a loop starting from the two clusters and going up until I observe the pair of clusters that are closer to each other than my minimal distance. The upper boundary of the loop is the number of points we need to cluster.
Are there any well known algorithms and approaches estimate the approximate number of centroids from the set of points and required minimal distance between centroids?
I am currently using FAISS under Python, but with the right idea I could also implement in C myself.
","['machine-learning', 'k-means', 'clustering']","
If you look at Kaufman & Rousseeuw (1990), Finding Groups in Data, they describe an algorithm to evaluate the quality of clusters in agglomerative clustering. You run the clustering algorithm with a specific value k for the number of clusters you want, and that routine then gives you a score to reflect the cohesion of the clustering. If you then cluster again with a different value for k, you will get another score. You repeat this process until you have found a maximum score, and then you have the clustering with the optimum number of clusters.
"
How does the CTC loss work?,"
I am trying to implement CTC loss in TensorFlow, but their documentation is pretty limited. So I am not sure how to approach the problem. I found a good example in Theano.
Are any other resources that explain the CTC loss?
I am also trying to understand how its forward-backward algorithm works and what the beam decoder in the case of the CTC loss is.
","['reference-request', 'tensorflow', 'recurrent-neural-networks', 'speech-recognition', 'ctc-loss']","
Connectionist Temporal Classification (CTC) can be useful for sequence modeling problems, like speech recognition and handwritten recognition, where the input and output sequences might have different sizes, so there's the problem of aligning the sequences. For instance, in speech recognition, not all sounds in speech correspond to a character, so how do we know if a sound should be converted to one of the possible chars? Moreover, we assume that we don't have a training dataset where the input and output sequences are aligned. The creation/labeling of such a dataset would be quite consuming. That's why CTC was introduced.
So, mathematically, the speech/handwritten recognition task can be written as
$$
Y^* = \text{argmax}_Y p(Y \mid X),
$$
where $Y^* = [y_1, \dots, y_M]$ is the ideal output sequence for $X = [x_1, \dots, x_T]$. Note that $T$ may be different from $M$.
Now, let's say that you have a speech $X = [x_1, \dots, x_N]$ and the output should be the sequence $Y^* = [h, e, l, l, o]$. One naive approach to solve this problem would be, for each input $x_i$, we would predict the most likely char, so we could end up with an output sequence like $\hat{Y} = [h, e, e, l, l, l, o]$ (when $T = 7$ and $M = 5$), then we could remove all duplicates, so we would end up with $\hat{Y}' = [h, e, l, o]$. However, this is not the correct approach, because, as you can see, there are words where the same letter appears twice in a row.
To solve this problem, we can introduce a special character, which we can denote by $\epsilon$. So, in this case, the idea is that $\epsilon$ should be predicted around exactly two (and not more or less) ""l"". Once the sequence is predicted, we can remove $\epsilon$ and, hopefully, we have a valid word, rather than a word like ""helo"" or ""hellllo"".
The idea of CTC is, for each $x_i$, the neural network produces a probability distribution over the possible chars, which we can denote by $p_t(a_t \mid X)$. In the example above, a probability distribution over $\{h, e, l, o, \epsilon \}$. So, for example, the probability vector for $x_1$ could be $p_1(a_1 \mid X) = [0.6, 0.1, 0.1, 0.1, 0.1]$. Given all probability vectors $p_t(a_t \mid X)$, for $t=1, \dots, T$, we can compute the probability of an alignment (a specific output sequence). Then we marginalize over the set of alignments.
To reflect these ideas, the CTC loss function is defined as follows
$$
p(Y \mid X)=\underbrace{\sum_{A \in \mathcal{A}_{X, Y}} }_{\text{Marginalization}\\\text{over set of}\\ \text{alignments}} \underbrace{\prod_{t=1}^{T} p_{t}\left(a_{t} \mid X\right)}_{\text{Probability}\\\text{for a single}\\\text{alignment}}
$$
We can then use an RNN to model $p_t(a_t \mid X)$ given that RNNs are good for sequence prediction.
Now, another problem is that there can be many alignments, so the computation of the loss may be expensive if done naively. To solve this problem, you can use a dynamic programming algorithm, the CTC forward-backward algorithm. The details of how this is done, how the gradient of the CTC loss is computed, how inference is done in this context (including the details of beam search), and other details can be found in this nice article Sequence Modeling With CTC (2017) by Awni Hannun, which this answer is based on.
You can also read the original paper Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with Recurrent Neural Networks (2006), by Alex Graves et al., mentioned in the linked TensorFlow documentation, which presents and explains the CTC loss and the CTC forward-backward algorithm (in section 4.1).
"
Examples of time-varying graph-structured data in real world,"
I'm looking for examples of time-varying graph-structured data for time-varying graph CNNs. First, I came up with the idea of infection network. Is there anything more? If possible, I want data that can be easily obtained online.
","['machine-learning', 'convolutional-neural-networks', 'datasets', 'geometric-deep-learning']",
Why does the DQN not converge when the start or goal states can change dynamically?,"
I'm trying to apply a DQN to a stochastic environment, but I'm having trouble getting it to converge.
I found some similar questions asked here, but no solutions yet.
I can easily get the DQN to converge on a static environment, but I am having trouble with a dynamic environment where the end is not given.
Example: I have made a really simple model of the Frozen Lake (without any holes) - simply navigation from A to B. This navigation works fine when A and B are always the same, but when I shuffle the position of A or B for each session, the DQN cannot converge properly.
I am using the grid (3x3, 4x4 sizes) as input neurons. Each with ""0"" value. I assign the current position ""0.5"" and the end position ""1"". 4x4 grid gives us 16 input neurons. Example of 3x3 grid:
 0.5  0  0 
  0   0  0 
  0   0  1

I have a few questions in this regard:

When training the DQN, how do I apply Q-values? (Or do I really need to? I'm not sure how to correctly ""reward"" the network. I'm not using any adversarial network or target network at this point.)

I train the network using only a short replay memory of the last move, or the last N moves that led to success. Is this the right way to approach this?

I use Keras, and am simply training the network every time it does something right - and ignoring failed attempts. - But is this anywhere near the right approach?

Am I missing something else?


Perhaps I should note that my math skills are not that strong, but I try my best.
Any input is appreciated.
","['deep-learning', 'reinforcement-learning', 'python', 'dqn']",
Is there any useful source on High Bias vs High variance issue on Neural Network?,"
I've been struggling to analyize my NN model. I've studied through andrew ng's course, but there are some results that cannot be explained by the course. Is there any useful source on  High Bias vs High variance issue on NN?
",['neural-networks'],
How to voxelize multiple frames at the time and append them together?,"
I'm trying to implement this approach for object detection and tracking.
In this approach, the first step is voxelize each frame to construct a 3D tensor, the second step is to append multiple voxels at the time along a new axis to create a 4D tensor. 
What I want to understand is how to voxelize multiple frames at the time and append them together.
","['deep-learning', 'convolutional-neural-networks', 'python', 'tensorflow', 'object-detection']",
How is the bias caused by a max pooling layer overcome?,"
I have constructed a CNN that utilizes max-pooling layers. I have found with these layers that, should I remove them, my network performs ideally with every output and gradient at each layer having a variance close to 1. However, if they are included, the variance skyrockets.
This makes sense, of course, as a max-pooling layer takes the maximum of an area, which must incur a positive bias as larger numbers are chosen.
I would just like to know what methods are typically used to combat this.
","['machine-learning', 'convolutional-neural-networks', 'relu', 'bias-variance-tradeoff']",
What are some neural network models that can use auxiliary info during training for image segmentation?,"
What are some deep learning models that can use supplementary information other than RGB channels for image segmentation?

For example, imagine a poorly shot image of a river (blue) that shows a gap, and the supplementary information is detailed flow directions (arrows), which helps to show the river's true shape (no gap in reality). To get the river shape, most image segmentation models I see, such as U-Net, only use RGB channels.
Are there any neural network models that can use this kind of auxiliary information along with RGB channels during training for the image segmentation task?
","['convolutional-neural-networks', 'reference-request', 'image-segmentation', 'u-net', 'model-request']",
Dynamic frames processing with CNN LSTM combination or otherwise,"
I have a unique implementation where I have to process videos with dynamic frame rates (that is the number of frames is different for each video in a batch). I am stacking all the frames in a single tensor and processing the same from there on. This works fine with Conv2D layer but creating a 2D tensor (batch_size, features) by a flattening operation this has to be fed to a Dense layer. I can't find a suitable way to implement this. 
For more information on why it should be like this kindly explore: explore this link. Instead of the MNIST images, I have multiple videos in a single bag, each with a variable number of frames. 
","['tensorflow', 'recurrent-neural-networks']",
Which model to use when selecting objects of interest?,"
I have a set of polygons for each image. Those polygons consist of four $x$ and $y$ coordinates. For each image, I need to extract the ones of interest. This could be formulated as an Image Segmentation task where, for example I want to extract the objects of interest, here: cars.

But since I already get the polygons through a different part of my pipeline I would like to create a simpler machine learning model. The input will not be the image but only the coordinates of the polygons. 
In this model each sample should consist of multiple polygons (those can vary in number) and the model should output the ones of interest. 
In my mind, I formulated the problem as follows:

The polygons are the features. Problem: Samples will have varying number of features.
The output will consist of the indices of the ""features"" (polygons) I am interested in.

First, I created a decision tree and classified each coordinate as $0$ (not interested in) or $1$ (of interest). But, by doing this, I don't consider the other coordinates that belong to the image. The information of the surrounding is lost.
Does someone have an idea of how to model this problem without using Image Segmentation?
","['machine-learning', 'deep-learning', 'image-segmentation']",
What is the difference between return and expected return?,"
At a time step $t$, for a state $S_{t}$, the return is defined as the discounted cumulative reward from that time step $t$.
If an agent is following a policy (which in itself is a probability distribution of choosing a next state $S_{t+1}$ from $S_{t}$), the agent wants to find the value at $S_{t}$ by calculating sort of ""weighted average"" of all the returns from $S_{t}.$ This is called the expected return.
Is my understanding correct?
","['reinforcement-learning', 'comparison', 'q-learning', 'return', 'expectation']","
You're correct, the return is the discounted future reward from the one iteration while the expected return is averaged over a bunch of iterations.
"
Does coarse coding with radial basis function generate fewer features?,"
I am learning about discretization of the state space when applying reinforcement learning to continuous state space. In this video the instructor, at 2:02, the instructor says that one benefit of this approach (radial basis functions over tile coding) is that ""it drastically reduces the number of features"". I am not able to deduce this in the case of a simple 2D continuous state space.
Suppose we are dealing with 2D continuous state space, so any state is a pair $(x,y)$ in the Cartesian space. If we use Tile Coding and select $n$ tiles, the resulting encoding will have $2n$ features, consisting of $n$ discrete valued pairs $(u_1, v_1) \dots (u_n, v_n)$ representing the approximate position of $(x,y)$ in the frames of the $n$ 2-D tiles. If instead we use $m$ 2-D circles and encode using the distance of $(x,y)$ from the center of each circle, we have $m$ (continuous) features.
Is there a reason to assume that $m < 2n$?
Furthermore, the $m$-dimensional feature vector will again need discretization, so it is unclear to me how this approach uses fewer features.
","['reinforcement-learning', 'function-approximation', 'features', 'tile-coding', 'coarse-coding']",
How to stop evaluation phase in reinforcement learning with epsilon-greedy Monte Carlo agent?,"
I have implemented an epsilon-greedy Monte Carlo reinforcement learning agent like suggested in Sutton and Barto's RL book (page 101). As far as I understood epsilon-greedy agents so far, the evaluation has to stop at some point to exploit the gained knowledge. 
I do not understand, how to stop the evaluation here, because the policy update is linked to epsilon. So just setting epsilon equal to zero at some point does not seem to make sense to me.
","['reinforcement-learning', 'q-learning', 'monte-carlo-methods']","
Before answering how to stop the evaluation phase and begin exploitation of those results, one must first answer when to stop it whereby the balance the project stakeholder wants between quality and cost is found. You won't always find that in books discussing pure research, so your question is an excellent one.
The algorithm the authors are discussing on that page (101) is based on the policy improvement theorem on page 78, and the appearance of the endless loop in the algorithm in the pseudo-code line ""Repeat forever (for each episode)"" is obviously worse than useless in a data center if the loop is not terminated, unless it is multi-agent, exploiting multiple threads, processes, virtual hosts, hardware accelerators, cores, or hosts, and the improvements are accessed for exploitation independently or symbiotically using some scheme.
In a deployed robot, an endless loop often has a legitimate a use case. ""Repeat until shutdown,"" might be appropriate in a production algorithm or hardware embodiment if the robot's goal is, ""Keep the living room clean."" One must always try to place this theory in context when taking pure research and considering the applied research that may stem from it.
In real product and service development environments, how the balance is struck between quality of action and cost of determining it depends upon the problem size, expectations of the user or business, and the architecture of the computational resources you have. Consider some of these factors in more detail.

Maximum number of rounds of evaluation-exploitation cycles
Requirements for precision in terms of optimality
Requirements for reliability in terms of completion
Distribution of the number branches from nodes
Distribution of lengths of possible action traversal sequences to the goal
Average cost (in time and energy) of each evaluation
Average cost (in time and energy) of each exploitation

In a single thread, single core, von Neumann architecture, as is sometimes the case in an embedded environment, evaluation and exploitation are time sliced. In such a case, evaluation should stop and exploitation should begin when the probability that further evaluation will produce an improved result drops below the cost of further evaluation, based on some estimation of return and cost. This is a function of the above factors, although not a linear one.
We have considered training an LSTM network to determine the function in the epoch domain (roughly related to the time domain), although it is low on our priority list.
In an embedded process, a function that approximates return on further evaluation cost can be constructed, based on statistics gathered up to that point in current learning or over a longer period of operations. The function should be fast and inexpensive. In each cycle within the evaluation phase, the function can be evaluated and compared against a configurable probability threshold. Its configuration value can be an educated guess based on the perception of the value of further path exploration.
In a simulation environment, when more computing resources for parallel processes, exploiting OS or hardware facilities for that, the time slicing is either opaque or nonexistent respectively. In those cases, continuous improvement may be unbounded because the state-action graph is not finite.
"
Predicting Hot Categories In a Reference Manager,"
Reference managers like Zotero or Mendeley allow researchers to categorize papers into hierarchical categories called collections. The User navigates through a listing of these collections when filing a new item. The retrieval time grows something like the logarithm of the number of collections; looking for a collection can quickly become a nuisance.

 Fig 1. A top level list of collections in the Zotero reference manager 
One way to reduce navigation time is to allow users to search collections by name. A complementary solution is to provide a view of the currently ""hot"" collections. The user may interact with a list of suggested collections, or receive relevant completions when typing into a collections search bar.
This raises a basic learning problem:

Let $K = K_1, \ \dots, \ K_m$ be the sequence of collections the user has visited (possibly augmented with visit times). Let $H_m$ be a set of $n$ collections the user is likely to visit next. How can we construct $H_m$?

A technique that does this this might exploit a few important features of this domain:

Project Clusters: Users jump between collections relevant to their current projects
Collection Aging: Users tend to focus on new collections, and forget older ones
Retrieval Cost: There's a tangible cost to the user (time, distraction) when navigating collections; this applies to the reduced view (the technique might keep $n$ as small as possible)

Two ideas so far
LIFO Cache

Reduce $K_{m-1}, K_{m-2},\ \dots$ into the first $n$ unique entries which do not match $K_m$.

This heuristic is very simple to implement and requires no learning. It encompasses clusters and aging given suitably large $n$. But with large $n$ it incurs a retrieval cost of its own.
Markov Chain/Hidden Markov Model

Use $K$ to train a MC or HMM. Build $H_m$ using estimates from this model.

The simplest version is an order $k\ $ MC transition matrix built using k-gram statistics of $K_i$. This might be sensitive to project clusters, but I don't think it will recognize new collections without a hard coded aging heuristic.
I'm not clear on how HMMs would be trained here, and I'm not very taken with the $k$-gram MC approach. My next task is to read about MCs/HMMs in context of suggestion systems.
Other Models
I am brand new to suggestion systems. Reading leads are quite welcome!
I would be especially excited about unsupervised techniques, and neural network techniques I could train on a GPU. Apart from improving Zotero, I would like this problem to give me an opportunity to learn about cutting edge techniques.
Valuable Goals
An ideal technique would cast light on questions like

How should we measure the performance of this kind of system? (I suspect cache miss rate is a good metric, as well as the ratio between cache miss rate and cache size)
How should we translate these human-centric performance metrics into human independent objective functions for learning?
How much better than LIFO can we theoretically expect to do with a more sophisticated technique (say, in terms of cache size for a given cache miss rate)?
How can a technique learn patterns like clusters and aging without hand tuned objectives?

I am interested in learning theory and building an implementation, so resources with publicly available code would be be preferable. Apart from potentially being overkill for the problem, I would not mind if the final model depends on a GPU.
Please forgive me for the long and vague question. I wish I had done more reading before posing this question, but I feel a bit stuck! I hope to get unstuck with some good reading resources. Thanks!
","['unsupervised-learning', 'prediction', 'reference-request', 'markov-chain', 'hidden-markov-model']",
What parameters can be tweaked to avoid a generator or discriminator loss collapsing to zero when training a DC-GAN?,"
Sometimes when I am training a DC-GAN on an image dataset, similar to the DC-GAN PyTorch example (https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html), either the Generator or Discriminator will get stuck in a large value while the other goes to zero. How should I interpret what is going on right after iteration 1500 in the example loss function image shown below ? Is this an example of mode collapse? Any recommendations for how to make the training more stable? I have tried reducing the learning rate of the Adam optimizer with varying degrees of success. Thanks!
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'generative-adversarial-networks', 'generative-model']","
GANs are notably hard to train and it is not uncommon to have large bumps in the losses. The learning rate is a good start but the instability may come from a wide variety of reasons. I'm assuming that you have no bug in your code or data.
For one, gradient descent is not well suited to the 2-player game we're playing. I've personally found ExtraAdam to yield much more stable training (code, paper). 
It could also come from the loss and many tricks have been developed and one of the most popular one is enforcing smoothness in the gradient (see W-GAN, W-GAN-GP etc.). SpectralNorm (code paper) is a very popular and recent normalization technique for the discriminator.
There are a number of additional tricks to make GANs work like label smoothing and flipping, different update rates for the discriminator and generators (as in BigGAN for instance). I suggest you have a look at this nice repo of (somewhat seasoned) tricks: ganhacks.
"
Is there any difference between a control and an action in reinforcement learning?,"
There are reinforcement learning papers (e.g. Metacontrol for Adaptive Imagination-Based Optimization) that use (apparently, interchangeably) the term control or action to refer to the effect of the agent on the environment at each time step. 
Is there any difference between the terms control or action or are they (always) used interchangeably? If there is a difference, when is one term used as opposed to the other?
The term control likely comes from the field of optimal control theory, which is related to reinforcement learning.
","['reinforcement-learning', 'terminology', 'comparison', 'control-theory']",
Can PDDL be utilized for action recognition?,"
The Planning Domain Definition Language (PDDL) is known for its capabilities of symbolic planning in the state space. A solver will find a sequence of steps to bring the system from a start state to the goal state. A common example of this is the monkey-and-banana problem. At first, the monkey sits on the ground and, after doing some actions in the scene, the monkey will have reached the banana.
The way a PDDL planner works is by analyzing the preconditions and effects of each primitive action. This will answer the question of what happens if a certain action is executed.
However, will a PDDL domain description work the other way around as well, not for planning, but for action recognition?
I've searched in the literature to get an answer, but all the papers I've found are describing PDDL only as a planning paradigm.
My idea is to use the given precondition and effects as a parser to identify what the monkey is doing and not what he should do. That means, in the example, the robot ape knows by itself how to reach the banana and the AI system has to monitor the actions. The task is to identify a PDDL action that fits the action by the monkey.
","['planning', 'action-recognition', 'pddl']","
This is theoretically possible for an exhaustive set of sequential, non-concurrent primitive actions:
$$
\forall s_1, s_2 \left(  \exists a \left(  \text{possible}(a, s_1) \land \text{do}(a, s_1, s_2) \right) \right)
$$
where $s_1$ is the prior situation, $s_2$ is the result of doing action $a$ in $s_2$, and $\text{possible}(a, s_1)$ is true if it is possible to do an action in a situation (See Situation Calculus).
So, for a given situation, reduce your search space to those actions that are possible, then reduce your search space again to those that result in the following situation.
"
"In NN, as iterations of Gradient descent increases, the accuracy of Test/CV set decreases. how can i resolve this?","
As mentioned in the title I'm using 300 Dataset example with 500 feature as an input.
As I'm training the dataset, I found something peculiar.  Please look at the data shown below.

Iteration 5000 | Cost: 2.084241e-01
Training Set Accuracy: 100.000000
CV Set Accuracy: 85.000000
Test Set Accuracy: 97.500000
Iteration 3000 | Cost: 2.084241e-01
Training Set Accuracy: 98.958333
CV Set Accuracy: 85.000000
Test Set Accuracy: 97.500000
Iteration 1000 | Cost: 4.017322e-01
Training Set Accuracy: 96.875000
CV Set Accuracy: 85.000000
Test Set Accuracy: 97.500000
Iteration 500 | Cost: 5.515852e-01
Training Set Accuracy: 95.486111
CV Set Accuracy: 90.000000
Test Set Accuracy: 97.500000
Iteration 100 | Cost: 8.413299e-01
Training Set Accuracy: 90.625000
CV Set Accuracy: 95.000000
Test Set Accuracy: 97.500000
Iteration 50 | Cost: 8.483802e-01
Training Set Accuracy: 90.277778
CV Set Accuracy: 95.000000
Test Set Accuracy: 97.500000

The trend is that as the Iteration(cost) increases(cost decreases), the training set accuracy increases as expected, but the CV Set/Test Set Accuracy decreases. My initial thought is that this has to do with precision/bias issue, but I really can't buy it. 
Anyone know what this entails? Or any reference? 
","['neural-networks', 'machine-learning', 'gradient-descent']",
Random graph as input in geometric deep learning on time-varying graph,"
I want to create a framework that allows GDL to be applied to time-varying graphs.
I came up with the Erdos-renyi model as an example of a time-varying graphs.
GDL for graphs takes node information as input and takes correspondence with correct data as accuracy. However, how should I deal with time-varying data, even random, and ground-truth data for such graphs? Or is there other better way?
And is it nonsense to use pseudo-coordinate as input, as it refers to the traditional approach to time-invariant graphs?
Also, an application of time-varying graphs has been anomaly detection in financial networks. How does this work specifically? Also, please let me know if there are other application examples.
","['machine-learning', 'graphs', 'geometric-deep-learning', 'graph-theory']",
How are exploding numbers in a forward pass of a CNN combated?,"
Take AlexNet for example:

In this case, only the activation function ReLU is used. Due to the fact ReLU cannot be saturated, it instead explodes, like in the following example:
Say I have a weight matrix of [-1,-2,3,4] and inputs of [ReLU(4), ReLU(5), ReLU(-2), Relu(-3)]. The resultant matrix from these will have large numbers for the inputs of ReLU(4) and ReLU(5), and 0 for ReLU(-2) and ReLU(-3). If there are even just a few more layers, the numbers are quick to either explode or be 0.
How is this typically combated? How do you keep these numbers towards 0? I understand you can take subtract the mean at the end of each layer, but for a layer that is already in the millions, subtracting the mean will still result in thousands.
","['machine-learning', 'convolutional-neural-networks', 'relu']",
What are the differences between network analysis and geometric deep learning on graphs?,"
Both of them deal with data of graph structure like a network community. Is there a big difference there?
","['convolutional-neural-networks', 'comparison', 'graphs', 'geometric-deep-learning']","
Both study properties of a network. The literature under respective titles seems to focus on certain topics.
Network analysis seems to focus on understanding the structure of a network. Centrality , modularity, assortativity etc are metrics used to study properties of networks. Key areas of research are for egs community detection, centrality measures, clustering algorithms, link prediction
GDL is oriented more towards using dataset structured as a graph as an input to machine learning problems like classification, regression.
Key areas of research include graph representation, neural architecture.
Some problems like link prediction for example are present in both domains. 
Some problems like network construction for egs isn't covered deeply in both areas. 
Some disciplines like algebraic graph theory appear in both disciplines. 
Fiedler vector for egs  is studied in network analysis for community 
detection . Spectral analysis, matrix factorisation are ideas being explored in representation learning. 
"
Would you term Google's Captchas as Turing Test?,"
Quoting from Wikipedia page on Turing Test

The Turing test, developed by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel such as a computer keyboard and screen...

The definition made me wonder if we would term the captchas that show up on Google here it gives textual instructions to the user, bot or not, asking them to select images containing a specific object out of the given set of images.
","['terminology', 'turing-test', 'google']",
Where could I find information on the learning methods used in Neurogrid?,"
I have been searching for more than one week which learning methods were used in Neurogrid.
But I only found descriptions of its architecture (chips, circuits, analog and/or digital components, performance results), everything but no clue on how it updates the weights. 
In my opinion, I think that it cannot be gradient descent (with back-propagation), as the topology of the neurons in a chip, for example in the neurocore of Neurogrid, is a mesh or grid. 
Do you know where I could find this kind of information?

","['machine-learning', 'neuromorphic-engineering']",
What is the difference between a stationary and a non-stationary policy?,"
In reinforcement learning, there are deterministic and non-deterministic (or stochastic) policies, but there are also stationary and non-stationary policies. 
What is the difference between a stationary and a non-stationary policy? How do you formalize both? Which problems (or environments) require a stationary policy as opposed to a non-stationary one (and vice-versa)?
","['reinforcement-learning', 'comparison', 'policies', 'stationary-policy']",
I am looking for research related to the use of AI and ML in automotive and aeronautics safety design,"
I am specifically interested in the topic of edge cases.
I have the presentation Edge Cases and Autonomous Vehicle Safety as a starting point, in particular on page 6:

Machine Learning (inductive training)

No design insight

Generally inscrutable; prone to gaming and brittleness.



I'd like to find more hard data on how ML may do very well until an edge case is encountered.
","['neural-networks', 'machine-learning', 'reference-request']",
Can we use the Tierra approach to optimize machine code?,"
Thomas Ray's Tierra is a computer program which simulates life.
In the linked paper, he argues how this simulation may have real-world applications, showing how his digital organisms (computer programs) evolve in an interesting way: they develop novel ways of replicating themselves and become faster at it (he argues that the evolved organisms employ an algorithm which is 5 times faster than the original one he wrote).
Tierra's approach is different from standard GAs:

While in GAs usually there is a set of genomes manipulated, copied and mutated by the program, in Tierra everything is done by the programs themselves: they self-replicate.
There is no explicit fitness function: instead, digital organisms compete for energy resources (CPU time) and space resources (memory). 
Organisms which take a long time to replicate reproduce less frequently, and organisms who create many errors are penalized (they die out faster).
Tierran machine language is extremely small: operands included, it only has 32 instructions. Oftentimes, so called RISC instruction sets have a limited set of opcodes, but if you consider the operands, you get billions of possible instructions.
Consequentially, Tierran code is less brittle, and you can mutate it without breaking the code. In contrast, usually, if you mutate randomly some machine code, you get a broken program.

I was wondering if we could use this approach to optimize machine code. For instance, let's assume we have some assembly-like program which computes a certain function $f$. We could link reproduction time with efficiently computing $f$, and life-span with correctly computing it. This could motivate programs to find novel and faster ways to compute $f$.
Has anything similar ever been tried? Could it work? Where should I look into?
","['genetic-algorithms', 'optimization', 'genetic-programming', 'artificial-life']","
Yes it has been tried. In fact there is a whole field, dubbed Genetic Programming.
There is an annual competition to obtain ""Human-Competitive"" algorithms, and many instances of those have been found over the years.
"
Why can a fully convolutional network accept images of any size?,"
On this article, it says that: 

The UNET was developed by Olaf Ronneberger et al. for Bio Medical Image Segmentation. The architecture contains two paths. First path is the contraction path (also called as the encoder) which is used to capture the context in the image. The encoder is just a traditional stack of convolutional and max pooling layers. The second path is the symmetric expanding path (also called as the decoder) which is used to enable precise localization using transposed convolutions. Thus it is an end-to-end fully convolutional network (FCN), i.e. it only contains Convolutional layers and does not contain any Dense layer because of which it can accept image of any size.

What I don't understand is how an FCN can accept images of any size, while an ordinary object detector, such as YOLO with a dense layer at the very end, cannot accept images of any size. 
So, why can a fully convolutional network accept images of any size?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'fully-convolutional-networks']",
Understanding the proof that A* search is optimal,"
I don't understand the proof that $A^*$ is optimal. 
The proof is by contradiction:

Assume $A^*$ returns $p$ but there exists a $p'$ that is cheaper. When $p$ is chosen from the frontier, assume $p''$ (Which is part of the path $p'$) is chosen from the frontier. Since $p$ was chosen before $p''$, then we have $\text{cost}(p) + \text{heuristic}(p) \leq \text{cost}(p'') + \text{heuristic}(p'')$. $p$ ends at goal, therefore the $\text{heuristic}(p) = 0$. Therefore $\text{cost}(p) \leq \text{cost}(p'') + \text{heuristic}(p'') \leq \text{cost}(p')$ because heuristics are admissible. Therefore we have a contradiction.

I am confused: can't we also assume there's a cheaper path that's in a frontier closer to the start node than $p$? Or is part of the proof that's not possible because $A^*$ would have examined that path because it is like BFS with lowest cost search, so, if there's a cheaper path, it'll be at a further frontier?
","['search', 'proofs', 'heuristics', 'a-star', 'admissible-heuristic']","
The key phrase here is

because heuristics are admissible

In other words, the heuristics never overestimate the path length:
$$cost(n) + heuristic(n) \le cost(\text{any path going through n})$$
And since the frontier is ordered by $\textbf{cost + heuristic}$, when a completed path $p$ is dequeued from the frontier, we know that it must necessarily be $\le$ any path going through some other frontier node $q$, because
$$cost(p) = cost(p) + heuristic(p)$$
$$\le cost(q) + heuristic(q)$$
$$\le cost(\text{any path going through q})$$
"
What are the benefits of using the state information that maintains the graph structure?,"
When you applying a graph structured data to the graph convolution network,  what are the benefits of using the state information that maintains the graph structure?
","['machine-learning', 'deep-learning', 'graphs', 'geometric-deep-learning', 'graph-theory']",
How do I know if my dataset is ready for a machine learning model?,"
I am new in this area of Machine Learning and Neural Networks. Currently, I'm taking some courses on Udemy and reading a book about it, but I still have one big question regarding data pre-processing.
In all of those Udemy's lessons, people always use a perfect dataset and ready to input in a model. So all you have to do is run it.
How do I know if my dataset is ready for a model? What do I have to do to make it ready? Which evaluations?
I had a few statistics classes in college already and I learned a lot about correlations matrices, autocorrelation functions, and its lags, etc. and I didn't see yet in anywhere someone explaining how can I evaluate my data and then proceed to implement a model to solve my problem.
If anyone could point me a direction, give me some material, show me where I can learn this, anything, it would be really helpful!
","['neural-networks', 'machine-learning', 'datasets', 'data-science', 'data-mining']",
What is the purpose and benefit of applying CNN to a graph?,"
I'm new to the graph convolution network. I wonder what is the main purpose of applying data with graph structure to CNN?
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'geometric-deep-learning']",
"Clarifications on ""Prioritized Experience Replay"" (Deepmind, 2015) [closed]","







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



Paper link : Prioritized Experience Replay 
About the blind cliffwalk setup:

Why is the number of possible action sequences equal to 2^N? I cant think of sequences more than (N + 1) where one sequence is the sequence of all right actions and the other N sequences are due to wrong actions at each state.

Generally for prioritized experience replay:

The replay memory consists of some transitions which are repeated.In the priority queue I feel that there should only be a single priority for each transition to speed up learning. Is there any advantage of having priority values for each repeated instance of the transition?

Edit for 2nd question:
Consider algorithm 1 on page 5 of the article.

Lets consider one of the transitions to be repeated in the replay memory. If one of them is sampled (line 9) and the priority updated (line 12). Will the priority update on the other instance of the same transition?
","['deep-learning', 'prioritized-sweeping']","
for 1) i think your confusing elements touched vs sequences. At each point for N turns you have 2 possible options, therefore you have $\prod_{i=1}^N 2$ or $2^N$ possible sequences.   
for 2) The priorities are updated based on the expected rewards. They do not add new elements each time, they update
"
Are there any better visual models for transfer rather than ImageNet?,"
Similar to the recent pushes in Pretrained Language Models (BERT, GPT2, XLNet) I was wondering if such a thrust exists in Computer Vision?
From my understanding, it seems the community has converged and settled for ImageNet trained classifiers as the ""Pretrained Visual Model"". But relative to the data we have access too, shouldn't there exist something stronger? Also, classification as a sole task has its own constrictions on domain transfer (based on the assumption of how these loss manifolds are).
Are there any better visual models for transfer rather than ImageNet successes? If no, why? Is it because of the domains fluidity in shape, resolution, etc., in comparison to text?
","['computer-vision', 'transfer-learning', 'pretrained-models']","
Why is ImageNet so popular for transfer learning?
Models pre-trained on the ImageNet datasets have been the de-facto choice for many years now. Many popular reasons as to why people think that ImageNet is so effective for transfer learning are the following:

ImageNet is a truly large-scale dataset that contains over 1 million images, each of which has a decent resolution.
ImageNet has a wide and diverse set of classes (1000 in total) ranging from animals to humans, cars, trees, etc. Most Computer Vision tasks operate in similar domains, however there are some notable exceptions (e.g. medical images).
For example, an object detection model for autonomous driving would benefit from ImageNet transfer learning, as the pre-trained model has seen images with similar content (e.g. roads, people, cars, street signs), even though it tries to solve a different task (i.e. object detection not classification). 
The above two reasons allow models trained on ImageNet to identify and extract very generic features, especially in their initial layers, that can be effectively re-used.
ImageNet has a lot of similar classes. This is an interesting argument because it contradicts the second one. Due to the closeness of some classes (e.g. multiple breeds of cats), networks learn to extract more fine-grained features.

Another overlooked reason I find very important is that:

ImageNet has been the benchmark for performance for image classifiers for years now. When, for example, you are selecting a re-trained ResNet to use, you know that that model is guaranteed to operate at a high level of performance. Other datasets don't have such notable challenges as the ILVRC. That challenge is what make the VGG and ResNet popular in the first case, so it comes natural that people would want to use those weights.

In practice, due to the way in which CNNs identify and extract features from images, they can easily be ""transferred"" from task to task.
Is it actually better than other datasets?
This question was widely explored by Huh et al., who tried to identify the reasons that made the ImageNet dataset better than other ones for transfer learning.
In short they found out that most of the reasons that people thought made ImageNet so good (i.e. the ones I mentioned above) weren't necessarily correct. Furthermore, the amount and diversity of images and classes required to effectively train a CNN has been highly overestimated. So there is no particular reason people should choose this specific dataset.
Now, to answer your questions:

I was wondering if such a thrust exists in Computer Vision?

No, ImageNet is currently established as the de-facto choice, evident by the fact that all 10 keras.applications models offer weights only for ImageNet.

But relative to the data we have access too, shouldn't there exist something stronger?

This is an interesting question, as the consensus things that deep learning models keep getting better with more data. There is, however, evidence that indicates otherwise (i.e. that CNN models con't have as much capacity as we thought). You can read the aforementioned study for more details. In any case, this is still an open research question.
Even if models could get better, though, with more data, it is possible that it still wouldn't matter because ImageNet pre-trained models are strong enough.

classification as a sole task has its own constrictions on domain transfer 

There have been numerous cases where models initialized from pre-trained ImageNet weights have done well in settings other than classification (e.g. regression, object detection). I'd argue that initialization from ImageNet is almost always better than random initialization.

Are there any better visual models for transfer rather than ImageNet successes? If no, why? Is it because of the domains fluidity in shape, resolution, etc., in comparison to text?

Partly, yes. I think that in comparison to text, images have some useful properties that are exploited through CNNs, which makes their knowledge more transferable. This claim, however, is based on intuition; I can't back this up somehow.
"
Is predicting day of week straight forward?,"
I am using python and Xgboost. I have features: activity and location and time stamps of when the activity occurred. 
I want to predict day of week. Is this straight forward, ie y=day of week, X={activity, location}, or am I being naive and I need to do fancy time series things? I'd also like to predict time of day.
","['machine-learning', 'prediction']",
What is the point of converting conditional probability to factor for Variable Elimination?,"

I have this slide from my AI class on using a Bayes network to compute a conditional probability. I don't really understand the point of converting the conditional probabilities to factors (besides the fact that it looks weird to marginalize or multiply variables in a CP). It seems kind of arbitrary. Is there some benefit I'm not noticing? 
","['bayesian-networks', 'random-variable', 'conditional-probability']",
How does FastText support online learning?,"
I'm using FastText pre-trained-embedding for tackling a classification task, but I saw it supports also online training (incremental training) for adding domain-specific corpus.
How does it work? 
As far as I know, starting from the ""model.bin"" file it retrains the model only on the new corpus updating the old word-vectors, is it right? 
","['classification', 'word-embedding', 'online-learning', 'incremental-learning']","
The pull request #1327 (https://github.com/facebookresearch/fastText/pull/1327)
Allows for:

test after each epoch
checkpointing
training on large data which does not fit into memory (largest I tested was 1.6TB)
finetuning already trained models

The trained model is indistinguishable from a model that was created by an original tool and can be used for inference by the old code.
"
How do I classify strings with possibly no meaning?,"
I am quite new to text classification. 
Using EAST text detection model, I get multiple strings that aren't words and most often have no meaning. For example, IDs, brand names, etc. I would like to classify them into two groups. Which models work the best and how should I preprocess the strings?  I wanted to use Word2Vec, but I think it only works with real words and not with arbitrary strings.
","['machine-learning', 'natural-language-processing', 'classification', 'word2vec']","
I would just use a dictionary. A simple list lookup would tell you whether it's a recognised word or not. As an added bonus you can add some basic language processing, eg to identify inflected forms without listing them in your dictionary. Or use regular expressions to recognise ID numbers. ML is not really the right tool here. 
"
Designing state representation for board game,"
I am trying to write self-play RL (NN + MCTS http://web.stanford.edu/~surag/posts/alphazero.html) to ""solve"" a board game. However, I got stuck in designing boardgame same (input layer for NN).
1) What would be the best way to represent each cell, if there are ~10-100 cells in a game which could be occupied by any playing card.  Do one-hot-encoding and get 52 nodes for single cell ([0, 0, 1, ..., 0]) or just divide card_id by total number of cards and get single node for each cell ([0.0576...])?
2) Is it a good/bad practice to help NN by adding additional input that could be derived from the other nodes? For instance, imagine the game where whoever has most red cards wins. Input is 10 cards, and I am adding new input node (number of red cards) to emphasize it. Would that lead to a positive result or doing something like that is bad?
3) Would it help to reduce the number of illegal moves and increase the performance of NN by creating additional input stating which cards are available now and which are not?
","['neural-networks', 'reinforcement-learning', 'models']",
How to create neural network that predicates result of exam?,"
Actually, I am ""fresh-water"", and I've never known what is neural network. Now I am trying understand how to design simple neuronetwork for this problem:
I'd like to make up such neural network that after learning it could predicate a mark of passed exam (for example, math). There is such factors that influence on a mark:

Chosen topic (integral, derivative, series)
Perfomance (low, medium, high)
Does a student work? (Yes, No, flexible schedule)
Have a student ever gotten through a add course? (Yes, No)

The output is a mark (A,B,C,D,E,F)
I don't know should I add few layers between inputs and output

Moreover, I have few results from past years:

(integral, low, Yes, No, E) 
(integral, medium, Yes, Yes, B)
(series, high, No, Yes, A)
and so on. What do I need to know else for designing this NN?

",['neural-networks'],"
You can add as many layers (with any arbitrary number of nodes) as you want.
Please note that as you add more learning parameters (layers and nodes), your model complexity increases. This means the model can potentially learn a more complex input-output relationship. However, it also increases the risk of overfitting. Overfitting generally happens when the model you build is more complex than the data you have. In such a scenario, the model memorizes the data instead of learning from it. In other words, it can produce a very good result for the same data as it was trained on but cannot generalize well. So, it performs poorly when the inputs are slightly different from what is used to be fed at the training stage.
In practice, you may try different architectures and parameter configurations, and measure the generalization capacity of the models (via cross-validation for example) to choose the best model. In English, a generalizable model is the one that performs (almost) equally good on both training/validation and testing sets.
"
How do I locate a specific object in an image?,"
Some pictures contain an elephant, others don't. I know which of the pictures contain the elephant, but I don't know where it is or how does it look like.  
How do I make a neural network which locates the elephant on a picture if it contains one? There are no pictures with more than one elephant.
","['deep-learning', 'object-detection', 'semi-supervised-learning']",
Could the normalisation of the inputs make the neural network insensitive to changes in the inputs?,"
When using neural networks (NNs), we often normalized the inputs. I think this is done to equally capture the changes in any input feature, that is, if any feature takes huge values and other features take small values, we don't want the NN not to be able to ""see"" the change in the smaller value.
However, what if we cause the NN to become insensitive to the input, that is, the NN is not able to identify changes in the input because the changes are too small?
","['neural-networks', 'deep-learning', 'data-preprocessing', 'normalisation']","

When using neural networks (NNs), we often normalized the inputs. I
think this is done to equally capture the changes in any input
feature, that is, if any feature takes huge values and other features
take small values, we don't want the NN not to be able to ""see"" the change
in the smaller value.
However, what if we cause the NN to become insensitive to the input,
that is, the NN is not able to identify changes in the input
because the changes are too small?

We don't normalize the input to make the model less sensitive to small changes in the input (theoretically, given the correct optimization strategy, the model will learn to approximate the smaller-ranged input as well).
An example of this would be Convolutional Neural Networks. Traditionally, images were represented with integer values ranging from $0$ to $255$. This means that a given pixel could have only $256$ distinct values. However, assuming we normalize the input, let's say to $[0, 1]$, this gives the pixel a whole range of values to occupy, making the input more sensitive to changes.
Instead, normalization is done to help with the model's convergence.
"
What is the difference between Knowledge Representation and Automated Reasoning?,"
Knowledge Representation and Automated Reasoning are two AI subfields which seem to have something to do with reasoning. 
However, I can't find any information online about their relationship. Are they synonyms? Is KR a subfield of AR? What's the difference, if any?
To expand further, I believe representing knowledge is an essential part of ""reasoning"": how can you reason without a proper representation of the concepts you want to manipulate?
At the same time, reasoning seems to be an essential part of KR (we build Knowledge Bases in order to build computer programs which are able to make inferences from them).
So it seems to me that they are the same field, or at least deeply interrelated, but nobody on the internet seems to explicitly say that; furthermore, in this question, they are mentioned separately.
Another point of ambiguity is that the wikipedia page of KRR mentions reasoning and automated reasoning as a part of KR; it even lists ""automated theorem proving"" (a classical application of AR) as an application of KR.
But at the same time, we have a separate AR page which does not mention KR at all.
","['terminology', 'comparison', 'knowledge-representation', 'automated-reasoning']",
How do I determine whether a truck is inside its lane?,"
I have a bunch of images from different trucks passing the road. Here is an example.

The truck needs to be at a certain distance from the border of the lane. Some of the trucks are way close to the border (that you can see on the shoulder of the road).
I want to find a way to measure the distance between the truck and the border of the lane and, more importantly, to detect whether a truck is inside its lane.
I would like to solve this problem by training a deep learning-based classifier or image processing techniques. Painting the ground is also possible if I can train a classification algorithm with painted images.
","['machine-learning', 'deep-learning', 'applications', 'image-processing']","
One possible approach will be to use an algorithm which detects lines (Ex. Hough lines or any deep neural net trained to detect lanes) and use some threshold range so that we can get the lane and the edges of truck, then after extracting the lines, you can easily find the distance between them.
Then you need to experiment out on few images to get the threshold distance that you are expecting the truck to maintain as the real distance and the distance calculated using images are not same 

If you want to classify using deep learning, you may need to preprocess the images and send them. As it will become very difficult to directly learn to classify 
based on image, you may need to first detect the lanes, then apply a mask and then send the masked image to your network to make the network to converge. 
"
How can I use 1-channel images as input to a CNN?,"
I need to develop a convolutional neural network whose inputs are 1-channel images, but I dont know how to do it, given that most libraries use 3 channel images. Should I convert my images to RGB? Is there any way to implement a CNN that receive as input 1-channel images?
","['machine-learning', 'convolutional-neural-networks', 'supervised-learning']","
The libraries should allow you to specify the number of input channels of the convolutional layer, so no one should prevent you from passing 1-channel images as input to a CNN. For example, in PyTorch, you can specify the number of input channels of the Conv2d object. 
If your library does not provide such feature, you could convert your 1-channel images to 3-channel images, where e.g. all 3 channels of each image are equal to the only channel of the corresponding original 1-channel image.
"
How High and Low frequency filters effect activation in the next layer?,"
Generally, we come across terms such as High Frequency and Low frequency filters in Convolutional Neural Networks (CNN). In regards to this highlighted statement, in 'S1' section of this paper by Jason Yosinski (ref 1),  I thought, in order for high and low-frequency filters to produce a similar effect, weight of low-frequency filters should be greater than high-frequency filters. I would like to understand why I am wrong and I will be grateful if anyone can elaborate about High and Low frequency filters in CNN, in general, or in this context. Thank you. 
Ref 1: Yosinski, Jason, et al. ""Understanding neural networks through deep visualization."" arXiv preprint arXiv:1506.06579 (2015).
","['deep-learning', 'convolutional-neural-networks']",
How to approach a problem with infinite solutions,"
Think Angry Birds kind of game. You need to hit a target at some point by adjusting angle and power. There is infinite number of parabolas that will solve this problem.
My problem is not exactly that but similar, it also has infinite number of solutions. Could anyone please suggest how do I approach this kind of problems using Machine Learning?
","['machine-learning', 'algorithm']",
Convolutional Neural Networks for different-sized Source and Target,"
CNNs are often used in one of the following scenarios:

A known-sized image is encoded to an intermediate format for later use
An intermediate or precursor format is decoded into a known-sized image
An image is converted into a same-size image

(Usually 3 is done by sticking together 1 and 2.)
Are there any papers dealing with convolutional techniques where the image sizes vary?  Not only would the size of input X differ from input Y, but also input X may differ from output Y. The total amount of variation can probably be constrained by the statistics of the dataset, but knowledge of input X does not grant a priori knowledge of the size of output Y
(Masking is an obvious solution, but I am hoping for something more elegant if research already exists.  The problem domain need not be images.) 
",['convolutional-neural-networks'],"
Yes actually. There have been quite a few different adaptations to convolutional neural networks to do precisely what you are describing. 
Here is an earlier one. See section 3.2.5
Here He et al. create a method known as Spatial Pyramid Pooling(SPP). In this method, you are able construct a fixed length representation regardless of input size by pooling them into ""spatial bins"" which are proportional to the input size, and thusly do not need to modify input dimensions.
There are a few newer methods that improve on this in various capacities, as usual, your solution will be dependent on the problem and other situation-dependent constraints. I suggest you dig deeper into the literature to find the optimal solution in your case.
"
Problem over DQN Algorithm not converging on snake,"
I'm using a DQN Algorithm to play Snake.
The input of the neural network is a stack of 4 images taken from the games 80x80.
The output is an array of 4 values, one for every direction.
The problem is that the program does not converge and I've a lot of doubts in the replay function, where I train the neural network over a batch of 32 events.
That's the snippet:
def replay(self, batch_size):

    minibatch = random.sample(self.memory, batch_size)

    for state, action, reward, next_state, done in minibatch:

        target = reward

        if not done:
            target = (reward + self.gamma *
                      np.amax(self.model.predict(next_state)[0]))
        target_f = self.model.predict(state)
        target_f[0][action] = target
        self.model.fit(state, target_f, epochs=1, verbose=0)

    if self.epsilon > self.epsilon_min:
        self.epsilon *= self.epsilon_decay`

Targets are:

+1 for eating an apple
0 for doing a movement without dying
-1000 for hitting a wall or the snake hitting himself

","['reinforcement-learning', 'python', 'tensorflow', 'q-learning', 'dqn']","
I think the main issue here is that you are trying to train the snake (network) on images. This will create a lot of issues a there are no set parameters that the model can learn from.
From images, there is no logical way to define the boundary, directions and objects on the board. It will be much easier to write a simple computer vision script of game API to provide actual meaningful inputs to the model.
Here is a great article on building a model to play the snake game. The author also provides the game API for input along with example code to train the snake game.
Final results from the model

"
What is an agent in Artificial Intelligence?,"
While studying artificial intelligence, I have often encountered the term ""agent"" (often autonomous, intelligent). For instance, in fields such as Reinforcement Learning, Multi-Agent Systems, Game Theory, Markov Decision Processes.
In an intuitive sense, it is clear to me what an agent is; I was wondering whether in AI it had a rigorous definition, perhaps expressed in mathematical language, and shared by the various AI-related fields.
What is an agent in Artificial Intelligence?
","['reinforcement-learning', 'terminology', 'definitions', 'intelligent-agent']",
Why aren't compiled languages as popular as Python in AI? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            



Closed 3 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






One of if not the most popular programming language for data science and AI today is Python, with R being a frequently cited runner-up. However, both of them are interpreted languages, which do not execute as fast as compiled languages. Why is that the case? The main advantage of AI over humans is in computing speed, and with real world AI applications today handling big data, execution time will already increase considerably as it is. Why aren't compiled languages preferred for this very reason?
Sure, the key argument and strength going for Python is its vast range of third party libraries available for AI, like scikit etc. but such communities can take root and grow anywhere under the right circumstances. Why did this community end up growing around Python and not a faster, equally common compiled language like C++, C# or Java?
",['programming-languages'],"
There are a few advantages of interpreted languages (compared to compiled languages)

platform independence (you only need the interpreter for your platform, even though this is not true if e.g. your interpreted language is only a wrapper library around a library written in another programming language)
dynamic typing (no need to specify the types of the variables)
dynamic scoping (e.g. you can access variables in other scopes)
automatic memory management (but there are compiled languages, like Java, that also have a garbage collector)
Rapid prototyping (for various reasons, including dynamic typing), hence software can be written more quickly

Hence, the main advantages of an interpreted language compared to a compiled language are flexibility and dynamism. Given that AI is still an evolving field, these characteristics are widely appreciated.
There is also at least one disadvantage of interpreted languages (compared to compiled languages)

Slower running times compared to compiled languages, which, once compiled, are quite fast, because they are often compiled to a code that is quickly executable by the machine or virtual machine

Python and R are widely used in data science and artificial intelligence because of the advantages above, which possibly contributed to the rapid growth of the communities around them and the development of software libraries.
However, note that the core of the most common machine learning libraries today, including TensorFlow and PyTorch, is written in a compiled language like C and C++. In the specific case of TensorFlow, Python is just a wrapper library. Consequently, under the hood, the code is not executed by the Python interpreter, but first compiled, which implies that, when you're using e.g. TensorFlow, your code will run (more or less) as fast as if you were using a compiled language like C. A similar argument can be made for libraries like NumPy, where Python is just a wrapper library.
"
Does the encoding of a restricted Boltzmann machine improve with more layers?,"
I'm using a restricted Boltzmann machine (RBM) as an autoencoder. For now, I use a simple architecture of two layers, the input (~100 nodes) and the output (3 nodes) layers.  I'm thinking to add more hidden layers. 
Are there some improvements in encoding by adding multiple hidden layers? If yes, how can multiple layers improve the encoding?
","['hidden-layers', 'boltzmann-machine', 'restricted-boltzmann-machine']","
First of all, when you add hidden layers, or stack RBMs, you get a Deep Belief Network (DBN). Your question then deals with the comparison of DBNs and RBMs.
There are some elements to answer this question in the article Representational Power of Restricted Boltzmann Machines and Deep Belief Networks by Nicolas Le Roux, which can be found summarized in these course slides. The main results are:

Restricted Boltzmann Machines:

Increasing the number of hidden units improves representational ability.
With an unbounded number of units, any distribution over $\{0,1\}^n$ can be approximated arbitrarily well.

Deep Belief Networks:

Adding additional layers using greedy contrastive divergence training does not provide additional benefit.
There remain open questions about the benefits additional layers add.


I emphasized the $4^{th}$ point that covers the most your question. It does not mean that additional layers are useless, but since RBM are universal approximators ($2^{nd}$ point), the benefits of adding layers seem less straightforward. They seem dependent on the first layer, the training procedure... You can see the article for more details and the open questions raised about DBNs. Note that this answer is an entry point on the topic, there might be more recent results following this article which I'm not familiar with...
"
Understanding average precision (AP) in measuring object detector performance,"
I am trying to understand the average precision (AP) metrics in evaluating the performance of deep-learning based object detection models. Suppose we have the following ground true (four objects highlighted by four blue arrows):

where we have labelled four objects:
person 25 16 38 56
person 129 123 41 62
kite 45 16 38 56
kite 169 123 41 62

And when feeding the above image to an object detector, it gives the following outputs:

It's easy to see that the object detector identified another object with low confidence:
person 0.4 25 16 38 56
person 0.2 129 123 41 62
kite 0.3 45 16 38 56
kite 0.5 169 123 41 62
kite 0.1 769 823 141 162 <-------- a ""kite""

In my humble opinion, this is an erroneous behavior of the object detector, which should be counted as a ""false positive"".  
However, since the ""kite"" has a quite low confidence score (0.1), when using the standard mAP algorithm to compute the performance, I got the following output (I am using code from here to compute the mAP):
AP: 100.00% (kite)
AP: 100.00% (person)
mAP: 100.00%

So here are my questions and confusions:

from what kind of design intension, the AP is designed in a way such that objects with low confidence score are ignored and therefore in this case we are with flying colors. 
Is there any metrics that can take this extra ""kite"" into consideration and therefore would output one ""false positive"" of the object detection model? I am just thinking that in this way, we can further proceed to improve the accuracy of this model during training.

","['computer-vision', 'object-recognition', 'object-detection']",
why my regression model predict every datapoint to the same value,"
I am trying to train a SVR but I found that with some combination of features, the trained SVR predict every point in test set to the same value. this problem occurs much more when I use linear kernel than other kernels. The parameters are: C=1, gamma=0.5.
My question is what leads to this kind of problem. Is there a name for this phenomenon? Thank you!
","['machine-learning', 'support-vector-machine', 'regression']",
"How can I encode states where the environment consists of multiple identical elements, but each is characterised by different features?","
I am quite new to Deep Reinforcement Learning, and I'm trying to define states in a Reinforcement Learning problem. The environment consists of multiple identical elements, and each one of them is characterized by different features of the same type. In other words, let us say we have $e_0$, $e_1$, and $e_2$. Then, suppose that each one is characterized by features $f_0$ and $f_1$, where $f_0$ belongs to $[0, 1]$, and $f_1$ belongs to $\{0, 1, 2, 3, 4, 5\}$. Then, $e_0$ will have some value for the features $f_0$ and $f_1$, and the same goes for $e_1$ and $e_2$.
How can I encode such states?
Can I simply vectorize such state by concatenating the different features of each element obtaining $[f_{0e_0}, f_{1e_0}, f_{0e_1}, f_{1e_1}, f_{0e_2}, f_{1e_2}]$, or should I use a convolutional architecture of some sort?
","['reinforcement-learning', 'ai-design', 'deep-rl', 'markov-decision-process']",
How to transform a PDDL to search?,"
I have a question about search and planning:
I still haven't understood the difference from the two, but they seem very similar to me; here is a question I am struggling with: 

""Having formulated a PDDL problem, transform it into research,
  emphasizing what the differences are.""

Someone can do an example? 
I attached an example of simple PDDL from my book (I'm using  Russell & Norvig)

","['comparison', 'search', 'planning', 'pddl']","
Not all search is planning (is A connected to B), but all planning is search (how do I get from this to that). 
Here's an example in Prolog with a domain described in terms of actions, when they are possible, and what the result of the actions are. The description is of an uncomputed graph of un-calculated size where each node is a situation and each edge is an action. Then we have A* search algorithm that searches this graph, calculating it as it goes, to find a plan to reach the goal state. Running the final query will produce a plan via search.
"
From what aspect to measure the performance of an object detector?,"
I am on the hook to measure the prediction results of an object detector. I learned from some tutorials that when testing a trained object detector, for each object in the test image, the following information is provided:
    <object>
    <name>date</name>
    <pose>Unspecified</pose>
    <truncated>0</truncated>
    <difficult>0</difficult>
    <bndbox>
        <xmin>451</xmin>
        <ymin>182</ymin>
        <xmax>695</xmax>
        <ymax>359</ymax>
    </bndbox>
</object>

However, it is still unclear to me 1) how does these information is taken by the object detector to measure the accuracy, and 2) how does the ""loss"" is computed for this case. Is it something like a strict comparison? For instance, if for the object ""date"", I got the following outputs:
    <object>
    <name>date</name>
    <pose>Unspecified</pose>
    <truncated>0</truncated>
    <difficult>0</difficult>
    <bndbox>
        <xmin>461</xmin>  <---- different
        <ymin>182</ymin>
        <xmax>695</xmax>
        <ymax>359</ymax>
    </bndbox>
</object>

Then I will believe that my object detector made something wrong? Or they tolerant some small delta such that if the bounding box has a small drifting, then it's acceptable. But if the ""label"" is totally wrong, then that's wrong for sure? 
This is like a ""blackbox"" to me and it would be great if someone can shed some lights on this. Thank you.
","['object-recognition', 'object-detection']",
Describing the order of a tensor,"
When describing tensors of higher order I feel like there is an overloading of the term dimension as it may be used to describe the order of the tensor but also the dimensionality of the... ""orders""?
Assume one describes the third-order tensor produced by a convolutional layer and wants to refer to its width and height. Do you say spatial dimensions? Would you write about the channel dimension? Or rather the direction? Saying ""spatial order"" feels really weird. But staying with dimensions makes sentences like ""The spatial dimensions are of equal dimensionality."" (Disclaimer: Obviously you can avoid the issue here by restructuring, but doing this at every occasion does not feel like a satisfactory solution.).
",['terminology'],"
By definition, tensors can be of any order (usually named differently if the order is less than three). So, I use $d_i$ to indicate the dimensionality of the $i$th facet.
Unless you have three or four-order tensors which each facet has a very specific meaning, naming the orders by terms such as special or time would be limiting.
"
Analysis of Training Loss and Validation Loss Graph,"
Here I am Showing Two Loss graphs of an Artificial Neural Network.
Model 1 

Model 2

Blue -training loss
Red  -val training loss
Can you help me to analyse these graphs? I read some articles and post but doesn't give me any sense.
","['neural-networks', 'deep-learning', 'objective-functions']",
What sort of mathematical problems are there in AI that people are working on?,"
I recently got a 18-month postdoc position in a math department. It's a position with relative light teaching duty and a lot of freedom about what type of research that I want to do.
Previously I was mostly doing some research in probability and combinatorics. But I am thinking of doing a bit more application oriented work, e.g., AI. (There is also the consideration that there is good chance that I will not get a tenure-track position at the end my current position. Learn a bit of AI might be helpful for other career possibilities.)
What sort of mathematical problems are there in AI that people are working on? From what I heard of, there are people studying

Deterministic Finite Automaton
Multi-armed bandit problems
Monte Carlo tree search
Community detection

Any other examples?
","['research', 'math']","
Most of the math work being done in AI that I'm familiar with is already covered in nbro's answer. One thing that I do not believe is covered yet in that answer is proving algorithmic equivalence and/or deriving equivalent algorithms. One of my favourite papers on this is Learning to Predict Independent of Span by Hado van Hasselt and Richard Sutton.
The basic idea is that we may first formulate an algorithm (in math form, for instance some update rules/equations for parameters that we're training) in one way, and then find different update rules/equations (i.e. a different algorithm) for which we can prove that it is equivalent to the first one (i.e. always results in the same output). 
A typical case where this is useful is if the first algorithm is easy to understand / appeals to our intuition / is more convenient for convergence proofs or other theoretical analysis, and the second algorithm is more efficient (in terms of computation, memory requirements, etc.).
"
Is there any research work that attempts to combine neuroevolution with deep reinforcement learning?,"
Neuroevolution can be used to evolve a network's architecture (and weights, of course). Deep reinforcement learning, on the other hand, has been proven to be extremely powerful at optimising the network weights in order to train really well-performing agents. Can we use the following pipeline?

search for the best network topology/weights through neuroevolution
train the best candidate selected above through DQN or something similar

This seems reasonable to me, but I haven't found anything on the matter.
Is there any research work that attempts to combine neuroevolution with deep reinforcement learning? Is it feasible? What are the main challenges?
","['reinforcement-learning', 'evolutionary-algorithms', 'reference-request', 'neuroevolution']",
Decide Number of input Parameters and Output Parameters - ANN,"
I have to create a Neural Network for regression purpose. Basically, I created a Model which predict next 5 values when we give past 6 values. 

I want to make a change in this neural network. For example,
when giving 6 past values I have to predict the next 10 values. 
Here, is there any issue of selecting the number of output dimension greater than the input dimension. Which type of parameters arrangement makes the Neural Network achieve good accuracy? do I have to decide the number of input parameters always greater than output parameters?
Thanks in Advance!
","['neural-networks', 'datasets', 'artificial-neuron', 'regression']","
This should be possible given the fact that ANNs have the ability to do the feature engineering and feature selection tasks by themselves.
This means that given a lesser number of input parameters, the model will be able to generate and select additional features by itself. You will obviously not be able to understand or model these features manually.
The only thing to keep in mind is that you will need a large dataset and a number of iterations before you are able to achieve a decent accuracy.
For example, there are networks that can generate image from classes.
Give this a read and here is an example where the output layer is larger than the input layer.
"
Is there any way to classify Document Image without OCR? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I have multiple invoices images which need to classify invoice types such as fright, utility, goods, etc. Is there any way to classify without OCR?
","['neural-networks', 'machine-learning', 'natural-language-processing', 'classification', 'optical-character-recognition']","
It is possible to classify invoice scans without passing through an OCR component if they are visually different (they demonstrate different visual features). On the other hand, if the invoices look very similar, then the classifier might not be very accurate.
Another challenge would be the number of images you need to train a deep network for image classification (you may start with pretrained models and only perform the finetuning of you do not have enough images). On the other hand, the combination of pretrained OCR models and NLP-based document classifier may not need that many samples for training (for this specific task).
"
How to add variation in the results of a neural networks?,"
I would like to create a neural network that converts text into handwriting for use with a pen plotter. Before I start on this project, I'd like to be sure that artificial intelligence is the best way to do this. A problem that I foresee with this approach is a lack of human like variation in the results. For example, the word ""dog"", when inputted into the network, would be the same every time, assuming I'm not missing something. I am interested if there is any way to vary the output of the network in a realistic way, even when the input is exactly the same. Could I use a second network to make the results more random, but also still look human-like? Any thoughts/ideas would be greatly appreciated.
","['neural-networks', 'machine-learning', 'human-like', 'handwritten-characters']",
Which online machine learning technique to use for multi-class classification problem with multiple inputs?,"
I have the following problem. We have $4$ separate discrete inputs, which can take any integer value between $-63$ and $63$. The output is also supposed to be a discrete value between $-63$ and $63$. Another constraint is that the solution should allow for online learning with singular values or mini-batches, as the dataset is too big to load all the training data into memory.
I have tried the following method, but the predictions are not good.
I created an MLP or feedforward network with $4$ inputs and $127$ outputs. The inputs are being fed without normalization. The number of hidden layers is $4$ with $[8,16,32,64]$ units in each (respectively). So, essentially, this treats the problem like a sequence classification problem. For training, we feed the non-normalized input along with a one-hot encoded vector for that specific value as output. The inference is done the same way. Finding the hottest output and returning that as the next number in the sequence.
","['machine-learning', 'ai-design', 'classification', 'multilayer-perceptrons', 'online-learning']","
I suggest using Data Stream algorithms to try on your problems since you are asking for ""online learning with singular values or minibatches as the dataset is too big too load all the training data into memory.""
MOA is a good choice for these algorithms. Hoeffding Trees is also a good first  choice to try.
"
Evolving Machine Learning [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 3 years ago.







                        Improve this question
                    



It seems to me that, right now, the key to making a good Machine Learning model is in choosing the right combination of hyper-parameters.
Firstly: Am I right in saying, if a model is able to tune it's own hyper-parameters, we have in some sense achieved a general intelligence system? Or a glimpse of an actual artificial intelligence system? 
I feel the answer lies in what one means by "" tune it's own hyper-parameters"". If it means to be able to reach Bayesian levels of performance on that task then theoretically, after the tuning, the model is able to perform at par or better than humans and so it seems the answer would be yes.
Secondly: I understand that hyper-parameter tuning is done intuitively. But there are a  set of general directions that is discernable looking at results.  Here I am talking about a heuristic approach to perfect a learning model. 
Consider an example:
Say I hardcode a model to, while training, observe gradient values. If the gradient is too large or the cost is highly oscillatory, then restart training with a smaller learning rate.
Then obtain metrics on a test set. If it is poor, then again restart training with regularisation or increased regularisation. 
It can also observe various plot behaviours, etc.
The point is maybe not every trick up a researcher's sleeve can be hardcoded. But a decent level of basic tuning can be done. 
Thirdly: Let us say, we have a reinforcement learning system on top of a supervised learning system. That is an RL network sets some hyper-parameters. The action then is to train with these hyper-parameters. The reward would be the accuracy on the test set. 
Is it possible that such a system could solve the problem of hyper-parameter tuning? 
","['machine-learning', 'reinforcement-learning', 'hyperparameter-optimization']","
Such a system can and does solve the problem of hyperparameter tuning. Google's AutoML does this. Here is another example that uses a Genetic Algorithm to breed new neural network structures. 
AutoML has been shown to outperform humans in the rate that it improves network designs. It seems to favour Residual Network style topologies. 
"
Is There A Need For Stochastic Inputs To Mimic Real-World Biology And Environment?,"
I'm kind of new to machine learning/AI, but I was wondering if using thresholds/fuzzy logic-like functions and even networks of dependent, stochastic variables that change over time (LTL maybe?), would be ample enough to emulate natural processes like emotions, hunger, maybe even pain.
My dilemma is whether creating a basic library to do this for the developer community is worth it if everything can be modeled more-or-less mathematically deterministic, even if the formulas are really complicated (see research like: https://engineering.stanford.edu/news/virtual-cell-would-bring-benefits-computer-simulation-biology).
My initial reasoning was biological processes are connected to psychological functionality (e.g., being hungry might make someone irritable, but that irritability may wear-off, which triggers different paths of thought but not others).  But these are so inter-dependent that it may be random or it is essentially PRNG, in order to properly simulate the mood fluctuations and biological processes computers don't have but humans do have.  
Would we be better-off waiting for these complex physical/neurological models to come out?
","['emotional-intelligence', 'biology']",
Is explainable AI more feasible through symbolic AI or soft computing?,"
Is explainable AI more feasible through symbolic AI or soft computing?
How much each paradigm, symbolic AI and soft computing (or hybrid approaches), addresses explanation and argumentation, where symbolic AI refers e.g. to GOFAI or expert systems, and soft computing refers to machine learning or probabilistic methods.
","['comparison', 'statistical-ai', 'explainable-ai', 'symbolic-ai']",
Limits for a bottleneck,"
I have some 64x64 pixels frames from a (simulated) video, with a spaceship moving on a fixed background. The spaceship moves in a straight line with constant velocity from left to right (along the x-axis), and the frames are from equal time intervals.  I can also place the ship at different y positions and let it move. In total I have 8 y positions and 64 frames for each y position (the details don't matter that much). Intuitively, as the background is fixed, and the shape of the ship is the same, all the information to reconstruct the image is found in the x and y position of the spaceship. What I am trying to do is to have a NN with an encoded and a decoder and a bottleneck in the middle and I want that bottleneck to have just 2 neurons. Ideally, the network would learn in these 2 neurons some function of x and y in the encoder, and the decoder would invert that function to give the original image. Here is my NN architecture (in Pytorch):
class Rocket_E_NN(nn.Module):
    def __init__(self):
        super().__init__()

        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 4, 2, 1),          # B,  32, 32, 32
            nn.ReLU(True),
            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16
            nn.ReLU(True),
            nn.Conv2d(32, 64, 4, 2, 1),          # B,  64,  8,  8
            nn.ReLU(True),
            nn.Conv2d(64, 64, 4, 2, 1),          # B,  64,  4,  4
            nn.ReLU(True),
            nn.Conv2d(64, 256, 4, 1),            # B, 256,  1,  1
            nn.ReLU(True),
            View((-1, 256*1*1)),                 # B, 256
            nn.Linear(256, 2),             # B, 1
        )

    def forward(self, x):
        z = self.encoder(x)
        return z

class Rocket_D_NN(nn.Module):
    def __init__(self):
        super().__init__()
        self.decoder = nn.Sequential(
            nn.Linear(2, 256),               # B, 256
            View((-1, 256, 1, 1)),               # B, 256,  1,  1
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 64, 4),      # B,  64,  4,  4
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 64, 4, 2, 1), # B,  64,  8,  8
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  32, 16, 16
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 3, 4, 2, 1),  # B, 3, 64, 64
        )

    def forward(self, z):
        x = self.decoder(z)
        return x

And this is the example of one of the images that I have (it was much higher resolution but I brought it down to 64x64):

So after training it for around 2000 epoch with a bs of 128, with Adam, trying several LR schedules (going from 1e-3 to 1e-6) I can't get the loss below an RMSE of 0.010-0.015 (the pixel values are between 0 and 1). The reconstructed image looks ok by eye, but I would need a better loss for the purpose of my project. Is there any way I can push the loss lower, or am I asking too much from the NN to distill all the information in these 2 numbers?
","['objective-functions', 'autoencoders', 'pytorch']",
Will artificial intelligence cause mass unemployment?,"
Everyone is afraid of losing their job to robots. Will or does artificial intelligence cause mass unemployment?
","['agi', 'social', 'futurism']","
The nuanced, boring answer is that it depends on your definition of AI. Most people wouldn't say that the rule-based systems designed in the 70's are AI. The amazing leaps in machine learning are almost taken for granted as well (think about how normal speech and facial recognition have become). This is known as the AI effect; when we become accustomed to the technology, it loses it's 'magical aspect' and is thus no longer labelled as AI.
Since AI is so diverse and difficult to define, the question becomes incredibly abstract. Did Siri cause all secretaries to become unemployed? Did TurboTax replace all accountants? Some parts of AI will affect jobs, or even make them redundant yes. On the other hand, it will give rise to new jobs as well. It is therefore impossible to generalize it as 'AI will cause massive unemployment'. 
This is not a new phenomenon, however, it has been part of the human economy ever since the industrial revolution (probably even before that, but I am not a historian). The invention of the car crippled the horse-and-wagon industry, but it brought along new jobs as well. 
"
Neural networks when gradient descent is not possible,"
I am looking for an example in which it is simply impossible to use some sort of gradient descent to train a neural network. Is this available? 
I have read quite some papers about gradient-free optimization tools, but they always use it on a network for which you can also use gradient descent. I want to have a situation in which the only option to train the network is by, for example, a genetic algorithm. 
","['neural-networks', 'gradient-descent']",
How can I use the bottleneck layer of the U-net to calculate the similarity between two images?,"
I would like to use the bottleneck layer of U-Net (the last layer of the encoder) to calculate the similarity between two images. For that, I have to somehow flatten the last layer of the encoder. In my opinion, there are two approaches:

Take the last layer which in my case is $4 \times 4 \times 16$ and flatten it to 1D
Add a dense before the decoder and then reshape the dense 1D layer into 3D

For the second case, I am not sure how this would affect the network. Arbitrarily reshaping a 1D array into a 3D tensor. Could that introduce weird artifacts? Does someone have experience in a similar problem?
","['convolutional-neural-networks', 'unsupervised-learning', 'u-net']",
How can I stabilise a recurrent neural network used for binary classification?,"
Im looking for some help with my neural network. Im working on a binary classification on a recurrent neural network that predicts stock movements (up and down) Lets say Im studying Eur/Usd, Im using all the data from 2000 to 2017 to train et Im trying to predict every day of 2018. 
The issue Im dealing with right now is that my program is giving me different answers every time I run it even without changing anything and I dont understand why?
The accuracy during the train from 2000 to 2017 is around 95% but Ive noticed another issue. When I train it with 1 new data every day in 2018, I thought 2 epochs was enough, like if it doesnt find the right answer the first time, then it knows what the answer is since the problem is binary, but apparently that doesnt work. 
Do you guys have any suggestion to stabilize my NN?
","['machine-learning', 'classification', 'recurrent-neural-networks', 'prediction']","
Firstly, dealing with the issue that the program gives different answers every time without making any changes can be due to a couple of things.

Assigning random values to weights and bias. This can be solved by setting a seed manually at the start of the program.
Make sure you have set the model to the testing mode after training. For some frameworks, this has to be done manually.

Secondly, regarding your expected results.
To generate a proper accuracy metric, you will have to sample your dataset into training and testing data, making sure there is now overlap between them. This might be an issue as you have stated training on data till 2017 and then again training on data of 2018.
Lastly, don't expect that the model will know that the output is wrong and directly change it because it's binary classification. This is not how neural networks work. The model fits the solution better by gradually updating its weights and biases over a number of iterations. So it will take a number of epochs to learn new trends in the data for 2018.
"
"Can HMM, MRF, or CRF be used to classify the state of a single observation, not the entire observation sequence?","
I learn that the Viterbi algorithm used for Hidden Markov Model (HMM) can classify a sequence of hidden states from the corresponding observations; Markov Random Field (MRF) and Conditional Random Field (CRF) can also do it. 
Can these algorithms be used to classify a single future state?
","['markov-chain', 'sequence-modeling', 'time-series', 'hidden-markov-model']","
Yes this is possible and is exactly what a Markov process aims to accomplish.
The Hidden Markov Model can be considered a simple dynamic Bayesian network. This means that it will generate a probabilistic outcome for a number of states (classes) given a sequence of inputs.
This can be used for a classification task based on a threshold probability to make a confirmed decision.
Let's take a simple example

The above example is a model to determine  how a person will be feeling given some sample data of the previous days.
# Sample data
data = ['Healthy', 'Healthy', 'Fever']

Using this data, it's possible to calculate the probability of each possible state (outcome).
# Probability after 2 Healthy and 1 Fever day
P = 0.6 * 0.7 * 0.3

# Final day state 'Fever'
# Calculating probabilities for each state
Dizzy  = P * 0.6
Cold   = P * 0.3
Normal = P * 0.1

We can calculate the probability of each state and this will be unique for every unique sequence of historical data.
"
What knowledge is required for understanding the AlphaZero paper?,"
My goal is to understand AlphaZero paper published by deepmind. I'm beginning my journey trying to get the basic intuition of reinforcement learning from the book by Barto and Sutton.
As per my background, I'm familiar with MDPs, value iteration and policy iteration.
I wanted to ask until what chapter of Barto and Sutton's book is one required to read in order to fully comprehend AlphaZero's paper. Monte-Carlo Tree Search is discussed in Chapter-8 of the book. Will it be enough till that? Or would I be needing more resources apart from this book?
","['reinforcement-learning', 'papers', 'alphazero', 'sutton-barto']",
Deep Reinforcement Learning: Rewards suddenly dip down,"
I am working on a deep reinforcement learning problem. The policy network has the same architecture as the one Deepmind published in 'Playing Atari with Deep Reinforcement Learning'. I am also using Prioritized Experience Replay. In the initial stage the behavior seems to be normal, i.e the agent is learning gradually. However, after a while the rewards suddenly go down by a lot. The TD erros also seem to be going up at the same time. I'm not sure how to interpret this problem. 
My hypotheses are:

The policy network is overfitting
Some filters fail to activate thereby misrepresenting the state information

I would really appreciate if you guys could give me some tips to narrow down this problem debug it. Cheers.
","['reinforcement-learning', 'q-learning', 'dqn']","
Without digging into your diagnostics more deeply, on it's face this seem like a local optima issue. Assuming you are optimizing via GD, there are many local optima that a network or agent in this case can converge to and stay at which will cause symptoms like seeing above.
With that being said, assuming this is our issue, here are some things you can try:

Regularization, try adding dropout or L2 and see how that affects convergence and learning.
Adjust network architecture, number of layers, nodes, etc. 
Try a different type of RL(Q learning for example), this will be dependent on your problem of course.
Adjust starting seed. Assuming you have a static seed used for weight initialization, you will always converge to the same solution. It could be as simple as adjusting the seed value.

If all these steps fail, you likely have a deeper issue at work, and I would suggest coming back after with some additional detail if this does not succeed.  
"
How to handle proper names or variable names in word2vec?,"
The input in word2vec is known word (spellings), each tagged by its ID.

But if you process real text, there can be not only dictionary words but also proper nouns like human names, trade marks, file names , etc, how to make an input for that? 
Is you consider some input where items are variables, like the meaning of input would be x = something, and after some time you acces to x value and define some other stuff with it. That would be format for this input, and will this approach work at all?

","['neural-networks', 'recurrent-neural-networks']","
Word2vec works on the concept of  typical word co-occurrences. This means that it will work well only for words that occur frequently in the dataset. So proper nouns will not play any role in training the model. You can keep the proper nouns as they are or use only the words the occur more frequently than some threshold value based on the size of your dataset.
Once you use the value stored in variable x for something, and then change the stored value, it will not reflect anywhere unless you us use the variable x again somewhere in the program.
# Example
x = ""something something""
print(x + ""..."")

# Result
something something ...

# Changing x
x = ""new value""

# This new value of x will not reflect anywhere in the program
# Unless you use the variable x again.

"
Viterbi versus filtering,"
In Chapter 15 of Russel and Norvig's Artificial Intelligence -- A Modern Approach (Third Edition), they describe three basic tasks in temporal inference: 

Filtering, 
Likelihood, and 
Finding the Most Likely Sequence.

My question is on the difference between the first and third task.  Finding the Most Likely Sequence determines, given evidences $e_1,\dots,e_n$, the most likely sequence of states $S_1,\dots,S_n$.  This is done using the Viterbi algorithm.  On the other hand, Filtering provides the probability distribution on states after seeing $e_1,\dots,e_n$.  You could then pick the state with the highest probability, call it $S'_n$. I am guessing that $S'_n$ should always be equal to $S_n$.  Likewise, you can already do the same after any prefix $e_1,\dots,e_i$, again picking the most likely state $S'_i$.  I would love to have a simple example where $S'_1,\dots,S'_n$ is not equal to the sequence $S_1,\dots,S_n$ produced by the  Viterbi algorithm.
",['probability'],"
Welcome to AI.SE @vdbuss, and great first question!
This point is touched on in Section 15.2.3 (page 576 in my copy), in the second paragraph, and there's a good exercise at the end of the chapter (15.4) that is designed to get you to think through exactly why these are different procedures. If you want to really absorb it, I suggest trying to work out that exercise! If you want the quick answer, read on.
The basic action of filtering is to generate a probability distribution $P(X_{t+1} | e_{1:t+1})$ using only two pieces of information, specifically the current state distribution $P(X_{t} | e_{1:t})$, and the new piece of evidence $e_{t+1}$. So, when computing the most likely sequence, the algorithm cannot take into account the sequences that are actually possible, while Vitirbi can.
Here's a simple example: suppose I tell you that I'm going to drop you in a maze at one of two locations. I drop you near the top right corner with probability 0.75, and near the bottom left corner with probability 0.25. Suppose further that a Grue is known (with certainty) to live somewhere near the bottom left corner. Using filtering, your maximum aposterori estimate for your location after being dropped in the maze ($t=1$) is that you are in the top right corner. You then move 1 step to the right and can see a Grue. Clearly your estimate for your position in the second timestep $(t=2)$ must be the bottom left, because Grues only live there. But, you definitely can't end up moving to the bottom left by moving right from the top right, so your sequence has probability zero overall, despite using the maximum aposterori estimate for position at every step. To avoid this, Vitirbi uses a linear amount of extra space to select the maximum aposterori sequence, which in this case is clearly that you are near the bottom left in both timesteps.  
"
How to choose our data set wisely?,"
I have a couple of questions and I was wondering if you could answer them.
I have a bunch of images of the cars, side view only. I want to train the model with those images. My objects of interest are 3 types of trucks that have different trailers. I Rarely see two target object at one image (maybe 1 or 2 in every 1000 images). However, I do see other types of cars that I do not want to detect.
My questions are:

Do you think I should tackle this problem as a detection task or classification task? (for example, should I consider multi-label classification or omit those pictures)
Should I also include other vehicles that I do not want to detect in my training dataset? let say I do not assign bounding box to them but include them in training dataset just to make the system robust. 

I trained YOLO with 200 images, sometimes the trained model confused and detected the wrong object that is not in any of classes, this happens when training with 2000 images per class? 
Is this due to a small number of dataset or it is because of not including those images with no bounding boxes?
Thank you in advance!
","['classification', 'datasets', 'object-detection']",
What is the difference between GAT and GaAN?,"
I was looking at two papers 

Graph Attention Networks (GAT) by Petar Velikovi and
GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs by Jiani Zhang.

I'm trying to implement the second paper and I'm having some troubles understanding the differences between GAT and GaAN. By looking at equation 1 in GaAN paper I can see only two differences from GAT. 

The first difference is that they are doing a dot product with the initial feature map and 
Have another fully connected layer to project the result. 

Is there something else that I'm missing?
","['neural-networks', 'deep-learning', 'comparison', 'attention', 'geometric-deep-learning']",
What are the common myths associated with Artificial Intelligence?,"
What are some interesting myths of Artificial Intelligence and what are the facts behind them?
","['philosophy', 'agi', 'mythology-of-ai']","
As Artificial Intelligence is rapidly invading in our lives the myths around AI is also fabricating rapidly. Before getting into details one need to get clear off from this myths.
Myth 1: AI will take away our jobs:
Reality: AI is not completely different from other technologies and AI will not take away jobs but AI will change the way we work and helps us to increase the productivity by removing monotonous works.
Myth 2: Artificial intelligence will take over the world:
Reality: AI controlling the world. According to me it will not possible unless we give it that power. AI or robots will assist in our work and helps us to solve some tedious works that are difficult for human to solve easily.
Myth 3: Intelligent machines can learn on their own
Reality: It seems that a Intelligent machine can learn by it own. But the fact is that a AI Engineer or AI specialist should develop the algorithm and feed the machine with datasets and instructions and continuous monitoring should be done and most importantly regular update of software should be done.
Myth 4: Artificial Intelligence, Machine learning and Deep learning all three are same: 
Reality: No not at all. To be clear machine learning is a part of AI and deep learning is the subset of ML. All three- AL, ML and DL are different but they are inter related with each other.
"
When does the selection phase exactly end in MCTS?,"
All sources I can find provide a similar explanation to each phase. 
In the Selection Phase, we start at the root and choose child nodes until reaching a leaf.  Once the leaf is reached (assuming the game is not terminated), we enter the Expansion Phase. 
In the Expansion Phase, we expand any number of child nodes and select one of the expanded nodes. Then, we enter the Play-Out Phase.
Here is my confusion. If we choose to only expand a single node, the nodes that were not expanded will never be considered in future selections as we only select child nodes until a leaf is reached during the Selection Phase. Is this correct? If not, what am I misunderstanding about the Selection Phase?
","['game-ai', 'monte-carlo-tree-search']",
What is the credit assignment problem?,"
In reinforcement learning (RL), the credit assignment problem (CAP) seems to be an important problem. What is the CAP? Why is it relevant to RL?
","['reinforcement-learning', 'definitions', 'credit-assignment-problem']",
How do I know if the assumption of a static environment is made?,"
An important property of a reinforcement learning problem is whether the environment of the agent is static, which means that nothing changes if the agent remains inactive. Different learning methods assume in varying degrees that the environment is static. 
How can I check if and (if so) where in the Monte Carlo algorithm, temporal difference learning (TD(0)), the Dyna-Q architecture, and R-Max a static environment is implicitly assumed? 
How could I modify the relevant learning methods so that they can in principle adapt to changing environments? (It can be assumed that $\epsilon$ is sufficiently large.)
","['reinforcement-learning', 'monte-carlo-methods', 'temporal-difference-methods', 'environment', 'dyna']",
Changes in flow detection neural network?,"
Do you have any advice, what architecture of neural network is the best for following task?
Let input be some (complex function), the neural network gains a flow of its values, so I guess there will be some kind of RNN or CNN? 
The output is classifier like is the function same or not

If the neural network thinks, that the input is still the same function, the output is 0. 
If the input function changes, the output will be 1.

The input function is of course not one value or simple math function (what will be trivial) but may be really sophisticated. So the neural network learns abstraction about same and different over any complex flow ?
How would you approach to that task ?
","['neural-networks', 'recurrent-neural-networks', 'pattern-recognition', 'function-approximation']","
A network is able to fit to a certain function over several iteration while training. Now you want the model to be able to detect a change in a list of inputs from the function. This is not possible without first training the model on some data.
Say you want to use a simple function
# Sample function
f(x) = x

Say you create inputs for function using sets of 3 integers for x.
# Data
f1 = [[1, 2, 3],
      [4, 5, 6],
      ...      ]

# Some random values
f2 = [[1, 4, 19],
      [16, 35, 36],
      ...      ]

Now if you use data f1 labelled as 0 and data f2 labelled as 1, in the best case the ANN will only learn to differentiate between data from function f1 and some other data.
To detect change, first the model has to fit to a certain function, this requires it to be trained over a number of epochs. Then it will be able to detect if the values don't match the function, but such a model will only be able to differentiate a single function.
"
Adding BERT embeddings in LSTM embedding layer,"
I am planning to use BERT embeddings in the LSTM embedding layer instead of the usual Word2vec/Glove Embeddings. What are the possible ways to do that? 
","['deep-learning', 'keras', 'word-embedding', 'long-short-term-memory', 'bert']","
Instead of using the Embedding() layer directly, you can create a new bertEmbedding() layer and use it instead.
# Sample code
# Model architecture

# Custom BERT layer
bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)

# Build the rest of the classifier 
dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)
pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)

model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(...)

This article will walk you through the entire process of creating the custom BERT layer along with example code. Give it a read.
"
Why not go another layer deeper with Auto-AutoML?,"
So I'm finding AutoML to be pretty interesting but I'm still learning how it all works. I've played with the incredibly broken AutoKeras and got some decent results.
The question is, if you are using a NN to optimize the architecture of another network, why not take it another layer deeper and use another network to find the optimum architecture for your Parent network with a grand-parent network?
The problem doesn't necessarily need to expand exponentially as the grand-parent network could do few-shot training sessions on the parent network which itself is doing few or one-shot training. 
","['ai-design', 'automation', 'meta-learning']","
Logically it is possible, but you will just end up complicating the entire task.
The aim of AutoML is to provide a drop in solution to the customers. To do this, a trained network decides and generates the model architecture. This is done so that anyone with basic experience is able to integrate the solution into their systems.
Currently, the complicated architectures and networks require experienced data scientist to build, train and deploy. To overcome this bottleneck and make ML accessible to all, AutoML is being developed.
So adding another grand-parent network to optimize the autoML network will just complicate the task in terms to computation time and hyper parameter optimization. 
In case we decide to add another network, now the researchers must look at this network and tune it in regards to the inner network and the model both. This means more work and no direct way to understand how the hyper parameters are affecting the final results.
"
Are Relational DBs and SQL used in Expert Systems?,"
In the book Prolog Programming for Artificial Intelligence, a large and intricate chapter (chapter 14) is dedicated to Expert Systems. In these systems, a  knowledge-database is represented through facts and rules in a declarative manner, and then we use the PROLOG inference engine to derive statements and decisions.
I was wondering: are there any examples of expert systems that represent knowledge through a standard Relational Database approach and then extract facts through SQL queries? Is there any research in this area? If not, why is a rule-based approach preferred?
","['reference-request', 'knowledge-representation', 'expert-systems', 'rule-based-systems']","
A recent research example is the ""Grind"" system. Take a look at the paper Computing FO-Rewritings in $\mathcal{E} \mathcal{L}$ in Practice: from Atomic to Conjunctive Queries (2018) by Peter Hansen and Carsten Lutz. Here's the abstract.

A prominent approach to implementing ontology-mediated queries (OMQs) is to rewrite into a first-order query, which is then executed using a conventional SQL database system. We consider the case where the ontology is formulated in the description logic $\mathcal{E} \mathcal{L}$ and the actual query is a conjunctive query and show that rewritings of such OMQs can be efficiently computed in practice, in a sound and complete way. Our approach combines a reduction with a decomposed backwards chaining algorithm for OMQs that are based on the simpler atomic queries, also illuminating the relationship between first-order rewritings of OMQs based on conjunctive and on atomic queries. Experiments with real-world ontologies show promising results.

"
Why are state transitions in MDPs probabilistic rather than deterministic?,"
I've read that for MDPs the state transition function $P_a(s, s')$ is a probability. This seems strange to me for modeling because most environments (like video games) are deterministic.
Now, I'd like to assert that most systems we work with are deterministic given enough information in the state (i.e. in a video game, if you had the random number seed, you could predict 'rolls', and then everything else follows game logic).
So, my guess for why would MDP state transitions are probabilities is because the state given to the MDP is typically a subset (i.e. from feature engineering) of total information available. That, and of course to model non-deterministic systems.
Is my understanding correct?
","['machine-learning', 'reinforcement-learning', 'markov-decision-process']",
Questions regarding rrn-writer by Robin Sloane?,"
https://github.com/robinsloan/rnn-writer
I preface this by saying I do not know much about this topic, only that I have an intense interest in it, so I'm hoping I can make my questions as clear as possible.
This writing assistant was released with full code and instructions on how to make it work.  I was halfway through this process when I was told, basically, that this could not work on a Windows PC.  Torch, specifically, either doesn't work on PC, or doesn't work very well, and I am hesitant to continue.
First of all, does anyone know if that's true?
If it is true, is it theoretically possible to recreate this in a different way that will work on Windows PCs?
If so, has anyone ever done it before, or know how to do it?
If there is a way to make Torch work on PCs, is someone willing to tell me how?
I apologize if this isn't meant for this specific area of discussion.  I just don't know where else to go with my questions that I will get any kind of helpful responses.  Even if it's just telling me where I can go for more pertinent responses, that would still be appreciated.
Thank you for any help you might be willing to give.  Please let me know if I need to clarify anything.
","['neural-networks', 'machine-learning', 'deep-learning']",
Further Normalization of Standardized data - ANN,"
I want to develop a regression model using the artificial neural network. For developing such a model I use standardised ( z-score normalised ) data.
given below is the sample data set. Here MAX is the real data But I am using MAX-ZS (these values are continues)

So my question is while developing the model do I have to perform further normalization such as Min-Max scaling on my training data?
Any Kind of help is appreciated!
","['neural-networks', 'datasets', 'deep-neural-networks']","
Data scaling or normalization is a process of making model data in a standard format so that the training is improved, accurate, and faster.
So you just have to scale the data once. Doesn't matter what scaler you are using. Just make sure to initialize the scaler with the training data and then use the same parameters to scale the test data.
The z-score normalized data (MAX-ZS) can be used directly to train the network.
"
What is the use of softmax function in a CNN?,"
What is the use of softmax function? Why was it used at the end of fully connected layer in convolution neural network?
","['neural-networks', 'convolutional-neural-networks']","
The main purpose of the softmax function is to transform the (unnormalised) output of $K$ units (which is e.g. represented as a vector of $K$ elements) of a fully-connected layer to a probability distribution (a normalised output), which is often represented as a vector of $K$ elements, each of which is between $0$ and $1$ (a probability) and the sum of all these elements is $1$ (a probability distribution). 
In the case of a classification task, the $i$th element of the vector produced by the softmax function corresponds to the probability of the input of the network of belonging to the $i$th class (e.g. a dog).
"
"If loss reduction means model improvement, why doesn't accuracy increase?","
Problem Statement
I've built a classifier to classify a dataset consisting of n samples and four classes of data. To this end, I've used pre-trained VGG-19, pre-trained Alexnet and even LeNet (with cross-entropy loss). However, I just changed the softmax layer's architecture and placed just four neurons for that (because my dataset includes just four classes). Since the dataset classes have a striking resemblance to each other, this classifier was unable to classify them and I was forced to use other methods.
During the training section, after some epochs, loss decreased from approximately 7 to approximately 1.2, but there were no changes in accuracy and it was frozen on 25% (random precision). In the best epochs, the accuracy just reached near 27% but it was completely unstable.
Question
How is it justifiable? If loss reduction means model improvement, why doesn't accuracy increase? How is it possible to the loss decreases near 6 points (approximately from 7 to 1) but nothing happens to accuracy at all?
","['deep-learning', 'classification', 'training', 'objective-functions']","
Loss reduction means model improvement, it does not in the wrong setup, wher random choise produces least loss. So it is some critical setup error. What classes do you have?
I got also thet recently experimenting with an encoder with too narrow coding layer - it just EQUILIZES the output with average values cause this state has minimum loss.
"
Is the Markov property assumed in the forward algorithm?,"
I'm majoring in pure linguistics (not computational), and I don't have any basic knowledge regarding computational science or mathematics. But I happen to take the ""Automatic Speech Recognition"" course in my graduate school and struggling with it.
I have a question regarding getting the formula for a component of the forward algorithm.
$$
\alpha_t(j) = \sum_{i=1}^{N} P(q_{t-1} = i, q_t=j, o_1^{t-1}, o^t|\lambda)
$$
When $q$ is a hidden state, $o$ is a given observation, and $\lambda$ contains transition probability, emission probability and the start/end state.
Is the Markov assumption (the current state is only dependent upon the one right before it) assumed here? I thought so, because it contains $q_{t-1}=i$ and not $q_{t-2}=k$ or $q_{t-3}=l$.
","['machine-learning', 'markov-property', 'hidden-markov-model']",
Would an artificial general intelligence have to be Turing complete?,"
For the purposes of this question, let's suppose that an artificial general intelligence (AGI) is defined as a machine that can successfully perform any intellectual task that a human being can [1].
Would an AGI have to be Turing complete?
","['philosophy', 'agi', 'theory-of-computation', 'computational-theory-of-mind', 'turing-completeness']","
A system is Turing complete if it can be used to simulate any Turing machine. 
Given the Church-Turing thesis (which has not yet been proven), a human brain can compute any function that a Turing machine can (given enough time and space), but the reverse is not necessarily true, given that the human brain might be able to compute more functions than a Turing machine. Intuively, humans are thus Turing complete (even though, to prove this, you need a formal model of the human), that is, given enough time and space, a human can compute anything that a Turing machine can. 
Hence, an AGI, defined as an AI with human-level intelligence, needs to be Turing complete, otherwise there would be at least one function that a human can calculate but the AGI cannot, which would not make it as general as a human.
"
How is G(z) related to x in GAN proof?,"
In the proofs for the original GAN paper, it is written:
$$_x p_{data}(x) \log D(x)dx+_zp(z)\log(1D(G(z)))dz
=_xp_{data}(x)\log D(x)+p_G(x) \log(1D(x))dx$$
I've seen some explanations asserting that the following equality is the key to understanding:
$$E_{zp_z(z)}log(1D(G(z)))=E_{xp_G(x)}log(1D(x))$$
which is a consequence of the LOTUS theorem and $x_g = g(z)$. Why is $x_g = g(z)$?
","['neural-networks', 'machine-learning', 'math', 'generative-adversarial-networks', 'proofs']",
Which explainable artificial intelligence techniques are there?,"
Explainable artificial intelligence (XAI) is concerned with the development of techniques that can enhance the interpretability, accountability, and transparency of artificial intelligence and, in particular, machine learning algorithms and models, especially black-box ones, such as artificial neural networks, so that these can also be adopted in areas, like healthcare, where the interpretability and understanding of the results (e.g. classifications) are required.
Which XAI techniques are there?
If there are many, to avoid making this question too broad, you can just provide a few examples (the most famous or effective ones), and, for people interested in more techniques and details, you can also provide one or more references/surveys/books that go into the details of XAI. The idea of this question is that people could easily find one technique that they could study to understand what XAI really is or how it can be approached.
","['reference-request', 'ethics', 'explainable-ai']","
There are a few XAI techniques that are (partially) agnostic to the model to be interpreted

Layer-wise relevance propagation (LRP), introduced in On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation (2015)
Local Interpretable Model-agnostic Explanations (LIME), introduced in ""Why Should I Trust You?"" Explaining the Predictions of Any Classifier (2016)
Model Agnostic Contrastive Explanations Method (MACEM), introduced in Model Agnostic Contrastive Explanations for Structured Data (2019)

There are also ML models that are not considered black boxes and that are thus more interpretable than black boxes, such as

linear models (e.g. linear regression)
decision trees
naive Bayes (and, in general, Bayesian networks)

For a more complete list of such techniques and models, have a look at the online book Interpretable Machine Learning: A Guide for Making Black Box Models Explainable, by Christoph Molnar, which attempts to categorise and present the main XAI techniques.
"
Neural Network for Error Prediction of a Physics Model?,"
I have physical model prediction data as well as actual data. From this I can calculate the error of each prediction data point through simple subtraction. I am hoping to train a neural network to be able to assign an error to the input of the physical model.
My current plan is to normalize the error of each data point and assign it as a label to each model input. So the NN would be trained (and validated)on a 1000 data points with the associated error as a label. Once the model is trained I would be able to input one data point and the output of the neural network would be a single class, that is the error. The purpose this would serve would be to tune the physical prediction model. Would this kind of architecture work? If so, would you recommend a feedforward or RNN? Thank you.
",['neural-networks'],
How to make a distinction between item feature and environment feature?,"
My data is stock data with features such as stocks' closing prices.I am curious to know if I can put the economy feature such as 'national interest rate' or 'unemployment rate' besides each stocks' features.
Data:
  Date  Ticker  Open  High  Low  Close  Interest  Unemp. 
  1/1    AMZN    75    78     73   76     0.015     0.03
  1/2    AMZN    76    77     72   72     0.016     0.03
  1/3    AMZN    72    78     76   77     0.013     0.03
  ...    ...     ...   ...    ...  ...    ...       ...
  1/1    AAPL    104   105    102  102    0.015     0.03
  1/2    AAPL    102   107    104  105    0.016     0.03
  1/3    AAPL    105   115    110  111    0.013     0.03
  ...    ...     ...   ...    ...  ...    ...      ...

As you can see from the table above, daily prices of AMZN and AAPL are different but the Interest and Unemployment rates are the same. Can I feed the data to my neural network like the table above?
In other words, can I put the individual stocks' information besides the environment feature such as interest rates?
","['neural-networks', 'deep-learning', 'data-science']","

I am curious to know if I can put the economy feature such as 'national interest rate' or 'unemployment rate' besides each stocks' features.

The variables are macro-econometric and they, in general, seem to have some influence on stocks' prices. This inclusion might as well increase your model's prediction accuracy. You can definitely use them as predictors. As mentioned in comments - Experimentation is the way to go.

Can I feed the data to my neural network like the table above?

In general, you can have any kind of numeric variables as input to a neural network. Things will work out fine. The important thing is the selection of relevant predictor variables that, potentially, have some relationship with the response variable.
"
How do I perform object detection if there is only one type of object?,"
How do I do object detection (or identify the location of an object) if there is only one kind of object, and they are more of less similar size, but the picture does not look like standard scenes (it is detection of drops on a substrate in microscopic images)?  Which software is good for it?
","['deep-learning', 'object-detection']","
The hardest part is image annotation: here the difference between object recognition and object detection becomes important. If you just want to answer the question ""does this image contain object X?"", then you just need to provide as many images that contain object X as possible, together with as many images  that don't contain object X (but are otherwise similar). However if you want to answer the question ""Where exactly object X is located in this image?"" then you will need to manually provide a bounding box for each instance of object X in each image. Obviously, the second scenario is a lot more labor intensive. 
After you've done this part, train either a binary image classifier (typically this will be a convolutional neural network) on your annotated images (split them into train and test partitions), or an object detector (googling ""custom object detection"" produces lots of code examples how to train it (e.g. https://towardsdatascience.com/tutorial-build-an-object-detection-system-using-yolo-9a930513643a start with Step 2B).
"
How does ARKit's Facial Tracking work?,"
iPhone X allows you to look at the TrueDepth camera and reports 52 facial blendshapes like how much your eye is opened, how much your jaw is opened, etc.
If I want to do something similar with other cameras (not TrueDepth), what are my alternative methods? Currently, I just use a simple ConvNet which takes in an image and predict 52 sigmoid values.
What do you think could be the underlying technology behind ARKit Face Tracking?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'facial-recognition']",
What approach should I take to model forecasting problem in machine learning?,"
I have a dataset which contains 4000k rows and 6 columns. The goal is to predict travel time demand of a taxi. I have read many articles regarding how to approach the problem. So, every writer tell his own way. The thing which I have concluded from all my readings is that I have to use multiple algorithms and check the accuracy of each one. Then I can ensemble them by averaging or any other approach.
Which algorithms will be best for my problem accuracy-wise? Some links to code will be helpful for me.
I currently only have training set of data. After I work on it, it will be evaluated on any testing set by my professor. So, what should I do now? Either split data I have into my own testing and training set or separately generate dummy data as a testing set?
","['machine-learning', 'models', 'forecasting']","
In general, this type of problem is called a regression problem since the target variable (i.e. travel time) can take any value in a continuous domain. In theory, you can use any regression algorithms (a subset supervised learning techniques) to solve this problem. Some of the most popular ones are linear regression, K-nearest neighbor (regressor), and neural networks.
As you observed already, different algorithms result in (sometimes significantly) different results. Also, the parameter configurations (e.g., number of hidden layers in Neural Networks) can make a big difference. Sometimes, ensembling different models can be helpful, but in general, you should try to avoid overfitting (when your model is more complex than your data such that it memorizes the training set instead of learning it!). That may result in a very good performance on your training set but perform very poorly on your professor's testing set.
What I would do is:

exploring the dataset to see what are the contributing factor in travel time (any correlation between the columns).
cleaning and preprocessing my dataset (duplicates, null values, outliers)
reshaping my dataset if needed (normalizing some columns, merging or splitting columns)
dividing my dataset to training and evaluation subsets (so I train on one part and test on the other part to avoid overfitting)
choosing a simple baseline, applying and measuring the accuracy metrics.
trying to fine tune parameters of my baseline or trying other more advanced techniques.
comparing the results and improving any part of the pipeline when necessary (more/less cleaning, parameter tuning, ensembling).

"
Why are we using all hyperparameters in RL? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I am new in RL and I am trying to understand why do we need all these hyperparameters.
Can somebody explain me why we use them and what are the best values to use for them?

total_episodes = 50000        # Total episodes
total_test_episodes = 100     # Total test episodes
max_steps = 99                # Max steps per episode
learning_rate = 0.7           # Learning rate
gamma = 0.618                 # Discounting rate
Exploration parameters
epsilon = 1.0                 # Exploration rate
max_epsilon = 1.0             # Exploration probability at start
min_epsilon = 0.01            # Minimum exploration probability
decay_rate = 0.01             # Exponential decay rate

I am currently working on taxi_v2 problem from GYM.
Link: https://learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/
","['machine-learning', 'reinforcement-learning', 'deep-rl']",
Why feeding the correct output as input during training of seq2seq models?,"
I've read about seq2seq for time-series and it seemed really promising, but, when I went to implement it, all the tutorials I've found use the correct output as input to the decoder phase during training, instead of using the actual prediction made by the cell before it. Is there a reason why not do the latter?
I've been using the tutorial from here
But all the other tutorials that I've found followed the same principle.
","['deep-learning', 'time-series', 'sequence-modeling', 'seq2seq', 'teacher-forcing']","
The reason why you would use the ground truth as input to the decoder is to lean the testing distribution. From what I've seen so far, most of the papers are using scheduled sampling (Bengio et al.). Meaning that you will introduce a new term $p$, which will be the probability of the network to feed as input its own prediction. Initially, $p$ will be very small, so that the network will use the ground truth, but later the more iterations pass by, the probability will decrease and the network will start using its own predictions.
"
Deep Q-Network (DQN) to learn the game 2048,"
I am trying to build a Deep Q-Network (DQN) agent that can learn to play the game 2048. I am orientating myself on other programs and articles that are based on the game snake and it worked well (specifically this one).
As input state, I am only using the grid with the tiles as numpy array, and as a reward, I use (newScore-oldScore-1) to penalize moves that do not give any points at all. I know that this might not be optimal, as one might as well reward staying alive for as long as possible, but it should be okay for the first step, right? Nevertheless, I am not getting any good results whatsoever.
I've tried to tweak the model layout, the number of neurons and layers, optimizer, gamma, learning rates, rewards, etc.. I also tried ending the game after 5 moves and to optimize just for those first five moves but no matter what I do, I don't get any noticeable improvement. I've run it for thousands of games and it just doesn't get better. In fact, sometimes I get worse results than a completely random algorithm, as sometimes it just returns the same output for any input and gets stuck.
So, my question is, if I am doing anything fundamentally wrong? Do I just have a small stupid mistake somewhere? Is this the wrong approach completely? (I know the game could probably be solved pretty easily without AI, but it seemed like a little fun project) 
My Jupyter notebook can be seen here Github. Sorry for the poor code quality, I'm still a beginner and I know I need to start making documentation even for fun little projects...
Thank you in advance,
Drukob
edit:
some code snippets:
Input is formatted as a 1,16 numpy array, also tried normalizing the values or using only 1 and 0 for occupied and empty cells, but that did not help either. Which is why I assumed it's maybe more of a conceptual problem?
    def get_board(self):
        grid = self.driver.execute_script(""return myGM.grid.cells;"")
        mygrid = []
        for line in grid:
            a = [x['value'] if x != None else 0 for x in line]
            #a = [1 if x != None else 0 for x in line]
            mygrid.append(a)
        return np.array(mygrid).reshape(1,16)

The output is an index of {0,3}, representing the actions up, down, left or right and it's just the value with the highes prediction score.
prediction = agent.model.predict(old_state)
predicted_move = np.argmax(prediction)

I've tried a lot of different model architectures, but settled for a simpler network now, as I have read that unnecessary complex structures are often a problem and unneeded. However, I couldn't find any reliable source for a method, how to get the optimal layout except for experimenting, so I'd be happy to have some more suggestions there.
model = models.Sequential()
        model.add(Dense(16, activation='relu', input_dim=16))
        #model.add(Dropout(0.15))
        #model.add(Dense(50, activation='relu'))
        #model.add(Dropout(0.15))
        model.add(Dense(20, activation='relu'))
        #model.add(Dropout(0.15))
        #model.add(Dense(30, input_dim=16, activation='relu'))
        #model.add(Dropout(0.15))
        #model.add(Dense(30, activation='relu'))
        #model.add(Dropout(0.15))
        #model.add(Dense(8, activation='relu'))
        #model.add(Dropout(0.15))
        model.add(Dense(4, activation='linear'))
        opt = Adam(lr=self.learning_rate)
        model.compile(loss='mse', optimizer=opt)

","['python', 'game-ai', 'dqn', 'deep-rl']",
How can I build an AI with NLP that read stories [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I want to do an NLP project but I don't know if it's doable or not as I have no experience or knowledge in NLP or ML yet.
The idea is as follows: Let's say we have a story (in the text) that has 10 characters. Can we define them, their characteristics, whole sentences they said, and then analyze emotions within those sentences.
After that, is it possible to generate an audio version of the story where: the text, in general, is narrated by one voice, each individual character's sentences are read in a different voice generated specifically for that character. Finally is it possible to make the tones of the characters voices change depending on the emotions detected in their sentences?
","['machine-learning', 'deep-learning', 'natural-language-processing']","
This is quite an ambitious project, and IMHO well beyond the scope of what a single individual can do (within a reasonable time span) at present. 
You need to first analyse the story text to identify the characters. This can already be quite a tricky task, as pronouns and other reference expressions are generally used to make a text less monotonous. If a character is referred to by name, say Jane, then you can assume that a follow-up the young woman will refer to her and not a male character mentioned in the same paragraph. But what about the young scientist? Such expressions can be very opaque, and you'd need a lot of world-knowledge to decode them correctly, as they can refer to any distinctive attribute of the character.
Identifying speech is a bit easier, unless you're talking about indirect speech. Jane was thinking aloud. She wasn't going to be able to do that. It was too hard. -- is that speech or not? Compare to Jane was thinking aloud: ""I am not going to be able to do that. It is too hard."", which is the direct speech equivalent. Also, unless you're dealing with a play, most of the text will probably not be speech. For the audio version you will probably only want to deal with direct speech, which is usually (but not always) indicated by quote marks.
Analysing emotions seems to be comparatively easy if you have reached this stage, though if it is just based on keywords in the speech it probably won't be very accurate. If you can assign any descriptive statements to characters, that might be more successful, though by no means trivial.
Generating the text as audio should be straight forward. Most operating systems nowadays have speech synthesis integrated, and you can generally choose different voices, so if your text is marked up properly with which voice should speak which part it would be trivial.
To summarise: The NLP part is the hardest bit of it. As has been mentioned in the comments already, I don't think it's a problem that machine learning can help with, and I would stick to traditional methods of parsing the text into a structural representation and then applying rules to identify the bits you are interested in. The recognition of emotion might be a subtask that is suitable for ML, but in the past I have only applied pattern matching to similar tasks, so I can't really say much about that.
From my own experience in text analysis I would think that you might be able to get decent results with a few simple heuristics, but those will likely fail when it becomes a bit more complicated. A lot hinges on the type of story: children's fairy tales might be easier than War and Peace in that respect.
"
It is possible to solve a problem with continuous action spaces and no states with reinforcement learning?,"
I want to use Reinforcement Learning to optimize the distribution of energy for a peak shaving problem given by a thermodynamical simulation. However, I am not sure how to proceed as the action space is the only thing that really matters, in this sense:

The action space is a $288 \times 66$  matrix of real numbers between $0$ and $1$. The output of the simulation and therefore my reward depend solely on the distribution of this matrix.

The state space is therefore absent, as the only thing that matters is the matrix on which I have total control. At this stage of the simulation, no other variables are taken into consideration.


I am not sure if this problem falls into the tabular RL or it requires approximation. In this case, I was thinking about using a policy gradient algorithm for figuring out the best distribution of the $288 \times 66$ matrix. However, I do not know how to behave with the ""absence"" of the state space. Instead of a tuple $\langle s,a,r,s' \rangle$, I would just have $\langle a, r \rangle$, is this even an RL-approachable problem? If not, how can I reshape it to make it solvable with RL techniques?
","['reinforcement-learning', 'policy-gradients', 'multi-armed-bandits', 'state-spaces', 'continuous-action-spaces']","
A stateless RL problem can be reduced to a Multiarmed Bandit (MAB) problem. In such a scenario, taking an action will not change the state of the agent.
So, this is the setting of a conventional MAB problem: at each time step, the agent selects an action to either perform an exploration or exploitation move. It then records the reward of the taken action and updates its estimation/expectation of the usefulness of the action. Then, repeats the procedure (selection, observing, updating). 
To chose between exploration and exploitation moves, MAB agents adopt a strategy. The simplest one would probably be $\epsilon$-greedy which agent chooses the most rewarding actions most of the time (1-$\epsilon$ probability) or randomly selects an action ($\epsilon$ probability). 
"
Do I need to use a pre-processed dataset to classify comments?,"
I want to use Machine Learning for text classification, more precisely, I want to determine whether a text (or comment) is positive or negative. I can download a dataset with 120 million comments. I read the TensorFlow tutorial and they also have a text dataset. This dataset is already pre-processed, like the words are converted to integers and the most used words are in the top 10000. 
Do I also have to use a pre-processed dataset like them? If yes, does it have to be like the dataset from TensorFlow? And which pages could help me to implement that kind of program? 
My steps would be:

find datasets
preprocess them if needed
feed them in the neural network

","['neural-networks', 'natural-language-processing', 'classification', 'tensorflow', 'data-preprocessing']","
Here's a list of some of the best python libraries for natural language processing.

Natural Language Toolkit (nltk)
Covers all the basic functions and NLP tools such as tokenization etc.
TextBolb
This is a good library of beginners, it provides the nltk toolkit in a simplified format.
Spacy
It is an advanced library and can be used in production code.

You can preprocess textual data in a number of ways. It depends on the type of task at hand and the size of the data.
From your question, I think you are referring to converting the words to a vector form word2vec. Here is a massive word2vect list from Google.
Also have a look at preprocessing techniques in NLP such as tf-idf etc.
"
Can Machine Learning make economic decisions of human quality or better?,"
Basically, economic decision making is not restricted to mundane finance, the managing of money, but any decision that involves expected utility (some result with some degree of optimality.)  

Can Machine Learning algorithms make economic decisions as well as or better than humans?

""Like humans"" means understanding classes of objects and their interactions, including agents such as other humans. 
At a fundamental level, there must be some physical representation of an object, leading to usage of an object, leading to management of resources that the objects constitute. 
This may include ability to effectively handle semantic data (NLP) because mcuh of the relevant information is communicated in human languages.
","['machine-learning', 'economics']","
Consider managing a memory structure as an economic function.  (Where to put, and how to manage, the resources constituted by data.)  This is something computers can do better and faster than any human.  The reason is that the system in which the economic decisions are being made is fully defined.  
Routing of packages is a similar, economic function that computers do much better than humans.
These functions haven't been handled by Machine Learning in the past, but, soon after the AlphaGo milestone, Google found an economic application for Machine Learning.  Google's DeepMind trains AI to cut its energy bills by 40% (Wired)
So it's entirely context dependent.  
As the model increases in complexity and nuanced, utility will be reduced. (In the former case it's a time and space issue related to computational complexity, and in the latter case, often a function of incomplete information or inability to define parameters.)
But as the sophistication of the machine learning algorithms increases, and the models continue to be refined, the algorithms will get better and better at managing intractability and incomplete information.
"
Feasibility of a team-based FPS AI?,"
We have seen advances in top down, RTS team games like Dota 2 and Starcraft II from companies like OpenAI who developed agents to beat real pro players most of the time. How would similar learning techniques compare to games like Overwatch that require faster reaction times and complex understanding of 3d space and effect? 
Or have we not developed solutions that could be tasked with this problem?
",['game-ai'],
How do I classify an image that contains only polygons?,"
I have two closed polygons, drawn as connected straight black lines on a white background. I need to classify such images in to three forms

Two separate polygons
One polygon encloses the other
The two polygons overlap each other.

The polygons vary in sizes and location on the image, and the image contains only the polygons and the white background.
Which neural network architecture should I use to solve this problem?
","['neural-networks', 'machine-learning', 'deep-learning', 'classification']","
To make it easy take small known CNN network like Alex Net, train like : input  is image, output is [1,0,0,] separate, [0,1,0], encloses, [0,0,1] overlap. Cause you task is easy i guess that will be fine. But it can be done without ML just analysing image with constant algorithm by where are the points of poligons.
"
Can next state and action be same in Deep Deterministic Policy Gradient?,"
I am trying to apply deep deterministic policy gradient (DDPG) on a robotic application. My states consist of the joint angle positions of the robot and my actions are also its joint angle positions. Since, DDPG produces a continuous policy output where states are directly mapped onto the actions, can I say that my next state and action will be same? Simplistically, the input of the policy network will be the current state and the output will be the next state?
","['reinforcement-learning', 'deep-rl']",
DCGAN loss determining data normalization problems,"
I'm working with a DCGAN, a deep CNN for classifying images with a GAN that competes with the classifier to generate images of what we are classifying. 
The goal of the project at the moment is to produce AI generated memes in the form of pepe the frog, based on a dataset found on the internet of roughly 2000 images. I scaled them all maintaining aspect ratio as my only form of normalization. 
As I train my data (I've tried many combinations of hyperparameters) upwards of 100k epochs (batch 32) my classifying network gets around 1^-6 average while my GAN approaches nearly 16, yes 16, with a properly defined loss function.
Now because my images are typically made with random features, some containing text, others containing full body renditions, turned away from viewport etc... I'm  assuming that this is because of the data I'm training with and it's diverse amount of features, is my reasoning correct? Also if allowed to continue is it possible that the GAN learns to properly generate the data?
The main reason I have come to the above conclusion is that if I train on a few hand picked examples that have similar artistic styles/orientations and such (less than 100) and let it train my GAN will generate decent images, however they have low variability.
","['neural-networks', 'generative-adversarial-networks']",
RNN weights when varying the input size,"
I have a time-varying input size vector for a RNN. However, I am facing some difficulties understanding how to deal with my network weights when the input changes. 
Say we have a set of natural positive integers
$$
\Gamma=\{1,2,\dots,F\},
$$
where $F=100$ for the sake of the example.
A valid observation vector of my agent at time $t$ might be 
$$
\gamma_t=[1,3,5,1].
$$
Thus, at time $t$ a set of weights will be produced by my RNN, according to $\gamma_t$. Sat that at time $t+1$, my observation vector changes as
$$
\gamma_{t+1}=[3,5,8],
$$
and there is my problem. If I now continue training my RNN with the previous weights, the output would be inevitably affected. Also, Which weight shall I remove? I see RNN can face the issue but how shall I deal with the previously computed weights? Which one to remove? How to initialize a new one in case the cardinality of $\gamma_{t+1}$ is higher than that of $\gamma_t$?
","['neural-networks', 'recurrent-neural-networks']",
How does one make a neural network learn the training data while also forcing it to represent some known structure?,"
In general, how does one make a neural network learn the training data while also forcing it to represent some known structure (e.g., representing a family of functions)?
The neural network might find the optimal weights, but those weights might no longer make the layer represent the function I originally intended.
For example, suppose I want to create a convolutional layer in the middle of my neural network that is a low-pass filter. In the context of the entire network, however, the layer might cease to be a low-pass filter at the end of training because the backpropagation algorithm found a better optimum.
How do I allow the weights to be as optimal as possible, while still maintaining the low-pass characteristics I originally wanted?
General tips or pointing to specific literature would be much appreciated.
","['neural-networks', 'reference-request', 'constrained-optimization']","
Extending @mirror2image's comment, if you have a certain metric that allows you to measure how close the intended layer is to a low pass filter (something that compares its output with what a low pass filter would have produced, for example), the simplest way to achieve what you want would be to add a term in your loss function that calculates the value of this metric. This way, each time you do a training step, the network now is not only made to output the correct predictions but is also forced to do so while also keeping that specific layer's behavior as close to a low-pass filter as possible.
This is the most common way of tweaking the behavior of neural networks and is often encountered in many research papers.
"
Validation Loss Fluctuates then Decrease alongside Validation Accuracy Increases,"
I was working on CNN. I modified the training procedure on runtime.

As we can see from the validation loss and validation accuracy, the yellow curve does not fluctuate much. The green curve and red curve fluctuate suddenly to higher validation loss and lower validation accuracy, then goes to the lower validation loss and the higher validation accuracy, especially for the green curve.
Is it happening because of overfitting or something else?
I am asking it because, after fluctuation, the loss decreases to the lowest point, and also the accuracy increases to the highest point.
Can anyone tell me why is it happening?
","['neural-networks', 'deep-learning', 'data-preprocessing', 'data-augmentation']","
This question has been asked a year ago when I faced this problem I searched and it hasn't any answers on it, I tried different ways and finally, data augmentation helped me.
I used data augmentation and a very small learning rate. If the fluctuations are big, the batch size should be increased, and the learning rate should be decreased. After all, using more epochs helps you to have an almost smooth plot.
"
How important is architectural similarity between the discriminator and generator of a GAN?,"
Shouldn't the discriminator and generator work fine even if they don't process data symmetrically? I mean, they don't only receive the final layer results of each other, they don't use data that from hidden layers.
","['neural-networks', 'generative-adversarial-networks']",
Proof of Correctness of Monte Carlo Tree Search,"
I'm trying to write the proof of correctness of Monte Carlo Tree Search. Any help would be really appreciated.
","['monte-carlo-tree-search', 'proofs']",
What is the time complexity of an unparellelized Monte Carlo tree search?,"
I am writing a report where I used a slightly modified version of MCTS (not parallelized). I thought It could be interesting if I could calculate its time complexity. I'd appreciate any help I could get.
Here's the rough idea of how it works:
Instead of tree search, I'm using graph search meaning I keep a list of visited nodes in order to avoid adding duplicate nodes.
So in the expansion phase, I add all child nodes of the current node that aren't present elsewhere in the tree.
For the remaining phases, it's essentially the same as the basic version of MCTS, with a default random policy in the simulation step.
","['monte-carlo-tree-search', 'time-complexity']",
Exploding population size in neat-python,"
I am trying to make my AI win the board game ""Catan"" against my friends.
Therefore i am using the python implementation of NEAT. 
As I changed the values of weight_mutate_power, response_mutate_power, bias_mutate_power and compatibility_threshold in the config the number of individuals and species exploded (~Doubled every generation and exceeded pop_size).
weight_mutate_power = 12
response_mutate_power = 0
bias_mutate_power = 0.5
compatibility_threshold = 3

Playing with those values I discovered that the rate of the explosion changes in relation to those values (Everything worked fine with standard Values from the documentation).
Any idea how to control this behavior? 
My cluster is drowning in genomes...
","['python', 'genetic-algorithms', 'evolutionary-algorithms', 'neat']",
Is there a way to break a piece of dialogue into components?,"
In many chatbots, I've seen a lot of hardcoded responses, but nothing that allows an AI to break a piece of dialogue into components (say that the speaker sounds happy or is trying to be manipulative) and model a response based on this.
I envision a coding scheme for different core components of conversation. This would allow an AI to be more dynamic in its responses, and would be closer to actually being able to hold a conversation.
I'm not looking for AI-generated text, at least not in the sense of some NN or the like being fed a diet of literature and seeing what it spits out - there's nothing dynamic about that.
","['natural-language-processing', 'reference-request', 'dialogue-systems']",
How to deal with invalid output in a policy network? [duplicate],"







This question already has answers here:
                                
                            




How should I handle invalid actions (when using REINFORCE)?

                                (5 answers)
                            

Closed 4 years ago.



I am interested in creating a neural network-based engine for chess. It uses a $8 \times 8 \times 73$ output space for each possible move as proposed in the Alpha Zero paper: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.
However, when running the network, the first selected move is invalid. How should we deal with this? Basically, I see two options.

Pick the next highest outputted move, until it is a valid move. In this case, the network might automatically over time not put illegal moves on top.
Process the game as a loss for the player who picked the illegal move. This might have the disadvantage that the network might be 'stuck' on only a few legal moves.

What is the preferred solution to this particular problem?
","['neural-networks', 'ai-design', 'game-ai', 'chess', 'alphazero']","
You should have a method to generate a possible moves output based on the board state. Use this as a mask before normalization in the policy head.
"
Why do the inputs and outputs of a convolutional layer usually have the same depth?,"
Here's the famous VGG-16 model.

Do the inputs and outputs of a convolutional layer, before pooling, usually have the same depth?  What's the reason for that?
Is there a theory or paper trying to explain this kind of setting?
","['neural-networks', 'convolutional-neural-networks', 'convolution', 'convolutional-layers']","
Keeping the same channel size allows the model to maintain rank but i would say the main reason is convenience. Its easier book keeping.
Also in many model cases output features need some form of alignment with the input (example being all models using residual units -- $\hat{x} = F(x) + x$ 
"
Why is an expectation used instead of simple sum in GANs?,"
Why do the GAN's loss functions use an expectation (sum + division) instead of a simple sum?
","['neural-networks', 'machine-learning', 'generative-adversarial-networks']",
"If I use MobileNetV2 for the encoder, can I use a different architecture for the decoder?","
I have way more unlabeled data than labeled data. Therefore I would like to train an autoencoder using MobileNetV2 as the encoder. Then I will use the pre-trained model for the classification of the labeled data.
I think it is rather difficult to ""invert"" the MobileNet architecture to create a decoder. Therefore, my question is: can I use a different architecture for the decoder, or will this introduce weird artefacts?
","['convolutional-neural-networks', 'computer-vision', 'autoencoders', 'mobile-net-v2']","
Other replies are commenting on the skip connections for a U-Net. I believe you want to exclude these skip connections from your auto-encoder. You say you want to use the auto-encoder for unsupervised pretraining, for which you want to pass the data through a bottle neck, so adding skip connections would work against you if you want to use the encoder for a classification task.
You ask whether the decoder should 'mirror' the MobileNet encoder. This is actually an interesting one, and I think it could work even if the decoder does not look like the encoder at all. Since you don't need to (and in fact shouldn't) add skip connections, this should be easy to try.
"
How does AlphaZero use its value and policy heads in conjunction?,"
I have a question about how the value and policy heads are used in AlphaZero (not Alphago Zero), and where the leaf nodes are relative to the root node. Specifically, there seem to be several possible interpretations:

Policy estimation only. This would be most similar to DFS in which an average evaluation is computed through MCTS rollouts (though as others have noted the AlphaZero implementation actually seem to be deterministic apart from the exploration component, so 'rollout' may not be the most appropriate term) after reaching the . Here each leaf node would be at the end of the game.
Value estimation only. It seems that if the value network is to be used effectively, there should be a limit on the depth to which any position is searched, e.g. 1 or 2 ply. If so, what should the depth be?
They are combined in some way. If I understand correctly, there is a limit on the maximum number of moves imposed - so is this really the depth? By which I mean, if the game has still not ended, this is the chance to use the value head to produce the value estimation? The thing is that the paper states that the maximum number of moves for Chess and shogi games was 512, while it was 722 moves for Go. These are extremely deep - evaluations based on these seem to be rather too far from the starting state, even when averaged over many rollouts.

My search for answers elsewhere hasn't yielded anything definitive, because they've focused more on one side or the other. For example, https://nikcheerla.github.io/deeplearningschool/2018/01/01/AlphaZero-Explained/ the emphasis seems to be on the value estimation.
However, in the Alphazero pseudocode, e.g. from https://science.sciencemag.org/highwire/filestream/719481/field_highwire_adjunct_files/1/aar6404_DataS1.zip the emphasis seems to be on the policy selection. Indeed, it's not 100% clear if the value head is used at all (value seems to return -1 by default).
Is there a gap in my understanding somewhere? Thanks!
Edit: To explain this better, here's the bit of pseudocode given that I found slightly confusing:
class Network(object):

  def inference(self, image):
    return (-1, {})  # Value, Policy

  def get_weights(self):
    # Returns the weights of this network.
    return []

So the (-1,{}) can either be placeholders or -1 could be an actual value and {} a placeholder. My understanding is that they are both placeholders (because otherwise the value head would never be used), but -1 is the default value for unvisited nodes (this interpretation is taken from here, from the line about First Play Urgency value: http://blog.lczero.org/2018/12/alphazero-paper-and-lc0-v0191.html). Now, if I understand correctly, inference is called in both during training and playing by the evaluate function. So my core question is: how deep into the tree are the leaf nodes (i.e. where the evaluate function would be called)?
Here is the bit of code that confused me. In the official pseudocode as below, the 'rollout' seems to last until the game is over (expansion stops when a node has no children). So this means that under most circumstances you'll have a concrete game result - the player to move doesn't have a single move, and hence has lost (so -1 also makes sense here).
def run_mcts(config: AlphaZeroConfig, game: Game, network: Network):
  root = Node(0)
  evaluate(root, game, network)
  add_exploration_noise(config, root)

  for _ in range(config.num_simulations):
    node = root
    scratch_game = game.clone()
    search_path = [node]

    while node.expanded():
      action, node = select_child(config, node)
      scratch_game.apply(action)
      search_path.append(node)

    value = evaluate(node, scratch_game, network)
    backpropagate(search_path, value, scratch_game.to_play())
  return select_action(config, game, root), root

But under such conditions, the value head still doesn't get very much action (you'll almost always return -1 at the leaf nodes). There are a couple of exceptions to this. 

When you reach the maximum number of allowable moves  - however, this number is a massive 512 for chess & Shogi and 722 for Go, and seems to be too deep to be representative of the 1-ply positions, even averaged over MCTS rollouts.
When you are at the root node itself - but the value here isn't used for move selection (though it is used for the backprop of the rewards)

So does that mean that the value head is only used for the backprop part of AlphaZero (and for super-long games)? Or did I misunderstand the depth of the leaf nodes?
","['reinforcement-learning', 'alphazero']",
How can I interpret the following error graph?,"
I am training a neural network which produces the following errors (epoch number on the x axis). I have some questions regrading interpreting it.

When I say model.predict is it giving me the result based on the final state (that is epoch 5000)?
Towards the end (and some places in the middle) there are places where the training error and validation error are farther apart. Does this mean that the model was over-fitting on those epochs?
Based on the graph, can one determine that the model was best at a certain epoch? 
Does Keras have API methods to retrieve the model at a specific epoch so that I can retrieve the best model?


","['deep-learning', 'keras']",
Make 9 AIs to replace Supreme Court justices,"
Since the supreme court is always political, why not program 9 AI robots that use different methods to determine whether a law is constitutional and the outcome of cases. How would engineers go about building this? Would it work? 
",['robots'],"
Politics are driven by a combination of things such as culture, ideology, religion, ethics, morality, desire for money, fame, and power, etc.  Although God has one truth, most people don't know this truth, a few do, and the rest either disagree about it or just don't believe in Him. The Constitution was written by man.  It does not tell us what to do in every situation. It is open to interpretation.  A person's interpretation of the Constitution is driven by his morality, culture, and all the things I mentioned above. AI systems are designed by people so they are driven by their biases. 
"
Why should each filter have different weights for each input channel?,"
From the answers to this question In a CNN, does each new filter have different weights for each input channel, or are the same weights of each filter used across input channels?, I got the fact that each filter has different weights for each input channel. But why should that be the case? What if we apply the same weights to each input channel? Does it work or not?
","['machine-learning', 'convolutional-neural-networks', 'weights']","
For simplicitly, let's consider only the first convolutional layer, that is, the one applied to the image. If you consider an RGB image, then there are $3$ channels: the red channel, the green channel and the blue channel. Thus, a kernel that is applied to this image will also have $3$ channels: the red channel, the green channel and the blue channel. In general, the distributions of the intensity of the red, green and blue colors in the image are different, so, in general, the red, green and blue channels of the kernel will also be different because they need to keep track of different information.
"
Prediction of values with an unsupervised model,"
Given a set of historical data points, I am trying to predict a continuous output of which I have no historical record of, therefore the problem is of an unsupervised nature.
I am wondering if there is any method or approach I should take to tackle this problem? Essentially, how to build a model that will provide an output that is not clustered?
","['models', 'unsupervised-learning']",
Can GANs be used to generate matching pairs to inputs?,"
I have some limited experience with MLPs and CNNs. I am working on a project where I've used a CNN to classify ""images"" into two classes, 0 and 1. I say ""images"" as they are not actually images in the traditional sense, rather we are encoding a string from a limited alphabet, such that each character has a one-hot encoded row in the ""image"". For example, we are using this for a bioinformatics application, so with the alphabet {A, C, G, T} the sequence ""ACGTCCAGCTACTTTACGG"" would be:

All ""images"" are 4x26. I used a CNN to classify pairs of ""images"" (either using two channels, i.e. 2x4x26 or concatenating two representations as 8x26) according to our criteria with good results. The main idea is for the network to learn how 2 sequences interact, so there are particular patterns that make sense. If we want to detect a reverse complement for example, then the network should learn that A-T and G-C pairs are important. For this particular example, if the interaction is high/probable, the assigned label is 1, otherwise 0.
However, now we want to go one step further and have a model that is able to generate ""images"" (sequences) that respect the same constraints as the classification problem. To solve this, I looked at Generative Adversarial Networks as the tool to perform the generation, thinking that maybe I could adapt the model from the classification to work as the discriminator. I've looked at the ""simpler"" models such as DCGAN and GAN, with implementations from https://github.com/eriklindernoren/Keras-GAN, as I've never studied or used a GAN before.
Say that we want to generate pairs that are supposed to interact, or with the label 1 from before. I've adapted the DCGAN model to train on our 1-labelled encodings and tried different variations for the discriminator and generator, keeping in mind rules of thumb for stability. However, I can't get the model to learn anything significant. For example, I am trying to make the network learn the simple concept of reverse complement, mentioned above (expectation: learn to produce a pair with high interaction, from noise). Initially the accuracy for the discriminator is low, but after a few thousand epochs it increases drastically (very close to 100%, and the generator loss is huge, which apparently is a good thing, as the two models ""compete"" against each other?). However, the generated samples do not make any sense.
I suspect that the generator learns the one-hot encoding above - since early generator output is just noise, it probably learns something like ""a single 1 per column is good"", but not the more high level relation between the 1s and 0s. The discriminator probably is able to tell that the early generated outputs are garbage as there are 1s all over the place, but perhaps at some point the generator can match the one-hot encoding and thus the discriminator decides that is not a fake. This would explain the high accuracy, despite the sequences not making sense.
I am not sure of this is the case or not, or if it makes sense at all (I've just started reading about GANs yesterday). Is there a way to capture the high level features of the dataset? I am not interested in just generating something that looks like a real encoding, I'd like to generate something that follows the encoding but also exhibits patterns from the original data.
I was thinking that maybe pretraining the discriminator would be a good idea, because it would then be able to discern between real-looking encodings for both the 0 and 1 classes. However, the pretraining idea seems frowned upon.
I'd appreciate any ideas and advice. Thanks!
","['convolutional-neural-networks', 'image-recognition', 'classification', 'generative-model', 'generative-adversarial-networks']",
Unit integral condition on the output layer,"
I want to train a neural network on some input data from a probability distribution (say a Gaussian). The loss function would normally be $-\sum\log(f(x_i))$, where the sum is over the whole data (or in this case a mini batch) and $f$ is the NN function. However I need to enforce the fact that $\int_0^\infty f(x)dx=1$, in order for $f$ to be a real probability distribution. How can I add that to the loss function? Thank you! 
","['objective-functions', 'probability-distribution']",
Training Haar Cascade model with grey vs color images,"
Most examples if not all, are models that have been trained with images that are turned grey. Does this mean that models only detect edges? Why wouldnt you want to keep color so that model could learn that as well?
","['classification', 'object-recognition']",
"In addition to matrix algebra, can GPU's also handle the various Kernel functions for Neural Networks?","
I've read a number of articles on how GPUs can speed up matrix algebra calculations, but I'm wondering how calculations are performed when one uses various kernel functions in a neural network.  
If I use Sigmoid functions in my neural network, does the computer use the CPU for the Sigmoid calculation, and then the GPU for the subsequent matrix calculations?  
Alternatively, is the GPU capable of doing nonlinear calculations in addition to the linear algebra calculations?  If not, how about a simple Kernel function like ReLU?  Can a GPU do the Relu calculation, or does it defer to the CPU?
Specifically, I'm using Keras with a Tensorflow backend, and would like to know what TensorFlow can and cannot use the GPU for, but I'm also interested in the general case.
","['neural-networks', 'tensorflow', 'keras', 'performance', 'gpu']",
How Does AlphaGo Zero Implement Reinforcement Learning?,"
AlphaGo Zero (https://deepmind.com/blog/alphago-zero-learning-scratch/) has several key components that contribute to it's success:

A Monte Carlo Tree Search Algorithm that allows it to better search and learn from the state space of Go
A Deep Neural Network architecture that learns the value and policies of given states, to better inform the MCTS.  

My question is, how is this Reinforcement Learning?  Or rather, what aspects of this algorithm specifically make it a Reinforcement Learning problem?  Couldn't this just be considered a Supervised Learning problem?  
","['reinforcement-learning', 'monte-carlo-tree-search', 'supervised-learning', 'alphago-zero', 'go']",
How is regression machine learning?,"
In regression, in order to minimize an error function, a functional form of hypothesis $h$ must be decided upon, and it must be assumed (as far as I'm concerned) that $f$, the true mapping of instance space to target space, must have the same form as $h$ (if $h$ is linear, $f$ should be linear. If $h$ is sinusoidal, $f$ should be sinusoidal. Otherwise the choice of $h$ was poor). 
However, doesn't this require a priori knowledge of datasets that we are wanting to let computers do on their own in the first place? I thought machine learning was letting machines do the work and have minimal input from the human. Are we not telling the machine what general form $f$ will take and letting the machine using such things as error minimization do the rest? That seems to me to forsake the whole point of machine learning. I thought we were supposed to have the machine work for us by analyzing data after providing a training set. But it seems we're doing a lot of the work for it, looking at the data too and saying ""This will be linear. Find the coefficients $m, b$ that fit the data.""
","['machine-learning', 'linear-regression', 'supervised-learning', 'regression']","
So in a sense you are correct. Using your jargon: linear regression will only ""work"" if the true function is approximately $y=h(x)=\beta^{T}x+\beta_0$. Advantages to using this is that its light, its convex, and all-around easy.
but for alot of larger problems, this wont work. As you said you want the machine to do the work, so this is (kinda) where deeper models come into play: You allow a learn-able featurization and classification/regression. Think about it this way, the result of your regression is most likely linearly associated with some set of features, they just may not be the ones you are interested in (you can prove this actually with any infinitely wide network :: Universal approx Thm). Unfortunately we cant use an infinitely dimensional model, so we run with these giant over-parametrized models where we hope the a good function can be described by a sub-structure (only recently are we starting to pay attention, to how these sub-structures form -- look at this paper)
But the way you bring about thinking about it is a large pit fall for many trying to move forward. Alot of ML people now of days gain success by throwing a function without alot of parameters on a big data problem, but youll see the largest advancements in the field come from a theoretical understanding of the featurization and optimization.
I hope this helped
"
Are absence of labels for classes of interest in a vision dataset a big problem?,"
I wish to be able to detect: pedestrians, cars, traffic lights
I have two large datasets:
 - One contains instances and labels of all three classes.
 - The other contains instances of all three but only labels for pedestrians and cars. ie. there are many unlabelled traffic lights.
I want to combine the two datasets and train Yolov3 on it. Will the unlabelled presence of objects of interest significantly affect detection performance of that category?
","['datasets', 'object-recognition']",
Ideal score of a model on training and cross validation data,"
The question is little bit broad, but I could not find any concrete explanation anywhere, hence decided to ask the experts here.
I have trained a classifier model for binary classification task. Now I am trying to fine tune the model. With different sets of hyperparameters I am getting different sets of accuracy on my train and test set. For example:
(1) Train set: 0.99 | Cross-validation set: 0.72
(2) Train set: 0.75 | Cross-validation set: 0.70
(3) Train set: 0.69 | Cross-validation set: 0.69

These are approximate numbers. But my point is - for certain set of hyperparameters I am getting more or less similar CV accuracy, while the accuracy on training data varies from overfit to not so much overfit.
My question is - which of these models will work best on future unseen data? What is the recommendation in this scenario, shall we choose the model with higher training accuracy or lower training accuracy, given that CV accuracy is similar in all cases above (in fact CV score is better in the overfitted model)?
","['machine-learning', 'training', 'cross-validation']","
Assuming that your cross-validation scores(both on train set and test set) indicate model's prediction performance correctly, you should definitely decide which trained model to use based on your validation accuracy only, regardless your model is overfitted or not. 
"
How to distinguish between proper nouns and other words in NLP?,"
If an NLP system processes a text containing proper nouns like names, trade marks, etc. without knowing anything about the language (ie no lexicon), is it possible to recognise them?
",['natural-language-processing'],"
Essentially you would need to train a named entity recognizer (NER) to recognize the names out of the common words.
There are many works that try to use a similar language to the language in question as a pivot to train a full NER model (for example, Cheap Translation for Cross-Lingual Named Entity Recognition).
Your task might be slightly simpler, as you are interested only in one class: whether it is a named entity or not. But in general it is very similar to this setup.
"
Why is an average of all returns used to update the value in the first-visit MC control?,"
In Sutton & Barto's Reinforcement Learning: An Introduction,  in page 83 (101 of the pdf), there is a description of first-visit MC control. In the phase where they update $Q(s, a)$, they do an average of all the returns $G$ for that state-action pair. 
Why don't they just update the value with a weight for the value from previous episodes $\alpha$ and a weight $1- \alpha$ for the new episode return as it is done in TD-Learning?
I have also seen other books (for example, Algorithms for RL (page 22) where they update it using $\alpha$. What is the difference?
","['reinforcement-learning', 'monte-carlo-methods', 'sutton-barto']",
What qualifies as 'fitness' for a genetic algorithm that minimizes an error function?,"
Suppose I have a set of data that I want to apply a segmented regression to, fitting linearly across the breakpoint. I aim to find the offsets and slopes of either line and the position of the breakpoint that minimizes an error function given the data I have, and then use them as sufficiently close initial guesses to find the exact solutions using a curve fit. I'll elect to choose the bounds as the mins and maxes of $x$ and $y$ of my data, and an arbitrary bound for a slope with $slope = a * \frac{y_{max}-y_{min}}{x_{max}-x_{min}}$ for a suitable $a$ where I can safely assume the magnitude of $a$ is greater than any possible slope that realistically represents the data. Let's suppose I define a function (in Python):
 def generate_genetic_Parameters():
            initial_parameters=[]
            x_max=np.max(xData)
            x_min=np.min(xData)
            y_max=np.max(yData)
            y_min=np.min(yData)
            slope=10*(y_max-y_min)/(x_max-x_min)

            initial_parameters.append([x_max,x_min]) #Bounds for module break point
            initial_parameters.append([-slope,slope]) #Bounds for slopeA
            initial_parameters.append([-slope,slope]) #Bounds for slopeB
            initial_parameters.append([y_max,y_min]) #Bounds for offset A
            initial_parameters.append([y_max,y_min]) #Bounds for offset B

            result=differential_evolution(sumSquaredError,initial_parameters,seed=3)

            return result.x

        geneticParameters = generate_genetic_Parameters() #Generates genetic parameters

fittedParameters, pcov= curve_fit(func, xData, yData, geneticParameters) 

This will do the trick, but what is the implicit standard of fitness that the differential evolution here deals with?
","['python', 'genetic-algorithms', 'evolutionary-algorithms']",
"Deep Generative Networks Probability of ""Success""","
I have built various ""successful"" GANs or VAEs that can generate realistic images reliably, but in either case the generative step is sampling a latent feature vector from some distribution and running it through a generator/decoder $G(x) \  s.t. \  x \sim \mathbb{D} $.   
Generally $\mathbb{D}$ has a continuous domain (normally distributed is a common choice), and $G(x)$ is a continuous and at-least singly differential function (by construction so it could be optimized with a gradient scheme).
Now assume we have 2 images $y_1, y_2$ generated by $x_1, x_2 \sim \mathbb{D}$ and that shortest path from $x_1$ to $x_2$ is also in $\mathbb{D}$'s domain. Crawling along this path, we get a continuous path from $y_1$ to $y_2$ in image space. Wouldnt you assume along this continuous transformation that the images would be a ""Garbage"" as they are essentially a fusion of $y_1, y_2,$ and $\{y_k\}_k$ where $\{y_k\}_k$ describes a set of ""succesful"" stops along the way? 
 --Especially in cases of single mode distributions (like normal) where all $x$ on the path must have higher likelihood than atleast one of the paths endpoints.
Trying to summarize my point, how do continuous generative models not with high probability produce garbage when given any 2 ""succesful"" images, there probably is a high range of ""garbage"" examples along a path between them in image space?
Note: by ""succesful"" i mean the generated image could be considered a draw from the true distribution you are trying to capture by the generator, and by ""garbage"" i mean ones that are obviously not.
","['deep-learning', 'generative-adversarial-networks', 'autoencoders']",
"What is wrong with this CNN network, why are there hot pixels? [closed]","







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I'm building a CNN decoder, which mirrors (in reverse) the VGG network structure from Conv-4-1 layer. 
The net seems to be working fine, however, the output looks broken. Please note that the colour distortion is fine, it's the the [255/0 RGB pixels] e.g. green that I'm worrying about.
I tried to overfit a single image, but even then I get these hot pixels. Does anyone know why they appear?

My net:
    activation = 'elu'

    input_ = Input((None, None, 512))
    x = Conv2D(filters=256, kernel_size=self.kernel_size, padding='same', bias_initializer='zeros', activation=activation)(input_)

    x = UpSampling2D()(x)
    for _ in range(3):
        x = Conv2D(filters=256, kernel_size=self.kernel_size, padding='same', activation=activation)(x)
    x = Conv2D(filters=128, kernel_size=self.kernel_size, padding='same', activation=activation)(x)

    x = UpSampling2D()(x)
    x = Conv2D(filters=128, kernel_size=self.kernel_size, padding='same', activation=activation)(x)
    x = Conv2D(filters=64, kernel_size=self.kernel_size, padding='same', activation=activation)(x)

    x = UpSampling2D()(x)
    x = Conv2D(filters=64, kernel_size=self.kernel_size, padding='same', activation=activation)(x)
    x = Conv2D(filters=3, kernel_size=self.kernel_size, padding='same')(x)

    model = Model(inputs=input_, outputs=x)

","['convolutional-neural-networks', 'computer-vision']","
I've seen this too many times - it's not a problem with your network, it's a problem with matplotlib and how it displays the image. You are probably trying to display a float with range $<0, 255>$. When matplotlib sees float type as input, it assumes a range of $<0, 1>$, and thresholds everything outside of that range, and the results you can see.
"
Are there neural networks that accept graphs or trees as inputs?,"
As far I know, the RNN accepts a sequence as input and can produce as a sequence as output.
Are there neural networks that accept graphs or trees as inputs, so that to represent the relationships between the nodes of the graph or tree?
","['neural-networks', 'recurrent-neural-networks', 'geometric-deep-learning', 'graphs', 'graph-neural-networks']","
Yes, there are numerous, coming under the umbrella term Graph Neural Networks (GNN).
The most common input structures accepted by these techniques are the adjacency matrix of the graph (optionally accompanied by its node feature matrix and/or edge feature matrix, if the graph has such information).
A Comprehensive Survey on Graph Neural Networks, Wu et al (2019) divides GNN's into four subgroups:

Recurrent graph neural networks (RecGNN)
Convolutional graph neural networks (ConvGNN)
Graph autoencoders (GAE)
Spatial-temporal graph neural networks (STGNN)

ConvGNN's can themselves be classified by whether they use Spectral methods or Spatial methods, and GAE's by whether they are designed for Network embedding or Graph generation.
"
What are the most compact Real Time-Strategy Games?,"
There was a recent informal question on chat about RTS games suitable for AI benchmarks, and I thought it would be useful to ask a question about them in relation to AI research.
Compact is defined as the fewest mechanics, elements, and smallest gameboard that produces a balanced, intractable, strategic game. (This is important because greater compactness facilitates mathematical analysis.)
","['game-ai', 'rts', 'benchmarks']","
This is an interesting question. One good place to start would be the gargantuan catalogue of rather simple RTS flash games and the like. There are many sites that have 100s of such titles, and it would not be too difficult to build the framework for these types of games(there are open source tools that can help as well).
As far as ""real"" mainstream games, the pickings are rather slim for simple benchmark RTS titles. One that comes to mind is Rome: Total War. At least in exhibition, there is not much to strategize over besides who to attack and from where. Also, as I assume this relates to starcraft, one good simplified predecessor to starcraft would be the Dune series, and in particular Dune 2.
"
Do I need LSTM units everywhere in the network?,"
I have recently begun researching LSTM networks, as I have finished my GA and am looking to progress to something more difficult. I believe I am using the classic LSTM (if that makes any sense) and have a few questions.
Do I need LSTM units everywhere in the network? For example, can I only use LSTM units for the first and last layer and use feedforward units everywhere else?
How do I go about implementing bias values into an LSTM?
Assuming I create a network that predicts the next few words of a sentence, does that mean my outputs should be every possible word that the network could conceivably use?
",['long-short-term-memory'],"
For question 1) I dont understand what you are getting at... LSTM cells will work on a contiguous block of inputs, where it sequentially uses the states from a previous time step and the new input to generate the next ones.
question 2) Please look into the LSTM atchitecture 
As you see, biases are already there, is there somewhere specific you want it, that it isnt?
question 3) Generally yes, but the normalization step can be expensive (such as softmax), so if you want to get clever, you can use negative sampling or hierarchial softmax-- but generally, you you predict a probability over all possible words given the previous text
"
How does NEAT find the most successful generation without gradients?,"
I'm new to NEAT, so, please, don't be too harsh. How does NEAT find the most successful generation without gradient descent or gradients?
","['genetic-algorithms', 'optimization', 'gradient-descent', 'neat']",
Are there tools to help labelling images? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I need to manually classify thousands of pictures into discrete categories, say, where each picture is to be tagged either A, B, or C.
Edit: I want to do this work myself, not outsource / crowdsource / whatever online collaborative distributed shenanigans. Also, I'm currently not interested in active learning. Finally, I don't need to label features inside the images (eg. Sloth) just file each image as either A, B, or C.
Ideally I need a tool that will show me a picture, wait for me to press a single key (0 to 9 or A to Z), save the classification (filename + chosen character) in a simple CSV file in the same directory as the pictures, and show the next picture. Maybe also showing a progress bar for the entire work and ETA estimation.
Before I go ahead and code it myself, is there anything like this already available?
","['classification', 'datasets']","
There are a few tools that you can use to annotate (or label) data. For example, labelme or Labelbox. Have a look at this question for more alternatives.
"
Inverse Reinforcement Learning for Markov Games,"
This is an Inverse Reinforcement Learning (IRL) problem. I have data (observations) on actions taken by a (real) agent. Given this data I want to estimate the likelihood of the observed actions in a Q-learning agent. Rewards are given by a linear function on a parameter, say alpha.
Thus, I want to estimate the alpha that makes the observed actions more likely to be taken by a Q-agent. I read some papers (i.e. Ng & Russel 2004), but I found them rather generalistic.
",['reinforcement-learning'],
How does adding a small change to an neuron's weighted input affect the overall cost?,"
I was reading the following book: http://neuralnetworksanddeeplearning.com/chap2.html
and towards the end of equation 29, there is a paragraph that explains this: 

However I am unsure how the equation below is derived:

","['neural-networks', 'backpropagation']","
I believe he's just saying that:
$$
\frac{\partial C}{\partial z_j^l} \Delta z_j^l \approx \frac{\partial C}{\partial z_j^l} \partial z_j^l \approx \partial C
$$
so that the change in cost function can be arrived at simply for a small enough perturbation $\Delta z_j^l$.
Or, taking that line of approximations backwards, the change in the cost function for a given perturbation is just:
$$
\partial C \approx \frac{\partial C}{\partial z_j^l} \partial z_j^l \approx  \frac{\partial C}{\partial z_j^l} \Delta z_j^l
$$
"
Is there a Logistic Regression classifier that can perfectly classify the given data in this problem?,"
I have the following problem.

A bank wants to decide whether a customer can be given a loan,
based on two features related to (i) the monthly salary of the customer, and (ii) his/her account balance. For simplicity, we model the two features with two binary variables $X1$, $X2$ and the class $Y$ (all of which can be either 0 or 1). $Y=1$ indicates that the customer can be given loan, and Y=0 indicates otherwise.
Consider the following dataset having four instances:
($X1 = 0$, $X2 = 0$, $Y = 0$)
($X1 = 0$, $X2 = 1$, $Y = 0$)
($X1 = 1$, $X2 = 0$, $Y = 0$)
($X1 = 1$, $X2 = 1$, $Y = 1$)
Can there be any logistic regression classifier using X1 and X2 as features, that can perfectly classify the given data?

The approach followed in the question was to calculate respective probabilities for Y=0 and Y=1 respectively.  The value of $p$ obtained was $0.25$ and $(1-p)$ as $0.75$. The $\log(p/1-p)$ is coming as negative.
However, I don't understand what I need to do to understand whether there is a Logistic Regression classifier that can perfectly classify the given data.
","['machine-learning', 'logistic-regression']",
Is there a way to calculate the closed-form expression of the function that a neural network computes?,"
As stated in the universal approximation theorem, a neural network can approximate almost any function.
Is there a way to calculate the closed-form (or analytical) expression of the function that a neural network computes/approximates?
Or, alternatively, figure out if the function is linear or non-linear?
","['neural-networks', 'function-approximation', 'universal-approximation-theorems']","
The network is the function.
A network is a function, that is modeled by terms describing the architecture and coefficients that are learned.
Look at a simple model:
$$f(x) = ax+b$$
Your solver determines $a$ and $b$, and you substitute them into $f(x)$ and then you're able to calculate $f(42)$. The function is linear by definition, but may not be a good fit for your data.
To see if the input data may belong to a linear function can be achieved, when you have a network, that can both fit linear functions and higher order functions. For example try to fit your data to
$$f(x)=ax^2+bx+c$$
The function is linear, when $a=0$. If the function is quadratic you get $a\ne 0$ and if it has a higher order than $2$ you will get not good fit with any $a, b, c$.
When looking at MLP and similar networks, the question ""is it linear"" may not be that easy to answer, but the question how to get the function is the same: Your trained network is the function.
I guess one option to see if the function approximated by an existing network is linear is to train a second network that only contains linear elements. If it is able to approximate your first network, the first network is linear as well. Of course you will need the right test data, as you may accidentally choose a training set that has linearly correlated output in a non-linear function. This is of course the same problem as always, that your network will always can only be as good as the training data.
"
How do we give a kick start to the Facenet network? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I read the Facenet paper and one thing I am not sure about (it might be trivial and I missed it) is how do we give the kick start to the network.
The embeddings, in the beginning, are random, so picking hard (or semi-hard) negatives, based on the Euclidean distance, would give random images in the beginning.
Do we hope that over time this will converge to the actual desired hard images? Is it any reason to expect that this convergence will be attained?
","['convolutional-neural-networks', 'convergence', 'facenet']",
Neural nets for novices [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



Stories like this one are quite popular these days.
The idea of training a neural net to do something silly like this may sound trivial to experts like you,  but for a novice like me it could be an interesting learning experience.
Is there novice-friendly software I could play with to train a neural net to do something like this or is there necessarily a steep learning curve?
",['neural-networks'],"
An intuitive NN playground can be found in TensorFlow Playground 
Also, check the Google ML crash course for coders as they promised to add more practicals.
"
Calculating Parameter value Using Gradient Descent for Linear Regression Model,"

Consider the following data with one input (x) and one output (y):
  (x=1, y=2)
  (x=2, y=1)
  (x=3, y=2)
  Apply linear regression on this data, using the hypothesis $h_(x) = _0 + _1 x$, where $_0$ and $_1$ represent the parameters to be learned. Considering the initial values $_0$= 1.0, and $_1$ = 0.0, and learning rate 0.1, what will be the values of $_0$ and $_1$ after the first three iterations of Gradient Descent

From least squares method I took the derivative with respect to $_0$ and $_1$ and plugged in the initial values to get the slope/intercept and multiplied it by the learning rate 0.1 to get the step size.The step size was used to calculate the new $_0$ and $_1$ values.  
I am getting $_0$ as 1.7821 when following the above. Please let me know if the approach followed and the solution correct or there is a better way to solve
","['machine-learning', 'linear-regression']",
Convolutional neural network debugging,"
Im trying to implement CNN for small images classification (36x36x1) (grayscale). I've checked every forward/backward pass function on small example, and still my cnn is not doin any progress on training. Tests were done on learning rate [0.001 - 0.01].
Network structure:
Conv -> Relu -> Conv -> Relu -> MaxPooling -> Conv -> Relu -> Conv -> Relu -> MaxPooling -> Flattening -> sigmoid -> Fully connected -> sigmoid ->Fully connected -> softmax
Is there a mistake in forwardPass/ backwardPass function?
    def forwardPass(self, X):
    """"""
        :param X:   batch of images (Input value of entire convolutional neural network)
                    image.shape = (m,i,i,c) - c is number of channels
                    for current task, first input c = 1 (grayscale)
                    example: for RGB c = 3
                    m - batch size
                    X.shape M x I x I x C

        :return :   touple(Z, inValues)
                    Z - estimated probability of every class
                    Z.shape M x K x 1


    """"""

    W = self.weights

    inValues = {
        'conv': [],
        'fullyconnect': [],
        'mask' : [],
        'pooling' : [],
        'flatten' : [],
        'sigmoid' : [],
        'relu' : []
    }

    """"""
        Current structure:
        Conv -> Relu -> Conv -> Relu -> MaxPooling -> Conv -> Relu -> Conv -> Relu -> MaxPooling -> Flattening -> 
        -> sigmoid -> Fully connected -> sigmoid ->Fully connected -> softmax
    """"""



    inValues['conv'].append(X)
    Z = self.convolution_layer(X, W['conv'][0]);z = Z

    inValues['relu'].append(z)
    Z = self.relu(z);z =Z

    inValues['conv'].append(z)
    Z = self.convolution_layer(z, W['conv'][1]);z = Z


    inValues['relu'].append(z)
    Z = self.relu(z);z =Z


    inValues['pooling'].append(z)
    Z, mask = self.max_pooling(z);z = Z
    inValues['mask'].append(mask)



    inValues['conv'].append(z)
    Z = self.convolution_layer(z, W['conv'][2]);z = Z

    inValues['relu'].append(z)
    Z = self.relu(z);z = Z

    inValues['conv'].append(z)
    Z = self.convolution_layer(z, W['conv'][3]);z = Z

    inValues['relu'].append(z)
    Z = self.relu(z);z = Z

    inValues['pooling'].append(z)
    Z, mask = self.max_pooling(z);z = Z
    inValues['mask'].append(mask)


    inValues['flatten'].append(z)
    Z = self.flattening(z);z = Z


    inValues['sigmoid'].append(z)
    Z = self.sigmoid(z); z = Z


    inValues['fullyconnect'].append(z)
    Z = self.fullyConnected_layer(z, W['fullyconnect'][0]); z = Z


    #dropout here later

    inValues['sigmoid'].append(z)
    Z = self.sigmoid(z); z = Z


    inValues['fullyconnect'].append(z)
    Z = self.fullyConnected_layer(z, W['fullyconnect'][1]);z = Z


    Z = self.softmax(z)


    return Z, inValues

Backpropagation:
 def backwardPass(self, y, Y, inValues):

    """"""

    :param Y: estimated probability of all K classes
                ( Y.shape = M x K x 1 )
    :param y: True labels for current
                M x K x 1
    :param inValues: Dictionary with input values of conv/ff layers
                     example: inValues['conv'][1] - Values encountered during feedForward on input of Conv layer with index 1
    :return:  Gradient of weights in respect to L
    """"""

    np.set_printoptions(suppress=True)
    W = self.weights

    G = {
        'conv' : [],
        'fullyconnect' : []
    }


    Z = self.softmax_backward(Y, y); z = Z



    Z, dW, dB = self.fullyConnected_layer_backward(z, W['fullyconnect'][1],inValues['fullyconnect'][1]);z = Z
    weight = {
        'W': dW,
        'B': dB
    }
    G['fullyconnect'].append(weight)



    Z = self.sigmoid_deriv(z, inValues['sigmoid'][1]); z = Z

    Z, dW, dB = self.fullyConnected_layer_backward(z, W['fullyconnect'][0],inValues['fullyconnect'][0]);z = Z;
    weight = {
        'W': dW,
        'B': dB
    }
    G['fullyconnect'].append(weight)


    Z = self.sigmoid_deriv(z, inValues['sigmoid'][0]);z=Z


    Z = self.flattening_backward(z, inValues['flatten'][0]); z = Z


    Z = self.max_pooling_backward(z,inValues['mask'][1]); z = Z


    Z = z * self.relu(inValues['relu'][3], deriv=True); z = Z

    Z, dW, dB = self.convolution_layer_backward(z, W['conv'][3],inValues['conv'][3]); z = Z
    weight = {
        'W': dW,
        'B': dB
    }
    G['conv'].append(weight)

    Z = z * self.relu(inValues['relu'][2], deriv=True);z = Z


    Z, dW, dB = self.convolution_layer_backward(z, W['conv'][2],inValues['conv'][2]); z = Z
    weight = {
        'W': dW,
        'B': dB
    }
    G['conv'].append(weight)


    Z = self.max_pooling_backward(z,inValues['mask'][0]);z = Z


    Z = z * self.relu(inValues['relu'][1], deriv=True);z = Z

    Z, dW, dB = self.convolution_layer_backward(z, W['conv'][1],inValues['conv'][1]); z = Z
    weight = {
        'W': dW,
        'B': dB
    }
    G['conv'].append(weight)

    Z = z * self.relu(inValues['relu'][0], deriv=True);z = Z

    Z, dW, dB = self.convolution_layer_backward(z, W['conv'][0],inValues['conv'][0]); z = Z
    weight = {
        'W': dW,
        'B': dB
    }
    G['conv'].append(weight)

    G['conv'].reverse()
    G['fullyconnect'].reverse()

    return G

update:
    def update(self, alfa, W, G):

    W['fullyconnect'][0]['W'] -= alfa * np.sum(G['fullyconnect'][0]['W'],axis=0)
    W['fullyconnect'][1]['W'] -= alfa * np.sum(G['fullyconnect'][1]['W'],axis=0)
    W['fullyconnect'][0]['B'] -= alfa * np.sum(G['fullyconnect'][0]['B'],axis=0)
    W['fullyconnect'][1]['B'] -= alfa * np.sum(G['fullyconnect'][1]['B'],axis=0)

    W['conv'][0]['W'] -= alfa * np.sum(G['conv'][0]['W'],axis=0)
    W['conv'][1]['W'] -= alfa * np.sum(G['conv'][1]['W'],axis=0)
    W['conv'][2]['W'] -= alfa * np.sum(G['conv'][2]['W'],axis=0)
    W['conv'][3]['W'] -= alfa * np.sum(G['conv'][3]['W'],axis=0)
    W['conv'][0]['B'] -= alfa * np.sum(G['conv'][0]['B'],axis=0)
    W['conv'][1]['B'] -= alfa * np.sum(G['conv'][1]['B'],axis=0)
    W['conv'][2]['B'] -= alfa * np.sum(G['conv'][2]['B'],axis=0)
    W['conv'][3]['B'] -= alfa * np.sum(G['conv'][3]['B'],axis=0)

    return W

","['neural-networks', 'convolutional-neural-networks']",
What is the actual quality of machine translations?,"
As an AI layman, till today I am confused by the promised and achieved improvements of automated translation.
My impression is: there is still a very, very far way to go. Or are there other explanations why the automated translations (offered and provided e.g. by Google) of quite simple Wikipedia articles still read and sound mainly silly, are hardly readable, and only very partially helpful and useful?
It may depend on personal preferences (concerning readability, helpfulness, and usefulness), but my personal expectations are disappointed sorely.
The other way around: Are Google's translations nevertheless readable, helpful, and useful for a majority of users?
Or does Google have reasons to retain its achievements (and not to show to the users the best they can show)?
","['natural-language-processing', 'natural-language-understanding', 'machine-translation', 'google-translate']","
Who claimed that machine translation is as good as a human translator? For me, as a professional translator who makes his living on translation for 35 years now, MT means that my daily production of human quality translation has grown by factor 3 to 5, depending on complexity of the source text.
I cannot agree that the quality of MT goes down with the length of the foreign language input. That used to be true for the old systems with semantic and grammatical analyses. I don't think that I know all of the old systems (I know Systran, a trashy tool from Siemens that was sold from one company to the next like a Danaer's gift, XL8, Personal Translator and Translate), but even a professional system in which I invested 28.000 DM (!!!!) failed miserably. 
For example, the sentence: 

On this hot summer day I had to work and it was a pain in the ass.

can be translated using several MT tools to German.
Personal Translator 20:

Auf diesem heien Sommertag musste ich arbeiten, und es war ein Schmerz im Esel.

Prompt: 

An diesem heien Sommertag musste ich arbeiten, und es war ein Schmerz im Esel.

DeepL: 

An diesem heien Sommertag musste ich arbeiten und es war eine Qual.

Google: 

An diesem heien Sommertag musste ich arbeiten und es war ein Schmerz im Arsch.

Today, Google usually presents me with readable, nearly correct translations and DeepL is even better. Just this morning I translated 3500 words in 3 hours and the result is flawless although the source text was full of mistakes (written by Chinese).
"
Do you need to store prevous values of weights and layers on recurrent layer while BPTT?,"
The Back propagation through time on recurrent layer is defined similar to  normal one, means somethin like 
self.deltas[x] =  self.deltas[x+1].dot(self.weights[x].T) * self.layers[x] * (1- self.layers[x]) where 
self.deltas[x+1] is error from prevous layer, self.weights[x] is weights map and self.layers[x](1- self.layers[x]) is bakwards activation of sigmoid function where self.layers[x] is vector of sigmoid. But while normal backpropagation the values are there, while BPTT i can not take the current self.layers[x] : i need the previous ones, right ? 
So unlike normal BP, do i need extra store old weights and layers, for example in circular queue, and then apply the formula where self.deltas[x+1] is layer from next time ?
Not realy implementation, just basic understanding in order to can implement it.
Lets see the picture:

Here are : self.layers[0] = $x_{t+1}$, self.layers[1] = $h_{t+1}$ , self.layers[2] = $o_{t+1}$, in order to perform backprop $h_{t+1}$ -> $h_{t}$ -> $h_{t-1}$...  I DO NEED to have layers $h_t$ ,$h_{t-1}$... and weights $v_{t+1}$, $v_t$...  EXTRA stored in additional to the network $x_{t+1}$ -> $h_{t+1}$ -> $o_{t+1}$, right? 
Thats all the question.
And i do not need to store previous outputs $o[t, o_{t-1}, etc..]$, because backprop from them ot->ht, etc was already calculated. 
","['recurrent-neural-networks', 'backpropagation']",
Will BERT embedding be always same for a given document when used as a feature extractor,"
When we use BERT embeddings for a classification task, would we get different embeddings every time we pass the same text through the BERT architecture? If yes, is it the right way to use the embeddings as features? Ideally, while using any feature extraction technique, features values should be consistent. How do I handle this if we want BERT to be used as a feature extractor?
","['machine-learning', 'natural-language-processing', 'word-embedding', 'bert']","
BERT is deterministic. There is no variation unless you parse your tokens differently in succeeding runs. Here is the original paper the model architecture is based off of Transformer Paper. Note that in every layer, the only operations used for the most part are matrix multiplications, concatenations, basic ops, and layer normalizations, all of which are deterministic.
"
Generate QA dataset from large text corpus,"
I have a corpus of a domain data in form of 10-15 books pdf and some articles and my end-goal is to make a question-answering system particular to that domain.
For that, I would need a dataset on Q/A which I can use on top of something like SQuAD(Stanford Question Answering Dataset) for domain-specific knowledge
My stuck point is how to convert this corpus into a usable question-answering dataset.
My current strategy is something AllenAI has been working with. A list of their research papers on it can be found here
As I understand they use a combination of Knowledge Extraction, Natural Language Understanding, and Inference to get the job done. But I cannot find any good practical implementation. 
Where can I find a good resource?
","['natural-language-processing', 'python', 'datasets']",
How do I identify a monologue or dialogue in a conversation?,"
How do I identify monologues and dialogues in a conversation (or transcript) using natural language processing? How do I distinguish between the two?
","['machine-learning', 'natural-language-processing', 'machine-translation']","
There are a few different ways you could frame the problem...
For example, the simplest, yet probably not the most effective, would be to treat it as a supervised classification task.
In this case, you would gather data, split it into 2 classes(binary). With one dataset consisting of dialogues and the other monologues. Obviously, this framing comes with it's own set of problems. Namely, how do you prepare the dataset, or how do you deal with live examples?
Of course, there are other ways you could tackle the problem, perhaps the detection context. It all depends on what you feel is the easiest tenable solution given the variables involved in your specific instance.
"
Neural Nets: CNN confirming layer/filter arithmetic [duplicate],"







This question already has answers here:
                                
                            




How is the depth of a convolutional layer determined?

                                (3 answers)
                            


How to calculate the number of parameters of a convolutional layer?

                                (2 answers)
                            

Closed 1 year ago.



I was hoping someone could just confirm some intuition about how convolutions
work in convolutional neural networks. I have seen all of the tutorials on
applying convolutional filters on an image, but most of those tutorials
focus on one channel images, like a 128 x 128 x 1 image. I wanted to clarify
what happens when we apply a convolutional filter to RGB 3 channel images.
Now this is not a unique question, I think a lot of people ask this question as well. It is just that there seem to be so many answers out there, each with their own variations, that it is hard to find a consistent answer. I included a post below that seems to comport with what my own intuition, but I was hoping one of the experts on SE could help validate the layer arithmetic, to make sure my intuition was not off.
How is the depth of the input related to the depth of the output of a convolutional layer?
Consider an Alexnet network with 5 convolutional layers and 3 fully connected
layers. I borrowed the network from this post.  Now, say the input is 227 x 227, and the filter is specified
as 11 x 11 x 96 with stride 4. That means there are 96 filters each with dimensions 11x11x3, right?
So there are a total of 363 parameters per filter--excluding the bias term--
and there are 96 of these filters to learn. So the 363*96 = 34848 filter values are learned
just like the weights in the fully connected layers right?
My second question deals with the next convolutional network layer. In the next
layer I will have an image that is 55 x 55 x 96 image. In this case, would the
filter be 5x5x96--since there are now 96 feature maps on the image? So that means
that each individual filter would need to learn 5x5x96 = 2400 filter values (weights),
and that across all 256 filters this would mean 614,400 filter values?
I just wanted to make sure that I was understanding exactly what is being learned
at each level.
","['deep-learning', 'convolutional-neural-networks', 'weights', 'convolution-arithmetic']",
Online normalization of database for DQN,"
I have an issue with the normalization of the database (a large time series) for my DQN. I obtained optimal results and saved the NN (5 LSTM layers) weights training on a database normalized as such: I divided it into consecutive batches of 96 steps (the window size that my NN gets as input) and I normalized each batch respectively with Z-score. However, I am unable to extend these results to an online setting, as online I only have access to the last 96 elements, and thus I can only normalize according to the last 96. This small difference actually causes a sharp decrease in the performance of my DQN, as the weights of the NN were perfectly tuned for the first normalization but are not great with the online normalized database. In a nutshell, the problem is that only every 96 steps the first normalized database and the online one are the same, for all steps in between this is not happening. I have the weights for the first one, but I cannot find a way to exploit them for the online one.
What I have tried so far with the online database:

If I normalize every last 96 steps, and act for every new step (as it should be), the performances are quite bad.
If I normalize every last 96 steps, and act just every 96 steps (repeating the same action in between), the agent is actually picking the optimal action every 96 steps (like in the offline setting), so the results are somewhat decent but far from optimal for the long period between the actions. If I try with shorter periods, like 48, performances decrease sharply as it only acts optimally every 2 actions.

I don't know if there is a way to tune the optimal weights for the online database, acting directly on them without going through training again. It would be nice to understand why the NN picks its actions at each step in the optimal setting, so that I would be able to follow its strategy, but I'm not aware if it's possible to actually deduct this from the analysis of weights and features, especially for a multi-layer LSTM network. 
Otherwise, I was thinking about something like normalizing the online database directly through similarities with the old batches of 96 (using their mean and std) or something like that. Anything that would help reducing the time between optimal actions to around 50-60 steps instead of 96 would be enough to provide a nearly optimal strategy, so at this point, I would consider any kind of (unelegant) method to get what I want.
I don't know if any of these is feasible, but retraining the agent is very difficult as every single time but once the agent got stuck in suboptimal strategies, this is why I am trying to get around this problem using the optimal weights I have instead of retraining. 
","['reinforcement-learning', 'python', 'dqn', 'online-learning']",
Does the human brain use beam search for text generation?,"
As far as I understand, beam search is the most widely used algorithm for text generation in NLP. So I was wondering: does the human brain also use beam search for text generation? If not, then what?
","['natural-language-processing', 'brain']","
I thought the answer might be no. 

In this 2020 ICLR paper: The Curious Case of Neural Text Degeneration, researchers found that beam search text is less surprising compared to human natural language. And they proposed a nucleus sampling method which generates more human like text.  
"
How to represent action space in reinforcement learning?,"
I started to learn reinforcement learning a few days ago. And I want to use that to solve resource allocation problem something like given a constant number, find the best way to divide it into several real numbers each is non-negative.
For example, to divide the number 1 into 3 real numbers, the allocation can be:
[0.2, 0.7, 0.1]
[0.95, 0.05, 0]
...
I do not know how to represent the action space because each allocation is 3-dimensional and each dimension is real-valued and each other correlated.
In actor-critic architecture, is it possible to have 3 outputs activated by softmax in the actor's network each represents one dimension in the allocation?

Appended:
There is a playlist of videos. A user can switch to the next video at any time. More buffer leads to better viewing experience but more bandwidth loss if user switches to the next video. I want to optimize the smoothness of playback with minimal bandwidth loss. At each time step, the agent decides the bandwidth allocation to download current video and the next 2 videos. So I guess the state will be the bandwidth, user's behavior and the player situation.
",['reinforcement-learning'],
Super Resolution on text documents,"
I want to implement super-resolution and deblurring on images from text documents. Which is the best approach? Are there any Git-hub links which will help me to start? I am new to the field. Any help would be appreciated. Thanks in advance.
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks']",
Reward does not increase for a maze escaping problem with DQN,"
I am using deep reinforcement learning to solve a classic maze escaping task, similar to the implementation provided here, except the following three key differences:

instead of using a numpy array as the input of a standard maze escaping task, I am feeding the model with an image at each step; the image is a 1300 * 900 RGB image, so it is not too small.
reward: 

each valid move has a small negative reward (penalize long move) 
each invalid move has a big negative reward (run into other objects or boundaries) 
Each blocked move has the minimal reward (not common) 
Find the remote detectors defect has a positive reward (5)

I tweaked the parameters of replay memory, reduced the size of the replay memory buffer.

Regarding the implementation, I basically do not change the agent setup except the above items, and I implemented my env to wrap my customized maze. 
But the problem is that, the accumulated reward (first 200 rounds of successful escaping) does not increase:

And the number of steps it takes to escape one maze is also stable somewhat:

Here are my question, on which aspect I could start to look at to optimize my problem? Or is it still too early and I will need to train more time?
","['deep-learning', 'reinforcement-learning', 'deep-rl']",
"Understanding the equation of TD(0) in the paper ""Learning to predict by the methods of temporal differences""","
In the paper Learning to predict by the methods of temporal differences (p. 15), the weights in the temporal difference learning are updated as given by the equation
$$
\Delta w_t
= \alpha \left(P_{t+1} - P_t\right) \sum_{k=1}^{t}{\lambda^{t-k} \nabla_w P_k}
\tag{4}
\,.$$
When $\lambda = 0$, as in TD(0), how does the method learn? As it appears, with $\lambda = 0$, there will never be a change in weight and hence no learning. 
Am I missing anything?
","['reinforcement-learning', 'temporal-difference-methods', 'notation']",
What does the notation $p_t = \text{max}_{i<t} p_i$ mean in algorithm 1 of the prioritized experience replay paper?,"
I am having a hard time converting line 6 of the prioritized experience replay algorithm from the original paper into plain English (see below):

I understand that new transitions (not visited before) are given maximal priority. On line 6 this would be done for every transition in an initial pass since the history is initialized as empty on line 2.
Im having trouble with the notation $p_t = \text{max}_{i<t}  p_i$. Can someone please state this in plain English? If $t$ = 4 for example, then $p_t$ = 4? How is this equal to max$_{i<t}  p_i$.
It seems in my contrived example here, max$_{i<t}  p_i$ would be 3. I must be misreading this notation.
","['reinforcement-learning', 'dqn', 'deep-rl', 'experience-replay', 'double-dqn']",
How can I train a deep learning model to predict a matrix? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am trying to train a deep learning model to predict an 8*2 matrix. The predicted matrix would have complex values and the input matrix would be real numbers. Can it be done? Thank you for your time.
","['deep-learning', 'convolutional-neural-networks']","
You could use a CNN or Fully Connected Network and output a matrix of size 8*2*2. The first 8*2 matrix is the real number and the second is the imaginary number. Example code below uses keras.
'''
input: numpy array of shape(batch_size,input_dim_0,input_dim_1,1)
Y: keras tensor of shape(batch_size,8,2,2)

'''

model = Sequential()
model.add(Conv2d(64,3,input_dim =(input_dim_0,input_dim_1,1))
model.add(Conv2d(64,3)
model.add(Conv2d(2,3)

model.compile(loss='MSE',metrixs=['accuracy'])
model.fit(input,Y)




"
Infinite horizon in Reinforcement Learning,"
I read this article: ""Towards Autonomous Data Ferry Route Design through Reinforcement Learning"" by Daniel Henkel and Timothy X Brown. It specifies an infinite horizon problem where they use as a reward function for TD[0] the following:
\begin{equation}
r(s,a) = - \int_{t_0}^{t_1} (Ft +N_0)e^{-\beta t} dt
\end{equation}
where $N_0$ and $F$ are constant, $\beta$ is used to adjust the discount factor and $t_0 , t_1$ are the initial and final time.
Then they proceed to use $e^{-\beta t}$ as the $\gamma$ (discount factor) in the TD[0] update formula and in the policy formula.
Why is the discount factor in the infinite horizon problem $e^{-\beta t}$, and why is it used as $\gamma$ in $V(s)$ update, since it is a variable factor?
Also, in the formula of the TD[0] update they don't subtract $\alpha V(s)$. They do:
\begin{equation}
V_{t+1}(s) = V_t(s) + \alpha( r(s,a) + e^{-\beta t_a} V_t(s'))
\end{equation}
I really think this is a mistake, and the values of $V$ will explode without it, even in an infinite horizon problem. Am I correct ? Is $- V(s)$ missing inside the brackets?
Finally, if someone is willing and has the time to directly read this part of the article and enlighten me on if $t_0$ and $t_1$ represent the initial and final time of an action OR $t_0$ is always 0 and $t_1$ is the duration of the action, I would appreciate it. I ask this because from what is written in the paper $t_0$ seems to be the current time in the simulation, but I'm afraid that would just decay too fast and after some actions the reward would be close to 0. It is really not well explained and I'm a bit confused. 
Thank you for your time if you got this far reading. Any guideline answer will be very much appreciated.
","['reinforcement-learning', 'papers', 'temporal-difference-methods']",
What could be causing the drastic performance drop of the DQN model on the Pong environment?,"
I am running a basic DQN (Deep Q-Network) on the Pong environment. Not a CNN, just a 3 layer linear neural net with ReLUs.
It seems to work for the most part, but at some point, my model suffers from catastrophic performance loss:


What is really the reason for that?
What are the common ways to avoid this? Clipping the gradients? What else? 

(Reloading from previous successful checkpoints feels more like a hack, rather than a proper solution to this issue.)
","['reinforcement-learning', 'dqn', 'deep-rl']",
What type of network for a repeated experiment,"
I have a problem where I have 9 data points that are collected every minute for 40 minutes, and, by the 40th minute, the solution would be either end up being black or white.
I would like to set up a neural network, which would take the live input of every minute; and I was hoping within the 25-30 minute mark to predict the outcome of what the results would be at 40 minute; which is a classification.
I have over 3000 historical runs of this experiment; each containing 40 rows of 9 columns data per experiment.
What network would I need to set up; so that it can learn from each run at every minute mark per experiment with the results; and then set it up for live input, when the experiment is running again.
I feel like I might need more than one system to accomplish this; any help in pointing me towards the right path would be greatly appreciated
I am using python (keras) to try to solve this problem.
","['neural-networks', 'long-short-term-memory', 'keras', 'time-series']",
"Do VR, AR and MR use any machine learning or deep learning?","
I wonder if Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) use any machine learning or deep learning?
For example in AR, the virtual objects are brought into the real world, does this process involve any object detection and localization?
","['machine-learning', 'deep-learning', 'applications']","
Certainly in Virtual Reality the new Deep Convolutional Neural Networks algorithms applied in image processing are playing a role! Object detection, segmentation and semantic imaging are all features of these new ways of image processing. Augmented Reality then uses some advanced Optics features to accomplish its job. And so on...
"
Can a CNN be trained incrementally?,"
Like our human brain, we can first learn (train) the handwriting 0 and 1. After the traing (and test) accuray is good enough, we only need to study (traing) the hardwriting 2,  Instead of cleaning all of learned memory, and relearn handwriting data 0, 1, and 2 at the same time. 
Can CNN do the same thing? Can CNN learn something new, but keep the previous memory? If yes, the efficiency could be high.  Right now, I have to give all of data at the same time, the efficiency is very very low.
","['machine-learning', 'convolutional-neural-networks', 'incremental-learning', 'online-learning']","
You are looking for incremental (or online) learning.
A CNN can be trained incrementally. For example, in the paper Incremental Learning of Convolutional Neural Networks, the authors propose an incremental learning algorithm (inspired by AdaBoost and Learn++, which is another incremental learning algorithm for supervised learning of neural networks) for CNNs. 
However, note that incremental learning is a challenging task, given the  stability-plasticity dilemma: a completely stable model, in order to keep being stable, will attempt to preserve the existing knowledge, so it will not learn new knowledge; similarly, a completely plastic model, in order to keep being plastic, it will keep forgetting previously acquired knowledge so that to learn new information.
"
How to learn to sample?,"
Imagine you have access to a dataset of pairs $(s, \hat{\pi}(s))$ where s is a state in a high dimension continuous space $S$, $\pi(s)$ is a probabilistic distribution on a large discrete space $D$ (size around $10^{9}$), and $\hat{\pi}(s)$ is a sample from this distribution.
A detail is important : $\pi$ is not a distribution right, it's $\pi(s)$ who is one.
My question is simple : how can an algorithm learn to, given a state s, sample from $\pi(s)$ ?
","['neural-networks', 'reinforcement-learning', 'autoencoders']",
Which function $(\hat{y} - y)^2$ or $(y - \hat{y})^2$ should I use to compute the gradient?,"
The MSE can be defined as $(\hat{y} - y)^2$, which should be equal to $(y - \hat{y})^2$, but I think their derivative is different, so I am confused of what derivative will I use for computing my gradient. Can someone explain for me what term to use?
","['math', 'gradient-descent', 'objective-functions']","

The MSE can be defined as $(\hat{y} - y)^2$, which should be equivalent to $(y - \hat{y})^2$

They are not just ""equivalent"". It is actually the exact same function, with two different ways to write it.
$$(\hat{y} - y)^2 = (\hat{y} - y)(\hat{y} - y) = \hat{y}^2 -2\hat{y}y + y^2$$
$$(y - \hat{y})^2 = (y -\hat{y})(y - \hat{y}) = y^2 -2y\hat{y} + \hat{y}^2$$
These are exactly the same function. Not just ""equivalent"" or ""equivalent everywhere"", but actually the same function. It is therefore no surprise that any derivative is also the same - including the partial derivative with respect to $\hat{y}$ which is what you typically use to drive gradient descent.
The two ways of writing the function is because it is a square and thus has two factorisations. When you write it as a square you can choose which form to use for the inner term.

Which function [form] should I use to compute the gradient?

You can use either form, it does not matter. They represent the same function and have the same gradient.
"
"Why is a mix of greedy and random usually ""best"" for stochastic local search?","
I read that a mix of ""greedy"" and ""random"" are ideal for stochastic local search (SLS), but I'm not sure why. It mentioned that the greedy finds the local minima and the randomness avoids getting trapped by the minima. What is the minima and how can you get trapped? Also, how does randomness avoid this? It seems like if it's truly random there's always a chance of ending up searching solutions that lead to dead ends multiple times (which seems like a waste of processing and avoidable)?
","['optimization', 'search', 'problem-solving', 'local-search']","
As an example of local/global minima, imagine being on a rugged, mountainous landscape, and you want to find the lowest point within some area. For a greedy search, every step you take will take you downhill. If you go downhill long enough, you'll eventually find a flat spot, which is a minimum - from here, there's no step you can take that will get you any lower. However, there's a nearby ridge, which if you crossed it, you could continue downhill to find an even lower spot, the global minimum (the true lowest point). Using your greedy approach, you'll never go uphill to cross the ridge, so you'll be stuck in the local minimum forever. If you occasionally take random steps (other than directly downhill), you have the opportunity to cross ridges that separate local minima, and you have a better chance of finding the global minimum. You are correct that in many cases, the random step won't help you cross a ridge, and will just take you up a mountain in the wrong direction, which is a waste of time. But unless we allow the algorithm to ""explore"" a bit, it will be content that the first minimum it finds is the best one, and will never get to the bottom.
"
Can we evolve 0 and 1? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



Is it possible to combine or create conditional statements of 0 and 1, and optimize with an evolutionary algorithm (given that all computers use a binary system)?
There may be an algorithm that maps input and output to 0 and 1, or a conditional statement that edits a conditional statement.
An example of a binary conditional statement is if 11001 then 01110.
Just as molecules are combined to form living beings, we could begin with the most fundamental operations (0 and 1, if then) to develop intelligence.
","['machine-learning', 'genetic-algorithms', 'evolutionary-algorithms']",
Why do authors track $\gamma_t$ in Prioritized Experience Replay Paper?,"
In the original prioritized experience replay paper, the authors track $\gamma_t$ in every state transition tuple (see line 6 in algorithm below):

Why do the authors track this at every time step?  Also, many blog posts and implementations leave this out (including I believe the OpenAI implementation on github).
Can someone explain explicitly how $\gamma_t$ is used in this algorithm?
Note: I understand the typical use of $\gamma$ as a discount factor. But typically gamma remains fixed. Which is why Im curious as to the need to track it. 
","['dqn', 'deep-rl', 'experience-replay']",
Other deep learning image generation techniques besides GANs?,"
Can you please point me to some resources about image genereation besides GANs?
Are there any other techniques throughout history?
How did idea of image generation evolved and how it started?
I tried googling ""image generation before gans"" and similar alternatives but without any success.
","['deep-learning', 'reference-request', 'generative-model', 'image-generation']",
How can I implement a GAN network for text (review) generation? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



How can I implement a GAN network for text (review) generation?
Please, can someone guide me to resource (code) to help in text generation? 
","['deep-learning', 'natural-language-processing', 'generative-adversarial-networks', 'software-evaluation']","
As @Clement mentions, text_gen_description gives a good overview!, but the paper seqGAN paper describes the REINFORCE approach more in depth, as they are the first to do it (i believe). This is probably the approach most take now of days when going the GAN route.  
Note that just basic MLE training has shown promise with openAI's GPT2. When i need a text generator, fine tuning one of the provided models is usually my goto.  
Also if your looking for seq gans code base (you asked for example code) here is is: git repo
Good Luck!
"
Which API can I use for tracking the position of animal in one or more images?,"
I'd like to build an application for tracking the position of a given animal (e.g. a cat) in a series of images.
Is there any off-the-shelf API I could use?
Azure has some Vision APIs, but it seems to me they can't be used to get the position of something in an image.
","['machine-learning', 'computer-vision', 'object-recognition']",
Understanding the configuration of replay memory and epsilon in deep reinforcement learning,"
I am tentatively reusing a codebase of pacman to train my own deep reinforcement learning model. While most of the components seems reasonable and understandable to me, there are two things that seem obscure to me:

How to decide the size of the replay memory? Currently, since I set the total step of learning as 4000 (note that in the referred codebase this value is set as 4000000), so I just proportionally decrease the replay_memory_size as 400. Would that make sense? 
What is the return value epsilon when calling function PiecewiseSchedule? I also proportionally decrease its parameters as follows:

        epsilon = PiecewiseSchedule([(0, 1.0),
                                     (40, 1.0), # since we start training at 10000 steps
                                     (80, 0.4),
                                     (200, 0.2),
                                     (400, 0.1),
                                     (2000, 0.05)], outside_value=0.01)
        replay_memory = PrioritizedReplayBuffer(replay_memory_size, replay_alpha)

where the original function call is like this:
        epsilon = PiecewiseSchedule([(0, 1.0),
                                     (10000, 1.0), # since we start training at 10000 steps
                                     (20000, 0.4),
                                     (50000, 0.2),
                                     (100000, 0.1),
                                     (500000, 0.05)], outside_value=0.01)
        replay_memory = PrioritizedReplayBuffer(replay_memory_size, replay_alpha)

And in general, what is the principle (guideline) behind setting a good size of ""replay memory"" and calling function PiecewiseSchedule? Thank you!
","['deep-learning', 'reinforcement-learning']",
Will balancing dataset of images for object detection for a single-shot OD (Yolov3-spp) by cropping lower the quality of the model?,"
I'm trying to work out an approach to balancing my dataset, which is a subset of a google openimages - some classes are represented orders of magnitude more than others and I am hesistant to simply throw away data. The approach I want to try next is roughly:
Add a single instance of every image to the training set.
Crop out instances of the majority class from entire dataset and then add these cropped images to the dataset. keep adding these cropped images to training set until number of instances of 2nd majority class is close to number of instances of 1st majority class.
Crop out instances of second majority class and repeat (2)
keep going until all images have been cropped down to the minority class and dataset is roughly balanced.

This approach would mean that no instance of any class would be repeated more often than any other instance of the same class. My concern is that the minority classes will end up being predominantly represented in training by heavily cropped images. I'm not sure but I don't think this would matter in a two-stage object detector but I'm concerned that it might be a problem for a one-stage detector if, for some classes, it was predominantly trained on images that were cropped to various degrees.
The alternative to doing this would be undersampling ie. just throwing away most of the data which I'm hesistant to do.
Anyway, it would be really great to get some opinions on this in the context of Yolov3? Is yolov3 sufficiently robust at scale-invariance that training on many examples that are cropped/larger than what they are likely to be at test time?
(I'm fairly commited to some sort of oversampling because of the results in this paper which found best results generally from oversampling with CNN classifiers as opposed to undersampling, threshold adjustment etc. https://arxiv.org/pdf/1710.05381.pdf)
","['datasets', 'object-recognition']",
How do GANs create an image with specific characteristics?,"
I've seen GANs that do things like convert an image to a painting or this GAN here https://make.girls.moe/#/ that takes in a set of characteristics and generates a waifu with those characteristics.
My understanding of a GAN is that the generator upsamples random noise and the discriminator detects if an image is within the real or fake. So if the generator say, generates a waifu with the wrong hair color, how would the discriminator know?
","['neural-networks', 'generative-model', 'generative-adversarial-networks', 'image-generation']",
Are hadoop ecosystem tools main goal is to break up large data sets into fast readable files?,"
I am  new to big data theory, and during the past 3 days, I took an official big data course with some of the best instructors available in my country in this domain. The things was little bit obscure for me as I have an engineering background but with no knowledge in AI techniques and domains.
After getting an intro on big data and the 5Vs (Volume, Velocity, ...), we got an intro on hadoop and hadoop ecosystem tools (Hive, Pig, ...). Then a simple example on how to run MapReduce Java script on small data file.
So to make things clear with me, are hive, pig and other hadoop ecosystem tools, are tools to break up my large data files from different sources and servers into fast-readable files, by which we create new tables with our required fields to use them later on in machine learning scripts and feature extractions ?
P.S. by fast readable files I mean, using a scripting tools that normal relational database tools like SQL and oracle don't have it on huge data sets (1 terrabytes and above) to manage and get info from it as fast as possible ?
","['machine-learning', 'data-science']",
Why can we approximate the joint probability distribution using the output vector of an LSTM?,"
In the paper, Contextual String Embeddings for Sequence Labeling, the authors state that
\begin{equation}
P(x_{0:T}) = \prod_{t=0}^T P(x_t|x_{0:t-1})
\end{equation}
They also state that, in the LSTM architecture, the conditional probability $P(x_{t}|x_{0:t})$ is approximately a function of the network output $h_t$.
\begin{equation}
P(x_{t}|x_{0:t}) \approx \prod_{t=0}^{T} P(x_t|h_t;\theta)
\end{equation}
Why is this equation true?
","['machine-learning', 'natural-language-processing', 'long-short-term-memory', 'probability-distribution', 'sequence-modeling']",
How to create a custom environment for reinforcement learning,"
I am a newbie in reinforcement learning working on a college project. The project is related to optimizing the hardware power. I am running proprietary software in Linux distribution (16.04). The goal is to use reinforcement learning and optimize the power of the System (keeping the performance degradation of the software as minimum as possible).
For this, I need to create a custom environment for my reinforcement learning. From reading different materials, I could understand that I need to make my software as a custom environment from where I can retrieve the state features. Action space may include instructions to Linux to change the power (I can use some predefined set of power options).
The proprietary software is a cellular network and the state variables include latency or throughput. To control the power action space, rapl-tools can be used to control CPU power.
I just started working on this project and everything seems blurry. What is the best way to make this work? Is there some tutorials or materials that would help me make things clear. Is my understanding of creating a custom environment for reinforcement learning true?
","['reinforcement-learning', 'environment']","
This answer assumes that your ""proprietary software"" is a simulation of, or controller for a real environment.

Yes you will very likely need to write software to represent your environment in some standard way as a Reinforcement Learning (RL) environment. Depending on details, this may be trivially easy or it might be quite involved.
An environment in RL must have the following traits in general, in order to interface with RL agent software:

A state representation. This will typically be an object or array of data that matches sensor readings from the real environment. It is important to RL that the state has the Markov property so that predictions of value can be accurate. For some environments that will mean calculating derived values from observations, or representing a combined history of last few observations from sensors as the state.

The state can either be held inside an internal representation of the environment, which is a typical object-oriented approach, or it can be passed around as a parameter to other functions.
A simple state might just be a fixed size array of numbers representing important traits of the environment, scaled between -1 and 1 for convenience when using it with neural networks.

An action representation.

A simple action representation could just be an integer which identifies which of N actions has been chosen, starting from 0. This allows for a basic index lookup when checking value function estimates.

A reward function. This is part of a problem definition, and you may want to have that code as part of the environment or part of the agent or somewhere in-between depending on how likely it is to change - e.g. if you want to run multiple experiments that optimise different aspects of control but in the same environment, you may make a totally separate reward calculation module that you combine at a high level with the agent and environment code.

Reward function design is a complex subject. However, the basics are that this should always be a single real valued number (i.e. a floating point number in most programming languages).

A time step function. This should take an action choice, and should update the state for a time step - returning the next state, and the immediate reward. If the environment is real, then the code will make actual changes (e.g. move robot arm), potentially wait for the time step to elapse, then read sensors to get the next state and calculate reward. If the environment is simulated, then the code should call some internal model to calculate the next state. This function should call the proprietary software you have been provided for your task.

If actions available depend on the current state, then code for that could live in the environment simulation or the agent, or be some helper function that the agent can call, so it can filter the actions before choosing one.
If you are working in Python, to help make this more concrete, and follow an existing design, see ""How to create a new gym environment in OpenAI?"" on Stack Overflow. The Open AI environments all follow the same conventions for environment definitions, which helps when writing agents to solve them. I also recommend finding an Open AI Gym environment that seems similar to your problem, seeing how that works and trying to train an agent to solve it.
There may still be work to match your environment to an agent that can solve it. That depends on what agent software you are using.
Even if you write your own agent software, it helps to separate out the environment code like this. The environment is the problem to solve, and the RL agent is one way to search for a solution to it. Keeping those parts separate is a useful design that will allow you to try different types of agent and compare their performance for example.
"
How can auto-encoders compute the reconstruction error for the new data?,"
Autoencoders are used for unsupervised anomaly detection by first learning the features of the data set with mainly ""normal"" data points. Then new data can be considered anomalous if the new data has a large reconstruction error, i.e. it was hard to fit the features as in the normal data.
Even if the training is supervised by learning to reconstruct the same data, how is the reconstruction error computed for the new data?
","['unsupervised-learning', 'autoencoders', 'anomaly-detection']","
It is computed just like in training. You take an MSE or something along these lines between the input and the output. You set a threshold for it. If new data's reconstruction error is higher than your threshold, then it is anomalous otherwise it isn't.
"
Object size identification and maximum number of classes with convolutional neural networks,"
I am working on a project that involves using a ConvNet to identify screws. I am able to train from scratch a ConvNet based on the first version of the inception network, but shallower (only 3 inception modules), and at the moment classifying only 45 different screws (the goal is to cover a significant part of a catalog containing ~ 4000 different itens).
My training set consists of rectangular grayscale images of the screws (150 x 300 pixels), approx. 700 images for each class. 
The prototype of this model has been working pretty well with 45 classes (test set accuracy ~98%), but I am starting to worry about two things:
1) Many screws in the catalog have similar shapes, but different sizes, so the production model will have to be able to infer the scale of the objects. This is important because future users will image screws with different smartphone cameras, yielding different screw sizes in the images fed to the ConvNet. I haven't been able to find much about this in the literature. And from what I have read about ConvNets, they are good at detecting shapes, which mean that two objects, the first 1 meter long and the other 1 centimeter long, would be considered ""equal"" by a ConvNet if they appeared similar in an image. One (not very elegant) solution I imagined would be to include a scale in the training images, by means either of a ruler or a common object (a coin, for example). Anyway, I wonder if this problem has a simple solution, since I believe many people might have faced it.
2) All of the notorious ConvNets I know of are trained with the ImageNet dataset, which comprises 1000 different classes. My screw dataset will ultimately have more than that. Is that an issue? Assuming I have the hardware resources to train very large fully connected layers and softmax output layers, is there an upper bound to the number of classes a ConvNet can identify?
","['convolutional-neural-networks', 'computer-vision', 'object-recognition']",
How do we get the true value in the prediction objective in reinforcement learning?,"
In the book ""Reinforcement Learning: An Introduction"" (2018) Sutton and Barto define the prediction objective ($\overline{VE}$) as follows (page 199):
$$\overline{VE}\doteq\sum_{s\epsilon S} \mu(s)[v_{\pi}(s)-\hat{v}(s,w)]^2$$
Where $v_{\pi}(s)$ is the true value of $s$ and $\hat{v}(s,w)$ is the approximation of it. Furthermore it is stated that this is ""often used in plots"".
How do I get the true value $v_{\pi}(s)$? And if there is a way to obtain the value, why would I need to approximate it?
","['reinforcement-learning', 'objective-functions', 'sutton-barto']","
I may be wrong about this but my best interpretation without having access to the book is that
How do I get the true value v(s)? 
The True value I think is whatever the most correct answer is for the prediction. This should be training data. Some companies like facebook spend a lot of money to hire people to create hand-detailed data to fill in this value.
And if there is a way to obtain the value, why would I need to approximate it?
You are approximating it to test the accuracy of your model - your prediction. It seems to me this equation is only necessary when training the model.
The result of this is your total error between all your predictions. The lower the value, the better your model.
https://en.wikipedia.org/wiki/Mean_squared_error
"
Training a reinforcement learning model with multiple images,"
I am tentatively trying to train a deep reinforcement learning model the maze escaping task, and each time it takes one image as the input (e.g., a different ""maze"").
Suppose I have about $10K$ different maze images, and the ideal case is that after training $N$ mazes, my model would do a good job to quickly solve the puzzle in the rest $10K$ - $N$ images.   
I am writing to inquire some good idea/empirical evidences on how to select a good $N$ for the training task.
And in general, how should I estimate and enhance the ability of ""transfer learning"" of my reinforcement model? Make it more generalized? 
Any advice or suggestions would be appreciate it very much. Thanks.
","['reinforcement-learning', 'convolutional-neural-networks', 'deep-rl', 'transfer-learning']",
How to define cost function for custom nonlinear functions?,"
For logistic regression, the Cost function is defined as:
\begin{equation}
Cost(h_{\theta}(x)-y) = -ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))
\end{equation}
I now have a nonlinear function
\begin{equation}
 h_{\theta}^{(i)}(x)=xe^{-j\theta_i|x|^2}
\end{equation}
where $i$ denotes the $i$th training sample. How should I define cost function for this particular nonlinear function? 
","['deep-learning', 'deep-neural-networks', 'logistic-regression']",
Is it possible for a neural network to be used to compress data?,"
When training a neural network, we often run into the issue of overfitting.
However, is it possible to put overfitting to use? Basically, my idea is, instead of storing a large dataset in a database, you can just train a neural network on the entire dataset until it overfits as much as possible, then retrieve data ""stored"" in the neural network like it's a hashing function.
","['neural-networks', 'autoencoders', 'overfitting', 'data-compression']","
The auto-encoder (AE) can be used to learn a compressed representation (a vectorised hash value) of each observation in the training dataset, $z$, which can then be used to later retrieve the original (or similar) observation.
The variational auto-encoder (VAE), a statistical variation of AE, can also be used to generate objects similar to the observations (or inputs) in the training set.
There are other data compressor models, for example, Helmholtz machine, which precedes both the AE and VAE.
"
Application of Blockchain in Fraud detection in stock market,"
I want to develop a fraud detection application in the stock market Using Blockchain technology, we have some pattern that defines the anomaly for use of supervised machine learning but there is one question remain:
What is role of machine learning and Blockchain to detect anomaly like a fraud?
","['machine-learning', 'ai-design', 'anomaly-detection', 'blockchain']",
One end to end Neural network or many task-specific ones?,"
Is it better to train one neural network for a dispersed labeled data with large number of classes or first classify data by unsupervised learning then train each part by a separate NN?
I mean by unsupervised learning we help each NN to classify in lower dispersed data with lower number of labels.
So for test data  the class of data is found by unsupervised learning then the final label is found by the network associated with that class.
Does this question generally have an answer or it depends on data and needs to be answered in practice?
","['neural-networks', 'machine-learning', 'deep-learning', 'deep-neural-networks']",
Write Constraint Satisfaction Formulation for problem,"

Given $F_1,F_2,..,F_n$ as set of final exams of subjects taken by students $S_1,..,S_k$ in h slots such that no student takes two exams in a single slot.Here the objective is to maximize the number of exams taken by a student in a single slot.  

I reduced the problem to graph coloring problem where the nodes of the graph are the final exams of subjects and edges would be the students.  The chromatic number of graph would be h.
But I am not sure how to represent edges as they can me multiple also as same exams are taken by multiple students and would it have any impact on constraint satisfaction formulation?
","['graphs', 'graph-theory', 'constraint-satisfaction-problems', 'satisfiability']",
What is machine learning?,"
What is the definition of machine learning? What are the advantages of machine learning? 
","['machine-learning', 'terminology', 'definitions']","
Here's a definition by Tom Mitchel (1997): 

Computer program is said to learn from experience E with respect to
  some task T and some performance measure P, if its performance on T,
  as measured by P, improves with experience E.

So, the programmer gives some instructions/rules to the computer, so that it can learn how to solve the problem from the given examples by themselves. 
In some tasks, the computer can perform better than humans. For example, the Dota 2 bot (made by OpenAI) defeated the world champion. 
With machine learning, many cases can be automated. They also have the ability to improve solutions by learning the given data from time to time. It can process and analyze large data well.
Machine learning already applied in many fields such as machine translation in google translate and face recognition that is widely used by today's society for security.
"
beautify an image with reinforcement learning,"
I am trying to formulate and solve the following problem of image mutation. Suppose I am trying to insert an object image into a ""background"" image of several objects, and I will need to look for a ""sweet spot"" to insert the image:

I am tentatively trying to formulate the problem into a reinforcement learning process, with the following elements:
0. initial stage:

a background image where the location of objects within the image has been marked (let's suppose we have a perfect object detector)
another image of a new object, let's say, a human

1. action space:

location (x, y) for the object image to be inserted; in that sense the action space is quite large.

2. environment:

each step I will have a new image to ""learn from"".
An oracle function F returns 1 or 0 (roughly one computation of F takes 30 seconds). 
 This function tells me the latest synthesized image hits the ""sweet spot"" or not (1 means hit). If so, I will stop the search and return the image.

3. constraint:
the newly inserted object shouldn't overlap with the original objects in the figure.  
While my gut feeling is that this problem is somehow similar to the classic ""maze escape"" problem which can be solved well with reinforcement learning, the action space seems quite large in this problem.
So here are my questions:

In case I would like to formulate this ""beautify"" image problem into a ""deep"" reinforcement learning problem, how can I learn from such large action space? Or is it really suitable for a reinforcement learning process? 
Can I somehow subsume the ""non-overlapping"" constraint into the oracle function F? If so, how should I decide the reward score? Any principled or empirical way of deciding so? 

","['reinforcement-learning', 'deep-rl', 'image-generation']",
OpenAI Gym interface when reward calculation is delayed? (continuous control with considerable reaction time),"
I'm about to create an OpenAI Gym environment for a flight simulator. I'm wondering, how to cope with the fact, that the result and reward for some action needs a considerable time to advance through the system due to the inherent time constants.
In the easy example Gym-environments (e. g. cartpole, or some games) the step can anstantly be executed and the resulting reard can be calculated.
In my continuous control system (aka flight simulator), There is some reaction time needed, until I can calculate the result from my action. E. g. When I pull the stick, it takes some time, until the aircraft lifts it's nose. So there is a considerable delay (maybe in the seconds ballpark) from commanding the action to the environment, and the earliest observation of that result and it's corresponding reward.
How can I cope with that. As far as I understood, the env.step(action) function blocks till it comes back with a new observation and a corresponding reward.

How can I cope with long lasting reward caclulations?
Is it possible to have overlapping actions somehow? E. g. command a new action every 100ms, but get the reward for that action only 1 second later. In this case there would be always 10 rewards pending.

I hope I made my point clear. Don't hesitate to ask for further details in the comments.
Any hints are welcome. Is there anything to read out in the wild dealing with a similar issue?
Cheers,
Felix
","['reinforcement-learning', 'open-ai', 'gym']","
It's not on your end, as a creator of flight simulator, to worry about what action should get the credit for the reward that happened some time after the action was taken. You should return the reward when the actual event happens not when the action that caused it happened. It's the job of the reinforcement learning agent to figure that out. For example if you want to give a reward when airplane nose is at 45 degrees from the horizontal axis, you should return the reward when that event actually happens, RL agent should figure out that the crucial action happened some time ago. This may be difficult for the agent but its up to the user to use a proper algorithm and proper exploration strategy to solve the problem.
"
How to add some data input in a CNN?,"
There is this problem I have encountered, I was trying to classify the pixels from input image into classes, sort of like segmentation, using a encoder-decoder CNN. The interested pixels usually locate in the top right corner of the input image, but the input images are too big, which I have to slice them in patches, by doing this, each input patch loses its which region of the whole picture its from information.
I'm using pytorch, I thought of manually add this patch location info into the input, but then it will be convoluted, which does make sense to me since it's not a part of an image.
I'm new to this, not sure if I'm thinking the whole thing right, how should I manually add this info into the input correctly or if there is some keywords I can do some researches, in order to let the CNN taking position into account? Thank you.
","['convolutional-neural-networks', 'autoencoders', 'pytorch', 'image-segmentation']",
What are the applications of ant colony optimization algorithms?,"
I'm interested in ant colony optimization algorithms and bee algorithms,but I'm confused what are the applications of these algorithms
Can you suggest me some examples of applications can I work on?
","['applications', 'swarm-intelligence', 'ant-colony-optimization']",
Reinforcement Learning in Real Life/Practical Terms,"
In every day life, it seems that we all have various habits and actions that we perform. For example, we wake up and check our email/facebook etc. on our phones. We don't look at are current state right now, and consider the values of all the possible future trajectories. We basically choose the action that maximizes our ""reward"" at our current state. 

Question. Is it practical to randomly initialize actions $a \in A$, states $s \in S$ and a policy $\pi(a|s)$ and update this according to some algorithm (e.g. REINFORCE, exploration, etc.) to achieve some desired
  goal in your life? This could be done, for example, by uniformly sampling a random number in the interval $(0,1)$ and acting according to a policy $\pi(a|s)$. For example, suppose your goal is to get married,
  get a new house etc. What would be appropriate return functions in
  this case? Return is usually defined as the immediate reward plus the
  discounted cumulative future rewards. But is this the right definition for
  practical problems like marriage/dating, buying a house, etc.? Would you define return as $R_{\text{total}} = R_{\text{marriage}}+R_{\text{buying house}}$ in our example, where each of those individual returns are typical immediate plus discounted rewards and try to maximize that? Or would it be better to maximize $R_{\text{marriage}}$ and $R_{\text{buying house}}$ individually?

",['reinforcement-learning'],
How to know when a Environment will yield a deterministic model,"
Given enough experiment data on time taken for objects to fall to earth from different heights, one can create various models that will accurately predict the time it will take for an object falling at any height (in the inner the atmosphere, this is a toy example).
In this simple example the model is deterministic, it will always produce an output given an input regardless of the amount of data over a small thresholdsomething akin to Newtons gravity equation. 
Try modelling something like the stock-market (the other extreme end of the scale) and the predictions will never reliably converge on a predictable accurate model.
Is there any way of knowing whether your domain will yield a deterministic or non-deterministic model or not?
","['models', 'convergence']",
"Implementation of PPO - Value Loss not converging, return plateauing","
Copy from my Reddit post:
(Sorry if this does not fit here, please tell me and I delete it)
Help regarding
I'm working on an implementation of PPO, which I plan to use in my (Bachelors) Thesis. To test whether my implementation works, I want to use the LunarLanderContinuous-v2 Environment. Now my implementation seems to work just fine, but plateaus much too early - At an average reward of ~ -1.8 reward per timestep, where the goal should be somewhere around ~ +2,5 reward per timestep. As the implementation generally learns I am somewhat confused, as to why it then plateaus so early.
Some details regarding my implementation, also here is the GitHub repo:

I use parallelized environments via openAi's subproc_vecenv

I use the Actor Critic Version of PPO

I use Generalized Advantage Estimation as my Advantage term

I only use finished runs (every run used in training has reached a terminal state)

Even though Critic loss in the graphic below looks small it is actually rather large, as the rewards are normalized and therefore the value targets are actually rather small

The Critic seemingly predicts a value independent of the state it is fed - that is it predicts for every state just the average over all the values. That seems like harsh underfitting, which is weird as the network is already rather large for the problem in my opinion. But this seems to be the most likely cause for the problem in my opinion.
Edit1: Added image


","['reinforcement-learning', 'proximal-policy-optimization', 'advantage-actor-critic']",
A2C Critic Loss Interpretation,"
I'm working on an Advantage A2C implementation, and I just finished creating the value network $\hat{V_{\phi}}$. I train this network with the standard MSE loss of discounted rewards-to-go:$$\|\hat{V_{\phi}}(s_{t'}) - \sum_{t=t'}^{T}\gamma^{t-t'} r(s_t, a_t)\|^2$$
I would like to be able to evaluate and assess the performance and ability of the value network as I train, especially to see how that relates and interacts with the changes and improvements in the policy, however I'm not sure how to do this. 
My first instinct was to track the loss after each batch of experiences, but this doesn't work. As the policy improves, episodes last longer, and the value loss increases. I understand why this happens, as it is harder to predict the rewards-to-go when the length of the future is more undetermined. 
To fix this, I tried dividing the loss that I'm computing by the total number of steps in the batch of episodes. However, the loss is still increasing as the policy improves (as the episodes get longer). Why is this still happening? Then, is there anything I can do to get a better assessment of the quality of the value network?
","['neural-networks', 'deep-learning', 'reinforcement-learning', 'objective-functions', 'actor-critic-methods']","
Unfortunately no, the way to go is track the total reward and see if it's increasing and converging eventually. Value loss isn't a useful metric as the loss can be 0 when the value network always predicts 0 and the agent doesn't collect any reward, meaning very poor behavior.
"
Binary classification for a series of data (using Keras) to tell if it is a straight line or not a straight line,"
I am new to machine learning and I would like to seek some advice/help for directions on implementing a binary classification for a series of data and tell if it is a straight line or a not? 
for example I have the following data e.g.
Training data:

0,2,4  -- straight line 
0,10,5  -- not a straight line
0,99,99  -- not a straight line
0,1,2  -- straight line

My Validation data would be:

0,60,120 -- straight line
0,120,60 -- not a straight line

","['machine-learning', 'keras']",
Bert super easy implementation,"
I myself am not new to NLP, but for some reason I am unable to grasp purity of BERT. I have seen a ton of blogs, github repos, but none could clarify BERT usage to me.
It would be helpful if you could provide two things :

A clear implementation of BERT, preferably in ipython notebook.
Some papers on BERT excluding the original paper by google.

","['natural-language-processing', 'python']",
Being confused of distribution notations in Deep Learning book,"
In chapter 5 of Deep Learning book of Ian Goodfellow, some notations in the 
loss function as below make me really confused. 
I tried to understand $x,y \sim p_{data}$ means a sample $(x, y)$ sampled from original dataset distribution (or $y$ is the ground truth label).
The loss function in formula 5.101 seems to be correct for my understanding. Actually, the formula 5.101 is derived from 5.100 by adding the regularization. 
Therefore, the notation $x,y \sim \hat{p}_{data}$ in formula 5.96 and 5.100 is really confusing to me whether the loss function is defined correctly (kinda typo error or not). If not so, could you help me to refactor the meaning of two notations, are they similar and correct?



Many thanks for your help.
","['machine-learning', 'deep-learning', 'notation']",
How do you go from the last convolutional layer to your first fully connected layer?,"
I'm implementing a neural network framework from scratch in C++ as a learning exercise. There is one concept I don't see explained anywhere clearly:
How do you go from your last convolutional or pooling layer, which is 3 dimensional, to your first fully connected layer in the network?
Many sources say, that you should flatten the data. Does this mean that you should just simply create a $1D$ vector with a size of $N*M*D$ ($N*M$ is the last convolution layer's size, and $D$ is the number of activation maps in that layer) and put the numbers in it one by one in some arbitrary order?
If this is the case, I understand how to propagate further down the line, but how does backprogation work here? Just put the values in reverse order into the activation maps?
I also read that you can do this ""flattening"" as a tensor contraction. How does that work exactly?
","['neural-networks', 'convolutional-neural-networks', 'convolutional-layers', 'dense-layers', 'c++']",
Is neural networks training done one-by-one? [duplicate],"







This question already has an answer here:
                                
                            




Is back-propagation applied for each data point or for a batch of data points?

                                (1 answer)
                            

Closed 3 years ago.



I'm trying to learn neural networks by watching this series of videos and implementing a simple neural network in Python.
Here's one of the things I'm wondering about: I'm training the neural network on sample data, and I've got 1,000 samples. The training consists of gradually changing the weights and biases to make the cost function result in a smaller cost.
My question: Should I be changing the weights/biases on every single sample before moving on to the next sample, or should I first calculate the desired changes for the entire lot of 1,000 samples, and only then start applying them to the network?
","['neural-networks', 'deep-learning', 'gradient-descent', 'stochastic-gradient-descent', 'mini-batch-gradient-descent']","
Ideally, you need to update weights by going over all the samples in the dataset. This is called as Batch Gradient Descent. But, as the no. of training examples increases, the computation becomes huge and training will be very slow. With the advent of deep learning, training size is in millions and computation using all training examples is very impractical and very slow. 
This is where, two optimization techniques became prominent. 

Mini-Batch Gradient Descent
Stochastic Gradient Descent (SGD)

In mini-batch gradient descent, you use a batch size that is considerably less than total no. of training examples and update your weights after passing through these examples. 
In stochastic gradient descent, you update the weights after passing through each training example. 
Coming to advantages and disadvantages of the three methods we discussed.

Batch gradient descent gradually converges to the global minimum but
it is slow and requires huge computing power.
Stochastic gradient descent converges fast but not to the global
minimum, it     converges somewhere near to the global minimum and
hovers    around that point, but doesn't converge ever to the global
minimum.    But, the converged point in Stochastic gradient descent
is good    enough for all practical purposes.
Mini-Batch gradient is a trade-off the above two methods. But, if you
have a vectorized implementation of the weights updation and you
are training with a multi-core setup or submitting the training to
multiple machines, this is the best method both in terms of time for 
training and convergence to global minimum.

You can plot the cost function, w.r.t the no. of iterations to understand the difference between convergence in all the 3 types of gradient descent. 

Batch gradient descent plot falls smoothly and slowly and gets stabilized and
gets to global minimum. 
Stochastic gradient descent plot will have    oscillations, will fall
fast but hovers around global minimum.

These are some blogs where there is detailed explanation of advantages, disadvantages of each method and also graphs of how cost function changes for all the three methods with iterations. 
https://adventuresinmachinelearning.com/stochastic-gradient-descent/
https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/
"
How long it takes to train face recognition deep neural network? (rough estimation),"
If I use a desktop PC with a GPU, how long it might take to train face recognition deep neural network on let's say dataset of 2.6 million images and 2600 identities? I guess it should depend on various properties (e.g., type of the DNN). But I am just looking for a rough estimation. Is it a matter of hours/days or years?
Thanks!
","['deep-learning', 'deep-neural-networks', 'hardware', 'facial-recognition']","
YELP Dataset (200k images) used to take 5 hr for training to identify Five (5) classes on GPU - Nvidia 1080 Ti with 11 GB RAM. So I guess in your case it will take days. Again it will depend on the type of your GPU configuration and type of Architecture you will be using.  
"
Are filters fixed or learned?,"
No matter what I google or what paper I read, I can't find an answer to my question. In a deep convolutional neural network, let's say AlexNet (Krizhevsky, 2012), filters' weights are learned by means of back-prop.
But how are kernels themselves selected? I know kernels had been used in image processing long before CNNs, hence I'd imagine there would be a set of filters based on kernels (see, for example, this article) that are proven to be effective for edge detection and the likes.
Reading around the web, I also found something about ""randomly generated kernels"". Does anyone know if and when this practice is adopted?
","['convolutional-neural-networks', 'backpropagation', 'gradient-descent', 'filters']","
If you're looking for filters with known effect, the Gaussian filters do smoothing, the Gabor filters are useful for edge detection, etc.
Usually, in deep learning models where things are trained from scratch, the filters are randomly initialized and then learned by the model's training scheme. For the most part, without using any of the well-known kernels mentioned above.
Clarification on for the most part: so filters aren't initialized with the goal of knowing exactly which feature it will activate on, but what will assist the training procedure. Recently, people have even trained ResNets with or without batch normalization by finding good initial points -- it's an ongoing field of research.
"
Any good resources for learning programming GPU level operations? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I want to be able to improve my lower level device specific programming abilities to assist in future endeavors. Examples would be learning to write custom tensorflow operations in C++ optimized to work on GPUS. Does anyone know good resources to find tutorials showing which packages to use for which devices and etc.?  
An approach other than just reading source and replicating until understanding would be nice. 
","['c++', 'gpu']",
A* is similar to Dijkstra with reduced cost,"
According to this Wikipedia article

If the heuristic $h$ satisfies the additional condition $h(x) \leq d(x, y) + h(y)$ for every edge $(x, y)$ of the graph (where $d$ denotes the length of that edge), then $h$ is called monotone, or consistent. In such a case, $A^*$ can be implemented more efficiently  roughly speaking, no node needs to be processed more than once (see closed set below)  and $A^*$ is equivalent to running Dijkstra's algorithm with the reduced cost $d'(x, y) = d(x, y) + h(y)  h(x).$

Can someone intuitively explain why the reduced cost is of this form ?
","['comparison', 'search', 'heuristics', 'a-star', 'dijkstras-algorithm']","
What you are doing when calculating $d'(x,y)$:

$d(x,y)$: calculating the original edge distance from $x$ to $y$
$h(y)$: plus the heuristic from $y$ to the goal
$h(x)$: minus the heuristic from $x$ to the goal

So, using this recalculation of the original edge-values ($1.$) in Dijkstra's algorithm you are inherently accounting for the heuristic component of A* by incorporating it ($2.$) into the value of the edge traversed, and discarding ($3.$) the accumulated previous heuristic values of previous nodes in the path.
The additional condition $h(x)  d(x, y) + h(y)$ ensures the new edge-values are strictly positive.
"
Multi Agent Sokoban Search Solvers state of the art,"
I also asked this question here but I'm repeating it on this SE because I feel it is more relevant. No intention to spam.
I am researching into coding a solver for a variant of the Sokoban game with multiple agents, restrictions (eg. colors of stones, goals) and relaxations (push AND pull possible, etc.)
By researching online I have found classic papers in the field, like the Rolling Stone paper (by Andreas Junghanns and Jonathan Schaeffer) and Sokoban: Enhancing general single-agent search methods using domain knowledge from the same authors.
These solutions seem to be outdated and I am currently structuring my solver per the notes of the two most performant solvers: YASS and Sokolution.
From the research, I've done these two seem to be my best bet.
It is apparent that they are not enough by themselves to solve a multi-agent environment. Those solvers are made for a single agent. So far, I have failed to find useful multi-agent proposals.
In this context, my question is:

What can be considered state-of-the-art in order to:


coordinate multiple agents with different goals, and
plug a solver's solution in and validate/edit it?

What are some search terms I can use to research this further?

Thank you very much
","['research', 'intelligent-agent', 'multi-agent-systems']",
Why is it called Latent Vector?,"
I just learned about GAN and I'm a little bit confused about the naming of Latent Vector.

First, In my understanding, a definition of a latent variable is a random variable that can't be measured directly (we needs some calculation from other variables to get its value). For example, knowledge is a latent variable. Is it correct?
And then, in GAN, a latent vector $z$ is a random variable which is an input of the generator network. I read in some tutorials, it's generated using only a simple random function:
z = np.random.uniform(-1, 1, size=(batch_size, z_size))


then how are the two things related? why don't we use the term ""a vector with random values between -1 and 1"" when referring $z$ (generator's input) in GAN?
","['terminology', 'generative-adversarial-networks']","
Latent is a synonym for hidden. 
Why is it called a hidden (or latent) variable? For example, suppose that you observe the behaviour of a person or animal. You can only observe the behaviour. You cannot observe the internal state (e.g. the mood) of this person or animal. The mood is a hidden variable because it cannot be observed directly (but only indirectly through its consequences).
A good example of statistical model that is highly based on the notion of latent variables is the hidden Markov model (HMM). If you understand the HMM, you will understand the concept of a hidden variable.
"
What is the advantage of using a VAE over a deterministic auto-encoder?,"
What is the advantage of using a VAE over a deterministic auto-encoder?
For example, assuming we have just 2 labels, a deterministic auto-encoder will always map a given image to the same latent vector. However, one expects that after the training, the 2 classes will form separate clusters in the encoder space.
In the case of the VAE, an image is mapped to an encoding vector probabilistically. However, one still ends up with 2 separate clusters. Now, if one passes a new image (at the test time), in both cases the network should be able to place that new image in one of the 2 clusters.
How are these 2 clusters created using the VAE better than the ones from the deterministic case?
","['neural-networks', 'applications', 'autoencoders', 'variational-autoencoder']","
It seems that you think that we want to perform classification with VAEs or that images that we pass to the encoder fall into more than one category. The other answer already points out that VAEs are not typically used for classification but for generation tasks, so let me try to answer the main question.
The variational auto-encoder (VAE) and the (deterministic) auto-encoder both have an encoder and a decoder and they both convert the inputs to a latent representation, but their inner workings are different: a VAE is a generative statistical model, while the AE can be viewed just as a data compressor (and decompressor).
In an AE, given an input $\mathbf{x}$ (e.g. an image), the encoder produces one latent vector $\mathbf{z_x}$, which can be decoded into $\mathbf{\hat{x}}$ (another image which should be similar or related to $\mathbf{x}$). Compactly, this can be presented as $\mathbf{\hat{x}}=f(\mathbf{z_x}=g(\mathbf{x}))$, where $g$ is the encoder and $f$ is the decoder. This operation is deterministic: so, given the same $\mathbf{x}$, the same $\mathbf{z_x}$ and $\mathbf{\hat{x}}$ are produced.
In a VAE, given an input $\mathbf{x} \in X$ (e.g. an image), more than one latent vector, $\mathbf{z_{x}}^i \in Z$, can be produced, because the encoder attempts to learn the probability distribution $q_\phi(z \mid x)$, which can be e.g. $\mathcal{N}(\mu, \sigma)$, which we can sample from, where $\mu, \sigma = g_\theta(\mathbf{x})$. In practice, $g_\theta$ is a neural network with weights $\phi$. We can sample latent vectors $\mathbf{z_{x}}^i$ from $\mathcal{N}(\mu, \sigma)$, which should be ""good"" representations of a given $\mathbf{x}$.
Why is it useful to learn $q_\phi(z \mid x)$? There are many uses cases. For example, given multiple corrupted/noisy versions of an image, you can reconstruct the original uncorrupted image. However, note that you can use the AE also for denoising. Here you have a TensorFlow example that illustrates this. The difference is is that, again, given the same noisy image, the model will always produce the same reconstructed image. You can also use the VAE for drug design [1]. See also this post.
"
Can the decoder in a transformer model be parallelized like the encoder?,"
Can the decoder in a transformer model be parallelized like the encoder?
As far as I understand, the encoder has all the tokens in the sequence to compute the self-attention scores. But for a decoder, this is not possible (in both training and testing), as self-attention is calculated based on previous timestep outputs. Even if we consider some techniques, like teacher forcing, where we are concatenating expected output with obtained, this still has a sequential input from the previous timestep. 
In this case, apart from the improvement in capturing long-term dependencies, is using a transformer-decoder better than say an LSTM, when comparing purely on the basis of parallelization?
","['deep-learning', 'comparison', 'long-short-term-memory', 'sequence-modeling', 'transformer']","
Can the decoder in a transformer model be parallelized like the encoder?
The correct answer is: computation in a Transformer decoder can be parallelized during training, but not during actual translation (or, in a wider sense, generating output sequences for new input sequences during a testing phase).
What exactly is parallelized?
Also, it's worth mentioning that ""parallelization"" in this case means to compute encoder or decoder states in paralllel for all positions of the input sequence. Parallelization over several layers is not possible: the first layer of a multi-layer encoder or decoder still needs to finish computing all positions in parallel before the second layer can start computing.
Why can the decoder be parallelized position-wise during training?
For each position in the input sequence, a Transformer decoder produces a decoder state as an output. (The decoder state is then used to eventually predict a token in the target sequence.)
In order to compute one decoder state for a particular position in the sequence of states, the network consumes as inputs: 1) the entire input sequence and 2) the target words that were generated previously.
During training, the target words generated previously are known, since they are taken from the target side of our parallel training data. This is the reason why computation can be factored over positions.
During inference (also called ""testing"", or ""translation""), the target words previously generated are predicted by the model, and computing decoder states must be performed sequentially for this reason.
Comparison to RNN models
While Transformers can parallelize over input positions during training, an encoder-decoder model based on RNNs cannot parallelize positions. This means that Transformers are generally faster to train, while RNNs are faster for inference.
This observation leads to the nowadays common practice of training Transformer models and then using sequence-level distillation to learn an RNN model that mimicks the trained Transformer, for faster inference.
"
Sails size recognition,"
Is it possible to recognize the height and width of the sails of different kitesurfers and windsurfers taken from public webcams? And show these information on video in real time? Or on screenshots?
","['machine-learning', 'image-recognition', 'object-recognition']",
How should I design the LSTM architecture for multivariate time series forecasting problems?,"
There is plenty of literature describing LSTMs in a lot of detail and how to use them for multi-variate or uni-variate forecasting problems. What I couldn't find though, is any papers or discussions describing time series forecasting where we have correlated forecast data.
An example is best to describe what I mean. Say I wanted to predict number of people at a beach for the next 24 hours and I want to predict this in hourly granularity. This quantity of people would clearly depend on the past quantity of people at the beach as well as the weather. Now I can make an LSTM architecture of some sort to predict these future quantities based upon what happened in the past quite easily. But what if I now have access to weather forecasts for the next 24 hours too? (and historical forecast data too).
The architecture I came up with looks like this:

So I train the left upper branch on forecast data, then train the right upper branch on out-turn data, then freeze their layers to and join them to form the final network in the picture and train that on both forecasts and out-turns. (when I say train, the output is always the forecast for the next 24 hours). This method does in fact have better performance than using forecasts or out-turns alone.
I guess my question is, has anyone seen any literature around on this topic and/ or knows a better way to solve these sort of multivariate time series forecasting problems and is my method okay or completely flawed?
","['deep-learning', 'ai-design', 'long-short-term-memory', 'time-series']","
I have not come across a model like this yet. BUT
If you have not tried smaller models I'd recommend trying that first.
Justification: this lets you use learning curves to diagnose what to do next. 
Also, you might try starting with a GRU (the LSTM overhead may not be needed).
One Idea for a starting model
Observe that turnout does not affect weather. So the weather forecast model will be independent of the turnout model but not vice versa.
(Note: I'm using RNN to denote which ever recurrent architecture you choose)
Formulation for simultaneous prediction:
Weather
$\text{RNN}_w$ is your weather prediction model
$\hat w_t = \text{RNN}_w(\hat w_{t-1})$ be the predicted weather at time $t$
Turnout: The idea here is that people will or will not go to the beach for various reasons (overcrowded, too hot or stormy etc). So in the beach population prediction task we use all these as features to predict the population at the next time-step. This reduces the problem to any one of the classical models already developed. 
$\text{RNN}_p$ is your turnout (beach population) prediction model
$\hat p_t$ be the predicted turnout at time $t$
$\hat c_t=[\hat p_t, \hat w_t]$ is the concatenation at time t
$\hat p_{t+1} = \text{RNN}_p(\hat c_t)$
Formulation in the presence of weather forecast:
Simply replace $\hat w_t$ with the true weather forecasts $\hat f_t$.
A final warning
Unless you think your RNN is better at weather prediction than the forecasting systems I would not recommend using $\text{RNN}_w$ in a production application.
I hope this helps.
"
When should I use Reinforcement Learning vs PID Control?,"
When designing solutions to problems such as the Lunar Lander on OpenAIGym, Reinforcement Learning is a tempting means of giving the agent adequate action control so as to successfully land.  
But what are the instances in which control system algorithms, such as PID controllers, would do just an adequate job as, if not better than, Reinforcement Learning?
Questions such as this one do a great job at addressing the theory of this question, but do little to address the practical component.
As an Artificial Intelligence engineer, what elements of a problem domain should suggest to me that a PID controller is insufficient to solve a problem, and a Reinforcement Learning algorithm should instead be used (or vice versa)?
","['reinforcement-learning', 'ai-design', 'control-theory']",
"How do big companies, like Facebook, model individuals and their interaction?","
As a layman in AI, I want to get an idea of how big data players, like Facebook, model individuals (of which they have so many data).
There are two scenarios I can imagine:

Neural networks build clusters of individuals by pure and ""unconscious"" big data analysis (not knowing, trying to understand and naming the clusters and ""feature neurons"" on intermediate levels of the network) with the only aim to predict some decisions of the members of these clusters with highest possible accuracy.

Letting humans analyze the clusters and neurons (trying to understand what they mean) they give names to them and possibly add human-defined ""fields"" (like ""is an honest person"") if these were not found automatically, and whose values are then calculated from big data.


The second case would result in a specific psychological model of individuals with lots of ""human-understandable"" dimensions.
In case there is such a model, I would be very interested to know as much about it as possible.
What can be said about this:


Is there most probably such a model (that is kept as a secret e.g. by Facebook)?

Has somehow tried to guess how it may look like?

Are there leaked parts of the model?



My aim is to know and understand by which categories Facebook (as an example) classifies its users.
","['neural-networks', 'machine-learning', 'social']","
This article may shed some light on this question:
Facebook Doesnt Tell Users Everything It Really Knows About Them
"
The problem with the Gambler's Problem in RL,"
Recently I simulated the Gambler's Problem in RL:

Now, the problem is, the curve does not at all appear the way as given in the book. The ""best policy"" curve appears a lot more undulating than it is shown based on the following factors:

Sensitivity (i.e. the threshold for which you decide the state values have converged).
Probability of heads (expected).
Depending the value of sensitivity it also depends on whether I find the policy by finding the action (bet) which cause the maximum return by using $>$ or by using $>=$ in the following code i.e:


 initialize maximum = -inf
 best_action = None
 loop over states:
    loop over actions of the state:
       if(action_reward>maximum):
          best_action = action

Also note that if we make the final reward as 101 instead of 100 the curve becomes more uniform. This problem has also been noted in the following thread.
So what is the actual intuitive explanation behind such a behaviour of the solution. Also here is the thread where this problem is discussed.
","['reinforcement-learning', 'probability']","
The intuitive explanation is that there are many equally good ""optimal"" policies. This is mentioned at the end of the example problem description you posted. My gut says that the family of optimal policies would be any policy from the double/nothing family. So, for example, if you bet 25 on the first bet instead of 50, I think your overall chances of winning should be the same as if you bet 50, it'll just take longer in expectation. The resulting family of policies will look more undulating than the one in the book.
As Neil notes, for low values of $p$, the probability that you win a gamble, it is the case that there is a unique optimal policy. 
"
How are the attention weights normalised in the transformer?,"
In the Transformer (adopted in BERT), we normalize the attention weights (dot product of keys and queries) using a softmax in the Scaled Dot-Product mechanism. It is unclear to me whether this normalization is performed on each row of the weight matrix or on the entire matrix. In the TensorFlow tutorial, it is performed on each row (axis=-1), and in the official TensorFlow code, it is performed on the entire matrix (axis=None). The paper doesn't give many details. 
To me, both methods can make sense, but they have a strong impact. If on each row, then each value will have a roughly similar norm, because the sum of its weights is 1. If on the entire matrix, some values might be ""extinguished"" because all of its weights can be very close to zero. 
","['tensorflow', 'machine-translation', 'bert', 'transformer']",
Recognize pattern in dataset,"
I'm currently working on a group project where we need to find a pattern in a given dataset. The dataset is a collection of X, Y, Z values of a gyroscope from someone who is walking. If you plot these values you'll get a result like this.

And this is how our dataset looks like.

We are new to AI and ML so we first did some general research like understanding how matrices work and how to do some basic predictions with frameworks like TensorFlow and PyTorch. Now we want to start on this problem. What we need to do is to find a pattern in the dataset and count how many times this pattern appears in this dataset.
We started of with some none AI functions to count, we managed to do that but the way we counted will probably only work on this specific dataset. So that's why we decided to do this with AI.
We would love to hear as many different approaches as possible to count the steps since we are still learning.
","['machine-learning', 'datasets', 'pattern-recognition', 'supervised-learning']","
For such time-series data that has a significant amount of periodicity, I would recommend converting data to the frequency domain and performing various spectral analysis methods as @firion has already mentioned. For example, you could perform Fourier Analysis and study the individual components and identify patterns there.
Also, it generally not recommended to perform the normal pattern extraction approaches to time-series data as they fail to understand the temporal relationship between subsequent data points.
Hope this helps!
"
How do I encrypt and decrypt my model when I run inference on it?,"
I want my models to be accessible only by my programs. How do I encrypt and decrypt my model when I run inference on it? Is there any existing technology that is widely used?
","['machine-learning', 'models', 'ai-security']","
While @Oliver Mason's comment is correct, and your proposed method won't provide perfect security, you can still protect your models at rest, so that they are stored encrypted in the memory, and your software feed the key at runtime to decrypt it.
On whatever DL inference engine that you have, once it supports loading the model from a buffer (e.g. void*) rather than a file path, you can do the following: read the encrypted model, decrypt it into a buffer, and initialize your neural network model from the decrypted buffer. OpenVino supports loading the model from a buffer.
For encrypt / decrypt the model, any framework such as OpenSSL, or even tiny-AES, can work.
Mentioning again, how to store and use the key is something that should be handled carefully, and a user with sufficient knowledge can read the model and the keys at runtime from the application's memory.
"
Algorithm that creates new images based on other images,"
Are there any open sourced algorithms that can take a couple of images as an input and generate a new, similar image based on that input? Or are there any resources where I can learn to create such an algorithm?
","['neural-networks', 'algorithm', 'image-generation', 'art-aesthetics']","
I'm not an expert on that so you could probably get a better answer.
I'm not sure to understand what you're looking for. Are the couple of images about the same thing? Like pictures of cats and you want to generate a new cat based on these pictures? If that's what you want, you could probably take a look at Generative Adversarial Network (GAN) : Introduction.
A GAN is made up of a Generator and a Discriminator. The goal of the discriminator is to distinguish the real data from the generated data. And the goal of the generator is to improve its generated data to look similar to real data. Then, if there are different cat images in your dataset, the generator will learn to create a new cat based on that dataset.
If what you're looking for is to take different images like a cat and a dog and generate a ""catdog"", you can take a look at Variational AutoEncoder (VAE). For example you can train two different VAE (Encoder/Decoder). One for cats, and one for dogs. Then you take the encoder of dogs and the decoder of cats. That what I saw one day, not sure if it really works.
Correct me if I'm wrong
"
"When using hashing in tile coding, why are memory requirements reduced and there is only a little loss of performance?","
In the book ""Reinforcement Learning: An Introduction"" (2018) Sutton and Barto explain, on page 221, a form of tile coding using hashing, to reduce memory consumption.
I have two questions about that:

How can this approach reduce memory consumption? Doesn't it just depend on the number of tiles (you have to store one weight for each tile)?

They state that there is only a ""little loss of performance"". In my understanding, the sense of tile coding (and coarse coding) is, that near-by states have many tiles in common and far-away states have only few tilings in common. With tilings ""randomly spread throughout the state space"" this isn't the case. How does this not influence performance?


","['reinforcement-learning', 'function-approximation', 'sutton-barto', 'computational-complexity', 'tile-coding']",
How to reduce amount of species in NEAT?,"
I am using the following library:
https://github.com/vishnugh/evo-NEAT
which seems to be a pretty simple NEAT-implementation. 
Therefore I am using the following Config:
package com.evo.NEAT.com.evo.NEAT.config;

/**
 * Created by vishnughosh on 01/03/17.
 */
public class NEAT_Config {

    public static final int INPUTS = 11;
    public static final int OUTPUTS = 2;
    public static final int HIDDEN_NODES = 100;
    public static final int POPULATION =300;

    public static final float COMPATIBILITY_THRESHOLD = Float.MAX_VALUE;
    public static final float EXCESS_COEFFICENT = 1;
    public static final float DISJOINT_COEFFICENT = 1;
    public static final float WEIGHT_COEFFICENT = 5;

    public static final float STALE_SPECIES = 2;


    public static final float STEPS = 0.1f;
    public static final float PERTURB_CHANCE = 0.9f;
    public static final float WEIGHT_CHANCE = 0.5f;
    public static final float WEIGHT_MUTATION_CHANCE = 0.5f;
    public static final float NODE_MUTATION_CHANCE = 0.1f;
    public static final float CONNECTION_MUTATION_CHANCE = 0.1f;
    public static final float BIAS_CONNECTION_MUTATION_CHANCE = 0.1f;
    public static final float DISABLE_MUTATION_CHANCE = 0.1f;
    public static final float ENABLE_MUTATION_CHANCE = 0.2f ;
    public static final float CROSSOVER_CHANCE = 0.1f;

    public static final int STALE_POOL = 10;
}

However, there are way too much species (about 60). I do not know how to reduce this number, given the fact that the COMPATIBILITY_THRESHOLD is already maximized.
So what's my fault?
Note: I am not using: http://nn.cs.utexas.edu/keyword?stanley:ec02 
since this algorithm seems not to work in a changing environment (where fitness can vary hardly)
","['evolutionary-algorithms', 'neat']","
Your species count will increase as the chance of mutation increases. This is because in every generation, so many genes will be mutated that they have little resemblance of each other, and the distance function doesn't factor in historical markings / innovation numbers.
Try lowering the mutation rates.
Below is the distance function from here page 110
$$\delta = \frac{c_1E}{N} + \frac{c_2D}{N} + c_3 \cdot \overline{W}.  $$
If your fitness vary a lot, try ranking the fitnesses in each specie and setting the survival chance based on its rank.
If you mean a large action space by changing environment, you can set the number of output nodes to the total number of actions, and rank each action, best to worst, then pick the best available action for the state. 
"
Action spaces for an RTS game,"
I think reinforcement learning would be a good fit for this problem, but I am not sure of how to deal with a seemingly infinite number of actions. In the beginning of each game (generic RTS game), the player places down units anywhere on the map. Then as the game progresses, the player can move units around by selecting on them and clicking on a valid location on the map. They must take into consideration things like distance and travel time. An AI agent must do the same. 
How would I represent these actions? Its not as simple as selecting up, down, right,...etc. Should the agent just randomly pick locations on the map? 
Are there any papers or implementations I can look at to help me get started?
","['reinforcement-learning', 'algorithm', 'game-ai']",
Can you learn parameters in nonlinear function?,"
In the paper Nonlinear Interference Mitigation via Deep Neural Networks, the the following network is illustrated.
The network structure is 
The network parameters are $\theta = \{W_1^{1},...,W_1^{l-1},W_2^{1},...,W_2^{l-1},W^{l},\alpha_1,...,\alpha_{l-1}\}$, where $W_1$ and $W_2$ are linear matrices and $\rho^{(i)}(x)=xe^{-j\alpha_i|x|^2}$ is element-wise nonlinear function ($i$ is the index of layer). 
Where should I add this $\rho^{(i)}(x)$? Is it possible to learn the parameter $\alpha$? I don't think it is the same idea as activation function since it is positioned in the middle of two linear matrices... Or can it be added as embedding layer?
","['deep-learning', 'deep-neural-networks']","
In general, you can learn any parameter of the network, provided you can find the partial derivative of the loss function with respect to the desired parameter. Given that $\rho$ is assumed to be differentiable (as the authors state in the paper), you can take the partial derivative of the loss function with respect to the parameter $\alpha$.
In this paper, $\rho$ is a non-linear function (that is, a function that is not linear, e.g. the sigmoid function) that applies element-wise to its input. So, if you pass a vector to this $\rho$, you will get a vector of the same shape out of it. The authors do not explicitly call it an ""activation function"", but $\rho$ does an analogous job of an activation function, that is, it introduces non-linearity. Furthermore, in this architecture, $\rho$ is also followed by a matrix. In general, this is not forbidden, even though it is not common. 
In general, in each layer of a neural network, you can have several different learnable parameters or weights. A parameter is learnable if you can differentiate the loss function with respect to it. You can have more than one weight matrix. For example, recurrent neural networks have usually more than one weight matrix associated with each layer: one matrix is associated with the feed-forward connections and the other matrix is associated with the recurrent connections. 
"
What is a simple game for validation of MCTS?,"
What is a simple turn-based game, that can be used to validate a Monte-Carlo Tree Search code and it's parameters?
Before applying it to problems where I do not have a possiblity to validate its moves for correctness, I would like to implement a test case, that makes sure that it behaves as expected, especially when there are some ambiguities between different implementations and papers.
I built a connect-four game in which to MCTS-AI play against each other and an iterated prisoners dilemma implementation, in which a MCTS-AI plays against common strategies like Tit-for-Tat, but I am still not sure if there is a real good interpretation if the MCTS-AI finds the best strategy.
Another alternative would be a tic-tac-toe game, but MCTS will exhaust the whole search space within little steps, so it is hard to tell how the implementation will perform on other problems.
In addition, expanding a full game tree does not tell you if any states before the full expansion are following the best MCTS strategy.

Example:
You can alternate in the expand step of player 1's tree between optimize for player 1 and optimize for player 2, assuming that player 2 will not play the best move for player 1, but the best move for himself. Not doing so would result in an optimistic game tree, that may work in some cases, but probably is not the best choice for many games, while it would be useful for cooperative games.
When the game tree is fully expanded, you can find the best move, even when the order of the expand steps was not optimal, so using a game that can be fully expanded is no good test to validate the in-between steps.

Is there a simple to implement game, that can be used for validation, in which you can reliably check for each move, if the AI did find the expected move?
",['monte-carlo-tree-search'],"
A good choice might be smaller-scale games of Go, like a 9x9 board. This was the original application domain MCTS was designed for, and the original paper by Brugmann from 1993 details parameters that should lead to an agent that can play above beginner level in what is today a minuscule amount of computational time, in a scaled-down 9x9 grid. 
Go is a good choice for a benchmark because most learning algorithms fail at it pretty badly. The fact that MCTS worked here was a major breakthrough at the time, and helped cement it as a technique for game playing. If your algorithm is not working properly, it is therefore unlikely that it can learn to play Go at the level described in Brugmann's paper.
"
How to deal with Neural network input data with different length and type,"
I'm trying to make use of sensor data from VOC, Humidity, Age, Sampling rate and use them as NN input data. Below are the technical questions I'm struggling to find out.
For each training set, I have 2500 data points for VOC and Humidity. However, for the age and sampling rate, I have only one for each. I'm wondering if it would work to just put 5002(=2500(VOC)+ 2500(hum)+1(age) + 1 (sampling rate)) input data points to the layer.
*Note: sampling rate is in the data set because the VOC and HUM data have different data points for each training set. I reduced the data points by sampling from the original data points. However, I know for sure that timespan does matter.
Please help me out! A good reference is also a huge welcome!
","['neural-networks', 'machine-learning']",
Learning utility function for AIS data,"
I am trying to learn utility functions for ships through their AIS data.
I have a lot of data available and plan on focusing on fishing boats.
So far I've researched a lot of IRL algorithms but I'm not sure if I missed a important one that could be applied.

I've found this paper https://journals.sagepub.com/doi/citedby/10.1177/0278364917722396, but I'm not sure if this is really applicable.
I would need to transform the ais data as trajectorys, add material from openseamap and plot this to images of trajectories. Or did I misunderstand the paper completely?
My other found approach would be selecting features as position, distance to other vessels and others. And then try to apply continous max entropy deep inverse reinforcement learning.

Is there another approach that may be easier in your eyes?
","['reinforcement-learning', 'convolutional-neural-networks', 'utility-based-agents']",
"Would it be ethical to use AI to determine a users gender from the content they upload, without them knowing?","
Ive been thinking about this for a few days and cant tell if this would feel morally just to an average user
","['applications', 'ethics', 'social']","
Ethics aside, I think this could potentially create a number of different issues. What if your algorithm guesses wrong and I'm stuck with a UI that's targeting incorrectly? What if my wife and I share an account? What if I'm a (insert orientation here) (insert gender here) who [coaches, supports] a [men's, women's] [volleyball, football] team? People are very diverse, and you could make many mistakes. Would you then provide tools for the user to correct these mistakes? How would you be able to do so without the user being offended?
Instead, I see many fewer issues that might arise from a section like the following:

Let's get to know each other!
If you'd like, we can help tune your profile to match your interests.
You can start by selecting some of your interests from the list below, or try searching for your own.
Fishing Cooking Soccer Video Gaming Social Media
Search for my interest...
Skip this section 

You could continue with other questions that might actually be relevant to your shaping your UI or application, allowing the user to omit/delete details for any or all questions.
Being purely an opt-in experience prevents any issues with the user feeling like your app may be ""talking about them"" behind their back.
In this case, specifically at this point in time, I think it's wise to be transparent about what data your application knows (or thinks it knows) about your users.
"
How is neural architecture search performed?,"
I have come across something that IBM offers called neural architecture search. You feed it a data set and it outputs an initial neural architecture that you can train.
How is neural architecture search (NAS) performed? Do they use heuristics, or is this meta machine learning?
If you have any papers on NAS, I would appreciate if you can provide a link to them.
","['neural-networks', 'reference-request', 'neuroevolution', 'hyperparameter-optimization', 'neural-architecture-search']","
You could say that NAS fits into the domain of Meta Learning or Meta Machine learning.
I've pulled the NAS papers from my notes, this is a collection of papers/lectures that I personally found very interesting. It's sorted in rough chronological descending order, and *** means influential / must read.
Quoc V. Le and Barret Zoph are to good authors on the topic.

The Evolved Transformer
Exploring Randomly Wired Neural Networks for Image Recognition
GRAPH HYPERNETWORKS FOR NEURAL ARCHITECTURE SEARCH
Backprop Evolution
Progressive Neural Architecture Search
*** DARTS: Differentiable Architecture Search
*** Efficient Neural Architecture Search via Parameter Sharing - ENAS
*** Progressive Neural Architecture Search
AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search
Automatic Machine Learning - Prof. Frank Hutter
Google Brain - Neural Architecture Search - Quoc Le
*** Regularized Evolution for Image Classifier Architecture Search
Autostacker: A Compositional Evolutionary Learning System
Generating Neural Networks with Neural Networks
Finding Competitive Network Architectures Within a Day Using UCT
Neuroevolution: A different kind of deep learning
Evolving Deep Neural Networks
Pieter Abbeel: Deep Learning-to-Learn Robotic Control
*** SMASH: One-Shot Model Architecture Search through HyperNetworks

"
"Are the ideas in the paper ""Governance by Glass-Box: Implementing Transparent Moral Bounds for AI Behaviour"" novel?","
In the paper Governance by Glass-Box: Implementing Transparent Moral Bounds for AI Behaviour, the authors seem to be presenting a black box method of testing. Are these ideas really new? Weren't these ideas already proposed in Translating Values into Design Requirements (by Ibo Van de Poel)? Black-box testing had already been proposed much earlier.
","['machine-learning', 'papers']",
What are the value of the pixels of the convolved image?,"
I'm studying convolutional neural networks from the following article https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/.
If we take a grayscale image, the value of the pixel will be between 0 and 255. Now, if we apply a filter to our ""new image"", can we have pixels whose values are not included in this range? In this case, how can we create the convolved image?
","['neural-networks', 'convolutional-neural-networks']","
The convolved image can be considered a feature map, where each new neuron represents some indication (or lack-there-of) of a feature in some receptive field of the original image, so no it does not need to be a valid image in the output.  
If you specifically care for it to be an image as an output, you can do a couple of things:  
1) normalize the produced feature map to some set range that youre working in (0-255 or 0-1)  
2) make the filter a valid probability distribution, and you know the output will stay in the same range as the input (ex: Gaussian filters) 
"
Are there tools to help clean a large dataset so that it only contains faces? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I have a fairly large dataset consisting of different images with and without persons that I want to use for a project.
The problem is that I only want the pictures that contain faces, and it is best if there is only a crop of the face.
I already looked at Facenet and Openface, but I thought that there must be a simpler already trained solution just to sort the dataset so I can get started with my own project.
","['machine-learning', 'image-recognition', 'datasets', 'data-preprocessing']","
I don't know of a tool but you could write a simple script to detect faces and crop it. It's quite simple with the Haar cascade in openCV to detect faces and use inbuilt functions to crop your image based on the size of the detected face.
Hope that helps !
"
Why am I getting the incorrect value of lambda?,"
I am trying to solve for $\lambda$ using temporal-difference learning. More specifically, I am trying to figure out what $\lambda$ I need, such that $\text{TD}(\lambda)=\text{TD}(1)$, after one iteration. But I get the incorrect value of $\lambda$.
Here's my implementation.
from scipy.optimize import fsolve,leastsq
import numpy as np



 class TD_lambda:
        def __init__(self, probToState,valueEstimates,rewards):
            self.probToState = probToState
            self.valueEstimates = valueEstimates
            self.rewards = rewards
            self.td1 = self.get_vs0(1)

        def get_vs0(self,lambda_):
            probToState = self.probToState
            valueEstimates = self.valueEstimates
            rewards = self.rewards
            vs = dict(zip(['vs0','vs1','vs2','vs3','vs4','vs5','vs6'],list(valueEstimates)))

            vs5 = vs['vs5'] + 1*(rewards[6]+1*vs['vs6']-vs['vs5'])
            vs4 = vs['vs4'] + 1*(rewards[5]+lambda_*rewards[6]+lambda_*vs['vs6']+(1-lambda_)*vs['vs5']-vs['vs4'])
            vs3 = vs['vs3'] + 1*(rewards[4]+lambda_*rewards[5]+lambda_**2*rewards[6]+lambda_**2*vs['vs6']+lambda_*(1-lambda_)*vs['vs5']+(1-lambda_)*vs['vs4']-vs['vs3'])
            vs1 = vs['vs1'] + 1*(rewards[2]+lambda_*rewards[4]+lambda_**2*rewards[5]+lambda_**3*rewards[6]+lambda_**3*vs['vs6']+lambda_**2*(1-lambda_)*vs['vs5']+lambda_*(1-lambda_)*vs['vs4']+\
                                (1-lambda_)*vs['vs3']-vs['vs1'])
            vs2 = vs['vs2'] + 1*(rewards[3]+lambda_*rewards[4]+lambda_**2*rewards[5]+lambda_**3*rewards[6]+lambda_**3*vs['vs6']+lambda_**2*(1-lambda_)*vs['vs5']+lambda_*(1-lambda_)*vs['vs4']+\
                                (1-lambda_)*vs['vs3']-vs['vs2'])
            vs0 = vs['vs0'] + probToState*(rewards[0]+lambda_*rewards[2]+lambda_**2*rewards[4]+lambda_**3*rewards[5]+lambda_**4*rewards[6]+lambda_**4*vs['vs6']+lambda_**3*(1-lambda_)*vs['vs5']+\
                                        +lambda_**2*(1-lambda_)*vs['vs4']+lambda_*(1-lambda_)*vs['vs3']+(1-lambda_)*vs['vs1']-vs['vs0']) +\
                    (1-probToState)*(rewards[1]+lambda_*rewards[3]+lambda_**2*rewards[4]+lambda_**3*rewards[5]+lambda_**4*rewards[6]+lambda_**4*vs['vs6']+lambda_**3*(1-lambda_)*vs['vs5']+\
                                        +lambda_**2*(1-lambda_)*vs['vs4']+lambda_*(1-lambda_)*vs['vs3']+(1-lambda_)*vs['vs2']-vs['vs0'])
            return vs0

        def get_lambda(self,x0=np.linspace(0.1,1,10)):
            return fsolve(lambda lambda_:self.get_vs0(lambda_)-self.td1, x0)

The expected output is: $0.20550275877409016$, but I am getting array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

I cannot understand what am I doing incorrectly.
TD = TD_lambda(probToState,valueEstimates,rewards)
TD.get_lambda()
# Output : array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])

I am just using TD($\lambda$) for state 0 after one iteration. I am not required to see where it converges, so I don't update the value estimates.
","['reinforcement-learning', 'python', 'markov-decision-process', 'temporal-difference-methods', 'td-lambda']","
The previous answer from Brale is mostly correct but is missing a large detail to get the precise answer.
Given this is a question from a GT course homework, I only want to leave pointers so those seeking help can understand the required concept.
"
How are filters weights updated for a CNN?,"
I've been trying to learn backpropagation for CNNs. I read several articles like this one and this one. They all say that to compute the gradients for the filters, you just do a convolution with the input volume as input and the error matrix as the kernel. After that, you just subtract the filter weights by the gradients(multiplied by the learning rate). I implemented this process but it's not working.
Here's a simple example that I tried: 
Input volume (randomised)
 1 -1  0
 0  1  0
 0 -1  1

In this case, we want the filter to only pick up the top left 4 elements. So the target output will be: 
 1  0(supposed to be -1, but ReLU is applied) 
 0  1

We know that the desired filter is: 
 1  0
 0  0

But we pretend that we don't know this. 
We first randomise a filter: 
 1 -1
 1  1

The output right now is: 
 3  0 
-2  1

Apply ReLU: 
 3  0
 0  1

Error (target - output): 
-2  0
 0  0

Use error as kernel to compute gradients: 
-2  2
 0 -2

Say the learning rate is 0.5, then the new filter is: 
2 -2
1  2

This is still wrong! It's not improving at all. If this process is repeated, it won't learn the desired filter. So I must have understood the math wrong. So what's the problem here? 
","['deep-learning', 'convolutional-neural-networks', 'backpropagation', 'math']",
Can $\Phi$ measure of Integrated Information Theory serve as reward for reinforcement learning system?,"
Can $\Phi$ measure (computed rigorously or approximately) of Integrated Information Theory serve as reward for self-evolving/learning reinforcement learning system and hence we let this system to become/evolve into conscious system?
","['reinforcement-learning', 'rewards', 'artificial-consciousness', 'integrated-information-theory']",
How do I plot a matrix of ratings?,"
I have a .csv file called ratings.csv with the following structure:
userID, movieID, rating
3,      12,      5
2,      7,       6

The rating scale goes from 0 to 5 stars. I want to be able to plot the sparsity of the matrix like it's done in the following picture: 

As you can see, ratings scale goes from 0 to 5 on the right. It is a very well thought plot. 
I have Matlab, Python, R etc. Could you come up with something and help me? Ive tried hard but I cannot find the way to do it.
","['machine-learning', 'recommender-system', 'data-visualization']","
You're looking for a heatmap. Check out e.g. https://stackoverflow.com/q/33282368/3924118 (if you like Python more than the others). See also this documentation.
"
How much Physical memory does Alpha Zero's Neural Net require?,"
I appreciate that there are many ways to arrange the memory in a NN, and that the numerical representations may be with bytes to floats depending on an implementation.  What is typical amount of memory required for the ""application side"" for the better NN programs such as Alpha Zero or an automated driving AI?  How much does it matter?
","['neural-networks', 'memory']",
What is the proof that the branch and bound algorithm always finds optimal path in a graph?,"
I've been studying Branch and Bound's graph algorithm and I hear it always finds the optimal path because it uses previously found solutions to find others
However, I haven't been able to find a proof of why it finds the optimal path. (In fact, most sites kind of do a bad job generalizing the algorithm itself.) 
What is the proof that this algorithm always finds the optimal path in the case of a graph with 1 or more goal nodes?
","['search', 'proofs', 'graphs', 'branch-and-bound']","
Branch and Bound is similar to an exhaustive search, except it incorporates a method for computing lower bounds on branches. If the lower bound on a given branch is greater than the upper bound on the problem (i.e. the current best solution encountered), that branch can be discarded since it will never produce an optimal solution.
Hence, since you explore all options except those you know will produce values less optimal than your current best value, you are guaranteed to encounter the global optimum.
Note this is a generic algorithm, and you will need to reference a specific implementation if you want proof of why it satisfies these criteria.
"
How to make the RL player a perfect/expert (tic-tac-toe/chess) player?,"
I asked a question related to tic-tac-toe playing in RL. From the answer, it seems to me a lot is dependent on the opponent (rightly so, if we write down the expectation equations).
My questions are (in the context of tic-tac-toe or chess):

How to make the RL player a perfect/expert (tic-tac-toe/chess) player?
As far as TTT is concerned, when playing against a perfect player, an RL will become perfect, provided that the opponent is perfect.
So, will this hold true if the same RL algorithm, with its learned values, are used to play some other lesser perfect players?

Can an RL player with pre-trained values (assume from a perfect or expert opponent) be used in any scenario with best results?


Note: The problem is more severe in chess, since experts will use some kind of opening moves which will not match with say a random player and thus finding values for those states becomes a problem, since we have not encountered it during training time.
Footnote: Any resources on Game Playing RL is appreciated.
","['reinforcement-learning', 'reference-request', 'game-ai', 'chess', 'tic-tac-toe']","
I would say a good way to make a good agent would be making it play against itself. As you go through several episodes, with a good exploration and exploitation balance, both will gradually learn and converge to $q_*(s,a)$.

So, will this hold true if the same RL algorithm, with its learned values, are used to play some other lesser perfect players?

As long as the states that are played (or approximations if you are using function approximation methods) were simulated enough times during training, it will play well against any kind of opponent.
If you are training against a completely perfect opponent and you are not using function approximation, I believe you could get to an incomplete $Q(s,a)$ table, and so not be able to predict the best play when facing certain states.
"
"Is there research about teaching AI to ""analyze the problem and design a solution""?","
Update on 2019-05-19:
My question is about teaching AI to solve the problem, not letting AI teach a human developer to solve a problem.

Original post:
I'm a software developer but very new to AI.
Today my friend and I chatted about the development of AI. One topic was about implementing the capability of ""given a problem, analyzing the problem and designing a solution"".
Since we are both software developers, we used a simple example problem in our discussion to see how AI might possibly find a solution:
Print the following three lines on the console:

*
***
*****

My friend and I thought we may use some formal method to describe WHAT we want but NOT how we implement it. It's the AI's job to figure out the solution.
Then we came to the question I'm asking here: Since my friend and I are both outsiders of AI research, we don't know if there is any existing research (we believe such research must have existed somewhere) that teaches AI to analyze the problem (which is formally defined) and design a solution using the given tools.
For us human beings, our analysis of the problem and designing might look like the following:

Let me choose a programming language. For example, C.
Let me see what tools I have in the chosen programming language. Oh, here they are:


putchar(ch) which prints a single character on the console.
printf(str) which prints a string on the console.
for-loop; if-else; support of subroutines; etc.

I see the result has three lines of characters: line 1, 2, and 3.
I see the numbers of '*' in the three lines are an arithmetic progression and there is a connection of line number and character number: given the line number i, the character number is 2*i-1, where i is 1, 2, and 3. This is repetition and I can use a for-loop.
Each line is the repetition of '*' so I may implement a function to do this.

void print_line(int N) {
  for (int i = 0; i < N; i++) {
    putchar('*');
  }
  putchar('\n');
}

int main(int argc, char * argv[]) {
  for (int i = 1; i <=3; i++) {
    print_line(2 * i - 1);
  }
  return 0;
}

Alternatively, I may design a naive solution of using printf() three times and hard-code each string:
printf(""*\n"");
printf(""***\n"");
printf(""*****\n"");

We think an AI that can do this may follow a similar analyzing and designing approach as a human developer does. In general, we think:

This AI should have a toolbox using which it can solve some problems (possibly not all problems). In my example above, this toolbox may be a programming language and its corresponding library.
This AI should have the knowledge about some concepts (such as console and string in the example above) and their relationships.
This AI should have the knowledge that connects the toolbox and the concepts, so the AI knows how a tool can manipulate the properties of a concept.
Most importantly, this AI should have the capability of figuring out one or more paths that connect the input to the desired output, using the toolbox. This process, we think, needs the capability of ""analysis"" and ""design"".

Excuse us if the description is still vague. My friend and I are both new to AI so, in fact, we don't even know if ""analysis"" and ""design"" are the proper words to use. We will be glad to clarify if needed.
BTW, we did some quick search about such AI:

Bayou by Rice University doesn't look like understanding the problem, either.
DeepCoder uses Deep Learning and I doubt whether it understands the problem, either.
The AI-Programmer uses genetic algorithms to generate the desired string in BrainFuck. But this AI doesn't look like understanding the problem. It looks like a trial-and-error with feedback.

","['machine-learning', 'algorithm', 'research', 'human-like']",
What is the purpose of the actor in actor-critic algorithms?,"
For discrete action spaces, what is the purpose of the actor in actor-critic algorithms? 
My current understanding is that the critic estimates the future reward given an action, so why not just take the action that maximizes the estimated return?
My initial guess at the answer is the exploration-exploitation problem, but are there other, more important/deeper reasons? Or am I underestimating the importance of exploration vs. exploitation as an issue?
It just seems to me that if you can accurately estimate the value function, then you have solved the RL challenge. 
","['reinforcement-learning', 'actor-critic-methods', 'exploration-exploitation-tradeoff']",
Why use the output of the generator to train the discriminator in a GAN?,"
I've been doing some reading about GANs, and although I've seen several excellent examples of implementations, the descriptions of why those patterns were chosen isn't clear to me in many cases.
At a very high level, the purpose of the discriminator in a GAN is establish a loss function that can be used to train the generator.
ie. Given the random input to the generator, the discriminator should be able to return a probability of the result being a 'real' image.
If the discriminator is perfect the probability will always be zero, and the loss function will have no gradient.
Therefore you iterate:

generate random samples
generate output from the generator
evaluate the output using the discriminator
train the generator
update the discriminator to be more accurate by training it on samples from the real distribution and output from the generator.

The problem, and what I don't understand, is point 5 in the above.
Why do you use the output of the generator?
I absolutely understand that you need to iterate on the accuracy of the discriminator. 
To start with it needs to respond with a non-zero value for the effectively random output from the generator, and slowly it needs to converge towards correctly classifying images at 'real' or 'fake'.
In order to achieve this we iterate, training the generator with images from the real distribution, pushing it towards accepting 'real' images.
...and with the images from the generator; but I don't understand why.
Effectively, you have a set of real images (eg. 5000 pictures of faces), that represent a sample from the latent space you want the GAN to converge on (eg. all pictures of faces).
So the argument goes: 
As the generator is trained iteratively closer and closer to generating images from the latent space, the discriminator is iteratively trained to recognise from the latent space, as though it had a much larger sample size than the 5000 (or whatever) sample images you started with.
...ok, but that's daft.
The whole point of DNN's is that given a sample you can train it to recognise images from the latent space the samples represent.
I've never seen a DNN where the first step was 'augment your samples with extra procedurally generated fake samples'; the only reason to do this would be if you can only recognise samples in the input set, ie. your network is over-fitted.
So, as a specific example, why can't you incrementally train the discriminator on samples of ('real' * epoch/iterations + 'noise' * 1 - epoch/iterations), where 'noise' is just a random input vector.
Your discriminator will then necessarily converge towards recognising real images, as well as offering a meaningful gradient to the generator.
What benefit does feeding the output of the generator in offer over this?
",['generative-adversarial-networks'],"
The main reason that the discriminator is trained concurrently with the generator is to provide (at least in theory) a smooth and gradual learning signal for the generator.
If we trained the discriminator on only the input data, then, assuming our training algorithm converges well, it should quickly converge to a fixed model. The generator can then learn to fool this fixed model, but it will likely still be easy to spot the generator's fakes for a human. For example, the discriminator may have learned that, coincidentally, in the sample you provided, all images of trucks have a fully white pixel in the top left corner. If it learns that pattern, the generator can fool it by generating noise with a white pixel. Once the generator has learned this pattern, all learning stops.
In contrast, suppose that the discriminator is repeatedly re-trained on a mixture of real and generated examples. The discriminator will be forced to learn more complex patterns than ""white pixel in upper left"", which improves its quality beyond the raw patterns in the sample data.
The converse relationship is also true. If the generator is trained only on the training data, it will also likely pick out only the most obvious patterns. These patterns are likely to create many local minima in the weight space for the network. However, if the error signal from the discriminator is fed to the generator, then the generator must adapt: in effect, we are telling it ""making the top left pixel white is not good enough to fool observers. Find more complex patterns"". 
"
How to solve peg solitaire with a graph search?,"
Problem
I've been reading research papers on how to solve a peg solitaire using graph search, but all the papers kind of assume you know how to do the reduction(polynomial time conversion) from the peg solitaire to the graph, which I do not, but this is how I assumed it was done.
For those of you unfamiliar, here is a video that illustrates how to play this game, and here's an image of a board.

The goal is to only have one peg on the board and you get rid of pegs by jump one peg over another. A peg can only jump if the it's jumping into an empty space as shown in the picture above.
What I've tried
I had the idea of converting the problem to a tree where each node represents the state after an action is taken and each edge represents the action taken. So the root node would the initial state which is the board shown above then it's children would be the state of the board after any of the possible legal actions that can be taken.
So, for example:

Then the children of each of those nodes would be the possible moves for them and you can find a solution once you've reached depth 31 in the tree because there are 32 pegs and you win the game where there's only 1 left.
Is this the right approach? It feels a little too abstract because I'd have to represent the edges as peg moves, but that's weird cause they're usually numbers or constraint.
","['game-ai', 'search', 'graphs']","
Your approach seems reasonable to me. The edges do not necessarily have to be numbers, but, if you wish, you could also encode the actions as numbers. For example, the weight of an edge could represent the ""cost"" of the corresponding action. If there's no natural cost associated with an action, then you can add a unit cost for each action.
"
How to implement a neural network for Flappy Bird in Python? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I am new in the field of AI. I am working to create the flappy bird using Genetic Algorithm. After reading and seeing some examples, I saw that most implementations use a Neural Network + Genetic Algorithm and after certain generations, you achieve a very good agent that survives very long.
I currently struggling to implement the Neural Network since I have never taken a Machine Learning course.
On many examples that I have read neural networks require training inputs and outputs. For the flappy bird, I can't think of output since you don't really know if the action of flapping will benefit you or not.
In the example that I followed, Synaptic.js is used and it is pretty straight-forward. However, in Python, I can't find a simple library that will initialize randomly and adjust the weights and biases depending on the good agents that survive longer.
What would be the right way to implement this Neural Network without having a training dataset?
Is there any way to create Flappy Bird without using Neural Networks, just Genetic Algorithm?
The example in Javascript that I am referring to: Flappy Bird Using Machine Learning
","['neural-networks', 'python', 'genetic-algorithms']","
What you are trying to achieve, is a game that learns to play flappy bird. For doing this you need a neural network AND a genetic algorithm, those two things work together.
About your concerns on the output, you don't have to know if the action will benefit or not, i will soon explain why.
The neural network part
So, what you need is to know how to build a neural network, i don't know your knowledge about it, but i suggest starting from the basics. In this scenario, you need a feed forward neural network, because you just take the inputs from the current flappy bird scene/frame (such as the y position of the bird, the distance from the closes pipe ecc..) and feed it through a network that outputs either 1 or 0 (jump or don't jump) in the only output neuron we just decided it has.
In python you can implement a neural network from scratch, or using a neural network framework that does al the dirty work for you.

From scratch you would need to use numpy for matrix calculations, and you would need to learn matrix multiplication, dot products and all that fancy stuff (You can just let numpy taking care of the matrix calculations, but understanding how it works behind the scenes always helps understand new problems that you might come across when doing more advanced stuff)
Using a framework like Tensorflow for python, the only thing you need to do is find the right structure for the network you want to use. You will not have to worry about how activations work, or how the feed forward is performed (But again, it's a good thing to know when working with neural nets)

The genetic algorithm part or """"learning""""
I say """"learning"""" because at first sight it might look like learning, but really it is not.
The genetic algorithm works like ""the survival of the fittest"", where the ""smarter"" birds, which are the ones that reached the higher score on the current generation, will have a chance to have their child little birds, that have the same brain as their parent, with either some minimal modifications, or a mix of their parent brains.
The process of this """"learning"""", so the genetic algorithm, works like so:

Create a generation of let's say 200 birds, every bird has a brain with random weights, so at the first run, they are all very...not smart
The game starts, and every frame of the game, the brain of the bird recieves as input some data that is taken from the current frame ( y pos of the bird, distance from pipe...)
The brain ( neural network ) of each bird, performs a feed forward with that data, and outputs what at the beginning is a very random result, let's say 0.75 for one bird
At this point you decided that 0.75 is greater than 0.5, so you take that as a 1, which stands for ""jump"", while if it was 0.3, so 0, the bird does nothing and keeps falling
Shortly the bird will die cause he has no idea of what he is doing, so he most likely collides with a pipe or the ground.
After all birds met their fate, you see that some birds reached further than others, so you choose, for example, 5 of the best performing ones.
Now you try to create a new generation of 200 birds using only the brains of those 5 that were choosen, by mixing and modifying theyr brains
Now the new birds have a brand new brain, that in some cases might be better than the previous one, so chances are that some of those birds will reach a higher score, therefore flap further into the level.
Repeat from point 6

In practice your ""perform_genetic_algorithm"" function in python, will have to choose the birds with the highest score, and as wild as it sounds, mix their brains and modify them, hoping that some modifications will improve the performance of the bird.

I can't think of output since you don't really know if the action of flapping will benefit you or not

The mechanism above explains why you basically do not care at all about the output, except saying to the game engine: ""hey the bird decided to flap, do it"". Whether it's the right action or not, doesn't matter, as the smarter birds are naturally gonna get further and so be more likely to be choosen for next generation.
Hopefully now it's all more clear.

Here is some useful links for building a neural network and for understanding the genetic algorithm:

How to build a neural network: I am linking this because it contains all useful information about how to build a very basic neural network in python. In your case, you would have to ignore all the part about backpropagation, loss & error calculation and SGD, and just look at the feed forward part.
How to build a neural network - 2: This is another example of building a neural network that i found really useful, probably it's simpler and more straight forward than the previous link, but again, the backpropagation part is not needed for this genetic based learning.
Video tutorials on genetic algorithm: This is a very long but very explanatory playlist of videos that dives into the nature of genetic algorithms and how to implement one
Genetic algorithm optimization: Other source about genetic algorithms

"
How define a reward function for a humanoid agent whose goal is to stand up from the ground?,"
I'm trying to teach a humanoid agent how to stand up after falling. The episode starts with the agent lying on the floor with its back touching the ground, and its goal is to stand up in the shortest amount of time.
But I'm having trouble in regards to reward shaping. I've tried multiple different reward functions, but they all end up the same way: the agent quickly learns to sit (i.e. lifting its torso), but then gets stuck on this local optimum forever.
Any ideas or advice on how to best design a good reward function for this scenario?
A few reward functions I've tried so far:

current_height / goal_height
current_height / goal_height - 1
current_height / goal_height - reward_prev_timestep
(current_height / goal_height)^N (tried multiple different values of N)
...

","['reinforcement-learning', 'reward-functions', 'reward-design', 'reward-shaping']","
See https://gymnasium.farama.org/environments/mujoco/humanoid_standup/#rewards for a description of how to get a humanoid ragdoll to stand up!
"
Should I use leave-one-out cross-validation for testing?,"
I am currently working with a small dataset of 20x300. Since I have so few data points, I was wondering if I could use an approach similar to leave-one-out cross-validation but for testing.
Here's what I was thinking:

train/test split the data, with only one data point in the test set.

train the model on training data, potentially with grid-search/cross-validation

use the best model from step 2 to make a prediction on the one data point and save the prediction in an array

repeat the previous steps until all the data points have been in the test set

calculate your preferred metric of choice (accuracy, f1-score, auc, etc) using these predictions


The pros of this approach would be to:

You don't have to split the data into train/test so you can train
with more data points.

The cons would be:

This approach suffers from potential(?) data leakage.

You are calculating an accuracy metric from a bunch of predictions that potentially came from different models, due to the grid searches, so I'm not sure how accurate it is going to be.


I have tried the standard approaches of train/test splitting, but since I need to take out at least 5 points for testing, then I don't have enough points for training and the ROC AUC becomes very bad.
I would really appreciate some feedback about whether this approach is actually feasible or not and why.
","['training', 'cross-validation', 'testing', 'loocv']","
Concerning $k$-fold Cross Validation, I like to think of it by considering two extremes you can do: Leave-One-Out Cross-Validation where you leave one sample each time and train your model on the remaining $n-1$, and 2-fold Cross Validation at which you split your dataset in half and train (and validate) two models on two different halves.
The important aspect when choosing $k$ is a bias-variance tradeoff. Note that in LOOCV you train each model using almost as many samples as there are available ($n-1$), so the validation step should give you an unbiased estimate of the real test error. However, each model in LOOCV is trained each time on almost exactly same dataset!. This has important consequences, since the output of each model is highly correlated with each other. Since the mean of highly correlated variables has big variance, LOOCV will suffer from huge variance.
On the other hand, in 2-fold CV the models share no common samples, so they are not correlated and therefore their outputs have low variance. But since we train each model using only a half of available samples, the procedure will have a high bias (the estimated test error will be off from the true test error).
What to do in this scenario? Choose something in the middle. Usually $k=5$ and $k=10$ should be a good choice.
"
"If I wanted to calculate multiple feature maps in a convolutional layer, should the filters be trained individually?","
Assume I have an input of size $32 \times 32 \times 3$ and pass it to a convolution layer. Now, if my kernel size were to be $5 \times 5 \times 3$ and the depth of my convolution layer were to be 1, only one feature map would be produced for the image. Here, each neuron would have $5 \times 5 \times 3 = 75$ weights (+1 bias).
If I wanted to calculate multiple feature maps in this layer, say 3, is each local section (in this example, $5 \times 5 \times 3$) of the image looked on by three different neurons and each of their weights trained individually? And what would be the output volume of this layer?
","['convolutional-neural-networks', 'features', 'convolutional-layers', 'filters']",
How to manage large amounts of image data for training?,"
Right now, I am trying to synthesize training images for a CNN and due to the nature of the application, there is a finite number of sample images to learn from.
From other research, I expect to be using about 200,000 training images at a resolution of 1280*720, which with 3 channel at 8 bits will take about 550 GB to save uncompressed. This number can and probably will rise in the future, meaning more memory that I will need to provide.
I imagine that there are applications that required even more training data with higher complexity and that there are solutions to handling that such as compression techniques and the like.
My question: Are there solutions for the memory management beyond compressing the images with JPEG and such besides generating and instantly consuming the pictures without saving them to permanent memory?
","['convolutional-neural-networks', 'datasets', 'memory']","
I suggest you fine-tune an existing model. Knowledge transfer models in many image processing tasks are now open sourced and you can build your model on top of them. Also, knowledge transfer models are trained on a large datasets and can quickly converge to your case-study with a little of task-specific extra training. 

This way you will use few data to tune the model which leads to less
  memory use and less training time. You will also take advantage from a
  ready-to-use architecture and get accurate results.

Depending on your case-study you can choose from this list of computer vision pre-trained models.
"
Why are RNNs better than MLPs at predicting time series data?,"
Understandably RNNs are very good at solving problems involving audio, video and text processing due to the arbitrary input's length of this sort of data.
What I don't understand is why RNNs are also superior at predicting time series data and why we use them over simple MLP DNNs.
Say I wanted to predict what the value in the time series is at $t+1$. I would take a window of, let's say, $t-50, t-49, \dots, t$, and then feed loads of sampled training data into a network. I could either choose to have a single LSTM unit remembering the entire window and basing the predictions on that, or I could simply make a 50 neuron wide MLP network.
What exactly is it about RNNs that makes them better in this scenario or any scenario for that matter?
I understand that the LSTM would have substantially fewer weights (in this scenario) and should be less computationally intensive, but, apart from that, I don't see any difference in these two methods.
","['neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'time-series', 'feedforward-neural-networks']","
As noted in LeCun et al.'s Deep Learning paper:

RNNs, once unfolded in time ... can be seen as very deep feedforward networks in which all the layers share the same weights

So, if we ignore how easy they are to train, there is theoretically no real advantage of RNNs over MLPs, on any task, including time series modeling.
Perhaps the key advantage of RNNs is that they share parameters over time.  That means they have fewer parameters, and the parameters they do have get used more often.  This makes them easier to train (and also allows handling variable-length input sequences, which, as you noted, does not apply when forecasting from a fixed historical window).
Also, the ""compression"" of the historical information into a state that can pass forward in time, as noted in this excellent answer on quora, may force RNNs to learn general patterns that may generalize better to unseen data.
"
Train and Test Accuracy of GRU network not increasing after 2nd epoch,"
So Im currently implementing my first neural network using GRUs as a model and Keras as an implementation since its pretty highlevel.
My problem is about the classification of 8 hour long timeseries with 11 different events with 1 second timesteps or to be more specific: sleep recordings.
Since the GRU cant handle the whole timeseries at once, I split it in 500 timepoint pieces with 50 seconds (10%) overlap.  Also since I dont have much data, after the training/test split Im oversampling the train data by duplicating the underrepresented classes up to 10 times. So at the end I get a set of ca. 14.000 training snippets and ca 1.800 to test the model. That to the data.
Next thing is the model, represented with the Keras Code:
verbose, epochs, batch_size = 2, 50, 1200  # 1200 batch_size was the maximum the GPU can handle
    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]

    model = Sequential()
    model.add(GRU(128, input_shape=(n_timesteps, n_features), return_sequences=True, dropout=0.7,
                  kernel_regularizer=regularizers.l2(0.01), activation=""relu""))
    model.add(
        GRU(64, return_sequences=True, go_backwards=False, dropout=0.7, kernel_regularizer=regularizers.l2(0.01),
            activation=""relu""))
    model.add(
        GRU(32, return_sequences=True, go_backwards=False, dropout=0.7, kernel_regularizer=regularizers.l2(0.01),
            activation=""relu""))
    model.add(
        GRU(16, return_sequences=True, go_backwards=False, dropout=0.7, kernel_regularizer=regularizers.l2(0.01),
            activation=""relu""))
    model.add(TimeDistributed(Dense(units=16, activation='relu')))
    model.add(Flatten())
    model.add(Dense(n_outputs, activation='softmax'))

    # define custom optimizer with configurable learning rate
    sgd = optimizers.SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

So as a short summary:
I am using 4 GRU layers with a ""pyramid"" shape, so the network has to be more specific towards the end. Also one fully connected layer as some kind of ""adapter"" and one output layer with the size of my features. I am using SGD as an optimizer.
The results are always pretty bad, here are the loss and accuracy plots of an example run featuring 25 epochs:


Despite the huge dropout each stage, it still seems to be overfitting. Also as you can see, the accuracy of test and train is not changing after the 2nd epoch. The result is this sad looking confusion-matrix, showing only one class beeing detected by the network, which is not even the one with the highest amount of data in the dataset:  
[[231   0   0   0   0   0   0   0]
 [ 55   0   0   0   0   0   0   0]
 [647   0   0   0   0   0   0   0]
 [141   0   0   0   0   0   0   0]
 [444   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0]
 [118   0   0   0   0   0   0   0]
 [ 74   0   0   0   0   0   0   0]]

So what are possible approaches in order to:
a) fight the overfitting,
b) just one class beeing detected and
c) fix the stationary accuracy to be growing again?
Since Im pretty new to neural networks I am thankful for your time and effort!
","['neural-networks', 'recurrent-neural-networks', 'keras', 'objective-functions', 'time-series']","
Couple reccomendations:
1) I dont think your overfitting, your test loss is not ever increasing and is staying reasonbly proportional to train loss -- This may indicate that whatever loss your using is not a good indicator of the metric of interest (in this case, it seems you want that to be accuracy, but data is imbalnced so maybe look at avg precision?)
2) Use a lr scheduler, or something like a ReduceLROnPlateau callback to reduce the lr once the loss converges
3) Adding more sources of the underrepresented class is valid but i reccomend just using class weighting. Effectively its the same (as an expectation) and will save you train time.  
Good Luck!
"
Divide classes into truncated and non-truncated objects,"
At the moment I am working on a vehicle counting & classification project. 
For a specific part in the project I need to get back only the completely visible vehicles from my input data (images).  I am wondering if this could be done (more) automatically in the following way:

zoom in such that only approximately one van would be visible
divide the vehicles into two categories: truncated and non-truncated
train on these two classes
After training and testing, use the model to find the completely visible vehicles. 

So the main question is, is it possible that this would give sufficient results or should I try to find another solution?
","['neural-networks', 'training']","
It could work. I think itll be hard to find labelled bounding box data describing full/truncated cars so this is a good idea for self labelling.
Theres probably also some other ways for you to get labelled data without the crowd sourcing or self labelling. Take a large labelled car dataset with bounding boxes (COCO has some, and if your good with aerial imagery theres alot more out there), and just enforce that any car bounding boxes that are touching the boundary are truncated, while ones fully encompassed in the imaged are not, since with high probability that will be the case. 
Good luck!
"
Is the number of feature maps equal to the number of kernels in the LeNet 5 architecture?,"
In LeNet 5's first layer, the number of feature maps is equal to the number of kernels. However, the second convolutional layer has a depth different from the 3rd layer. Does the filter size dictate the number of feature maps?
","['deep-learning', 'convolutional-neural-networks', 'features']","
The # of kernels will be the channel length, looking at the image you posted in your comment from post, I do not understand where you see the inconsistency.
"
Is there a machine learning algorithm to find similar sales patterns?,"
I have a dataset as follows

(and the table extends to include an extra 146 columns for companies 4-149)
Is there an algorithm I could use effectively to find similar patterns in sales from the other companies when compared to my company? 
I thought of using k-means clustering, but as I'm dealing with 150 companies here it would likely become a bit of a mess, and I don't think linear regression would work here.
","['machine-learning', 'unsupervised-learning', 'linear-regression', 'pattern-recognition', 'k-means']","
I would recommend a hierarchical cluster algorithm, after normalising your numbers into proportions. Then the clustering should be able to identify similar patterns. Depending at which level you make the cut, you can decide how many clusters you want.
A great resource on this topic is Kaufman, L., & Roussew, P. J. (1990). ""Finding Groups in Data - An Introduction to Cluster Analysis"". John Wiley & Sons
"
How do I represent a multi-dimensional state using a neural network?,"
I have a set of 15 unique playing cards from a deck of 52 playing cards. A given state is represented by the respective card values in the set of 15 cards, where the card value is a prime number associated with that card. For example, AH is represented by 3.
How should I represent a single state for the NN? Should it be a list of the 15 prime numbers representing the list of cards? I was hoping that I could represent a single state as the sum of each of all 15 prime numbers and then throw that sum through a sigmoid function. My concern, however, is that the NN will lose information if I reduce the dimension of the state to a single attribute (even if that attribute is unique to that state - the sum of n prime numbers is unique compared to the sum of any other n prime numbers).
How important is the dimensionality of each state for Deep Q Learning? I'd really appreciate even some general direction.
","['machine-learning', 'deep-learning', 'q-learning', 'dqn', 'function-approximation']","
In my humble opinion, it seems like it is important to have them separated, if having a certain card can influence the result in some way that is not its prime value, instead of not only using the sum. But it depends on the game and its rules. For example:
If having 5 cards of hearts in the set of 15 cards makes you win the game, then if you only represent the state as the sum of the value of the cards, the DQN will never learn that it was not the whole 15 cards that caused you to win, but only a specific part of the set.
"
Which model is better given their training and validation errors?,"
Below you have the plots of the training and validation errors for two different models. Both plots show the RMSE values for the validation dataset versus the number of training epochs. It is observed that models get lower RMSE value as training progresses.
The model associated with the first plot is performing quite well. The gap is quite narrowed.

I think the model associated with this second plot is doing pretty good, but not as well as the other. The gap is much broader. 

The model of the first plot was trained using a data set containing 1 million of ratings, while the second one used only 100K. I'm implementing the collaborative filtering (CF) algorithm. I am optimising it using SGD.
 
Are any of these models overfitting or underfitting?
","['machine-learning', 'overfitting', 'recommender-system', 'cross-validation', 'underfitting']","
One possibility: If you are using a dropout regularization layer in your network, it is reasonable that the validation error is smaller than the training error. Because usually dropout is activated when training but deactivated when evaluating on the validation set. You get a more smooth (usually means better) function in the latter case.
"
"Should I call the error ""validation error"" or ""test error"" during cross validation?","
I'm using 10-fold cross validation on all models. Here you can see both plots:


Since I am using k-fold cross validation, is it okay to name it ""validation error vs training error"" or ""test error vs training error"" would be better? 
","['machine-learning', 'terminology', 'overfitting', 'cross-validation']",
Why do we need multiple LSTM units in a layer?,"
What is the point of having multiple LSTM units in a single layer?
Surely if we have a single unit it should be able to capture (remember) all the data anyway and using more units in the same layer would just make the other units learn exactly the same historical features?
I've even shown myself empirically that using multiple LSTMs in a single layer improves performance, but in my head it still doesn't make sense, because I don't see what is it that other units are learning that others aren't? Is this sort of similar to how we use multiple filters in a single CNN layer?
","['machine-learning', 'deep-learning', 'long-short-term-memory']",
What is the difference between Sentiment Analysis and Emotion Recognition?,"
I found Sentiment Analysis and Emotion Recognition as two different categories on paperswithcode.com. Should both be the same as my understanding? If not what's the difference?
","['definitions', 'comparison', 'emotional-intelligence', 'sentiment-analysis']","
Sentiment Analysis -- the most common text classification that analyses an incoming message and tells whether the underlying sentiment is positive, negative, or neutral.
Emotion Recognition-- emotion recognition refers to the cognitive and behavioral strategies people use to influence their own emotional experience.
Note: This explanation is based on the paper that I have read.
"
"Q-learning, am I interpreting correctly $Q(s,a) = r + \gamma \max_{a'} Q(s',a')$?","
Ok, due to previous question I was pointed to use reinfrocement learning.
So far what I understood from random websites is the following:

there is a Q(s,a) function involved
I can assume my neural network ~ Q(s,a)
my simulation has a state (N input variables)
my actor can perform M possible actions (M output variables)
at each step of the simulation my actor perform just the action corresponding to the max(outputs)
(in my case the actions are 1/2/3 % increase or decrease to propellers thrust force.)

From this website I found that at some point I have to:

Estimate outputs Q[t] (or so called q-values)
Estimate outputs over next state Q[t+1]
Let the backpropagation algorithm perform error correction only on the action performed on next state.

The last 3 points are not clear at all to me, infact I don't have yet the next state what I do instead is:

Estimate previous outputs Q[t-1]
Estimate current outputs Q[t]
Let backpropagation fix the error for max q value only

Actually for code I use just this library which is simple enough to allow me understand what happens inside:
NeuralNetwork library
Initializing the neural network (with N input neurons, N+M hidden neurons and M output neurons) is as simple as
Network network = new NeuralNetwork( N, N+M, M);

Then I think to understand there is the need for an arbitrary reward function
public double R()
{
     double distance = (currentPosition - targetPosition).VectorMagnitude();
     if(distance<100)
         return 100-distance; // the nearest the greatest the reward
     return -1; // too far
}

then what I do is:
// init step
var previousInputs = ReadInputs();
UpdateInputs();
var currentInputs = ReadInputs();

//Estimate previous outputs Q[t-1]
previousOutputs = network.Query( previousInputs );

//Estimate current outputs Q[t]
currentOutputs = network.Query( currentInputs);

// compute modified max value
int maxIndex = 0;
double maxValue = double.MinValue;
SelectMax( currentOutputs, out maxValue, out maxIndex);

// apply the modified max value to PREVIOUS outputs
previousOutputs[maxIndex] = R() + discountValue* currentOutputs[maxIndex];

//Let backpropagation fix the error for max q value only
network.Train( previousInputs, previousOutputs);

// advance simulation by 1 step and see what happens 
RunPhysicsSimulationStep(1/200.0);
DrawEverything();

But it doesn't seem to work very nice. I let simulation running for over one hour without success. Probably I'm reading the algorithm in a wrong way.
","['neural-networks', 'reinforcement-learning', 'q-learning']",
"Drone training, how to train without training data?","
I setupped a small drone simulator using PhysX, the time step is at 200 hz, while motors update like regular ESCs (at 50 Hz). I computed the inertia matrix, tweaked a bit mass of components to be real, air drag, gravity etc. After a first partial success in tuning a PID algorithm I got bored to find and hunt perfect values, and started thinking to tune it with a NN, but then I thinked, why using PID at all if NN can find better solutions?.
I started playing with NN, but then I realized I have no traning data.
Sure I could do
NN.Train(input,expectedOutput);

but what is actually the expected output? To be useful it has to be the thrust force of propellers tuned to keep input (position and orientation) stable in the desired place.
But actually I don't (Want to) know in advance the thrust that every single propeller has.
Since it is a simulation it is ok spending some computing time between each simulation step (eventually I'll try to optimize it later to fit it in a microcontroller)..
So, is it possible given a regular NN implementation where I can select number of:

Input neurons
Hidden neurons
Output neurons

find a way to train my model live? I need a way to tell the NN 

hei this time you performed X keep going or go the opposite way.

","['neural-networks', 'training']","
It's debatable whether neural networks can find a better solution than PID, if your goal is to simply keep the output around a certain reference point PID should do a perfect job pretty much. If you really want to use ""intelligent"" control with NN you can look into reinforcement learning.
Few interesting papers that directly adress your problem:  
Autonomous helicopter flight via reinforcement learning
An Application of Reinforcement Learning to Aerobatic Helicopter Flight
Autonomous Helicopter Control Using Reinforcement Learning Policy Search Methods 
You can also find more in the references of these papers.
"
How to fix time dimension in time varying data-sets using deep learning model for classification?,"
Dataset Description
I am working on the famous ABIDE Autism Datasets. The dataset is very big in the sense that it has more than 1000 subjects containing half of them as autistic and the other half as healthy controls. The dataset is taken from 17 sites across the world and each site used a varying time dimension when recording the subjects fMRI.
My Question
I want to use this dataset for a classification task but the only issue is time-varying subjects as features set are fixed to 200 so you can say that I have subjects dimensions like 150 x200, 75 x 200 , 300 x 200... so on. So what are advanced AI or deep learning techniques that I can use to fix this time dimension for every subjects or can anybody suggest some deep learning framework or model that I could use to fix these varying time dimensions across subjects?
My Effort
Approach 1
I have applied PCA to the time dimension and fixed them to 50 and tried other numbers also but it did not produce good accuracy for classification
Approach 2
I also tried to use only specific time points from every subject like taking only 40 time points from every subject to fix the dimension but again it did not work as definitely filtering some time series data on every subject would lose crucial information.
","['deep-learning', 'datasets', 'time-series']","
In deep learning, it is very common to use Recurrent Neural Networks (RNNs) to handle time-series data with varying input sequence lengths. Check out the RNN Wikipedia for more detail.
"
A mathematical explanation of Attention Mechanism,"
I am trying to understand why attention models are different than just using neural networks. Essentially the optimization of weights or using gates for protecting and controlling cell state (in recurrent networks), should eventually lead to the network focusing on certain parts of the input/source. So what is attention mechanism really adding to the network? 
A potential answer in the case of Encoder-Decoder RNNs is:  

The most important distinguishing feature of this approach from the
  basic encoderdecoder is that it does not attempt to encode a whole
  input sentence into a single fixed-length vector. Instead, it encodes
  the input sentence into a sequence of vectors and chooses a subset of
  these vectors adaptively while decoding the translation. This frees a
  neural translation model from having to squash all the information of
  a source sentence, regardless of its length, into a fixed-length
  vector. We show this allows a model to cope better with long
  sentences.
  - Neural Machine Translation by Jointly Learning to Align and Translate

which made sense and the paper says that it worked better for NMT.
A previous study indicated that breaking down the sentence into phrases could lead to better results:

In this paper, we propose a way to address this issue by automatically
  segmenting an input sentence into phrases that can be easily
  translated by the neural network translation model. Once each segment
  has been independently translated by the neural machine translation
  model, the translated clauses are concatenated to form a final
  translation. Empirical results show a significant improvement in
  translation quality for long sentences.
   - Overcoming the Curse of Sentence Length for Neural Machine
  Translation using Automatic Segmentation

which paved the way for further research resulting in attention models.
I was also going through an article on Attention is not quite all you need where it said something similar:  

An LSTM has to learn to sequentially retain past values together in a
  single internal state across multiple RNN iterations, whereas
  attention can recall past sequence values at any point in a single
  forward pass.

and a more curated blog on the family of attention mechanism gives insight on how different ways have been formulated for implementing the concept: Attention? Attention!
Specifically, I want to know how attention mechanism is formulated for this task (aforementioned) or in general. A detailed mathematical insight would be helpful, probably somewhat on these lines: Understanding Attention in NN mathematically
","['neural-networks', 'machine-learning', 'deep-learning', 'recurrent-neural-networks', 'attention']",
How do I integrate Tensorflow with EV3DEV? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I've got a Lego Mindstorms EV3 with EV3DEV and EV3-Python installed. I wanted to try out artificial intelligence with the robot, and my first project is going to be to get the robot to try and recognize some images (using convolutions) and do an action related to the image it has seen. However, I can't find a way to use Tensorflow (or any AI module for that matter) on an EV3. Does anyone know how to incorporate Tensorflow or any other modules into the EV3? Help would be gladly appreciated.
","['python', 'tensorflow', 'robotics']","
There is a YouTube video LEGO EV3 Raspberry Pi Tensorflow Sorting Machine by ebswift that should help you although you will need a Raspberry Pi.  From the abstract:
This is a sorting machine based on the EV3 45544 education kit sorting machine.  The colour sorting camera is substituted with a Raspberry Pi with the v2 camera.  The EV3 is controlled over wifi via RPyC and the object recognition work is done via Tensorflow. 
A viewer asked: Can you share links about how to train a model on the PC and move the trained model to the Pi?
ebswifft replied:
Glad you like it :).  Training the model on the PC is following the guide for Tensorflow for Poets https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/.  All you do is install Tensorflow on the Raspberry Pi, clone the github repository onto it and copy the model you trained from the codelabs article onto the Raspberry Pi to run the classifier as per step 5 of the codelabs article.  Onboard image capture on the Raspberry Pi is just done using picamera.  I used a tensorflow version that a user compiled for the Raspberry Pi from here: https://github.com/samjabrahams/tensorflow-on-raspberry-pi/issues/92.  I might do up more of a general step-by-step sometime on my site, I'll report back if I can get that up and running.
There is also BrickClassifi3r:
This Lego Mindstorms EV3 robot uses a neural network to recognize a cube, a cylinder or a small cube put on a conveyor belt. See the video how it works. Each object on the conveyor belt is scanned by an IR-sensor every 40ms for about a second. The resulting data are 24 distance values representing one of the three objects. This data is fed into the neural network on the robot to classify the object within 180ms. In a test with 300 objects it reaches 95,6% accuracy. The neural network has been trained before by a machine learning algorithm with TensorFlow on a PC using a set of 375 training examples - 125 examples for each object.
The ev3-myo project uses LEGO Mindstorms EV3, TRACK3R, a Myo armband and TensorFlow for gesture recognition.
"
Modifying LSTM to include forecast [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am looking at LSTM example here
However, I am not sure how to modify the setup if I have forecast available (assuming perfect forecast) for TEMP: Temperature and PRES: Pressure at time t.
i.e. pollution_t = fn(TEMP_t, PRES_t, TEMP_t_1, PRES_t_1, ..., othervariables and lagged values) 
_t represents the perfect forecast available
_t_1 represents the variable values at previous time steps
Basically I am looking for something of the following setup:

",['long-short-term-memory'],
"After a model has been trained, how do I use it to address the real-world problems?","
I understand the way we build and train a model, but all of the online courses I've found end with this. I can't find any course explaining the process of utilizing the trained model to address the problem.
Is there any course out there that explains the whole process from data collection, model building, and utilizing the model to solve the real-world problems?
","['machine-learning', 'applications', 'models', 'ai-development']","
Using a machine learning or AI-powered model once it has been built and tested, is not directly an AI issue, it is just a development issue. As such, you won't find many machine learning tutorials that focus on this part of the work. But they do exist.
In essence it is the same as integrating any other function, which might be in a third-party library:

Package the new function so that it can be called from your production system (this may be the hardest part)
Decide where in the code of your production system to call the new function. In your example case maybe after form is completed describing an incident you could link the top recommended KB articles from the new ticket.
Design and check the associated user experience/UI as appropriate for your development team (in small projects you may skip this step and just implement)
Change your production code to call the packaged function. In any professional development team, this part will have multiple stages, but not really relevant to the question - if you are the ML specialist and delivering your new model to an existing team, you need to talk to them about both the packaging part and the steps involved here.

Often, machine learning models come with a whole bunch of dependencies that the rest of a system does not have. There are a variety of solutions for that, depending on the libraries that you are using, whether you are using a cloud PaaS service etc. You could just build a Docker image to hold all the AI parts and call it passing the input data.
Deploying ML models to production as a job is often a task for Machine Learning Engineer roles. There are courses and articles covering the practical aspects of these steps, if you search for them. Here are a couple:

Deploying Machine Learning Models from Coursera
A blog article on Medium: There are two very different ways to deploy ML models . . .

I have no affiliation with any of the above, have not read articles or taken the courses, and am not able to make any recommendation, even if you told me the technologies you were using for ML and in production currently.
After deploying to production . . .
Your work is not necessarily done. You will want to monitor behaviour of the system. During integration, you should of added logging, or some way to get feedback of performance in the wild.
For instance, if this is an ML system, does the accuracy seen in testing hold up against real life? Is the target population drifting? If the system makes interventions - by e.g. suggesting links, or showing categories to end users - how well are those working in practice? Is the performance and responsiveness of the system fast enough when the service is under load?
If it is more reactive AI system, that takes actions itself, you will similarly want to monitor what it is supposed to optimise, or sample its output for errors and quality control.
All this feedback can go back into a new iteration of design and integration. How to incorporate that will depend a lot on the nature of the system and what you discover, so could be the subject of further questions.
"
How to debug and find neurons that most influenced a pixel in the output image?,"
I'm building CNN network of Image to Image.
After training, I have some bad results in part of the Image.
I would like to find the neurons that most influenced those pixels and do retraining only for them.
I have seen some previous works about visualizing networks, like here:
https://github.com/utkuozbulak/pytorch-cnn-visualizations
But they are only finding activations maps or visualizing with softmax in the output layer.
How can I do that for Image to Image?
","['deep-learning', 'convolutional-neural-networks', 'image-generation']",
Estimate distance between points in perspective image,"
I am trying to estimate the real world distance (in metres) between two points in a perspective image using an uncalibrated camera. However, the dimensions of an object in the image are known. 
I thought of using pixel per metre, but, being a perspective image, that did not seem like a viable approach. I believe I need the camera matrix from the image (maybe using the known object) and compute the 3D coordinates of the point, then simply compute the Euclidean distance between the points. 
If that is the right approach, how may I do it? If there are any alternatives to this approach, kindly let me know.
","['machine-learning', 'computer-vision']",
Advice on creating a new environment using OpenAI Gym [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I'm looking for some general advice here before I dive in.
I'm interested in creating a new environment for OpenAI gym to provide some slightly more challenging continuous control problems than the ones currently in their set of Classic Control environments.  The intended users of this new environment are just me and members of a meetup group I am in but obviously I want everyone to be able to access, install and potentially modify the same environment.
What's the easiest way to do this?

Can I simply import and sub-class the OpenAI gym.Env module similar
to the way cartpole.py does.
Or do I need to create a whole new
PIP package as this article suggests?

Also, before I invest a lot of time on this, has anyone already created a cart-pole system environment where the goal is to stabilize the full state (not just the vertical position of the pendulum)?  (I tried googling and couldn't find any variants on the original cart-pole-v1 but I'm suspicious as I can't be the first person to make modified versions of some of these classic control environments).
Thanks, (I realize this question is a bit open-ended) but hoping for some good advice that will point me in the right direction.
","['python', 'open-ai', 'environment']",
Predict best price using neural network?,"
Our customer runs a tour agency. He has an excel spreadsheet containing the following information for people that have contacted them:
Customer name, country, tour duration (requested by customer), tour date, number of people in the tour (usually from 1 to 3), price given to the customer, answer: accepted/rejected (indicates if customer accepted or rejected the price given by the tour agency).
My customer wants a predictor or tool that can let him enter the details given by future customers, e.g:
Number of participants,
Tour duration,
Country (not sure if necessary?)
And the system will return the best price to charge the customer (so he won't reject the proposal but pay the maximum possible). 
Another option would be that the tour agency owner will enter the price and the system will answer ""Customer will accept that price"" or ""Customer will reject that price"".
Is this even possible? I think it may be done using neural networks trained with the previous answers from customers that the tour agency owner has in his excel spreadsheet? 
","['neural-networks', 'deep-learning', 'prediction']","
This is possible.... but there's no reason to use a neural network! Your best bet on a problem like this is likely to use a logistical regression for the yes/no aspect of the question and a linear regression (or combination of linear regressions) to answer the pricing question - there are also ways of simply using linear regressions and setting up cutoffs to answer the yes/no question.
The reality is that the accuracy of such a model/series of models would depend entirely on the quality and quantity of the data, but it's unlikely that in this case a neural network would provide a better result than smart usage of simpler models.
"
Why should we use TD prediction as opposed to TD control algorithms?,"
Consider a problem where we have a finite number of states and actions. Given a state and an action, we can easily know the reward and the next state (deterministic). The state space is really large and has no final state.  
There was a paper that for a problem of this type used TD(0) by filling the value table and chose its actions by:
$$\pi(s) = \text{argmax}_a (r(s,a) + \gamma V(s_{(t+1)}))$$
I've read somewhere that is OK to use prediction algorithms when the model is well-described with the objective of choosing best actions and not only evaluating the policy. 
What is the purpose, advantages and disadvantages of using TD prediction here instead of a TD control algorithms (and just saving the $Q(s, a)$ table)? If it was about space, you still have to store a table with all the rewards for each state-action pair, right? 
I'm not sure if I was able to explain myself very well as I was trying to keep it short, if some clarification is needed please tell me.
","['reinforcement-learning', 'temporal-difference-methods']",
Are there real-world problems where case-based reasoning is not suitable?,"
I know case-based reasoning has four stages: retrieve, retain, re-use and revise.
Used for solving new problems by adapting solutions that were used to solve old problems, like car issues.
The advantages of it are that solutions are proposed quickly and there's no need to start from scratch, but is there a real-world problem where using this would not be suitable?
Just trying to better my understanding of it.
",['reinforcement-learning'],
What is the relation between Monte Carlo and model-free algorithms?,"
Monte Carlo (MC) methods are methods that use some form of randomness or sampling. For example, we can use an MC method to approximate the area of a circle inside a square: we generate random 2D points inside the square and count the number of points inside and outside the circle. 
In reinforcement learning, an MC method is a method that ""samples"" experience (in the form of ""returns"") from the environment, in order to approximate e.g. a value function. 
A temporal-difference algorithm, like $Q$-learning, also performs some form of sampling: it chooses an action using a stochastic policy (e.g. $\epsilon$-greedy) and observes a reward and the next state.  So, couldn't $Q$-learning also be considered a MC method? Can an MC method be model-based?
","['reinforcement-learning', 'monte-carlo-methods', 'temporal-difference-methods', 'comparison', 'model-free-methods']","
In Reinforcement Learning (RL), the use of the term Monte Carlo has been slightly adjusted by convention to refer to only a few specific things.
The more general use of ""Monte Carlo"" is for simulation methods that use random numbers to sample - often as a replacement for an otherwise difficult analysis or exhaustive search.
In RL, Monte Carlo methods are generally taken to be non-bootstrapping sample-based approaches to estimating returns. This is a labelling convention within RL - probably because someone called an initial model-free learner a ""Monte Carlo method"", and the name stuck whilst many refinements and new ideas have since been published under different names.
The historical use of the term is important, since if you mention you are using ""Monte Carlo Control"", it usually means a very specific subset of methods within RL, to most readers.

"
Choice of inputs features for Snake game,"
I am designing a neural network using Deep Q-Learning, which teaches an agent how to play Snake (The classic Nokia game from the 90'ies). The goal of the game is to navigate the snake on a playing field (2D), and to eat a randomly placed fruit. As the Snake eats the fruit, it grows in length. The game ends if the snake hits the game border, or it self, so as the number of fruits consumed increases, so does the difficulty of navigating without hitting something.
I have trained the Snake game on a 10x10 playing field using the following inputs:

x direction of the snake
y direction of the Snake 
The fraction of playing field occupied by the snake itself 
A bool which says if the fruit is in front of the snake (from it's current direction)
A bool which says if the fruit is to the left of the snake (from
it's current direction) 
A bool which says if collision (game over) in front of the snake is possible 
A bool which says if collision to the right is possible
A bool which says if collision to the left is possible

With this choice of inputs I am getting the Snake agent to work reasonably well, and score until it plateaus out. I have examined the cases where the snake dies, and it all happens when it has no way of escaping, for example, it turns around and blocks its own path, until it finally has no where to go. This is more likely to happen as the Snake increases in lenght.
I was thinking on how I could improve this performance. It seems to me, that the reason the Snake can make a self-blocking turn is because of the inputs. Since the path it takes is clear, there's no information, that the next path is not, or that continuing further down will eventually lead to game over. If the Snake agent was aware of all obstacles in each step, i.e. the entire state-space, then I could imagine that would help train the network towards finding the optimal path without ending up blocking itself.
Since I have made the Snake game myself, I can return to the agent a matrix, or an unfolded vector, which contains the inputs for each column / row on the playing field. A blocked cell would be set to 0, a free cell 1, the fruit cell has value 0.75 and the snake head (which moves) could be assigned value 0.25. After trying this approach, I have to say I was unsuccesful. The snake ends up just turning in circles, even if I use the same reward system as the 8 input case shown above.
I am therefore trying to understand what is happening here. Am I missing information? I would think the full 10x10 state space would give me exactly enough information to lead to the correct evaluation of the next path. I would very much like to hear someone elses input to this approach.
Thanks a lot
","['neural-networks', 'python', 'q-learning']","
Before we start to tweak with you Agent-Environment setup, there are couple of important things to note

Q-Learning
Q-learning as a fundamental is a greedy approach based on Action Value functions. When I say action value what I mean to say is this
$q_\pi(s,a) = p(\hat{s},r | s, a)[r + \gamma(V(\hat{s}))]$. 
This means according to $\pi$ policy the Action value for taking action ""$a$"" at state ""$s$"" is the probability of taking reaching state ""$\hat{s}$"" and getting a reward ""$r$"" given the current state is ""$s$"" and action taken is ""$a$"" multiplied by sum of immediate reward ""$r$"" and expected future reward given by future state value(s) ""$V(\hat{s})$"". 
Please note the $p(\hat{s},r | s, a)$ is only available in deterministic modelling technique (model based methods) and this part is not available for stochastic modelling techniques (model-free methods). For model-free methods we execute multiple experiments and take the expected value in accordance with Law of Large Numbers. Which is what we do in common Deep Reinforced Learning implementations.
The above formulation for future rewards can be expanded in series using the state-value formulation as $V(s) = \pi(a,s)q_\pi(s,a)$, where $\pi$ is the policy we are following aka probability of taking action $a$ when in state $s$.  
Why is it termed Greedy?
This is because while using the fundamental of multiple experiments, with every action I take the immediate reward, and the max q_value ($q$) associated with the next_state to update my current q_value.

In Q-learning the future state reward is what you calculate by submitting the next_state to the network and taking max of the q_values attained. (Hence, Greedy!!)
Crux of the above
Having said all the above (Duh!), what is in it for us?
In Q-Learning, your reward is key for your model learn, i.e. ""$r$"" immediate reward and the future reward ($V(\hat{s})$)

When the state-space is limited
For example, a numerical tic-tac-toe game, in such cases it is ideal to have discrete reward system, such as the one suggested by you. 
When the state-space is very large (the problem statement you are tackling)
In such cases, it is ideal to have a continuous reward mechanism, i.e. rewards should be more variable in nature, like continuous values.
Suggestions

You can define the reward function in the environment has the 10$\sqrt{2}$ - Euclidean distance between the face of your snake and the fruit it is trying to reach. The reason I am doing it (10$\sqrt{2}$ minus) is because the maximum distance could be 10$\sqrt{2}$ (considering it is 10X10 board). This would help you to have a more variable/continuous rewards for your model to learn with (since you are using neural networks for training them). As your Snake goes closer to the fruit this value will keep on increasing. One cavet, if by taking an action, your snake reaches fruit, you can provide it a specific constant (extra) reward, and if it hits its own self, or the borders you penalize it with a specific constant. So that the model understand better.
You can define your state (input vector to your NN) as 100 dimension array (10X10 grid) with each element signifying if there a object in the that area or not. 1 means it is fruit, 0 means it is snakes own body and 0.5 means it is free for movement. The idea is to limit the input values of the vectors between 0~1.  As your agent takes action, the values in the state will keep on changing.
The output of your network should have 3 neurons, each signifying the $q$ of each of action (front/right/left movement). For get_action function you will get the action pertaining to max(q_value), and keep on adding things in your memory d_que. Subsequently you can trigger model_train methods at every episode/or even at after every step.
Lastly, the hyper parameters would be the discount factor $\gamma$ you choose and the learning rate $\alpha$ you use for the NN network. That's something you will have to experiment. Along the depth of your ANN. I think a single hidden layer should be enough.
For the last layer of your NN, please use a linear activation function, and not a relu activation function.
Please run many-many-many experiments, something in millions for optimum training. There could be multiple ways of monitoring your model's learning process. How many games won, how much fruit eating it is able to achieve in 1 game etc.


Hope the above helps.
"
High variance in performance of q-learning agents trained with same parameters,"
I am training an agent to play a simple game using double deep q learning. However, the variance in agent performance is very high, even for agents trained with same model parameters. For example, I can train agent A and agent B using the exact parameters and agent A beats B 800 to 200.
I think I understand why this is happening, when training starts the model is initialized with different weights, and this leads the model to find different local max/min. 
The above makes it difficult to find optimal parameters.
What are the strategies to reduce this variance? What parameters should I look at tweaking?
More details about the environment:
This is a two player game (Zombie Dice); however, in my implementation so far the agents are learning to maximize expected score on their turn, so the actions and score of the opponent is ignored.
The variance is higher when I am using purely greedy strategy with no exploitation at all. Though it exists in both cases. I would say roughly 2/3 wins for stronger side with greedy and 3/5 with exploration out of 1000 matches.
The environment is stochastic; I have not done many assessment runs maybe 20 or 30, it is mostly eyeballing, but the differences are fairly large; therefore, I am confident that this is not due to chance.
I tested the models against themselves, and I get scores very close to 50/50. However, two different models trained with same parameters give results very different from 50/50. I tested this with models trained with different types of parameters and it is generally the same problem.
","['deep-learning', 'reinforcement-learning', 'q-learning']",
Are there reinforcement learning algorithms that ensure convergence for continuous state space problems?,"
The Q-learning does not guarantee convergence for continuous state space problems (Why doesn't Q-learning converge when using function approximation?). In that case, is there an algorithm which can guarantee convergence? 
I am looking at model-based RL specifically iLQR but all the solutions I find are for the continuous action space problem.
","['reinforcement-learning', 'convergence']",
What is the difference between a stochastic and a deterministic policy?,"
In reinforcement learning, there are the concepts of stochastic (or probabilistic) and deterministic policies. What is the difference between them?
","['reinforcement-learning', 'comparison', 'policies', 'deterministic-policy', 'stochastic-policy']","
Deterministic Policy  :
Its means that for every state you have clear defined action you will take
For Example: We 100% know we will take action A from state X.
Stochastic Policy :
Its mean that for every state you do not have clear defined action to take but you have probability distribution for actions to take from that state.
For example there are 10% chance of taking action A from state S, There are 20% chance of taking B from State S and there are 70% chance of taking action C from state S, Its mean we don't have clear defined action to take but we have some probability of taking actions.
"
String matching algorithm for product recognition,"
I'm building a web application that collects schema.org data from different webshops as Amazon, Shopify, etc. It collects data every 6h and shows the current and lowest price. It is used for monitoring products and buying at the lowest price.
My goal is to recognize products from different shops as the same product. Every shop has its own title for the same product. 
Example: 
Google Pixel 2 64GB Clearly White (Unlocked) Smartphone 
Google Pixel 2 GSM/CDMA Google Unlocked (Clearly White, 64GB, US warranty) 

Problems:

don't have a lot of data (only products chosen by the user)
needs to support every new product that app doesn't have data history

","['machine-learning', 'datasets', 'similarity']",
How do I solve the problem of positioning 11 pieces into a 8x8 puzzle?,"
I was trying to figure out how to create a solver to the puzzle of putting 11 pieces in a board (8 x 8). I created the game in http://www.xams.com.br/quebra. It is possible to turn the piece 90 degrees each time counterclockwise (Girar) and mirror it vertically (Inverter), and so the piece can assume 8 forms.
When clicking in solver button (Resolver), it tries to put pieces randomly in board in brutal force method (it spends a LOT of time). Using this method, I was not able to achieve the result.
I would like to try something smarter than this and having a machine learning algorithm for this would be great. I dont know how to formulate the problem. How would you start this please?

EDIT:
I am still building the page and there is so much to improve. You need to click the center of 3x3 matrix where you want to put the piece.

EDIT2:

","['machine-learning', 'ai-design', 'game-ai', 'combinatorial-games']","
Using your app, I was able to find a (spoiler alert!) solution manually. At least now you know your puzzle is solvable and you did not waste your money :)
It seems your app has a bug, though. I was unable to put the last piece, as shown in the picture. I was wondering if your solver, as it stands, will ever find a solution. 
Now the idea. It may be useful for your solver.
The board has 8x8=64 squares. Each piece will occupy 5 squares and you want to fit 11 pieces, so the final position will have 9 empty squares. Divide the board in two 8x4 rectangles, left and right. Now, it seems only fair that one rectangle should have 5 empty squares leaving the other with 4; with that in mind I've proceeded to fill the right part first and only then the left. After some trial and error, I got lucky.
I don't know if you can reach all solutions with this method. Notice that, in the solution given above, the bottom rectangle will end up with 6 empty squares. 
I don't know how to write an efficient solver either. For starters:

Build up a list of bad configurations: small sets of pieces/positions such that whenever you reach them, you know there is no hope.
At each step, put a piece in such way it minimizes the number of forced empty squares. 
A variant of the idea above: divide the board in four 4x4 parts; it seems reasonable each part should have at least one empty square but no so many; so, look for solutions forcing these parts to have (1,2,3,3) or (2,2,2,3) empty squares. 
Lookup whether this puzzle is known. Names that come to mind: Martin Gardner, Ian Stewart, Sam Lloyd. 

Can't say you will ever be able to see a list of all possible solutions. 
Nice puzzle, nice app that one you wrote, I've had a good time. Thank you.
"
Understanding the update rule for the policy in the policy iteration algorithm,"
Consider the grid world problem in RL. Formally, policy in RL is defined as $\pi(a|s)$. If we are solving grid world by policy iteration then the following pseudocode is used:

My question is related to the policy improvement step. Specifically, I am trying to understand the following update rule.
$$\pi(s) \leftarrow arg max_a \sum_{s'}p(s'|s,a)[r(s,a,s') + \gamma v(s')] $$
I can have 2 interpretations of this update rule.

In this step, we check which action (say, going right for a particular state) has the highest reward and assign going right a probability of 1 and rest actions a probability of 0. Thus, in PE step, we will always go right for all iterations for that state even if it might not be the most rewarding function after certain iterations.
We keep the policy improvement step in mind, and, while doing the PE step, we update the $v(s)$, based on the action giving highest reward (say, for 1st iteration $k=0$, going right gives the highest reward, we update based on that, while, for $k=1$, we see going left gives the highest reward, and update our value based on that likewise. Thus action changes depending on maximum reward).

For me, the second interpretation is very similar to value iteration. So, which one is the correct interpretation of a policy iteration?
","['reinforcement-learning', 'value-iteration', 'policy-iteration', 'policy-improvement']",
"In Q-learning, shouldn't the learning rate change dynamically during the learning phase?","
I have the following code (below), where an agent uses Q-learning (RL) to play a simple game.
What appears to be questionable for me in that code is the fixed learning rate. When it's set low, it's always favouring the old Q-value over the learnt/new Q-value (which is the case in this code example), and, vice-versa, when it's set high.
My thinking was: shouldn't the learning rate be dynamic, i.e. it should start high because at the beginning we don't have any values in the Q-table and the agent is simply choosing the best actions it encounters? So, we should be favouring the new Q-values over the existing ones (in the Q-table, in which there's no values, just zeros at the start). Over time (say every n number of episodes), ideally we decrease the learning rate to reflect that, over time, the values in the Q-table are getting more and more accurate (with the help of the Bellman equation to update the values in the Q-table). So, lowering the learning rate will start to favour the existing value in the Q-table over the new ones. I'm not sure if my logic has gaps and flaws, but I'm putting it out there in the community to get feedback from experienced/experts opinions.
Just to make things easier, the line to refer to, in the code below (for updating the Q-value using the learning rate) is under the comment: # Update Q-table for Q(s,a) with learning rate
import numpy as np
import gym
import random
import time
from IPython.display import clear_output

env = gym.make(""FrozenLake-v0"")

action_space_size = env.action_space.n
state_space_size = env.observation_space.n

q_table = np.zeros((state_space_size, action_space_size))

num_episodes = 10000
max_steps_per_episode = 100

learning_rate = 0.1
discount_rate = 0.99

exploration_rate = 1
max_exploration_rate = 1
min_exploration_rate = 0.01
exploration_decay_rate = 0.001


rewards_all_episodes = []

for episode in range(num_episodes):
    # initialize new episode params
    state = env.reset()
    
    done = False 
    rewards_current_episode = 0 
    
    for step in range(max_steps_per_episode):
        
        # Exploration-exploitation trade-off
        exploration_rate_threshold = random.uniform(0, 1)
        if exploration_rate_threshold > exploration_rate:
            action = np.argmax(q_table[state,:])
        else:
            action = env.action_space.sample()
            
        new_state, reward, done, info = env.step(action)
        
        # Update Q-table for Q(s,a) with learning rate
        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \
            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))
        
        state = new_state
        rewards_current_episode += reward
        
        if done == True:
            break
        
        
    # Exploration rate decay
    exploration_rate = min_exploration_rate + \
        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)
    
    rewards_all_episodes.append(rewards_current_episode)

# Calculate and print the average rewards per thousand episodes
rewards_per_thousands_episodes = np.array_split(np.array(rewards_all_episodes), num_episodes/1000)
count = 1000
print(""******* Average reward per thousands episodes ************"")
for r in rewards_per_thousands_episodes:
    print(count, "": "", str(sum(r/1000)))
    count += 1000
    
# Print updated Q-table
print(""\n\n********* Q-table *************\n"")
print(q_table)

","['reinforcement-learning', 'python', 'q-learning', 'hyperparameter-optimization', 'learning-rate']",
What is the relation between semi-supervised and self-supervised visual representation learning?,"
What's the differences between semi-supervised learning and self-supervised visual representation learning, and how they are connected? 
","['machine-learning', 'comparison', 'supervised-learning', 'self-supervised-learning', 'semi-supervised-learning']","
Both semi-supervised and self-supervised methods are similar in the sense that the goal is to learn with fewer labels per class. The way both formulate this is quite different:  

Self-Supervised Learning: 

This line of work aims to learn image representations without requiring human-annotated labels and then use those learned representations on some downstream tasks. For example, you could take millions of unlabeled images, randomly rotate them by either 0, 90, 180 or 270 degrees and then train a model to predict the rotation angle. Once the model is trained, you can use transfer learning to fine-tune this model on a downstream task like cat/dog classification just like how you finetune ImageNet pretrained models. You can view an overview of the methods and also look at contrastive learning methods that are currently giving state-of-the-art results such as SimCLR and PIRL.


Semi-supervised Learning 

Different from self-supervised learning, semi-supervised learning aims to use both labeled and unlabeled data at the same time to improve the performance of a supervised model. An example of this is FixMatch paper where you train your model on labeled images. Then, for your unlabeled images, you apply augmentations to create two images for each unlabeled image. Now, we want to ensure that the model predicts the same label for both the augmentations of the unlabeled images. This can be incorporated into the loss as a cross-entropy loss. 

"
How do we define the reward function for an environment?,"
How do you actually decide what reward value to give for each action in a given state for an environment?
Is this purely experimental and down to the programmer of the environment? So, is it a heuristic approach of simply trying different reward values and see how the learning process shapes up?
Of course, I understand that the reward values have to make sense, and not just put completely random values, i.e. if the agent makes mistakes then deduct points, etc.
So, am I right in saying it's just about trying different reward values for actions encoded in the environment and see how it affects the learning?
","['reinforcement-learning', 'reward-functions', 'reward-design', 'reward-shaping']","
Yes, you are right. It is somehow an arbitrary choice, although you should consider the reasonable numerical ranges of your activation functions if you decide to go beyond the values +/- 1.
You can also have a think about whether you want to add a small reward for the agent reaching states that are near the goal, if you have an environment where such states are discernable.
If you want to have a more machine learning approach to reward values, consider using an Actor-Critic arrangement, in which a second network learns reward values for non-goal states (by observing the results of agent exploration), although you still need to determine end-state values according to your handcrafted heuristic.
"
Training actor-critic algorithms in games with opponents,"
I am wondering how am I supposed to train a model using actor/critic algorithms in environments with opponents. I tried the followings (using A3C and DDPG):

Play against random player. I had rather good results, but not as good as expected since most interesting states cannot be reached with a random opponent.
Play against list of specific AIs. Results were excellent against those AIs, but very bad with never seen opponents
Play against itself. Seemed the best to me, but I could not get any convergence due to non-stationary environment. 

Any thought or advice about this would be very welcome.
","['neural-networks', 'deep-learning', 'reinforcement-learning', 'ddpg']","
The specific approaches you mentioned (A3C, DDPG), and usually also other Actor-Critic methods in general, are approaches for the standard single-agent Reinforcement Learning (RL) setting. When trying to apply such algorithms to settings that are actually multi-agent settings, it is indeed common to encounter the problems you describe. They can all be viewed as your agent ""overfitting"" to the opponent they're training against:


Play against random player. I had rather good results, but not as good as expected since most interesting states cannot be reached with a random opponent.


In this case, your agent will ""overfit"" against random opponents. It will likely become capable of beating them relatively easily, and to some extent this may also still translate to relatively weak non-random agents (like simple heuristic approaches), but it is unlikely to generalise to strong opponents.


Play against list of specific AIs. Results were excellent against those AIs, but very bad with never seen opponents


Here you pretty much exactly give a description of what I mean when I say ""overfitting"" against the opponent you're training against.


Play against itself. Seemed the best to me, but I could not get any convergence due to non-stationary environment. 


When training using self-play like this, there is still a danger of overfitting against ""itself"". This may turn out to work well in some games, but in other games, there are indeed risk of instability / lack of convergence. This can, for example, be due to the agent ""rotating"" through a circle of strategies that beat each other. Intuitively, you could think of Rock-Paper-Scissors. A randomly-initialised agent may, for example, have a slight tendency to play Rock more often than the others. Self-play will then lead to a strategy that primarily plays Paper, to beat the current ""Rock"" strategy. Continued self-play training can then figure out that a tendency to play more Scissors will be strong against itself. Etc., these strategies can keep rotating forever.

The standard approach for self-play training in games is the one popularised by AlphaGo Zero and AlphaZero, and also around the same time independently published with results on the game of Hex. That last paper coined the term ""Expert Iteration"" for the approach, which I like to use. The basic idea of Expert Iteration is to have:

A policy that we're training (parameterised by some function approximator like a Deep Neural Net)
A tree search algorithm, like Monte Carlo tree search (MCTS).

These two components can then iteratively improve each other. The trained policy can be used to guide the search behaviour of MCTS. MCTS can be used to generate a new distribution over actions which, thanks to additional search effort, is typically expected to be a little bit better than then trained policy. That slightly better distribution is then used as a training target for the policy, using a Cross-Entropy loss. This means that the policy is trained to mimic the MCTS search behaviour, and also used to improve that same MCTS search behaviour.
This training procedure has been found to lead to strong results. One of the prevailing hypotheses is that the additional search performed by MCTS can help to stabilise training, and avoid overfitting to the ""self"" opponent; the search algorithm can actually ""reason"" about what a strong opponent (not necessarily the ""self"" agent) could do.

The Expert Iteration training procedure I described above often works well, but it doesn't really answer your question about training Actor-Critic approaches in games with opponents... because it's not really an Actor-Critic algorithm!
In Learning Policies from Self-Play with Policy Gradients and MCTS Value Estimates, we (I'm first author on this paper) propose an approach where the policy in self-play is trained using a training target based on the value estimates of MCTS, rather than the visit counts of MCTS (which is what we use in the standard Expert Iteration framework). 
I don't know if it necessarily should exactly be called an Actor-Critic algorithm, but you could intuitively view it as an ""Actor-Critic"" approach where MCTS is the critic. As mentioned right before Section IV, the resulting gradient estimator also turns out to be very similar to that of the ""Mean Actor Critic"".

If Expert-Iteration-style solutions are not an option (for example because tree search is infeasible), I would suggest taking a look at some of the following Policy-Gradient-based approaches (and possibly later research that cites these papers):

Learning with Opponent-Learning Awareness
Policy Gradient Search: Online Planning and Expert Iteration without Search Trees

"
Can Q-learning be used for continuous (state or action) spaces?,"
Many examples work with a table-based method for Q-learning. This may be suitable for a discrete state (observation) or action space, like a robot in a grid world, but is there a way to use Q-learning for continuous spaces like the control of a pendulum?
","['reinforcement-learning', 'q-learning', 'dqn', 'continuous-action-spaces', 'continuous-state-spaces']","
Q-learning for continuous state spaces
Yes, this is possible, provided you use some mechanism of approximation. One approach is to discretise the state space, and that doesn't have to reduce the space to a small number of states. Provided you can sample and update enough times, then a few million states is not a major problem. 
However, with large state spaces it is more common to use some form of function approximation for the action value. This is often noted $\hat{q}(s,a,\theta)$ to show that it is both an estimate (the circumflex over $\hat{q}$) and that you are learning some function parameters ($\theta$). There are broadly two popular approaches to Q-learning using function approximation:

Linear function approximation over a processed version of the state into features. A lot of variations to generate features have been proposed and tested, including Fourier series, tile coding, radial basic functions. The advantage of these methods are that they are simple, and more robust than non-linear function approximations. Which one to choose depends on what you state space represents and how the value function is likely to vary depending on location within the state space.
Neural network function approximation. This is essentially what Deep Q Networks (DQN) are. Provided you have a Markov state description, you scale it to work sensibly with neural networks, and you follow other DQN best practices (experience replay table, slow changing target network) this can work well.

Q-learning for continuous action spaces
Unless you discretise the action space, then this becomes very unwieldy.
The problem is that, given $s,a,r,s'$, Q-learning needs to evaluate the TD target:
$$Q_{target}(s,a) = r + \gamma \text{max}_{a'} \hat{q}(s',a',\theta)$$
The process for evaluating the maximum becomes less efficient and less accurate the larger the space that it needs to check.
For somewhat large action spaces, using double Q-learning can help (with two estimates of Q, one to pick the target action, the other to estimate its value, which you alternate between on different steps) - this helps avoid maximisation bias where picking an action because it has the highest value and then using that highest value in calculations leads to over-estimating value.
For very large or continuous action spaces, it is not usually practical to check all values. The alternative to Q-learning in this case is to use a policy gradient method such as Actor-Critic which can cope with very large or continuous action spaces, and does not rely on maximising over all possible actions in order to enact or evaluate a policy.
Controlling a pendulum
For a discrete action space e.g. applying one of a choice of forces on each time step, then this can be done using a DQN approach or any other function approximation. The classic example here might be an environment like Open AI's CartPole-v1 where the state space is continuous, but there are only two possible actions. This can be solved easily using DQN, it is something of a beginner's problem.
Adding continuous action space ends up with something like the Pendulum-v0 environment. This can be solved to some degree using DQN and discretising the action space (to e.g. 9 different actions). However, it is possible to make more optimal solutions using an Actor-Critic algorithm like A3C.
"
Possible inconsistency in the Policy Improvement equation,"
I came across this formula in Sutton And Barto: RL an Intro (2nd Edition) equation number 4.7 (page number 78).
If $\pi$ and $\pi'$ are deterministic policies and $q_\pi(s, \pi'(s)) \geq v_\pi(s)$ then the policy $\pi'$ is as good or better than $\pi$.
NOTE on Convention: As per the convention of the book goes, I think they are using rewards for state to action to state transition sequence rather than state to action transition.
My questions are:

Why are they comparing state value function to action value function?
Isn't it obvious the above equation might hold true (provided we select the best action among the possible actions) even for the policy $\pi$ since then the equation will change to $q_\pi(s, \pi(s)) \geq v_\pi(s)$ and we know $v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a|s)q_\pi(s, a)$?

What is the inconsistency here?
","['reinforcement-learning', 'sutton-barto', 'policies']",
How to locate the invoice within a camera captured image?,"
The demand is to locate the invoice within a camera captured image about that invoice. The invoice is always a white paper with printed black or blue characters, tables and red stamps. Sometimes the background behind the invoice is dark(about 60% of all the samples), but others are not. Sometimes there is shadow on it.
The question is how to detect the vertices, edges and corners of the invoice in this image? What algorithms should be applied? The android application CamScanner seems to have this function but not is effective every time. So does the CIDetector interface on iOS. What algorithms does CamScanner or iOS use(traditional or deep)?
",['image-recognition'],
What is the reward system of reinforcement learning?,"
Can you describe this reward system in more detail?
I understand that the environment sends a signal indicating whether or not the action taken by the agent was 'good' or not, but it seems too simple.
Basically, can you detail the nitty-gritty workings of this system? I dunno, I may just be overthinking things.
","['reinforcement-learning', 'markov-decision-process', 'rewards', 'reward-functions']",
Learning Features from a Pre-trained Network,"
I am currently working on learning the features provided by a pre-trained network for image retrieval. Currently I take the features provided by the pre-trained network, use global max pooling to essentially provide me with a vector and then use fully connected layers to learn the feature vector. This has provided good results, although prone to over-fitting, particularly without dropout. 
Is it possible/would it be beneficial to use a 1D convolutional layer instead of the fully connected layers to learn the features? Bearing in mind this is essentially still image data that has just been transformed. 
model.add(GlobalAveragePooling2D(input_shape=input_shape))
model.add(Dense(256, activation=""relu""))
model.add(Dropout(0.2))
model.add(Dense(256, activation=""relu""))

I'm not sure how to try this practically in Keras as 1D convolutional layers only seem to accept a 3 dimensional input tensor.
Any suggestions welcome!
","['convolutional-neural-networks', 'feature-extraction']",
Can we use Autoencoders for unsupervised CNN feature learning?,"
I searched through the internet but couldn't find a reliable article that answers this question.
Can we use Autoencoders for unsupervised CNN feature learning of unlabeled images like the below

and use the encoder part of the Auto-encoder for Transfer learning of few labeled images from the dataset? as shown below.

I believe this will reduce the labeling work and increase the accuracy of a model.
However, I have concerns like the more cost in computing, failing to learn all required features, etc..
Please let me know if any employed this method in large scale learning such as image-net.
PS: Pardon if it is Trivial or Vague as I am new to the field of AI and computer vision.
","['convolutional-neural-networks', 'computer-vision', 'autoencoders', 'feature-extraction']",
Convolutional Sequence to Sequence Learning kernel parameters,"
I am reading the paper Convolutional Sequence to Sequence Learning by Facebook AI researchers and having trouble to understand how the dimensions of convolutional filters work here. Please take a look at the relevant part of the paper below. 

Let's say the input to the kernel X is k*d (say k=5 words of d=300 embedding dimenisonality). Therefore the input is 5*300. In a computer vision task a kernel would slide over parts of the image, in NLP you usually see kernel taking up the whole width of the input matrix. So I would expect kernel to be m*d (e.g. 3*300 - slide over 3 words and look at their whole embeddiings).
However, the kernel here is of dimensionality 2d x kd which in our hypothetical example would be 600*1500. I don't understand how this massive kernel would slide over an input that is by far lower dimensional (5*300). In computer vision you could zero-pad the input, but here zero-padding would basically turn the input matrix into mostly zeros with only a handful of meaningful numbers.
Thanks for shedding some light on it!
","['convolutional-neural-networks', 'natural-language-processing', 'machine-translation']","
They are doing a matrix multiplication: consider $y = Ax, y \in \mathbb{R}^m, x \in \mathbb{R}^n, A \in M_\mathbb{R}(m,n)$. In the paper $x$ is a concatenation of $k$ elements of $\mathbb{R}^d$, so $x$ is long $kd$; $y$ is long $2d$.
"
Which machine learning models are universal function approximators?,"
The universal approximation theorem states that a feed-forward neural network  with a single hidden layer containing a finite number of neurons can approximate any continuous function (provided some assumptions on the activation function are met).
Is there any other machine learning model (apart from any neural network model) that has been proved to be an universal function approximator (and that is potentially comparable to neural networks, in terms of usefulness and applicability)? If yes, can you provide a link to a research paper or book that shows the proof?
Similar questions have been asked in the past in other places (e.g. here, here and here), but they do not provide links to papers or books that show the proofs.
","['neural-networks', 'machine-learning', 'reference-request', 'function-approximation', 'universal-approximation-theorems']",
Difference between retraining on different portions of data and training initially on larger data set,"
I have a large data set that doesn't fit in memory and would have to use something like Keras's model.fit_generator if I would like to train the model on all of the available data. The problem is that my data load time is larger than a single epoch and I would hate to incur that data load cost for each epoch.
The alternative approach that yields some value is to load as much data as possible, train the model for a few hundred epochs, then load the next portion of the data and reiterate for the same amount of epochs. And repeat this until all my data is ""seen"" by the model. 
Intuitively I understand that this is sub-optimal as the model will tend to optimize for the latest portion of the data and ""forget"" the previous data but I would like a more in-depth explanation of the downsides of that method and if there are any ways to overcome them.
","['training', 'datasets', 'keras']",
Why is it so common to initialize weights with a Guassian distribution divided by the square root of number of neurons in a layer?,"
I have seen in several jupyter notebooks people initializing the NN weights using:
np.random.randn(D, M) / np.sqrt(D)

Other times they just do:
np.random.randn(D, M)

What is the advantage of dividing the Gaussian distribution by the squared root of the number of neurons in the layer?
Thanks
",['neural-networks'],
Would this NN for my chip outputs work?,"
I'm a grad student from EE.
So, basically, there's an electrical circuit that is supposed to output ""0"" or ""1"" by exactly 50 to 50 chance. It generates a number of big arrays of 0s and 1s, each of which amounts to more than 4,000 of the numbers.
But because these arrays are physically generated in a fab, I assume it might develop some dependencies among numbers and some output could be predicted by more than 50% chance. For example, due to some variations in the process, ""1"" can be more likely to come than ""0"" after a sequence of ""001100"".
Then let's say I make a simple deep neural network which takes 7 inputs and gives 1 output. I simply slice my array by 8 numbers, 7 of which are given to the input and the last one is used as a label (the true answer). I train my simple DNN using all these sliced numbers and it will learn some sequences. Finally, I apply my NN to a test set, and if it predicts the next number with an accuracy of more than 50%, that proves my assumption, and if it doesn't that is also good for me because it says my circuits are good.
Would it work?
","['neural-networks', 'deep-learning', 'classification']",
"If the average rewards start high and then decrease, could that indicate that the PPO is stuck at a local maximum?","
I'm trying to train a PPO agent in a 3D balance ball environment. My action space is continuous.
In the following graph, each dot shows the average reward from 100 episodes.

Could this graph indicate that it's stuck at a local maximum?  Do I need to promote exploring by increasing the entropy, or does this look like a bug with my implementation?
I am trying to maximize the average rewards.
When I look at the loss, it does seem like it is minimizing the cost.  I thought that the loss function is dependent on the rewards because the advantage is calculated based on them.  The normalized advantage is then factored into the policy loss, which should say which direction for the policy to step toward.
It seems like the actions are a bit noisy but the platform sometimes seems to try to keep the ball from falling off.
Hyperparameters:

Learning rate = 0.01
Entropy Coefficient = 0.01
Value Function Loss Coefficient = 0.5
Gamma/Discount Factor = 0.995
MiniBatch Size = 512
Epochs = 3
Clip Epsilon = 0.1

","['reinforcement-learning', 'policy-gradients', 'proximal-policy-optimization', 'continuous-action-spaces']","
I had the same problem where the reward kept decreasing and started to search for answers in the forum.
I let the model trained while I search. As the model trained, the reward started to increase. You can see the tensorboard graph for rewards in validation time.
. 
The fall continued until around 100k~ steps and did not change a lot for 250k~ steps. 
After 350k~ th step, it slowly started to increase. Without knowing your number of steps trained, I would suggest training for more steps.
Also, I read about this (Reward first decreasing and then increasing) in an RL paper, if I find it I will mention it here.
"
How to do speech recognition on a single word,"
I would provide a sound signal of about 2-3 seconds to my neural network. I have trained my network with a single word, like if I speak ""Hello"" the network may tell if ""Hello"" is spoken or not, but some other word like ""World"" is spoken, it will say ""Hello"" is not spoken. I just want classification of sound if its a specific command or word.
What is the best way to do this, I am not a that much advanced in DNN, I only know about NN and CNN, I want to know if there is some research paper or tutorial, or need some explanation about the work.
","['neural-networks', 'deep-learning', 'voice-recognition']",
What is the relationship between MLE and naive Bayes?,"
I have found various references describing Naive Bayes and they all demonstrated that it used MLE for the calculation. However, this is my understanding:
$P(y=c|x)$ $\propto$ $P(x|y=c)P(y=c)$
with $c$ is the class the model may classify $y$ as.
And that's all, we can infer $P(x|y=c)$ and $P(c)$ from the data. I don't see where the MLE shows its role.
","['comparison', 'naive-bayes', 'maximum-likelihood']","

And that's all, we can infer P(x|y=c) and P(c) from the data. I don't see where the MLE shows its role.

Maximum likelihood estimate is used for this very purpose, i.e. to estimate the conditional probability $p(x_j \mid y)$ and marginal probability $p(y)$ .

In Naive Bayes Algorithm ,using the properties of conditional probability, we can estimate the joint probability
$$
p(y, x_1, x_2, \dots, x_n) =  p(x_1, x_2, \dots, x_n \mid y) \; p(y)
$$
which is the same as
$$
p(y=c|x)\propto p(x|y=c)p(y=c)
$$
What we need to estimate in here, are the conditional $p(x_j \mid y)$ and marginal $p(y)$ probabilities, and we use maximum likelihood for this. 
To be more precise: Assume that the $x$ is a feature vector $(x_1, x_2, ...x_n)$ - which means nothing more than the data point is a vector. In Naive Bayes Classification the assumption is that these features are independent of each other conditional on the class. So, in that case this term 
$$p(x|y=c)$$ is replaced by $$\prod_{i=1}^{n} p(x_i|y=c)$$ -- this is because of the independence assumption that lets us simply multiply the probabilities. 
The problem of estimating the class to which the data point belongs reduces to the problem of maximizing the  likelihood $$\prod_{i=1}^{n} p(x_i|y=c)$$ -- which means assigning the data $(x_1, x_2, ...x_n)$  to the class $k$ for which this likelihood is highest. This will give the same result as MLE which takes the form  of maximizing this quantity -- $$\prod_{i=1}^{n} p(x_i|\theta)$$ where $\theta$ are the assumed parameters.
"
DQN not able to learn in a game where other agents perform random walks,"
I am making a school project where I should develop any kind of game where I can have one reactive agent and one agent based on machine learning competing with each other.
My game consists of a salesmen problem. Basically, I have 3 types of entities, consumers, salesmen, and hotspots. 
The consumers are represented by the person with a green background. There are 8 of them. They basically move around the whole game using random walks and they tend o aggregate on the HotSpots (the orange icon with the router in it).
The salesmen are represented by the person with the dark grey background. One of them is controlled by a reactive agent that has in it some rules that I programmed and the other one is controlled by my DQN model.
The salesmen have 5 available actions, move up, right, down, left or sell.
When they choose to sell the simulation will try to sell to the closest consumer in a predetermined max range. If no consumers exist in that range or if the consumer rejects to buy then the sell fails.
I started training a Deep Q Network that I built using TensorFlow. As input features, I am giving the agent current position, the position of each consumer and a boolean saying if the consumer was recently asked to buy or not (consumers that were asked to buy something will reject future offers for a determined amount of time with 100% probability).
For the output layer, I have 5 nodes, one for each available action.
Here is a screenshot of the game:
The red number on the right-bottom corner of each agent represents their total utility.

I decided to give the agents the following rewards:

SELL_SUCCESSED_REWARD = 3 - The agents receives 3 points for each success sell. 
SELL_FAILED_REWARD = -0.010 - The agent loses 0.01 points for each failed sell
MOVING_REWARD = -0.001 - The agent loses 0.001 points for each move
NOT_MOVING_REWARD = -0.0125 - The agent loses 0.0125 points for standing in the same position (ie. not moving or trying to move against a wall)

I started training my agent but I seems to do not learn anything! I left it training for around 3 hours and I could not see any improvement. I tried different activation functions, batch sizes, exploration rates etc but no noticiable effect.
My question is: Can a DQN learn in this type of enviroment where there are a lot of random walks?
If yes what could be my problem? Not enought training time? Bad input features? Bad implementation?
Here are the files with my implementation of DQN:
Agent: https://github.com/daniel3303/aasma-project/blob/master/src/Agent/DeepLearningAgent/DeepLearningAgent.py
Training: https://github.com/daniel3303/aasma-project/blob/master/train_dqn.py
Thanks.
","['neural-networks', 'machine-learning', 'deep-learning', 'reinforcement-learning', 'tensorflow']",
Creating a zero element in embedding space,"
I have some variable length input vectors for my own use case of a 'stylistic transfer'-esque process, and I am wondering if anyone knows of a way to engineer an input that maps to a 0 element in embedding space. This would be an element that simply holds space but would be readily overlaid with vector addition of another embedded input.
My rationale is that I could pad the inputs with these zero elements to mask what I don't care about and have a semantically meaningful vector addition in the embedding space.
I wonder if I could permute some training examples with a chosen value which all map to the same output and this would allow a neural net to learn such a feature.
","['neural-networks', 'deep-learning', 'word-embedding']",
DQN ANN input vs Linear function approximator feature vector,"
So when using semi-gradient td(0) you need to convert your state representation into a feature vector that represents the state and as far as I know, should not be correlated.
Is the input on the ANN of a DQN the same? should it be a feature vector that represents the state? What considerations should one have when creating such vector?
","['reinforcement-learning', 'dqn']",
DQN Agent helped by a controller: on which action should I perform backprop?,"
Background
I am working on a robotic arm controlled by a DQN + a python script I wrote.
The DQN receives the 5 joint states, the coordinates of a target, the coordinates of the obstacle and outputs the best action to take (in terms of joint rotations).
The python script checks if the action suggested by the DQN is safe. If it is, it performs it. Otherwise, it performs the second highest-ranking action from the DQN; and so on. If no action is possible, collision: we fail.
During training, this python functionality wasn't present: the arm learned how to behave without anything else to correct his behaviour. With this addition on the already-trained network, the performance raised from 78 to 95%.
Now my advisor (bachelor's thesis) asked me to leave the external controller on during training to check whether this improves learning.
Question
Here's what happens during training; at each step:

the ANN selects an action
if it is legal, the python script executes it, otherwise it chooses another one.

Now... On which action should I perform backprop? The one proposed by the arm or the one which was really executed? (So, which action should I perform backprop on?)
I am really confused. On the one hand, the arm did choose an action so my idea was that we should, in fact, learn on THAT action. On the other hand, during the exploration phase ($\epsilon$ greedy), we backprop on the action which was randomly selected and executed, with no interest on what was the output of the arm. So, it would be rational too, in this case, to perform backprop on the action really executed; so the one chosen by the script.
What is the right thing to do here?. (Bonus question: is it reasonable to train with this functionality on? Wouldn't it be better to train the Network by itself, and then later, enhance its performance with this functionality?)
",['reinforcement-learning'],
"How can $\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)$ be 1 for $S_{t}$ 's group's component and 0 for the other components?","
In Sutton's RL:An introduction 2nd edition it says the following(page 203):

State aggregation is a simple form of generalizing function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group. The value of a state is estimated as its group's component, and when the state is updated, that component alone is updated. State aggregation is a special case of SGD $(9.7)$ in which the gradient, $\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)$, is 1 for $S_{t}$ 's group's component and 0 for the other components.

and follows up with a theoretical example.
My question is, imagining my original state space is $[1,100000]$, why can't I just say that the new state space is $[1, 1000]$ where each of these numbers corresponds to an interval: so 1 to $[1,100]$, 2 to $[101,200]$, 3 to $[201,300]$, and so on, and then just apply the normal TD(0) formula, instead of using the weights?
My main problem with their approach is the last sentence:

in which the gradient, $\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)$, is 1 for $S_{t}$ 's group's component and 0 for the other components.

If $\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)$ is the linear combination of a feature vector and the weights (w), how does the gradient of that function can be 1 for a state and 0 for others? There are not as many w as states or groups of states.
Let's say that my feature vector is 5 numbers between 0 and 100. For example, $(55,23,11,44,99)$ for a specific state, how do you choose a specific group of states for state aggregation?
Maybe what I'm not understanding is the feature vector. If we have a state space that is $[1, 10000]$ as in the random walk, what can be the feature vector? Does it have the same size as the number of groups after state aggregation?
","['reinforcement-learning', 'function-approximation', 'features']",
What is the right formula for weight update rule in Logistic Regression using stochastic gradient descent,"
Apologies for the lengthy title.
My question is about the weight update rule for logistic regression using stochastic gradient descent.
I have just started experimenting on Logistic Regression. I came across two weight update expressions and did not know which one is more accurate and why they are different.
The first Method:
Source: (Book) Artificial Intelligence: A Modern Approach by Norvig, Russell
 on page 726-727:
using the L2 loss function:


where
g stands for the logistic function
g' stands for g's derivative
w stands for weight
hw(x) represents the logistic regression hypothesis
The other method:
Source (Paper authored by Charles Elkan): Logistic Regression and Stochastic Gradient Training.
can be found here

","['machine-learning', 'gradient-descent', 'logistic-regression']",
Can reinforcement learning be used for tasks where only one final reward is received?,"
Is reinforcement learning problem adaptable to the setting when there is only one - final - reward. I am aware of problems with sparse and delayed rewards, but what about only one reward and a quite long path?
","['reinforcement-learning', 'reward-design', 'sparse-rewards', 'credit-assignment-problem', 'delayed-rewards']",
How fast does Monte Carlo tree search converge?,"
How fast does Monte Carlo Tree Search converge? Is there a proof that it converges?
How does it compare to temporal-difference learning in terms of convergence speed (assuming the evaluation step is a bit slow)?
Is there a way to exploit the information gathered during the simulation phase to accelerate MCTS?
Sorry if too many questions, if you have to choose one, please choose the last question.
","['reinforcement-learning', 'comparison', 'monte-carlo-tree-search', 'convergence', 'temporal-difference-methods']",
"If I am interested in theoretical computer science, is AI a bad choice? [closed]","







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            



Closed 3 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I hope this question is ok here, but since I have found a tag which deals with these issues (profession), I'll ask away. I also hope this may be useful to other people with similar doubts, since I am failing to find valuable information on this topic online.

I am interested in the theoretical side of CS, such as computability, logic, complexity theory and formal methods. At the same time, I am deeply fascinated by Artificial Intelligence and the questions it poses to our understanding of the notion of intelligence and what does it mean to be a human being.
In general, is AI a more ""applied""/engineeristic field, or are there theoretical aspects to research in?
In short: If I prefer formal/theoretical compsci, is AI a bad career choice?
(note: I am asking this because I am a CS undergrad considering getting into a AI MSc).
",['profession'],"
There are certainly results in theoretical computer science / pure math with deep implications for AI. But to my knowledge these results typically aren't labeled as results of artificial intelligence, but as something more congruent in that particular field (For example in CS, we might say ""agent with unbounded computational power""; in math might say some statement is ""decidable/undecidable"" with respect to some system). Of course they still matter in the field of AI, but you need to know what you are looking for.
See my question What are some implications of Gdel's theorems on AI research? for some examples. Or you can look up MIRI's research guide for a better idea of what existing work is out there that links formal math / CS to AI research.
Another point to raise is that there is no good definition of AI in fields outside of normal discourse (or even within, perhaps), so its difficult to decide what discussions pertains to the study of AI. Questions like whether ZFC with/without choice is expressive enough might not be on the mind of most AI researchers, but could still have some implications.
So to answer you question more directly, there is certainly a field of study regarding theoretical AI. Regarding whether or not its a good choice is something for you to decide, but it is (in my humble and not-very-well-educated opinion) very difficult field that isn't very popular, and has not seen major progress in many years.
"
What is a temporal feature?,"
What is a temporal feature, what features make something temporal in nature? Is this problem agnostic? How does it change from different fields of study?
","['machine-learning', 'definitions', 'signal-processing', 'features']",
N fold Cross Validation,"
I want to make 8 fold cross validation from the dataset. The dataset is the musical onsets annotated which has txt format at each songs. 

How to make the another folder like called splits (the name of folder) which is contains 8 fold of songs. Each fold will be written as a txt format.


I need some code references to make 8 fold like that. Do I need use sklearn Kfold? And how to save each song like stated in the third picture?
Thank you.
",['machine-learning'],
what are the similarity measure use for both continuous and categorical data?,"
I have searched but found that some similarity measures are for continuous data and some are for categorical data. But i want to know the similarity measures which are use for both data, continuous and categorical?
","['machine-learning', 'search']","
Sometimes continuous data can be represented as parametric distribution with distribution parameters as variable, essentially continuous stochastic process. In that case cross-entropy would work on that type of continuous data.
"
Do you know any examples of geometric deep learning used in industry?,"
I'm interested in the industrial use of GDL (see https://arxiv.org/abs/1611.08097). Is it used in industry? That is, does any company have access to non-Euclidean data and process it directly instead of converting it to a more standard format?
","['geometric-deep-learning', 'applications']","
Recursive artificial networks are a type aligned well with GDL, and it is an increasingly popular one. Using non-orthogonal geometries is natural when using directed graphs as semantic association, data flow, or composition networks.
This more free-form approach enters into industry slowly because most programming languages use orthogonal structures designed around multidimensional arrays and nested loops to iterate through them, made popular in the FORTRAN era. It is common that a student learns to loop through an array before learning to call a function or test and branch. Computer science is, in some ways, entrenched in the orthogonal structure.
To call the two categories Euclidean and non-euclidean is inaccurate. The initial trend in machine learning followed a Cartesian paradigm, one locked into 90 angles in data structures. The free-form graph is non-cartesian but still just as Euclidean, as Euclid worked with angles other than 90 frequently.
Apple and Google are likely candidates for this seepage of recursive networks into industry data centers, for NLP applications. Language associations are free-form even though these structures are serialized for speech and written text.
Cognition is also non-cartesian. Graph based optimization (pre-GDL) has been part of cognitive research since the 1970s, especially in the LISP space at MIT and CMU and the U.S. Naval Research facilities for strategic analysis, Optimal strategies for a class of constrained sequential problems by JB Kadane, HA Simon, The Annals of Statistics, 1977. Algorithm 2 is a convergence strategy, but without layers of cells, yet the searching concepts of gradient are already present in the maximal and minimal operations in the search in steps c and f.
The aeronautics and auto industry is a likely candidates for seepage into the embedded computing for automated driving and flight. Most of this is either company confidential or classified.
This answer to the question about topological sophistication provides some background why non-euclidean space may be a more natural way to map associations, flow, and composition.
These are some practical applications.

Convolutional networks on graphs for learning molecular fingerprints by DK Duvenaud, D Maclaurin, J Iparraguirre, Advances in neural, 2015
CD-aware recursive neural networks for jet physics by Gilles Louppe, Kyunghyun Cho, Cyril Becot, Kyle Cranmer, 2019
Stability Properties of Graph Neural Networks by Fernando Gama, Joan Bruna, Alejandro Ribeiro
Isospectralization, or how to hear shape, style, and correspondence by Luca Cosmo, Mikhail Panine, Arianna Rampini, Maks Ovsjanikov, Michael M. Bronstein, Emanuele Rodol 
Fake News Detection on Social Media using Geometric Deep Learning, by Federico Monti, Fabrizio Frasca, Davide Eynard, Damon Mannion, Michael M. Bronstein

We've used recursive networks to align complex models with data acquired during laboratory experimentation and hope to publish on that approach as part of another publication. It is effective, since the relationships between features in the real world form squares and cubes only by chance, so it is not very common. Research into materials and energy cannot be confined by Cartesian conceptions, even if the graphs published are on Cartesian coordinate axes, so that others with traditional analytic geometry backgrounds can understand the graphs quickly.
"
Object IN/OUT counting using CNN+RNN,"
I am building a video analytics program for counting moving things in a video. I am detecting bicycles and nothing else. I run object detection using the SSD mobile-net model in all the frames and store the bounding box coordinates (x,y,w,h) of each detection to a CSV file.
So for a video, I have a CSV file of one row each for a frame and each row has multiple detections of D1, D2, D3,.., Dn. Each detection has the bounding box coordinates as values. D1 is x,y,w,h.
Based on the x,y values of each detection, I am trying to find the direction of the bicycles and if the bicycle crosses the whole frame to do a UP/DOWN count.
How do I count/track(Don't want to use classic tracking algos) these bounding boxes moving in the video?
I see LSTM/RNN coming up in my search results when I search for video analytics. Being a noob, I am not able to find any tutorial that suits my needs.
I would like to check if my approach towards the problem is correct.
I don't want to use the classical tracking solutions for two reasons

I feel the tracking and counting conditions that I program in Python is always leaky/fails in certain conditions, hence I want to see how AI manages to count the objects.
The video stream I am using has heavy distortion on the objects that I track, hence the shape and size of the object changes drastically within 10s/20s of frames.

Any help or suggestion towards other better approaches is much appreciated.
Edit 1: The area of view under the camera is fixed. And we expect the bicycles to move from one entry side. Lets assume that the view and entry/exit is like shown in this video https://www.youtube.com/watch?v=tW7Pl3bSzR4
","['computer-vision', 'long-short-term-memory']","
Assessing the Question

Based on the x,y values of each detection, I am trying to find the direction of the bicycles and if the bicycle crosses the whole frame to do a UP/DOWN count.

It appears from the question that there is an interest in both counting bicycles and determining the direction of travel of each. from within the frames of a video stream or file. We can assume that each bicycle has at least one rider. It also appears that it is not a problem involving a fixed optical system positioned to point at a path that is tangential to the optical path, which would make the problem much easier, locking the approximate distance of the bicycle wheels to the optical system to near constant.
The use of the SSD mobile-net model seems reasonable as a starting point for developing expertise.
Starting With ML Design Basics
Let's consider the purpose of CNN and RNN designs.

The purpose of a convolutional network is to deal equally with regions in a multi-dimensional array of values in discrete samples of \R^n during an adaptive (learning) process.
The purpose of a recurrent network is to adapt to (learn) potentially complex temporal (time-wise) trends in potentially complex nonlinear systems.

Understand that the SSD class of algorithms do not do what natural visual systems do. They do not zoom in and out on independent objects within the network seamlessly. They cannot note that a base ball player is running to first base and a ball is coming from the catcher at the same time, requiring independent conceptual zoom operations within the neural network. This cannot be done with a zoom lens. That is why the Director of Photography is such a key role in movie making. The visual data must contribute well to the story telling, using lighting, camera orientation, panning, zooming, and depth of focus.
Although one can create several bicycle concept classes to cover various bicycle sizes, orientations relative to the optics, and distances away, there are limitations to this approach, which can be diminished with circuit parallelism in hardware. Multi-threading and serial evaluation can, depending on resources and patience factors, increase training time beyond what is practical. The challenge is to create seamlessness between low level concept classes of a bicycle in a frame as the bicycle angle and distance changes relative to the optical path.
Deeper into Details
The, ""heavy distortion on the objects,"" could be a show-stopper if the root cause of the distortion mentioned is poor resolution in the time, horizontal, or vertical dimensions. The most significant and consistent image-oriented feature of a bicycle is two ellipses (not always circles) in close horizontal proximity and even closer vertical proximity  the two wheels. The wheels need to be recognizable.
Two general categories of networks were mentioned in the question, CNNs and RNNs, which, in general are the two most relevant overall categories of components in a visual system that recognizes motion. We have some nomenclature in the question, which begins the mathematical theory behind the design of the training of the networks and the real time requirements on those network components once the network components are trained.

... each ... frame ... has multiple detections of D1, D2, D3, ..., Dn

$$ D_i \land i \in {1, 2, ..., n} \\
\Downarrow \\
D_{xywh} $$
The above nomenclature presumably refers to a post-learning detection of concept classes $C_a, C_b, C_d$, where there is a many-to-one relationship between the above numeric indices for detections and these letter indices for the concepts of a bicycle to be recognized. Each concept class $C$ might correspond to a particular recognizable bicycle feature set given a particular range of distances to the optics and orientation of the wheels relative to the direction of light rays between the wheels and the camera. The designer, considering this correspondence cannot dismiss the turning of the front wheel. We cannot assume that the eccentricity of the visual representations of the two wheels will be the same, since the bicycle may be turning. Even in this more complex case, the ellipses are likely the two most differentiating features of bicycles in common scenes.
This may be a good time to point out that tricycle recognition may require the recognition of an entirely distinct set of concept classes.
Also notice that, if the optics (camera) is at a drastically different altitude than the wheels of the bicycles, such as images from a drone or a camera on a tall pole, the problem is a different one. This is the intensely effective quality of natural vision systems. Over millions of years, the kind of training that recognizes a bicycle from a drone video stream having only been trained to recognize a bicycle from ground level has emerged. Nature's ability to apply cognitive abilities to visual sequence recognition to use in trajectory prediction is not yet realized in software and hardware and the main problem in automated vehicle piloting and driving.
Two Different Output Requirements
There are two somewhat distinct problems that must be considered in analysis of the project requirements. The output could be either of these two, depending on whether counting bicycles is the primary goal or whether metrics of travel is its own independent objective.

Unit vector $\vec{r}$, presumably in $\mathbb{R}^2$ as a pixel vector, essentially a normalized vector of the first derivative of pixel position with respect to time. The center of the bike, in this case, would be based on the features of the bike in the field of view.
Unit vector $\vec{r}$, presumably in $\mathbb{R}^2$ as a geocoordinate unit vector, essentially a normalized vector of the first derivative of position with respect to time. The center of the bike, in this case, would be based on the features of the bike in geo-space without the altitude coordinate.

Approaches to AI Design
The common, but inefficient, artificial network approach is to locate the bicycle in each frame with a CNN and then use one of the progressive RNN types (either a GRU or a b-LSTM network) to recognize motion trends. One of the largest drawbacks is that you may have many concept classes that represent adjacent size-distance-orientation concepts (kernel based recognition models) of a bicycle to train into the CNN. If the bike is traveling toward or away from the optics at some angle, then the disappearance of the bike from $D_a$ and its appearance in $D_b$ needs to be construed as the contiguous motion of one bike. This is not an easy challenge but is heavily covered in the literature.
It is recommended to use web searches designed to search scholarly articles, not dummies guides, which are not reliable. There many academic publications that can be found with the search term, ""Image recognition changing distance orientation."" Looking at old articles from the 90s will provide a good historical context. Looking at new ones from the last three years will provide a survey of the current state of research.
Other Questions Within the Primary Question
The original types of recurrent networks are essentially for historical context. The dominant in-field recurrent network successes are often of the LSTM, b-LSTM, or GRU types.
The language (Scala, Python, Java, C, C++) is not particularly relevant if you are delegating the training to a GPU (which always runs C/C++ code),
so it may be unwise to consider reliability concerns as a primary driver for programming language selection.
Regarding, ""How AI manages to count the objects,"" AI doesn't  not at the current state of technology. There is no one approach or algorithm across AI technology that dominates over all other approaches for all domains, into which bicycles can be plugged in and it works.
Currently, the AI engineer designs how the objects will be counted based on the characteristics of the objects to be counted, the meta-features of the incoming stream or data set, and the specifics of the recognition challenge. This is again because the wider capabilities of natural vision systems using the more sophisticated neural nets in animals and people has not yet been invented.
Final Recommendations Regarding System Design
The division between the use of kernels, in the CNN context, and the use of one of the recurrent network types is critical. If the engineer tries to delegate too much to the kernel, the above issue of bicycle distance to the optics and turning corners is exacerbated, because kernel operations do not lend themselves well to orientation and distance complexity. However, the CNN approach is excellent for the most upstream operations, such as edge detection and primitive object detection.
Let the recurrent network (of the more advanced types mentioned above) detect the bicycles as their distance and orientation in relation to the optical path changes, unless you have a sizable GPU farm that will perform CNN operations covering many distance and orientation ranges in parallel. Even if you do or have the patience of a saint, it may be best to delegate total bicycle recognition to the recurrent network anyway, since this is likely closer to the way natural systems do it and the modeling of bicycle travel between distance and orientation categories can be made more naturally seamless.
Recapping the comment above in this context, the problem complexity would be much lower if the bicycles were on a path, could not turn corners, and must travel either right to left or left to right within a speed range governed by the flow of traffic.

Response to Comments
Regarding Edit 1,

Edit 1: The area of view under the camera is fixed. And we expect the bicycles to move from one entry side. Lets assume that the view and entry/exit is like shown in this video, 

YouTube.com reports, ""This video does not exist."" That the camera is fixed, had been assumed in the writing of this answer because no camera trajectory described in the question. Had the expectation that bicycles will move from one entry side of the frame been included in the question, the answer would have addressed that case, but there was no hint of that requirement prior to Edit 1. Nonetheless, much of the content in this answer to the more general case still applies. 
Regarding practicality, let's differentiate practical from prefabricated. The problem originally described before Edit 1 has no prefabricated solution into which one can plug bicycle data and tweak a few parameters to achieve success. In fact, the general interest on the web in seeking prefabricated solutions is usually met with the practical reality that such plug and play cases in machine learning are rare. Most of the time an approach that involves design and experimentation is usually the case.
Those that have hired and managed human beings know that even the hoped-for high level AI of the future, although possibly quite practical, may be as prefabricated as idealized. For instance, hiring an EE does not mean that the electrical engineering department will immediately see a practical improvement in throughput. Management, training, and workflow design will still be prerequisites to employee effectiveness. To have a practical and reliable bicycle counting, some comprehension of concepts to guide initial experimentation, design, and tuning of the training and use scenarios will likely be necessary.
If the author of the question has one of those very rare cases where bicycles travel in exactly one direction and in series, with no foot traffic, trikes, walking pedestrians, or pets, then AI is not necessary at all. A simple LED and photo-transistor with a passive low pass filter and a digital input to a counter circuit will perform a reasonable count. However, if two bicycles might pass in parallel, then we are back with the camera, the need for concept classes, and challenges much like those discussed in detail above.
Regarding steps to a solution, if approaching from the side of a technology to investigate, this answer includes a recommendable sequence, although there is usually quite a bit of overlap and occasional backtracking in actual practice. If this is not a learning exercises and the bicycle problem is one that is actually needed to operate in the field, then the above answer is the correct background to comprehend what it is the machine is required to perform. Following that comprehension would be the investigation of various designs and algorithms, beginning with a search of scholarly articles using the key phrases in the original answer.
"
Which unsupervised learning algorithm can be used for peaks detection?,"
So, I have a dataset that has around 1388 unique products and I have to do unsupervised learning on them in order to find anomalies (high/low peaks).
The data below just represents one product. The ContextID is the product number, and the StepID indicates different stages in the making of the product.
    ContextID   BacksGas_Flow_sccm  StepID  Time_ms
427 7290057 1.7578125   1   09:20:15.273
428 7290057 1.7578125   1   09:20:15.513
429 7290057 1.953125    2   09:20:15.744
430 7290057 1.85546875  2   09:20:16.814
431 7290057 1.7578125   2   09:20:17.833
432 7290057 1.7578125   2   09:20:18.852
433 7290057 1.7578125   2   09:20:19.872
434 7290057 1.7578125   2   09:20:20.892
435 7290057 1.7578125   2   09:20:22.42
436 7290057 16.9921875  5   09:20:23.82
437 7290057 46.19140625 5   09:20:24.102
438 7290057 46.19140625 5   09:20:25.122
439 7290057 46.6796875  5   09:20:26.142
440 7290057 46.6796875  5   09:20:27.162
441 7290057 46.6796875  5   09:20:28.181
442 7290057 46.6796875  5   09:20:29.232
443 7290057 46.6796875  5   09:20:30.361
444 7290057 46.6796875  5   09:20:31.381
445 7290057 46.6796875  5   09:20:32.401
446 7290057 46.6796875  5   09:20:33.431
447 7290057 46.6796875  5   09:20:34.545
448 7290057 46.6796875  5   09:20:34.761
449 7290057 46.6796875  5   09:20:34.972
450 7290057 46.6796875  5   09:20:36.50
451 7290057 46.6796875  5   09:20:37.120
452 7290057 46.6796875  7   09:20:38.171
453 7290057 46.6796875  7   09:20:39.261
454 7290057 46.6796875  7   09:20:40.280
455 7290057 46.6796875  12  09:20:41.429
456 7290057 46.6796875  12  09:20:42.449
457 7290057 46.6796875  12  09:20:43.469
458 7290057 46.6796875  12  09:20:44.499
459 7290057 46.6796875  12  09:20:45.559
460 7290057 46.6796875  12  09:20:45.689
461 7290057 47.16796875 12  09:20:46.710
462 7290057 46.6796875  12  09:20:47.749
463 7290057 46.6796875  15  09:20:48.868
464 7290057 46.6796875  15  09:20:49.889
465 7290057 46.6796875  16  09:20:50.910
466 7290057 46.6796875  16  09:20:51.938
467 7290057 24.21875    19  09:20:52.999
468 7290057 38.76953125 19  09:20:54.27
469 7290057 80.46875    19  09:20:55.68
470 7290057 72.75390625 19  09:20:56.128
471 7290057 59.5703125  19  09:20:57.247
472 7290057 63.671875   19  09:20:58.278
473 7290057 70.5078125  19  09:20:59.308
474 7290057 71.875  19  09:21:00.337
475 7290057 69.82421875 19  09:21:01.358
476 7290057 69.23828125 19  09:21:02.408
477 7290057 69.23828125 19  09:21:03.548
478 7290057 72.4609375  19  09:21:04.597
479 7290057 73.4375 19  09:21:05.615
480 7290057 73.4375 19  09:21:06.647
481 7290057 73.4375 19  09:21:07.675
482 7290057 73.4375 19  09:21:08.697
483 7290057 73.4375 19  09:21:09.727
484 7290057 74.21875    19  09:21:10.796
485 7290057 75.1953125  19  09:21:11.827
486 7290057 75.1953125  19  09:21:12.846
487 7290057 75.1953125  19  09:21:13.865
488 7290057 75.1953125  19  09:21:14.886
489 7290057 75.1953125  19  09:21:15.907
490 7290057 75.9765625  19  09:21:16.936
491 7290057 75.9765625  19  09:21:17.975
492 7290057 75.9765625  19  09:21:18.997
493 7290057 75.9765625  19  09:21:20.27
494 7290057 75.9765625  19  09:21:21.55
495 7290057 75.9765625  19  09:21:22.75
496 7290057 75.9765625  19  09:21:23.95
497 7290057 76.85546875 19  09:21:24.204
498 7290057 76.85546875 19  09:21:25.225
499 7290057 76.85546875 19  09:21:25.957
500 7290057 76.85546875 19  09:21:26.984
501 7290057 75.9765625  19  09:21:27.995
502 7290057 75.9765625  19  09:21:29.2
503 7290057 76.7578125  19  09:21:30.13
504 7290057 76.7578125  19  09:21:31.33
505 7290057 76.7578125  19  09:21:32.59
506 7290057 76.7578125  19  09:21:33.142
507 7290057 76.7578125  19  09:21:34.153
508 7290057 75.87890625 19  09:21:34.986
509 7290057 75.87890625 19  09:21:35.131
510 7290057 75.87890625 19  09:21:35.272
511 7290057 75.87890625 19  09:21:35.451
512 7290057 76.7578125  19  09:21:36.524
513 7290057 76.7578125  19  09:21:37.651
514 7290057 76.7578125  19  09:21:38.695
515 7290057 76.7578125  19  09:21:39.724
516 7290057 76.7578125  19  09:21:40.760
517 7290057 76.7578125  19  09:21:41.783
518 7290057 76.7578125  19  09:21:42.802
519 7290057 76.7578125  19  09:21:43.822
520 7290057 76.7578125  19  09:21:44.862
521 7290057 76.7578125  19  09:21:45.884
522 7290057 76.7578125  19  09:21:46.912
523 7290057 76.7578125  19  09:21:47.933
524 7290057 76.7578125  19  09:21:48.952
525 7290057 76.7578125  19  09:21:49.972
526 7290057 76.7578125  19  09:21:51.72
527 7290057 77.5390625  19  09:21:52.290
528 7290057 77.5390625  19  09:21:52.92
529 7290057 77.5390625  19  09:21:53.361
530 7290057 77.5390625  19  09:21:54.435
531 7290057 76.66015625 19  09:21:55.602
532 7290057 76.66015625 19  09:21:56.621
533 7290057 72.94921875 22  09:21:57.652
534 7290057 3.90625 24  09:21:58.749
535 7290057 2.5390625   24  09:21:59.801
536 7290057 2.1484375   24  09:22:00.882
537 7290057 2.05078125  24  09:22:01.259
538 7290057 2.1484375   24  09:22:01.53
539 7290057 1.953125    24  09:22:02.281
540 7290057 1.953125    24  09:22:03.311
541 7290057 2.1484375   24  09:22:04.331
542 7290057 2.1484375   24  09:22:05.351
543 7290057 1.953125    24  09:22:06.432
544 7290057 1.85546875  24  09:22:07.519
545 7290057 1.7578125   24  09:22:08.549
546 7290057 1.85546875  24  09:22:09.710
547 7290057 1.7578125   24  09:22:10.738
548 7290057 1.85546875  24  09:22:11.798
549 7290057 1.953125    24  09:22:12.820
550 7290057 1.85546875  1   09:22:13.610
551 7290057 1.85546875  1   09:22:14.629
552 7290057 1.953125    1   09:22:15.649
553 7290057 1.85546875  2   09:22:16.679
554 7290057 1.85546875  2   09:22:17.709
555 7290057 1.85546875  2   09:22:18.729
556 7290057 1.953125    2   09:22:19.748
557 7290057 1.85546875  2   09:22:20.768
558 7290057 1.7578125   3   09:22:21.788
559 7290057 1.7578125   3   09:22:22.808
560 7290057 1.85546875  3   09:22:23.829
561 7290057 1.953125    3   09:22:24.848
562 7290057 1.85546875  3   09:22:25.898
563 7290057 1.953125    3   09:22:27.39
564 7290057 1.953125    3   09:22:28.66
565 7290057 1.7578125   3   09:22:29.87
566 7290057 1.85546875  3   09:22:30.108
567 7290057 1.7578125   3   09:22:31.129
568 7290057 1.953125    3   09:22:32.147
569 7290057 1.85546875  3   09:22:33.187

I use the following code to plot a graph.
Code:
lineplot = X.loc[X['ContextID'] == 7290057]
x_axis = lineplot.values[:,3]
y_axis = lineplot.values[:,1]

plt.figure(1)
plt.plot(x_axis, y_axis)

and the graph:

In this graph, the peaks (marked in red circles) are the anomalies that need to be detected.
And when I have a graph like this: No anomalies must be caught since there are no undesirable peaks.

I tried using OneClassSVM, but I am somehow not satisfied with the results.
I would like to know which unsupervised learning algorithm can be used for such a task at hand.
","['machine-learning', 'deep-learning', 'unsupervised-learning', 'anomaly-detection']",
Transposed convolution as upsampling in DCGAN,"
I read several papers and articles where it is suggested that transposed convolution with 2 strides is better than upsampling then convolution.
However implementing such model with the transposed convolution resulted in heavy checkboard effect, where the whole generated image is just a pattern of squares and no learning takes place. How to properly implement it without totally messing up the generation? With the upsampling+convolution I got okay result but I want to improve my model. I am trying to generate images based on the CelebA dataset.
I use keras with tf and I used the following code:
model.add(Conv2DTranspose(256, 5, 2, padding='same'))

model.add(LeakyReLU(alpha=0.2))

model.add(BatchNormalization(momentum=0.9))



model.add(Conv2DTranspose(128, 5, 2, padding='same'))

model.add(LeakyReLU(alpha=0.2))

model.add(BatchNormalization(momentum=0.9))



model.add(Conv2DTranspose(64, 5, 2, padding='same'))

model.add(LeakyReLU(alpha=0.2))

model.add(BatchNormalization(momentum=0.9))

Here I try to turn a 4x4 image into a 32x32. Later it will be turned into a 64x64 image with 1 or 3 channels depending on the image. However I get the following pattern always. Some tweaking usually leads to some other pattern but it does not really change:
Checkboard effect
Thank you for your answers in advance
","['deep-learning', 'computer-vision', 'generative-adversarial-networks']",
Why is it harder to achieve good results using neural network based algorithms for multi step time series forecasting?,"
There are different kinds of machine learning algorithms, both univariate and multivariate, that are used for time series forecasting: for example ARIMA, VAR or AR.
Why is it harder (compared to classical models like ARIMA) to achieve good results using neural network based algorithms (like ANN and RNN) for multi step time series forecasting?
","['deep-learning', 'prediction', 'comparison', 'forecasting', 'time-series']","
Given the (usual) higher architectural complexity of ML models compared to more classical forecasting models, ML models might also require more data, otherwise they might just overfit the training dataset. 
Furthermore, online learning (or training) of a neural network using stochastic gradient descent (that is, one example at a time) might also be numerically unstable and statistical inefficient (so convergence might be slow). See Towards stability and optimality in stochastic gradient descent for more details and a solution (AI-SGD).
"
A3C fails to solve MountainCar-v0 enviroment (implementation by OpenAi gym),"
While I've been able to solve MountainCar-v0 using Deep Q learning, no matter what I try I can't solve this enviroment using policy-gradient approaches. As far as I learnt searching the web, this is a really hard enviroment to solve, mainly because the agent is given a reward only when it reaches the goal,which is a rare event. I tried to apply the so called ""reward engineering"", more or less substituting the reward given by the enviroment with a reward based upon the ""energy"" of the whole system (kinetic plus potential energy), but despite this no luck.
I ask you:

is correct to assume that MountainCar-v0 is beyond the current state of the art A3C algorithm, so that it requires some human intervention to suggest the agent the policy to follow, for example adopting reward engineering?
could anyone provide any hint about which reward function could be used, provided that reward engineering is actually needed ?

Thanks for your help.
",['reinforcement-learning'],"
I don't know about your first question, but I got a  basic policy gradient approach with the kinetic energy as reward working on MountainCar-v0. 
You can implement it based on this blog and the notebook you find there. It uses an MLP with one hidden layer of size 128 and standard policy gradient learning. 
The reward engineering boils down to replacing the reward variable with the kinetic energy $v^2$ (no potential energy and no constant factor, the reward itself is not used). I takes $>1000$ episodes to solve the environment consistently.
I'm afraid the solution is not very satisfactory and I don't have the feeling there is much to learn from it. The solution is originally for the cartpole problem and it stops working for me if I change hyperparameters/optimizer or the specifics of the reward. 
"
Speech to text models [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I'm currently using google speech to text api to transcribe audio in real time (police scanner audio dispatches). The audio quality isn't great and I've been putting in key words to try to help train it. Is there a way to either use google, amazon aws, IBM Watson, to create a model based on past audio dispatches where I can manually type in what was said to help train it? It seems like putting in key words won't really cut it. Any other suggestions to help make it more accurate?  
","['machine-learning', 'audio-processing']",
How is GARB implemented in PGRD-DL to calculate gradients w.r.t. internal rewards?,"
In section 3 of this paper the author outlines how GARB was adapted to reduce the variance in updating parameters to an internal reward function estimator. 
I have read it a number of times and understand it up through the end of the explanation of GARB. The author then goes on to explain how they use backprop to implement this procedure, which is the point at which I stop understanding. 


Is there an open source implementation available to look at? I cant figure out if $g_T$ is actually computed and used or not? And not certain how the internal reward gradient is calculated. 
Any insight you can provide would be helpful. 
EDIT: after reading it several times and several related papers, I think I have more understanding but not quite there. So my main questions are:
1) are we keeping a full eligibility trace vector with the dimensionality of the vector equal to the number of parameters in $\theta$ (all NN params)?
2) do we use the gradient calculation via backdrop at every step to calculate $g_t$?
3) do we have to maintain $3 * \theta$ parameters, one for theta, one for $e$ and one for $g_t$?
3) what then is the procedure at terminal $g_T$ to update the parameters? A simple element wise matrix operation?
4) How often to update the parameters of $\theta$?
","['neural-networks', 'reinforcement-learning', 'monte-carlo-tree-search', 'rewards', 'monte-carlo-methods']",
"Is there any paper, article or book that analyzes the feasibility of acheiving AGI through brain-simulation?","
In my understanding, the mind arises from a physical system, the brain. I see that there is a big research under the topic of simulating physical systems efficiently (especially in quantum computing). Hence, in theory, we could achieve AGI by simulating the physical brain.
Is there any research I should look into regarding this topic? I would like to hear if it is possible, why, why not, what are the limitaions, how far we are from achieving this, and anything else, really. Also, I would like to read something about my first assumption (""the mind arises from a physical system, the brain"").
I have searched AI.SE but I've found only related questions, so I don't think this is duplicate. For reference:

Why are we asking, How can we simulate the brain?
How powerful a computer is required to simulate the human brain? 
Are there any artificial neuromorphic systems which can mimic the
brain?
Is it possible to build human-brain-level artificial intelligence based on neuromorphic chips and neural networks? (this one is pretty similar but I think it's slighly different, and it doesn't answer my question).

Note: I am not asking for the possibility NOW, but in general, so telling me that ""we don't know the brain enough"" is not on topic.
","['agi', 'human-like', 'brain', 'human-inspired']",
"Is the Turing test still relevant, as of 2019?","
Given the Chinese room argument, and given the development in chatbots and machine learning, isn't Turing test superseded by some other way of evaluating AI's inteligence? Would an positive result of a Turing test provide any value, besides telling that a machine is good at conversations (but possibly nothing more)?
",['turing-test'],"
The Turing test is a good test for AI applications like Siri and Alexa (or, in general, intelligent personal assistants), but it doesn't test a lot of features (e.g. vision) that an artificial general intelligence requires. There are several arguments against the Turing test (and it is definitely not flawless), but is it a necessary (but insufficient) test for AGI. The Turing test is still quite relevant: e.g. if IPAs passed the test, people would use them more often and they would be a lot more useful.
"
Which kind of prioritized experience replay should I use?,"
The Prioritized Experience Replay paper gives two different ways of sampling from the replay buffer. One, called ""proportional prioritization"", assigns each transition a priority proportional to its TD-error. 
$$p_i = |\delta_i|+\epsilon$$
The other, called ""rank-based prioritization"", assigns each transition a priority inversely proportional to its rank.
$$p_i = 1/\text{rank}(i)$$
where $\text{rank}(i)$ is the rank of transition $i$ when the replay buffer is sorted according to $|\delta_i|$. 
The paper goes on to show that the two methods give similar performance for certain problems.
Are there times when I should choose one sampling method over the other?
","['deep-learning', 'reinforcement-learning', 'dqn', 'experience-replay']","
The authors of that paper hypothesized that rank-based prioritization would be more robust to outliers. They suggested that rank-based sampling would be preferred for this reason. However, as they noted later, the fact that DQN clips rewards anyways weakens this argument.
If you're going to use someone else's ready-made code for your prioritized experience replay, then you'll probably end up using proportional sampling. Every implementation I've been able to find, including OpenAI's baselines implementation, uses proportional sampling. If you were going to write your own, proportional sampling might be preferred for being simpler to implement (so less error-prone).
Comparing these two sampling methods is complicated by the fact that rank-based sampling involves a hyperparameter that determines how often you sort your replay buffer. The authors of the original PER paper only sorted every $n$ timesteps, giving the nice amortized time of $O(\log n)$, where $n$ is the size of the replay buffer. Sampling takes constant time, so sampling a minibatch of size $k$ takes $O(k)$ time. 
Proportional sampling doesn't involve sorting, but it does need to maintain a sum tree structure, which takes $O(\log n)$ time each time we add to the buffer. Sampling also takes $O(\log n)$ time, so sampling a minibatch of size $k$ takes $O(k\log n)$ time. 
If we only sort our replay buffer every $n$ timesteps, then rank-based sampling is faster. However, because the buffer is almost always only approximately sorted, the distribution we sample from is only a rough estimate of the distribution we wanted. It's not clear that this estimation would be accurate enough to be performant when the replay buffer is scaled up in size past the $n=10^6$ transitions used in the paper. I haven't seen a study that compares performance for different frequencies of sorting and different buffer sizes.
So, rank-based sampling might be faster, but it also might not work as well. Adjustment of the sorting frequency hyperparameter might be necessary. The simpler and surer approach would be to use proportional sampling with clipped TD-errors.
"
Problems that only humans will ever be able to solve,"
With the increasing complexity of reCAPTCHA, I wondered about the existence of some problem, that only a human will ever be able to solve (or that AI won't be able to solve as long as it doesn't reproduce exactly the human brain).
For instance, the distorted text used to be only possible to solve by humans. Although...

The computer now got the [distorted text] test right 99.8%, even in the most challenging situations.

It seems also obvious that the distorted text can't be used for real human detection anymore. 
I'd also like to know whether an algorithm can be employed for the creation of such a problem (as for the distorted text), or if the originality of a human brain is necessarily needed. 
","['agi', 'problem-solving', 'intelligence-testing', 'ai-completeness']","
This is more of a comment and philosophical opinion, but I dont believe that there are any problems an AI couldnt solve, that a human can.  Being new to this forum, I cannot make it a comment on the question (and it would probably be too long)  I preemptively ask for your forgiveness.
AI Eventually Will Mimic Humans (and surpass them)
Humans by nature are logical. Logic is learned  or hardwired, and influenced by observation and chemical impulses.
As long as an AI can be trained to act like a human, it will be able to act like one. Currently, those behaviors are limited to technology (space, connections, etc), which the human brain has been optimized to rule out or disregard certain fluff automatically enabling it certain super capabilities. For instance, not everything seen is registered through the brain; often, the brain performs differential comparisons and updates for changes to reduce processing time and energy.  It will only be a matter of time before AI can also be programmed to behave this way, or technological advancements will allow it to not need some of this function, which will allow it to leapfrog humans.
In the current state, we recognize humans are sometimes irrational or inconsistent. In those cases, AI could mimic human limitations with configured randomization patterns, but again, there really wont be a need since it can be programmed and learn those patterns automatically (if necessary).
It all comes down to consumption of data, retention of information, and learned corrections. So, there is no problem that a human can perform (to my knowledge) that AI couldnt theoretically ever perform. Even in the case of chemistry. As we are manufacturing foods and organs, an AI could also, theoretically, one day reproduce and survive via biological functions.
Instead of the question being binary about human capability vs that of artificial intelligence, Id be more interested to see what people think are the more challenging things humans can do, which will take AI time to accomplish.
"
Estimating Baselines using ALS,"
I am trying to figure out how ALS works when minimizing the following formula:
$\\ \\$
$\text{min}_{\lbrace b_u,b_i \rbrace} \sum_{(u,i)\in \mathcal{K}} (r_{ui} -  \bar{r} - b_u - b_i )^2 + \lambda_{1}(\sum_{u} b_u^{2} +\sum_{i} b_i^{2})$
$\\ \\$

$\textbf{Question 1}$:
I would like to know how does Alternating Least Squares work in this case. How does it minimize the equation on the picture? The idea of the whole equation that needs to be minimized, I think, it is like when we do a simple linear regression and we have to fit the line. Am I right? In the Lineal Regression we do $(y - \hat y)^2$. In the case of the paper we do $(r_{ui} - \mu -b_{i}-b_{u})^2 +\lambda(...)$ 
Just in case I leave the link: paper
","['machine-learning', 'optimization', 'linear-algebra', 'recommender-system']",
Reinforcement learning with uniformly random dynamics,"
Suppose I have an MDP $(S, A, p, R)$ where the $p(s_j|s_i,a_i)$ is uniform, i.e given an state $s_i$ and an action $a_i$ all states $s_j$ are equally probable.
Now I want to find an optimal policy for this MDP. Can I just apply the usual methods like policy gradients, actor-critic to find the optimal policy for this MDP? Or is there something I should be worried about?
At least, in theory, it shouldn't make any difference. But I'm wondering are there any practical considerations I should be worried about? Should the discount factor, in this case, be high?
The reward function here depends both on states and actions and is not uniformly random.
","['reinforcement-learning', 'markov-decision-process']","
When the next state selection is not driven by any meaningful dynamics i.e. it is independent of starting state $s$ and action taken $a$, but the rewards received do depend somehow on the $s$ and $a$, then the MDP you describe also fits with something called a Contextual Bandit Problem where there is no control over state due to action choice, and thus no incentive to choose actions other than for their potential for immediate reward.
Any algorithm capable of solving a full MDP can also be put to use attempting to solve a contextual bandit problem, as the MDP framework is a strictly more general case of the contextual bandit problem, and can model such an environment. However, this is typically going to be inefficient, as MDP solvers make no assumptions about state transition dynamics and need to experience and learn them. Whilst if you start with an algorithm designed to solve a contextual bandit problem, you have the assumption of randomised state built in to the algorithm, it does not need to be learned, and the learning process should be more efficient.
Alternatively, if you only have RL solvers available, you can reduce variance and get the same effective policy by setting discount factor, $\gamma = 0$. 
If for some reason you still want or need a long-term discounted value prediction from your policy, you can take the mean predicted value of some random states (or even of all the states if there are few enough of them) and multiply by $\frac{1}{1-\gamma}$ for whatever discount factor you want to know it for. Or if predicting for a time horizon, just multiply by number of steps to the horizon.
"
"Why do we use the word ""kernel"" in the expression ""Gaussian kernel""?","
I've heard the expression ""Gaussian kernel"" in several contexts (e.g. in the kernel trick used in SVM). A Gaussian kernel usually refers to a Gaussian function (that is, a function similar to the probability density function of a Gaussian distribution) that is used to measure the similarity between two vectors (or numbers). 
Why is this Gaussian function called a ""kernel""? Why not just calling it a Gaussian (similarity) function? Does it have something to do with the kernel of a linear transformation?
","['machine-learning', 'terminology', 'math', 'history', 'support-vector-machine']","
The usage of the word ""kernel"" in the context of support vector machines probably comes from its usage in the context of integral transforms. 
See the article Kernel of an integral operator, and the questions What is the difference between a kernel and a function? and Why is the kernel of an integral transform called kernel?.
The word ""kernel"" has been used in many other contexts, such as in computer vision, to refer to a certain function with a special purpose. See e.g. the paper that introduced SIFT.
"
What is the intuition behind the Label Smoothing in GANs?,"
I was learning about GANs when the term ""Label Smoothing"" surfaced. In the video tutorial that I watched, they use the term ""label smoothing"" to change the binary labels when calculating the loss of the discriminator network. Instead of using 1, they use 0.9 for the label. What is the main purpose of this label smoothing?
I've skimmed through the original paper, and there is a lot of maths that, honestly, I have difficulty understanding. But I notice this paragraph in there:

We propose a mechanism for encouraging the model to be less confident. While this may not be desired if the goal is to maximize the log-likelihood of training labels, it does regularize  the  model  and  makes  it  more  adaptable

And it gives me another question:

why ""this may not be desired if the goal is to maximize the log-likelihood of training labels""?

what do they mean by ""adaptable""?


","['generative-adversarial-networks', 'generative-model', 'regularization', 'labels']","
From the previous paragraph:

""This, however, can cause two problems. First, it may result in
over-fitting: if the model learns to assign full probability to the
groundtruth label for each training example, it is not guaranteed to
generalize. Second, it encourages the differences between the largest
logit and all others to become large, and this, combined with the
bounded gradient ` zk , reduces the ability of the model to adapt.
Intuitively, this happens because the model becomes too confident
about its predictions.""

In other words, label smoothing increases your model's entropy in its output logits, which, in turn, makes it less confident in its predictions. As the authors point out, this may lead to a decrease in training performance w.r.t. the log-likelihood of the training labels. In particular, the model's capacity may be high enough to find a complex function to map over all input examples without label smoothing. This function, however, may not generalise well, making it less adaptable to unseen data. In the GAN space, there are other benefits to label smoothing as well, such as preventing the discriminator from relaying a too high gradient signal to the generator (see here: https://www.kth.se/social/files/59086d09f2765460c378ca73/GANs.pdf). Large gradients are known to make the training process more instable and may prevent convergence. For a deeper understanding of label-smoothing and entropy, I recommend this paper:
https://arxiv.org/pdf/2005.00820.pdf
Also, check out this related question:
https://datascience.stackexchange.com/questions/28764/one-sided-label-smoothing-in-gans
"
"Given the generative model $P(A, B, C)$, would it be faster to learn $P(A,B,C,D)$, or compose $P(A, B, C)$ with $P(D \mid A,B,C)$?","
If we have a neural network that learns the generative model for $P(A, B, C)$, the joint PDF of the random variables $A$, $B$, and $C$.
Now, we want to learn the generative model for $P(A, B, C, D)$.
Is there any theory that says learning $P(A,B,C$) and then composing it with $P(D \mid A,B,C)$ is faster than learning $P(A,B,C,D)$ from scratch?
","['neural-networks', 'reference-request', 'generative-model']",
How to deal with a small amount of labeled samples?,"
I'm trying to develop skills to deal with very small amounts of labeled samples (250 labeled/20000 total, 200 features) by practicing on Kaggle ""Don't Overfit"" dataset (Traget_Practice have provided all 20,000 Targets). I've read a ton of papers and articles on this topic, but everything I've tried did not improve simple regularized SVM results (best accuracy was 75 and AUC was 85) or any other algorithm result (LR, K-NN, NaiveBayes, RF, MLP). I believe the result can be better (on Leaderboard they go even over AUC 95)
What I've tried without success:

Remove outliers  I've tried to remove 5%-10% outliers with EllipticEnvelope and with IsolationForest.

Feature Selection I've tried RFE (with or without CV) + L1/L2 regularised LogisticRegression, and SelectKBest (with chi2).

Semi-Supervised techniques I've tried co-training with different combinations of two complementary algorithms and :100-100: split features. I've also tried LabelSpreading, but I don't know how to provide the most uncertain samples (I tried predictions from other algorithms, but there were many mislabeled samples, hence was unsuccessful).

Ensembling Classifiers StackingClassifier with all possible combinations of algorithms and this also didn't improve the result (the best is the same as SVM accuracy 75 and AUC 85).


Can anyone give me advice on what I'm doing wrong or what else to try?
","['machine-learning', 'semi-supervised-learning', 'data-labelling', 'labeled-datasets', 'unlabeled-datasets']","
In that particular competition, you can try using GAN to generate new data or adding noise to existing data. You can also use K-means algorithm. You can try using a smaller network and remove bias. May be you can use logistic regression to compare the result. You can also use a PCA method.
"
How to identify whether images contain driver's licenses or ID cards,"
Suppose I have a lot of scans of hardcopy documents, in the form of jpegs. Some of them are potentially scans of driver's licenses or identification cards. I wonder what would be a good way to identify those scans that contain driver's license/ID cards.
One thought I had was training a model or use an existing pretrained model that can detect faces. However, if the data set I have has a lot of scans of photos of people, it would cause false positives. So I am not sure how I might approach this problem.
Any thought would be much appreciated!
","['image-recognition', 'classification', 'facial-recognition']","
Why not make a detector/classifier to look for the text ""Drivers Licence"" or some very generic keywords related to license? As you mentioned images of people may be present in any sort of documents. Looking for text which is super specific to IDs/drivers license seem to be a better way. 
"
Should I use the hyperbolic distance loss in the case of Poincar Disk Model?,"
I trained a neural network which makes a regression to a Poincar Disk Model with radius $r = 1$.
I want to optimize using the hyperbolic distance 
$$
\operatorname{arcosh} \left( 1 + \frac{2|pq|^2|r|^2}{(|r|^2 - |op|^2)(|r|^2 - |oq|^2)} \right)
$$
where $|op|$ and $|oq|$ are the distances of $p$ and respectively $q$ to the centre of the disk, $|pq|$ the distance between $p$ and $q$, $|r|$ the radius of the boundary circle of the disk and  $\operatorname{arcosh}$ is the inverse hyperbolic function of hyperbolic cosine.
But there is a problem

In the Poincar Disk Model with $r = 1$, the distance is defined only for vectors which have norm less than $1$.
A neural network does not know this rule, so it can predict vectors with norm greater than $1$.

So, I tried to use the distance defined in a space with $r = 2$, and it works very well for the learning task, but I'm doubtful because the distance doesn't scale in a linear way.
Will there be unwanted effects, in your opinion?
","['machine-learning', 'math', 'objective-functions', 'geometric-deep-learning']",
Why don't people use nonlinear activation functions after projecting the query key value in attention?,"
Why don't people use nonlinear activation functions after projecting the query key value in attention?
It seems like doing this would lead to much-needed nonlinearity, otherwise, we're just doing linear transformations.
This observation applies to the transformer, additive attention, etc.
","['neural-networks', 'attention', 'transformer']","

It seems like doing this would lead to much-needed nonlinearity, otherwise, we're just doing linear transformations.

Attention is broadly defined as a following operation ($\text{softmax}$ is sometimes replaced by $\tanh$) :
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
Where $Q$, $K$ and $V$ are matrices that are some functions of the inputs.
There are three nonlinear operations there:

The inner projection $QK^T$ is nonlinear. We have multiplication of two functions of the inputs. For example, in case of self-attention $Q=X W_Q$ and $K = XW_K$ are two linear transforms of the same $X$, so $QK^T = X \left(W_Q W_K^T\right) X^T$ is a quadratic function of the inputs.
The $\text{softmax}(x_i) = e^{x_i} /\sum_n e^{x_n} $ function is obviously nonlinear ($\tanh$ as well)
The final $\text{softmax}(\dots) V$ product is also nonlinear for the same reasons as (1)

I would say that it is pretty clear that it is definitely not just a linear transformation - there's quite a lot of nonlinearities in the attention block.


This observation applies to the transformer, additive attention, etc.

Let's see what happens next with the outputs of the attention layers:
In the transformer model, outputs of the multi-head-self-attention are fed into a feed-forward network inside each block:

""Feed-forward"" means that the inputs are multiplied by a weight matrix and then a nonlinear activation function is applied.
The additive attention approach, directly applies another $\text{softmax}$ on the outputs of what one would call the attention block:
$$e_{ij} = v_a^T \tanh\left(W_as_{i-1} + U_a h_j\right)$$
$$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_k \exp(e_{ik})}$$

To summarize - I don't think that the premise of the question is correct. Various nonlinearities are present both inside the attention blocks and, typically, are applied after the attention is computed.
"
Are self-driving cars using single frame or multiple frame to make decision?,"
This might be a trivial question but I couldn't find any reliable answers on the internet.
Almost all the neural network architectures for self-driving cars that I have seen on the internet have a feedforward network, previous frames will not help in making the current decision.
I have read somewhere that Tesla uses two last frames captured to make a decision, even then 2 frames will not be that useful in this case. 
This might not be very helpful when predicting things ie.. lane cut-ins, as the system needs to observe the vehicle (that is going to cut in) behavior such as turn indicator, vehicle veering towards center lane over time in order to predict.
Can someone explain if this is the way production self-driving card such as Tesla work?

Or Is it something like the below?

Or are they using something like Many to one Recurrent net where inputs are CNN vectors of previous few frames and output is the control?
",['autonomous-vehicles'],"
It varies quite substantially between different self-driving paradigms(rather obviously) but for the most part the vast majority of implementations are using a variety of different reference frames in order to make predictions.
For example, Tesla's Autopilot is being fed many different camera feeds as well as radar and ultrasonic signals that are processed in a variety of temporal contexts.
While, for the most part, all of these programs are very tight-lipped, we can make a variety of assumptions based on the information available and educated assumptions.
As with many large, complex ML/AI systems, there is a large amount of compartmentalization where many different connectionist(or sometimes classic) models are combined( la youtube recommendation system). Tesla is likely utilizing recurrent and convolutional networks where particular modules(combinations of models) are deciding on specific contexts(temporal or signal-based). These outputs are then most likely fed into an actor network which makes real time decisions. 
"
What data formats/pipelining are best to store and wrangle data which contains both text and float vectors?,"
Often in NLP project the data points contain both text and float embeddings, and it's very tricky to deal with. CSVs take up a ton of memory and are slow to load. But most the other data formats seem to be meant for either pure text or pure numerical data.
There are those that can handle data with the dual data types, but those are generally not flexible for wrangling. For example, for pickle you have to load the entire thing into memory if you want to wrangle anything. You can just append directly to the disk like you can with hdf5, which can be very helpful for huge datasets which can not be all loaded into memory?
Also, any alternatives to Pandas for wrangling Huge datasets? Sometimes you can't load all the data into Pandas without causing a memory crash.
","['natural-language-processing', 'datasets', 'data-science', 'structured-data', 'data-mining']",
What approaches are there to apply AI to global economic processes?,"
Today, AI is mainly driven by own-profit-oriented companies (e.g. Facebook, Amazon, Google). Admittedly, there's a lot of AI in the health sector (even in the public health sector) and there's a lot of AI in the sustainability sector  but also mostly driven by obviously own-profit-oriented companies (e.g. Tesla, Uber, Google).
On the other side, one often hears from hard-core economists that centrally planned (= public-profit-oriented) economies (or economic principles) are ""the work of the devil"" - and that they failed all over history (sometimes for understandable reasons).
But intelligently planning global economic processes and applying these plans with the help of state-of-the-art AI - given the huge amounts of really big data available, and given the argument that globalization is finally for the benefit of all - would seem to be a rewarding endeavour, at least for parts of the AI community.
Why isn't this endeavour undertaken more decidedly? (Or is it?)
Where do I find approaches to apply AI to global economic processes? (Not only describing and understanding but mainly planning and executing?)
","['applications', 'social', 'ethics', 'game-theory']","
It's currently just too complex
The different sources of information are too varied, in economics this is often referred to as a local knowledge problem, which hampers many large scale plans. Humans can react to slight differences like respecting local traditions, landscapes, history but an artificial intelligence would (currently at least) struggle not to generalise over such a large scale as a whole country's economy.
The real work in this case (and actually most 'AI' tasks) would be collecting all the necessary data. Here that part job is currently insurmountable.
Currently a human lead planned economy would do a better job.
"
How does the TRPO surrogate loss account for the error in the policy?,"
In the Trust Region Policy Optimization (TRPO) paper, on page 10, it is stated

An informal overview is as follows.  Our proof relies on the notion of  coupling,  where  we  jointly  define  the  policies $\pi$ and $\pi'$so  that  they  choose  the  same  action  with  high  probability $(1\alpha)$.   Surrogate loss $L_(\hat\pi)$ accounts for the the advantage of $\hat\pi$ the first time that it disagrees with $\pi$,  but not subsequent disagreements. Hence, the error in $L_\pi$ is due to two or more disagreements between $\pi$ and $\hat\pi$, hence, we get an $O(\alpha^2)$ correction term, where $\alpha$ is the probability of disagreement.

I don't see how this holds? In what way does $L_\pi$ account for one disagreement? Surely when $\hat\pi$ disagrees with $\pi$ you will have different trajectories under expectation for each so then $L_\pi$ immediately is different from $\eta$ ?
I understand the proof given, but I wanted to try and capture this intuition.
","['reinforcement-learning', 'deep-rl', 'papers', 'policy-gradients', 'trust-region-policy-optimization']",
Do we need to use the experience replay buffer with the A3C algorithm?,"
I have skimmed through a bunch of deep learning books, but I have not yet understood whether we must use the experience replay buffer with the A3C algorithm.
The approached I used is the following:

I have some threaded agents that play their own copy of an enviroment; they all use a shared NN to predict 'best' move given a current state;
At the end of each episode, they push the episode (in the form of a list of steps) in a shared memory
A separated thread reads from the shared memory and executes the train step on the shared NN, training it episode after episode.

Is this an appropriate approach? More specifically, do I need to sample data to train the NN from the shared memory? Or should I push in the shared memory each step, just when it's done by a single agent? In this last case, I wonder how I could calculate discounted rewards.
I'm afraid that with my current approach I'm doing nothing more that presenting n episodes to the NN, with the hope that each agent explores the enviroment differently from other agents (so that NN is presented a richer variety of states).
","['reinforcement-learning', 'ai-design', 'deep-rl', 'actor-critic-methods', 'experience-replay']",
Formulation of a sentence using FOL,"
I want to formulate the following sentence using FOL; I want also to know whether there any contradictions in it, or if it is consistent.
Assuming human relations are binary:

All Human Relations are Utilitarian (Human relations are Utilitarian
  only when people in those relations are selfish and calculative).
  I am a human. People are Humans. Unselfish people are nice. Therefore, I am nice.

","['logic', 'knowledge-representation']",
"Using UMAP, PCA or t-SNE to find the separating hyperplane?","
Is it possible to use t-SNE, PCA or UMAP to find separating hyperplane?
Assume we have data points in high dimensional space and we want to phase separate it into two sets of points? 
Is there a way to use t-SNE, PCA or UMAP for such a task?
","['machine-learning', 'deep-learning']","
All the methods are basically manifold methods which are used to squeeze hyper dimensions to two or three dimensions with a certain amount of information loss. So whatever you ""see"" with these methods are not real and deceiving. You may see a separation of data points in 3D but when they are mapped onto the actual dimensions it may be complete rubbish. 
By definition, these methods do not provide a way to find separating hyperplane. They just tell you some condensed information about the whole data. It is generally advised to use methods like t-SNE with caution. 
"
What is local consistency in constraint satisfaction problems?,"
In the Constraint Propagation in CSP, it is often stated that pre-processing can solve the whole problem, so no search is required at all. And the key  idea is local consistency. What does this actually mean?
","['terminology', 'definitions', 'constraint-satisfaction-problems']","
If we can do some reduction in the search space using CSP (constraint propagation) we can drastically reduce the search space or sometimes completely avoid the need for a search by directly reaching the solutions (for e.g. with variables having their domains reduced to size one). It could also happen that we come to a point when a variable domain size becomes zero, in that case no solution exists, given the constraints, so no need for a search. 
Constraint propagation basically involves the concept of enforcing local consistency (this is done by enforcing node-consistency, arc-consistency, path-consistency and also global constraints using Alldiif or Atmost). 
The terms: nodes, arc, path, etc. basically reflects a CSP problem represented as a graph with nodes as the variables and the arcs/edges as constraints. The process is simply to remove values from the domains of the variables that are inconsistent. Algorithms such as AC-3, PC-2, etc. precisely are for these purposes.
"
What are a list of board game environments for RL practice?,"
Recently OpenAI removes their board game environments. (It may be possible to install an older version to get access to them, but I havent downgraded). 
Is there a list of repositories or resources of board game or similar environments that can be used to practice RL implementations? Things like checkers, chess, backgammon, or even grid world and mazes would be excellent. 
A running list might be useful for many in this community.  
","['reinforcement-learning', 'deep-rl', 'environment', 'open-source']",
How does Monte Carlo Tree Search UCT exploitation value change based on perspective?,"
In this blog toward the end, the author writes the following:


For the sake of my question, lets assume that a terminal state gives a reward of +1 for a win and -1 for a loss. 
When the author says for any two consecutive nodes this perspective is opposite, does that mean that if $Q_i$ is positive (for example, 4) for player A at a given node, the same node will have the negative of that value for player B (-4 in my hypothetical)?
Do I need to compute two statistics to store the node value (one for each player) or can I simply store statistics for one player and flip the sign at every consecutive node?
","['monte-carlo-tree-search', 'implementation']",
Name of paper for encoding/representing XY coordinates in deep learning,"
It this podcast between Oriol Vinyals and Lex Friedman: https://youtu.be/Kedt2or9xlo?t=1769, at 29:29, Oriol Vinyals refers to a paper:

If you look at research in computer vision where it makes a lot of sense to treat images as two dimensional arrays...
  There is actually a very nice paper from Facebook. I forgot who the
  authors are but I think [it's] part of Kaiming He's group. And what
  they do is they take an image, which is a 2D signal, and they
  actually take pixel by pixel, and scramble the image, as if it
  was a just a list of pixels, crucially they encode the position of the pixels
  with the XY coordinates. And this is a new architecture which we
  incidentally also use in Starcraft 2 called the transformer, which is
  a very popular paper from last year which yielded very nice results in
  machine translation.

Do you know which paper he is referring to? 
I'm guessing maybe he is talking about non-local neural networks, but I'm probably guessing wrong.
Edit: after reviewing the recent publications of Kaiming He (http://kaiminghe.com/), maybe I'm guessing right. Any thoughts?
","['neural-networks', 'deep-learning', 'computer-vision', 'papers']","
So this paper is by google, but is very similar where they use 2D positional embeddings and perform MHA on the flattened image. Are you talking about Attention Augmented Convolutional Networks
"
Adding input features - is complete re-training required?,"
I've never worked with very large models that require weeks or months of training, but in such a situation, what happens if you want to add extra features inputs, do you need to re-train the entire model again from scratch, or how is it handled?  E.g. if Tesla added an extra sensor to its cars, would it need to re-train its network again from scratch to include this extra sensor input?
","['ai-design', 'training']",
Is it possible to use AI to find music that have a distinct tune?,"
Without considering lyrics, there are two type of songs: songs that have a distinct tune which you can easily remember, and songs that are the opposite. Can we design a network to identify the songs that have a distinct tune? It's very hard to find good music, I can't get a song that has a distinct tune for even 100+ songs which consumes a lot of time. 
",['machine-learning'],
Is it possible for a NN to reach the same results as CNNs?,"
Can a normal neural network work as good as a convolutional network? If yes, how much more time and neurons would it need compared to a CNN?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']","
Yes. In theory, a single layer neural network can compute any function. In practice, such a network would have to be much larger than a CNN with equivalent functionality and would therefore be much harder to train. 
"
Is there any other rotated object detection datasets?,"
I have googled for a long time for rotated object detection datasets. Most of papers focused on rotated object detection using DOTA, HRSC2016 or coco text detection dataset. Some researcher also collect their own datasets but almost all of their theme is areal object detection. Is there any others dataset focus on rotated object detection? 
","['computer-vision', 'object-recognition']","
Multiscale Rotated Bounding Box-Based Deep Learning Method 
Here's a link for a reference 
"
What are the various methods for speeding up neural network for inference?,"
One way to speed up a neural network is to prune the network and reducing number of neurons in each layer. What are the other methods to speed up inference?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']",
Should noise (such as OU) be decreased over time in actor / critic algorithms?,"
In most of RL algorithms I saw, there is a coefficient that reduces actions exploration over time, to help convergence.
But in Actor-Critic, or other algorithms (A3C, DDPG, ...) used in continuous action spaces, the different implementation I saw (mainly using Ornstein Uhlenbeck process) is correlated over time, but not decreased.
The action noises are clipped into a range of [-1, 1] and are added to policies that are between [-1, 1] too. So, I don't understand how it could work in environments with hard-to-obtain rewards.
Any thought about this ?
","['deep-learning', 'reinforcement-learning', 'actor-critic-methods', 'ddpg']",
Why overfitting is bad in DQN?,"
It is mentioned by Fu 2019 that overfitting might have a negative effect on training DQN. They showed that with either early stopping or experience replay this effect could be reduced. The first is reducing overfitting, the latter is increasing data.
It doesn't only have negative effects on the returns though, my test shows that it has a negative effect on value errors as well (diff. between predict V and ground truth V). I observed frequently with limited data that the training diverged almost 100% of the time (on small nets). Since increasing the amount of data could reduce the chance of divergence, I think this is an effect from overfitting. 
Overfitting should mean low training loss, however, my observation is that there is a strong correlation between TD loss and value error. That is if I see a jump in TD loss, I could expect to see a jump in value error around that moment. 
Or it is not overfitting because it is not really fit (i.e. high loss) but over-optimization that is for sure. 
Now the question is why? 
There are two points:

If it is overfitting, overfitting should have a positive effect because remembering values for all training states correctly is hardly a bad thing. (In fact, my training data is a superset of my testing data, so remembering should be fine.)
If it doesn't fit, this begs a question what over-optimization really does. It doesn't seem to fit, but it does have a negative effect. How could that be?

","['reinforcement-learning', 'deep-rl']",
What's the advantage of log_softmax over softmax?,"
Previously I have learned that the softmax as the output layer coupled with the log-likelihood cost function (the same as the the nll_loss in pytorch) can solve the learning slowdown problem.
However, while I am learning the pytorch mnist tutorial, I'm confused that why the combination of the log_softmax as the output layer and the nll_loss(the negative log likelihood loss) as the loss function was used (L26 and L34).
I found that when log_softmax+nll_loss was used, the test accuracy was 99%, while when softmax+nll_loss was used, the test accuracy was 97%.
I'm confused that what's the advantage of log_softmax over softmax? How can we explain the performance gap between them? Is log_softmax+nll_loss always better than softmax+nll_loss?
","['objective-functions', 'activation-functions']","
The short answer is yes, log_softmax + nll_loss will work better. 
I dont know the implementation details under the hood in PyTorch, but see the screenshot below from the documentation:

"
Standard deviation of the total input to a neuron,"
Raul Rojas' Neural Networks A Systematic Introduction, section 8.2.1 calculates the standard deviation of the output of a hidden neuron.
From:
$$
\sigma^2 = \sum^n_{i=0}E[w_i^2]E[x_i^2]
$$
When I try what I get is (with $E[x_i^2] = \frac{1}{3}$ and $w_i \in [-\alpha, \alpha]$):
$$
\sigma^2 = \sum^n_{i=0}E[w_i^2]E[x_i^2]
= [n\frac{(\alpha-(-\alpha))^2}{12}][n\frac{1}{3}]=n^2\alpha^2\frac{1}{9}
$$
$$
\sigma = \sqrt{n^2\alpha^2\frac{1}{9}}=n\alpha\frac{1}{3}
$$
But Raul Rojas concludes:
$$
\sigma = \frac{1}{3}\sqrt{n}\alpha
$$
Am I missing some implicance of the law of large numbers use for the input to the node?
Thank you for your time :)
","['neural-networks', 'math', 'probability-distribution']",
What algorithms are considered reinforcement learning algorithms?,"
What are the areas/algorithms that belong to reinforcement learning?
TD(0), Q-Learning and SARSA are all temporal-difference algorithms, which belong to the reinforcement learning area, but is there more to it?
Are the dynamic programming algorithms, such as policy iteration and value iteration, considered as part of reinforcement learning? Or are these just the basis for the temporal-difference algorithms, which are the only RL algorithms?
","['reinforcement-learning', 'terminology', 'temporal-difference-methods', 'planning', 'dynamic-programming']","
In Reinforcement Learning: An Introduction the authors suggest that the topic of reinforcement learning covers analysis and solutions to problems that can be framed in this way:

Reinforcement learning, like many topics whose names end with ing, such as machine
  learning and mountaineering, is simultaneously a problem, a class of solution methods
  that work well on the problem, and the field that studies this problem and its solution
  methods. It is convenient to use a single name for all three things, but at the same time
  essential to keep the three conceptually separate. In particular, the distinction between
  problems and solution methods is very important in reinforcement learning; failing to
  make this distinction is the source of many confusions.

And: 

Markov decision processes are intended to include just
  these three aspectssensation, action, and goalin their simplest possible forms without
  trivializing any of them. Any method that is well suited to solving such problems we
  consider to be a reinforcement learning method.

So, to answer your questions, the simplest take on this is yes there is more (much more) to RL than the classic value-based optimal control methods of SARSA and Q-learning. 
Including DP and other ""RL-related"" algorithms in the book allows the author to show how closely related the concepts are. For example, there is little in practice that differentiates Dyna-Q (a planning algorithm closely related to Q-learning) from  experience replay. Calling one strictly ""planning"" and the other ""reinforcement learning"" and treating them as separate can reduce insight into the topic. In many cases there are hybrid methods or even a continuum between what you may initially think of as RL and ""not RL"" approaches. Understanding this gives you a toolkit to modify and invent algorithms.
Having said that, the book is not the sole arbiter of what is and isn't reinforcement learning. Ultimately this is just a classification issue, and it only matters if you are communicating with someone and there is a chance for misunderstanding. If you name which algorithm you are using, it doesn't really matter whether the person you are talking to thinks it is RL or not RL. It matters what the problem is and how you propose to solve it.
"
Measure grid-world environments difference for reinforcement learning,"
I'd like to measure the difference between 2 grid-worlds to determine the generalization capacity of my agent using tabular Q-learning.
Example (OpenAI Frozen Lake) :
SFFF
FHFH
FFFH
HFFG  
and  :
SFFG
FHFH
FFFH
HFFH
are not very different but the tabular policy that I found on the first environement will completely fail on the new environment. 
A correct distance should to measure the policy on the first environment and compute the norm between this one and the optimal policy (not found with RL) of the new environment. Is it accurate ?
I think this is a bit strange because measure the difference between 2 policies is the intrinsic answer.. How can I measure accurately 2 environments, not forgetting the transition matrix of the environment ?
","['reinforcement-learning', 'q-learning', 'policies']",
How is parallelism implemented in RL algorithms like PPO?,"
There are multiple ways to implement parallelism in reinforcement learning. One is to use parallel workers running in their own environments to collect data in parallel, instead of using replay memory buffers (this is how A3C works, for example).
However, there are methods, like PPO, that use batch training on purpose. How is parallelism usually implemented for algorithms that still use batch training?
Are gradients accumulated over parallel workers and the combined? Is there another way? What are the benefits of doing parallelism one way over another?
","['reinforcement-learning', 'actor-critic-methods', 'implementation', 'proximal-policy-optimization']","
OpenAI have a post on that: https://openai.com/blog/openai-five/
They use a myriad of rollout workers that collect data for 60 seconds and push that data to a GPU cluster where gradients are computed for batches of 4096 observations which are then averaged.
PPO is actually designed to allow this kind of parallelisation as it uses trajectory segments with a fixed size of $T$ to collect data, e.g. 60 seconds for OpenAI Five, where $T$ is supposed to be ""much less than the episode length"" (p.5 of PPO paper).
"
Experience Replay Not Always Giving Better Results,"
I have recently started working on a control problem using a Deep Q Network as proposed by DeepMind (https://arxiv.org/abs/1312.5602). Initially, I implemented it without Experience Replay. The results were very satisfying, although after implementing ER, the results I got were relatively bad. Thus I started experimenting with BATCH SIZE and MEMORY CAPACITY.

(1) I noticed that if I set BATCH SIZE = 1 and MEMORY CAPACITY = 1 i.e. the same as doing normal online learning as previously, the results are then (almost) the same as initially. 
(2) If I increased CAPACITY and BATCH SIZE e.g. CAPACITY = 2000 and BATCH SIZE = 128, the Q Values for all actions tend to converge to very similar negative values.

A small negative reward -1 is received for every state transition except of the desired state which receives +10 reward. My gamma is 0.7. Every state is discrete and the environment can transition to a number of X states after action a, with every state in X having a significant probability.
Receiving a positive reward is very rare as getting to a desired state can take a long time. Thus, when sampling 128 experiences if 'lucky' only a small amount of experiences may have a positive reward.
Since, when doing mini-batch training we average the loss over all the samples and then update the DQN I was wondering whether generally the positive rewards can become meaningless as they are 'dominated' by the negative ones. Which means that this would result in a very slower convergence to actual values ? And also justifies the the convergence to similar negative values as in (2) ? Is this something expected? I am looking to implement. Prioritised ER as a  potential solution to this, but is there something wrong inn the above logic? 
I hope this does makes sense. Please forgive me if I make a wrong assumption above as I am new to the field.
Edit: The problem seemed to be that indeed finding rewards very rarely would result in sampling almost never, especially at the begging of training, which in turn resulted in very slow convergence to the actual Q values. The problem was successfully solved using Prioritised ER -but I believe any form of careful Stratified Sampling would result in good results
","['reinforcement-learning', 'q-learning', 'dqn', 'experience-replay']","
What you describe sounds to me like a problem inherent to off policy learning, and what you describe seems to me to be a reasonable interpretation of what may be happening.
When you implemented experience replay with capacity = 1 and batch_size = 1 you said you got almost the same results as before. There are probably two reasons for this being almost the same. One is simply the random initializations of the networks, so as you train, you will potentially converge around the same point but not exactly to it (also the stochastic nature of generating the training samples).  The other reason might be to do with what has already occurred each time you update the target net weights, so your error terms may differ slightly at each point in time, but asymptotically converge. 
This is essentially following an on policy training, so every training sample is following a trajectory (they are all sequential states). Eventually, given enough time, this trajectory will reach a goal state and be rewarded. The reward will be propagated through the network and effect the backed up values of other states will be updated as well. So essentially each episode ends with a reward (I presume), and the average reward given per episode is proportional to the average length of an episode. 
When increasing capacity and batch size > 1, we move to true off policy training. When sampling, the updates are not following a trajectory. And as such, there is no guarantee that we EVER sample any positive reward (although, Im sure you will at some point). So, if we are averaging over the rewards in the updates, the average reward given per episode is no longer proportional to the average episode length (and the idea of episode starts to lose some of its relevance - since we arent following a trajectory). This the effect of the reward on all other states is not in proportion to what it was when following an on-policy trajectory. 
You could try some hacks to investigate, for example try making your positive reward > batch_size. Or, if you have some statistics on how often your goal state is being sampled, perhaps scale it up by that factor. (Or if you know the size of your state space, make your reward greater than that).
This blog post offers some more elegant refinements, like prioritized ER which you mentioned. But it would be interesting to see the effect of scaling up your reward will overcome the effect of averaging over many negative rewards. 
"
Size of dataset for feature vector with rare event,"
I'm doing a model to detect duplicate in my database (there is a lot of features that can be different but mean same object in the end)
So I have my feature vector for my duplicate dataset which contains score, distance and relation between 2 identical object that I labelised
such as (0, 0, 123, 14000, 5, 10, 0, 0, -1)
Since duplicate are a rare event I was wondering what size of dataset should I use for non duplicate features, since I want my model to learn about the disparity of the multiple features I have I though I should have like 10 times the number of example of non-duplicate and in my model change the weight of duplicate class by multiplying by 10.
Is that a good thing to do or is it better to take 50%/50% of duplicate/non duplicate features for my model ?
Also, should I apply filter to chose my non-duplicate dataset in order to have like close object on some features but different on other. Or should I take them randomly among all the data I have ?
","['machine-learning', 'datasets']",
Why is there an inconsistency in the definitions of the retrace?,"
In Section 4.3 of the paper Learning by Playing - Solving Sparse Reward Tasks from Scratch, the authors define Retrace as
$$
Q^{ret}=\sum_{j=i}^\infty\left(\gamma^{j-i}\prod_{k=i}^jc_k\right)[r(s_j,a_j)+\delta_Q(s_i,s_j)],\\
\delta_Q(s_i,s_j)=\mathbb E_{\pi_{\theta'}(a|s)}[Q^\pi(s_i,\cdot;\phi')]-Q^\pi(s_j,a_j;\phi')\\
c_k=\min\left(1,{\pi_{\theta'}(a_k|s_k)\over b(a_k|s_k)}\right)
$$
where I omit $\mathcal T$ for simplicity.
I'm quite confused about the definition of $Q^{ret}$, which seems not consistent with Retrace defined in Safe and efcient off-policy reinforcement learning:
$$
\mathcal RQ(x,a):=Q(x,a)+\mathbb E_\mu[\sum_{t\ge0}\gamma^t\left(\prod_{s=1}^tc_s\right)(r_t+\gamma\mathbb E_\pi Q(x_{t+1},\cdot)-Q(x_t,a_t)]
$$
What should I make of $Q^{ret}$ in the first paper?
","['reinforcement-learning', 'q-learning', 'papers', 'retrace']",
What's the difference between a static AI and a dynamic AI? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I recently watched a YouTube video (sorry, can't remember the link) where (a very talented) someone created what they called a ""static AI"".
Somewhere in the video they said something along the lines of:

""this is a static AI, it's very simple and not dynamic at all""

What does this mean? What's the difference between a static AI and a dynamic AI?
",['terminology'],
What are examples of good reference books on unsupervised learning?,"
I am looking for good introductory and advanced books on unsupervised learning. I have already read books like Probabilistic Graphical Models from D. Kholler and Pattern Recognition and Machine Learning from C. M. Bishop. I am also very familiar with the Ph.D. thesis of K. P. Murphy, on Dynamic Bayesian Networks.
I have read all of the above mostly for the probability aspects, not really the applications to AI and ML. I would like to know what are the good books (or references) for unsupervised learning, that focuses on practical exercises and examples instead of deep and abstract concepts.
","['reference-request', 'unsupervised-learning', 'books']",
What is a recurrent neural network?,"
Surprisingly, this wasn't asked before - at least I didn't find anything besides some vaguely related questions.
So, what is a recurrent neural network, and what are their advantages over regular (or feed-forward) neural networks?
","['neural-networks', 'comparison', 'recurrent-neural-networks', 'definitions', 'feedforward-neural-networks']","
A recurrent neural network (RNN) is an artificial neural network that contains backward or self-connections, as opposed to just having forward connections, like in a feed-forward neural network (FFNN). The adjective ""recurrent"" thus refers to this backward or self-connections, which create loops in these networks.
An RNN can be trained using back-propagation through time (BBTT), such that these backward or self-connections ""memorise"" previously seen inputs. Hence, these connections are mainly used to track temporal relations between elements of a sequence of inputs, which makes RNNs well suited to sequence prediction and similar tasks.
There are several RNN models: for example, RNNs with LSTM or GRU units. LSTM (or GRU) is an RNN whose single units perform a more complex transformation than a unit in a ""plain RNN"", which performs a linear transformation of the input followed by the application of a non-linear function (e.g. ReLU) to this linear transformation. In theory, ""plain RNN"" are as powerful as RNNs with LSTM units. In practice, they suffer from the ""vanishing and exploding gradients"" problem. Hence, in practice, LSTMs (or similar sophisticated recurrent units) are used.
"
Code examples of controlling multiple units with RL,"
Anyone knows a resources (papers, articles and especially repositories) regarding controlling multiple units with RL.
The controlled units should not be fixed, for example in Real Time Strategy the agent builds various units (workers, soldiers ...) and later controls them. During the game various units could die and new ones are built.
I think good contemporary example is AlphaStar, while OpenAI Five controls just a single agent. This might be incorrect since I've never played those games.
",['reinforcement-learning'],
"Is ""AIAngel"" (Patreon) a fake?","
These guys here: https://www.patreon.com/AiAngel are saying that they've created a AI who can chat and stream. As the so-called administrator ""Rogue"" said:

this chat/streamer bot are no fake. 
Also, there's more about the dynamics of this chat/streamer bot on youtube:
https://www.youtube.com/watch?v=WyFwjHQhlgo&t=463s
https://www.youtube.com/watch?v=GtvivssqLhE
Considering that videos I realy think that this bot is totaly fake. I mean, I think that even the most advanced AI bot do not get even close to a real conversation like this one.
Now, of course that you can say that this is a artistic project or something, but the people behind all of this  are on Patreon, and the people who are paying to these guys possibly are getting totally fooled, which is a serious thing when we're talking about real money. 
So, is AIAngel a real bot? (With this question I'm spreading this possible fake to community)  
","['chat-bots', 'turing-test', 'ai-hoaxes']","
No, AiAngel is not a bot.
It's Rouge's software that changes his voice along with facial recognition software which tracks his movement and copies it to the avatar.
That being said, he has created a very entertaining channel and line of work for himself. By looking at the videos, you can see that he is a true genius at work. Single handedly puts on all hats for that project, from hardware infrastructure development, to software design to graphic art to acting and video editing. Truely one of the great minds of this era. 
You can find Ai Angel (Angelica) on Rogue Shadow's 

Youtube Channel: https://www.youtube.com/user/TheRogueShadow1/videos
Twitch https://www.twitch.tv/aiangellive
Discord https://discordapp.com/invite/DFhaYgj
Twitter https://twitter.com/AiAngel13
Patreon: https://www.patreon.com/AiAngel

..along with several other platforms.
Hope than answers your question with certainty (instead of philosophically)
"
Use Machine Learning/Artificial Intelligence to predict next number (n+1) in a given sequence of random increasing integers,"
The AI must predict the next number in a given sequence of incremental integers (with no obvious pattern) using Python but so far I don't get the intended result!
I tried changing the learning rate and iterations but so far no luck!
Example sequence: [1, 3, 7, 8, 21, 49, 76, 224]
Expected result: 467
Result found : 2,795.5
Cost: 504579.43
This is what I've done so far:
import numpy as np

# Init sequence
data =\
    [
        [0, 1.0], [1, 3.0], [2, 7.0], [3, 8.0],
        [4, 21.0], [5, 49.0], [6, 76.0], [7, 224.0]
    ]

X = np.matrix(data)[:, 0]
y = np.matrix(data)[:, 1]

def J(X, y, theta):
    theta = np.matrix(theta).T
    m = len(y)
    predictions = X * theta
    sqError = np.power((predictions-y), [2])
    return 1/(2*m) * sum(sqError)

dataX = np.matrix(data)[:, 0:1]
X = np.ones((len(dataX), 2))
X[:, 1:] = dataX

# gradient descent function
def gradient(X, y, alpha, theta, iters):
    J_history = np.zeros(iters)
    m = len(y)
    theta = np.matrix(theta).T
    for i in range(iters):
        h0 = X * theta
        delta = (1 / m) * (X.T * h0 - X.T * y)
        theta = theta - alpha * delta
        J_history[i] = J(X, y, theta.T)
     return J_history, theta
print('\n'+40*'=')

# Theta initialization
theta = np.matrix([np.random.random(), np.random.random()])

# Learning rate
alpha = 0.02

# Iterations
iters = 1000000

print('\n== Model summary ==\nLearning rate: {}\nIterations: {}\nInitial 
theta: {}\nInitial J: {:.2f}\n'
  .format(alpha, iters, theta, J(X, y, theta).item()))
print('Training model... ')

# Train model and find optimal Theta value
J_history, theta_min = gradient(X, y, alpha, theta, iters)
print('Done, Model is trained')
print('\nModelled prediction function is:\ny = {:.2f} * x + {:.2f}'
  .format(theta_min[1].item(), theta_min[0].item()))
print('Cost is: {:.2f}'.format(J(X, y, theta_min.T).item()))

# Calculate the predicted profit
def predict(pop):
    return [1, pop] * theta_min

# Now
p = len(data)
print('\n'+40*'=')
print('Initial sequence was:\n', *np.array(data)[:, 1])
print('\nNext numbers should be: {:,.1f}'
  .format(predict(p).item()))

UPDATE Another method I tried but still giving wrong results
import numpy as np
from sklearn import datasets, linear_model

# Define the problem
problem = [1, 3, 7, 8, 21, 49, 76, 224]

# create x and y for the problem

x = []
y = []

for (xi, yi) in enumerate(problem):
    x.append([xi])
    y.append(yi)

x = np.array(x)
y = np.array(y)
# Create linear regression object
regr = linear_model.LinearRegression()
regr.fit(x, y)

# create the testing set
x_test = [[i] for i in range(len(x), 3 + len(x))]

# The coefficients
print('Coefficients: \n', regr.coef_)
# The mean squared error
print(""Mean squared error: %.2f"" % np.mean((regr.predict(x) - y) ** 2))
# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % regr.score(x, y))

# Do predictions
y_predicted = regr.predict(x_test)

print(""Next few numbers in the series are"")
for pred in y_predicted:
    print(pred)

","['neural-networks', 'machine-learning', 'training', 'python']","
I think your code works fine for what is meant to be doing - fitting a linear regression model. The problem here is that you are using a linear model. Linear model does not have an adequate approximation capacity, it will only be able to fit data that is described by a linear function. Here, you gave a random sequence of numbers, that is very difficult for linear model to approximate. I would advise you to try 2 things:  
1) Try something simpler first. Instead of doing a random sequence of numbers do a linear sequence of numbers for example a function like $y = 2x$ or maybe affine function like $y = 2x + 5$. So you would have a sequence like:  
$2, 4, 6, 8 ...$ or $7, 9, 11, 13, ...$ 
If you manage to get that working try a nonlinear function like $x^2$ for example.  
2) Instead of using a linear model try a nonlinear model. For example a polynomial regression model. Especially powerful function approximators are neural networks. In theory, a neural network with single hidden layer can approximate an arbitrary continuous function under some conditions 
(Universal approximation theorem) , so you could try to see how would a neural network solve the problem, there are several open source neural network libraries that you could try.
"
Is this ExpectiMinimax Tree correctly drawn?,"
I need help with ExpectiMinimax problem:

Start a game.
The first player flips a coin.
The second player flips a coin.
The first player decides if he wants to flip another coin.
The second player decides if he wants to flip another coin.
Game over.

The winner is the player who has earned more points.
The points are computed as follows:

If the player flips ones and got the head - 1 point.
If the player flips ones and got the tail - 2 points.
If the player flips twice and got 2 heads - 4 points.
If the player flips twice and got 2 tails - 4 points.
If the player flips twice and got one head and one tail - 0 points.

I need to draw the ExpectiMinimax tree associated with this problem, and write the value of each node.
Did I draw the tree properly?

","['search', 'minimax', 'expectiminimax', 'homework']","
No, your tree is not an accurate representation of the problem.
Hint: Consider the case where both players flip an H on their first coin. Player 1 decides not to flip a second coin. Is the game over?
"
Do we train a logistic regression model using a dataset that is 3 times bigger than the validation dataset?,"
Suppose we have a data set $X$ that is split as $X_{\text{train}}$, $X_{\text{val}}$ and $X_{\text{test}}$ and the outcome variable is binary. Let's say we train three different models (logistic regression, random forest, and a support vector machine) using $X_{\text{train}}$. We then get predictions for $X_{\text{val}}$ using each of the three models. 
In stacking, is it correct to say that we train a logistic regression model on a data set of dimension $|X_{\text{val}}| \times 3$ with the predicted values and actual values of the validation set? This logistic regression model is then used to predict outcomes for data in $X_{\text{test}}$? 
","['machine-learning', 'datasets']","
The predictions of each of your initial models will become a feature to feed the meta learner. If you use $n$ initial models, then for each example you will feed the meta learner $n$ features, each feature being a prediction from one of the initial models. 
Note that this doesnt mean the size of your dataset increases. Instead, each member of $X_{val}$ will be represented by $n$ features, with the $k$th feature being equal to the initial prediction of the $k$th initial predictor.
This medium link recommends a package that can help you build a pipeline. 
Note: in a sense if you are caching the intermediate predictions of each initial predictor, youll end up with $n$ times as many data points as there are entries in $X_{val}$. But these are simply each going to be a feature of another data point, so the dataset hasnt really increased in size. 
"
Which libraries can be used for image caption generation? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.







                        Improve this question
                    



Which libraries can be used for image caption generation?
","['machine-learning', 'image-recognition', 'generative-model']","
Image Caption Generation is an interesting problem to work on. I think your question was to know if there are any open-source libraries with built-in functions for Image Captioning. You can build Image Caption Generation models using Frameworks like Tensorflow, PyTorch, and Trax.
I'd also recommend you to read the following papers:

Show and Tell: A Neural Image Caption Generator. Link
Transfer learning from language models to image caption generators: Better models may not transfer better. Link
Image Captioning with Unseen Objects. Link

Also, here are a couple of blog posts you can read:

How to Develop a Deep Learning Photo Caption Generator from Scratch
Learn to Build Image Caption Generator with CNN & LSTM
Image Captioning with Keras

"
Neural Network with varying inputs (for a game ai),"
I want to create a simple game which basically consists of 2d circles shooting smaller circles at each other (to make hitbox detection easier for the start). My goal is to create an ai which adapts its own behaviour to the players. For that, i want to use a NN as brain. Every frame, the NN is fed with the same inputs as the player and his output is compared to the players output. (outputs in this case are pressed keys like the up-arrow) 
As inputs, I want to use a couple different important factors:
for example, the direction of the enemy player as number from 0 to 1
I also want to input the direction, size and speed of enemys and own projectiles and this is where my problem lies. If there was only one bullet per player, it would be easy but I want the number of bullets to be variable so the number of input neurons would have to be variable.
My approaches:
1) use a big amount of neurons and set unused ones to 0
( not elegant at all)
2) Instead of specific values, just use all the pixels rgb values as inputs (would limit the game as colours would deliver all the information) (+factors like speed and direction would probably not have any impact)
Is there a more promising approach to this problem ?
I hope you can give me some inspiration.
Also, is there a difference in ranging input values between 0/1 or -1/1 ?
Thank you in advance, Mo
Edit: In case there arent enough questions for you, is there a way to make the NN remember things ? For example, if I added a mechanic to the game which involves holding a key, I would add an input neuron which inputs 1 if the certain key is pressed and 0 if it isnt but I doubt that would work.
","['neural-networks', 'game-ai', 'neurons', 'java']","
The most generic approach is to input all pixels as you have suggested. A CNN would be the best architecture for that. To provide information like speed or velocity, you can feed more than one frame to the CNN (e.g. the last 5 frames or whatever provides enough information). The CNN can learn movement information by comparing those images.
If you want to store additional information (like an inventory item), an input neuron for each value would be an option. You can also look up LSTM (long-short term memory) models, but for your specific situation a hardcoded neuron would be the easier solution.
"
Minimax combined with machine learning to determine if a path should be explored,"
I have an idea for a new type of AI for two-player games with alternating turns, like chess, checkers, connect four, and so on.
A little background: Traditionally engines for such games have used the minimax algorithm combined with a heuristic function when a certain depth has been reached to find the best moves. In recent days engines using reinforcement learning, etc (like AlphaZero in chess) have increased popularity, and become as strong as or stronger than the traditional minimax engines.
My approach is to combine these ideas, to some level. A minimax tree with alpha-beta pruning will be used, but instead of considering every move in a position, these moves will be evaluated with a neural net or some other machine learning method, and the moves which seem least promising will not be considered further. The more interesting moves are expanded like in traditional minimax algorithms, and the same evaluation are again done for these nodes' children.
The pros and cons are pretty obvious: By decreasing the breadth (number of moves in a position), the computation time will be reduced, which again can increase the search depth. The downside is that good moves may not be considered, if the machine learning method used to evaluate moves are not good enough.
One could of course hope that the position evaluation itself (from the neural net, etc) is good enough to pick the best move, so that no minimax is needed. However, combining the two approaches will hopefully make better results.
A big motivation for this approach is that it resembles how humans act when playing games like chess. One tends to use intuition (which will be what the neural net represents in this approach) to find moves which looks interesting. Then one will look more thoroughly at these interesting moves by calculating moves ahead. However, one does not do this for all moves, only those which seem interesting. The idea is that a computer engine can play well by using the same approach, but can of course calculate much faster than a human.
To illustrate the performance gain: The size of a minimax tree is about b^d, where b is the average number of moves possible in each position, and d is the search depth. If the neural net can reduce the size of considered moves b to half, the new complexity will be (b/2)^d. If d is 20, that means reducing the computation time by approx. 1 million.
My questions are:

Does anyone see any obvious flaws about this idea, which I might have missed?
Has it been attempted before? I have looked a bit around for information about this, but haven't found anything. Please give me some references if you know any articles about this.
Do you think the performance of such a system could compete with those of pure minimax or those using deep reinforcement learning?

Exactly how the neural net will be trained, I have not determined yet, but there should be several options possible.
","['neural-networks', 'game-ai', 'minimax', 'chess', 'alpha-beta-pruning']","
The use of of a neural network to push the search algorithm to continually only along a promising path is the same that was described in the AlphaZero paper. In AlphaZero, the NN loop contained the search function and would encourage the continued search of high probability moves that were then simulated by the same NN that now contained the Value Net. The use of alpha-beta specifically is not necessary. Just a search function aptly known as PUCT (Predictor + Upper Confidence Bounds applied to Trees)
"
How to find optimal mutation probability and crossover probability?,"
I have a genetic algorithm that maximizes a fitness function with two variables f(X,Y).
I have been running the algorithm with various parameters in mutation and crossover probability (0.1, 0.2, ...)
Since I dont have much theoretical knowledge of GA, how could I proceed in order to find the optimal values for mutation and crossover probability, and if necessary the optimal population size ?
","['algorithm', 'genetic-algorithms', 'mutation-operators']",
Is this neural network with a softmax in the output layer suitable for multi-label classification?,"
I have data with about 100 numerical features and a multi-labelling that encodes ownership of a certain product (i.e. my labels are of the form $[x_i, i=1, \dots, n]$, where $n$ is the number of products and $x_i$ is either 0 or 1).
My neural network approach to this currently looks like this (in Keras)
model = Sequential()
model.add(Dense(1024, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(1024, activation='relu'))
model.add(Dense(1024, activation='relu'))
model.add(Dense(1024, activation='relu'))
model.add(Dense(y_train.shape[1], activation='softmax'))

So, it has a couple of dense layers with ReLu activation, then an output layer with softmax.
Now, my question is: will the neural network consider labels of the other products when assigning a probability to the label of one product?
I would like that to happen, but I can't quite grasp whether it does (my suspicion is no).
I'm new to multi-label classification and relatively new to NN in general, so I hope this isn't too inept a question.
","['neural-networks', 'keras', 'objective-functions', 'softmax', 'multi-label-classification']",
Understanding Hebb network [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



Can anyone help me in understanding Hebb networking and how different function like AND, OR used to solve by this network.
I couldnt understand properly through the google.
","['neural-networks', 'machine-learning', 'hebbian-learning']","
In machine learning, the idea behind Hebbian learning is to strengthen (or weaken) the connection (the weight) between the neurons that have similar (or, respectively, dissimilar) outputs, where ""similar"" can be defined in different ways (e.g. it could be based on the sign of the output of the neurons). 
Hebbian learning is a more biologically plausible way of learning than back-propagation, because it is a ""local learning strategy"" (you locally update the connections and not all the connections of the model at the same time), as opposed to back-propagation, which is a ""global learning strategy"" (where all connections are usually updated at once, given a ""global error"" of the network).
There are several neural networks (or models) that can learn in a Hebbian fashion: for example, the Hopfield network or Numenta's temporal memory. 
"
How can I suppress a CNNs translation invariant or translation equivariant?,"
I am trying to understand this post, but I get confused by the definitions and the differences. What's definition of equivariant?
If I remove all the pooling layers from a CNN, will it make the network to detect features in pixel resolution? For example, detecting the local maximum of a pixel. For example, can a CNN be designed to return True for the following case?

And False for the shifted window:

In the second case it returns false because the 3x3 submatrix isn't centered (yellow dash line) around the local maximum.
Will an architecture that is
from keras.layers import Dense, Conv2D, Flatten
model = Sequential()
model.add(Conv2D(128, kernel_size=2, activation=relu, padding='same', input_shape=(3,3,1)))
model.add(Conv2D(64, kernel_size=2, activation=relu, padding='same'))
model.add(Flatten())
model.add(Dense(10, activation=softmax))

be able to differentiate between the tiling of the larger grayscale image?
","['machine-learning', 'convolutional-neural-networks', 'keras']","
Question 1
To make it simple

You have a transformation $T$ and an operator $C$ acting on a given input $x$
Let's say you do this experiment

compute $y_{1} = C(T(x))$
compute $y_{2} = C(x)$

You can get three different results:

$y_{1} = y_{2}$ then you can say the operator is invariant with respect to the given transformation
$y_{1} = T(y_{2})$ then you can say the operator is equivariant to the given transformation as applying it to the input basically reflects its effect completely on the output
none of the 2


Questions 2
Q2.1

Spatial Pooling is not the only way to perform dimensionality reduction. You can achieve it even simply applying the Conv Kernel with no padding.

For example, let's say your input is a WxHxC tensor and you are applying a kernel which is KxKxC the result will have spatial domain size (W-(K/2))x(H-(K/2)) with K/2 the integer truncation so if K=5 then K/2=2.
Alternatively you can reduce the spatial domain with stride.
Q2.2

It seems to me you are talking about a sort of Non Max Suppression Operator rather an emergent behaviour, but certainly with the proper supervision signal you can train a CNN to do this work (even if practically it does not make sense as you can explicitly define it).

"
How to properly optimize shared network between actor and critic?,"
I'm building an actor-critic reinforcment learning algorithm to solve environments. I want to use a single encoder to find representation of my environment.
When I share the encoder with the actor and the critic, my network isn't learning anything:
class Encoder(nn.Module):
  def __init__(self, state_dim):
    super(Encoder, self).__init__()

    self.l1 = nn.Linear(state_dim, 512)

  def forward(self, state):
    a = F.relu(self.l1(state))
    return a

class Actor(nn.Module):
  def __init__(self, state_dim, action_dim, max_action):
    super(Actor, self).__init__()

    self.l1 = nn.Linear(state_dim, 128)
    self.l3 = nn.Linear(128, action_dim)

    self.max_action = max_action

  def forward(self, state):
    a = F.relu(self.l1(state))
    # a = F.relu(self.l2(a))
    a = torch.tanh(self.l3(a)) * self.max_action
    return a

class Critic(nn.Module):
  def __init__(self, state_dim, action_dim):
    super(Critic, self).__init__()

    self.l1 = nn.Linear(state_dim + action_dim, 128)
    self.l3 = nn.Linear(128, 1)

  def forward(self, state, action):
    state_action = torch.cat([state, action], 1)

    q = F.relu(self.l1(state_action))
    # q = F.relu(self.l2(q))
    q = self.l3(q)
    return q

However, when I use different encoder for the actor and different for the critic, it learn properly.
class Actor(nn.Module):
def __init__(self, state_dim, action_dim, max_action):
    super(Actor, self).__init__()

    self.l1 = nn.Linear(state_dim, 400)
    self.l2 = nn.Linear(400, 300)
    self.l3 = nn.Linear(300, action_dim)

    self.max_action = max_action

def forward(self, state):
    a = F.relu(self.l1(state))
    a = F.relu(self.l2(a))
    a = torch.tanh(self.l3(a)) * self.max_action
    return a

class Critic(nn.Module):
  def __init__(self, state_dim, action_dim):
    super(Critic, self).__init__()

    self.l1 = nn.Linear(state_dim + action_dim, 400)
    self.l2 = nn.Linear(400, 300)
    self.l3 = nn.Linear(300, 1)

  def forward(self, state, action):
    state_action = torch.cat([state, action], 1)

    q = F.relu(self.l1(state_action))
    q = F.relu(self.l2(q))
    q = self.l3(q)
    return q

Im pretty sure its becuase of the optimizer. In the shared encoder code, I define it as foolow:
self.actor_optimizer = optim.Adam(list(self.actor.parameters())+
                                      list(self.encoder.parameters()))
self.critic_optimizer = optim.Adam(list(self.critic.parameters()))
                                         +list(self.encoder.parameters()))

In the seperate encoder, its just:
self.actor_optimizer = optim.Adam((self.actor.parameters()))
self.critic_optimizer = optim.Adam((self.critic.parameters()))

two optimizers must be becuase of the actor critic algorithm, in which the loss of the actor is the value.
How can I combine two optimizers to optimize correctly the encoder?
","['neural-networks', 'reinforcement-learning', 'optimization', 'actor-critic-methods', 'pytorch']","
Just use one class inheriting from nn.Module called e.g. ActorCriticModel. 
Then, have two members called self.actor and self.critic and define them to have the desired architecture.Then, in the forward() method return two values, one for the actor output (which is a vector) and one for the critic value (which is a scalar). 
This way you can use only one optimizer.
"
Picking a random move in exploitation in Q-Learning,"
I've been unsure about a principle of Q-Learning, I was hoping someone could clear it up.
When a new state is encountered, and thus there are no existing Q values, and that the algorithm decides to exploit, and not explore, how is the move chosen, since all the values are 0?
Is it chosen randomly? This intuitively would make sense, since after this, the state-move pair would have a value  and thus the matrix would get filled up throughout the iterations. But I just want to make sure I understand this correctly...
Thanks
","['reinforcement-learning', 'q-learning']",
What is the goal of a constraint solver?,"
What is the goal of a constraint solver? How are constraints propagated in a constraint satisfaction search?
Any references are also appreciated. 
","['definitions', 'constraint-satisfaction-problems']","
You want Chapter 6 of Russell & Norvig's AI: A Modern Approach, for a starting place.
The goal of a constraint solver is to find an assignment of values to variables such that every variable is assigned a value, but none of a list of constraints are violated.
An example problem that a constraint solver can solve is the problem of assigning students to groups for a project. The variables are the names of the students. Each variable has a set of possible values it could take on that correspond to the names of the groups. The constraints prohibit assigning certain students to the same group (perhaps they hate each other), or require certain students to be assigned to the same group (perhaps they like each other). More complex constraints are also possible (e.g. at least 2 of these 5 students need to be in the same group).
Constraint propagation is the idea that several constraints might logically imply a stronger constraint. As a simple example, suppose that student $S$ must be assigned to group $1$. That's one constraint. Suppose further that student $T$ must be assigned to the same group as $S$. That's another constraint. Together, however, they imply that $T$ must be assigned to group $1$. 
Most constraint solvers operate by iteratively trying to assign values to variables and then backtracking when a constraint is violated. When assignments are made, different constraint propagation methods make different decisions about how much computational effort to spend deriving the logical consequences of constraints, as opposed to simply trying more assignments of values to variables until the consequences become apparent (one of the existing constraints gets violated). 
The simplest form of constraint propagation is called forward checking. After each speculative assignment of a value to a variable, forward checking looks at all the constraints the variable is involved in. It then derives the logical consequences of the assignments. For example, continuing with the student problem from above, if $S$ was assigned value $1$, then forward checking would notice the constraint $S=T$, and would, effectively, add the constraint $T=1$ (it actually does something a little different, but it has this effect). As another example, if the assignment $T=2$ were made, forward checking would notice that $S=T$, and would add the constraint $S=2$. It would also detect the contradiction that $S=1$ and $S=2$ are both constraints, and would declare the assignment $T=2$ invalid. 
More advanced constraint propagation methods can reason about these kinds of logical consequences before assignment are made. For example, they could derive that $T=1$ before the program starts. Wikipedia has a good summary of these, starting with arc consistency.
"
Point Cloud Alignment using a Neural Network?,"
Having two point clouds, the second being a transformation of the first, how could I utilize a neural network in order to solve the pose (transformation in terms of x, y, z, rx, ry, rz) of the second point cloud?
Since the point clouds can be rather large (~200,000 points), I think it'd be best to first select regions from each point cloud and see if their geometries are similar (still researching the optimal method for this). If they are not similar, I'd choose two new points. If they are similar, I'd use those two regions when implementing the neural network to discern the pose.
My preliminary research has led me to believe that a Siamese neural network may work in this scenario but I'm not sure if there are better alternatives. One of my goals is to accomplish this without relying on Iterative Closest Point. Any and all insight is appreciated. Thanks.
","['neural-networks', 'convolutional-neural-networks']",
Is there an LSTM-based unsupervised learning algorithm to label a dataset of curves?,"
I have a big amount of light curves (image below). 

I am trying to label the points as signal or background (the signal appears usually periodically, several times, for a given light curve). 
More precisely, I want to identify the downward spikes (class label = 1) from the background (class label = 0).
However, the data is not labeled. I tried labeling it by hand, and using a bi-directional LSTM succeeds in labeling the data points properly. However, there are thousands of light curves and labeling all of them would take very long. 
Is there any good unsupervised approach to do this (unsupervised LSTM maybe, but any other method that might work on time series would do just fine)? 
","['long-short-term-memory', 'datasets', 'unsupervised-learning', 'data-preprocessing']","
You can try to work with Gated Recurrent Units or GRU. This will solve your problem of with too much latency time that LSTM required. LSTM also doesn't give preference on newer data too. For more information, you can follow more on this great article
GRU Cells
"
How to stop DQN Q function from increasing during learning?,"
Following the DQN algorithm with experience replay:
Store transition $\left(\phi_{t}, a_{t}, r_{t}, \phi_{t+1}\right)$ in $D$ Sample random minibatch of transitions $\left(\phi_{j}, a_{j}, r_{j}, \phi_{j+1}\right)$ from $D$ Set
$$y_{j}=\left\{\begin{array}{cc}r_{j} & \text { if episode terminates at j+1} \\ r_{j}+\gamma \max _{d^{\prime}} \hat{Q}\left(\phi_{j+1}, a^{\prime} ; \theta^{-}\right) & \text {otherwise }\end{array}\right.$$
Perform a gradient descent step on $\left(y_{j}-Q\left(\phi, a_{j} ; \theta\right)\right)^{2}$ with respect to the network parameters $\theta$.

We calculate the $loss=(Q(s,a)-(r+Q(s+1,a)))^2$.


Assume I have positive but changing rewards. Meaning, $r>0$.

Thus, since the rewards are positive, by calculating the loss, I notice that almost always $Q(s)< Q(s+1)+r$.
Therefore, the network learns to always increase the $Q$ function , and eventually, the $Q$ function is higher in same states in later learning steps.
How can I stabilize the learning process?
","['reinforcement-learning', 'q-learning', 'dqn', 'objective-functions', 'value-functions']","

You can use discount factor gamma less then one.
You can use finite time horizon - only for states which are no farther away then T time steps reward propagate back
You can use sum of rewords averaged over time for Q

All of those are legitimate approaches.
"
How does a PDDL solver find a solution for a given problem?,"
As far as I know, in PDDL, an environment is designed as well as the initial state described. When we describe the target state, the solver creates some sort of a graph. How is the graph built and what are the keys (keywords) in PDDL referring to?
I know that there are many flavours of PDDL, but let's go with the standard or the most common version of PDDL.
","['planning', 'pddl']","
The question doesn't really make sense: PDDL is a description language that is used to formulate a problem. This description then is the input to a planner; how the planner arrives at the intended solution is not related to the PDDL description.
There are a number of planning algorithms, and you can implement any of them to make use of a PDDL description. The output of the solver is a plan, which is usually an ordered sequence of actions, and a tree or graph structure might be a good way of capturing this.
"
Alphazero policy head loss not decreasing,"
I am now working on training an alphazero player for a board game. The implementation of board game is mine, MCTS for alphazero was taken elsewhere. Due to complexity of the game, it takes a much longer time to self-play than to train.
As you know, alphazero has 2 heads: value and policy. In my loss logging I see that with time, the value loss is decreasing pretty significantly. However, the policy loss only demonstrates fluctuation around its initial values. 
Maybe someone here has run into similar problems? I would like to know if its my implementation problem (but then the value loss is decreasing) or just a matter of not enough data.
Also, perhaps importantly, the game has ~17k theoretically possible moves, but only 80 at max are legal at any single state (think chess - a lot of possibles but very few are actually legal at any given time). Also, if MCTS has 20 simulations, then the improved probabilities vector (against which we train our policy loss) will have at most 20 non-zero entries. My idea was that it might be hard for the network to learn such sparse vectors.
Thank you for any ideas!
","['neural-networks', 'reinforcement-learning', 'objective-functions', 'alphago', 'alphazero']",
How to combine features with different temporal scale in machine learning,"
We have various types of data features with different temporal scale. For example, some of them describe the state per second while others may describe the state per day or per month from another aspect. The former features are dense on the time scale and latter features are sparse. Simply concatenate them into one feature vector seems not proper. Is there any typical method in machine learning can handle with problem ?
","['machine-learning', 'feature-selection']",
Does reinforcing correct predictions increase model accuracy further?,"
Let's say I've trained a CNN that is predicting/inferring live samples that it hasn't seen before. In the event the network makes a correct prediction, would including this as a new sample in its training set increase the model accuracy even further when re-training the network?
I'm unsure about this since it seems as though the network has already learnt the necessary features for making the correct prediction, so adding it as a new training sample might be redundant. On the other hand it might also reinforce to the network that it's on the right track, perhaps giving it further confidence to generalize with whatever features its learnt in regards to that class, that it might be able to apply to the same class in other images it might otherwise make an incorrect prediction with?
The reason I'm thinking of this is that manually labeling each image is a time-consuming process, however if a simple ""Correct/Incorrect"" popup box was presented after the network made a live prediction, then it's simply a matter of clicking a single button to generate a new labelled training sample, which would be a far easier labeling task.
So how useful would it be to do something like this?
",['neural-networks'],
"What are some examples of tasks in which, currently, neuroevolution outperforms gradient-based approaches?","
Note: I am NOT asking for general advantages of neuroevolution over standard approaches (e.g.: architecture search, parallelization), I am asking for examples of tasks in which, currently, neuroevolved networks outperform ANNs trained with gradient-based techniques. Of course, this is not opinion based, as I am asking for examples based on facts.
","['neural-networks', 'neat', 'neuroevolution']",
How is it possible that the MSE used to train neural networks with gradient descent has multiple local minima?,"
We often train neural networks by optimizing the mean squared error (MSE), which is an equation of a parabola $y=x^2$, with gradient descent.
We also say that weight adjustment in a neural network by the gradient descent algorithm can hit a local minimum and get stuck in there.
How are multiple local minima on the equation of a parabola possible, if a parabola has only one minimum?
","['neural-networks', 'deep-learning', 'optimization', 'gradient-descent', 'mean-squared-error']","

How are multiple local minima on the equation of a parabola possible, if a parabola has only one minimum?

A parabola has one minimum, and no separate local minima. So it isn't possible.
However...

Gradient descent works on the equation of mean squared error, which is an equation of a parabola $y=x^2$

Just because the loss function is a parabola with respect to the direct input, does not mean that the loss function is a parabola with respect to the parameters that indirectly cause that error.
In fact it only remains true for linear functions. When considering linear regression $\hat{y} = \sum_i w_i x_i + b$, there is only one global minimum (with specific values of $w_i$ or specific vector $\mathbf{w}$), and your assertion is true.
Once you add nonlinear activations, as in neural networks, then the relationship between error function and parameters of the model becomes far more complex. For the last/output layer you can carefully choose a loss function so that this cancels out - you can keep your single global minimum for logistic regression and softmax regression. However, one or more hidden layers, and all bets are off.
In fact you can prove quite easily that a neural network with a hidden layer must have multiple stationary points (not necessarily local minima). The outline of the proof is to note that there must be multiple equivalent solutions, since in a fully-connected network you can re-arrange the nodes into any order, move the weights to match, and it will be a new solution with exactly the same behaviour, including the same loss on the dataset. So a neural network with one hidden layer with $n$ nodes must have $n!$ absolute minimums. There is no way for these to exist without other stationary points in-between them.
There is theory to suggest that most of the stationary points found in practice will not be local minima, but saddle points.
As an example, this is an analysis of saddle points in a simple XOR approximator.
"
Binary vector expected value,"
Raul Rojas' Neural Networks A Systematic Introduction, section 8.2.1 calculates the variance of the output of a hidden neuron.
Raul Rojas says that ""for binary vectors we have $E[x_i^2] = \frac{1}{3}$"" where $x_i$ is the input value transported through each edge to a node.
I don't quite get how he reaches this result.
Thank you for your time :)
","['neural-networks', 'probability-distribution']",
Does changing the order of the convolution layers in a CNN have any impact?,"
Could changing the order of convolution layers in a CNN improve accuracy or training time?
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks']",
Is there any example of using Q-learning with big data?,"
Could we even use reinforcement learning with big datasets? 
Or in RL does the agent built its own dataset ?
",['reinforcement-learning'],
Is anyone able to reproduce Hinton's matrix capsule networks?,"
I've been working on Hinton's matrix capsule networks for several months. I searched each corner of the internet. But I couldn't find anyone that can reproduce Hinton's matrix capsule network. Can anyone get the reported accuracy on SmallNORB and Cifar10 dataset?
PS: I know Hinton's another paper on capsule networks Dynamic Routing Between Capsules is reproducible. Please, do not confuse the two papers.
","['deep-learning', 'capsule-neural-network']",
How do I choose the search algorithm for a particular task?,"
How do I choose the search algorithm for a particular task? Which criteria should I take into account?
","['algorithm', 'search']",
What are the domains where SVMs are still state-of-the-art?,"
It seems that deep neural networks and other neural network based models are dominating many current areas like computer vision, object classification, reinforcement learning, etc.
Are there domains where SVMs (or other models) are still producing state-of-the-art results?
","['machine-learning', 'support-vector-machine', 'state-of-the-art']","
State-of-the-art is a tough bar, because it's not clear how it should be measured. An alternative criteria, which is akin to state-of-the-art, is to ask when you might prefer to try an SVM.
SVMs have several advantages:

Through the kernel trick, the runtime of an SVM does not increase significantly if you want to learn patterns over many non-linear combinations of features, rather than the original feature set. In contrast, a more modern approach like a deep neural network will need to get deeper or wider to model the same patterns, which will increase its training time.
SVMs have an inherent bias towards picking ""conservative"" hypotheses, that are less likely to overfit the data, because they try to find maximum margin hypotheses. In some sense, they ""bake-in"" Occam's razor.
SVMs have only two hyperparameters (the choice of kernel and the regularization constant), so they are very easy to tune to specific problems. It is usually sufficient to tune them by performing a simple grid-search through the parameter space, which can be done automatically.

SVMs also have some disadvantages:

SVMs have a runtime that scales cubically in the number of datapoints you want to train on (i.e. $O(n^3)$ runtime)1. This does not compare well with, say, a typical training approach for a deep neural network which runs in $O(w*n*e)$ time, where $n$ is the number of data points, $e$ is the number of training epochs, and $w$ is the number of weights in the network. Generally $w, e << n$.
To make use of the Kernel trick, SVMs cache a value for the kernelized ""distance"" between any two pairs of points. This means they need $O(n^2)$ memory. This is far, far, more trouble than the cubic runtime on most real-world sets. More than a few thousand datapoints will leave most modern servers thrashing, which increases effective runtime by several orders of magnitude. Together with point 1, this means SVMs will tend to become unworkably slow for sets beyond maybe 5,000-10,000 datapoints, at the upper limit. 

All of these factors point to SVMs being relevant for exactly one use case: small datasets where the target pattern is thought, apriori, to be some regular, but highly non-linear, function of a large number of features. This use case actually arises fairly often. A recent example application where I found SVMs to be a natural approach was building predictive models for a target function that was known to be the result of interactions between pairs of features (specifically, communications between pairs of agents). An SVM with a quadratic kernel could therefore efficiently learn conservative, reasonable, guesses.

1 There are approximate algorithms that will solve the SVM faster than this, as noted in the other answers.
"
Can bounding boxes further improve the performance of a CNN classifier?,"
Suppose I have a standard image classification problem (i.e. CNN is shown a single image and predicts a single classification for it). If I were to use bounding boxes to surround the target image (i.e. convert this into an object detection problem), would this increase classification accuracy purely through the use of the bounding box? 
I'm curious if the neural network can be ""assisted"" by us when we show it bounding boxes as opposed to just showing it the entire image and letting it figure it all out by itself.
","['convolutional-neural-networks', 'object-detection', 'object-recognition', 'bounding-box']","
Another way to ask the question is: Does sound get clearer when you remove the background noise? 
The obvious answer is yes and in the case of image classification, the answer is also generally yes.
In most cases reducing the noise (irrelevant pixels) will strengthen the signal (activations) the neural network is trying to find. 
"
Will robots rebel against their human creators?,"
There are several science fiction movies where the robots rebel against their creators: for example, the Terminator's series or I Robot. 
In the future, is it possible that robots will rebel against their human creators (like in the mentioned movies)?
","['control-problem', 'mythology-of-ai', 'neo-luddism', 'ai-takeover']",
Is there any AI system for finding the best way to schedule university classes?,"
I was wondering whether there is an AI system which could be used to resolve the class clashes problem which mostly happens in universities. In almost every university students face this problem, where two or more courses that many students want to take together get scheduled at the same time. Does anyone know about a system which resolves this issue or someone who works on this problem?
","['planning', 'symbolic-ai']",
Why experience reply memory in DQN instead of a RNN memory?,"
I was trying to implement a DQN without experience reply memory, and the agent is not learning anything at all. I know from readings that experience reply is used for stabilizing gradients. But how important is experience reply in DQN and similar RL algorithms? If the model needs to learn from memory, why don't we use a recurrent network, which has inbuilt memory to it? What is the advantage of experience reply over a recurrent memory?
","['reinforcement-learning', 'dqn', 'deep-rl', 'long-short-term-memory', 'experience-replay']",
How should the values of the filters of a CNN change?,"
I wrote a convolutional neural network for the MNIST dataset with Numpy from scratch. I am currently trying to understand every part and calculation.
But one thing I noticed was the ""just positive"" derivative of the ReLU function.
My network structure is the following:

(Input 28x28)
Conv Layer (filter count = 6, filter size = 3x3,  stride = 1)
Max Pool Layer (Size 2x2) with RELU
Conv Layer (filter count = 6, filter size = 3x3,  stride = 1)
Max Pool Layer (Size 2x2) with RELU
Dense (128)
Dense (10)

I noticed, when looking at the gradients, that the ReLU derivative is always (as it should be) positive. But is it right that the filter weights are always decreasing their weights? Or is there any way they can increase their weight?
Whenever I look at any of the filter's values, they decreased after training. Is that correct?
By the way, I am using stochastic gradient descent with a fixed learning rate for training.
","['convolutional-neural-networks', 'backpropagation', 'relu', 'convolution']","
The weights of the filters do not always and necessarily decrease. Consider the extreme case when you initialise them to $-\infty$ and you want to approximate a function different than the one the CNN represents initially with all weights set to $-\infty$. You will have to increase one or more weights. 
"
DQN Agent not learning anymore - what can I do to fix this?,"
I am trying to use Deep-Q-Learning to learn an ANN which controls a 7-DOF robotic arm. The robotic arm must avoid an obstacle and reach a target.
I have implemented a number of state-of-art techinques to try to improve the ANN performance. Such techniques are: PER, Double DQN, adaptive discount factor, sparse reward. I have also tried Dueling DQN but it performed poorly. I have also tried a number of ANN architectures and it looks like that 2 hidden layers with 128 neurons is the best one so far. My input layer is 12 neurons, the output 10 neurons.
However, as you can see from the image down here, at a certain point the DQN stops learning and gets stuck at around 80% of success rate. I don't understand why it gets stuck, because in my opinion we could reach an higher success rate, 90% at least, but I just can't get out of that ""local minimum"".
so, my question is: What are some techniques I can try to unstuck a DQN from something that looks like a local minimum?
figure:

note: the success rate in this picture is computed as the number of successes in the last 100 runs.
","['deep-learning', 'dqn', 'deep-rl']",
What is the input to AlphaGo's neural network?,"
I have been reading an article on AlphaGo and one sentence confused me a little bit, because I'm not sure what it exactly means. The article says: 

AlphaGo Zero only uses the black and white stones from the Go board as its input, whereas previous versions of AlphaGo included a small number of hand-engineered features.

What exactly is the input to AlphaGo's neural network? What do they mean by ""just white and black stones as input""? What kind of information is the neural network using? The position of the stones?
","['neural-networks', 'deep-learning', 'reinforcement-learning', 'architecture', 'alphago-zero']","

The input to the neural network is a $19  19  17$ image stack
  comprising $17$ binary feature planes. $8$ feature planes $X_t$ consist of binary values indicating the
  presence of the current players stones ($X^i_t = 1$ if intersection $i$ contains a stone of the players
  colour at time-step $t$; $0$ if the intersection is empty, contains an opponent stone, or if $t < 0$). A
  further $8$ feature planes, $Y_t$
  , represent the corresponding features for the opponents stones. The
  final feature plane, $C$, represents the colour to play, and has a constant value of either $1$ if black
  is to play or $0$ if white is to play. These planes are concatenated together to give input features
  $s_t = [
X_t, Y_t, X_{t1}, Y_{t1}, ..., X_{t7}, Y_{t7}, C]$.

This and all the other architecture details can be found in the ""Neural Network Architecture"" section in the paper.
"
How is the policy gradient calculated in REINFORCE?,"
Reading Sutton and Barto, I see the following in describing policy gradients:

How is the gradient calculated with respect to an action (taken at time t)? I've read implementations of the algorithm, but conceptually I'm not sure I understand how the gradient is computed, since we need some loss function to compute the gradient. 
I've seen a good PyTorch article, but I still don't understand the meaning of this gradient conceptually, and I don't know what I'm looking to implement. Any intuition that you could provide would be helpful. 
","['reinforcement-learning', 'policy-gradients', 'sutton-barto', 'notation', 'reinforce']",
Will we be able to build an artificial intelligence that feels empathy?,"
Nowadays, robots or artificial agents often only perform the specific task they have been programmed to do.
Will we be able to build an artificial intelligence that feels empathy, that understands the emotions and feelings of humans, and, based on that, act accordingly?
","['ai-design', 'philosophy', 'emotional-intelligence']","
It seems to me that empathy is based on understanding the experience of another entity:

originally Psychology. The ability to understand and appreciate another person's feelings, experience, etc. SOURCE: OED

Using this definition, the AI would have to understand human experience.  (There may be a ""Chinese Room"" issue in terms of whether one considers the algorithm to truly ""understand"".  But, if it can classify the input sufficiently to produce an appropriate response, that can constitute understanding.)
The underlying problem is that the algorithm likely doesn't ""feel"" in the same way humans experience emotions, in that the human experience is colored by chemical response.  So while the algorithm might be able to demonstrate sufficient ""understanding"" of a human's experience, and act in an empathetic manner, the degree of understanding may always be limited.
"
Training LSTM with class imbalance,"
I need to train an LSTM on some light curves, in order to find a signal (there are 2 classes signal and background). However the signal (data points corresponding to signal) is around 100 times less frequent than the background so I have a huge class imbalance and in the end all points are labelled as background. I tried to use focal loss, but it doesn't help. Is there a way to make it work?
",['long-short-term-memory'],
Encourage Deep Q to seek short-term reward,"
I understand that gamma is an important factor in determining the rewards for a deep Q agent, however during testing of my network I am noticing that the agent is outputting more actions to ""do nothing"" as it learns more about it's given data set (financial stock data).
I have tried tweaking the gamma at different levels ranging from 0 - 1 and everywhere in between, however as the agent continues to learn, the times between actions is getting longer, and longer. This behaviour is undesirable, and it is preferable that the agent be making more often, short-term actions even if they result in reduced reward.
Does anyone have any tips on how to achieve this? Would a minus gamma have adverse effects on the network?
TLDR:
Time between actions becoming increasingly long over time, would prefer an agent that makes many actions over long-term ones.
","['deep-learning', 'q-learning', 'dqn', 'rewards']",
How should I encode a categorical input?,"
Let's say you have an input which can take one of 10 different unique values. How would you encode it?

Have input length 10 and one-hot encode it.
Have 1 input but normalise the value between the input range.

Would the end result be the same?
","['neural-networks', 'machine-learning', 'ai-design']",
Policy gradient loss for neural network training,"
Say i want to train a neural network with 10 classes as outputs and use categorical_cross_entropy as a loss function in keras. This will try
to fit the training data as best as possible irregardless of the outcome (i.e. value). If I want to take value into account, I have to use something like a policy gradient RL algorithm. How do I formulate the loss of policy gradient algorithm in this case ?
The standard categorical cross entropy loss function is as follows where y_ = true value, and y = predicted value:
   loss = -mean( y_ * log(y))

I am thinking to just multiply the true value by the reward and still use the categorical cross entropy of keras i.e.
   y_ = y_ * reward
   loss = -mean( y_* log(y) )

Is my interpretation correct ?
","['reinforcement-learning', 'keras', 'policy-gradients', 'reinforce']",
Auto-regression - Reduce error in prediction,"
I am trying to develop a time series model using autoregression. The data set is like as follows
INDEX MAXIMA
  0   0.743
  1   0.837
  2   0.838
  4   0.896
  5   1.014
  6   1.003
  7   1.01
  8   1.101
  9   1.097

The Maxima point is given is the largest points on each curve. Basically, I have to perform multi-step forecasting (at least 9 steps ahead). I've done it using the recursive approach. but the accuracy of the prediction getting worse as it reaches the end.

Predicted Result


PYTHON CODE
Using the AR model from stats model
# fit model for MAX VALUE
  model = AR(data)
  model_fit = model.fit()
  yhat_max = model_fit.predict(len(data),len(data]))

For obtaining an accurate prediction, What changes should be done in the approach? or Do I have to change the model?
Any kind of help is appreciated.
","['python', 'linear-regression', 'statistical-ai']","
The predictions tend to move towards the mean of the series as one predicts for longer horizons. Also, in general, optimal long range forecast is the process mean.
In other words, the past of the process contains no information on the development of the process in the distant future.
And, this might be the reason that you are getting poor forecasts. 

What changes should be done in the approach? or Do I have to change the model?

You might want to move to ARIMA models. See how they perform. 
If you have some other time series that might act as explanatory variables then you might want to augment the dataset and use the ARIMAX model. 
Third option would be to try out RNN if you have lots of data. You can also try hybrid models like ARIMA and RNN together. 
"
Understanding how the loss was calculated for the SQuAD task in BERT paper,"
In the BERT paper, section 4.2 covers the SQuAD training. 
From my understanding, there are two extra parameters trained, they are two vectors with the same dimension as the hidden size, so the same dimensions as the contextualized embeddings in BERT. They are S (for start) and E (for End). For each, a softmax is taken with S and each of the final contextualized embeddings to get a score for the correct Start position. And the same thing is done for E and the correct end position.
I get up to this part. But I am having trouble figuring out how the did the labeling and final loss calculations, which is described in this paragraph

and the maximum scoring span is used as the prediction. The training objective is the loglikelihood of the correct start and end positions.

What do they mean by ""maximum scoring span is used as the prediction""?
Furthermore, how does that play into 

The training objective is the loglikelihood of the correct start and end positions

From this source: https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/
It says the log-likelihood is only applied to the correct classes. So, we are only calculating the softmax for the correct positions only, not any of the incorrect positions.
If this interpretation is correct, then the loss will be
Loss = -Log( Softmax(S*T(predictedStart) / Sum(S*Ti) ) -Log( Softmax(E*T(predictedEnd) / Sum(S*Ti) )

","['natural-language-processing', 'objective-functions', 'papers', 'bert']",
How to identify the areas to reduce over fitting?,"
I am trying classify CIFAR10. The CNN that I generated over fits when the accuracy reaches ~77%. The code and the plot is given below. I tried DropOut, Batch Normalization and L2 Regularization. But the accuracy does not go beyond ~77.  
How can I identify the areas to be corrected to reduce over fitting? 
convolutional_model = Sequential()

# 32
convolutional_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), kernel_regularizer=regularizers.l2(.0002)))
convolutional_model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(.0002)))
convolutional_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

# 64
convolutional_model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(.0002)))
convolutional_model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(.0002)))
convolutional_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

convolutional_model.add(Flatten())
convolutional_model.add(Dropout(0.5))

convolutional_model.add(Dense(128, activation='relu'))
convolutional_model.add(Dense(10, activation='softmax'))

print(convolutional_model.summary())
convolutional_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=8)
history = convolutional_model.fit(X_Train_Part, Y_Train_Part, epochs=200, verbose=2,validation_data=(X_Train_Validate, Y_Train_Validate), callbacks=[es])

scores = convolutional_model.evaluate(X_Test, Y_Test, verbose=2)


","['deep-learning', 'convolutional-neural-networks', 'overfitting']","
It is a difficult task to identify the areas to be corrected to improve accuracy unless that area is looking into your face. By that I mean unless the regularizing parameter has unusually large or small values it is difficult to pin down and identify which regularizer to tweak. You need to do a grid search or random search over the hyper-parameter space to come up with an optimal combination of hyper parameters. 
There could be a lot of things that you could consider to reduce over fitting. Some of them are $L_2$ regularizers, Dropout, depth of the network, number of neurons in the layers, the optimizer,  etc. 
"
What does the formula $1-\sum_i(e_i-a_i)^2$ mean in this NEAT Python API?,"
I have looked at the documentation for the NEAT Python API found here, where it's written

The error for each genome is $1-\sum_i(e_i-a_i)^2$

I have not yet learned calculus, so I can't understand this formula. So, can someone please explain what the calculation means?
","['math', 'neat', 'neuroevolution', 'notation']","
$$1-\sum_i(e_i-a_i)^2$$
$\sum$ - there just means sum. It is the greek letter for S.  You can rewrite the above formula as
$$1 -[(e_1 - a_1)^2+(e_2-a_2)^2+(e_3-a_3)^2+\ldots ]$$
$\sum$ just helps us avoid writing dozens of $+$ signs. Read more here.
What they are doing here is taking the difference of expected value $e_1$ and the actual value $a_1$ for the 1st example, and so on. The difference can be positive ($e_1 > a_1$) or negative ($e_1 < a_1$), so usually we square the difference to make it positive number.
The rest is there in the docs. Try putting in concrete imagined values for $a_i$ and $e_i$.
"
Intuition behind $\gamma$-discounted state frequency,"
At the appendix A of paper ""near-optimal representation learning for hierarchical reinforcement learning"", the authors express the $\gamma$-discounted state visitation frequency $d$ of policy $\pi$ as
$$
d=(1-\gamma)A_\pi(I-\gamma^cP_\pi^c)^{-1}\mu\tag 1
$$
I've simplifed the notation for easy reading, hoping it does not introduce any error. In the above definition, $P_\pi^c$ the $c$-step transition matrix under the policy $\pi$, i.e., $P_{\pi}^c=P_\pi(s_{c}|s_0)$, $\mu$ a Dirac $\delta$ distribution centered at start state $s_0$ and 
$$
A_\pi=I+\sum_{k=1}^{c-1}\gamma^kP_\pi^k\tag 2
$$
They further give the every-$c$-steps $\gamma$-discounted state frequency of $\pi$ as
$$
d^c_\pi=(1-\gamma^c)(I-\gamma^cP_\pi^c)^{-1}\mu\tag 3
$$
To my best knowledge, $A_\pi$ seems to be the unnormalized $\gamma$-discounted state frequency, but I cannot really make sense of the rest.
I'm hoping that someone can shed some light on these definitions.
Update
Thank @Philip Raeisghasem for pointing out the paper CPO. Here's what I've gotten from that.
Applying the sum of the geometric series to Eq.$(2)$, we have
$$
A={(I-\gamma^cP_\pi^c)(I-\gamma P_\pi)^{-1}}\tag4
$$
Plugging Eq.$(4)$ back into Eq.$(1)$, we get the same result as Eq.$(18)$ in the CPO paper:
$$
d=(1-\gamma)(I-\gamma P_\pi)^{-1}\mu\tag 5
$$
where $(1-\gamma)$ normalizes all weights introduced by $\gamma$ so that they are summed to one. However, I'm still confused. Here are the questions I have

Eq.$(5)$ indicates Eq.$(1)$ is the state frequency in the infinite horizon. But I do not understand why we have it in the hierarchical policy. To my best knowledge, policies here are low-level, which means they are only valid in a short horizon ($c$ steps, for example). Computing state frequency in the infinite horizon here seems confusing.
What should I make of $d_\pi^c$ defined in Eq.$(3)$, originally from Eqs.$(26)$ and $(27)$ in the paper? The authors define them as every-$c$-steps $\gamma$-discounted state frequencies of policy $\pi$. But I do not see why it is the case. To me, they are more like the consequence of Eq.$(30)$ in the paper.

Sorry if anyone feels that this update makes this question too broad. This is kept since I'm not so sure whether I can get a satisfactory answer without these questions.
Any partial answer will be sincerely appreciated. Thanks in advance.
",['reinforcement-learning'],
"Monte-Carlo, every-visit gridworld, exploring starts, python code gets stuck in foreverloop in episode generation","
I've been trying to implement policy improvement for Q(s,a) function as per Sutton&Barto reinforcement learning book. The original algorithm with first-visit MonteCarlo is pictured below.

I remember the book earlier mentioning that the every-visit variation simply omits the first-visit check ""unless the pair blah,blah..."" but otherwise the algorithms should be the same (???)
Initial policy in the very first iteration (first episode), should be equiprobable randomwalk. In general terms, if action takes you outside the border of the gridworld (4x4), then you simply bounce back into where you started from, but reward will have been given, and action will have been taken.
I have verified that my code gets stuck (sometimes) in the episode generation portion of my code in foreverloop for some reason, early on in the code (iterations amount in the outerloop is small). Even though, I thought I followed the pseudocode rather well, but it's really annoying that sometimes the code gets stuck in foreverloop.
The reason must be that for some reason my code updates from the equiprobable randomwalk policy => into deterministic policy but in a wrong way, such that it can create foreverloops in the episode generation after the first episode has ran (it must have ran the entire first episode with equiprobable randomwalk). The below picture shows that if you get into any state in the marked box, you cannot get out from there and get stuck in episode generation...

If the random generator seed it lucky, then it can usually get ""over the hump"" and proceed to make the required number of iterations in the outerloop, and output an optimal policy for the gridworld.
Here is the python code (from my experience I ran it in debugger and had to restart it a couple of times, but usually it will show quite fast that it gets stuck into the episode generation, and cannot proceed in the iterations in outerloop.
import numpy as np
import numpy.linalg as LA
import random
from datetime import datetime

random.seed(datetime.now())
rows_count = 4
columns_count = 4

def isTerminal(r, c):  # helper function to check if terminal state or regular state
    global rows_count, columns_count
    if r == 0 and c == 0:  # im a bit too lazy to check otherwise the iteration boundaries
        return True  # so that this helper function is a quick way to exclude computations
    if r == rows_count - 1 and c == columns_count - 1:
        return True
    return False


maxiters = 100000
reward = -1
actions = [""U"", ""R"", ""D"", ""L""]
V = np.zeros((rows_count, columns_count))
returnsDict={}
QDict={}
actDict={0:""U"",1:""R"",2:""D"",3:""L""}
policies = np.array([ ['T','A','A','A'],
                     ['A','A','A','A'],
                     ['A','A','A','A'],
                     ['A','A','A','T'] ])





""""""returnsDict, for each state-action pair, maintain (mean,visitedCount)""""""
for r in range(rows_count):
    for c in range(columns_count):
        if not isTerminal(r, c):
            for act in actions:
                returnsDict[ ((r, c), act) ] = [0, 0] ## Maintain Mean, and VisitedCount for each state-action pair



"""""" Qfunc, we maintain the action-value for each state-action pair""""""
for r in range(rows_count):
    for c in range(columns_count):
        if not isTerminal(r, c):
            for act in actions:
                QDict[ ((r,c), act) ] = -9999  ## Maintain Q function value for each state-action pair






def getValue(row, col):  # helper func, get state value
    global V
    if row == -1:
        row = 0  # if you bump into wall, you bounce back
    elif row == 4:
        row = 3
    if col == -1:
        col = 0
    elif col == 4:
        col = 3
    return V[row, col]

def getRandomStartState():
    illegalState = True

    while illegalState:
        r = random.randint(0, 3)
        c = random.randint(0, 3)
        if (r == 0 and c == 0) or (r == 3 and c == 3):
            illegalState = True
        else:
            illegalState = False
    return r, c

def getState(row, col):
    if row == -1:
        row = 0  # helper func for the exercise:1
    elif row == 4:
        row = 3
    if col == -1:
        col = 0
    elif col == 4:
        col = 3
    return row, col



def getRandomAction():
    global actDict
    return actDict[random.randint(0, 3)]


def getMeanFromReturns(oldMean, n, curVal):
    newMean = 0
    if n == 0:
        raise Exception('Exception, incrementalMeanFunc, n should not be less than 1')
    elif n == 1:
        return curVal
    elif n >= 2:
        newMean = (float) ( oldMean + (1.0 / n) * (curVal - oldMean) )
        return newMean


""""""get the best action 
returns string action
parameter is state tuple (r,c)""""""
def getArgmaxActQ(S_t):
    global QDict
    qvalList = []
    saList = []

    """"""for example get together
    s1a1, s1a2, s1a3, s1a4
    find which is the maxValue, and get the action which caused it""""""
    sa1 = (S_t, ""U"")
    sa2 = (S_t, ""R"")
    sa3 = (S_t, ""D"")
    sa4 = (S_t, ""L"")
    saList.append(sa1)
    saList.append(sa2)
    saList.append(sa3)
    saList.append(sa4)

    q1 = QDict[sa1]
    q2 = QDict[sa2]
    q3 = QDict[sa3]
    q4 = QDict[sa4]
    qvalList.append(q1)
    qvalList.append(q2)
    qvalList.append(q3)
    qvalList.append(q4)

    maxQ = max(qvalList)
    ind_maxQ = qvalList.index(maxQ)  # gets the maxQ value and the index which caused it

    """"""when we have index of maxQval, then we know which sa-pair
    gave that maxQval => we can access that action from the correct sa-pair""""""
    argmaxAct = saList[ind_maxQ][1]
    return argmaxAct

""""""QEpisode generation func
returns episodeList
parameters are starting state, starting action""""""
def QEpisode(r, c, act):
    global reward
    global policies

    """"""NOTE! r,c will both be local variables inside this func
    they denote the nextState (s') in this func""""""
    stateWasTerm = False
    stepsTaken = 0
    curR = r
    curC = c
    episodeList = [ ((r, c), act, reward) ]  # add the starting (s,a) immediately

    if act == ""U"":  ##up
        r -= 1
    elif act == ""R"":  ##right
        c += 1
    elif act == ""D"":  ## down
        r += 1
    else:  ##left
        c -= 1
    stepsTaken += 1
    r, c = getState(r, c)  ## check status of the newState (s')
    stateWasTerm = isTerminal(r, c)  ## if status was terminal stop iteration, else keep going into loop

    if not stateWasTerm:
        curR = r
        curC = c

    while not stateWasTerm:
        if policies[curR, curC] == ""A"":
            act = getRandomAction()  ## """"""get the random action from policy""""""
        else:
            act = policies[curR, curC]  ## """"""get the deterministic action from policy""""""

        if act == ""U"":  ## up
            r -= 1
        elif act == ""R"":  ## right
            c += 1
        elif act == ""D"":  ## down
            r += 1
        else:  ## left
            c -= 1
        stepsTaken += 1

        r, c = getState(r, c)
        stateWasTerm = isTerminal(r, c)
        episodeList.append( ((curR, curC), act, reward) )
        if not stateWasTerm:
            curR = r
            curC = c

    return episodeList




print(""montecarlo program starting...\n"")
"""""" MOnte Carlo Q-function, exploring starts, every-visit, estimating Pi ~~ Pi* """"""
for iteration in range(1, maxiters+1): ## for all episodes

    print(""curIter == "", iteration)
    print(""\n"")
    if iteration % 20 == 0: ## get random seed periodically to improve randomness performance
        random.seed(datetime.now())



    for r in range(4):
        for c in range(4):
            if not isTerminal(r,c):
                startR = r
                startC = c
                startAct = getRandomAction()


   ## startR, startC = getRandomStartState() ## get random starting-state, and starting action equiprobably
  ##  startAct = getRandomAction()
                sequence = QEpisode(startR, startC, startAct)  ## generate Q-sequence following policy Pi, until terminal-state (excluding terminal)
                G = 0

                for t in reversed(range(len(sequence))): ## iterate through the timesteps in reversed order
                    S_t = sequence[t][0] ## use temp variables as helpers
                    A_t = sequence[t][1]
                    R_t = sequence[t][2]
                    G += R_t ## increment G with reward, NOTE! the gamma discount factor == 1.0
                    visitedCount = returnsDict[S_t, A_t][1] ## use temp visitedcount
                    visitedCount += 1

                    if visitedCount == 1: ## special case in iterative mean algorithm, the first visit to any state-action pair
                        curMean = 9999
                        curMean = getMeanFromReturns(curMean, visitedCount, G)
                        returnsDict[S_t, A_t][0] = curMean ## update mean
                        returnsDict[S_t, A_t][1] = visitedCount ## update visitedcount
                    else:
                        curMean = returnsDict[S_t, A_t][0] ## get temp mean from returnsDict
                        curMean = getMeanFromReturns(curMean, visitedCount, G) ## get the new temp mean iteratively
                        returnsDict[S_t, A_t][1] = visitedCount ## update visitedcount
                        returnsDict[S_t, A_t][0] = curMean ## update mean


                    QDict[S_t, A_t] = returnsDict[S_t, A_t][0] ## update the Qfunction with the new mean value
                    tempR = S_t[0] ## temp variables simply to disassemble the tuple into row,col
                    tempC = S_t[1]
                    policies[tempR, tempC] = getArgmaxActQ(S_t) ## update policy based on argmax_a[Q(S_t)]


print(""optimal policy with Monte-Carlo, every visit was \n"")
print(""\n"")
print(policies)

Here is the updated ""hacky fix"" code that seems to get the algorithm ""over the hump"" without getting stuck into foreverloop with deterministic policy. My teacher had recommended that you don't need to update policy at every step in this kind of Monte Carlo, so you could have made the policy updates at periodic intervals using python's modulo operator on the iterationsCount or something.
Also, I had the bright idea that the Sutton&Barto book described that all state-action pairs must be visited, very large amount of times, for the exploring starts pre-condition of the algorithm to be fulfilled.
So, I then decided to enforce the algorithm to have run at least once for all state-action pairs so you start from each state-action pair deterministically one-by-one (for each episode actually). This would still be run with the old randomwalk policy in this early exploration phase, where we are gathering data into the returnsDict, and Qdict, but not yet improving deterministic policy.
import numpy as np
import numpy.linalg as LA
import random
from datetime import datetime

random.seed(datetime.now())
rows_count = 4
columns_count = 4

def isTerminal(r, c):  # helper function to check if terminal state or regular state
    global rows_count, columns_count
    if r == 0 and c == 0:  # im a bit too lazy to check otherwise the iteration boundaries
        return True  # so that this helper function is a quick way to exclude computations
    if r == rows_count - 1 and c == columns_count - 1:
        return True
    return False



""""""NOTE about maxiters!!!
the Monte-Carlo every visit algorithm implements total amount of iterations with formula
totalIters = maxiters * nonTerminalStates * possibleActions
totalIters = 5000 * 14 * 4
totalIters = 280000

in other words, there will be 5k iterations per each state-action pair
in other words there will be an early exploration phase where policy willnot be updated,
but the gridworld will be explored with randomwalk policy, gathering Qfunc information, 
and returnDict information.

in early phase there will be about 27 iterations for each state-action pair during,
non-policy-updating exploration 
(maxiters * explorationFactor) / (stateACtionPairs) = 7500 *0.2 /56

after that early exploring with randomwalk,
then we act greedily w.r.t. the Q-function, 
for the rest of the iterations to get the optimal deterministic policy
""""""
maxiters = 7500
explorationFactor = 0.2 ## explore that percentage of the first maxiters rounds, try to increase it, if you get stuck in foreverloop, in QEpisode function
reward = -1
actions = [""U"", ""R"", ""D"", ""L""]
V = np.zeros((rows_count, columns_count))
returnsDict={}
QDict={}
actDict={0:""U"",1:""R"",2:""D"",3:""L""}
policies = np.array([ ['T','A','A','A'],
                     ['A','A','A','A'],
                     ['A','A','A','A'],
                     ['A','A','A','T'] ])





""""""returnsDict, for each state-action pair, maintain (mean,visitedCount)""""""
for r in range(rows_count):
    for c in range(columns_count):
        if not isTerminal(r, c):
            for act in actions:
                returnsDict[ ((r, c), act) ] = [0, 0] ## Maintain Mean, and VisitedCount for each state-action pair



"""""" Qfunc, we maintain the action-value for each state-action pair""""""
for r in range(rows_count):
    for c in range(columns_count):
        if not isTerminal(r, c):
            for act in actions:
                QDict[ ((r,c), act) ] = -9999  ## Maintain Q function value for each state-action pair






def getValue(row, col):  # helper func, get state value
    global V
    if row == -1:
        row = 0  # if you bump into wall, you bounce back
    elif row == 4:
        row = 3
    if col == -1:
        col = 0
    elif col == 4:
        col = 3
    return V[row, col]

def getRandomStartState():
    illegalState = True

    while illegalState:
        r = random.randint(0, 3)
        c = random.randint(0, 3)
        if (r == 0 and c == 0) or (r == 3 and c == 3):
            illegalState = True
        else:
            illegalState = False
    return r, c

def getState(row, col):
    if row == -1:
        row = 0  # helper func for the exercise:1
    elif row == 4:
        row = 3
    if col == -1:
        col = 0
    elif col == 4:
        col = 3
    return row, col



def getRandomAction():
    global actDict
    return actDict[random.randint(0, 3)]


def getMeanFromReturns(oldMean, n, curVal):
    newMean = 0
    if n == 0:
        raise Exception('Exception, incrementalMeanFunc, n should not be less than 1\n')
    elif n == 1:
        return curVal
    elif n >= 2:
        newMean = (float) ( oldMean + (1.0 / n) * (curVal - oldMean) )
        return newMean


""""""get the best action 
returns string action
parameter is state tuple (r,c)""""""
def getArgmaxActQ(S_t):
    global QDict
    qvalList = []
    saList = []

    """"""for example get together
    s1a1, s1a2, s1a3, s1a4
    find which is the maxValue, and get the action which caused it""""""
    sa1 = (S_t, ""U"")
    sa2 = (S_t, ""R"")
    sa3 = (S_t, ""D"")
    sa4 = (S_t, ""L"")
    saList.append(sa1)
    saList.append(sa2)
    saList.append(sa3)
    saList.append(sa4)

    q1 = QDict[sa1]
    q2 = QDict[sa2]
    q3 = QDict[sa3]
    q4 = QDict[sa4]
    qvalList.append(q1)
    qvalList.append(q2)
    qvalList.append(q3)
    qvalList.append(q4)

    maxQ = max(qvalList)
    ind_maxQ = qvalList.index(maxQ)  # gets the maxQ value and the index which caused it

    """"""when we have index of maxQval, then we know which sa-pair
    gave that maxQval => we can access that action from the correct sa-pair""""""
    argmaxAct = saList[ind_maxQ][1]
    return argmaxAct




""""""QEpisode generation func
returns episodeList
parameters are starting state, starting action""""""
def QEpisode(r, c, act):

    """"""ideally, we should not get stuck in the gridworld...but,
    but sometiems when policy transitions from the first episode's policy == randomwalk,
    then, on second episode sometimes we get stuck in foreverloop in episode generation
    usually the only choice then seems to restart the entire policy into randomwalk ??? """"""

    global reward
    global policies

    """"""NOTE! r,c will both be local variables inside this func
    they denote the nextState (s') in this func""""""
    stepsTaken = 0
    curR = r
    curC = c
    episodeList = [ ((r, c), act, reward) ]  # add the starting (s,a) immediately

    if act == ""U"":  ##up
        r -= 1
    elif act == ""R"":  ##right
        c += 1
    elif act == ""D"":  ## down
        r += 1
    elif act == ""L"":  ##left
        c -= 1
    stepsTaken += 1
    r, c = getState(r, c)  ## check status of the newState (s')
    stateWasTerm = isTerminal(r, c)  ## if status was terminal stop iteration, else keep going into loop

    if not stateWasTerm:
        curR = r
        curC = c

    while not stateWasTerm:
        if policies[curR, curC] == ""A"":
            act = getRandomAction()  ## """"""get the random action from policy""""""
        else:
            act = policies[curR, curC]  ## """"""get the deterministic action from policy""""""

        if act == ""U"":  ## up
            r -= 1
        elif act == ""R"":  ## right
            c += 1
        elif act == ""D"":  ## down
            r += 1
        else:  ## left
            c -= 1
        stepsTaken += 1

        r, c = getState(r, c)
        stateWasTerm = isTerminal(r, c)
        episodeList.append( ((curR, curC), act, reward) )
        if not stateWasTerm:
            curR = r
            curC = c
        if stepsTaken >= 100000:
            raise Exception(""Exception raised, because program got stuck in MC Qepisode generation...\n"")


    return episodeList




print(""montecarlo program starting...\n"")
"""""" MOnte Carlo Q-function, exploring starts, every-visit, estimating Pi ~~ Pi* """"""

""""""It appears that the Qfunction apparently can be unreliable in the early episodes rounds, so we can avoid getting 
stuck in foreverloop because of unreliable early episodes, BUT...

we gotta delay updating the policy, until we have explored enough for a little bit...
so our Qfunction has reliable info inside of it, to base the decision on, later...""""""
Q_function_is_reliable = False ## variable shows if we are currently updating the policy, or just improving Q-function and exploring


for iteration in range(1, maxiters+1): ## for all episodes

    print(""curIter == "", iteration, "", QfunctionIsReliable == "", Q_function_is_reliable )
    print(""\n"")
    if iteration % 20 == 0: ## get random seed periodically to improve randomness performance
        random.seed(datetime.now())

    for r in range(4):  ## for every non-terminal-state
        for c in range(4):
            if not isTerminal(r,c):
                startR = r
                startC = c
                for act in actions: ## for every action possible
                    startAct = act
                    sequence = QEpisode(startR, startC, startAct)  ## generate Q-sequence following policy Pi, until terminal-state (excluding terminal)
                    G = 0

                    for t in reversed(range(len(sequence))): ## iterate through the timesteps in reversed order
                        S_t = sequence[t][0] ## use temp variables as helpers
                        A_t = sequence[t][1]
                        R_t = sequence[t][2]
                        G += R_t ## increment G with reward, gamma discount factor is zero
                        visitedCount = returnsDict[S_t, A_t][1]
                        visitedCount += 1

                       ## if (S_t, A_t, -1) not in sequence[:t]: ## This is how you COULD have done the first-visit MC, but we do every-visit now...
                        if visitedCount == 1: ## special case in iterative mean algorithm, the first visit to any state-action pair
                            curMean = 9999
                            curMean = getMeanFromReturns(curMean, visitedCount, G)
                            returnsDict[S_t, A_t][0] = curMean ## update mean
                            returnsDict[S_t, A_t][1] = visitedCount ## update visitedcount
                        else:
                            curMean = returnsDict[S_t, A_t][0] ## get temp mean from returnsDict
                            curMean = getMeanFromReturns(curMean, visitedCount, G) ## get the new temp mean iteratively
                            returnsDict[S_t, A_t][1] = visitedCount ## update visitedcount
                            returnsDict[S_t, A_t][0] = curMean ## update mean


                        QDict[S_t, A_t] = returnsDict[S_t, A_t][0] ## update the Qfunction with the new mean value
                        tempR = S_t[0] ## temp variables simply to disassemble the tuple into row,col
                        tempC = S_t[1]

                        if iteration >= round(maxiters * explorationFactor): ## ONLY START UPDATING POLICY when we have reliable estimates for Qfunction, that is when iteration > maxiter/10
                            Q_function_is_reliable = True
                            policies[tempR, tempC] = getArgmaxActQ(S_t) ## update policy based on argmax_a[Q(S_t)]


print(""optimal policy with Monte-Carlo, every visit was \n"")
print(""\n"")
print(policies)

","['python', 'monte-carlo-methods']","
Your implementation of Monte Carlo Exploring Starts algorithm appears to be working as designed. This is a problem that can occur with some deterministic policies in the gridworld environment. 
It is possible for your policy improvement step to generate such a policy, and there is no recovery from this built into the algorithm. First visit and every visit variants will converge differently to the true action values, however neither offers an improvement here. It is most likely that your loops are occurring via state/action pairs that did not occur in the first episode, so the value estimates are default 0, which then looks like the best choice when creating the deterministic policy*.
In Sutton & Barto to demonstrate Monte Carlo ES, the authors choose an environment where such loops are impossible (a simplified Blackjack game). They then quickly move on to removing the need for exploring starts by using $\epsilon$-greedy policies. So this issue is not covered in detail, although there are assertions in a couple of places that it must be possible to complete episodes.
To resolve this whilst still using Monte Carlo ES, you will need to alter the environment so that such loops are not possible. The simplest change is to terminate the episode if it gets too long. This is hacky, because done simply on the gridworld it violates the Markov property (because now the time step should technically be part of the state if you want to predict value). However, it will get you out of the immediate problem, and provided you set the termination point high enough - e.g. 100 steps for your small gridworld - the agent should still discover the optimal policy and associated action values.
This ""timeout"" patch is also used by many OpenAI Gym environments, because although most other algorithms can find their way out of infinite loops like this, they can still suffer slow learning from over-long episodes as a result.

* This leads to a possible fix of initialising your Q values pessimistically - e.g. with -50 starting value. That should help the first deterministic policy join up to the terminal state - although if you make it strictly deterministic and resolve value ties deterministically too, then even this may not be enough. You might want to give that a try to verify what I am saying here, although I would recommend the timeout hack instead, as that is a more general solution. Pessimistic start values are bad for exploration when using other algorithms.
"
Is it mostly the case to train with available models,"
I quite often find projects using pre-trained model and using them as a starting point for their new model that learns something novel from thier dataset or on-live learning process - e.g. using a webcam or live audio.
Is this quite usual and recommended to speed up training a model? For example using a model trained on ImageNet as a first layer to your model that will categorise faces specifically.
","['training', 'models']",
How to detect multiple playing cards of the same class with a neural network?,"
I want to train an AI to detect the class (i.e. suit and rank) of playing cards. Playing cards from different decks may use slightly different shapes or colors to represent these attributes, and I want the system to work across many decks. I bought many different decks, scanned and labeled them. Next up would be to create training data with an augmentation library. I found two examples of how other people did that:

Image detection using YOLO algorithm and poker cards
Playing card detection with YOLO

The problem is that I want my AI to be able to detect multiple cards of the same class in one picture. In the examples above, they put a label on the top left corner of a card. This makes sense since it is a very good indicator of what class the card is in. Unfortunately, every card has two of these labels.
I think their solution returns ""detected"" if any instance of a given label is found. But many cards with the same label could be in the same picture, and I am not sure if I can detect the quantity of the class with this solution. For example, if there are 2 Ace of Clubs in a picture, I would like my system to output ""2"", rather than ""detected"".
Do you think it is feasible to mark the whole card, prepare the data accordingly, and train an AI that detects the count as well?
","['neural-networks', 'classification', 'image-recognition', 'yolo', 'data-labelling']","
Although it was not crystal clear, we'll assume that by, ""Multiple cards of the same class in one picture,"" is meant that multiple cards of identical suit and rank will be grouped together in the same example image but each card in the image will be selected from a unique deck.
That arrangement would only be fruitful if the objective of training was to classify, flag, or otherwise analyze the same kind of groupings after training. Otherwise, it would likely be most productive and efficient to first devise a way to divide up the images by deck so that the focus of learning is detection of the three dimensions or perhaps just the first two, depending on the intended use of the trained network.

Rank
Suit
Style

"
How do I choose an appropriate fitness function and hyper-parameters to train a 7-DOF arm?,"
I am trying to train an ANN to control a 7 Degrees-Of-Freedom arm. It should reach a target avoiding a single obstacle. Given my modeling of the situation, my input layer is composed of 12 nodes:

5 nodes for the 5 joint states 
3 nodes for the cartesian coordinates of the target
3 nodes for the cartesian coordinates of the obstacle
1 node for the radius of the obstacle (it's a spherical object).

I have already tried training the ANN with DQN. I want to try neuroevolution (NEAT, in particular) and see how the results compare. I am using NEAT-python. As seen in this paper, this should be feasible.
However, I am having trouble choosing the best fitness function and also some other hyperparameters, namely the population size. (I am also puzzled by the extremely long training time for a single generation, but that's another story.)
So, the fitness function. I have tried to replicate what I have done with DQN. So, basically, my function evaluates a genome (so, an ANN) as follows (pseudocode):
counter = 0
reapeat for NUM_OF_EPISODES times:
    generate a random target
    generate an obstacle which lies about halfway from the end-effector to the target
    repeat for TIMEOUT times:
        use the ANN to decide the next_action and execute it
        if OBSTACLE_REACHED or TARGET_REACHED, stop 
    counter += 0.3 * relative_distance + 0.2 * relative_path + 0.5 * didntHitObstacle()
fitness = counter / NUM_OF_EPISODES

So, humanly speaking, for each ANN we try to execute NUM_OF_EPISODES times (how many times should be enough? 100 times seems ok but it get's really slow) a scenario. In this scenario, we use the ANN to search for the target, and if we reach it or the obstacle, we stop. Now for each one of these scenarios, we should ""rank"" how well the ANN performed (see the counter += ... part). But how should I do this? My idea (stolen from the paper above) was to compute something like this:
0.3 * relative_distance + 0.2 * relative_path + 0.5 * didntHitObstacle()

So, basically, we see how much we are closer to the target compared to when we started, how ""short"" the path was (compared to the ideal straight line start point-to-target), and whether we did or did not hit the target.
Does this function make sense? My concern is mainly about how we deal with the obstacle: 50% of the fitness. Is it correct? I am asking this because I am receiving poor results.
Another problem that I have is population size. Of course, the bigger the better, but this thing takes a lot to train. How big is ok, in your experience?
","['evolutionary-algorithms', 'neat', 'robotics', 'neuroevolution']",
Maximum Q value for new state in Q-Learning never exists,"
I'm working on implementing a Q-Learning algorithm for a 2 player board game.

I encountered what I think may be a problem. When it comes time to update the Q value with the Bellman equation (above), the last part states that for the maximum expected reward, one must find the highest q value in the new state reached, s', after making action a.
However, it seems like the I never have q values for state s'. I suspect s' can only be reached from P2 making a move. It may be impossible for this state to be reached as a result of an action from P1. Therefore, the board state s' is never evaluated by P2, thus its Q values are never being computed.
I will try to paint a picture of what I mean. Assume P1 is a random player, and P2 is the learning agent.

P1 makes a random move, resulting in state s.
P2 evaluates board s, finds the best action and takes it, resulting in state s'. In the process of updating the Q value for the pair (s,a), it finds maxQ'(s', a) = 0, since the state hasn't been encountered yet.
From s', P1 again makes a random move.

As you can see, state s' is never encountered by P2, since it is a board state that appears only as a result of P2 making a move. Thus the last part of the equation will always result in 0 - current Q value.
Am I seeing this correctly? Does this affect the learning process? Any input would be appreciated.
Thanks.
",['q-learning'],
how to benefit from previous training weights in training again to increase accuracy?,"
I have trained  a modified VGG classification CNN, with random initialized weights; therefor the validation accuracy was not high enough for me to accept (around 66%). 
now using the weights resulted from training the network, how can i use those weights in training the network again to improve accuracy? (e.g. using previous training weights with different learning rate, or increase epochs, ..)
","['convolutional-neural-networks', 'training', 'keras']","
First, I assume you've tuned your hyperparameters. Because, instead of re-train the network (use the weights that resulted from the previously training process) that needs more times, I'll invest more on hyperparameters tuning of the available network.
Then, there are several methods and considerations:

You can use the weight resulted from your first network as the initial of your next training process. But as the network will face the same data/problem if you use ""initial"" value for your hyperparameters (e.g. high learning rate) then I'm afraid it will lead to overfitting. Because it's simply ""nothing change"" in your architecture.
If you previously didn't use adaptive learning for your training process (e.g. Adadelta, Adam), you can re-train your network with a smaller learning rate. So, your model can find a better result.
Or, you can use the concept of the selffer network, you use the weight for some layers from previous training process (you can freeze it or not) and randomly initialize other layers. And then, train the network using ""initial"" value of hyperparameters

You can read more about the transfer learning concept (or selffer network) to get the most appropriate method for your case. There is also a paper about ""incremental training on CNN"" that I think is similar with the selffer network but with some modifications.
Hope it helps
"
Variational Autoencoder task for better feature extraction,"
I have a CNN with the regression task of a single scalar.
I was wondering if an additional task of reconstructing the image (used for learning visual concepts), seen in a DeepMind presentation with the loss and re-parametrization trick of Variational Autoencoder, might help the principal task of regression.
So you can imagine some convolutions with the role of feature extraction with some output X (let's say a vector of 256 values), that X goes into the VAE which computes Z and then the reconstructed image. And then the original regression task would take either X or Z in order to compute that scalar value.
Has anyone tried such an approach, is it worth the work?
Thank you
","['neural-networks', 'convolutional-neural-networks', 'unsupervised-learning', 'autoencoders', 'supervised-learning']",
Why is dot product attention faster than additive attention?,"
In section 3.2.1 of Attention Is All You Need the claim is made that:

Dot-product attention is identical to our algorithm, except for the scaling factor of $\frac{1}{\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.

It does not make sense why dot product attention would be faster. Additive attention is nearly identical computation wise; the main difference is $Q + K$ instead of $Q K^T$ in dot product attention. $Q K^T$ requires at least as many addition operations as $Q + K$, so how can it possibly be faster?
","['neural-networks', 'machine-learning', 'deep-learning', 'attention']","
The additive attention method that the researchers are comparing to corresponds to a neural network with 3 layers (it is not actually straight addition). Computing this will involve one multiplication of the input vector by a matrix, then by another matrix, and then the computation of something like a softmax. Smart implementation of a dot-product will not break out the whole matrix multiplication algorithm for it, and it will basically be a tight, easily parallelized loop.
"
How can the A* algorithm be optimized?,"
How can the A* algorithm be optimized?
Any references that shows the optimization of A* algorithm are also appreciated.
","['algorithm', 'optimization', 'search', 'a-star']","
The first step of optimisation is to measure where inside the implementation most time is spent -- you don't actually optimise the algorithm itself, but a specific implementation of it. This step should give you an overview of where you can make improvements. Speculative changes usually don't do much.
There are several aspects of A* which would be candidates. You are keeping a list of paths currently under investigation. If it is very long, processing might take more time. You could try and prune this list (though it might mean that you won't actually find the best path anymore). Or you could investigate efficient ways of storing it.
Each step you need to compute the 'cost' of each path. This could be cached to stop you from repeating calculations. The heuristic function could be slow -- again something to investigate.
Let me stress again that you need to measure the performance first. There is no point in randomly trying to make things faster; you could waste a lot of time for very little gain. And typically with optimisation, readability of the code suffers, and it becomes harder to maintain. You might find that other parts of your system take a lot longer than the actual path finding, so your time might better be spent on those.
"
Why is informed search more efficient than uninformed search?,"
Why does informed search more efficiently finds a solution than an uninformed search? 
","['search', 'comparison', 'efficiency']",
What are stable ways of doing online machine learning?,"
I am trying to deploy a machine learning solution online into an application for a client.  One thing they requested is that the solution must be able to learn online because the problem may be non-stationary and they want the solution to track the non-stationarity of the problem.  I thought about this problem a lot, would the following work?

Set the learning rate (step-size parameter) for the neural network at a low fixed value so that the most recent training step is weighted more.
Update the model only once per day, in a mini-batch fashion.  The mini-batch will contain data from the day, mixed with data from the original data set to prevent catastrophic interference.  By using a mini batch update, I am not prone to biasing my model to the latest examples, and completely forgetting the training examples from months ago.

Would this set-up be ""stable"" for online/incremental machine learning?  Also, should I set up the update step so it samples data from all distributions of my predicted variable uniformly so it gets an ""even"" update (i.e., does not overfit to the most probabilistic predicted value)?
","['neural-networks', 'machine-learning', 'incremental-learning', 'online-learning']",
How do I choose the number of neurons in the fully-connected layer before the softmax layer?,"
I am solving a classification problem with CNN. The number of classes is 5. 

How can I decide the number of neurons in the FC layer before the softmax layer?
Is it $N * 5$, where $N$ is the number of classes?
Is there any documentation for deciding the number of neurons in the FC layer (before SoftMax layer)

","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'ai-design']","

How can I decide the number of neurons in the FC layer before the softmax layer?

Train different network architectures (with different numbers of neurons in the last FC layer). Use cross-validation - a set of data you have not trained on - to measure the performance of the network, using a metric that you have decided beforehand is a good proxy for your experiment goals. For instance, you might choose getting the highest classification accuracy as your goal - but might choose something different for an unbalanced dataset, because 90% accuracy is not meaningful.
There is not usually a good reason to over-tune your network. Trying some variations with 1.5 x or 2 x geometric series (e.g. 5, 10, 20, 40 neurons in layer) is probably enough to find a good hidden layer size.

Is it $N * 5$, where $N$ is the number of classes?

No, in general. That may work just fine for your problem though.

Is there any documentation for deciding the number of neurons in the FC layer (before the softmax layer)

Not really. The most direct thing to do is find solutions that work well for similar problems to yours, and adapt as necessary. So you can search for any similar problem domain and see what the researchers used there.
There is not much theory to guide you here. However, if it helps with your intuition, more neurons can make a better fit to higher frequency variations in the target function you are learning, whilst more layers will mean better handling of complex function spaces (e.g. where rules about the mapping between input and output can be made according to combining simpler rules) - provided in both cases that you have enough training data that it is possible for the NN to learn the function approximately. These are not strictly defined traits of functions for machine learning, so many NN users will work with this intuitive view.
If in doubt, assuming this is not an image, NLP or other well-studied problem, then I might just guess at e.g. 64 neurons per hidden layer, and try 1, 2, and 3 hidden layers as a starting point (all the same size). I cannot say if that will work for you and your problem, but it might help get past the ""blank page effect"" and start you training and testing some variations.
"
Do we have cross-language vector space for word embedding?,"
Do we have cross-language vector space for word embedding?
When measure similarity for apple/Pomme/mela/Lacus//, they should be the same
If would be great if there's available internet service of neuron network which already be trained by multiple language
","['natural-language-processing', 'word-embedding']","
You can try to read about MUSE (Multilingual Unsupervised and Supervised Embeddings) by Facebook. You can read it from its Github or this article. They also provide the FastText dictionary format (.vec file) for some languages. 
Their original paper shows how it aligns the vector of words from two different languages:

"
How can we create a vector space where word spelling and pronunciation can be easily compared?,"
In natural language processing, we can convert words to vectors (or word embeddings). In this vector space, we can measure the similarity between these word embeddings.
How can we create a vector space where word spelling and pronunciation can be easily compared? For example, ""apple"" and ""ape"", ""start"" and ""startle"" are very similar, so they should also be similar in this new vector space.
I am eventually looking for a library that can do this out of the box. I would like to avoid implementing this myself.
","['natural-language-processing', 'word-embedding']","
If you only need the vector space as a way to obtain a similarity measure, you may want to consider a distance measure instead. Similarity and distance are inversely related: identical words have maximum similarity or zero distance, and as the similarity decreases, the distance increases. 
For instance, the Wagner-Fischer algorithm computes the edit distance between two strings of characters. This edit distance takes into acccount insertions and deletions, as in your examples, but also substitutions (for example ""gray"" vs. ""grey"").
The article linked above includes pseudocode that should translate easily to actual code.
"
How do I create syntactically correct sentences given several words?,"
Is there an AI application that can produce syntactically (and semantically) correct sentences given a bag of words? For example, suppose I am given the words ""cat"", ""fish"", and ""lake"", then one possible sentence could be ""cat eats fish by the lake"".
","['machine-learning', 'natural-language-processing']","
One way is by defining a grammar for your sentences manually. But that's going to be a trivial task because after creating a grammar you have to create a parsing tree based on your defined grammar to generate a sentence.
Other way is to use LSTM(Long Short term memory) architecture for this purpose but you have to fine tune your model perfectly (which will not give a very good result but it can give you a above average result) or you will get a gibberish.
In this case you can use BLEU (Bil-lingual evaluation under study) instead by evaluating your results manually for selecting the statements with above 0.6 score to get a above average result
You can also check this research paper : https://arxiv.org/abs/1702.04066
Repo : https://github.com/keisks/jfleg
JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction
"
Can AI help summarize article or abstract sentence keyword?,"
I'm wondering if AI now can help us abstract summary or general idea of long article, for example novel or historical stories, or abstract most important keyword from sentence;
Would you please tell me if any of this kind of project is done?
I wish I can improve my reading speed and effectiveness with AI help.
","['natural-language-processing', 'artificial-neuron']","
Yes. Text summarisation has been a research topic in (computational) linguistics for literally decades. Have a look at the Wikipedia page on Automatic Summarisation for an overview.
There are basically various different approaches: either, selecting salient sentences (or parts of sentences) which represent the gist of the text, or, trying to 'understand' the text and generating new sentences. The former is generally easier, and works on any text, while the latter would probably be able to produce better results, but is more complicated and would not work on any text, as it would be specific to a particular topic.
"
Are successive actions independent?,"
The proof of the consistency of the per-decision importance sampling estimator assumes the independence of 
$$\frac{\pi(A_t|S_t)}{b(A_t|S_t)}R_{t+1}\quad\text{ and }\quad \prod_{k=t+1}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}.$$ 
See the proof of Theorem 1 in ""Eligibility Traces for Off-Policy Policy Evaluation"".
The result is also stated in Equation (5.14) of Sutton and Barto's RL book.
I'm guessing that this is itself a consequence of an assumption of independence between 
$$\frac{\pi(A_t|S_t)}{b(A_t|S_t)}\quad\text{ and }\quad \frac{\pi(A_{t+1}|S_{t+1})}{b(A_{t+1}|S_{t+1})}.$$ 
I don't understand how this assumption can be justified. Consider the extreme case of a nearly deterministic policy $\pi$ and deterministic MDP dynamics. It would seem to me that the two values above are then surely not independent.
Am I missing something?
","['reinforcement-learning', 'markov-decision-process', 'importance-sampling']",
What loss function to use when labels are probabilities?,"
What loss function is most appropriate when training a model with target values that are probabilities? For example, I have a 3-output model. I want to train it with a feature vector $x=[x_1, x_2, \dots, x_N]$ and a target $y=[0.2, 0.3, 0.5]$. 
It seems like something like cross-entropy doesn't make sense here since it assumes that a single target is the correct label.
Would something like MSE (after applying softmax) make sense, or is there a better loss function?
","['neural-networks', 'machine-learning', 'objective-functions', 'probability-distribution']",
What is the meaning of the statement $\forall x \exists y \forall z (z \neq y \iff f(x) \neq z)$?,"
I need to understand the meaning of the FOL statement below.
$$
\forall x \exists y \forall z (z \neq y \iff f(x) \neq z)
$$
Does this imply that $x$, $y$, and $z$ cannot be the same or $f(x)$ has no value?
",['logic'],
How important is it that the generator of a generative adversarial network doesn't take in information about input classes?,"
I'm building a generative adversarial network that generates images based on an input image. From the literature I've read on GANs, it seems that the generator takes in a random variable and uses it to generate an image.
If I were to have the generator receive an input image, would it no longer be a GAN? Would the discriminator be extraneous?
","['keras', 'generative-model', 'generative-adversarial-networks']","
If you're building a straight ""vanilla"" generative adversarial network, it's best to understand the network as a statistical engine:  You are training the generator on samples of a statistical distribution.  (And you're training the discriminator to distinguish between ""ground truth"" images, and images from that generator.)
Once you replace the input noise with another image... well.  Strictly speaking, it is probably still a generative adversarial network, if you're still doing everything else the same.  It is still a generator and a discriminator, acting in an adversarial fashion. 
But you've radically altered the input distribution, so there is a good chance that you're no longer accomplishing what you want to accomplish unless you're being very careful and clever. 
That said, there are GAN variants which do take images rather than noise as inputs.  See the wonderful paper on CycleGANs by Zhu, et al, along with a substantial body of followup literature. And note that CycleGANs use not one, but two discriminators, so even here the discriminator is necessary.
"
Are there reinforcement learning algorithms that scale to large problems?,"
Given a large problem, value iteration and other table based approaches seem to require too many iterations before they start to converge. Are there other reinforcement learning approaches that better scale to large problems and minimize the amount of iterations in general?
",['reinforcement-learning'],
What is the difference between search and learning?,"
I came across an article, The Bitter Truth, via the Two Minute Papers YouTube Channel. Rich Sutton says...

One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.

What is the difference between search and learning here? My understanding is that learning is a form of search -- where we iteratively search for some representation of data that minimizes a loss function in the context of deep learning.
","['machine-learning', 'deep-learning', 'comparison', 'philosophy', 'search']","
One way to think of the difference between search and learning is that search usually entails a search key, and an algorithm hunts through the structure looking for a match between the key and an already-existing item. Whereas learning is the creation of the structure in the first place. But search and learning are related in that on receipt of an input (say from one or more sensors) the structure is initially searched to see if the input already exists, but if it doesn't then current input (when certain conditions are met) is added to the structure, and learning follows a failure of search. 
"
Evaluation metrics multi-class classification (ROC- PR curves),"
Facing with a multi-class classification task, 
my question is:
are ROC and Precision-Recall (One-vs-All-Rest) curves useful to evaluate and visualize the performance of a model? 
or Confusion matrix, Precision, Recall, F-score (micro and macro) are enough?
What do you think about?
","['machine-learning', 'classification']",
Deciding the rewards for different actions in Pong for a DQN agent,"
I am attempting to implement an agent that learns to play in the Pong environment, the environment was created in PyGame and I return the pixel data and score at each frame. I use a CNN to take a stack of the last 4 frames as input and predicts the best action to take, I also make use of training on a minibatch of experiences from an experience replay at each timestep.  
I have seen an implementation where the game returned a reward of 10 for each time the bot returns the ball and -10 for each time the bot misses the ball. 
My question is whether it would be better to reward the bot significantly for managing to get the ball passed the opponent, ending the episode. I was thinking of rewarding 10 for winning the episode, -10 for missing the ball and 5 for returning the ball. 
Please let me know if my approach is sensible, has any glaring problems or if I need to provide more information. 
Thank you!
","['reinforcement-learning', 'dqn', 'rewards', 'reward-clipping']",
What are possible functions assigned on decision nodes for decision tree prediction?,"
In Decision Tree or Random Forest, each tree has a collection of decision nodes (in which each node has a threshold value) and a class labels (or regression values). 
I know that threshold values are used for comparison with a corresponding feature value. As far as I know, the comparison is performed either ""<"", "">""  or ""=="" predicate.
Anything else for the functions taking threshold value and a feature value as inputs??
",['decision-trees'],
"What is the meaning of $V(D,G)$ in the GAN objective function?","
Here is the GAN objective function.
$$\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]$$
What is the meaning of $V(D, G)$?
How do we get these expectation parts?
I was trying to understand it following this article: Understanding Generative Adversarial Networks (D.Seita), but, after many tries, I still can't understand how he got from $\sum_{n=1}^{N} \log D(x)$ to $\mathbb{E}(\log(D(x))$.
","['machine-learning', 'generative-model', 'generative-adversarial-networks', 'notation']",
Difficulty understanding Monte Carlo policy evaluation (state-value) for gridworld,"
I've been trying to read Sutton & Barto book chapter 5.1, but I'm still a bit confused about the procedure of using Monte Carlo policy evaluation (p.92), and now I just cant proceed anymore coding a python solution, because I feel like I don't fully understand how the algorithm works, so that the pseudocode example in the book doesn't seem to make much sense to me anymore. (the orange part)
I've done the chapter 4 examples with the algorithms coded already, so I'm not totally unfamiliar with these, but somehow I must have misunderstood the Monte Carlo prediction algorithm from chapter 5.


My setting is a 4x4 gridworld where reward is always -1. 
Policy is currently equiprobable randomwalk. If an action would take the newState (s') into outside the grid, then you simply stay in place, but action will have been taken, and reward will have been rewarded. 
Discount rate will be 1.0 (no discounting). 
Terminal states should be two of them, (0,0) and (3,3) at the corners.

On page 92 it shows the algorithm pseudocode and I feel as though I coded my episode generating function correctly thusfar. I have it such that, the results are that I always start in the same starting state (1,1) coords in the gridworld.
Currently, I have it so that if you started always in state (1,1), then a possible randomly generate episode could be as follows (in this case also optimal walk). Note that I currently have the episodes in form of list of tuple (s, a, r). Where s will also be a tuple (row,column), but a = string such as ""U"" for up, and r is reward always -1. 
so that a possible episode could be like: [( (1,1), ""U"", -1 ), ( (0,1), ""L"", -1 )] So that the terminal state is always excluded, so that the last state in episode will be the state immediately close to terminal state. Just like the pseudocode describes that you should exclude the terminal state S_T. 
But, the random episode could have been one where there are repeating states such as [( (1,1), ""U"", -1), ( (0,1), ""U"", -1 ), ( (0,1), ""U"", -1 ), ( (0,1), ""L"", -1 )]
I made the loop for each step of episode, such as follows: once you have the episodeList of tuples, iterate for each tuple, in reversed order. I think this should give the correct amount of iterations there...
G can be updated as described in pseudocode.
currently the Returns(S_t) datastructure that I have, will be a dictionary where the keys are state tuples (row,col), and the values are empty lists in the beginning.
I have a feeling that I'm calculating the average into V(S_t) incorrectly because I origianlly thought that you could even omit the V(S_t) step totally from the algorithm, and only afterwards compute for a separate 2D array V[r,c] for each state get the sum of the appropriate list elements (accessed from the dict), and divide that sum by the amount of episodes that you ran???


But I don't suddently know how to implement the first visit check in the algorithm. Like, I literally don't understand what it is actually checking for.
And furthermore I don't understand how the empirical mean is now supposed to be calculates in the monte carlo algorithm where there is the V(s_t) = average( Returns(S_t) )
I will also post my python code thusfar.
import numpy as np
import numpy.linalg as LA
import random

# YOUR CODE



rows_count = 4
columns_count = 4
V = np.zeros((rows_count, columns_count))
reward = -1 #probably not needed
directions = ['up', 'right', 'down', 'left'] #probably not needed
maxiters = 10000
eps = 0.0000001
k = 0 # ""memory counter"" of iterations inside the for loop, note that for loop i-variable is regular loop variable

rows = 4
cols = 4

#stepsMatrix = np.zeros((rows_count, columns_count)) 





def isTerminal(r,c):      #helper function to check if terminal state or regular state
    global rows_count, columns_count
    if r == 0 and c == 0: #im a bit too lazy to check otherwise the iteration boundaries        
        return True       #so that this helper function is a quick way to exclude computations
    if r == rows_count-1 and c == columns_count-1:
        return True
    return False

def getValue(row, col):    #helper func, get state value
    global V
    if row == -1: row =0   #if you bump into wall, you bounce back
    elif row == 4: row = 3
    if col == -1: col = 0
    elif col == 4: col =3

    return V[row,col]

def getState(row,col):
    if row == -1: row =0   #helper func for the exercise:1
    elif row == 4: row = 3
    if col == -1: col = 0
    elif col == 4: col =3
    return row, col


def makeEpisode(r,c):  #helper func for the exercise:1
## return the count of steps ??
#by definition, you should always start from non-terminal state, so
#by minimum, you need at least one action to get to terminal state
    stateWasTerm = False
    stepsTaken = 0
    curR = r
    curC = c
    while not stateWasTerm:

        act = random.randint(0,3)
        if act == 0: ##up
            curR-=1
        elif act == 1: ##right
            curC+=1
        elif act == 2: ## down
            curR+=1
        else:##left
            curC-=1
        stepsTaken +=1
        curR,curC = getState(curR,curC)
        stateWasTerm = isTerminal(curR,curC)
    return stepsTaken


V = np.zeros((rows_count, columns_count))
episodeCount = 100
reward = -1
y = 1.0 #the gamma discount rate


#use dictionary where key is stateTuple, 
#and value is stateReturnsList
#after algorithm for monte carlo policy eval is done, 
#we can update the dict into good format for printing
#and use numpy matrix
returnsDict={} 
for r in range(4):
    for c in range(4):
        returnsDict[(r,c)]=[]





#""""""first-visit montecarlo episode generation returns the episodelist""""""
def firstMCEpisode(r,c):
    global reward
    stateWasTerm = False
    stepsTaken = 0
    curR = r
    curC = c
    episodeList=[  ]

    while not stateWasTerm:

        act = random.randint(0,3)
        if act == 0: ##up
            r-=1
            act=""U""
        elif act == 1: ##right
            c+=1
            act=""R""
        elif act == 2: ## down
            r+=1
            act=""D""
        else:##left
            c-=1
            act=""L""
        stepsTaken +=1

        r,c = getState(r,c)
        stateWasTerm = isTerminal(r,c)
        episodeList.append( ((curR,curC), act, reward) )
        if not stateWasTerm:

            curR = r
            curC = c


    return episodeList


kakka=0 #for debug breakpoints only!
#first-visit Monte Carlo with fixed starting state in the s(1,1) state
for n in range(1, episodeCount+1):

    epList = firstMCEpisode(1,1)
    G = 0
    for t in reversed( range( len(epList) )):
        G = y*G + reward #NOTE! reward is always same -1
        S_t = epList[t][0] #get the state only, from tuple

        willAppend = True
        for j in range(t-1):
            tmp = epList[j][0]
            if( tmp == S_t ):
                willAppend =False
                break
        if(willAppend):
            returnsDict[S_t].append(G)
            t_r = S_t[0] #tempRow from S_t
            t_c =S_t[1] #tempCol from S_t
            V[t_r, t_c] = sum( returnsDict[S_t] ) / n


kakka = 3 #for debug breakpoints only!
print(V)

","['reinforcement-learning', 'monte-carlo-methods', 'policy-evaluation']",
What is the best method to deal with heterogeneous multi agent system MAS?,"
Heterogeneity: Based on the heterogeneity of agents MAS can be divided into two categories namely: homogeneous and heterogeneous. Homogeneous MAS include agents that all have the same characteristics and functionalities, while heterogeneous MAS include agents with diverse features.
As I read in this paper that these methods can deal with the heterogeneity of agents MAS :
The dueling double deep Q-network (DDDQN) and Independent Deep Q-Network (IDQN): first approach to address heterogeneous multi-agent learning in urban traffic control.
deep Q-network (DQN): To handle heterogeneity,  each agent has different experience replay memory and different network policy. 
The asynchronous advantage actor-critic (A3C) algorithm is used to learn optimal policy for each agent, which can be extended to multiple heterogeneous agents.  So, Can someone tell me What is the best method to deal with heterogeneous multi-agent system MAS?
","['deep-learning', 'multi-agent-systems']",
How can alpha zero learn if the tree search stops and restarts before finishing a game?,"
I am trying to understand how alpha zero works, but there is one point that I have problems understanding, even after reading several different explanations. As I understand it (see for example https://applied-data.science/static/main/res/alpha_go_zero_cheat_sheet.png), alpha zero does not perform rollouts. So instead of finishing a game, it stops when it hits an unknown state, uses the neural network to compute probabilities for different actions as well as the value of this state (""probability of winning""), and then propagates the new value up the tree.
The reasoning is that this is much cheaper, since actually completing the game would take more time then just letting the neural network guess the value of a state.
However, this requires that the neural network is decent at predicting the value of a state. But in the beginning of training, obviously it will be bad at this. Moreover, since the monte carlo tree search stops as soon as it hits a new state, and the number of different game states is very large, it seems to me that the simulation will rarely manage to complete a game. And for sure, the neural network can not improve unless it actually completes a significant number of games, because that is only real feedback that tells the agent if it is doing good or bad moves.
What am I missing here?
The only plausible explanation I can come up with is:  If the neural network would be essentially random in the beginning, well then for sure the large number of game states would prevent the tree search from ever finishing if it restarts as soon as it hits a previously unknown game state, so this can not be the case. So perhaps, maybe even if the neural network is bad in the beginning, it will not be very ""random"", but still be quite biased towards some paths. This would mean that the search would be biased to some smaller set of states among the vast number of different game states, and thus it would tend to take the same path more than once and be able to complete some games and get feedback. Is this ""resolution"" correct?
One problem I have though with the above ""resolution"", is that according to the algorithm, it should favor exploration in the beginning, so it seems that in the beginning it will be biased towards choosing previously not taken actions. This makes it even more seem like the tree search will never be able to complete a game and thus the neural net would not learn.
","['deep-learning', 'reinforcement-learning', 'monte-carlo-tree-search', 'alphazero']","
Sorry this is more of a comment than an answer. I'm wondering if you have found a definitive answer to your question, because I've a very related question.
I'm also confused by the AlphaZero algorithm - my explanation for my confusion is specified here: How does AlphaZero use its value and policy heads in conjunction?.
The thing is that I think the AlphaZero algorithm is also different from the Alphago Zero algorithm. A lot of sources that I've tried referred to really mix the two together.
In particular, there's this function in the official pseudocode, which really confused me:
def run_mcts(config: AlphaZeroConfig, game: Game, network: Network):
  root = Node(0)
  evaluate(root, game, network)
  add_exploration_noise(config, root)

  for _ in range(config.num_simulations):
    node = root
    scratch_game = game.clone()
    search_path = [node]

    while node.expanded():
      action, node = select_child(config, node)
      scratch_game.apply(action)
      search_path.append(node)

    value = evaluate(node, scratch_game, network)
    backpropagate(search_path, value, scratch_game.to_play())
  return select_action(config, game, root), root


def select_action(config: AlphaZeroConfig, game: Game, root: Node):
  visit_counts = [(child.visit_count, action)
                  for action, child in root.children.iteritems()]
  if len(game.history) < config.num_sampling_moves:
    _, action = softmax_sample(visit_counts)
  else:
    _, action = max(visit_counts)
  return action

Mainly because, if you look at the definition of expanded...
  def expanded(self):
    return len(self.children) > 0


I.e. when there are no more moves, and we are at the end of the game. I wonder what I'm missing here.
"
How do map providers like Google calculate the distance between two coordinates and find turn by turn directions?,"
I have searched on how Google or any map provider calculates distance between two coordinates. The closest I could find is Haversine formula.
If I draw a straight line between two points, then Haversine formula can be helpful. But since no one will travel straight and typically move through the streets, I want to know if there are any methods to calculate turn by turn points and see how to find multiple ways to travel to the destination from the source.
Right now my idea is

Have the two coordinates within a map window.
Make an algorithm detect the white lines (path) in the window.
Make it understand how they are connected.
Feed it to an algorithm to solve the Travelling Salesman Problem to find the best path between them.

But these things see very memory and process intensive. Even with the knowledge that Google has the powerhouse to process, to serve so many directions and distance matrix in fractions of seconds in amazing. I want to know if there are different approaches to this?
","['reinforcement-learning', 'object-recognition']","
Roads maps are well defined and you can access them online. For example you can visit OpenStreetMap to download road network of a given region. 
Such a road network definition contains nodes (junctions) with lat/lon coordinates and edges between nodes (roads). An edge is defined by a connection of two nodes. Edges can represent one-way roads: you need to define double ways as two edges going in both ways. Haversine formula gives you the straight distance between two nodes: the length of the edge. 
This definition of a road network gives you a graph (in the mathematical point of view) and is largely covered by the literature. You can search the graph and compute many of its parameters.
From a road network definition you can run shortest path algorithm to find the closest way between two nodes. From a given coordinate you can either pick the closest node or get the closest edge (and compute distance to the edge and the coordinates of the perpendicular projection point). 
Algorithms like A* and Dijkstra are not so time consuming and you can find shortest path efficiently.
"
How can the $\lambda$-return be defined recursively?,"
The $\lambda$-return is defined as
$$G_t^\lambda = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_{t:t+n}$$
where
$$G_{t:t+n} = R_{t+1}+\gamma R_{t+2}+\dots +\gamma^{n-1}R_{t+n} + \gamma^n\hat{v}(S_{t+n})$$
is the $n$-step return from time $t$.
How can we use this definition to rewrite $G_t^\lambda$ recursively?
","['reinforcement-learning', 'sutton-barto', 'return', 'eligibility-traces']",
Why don't people use projected Bellman error with deep neural networks?,"
Projected Bellman error has shown to be stable with linear function approximation. The technique is not at all new. I can only wonder why this technique is not adopted to use with non-linear function approximation (e.g. DQN)? Instead, a less theoretical justified target network is used. 
I could come up with two possible explanations:

It doesn't readily apply to non-linear function approximation case (some work needed)
It doesn't yield a good solution. This is the case for true Bellman error but I'm not sure about the projected one.

","['reinforcement-learning', 'dqn', 'deep-rl', 'function-approximation']","
I have found some clues in Maei's thesis (2011): Gradient Temporal-Difference Learning Algorithms.
According to the thesis: 

GTD2 is a method that minimizes the projected Bellman error (MSPBE).
GTD2 is convergent in non-linear function approximation case (and off-policy).
GTD2 converges to a TD-fixed point (same point as semi-gradient TD).
GTD2 is slower to converge than usual semi-gradient TD. 


It doesn't readily apply to non-linear function approximation.

No, it does.

It doesn't yield a good solution. 

No, it does. TD-fixed point is the same point for the solution of semi-gradient TD (which is generally used). There is no edge on that.
The only explanation seems to be practical convergence rate.
To quote his words: 

Some of our empirical results suggest that gradient-TD method maybe slower than conventional TD methods on problems on which conventional TD methods are sound (that is, on-policy learning problems).

"
Machine learning to find drivers of an event with presence-only data (no absence),"
I have some ecological data on the confirmed presence of a certain animal. I have data on the:
Date
Relevant metadata about the site 
Simple metrics on the animal
A complete weather record for the site. 

I'm assuming the presence of this animal is driven by weather events, and that the metadata about the site may be an important factor for patterns in the data. What my data ends up looking like is this.
#Date of observation
Date<-as.POSIXct(c(""2015-01-01"",""2015-01-11"",""2015-01-19"",""2015-02-04"",""2015-02-12"",""2015-02-23"",""2015-04-01"",""2015-04-10"",""2015-04-16"",""2015-04-20""))

#Data about animal 
Size<-c(1,1,1,2,2,3,4,1,2,5)
Color<-c(""B"",""B"",""R"",""R"",""R"",""R"",""B"",""B"",""B"",""Y"" )
Length<-c(1,10,12,4,5,2,1,2,7,12)

#Weather Data
AirTempDayOf<-c(20,40,20,23,24,25,24,25,25,22)
WindSpeedDayOf<-c(2,3,2,3,4,3,2,3,4,5)
AirTempDayBefore<-c(21,40,22,23,24,24,24,27,25,22)
WindSpeedDayBefore<-c(2,5,2,6,4,3,6,3,2,5)
AirTemp2DayBefore<-c(21,45,22,23,34,24,24,23,25,23)
WindSpeed2DayBefore<-c(8,5,3,6,4,7,6,3,2,6)

#Metadata about site
Type<-c(""Forest"",""Forest"",""Forest"",""Forest"",""Forest"",""Beach""""Beach""""Beach"",""Swamp"",""Swamp"")
Population<-c(20,30,31,23,32,43,23,43,23,33)
Use<-c(""Industrial"",""Commercial"",""Industrial"",""Commercial"",""Industrial"",""Commercial"",""Industrial"",""Commercial"",""Industrial"",""Commercial"",)


DF<-data.frame(Date,Size,Color,Length,AirTempDayOf, WindSpeedDayOf,AirTempDayBefore,WindSpeedDayBefore,AirTemp2DayBefore,WindSpeed2DayBefore)

What I don't have is absence data, so I can't make any assumptions about when an organism was not at the site. I'd like to look for patterns in weather that my be driving the arrival of this organism, but all I have is data on when the organism was spotted. 
Is it possible to apply some sort of machine learning to look for patterns that may be driving the arrival of this animal? If I don't have absence data, I'm assuming I cant. I've looked into pseudo-absence models, but I don't know how they might apply here. 
If I can't use machine learning to look at drivers for the presence of these animals, is it possible to use ML to look at possible weather patterns that may be associated with some of the metadata about the site? For example, weather patterns that may be associated with Forrest vs Beach habitats? 
I usually use R for my stats, so any answers including R packages would be helpful. 
Also, note that this is just an example dataset above. I don't expect to find any patterns in the above data, and my actual dataset is much larger. But any code developed for the above data should be applicable
","['neural-networks', 'machine-learning', 'r']","
Yes.
Since you have only one type of data, cluster analysis may be a good choice. 
You can also try '1-class learning' approaches, although I have found these to be unreliable in the past. 
An example of a cluster analysis algorithm in R is kmeans. There are many others. These approaches will reveal points that typify large portions of the dataset. By examining 'typical' cases, and how they differ, you can spot potential causal factors to test experimentally.
An example of a 1-class learning algorithm is a one-class svm. Most svm libraries will accept data of a single class and do the right thing with it. Here's an example with R's e1071 package.
"
Deep Q-Learning agent poor performing actions. Need help optimizing,"
I'm trying to make deep q-learning agent from https://keon.io/deep-q-learning
My environment looks like this:
https://i.stack.imgur.com/EJHTD.jpg
As you can see my agent is a circle and there is one gray track with orange lines (reward gates). The bolder line is an active gate. 
The orange line from the circle pointing to his direction.
The agent has constant velocity and it can turn left/right 10 degrees or do nothing
On the next image are agents sensors
https://i.stack.imgur.com/LqG8J.jpg
They are rotating with the agent.
The states are a distance from the agent to active gate and lengths of sensors.
In total there are 1+7 states and it is q-learning neural net input dimension.
Actions are turn left, turn right and do nothing. 
Reward function returns 25 when the agent intersects reward gate; 125 when agent intersects the last gate; -5 if agent intersects track border
If none of this, reward function compare the distance from the agent to the active gate for current state and next state:
If current state distance > next state distance:
 return 0.1
else
 return -0.1
Also, DQNAgent has negative, positive and neutral memory.
If reward is -5, (state, action, reward, next_state, done) go to the negative memory, 
if reward is >= 25, to positive 
else to neutral
That is because when I'm forming minibatch for training, I'm taking 20 random samples from neutral memory, 6 from positive and 6 from negative.
Every time when agent intersects track border or when he is stuck for more than 30 seconds, I'm doing training (replay) and agent starts from the beginning.
This is my model 
model = Sequential()
model.add(Dense(64, input_dim=self.state_size,activation='relu', 
                  kernel_initializer=VarianceScaling(scale=2.0)))
model.add(Dense(32, 
    activation='relu',kernel_initializer=VarianceScaling(scale=2.0)))
model.add(Dense(self.action_size, activation='linear'))
model.compile(loss=self._huber_loss,
                  optimizer=Adam(lr=self.learning_rate))
return model

I tried different kinds of model, a different number of neurons per layer, other activation and loss functions, dropout, batch normalization, and this model works the best for now
I tried different reward values
Also, I tried to use static sensors (they are not rotating with the agent)
https://i.stack.imgur.com/UCLGM.jpg (green lines on the photo)
Sometimes my agent manages to intersect a few gates before hits the border. Rarely he manages to traverse half of the track and once, with this settings, he traversed two laps before he stuck.
More often, he is only rotating in one place.
I think that the problem lays in state representation or reward function.
Any suggestions would be appreciated
","['reinforcement-learning', 'python', 'q-learning', 'keras', 'deep-rl']",
Why does precision-recall curve become more stable when neural net begins to overfit?,"
I am training a convLSTM with a dropout layer (with prob 0.5). 
If I train over more than 5 epochs I notice that the network starts to overfit: my validation set loss becomes stationary while the train loss keeps going down with every epoch. 
And if I train for 20 or more epochs the gap between the validation and train loss is quite substantial. At the same time precision-recall curve becomes much more stable (i.e. monotonic) if i train with a large number of epochs (e.g. 20). Why is that? Is this behaviour a common occurrence?
","['neural-networks', 'overfitting', 'performance']",
Can I use neural networks for a problem (in description)?,"
I am modelling a process with 4 input parameters x1 x2 x3 x4. The output of the process is 2 variable y1 y2that varies with length and time. 
I also have data from experiments basically recording the trends in the two output variables as I vary my input variable. 
So far I have only seen neural network examples which will take input x1 x2 x3 x4 t 
(t  is time) and predict y1 y2 at said time t (no consideration of location). I would however like to also like to see variation with length as well at a given time t [y1a y1b... y1z; y2a y2b... y2z] where (a, b...z) are location points at an incremental distance dh from the start.
Any help is appreciated. TIA
",['neural-networks'],"
To answer the titular question first: Yes, of course you can. Whether a neural network can give you better results than a simpler model, however, depends on:

How complex/non-linear the relationship between your input and output variables is;
Whether the neural network you have specified is able to learn this relationship efficiently;
How much training data you have.

With the information you have provided in the question, there's really no telling how it will perform against a simpler model, so I would advice trying both. 
As for how your output varies over time and location, just include both in your model. In case of a linear regression model, accounting for spatiotemporal autocorrelation could be done with a mixed model using an appropriate covariance structure. The challenge for a neural network would be how to specify one that can learn this type of relationship (for starters, read up on RNNs for longitudinal data). 
"
"Actor-critic algorithm using gaussian Radial Basis Function, Local Linear Regression and shallow Neural Network","
I'm attempting to implement the actor-critic algorithm on Matlab using Radial Basis Function, Local Linear Regression, and shallow Neural Network for inverted pendulum system.
the state space and the action space are continuous.

states are the angle x_1 wrapped into [-pi pi] and the angle velocity x_2 in [-8*pi 8*pi]
the continuous action u, which is bound between [-3 3].
reward function is quadrat rho=x'Q x+u'R u where Q=diag(1,5) and R=0.1 
the desired point is upright position  [0 0]'


some notes will be added

the used solver is ode45.
the sampling time 0.03.
it explores random u every step, with normal distribution zero mean sigma=1
model of the system (to save place the parameters of the model are not written)
 function dy =pendulum(y,u)
dy(1,1)=y(2);
dy(2,1)=1/J*(M*g*l*sin(y(1))-(b+K^2/R)*y(2)+K/R*u);
end_function



function to calculate RBF: the idea is to define centers and widths for N RBFs which cover the entire state space to approximate the value function and policy separately. The RBF is normalized.
function phi=RBF(x,C,B,N)         % x:state, C: centres, B: width, N: nombre of used RBfs
 Phi_vec=[];
 Phi_sum=0;
 for i=1:N                        % loop for to calculate the vector phi
     Phi_i=exp(-1/2*(x-C(i,:)')'*B^(-1)*(x-C(i,:)'));  % gaussian function
     Phi_vec=[Phi_vec;Phi_i];                    % not normalized phi vector
     Phi_sum=Phi_sum+Phi_i; % sum for normalisation

 end
 phi=Phi_vec/Phi_sum; % normalized phi vector


 % after tuning the learning rate for actor and critic alpha_a and alpha_c 

 %  every step the following updates shall be carried out: 

 %% generally

 %  Value function V=Theta_O'*RBF(x,C,B,N)

 %  policy pi= Theta_v'*RBF(x,C,B,N)


 % determine u(k) with exploration term
 u(k)=Theta_V'*RBF(x,C,B,N)+Delta_U(k-1)

 %% aplly u(k) and gain x(k+1)

 [t,y] = ode45(@(t,y) pendulum(y,u(k)),tspan,x(k)');
        :
 x(k+1,1)= wrapToPi(x(k+1,1)); % wrpping to pi


 % determine Temporal difference Error 
 Delta(k)=r(k)+gamma*Theta_O'*RBF(x(k),C,B,N)-Theta_O'*RBF(x(k-1),C,B,N);

 % eligibility trace
 z=lamda*gamma*z+RBF(x,C,B,N);

 % Critic update
 Theta_O=Theta_O+alpha_c*Delta(k)*z;

 %actor update
 Theta_V=Theta_V+alpha_a*Delta(k)*Delta_U(k- 1)*RBF(x,C,B,N);

","['neural-networks', 'reinforcement-learning', 'linear-regression', 'actor-critic-methods']",
How is the number of parameters reduced in the group convolution?,"
I think I don't understand group convolutions well.
Say you have 2 groups. This means that the number of parameters would be reduced in half. So, assuming you have an image and 100 channels, with a filter size of $3 \times 3$, you would have 900 parameters (ignore the bias for this example). If you separate this into 2 groups, if I understand it well, you would have 2 groups of 50 channels.
This can be made faster, by running the 2 groups in parallel, but how does the number of parameters get halved? Isn't each group having $50*9=450$ parameters, so, in total, you still have 900 parameters? Do they mean that the number of parameters that the backpropagation goes over (in each branch) gets halved?
Because overall, I don't see how it can get reduced. Also, is there a downside in using more groups (even going to 100 groups of 1 channel each)?
","['convolutional-neural-networks', 'convolution', 'filters']","
The number of parameters is filters*input_channels*output_channels
Groups are formed among input and output channels.
So instead of input_channels*output_channels with two groups you get (input_channels/2)*(output_channels/2) + (input_channels/2)*(output_channels/2)
"
What is the benefit of scaling the hyperparameter C of an SVM?,"
Please read the following page of the Sklearn documentation.  
The figure shown there (see below) illustrates why C should be scaled when using a SVM with 'l1' penalty, whereas it shouldn't be scaled C when using one with 'l2' penalty.

The scaling however does not change the scores of the models examined within the GridSearch. So what exactly is this scaling-step good for?
","['hyper-parameters', 'regularization', 'support-vector-machine']",
Is it possible to make a 'forked path' neural network?,"
I want to make a network, specifically a CNN for image recognition, that takes an input, processes it the same way for several layers, and then at some point splits before coming to two different outputs. Is it possible to create a network such as this? It would look something like this:
Input -> Conv -> Pool -> Conv -> Pool ---------> Dense -> Output 1 
                                  ||

                                     ----> Dense -> Output 2

I.E. it splits off after the second pooling layer into separate fully connected layers. Of course, it has to train to both outputs, so that it is producing minimal error on both separate outputs using these common convolutional layers. Also, I am using Python Keras, and it would help if there was some way to do this using Keras in some way. Thank you!  
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'image-recognition']",
What are the key differences between cellular neural network and convolutional neural network?,"
What are the key differences between cellular neural networks and convolutional neural networks in terms of working principle, implementation, potential performance, and applicability?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'comparison', 'cellular-neural-networks']",
Feature Selection using Monte Carlo Tree Search,"
I'm trying to tackle the problem of feature selection as an RL problem, inspired by the paper Feature Selection as a One-Player Game. I know Monte-Carlo tree search (MCTS) is hardly RL.
So, I used MCTS for this problem, where nodes are subsets of features and edges are (""adding a feature to the subset"") and it does converge to the optimal subset slowly.
I have a few questions

Is there a clever way to speed up the convergence of MCTS, besides parallelizing the rollout phase?
Adding nodes to the tree takes time and memory for datasets with a large number of features, for 10000 features, it takes up all my RAM (8GB) from the second iteration, (although it runs for 2000+ iterations for a dataset with 40 features which doesn't make sense to me). Is this expected or is my implementation likely wrong? Are there any workarounds for this?
What are your opinions on using MCTS for this task? Can you think of a better approach? The main problem of this approach is running the SVM as an evaluation function which may make the algorithm impractically slow for large datasets (a large number of training examples).

I was thinking of trying to come up with a heuristic function to evaluate subsets instead of the SVM. But I'm kind of lost and don't know how to do that. Any help would be really appreciated. 
","['reinforcement-learning', 'monte-carlo-tree-search', 'feature-selection']",
Why are not validation accuracy and loss as smooth as train accuracy and loss?,"
I am training a modified VGG16 network for classification (adding 0.5 dropout after each of the last FC layers). In the following plot I am training for a small number of epochs as an example, and it shows the accuracy and loss curves of training process on both training and validation datasets. My training set size is $1725$, and $429$ for validation. Also I am training with weights=None

My question is about the validation curves, why do not they appear to be as smooth as the training ones? Is this normal during the training stage?
","['neural-networks', 'convolutional-neural-networks', 'training']",
Is the minimum and maximum of a set of admissible and consistent heuristics also consistent and admissible?,"
Let's suppose I have a set of heuristics $H$ = {$h_1, h_2, ..., h_N$}.

If all heuristics in $H$ are admissible, does that mean that a heuristic that takes the $\min(H)$ (or $\max(H)$ for that matter) is also admissible?
If all heuristics in $H$ are consistent, does that mean that a heuristic that takes the $\min(H)$ (or $max(H)$ for that matter) is also consistent?

I'm thinking about a search problem in a bi-dimensional grid that every iteration of an algorithm, the agent will have to find a different goal. Therefore, depending on the goal node, a certain heuristic can possibly better guide the agent than the others (hence the use of $\min$ and $\max$).
","['search', 'proofs', 'heuristics', 'admissible-heuristic', 'consistent-heuristic']",
"How does bidirectional encoding allow the predicted word to indirectly ""see itself""?","
Before the release of BERT, we used to say that it is not possible to train bidirectional models by simply conditioning each word on its previous and next words, since this would allow the word that's being predicted to indirectly ""see itself"" in a multi-layer model. How does this happen? 
","['deep-learning', 'natural-language-processing', 'recurrent-neural-networks', 'bert']",
How to make meaningful sentences from a set of words?,"
I have set of topics generated using LDA and  like {code, language, test , write, function}, {class, public, method, string, int} etc and I want to make meaningful sentence/sentences from these words using  api or libraries.  How do I implement this with NLTK and(or) Machine Learning? Any suggestions as to how I should go about this?
","['machine-learning', 'natural-language-processing']","
How do you define ""meaningful""? Generally, you would start from concepts and meanings, and then realise them in syntactic structures using lexical items (words). You seem to want to start in the middle somehow.
For turning a semantic representation into a valid sentence, you would use a generator; these are often based on grammars. Examples exist which take a grammar, fill in random words, and create a syntactically well-formed sentence; often they will, however, be rather non-sensical or meaningless. Have a look at this site which describes the Syntax Construction Kit. The author, Mark Rosenfelder, links to a number of toy programs which do exactly that. Just substitute his lexicon with the list of words created by your LDA process. See for example this generator based on generative grammar.
"
Shortest route GA: One loop through one dataset vs multiple loops through subsets of the same data?,"
I've a rather simple question for a school project. We're developing a GA solution for the following problem:
Chromosome: A location with lat-lon coords. There are two types of locations - up to 15 waypoints from user input, and a dataset of about 3-400 stations.
Gene: A route consisting of all waypoints (incl. a fixed start and end) + 1 station.
Fitness function: Shortest path.
Stop condition: Run duration - configurable, default 3 seconds.
We're discussing two possible implementations:

A problem set of all waypoints and all stations, kinda like soccer team assignment or nurse rostering design. Run GA once on all of that.
A problem set of all waypoints and one station, do TSP. Run GA for number-of-stations-in-dataset iterations.

Which is better, in terms of design, efficiency, and performance?
","['genetic-algorithms', 'evolutionary-algorithms', 'genetic-programming']",
How do the achievements met in the gaming field (ex. AlphaGo Zero) impact other fields of application?,"
How can we use the ability of AlphaGo Zero computer, to do something in any other life important related field? Is it possible to make something important besides having created something so smart that can play mind games way better than humans?
","['machine-learning', 'gaming', 'alphago-zero']","
Yes it's created something important. Until Alpha(Go) Zero all (or almost all) of Deep Learning approach to Reinforcement Learning was based on Time Difference loss function. The weakness of Time Difference loss function was that it was essentially training on itself, that is data produced by the same method was used as part of regression target. That was producing the problem of ""extrapolation error"" - solution would blow up, or oscillate wildly. There was attempts to mitigate that problem (n-steps algorithms), but they weren't helping much. Alpha Zero combined Deep Network with tree search instead (Monte Carlo Tree Search). Tree search algorithm produced wide and long fields of high precision data (value function), with network influence on value much diminished. That way network was training mostly not on itself, but on data produced by tree, and tree search itself was accelerated greatly by network (using it as heuristics). The whole happens to be much more then sum of parts.
This approach is not limited to broad game or RL theory. It may work for any problem for which high-precision simulator could be built. Essentially if problem allow Monte Carlo Tree Search, or other tree search which could be augmented with heuristic, Alpha Zero approach would probably work on it. Of cause Alpha Zero approach is computationally expensive, so it's not always efficient to apply it.
"
How do we ensure that training GANs will fall in the desirable Nash equilibrium?,"
One Nash equilibrium of every GANs model has is when the generator creates  perfect samples indistinguishable from the training data and the discriminator just output 1 with probability 1/2. And I think this is the desirable outcome since we are most interested in the generator part of the GAN model. I know that we probably try to converge to this equilibrium with some hacks in training such as ""mode collapse avoidance"" and so on. But is there any theoretical work trying to go in another ways (say, by reduce the number of Nash equilibria somehow?)
","['neural-networks', 'machine-learning', 'generative-model']",
Models of reward (possibly mimicking dopamine) in artificial neural networks?,"
How one can model physiological reward mechanisms occuring in the brain using artificial neural networks? E.g. are there efforts to use the notion of dopamine or similar substances in the artificial neural networks. Maybe introduction of the physiological reward mechanism can lead to the emergence of consciousness or at least enhance the effectiveness of reinforcement learning?
Essentially - how neural network models reward? People's brain perceive money as the ultimate reward because almost everything other can be bought by this. So - mental perception of owning money gives reward. But how this notion of reward is modeled in artificial neural networks? How networks know that some money is assigned to the network's account and so, the network should feel happy and rewarded and should strive to repeat successful behavior?
I am reading https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5293493/pdf/elife-21492.pdf and I hope that it will move me in the right direction.
It is quite confusing. The old-school neural networks expect that there are 2 separate phases: training and inference. So, the network receives all the feedback (let it be called reward) in the training phase and network receives nothing in the inference phase. But maybe network should receive some reward during acting-inference phase as well, kind a lifelong learning.
","['neural-networks', 'brain']",
What are some simple open problems in multi-agent RL that would be suited for a bachelor's thesis?,"
I've decided to make my bachelor thesis in RL. I am currently struggling to find a good problem. I am interested in multi-agent RL with the dilemma between selfishness and cooperation.
I only have 2 months to complete this and I'm afraid that multi-agent RL is too difficult and I don't have the knowledge and time to nicely learn this topic.
What are some simple open problems in multi-agent reinforcement learning that would be suited for a bachelor's thesis?
I've only done applied the Q-learning algorithm to solve a text-based environment in OpenAI's gym.
","['reinforcement-learning', 'reference-request', 'research', 'academia', 'multi-objective-rl']","
I have several undergraduates working on multiagent deep RL problems for their theses, but most of them have been working for 8-9 months. 2 might be a stretch.
Good multiagent deep RL problems for a bachelor's thesis might look something like:

Pick an older video game, which has been studied using Deep RL, but not in depth. Right now my students have been liking Nintendo 64 games.
Read the papers that study this game already.
Pick one of the described approaches and reproduce the paper's results in your own system.
Pick one of the parameters that the paper does not explore changing, and see what happens as you change it.

This probably does not lead to a publishable result, but it is real science and can make for a fine undergraduate thesis.
A slightly harder project, which may require more time, would be to examine the ""future work"" sections of these papers, and perform one of the experiments suggested there. These experiments often lead to small publishable results.
"
How do I denoize a microscopic image?,"
I'm working in a computer vision project, where the goal is to detect some specific parasites, but now that I have the images, I noticed that they have a watermark that specifies the microscope graduation.
I have some ideas of how to remove this noise, like detecting the numbers and replace for the most common background or split the image but if I split the image I'll lose information.
But I would like to hear some recommendations and guidelines of experts. 
I added an example image below.

","['convolutional-neural-networks', 'image-recognition', 'computer-vision']",
Why is Monte Carlo used as the tree search algorithm for AlphaGo?,"
Could a better algorithm other than Monte Carlo be used for the AlphaGo computer? Why didn't the DeepMind team think of choosing another kind of algorithm rather than spending time on their neural nets?
","['monte-carlo-tree-search', 'alphago', 'monte-carlo-methods', 'alphago-zero']","
The paper that introduced AlphaGo, Mastering the game of Go with deep neural networks and tree search, motivates the use of MCTS

Monte Carlo tree search (MCTS) uses Monte Carlo rollouts to estimate the value of each state in a search tree. As more simulations are executed, the search tree grows larger and the relevant values become more accurate. The policy used to select actions during search is also improved over time, by selecting children with higher values. Asymptotically, this policy converges to optimal play, and the evaluations converge to the optimal value function. The strongest current Go programs are based on MCTS, enhanced by policies that are trained to predict human expert moves. These policies are used to narrow the search to a beam of high-probability actions, and to sample actions during rollouts. This approach has achieved strong amateur play.

"
How to predict a preferred route based on weather and distance,"
I want to train a neural network to predict what my favourite home-work route will be for a particular day. I have these features for routes on a day: 
temperature, humidity, congestion, distance, duration

I have come up with this concept of training/testing a network:
//
// training features result in route 1,2 or 3
//
Network.train([30,10,12,20,12] , 1)
Network.train([20,10,22,20,14] , 3)
Network.train([23,10,2,20,10] , 2)
Network.train([20,10,22,20,12] , 2)

//
// On a new day, predict which route the user is most likely to take:
//
var route = Network.test([25,8,12,22,12])

My question is: is this a viable approach? Can I make relevant predictions this way if I have enough training data? 
Can I generate an outcome between 1 and 3 this way? 
","['neural-networks', 'machine-learning']",
How can we reach global optimum?,"
Gradient descent can get stuck into local optimum. Which techniques are there to reach global optimum?
","['machine-learning', 'optimization', 'gradient-descent']","
In Deep Learning there are several methods to improve ""stuck"" gradient - decrease learning rate, use cyclic learning rate - cycle it from bigger to smaller value. More radical method is completely reinitialize last or two last (before loss) layers of the network. 
In non-Deep Learning ML out of those only decrease learning rate will work, but there is plethora Numerical Optimization methods to help, like second-order methods - variation of Gauss-Newton, or methods specific to the problem which may include projective methods, alternate directions, conjugate gradients etc. There are a lot of methods which are better then gradient descent for non-Deep Learning optimization.
"
When is bias values updated in back propagation?,"
I am new to deep learning. I have doubts on modifying bias values during back propagation. My doubts are

Does the back propagation algorithm modifies the weigh values and bias values in the same pass?
How does the algorithm decide whether it has to change the weight value or bias value to reduce the error in a pass?
Will the learning rate same for bias and weights? 

Thanks!
","['deep-learning', 'convolutional-neural-networks', 'backpropagation', 'gradient-descent']",
CNN output generally has more than one category in one-hot categorization?,"
I'm a bit of a CNN newbie, and I'm trying to train one to image classify pictures of pretty similar looking particles. I'm making the inputs and labels by hand from a set of 48x48 grayscale images, and labeling them with a one-hot vector based on their position in the sequence (for example, the 400/1000th image might have a one-hot in the 4th position if I have 10 categories in the run). I'm using sigmoidal output activation and categorical cross entropy loss. I've played around with a few different optimizers, as well. I'm implementing in python keras. 
Unfortunately, although I have pretty good accuracy numbers for the training and validation, when I actually look at the outputs being produced, it generally gives multiple categories, which is not at all what I want. For example, if I have 6 categories and a label of 3, it might give the following probability vector:
[ .99 .98  1.0  .99  0.02  0.05 ]
It was my understanding that categorical cross entropy would not allow this type of categorization, and yet it is prevalent in my code. I am under the impression that I'm doing something fundamentally wrong, but I cant figure out what. Any help would be appreciated. 
","['convolutional-neural-networks', 'image-recognition', 'training', 'categorical-data']",
Can I calculate the training performance of GPUs by comparing their specification? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I am currently using Nvidia GTX1050 with 640 CUDA cores and 2GB GDDR5 for Deep Neural Network training. I want to buy a new GPU for training, but I am not sure how much performance improvement I can get.
I wonder if there is a way to roughly calculate the training performance improvement by just comparing GPUs' specification?
Assuming all training parameters are the same. I wonder if I can roughly assume the training performance improvement is X times because the CUDA core number and memory size increased X times?
For example, Is RTX2070 with 2304 CUDA cores and 8GB GDDR6 roughly 4 times faster than GTX1050? And is RTX2080Ti with 4352 CUDA cores and 11GB GDDR6 roughly 7 times faster than GTX1050?
Thanks.
","['training', 'gpu']",
What can be considered a deep recurrent neural network?,"
In the paper Deep Recurrent Q-Learning for Partially Observable MDPs, the DRQN is described as DQN with the first post-convolutional fully-connected layer replaced by a recurrent LSTM.
I have DQN implementation with only two dense layers. I want to change this into DRQN with the first layer as an LSTM and leave the second dense layer untouched. If I understood correctly, I would also need to change the input data appropriately. 
Are there any other things that need to be modified in order to make DRQN work?
","['reinforcement-learning', 'ai-design', 'long-short-term-memory', 'dqn', 'deep-rl']",
Can DQN perform better than Double DQN?,"
I'm training both DQN and double DQN in the same environment, but DQN performs significantly better than double DQN. As I've seen in the double DQN paper, double DQN should perform better than DQN. Am I doing something wrong or is it possible?
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'double-dqn']","
That may happen when the value of the state is bad. You can find the example and explain about that in the link below.
See this:https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682
"
Why do DQNs use linear activations on cartpole?,"
I've been reading a lot of tutorials on DQNs for cartpole. In many of them, they have the funnel layer of the neural net be a linear activation.
Why is this?
Is it just a choice made by the implementer?
Is this Choice specific to cartpole, or do most control task dqns use it?
Thanks. 
","['neural-networks', 'dqn']",
Can gamma be greater than 1 in a DQN?,"
If I have a DQN, and I care A LOT about future rewards (moreso than current rewards), can I set gamma to a number greater than 1? Like 1.1 perhaps?
","['q-learning', 'dqn']","
$ \gamma $ goes up to 1, but cannot be greater than or equal to 1 (this would make the discounted reward infinite).
The discount factor $ \gamma $  determines the importance of future rewards. A factor of 0 will make the agent ""myopic"" (or short-sighted) by only considering current rewards, while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the action values may diverge. For $ \gamma =1$, without a terminal state, or if the agent never reaches one, all environment histories become infinitely long, and utilities with additive, undiscounted rewards generally become infinite.
Source:https://en.wikipedia.org/wiki/Q-learning
https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html
"
What research has been done on learning non-Markovian reward functions?,"
Recently, some work has been done planning and learning in Non-Markovian Decision Processes, that is, decision-making with temporally extended rewards. In these settings, a particular reward is received only when a particular temporal logic formula is satisfied (LTL or CTL formula). However, I cannot find any work about learning which rewards correspond to which temporally extended behavior.
In my searches, I came across k-order MDPs (which are non-Markovian). I did not find RL research done on k-order MDPs.
","['reinforcement-learning', 'reference-request', 'reward-functions', 'markov-property']",
Has any research been done to solve word searches with AI?,"
A simple word search seems too simple to solve with a computer AI. But what I'm interested is how human's solve it. They build up strategies over the course of solving the puzzle. For example:
1) first just look to see if any words ""jump out"".
2) look for the horizontal words.
3) look for the letter ""O""
4) look for the letter ""p"" next to a letter ""o""
5) methodically look along rows and columns and diagonals. 
6) cross off words
7) take the first letter of a word in the list and put it in memory.
Things of that sort. I would like to build such a program. With built in search capabilities, some of which are faster than others. And the AI can combine different search methods and try to solve the puzzle as fast as possible.  
It should also store strategies that work. Or think about why strategies work sometimes and not others. 
I would like to read a bit more about AI and strategies if you know any good references?
",['ai-design'],
What is the feasible neural network structure that can learn to identify types of trajectory of moving dots?,"
I have multiple image sequences, each of which contains an animation of two moving dots. The trajectory of the dots in a sequence is always cyclic (not necessarily circular). There are two types of sequences. In some sequences the two dots are moving in phase but in the other sequences they are moving out of phase.
Is it possible to classify these two types of sequences using a neural network? What is the simple and feasible neural network structure for this classification?
Here's an example set of animations. The in-phase patterns are in the above row and the out-of-phase patterns are in the bottom row.

","['recurrent-neural-networks', 'deep-neural-networks']",
Algorithms to indentify people in pictures without using face recognition,"
There are lot of researches about face detection in pictures, but is it the only way one can say ""this person I'm looking for is here in this picture""? Aren't there algorithms that you can provide with information like lateral or back pictures of a person and making calculations on the height, the width, the anatomic distance between parts of its body, the analysis of the hair can determine: ""there's a X per cent chance this is the person you're looking for""? Is it possible to accomplish?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'image-recognition']",
RNN: Different test results on balanced and unbalanced data,"
I trained a recurrent neural network (if it matters - it contains three CuDNNLSTM cells and 3 Dense layers, Dropout = 0.2). The result of data preparation is one array of ~330.000 sequences. Each contains 256 time steps and 24 features in each time step. This array is normalized, shuffled and balanced. Then it is split into two arrays - train array contains 90% of data (so ~297k) and validation array contains ~10%.
During training process (Adam optimizer, 128 or 256 batches) max accuracy of validation data set is 90%. Epoch accuracy is 94%.
Then I run my additional validation test script with more realistic, unbalanced data set. The accuracy of test is just ~50%. I checked this second test if it is correct and there is no errors in the code, but when I run it on data, that was included in the training set, the accuracy was 87%, so it looks good.
What is going on? I suppose, that wrong architecture is used.
Here is a graph of epoch accuracy during training:

Here is a graph of validation accuracy during training

Thank you for support.
UPDATE:
Today I trained the network one more time. I used the unbalanced data from second test as validation data set in traininge process. You can see the results below.I stopped training after 29 epochs. 
Blue: balanced valitadion data set
Orange: unbalanced validation data set
Epoch accuracy:

Validation accuracy

Validation loss

It doesn't look good at all.
","['neural-networks', 'tensorflow', 'long-short-term-memory', 'intelligence-testing']",
Deep Learning for radio signal classification with DeepSig dataset,"
I want to see if I can make my Software Defined Radio, SDR, to classify unknown radio signals with the help of an artificial neural network. That is, my SDR outputs a sequence of complex numbers (IQ-data), which I want to use to determine if the receieved signal is, for instance, FM or AM modulated. This approach was used in a paper (https://arxiv.org/pdf/1712.04578.pdf) and they created and used a freely downloadable dataset (https://github.com/sofwerx/deepsig_datasets/blob/master/README.md).
Being new to both SDR and Deep Learning I have now tried for a couple of months to create an LSTM network, train it on the dataset and then use it for classification, but have sadly failed. I have concluded that it is likely due to the fact that I do not seem to understand how the dataset is structured. I have not been able to find any documentation concerning this dataset (other than a text file with a list of the modulation forms used in the dataset) though. My hope is that someone on this forum has some prior experience to share about how to use it.
The dataset (DEEPSIG DATASET: RADIOML 2016.10A) is split in three matrixes, X: 2x1024x2555904 cells, Y: 24x2555904 cells and Z: 1x2555904 cells. My belief has up until now been that ""X"" contains the complex time series, i.e. one row for the real component and one row for the imaginary component both in a sequence of 1024 samples. The ""Y"" to contain the corresponding 24 classes, whereas the ""Z"" somehow contain the signal to noise level for each signal.
After training and deploying the network in Matlab I found out that it does not even manage to classify the local FM radio station. I then looked at the data I had used for training and plotted one of (what I thought to be) the FM sample sequences from the dataset in the complex plane, but I did not get the ""circle"" I had expected from a frequency or phase modulated signal. I am thus totally lost, the dataset has been used for a high class scientific paper so the problem lies with me, and my lack of understanding. I apologize for the long and still very unconcise question, but I don't want to infer any of my own misconceptions into the query. Thanks for any help!
","['datasets', 'matlab']",
At which point we have to stop post pruning in decision tree?,"
Post pruning is start from downward discarding subtree and include leaf node performance. so what is the best point or condition of the tree where we have to stop further pruning. 
","['machine-learning', 'decision-trees']","
There are a variety of conditions we can use when deciding whether to prune a sub-tree or not after generating a decision tree model. There are three common approaches. 

We can prune branches with less support than a specific threshold. These are branches which were constructed using very few points from the training data.
We can prune branches where the information gain from a split (or any other splitting measure we are using, like GINI), is smaller than a threshold.
You can do what is done in Quinlan's 4.5 & C5.0 learners (which are the standard approaches; J48 is another implementation of the same algorithm). Quinlan performs a Chi-squared-like test for the relationship between the attribute we split upon and the target attribute. If the relationship is statistically significant, then the split is preserved. If not, it is not. The ""confidence factor"" parameter found in most implementations of these algorithms corresponds to the $\alpha$ value used in determining whether the relationship is considered significant. This approach captures the idea that we should prefer to keep branches that have few datapoints, but a very strong signal, or that have a weak signal, but very many datapoints supporting the pattern, since both cases are less likely to be overfitting that cases where we have weak signals and small numbers of points.

"
Can a neuron have both a bias and a threshold?,"
I have not seen a neuron that uses both a bias and a threshold. Why is this? 
","['neural-networks', 'neurons', 'perceptron']","
I assume you're talking about a perceptron threshold function. One definition of it with an explicit threshold is
$$f(\textbf{x})= 
\begin{cases}
1& \text{if } \textbf{w}\cdot\textbf{x} > t\\
0& \text{otherwise}
\end{cases}.$$
Another form with a bias is
$$f(\textbf{x})= 
\begin{cases}
1& \text{if } \textbf{w}\cdot\textbf{x} + b > 0\\
0& \text{otherwise}
\end{cases}.$$
But these forms are of course equivalent if you set $b=-t$.
There's nothing stopping you from using a perceptron definition with both a bias and a threshold:
$$f(\textbf{x})= 
\begin{cases}
1& \text{if } \textbf{w}\cdot\textbf{x} +b > t\\
0& \text{otherwise}
\end{cases}.$$
But this is also equivalent to the other two forms. We can rewrite this as
$$f(\textbf{x})= 
\begin{cases}
1& \text{if } \textbf{w}\cdot\textbf{x} > t'\\
0& \text{otherwise}
\end{cases}$$
where $t'=t-b$ is the new threshold. Or, we could rewrite it as 
$$f(\textbf{x})= 
\begin{cases}
1& \text{if } \textbf{w}\cdot\textbf{x} + a> 0\\
0& \text{otherwise}
\end{cases}$$
where $a=b-t$ is the new bias.
You never see definitions of this function with both a threshold and a bias because it has simpler forms.
"
Concrete examples of OpenCog's functionality,"
Does anyone know what specific tasks the OpenCog environment is capable of performing? I have glanced though their wiki and a few of the pages on Goertzel's site and the AI.SE. So far I could only find some technical documentation regarding theory and engineering, but nothing on concrete results.
From the technical description of AtomSpaces it seems that OpenCog is capable of some ""representational inference"", but I haven't come across any sources that concretely describes what it is capable of doing.
Apparently there is some collaboration between Sophia the Robot and OpenCog, but to what extent I am unclear. I am aware however that the dialogue functions is powered by ChatScript (though I also suspect that the high profile interviews Sophia gives are completely scripted...)
Can anyone provide concrete examples or evidence of OpenCogs' functional behavior. Like transcripts of chat, examples of reasoning, video or demonstrations of its emotion-emulating; and not just claims of functions.
","['applications', 'agi', 'open-cog']","
It depends what you mean by ""what OpenCog can do?"". 
OpenCog, at a high-level, is a loosely coupled collection of various theoretical and variants of conventional methods aimed at constructing the beginnings of an AGI.
With that said, it's purely applied uses are fairly limited. It can do some typical NLP and ML tasks if used correctly, albeit, it will almost always be ineffective when compared to a problem specific solution. However, this is understandable, as OpenCog is not geared at solving narrow problems.
OpenCog, as well as Goertzel's other projects(SingularityNet, AGI society), are broad, top-down attempts at formulating an AGI system.
Outside of AGI circles they are also not oft referenced or researched(although, I am utilizing some aspects of the system in work I am currently crafting). 
"
Why doesn't Q-learning converge when using function approximation?,"
The tabular Q-learning algorithm is guaranteed to find the optimal $Q$ function, $Q^*$, provided the following conditions (the Robbins-Monro conditions) regarding the learning rate are satisfied

$\sum_{t} \alpha_t(s, a) = \infty$
$\sum_{t} \alpha_t^2(s, a) < \infty$

where $\alpha_t(s, a)$ means the learning rate used when updating the $Q$ value associated with state $s$ and action $a$ at time time step $t$, where $0 \leq  \alpha_t(s, a) < 1$ is assumed to be true, for all states $s$ and actions $a$.
Apparently, given that $0 \leq  \alpha_t(s, a) < 1$, in order for the two conditions to be true, all state-action pairs must be visited infinitely often: this is also stated in the book Reinforcement Learning: An Introduction, apart from the fact that this should be widely known and it is the rationale behind the usage of the $\epsilon$-greedy policy (or similar policies) during training.
A complete proof that shows that $Q$-learning finds the optimal $Q$ function can be found in the paper Convergence of Q-learning: A Simple Proof (by Francisco S. Melo). He uses concepts like contraction mapping in order to define the optimal $Q$ function (see also What is the Bellman operator in reinforcement learning?), which is a fixed point of this contraction operator. He also uses a theorem (n. 2) regarding the random process that converges to $0$, given a few assumptions. (The proof might not be easy to follow if you are not a math guy.)
If a neural network is used to represent the $Q$ function, do the convergence guarantees of $Q$-learning still hold? Why does (or not) Q-learning converge when using function approximation? Is there a formal proof of such non-convergence of $Q$-learning using function approximation? 
I am looking for different types of answers, from those that give just the intuition behind the non-convergence of $Q$-learning when using function approximation to those that provide a formal proof (or a link to a paper with a formal proof).
","['reinforcement-learning', 'q-learning', 'deep-rl', 'proofs', 'function-approximation']","
Here's an intuitive description answer:
Function approximation can be done with any parameterizable function. Consider the problem of a $Q(s,a)$ space where $s$ is the positive reals, $a$ is $0$ or $1$, and the true Q-function is $Q(s, 0) = s^2$, and $Q(s, 1)= 2s^2$, for all states. If your function approximator is $Q(s, a) = m*s + n*a + b$, there exists no parameters which can accurately represent the true $Q$ function (we're trying to fit a line to a quadratic function). Consequently, even if you chose a good learning rate, and visit all states infinitely often, your approximation function will never converge to the true $Q$ function. 
And here's a bit more detail:

Neural networks approximate functions. A function can be approximated to greater or lesser degrees by using more or less complex polynomials to approximate it. If you're familiar with Taylor Series approximation, this idea should seem pretty natural. If not, think about a function like a sine-wave over the interval [0-$\pi/2$). You can approximate it (badly) with a straight line. You can approximate it better with a quadratic curve. By increasing the degree of the polynomial we use to approximate the curve, we can get something that fits the curve more and more closely.
Neural networks are universal function approximators. This means that, if you have a function, you can also make a neural network that is deep or wide enough that it can approximate the function you have created to an arbitrarily precise degree. However, any specific network topology you pick will be unable to learn all functions, unless it is infinitely wide or infinitely deep. This is analogous to how, if you pick the right parameters, a line can fit any two points, but not any 3 points. If you pick a network that is of a certain finite width or depth, I can always construct a function that needs a few more neurons to fit properly.
Q-learning's bounds hold only when the representation of the Q-function is exact. To see why, suppose that you chose to approximate your Q-function with a linear interpolation. If the true function can take any shape at all, then clearly the error in our interpolation can be made unboundedly large simply by constructing a XOR-like Q-function function, and no amount of extra time or data will allow us to reduce this error. If you use a function approximator, and the true function you try to fit is not something that the function can approximate arbitrarily well, then your model will not converge properly, even with a well-chosen learning rate and exploration rate. Using the terminology of computational learning theory, we might say that the convergence proofs for Q-learning have implicitly assumed that the true Q-function is a member of the hypothesis space from which you will select your model. 

"
Which algorithm should I use to map an input sentence to an output sentence?,"
I am new to NLP realm. If you have an input text ""The price of orange has increased"" and output text ""Increase the production of orange"". Can we make our RNN model to predict the output text? Or what algorithm should I use?
","['machine-learning', 'natural-language-processing', 'recurrent-neural-networks']","
In your case you can use either RNN (especially BiLSTM with ELMo and attention mechanism for better accuracy) or Transformer based architectures (the best of them today is BERT). But for both cases you need data to train the model (i.e sequences of input/output like in your question).
I believe the best choice in your case is BERT as it's achieving state of the art performance in most NLP tasks and is already pretrained so you don't need a massive data to retrain the model. Also, BERT is pretrained on ""Next Sentence Prediction"" and allows ""2 separate sentences"" as input which helps a lot in your case. The only drawback, compared to other methods, is that you need fine-tuning the model so it's not a completly ""ready to use"" model. 
For more information:
Here is the github repository of BERT: BERT-repo. For quick explanation check: The Illustrated BERT. For detailed explanation see BERT paper: BERT-paper.
"
Understanding the n-step off-policy SARSA update,"
In Sutton & Barto's book (2nd ed) page 149, there is the equation 7.11

I am having a hard time understanding this equation.
I would have thought that we should be moving $Q$ towards $G$, where $G$ would be corrected by importance sampling, but only $G$, not $G-Q$, therefore I would have thought that the correct equation would be of the form
$Q \leftarrow Q + \alpha (\rho G - Q)$
and not
$Q \leftarrow Q + \alpha \rho (G - Q)$
I don't get why the entire update is weighted by $\rho$ and not only the sampled return $G$.
","['reinforcement-learning', 'sutton-barto', 'off-policy-methods', 'temporal-difference-methods', 'sarsa']",
Extending a neural network to classify new objects,"
Suppose a model M classifies apples and oranges. Can M be extended to classify a third class of objects, e.g., pears, such that the new images for 'retraining' only have pears annotated and apples and oranges ignored? That is, since M already classifies apples and oranges, can the old weights be somehow preserved and let the retraining focus specifically on learning about pears?
Methods such as fine-tuning and learning without forgetting seem to require all objects in the new images annotated though.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']","
Yes this is standard transfer learning. Using a trained model, we can freeze the first N hidden layers of a classifier, except for the last few. This will allow our previous relevant training to be retained whilst also being able learn new features and target new classes.
We will then initialize a new output layer that works our new context(i.e sigmoid, 1 node for binary classifier). Everything is now set to resume training on our new y_targets.
Take a look at this link for some more info on transfer learning.
If you want no perturbance to your past learning, I would recommend freezing all previous hidden layers and then tacking some additional ones on.
"
What kind of functions can be used as activation functions? [duplicate],"







This question already has an answer here:
                                
                            




Which functions can be activation functions?

                                (1 answer)
                            

Closed 2 years ago.



I read that functions are used as activation functions only when they are differentiable. What about the unit step activation function? So, is there any other reason a function can be used as an activation function (apart from being differentiable)?
","['neural-networks', 'machine-learning', 'activation-functions']","
Not completely sure your question. Do you mean
Q. why should we use activation function? 
Ans: we need to introduce non-linearity to the network. Otherwise, multiple layers are no difference from single layer network. (It is obvious as we write things in matrix form, and say when we have two layers with weights $W_1$ and $W_2$, the two layer is no difference from a single layer with weight $W_2 W_1$.
Q. why they need to be differentiable?
Ans: Just for sake that we can back-propagate gradients back to earlier layers. Note that back-propagation is nothing but the chain rule in calculus. Say $f(\cdot)$ is an activation function in one layer and the output of that activation function is $\bf y$ and the input is ${\bf u}=W \bf x$, where $\bf x$ is output from the previous layer and mix with weights $W$ in current layer. Of course, the final loss $L$ will depend on ${\bf y} = f({\bf u})= f(W {\bf x})$. Say, loss $L=g(\bf y)$ somehow. To train the weights $W$, we have to find the gradient $\frac{\partial L}{\partial W}$ so that we can adjust weight $W$ to minimize $L$. But $\frac{\partial L}{\partial W}=\frac{\partial g({\bf y})}{\partial W}=\frac{\partial g({\bf y})}{\partial \bf y}\frac{\partial {\bf y}}{\partial {\bf u}}\frac{\partial {\bf u}}{\partial W}$. Each of these product terms can be computed locally and will be accumulatively multiplied as we apply backprop.  And note that the middle term $\frac{\partial {\bf y}}{\partial {\bf u}}=\frac{\partial f({\bf u})}{\partial {\bf u}}$ is just ""derivative"" of $f(\cdot)$, thus we require the activation function to be differentiable and ""informative""/non-zero (at least most of the time). Note that ReLU is not differentiable everywhere and that is why researchers (at least Yoshua Bengio) worried about that when they first tried to adopt ReLU. You may check out the interview of Bengio by Andrew Ng for that. 
Q. why step function is a bad activation function?
Ans: Note that step function is differentiable almost everywhere but is not ""informative"" though. For places (flat region) where it is differentiable, the derivative is simply zero. Consequently, any later layer gradient (information) will get cut off as it passes through a step function activation function.
"
Why isn't the reverse KL divergence commonly used in supervised learning?,"
Forward KL Divergence (also known as cross entropy loss) is a standard loss function in supervised learning problems. I understand why it is so: matching a known a trained distribution to a known distribution fits $P \log(P/Q)$ where $P$ is the known distribution. 
Why isn't the reverse KL divergence commonly used in supervised learning?
","['machine-learning', 'optimization', 'objective-functions', 'supervised-learning']","
Its not an exhaustive answer to your question, but here some aspects that might be helpful:
A common problem in supervised learning where you will see KL-divergence used are classification tasks. Very often in those cases the data points in the training set are assumed to belong to a single class $c_i$ where $i$ is from some index set $I$. The class membership is represented by one-hot encoding, which corresponds to a distribution $P$ with $p_i = 1$ for the class the data point belongs to and $p_j = 0$ for all $i \neq j$. The loss function mostly used in this problems is the average categorical cross-entropy $\langle E_P[-log(Q)]\rangle_{\rm data} = \langle H(P) - D(P || Q)\rangle_{\rm data}$, which in the case of single-class membership reduces to the binary cross-entropy. Because only one probability in $P$ is non-zero in this case, the cross-entropy simplifies to $\langle -\log(q_i)\rangle_{\rm data}$. The reverse KL-Divergence $D(Q || P)$ is strictly speaking not even defined in this case, as $Q$ needs to be absolute continuous w.r.t. $P$. 
There are of course cases were class membership is probabilistic also in the training data and the above argument doesn't apply in full. From a more conceptual point of view, this thread summaries nicely the relation of cross-entropy to maximizing the likelihood of the observed data under the model. Using the reverse cross-entropy would maximize the likelihood of a typical sample drawn from the model under the (empirical) distribution of the observed data. 
A small side note when you read the above link is that in the above described classification problem we have a probability model for the class membership for each point in the input space, this is why the average cross-entropy is minimized.
Classification problems are just one example of supervised learning, so this answer might not fully cover your question.
"
Is back-propagation applied for each data point or for a batch of data points?,"
I am new to deep learning and trying to understand the concept of back-propagation. I have a doubt about when the back-propagation is applied.  Assume that I have a training data set of 1000 images for handwritten letters,

Is back-propagation applied immediately after getting the output for each input or after getting the output for all inputs in a batch?

Is back-propagation applied $n$ times till the neural network gives a satisfactory result for a single data point before going to work on the next data point?


","['neural-networks', 'backpropagation', 'gradient-descent', 'stochastic-gradient-descent', 'mini-batch-gradient-descent']",
Which loss function is the brain optimizing in order to learn advanced visual skills without expert/human supervision?,"
In computer vision is very common to use supervised tasks, where datasets have to be manually annotated by humans. Some examples are object classification (class labels), detection (bounding boxes) and segmentation (pixel-level masks). These datasets are essentially pairs of inputs-outputs which are used to train Convolutional Neural Networks to learn the mapping from inputs to outputs, via gradient descent optimization. But animals don't need anybody to show them bounding boxes or masks on top of things in order for them to learn to detect objects and make sense of the visual world around them. This leads me to think that brains must be performing some sort of self-supervision to train themselves to see.
What does current research say about the learning paradigm used by brains to achieve such an outstanding level of visual competence? Which tasks do brains use to train themselves to be so good at processing visual information and making sense of the visual world around them? Or said in other words: how does the brain manage to train its neural networks without having access to manually annotated datasets like ImageNet, COCO, etc. (i.e. what does the brain use as ground truth, what is the loss function the brain is optimizing)? Finally, can we apply these insights in computer vision?

Update: I posted a related question on Psychology & Neuroscience StackExchange, which I think complements the question I posted here: check it out
","['machine-learning', 'computer-vision', 'cognitive-science', 'biology']","
I think you are slightly confusing 2 problems. 1 being classification of meta visual elements and the other being the visual system itself.
Our visual system, when it comes to processing information, has had billions of years of iteration(training), so that at birth(and before), we are already tuned for the processing of visual stimuli, as well as have the mechanisms to decipher objects in our spatial field of view. 
These two papers(L1, L2), have a great deal of information about the evolution of our visual system and its processing. The second speculates on the connection of said evolution and the construction of ""seeing systems"" very interesting.
For further inquiry on this in particular, check out David Marr. He was probably the most influential early computer vision mind. He still is mentioned in many top-down AGI and computer vision research projects to this day.
"
Architecture and Use of Different Algorithms for Health Goal Feedback,"
I wanted to get some opinions from the community for a certain problem that I will be approaching.
The problem is to provide feedback to a user based on a image of the upper male torso. The image would either reflect something positive like increasing muscle mass or decreasing muscle mass or both and gaining adipose tissue would be seen as negative as well as muscle atrophy.
Using the users input such as (sleep data, food, training routine) among some other data I would like to provide feedback such as ""no John, this exercise has not yielded desirable results"" or ""a combination of your recent dietary change has caused strength loss"" obviously this is a complex issue and has a lot of interconnected variables and potentials but you get the idea high-level at least and if you don't - Please ask.
So my idea so far would be to use a CNN that holds the picture of the torso, using a softmax function we could run this through a model to estimate bodyfat and  doing the same  with a model trained on muscle mass using those two models we could paint a pretty accurate picture of someones physique if they're going in the right direction or not; we could then go on to analyse what that user may have done/has not done to yield a result - Obviously there would be connected models here and many different combinations of algorithms applied such as CNN, RNN and others. Really curious to hear your response(s) thank you in advance.
","['machine-learning', 'architecture', 'model-based-methods']",
Is max-pooling really bad?,"
Hinton doesn't believe in the pooling operation (video). I also heard that many max-pooling layers have been replaced by convolutional layers in recent years, is that true?
","['deep-learning', 'convolutional-neural-networks', 'pooling', 'max-pooling']","
In addition to JCP's answer I would like to add some more detail. At best, max pooling is a less than optimal method to reduce feature matrix complexity and therefore over/under fitting and improve model generalization(for translation invariant classes).
However as JCP begins to hit on.. there are problems with this method. Hinton perhaps sums the issues in his talk here on what is wrong with CNNs. This also serves as motivation for his novel architecture capsule networks or just capsules. 
As he talks about, the main problem is not translational variance per se but rather pose variance. CNNs with max pooling are more than capable of handling simple transformations like flips or rotation without too much trouble. The problem comes with complicated transforms, as features learned about a chair facing forwards, will not be too helpful towards class representation if the real-world examples contain chairs upside down, to the side, etc. 
However there is much work being done here, mostly constrained to 2 areas. Those being, novel architectures/methods and inference of the 3d structure from images(via CNN tweaks). This problem was one of the bigger motivators for researchers throughout the decades, even David Marr with his primal sketches.
"
What is the motivation behind using a deterministic policy?,"
What is the motivation behind using a deterministic policy? Given that the environment is uncertain, it seems stochastic policy makes more sense. 
","['reinforcement-learning', 'deterministic-policy']",
How should I implement the backward pass through a flatten layer of a CNN?,"
I am making a NN library without any other external NN library, so I am implementing all layers, including the flatten layer, and algorithms (forward and backward pass) from scratch. I know the forward implementation of the flatten layer, but is the backward just reshaping it or not? If yes, can I just call a simple NumPy's reshape function to reshape it?
","['neural-networks', 'convolutional-neural-networks', 'backpropagation', 'implementation']","
The Flatten layer has no learnable parameters in itself (the operation it performs is fully defined by construction); still, it has to propagate the gradient to the previous layers.
In general, the Flatten operation is well-posed, as whatever is the input shape you know what the output shape is.
When you backpropagate, you are supposed to do an ""Unflatten"", which maps a flattened tensor into a tensor of a given shape, and you know what that specific shape is from the forward pass, so it is also a well-posed operation.
More formally
Say you have Img1 in input of your Flatten layer
$$
\begin{pmatrix}
f_{1,1}(x; w_{1,1}) & f_{1,2}(x; w_{1,2}) \\ 
f_{2,1}(x; w_{2,1}) & f_{2,2}(x; w_{2,2})
\end{pmatrix}
$$
So, in the output you have
$$
\begin{pmatrix}
f_{1,1}(x; w_{1,1}) & f_{1,2}(x; w_{1,2}) & f_{2,1}(x; w_{2,1}) & f_{2,2}(x; w_{2,2})
\end{pmatrix}
$$
When you compute the gradient you have
$$
\frac{df_{i,j}(x; w_{i,j})}{dw_{i,j}}
$$
and everything in the same position as in the forward pass, so the unflatten maps from the (1, 4) tensor to the (2, 2) tensor.
"
How large should the replay buffer be?,"
I'm learning DDPG algorithm by following the following link: Open AI Spinning Up document on DDPG, where it is written

In order for the algorithm to have stable behavior, the replay buffer should be large enough to contain a wide range of experiences, but it may not always be good to keep everything. 

What does this mean? Is it related to the tuning of the parameter of the batch size in the algorithm?
","['reinforcement-learning', 'deep-rl', 'hyper-parameters', 'ddpg', 'experience-replay']","
You need to read this 2020 paper by Deepmind:
""Revisiting Fundamentals of Experience Replay""
They explicitly test the size of the experience replay, the replay-ratio of each experience and other parameters.
Also, to add to the answer by @nbro
Assume you implement experience replay as a buffer where the newest memory is stored instead of the oldest. Then, if your buffer contains 100k entries, any memory will remain there for exactly 100k iterations.
Such a buffer is simply a way to ""see"" what was up to 100k iterations ago.
After the first 100k iterations you fill the buffer and begin ""moving"" it, much like a sliding window, by inserting new memories instead of the oldest.

The size of the buffer (relative to the total number of iterations you plan to ever train with) depends on ""how much you believe your network architecture is susceptible to catastrophic forgetting"".
A tiny buffer might force your network to only care about what it saw recently.
But an excessively large buffer might take a long time to ""become refreshed"" with good trajectories, when they finally start to be discovered. So the network would be like a university student whose book shelf is diluted with first-grade school books.
The student might have already decided that he/she wishes to become a programmer, so re-reading those primary school books has little benefit (time could have been spent more productively on programming literature) + it takes a long time to replace those with relevant university books.
"
How can certain information about the goal be given to the RL learning algorithm?,"
Policy learning refers to mapping an agent state onto an action to maximize reward. A linear policy, such as the one used in the Augmented Random Search paper, refers to learning a linear mapping between state and reward.
When the entire state changes at each time-step, for example in the Continuous Mountain Car OpenAI Gym, the position and speed of the car changes at each time-step.
However, assume we also wanted to communicate the constant position of one or more goals. By ""constant"", I mean does not change within a training episode, but may change between episodes. For example, if there was a goal on the left and right of the Mountain Car.
Are there examples of how this constant/static information be communicated from the environment other than appending the location of the two goals to the state vector? Can static/constant state be differentiated from state which changes with each action?
","['reinforcement-learning', 'ai-design']","
I see three main ways to do this, which one makes more sense will depend on your application.
One is to append that information to the state/observations like you mentioned. While this information is static for a particular episode, it will be different across episodes and the policy should learn to condition the actions it chooses based on this information.
Another would be to leave goal information out entirely and force the agent to learn a policy that works when the goal is unknown. This will likely be more difficult to learn and you may end up with a policy that moves to the average of all goals or explores and tries them all.
A third option and probably the most natural is to have some context cue that the agent can observe (e.g. often in lab experiments with rats, a cue is placed on a wall that lets the rat know which way to go to get a reward). This is similar to the first method, except that the cue has to be observed rather than given directly. For the Mountain Car example, this could be an extra signal that the agent only sees in a particular location, such as the bottom of the valley or when it moves close to a particular side.
"
IQN bellman target: using Z vs using Q,"
IQN paper (https://arxiv.org/abs/1806.06923) uses distributional bellman target:
$$ \delta^{\tau,\tau'}_t = r_t + \gamma Z_{\tau'}(x_{t+1}, \pi_{\beta}(x_{t+1})) - Z_{\tau}(x_t, a_t) $$
And optimizes: 
$$ L = \frac{1}{N'} \sum^{N}_i \sum^{N'}_j \rho^\kappa_{\tau_i} \delta^{\tau_i,\tau_j}_t $$
But similar quantiles can be got just from Q values, when doing so:
$$ \delta^\tau_t = r_t + \gamma \frac{1}{N'} \sum_{j}^{N'} Z_{\tau_j}(x_{t+1}, \pi_{\beta}(x_{t+1})) - Z_\tau(x_t, a_t) \\ = r_t + \gamma Q (x_{t+1}, \pi_\beta(x_{t+1})) - Z_\tau(x_t, a_t) $$
optimizing:
$$ L = \sum^N_i \rho^{\kappa}_{\tau_i} \delta^{\tau_i}_t $$
Both lead to similar performance on CartPole env. The loss function of the 2nd one is more simpler and intuitive (atleast to me). So i was thinking if there are any obvious reason why authors didin't use it?
","['deep-learning', 'reinforcement-learning', 'papers', 'atari-games']","
Replacement you suggest is replacement of random variable by its expectation in forward part of TD. It would make IQN into modification of C51 with randomly sampled function approximator instead of discrete distribution. Both distribution produced and especially exploration behavior with your replacement would be very different. The authors of paper explicitly said that ""more randomness"" in their opinion benefit training, so reducing randomness would go aginst spirit of the paiper.  That they produce similar results on single toy test mean very little. IQN could be better then C51 or it could be worse then C51 but single toy example is not enough to say they are close. Nevertheless I agree that IQN looks overly complex and may require more training time, C51 approach could be more practical. 
"
How to build a neural network that can learn to predict output images?,"
I am working with a dataset where each input sample is a matrix, and the output corresponding to each input is also a matrix (of shape (400, 10)). The input samples do not have translation invariance. Each output image has shape (16, 16) . The output matrices have translation invariance. 
I want to build a neural network which can learn how to predict the output images from the aforementioned data. It seems to me that one needs to think of a neural network here which does regression on the output images to learn. Presently, I am using 1000 data samples for learning in the neural network (input samples and corresponding output images). What is the best way to build a neural network for this task?
Presently, I am using multi-layer perceptron (MLP) with mean square error (MSE) loss for this task. I flatten the input matrices before I feed them into the MLP, and use a standard MLP with multiple hidden layers (4-5) with many hidden units for this task. 
While a visual inspection shows that the true and predicted output images are in relatively good agreement for training data, I find a mismatch between true and predicted (reconstructed) output images for validation data. The bottom plot shows pictures of true and predicted (reconstructed) output images for training and validation data for chosen samples. 

Presently, I am using training and validation loss curves (with respect to iteration) to measure performance. I want to have a robust metric for comparison which can tell me whether the prediction is a random image or not.
How can I get the model to generalize to the validation set better?
The Python code that I am using for this MLP and the required data can be found here and here respectively.
","['neural-networks', 'machine-learning', 'deep-learning']",
Is any classifier not subject (or less susceptible) to fooling?,"
Is any classifier not subject to fooling as in here?
My question is related to this, but not an exact duplicate.
What I wanted to ask is that any classifiers inherently do not subject (or less prone) to attack. I have a feeling that non-linear classifiers should be less susceptible to attack.
","['machine-learning', 'models', 'adversarial-ml', 'model-request']","
I would say this is not necessarily a duplicate but quite similar to some other questions. However, I will answer the question posed here. 
At a theoretical level, what you are asking is there any algorithm that cannot be tricked into predicting the wrong class?
The answer is: No
It is analogous to asking whether there is a perfect architecture for an arbitrary classification problem, and that answer is quite obviously not. It would also not likely to be terribly difficult to show this is provably the case, at least for a class of algorithms(i.e connectionist models).
"
Does everyone still use discount rates?,"
In Section 10.4 of Sutton and Barto's RL book, they argue that the discount rate $\gamma$ has no effect in continuing settings. They show (at least for one objective function) that the average of the discounted return is proportional to the undiscounted average reward $r(\pi)$ under the given policy.$^*$ They then advocate using average rewards rather than the usual returns of the discounted setting. 
I've never encountered someone using average rewards (and no discounting) in the wild, though. Am I just ignorant of some use case, or is pretty much everyone sticking to discounting anyways?
$$r(\pi)=\sum_s \mu_\pi (s) \sum_a \pi(a|s) \sum_{s',r}p(s',r|s,a)r$$ 
$\mu_\pi$ is the stationary state distribution while following policy $\pi$.
$^*$Their proof did use the fact that the MDP was ergodic. I'm not sure how often that assumption holds in practice.
","['reinforcement-learning', 'markov-decision-process', 'sutton-barto', 'discount-factor']",
Does backpropagation update weights one layer at a time?,"
I am new to Deep Learning.
Suppose that we have a neural network with one input layer, one output layer, and one hidden layer. Let's refer to the weights from input to hidden as $W$ and the weights from hidden to output as $V$. Suppose that we have initialized $W$ and $V$, and ran them through the neural network via the forward algorithm/pass. Suppose that we have updated $V$ via backpropagation.
When estimating the ideal weights for $W$, do we keep the weights $V$ constant when updating $W$ via gradient descent given we already calculated $V$, or do we allow $V$ to update along with $W$?
So, in the code, which I am trying to do from scratch, do we include $V$ in the for loop that will be used for gradient descent to find $W$? In other words, do we simply use the same $V$ for every iteration of gradient descent?
","['neural-networks', 'deep-learning', 'backpropagation', 'gradient-descent']",
How to detect LEGO bricks by using a deep learning approach?,"
In my thesis I dealt with the question how a computer can recognize LEGO bricks. With multiple object detection, I chose a deep learning approach. I also looked at an existing training set of LEGO brick images and tried to optimize it.
My approach
By using Tensorflow's Object Detection API on a dataset of specifically generated images (Created with Blender) I was able to detect 73.3% of multiple LEGO Bricks in one Foto. 
One of the main problems I noticed was, that I tried to distinguish three different 2x4 bricks. However, colors are difficult to distinguish, especially in different lighting conditions. A better approach would have been to distinguish a 2x4 from a 2x2 and a 2x6 LEGO brick.
Furthermore, I have noticed that the training set should best consist of ""normal"" and synthetically generated images. The synthetic images give variations in the lighting conditions, the backgrounds, etc., which the photographed images do not give. However, when using the trained Neural Network, photos and not synthetic images are examined. Therefore, photos should also be included in the training data set. 
One last point that would probably lead to even better results is that you train the Neural Network with pictures that show more than one LEGO brick. Because this is exactly what is required by the Neural Network when it is in use. 

Are there other ways I could improve upon this?

(Can you see any further potential for improvement for the Neural Network? How would you approach the issue? Do any of my approaches seem poor? How do you solve the problem?)
","['deep-learning', 'image-recognition', 'tensorflow', 'datasets', 'object-recognition']","
So I am assuming that you are trying to detect a lego brick from the image. One idea is that you can use transfer learning. Leveraging a pre-trained machine learning model is called transfer learning. The underlying idea behind transfer learning is that one takes a well-trained model from one dataset or domain, and applies it to a new one.  Franois Chollet has written a very comprehensive guide to transfer learning (https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)
I admit I took some of this information from Christopher Bonnett's article named Classifying e-commerce products based on images and text.
I also suggest using the Lego brick dataset from Kaggle on this link: https://www.kaggle.com/joosthazelzet/lego-brick-images
It has over 12,700 lego brick images.
If processing power is a problem, you can use Amazon Web Services for cloud computing. It is inexpensive for small scale operations like this.
Of course for the object detection, you can always increase the number of convolution layers. However, if you have too many layers, you should also include residual blocks/residual network. This would allow neural networks even with over a thousand layers to operate effectively. This video should help you understand how residual networks work (https://www.youtube.com/watch?v=ahkBkIGdnWQ)
Finally, make sure not to overfit during your training and if you do follow the residual network idea, you should also include upsampling in your convolution neural network( More in here: https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0)
I hope this helped and good luck on your endeavor. 
"
How do the relative number of cells between neighboring stacked LSTM layers affect the network's behavior?,"
It seems that stacking LSTM layers can be beneficial for some problem settings in order to learn higher levels of abstraction of temporal relationships in the data. There is already some discussion on selecting the number of hidden layers and number of cells per layer.
My question: Is there any guidance for the relative number of cells from one LSTM layer to a subsequent LSTM layer in the stack? I am specifically interested in problems involving timeseries forecasting (given a stretch of temporal data, predict the trend of that data over some time window into the future), but I'd also be curious to know for other problem settings. 
For example, say I am stacking 3 LSTM layers on top of each other: LSTM1, LSTM2, LSTM3, where LSTM1 is closer to the input and LSTM3 is closer to the output. Are any of the following relationships expected to improve performance?

num_cells(LSTM1) > num_cells(LSTM2) > num_cells(LSTM3) [Sizes decrease input to output]
num_cells(LSTM1) < num_cells(LSTM2) < num_cells(LSTM3) [Sizes increase input to output]
num_cells(LSTM1) < num_cells(LSTM2) > num_cells(LSTM3) [Middle layer is largest]

Obviously there are other combinations, but those seem to me salient patterns. I know the answer is probably ""it depends on your problem, there is no general guidance"", but I'm looking for some indication of what kind of behavior I could expect from these different configurations.
","['neural-networks', 'machine-learning', 'recurrent-neural-networks', 'long-short-term-memory']",
"What is a simplified way to explain why the AI researchers Bengio, Hinton, and Lecun, won the 2018 Turing Award?","
The Turing award is sometimes called Computer Sceince's Nobel Prize. This year's award goes to Bengio, Hinton, and LeCun for their work on artificial neural networks.
The actual work contributed by these authors is, of course, quite technical. It centers around the development of deep neural networks, convolutional neural networks, and effective training techniques. The lay press will tend to simplify these results to the point that they lose meaning.
I would like to have a concise, and yet still precise, explanation of their contributions to share with a lay audience. So, what is a simplified way to explain the contributions of these researchers?
I have my own ideas and will add them if no other satisfactory answer appears. For a ""lay"" audience, I want to assume someone who had taken a college level course in something scientific but not necessarily computer science. Explanations that are suitable for those with even less background are better still though, as long as they don't lose too much precision.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'turing-award']","
The related ACM article describes a few specific technical contributions, which led the ACM to award them.

Geoffrey Hinton
Backpropagation: In a 1986 paper, ""Learning Internal Representations by Error Propagation"", co-authored with David Rumelhart and Ronald Williams, Hinton demonstrated that the backpropagation algorithm allowed neural nets to discover their own internal representations of data, making it possible to use neural nets to solve problems that had previously been thought to be beyond their reach. The backpropagation algorithm is standard in most neural networks today.
Boltzmann Machines: In 1983, with Terrence Sejnowski, Hinton invented Boltzmann Machines, one of the first neural networks capable of learning internal representations in neurons that were not part of the input or output.
Improvements to convolutional neural networks: In 2012, with his students, Alex Krizhevsky and Ilya Sutskever, Hinton improved convolutional neural networks using rectified linear neurons and dropout regularization. In the prominent ImageNet competition, Hinton and his students almost halved the error rate for object recognition and reshaped the computer vision field.
Yoshua Bengio
Probabilistic models of sequences: In the 1990s, Bengio combined neural networks with probabilistic models of sequences, such as hidden Markov models. These ideas were incorporated into a system used by AT&T/NCR for reading handwritten checks, were considered a pinnacle of neural network research in the 1990s, and modern deep learning speech recognition systems are extending these concepts.
High-dimensional word embeddings and attention: In 2000, Bengio authored the landmark paper, ""A Neural Probabilistic Language Model"", that introduced high-dimension word embeddings as a representation of word meaning. Bengio's insights had a huge and lasting impact on natural language processing tasks including language translation, question answering, and visual question answering. His group also introduced a form of attention mechanism which led to breakthroughs in machine translation and form a key component of sequential processing with deep learning.
Generative adversarial networks: Since 2010, Bengio's papers on generative deep learning, in particular the Generative Adversarial Networks (GANs) developed with Ian Goodfellow, have spawned a revolution in computer vision and computer graphics. In one fascinating application of this work, computers can actually create original images, reminiscent of the creativity that is considered a hallmark of human intelligence.
Yann LeCun
Convolutional neural networks: In the 1980s, LeCun developed convolutional neural networks, a foundational principle in the field, which, among other advantages, have been essential in making deep learning more efficient. In the late 1980s, while working at the University of Toronto and Bell Labs, LeCun was the first to train a convolutional neural network system on images of handwritten digits. Today, convolutional neural networks are an industry standard in computer vision, as well as in speech recognition, speech synthesis, image synthesis, and natural language processing. They are used in a wide variety of applications, including autonomous driving, medical image analysis, voice-activated assistants, and information filtering.
Improving backpropagation algorithms: LeCun proposed an early version of the backpropagation algorithm (backprop), and gave a clean derivation of it based on variational principles. His work to speed up backpropagation algorithms included describing two simple methods to accelerate learning time.
Broadening the vision of neural networks: LeCun is also credited with developing a broader vision for neural networks as a computational model for a wide range of tasks, introducing in early work a number of concepts now fundamental in AI. For example, in the context of recognizing images, he studied how hierarchical feature representation can be learned in neural networks - a concept that is now routinely used in many recognition tasks. Together with Lon Bottou, he proposed the idea, used in every modern deep learning software, that learning systems can be built as complex networks of modules where backpropagation is performed through automatic differentiation. They also proposed deep learning architectures that can manipulate structured data, such as graphs.

"
What methods are there to generate artificial training examples based on existing training examples?,"
I have a small dataset (117 training examples) and many features (4005). Each of the training examples is binary labeled (healthy / diseased). Each feature represents the connectivity between two different brain regions.
The goal is to assign subjects to one of the two groups based on their brain activity.
What methods are there for generating new artificial training examples based on the existing training examples?
An example I could think of would be SMOTE. However, this technique is usually only used to balance unbalanced datasets. This would not be necessary for my set, since it has about the same number of training examples for both label classes.
","['neural-networks', 'machine-learning', 'deep-learning', 'training', 'datasets']",
How can I generate a document from a single word using GPT or BERT?,"
I have a dataset of 100000 documents each labelled with a topic to it. I want to create a model such that, given a topic, the model can generate a document from it. 
I came across language models GPT, GPT-2 and BERT. I learned that they can be used for generation purposes. But I did not find anywhere whether they can generate sentences given only a word.
I am inclined to use GPT for my task, but I am not sure how to proceed with it. I wanted to know whether it is possible or not? It would be helpful if anyone can help me give a start in the right direction.
","['natural-language-processing', 'bert', 'language-model', 'gpt']",
Classification of classes within meta-classes,"
TLTR:
I'm developing a CNN for a classification task. The data contains multiple classes some of which are very similar to each other and I know these meta-classes. In such a situation is it a good approach to use 2 Levels of CNNs:
1. Level detect the meta-classes.
2. Level detect the classes within the classified meta class (of Level 1).

Example:
Suppose I try to classify the following 9 classes:
Apple Tree, Plum Tree, Cherry Tree, Sports car, SUV car, Coupe car, Dog, Cat, Wolf
Now I could of course use one network on these classes and get a classification output for all of them. But the output (softmax) percentage e.g. for an apple tree would be for probably high for any tree class.
Thus is it a good approach to train and use 2 level of CNNs, like this:

Level classify tree, car, animal
--> Trained with all images
Level classify what kind of tree, car, animal --> trained only with the subsample of trees, cars, animals

So images are checked by CNN Level 1 and then depending on its classification with appropriate CNN Level 2.
So the questions are:

Is this a good approach ?


Does it help in terms of prediction quality/accuracy of the subclasses?
Is it easier for an CNN to detect the specific features of a subclass if the input is limited (like in level 2) ?


Or use another approach ?
Thanks
Swad
","['convolutional-neural-networks', 'classification']",
Combining deep reinforcement learning with alpha-beta pruning,"
I will explain my question in relation to chess, but it should be relevant for other games as well:
In short terms: Is it possible to combine the techniques used by AlphaZero with those used by, say, Stockfish? And if so, has it been attempted?
I have only a brief knowledge about how AlphaZero works, but from what I've understood, it basically takes the board state as input to a neural net, possibly combined with monte carlo methods, and outputs a board evaluation or prefered move. To me, this really resembles the heuristic function used by traditional chess engines like stockfish.
So, from this I will conclude (correct me if I'm wrong) that AlphaZero evaluates the current position, but uses a very powerful heuristic. Stockfish on the other hand searches through lots of positions from the current one first, and then uses a less powerful heuristic when a certain depth is reached.
Is it therefore possible to combine these approaches by first using alpha-beta pruning, and then using AlphaZero as some kind of heuristic when the max depth is reached? To me it seems like this would be better than just evaluating the current position like (I think) AlphaZero does. Will it take too much time to evaluate? Or is it something I have misunderstood? If it's possible, has anyone attempted it? 
","['deep-learning', 'chess', 'alphazero', 'alpha-beta-pruning']","
Yes it's possible to to combine AlphaZero with Minimax methods (including alpha-beta pruning). AlphaZero itself is combination of  Monte Carlo Tree Search (MCTS) and Deep Network, where MCTS is used to get data to train network and network used for tree leafs evaluation (instead of rollout as in classical MCTS). It's possible to combine selection-expansion part of AlphaZero MCTS with Minimax the same way as it was done for classical MCTS - ""Monte-Carlo Tree Search and Minimax Hybrids"", pdf.
"
Convolutional Sequence to Sequence Learning: Training vs Generation,"
I am struggling to understand the use of the Convolutional Sequence to Sequence (Conv-Seq2Seq) model.  The image below is take directly from the paper and is the nearly canonical diagram of the parallel training procedure.  After puzzling over it for quite some time, it has come to seem straight forward to me:

An input sentence of N tokens can be encoded in one step because the input sentence exists prior to the start of training, and therefore the token-wise convolution can be trivially parallelized. (Compare to RNN encoders, which require N steps)
During training, an output sentence can similarly be parallelized in the decoder because during training, the entire output sentence is known.
Therefore, during training, the attention function can be fully parallelized in the two dimensional array of dot products shown below
Finally, during training, the attention is used to weight the input embeddings and encodings, combined with the output training encodings (as such) and the final output assembled. 

This is clearly not the case after the network is trained and evaluation input sequences are translated without reference outputs.  I understand from various resources (including but not limited to Gehring's conference presentation) that post-training, output sequences are generated token by token in a fashion vaguely similar to earlier architectures, but I cannot find a clear description of that process.
(I speculate that this is because the parallel training routine was so revolutionary at the time, that the focus of the publications was rightly on the training routines.)
Can someone please help me understand the post-training generation algorithm, if possible in terms of the training diagram?
My current non-confident understanding is that the sentence below would be handled something like the following:

Prime the decoder with a default token string of <p> <p> <s>, this results (due to convolution) in a single decoder encoding (as such) input into the attention function, and would hopefully generate the single token <'Sie'> as output
Restart the decoder with the token string <p> <p> <s> <'Sie'> which would generate two inputs into the attention function and hopefully output <'Sie'> <'stimmen'>
Proceed with lengthening input sentences until the final token generated output ends with a </s> token signifying the end of the sentence. 

If that is a correct understanding, can someone confirm it?  If close, can someone correct me?
 
Convolutional Sequence to Sequence Learning, Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin, 2017
","['convolutional-neural-networks', 'sequence-modeling']","
I know it's too late to answer your query after 1.5 years
The inference algorithm is explained at https://charon.me/posts/pytorch/pytorch_seq2seq_5/#inference
Implementation of the algorithm is available at: https://github.com/pytorch/fairseq.
One could get a complete understanding from their source code.
"
Can Q-learning be used in a POMDP?,"
Can Q-learning (and SARSA) be directly used in a Partially Observable Markov Decision Process (POMDP)? If not, why not? My intuition is that the policies learned will be terrible because of partial observability. Are there ways to transform these algorithms so that they can be easily used in a POMDP?
","['reinforcement-learning', 'q-learning', 'pomdp', 'markov-decision-process', 'sarsa']",
Can this tic tac toe program be considered AI?,"
I coded a tic tac toe program, but I don't know if I can call it artificial intelligence.
Here's what I did.
There is a random player, which always makes random valid moves.
And then there is the AI player, which will receive input before every move, that input is the state of the board, and all the posibilities.
The AI, will try any move that it hasn't tried before. But if it knows every possibility, it will select the one that has the higher value.
This value is assigned by the outcome of the match, if the match was won ,+1, 0 for draw, -1 for lose.
Every move, will be stored in a database, or updated if it's known.
Eventually it will know every possible move.
I also added a threshold to compare the best moves, so it really select the best move. Example, two moves with a value of 100, the AI will keep trying them both, randomly until one has surpased the other by the threshold, say 50.
It takes about 20.000 games to make the AI perfect, it never loses a game, just draws and wins.
I'm new to AI, and I'm wondering, could this be really considered Artificial Intelligence?
And how is this different from a Neural Network approach? (I'm been reading about it, but I still don't quite get it.)
","['neural-networks', 'game-ai']",
Is there any system that generates website designs?,"
I was thinking about what I can do for my thesis in upcoming academic semester, and I came across an idea. The idea is like: ""If there is any kind of system that generates website designs itself."" If no, then I can go for it, and I will be lucky if anyone has the same idea. We can collaborate. In case, if there's any project or system (Open source or not) that can do this or has been initiated in this context, I want to contribute solely. If anyone has any clue or knowledge over this kind of system, please do inform me. As I haven't done anything like this before, I want to learn. Any kind of suggestion or assistance on this idea will be so helpful for me.
","['research', 'open-source', 'artificial-creativity']","
The closest research that I am aware of is in artificial intelligence designed user interfaces(1,2).
The scope of which vary tremendously. Some teams are trying to generate UIs based on some user-defined parameters, others are trying to generate based off of images(as in the second link). I think part of the reason research is focused here is two-fold: One, we aren't very good at generating UIs automatically yet, and 2, it is an exponentially harder problem to develop a backend that has integrated calls and features required for a fully-featured website or application.
With that being said, one could adapt this research into things like one-page sites or other simple implementations that might bear fruit.
"
DQN Q-values are static,"
I am working on a DDQN with 5 LSTM layers and 3 actions as output and state space of 21 features. I am dividing the dataset into episodes of 720 timesteps, for each episode the agent acts greedily for the first 480 steps without training, collecting a replay memory, and then update the parameters each step for the subsequent 240 steps using a window size (of 96 steps) randomly sampled from the replay memory (that always saves the last 480). 
My problem is that so far the agent learned the optimal policy just once, and it looks like this
(on the test set, training is off) where, as you can see, the agent dynamically changes its evaluation of the state and acts greedily accordingly. All works fine and the performances are optimal, however, I have to slightly change the normalization of the database and rerun the training to get new parameters fitted to the new database. 
Trying to get to the same result has been proven impossible so far, (even keeping all settings the same!) because most of the time the agent learns to keep its q-values static such as . 
Note: this is an extract of the end of an episode and the beginnig of a new one, the noisy behaviour at the extreme is due to the model being trained for each step, in the middle the training is off and the agent acts greedily (as I explained above). The problem is that the learned parameters give rise to static q values that do not change while the state does and inevitably make the agent stuck in suboptimal strategy, as they never changes actions, even on longer sequences on the test set. The middle part of the second picture, where trainig is off, should look like the first picture, however, I am unable to get back to that optimal behaviour, even keeping all the parameters as they were. 
Any idea on what can be the cause of this anomalous behaviour? 
","['reinforcement-learning', 'q-learning', 'long-short-term-memory', 'dqn', 'convergence']",
Is there a reason evolutionary algorithms are language-bound in research material?,"
I've been working on genetic algorithms & evolutionary strategies for a while now in a research context. Across the vast majority of the articles and content I've read, every single one of them will either use Python, Matlab, or Java/C++ to build & benchmark their algorithms.
Is there an objective reason for these languages to be the single ones used in a research environment? Mainly in contrast with other languages like C#, or Javascript, that are almost never used (despite being some of the most used programming languages in other areas), whereas it would definitely be possible to code in practice all current algorithms in them.
","['genetic-algorithms', 'evolutionary-algorithms', 'programming-languages']","
I would say there are quite a few different reasons for this, with the proportion of each dependent on a given researcher.
For example, I use python for the vast majority of what I do. And for me, it is due to a few different factors:

I was already familiar with python, and it is a simple, high-level abstraction. This is probably the reason a lot of people set it and forget it, particularly with python. It allows them to focus on the ideas and implementation of said ideas, without worrying about all the junk that comes with trying to write a program in a faster language like C++
The vast majority of ML/DS packages are only or primarily supported via python. I think this is probably the main reason for most in the field, as even if one can implement the architecture in a faster language, the time to do so would likely even out when taking into account the time required to prototype a given model. Tensorflow and others are supported for other languages but do not see the same level of dev support.
The ability to deploy models to multiple platforms without headache. When working in an environment where the work is also applied, the ability to deploy a given model without too much debugging cannot be understated. 

These are just a few of the main ones, and as I said the reasons for a particular language over another is primarily a preferential one, and can even vary by requirements(i.e speed)
"
How to get good results with GAN and some thousands of images?,"
I'm trying to generate images at minimum of size 128 x 128 with a Generative Adversarial Network. I already tried a SAGAN pytorch implementation, but I'm not very happy with results. The images look cool but and I see some correct shape but without explanation you wouldn't know what the images are about. I have a dataset of 4000 images. Lightness, colors and shapes vary a lot, but they are similar in style and on what they portray. 

With a Google Cloud V100 GPU the GAN would run a week to two with default parameters. Does this sound realistic time for this kind of dataset? It's definitely not feasible for me.
Is 4000 images enough to train a GAN from scratch?
Is there any implementation with pytorch/keras that would be good to get nice results with?

","['generative-model', 'generative-adversarial-networks']",
What are some possible projects that can be finished in a 4-week time-frame?,"
I am taking AI this semester and we have a semester project that will last 4 weeks. We can choose just about anything. 
So, what are some possible semester projects that can be finished in a 4-week time-frame?
Some background information: I am a graduate student in CS, but this is my first AI course. My research area is in the space of data mining and analytics. I am open to doing anything that seems interesting and creative. 
","['academia', 'homework']","
Welcome to AI.SE @Kate_Catelena!
I teach AI courses at the undergraduate level, and so have seen a lot of semester projects over the years. Here are some templates that often lead to exciting outcomes:

Pick a new board or card game, and write a program to play it. Your course has probably covered Adversarial Search, and may also have covered Monte Carlo Tree Search, or self-play reinforcement learning approaches. These projects are often fun to mark and creative because they are easy enough to be well done, and yet there are always new, exciting, domains to apply these algorithms to. Some examples of past projects that I thought were neat were an AI to play the boardgame Tac (mostly A* Search), and an AI to play the card game Love Letter (mostly Counter-factual minimax regret, the algorithm used to solve poker).
Pick a question that you would like to know the answer to, that could be addressed with machine learning. Then implement your own ML algorithm (decision tree learners are fairly easy), gather your own data, and show a result. Examples of interesting projects I've seen in the past are using ML to find out which of a number of factors most strongly influenced a students' subjective quality of sleep; and which items are most commonly purchased along with camping supplies (using association rule mining).
Anything involving reinforcement learning. RL projects are always neat to see if accompanied by a visualization showing the learner's behavior at different stages. A strong past project involved a student simply replicating Sutton & Barto's Acrobot experiments with their own implementation of the SARSA-Lambda algorithm.  Other things that might be neat include making a trainable ""pet"" that the user can influence, or solving games using self-play.
Theoretical results might seem intimidating but are often more accessible than one might think, especially if your discrete math skills are strong. I have had many student projects where the student went away to look at theory papers in ML or Multiagent Systems, found a suggestion in the future work sections that wasn't a big result, but that was fairly easy to prove, and proved it. Sometimes these are even publishable. 
Replications. Go find an interesting AI paper (use scholar.google.com), and then see if you can do exactly what the authors suggest and if you get the same result or not. Then, if you have time, see if you can improve on the results. These are often most interesting when you find a paper written in a different field that uses AI. Often the authors of such papers know less about AI than you do, and so it can be fairly easy to improve on their results. I have had several students do projects like this to great effect.

Those categories are a bit vague, but remember: AI touches almost anything. Pick your favorite hobby, and see whether you can relate it to AI using one of the approaches above. Nothing makes a project stand out like one that applies AI to solve some real issues in an exciting domain. Good luck!
"
Are decision tree learning algorithms deterministic?,"
Are decision tree learning algorithms deterministic? Given a fixed dataset, do they always produce a tree with the same structure? 
What about the random forest?
","['decision-trees', 'random-forests', 'id3-algorithm', 'c4.5-algorithm']",
Cold start collaborative filtering with NLP,"
Im looking to match two pieces of text - e.g. IMDb movie descriptions and each persons description of the type of movies they like. I have an existing set of ~5000 matches between the two. I particularly want to overcome the cold-start problem: what movies to recommend to a new user? When a new movie comes out, to which users should it be recommended? I see two options:

Run each description of a person through an LSTM; do the same for each movie description; concatenate the results for some subset of possible combinations of people and movies, and attach to a dense net to then predict whether its a match or not
Attempt to augment collaborative filtering with the output from running the movie description and person description through a text learner.

Are these tractable approaches?
","['natural-language-processing', 'recommender-system']","
From what I understood you will not have any cold start problem because you basically process the user preferences description against movies descriptions to get recommendations. So you don't use other users feedback at any time of the process which is not collaborative filtering.

Here is instead the approach I would suggest in your case to get movies recommendations for each user:


Compute the similarity between the user description and each movie description. This can be done using the Universal Sentence Encoder. It's an 2018 paper by Google which represents any sentence as a vector of 215 values (i.e embeddings). 
The semantic similarity between 2 sentences is computed using the dot product of their embeddings. Fortunately, the implementation was integrated to Tensorflow Hub and easily be used (see my answer here for details).
Choose the highest similarity values and recommend the corresponding movies to the user. Notice that you can still use this approach along with a collaborative filtering one.

"
Multi-field text input for LSTM,"
I'm using LSTM to categorize medium-sized pieces of text. Each item to be categorized has several free-form text fields, in addition to several categorical fields. What is the best approach to using all this information for categorization?  I see two options:

Concatenate the text from all fields, preceding each field content with a special token. Run concatenated text through LSTM.
Train one model per field. Concatenate output from each model in a hidden layer and pass into subsequent layers.

What are the benefits of each of the approaches? Is there an alternative I'm missing?
","['natural-language-processing', 'long-short-term-memory', 'hidden-layers', 'architecture']",
Why is description logic decidable but first order logic is not decidable?,"
Description logic is a fragment of first order logic, but description logic is decidable and first order logic not decidable. Why is that? what is the role of variables in first order logic to make it undecidable?
",['logic'],"
This has to do with the fact that you can define arithmetic inside the axiomatic system or not. In description logic you cannot speak about arithmetic sentences and in first order logic you do.
if you look at the proof of incompleteness you will understand this in depth. This demonstration depends on an arithmetic coding of statements, and this representation is fundamental to obtain the conclusion.
"
What causes a model to require a low learning rate?,"
I've pondered this for a while without developing an intuition for the math behind the cause of this.  
So what causes a model to need a low learning rate?
","['machine-learning', 'models', 'hyper-parameters', 'learning-rate']","
Gradient Descent is a method to find the optimum parameter of the hypothesis or minimize the cost function.

where alpha is learning rate 
If the learning rate is high then it can overshoot the minimum and can fail to minimize the cost function.

hence result in a higher loss.

Since Gradient descent can only find local minimum so, the lower learning rate may result in bad performance. To do so, it is better to start with the random value of the hyperparameter can increase model training time but there are advanced methods such as adaptive gradient descent can manage the training time.
There are lots of optimizer for the same task but no optimizer is perfect. It depends on some factors

size of training data: as the size of the training data increases training time for model increases. If you want to go with less training model time you can choose a higher learning rate but may result in bad performance.
Optimizer(gradient descent) will be slow down whenever the gradient is small then it is better to go with a higher learning rate.

PS. It is always better to go with different rounds of gradient descent
"
How do I normalise/un-normalise data when loading a model?,"
I am following this TensorFlow JS tutorial where you load car data. The data looks like this:
[{x:100, y:20}, {x:80, y:33}]

X is the horsepower of a car, Y is the expected miles per gallon usage. After creating the model I save it locally using:
async function saveModel(){
    await model.save('downloads://cars-model');
}

Next, I load the model in a separate project, to make predictions without needing the original data. 
NEW PROJECT
async function app(){
    let model = await tf.loadLayersModel('./cars-model.json');
    console.log(""car model is loaded!"");
}

I expect to be able to run predict here, on a single number (say, 120)
model.predict(tf.tensor2d([120], [1, 1]))

QUESTION
I think the number 120 needs to be normalised to a number between 0-1, just like the training data was. But how do I know the inputMin, inputMax, labelMin, labelMax values from the loaded model?
To un-normalise the prediction (in this case 0.6) I also need those original values. 
How do I normalise/un-normalise data when loading a model?
original prediction code uses label and input values from the original data
function testModel(model, inputData, normalizationData) {
    const { inputMax, inputMin, labelMin, labelMax } = normalizationData;

    // Generate predictions for a uniform range of numbers between 0 and 1;
    // We un-normalize the data by doing the inverse of the min-max scaling 
    // that we did earlier.
    const [xs, preds] = tf.tidy(() => {

        const xs = tf.linspace(0, 1, 100);
        const preds = model.predict(xs.reshape([100, 1]));

        const unNormXs = xs
            .mul(inputMax.sub(inputMin))
            .add(inputMin);

        const unNormPreds = preds
            .mul(labelMax.sub(labelMin))
            .add(labelMin);

        // Un-normalize the data
        return [unNormXs.dataSync(), unNormPreds.dataSync()];
    });


    const predictedPoints = Array.from(xs).map((val, i) => {
        return { x: val, y: preds[i] }
    });

}

","['tensorflow', 'models', 'javascript']",
"Similarities and differences between UCT algorithms in (i), (ii), (iii) and (iv)?","
I am trying to understand the similarities and differences between: (i) the UCT algorithm in Kocsis and Szepesvri (2006); (ii) the UCT algorithm in Section 3.3 of Browne et al (2012); (iii) the MCTS algorithm in Silver et al. (2016); (iv) the MCTS algorithm in Silver et al. (2017).
I would be really grateful for some help identifying the similarities and differences in these papers, I am doing some research and really struggling right now.
(i) http://ggp.stanford.edu/readings/uct.pdf 
(ii) http://mcts.ai/pubs/mcts-survey-master.pdf (Section 3.3)
(iii) https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf
(iv) https://deepmind.com/documents/119/agz_unformatted_nature.pdf
","['algorithm', 'monte-carlo-tree-search', 'alphago', 'alphazero', 'monte-carlo-methods']","
The algorithm in the 2012 survey article (your second link) is the most common / standard implementation. Whenever someone mentions using MCTS or UCT, without explicitly stating any other info, it's safe to assume that that pseudocode is what they're using.
The paper by Kocsis and Szepesvri from 2006 (your first link) is (one of) the original publication(s) on UCT. It is very similar to the ""standard"" implementation as described in the survey paper. If I recall correctly, the only important difference is that the algorithm as described in 2006 keeps track of at which point in time during an episode a reward is observed, and accounts for that timing in the backpropagation phase (i.e. if a reward is observed at time $t$ in an episode, credit for that reward is not assigned to states/action after time $t$). It also used a discounting factor $\gamma$ to discount the importance of reward depending on temporal distance, which is uncommon otherwise in MCTS literature.
Both of those differences are due to the original, 2006 paper having more of a ""Markov decision process"" or ""Reinforcement Learning"" flavour, whereas otherwise MCTS has especially become popular in AI for (board) games where we often by default assume that there is only a single nonzero reward (win or lose) at the end of every episode anyway, which makes those differences less meaningful.

Both of the AlphaGo papers (AlphaGo and AlphaGo Zero) use a ""foundation"" of MCTS that is mostly similar to the one from the 2012 survey article. The core components are all the same. Both of those systems add on a lot of (very important, and some quite complex) bells and whistles though. 
Going off the top of my head (should be mostly accurate, but best details are in the original source of course!) AlphaGo (your third link) added Neural Networks trained in various ways to output policies (mapping from states to probability distributions over actions) and value functions (mapping from states to ""value estimates"", or estimates of win percentage). Trained policy networks were used in a variant of the ""selection"" phase (no longer UCB1 strategy as in UCT) to bias selection. A different (simpler function, not a deep net) policy was used to run play-outs (no long uniform at random action selection as in UCT). A combination of observed reward at end of play-out + value function estimate at start of play-out was used for backpropagation (no longer only backpropagating reward observed in play-out as in UCT).
AlphaGo Zero is, if we're purely looking at the MCTS part, quite similar to AlphaGo. The Neural Networks were a bit different (a single one, with multiple outputs for policy + value), and play-outs were no longer used at all (just immediate backpropagation of value function estimates after MCTS' selection + expansion phases). Apart from that, the primary differences going from AlphaGo to AlphaGo Zero were in the learning process used to train the Neural Networks really, not so much in the MCTS side of things.
"
What is the internal state of a Simple Neural Attentive Meta-Learner(SNAIL)?,"
In the paper A Simple Neural Attentive Meta-Learner, the authors mentioned right before Section 3.1:

we preserve the internal state of a SNAIL across episode boundaries, which allows it to have memory that spans multiple episodes. The
  observations also contain a binary input that indicates episode termination.

As far as I can understand, SNAIL uses temporal convolutions to aggregate contextual information, from which causal attention learns to distill specific pieces of information. Temporal convolutions does not seems to maintain any internal state, and neither does the attention mechanism they use after this paper. This makes me wonder: ""What is the internal state of a SNAIL?""
","['deep-learning', 'reinforcement-learning', 'meta-learning']","
Here's what I understand, welcome to point out any mistakes.
When starting a new episode(but still in the same task), SNAIL does not clear its batches. Instead, it makes decisions based on the current observation and observation-action pairs from the previous episode. In this way, it keeps knowledge of the previous episode whereby achieving few-shot learning in the test time.
"
How to predict human future location?,"
I have billions of anonymized location coordinates of people movement collected from app. I want to improve user experience by using location data. 

For example identify if user is at home or at office so that what they view in app changes.
Where will they be tomorrow at particular hour -  so that I can suggest to secure their homes from IOT device if they are out.

Regarding the first point, I tried to use the following rule: at night they are at home, and at day they are at work or school. Regarding the 2nd point, I have no idea how to proceed. Is there any way I could use AI to predict future location and home location?
","['machine-learning', 'models', 'prediction']","
Your problem is often called (in the literature) human mobility prediction. There has been some research in this area. Have a look at it on the web.
In general, you might want to use any statistical or machine learning model that uses the historical data to predict the future. For example, you could try to use an hidden Markov model (for both point 1 and 2). However, before that, you might also need to do some feature engineering, if you only have locations (and not e.g. the time of the day when those locations were recorded).
"
What is the difference between A2C and running an agent in an environment vector?,"
I've implemented A2C. I'm now wondering why would we have multiple actors walk around the environment and gather rewards, why not just have a single agent run in an environment vector? 
I personally think this will be more efficient since now all actions can be calculated together by only going through the network once. I've done some tests, and this seems to work fine in my test. One reason I can think of to use multiple actors is implementing the algorithm across many machines, in which case we can have one agent on a machine. What else reason should we prefer multiple actors?
As an example of environment vector based on OpenAI's gym
class GymEnvVec:
    def __init__(self, name, n_envs, seed):
        self.envs = [gym.make(name) for i in range(n_envs)]
        [env.seed(seed + 10 * i) for i, env in enumerate(self.envs)]

    def reset(self):
        return [env.reset() for env in self.envs]

    def step(self, actions):
        return list(zip(*[env.step(a) for env, a in zip(self.envs, actions)]))

","['reinforcement-learning', 'actor-critic-methods', 'gym', 'advantage-actor-critic']","
I believe if you run a single agent in multiple parallel environments many times you will get similar actions in similar states, the reason behind multiple agents is that you will have different agents with different parameters and you can also have different explicit exploration policies so your exploration will be better and you will learn more from environment (see more state space). With single agent you can't really achieve that, you would have a single exploration policy, single parameter set for the agent and most of the time you would be seeing similar states (at least after a while). You would be speeding up your learning process but that's just because you're running multiple environments in parallel (compared to the regular actor-critic or Q-learning). I think quality of learning would be better with multiple different actors.
"
Can the inputs and outputs of a neural network be a neural network?,"
Can the inputs and outputs of a neural network (NN) be a neural network (that is, neurons and connections), so that ""if some NN exist, then edit any NN"".
I think that by creating NNs with various inputs and outputs, interacting with each other, and optimizing them with evolution, we can create strong intelligence.
",['neural-networks'],
"If the AI goal is the protection of humans, will it always pursue this goal?","
If the AI goal is to serve humans and protect them (if this ever happens) and AI someday realizes that humans destroy themselves, will it try to control people for their own good, that is, will it control man's will to not destroy himself?
","['philosophy', 'agi', 'ethics', 'ai-safety']","
If the main goal of AI (which I assume you mean an AGI) is to protect humans and AI will be effective, then AI will always attempt to pursue its main goal (otherwise the assumption of its effectiveness does not hold), even at the expense of other less important goals that it might have. However, if the destruction of a human (or a group of humans) protected or avoided the destruction of other humans, then AI would face a dilemma. In that case, I think it is hard to predict the actions of the AI. Will it act rationally or irrationally? What would it mean for the AI to act rationally? Which parameters will it take into account? Only the number of deaths, or will take into account the future and weight the importance of the lives? How will it define the importance of a human life?
"
Is reinforcement learning using shallow neural networks still deep reinforcement learning?,"
Often times I see the term deep reinforcement learning to refer to RL algorithms that use neural networks, regardless of whether or not the networks are deep.
For example, PPO is often considered a deep RL algorithm, but using a deep network is not really part of the algorithm. In fact, the example they report in the paper says that they used a network with only 2 layers.
This SIGGRAPH project (DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills)  has the name deep in it and the title even says 'deep reinforcement learning', but if you read the paper, you'll see that their network uses only 2 layers.
Again, the paper Learning to Walk via Deep Reinforcement Learning by researchers from Google and Berkeley, contains deep RL in the title, but if you read the paper, you'll see they used 2 hidden layers.
Another SIGGRAPH project with deep RL in the title. And, if you read it, surprise, 2 hidden layers.
In the paper Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, if you read table 1 with the hyperparameters, they also used 2 hidden layers.
Is it standard to just call deep RL to any RL algorithm that uses a neural net?
","['deep-learning', 'reinforcement-learning', 'terminology', 'deep-rl']","
Even after several years of success of deep learning systems (i.e. neural networks trained with gradient descent and back-propagation), as far as I know, there is not yet a consensus on what constitutes a deep neural network. Some people could use a neural network with 2 hidden layers and call it deep (like in your case), but other people may just dedicate the adjective deep to refer to neural networks with 10, 100, or more hidden layers. In fact, there are some good reasons to associate the term deep only to neural networks that have a significant number of hidden layers (e.g. 100): for example, the exploding (or vanishing) gradient problem does not typically arise if you only have one hidden layer but can easily occur with many (e.g. 100) hidden layers.
Nevertheless, a neural network with at least one hidden layer can approximate any continuous function, given enough (but finite number of) units (or neurons) in the layers. See the universal approximation theorem. For this reason, we could start denoting any such neural network as deep, but, although this rule would exclude perceptrons (which can only approximate linear functions, and nobody would probably call them deep anyway), this rule would be a bit redundant or useless (i.e. we may just not use the adjective deep to start with).
In your case, the rule that the authors are using seems to be the following: if it contains more hidden layers than the bare minimum (i.e. 1) to approximate any continuous function, then let's denote it as deep.
"
Do we also need to model a probability distribution for the decoder of a VAE?,"
I'm working on understanding VAEs, mostly through video lectures of Stanford cs231n, in particular lecture 13 tackles on this topic and I think I have a good theoretical grasp.
However, when looking at actual code of implementations, such as this code  from this blog of VAEs I see some differences which I can't quite understand.
Please take a look at this VAE architecture visualization from the class, specifically the decoder part. From the way it is presented here I understand that the decoder network outputs mean and covariance for the data distribution. To get an actual output (i.e. image) we need to sample from the distribution that is parametrized by mean and covariance - the outputs of the decoder.
Now if you look at the code from the Keras blog VAE implementation, you will see that there is no such thing. A decoder takes in a sample from latent space and directly maps its input (sampled z) to an output (e.g. image), not to parameters of a distribution from which an output is to be sampled. 
Am I missing something or does this implementation not correspond to the one presented in the lecture? I've been trying to make sense of it for quite some time now but still can't seem to understand the discrepancy. 
","['computer-vision', 'generative-model', 'autoencoders', 'latent-variable']","
The VAE architecture from the cs231n class is just a more general version of the code Keras provides, in which the covariance matrix is $\mathbf 0$. You can see this from the reparametrization trick
$$
\begin{align}
x&=\mu+\Sigma\epsilon\\
&=\mu&\mathrm{if}\ \Sigma=0
\end{align}
$$
"
Is superintelligence a function of strength or a category?,"
Super comes from the Latin and means ""above"".

University of Oxford philosopher Nick Bostrom defines superintelligence as ""any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest"". (wiki)

Bostrom's definition could be taken to imply this is a quantitative measure of degrees as a numeric relationship. (Under this definition, we have achieved narrow superintelligence, reduced to competency in a single task.)
Gibson, famously, sheds light on an another aspect via Wintermute & Neuromancer, where, once superintelligence is achieved, the AI just f-'s off and does it's own thing, motivations beyond human comprehensions.  (Essentially, ""next-level"" thinking.) The second measure is discrete and ordinal.
Is superintelligence a function of strength or a category?
","['terminology', 'definitions', 'agi', 'superintelligence']","
Ill have a stab at this. 
Cognitive performance in narrow domains is determined by competency, efficiency and speed. Take calculating numbers, extremely narrow domain but compared to humans the ability of a calculator to calculate numbers exceeds normal human performance, it is much competent in terms of speed. In a bit broader domain, AlphaGo has defeated Go players, which is more difficult than chess, and requires intuition. In fact, there is an instance where the AlphaGo makes a long-term move that was previously unimagined. In all domains however Humans are well rounded, therefore Human Intelligence is called general intelligence. An AlphaGo or Calculator cannot speak eloquently or make music, but AIs are gaining pace in these areas too.
I agree with @nbro that Bostrom wants to keep the interpretation of Superintelligence open. But if there is a rough category, these are- 

ANI- Artificial Narrow Intelligence
AGI- Artificial General Intelligence: Where the AIs performance is at par with humans. After AGI, it quickly takes off to SI.
SI- Superintelligence: Superintelligence is beyond our imagination, we have not figured out yet what will a SI do, think or want.

While these categories are discrete, the functions of strength are not. Id say they are rather discrete to continuous, because if you look at the computing power plots that follow Moores Law, a similar exponential graph can be drawn for AIs performance towards general intelligence. In that graph, it seems the AIs performance starts with discrete performance points, and then as it takes off, it becomes continuous.
This is why the term Singularity is often associated with Superintelligence. I hope this answers your question.
"
Where or for what could genetic algorithms be used in the context of project management?,"
Where or for what could genetic algorithms (GA) be used in the context of project management (PM)? I thought about task dispatching, but I'm looking for other potential uses of GAs in the context of PM.
","['genetic-algorithms', 'applications']","
GA is well suited to optimize a non-linear fitness function with a lot of variables. Each vector that is a possible solution is evaluated with the fitness function that we want to optimize.
So, GA is well suited to optimize schedules for a lot of people, optimizing resources, etc.
Your mission is to define all variables that you can move-tune and that affect your objectives, and assign an estimated weight on every variable.
For example, create a total project cost as the fitness function, to minimize. One variable will be the number of programmers, multiplied by cost and total months. If you don't want delays over 3 months, add a correction factor: +10000*max(0, months-3), penalizing the fitness function's results where months > 3, etc.
When you apply your GA algorithm to the fitness function, you will find some minimum for your function, if any exists, because the solution may not exist if you added too many constraints.
"
One dimension deconvolutions or fully connected layers?,"
Ive created a variational autoencoder to encode 1-dimensional arrays. The encoding is done through 3 1d-convolutional layers. Then, after the sampling trick, I reconstruct the series using 3 fully connected layers. Here are my questions in case some can shed some light on it:
I think it would be better if I use 1d-deconvolutional layers instead of fully connected, but I cannot understand precisely why.

Its because it would bring better results? But then, if the FC layer is complex enough should be able to archive the same results, right?
Its because would be more efficient? This is, would get the same results as a complex enough FC layer but with less training and parameters?
Or its because of other reasons that Im missing.

Thanks.
","['deep-learning', 'convolutional-neural-networks']",
How could we solve the TSP using a hill-climbing approach?,"
How could we solve the TSP using a hill-climbing approach?
","['applications', 'optimization', 'search', 'hill-climbing', 'travelling-salesman-problem']",
Shouldn't Gdel's incompleteness theorems disprove the physical symbol system hypothesis?,"
According to the Wikipedia page of the physical symbol system hypothesis (PSSH), this hypothesis seems to be a vividly debated topic in philosophy of AI. But, since it's about formal systems, shouldn't it be already disproven by Gdel's theorem?
My question arises specifically because the PSSH was elaborated in the 1950s, while Gdel came much earlier, so at the time the Incompleteness theorems were already known; in which way does the PSSH deal with this fact? How does it ""escape"" the theorem? Or, in other words, how can it try to explain intelligence given the deep limitations of such formal systems?
","['philosophy', 'symbolic-ai', 'theory-of-computation', 'incompleteness-theorems', 'physical-symbol-system-hypothesis']","
Although there seems to be an apt analogy between Gdel's theorems and the PSHH, there is nothing formal linking the two together. 
More concretely, Gdel's theorems are about systems that decide certain ""truths"" about mathematics, but unless I am mistaken, the PSSH doesn't imply that the symbol system of the mind needs to decide truths. Though implicitly us humans do decide facts about math, there isn't a formal interpretation of how that might be done in the PSHH, thus Gdel's theorems do not apply.
However, this answer is still good, under the assumption that the formal system we are talking about does indeed decide certain truths about math.
"
Why does all of NLP literature use noise contrastive estimation loss for negative sampling instead of sampled softmax loss?,"
A sampled softmax function is like a regular softmax but randomly selects a given number of 'negative' samples.
This is difference than NCE Loss, which doesn't use a softmax at all, it uses a logistic binary classifier for the context/labels. In NLP, 'Negative Sampling' basically refers to the NCE-based approach.
More details here: https://www.tensorflow.org/extras/candidate_sampling.pdf.
I have tested both and they both give pretty much the same results. But in word embedding literature, they always use NCE loss, and never sampled softmax.
Is there any reason why this is? The sampled softmax seems like the more obvious solution to prevent applying a softmax to all the classes, so I imagine there must be some good reason for the NCE loss.
","['natural-language-processing', 'word2vec', 'word-embedding']",
Detecting abnormalities in x-rays while taking into account demographics of a patient -automated,"
This is my first post so please forgive me for any mistakes.
I am working on an object detection algorithm that can detect abnormalities in an x-ray. As a prototype, I will be using yolov3 (more about yolo here: 'https://pjreddie.com/darknet/yolo/')
However, one radiologist mentioned that in order to produce a good result you need to take into account the demographics of the patient.
In order to do that, my neural network must take into account both text an an image. Some suggestions have been made by other people for this question. For example, someone recommended taking the result of a convolution neural network and a seperate text neural network.
Here is an image for clarification:

Image Credits: This image (https://cdn-images-1.medium.com/max/1600/1*oiLg3C3-7Ocklg9_xubRRw.jpeg) from Christopher Bonnett's article (https://blog.insightdatascience.com/classifying-e-commerce-products-based-on-images-and-text-14b3f98f899e)
For more details, please refer to above-mentioned article. It has explained how e-commerce products can be classified into various category hierarchies using both image and text data.
However, a when convolution  neural network is mention it usssualy means it is used for classification instead of detection 
https://www.quora.com/What-is-the-difference-between-detection-and-classification-in-computer-vision (Link for comparison between detection and classification)
In my case, when I am using yolov3, how would it work. Would I be using yolov3 output vector which would be like this format 
class, center_x, center_y, width and height
My main question is how would the overall structure of my neural network be like if I have both image and text as input while using yolov3.
Thank you for taking the time to read this.
","['neural-networks', 'convolutional-neural-networks', 'computer-vision', 'object-recognition', 'automation']","

you need to take into account the demographics of the patient

How, exactly? 

Is it a difference of, say, threshold? In this case you can do this serially (as @mirror2image mentions): process the image and then conclude by comparing the size of what you saw to, say, an age-dependent threshold.
Or has the whole processing to be different? In the extreme, you would not wait until the very end before asking whether the patient is a man if you are looking for prostate cancer.

To design the model, you need enough medical understanding to make such choices. The model can handle the parameters, but you have to choose the architecture.
"
Next step after lane detection in vehicle automation,"
I have an RC car with a camera, I have implemented so that i can detect lanes on my track (think like a nascar track). I want to get this car to be able to go around the track autonomous. But I am quite unsure what my next step should be.
Either I can do an algorithm that detects so that I stay in the middle of the lane (if i get to close to either of the lines I steer towards the center). 
Or perhaps go around the track manually and save the coordinates of the detected lines as well as the actions I take (steering) and try a DQN approach. 
I'm trying to minimize my 'trial and error' time a little here. Perhaps there are some important steps in between here, or a solution that I have not thought of (i.e. am I missing something)?
I'm doing this as a proof of concept therefore I can only spend maximum of a month on this, so what would you do here?
","['computer-vision', 'dqn', 'autonomous-vehicles']",
What types of machine learning model would fit? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am working on a supervised machine learning problem where I have more than 10 probably 50 or 100 predicting label categories. Which type of model can be used to work on this type of problem in anaconda python.
","['machine-learning', 'python', 'data-science']","
Welcome to AI.SE @Par!
What you have might be either a multi-label or a multi-class classification problem. If the classes are disjoint (each example belongs to just 1 of the 50 classes), it's a multi-class problem. If not (so each example can belong to several classes at once), it's a multilabel problem.
Multi-label classification is usually handled by training a separate model for each of the labels. If you want to label a new point, you then ask each model what it thinks, and assign the union of the labels that the models suggest together. An alternative approach is to use something like a neural network, which can have many outputs. You can then have one output neuron for each possible label.
Multi-class classification can be addressed by using the multilabel techniques, but this is usually not a good idea. The three main approaches that are used are ""one-v-one"", ""one-v-all"", and ""many-v-many"".

In 1-v-1, we train one model to discriminate each pair of classes (n(n+1)/2 models in total for n classes). To classify a new point, we ask each model which class it belongs to and assign a ""vote"" to the class the model returns. The class with the most votes overall is selected as the label for the new point. In case of a tie, we can report several possible answers to the user.
In 1-v-all, we train one model to discriminate each class from all the other classes (n models in total for n classes). To classify a new point, we ask each model whether it belongs to the model's primary class or not. Ideally, just one model claims the new point. If more than one does, we can report a tie, or use some notion of classifier confidence to select a winner.
In many-v-many, we train k models each of which is tasked with discriminating some subset of the classes from all the rest. To classify a new point, we ask all of these models to label the point. We then pick the class that that is most consistent with the results of all the models. This can also be done by deliberately constructing the models to form an error correcting code. 

So which approach should you use? Well, Rifkin & Klautau's 2004 JMLR paper argues convincingly that the answer is to use one-versus-all classification. This is also pretty easy to do in most packages. For example, in Python's ScikitLearn, you can do it with OneVsRest. If you're not sure what to try, that's probably a safe bet.
"
How to detect frauds in advertising business using machine learning?,"
I am very beginner to this world. I still learning the basics of Machine learning and AI but i have a problem at hand and i am not sure which technique or Algorithm can be applied on it.
I am working on Click-Fraud detection in advertising. I need to predict fraud and learn new frauds with ML.
The dataset I have is the view and click logs from adserver(Service Provider). This data have some fields few of them are listed below:
""auction_log_bid_id"": null, 
""banner"": 9407521, 
""browser"": 0, 
""campaign"": 2981976, 
""city"": 94965, 
""clickword"": null, 
""content_unit"": 4335438, 
""country"": 1, 
""external_profiledata"": {}, 
""external_user_id"": null, 
""flash_version"": null, 
""id"": 6665230893362053181, 
""ip_address"": ""80.187.103.98"", 
""is_ssl"": true, 
""keyword"": ""string""
""mobile_device"": -1, 
""mobile_device_class"": -1, 
""network"": 268, 
""new_user_id"": 6665230893362118717, 
""operating_system"": 14, 
""profile_data"": {}, 
""referrer"": null, 
""screen_resolution"": null, 
""server_id"": 61, 
""state"": 7, 
""target_url"": ""string""
""timestamp"": 1551870000, 
""type"": ""CLICK_COMMAND"", 
""user_agent"": ""Mozilla/5.0 (iPhone; CPU iPhone OS 12_1_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/16D57"", 
""user_id"": null, 
""view_log_id"": null

There are other fields.
I need to analyse these logs to find patterns for possible frauds but i am not sure where to start and which technique to use. e.g. Supervised, Unsupervised Semi-Supervised or Reinforcement Learning.
","['machine-learning', 'models', 'applications']","
There are a couple of different ways you can go about this depending on what kind of data you have.
If you have labels or can separate the normal data from the fraudulent, you can perform either binary classification, or likely more usefully, anomaly detection.
In Anomaly Detection(which is typically now done via autoencoder) you train your model on the normal data, so it learns a compressed representation of that 'signal', from there it will be able to detect any sample that does not fit the learned representation(in theory).
Here is a link to a tutorial in keras: link
"
Can AI research lead to new findings in general cognitive science?,"
Can AI be used as a tool to investigate our minds? 
To be more precise, what am I specifically asking for here are examples of discoveries on artificial intelligence (so algorithms, programs and computers that try to implement intelligent systems) that brought to light facts about intelligence and cognition in general. Have this ever happened? Is it frequent? How influential and important were these discoveries, if any?
A possible example of what I mean could be the PSSH, which states that a formal system is sufficient to simulate general intelligent behaviour. I believe that this is relevant to Cognitive Science in general because it entails our understanding of this phenomena. (Of course, this is just an hypotesis, but I believe that its importance in the AI debate makes it a really compelling result).
","['philosophy', 'cognitive-science']","
This is about hard AI and soft AI: proponents of hard AI work on systems that simulate the way human cognition works, with the eventual (hypothetical) goal of replicating it. This presupposes that you know how cognition works, and presumably you will learn about it as you attempt to replicate it.
Soft AI, on the other hand, tries to emulate the outcomes only. For example, Weizenbaum's ELIZA is clearly on this side, as it uses simple pattern matching, and does not 'understand' anything about the conversations it is having.
Obviously, we don't even know fully what it means to 'understand' something, and building working systems is not really possible with a hard approach. Hence, soft AI is more common, as researchers are usually measured by their outcomes rather than their ideas. As far as I am aware, the hard AI approach has been all but abandoned long ago.
As current AI seems to be dominated by statistical approaches, I doubt that we can find out many useful things about cognition this way.
One interesting side-note: it seems to me that the capabilities of modern AI systems have developed away from human capabilities. A three-year-old can do some things that a sophisticated AI system cannot do, but in some areas (chess, translation, ...) the capabilities of AI systems surpass what humans are capable of. Maybe imitation is indeed not the right way to approach AI.
"
Image-Specific Class Saliency Visualisation,"
In the paper ""Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"", https://arxiv.org/abs/1312.6034, at part 3, there is a first-order Taylor expansion(formula number 3 & 4) that I can't understand the logic behind it and how they are obtained.
Those formulas are about computing saliency map in convolutional neural networks.
","['neural-networks', 'convolutional-neural-networks', 'computer-vision']",
How do intermediate layers of a trained neural network look like?,"
Suppose I have a deep feed-forward neural network with sigmoid activation $\sigma$ already trained on a dataset $S$. Let's consider a training point $x_i \in S$. I want to analyze the entries of a hidden layer $h_{i,l}$, where
$$h_{i,l} = \sigma(W_l ( \sigma (W_{l-1} \sigma( \dots \sigma ( W_1 \cdot x_i))\dots).        $$
My intuition would be that, since gradient descend has passed many times on the point $x_i$ updating the weights at every iteration, the entries of every hidden layer computed on $x_i$ would be either very close to zero or very close to one (thanks to the effect of the sigmoid activation). 
Is this true? Is there a theoretical result in the literature which shows anything similar to this? Is there an empirical result which shows that?  
","['neural-networks', 'machine-learning', 'deep-learning', 'activation-functions']",
Heavy loss and inaccurate answer in pytorch [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



As my first AI model I have decided to make an AI model to predict multiplication of two numbers EX - [2,4] = [8]. I wrote the following code, but the loss is very high, around thousands, and it's very inaccurate. How do I make it more accurate?
import torch
import torch.nn as nn
import torch.nn.functional as F

data = torch.tensor([[2,4],[3,6],[3,3],[4,4],[100,5]],dtype=torch.float)
values = torch.tensor([[8],[18],[9],[16],[500]],dtype=torch.float)
lossfun = torch.nn.MSELoss()
model=Net()
optim = torch.optim.Adam(model.parameters(),lr=0.5)

class Net(nn.Module):
    def __init__(self):

        super(Net,self).__init__();

        self.fc1 = nn.Linear(in_features=2,out_features=3)
        self.fc2 = nn.Linear(in_features=3,out_features=6)
        self.out = nn.Linear(in_features=6,out_features=1)

    def forward(self,x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.out(x)
        return x
for epoch in range(1000):

    y_pred=model.forward(data)

    loss = lossfun(y_pred,values)

    print(loss.item())

    loss.backward()

    optim.step()

Note: I am a newbie in AI and ML.
","['objective-functions', 'pytorch']",
Can the normalization factor for the belief state update be zero?,"
In order to update the belief state in a POMDP, the following formula is used:
$$b'(s')=\frac{O(a, s', z) \sum_{s\in S} b(s)T(s, a, s')}{\mathbb{P}(z \mid b, a)}$$
where 

$s$ is a specific state in the set of states $S$
$b'(s')$ is the updated belief state of being in the next state $s'$
$T(s, a, s') = \mathbb{P}(s' \mid s, a)$ is the propability (function) of having been in $s$ and ending up in $s'$ by taking action $a$;
$O(a, s', z) = \mathbb{P}(z \mid s', a)$ the probability (function) of observing $z$, performing action $a$ and ending up in $s'$
$\mathbb{P}(z \mid b, a)$ is defined as follows $\sum_{s \in S}b(s)\sum_{s' \in S} T(s, a, s')O(a, s', z)$

Looking at $\mathbb{P}(z \mid b, a)$ it is possible that the result is $0$. This would be the case if the agent is in a state where no further actions are possible. But, in that case, there is a problem with updating $b'(s')$, since this causes a zero division. Is this a common problem and is the only possibility to avoid that a programming solution like an if-statement? Or is $\mathbb{P}(z \mid b, a)$ always non-zero?
","['reinforcement-learning', 'pomdp', 'probability-distribution']","
I think that the normalisation factor is assumed to be non-zero. So, in practice, I guess, you must eventually check that $P(z \mid b, a)$ is non-zero (even though, I guess, it will likely never be zero because of round-off errors in computers).
The formula to calculate $b'(s')$ comes from its definition, which is based on Bayes' theorem, where the denominator is assumed to be non-zero (in general).
The definition of $b'(s')$ is $P(s' \mid z, a, b)$, that is, the new belief $b'$ of being in state $s'$ is defined as the probability of landing in the next state $s'$, given that we have observed $z$, have taken action $a$ from the previous state $s$ and we had the previous belief $b$. We will expand this definition, but first let us recall a few probability definitions.
Recall that $P(A, B) = P(A \mid B) P(B) = P(B \mid A) P(A)$, where $A$ and $B$ can actually be multiple events (that is, $A$ could actually be the intersection of multiple events). In other words, suppose we want to calculate $P(A, B, C)$, we can actually consider e.g. $B$ and $C$ as one event. Let $(B \cap C) = (A, B) = D$ (note that the notation $(A, B)$ means the ""intersection"" of events $A$ and $B$, in the case $A$ and $B$ are events). Then
\begin{align}
P(A, B, C) 
& = P(A, (B, C)) \\
&= P(A, D) \\
&= P(A|D)P(D) \\ 
&= P(A|B, C)P(B, C) \\
&= P(A|B, C)P(B|C)P(C) \\
\end{align}
In general, this idea generalises to more variables/events.
Note also that $\frac{P(A, B)}{P(B)} = P(A \mid B)$. 
At this point, we are prepared to expand $P(s' \mid z, a, b)$ and understand its expansion.
We can expand $P(s' \mid z, a, b)$ as follows
\begin{align}
P(s' \mid z, a, b)
&= \frac{P(s', z, a, b)}{P(z, a, b)}\\[0.7em]
&= \frac{P(s', z, a, b)}{P(z \mid b, a)P(a|b)P(b)}\\[0.7em]
&= \frac{P(z \mid s', a, b) P(s' \mid a, b) P(a | b) P(b)}{P(z \mid b, a)P(a|b)P(b)} \\[0.7em]
&= \frac{P(z \mid s', a, b) P(s' \mid a, b)}{P(z \mid b, a)}
\end{align}
It then turns out that (I will maybe explain this more in detail later) 
\begin{align}
P(s' \mid z, a, b)
&= \frac{P(z \mid s', a, b) P(s' \mid a, b)}{P(z \mid b, a)} \\[0.7em]
&=\frac{O(a, s', z) \sum_{s\in S} b(s)T(s, a, s')}{\sum_{s \in S}b(s)\sum_{s' \in S} T(s, a, s')O(a, s', z)} \\[0.7em]
&=b'(s')
\end{align}
So, by assumption, $P(z \mid b, a) = \sum_{s \in S}b(s)\sum_{s' \in S} T(s, a, s')O(a, s', z)$ must be different from zero for the equality above to hold.
You can see this from a very simply example of the Bayes' theorem. Let $P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}$. Now, intuitively, $P(A \mid B)$ (which is what we want to calculate using Bayes' theorem) means the probability of $A$ occurring given that $B$ has occurred, which means that $B$ couldn't have had a probability of $0$ of happening if we wanted to calculate $P(A \mid B)$, so $P(B)$ couldn't have been zero if we wanted to calculate $P(A \mid B)$ using Bayes' theorem. We can also apply this reasoning to the definition of $b'(s')$ above.
For completeness, note also that, in the normalisation factor $$\sum_{s \in S}b(s)\sum_{s' \in S} T(s, a, s')O(a, s', z),$$ $b(s)$, $T(s, a, s')$ and $O(a, s', z)$ are probability distributions, which means that not all terms of $b(s)$, $T(s, a, s')$ and $O(a, s', z)$ can be zero, for all $s$, $s'$ and $a$ (given they must sum up to $1$).
Note also that $\sum_{s' \in S} T(s, a, s')O(a, s', z)$ is a convex combination of all $O(a, s', z)$ (for all $s'$), where the coefficients are $T(s, a, s')$. The normalisation factor is also a convex combination where the coefficients are $b(s)$ (for all $s$). 
"
DQN Q-mean values converge negatively,"
I'm trying to implement my own DQN. So far I think my code is good, but my Q-values (I'm getting the mean of all the values for every episode) tends to converge near-zero but negatively. It is normal? Or there is something wrong in my implementation?
My exploration vs explotation greedy strategy goes from 1.0 to 0.1 in 1 million steps (as DeepMind does), my learning rate is 0.00025 and my gamma 0.99. I read here that 
""The mean Q-values should smoothly converge towards a value proportionnal to the mean expected reward.""
So, it's my agent expecting a negative reward? If so, how can i fix it?
Here is a graph of the first training session:

You can see how the Q-values tend to converge near-zero after about 1300 episodes (1120000 steps aproximately). Actually it's showing values like -0.0117, -0.0145, etc.
Also, the agent seems very ""static"" after epsilon gets near 0.1, and when it reaches it doesn't move so much. (I'm training with PongDeterministic-v4)
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']",
Is REINFORCE the same as 'vanilla policy gradient'?,"
I don't know what people mean by 'vanilla policy gradient', but what comes to mind is REINFORCE, which is the simplest policy gradient algorithm I can think of. Is this an accurate statement?
By REINFORCE I mean this surrogate objective
$$ \frac{1}{m} \sum_i \sum_t log(\pi(a_t|s_t)) R_i,$$
where I index over the $m$ episodes and $t$ over time steps, and $R_i$ is the total reward of the episode. It's also common to replace $R_i$ with something else, like a baselined version $R_i - b$ or use the future return, potentially also with a baseline $G_{it} - b$.
However, I think even with these modifications to the multiplicative term, people would still call this 'vanilla policy gradient'. Is that correct?
","['reinforcement-learning', 'comparison', 'terminology', 'policy-gradients', 'reinforce']","
You can check the Open AI Introduction to RL series, they explain pretty neatly there what is the Policy Optimization and how to derive it. I think, that usually when we are talking about REINFORCE algorithm, we are talking about the one described in Sutton's book on Reinforcement learning. It is described as the policy optimization algorithm maximizing the Value Function $v_{\pi(\theta)}(s) = E[G_t|S_t = s]$ of initial state of the agent. Here $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$ is the $\gamma$ discounted return from given state, time  $s, t$. Or, shortly put.
$$
 J(\theta) = v_{\pi(\theta)}(s_0) = E[G_t|S_t = s_0]\\
\nabla J(\theta) = E_\pi\left[G_t\frac{\nabla \pi (A_t |S_t, \theta)}{\pi (A_t |S_t, \theta)}\right]
$$
But in the RL series of Open AI, the algorithm that is described as Vanilla policy gradient (If it is the one you are talking about) is optimizing finite-horizon undiscounted return $E_{\tau \sim \pi} [R(\tau)] $, where $\tau$ are possible trajectories. e.g.
$$
 J(\theta) =  E_{\tau \sim \pi} [R(\tau)] \\
\nabla J(\theta) = E_{\tau \sim\pi}\left[\sum_{t=0}^T R(\tau) \frac{\nabla \pi (A_t |S_t, \theta)}{\pi (A_t |S_t, \theta)}\right]
$$
They do look very similar in objective functions, but they are different. The way the gradient ascent is performed differs strongly since in REINFORCE method the gradient ascent is performed once for each action taken for each episode and the direction of ascent is taken as
$$
G_t\frac{\nabla \pi (A_t |S_t, \theta)}{\pi (A_t |S_t, \theta)}
$$
so the update becomes
$$
\theta_{t+1} = \theta_{t} + \alpha G_t\frac{\nabla \pi (A_t |S_t, \theta)}{\pi (A_t |S_t, \theta)}
$$
but in VPG algorithm the gradient ascents performed once over multiple episodes and direction of ascent taken as average
$$
\frac{1}{|\mathcal{T}|}\sum_{\tau\in\mathcal{T}} \sum_{t=0}^T R(\tau) \frac{\nabla \pi (A_t |S_t, \theta)}{\pi (A_t |S_t, \theta)}
$$
and gradient ascent step is
$$
\theta_{t+1} = \theta_{t} + \alpha \frac{1}{|\mathcal{T}|}\sum_{\tau\in\mathcal{T}} \sum_{t=0}^T R(\tau) \frac{\nabla \pi (A_t |S_t, \theta)}{\pi (A_t |S_t, \theta)}
$$
which looks a lot like what you have stated as REINFORCE algorithm.
I admit, that some form of mathematical equivalence can be derived between them, since expectation over policy and expectation over the trajectory sampled from the policy looks practically the same. But approaches differ at least in the way, the ascent is computed.
"
Is there any formal test for linear separability of 2-class data?,"
SVM is designed for two-class classification problem. If the data is not linear-separable, a kernel function is used. I want to know  if there is exists any method that will indicate if the data is linearly separable or not. 
","['neural-networks', 'machine-learning']",
Can neural networks evolve other neural networks?,"
Can neural networks change or evolve other neural networks? Also, could evolutionary algorithms be applied to evolve neural networks?
For example, suppose that we have neural networks A and B. The neural network B changes the neural network A. If B ""successfully"" changed it, NN A will survive.
","['neural-networks', 'evolutionary-algorithms', 'agi', 'neuroevolution']","
This answer points at some of the more modern approaches. This has been around for a long time in the form of NeAT: Neuroevolution of Augmenting Topologies, originally described in Kenneth Stanley's 2002 paper.
NeAT is available as a package for many languages, including Python, Java, and C++. The algorithm works as a form of genetic programming. A population of networks is generated with simple, random, topologies. Then they are evaluated according to a loss function for a specific task. The poorly performing networks are discarded, and the better performing ones are intermixed to generate new variations. This process is iterated until the user wishes it to stop, and typically results in gradual improvement of average population performance against the loss function.
"
How to build a DQN agent which can be trained through interactive learning?,"
I am trying to create a chatbot whose dialogue policy model will be trained through reinforcement learning. Dialogue Policy is responsible for selecting the action to take based on the given state of the conversation.
All implementations I see for RL are trained from an environment taken from Gym or created manually. These environments provide the next state, rewards etc to the model based on which it is trained.
Since I am creating a dialogue policy model which will be trained through real user conversations, I cannot provide a ""pre-defined"" environment which can provide the states and rewards. I am planning to train it myself by talking to it and providing rewards and next state (which I think is called interactive learning).
But I was not able to find any implementations, tutorials or articles on interactive learning. I am not able to figure out how to create such a model, how to take care of the episodes, sessions etc. This will be a continuous learning that will go on for months maybe. I have to save the model each day and continue training the next day by loading the model from that same state.
Can anyone guide me in the correct direction on how to approach this? Any githubs links, articles, tutorials of such implementations will be highly appreciated. I am aware this question seems too broad, but some hints will be very helpful for a newbie like me.
","['reinforcement-learning', 'q-learning', 'chat-bots', 'dqn']","
I suggest you to start reading this one on BERT and this one on GPT-2.

I am aware this question seems too broad, but some hints will be very helpful for a newbie like me.

I'm not sure you want to create your chatbot using RL architecture at all. But if you want to implement such an idea the right way to go with that is by using iterative approach and starting from a really simple base:

Define a list of actions for your agent. Might be 10-words
vocabulary with simple one-word answers. 
Define you simple environment. It might have 20 different states (e.g. greetings/conversation starters hey, hello)
Map good responses (hello -> hi) to positive rewards and -1 for the rest of them.
If you want to move forward with DQN try a simple Q-learning algorithm on your simple environment and train agent to properly response to those.
Once you have done the above you can make your vocabulary bigger
Now replace your table of input words which gives you a state for your network with NN/word2vec/any other NLP approach which will convert your input sentence to a state.
Manually provide a reward as a response for what you agent did according to learned policy.
Do that infinite amount of time and your agent probably will figure out how to respond properly.

NOTE: DQN uses discrete action space. So in such a case you will have only limited or REALLY HUGE actions space. It's all possible combinations between all possible words in you vocabulary (assuming action maps to some sentence)
"
Live video object detection with pose estimation,"
I was researching about hierarchical object detection, and end up reading that Yolo v3 is the state of art for that kind of tasks, besides, the inference time make it one of the best for run it on live video.
So, what I have in mind, is to run a pose estimation technology over the live video (LikeOpenPose), then focus only in the rectangles near the hands of the estimated pose in order to detect the object.
the previous approach sounds good, but I feel like I'm not taking advantage of the temporal features on the video, for example, YoloV3 could not be very sure that someone has a cellphone with only the rectangle of the hands, but if I add up, the movement of the estimated pose (hand near to the head for several frames), I could be more sure that he has a phone.
But I cannot find a paper, approach or something close to the idea I have on mind, so I was wondering if someone here could give me a little clue about what path should I follow.
Thanks in advance for any help!
",['computer-vision'],
"Why didn't champion of the Go game manage to win the last game against AlphaGo, after winning the 4th one?","
In the documentary about the match, it is said that after losing the 4th game, AlphaGo came back stronger and started to play in a weird way (not human-like) and it was pretty impossible to be beaten. Why and how did that happen? 
","['reinforcement-learning', 'game-ai', 'monte-carlo-tree-search', 'alphago']","
The technique used by AlphaGo is ""Monte Carlo Tree Search"", combined with a very well trained neural network. The network's job is to estimate the quality of different board states and moves. This estimation is deterministic. If you show AlphaGo the same board on two different occasions, it thinks it is exactly as good (or bad) on both occasions.
Monte Carlo Tree Search however, is a randomized algorithm. So as a simplified explanation, the way AlphaGo decides which moves to make is:

Look at the current board.
Pick a random move, and imagine what the board would look like if you made that move.
Pick a random move for your opponent, and imagine what the board would look like if they made that move.
Keep doing steps 2 & 3 for a while, so that we're imagining being at some board many moves in the future along a random line of play.
Ask the neural network how good this board state is.
Repeat steps 1-5 many times. Then make whichever move led to the best lines of play on average, to make right now.

What this means is, AlphaGo won't always play the same way, because it doesn't actually consider every move explicitly. It just thinks about enough lines of play to be pretty confident about whether one move is better than another. This is actually not so far removed from how humans play most of the middle of games like these.
So, in game 4, essentially, Sedol got lucky. The random lines of play that AlphaGo chose to look at did not capture some critical facts about one or more board states. This led it to make a mistake. If you asked it to play the same game through again, it might not make the same mistakes (it might think about different random lines of play, that do capture the critical facts it missed in the first game). Further, it might choose to play slightly differently on other moves, which could have a big impact on the rest of the game. These two factors prevented Sedol from simply playing game 4 over again.
"
Do we need to reset the DQN network after every episode?,"
I was going through this implementation of Reinforcement learning where model is being trained to manage the number of bikes at a station.
Here, line 78 represents the loop over all episodes (if I understood correctly). In line 92, the DQN Agent is defined meaning after each episode, the agent will be reset to default parameters.
But shouldn't we define the model before the loop starts because the after each episode, won't all the previous learning be lost if we initialize the class object in each iteration? 
Am I misinterpreting anything?
","['reinforcement-learning', 'q-learning', 'dqn']",
How can I generate keywords associated with a website given its URL?,"
I have a column with links to websites and another column with keywords from those websites. I have to find a map between these two, such that for a new input, which is a website's URL, I can generate the keywords associated with the contents of the website.
For example, given the URL chocolate.com, the keywords could be milk and dark. We can tell from this example that my keywords are types of chocolates.
Or, if the URL is career.com, then the keywords could be IT and medicine.
I have a feeling that some sort of supervised neural networks could be used here.   Which approach would be most suited here?
","['neural-networks', 'machine-learning', 'supervised-learning', 'text-summarization']",
How do I find whether this heuristic is or not admissible and consistent?,"
I was given the following problem to solve.

Given a circular trail divided by $n> 2$ segments labeled $0 \dots n-1$. In the beginning, an agent is at the start of segment number $0$ (the edge between segments $n-1$ and $0$) and the agent's speed ($M$) is $0$ segments per minute.
At the start of each minute the agent take one of three actions:

speed up: the agent's speed increases by $1$ segment per minute.
slow down:  the agent's speed decreases by $1$ segment per minute.
keep the same speed: stay at the same speed.

The action slow down cannot be used if the current speed is $0$ segments per minute.
The cost of each action is $1$.
The goal of the agent is to drive around the trail k times ($1 \leq k$)  and then park in the beginning spot (at speed 0 of course). The agent needs to do that in the minimum amount of actions.
The heuristic given is: If agent is in segment $z$ then: $n-z$ if $z \not = 0$ or $0$ if $z=0$.

I need to find if the given heuristic is admissible (or complete) and consistent.
I think:

Regarding consistency: A heuristic is consistent if its estimate is always $\leq$ estimated distance from any given neighbour vertex to goal plus cost of reaching a goal. So, in the given problem, it is consistent because ($n>2$), so heuristic function is well defined and because of circular trail divided by $n$ segments with a constant price of $1$ for each action, then the given definition of consistency holds because estimated distance from any given neighbour vertex to goal can be looked on as a difference between segments until reaching a goal and it is consistent because again the function is well defined.
Regarding admissibility: An admissible heuristic is one that the cost to reach goal is never more than the lowest possible cost from the current point to reach the goal. I am not sure if the given heuristic is admissible because it does not help much to know the difference between trail size ($n$ = segment size) and current place. But it does not create flaws, so it is probably admissible. I am not sure this is a proof.

Is my idea correct? How could I write it as a proof?
","['search', 'proofs', 'heuristics', 'admissible-heuristic', 'consistent-heuristic']","
Welcome to AI.SE @hpr16!
Your understanding of when a heuristic is admissible is correct, but your heuristic is inadmissible. An admissible heuristic must always underestimate the cost to move from a given state to a goal state. 
Notice that states in the search are not the same as positions on the circle in your problem. A state needs to capture all the information about the current environment the agent is in. In your problem, agents have a speed as well as a position. A state must, therefore, contain both.
To see why your heuristic is inadmissible because the agent can move (n-z) segments in less than n-z steps: it can speed up, and do them in, for example, (n-z)/2 steps, by moving with speed 2. 
"
Reinforcement Learning with limited number of episodes,"
I try to implement RL to a case something like this:

This game consist of several rounds. Every round the players need to generate a maze that consists of rooms. There are around 1000 different available rooms with different properties. At the beginning of a round, each player will be given 10 rooms one-by-one (the sequence is same for each player) from 1000 that available, then he/she try to create a maze from the taken rooms (by arranging each room). After the maze is done there is a Game Master who will judge the maze (give a score between 0-100). The player never know how the Game Master judges the maze, it can be judged based on the level of difficulty that is produced, the order of the room we compose, or others. The player who got the best score for the given rooms will win this round.
in this case, I have around 100,000 ""perfect"" mazes that have been created from different room combination and got a perfect score. I use this maze as episodes and try to train RL-Agent to find the pattern of how the Game Master judges a maze. For your information, there are rooms that not exist in the 100,000 perfect mazes, but I hope the RL-Agent can use its properties to find similar rooms that exist in  the ""perfect"" mazes, and make it as a reference

This case is different from other RL environments that I've ever met before, generating an episode is not an easy task because it needs an expert to validate it (The Game Master). So you could say, I can only build the RL agent using that 100,000 episodes.
But, even though it only consists of 100,000 episodes, my case has millions of states, so I plan to use Q-Learning with Neural Net as approximator.
My question is:

in this case, am I still need the Experience Replay process (I am afraid I don't need it because of the small number of available episodes)? 
has this case ever happened before? What is the best approach to deal with cases where the number of episodes is limited?

","['reinforcement-learning', 'q-learning', 'experience-replay']",
Are there any pathfinding algorithms that take customized rules into account when determining the shortest path?,"
I need a pathfinding algorithm that considers the history of visited nodes and varies its path depending on some rules (like already visited). Are there good approaches serving this purpose?
To be more specific: Let's say I have a graph representing a map. I want to find a Route from B to D: Once I am in D following B->C->D, I want to calculate a new path, let's say to A. The shortest path would be D->C->B->A. But I want a path with unvisited nodes even if it's longer than the shortest possible path. The new path should be the shortest among all possible paths according to the rules.
Another example is the game ""snake"". Seeing the grid as a graph I cannot visit already visited nodes as fas as the corpus (of the snake) is in that node (or I have visited the node t time steps ago)  
Maybe the problem is too specific and I have to implement some basic pathfinding algorithm in a wider algorithm.
",['path-finding'],"
Yes, this is easily solved using the A* algorithm. Once your agent has visited a particular node, increase the cost of that node to infinity and recalculate the path.
"
Can there be applications of byzantine neural networks on quantum computers?,"
This question came after I connected 2 pieces of information :

I recently listened to The Byzantine Generals Problem, Poisoning, and Distributed Machine Learning with El Mahdi El Mhamdi (Beneficial AGI 2019).
Today, I was laughing with a friend at ""quantum computing"" being mentioned in a job offer. 

Considering the probabilistic nature of results in quantum computers, what would be the advantages to using byzantine resistant neural networks on quantum computer? Has this already been attempted?
","['neural-networks', 'quantum-computing']",
When should we use algorithms like Adam as opposed to SGD?,"
As far as I know, Stochastic Gradient Descent is an optimization algorithm which belongs to the the category of algorithms where hyper-parameters have to be defined beforehand. They are useful in many cases, but there are some cases that the adaptive learning algorithms (like AdaGrad or Adam) might be preferable. 
When are algorithms like Adam and AdaGrad preferred over SGD? What are the cons and pros of adaptive algorithms, like Adam, when we compare them with learning algorithms like SGD?
","['machine-learning', 'optimization']",
"What does ""Wide"" vs. ""Deep"" mean in the context of Neural Networks?","
From this article, I read that ""to accurately classify data with neural networks, wide layers are sometimes necessary.""
However, I have seen many implementations and discussions on deep-learning, such as this, mention the concept of depth.  
What is the difference in the context of neural networks?  How does width vs depth impact a neural network's performance?
","['neural-networks', 'deep-learning']",
Is there a way to compare the similarities among different graphs and then cluster them using Unsupervised learning?,"
I have a dataset about (240000,23). For my task, I have to use an unsupervised learning method and apply it on every single column separately in order to detect anomalies that might exist. I have pre-processed the data and I am visualizing the TimeElapsed vs the parameter in Python (The graphs look like the one shown in an earlier post by me here).
I am wondering if there is a way wherein after the graphs are plotted, the graphs are compared with each other and then clustered together based on their similarities.
Example: If I have temporal data of about 200 products, I plot the graphs of all the 200 products (as can be seen in the link provided above) and these 200 graphs must be compared with each other and must be separately plotted as 200 different points on a scatter plot (by using some unsupervised learning techniques) based on how similar are the graphs of different products to each other.
I don't know if the code that I have would be helpful in guiding me, but the code that I have is here:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(1234)

dataset = pd.read_csv('Temporal_DataTable.CSV', header = 0)
dataset != 0
(dataset != 0).any(axis=0)
dataset = dataset.loc[:, (dataset != 0).any(axis=0)]

dataset['product_number'] = pd.factorize(dataset.Context)[0]+1
dataset['index'] = pd.factorize(dataset.Context)[0]+1

cols = list(dataset.columns.values)
cols.pop(cols.index('StepID'))
cols.pop(cols.index('Context'))
cols.pop(cols.index('product_number'))
cols.pop(cols.index('index'))
dataset = dataset[['index','product_number','Context','StepID']+cols]
dataset = dataset.set_index('index')

max_time_ID_without_drop = dataset.groupby(['product_number'])['TimeElapsed', 'StepID'].max()
avg_time_without_drop = np.average(max_time_ID_without_drop['TimeElapsed'])

dataset_drop = dataset.drop(index = [128, 133, 140, 143, 199])

max_time_ID_with_drop = dataset_drop.groupby(['product_number'])['TimeElapsed', 'StepID'].max()
avg_time_with_drop = np.average(max_time_ID_with_drop['TimeElapsed'])

dataset = dataset.drop(columns=['TimeStamp'])
dataset_drop = dataset_drop.drop(columns=['TimeStamp'])

grouped = dataset.groupby('product_number')
ncols = 4
nrows = int(np.ceil(grouped.ngroups/40))
for i in range(10):
    fig, axes = plt.subplots(figsize=(12,4), nrows = nrows, ncols = ncols)
    for (key, ax) in zip(grouped.groups.keys(), axes.flatten()):
        grouped.get_group((20*i)+key).plot(x='TimeElapsed', y=['Flow_Ar-EDGE'], ax=ax, sharex = True, sharey = True)
        ax.set_title('product_number=%d'%((20*i)+key))
        ax.get_legend().remove()
        handles, labels = ax.get_legend_handles_labels()
        fig.legend(handles, labels, loc='upper center')
plt.show()

Thanks in advance for the help.
","['machine-learning', 'deep-learning', 'python', 'unsupervised-learning']",
Can I export a trained machine learning model so that others case use it?,"
From what I know, AI/ML uses a large amount of data to train an algorithm to solve problems. But since its an algorithm, I was wondering if it's possible to export it. If I trained an AI with R, could I export a mathematical algorithm that could be imported by other users to use in their application, whether its written in R or another language?
So its like Ive discovered a secret message decoding method. I dont need to share the whole program for others to decode it. I just need to tell them the steps (algorithm) to decode it, and they can implement it in whatever application they want. 
","['machine-learning', 'training']","
Yes, once you've trained a model you'll have the details of that model in your workspace.
e.g.
B_Naive = naiveBayes(train_set[,-c(1)],train_set[,1]);

Will give you an object B_naive that can be 'exported'. These are the parameters of the model, you'll still need the nave bayes library (or whichever library).
"
Regarding the output layer's activation function for continuous action space problems,"
I'm interested in building a (deep) RL agent for solving a continuous problem (which splits something into portions).
In all examples I've seen so far, e.g., solving the continuous lunar lander, always a $\tanh$ output layer activation was used, which produces values between $-1$ and $+1$.
Is this just because it fits the use case or is this a general rule for RL agents with continuous action spaces?
What if I just want values between $0$ and $1$? Could I simply use a $\operatorname{softmax}$ activation for my output layer?
","['neural-networks', 'reinforcement-learning', 'deep-rl', 'activation-functions']","
the use of Tanh is purely because it fits the described problem (Especially for values that are min-max normalized). I have worked on couple of professional RL projects ( specifically with actions in the continuous space) and I did not use tanh at all. Hope that helped :)
"
Is it a good idea to use BERT to answer a FAQ with semantic similarity?,"
I have been looking for BERT for many tasks. I would like to compare the performance to answer an FAQ, using BERT semantic similarity and BERT Q/A. 
However, I'm not sure it is a good idea to use semantic similarity for this task. If it is, do you think it is possible to find a dataset to fine-tune my algorithm? 
","['deep-learning', 'natural-language-processing', 'ai-design', 'applications', 'bert']","
Maybe the following article can help you:
FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance (2019)
They evaluate their model in localgovFAQ and StackExchange datasets.
"
How did the OpenAI 5 for Dota concatenate units?,"
I am no expert in the field of AI so I apologize if this is a simple/easy question. I was trying to implement a network similar to OpenAI's for another game and I noticed that I did not fully understand how the network worked.
Below is the image of OpenAI five's network

Basic question
How is the data concatenated/ what is the dimension of the data right after the concatenation before the LSTMs or just before entering the LSTMs?
Below are my thoughts which I provide for clarification's sake.
1st Interpretation
In the blue area for units, my initial understanding was that for each visible unit, the output of the max-pool is concatenated along the columns. So, assuming the number of rows is 1(as a 1d max-pool is being applied for each unit), the number of columns is n and there are N visible units, the final size of the matrix when concatenated is $(1,n\cdot N)$ with few extra columns given by the pickups and the like as shown on the left-hand side of the model.
Problem with this interpretation
As the number of units a player can see per each turn is not constant, under this interpretation, I suspect that the fully connected layer after the concatenation layer cannot do its job as matrix multiplication becomes impossible with a variable number of columns.
Possible solution
One possible solution to this is to set a maximum to the number of observed units as $N_{max}$ and pad with constants if some units are not observed. Is this the case?
2nd Interpretation
My 2nd interpretation is that the data is concatenated along the rows. In this case, I can see that the data can pass through a fully connected layer because the number of columns can remain constant. Under this assumption, I decided that right before going through the LSTM, the data is reshaped to (batch size, number of rows, number of columns).
Problems with this interpretation
While I found this interpretation to be more appealing, I noticed that under this train of thought, the LSTM is used just to associate the input data and is not associated with time(the time step for the LSTM is simply the next row of data rather than actual time). I know that this is not especially a problem but I thought that there is no special need to use an LSTM here as in this second interpretation, the order of the data holds no special meaning. But is this the case?
I apologize in advance for any unclear points. Please tell me in the comments and I'll try to clarify as best as I can!
","['deep-learning', 'open-ai', 'deep-rl']","
Tl;dr max-pool
You can see in the diagram, everywhere there are a variable number of inputs (pickups, units, hero modifiers/abilities/items), a max-pool follows, though I don't know the specifics of the max-pool implementation.
From https://neuro.cs.ut.ee/the-use-of-embeddings-in-openai-five :

Notice that while the number of modifiers, abilities and items is variable, the network max-pools over each of those lists. This means that only the highest value in all those dimensions actually gets through. At first it does not seem to make sense  it might give the impression that you have an ability that is combination of all existing abilities, e.g. ranged passive heal. But it seems to work for them.


Above processing is done separately for each of the nearby units, the results from general attributes, hero modifiers, abilities and items are all concatenated together. Then different post-processing is applied depending if it was enemy non-hero, allied non-hero, neutral, allied hero or enemy hero.


Finally the results of post-processing are max-pooled over all units of that type. Again this seems to be questionable at the first sight, because different qualities of nearby units would be combined, e.g. if one of the dimensions would represent the health of a unit, then the networks sees only the maximum health over the same type of units. But, again, it seems to work fine.

"
Expressing Arbitrary Reward Functions as Potential-Based Advice (PBA),"
I am trying to reproduce the results for the simple grid-world environment in [1]. But it turns out that using a dynamically learned PBA makes the performance worse and I cannot obtain the results shown in Figure 1 (a) in [1] (with the same hyperparameters). Here is the result I got:

The issue I found is that the learning procedure is stuck due to bad PBA in the early stages of training. Without PBA, Sarsa can converge well. 
Did anyone try the method before? I am really puzzled and how the authors obtain these good results? There are some top conference papers using the same method in [1], for example, [2] and [3].
[1]  Expressing Arbitrary Reward Functions as Potential-Based Advice
[2]  Learning from demonstration for shaping through inverse reinforcement learning
[3]  Policy Transfer using Reward Shaping
Is the method itself defective or anything wrong with my code? Here is part of my codes:
import copy
import numpy as np
import pandas as pd

def expert_reward(s, action):
    if (action == RIGHT) or (action == DOWN):
        return 1.0
    return 0.0

class DynamicPBA:
    def __init__(self, actions, learning_rate=0.1, reward_decay=0.99):
        self.lr = learning_rate
        self.gamma = reward_decay
        self.actions = actions
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64) #q table for current time step
        self.q_table_ = pd.DataFrame(columns=self.actions, dtype=np.float64) #q table for the current time step
        self.check_state_exist(str((0,0)))

    def learn(self, s, a, r, s_, a_): #(s,a) denotes current state and action, r denotes reward, (s_, a_) denotes the next state and action
        self.check_state_exist(s_)
        q_predict = self.q_table.loc[s, a]
        q_target = r + self.gamma * self.q_table.loc[s_, a_]
        self.q_table.loc[s, a] = self.q_table.loc[s, a] + self.lr * (q_target - q_predict)

    def update(self):
        self.q_table = copy.deepcopy(self.q_table_)

    def check_state_exist(self, state):
        if state not in self.q_table.index:
            # append new state to q table
            self.q_table = self.q_table.append(
                pd.Series(
                    [0]*len(self.actions),
                    index=self.q_table.columns,
                    name=state,
                    )
                )
            self.q_table_ = self.q_table_.append(
                pd.Series(
                    [0]*len(self.actions),
                    index=self.q_table_.columns,
                    name=state,
                    )
                )

#######Main part

RL = SarsaTable(actions=list(range(len(actions_dict))), reward_decay=0.99, learning_rate=0.05)
expert = DynamicPBA(actions=list(range(len(actions_dict))), learning_rate=0.1, reward_decay=0.99)
for episode in range(100):
    # initial observation
    s = (0,0)
    env.reset(s)
    action = RL.choose_action(str(s))

    r_episode_s = 0
    r_episode = 0

    current_step = 0
    while True:        

        # RL take action and get next observation and reward
        s_, _, reward, status = env.step(action)
        current_step += 1

        action_ = RL.choose_action(str(s_))
        # update dynamic potentials
        expert_r = -expert_reward(s, action) 
        expert.learn(str(s), action, expert_r, str(s_), action_)

        # compute PBA
        F = expert.gamma * expert.q_table_.loc[str(s_), action_] - expert.q_table.loc[str(s), action]

        #update expert PBA table
        expert.update()

        RL.learn(str(s), action, reward+F, str(s_), action_, status)

        # swap observation
        s = s_
        action = action_

        # break while loop when end of this episode
        if status != 'not_over':
            break
        if current_step>10000:
            print(episode, r_episode, r_episode_s, current_step)
            break     
        # learning rate decay
        RL.lr = RL.lr*0.999
#     expert.update()

","['reinforcement-learning', 'reward-design', 'reward-shaping', 'inverse-rl', 'potential-reward-shaping']",
How can we recognise musical notes in low-resolution or blurry images?,"
I was looking for an approach to recognise musical notes from photos.
I found this repository https://github.com/mpralat/notesRecognizer. However, it doesn't seem good enough. If you look into the bad folder, you can see that just tiny variations of lightning can already cause problems. One should be able to read musical notes with lower quality images.
I found other projects. However, they all use high resolution images.

https://github.com/suyalcinkaya/music-note-recognition (example of image)
https://github.com/nikolalsvk/note-play (example)

Now, this is unsatisfying. If you want to snap a photo of some tunes, and want them to be recognized.
So, what could one do to achieve a good solution?
I was thinking about treating the musical notes just like written letters. The computer can easily learn written characters with the Arabic symbols.
I wonder, though, how easy would it be for a non-Arabic? For example, in Chinese or Japanese, several characters combine into one.
The same applies to musical notes, they can be connected and form something slightly different through that. For example,

or:

in contrast to just simple notes like:

What would be a good approach to recognize musical notes even for slightly low-resolution images or bit blurry deformed images?
I'm not saying to read out a symphony out of a thumbnail. But less than optimal captures.
","['image-recognition', 'optical-character-recognition']","

any subjective ideas or comments are more than welcome

Not a complete answer, but some ideas:
Your goal is subdivided into many tasks. It's not exactly the same as OCR because you also need to find the vertical alignment for each note.


One should be able to read musical notes with lower quality images.

If you want your model to perform on low quality image, you'll need such database.
But instead of labeling and taking pictures of printed sheets, you could just generate the images and virtually apply all sorts of distortion on them.
"
Are there existing examples of using neural networks for static code analysis?,"
Background Context:
In the past I've heavily applied various ""code quality metrics"" to statically analyze code to provide an inkling of how ""maintainable"" it is and using things like the Maintainability Index alluded to here.
However, a problem that I face is whether a language has libraries that effectively measure such metrics - only then is it usable else it's rather subjective/arbitrary. Given the plethora of languages that one has to deal with in an enterprise system, this is can get rather unwieldy. 
Proposed Idea:
Build and train an Artificial Neural Network that ""ingests a folder of code"" (i.e., all files within that folder/package are assumed to house the ""project"" whose quality metrics we'd like to compute). This may again be language dependent but let's assume it exists for a language that I'm having the hardest time with (for measuring ""maintainability""): Scala.
Using numeric metrics like McCabe's complexity or Cyclomatic complexity maybe ""convention"" but are not entirely relevant. Few things like class/method length are almost always relevant no matter the language. Thus, providing a few ""numeric metrics"" + abstract notion of readability by subjective evaluation to train an ANN would be a good balance of ""inputs"" to the ANN. The output being either a classification of maintainability like low, medium, high etc., or a number between 0 and 1.
Question:
Has this been tried and are there any references? I spent some time digging via Google Scholar but didn't find anything ""usable"" or worthwhile. It's okay if it's not Scala, but have ANNs been used for measuring code quality (i.e., static analysis) and what are the benefits or disadvantages of something like this? 
PS: Hopefully, the question isn't too broad, but if so, please let me know in the comments and I'll try make it as specific as possible.
","['neural-networks', 'machine-learning', 'quality-control']","
There's certainly literature on a related topic: code smell detection.
A ""code smell"" is a sign that code has a maintenance problem, and hints at the presence of technical debt. It is reasonable to suppose that code with a lot of smells is lower quality. Code smells include things like giant classes, high cyclomatic complexity, and more.
Fontana et al. have a good 2016 survey comparing different ML methods for detecting code smells. A reverse citation search on that paper uncovers many other papers that seem relevant, including:

Rasool et al. 2015, A review of code smell mining techniques
da Silva Maldonado et al. 2017 Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt
Maneerat & Muenchaisri, 2011, Bad-smell prediction from software design model using machine learning techniques
Mansoor et al. 2017, Multi-objective code-smells detection using good and bad design examples

This seems like a pretty well-studied area. I wasn't able to find a cross-language model though, but I suspect one may well exist.
"
How do I improve accuracy and know when to stop training?,"
I am training a modified VGG-16 to classify crowd density (empty, low, moderate, high). 2 dropout layers were added at the end on the network each one after one of the last 2 FC layers.
network settings:

training data contain 4381 images categorized under 4 categories (empty, low, moderate, high), 20% of the training data is set for validation. test data has 2589 images.
training is done for 50 epochs.(training validation accuracy drops after
50 epochs)
lr=0.001, decay=0.0005, momentum=0.9
loss= categorical_crossentropy
augmentation for (training, validation and testing data): rescale=1./255, brightness_range=(0.2,0.9), horizontal_flip

With the above-stated settings, I get the following results: 

training evaluation loss: 0.59, accuracy: 0.77
testing accuracy 77.5 (correct predictions 2007 out of 2589)

Regarding this, I have two concerns: 

Is there anything else I could do to improve accuracy for both training and testing? 
How can I know if this is the best accuracy I can get?

","['convolutional-neural-networks', 'classification', 'performance', 'vgg']","

Is there anything else I could do to improve accuracy for both training and testing? 

Yes, of course, there are a lot of methods if you want to try to improve your accuracy, some that I can mention:

Try to use a more complex model: ResNet, DenseNet, etc.
Try to use other optimizers: Adam, Adadelta, etc.
Tune your hyperparameters (e.g. change your learning rate, momentum, rescale factor, convolution size, number of feature maps, epochs, neurons, FC layers)
Try to analyze your data, with ~75% and 4 categories, is that possible there is one category that difficult to classified?

In essence, you have to do a lot of experiments with your model until you think ""it is enough"" (if you have a deadline). If you don't have a hard deadline, you can keep improving updating your ML model.

How can I know if this is the best accuracy I can get?

No, you can't until you compare it with other models/hyperparameters. If you do more experiments (some ways like the one I mentioned above) or compare with other people's experiments that using the same data, you'll find which one is the best. For an academic paper, for example, you need to compare at least 3 to 4 models that similar or experiment with hundreds of different hyperparameters combination.
"
Why Q2 is a more or less independant estimate in Twin Delayed DDPG (TD3)?,"
Twin Delayed Deep Deterministic (TD3) policy gradient is inspired by both double Q-learning and double DQN. In double Q-learning, I understand that Q1 and Q2 are independent because they are trained on different samples. In double DQN, I understand that target Q and current Q are relatively independent because their parameters are quite different. 
But in TD3, Q1 and Q2 are trained on exactly the same target. If their parameters are initialized the same, there will be no difference in their output and the algorithm will be equal to DQN. The only source of independence/difference of Q2 to Q1 I can tell is the randomness in the initialization of their parameters. But with training on the same target, I thought this independence will become smaller and smaller as they converge to the same target values. So I don't quite understand why TD3 works in combating overestimation in Q-learning.
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'ddpg']","
I emailed the author of the paper and he replied that randomness in the parameter initialization is the only difference between Q1 and Q2. This difference is enough in practice. Moreover, TD3 method is more concerned with overestimation induced by function approximation error rather than stochasticity in the environment.
"
Are on-line backpropagation iterations perpendicular to the constraint?,"
Raul Rojas' Neural Networks A Systematic Introduction, section 8.1.2 relates off-line backpropagation and on-line backpropagation with Gauss-Jacobi and Gauss-Seidel methods for finding the intersection of two lines.
What I can't understand is how the iterations of on-line backpropagation are perpendicular to the (current) constraint. More specifically, how is $\frac12(x_1w_1 + x_2w_2 - y)^2$'s gradient, $(x_1,x_2)$, normal to the constraint $x_1w_1 + x_2w_2 = y$?
","['backpropagation', 'math', 'gradient-descent']","
The equation $\frac12(x_1w_1 + x_2w_2 - y)^2$ is called the $Error (E)$ (assuming $y$ to be continuous which is not the case in case of classifiers). If you write this equation in Physics or Maths it represents a family of curves in 4D (the curves are continuous but for visualisation we will assume it to be a family of curves).
Here is a representative equation of what it would have looked like had the error been $\frac12(x_1w_1 - y)^2$ a 3D curve.

This is a scalar quantity which represents the value of error at different places for different values of $w1$ and $w2$. Now gradient of a scalar is defined as $\nabla F$, where $F$ is a scalar, on doing this operation you get a vector, which is perpendicular to the equi-potential or more suitably equi-error surface, i.e. if you trace all the points which give the same error, you will get a curve, and its gradient at any point is the vector perpendicular to the curve at that given point. There are many proofs for this but here is a very simple and nice proof.
Now lets look at the equation of the constraint $x_1w_1 + x_2w_2 = y$. In case of a 3D error curve, the constraint is giving us a plane which is parallel to the tangential plane of the equi-error surface at a given point. You can look at this method of how to find tangential planes and derive the plane yourself, where $z = Error(E)$ and $w1$ and $y$ are your $x$ and $y$. 
Thus it is quite clear that the gradient will be perpendicular to the constraint, and this is the reason we use gradients because according to mathematics if you move in a direction perpendicular to an equi-potential surface you get the maximum change than any other direction for same $dl$ movement.
I would highly suggest you check out these videos on gradient from Khan academy. This will hopefully give you a more intuitive understanding of why we do what we do in Neural Networks.
"
What are the purposes of autoencoders?,"
Autoencoders are neural networks that learn a compressed representation of the input in order to later reconstruct it, so they can be used for dimensionality reduction. They are composed of an encoder and a decoder (which can be separate neural networks). Dimensionality reduction can be useful in order to deal with or attenuate the issues related to the curse of dimensionality, where data becomes sparse and it is more difficult to obtain ""statistical significance"". So, autoencoders (and algorithms like PCA) can be used to deal with the curse of dimensionality. 
Why do we care about dimensionality reduction specifically using autoencoders? Why can't we simply use PCA, if the purpose is dimensionality reduction? 
Why do we need to decompress the latent representation of the input if we just want to perform dimensionality reduction, or why do we need the decoder part in an autoencoder? What are the use cases? In general, why do we need to compress the input to later decompress it? Wouldn't it be better to just use the original input (to start with)?
","['machine-learning', 'autoencoders', 'dimensionality-reduction', 'curse-of-dimensionality']","
It is important to think about what sort of patterns in the data are being represented. 
Suppose that you have a dataset of greyscale images, such that every image is a uniform intensity. As a human brain you'd realise that every element in this dataset can be described in terms of a single numeric parameter, which is that intensity value. This is something that PCA would work fine for, because each of the dimensions (we can think of each pixel as a different dimension) is perfectly linearly correlated. 
Suppose instead that you have a dataset of black and white 128x128px bitmap images of centred circles. As a human brain you'd quickly realise that every element in this dataset can be fully described by a single numeric parameter, which is the radius of the circle. That is a very impressive level of reduction from 16384 binary dimensions, and perhaps more importantly it's a semantically meaningful property of the data. However, PCA probably won't be able to find that pattern.
Your question was ""Why can't we simply use PCA, if the purpose is dimensionality reduction?"" The simple answer is that PCA is the simplest tool for dimensionality reduction, but it can miss a lot of relationships that more powerful techniques such as autoencoders might find. 
"
How can I make meaningful English sentences from given set of words?,"
I have a set of topics and each topic consists of a set of words. I want to make meaningful English sentences from these words. Each topic consist of 5 to 10 words and these words are relevant to each other, like {code, language, test, write and function} and {class, public, method, string, int} are two sets. I want to generate a sentence from these set of words using API.
","['machine-learning', 'natural-language-processing']","
Two Approaches:

Naive Bayes
LSTM

Train Naive Bayes on a whole dataset learning the probability of the next word given a word.
You can even go with any LSTM approaches, but I'd bet on Naive Bayes.
Eg:
text: hello how are you hello how are you hello No how
to get the suggestion of next word depending on current word - hello
p(how | hello) = 3/4
p(No | hello) = 1/4
take argmax of probabilities.
Also remember to smooth, and train on huge dataset. Training is just finding the probabilities before hand.
Hope it helps ;)
"
What Model Used for Forecasting Sales with Dynamic Holiday,"
I'm working on a project where I need to forecast sales data where I have history of 1 year (2017) daily data. I am new on Artificial Intelligence topic and after searching for a while, I think ARIMA or Multiple Linear Regression is a good model for seasonal forecasting (Correct Me If I'm Wrong). But then I think that my history data is exclusive for 2017 because on 2018 and 2019, holiday date is changing.
What model I have to used to forecast based on new holiday setup? Is ARIMA or Multiple Linear Regression still can be used? Or I need another model? Where do I have to start on this?
","['neural-networks', 'linear-regression', 'forecasting']",
What are the common techniques one could use to deal with collisions in a transposition table?,"
Consider an iterative deepening search using a transposition table. Whenever the transposition table is full, what are common strategies applied to replace entries in the table?
I'm aware of two strategies:

Replace the oldest entry whenever a new one is found.

Replace an entry if the new one has a higher depth.


I'm curious about other replacement approaches.
","['search', 'dynamic-programming', 'iddfs']",
Do we have to consider the feasability of an action when defining the reward function of a MDP?,"
Do we have to consider if (s is given) an action a can lead to s' when defining a reward function?
To be more specific:
Let's say I have a 1D Map like: |A|B|C|D|
To define a reward function, I simply defined a matrix for every action, where the columns and rows represent the states (A-D) and the entries represent the reward. But I made it even simpler. As reaching a specific state gives a reward of 1 I just assigned the reward to a column (C). 
$$
\begin{matrix} -1&0&1&0\\ -1&0&1&0\\-1&0&1&0\\-1&0&1&0 \end{matrix}
$$
However, lets say the matrix is specified for the action ""going right"". Now there are entries reading: D ==> going right ==> C getting reward of 1. This is actually not possible. However, I thought the transition function would handle this issue since I will define there what is possible and what is not. But anyway it is said that for a horizon of one the immidiate reward is given and the transition function isn't even considered. This leads to arbitrary result. So do I have to consider the ""physics"" of my world?
","['rewards', 'markov-decision-process']","
Your issue is related to how you are representing your rewards, and not anything to worry about for the MDP.
You have chosen to use a matrix to represent your reward function, which maps $(s,s')$ to $r$. If some transition $s \rightarrow s'$ doesn't happen, then it doesn't matter at all what you put there for the reward value for $(s,s')$. It is only because you have decided to use a matrix for this that you even need to think about it. 
A complete reward function would map $s, a, s'$ to a scalar value, and can also include a random factor. You don't need to use all the factors - a MDP can use any or all of them, and all the MDP theory is still correct. In your case, it looks like arriving in a ""goal state"" $s' = C$ is what triggers a +1 reward. So instead of a matrix you could define a function:
$$r(s,a,s') = \begin{cases}
    -1,& \text{if } s' = A\\
    1,& \text{if } s' = C\\
    0,              & \text{otherwise}
\end{cases} $$
This is equally valid in the MDP as your matrix, and avoids the issue of storing data about impossible transitions.

I thought the transition function would handle this issue since I will define there what is possible and what is not.

Yes that should be the case. If your transition function does not allow for certain state changes, then there is no need to handle them in any particular way in the reward function.

But anyway it is said that for a horizon of one the imm[e]diate reward is given and the transition function isn't even considered

I am not sure where you have read this, but it does not relate to your representation issue or resolving ""impossible"" state transitions.
If you are using a model-based method, such as Policy Iteration or Value Iteration, then you do use the transition function and reward function - more or less directly in the form of the Bellman equations. In this case, the transition function will assign a weight of 0 to the impossible transitions, so the reward value doe not matter (and efficient code would likely skip even looking up or calculating the reward in that case).
If you are using a model-free method, such as Q learning, then the agent does not use the transition function or the reward function directly. They are part of the environment that it is learning. However, any code for the simulated environment has to implement the physics of your world, and that includes using the transition function to resolve what happens when the agent takes an action. In that case, the simulated model of the environment would prevent the agent ever experiencing impossible transitions, so there is never any need to calculate the reward for them or allow for them as edge cases in a reward function (you might still choose to check validity and/or return something as defensive programming of the reward function, but it is not required for the MDP and reinforcement learning to work).
"
Why should we study causation in artificial intelligence?,"
Judea Pearl won the 2011 Turing Award

For fundamental contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning.

He is credited with the invention of Bayesian networks and a framework for causal inference.
Why should we study causation and causal inference in artificial intelligence? How would causation integrate into other topics like machine learning? There are facts or relations that cannot be retrieved from data (e.g. cause-effect relations), which is the driving force of ML. Are there other reasons for studying causation?
","['applications', 'causation']",
Hinton's reading list from the removed Coursera MOOC,"
Geoffrey Hinton's Coursera MOOC was recently discontinued:
https://twitter.com/geoffreyhinton/status/1085325734044991489?lang=en
The videos however are still available at both on Youtube and on Hinton's webpage:
https://www.cs.toronto.edu/~hinton/coursera_lectures.html
However in his MOOC Hinton also had some papers as required (or recommended, I can't really remember) readings after each lecture. They were generally old, seminal papers which provided some good insights and intuitions and were worth reading IMHO for learning purposes. I couldn't find these papers as a reading list online. Does anyone have this list?
",['deep-learning'],
What is the difference between reinforcement learning and optimal control?,"
Coming from a process (optimal) control background, I have begun studying the field of deep reinforcement learning.
Sutton & Barto (2015) state that

particularly important (to the writing of the text) have been the contributions establishing and developing the relationships to the theory of optimal control and dynamic programming

With an emphasis on the elements of reinforcement learning - that is, policy, agent, environment, etc., what are the key differences between (deep) RL and optimal control theory?
In optimal control we have, controllers, sensors, actuators, plants, etc, as elements. Are these different names for similar elements in deep RL? For example, would an optimal control plant be called an environment in deep RL?
","['reinforcement-learning', 'comparison', 'terminology', 'sutton-barto', 'control-theory']","
As a supplement to nbro's nice answer, I think a major difference between RL and optimal control lies in the motivation behind the problem you're solving. As has been pointed out by comments and answers here (as well as the OP), the line between RL and optimal control can be quite blurry.
Consider the Linear-Quadratic-Gaussian (LQG) algorithm, which is generally considered to be an optimal control method. Here a controller is computed given a stochastic model of the environment and a cost function.
Now, consider AlphaZero, which is obviously thought of as an RL algorithm. AlphaZero learns a value function (and thus a policy/controller) in a perfect information setting with a known deterministic model.
So, it's not the stochasticity that separates RL from optimal control, as some people believe. It's also not the presence of a known model. I argue that the difference between RL and optimal control comes from the generality of the algorithms.
For instance, generally, when applying LQG and other optimal control algorithms, you have a specific environment in mind and the big challenge is modeling the environment and the reward function to achieve the desired behavior. In RL, on the other hand, the environment is generally thought of as a sort of black box. While in the case of AlphaZero the model of the environment is known, the reward function itself was not designed specifically for the game of chess (for instance, it's +1 for a win and -1 for a loss, regardless of chess, go, etc.). Furthermore, the neat thing with AlphaZero is that we can use it to train agents in virtually any perfect information game without changing the algorithm at all. Another difference with RL here is that the agent iteratively improves itself, while optimal control algorithms learn controllers offline and then stay fixed.
"
Which unsupervised learning technique can be used for anomaly detection in a time series?,"
I've started working on anomaly detection in Python. My dataset is a time series one. The data is being collected by some sensors which record and collect data on semiconductor-making machines.
My dataset looks like this:
ContextID   Time_ms Ar_Flow_sccm    BacksGas_Flow_sccm
7289973 09:12:48.502    49.56054688 1.953125
7289973 09:12:48.603    49.56054688 2.05078125
7289973 09:12:48.934    99.85351563 2.05078125
7289973 09:12:49.924    351.3183594 2.05078125
7289973 09:12:50.924    382.8125    1.953125
7289973 09:12:51.924    382.8125    1.7578125
7289973 09:12:52.934    382.8125    1.7578125
7289999 09:15:36.434    50.04882813 1.7578125
7289999 09:15:36.654    50.04882813 1.7578125
7289999 09:15:36.820    50.04882813 1.66015625
7289999 09:15:37.904    333.2519531 1.85546875
7289999 09:15:38.924    377.1972656 1.953125
7289999 09:15:39.994    377.1972656 1.7578125
7289999 09:15:41.94     388.671875  1.85546875
7289999 09:15:42.136    388.671875  1.85546875
7290025 09:18:00.429    381.5917969 1.85546875
7290025 09:18:01.448    381.5917969 1.85546875
7290025 09:18:02.488    381.5917969 1.953125
7290025 09:18:03.549    381.5917969 14.453125
7290025 09:18:04.589    381.5917969 46.77734375

What I have to do is to apply some unsupervised learning technique on each and every parameter column individually and find any anomalies that might exist in there. The ContextID is more like a product number.
I would like to know which unsupervised learning techniques can be used for this kind of task at hand since the problem is a bit unique:

It has temporal values.
Since it has temporal values, each product will have many (similar or different) values as can be seen in the dataset above.

","['neural-networks', 'machine-learning', 'unsupervised-learning', 'time-series', 'anomaly-detection']",
Can Deep Learning be applied to Computational Fluid Dynamics,"
Can Deep Learning be applied to Computational Fluid Dynamics (CFD) to develop turbulence models that are less computationally expensive compared to traditional CFD modeling?
","['neural-networks', 'deep-learning']",
Algorithms and strategies to help judges rule cases,"
I'm a Rails developer with a lot of web experience, but none (still) in AI.
I'm working in a web text editor that judges use to writing their sentences.
The goal is to start to use AI to help the judge rule the case, either based on his own previous rulings, either based on his colleagues rulings.
The judge would provide the text for the plaintiffs and defendants petitions, and based on these two inputs the system would recommend previous rulings that apply for the case. 
I already have a considerable dataset of judges rulings inside the database, and they can be easily attached to the plaintiffs and defendants petitions for training (so this plaintiff petition + this defendant petition = this ruling). 
This is specially challenging because the complaints can contain different subjects combined into the same petition; but the fact is that many offices use the same standardized petitions, as the defendants do as well, so I think the system can have a great chance of prediction success.
What algorithms or strategies should I start studying to tackle this problem? 
Any similar articles, white papers or repositories that could help in my goal?
","['natural-language-processing', 'text-summarization']","
Genuine success in this area would be beyond the state-of-the-art in research, since it likely requires analogising from relational knowledge extracted from text. In recent years, techniques for working with natural language have tended to be statistical, and are therefore somewhat deficient in this respect. You could look at 'bag of words'/latent semantic analysis approaches, but they are likely to generate many false positives unless a lot of ad hoc conditions are added manually. More recent work on 'treenets' (paper here) is more structurally informed, but is still a relatively new area.
"
When is content-based more appropriate than collaborative filtering?,"
I know the difference between content-based and collaborative filtering approach in recommender systems. I also know some of the articles said collaborative filtering have some advantages than content-based, some of them also suggest to use both method (hybrid) to make a better system recommendation.
Is there a specific case where the use of one method (content-based, specifically) is better than another? Because if there is no case at all, why both methods are considered to be on the same ""level"", why not focus on just one method? For example, focus on collaborative filtering or hybrid method (as an extension for collaborative filtering).
","['research', 'definitions', 'comparison', 'recommender-system']",
"Can Reinforcement Learning solve problems, where certain elements in the environement are randomly located?","
I want to solve a problem using Reinforcement Learning on a 20x20 board. An agent (a mouse) has to get the highest possible rewards as fast as possible by collecting cheese, which there are 10 in total. The agent has a fixed amount of time for solving this problem, namely 500 steps per game. The problem is, the cheeses will be randomly assigned to the fields on the board, the agent knows however, where the cheeses is located.
Is there any way how this could be solved using only Reinforcement Learning (and not training for an absurd amount of time)? Or is the only way to solve this problem to use algorithms like the A*-algorithm?
I've tried many different (deep)-q-learning models, but it always failed miserably.
Edit: I could not get any meaningful behavior after 6 hours of learning while using a GTX 950M. Maybe my implementation was off, but i don't think so. 
","['deep-learning', 'reinforcement-learning', 'rewards']","
A assume here OP is familiar with DQN basics.
""Standard"" way to solve this problem with Deep RL would be using convolutional DQN.
Make net from 2 to 4 convolutional layers with 1-2 fully connected on top. 
The trick here is how you output Q-values. Input is board with cheese, without information about mouse. Instead net should output Q for every action from every field on the board (every possible position of mouse), that mean you output 20x20x(number_of_moves from the field) Q values. That would make net quite big, but that is most reliable way for DQN. For each move form the replay buffer only one Q value updated (gradient produced) with Time Difference equation
Because only one value form 20x20x(number_of_moves) updated per sample you need quite big replay buffer and a lot of training. After each episod (complete game) cheese should be randomly redistributed. Episodes should be mixed up in replay buffer, training on 1 episod is a big No.
Hopefully that should at least give you direction in which do research/development. Warning: DQN is slow to train, and with such big action space (20 x 20 x number_of_moves) could require million or tens of millions of even more moves.
Alternatively, if you don't want such big action space, is to use actor-critic architecture (or policy gradient, actor-critic  is a kind of policy gradient). Actor-critic network have small action space, with only number_of_moves outputs. On the down size complexity of method is much higher, and behavior could be difficult to predict or analyze. However if action space is too big it could be preferable solution. Practical issues and implementations of actor-critic is too huge area to go in depth here.
Edit: There is another way with lesser action space for DQN, but somehow less reliable and possibly more slow: shift the board in such way that mouse is in the center of the board and pad invalid parts with zero (size of the new board should be x2). In that case only number_of_moves should be in output. 
"
Are there any advantages of using rules-based approaches versus models for detecting spam?,"
Suppose that we have unlabeled data. That is, all we have are a collection of emails and want to determine whether any of them is spam or not. Let's say we have $1,000$ rules to determine whether a particular email is spam or not. For example, one rule could be that a sender's email address should not contain the text  no_reply . If an email does contain this text, then it would be classified as spam.
What are the advantages/disadvantages of a rules-based approach for detecting spam vs. a non-rules-based approach/unsupervised methods for detecting spam?
Would there even be a point in constructing a non-rules based model given that we already have a rules-based model? Could we use the rules-based model to create some labeled training data and then apply supervised techniques?
","['comparison', 'unsupervised-learning', 'anomaly-detection', 'binary-classification', 'rule-based-systems']","
Yes, there would be a point. Assuming your rule set is accurate, then you can use data classified with it to train a model. This model can be expected to be more robust and properly categorise emails that your rule-set will not handle.
Why? Machine learning algorithms generally work on features, and identify relationships between those features that lead to a classification decision. A human rule author basically does the same, but they might not notice subtle relationships; a good ML algorithm, however, might pick those up.
So you could have a hybrid model, where you first use your rule-based classifier, and anything that does not get classified is then run through the ML classifier.
"
Why validation performance is unstable for my LSTM based model (labelling problems)?,"
I have trained a recurrent neural network based on 1 stack of LSTM cells. I use it to solve a classification problem.
The RNN cell has 48 hidden states. The output of the last unfolded LSTM cell is linearly projected into two dimensions corresponding to two mutually exclusive classes. I train with softmax cross-entropy loss.
I also know that both my train and test sets are mislabeled(!) to a certain extent. Possibly about 10% of items labelled as class 1 are actually class 0. And the same hold in other direction.
What puzzles me is this. Every time I train the network from scratch and plot a precision recall curve for the validation set in the end. And every time it is different! Especially in the very beginning in the range that corresponds to high precision levels. Why is this happening (this instability)?
I tried various numbers of epochs, training rates, number of hidden states in LSTM. Every time it is the same, but for some combinations of these parameters the variability is less (e.g. 8 out of 10 train/test runs i see more or less the same precision recall curve and 2 times it is severely different and worse).
","['classification', 'training', 'recurrent-neural-networks', 'long-short-term-memory']",
"Why is KNNBasic better than KNNWithMeans with the default parameters, but KNNWithMeans performs better with folds?","
I'm learning a bit about the use of the Surprise library and I have a set of data with users and ratings. I'm training a network with this library, using KNNBasic and KNNWithMeans, this last algorithm is the same as KNN but averages the ratings before calculating the distances between the points.
If I don't use any measure of similarity, i.e. using the two algorithms with the default parameters, KNNBasic predicts the results better than using KNNWithMeans. But if I train the nets using subsets, 10 folds, where the algorithm iterates over 9 of them for training and the other one for validating, KNNWithMeans gives better results.
Do you know why this can happen? Why KNNBasic is better in the first case, and increasing the number of folds is better KNNWithMeans?
","['training', 'recommender-system', 'k-nearest-neighbors']",
Reinforcement Learning with long term rewards and fixed states and actions,"
I have read a lot about RL algorithms, that update the action-value function at each step with the currently gained reward. The requirement here is, that the reward is obtained after each step.
I have a case, where I have three steps, that have to be passed in a specific order. At each step the agent has to make a choice between a range of actions. The actions are specific for each step.
To give an example for my problem:
I want the algorithm to render a sentence of three words. For the first word the agent may choose a word out of ['I', 'Trees'], the second word might be ['am', 'are'] and the last word could be chosen from ['nice', 'high']. After the agent has made its choices, the reward is obtained once for the whole sentence.
Does anyone know which algorithms to use in this kind of problem?

To give a bit more detail on what I already tried:
I thought that using value iteration would be an reasonable approach to test. My problem here is that I don't know how to assign the discounted reward for the chosen operations.
For example after the last choice I get a reward of 0.9. But how do I update the value for the first action (choosing out of I and Trees in my example)?
","['reinforcement-learning', 'rewards']","
You don't need to have a reward on every single timestep, reward at the end is enough. Reinforcement learning can deal with temporal credit assignment problem, all algorithms are designed to work with it. Its enough to define a reward at the end where you, for example, give a reward of 1 if sentence is satisfactory or -1 if it isn't. Regular tabular Q-learning would easily solve the toy problem that you gave as an example.
"
What is graph clustering?,"
There are several (family of) algorithms that can be used to cluster a set of $d$-dimensional points: for example, k-means, k-medoids, hierarchical clustering (agglomerative or divisive). 
What is graph-based clustering? Are we clustering the nodes or edges of a graph instead of a set of $d$-dimensional (as e.g. in k-means)? Couldn't we just use k-means to also cluster a set of nodes?
","['unsupervised-learning', 'clustering', 'k-means']","
In graph clustering, we want to cluster the nodes of a given graph, such that nodes in the same cluster are highly connected (by edges) and nodes in different clusters are poorly or not connected at all. 
A simple (hierarchical and divisive) algorithm to perform clustering on a graph is based on first finding the minimum spanning tree of the graph (using e.g. Kruskal's algorithm), $T$. It then proceeds in iterations. At each iteration, we remove from $T$ the edge with the highest weight. Given that $T$ is a tree, the removal of an edge from $T$ will create a forest (with connected components). So, after the removal of the edge of highest weight from $T$, we will have two connected components. These two connected components will represent two clusters. So, after one iteration, we will have two clusters. At the next iteration, we remove the edge with the second highest weight, and this will create other connected components, and so on, until, possibly, all nodes are in their own cluster (that is, all edges have been removed from $T$). 
There are several limitations of this algorithm. For example, it only considers the edges of the initial graph that are shared with $T$ (that is, it only considers the edges of the minimum spanning tree). It also requires the edges of the graph to be weighted. It does not require the number of clusters to be known in advance (like any other hierarchical clustering algorithm), but we still need to choose the optimal number of clusters (after the algorithm has terminated). We can do that in several ways. A way to do it would be to have a threshold value $t$ that is used to decide when we should stop removing edges from $T$: more specifically, we will keep removing edges from $T$ until the next highest weight is higher than this threshold $t$.
There are numerous applications of this type of clustering. For example, we might want to discover certain groups of people in social networks.
The paper Graph clustering (by Satu Elisa Schaeffer, 2007) provides a readable and quite detailed overview of this field. 
There are algorithms based on k-means that can also work on graphs. See e.g. Graph-based k-means Clustering: A Comparison of the Set Median versus the Generalized Median Graph (by Ferrer et al.).
"
Running 2 NEAT nets on the same observations,"
So i have been playing around with neat-python. I made a program, applying neat, to play pinball on the Atari 2600. The code for that can be found in the file test2.py here
Now based on that, I would like to do the same, but on a 2 player game. I have already set up the environment to play a 2 player game, which PONG using OpenAI Retro. 
What I have no clue how to do, is run 2 nets at the same time, on the same observation. The way that neat-python works, is you get the observation from a single function that goes through each genome and runs the environment.
How would you create 2 eval_genome functions that can take in the same observation real-time? This means that they train based off of the same images and environmenrs.
Help?
","['deep-learning', 'reinforcement-learning', 'python', 'neat', 'open-ai']","
I'm not familiar with neat-python, but I have implemented NEAT to do openai tasks. If there is a class for initializing a population, you could just use that and have 2 objects like population1 and population2 and call them in the same loop.
"
How to chose dense layer size?,"
I am fine-tuning a VGG16 model on 20 classes with 500k images I was wondering how do you chose the size of the dense layer (the one before the prediction layer which has a size 20). I would prefer not to do a grid search seeing how long it take to train my model.
Also how many Dense layer should I put after my global average pooling ?
base_model = keras.applications.VGG16(weights='imagenet', include_top=False)

  x = base_model.output
  x = GlobalAveragePooling2D()(x)
  x = Dense(???, activation='relu')(x)
  x = Dropout(0.5, name='drop_fc1')(x)
  prediction_layer = Dense(class_number, activation='softmax')(x)

I haven't see particular rules about how its done, are there any ?
Is it link with the size of the convolution layer ?
","['deep-learning', 'convolutional-neural-networks', 'hidden-layers']","
I am also wondering about this.
It must depend both on convolutional sub-network output size (N) and number of classes (M).
Maybe there are some rules of thumbs depending on (N, M).

Why 2 dense layers and not, say, 3 or 4 ?
Is it better to have all dense layers (except last) the same size ? or decreasing ? or increasing ? or pyramidal ?
Is it better to have small dense layers or larger ones with dropout between layers ?

And bonus question:

Should we use batch normalization between dense layers ?

"
LSTM is not converging,"
I am writing my first LSTM network and I would really appreciate if someone can tell me if it is right (the loss seems to go down very slowly and before playing around with hyper parameters I want to make sure that the code is actually doing what I want). The code is meant to go through some time series and label each point according to some categories. In the version I am putting here there are just 2 categories: 0, if the value of the point is 1 and 1 otherwise (I know its a bit weird, but I didnt choose the labels). So this is the code:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torch.optim as optim
import numpy as np
from fastai.learner import *

torch.manual_seed(1)
torch.cuda.set_device(0)

bs = 2

x_trn = torch.tensor([[1.0000, 1.0000],
        [1.0000, 0.9870],
        [0.9962, 0.9848],
        [1.0000, 1.0000]]).cuda()

y_trn = torch.tensor([[0, 0],
        [0, 1],
        [1, 1],
        [0, 0]]).cuda()

n_hidden = 5
n_classes = 2

class TESS_LSTM(nn.Module):
    def __init__(self, nl):
        super().__init__()
        self.nl = nl
        self.rnn = nn.LSTM(1, n_hidden, nl)
        self.l_out = nn.Linear(n_hidden, n_classes)
        self.init_hidden(bs)

    def forward(self, input):
        outp,h = self.rnn(input.view(len(input), bs, -1), self.h)
        return F.log_softmax(self.l_out(outp),dim=1)

    def init_hidden(self, bs):
        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),
                  V(torch.zeros(self.nl, bs, n_hidden)))

model = TESS_LSTM(1).cuda()

loss_function = nn.NLLLoss()

optimizer = optim.Adam(model.parameters(), lr=0.01)

for epoch in range(10000):  
    model.zero_grad()
    tag_scores = model(x_trn)
    loss = loss_function(tag_scores.reshape(4*bs,n_classes), y_trn.reshape(4*bs))
    loss.backward()
    optimizer.step()

    if epoch%1000==0:
        print(""Loss at epoch %d = "" %epoch, loss)

print(model(x_trn), y_trn)

The (super reduced in size) time series should be [1,1, 0.9962,1], with labels [0,0,1,0] and [1, 0.9870, 0.9848,1] with labels [0,1,1,0] and the batch size should be 2. I really hope I didnt mess up the dimensionalities, but I tried to make it in a shape accepted by the LSTM. This is the output:
Loss at epoch 0 = tensor(1.3929, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
Loss at epoch 1000 = tensor(0.8939, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;) 
Loss at epoch 2000 = tensor(0.8664, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;) 
Loss at epoch 3000 = tensor(0.8390, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;) 
Loss at epoch 4000 = tensor(0.8339, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;) 
Loss at epoch 5000 = tensor(0.8288, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;) 
Loss at epoch 6000 = tensor(0.8246, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;) 
Loss at epoch 7000 = tensor(0.8202, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;) 
Loss at epoch 8000 = tensor(0.8143, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)
Loss at epoch 9000 = tensor(0.8108, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)

(tensor([[[-9.0142e-01, -1.2631e+01],
          [-9.3762e-01, -9.6707e+00]],

         [[-1.3467e+00, -3.9542e+00],
          [-2.2005e+00, -7.6977e-01]],

         [[-2.4500e+01, -1.9363e-02],
          [-2.3349e+01, -6.2210e-01]],

         [[-1.0969e+00, -2.1953e+01],
          [-6.9776e-01, -1.8608e+01]]], device='cuda:0',
        grad_fn=<LogSoftmaxBackward>), tensor([[0, 0],
         [0, 1],
         [1, 1],
         [0, 0]], device='cuda:0'))

The loss doesnt go down too fast (I expected it to overfit and go really close to zero). The actual values are okish (the smaller one is always the right one), but they can be definitely improved. Can someone tell me if my code is doing what I want (and maybe suggest why is the loss still big - maybe I need a smaller LR?)
","['long-short-term-memory', 'convergence', 'pytorch']","
I tried to play with your code and found changing loss function to the cross_entropy alternate of negative log-likelihood makes the difference between 2000th epoch's loss and  9000th epoch's loss is greater about 0.2 alternate of 0.09
I also tried to change optimizer and learning rate but no loss didn't improve.
you can explore the modified code may help you with another idea
"
How do we classify an unrecognised face in face recognition?,"
If we have classified 1000 people's faces; how do we ensure the network tells us when it encounters a new person?
",['image-recognition'],"
Specifically for face recognition (and other identification algorithms) there are better approaches than using classifiers directly. 
Most identity recognition algorithms generate some kind of metric - typically an ""embedding"" of the original image into an abstract space i.e. a vector of real numbers. The space might be based on real-world biometrics e.g. normalised measurements of eye distance, eyebrow arch etc, which would be trained as a regression algorithm. The problem with this is that it requires a lot of labelled data, and the biometrics are not necessarily good at differentiating between identities. An alternative is to get the neural network to find the best abstract space for identities. You can do this if you have at least two images of each identity, and using triplet loss to train the network - the loss function directly rewards embedding the same identity close and different identity far apart.
Once you have an embedding, you no longer directly classify identities using the neural network. Instead, you base identity on distance between measured embedding and stored embeddings. This requires implementing a search function that looks at known embeddings and sorts by distance. 

how do we ensure the network tells us when it encounters a new person?

Embeddings don't solve this problem directly, but give a useful heuristic - distance in embedding space. Typically a maximum allowed distance is set as a cutoff to consider an image as showing a new identity. This is a hyperparameter of the model. This is an area that triplet loss helps with, since it is trained to make the distance as large as possible between images that show different identities. If it has generalised well during training, then it should ignore differences due to lighting, pose, makeup etc, but still be able to differentiate similar looking people. 
As the embedding is approximate, any such system may make mistakes, and needs to be carefully tested. The quality and quantity of training data are important, and it should match images used in production. But that is no different to the pure classifier, which must in addition be re-built and re-trained for every new class added.
Whether to use a more basic classifier or something like triplet loss is a question of scale - if the number of identities that need to be tracked is high, or the rate of change in identities is high, then embeddings trained on triplet loss (or similar) are more practical.
"
Is it possible to analyse a very large 3D input volume at once with a 3D CNN?,"
I would like to use a 3D convolutional network on a 2000x2000x2000 volume for segmentation. I know I can break the volume into chunks that can fit in VRAM, but I was wondering if there was a way to analyze the entire 3D volume at once. 
",['convolutional-neural-networks'],
How agent's reasoning skills can improve its reinforcement learning?,"
Agent can have reasoning skills (prediction, taking calculated guesses, etc.) and those skills can help reinforcement learning of this agent. Of course, reinforcement learning itself can help to develop reasoning skills. Are there research that explores this impact of reasoning and consciousness on the effectivenes of reinforcement learning. Or maybe people just sit and wait such skills to emerge during reinforcement learning?
","['reinforcement-learning', 'artificial-consciousness', 'reasoning']",
Skip-Gram Model Training,"
Suppose we want to predict context words $w_{i-h}, \dots, w_{i+h}$ given a target word $w_i$ for a window size $h$ around the target word $w_i$. We can represent this as: $$p(w_{i-h}, \dots, w_{i+h}|w_i) = \prod_{-h \leq k \leq h, \ k \neq 0} p(w_{i+k}|w_i)$$ where we model the probabilities of a word $u$ given another word $v$ as $$p(u|v) = \frac{\exp(\left<\phi_u, \theta_v \right>)}{\sum_{u' \in W} \exp(\left<\phi_{u'}, \theta_v \right>)}$$ where $\phi_u, \theta_v$ are some vector representations for words $u$ and $v$ respectively and $\left<\phi_u, \theta_v \right>$ is the dot product between these vector representations (which represents some sort of similarity between the words) and $W$ is a matrix of all the words.
In Skip-Gram Negative Sampling, we want to learn the embeddings $\phi_u, \theta_v$ that maximize the following: $$\sum_{u \in W} \sum_{v \in W} n_{uv} \log \sigma(\left<\phi_u, \theta_v \right>) +k \mathbb{E}_{\bar{v}} \log \sigma(-\left<\phi_u, \theta_{\bar{v}} \right>)$$

Question. How exactly does this work? For example, suppose $k=5$, the target word $w_i$ is $\text{apple}$ and we want to find
  $p(\text{pie}| \text{apple})$. Let $n_{uv} = 10$ (number of times pie
  co-occurs with apple). Then we sample $5$ random words $\bar{v}$ that
  did not occur with $\text{apple}$ and whichever term in the sum is
  bigger is the one we predict? For example, if the first term in the
  sum is larger than the second term then we would predict that
  $p(\text{pie}| \text{apple}) \approx 1$? Otherwise we predict that
  $p(\text{pie}| \text{apple}) \approx 0$? Is this the correct
  intuition?

Source. Here at around the 10:05 mark.
","['natural-language-processing', 'word2vec', 'word-embedding']","
Almost, but no. When you maximize that objective function, you do so by adjusting the parameters $\phi$ and $\theta$. After you're done with training, you can use your word embeddings for other NLP tasks. You don't, however, do any prediction directly from the skip-gram model. 
To maximize the first term, co-occuring words must have large inner products. That is, they must be ""similar"". To maximize the second term**, the randomly sampled words must have a small inner product with $\phi_u$. That is, they must be ""dissimilar"" to $\phi_u$. Moving these word embeddings around in the vector space to make some words similar and others not is the only thing that happens during skip-gram training. 
$$$$
** $\sigma(-x)=1-\sigma(x)$, so maximizing $\sigma(-x)$ is minimizing $\sigma(x)$
"
How can we encourage data analysts and decision-makers to adopt AI?,"
Nowadays, there is too much data for humans to work on alone, and it is very normal for data analysts to use AI techniques to treat and process these data so it can lead to a faster and more accurate result. But many data analysts and decision-makers still don't trust AI methods or techniques and are reluctant to use them. How can we encourage them to accept or prefer these AI solutions? 
For example, if AU gives advice to solve a problem, then decision-makers must trust the results and data analysts must trust the mechanism, so that decision-makers can be confident in the AI work and also data analysts can concentrate their activities on added value.
How can we encourage data analysts and decision-makers to adopt AI?
","['applications', 'social']","
Not all of the mistrust aimed at AI systems is unjustified, particularly when it comes to neural networks and other such systems that rely on large training data sets. There are a number of high profile cases, facial recognition being one that has often (understandably) received a lot of flak, where improperly configured training data has resulted in skewed and questionable results. 
If you want to foster more trust in the systems, it will require better tools for analyzing how they are approaching problems and reaching decisions, as well as determining if there are holes in the training data. It will require a community working in the field that gives a lot more thought and care to how they approach their training data and what unintended biases they may be introducing by forgetting something than has often been displayed presently. 
Of course, some people are just distrustful of new technology, but I think it's more interesting to address the more legitimate concerns. 
"
How can I calculate the shortest path between two 2d vector points in an environment with obstacles?,"
I have a 2D plane, with a fixed height and width of 10M. The plane has an agent (or robot) in the point $(1, 2.2)$, and an electric outlet in the point $(8.2, 9.1)$. The plane has a series of obstacles.

Is there an algorithm to find the shortest path between the agent and the goal?
And if the point has a fixed wingspan? For example, that the space between O and N is smaller than the agent, and then the agent cannot cross?
","['search', 'robotics', 'a-star', 'shortest-path-problem', 'path-finding']",
How does Hindsight Experience Replay cope with multiple goals?,"
What if there are multiple goals? For example, let's consider the bit-flipping environment as described in the paper HER with one small change: now, the goal is not some specific configuration, but let's say for the last $m$ bits (e.g. $m=2$), I do not really care if there is $1$ or $0$.
In the paper, there is section 3.2 multi-goal RL, where they mention an example with two-dimensional coordinates ($x$ and $y$), but they are interested only in the $x$ coordinate, so they only use the $x$ coordinate as a goal.
Applying this strategy to my example would result in cutting the last $m$ bits from the goal and only use the other bits. Is this logic correct?
Another approach I could think of would be to train with all possible goal configurations, as there are not many in my case. But this seems impractical as the number of goal configurations grows.
","['reinforcement-learning', 'deep-rl', 'state-spaces', 'hindsight-experience-replay']",
Consecutive frames can be discarded when training an SSD/YOLO?,"
Let's say I have a number of videos, and I want to train an SSD/YOLO (or FRCNN) to detect objects. In the case of a large amount of videos,  there will be a lot of frames extracted and transferred to images. Can you take, for example, only the fifth frame everytime and thus lower the amount of memory required? If the frames contain a similar info, we can skip some and not impact the results? This is mainly to train faster..
","['deep-learning', 'training', 'datasets', 'object-recognition']",
