Head,Body,Tags,First Answer
What do we actually 'approximate' when dealing with large state spaces in Q-learning?,"
I realized that my state space is very large in size. I had planned to use tabular Q-learning (Bellman equation to update the $Q(s, a)$ after each action taken). But this 'large space' realization has now disappointed me and I read a lot of stuff on the internet. I have the following confusions.
I saw the 'approximation' term for the 'large space' scenario (for example, in this Medium blog post). But what it is exactly? I can't reduce the states I have nor can I club together different states and update the Q values. So, what is it I should do when they say 'approximate'? If it is the $Q(s,a)$ we approximate, then won't we anyway do for each state $s$ as and when it is encountered? How does this help in a 'large space' scenario?
","['reinforcement-learning', 'q-learning', 'value-functions', 'function-approximation', 'continuous-state-spaces']",
Is there any simple example for volumetric data except from physics and medicine?,"
Recently I heard about the term volumetric data. The definition for volumetric data is as follows
#1: Definition

Volumetric data is typically a set S of samples $(x, y, z, v)$,
representing the value v of some property of the data, at a 3D
location $(x, y, z)$. If the value is simply a 0 or a 1, with a value
of 0 indicating background and a value of 1 indicating the object,
then the data is referred to as binary data. The data may instead be
multivalued, with the value representing some measurable property of
the data, including, for example, color, density, heat, or pressure.

#2: Some more details

A volumetric dataset consists of information at sample locations in
some space. The information may be a scalar (such as density in a
computed tomography (CT) scan), a vector (such as velocity in a flow
field), or a higher-order tensor (such as energy, density, and
momentum in computational fluid dynamics (CFD)). The space is usually
3D, consisting of either three spatial dimensions or another
combination of spatial and frequency dimensions.

In simple words, I can say that volumetric data is nothing but a three-dimensional collection of tensors.
The articles linked above contain some examples of volumetric data in the medical (and probably physics) domain.
Are there any other simple real-world examples for volumetric data other than the medical and physics domain and physics?
","['datasets', 'applications']","
Whenever a system is being modelled in three dimensions, you can be sure that tensors either can or are being used. Most of the systems I can think of are either simulated volumetric data, or a combination of real measurements and interpolated values.
CAD and BIM
Computer-Aided Design, or CAD, if quite commonly used in engineering. It's quite natural that, if they are modelling a mechanical system, that they need information regarding the materials they are considering to use to make the device; A little bit of googling showed that Autodesks Inventor computes inertia tensors and their mechanical simulation software computes stress tensors.
BIM is essentially CAD for the construction industry, but again, a lot of the same kind of information regarding the materials used is needed; Else, the building may collapse.
Geology
Modelling the stability of soil to construct a skyscraper may require 3D data. There is an old question at gis.stackexchange discussing software used to model geological systems. Of those, this one and this other one still seems to be up.
Meteorology
Most meteorological data, to the best of my knowledge, is two-dimensional. However, weather radar can be used to extract 3D data with information about precipitation and motion.
"
Do any practical deep learning algorithms deal with tensors containing non-real entries?,"
In deep learning, most of the applications are from text and images. Both text and images can be converted into a tensor of real numbers.
Other than both mentioned above, there may be some other real-world data used by Deep learning algorithms. Any algorithm in general takes tensor as input and gives tensor as output. As far as I know, tensors always consist of real numbers.
I want to know whether there are any practical applications that deal with the tensors containing non-real numbers. Is there any such possibility?
","['deep-learning', 'datasets', 'applications', 'tensor']",
Is it possible to train an RL agent using images?,"
I have an image which consists of a start and an end point, the journey has some obstacles which have to be avoided.

Is it possible to train an RL agent using such images to find the best path avoiding objects.
Or what algorithm should be used in order to find the best path avoiding objects, where the input is an image.

For example a picture of a person on a field track and there are obstacles in between from the start to the end point. I want to predict the series of actions that are required to reach the final position.
","['machine-learning', 'reinforcement-learning', 'deep-learning', 'computer-vision', 'deep-rl']","
A quick search about reinforcement learning applied to video games will lead you to countless tutorials that describe exactly what you're asking for.
With images the way to go is usually deep reinforcement learning. A convolutional neural network (or any other deep learning architecture) is used to process the image and compress it to a latent vector used as the ""environment"" seen by the agent.
Given that you can then apply whatever reinforcement learning algorithm (sarsa, q-learning, monte carlo tree search, etc.) to train the agent itself on a specific task, in this case reaching the end pixels area without hitting obstacles.
If you're familiar with python a good starting point is OpenAI Gym, and I would say in particular the Super Mario tutorial, since conceptually the game is basically the same as you're task of interest.
"
How rewards are playing role in Deep Q Network,"
I have started working on Reinforcement Learning, specifically DQN. And I have watched some interesting videos on it. However, I have some doubts about how the model works.
Let's say we are playing Atari Breakout where we have only 3 actions: left, stay still, right. We have 2 networks- the policy_net and the target_net (technically both are the same) and they give 3 outputs which are the q values for the 3 actions. During exploration we choose:
random.randrange(3)

and during exploitation, we choose:
argmax(policy_net output)

where the input of the policy net is the current state.
Now, during each timeline of each episode, we are storing current_state, action, reward, and next_state in storage that we will later randomly shuffle and use in training, which we call experience replay memory. During training, let's say we extract a batch of (current_states, actions, rewards, next_states). Now, we get current_q_val and next_q_val as:
current_q_values = policy_net(current_states)
next_q_values = target_net(next_states)

We use the following equation to find the loss:
$$q_*(s,a) - q(s,a) = \text{loss}$$
$$E[R_{t+1} + \gamma \max_{a'} q_*(s', a')] - E[\Sigma_{k=0}^ \inf \gamma^k R_{t+k+1}] = \text{loss}$$
And for that, we find which one from the next_q_val is the best one and that we call $\max q_*(s',a')$ (we already have $q(s,a)=\text{current_q_values}$).
Now my question is, which is $R_{t+1}$ here? As we are taking a random batch from the experience replay memory, we don't have any specific time here and we cannot calculate the $R_{t+1}$ from the time $t$. And if we simply use $q*(s,a) - q(s,a)$ or $\text{next_q_val} - \text{current_q_val}$, then where is the importance of the reward? I don't really understand how we are using the rewards in the training. I mean, where are we making sure the positive and negative influences of good and bad rewards respectively? The fact that the agent takes an action (randomly or from the policy_net) which then gives a reward, I don't understand how to use this reward in the loss function so as to influence how the agent should take action given a state.
","['reinforcement-learning', 'python', 'dqn']",
"What does the line of code ""self.buffer[-1] = observation"" do in this BufferWrapper class for DQN?","
So the code is related to using a buffer
class BufferWrapper(gym.ObservationWrapper):
    def __init__(self, env, n_steps, dtype=np.int):
        super(BufferWrapper, self).__init__(env)
        self.dtype = dtype
        old_space = env.observation_space
        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),
                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)

    def reset(self):
        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)
        return self.observation(self.env.reset())

    def observation(self, observation):
        self.buffer[:-1] = self.buffer[1:]
        self.buffer[-1] = observation
        return self.buffer

It is used to basically do some image processing so that the DQN is fed some transformation of the image. This link provides some higher-level logic behind some operations.
How can I actually understand what's the reason behind the code? Almost all repos have the exact same lines with no explanation (e.g. atari games GitHub repo).
My specific question is what is the purpose of the line self.buffer[-1] = observation?
In my case, my observation is a (7*1) array and I have to return that in an appropriate manner from the observation function.
The book has some mention of this class but I couldn't understand much from it
","['reinforcement-learning', 'deep-rl', 'dqn', 'gym', 'experience-replay']",
Bias equal to 1 and neuron output equal to -1 in neural networks,"
I have read that bias in neural networks is used to avoid situation in which output of neuron is equal to 0. But what if the same output is equal to -1 and we add 1 to it? Isn't it the same issue as in case of zero output and no bias?
","['neural-networks', 'bias']",
Can a system of If-Then rules be regarded as AI? [duplicate],"







This question already has an answer here:
                                
                            




If expert systems are a bunch of if-then-else statements, then how are they termed as AI?

                                (1 answer)
                            

Closed 1 year ago.



Expert Systems (ES) are regarded as AI. However, ES can be as simple as a system of If-Then rules. But AI seems like a big name for a set of (could be rather simple) If-Then rules. Is this indeed the case that certain systems of If-Then rules are regarded as AI?
",['expert-systems'],"
AI is not an objective definition. It's extremely broad, and perpetually up for debate. However, I would say most would agree that yes, any system of If-Then rules can be considered an AI.
"
"Why ""large set of training data"" is needed in Neural Network AI training?","
I often heard people saying, ""large set of training data is needed for producing an accurate AI"".
But when I looked for articles explaining backpropagations online, it all seems like you could get the job done by ""one single set of input"", as long as you repeat the process enough times.
So what's the ""large set of training data"" for!?
After the optimized set of weights was calculated from the first input, plug in the second input and repeat the process again?
Won't that ""screw up"" the result from the first input since it was ""tailored"" from it?
","['neural-networks', 'backpropagation', 'training-datasets']",
"For a task that searches for an image artifact within a picture, can existing tools can be used or do I need to design the process myself?","
I am familiar only with basic AI/NN concepts but never worked with any libraries/tools as tensor flow. Currently, I have a task for which AI might be ideal: detection of a certain image artifact in a picture (lets say I want to detect a black circular spot of a variable size). Because the spot can be very small or very large, I guess the NN would have to somehow process the whole picture and then proceed in smaller regions? Anyway, for such a task, do I need to learn more about machine learning or there are already tools that I could simply train (e.g. providing ""clear"" and ""stained"" image samples in their training sets) without worrying about internal details?
","['neural-networks', 'image-recognition', 'ai-basics', 'pattern-recognition']",
How to fine-tune GPT-J with small dataset,"
I have followed this guide as closely as possible: https://github.com/kingoflolz/mesh-transformer-jax
I'm trying to fine-tune GPT-J with a small dataset of ~500 lines:
You are important to me. <|endoftext|>
I love spending time with you. <|endoftext|>
You make me smile. <|endoftext|>
feel so lucky to be your friend. <|endoftext|>
You can always talk to me, even if itâ€™s about something that makes you nervous or scared or sad. <|endoftext|>
etc...

Using the create_finetune_tfrecords.py script (from the repo mentioned above) outputs a file with 2 in it. I understand that means my data has 2 sequences.
I could really use some advice with the .json config file. What hyperparameters do you recommend for this small dataset?
The best I came up with trying to follow the guide:
{
  ""layers"": 28,
  ""d_model"": 4096,
  ""n_heads"": 16,
  ""n_vocab"": 50400,
  ""norm"": ""layernorm"",
  ""pe"": ""rotary"",
  ""pe_rotary_dims"": 64,

  ""seq"": 2048,
  ""cores_per_replica"": 8,
  ""per_replica_batch"": 1,
  ""gradient_accumulation_steps"": 2,

  ""warmup_steps"": 1,
  ""anneal_steps"": 9,
  ""lr"": 1.2e-4,
  ""end_lr"": 1.2e-5,
  ""weight_decay"": 0.1,
  ""total_steps"": 10,

  ""tpu_size"": 8,

  ""bucket"": ""chat-app-tpu-bucket-europe"",
  ""model_dir"": ""finetune_dir"",

  ""train_set"": ""james_bond_1.train.index"",
  ""val_set"": {},

  ""eval_harness_tasks"": [
  ],

  ""val_batches"": 2,
  ""val_every"": 400000,
  ""ckpt_every"": 1,
  ""keep_every"": 1,

  ""name"": ""GPT3_6B_pile_rotary"",
  ""wandb_project"": ""mesh-transformer-jax"",
  ""comment"": """"
}

The problem is that, when I test the fine-tuned model, I get responses that make no sense:

","['natural-language-processing', 'tensorflow', 'hyperparameter-optimization', 'gpt', 'fine-tuning']","
Far from expert, but at least I can shed some light.
Your dataset is simply too small. Finetuning means you ""interfere"" with what the GPT-J model sees as important for the continuation of a prompt. Because that is what the model does; continue a prompt in a way that logically makes sense to what it has seen. Since your dataset is very small, especially since the sentences are not even full sentences, things go south.
For one, the create_tf_records.py (that I presume you used) script already filters OUT all senquences that are not long enough (I'm not sure, but this could be why you end up with 2 sequences?).
Then the model now has 500 sentences (and maybe even just 2) it sees as the MOST IMPORTANT data to use for its continuation (I think this is where the term ""fine-tuning"" comes from, not that it's a small influence on the data, it requires ""fine"" precision ğŸ˜‚). I had to find this out myself the hard way - there is very limited documentation - and I'm still not 100% certain, but quite sure about this part; you trained the model into a brain dead zombie that only slightly remembers all of the knowledge that is stored deeply embedded behind your dataset.
Now, this is a far for complete or really correct explanation, but I hope it gives you some understanding. If you understand more of it or better - please don't forget to let me know :)
What you probably could do to actually get somewhere:
Create a script, that based on the sentence you want to ""inject"", creates more data. So if your happy/positive sentence set is what you want to achieve, then create context around those sentences and spin them. Many times, combined in all kinds of ways. I would even suggest adding in some stuff like the binary contents of an image (look at CLIP, or the v-jax implementation of it by kingoflolz) if you know how to. Some chat generated by another model / chatbot - like GPT-2 or Clara - and just append your sentence at the end. Translate your sentences (deepl is recommended atm I think), mix those in. Pose a question before your sentences; ""what would be a good way to cheer someone up or let them know you appreciate them? [insert item from dataset]"" and spin those questions. Try to keep repeating yourself to a minimum (but different combinations are fine, small deviations also, but won't teach the model that much more). Paste entire news articles as though it's being quoted in a chat conversation and then put one of your sentences, also will work.
Those are my findings so far. Available for further discussing all of this; I'm also still figuring this thing out.
"
Which introductory courses (preferably video lectures) could I use to learn ML for applying ML to black hole simulations? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I am a Ph.D. candidate in High Energy Physics and my research involves numerical simulations and data analysis. I am interested to learn Artificial Intelligence and Machine Learning from the basics so that I could implement the same in my research. Due to the huge popularity of AI & ML one could easily find a large number of articles on the internet on the topic, but it seems to me that a search for a proper course (that would teach the basics at the beginner level) is nearly impossible for me.
It would be helpful if someone suggest me an introductory course (preferably video lecture) on 'machine learning' with the goal of applying the concepts to simulations of black hole environments.
","['machine-learning', 'reference-request']","
Cornell University offers lecture notes and videos of over thirty machine learning related courses in this link. CS4780 Introduction to Machine Learning in particular is a great resource (lecture notes, recordings).
The Visual Computing Group of TU Munich also offers lecture notes and videos of modern computer vision related tasks in this link. Introduction to Deep Learning in particular is a great resource (lecture notes, recordings). There are also courses on advanced topics of deep learning, image segmentation, 3D scanning, as well as a lot of publications on scene reconstruction, simulations and more.
"
Why is gradient descent used over the conjugate gradient method?,"
Based on some preliminary research, the conjugate gradient method is almost exactly the same as gradient descent, except the search direction must be orthogonal to the previous step.
From what I've read, the idea tends to be that the conjugate gradient method is better than regular gradient descent, so if that's the case, why is regular gradient descent used?
Additionally, I know algorithms such as the Powell method use the conjugate gradient method for finding minima, but I also know the Powell method is computationally expensive in finding parameter updates as it can be run on any arbitrary function without the need to find partial derivatives of the computational graph. More specifically, when gradient descent is run on a neural network, the gradient with respect to every single parameter is calculated in the backward pass, whereas the Powell method just calculates the gradient of the overall function at this step from what I understand. (See scipy's minimize, you could technically pass an entire neural network into this function and it would optimize it, but there's no world where this is faster than backpropagation)
However, given how similar gradient descent is to the conjugate gradient method, could we not replace the gradient updates for each parameter with one that is orthogonal to its last update? Would that not be faster?
","['machine-learning', 'optimization', 'gradient-descent']","
When dealing with optimization problems, a fundamental distinction is whether the objective is a (deterministic) function, or an expectation of some function. I will refer to these cases as the deterministic and stochastic setting respectively.
Almost always machine learning problems are in the stochastic setting. Gradient descent is not used here (and indeed, it performs poorly, which is why it is not used); rather it is stochastic gradient descent, or more specifically, mini-batch stochastic gradient descent (SGD) that is the ""vanilla"" algorithm. In practice however, methods such as ADAM (or related methods such as AdaGrad or RMSprop) or SGD with momentum are preferred over SGD.
The deterministic case should be thought of separately, as the algorithms used there are completely different. It's interesting to note that the deterministic algorithms are much more complicated than their stochastic counterparts. Conjugate gradient is definitely going to be better on average than gradient descent, however it is quasi-Newton methods, such as BFGS (and its variants such as l-BFGS-b) or a truncated method that are currently considered state of the art.
Here's a NIPs paper that says CG doesn't generalize well. There are similar results for quasi-Newton methods. If you want something better than SGD, you should look into a method like ADAM, which was designed for the stochastic setting. CG and ADAM both use information from past gradient directions to improve the current search direction. CG is formulated assuming that the past gradients are the exact gradient. ADAM is formulated assuming that the past gradients are gradient estimates, which is the case in the stochastic setting.
"
How many unique angles of an object do you need in your image training set in order to correctly classify it?,"
I'm interested in using ResNet-50 to classify images of objects for around 1000 unique classes. I'm wondering if there is any way to estimate how many unique angles I need in my training set to classify images that can be taken from any angle. For example, if for a given object I had 500 training images from directly the front and 500 training images from directly the top, I'd have 2 unique angles.
A model trained with only those 2 unique angles probably wouldn't be able to classify the same object if it was given a photo from the top right looking down.
Is there anyway to figure out how many unique angles I would need in my training set to classify images that could be taken from any angle? If I had 12 unique angles (top, bottom, front, back, left, right, front-left, front-right, front-top, front-bottom, back-left, back-right, back-top, back-bottom) would I then be able to classify images of any arbitrary angle?
To clarify, if I had 12 unique angles, that would mean I would have many photos from each of the 12 angles, but the 12 angles would all be exactly the same with no variation. I.E. top would be exactly a 90-degree angle towards the object on the Z-axis and 0-degree angles on the X and Y axis, for many photos.
","['deep-learning', 'computer-vision', 'residual-networks', 'training-datasets']","
[I wanted it to be a comment but it's too long :)]
I don't think it's a good approach to split point of views into a group of 12 angles. The main purpose of using neural net is to have model that is able to generalize the data. That means the perfect model will be able to recognize an object in every orientation. Your task is to create a model that will be able to do similar thing. In my opinion You should try to make your dataset as differential as possible with not only different point views but also different lightning, background etc. According to the ResNet paper, they evalueted the model on ImageNet 2012 dataset. It was 1.28mil images for 1000 classes. That means approximately 1280 images per class. I guess that's good starting point for You. During training You'll be able to see if that is enough, or if You need to get more data or use some data augmentation techniques.
"
"Can I apply reparametrization trick on ""any"" deep neural network?","
I came across the  ""reparametrization trick"" for the first time in the following paragraph from the chapter named Vector Calculus from the test book titled Mathematics for Machine Learning by Marc Peter Deisenroth et al.

The Jacobian determinant and variable transformations will become
relevant ... when we transform random variables and probability
distributions. These transformations are extremely relevant in machine
learning in the context of training deep neural networks using the
reparametrization trick, also called infinite perturbation analysis.

The trick has been used in the context of neural networks training in the quoted paragraph. But when I search about the reparametrization trick, I found it only or widely in training autoencoders.
In the context of training a traditional deep neural network, is the trick useful?
","['neural-networks', 'training', 'autoencoders']","
The reparameterization trick (also known as the pathwise derivative or infinitesimal perturbation analysis) is a method for calculating the gradient of a function of a random variable. It is used, for example, in variational autoencoders or deterministic policy gradient algorithms.
If you plan on working with models that involve random variables, you definitely need to understand what the reparameterization trick is.
You will also need to understand the other method to calculate gradients for functions of random variables, which is known as the likelihood ratio (also known as the score function or  the REINFORCE gradient).
If your definition of a ""traditional"" neural network does not involve random variables, then such a method is irrelevant.
"
Can AI be used for grading code copy exercises and adjust difficulty based on these scores?,"
I'm a senior in a bachelor Multimedia and Creative Technology. My experience is mostly full-stack web app development.
For my bachelor's thesis, I need to do research in a subject I have no experience in. I want to build an application where students can exercise HTML and CSS. Teachers can upload simple code pieces (e.g. h1, h2, and list with elements) with difficulty levels and students can try to copy these exercises with a code editor on the web with live preview.
My question:

Is it possible to use AI for grading these ""copies"", give the students scores, and, based on these scores, adjust the difficulty level so the next exercise is harder or easier?

And if so, could you put me in the right direction?


","['applications', 'research', 'algorithm-request', 'education']",
How can I estimate how many photos I need to train ResNet-50 for image classification?,"
I am working on a project where I have to classify around 1000 unique objects. I'm trying to plan how much training data I will need to collect. I was planning on using ResNet-50. Is there anyway I can estimate the amount of photos I should plan to collect ahead of time (assuming I will collect an equal amount of photos of each class)?
","['computer-vision', 'computational-learning-theory', 'training-datasets', 'sample-complexity']",
How to fix high variance of the returns on a 2d env?,"
I'm trying to train an agent on a self-written 2d env, and it just doesn't converge to the solution.

It is basically a 2d game where you have to move a small circle around the screen and try to avoid collisions with randomly moving ""enemy"" circles and the edge of the screen. The positions of the enemies are initialized randomly, at a minimum distance of 2 diameters from the enemy.
The player circle has $n$ sensors (lasers) that measure the distance and speed of the closest object found.
The observation space is continuous and is made of concatenated distances and speeds, hence has the dimension of $\mathbb{R}^{n * 3}$.
I scale the distances by the length of the screen diagonal.
The action space is discrete (multidiscrete in my implementation) $[dx, dy] \in \{-1, 0, 1\}$
The reward is +1 for every game step made without collisions.
I use PPO implementation from Stable Baselines, but the return variance just gets bigger over the training. In addition to that, the agent hasn't run away from the enemies even once. I tried even setting the negative reward, to test if he can learn the suicide behavior, but no results either.
I thought maybe it's just possible for some degenerate policies like going to the corner of the screen and staying there to gain big returns, and that jeopardizes the training. Then I increased the number of enemies, thinking that it will enforce the agent to learn actually to avoid the enemies, but it didn't work as well.
I'm really out of ideas at this point and would appreciate some help on debugging this.
","['reinforcement-learning', 'training', 'convergence', 'on-policy-methods', 'online-learning']",
Is it a good practice to split sparse from dense features?,"
I have a mixture of real (float) and categorical features to use as input in a neural network. I encode the categorical features using one-hot / multi-hot encoding.
If I want to use all the features as input what is usually/empirically the best practice:

Concatenating all the features - sparse one-hot/multi-hot vectors and float values features - in one vector which is part dense part sparse and using this as input, or

Splitting the sparse one-hot/multi-hot vectors from the dense features and using an extra separate layer for the sparse features to make them dense before concatenating them with the other already dense features.

Same as 2 but maybe using a separate layer for the dense features too so we concatenate ""embeddings"" instead of features and embeddings.


What, in your experience / opinion, should I do, trial-and-error aside?
","['neural-networks', 'deep-learning', 'feature-engineering', 'input-layer']",
Which AI algorithm to use for identifying API for a specific use from a list of APIs?,"
We have a legacy code solution in C#. We have to change the code so that it fetches internal data via APIs and not via DB calls.
E.g. if the current code GETS Payment object from DB, we have to replace the logic so that the code calls the GET PAYMENT API instead.
Since there are 100s of code files and multiple DB hits in a single file, doing this manually is not feasible.
I was thinking of building an AI-based tool that would take my code file as input and point me out where I would need to replace the existing code and suggest what API to call at that place.
I have never worked on AI and it would be great if anyone suggests which algorithm to refer for my tool and also how should I proceed with solving the above problem.
","['machine-learning', 'natural-language-processing', 'python', 'ai-basics', 'text-classification']",
Multiple labels for the same rectbox?,"
My goal is to identify the horse in a photo. I'm dealing with about 500 unique horses.
My feeling is that the best way to distinguish one horse from another is by its face. So I trained Yolov5 successfully to find faces at reasonable angles.
I'd like to take this a step further, and teach it to identify which horse's face it sees.
I'm new to this sort of thing (though not programming in general), so the way I assume I should approach this is to add an additional label like face_horsename, with the unique name for the horse (or really, a unique reference to a database of horses).
Is that the right approach? It seems like the Yolo file format doesn't allow for multiple labels for the same box, so my guess is I should just make 2 rectboxes that are identical, but both point to different labels.
Frankly, I'd like to take it even further and label the same thing with the type of ""blaze"" of the horse's face, and its proper name for the horse's color. So now I'm talking about 4 labels.
Is that the right approach (duplicate boxes with unique labels)?
","['machine-learning', 'image-recognition', 'object-detection', 'object-recognition', 'yolo']",
Are there any resources that introduce the basics of online machine learning?,"
Are there any resources (either books, articles, or tutorials) that introduce the basics of online machine learning?
For example, this website has nice lecture notes (from lec16) on some of the aspects. Or this book.
I can't seem to find many resources on this. I'm trying to understand the basics, not read research papers.
","['machine-learning', 'reference-request', 'online-learning']","
Duplicate of answer previously posted on DataScience.SE:
On-line learning algorithms trains new data as it arrives. It is often referred to as incremental learning or continuous learning as it trains continuous stream of data incrementally
As requested some resources in the form of books, tutorial, lecture notes, YouTube links, pdf documents along with available packages that support online learning algorithms are mentioned
BOOKS

Online Algorithms: The State of the Art
Online learning and Online convex optimization
Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems
Convex Optimization: Algorithms and Complexity
Introduction to  Online Convex Optimization
Introduction to Online Optimization

TUTORIAL

An Introduction To Online Machine Learning
A Simple Introduction to Online Machine Learning
Beginner’s Guide to Online Machine Learning
what is online machine learning
Online Machine Learning Wikipedia
Online learning simplified

LECTURE

Online Methods in Machine Learning
Theory and Applications
Coursera lecture on online machine learning

YOU TUBE

Introduction to online machine learning algorithms 
what is Online Machine learning 
Online Machine learning with creme

PDF

Online Learning:
Theory, Algorithms, and Applications

Online Learning: A Comprehensive Survey

14 chapter on-line learning algorithms in machine learning 

Utilisation of on-line machine learning for SCADA system alarms forecasting

Online Machine Learning Algorithms Review and Comparison in Healthcare


ONLINE LEARNING ALGORITHMS

Vowpal Wabbit : provides support for reinforcement learning and supervised learning

Creme

Scikit-multiflow

River - online machine learning python package that combines python packages creme and scikit-multiflow

LIBOL: A Library for Online Learning Algorithms

Quasi-additive Online Classification Algorithms

Game Theory, on-line prediction and boosting Freund and Schapire relates boosting algorithms to online learning(Adaboost)


"
How to code an $\epsilon$-soft policy for on-policy Monte Carlo control?,"
I was trying to code the on-policy Monte Carlo control method. The initial policy chosen needs to be an $\epsilon$-soft policy.
Can someone tell me how to code an $\epsilon$-soft policy?
I know how to code the $\epsilon$-greedy. In $\epsilon$-soft, there are inequalities in place of equalities which is the issue for coding the $\epsilon$-soft.
","['reinforcement-learning', 'implementation', 'monte-carlo-methods', 'on-policy-methods', 'epsilon-greedy-policy']",
Why does the activation function for a hidden layer in a MLP have to be non-polynomial?,"
Across multiple pieces of literature describing MLPs or while describing the universal approximation theorem, the statement is very specific on the activation function being non-polynomial.
Is there a reason why it cannot be a higher-order polynomial? Is it just an attempt to use the least complex solution or we really cannot use higher-order polynomial?
I can understand the reason for the non-linear, but I am clueless about the non-polynomial requirement.
","['neural-networks', 'activation-functions', 'universal-approximation-theorems']",
How should I read a deep learning paper?,"
I have a background in mathematics and I am accustomed to reading papers with lemma and proofs. When I see a deep learning paper, they seem to be of practical nature.
How can I improve my reading and understanding of deep learning papers?
To truly understand, should I have to implement the code? What is the best approach (if any)?
","['machine-learning', 'deep-learning', 'reference-request', 'papers', 'academia']","
Adding something to nbro answer, from my personal experience there are also some hints that can quickly tell you if you're dealing with a good machine learning paper, i.e. worth to read in its entirety or not.
In random order:

Clear contribution description: machine learning and artificial intelligence in general are both broad fields. A paper can be about several things, and it should be clear already from the title what the main contribution is: a new model architecture? A new pre/post processing step? New loss function? If it's not clear after reading the abstract there are high chances that this point will not be clear also after reading the whole paper.

Clear architecture/algorithm description and visualization: images and code snippets make a huge difference. As you pointed out, machine learning is mostly an applied field, hence having a code snippet or a clear list of implementation passages reduce the overhead of thinking about how to turn into code the math in an efficient way, and it also reduces the chances of making interpretation mistakes when the passages are not clear or give for granted. Since you're from a mathematical background you are probably experiencing the opposite feeling and wondering why there's not that much math inside. Point is that most machine learning papers are structured like experimental papers. You have hypotheses, not theorems to prove, you run experiments to test them and you describe the results at the end.

Code availability: unfortunately, still not such a common practice, making your code available is fundamental, not to make other people's life easier, but to grantee the reproducibility of the published results. Moreover, machine learning is characterized by many subtle and arbitrary choices, especially when it comes to hyperparameters, which are many times not reported on the real papers, and, when that's the case, looking at the code becomes the last resource to find that information.

Proper benchmarking: evaluating the impact of a new loss/architecture is always hard. Many papers just report tables with random scores like ""97% accuracy"", which means nothing when not compared to other models. A good paper always reports the state of the art (SOTA) scores and test the proposed improvements ON EXACTLY THE SAME DATA. Furthermore, a paper ideally should report mean scores over several training runs. Unfortunately, due to the expensive hardware and long time required to train only a single model, this is almost never done.

Short but clear SOTA analysis: it is super hard to stay on track with the state of the art when it comes to machine learning, since dozens of papers are published every month. For this reason, the literature research section should be concise and point to works that are as closest as possible to what is going to be described (and improved upon) in the paper, otherwise, you know you're reading a survey instead.


"
"In a Temporal Convolutional Network, how is the receptive field different from the input size?","
I'm playing around with TCN's lately and I don't understand one thing. How is the receptive field different from the input size?
I think that the receptive field is the time window that TCN considers during the prediction, so I guess the input size shall be equal to it.
According to the WaveNet paper, I cannot see a reason why it should be otherwise. I'm using TensorFlow with this custom library.
Please help me understand.
","['convolutional-neural-networks', 'recurrent-neural-networks', 'time-series']",
Which is more popular/common way of representing a gradient in AI community: as a row or column vector?,"
Consider the following remark about writing gradients from the chapter named Vector Calculus from the test book titled Mathematics for Machine Learning by Marc Peter Deisenroth et al.

Remark (Gradient as a Row Vector). It is not uncommon in the
literature to define the gradient vector as a column vector, following
the convention that vectors are generally column vectors. The reason
why we define the gradient vector as a row vector is twofold: First,
we can consistently generalize the gradient to vector-valued functions
$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ (then the gradient becomes
a matrix). Second, we can immediately apply the multi-variate chain
rule without paying attention to the dimension of the gradient.

The above remark implicitly says that there is no standard in writing the gradients. So, I can write a gradient of a scalar-valued multivariate function either as a column vector or as a row vector.
But, I want to know which is more common in the AI community?
","['machine-learning', 'papers', 'notation', 'gradient', 'calculus']",
Is Positional Encoding always needed for using Transformer models correctly?,"
I am trying to make a model that uses a Transformer to see the relationship between several data vectors, but the order of the data is not relevant in this case, so I am not using the Positional Encoding.
Since the performance of models using Transformers is quite improved with the use of this part, do you think that if I remove that part I am breaking the potential of Transformers or is it correct to do so?
","['deep-learning', 'natural-language-processing', 'transformer', 'positional-encoding']",
"In TD(0) with linear function approximation, why is the gradient of $\hat v(S^{\prime}, \mathbf w)$ wrt parameters $\mathbf w$ not considered?","
I am reading these slides. On page 38, the update for the parameters for the linear function approximation of TD(0) is given. I have a doubt regarding this.
The cost function (RMSE) is given on page 37.
My doubt is: why is the gradient of $\hat v(S^{\prime}, \mathbf w)$ with respect to parameters $w$ not considered?
I think the parameter update should be:
$$\mathbf w \leftarrow \mathbf w +\alpha [R + \gamma \hat v(S', \mathbf w) - \hat v(S, \mathbf w)] (\nabla \hat v(S, \mathbf w)- \gamma \nabla \hat v(S', \mathbf w))$$
Instead in the material it is given as:-
$$\mathbf w \leftarrow \mathbf w +\alpha [R + \gamma \hat v(S', \mathbf w) - \hat v(S, \mathbf w)] \nabla \hat v(S, \mathbf w)$$
Could someone please explain?
","['reinforcement-learning', 'value-functions', 'function-approximation', 'weights', 'temporal-difference-methods']",
Why are today's neural networks not modeled with probability theory?,"
In the paper The Perceptron: A probabilistic model for information storage and organization in the brain, Rosenblatt used the probability theory to model his perceptron.
My professor told me that today's neural networks are not modeled with probability theory anymore. Why is that?
","['neural-networks', 'history', 'perceptron', 'probability-theory']","
Your professor is wrong (or maybe you misunderstood what he wanted to say, or he did not explain correctly what he wanted to say). It may be a good idea to ask your professor for some clarification and tell him about the info given in this answer.
Probability theory is widely used to model problems in machine learning, including in the context of today's neural networks. There are many examples of research papers or books where you will see probabilities and probability distributions. For example, you can take a look at the neural machine translation paper.
More importantly, we often formulate the problem of learning (in the context of neural networks) as a minimization of an objective function, which is equivalent to the maximization of a likelihood of the parameters (given the data). It's also often the case that neural networks produce a probability vector (from the so-called logits, often, by using a softmax function).
There are many other (maybe clearer) examples of the use of probability theory to model problems in machine learning. For example, you can take a look at Bayesian neural networks, variational auto-encoders, or generative adversarial networks. You will see a lot of probabilities and probability distributions in these linked papers.
You may also be interested in this answer.
"
Why is there a 1 in complexity formula of uniform-cost search?,"
I am reading the book titled Artificial Intelligence: A Modern Approach 4th ed by Stuart Russell and Peter Norvig. According to the book, the complexity of uniform-cost search is as
$$
O(b^{1+\lfloor{C^*/\epsilon}\rfloor}),
$$
where $b$ is the branching factor (i.e. the number of available actions in each state), $C^*$ is the cost of the optimal solution, and $\epsilon > 0$ is a lower bound of the cost of each action.
My question is: Why is there is a 1 in the formula?
For example, suppose in the following tree, the red node is the initial state and the green one is the goal state, and two actions are needed to reach the goal state from the initial state. If the cost of both actions is equal to $\epsilon = 1$, so, $C^*$ will be $2$. Therefore, the complexity will be $O(b^{2})$. But, from the above formula, the complexity will be $O(b^{3})$.

PS. I know there is a similar question in stackoverflow and have read the answer. But there is a disagreement between the answers about the 1.
","['time-complexity', 'norvig-russell', 'uniform-cost-search']","
To be fair I struggle too, to see a reason for it to be included.
I'll try to argumentate: assume that the optimal path has length N and therefore you have costs $c_1,\dots,c_N$ all positive and bigger than some $\epsilon>0$. Such $\epsilon$ can't be bigger than the minimum between $c_1,\dots,c_N$.
Now, to need that $+1$ correction factor it should be possible that
$$\cfrac{c_1+\dots+c_N}{\epsilon}<N$$
Since $\epsilon<min\{c_1,\dots,c_N\}$ I could try to prove instead that
$$\cfrac{c_1+\dots+c_N}{min\{c_1,\dots,c_N\}}<N$$
But you can divide both members by N and multiply by $min\{c_1,\dots,c_N\}$ obtaining that
$$\cfrac{c_1+\dots+c_N}{N}<min\{c_1,\dots,c_N\}$$
On the left hand side you have the average of the costs, which of course can't be smaller than the minimum of the costs, therefore that has no solution. This means that even without the +1 factor you can't underestimate the length of the path to the optimal solution using $\lfloor C^\star/\epsilon\rfloor$.
On the flip coin by the way when you compute that $\lfloor C^\star/\epsilon\rfloor$ you're actually trying to find an upper bound for the length of the optimal path, so if you add +1 (equivalent to compute the ceiling instead of floor) you still get an upper bound. Also, since you're computing an $O(\circ)$ keep in mind that if a function is an $O(b^k)$ then it's an $O(b^{k+c})$ for every $c>0$ so although not necessary the book isn't technically wrong.
EDIT: Take all I said with a grain of salt, I'm a student, too and still have to take the AI exam
EDIT 2: I thought that another possible reason why you would expand (at most) b more nodes after reaching the depth of the optimal goal is that you perform the goal test the moment you extract the node from the frontier and not before insertion. This is necessary cause it grants us that when we extract a node from the frontier we reached it through the path with lowest cost from the initial state, but it will cause us to expand at most one more level before realizing a node in the frontier is indeed a goal.
"
Why do language models produce different outputs for same prompt?,"
For conventional 'Neural Networks', the weights simply act as a transformation in highly multi-dimensional space; for a forward pass, the output is always the same since there is no stochastic weighting component in the process.
However, in Transformers (self-attention based encoder-decoder type architecture to be specific) we get different outputs with the same prompts (assuming $T > 0$). This doesn't make sense to me because the set of weights are always static, so the probability distribution produced should be the same; this simple decoding should yield the same output.
However, in practice, we observe that it is not actually the case.
Any reasons why?
","['deep-learning', 'transformer', 'language-model', 'natural-language-generation', 'forward-pass']","
Language models produce a probability distribution over a set of words. You determine the next word by sampling from this distribution. So, determining the next word is stochastic even though the distribution is the same given the initial prompt.
"
Can a GIoU loss (generalized intersection over union) be used after an STN module (spatial transformer network)?,"
I have a model that uses an STN module for number detection and Mean Squared Error loss. But I would like to replace it for GIoU, because MSE doesn't take into account how much of the target area has been detected, only how close individual coordinates are close to the target. But I wonder if this makes sense. Has anyone tried it, or has some insight?
","['deep-learning', 'objective-functions', 'object-detection', 'jaccard-similarity']",
Masked Autoencoder Structure,"
In the following structure when we use MADE due to the constraints for making a masked autoencoder, it seems some inputs do not have any connection to the next layer, and also there is the output that does not have a connection to the previous layer!
Can someone clarify?

","['neural-networks', 'machine-learning', 'autoencoders']",
Why do we multipy context_size with embedding_dim? (PyTorch),"
I've been using Tensorflow and just started learning PyTorch. I was following the tutorial: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py
Where we try to create an n-gram language model. However, there's something I don't understand.
class NGramLanguageModeler(nn.Module):

    def __init__(self, vocab_size, embedding_dim, context_size):
        super(NGramLanguageModeler, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear1 = nn.Linear(context_size * embedding_dim, 128)
        self.linear2 = nn.Linear(128, vocab_size)

at self.linear1 = nn.Linear(context_size * embedding_dim, 128) why did we multiply embedding_dim with context_size? Isn't the embedding_dim input size? So why do we multiply it by the context size?
","['natural-language-processing', 'pytorch', 'word-embedding']",
How do Tensorflow models and YOLO differ in terms of training steps?,"
Can anybody explain how the training steps work for the Tensorflow Object Detection algorithms available on the Tensorflow 2 Detection Model Zoo? For instance, YOLOv5 cycles through epochs. As I understand it, one epoch is completed after all the training data passes through the algorithm. However, the Tensorflow models I just described are set up so they pass through a certain amount of training steps (several are optimized for 100,000 training steps, including some with 200,000 and 300,000 steps, depending on the algorithm).
What is the difference between epochs and these steps? Just trying to understand how the algorithm trains my data.
","['training', 'object-detection']",
Is it generally advisable to have a low dimensional action space in Reinforcement Learning?,"
In supervised or unsupervised learning, it is advised to reduce the dimensionality due to the curse of dimensionality in general.
Is this also generally advisable for the action space of reinforcement learning?
As far as I understand (and inspired by the answer here Is it possible to tell the Reinforcement Learning agent some rules directly without any constraints), you can reduce the dimensionality of the action space always to 1, meaning that you solely have 1 action. This can be done by using a mapping (e.g. in the step function of Open AI Gym).
Let's have a look at an example: We have a heating device that can heat 3 storages and we have a discrete action variable for all of them with 11 steps [0.0, 0.1, 0.2, ..., 1.0]. So we have

action_heatStorage1: [0.0, 0.1, 0.2, ..., 1.0]
action_heatStorage2: [0.0, 0.1, 0.2, ..., 1.0]
action_heatStorage3: [0.0, 0.1, 0.2, ..., 1.0]

In this case, we would have a 3-dimensional action space

action = [action_heatStorage1, action_heatStorage2,
action_heatStorage3]

However, it is also possible to combine the 3 actions  into 1 action variable ""action_combined"" of the size [11 * 11 * 11=1331] by just using a mapping of this one action into the separate 3 actions. For example like this:

action_combined = 0 --> action_heatStorage1 =0, action_heatStorage2
=0, action_heatStorage3 =0
action_combined = 1 --> action_heatStorage1 =0.1, action_heatStorage2
=0, action_heatStorage3 =0
action_combined = 2 --> action_heatStorage1 =0.2, action_heatStorage2
=0, action_heatStorage3 =0

...

action_combined = 1331 --> action_heatStorage1 =1.0, action_heatStorage2
=1.0, action_heatStorage3 =1.0

Is it generally advisable to reduce the dimensionality of the action space (Option 2), or to use multidimensional action variables directly (Option 1)?
I know that the is most probably not an answer that is valid for all problems. But, as I am relatively new to reinforcement learning, I would like to know whether in the theory of reinforcement learning there is a general recommendation to do something like this or not or whether this question can't be answered in general as it is something that totally depends on the application and should be tested for each application individually?
Reminder: I have already received a good answer. Still, I would like to remind you on this question to maybe hear also the opinion and experience of others regarding this topic.
","['reinforcement-learning', 'action-spaces', 'dimensionality-reduction']",
Is the VC dimension of a MLP regressor a valid upper bound on how many points it can exactly fit?,"
I want to calculate an upper bound on how many training points an MLP regressor can fit with ~0 error. I don't care about the test error, I want to overfit as much as possible the (few) training points.
For example, for linear regression, it's impossible to achieve 0 MSE if the points don't lie on a line. An MLP, however, can overfit and include these points in the prediction.

So, my question is: given an MLP and its parameter, how can I calculate an upper bound on how many points it can exactly fit?
I was thinking to use the VC dimension to estimate this upper bound.
The VCdim is a metric for binary classification models but a pseudo dimension can be adapted to real-value regression models by thresholding the output:
$$Pdim(\mathcal{G}) = {VCdim}(\{(x,t) \mapsto 1_{g(x)-t>0}:g \in \mathcal{G}\})$$
(from the book Foundation of Machine Learning, 2nd edition, definition 11.5)
where $\mathcal{G}$ is the concept class of the regressor, $g(x)$ is the regressor out and $t$ is a threshold.
The model is an MLP with RELU activations. As far as I understood on Wikipedia, it should have a VCdim equal to the number of the weights (correct me if I'm wrong).
So, the question is: how to practically calculate the pseudo-dim for the regressor given the VCdim? Does it make sense for the purpose that I want to achieve?
","['regression', 'multilayer-perceptrons', 'vc-dimension', 'sample-complexity']",
Can I flip a video to generate more data for action recognition?,"
There are 8 distinct action classes and around 50+ videos per class. I was wondering if flipping videos from the training set can be a good option to generate additional data. Is it?
","['computer-vision', 'image-recognition', 'data-preprocessing', 'action-recognition', 'video-classification']","
Probably flipping a video left/right will be OK and useful for your case.
When considering data augmentation approaches, you should think about two things that may prevent it working:

Could the augentation change the label? E.g. would a human looking at the augmented data still label it the same way?

Does the augmentation create data that is too different from expected later use?


So in your case for action recognition you should only be concerned (and maybe not add the left-right flipped videos) if:

The activity label would change depending if tasks were performed left-handed or right-handed.

There is lots of content in the videos (e.g. writing) that is unrealistic when flipped. This is why you should only flip left/right, not top/bottom, in most cases. However, top/bottom and even arbitrary rotation might be fine if your videos are a top-down view, so it depends on your specific case.


As an aside, it is also important to keep the original and augmented copy in the same part of the dataset (training, cross-validation or testing), because they are correlated - not doing so may cause a data leak that will prevent you measuring performance correctly. To play this safe, you should only augment training data, so that you don't risk measuring generalisation of your model against imaginary production data that could not occur in reality.
Other augmentations you might consider for video could be small rotations, random crops, and minor colour, contrast and brightness adjustments.
"
Would AlphaZero work just with a value network?,"
There is a nice post about the intuition why AlphaZero works.
One of the advantages of using a policy network in the games where a perfect simulator is available (such as chess) is to save computation time by not generating all subsequent moves and then evaluating them using the value network. Instead, we can only focus on the good moves given by the policy network.
However, besides the computation time savings of the policy network, are there any requirements why it needs to be used during training?
What if we would replace the computation of the policy network with this logic: generate all subsequent moves, evaluate them using value network, and create policy from these predictions. Would this still work?
I would appreciate any references where this topic is discussed.
","['reference-request', 'game-ai', 'monte-carlo-tree-search', 'game-theory', 'alphazero']","
The solution approach for the linked work Mastering the game of Go with deep neural networks and tree search is not only valid for the game Go but can also be used for other games, e.g. chess.
The policy network has the task to learn the rules of the game and is then improved only with the help of the value network by focusing on winning games.
If I understand you correctly, you want to evaluate all the moves using the value network and pick the best one?
For this you need rules - i.e. which moves are allowed to be made at all.
That's what I think you need the policy network for.
"
Does Seq2Seq decoder take a special vector or the weights of the last encoder cell as an output?,"
I'm reading Sequence to Sequence Learning with Neural Networks and there's a thing that I couldn't quite grasp.
Paper says the encoder outputs a vector to be fed to the decoder. More precisely

Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector

However, when I look at the diagram:

there's no such vector here. What I understand from this diagram is decoder RNN takes the weights of the last encoder cell as an input.
Which one is correct? can you explain?
Stanford notes put it as

The final hidden state of the cell will then become C

So, is there no vector?
","['recurrent-neural-networks', 'papers', 'seq2seq']",
"To assess the quality of the reconstructed images, which metric is more reliable: PSNR or LPIPS?","
I am training a model for image reconstruction. I used several metrics to assess the quality of the reconstructed images. LPIPS is decreasing, which is good. PSNR goes up and down, but the L1 loss and SSIM loss are increasing.
So, which metric should I care more about?
My datasets are Paris Street View and CelebA.
I'm not sure if the VGG that extracts features for LPIPS is reliable here or not.
","['computer-vision', 'image-restoration', 'lpips', 'psnr', 'ssim']",
"Given a dataset of people with and without cancer, should I split it into training and test datasets such that the same person is not in both?","
I have a database that contains healthy persons and lung cancer patients. I need to design a deep neural network for the binary classification problem (cancer/no cancer). I need to split the dataset into 70% train and 30% test.
How can I do the splitting? According to persons?
I think that splitting according to persons is correct since this will ensure that the same person will not exist simultaneously in both the training and the test subsets. This is reasonable since we are recognizing the disease, not the person. If images from the same person exist in both subsets, the problem will be easy, and not reasonable, from a practical point of view. Do you agree?
","['deep-learning', 'binary-classification', 'cross-validation', 'training-datasets', 'test-datasets']","
we are recognizing the disease, not the person.
If you're training a computer vision model with only images and no auxiliary information then a randomized sampling should be enough to prevent the model from over fitting on x-ray scans taken on the same person.
If images from the same person exist in both subsets, the problem will be easy, and not reasonable, from a practical point of view. Do you agree?
Only partially. The aim of a test set is to allow you to quantify how well your model will perform on a real use case. Which is close but not the same as making the problem as hard as possible in testing phase.
Also, it is not necessarily true that having 20 examples from the same person in the training set will lead to high accuracy on even 1 single test instance coming again from the same person. This is because those training instances might contain bias (like many tumors on the left side of the scan rather than the right side).
This is why I personally would go for the following splitting approach:

sample from the whole dataset a small group of people (for example top n people with fewest amount of scans). Let's call this set between groups test set.
sample per person (from the remaining data) 70% training instances and 30% test instances. Let's call it within groups test set.

During validation I would then perform the following checks:

metrics calculation (precision, recall, f-score, i.e the whole package) on the between group test performed as usual.
metrics calculation per person (or clusters of people, paying attention in keeping the clusters the same at every validation step) on the within groups test set.

The second check is the most interesting. Of course there will be some random variation in the performance, but if the model will start performing better on some specific clusters of people that might be a hint that the model is learning some confounding variables probably related to the presence of really similar scans or other bias. testing only on the between groups will not let you to draw that conclusion.
"
Proximal Policy Optimization for continuous control problem,"
I am using clipped PPO to train a neural network to act as the controller for steering an aircraft, and am finding that my networks aren't learning. The goal is to keep the aircraft flying to cover the distance, and the code is implemented using Pytorch. I am wondering if anyone who is more experienced would be willing to take a look at my implementation.
Aircraft and actor model
I have a flight simulation environment with the dynamical system of an aircraft, where the simulation is propagated through time in 0.1s increments. The states of the aircraft serve as inputs to the actor network (airspeed, pitch, heading, height), and the controls include pitch and roll angles.
The actor network uses a multivariate normal distribution to output two values that serve as the means for the pitch and roll angle controls. The variances are fixed. This way, the policy is stochastic and allows for exploration. I am using two hidden layers of 256 neurons each for both actor and critic networks, with learning rates of 1e-4. ReLu activation on the hidden layers and sigmoid on the actor's output, with the critic's output being linear.
Reward structure
I have tried two different reward structures:

A reward of 1 for every time step that the aircraft is in the air (with a reward of 0 in the terminal state, which is when the aircraft crashes to the ground or if it achieves the maximum flight time)
A reward of 0 for every time step, but a single reward computed at the terminal state that is proportional to the displacement of the aircraft over the simulation, and an extremely large penalty that is subtracted from this reward if the aircraft crashes.

Neither of these reward structures seemed to allow learning. I am wondering if simply having a reward of 1 at every time step, much like the gym cartpole environment is insufficient in getting the model to learn how to fly. I was expecting the network to figure out ways of controlling the aircraft to fly longer to obtain a greater total reward. For the second reward structure, I expected the advantage estimation to carry the large reward/penalty backward through the trajectory so that the actor would learn to avoid flying in certain ways that end up in crashing the plane.
PPO implementation
For the clipped PPO implementation, I am:

Resetting the flight simulation environment
Propagating the simulation until the aircraft crashes or reaches maximum flight time
Training the actor and critic networks every 20 time steps (2s of simulation)
Repeating this for a maximum number of simulations

For training, I take the 20 time steps, shuffle them, and divide them into 5 batches. Then I train the network on these batches, reshuffle the 20 time steps and create 5 new batches, and train again.
I repeat this process for a total of 4 epochs per learning cycle.
The fastest that the aircraft can reach the ground (crash) is 1.7s of simulation time (17 time steps), whereas I want the aircraft to fly ultimately for 10m (6000 time steps). I am thinking that it is too difficult to train a network to fly for such a long period of time because it would have to learn all the states leading up to that point.
Results
I found that training the algorithm typically resulted in extreme volatility in the test simulation's score (sum of all rewards at every time step). The score history would look like: (moving average of 100 scores in orange).
What I've tried

Changing the standard deviation of the normal distribution of the actor output so that there is less exploration (stability of the aircraft is quite sensitive to the particular control values).
Advantage normalization
Increasing training time to 10 000 simulations (~40 000 learning iterations since there are 4 epochs per simulation)
Testing the algorithm on the gym's cartpole environment (discrete actions). I was able to successfully train the actor in 200ish games.

If anyone has any other suggestions of things to try, it would be greatly appreciated.
","['deep-learning', 'proximal-policy-optimization']","
Training using only 20 timesteps at a time is far too small, especially when the goal will ultimately consist of episodes of length 6000. You definitely need to increase that substantially and that will probably solve your problem immediately. You might try something like simulate 5 episodes and then train on all timesteps in those 5 episodes.
If that still doesn't work, another thing you can do is try training the value function (critic) differently, e.g. use monte carlo returns or more steps in the temporal difference, and you can keep a memory replay for the value function as well.
"
Which multi-agent reinforcement learning algorithm can I use when there are two types of agents with different action spaces?,"
Most of the papers on multi-agent RL (MARL) that I have encountered have multiple agents who have a common action space.
In my work, my scenario involves $m$ numbers of a particular agent (say type A) and $n$ numbers of another type of agent. Here, the type A agents deal with a similar problem due to which they have the same action space, and type B deal with another type of problem and they have the same action space.
The type A agents are involved in an intermediary task that doesn't reflect in the final reward, and the final reward comes from the actions of type B agents. But the actions of type B are dependent on type A agents.
Any idea on what kind of MARL algorithm is suitable for such a scenario?
","['reinforcement-learning', 'algorithm-request', 'multi-agent-rl']",
What is input (and shape) to K/V/Q of self-attention of EACH Decoder block of Language-translation model Transformer's tokens during Inference?,"
Transformer model of the original Attention paper has a decoder unit that works differently during Inference than Tranining.
I'm trying to understand the shapes used during decoder (both self-attention and enc-dec-attention blocks), but it's very confusing. I'm referring to this link and also the original Attention paper
In Inference, it uses all previous tokens generated until that time step (say kth time-step), as shown in the diagram below and explained at this link.

Another diagram that shows self-attention and enc-dec-attention within decoder:

Question:
However when I look at actual shapes of the QKV projection in the decoder self-attention, and feeding of the decoder self-attention output to the ""enc-dec-attention""'s Q matrix, I see only 1 token from the output being used.
Let's assume 6 deocder blocks one after the other in the decoder stack (which is the base transformer model).
I'm very confused how the shapes for all matrices in the Decoder blocks after decoder-1 of the decoder-stack (more specifically decoder-block-2 decoder-3, decoder-4..., decoder-6 of the decoder stack) self-attention and enc-dec-attention can match up with variable length of input to the decoder during inference. I looked at several online material but couldn't find answer.
I see only the BGemms in the decoder's self-attention (not enc-dec-attention) using the variable shapes until all previous k steps, but all other Gemms are fixed size.

How is that possible? Is only 1 token (last one from decoder output) is being used for qkv matmuls in self-attention and Q-matmul in enc-dec-attention (which is what I see when running the model)?
Could someone elaborate how all these shapes for QKV in self-attention and Q in enc-dec-attention match up with decoder input length being different at each time-step?**

","['transformer', 'attention', 'machine-translation', 'language-model', 'encoder-decoder']",
How to arrange test dataset distribution for an imbalanced classification problem?,"
I have a dataset that contains 560 datapoints, and I would like to do binary classification on it. 400 datapoints belong to class 1, and 160 points belong to class 2. In the case of an imbalanced dataset like this, how to arrange the test dataset to get valid performance results? Should I keep the same imbalanced data distribution for the test set which is similar to the distribution of the training data, or arrange it in a way that half of the test points belong to the first class and the remaining half belongs to the second class?
","['machine-learning', 'data-preprocessing', 'binary-classification', 'cross-validation', 'imbalanced-datasets']",
How does this TD(0) off-policy value update formula work?,"
The update formula for the TD(0) off-policy learning algorithm is (taken from these slides by D. Silver for lecture 5 of his course)
$$ \underbrace{V(S_t)}_{\text{New value}} \leftarrow \underbrace{V(S_t)}_{\text{Old value}} + \alpha \left( \frac{ \pi(A_t|S_t)}{\mu (A_t|S_t)} (\underbrace{R_{t+1} + \gamma V(S_{t+1}))}_{\text{TD target}} - \underbrace{V(S_t)}_{\text{Old value}} \right) $$
where $\frac{ \pi(A_t|S_t)}{\mu (A_t|S_t)}$ is the ratio of the likelihoods that policy $\pi$ will take this action at this state divided by the likelihood that behavior policy $\mu$ takes this action at this state.
What I do not understand is:
Assume the behavior policy $\mu$ took an action that is very unlikely to happen under policy $\pi$. I would assume this term goes towards $0$.
$$  \frac{ \pi(A_t|S_t)}{\mu (A_t|S_t)}  = 0 $$
But, if this term goes to $0$, the whole equation would become the following
$$ V(S_t) \leftarrow V(S_t) - \alpha V(S_t) $$
This would mean we decrease the value of this state.
But this doesn't make any sense to me, if the 2 policies are very different, we gain little to no information. Therefore, I would assume the value would be unchanged instead of decreased.
What is my misconception here?
","['reinforcement-learning', 'temporal-difference-methods', 'off-policy-methods', 'importance-sampling']",
Is there any other (possibly less popular) approach to create AI apart from statistical methods?,"
From what I have gathered so far, an AI has some prior (stored in the form of some probability distribution), and, based on experiences/data, changes the distribution (via Bayes rule) accordingly. This idea seems intuitively correct, as humans do something similar: we have some prejudice about certain things and refine it further based on additional observations.
I am wondering if there is a different (possibly, non-probabilistic) setting for designing an AI.
","['machine-learning', 'reference-request', 'symbolic-ai', 'expert-systems', 'knowledge-based-systems']","
Yes, there is symbolic AI. This was the 'original' approach to AI, at a time when there was very little data and/or processing power available. The focus was on logic and calculus, not on machine learning, which was just in its infancy.
A lot of natural language processing was developed using grammar rules (which only later were learned from data).
There still is a lot of this around, but often it's now hybrid, where human-authored rules correct the systematic mistakes of statistical approaches.
Update: as an example the Teneo system for building conversational agents (aka chatbots) uses both pattern matching rules and machine learning for intent recognition. The (human-created) patterns are more precise, but sometimes lack breadth of coverage, which the ML part provides, which works as a fallback.
"
How to deal with Q-learning having low variance in predicted Q-values?,"
I have a neural network that takes the state (which contains a lot of data), and the possible action (which is very little data), and predicts the Q-value of the action. I am double Q-learning.
I've noticed that, given a particular state $s$, the neural network will predict nearly identical $q(s, a_i)$ for all actions $a_1, \dots, a_n$. As the neural network gets trained, this situation gets worse.
I think it is predicting the mean Q value of the state. Perhaps, the small amount of data that comprises the action is being drowned out by state input?
I've considered using softmax to predict the best action, but it seems like we have nearly an unlimited number of possible actions to take, and I don't want to hard-code them.
EDIT, More Detail: The state is represented by all the text detected by an OCR of a GUI program + an embedding of screenshot image of the current GUI state. While in comparison the action is just 1-2 words (the text or tooltip of the GUI element we are considering clicking on), and I'm concerned that this imbalance of representation of state vs action in the input could be causing the network to accidentally (nearly) ignore the action input.
","['reinforcement-learning', 'deep-rl', 'q-learning']",
Can an optimal policy have a value function that has a smaller value for a state than a non-optimal policy?,"
I'm starting to learn about the Bellman Equation and a question came to my mind.
A policy $\pi$ is optimal if the value $v_\pi(s)$ is greater or equal than the value $v_{\pi'}(s)$ for all states $s \in S$.
Why does this work?
Can't it be that the optimal policy thinks a state isn't that good and gives him a low value but perform best in comparison with other policies which have higher values for this state?
","['reinforcement-learning', 'value-functions', 'policies', 'bellman-equations', 'optimal-policy']",
Do I need to tune the hyper-parameters or more data if SVR model performs poorly?,"
I am using non-linear data to SVR and have tried tuning the hyperparameters and still have a poor model performance. Do I need more data or format the data for more suitable results?
I get similar performance for ANNs, decision tree, and random forest (slightly better) and even negative for polynomial regression.
The graphs for test data performance and training data also get a DataConversionWarning
You can find the data I used here
The plots I obtained look like this:
actual vs predicted for test data
actual vs predicted for training data
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import r2_score
#
#
dataset = pd.read_csv('Data.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values 
#
#
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)
#
#
regressor = SVR(kernel = 'linear', gamma='auto')
regressor.fit(X_train, y_train.ravel())
y_predict = regressor.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_predict.reshape(len(y_predict),1), y_test.reshape(len(y_test),1 )), 1))
#
#
r2_score(y_test, y_predict)
#
#
#model performance
fig, ax = plt.subplots()
ax.scatter(y_test, y_predict)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Actual')
ax.set_ylabel('Predicted')
#regression line
y_test, y_predict = y_test.reshape(-1,1), y_predict.reshape(-1,1)
ax.plot(y_test, regressor.fit(y_test, y_predict).predict(y_test))
ax.set_title('Final Prediction-R2: ' + str(r2_score(y_test, y_predict)))
plt.show()
#
#
#training data performance
fig, ax = plt.subplots()
ax.scatter(y_train, y_train_predict)
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Actual')
ax.set_ylabel('Predicted')
#regression line
y_train, y_train_predict = y_train.reshape(-1,1), y_train_predict.reshape(-1,1)
ax.plot(y_train, regressor.fit(y_train, y_train_predict).predict(y_train))
ax.set_title('Final Prediction-R2: ' + str(r2_score(y_train, y_train_predict)))
plt.show()

","['regression', 'performance', 'support-vector-machine', 'scikit-learn']",
Why isn't a target network used for the critic in on-policy actor-critic methods?,"
Based on my research, I've seen so many on-policy AC approaches that utilise a critic network to estimate the value function $V$. The Bellman equation for the value function is as bellow:
$$
V_\pi(s_t) = \sum_a \pi(a|s_t)\sum_{r, s'}(r+V_\pi(s'))P(s', r|s, a)
$$
It makes sense not to have a replay buffer due to the current policy in the formula and the fact that our approach is on-policy. However, I really do not figure out why no one uses a target network to stabilize the training process of the critic, like what we have in DQN, namely the variant published in 2015. Does anyone have an idea for that with probably a citation?
I know that DDPG uses a critic with a fixed target network, but be aware that it is a real off-policy actor-critic. By ""real"" I mean it is not due to importance sampling.
I have to mention that I can imagine something, but I'm not sure whether it is true or not. If we have a target network, it means we are trying to find a deterministic, optimal in the case of DQN, policy, while we are learning the current policy's data for the actor-critic case with the critic.
","['reinforcement-learning', 'deep-rl', 'dqn', 'actor-critic-methods', 'target-network']","
I can't give a conclusive answer, but here's an attempt, largely based on An Empathic Approach to the Problem of Off-policy Temporal-Difference Learning. With function approximation, updating $V_\pi$ for state $s_t$ due to a recorded transition may update state $s^\prime$. Lets say the value estimate for $s_t$ is 1 and the estimate for $s^\prime$ is 2 and we have a linear network, with $\gamma \approx 1$ and learning rate $0.1$.
Lets also say we have recorded a transition that gives 0 reward.
In the off-policy setting, when learning from this transition, first we update the value for $s_t$ to 1.1 (and $s^\prime$ to 2.2). The second time the transition is used we update to 1.21 (and 2.42). Repeated off-policy learning can basically cause the learning to diverge as updates get larger. A target network fixes the target value for a longer period, which may mitigate this - the target remains at 2 for a longer time which reduces the updates - but it's still vulnerable to this issue.
In the on-policy setting, once we reach $s^\prime$ we need to learn from a new transition that starts in $s^\prime$, and our current overestimation bias of the value of $s^\prime$ will impact the new error, directly, hopefully counteracting the bias. This is why there are much stronger convergence guarantees in the on policy case for function approximation and bootstrapping. There is some research into target functions for on policy actor-critic but I don't think it has been shown to be more effective, rather the opposite.
"
model and trained model parameters on CIFAR-10 [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I'm looking for different models (specifically ResNet18/20, ResNet32/34, VGG16, MobileNet and SqueezeNet) and their parameters after training (i.e., .pth file) that were trained on CIFAR-10 or CIFAR-100. I tried looking for them for hours and couldn't find anything. perhaps someone could refer me to a site which have trained models for CIFAR-x?
Thank you.
","['models', 'weights']","
You can find pretrained models on CIFAR-10 in this GitHub repository.
Also, for fun you can take any backbone trained on ImageNet from TorchVision models.
Just replace the classification part from the Linear layer, outputting 1000 classes to 10 or 100. Then you can freeze most of the layers of the network and fit just some bottom layers with the classification head (last nn.Linear).
Modern papers often use CIFAR-10 for transfer learning and you can find the parameters in the paper or discussion on GitHub like here.
"
How will MLOps and lifelong learning be complementary?,"
According to [1], in MLOps, continuous training is

a new property, unique to ML systems, that's concerned with automatically retraining and serving the models.

While lifelong/incremental learning mainly studies how to incrementally learn rather than retrain. [2]

Lifelong Machine Learning or Lifelong Learning (LL) is an advanced machine learning (ML) paradigm that learns continuously, accumulates the knowledge learned in the past, and uses/adapts it to help future learning and problem-solving.

I can see some links or conflicts between the two but cannot explicitly explain, and I asked an author in the second link about this issue and he said that the two are complementary. I wonder how will the two help each other? Or will this kill that?
","['deep-learning', 'comparison', 'incremental-learning', 'mlops']","
Lifelong learning and MLOps are indeed complementary.
Lifelong learning (LL) can be defined as the set of learning algorithms and models that can deal with more and more data and/or tasks without forgetting (completely) the previous one and (usually) without fully retraining the model with all data that you have available now. So, in LL, we attempt to mimic the way humans continually learn (different tasks) throughout their lives and transfer knowledge/skills acquired in one task to other tasks (e.g. from the task of walking to the task of running) [1].
MLOps is the application of DevOps to the machine learning or data science context. In other words, it is more a software engineering practice, where you define a pipeline (or sequence) of tasks that need to be done (semi-)automatically until and after you deploy your software. So, MLOps is just DevOps, but, in addition to the common practices in DevOps, like continuous integration and continuous deployment, you also have ways to deal with the ML part of the software, like automatic retraining as more data is available, and automatic evaluation of the (new) trained model on a test dataset.
They are complementary, because MLOps is concerned with the automatization of the data science process (i.e. collection and cleansing of data, training of the model with the new data, evaluation, deployment of the new trained and evaluated model to a production server), while LL is (only/usually) concerned with the development of learning algorithms and models that can cope with new tasks and data (without being completely reconfigured or retrained), so it has nothing or little to do with continuous integration and continuous deployment. In MLOps, given that you may (automatically) retrain the model as more data is available, you may think that this is a form of lifelong learning, but you completely retrain the model and you may not be able to cope with different tasks, but you could indeed use LL techniques in MLOps.
"
How DFS may expand the same state many times via different paths in an acyclic state space?,"
I am reading the book titled Artificial Intelligence: A Modern Approach by Stuart Russell and Peter Norvig (4th edition) and came across this sentence about depth-first search (page 79, line 12):

For acyclic state spaces it may end up expanding the same state many
times via different paths, but will (eventually) systematically
explore the entire space.

My question is: how this is possible? Can you please show me some examples?
","['search', 'state-spaces', 'norvig-russell', 'depth-first-search', 'state-space-search']",
What is the rigorous and formal definition for the direction pointed by a gradient?,"
Consider the following definition of derivative from the chapter named Vector Calculus from the test book titled Mathematics for Machine Learning by Marc Peter Deisenroth et al.

Definition 5.2 (Derivative). More formally, for $h>0$ the derivative
of $f$ derivative at $x$ is defined as the limit
$$\dfrac{df}{dx} := \lim\limits_{h \rightarrow 0}^{} \dfrac{f(x + h) − f(x)}{h}$$
The derivative of $f$ points in the direction of the steepest ascent of $f$.

You can observe that the derivate of a function is another function. If we consider derivative at a single point then it will be a real number that quantifies the rate of change of the output of the function with respect to the input.
There are two kinds of directions we need to focus on that are related to gradients. One is the direction pointed by a gradient and another one is the direction for moving our input parameters using a gradient. This question is restricted to the direction of the first kind.
We can treat the sign of the derivative at a particular point as the direction to move our input parameters. And I am not sure about the rigorous definition for the direction pointed by a derivative. I have thus doubts about the direction pointed by a gradient.
What exactly is the direction pointed by a gradient? and I want to know the formal definition for the direction of a gradient
I know about the direction that is given by gradient to move our parameters. But, I am not sure about the rigorous definition for the direction of a gradient vector.
","['definitions', 'gradient', 'calculus', 'derivative']","
If $u$ is a vector, the direction pointed by the vector is defined as $\dfrac{u}{\lVert {u}\rVert}$ where $\lVert \cdot \rVert$ is the 2 norm (euclidean norm).
"
What AI algorithm could I use to trap an agent in a game?,"
Imagine a game with grid size 10x10, there is a good guy and a bad guy and obstacles in the grid, i.e. essentially a maze. The goal of the bad guy is to find the good guy and trap him by erecting walls around him. The good guy is able to move in the maze using a simple random movement, only in 4 directions - up, down, right or left.
I've used the A* algorithm to find the shortest path to the good guy but am still unsure as to how to go about trapping the good guy in his own space?
","['game-ai', 'algorithm-request']",
How are partial derivatives calculated in a computational graph?,"
I am trying to understand how are partial derivatives calculated in a computational graph. I understand reasoning behind computational graphs and I am bold enough to say I understand how they work, at least on high level of understanding.
But what I don't know is how are partial derivatives itself computed or better said, how are they implemented in code.
I have checked few resources like this lecture slides from CS231N, this blog post or this blog post on TowardsDatascience. They explain graphs and how are expressions calculated in graph, but they don't explain how are partial derivatives derived (or I didn't understand from those explanations). For example, blog post from TowardsDatascience says:

Next, we need to calculate the partial derivatives of each connection between operations, represented by the edges. These are the calculations of the partials of each edge:

And then they show image with values of partial derivatives but they newer explain how are these equations actually calculated in implementation of graph.
Yes, okay, I know how to calculate these partial derivatives on paper and then hardcode them in my code, but I don't know how are they actually automatically computed and implemented in code of libraries like Torch or Theano.
Do they have some basic rules implemented in code, like, for example:
$$ \frac{\partial (a + b)}{\partial a} = \frac{\partial a}{\partial a} + \frac{\partial b}{\partial a} = 1 $$
and then decompose expressions until they reach basic elements/rules or is there another way that libraries like Torch, Theano or TF do it?
Or to put it in another way, if I have this code in Torch:
from torch import Tensor
from torch.autograd import Variable

def element(val):
    return Variable(Tensor([val]), requires_grad=True)

# Input nodes
i = element(2)
j = element(3)
k = element(5)
l = element(7)

# Middle and output layers
m = i*j
n = m+k
y = n*l

# Calculate the partial derivative
y.backward()
dj = j.grad
print(dj)

how does Torch know, that is, how does it compute internally that $ \frac{\partial y}{\partial j} = l \cdot 1 \cdot i = l \cdot i $?
","['neural-networks', 'backpropagation', 'pytorch']",
Best practice for handling letterboxed images for non fully-convolutional deep learning networks?,"
I'm working on a depth estimation network.  It has two outputs:

A relative depth map
A scalar for scaling the relative depth map into an absolute depth map.  This second output uses dense layers so we cannot use variable-sized input.

We are trying to handle two different dimensions (192x256 and 256x192). The current approach is to letterbox the image, meaning apply black on the image so that it comes out to 256x256. We decided on this approach instead of center-cropping images to 192x192 because we believe we may lose valuable data with cropping.
When using letterboxes, I see two paths:

Ignore the letterbox portions of the image in my loss function.  The loss function will only perform calculations on the original portion of the image.
Set a static value for the letterbox portion and include it as part of the loss.

Is #1 the correct approach?  The network will then be able to predict any depth value for the black letterbox portions without being penalized. I'm concerned with #2 about confusing the network between the letterbox portion and actual dark portions of images.
","['deep-learning', 'convolutional-neural-networks', 'data-preprocessing', 'data-labelling', 'fully-convolutional-networks']",
Are the domains of objective functions in AI always equals to $\mathbb{R}^D$ or subset of it?,"
Consider the following paragraph from the chapter named Vector Calculus from the textbook titled Mathematics for Machine Learning by Marc Peter Deisenroth et al.

Central to this chapter is the concept of a function. A function $f$
is a quantity that relates two quantities to each other. In this book,
these quantities are typically inputs $x \in \mathbb{R}^D$ and targets
(function values) $f(x)$, which we assume are real-valued if not
stated otherwise. Here $\mathbb{R}^D$ is the domain of $f$, and the
function values $f(x)$ are the image/codomain of $f$.

we can notice that the textbook is taking $\mathbb{R}^D$ as the domain for objective functions. I want to know whether it is valid in general cases.
Do the objective functions that we generally use in artificial intelligence have $\mathbb{R}^D$ as the domain?
I am guessing it would not be since the loss functions are generally defined on the datasets which can also have discrete attributes and hence the objective function cannot be defined on every point in $\mathbb{R}^D$. So, I am guessing that the correct form of the bold statement from the quoted paragraph is ""Here the domain of $f$ should be a subset of $\mathbb{R}^D$"" if we are intended to deal with a general case. Am I correct or is there any arrangement such as defining $f$ as zero where the function is not defined?
","['math', 'objective-functions']","
I think, that $\mathbb{R}^{D}$ is most natural choice in practical situations since many kinds of data can be described in a this way:

Image is 2d array $H \times W$ with each pixel taking value in $\mathbb{R}^{c}$ (say, $c=3$) or a continuous subset of $\mathbb{R}^{c} = [-1, 1]$
In the sequence modeling problems one gives an embedding vector to each token in some $\mathbb{R}^{k}$. There is no reason priori to put constraints on the embedding vector (probably one may want to clip norm of these vectors)

However, for the applications in physics, or any other fields, by the virtue of the task arbitrary input in $\mathbb{R}^{D}$ may not make sense, and one can be restricted to consider functions only on a certain manifold $M$ (sphere $S^d$, hyperbolic spaces)
In applications of Quantum Machine Learning input is a vector in a Hilbert space - system of quits, whatever.
You may be interested to consult this review.
"
Is a Conv2DTranspose the same as a full convolution?,"
I am currently creating a GAN model from scratch (following this tutorial: https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/) but I can't find out how to implement Conv2DTranspose from scratch.  Is a Conv2DTranspose the same as a full convolution?  If not, how would one implement it?
","['convolutional-neural-networks', 'generative-adversarial-networks', 'transpose-convolution']","
Con2DTranspose is an upsampling method used to increase the size of an image.
When we perform convolution, the size of the image decreases,
but in some scenarios, we want our image size to be the same as the input image size. Hence we use this convolution.
Here you will find Keras implementation on Conv2DTranspose
https://github.com/keras-team/keras/blob/v2.7.0/keras/layers/convolutional.py#L1093-L1394
"
Reference needed for neural networks finding solutions of PDE's,"
DL-PDE prescribes a way to feed a neural network data, which in turn comes up with a PDE of the form
$$u_{t}(t,x,y) = F(x,y,u,u_{x},u_{y},u_{xx},u_{xy},u_{yy},...) \hspace{0.5cm} (x,y) \in \Omega \subset \mathbb{R}^{2}, t \in [0, T]$$
I am looking for a way to feed a neural network the same data and also prescribe a number of other variables (say 2, making the total number of variables 3), and the neural network could come up with a system of PDE's comprising of three equations with three variables.
Any leads for this?
","['neural-networks', 'reference-request']",
Where do the characteristics of self-attention come into play in Linformer's proof that self-attention is low rank?,"
In Linformer's proof that self-attention is low rank in their paper, I don't see how it doesn't generalize to every matrix. They don't utilize any specifics of self-attention (the entire proof feels like it's in equation 20 utilizing JL, and I do not see where characteristics of self-attention come into play).
What am I missing?

","['papers', 'transformer', 'proofs', 'linear-algebra', 'linformer']",
Is there a way to update the neural network to fit the new data without the time required for retraining?,"
I built a basic neural network in MATLAB. The neural network classifies points on the X-Y axis system into two classes (0 and 1).
(I try to get the function that represents a shape from this photo)

Every so often the values ​​of the points change slightly and some of the points defined in class 1 become class 0, like in this photo.

Is there a way to update the neural network to fit the new data without the time required for retraining?
","['neural-networks', 'machine-learning', 'classification', 'weights', 'binary-classification']","
If I understand your question, you want it to generalize to be conditioned on an image. If this is correct, you can do this via inserting a separate portion of the model that takes the image as an input, and compresses it into a dense feature vector (can be done many ways- most common is probably cnn) and then merge with the other inputs (in your case x,y pairs) and have the model train on that end-to-end.
"
how to decide the optimum model?,"
I have split the database available into 70% training, 15% validation, and 15% test, using holdout validation. I have trained the model and got the following results: training accuracy 100%, validation accuracy 97.83%, test accuracy 96.74%
In another trial for training the model, I got the following results: Training accuracy 100%, validation accuracy 97.61%, test accuracy 98.91%
The same data split is used in each run.
Which model should I choose, the first case in which the the test accuracy is lower than the validation? or the second case in which the test is higher than the validation?
","['deep-learning', 'test-datasets', 'validation-datasets', 'validation']","
Testing each time on a test set is against the point of a train-val-test split. The reason test is important, is that you are only supposed to test on it when you think your model is good and ready with all final model and hyperparameter decisions made.
A good description can be found in this article: https://machinelearningmastery.com/difference-test-validation-datasets/
but to sum it up, test should be unbiased. The more you test against it, the more you bias the result.
"
Decreasing number of neurons in CNN,"
the conventional way of creating a CNN is using increasing number of neurons:
model = models.Sequential([
  layers.Conv2D(32,(3,3),activation='relu',input_shape=input_size),
  layers.MaxPooling2D((2,2)),
  layers.Conv2D(64,(3,3),activation='relu'),
  layers.MaxPooling2D((2,2)),
  layers.Conv2D(128,(3,3),activation='relu'),
  layers.MaxPooling2D((2,2)),
  layers.Conv2D(128,(3,3),activation='relu'),
  layers.MaxPooling2D((2,2)),
  layers.Flatten(),
  layers.Dense(128,activation='relu'),
  layers.Dense(64,activation='relu'),
  layers.Dense(1,activation='sigmoid')
])

where in this case, the number of neurons increase from 32, 64, to 128. However, i have also found a paper https://pubmed.ncbi.nlm.nih.gov/33532975/ that uses decreasing number of neurons i.e. 128, 64, 32 , as the network goes deeper. but in this paper, not much explanation was given on how the NN work in decreasing number of neurons. Does it mean decreasing number of neurons assumes that ""there are less number of important features to be captured as the network goes deeper"" ?
Question:
Can someone explain to me

how does the increasing number of neurons work
how does the decreasing number of neurons work and why this is not the common practice
referring to 2, what keyword should i find, in order to get articles or writing related to 2?

","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks']",
How to label unsupervised data for deep learning multi-classification,"
I have unlabeled credit card transaction data that has the following columns:
Transaction_ID     Frequency       Amount    Fees
   192831             21            829       23
   382912             14            920       24
   483921            839           24059      87

Eventually, I'd like to build a deep learning model(e.g. LSTM) that can tell me whether a transaction(row) has a ""high"", ""moderate"", or ""low"" risk. However, since the data is unlabeled, I believe I need to label the data first before I feed the data into the deep learning model.
For example, transactions that have small frequency and amount values like the first two rows need to be labeled as ""low (0)"" while transactions that have large frequency and amount like the last row should be labeled as ""high (2)"". If both frequency and amount have moderate values, the row will be labeled as ""moderate(1)"".
I wonder if it is okay to use other machine learning techniques such as K-Means clustering to label the data before I feed the data into the deep learning model. Is it okay to use one Machine Learning algorithm (K-means) to label the data and feed the same labeled data into another Deep Learning model (LSTM)? Or is it a bad practice? For example, if the first model (K-means) is biased, will that bias(error) be carried over from the first model to the second model (LSTM)?
If it is a bad practice to use two different ML technologies, what else can I do to label the data?
","['deep-learning', 'data-labelling', 'multiclass-classification']","
""is it okay to use another machine learning technology such as K-Means clustering to label the data?""
In computer vision there's an entire branch called automatic image annotation dedicated to this topic. And after a 2 sec search online I found a tutorial that suggest precisely what you want to try. So yes, on the paper it's ok to try, the real question though should be:
Will it work?
And the unfortunate answer is: no. Or to be less harsh: only with toy datasets, not real ones.
Moreover take a step back and think about the possible outcomes of the approach and their implications. If you train a k-mean clustering on your data you have 2 possible scenarios:

it works, then you you have an unsupervised model that does the job, why bothering training a supervise model that at best will perform as good as the one you already have?
it doesn't work, you made your life harder and you're at the same point where you started.

what else can I do to label the data?
Even though it's not the answer you want to hear:
Labeling is like removing a plaster from a wounded cut, you spend hours thinking how to do it without suffering to just realize at the end that the best way is to just do it.
Answers to the most predictable counter argument, the data are too many:

Rely on online crowdsourcing platforms: it seems that you already have rules to use as an annotation guideline. But it costs money.
Label just a part of the data, train, and repeat till you get a good model. You can also leverage active learning to optimize and make the labeling process smarter.
Dig the web till you find an annotated dataset.
Move to unsupervised learning, but for the training itself. At the very least you'll rely on theories and model designed precisely for unlabelled data without having to train several models with a cascade of performance drops.

As a final note, there are several reasons to just start labeling the data yourself, the main one being that you'll have control over the quality of the annotations. Many people underestimate or don't even know about the importance of inter annotators agreement metrics like Cohen's Kappa that estimate not only the quality of a dataset but also how hard for a human the task you want to automatize is. Which is extremely relevant, cause if for humans the best agreement score is 80% instead of 100% then there's no way you'll obtain more than that from a model, due to the personal biases introduced by the annotators in the data. And by knowing that, you'll be happy about your 0.8 f-score without wasting hours thinking what you did wrong and why the model is not performing better.
Hope this will be be somehow helpful and good luck with your data!
"
Are the existential dangers of AI exaggerated?,"
I understand some of the inherent dangers involved with AGI and advanced machine learning. While I can see some of the more low-level risks associated with AI coming to fruition (deep-fakes, biased algorithms, etc.), some of the more existential dangers seem far-fetched and lack empirical examples. As someone outside of AI development circles and without a background in comp-sci, I would like to know how realistic those working in the field of AI find these fears. Are there any convincing hypotheticals about the risks of a superintelligence?
I've read some Stuart Russel and have recently been watching Robert Miles videos on the topic. Both of these observers seem worried. Is this opinion widely held or is it seen as a bit extreme among developers?
Provably Beneficial Artificial Intelligence
by Stuart Russell
Robert Miles YouTube channel
","['ethics', 'superintelligence', 'ai-safety', 'deepfakes', 'algorithmic-bias']",
How do the scale of an embedding affects a downstream task?,"
I am currently training a neural network in a self-supervised fashion, using Contrastive Loss and I want to use that network then to fine-tune it in a classification task with a small fraction of the data with labels. I'm basing this project on the paper titled A Simple Framework for Contrastive Learning of Visual Representations
by Ting Chen et al.
My questions go oriented on a very specific thing that I think is causing me some problems. After finishing the self-supervised training I extract the embeddings of a bunch of data and I get the following. These stats are calculated using a flattening of all the embedding of all the bunch of data.
Mean: -23.090446
Std: 91.78753
Min: -710.24493
Max: 651.8682

What bugs me is that when I extract the embeddings in a ResNet, for example, I get much much lower values (the highest value in absolute terms don't usually go beyond 15), but I'm getting much larger values.
When looking for similarities with the use of these embeddings, it is something that looks like it works, but my question is that if it is something that might affect when adding another layer and using it for a classification task.
I have the feeling that something here is wrong but I can not spot exactly what it is.
Could you give me any advice on this?
","['computer-vision', 'embeddings', 'self-supervised-learning']",
Explainable AI for complex input features,"
I have a model for binary classification that includes 2 linear layers with RELU activation function and Sigmoid in the last layer. The input features are FastText word embedding, frequency, and statistical signals.
This model has a 93% f1-score and I want to add an explanation to this model but I don't know how can I start.
My question is which models or papers good for these complex input features?
I appreciate any advice to achieve this goal.
","['binary-classification', 'explainable-ai']",
CNN: Difficulties understanding backward pass derivatives,"
I have really quite hard difficulties to understand what is actually going on in the backward pass of a CNN.
I am currently focusing on these references:

https://towardsdatascience.com/forward-and-backward-propagations-for-2d-convolutional-layers-ed970f8bf602
https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/convolution_layer

At the moment, I only want to compute the gradient which is then passed to the next lower layer.
I tried to implement the last equation on this image (from [1]):
https://miro.medium.com/max/2400/1*K2K0tfxmAlyRlqqbj4z0Rg@2x.png
For me, this formula looks like 6 nested for-loops.
The first for the channel c, the second for the output height i, the third for the output height j, the fourth for the number of kernels f, the fith for the kernel height m and the sixth for the kernel width n.
Am I wrong? I tried to implement this but I always get an out of bounds error.
Someone here who doesn't mind going a bit more into detail?
Any tips are greatly appreciated
","['convolutional-neural-networks', 'backpropagation']",
Do Vision Transformers handle arbitrary sequence lengths the same way as normal Transformers?,"
Does ViT do handle arbitrary sequence lengths using masking the same way the normal Transformer does?
The ViT paper doesn't mention anything about it, so I assume it uses masking like the normal Transformer.
","['neural-networks', 'computer-vision', 'transformer', 'vision-transformer']",
What is the name of this letter $\mathcal{J}$?,"
What is the name of this letter $\mathcal{J}$ in the following deep learning equation? And what alphabet it is from?
$$\mathcal{J} = \frac{1}{m} \sum_{i=1}^m \mathcal{L}^{(i)}$$
","['deep-learning', 'notation', 'loss']",
Why does the number of input tokens to an LSTM have an impact on the convergence of Integrated Gradients?,"
Background
I am computing the attribution scores for a simple LSTM model using Integrated Gradients. This method defines the contribution of a feature to a model prediction by integrating over the gradients along a path between the input and a fixed baseline:
$$IG_i(x) = (x_i - x'_i) \cdot\int_{\alpha=0}^1 \frac{\partial F(x'+\alpha(x-x'))}{\partial x_i}d\alpha$$
A common way of measuring the quality of the generated attributions is via the completeness axiom, which states that:
$$\sum_i IG_i(x) = F(x) - F(x')$$
The key to computing the IG scores is the approximation of the path integral, which can be approximated via Riemann sum, or a similar interpolation method. In section 5 of the IG paper, it is stated that, in practice, between 50 to 300 interpolation steps are sufficient to obtain IG scores that converge to satisfy the completeness axiom.
Issue
I am now testing the IG attributions on a simple LSTM model (1-layer, 16 hidden units). For shorter inputs (<20 tokens), convergence is reached in a reasonable number of steps, and the approximation of the integral is stable. However, when the length of the input increases, I find that the integral approximation diverges when the number of interpolation steps is increased! This can be seen in the following plot (N.B. the y axis is logarithmic):

Question
My question is: why does the number of input tokens to the LSTM have an impact on the convergence of integrated gradients?
It is stated in footnote 1 of the IG paper that the completeness axiom depends on whether the model satisfies Lebesgue's integrability condition. It would surprise me, however, that increasing the number of input tokens would dissatisfy this constraint: would it be possible that the model has become too nonlinear for numerical integration to still work? If so, are there alternative numerical integration methods that could be used here, instead of Riemann approximations or the Gauss-Legendre quadrature?
","['neural-networks', 'deep-learning', 'long-short-term-memory', 'papers', 'convergence']",
"Is it possible to use stochastic gradient descent at the beginning, then switch to batch gradient descent with only a few training examples?","
Batch gradient descent is extremely slow for large datasets, but it can find the lowest possible value for the cost function. Stochastic gradient descent is relatively fast, but it kind of finds the general area where convergence happens and it kind of oscillates around that area.
Is it possible to use stochastic gradient descent at the beginning and find the way to a general convergence and then use batch gradient descent on only a few training examples out of the huge dataset to get even closer to the exact point of convergence?
I know that a model with a cost function that's a bit away from the lowest value for the cost function performs well in stochastic gradient descent, but assuming you want better results, will this work well?
","['machine-learning', 'gradient-descent', 'stochastic-gradient-descent', 'mini-batch-gradient-descent']",
Why would SARSA diverge (but not Expected SARSA or Q-learning)?,"
In figure 6.3 (shown below) from Reinforcement Learning: An Introduction (second edition) by Sutton and Barto, SARSA is shown to perform worse asymptotically (after 100k episodes) than in the interim (after 100 episodes) for larger values of alpha (alpha > 0.9). The graph is for the cliff walking gridworld example whose description is also given (from the paper by van Seijen et al).


As the image mentions, the image is taken from a paper by van Seijen and others titled ""A Theoretical and Empirical Analysis of Expected Sarsa"". In the image below from the van Seijen paper from Section VII A Discussion, the authors mention that the reason for the better interim performance of SARSA as compared to its asymptotic performance for larger values of alpha, is the divergence of Q-values. The authors, however, fail to mention the reason for the divergence.

What would be the reason that SARSA diverges but not Expected SARSA or Q-learning?
According to me, SARSA might have a higher variance than Expected SARSA, but it should behave, on average, the same as Expected SARSA.
Additionally, shouldn't Q-learning be at greater risk of diverging Q values since, in its update, we maximise over actions (and I have in fact seen a number of instances where there is a problem of diverging Q values in DQNs)?
The majority of papers I have looked at only talk about the problem from the function approximation perspective.
","['reinforcement-learning', 'q-learning', 'sutton-barto', 'sarsa', 'expected-sarsa']","
I think a useful piece of information to answer this question is a representation of the safe and optimal policies that can be learned on the cliff grid world. SARSA learns the safe path while Q-learning (and on the long run also Expected SARSA) learns the optimal path.
The reason lies in how the different algorithms select the next action.

""shouldn't Q-learning be at greater risk of diverging Q values since in it's update, we maximise over actions""
This is actually the reason why Q-learning doesn't suffer of divergence issues in this simple world. Choosing always the action that maximize the reward is what allows Q-learning to learn directly the optimal policy. And you can see it on the graphs, the learning rate alpha doesn't really affect the final performance in the 50k runs case.
The same logic applies to Expected SARSA. Remember that SARSA choose a random action, while Expected SARSA take the reward expected value into account, which makes Expected SARSA closer to Q-learning than SARSA itself.
-SARSA $$Q(S_{t}, A_{t}) + \alpha((R_{t+1}) + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_{t}, A_{t}))
$$
-Q-learning $$Q(s_{t}, a_{t}) + \alpha((r_{t+1}) + \gamma max_{a} Q(s_{t+1}, a) - Q(s_{t}, a_{t}))
$$
-Expected SARSA $$Q(s_{t}, a_{t}) + \alpha((r_{t+1}) + \gamma \sum_{a} \pi(a | s_{t+1}) Q(s_{t+1}, a) - Q(s_{t}, a_{t}))
$$
This still doesn't explain why SARSA fails with high learning rates though. To answer that we just need to combine in a different light what we already said:

SARSA is not designed to depend on high future rewards
SARSA prefers policies that minimize risks

Combine these 2 points with a high learning rate, and it's not hard to imagine an agent struggling to learn that there is a goal cell G after the cliff, cause the high learning rate keeps giving high value to each random move action that keep the agent in the grid. Unfortunately for the agent, moving randomly gives you rewards but it also increase the chances of falling into the cliff. A small learning rate on the other hand push down the value of each action that simply keep you in the grid, and gives more change to SARSA to learn that the action of going down in the cell up to the bottom right corner is actually the real deal.
Hope this gives a bit more clarity, I suggest to maybe try to use some of the umpteenth noteboooks out there to visualize the policies learned by the algorithms, which is much more insightful than simply looking at graphs, even though is understandable that authors focus on the latter.
"
Why exactly was previously believed that the deterministic policy gradient did not exist?,"
I'm reading the paper Deterministic Policy Gradient Algorithms, David Silver et al.
First of all, in the introduction, the author says that

It was previously believed that the deterministic policy gradient did not exist

But, I wonder why it is. The general version of the policy gradient theorem does not have restrictions about the policy $\pi$. So, if we choose the policy $\pi$ as a Dirac measure, that is, $\pi(\cdot | s) = \delta_{a}$ for some $a \in \mathcal{A}$, then it is exactly the notion of deterministic policy, so we can apply the usual gradient descent theorem.
Indeed, in theorem 2, they showed that deterministic policy gradient theorem and usual gradient theorem matches when it comes to zero variance. (In fact, I can't understand the statement rigorously, because the policy is something about probability ""measure"", and the variance is something about ""random variable"".) However, my below computation shows some contradiction.
Let $\pi(\cdot | s) = \delta_a$, a Dirac measure for some atom $a \in \mathcal{A}$. Following the notation of the paper DPG, a policy gradient theorem says
$$\nabla_{\theta}J(\pi_{\theta}) = \mathbb{E}_{s, a}[\nabla_{\theta}log \pi_{\theta}(a | s) * Q^{\pi}(s, a)] $$
A definition of integral shows $$\nabla_\theta J(\pi_{\theta}) = \int_{\mathcal{S}, \mathcal{A}} \pi_{\theta}(da|s) \rho^{\pi}{(ds)} \nabla_\theta log \pi_\theta(a|s) * Q^{\pi}(s,a).
$$
But since $\pi_\theta$ has only an atom at generic element $a \in \mathcal{A}$, so it is  same with $$\int_\mathcal{S} \rho^{\pi}(s) \nabla_\theta log \pi_\theta(a | s) * Q^{\pi}(s, a) = \mathbb{E}_s[\nabla_\theta log \pi_\theta(a|s) * Q^{\pi}(s, a)].$$
(NOTE : the last term seems the same with the first line of the equation, but we get rid of $a$ from the expectation, by fixing $a$ corresponding to the atom of $s$.) However, note that $log\pi_\theta(a|s) = 1$, since $\pi_\theta(a|s) = \delta_a(\{a\}) = 1$ as we defined! Thus, $\nabla_\theta log \pi_\theta(a | s) = \nabla_\theta 1 = 0$ so we reached that gradient vanishes..
However, clearly not.
Can anyone help me?
","['reinforcement-learning', 'deep-rl', 'papers', 'policy-gradients', 'deterministic-pg-theorem']",
Is the graph considered as overfit?,"
I have a training dataset of 2000 images and 500 images for validation. I have executed 50 epochs, however I realized that my graph seems to be different as my accuracy is smaller than my loss. I am not sure on whether my graph is considered as overfitting. If it is, are there other ways to resolve it? I am currently running on Jupiter notebook. Attached below is my code
Edit:
By the way, I have made some changes to my code, such as reducing the dropout to 0.2. I have uploaded the new graph output as shown below. Is it considered as normal?
Code
import numpy as np
import keras
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
import os
import cv2

Training_dir = './datasets/train'

train_datagen = ImageDataGenerator(rotation_range =15, 
                         width_shift_range = 0.2, 
                         height_shift_range = 0.2,
                         rescale=1./255, 
                         shear_range=0.2, 
                         zoom_range=0.2, 
                         horizontal_flip = True, 
                         fill_mode = 'nearest', 
                         data_format='channels_last', 
                         brightness_range=[0.5, 1.5])

train_gen = train_datagen.flow_from_directory(
    Training_dir,
    batch_size = 64,
    class_mode='binary',
    target_size=(150, 150)
)

Validation_dir = './datasets/val'

val_datagen = ImageDataGenerator(rotation_range =15, 
                         width_shift_range = 0.2, 
                         height_shift_range = 0.2,  
                         rescale=1./255, 
                         shear_range=0.2, 
                         zoom_range=0.2, 
                         horizontal_flip = True, 
                         fill_mode = 'nearest', 
                         data_format='channels_last', 
                         brightness_range=[0.5, 1.5])


validation_gen = val_datagen.flow_from_directory(
    Validation_dir,
    batch_size = 64,
    class_mode = 'binary',
    target_size = (150, 150)
)

imgs = os.listdir(r""C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\datasets\train\cat"")
imgs1 = os.listdir(r""C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\datasets\train\dog"")
imgs2 = os.listdir(r""C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\datasets\val\dog"")
imgs3 = os.listdir(r""C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\datasets\val\cat"")
        
    

for img in imgs:
    img=cv2.imread(r""C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\datasets\train\cat"" + ""\\""+img)
    x = img_to_array(img)
    x = x.reshape((1,) + x.shape) 

    i = 0
    for batch in train_datagen.flow (x, batch_size=1, save_to_dir =r'C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\Preview\train\cat', save_prefix ='cat', save_format='jpg'):
        i+=1
        if i>5:
            break

for img in imgs1:
    img=cv2.imread(r""C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\datasets\train\dog"" + ""\\""+img)
    x = img_to_array(img)
    x = x.reshape((1,) + x.shape) 

    i = 0
    for batch in train_datagen.flow (x, batch_size=1, save_to_dir =r'C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\Preview\train\dog', save_prefix ='dog', save_format='jpg'):
        i+=1
        if i>5:
            break
            
for img in imgs2:
    img=cv2.imread(r""C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\datasets\val\dog"" + ""\\""+img)
    x = img_to_array(img)
    x = x.reshape((1,) + x.shape) 

    i = 0
    for batch in val_datagen.flow (x, batch_size=1, save_to_dir =r'C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\Preview\Validation\dog', save_prefix ='dog', save_format='jpg'):
        i+=1
        if i>5:
            break
            
for img in imgs3:
    img=cv2.imread(r""C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\datasets\val\cat"" + ""\\""+img)
    x = img_to_array(img)
    x = x.reshape((1,) + x.shape) 

    i = 0
    for batch in val_datagen.flow (x, batch_size=1, save_to_dir =r'C:\Users\User\Documents\IM4483 Mini Project (Tang Soon Loong Jefferson_U1921181B)\Preview\Validation\cat', save_prefix ='cat', save_format='jpg'):
        i+=1
        if i>5:
            break



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Dense, MaxPool2D, Flatten, Dropout

'''
SRC : https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/
----------------------------------
STRIDES = The stridesparameter is a 2-tuple of integers, specifying the “step” of the 
convolution along the x and y axis of the input volume.
PADDING = Typically, we set the values of the extra pixels to zero 'valid' or 'same'
KERNEL_INITIALIZER = The kernel_initializercontrols the initialization method used to initialize
all values in the Conv2D class prior to actually training the network.

FLATTEN = Return a copy of the array collapsed into one dimension.
DROPOUT = dropout refers to ignoring units (i.e. neurons) during the training 
phase of certain set of neurons which is chosen at random. When created, the dropout rate can be specified to the 
layer as the probability of setting each input to the layer to zero. TO PREVENT OVER-FITTING.!

DENSE = A Dense layer feeds all outputs from the previous layer
to all its neurons, each neuron providing one output to the next layer. 
----------------------------------
'''




model = Sequential()
model.add(Conv2D(16, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal',input_shape=(150, 150, 3)))
model.add(Conv2D(16, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(MaxPool2D((2,2)))
model.add(Dropout(0.2))

model.add(Conv2D(32, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(Conv2D(32, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(Conv2D(32, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(MaxPool2D((2,2)))
model.add(Dropout(0.2))

model.add(Conv2D(64, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(MaxPool2D((2,2)))
model.add(Dropout(0.2))

model.add(Conv2D(128, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(Dropout(0.2))
model.add(Conv2D(128, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(Dropout(0.2))
model.add(Conv2D(128, (3,3), strides=(1,1), padding='same', activation='relu',kernel_initializer='he_normal'))
model.add(MaxPool2D((2,2)))
model.add(Dropout(0.2))

model.add(Flatten())
model.add(Dense(units=256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=1, activation='sigmoid'))


import tensorflow as tf
check_point_path = './best.h6'
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath = check_point_path,
    monitor = 'val_accuracy',
    save_weights_only=False,
    save_best_only=True,
    verbose=1
)

model.compile(optimizer = tf.keras.optimizers.Adam(0.0005,decay=1e-5),
             loss = 'binary_crossentropy',
             metrics = ['acc'])


tb_callback = tf.keras.callbacks.TensorBoard(log_dir=""logs/"", histogram_freq=1)




print('Num Params : ',model.count_params())


'''
VERBOSE = By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.
'''
model_history = model.fit(
    train_gen,
    epochs=50,
    batch_size=128,
    verbose=1,
    callbacks = [tb_callback],
    validation_data=validation_gen
)









%matplotlib inline

import matplotlib.image  as mpimg
import matplotlib.pyplot as plt

#-----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
Acc=model_history.history['acc']
Val_acc=model_history.history['val_acc']
Loss=model_history.history['loss']
Val_loss=model_history.history['val_loss']

epochs=range(len(Acc)) # Get number of epochs

#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot(epochs, Acc, 'r')
plt.plot(epochs, Val_acc, 'b')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend([""Training Accuracy"",""Validation Accuracy""])
plt.show()

#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
plt.plot(epochs, Loss, 'r')
plt.plot(epochs, Val_loss, 'b')
plt.title('Training and validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend([""Training Loss"",""Validation Loss""])
plt.show()

Epochs output
Num Params :  3273585
Epoch 1/50
32/32 [==============================] - 97s 3s/step - loss: 0.7618 - acc: 0.4925 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 2/50
32/32 [==============================] - 70s 2s/step - loss: 0.6916 - acc: 0.5330 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 3/50
32/32 [==============================] - 71s 2s/step - loss: 0.6946 - acc: 0.5250 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 4/50
32/32 [==============================] - 72s 2s/step - loss: 0.6911 - acc: 0.5205 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 5/50
32/32 [==============================] - 73s 2s/step - loss: 0.6949 - acc: 0.5095 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 6/50
32/32 [==============================] - 74s 2s/step - loss: 0.6933 - acc: 0.5170 - val_loss: 0.6933 - val_acc: 0.5000
Epoch 7/50
32/32 [==============================] - 74s 2s/step - loss: 0.6926 - acc: 0.5190 - val_loss: 0.6932 - val_acc: 0.5000
Epoch 8/50
32/32 [==============================] - 72s 2s/step - loss: 0.6936 - acc: 0.5055 - val_loss: 0.6933 - val_acc: 0.5000
Epoch 9/50
32/32 [==============================] - 74s 2s/step - loss: 0.6940 - acc: 0.5120 - val_loss: 0.6933 - val_acc: 0.5000
Epoch 10/50
32/32 [==============================] - 72s 2s/step - loss: 0.6920 - acc: 0.5265 - val_loss: 0.6935 - val_acc: 0.5000
Epoch 11/50
32/32 [==============================] - 73s 2s/step - loss: 0.6929 - acc: 0.4975 - val_loss: 0.6934 - val_acc: 0.5000
Epoch 12/50
32/32 [==============================] - 92s 3s/step - loss: 0.6935 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.5000
Epoch 13/50
32/32 [==============================] - 89s 3s/step - loss: 0.6924 - acc: 0.5115 - val_loss: 0.6935 - val_acc: 0.5000
Epoch 14/50
32/32 [==============================] - 72s 2s/step - loss: 0.6929 - acc: 0.5095 - val_loss: 0.6935 - val_acc: 0.5000
Epoch 15/50
32/32 [==============================] - 71s 2s/step - loss: 0.6928 - acc: 0.5175 - val_loss: 0.6940 - val_acc: 0.5000
Epoch 16/50
32/32 [==============================] - 71s 2s/step - loss: 0.6916 - acc: 0.5170 - val_loss: 0.6941 - val_acc: 0.5000
Epoch 17/50
32/32 [==============================] - 71s 2s/step - loss: 0.6922 - acc: 0.5205 - val_loss: 0.6945 - val_acc: 0.5000
Epoch 18/50
32/32 [==============================] - 71s 2s/step - loss: 0.6894 - acc: 0.5280 - val_loss: 0.6949 - val_acc: 0.5000
Epoch 19/50
32/32 [==============================] - 71s 2s/step - loss: 0.6887 - acc: 0.5205 - val_loss: 0.6956 - val_acc: 0.5000
Epoch 20/50
32/32 [==============================] - 72s 2s/step - loss: 0.6870 - acc: 0.5400 - val_loss: 0.6963 - val_acc: 0.5000
Epoch 21/50
32/32 [==============================] - 72s 2s/step - loss: 0.6864 - acc: 0.5380 - val_loss: 0.6974 - val_acc: 0.5000
Epoch 22/50
32/32 [==============================] - 74s 2s/step - loss: 0.6836 - acc: 0.5505 - val_loss: 0.6965 - val_acc: 0.5000
Epoch 23/50
32/32 [==============================] - 74s 2s/step - loss: 0.6722 - acc: 0.5640 - val_loss: 0.6903 - val_acc: 0.5280
Epoch 24/50
32/32 [==============================] - 72s 2s/step - loss: 0.6712 - acc: 0.5755 - val_loss: 0.6980 - val_acc: 0.5100
Epoch 25/50
32/32 [==============================] - 72s 2s/step - loss: 0.6709 - acc: 0.5735 - val_loss: 0.6946 - val_acc: 0.5180
Epoch 26/50
32/32 [==============================] - 74s 2s/step - loss: 0.6687 - acc: 0.5890 - val_loss: 0.7016 - val_acc: 0.5120
Epoch 27/50
32/32 [==============================] - 73s 2s/step - loss: 0.6626 - acc: 0.5825 - val_loss: 0.7043 - val_acc: 0.5060
Epoch 28/50
32/32 [==============================] - 72s 2s/step - loss: 0.6675 - acc: 0.5830 - val_loss: 0.7053 - val_acc: 0.5060
Epoch 29/50
32/32 [==============================] - 72s 2s/step - loss: 0.6580 - acc: 0.5895 - val_loss: 0.7074 - val_acc: 0.5100
Epoch 30/50
32/32 [==============================] - 72s 2s/step - loss: 0.6632 - acc: 0.5875 - val_loss: 0.7036 - val_acc: 0.5200
Epoch 31/50
32/32 [==============================] - 73s 2s/step - loss: 0.6705 - acc: 0.5685 - val_loss: 0.6877 - val_acc: 0.5400
Epoch 32/50
32/32 [==============================] - 73s 2s/step - loss: 0.6595 - acc: 0.5880 - val_loss: 0.6821 - val_acc: 0.5520
Epoch 33/50
32/32 [==============================] - 72s 2s/step - loss: 0.6751 - acc: 0.5600 - val_loss: 0.7052 - val_acc: 0.5020
Epoch 34/50
32/32 [==============================] - 72s 2s/step - loss: 0.6536 - acc: 0.6015 - val_loss: 0.6853 - val_acc: 0.5580
Epoch 35/50
32/32 [==============================] - 72s 2s/step - loss: 0.6549 - acc: 0.6010 - val_loss: 0.6675 - val_acc: 0.5860
Epoch 36/50
32/32 [==============================] - 73s 2s/step - loss: 0.6530 - acc: 0.6050 - val_loss: 0.7023 - val_acc: 0.5160
Epoch 37/50
32/32 [==============================] - 75s 2s/step - loss: 0.6559 - acc: 0.5910 - val_loss: 0.6965 - val_acc: 0.5380
Epoch 38/50
32/32 [==============================] - 73s 2s/step - loss: 0.6423 - acc: 0.6225 - val_loss: 0.6719 - val_acc: 0.5920
Epoch 39/50
32/32 [==============================] - 72s 2s/step - loss: 0.6426 - acc: 0.6135 - val_loss: 0.7164 - val_acc: 0.5260
Epoch 40/50
32/32 [==============================] - 73s 2s/step - loss: 0.6430 - acc: 0.6080 - val_loss: 0.6936 - val_acc: 0.5640
Epoch 41/50
32/32 [==============================] - 72s 2s/step - loss: 0.6440 - acc: 0.5980 - val_loss: 0.6894 - val_acc: 0.5720
Epoch 42/50
32/32 [==============================] - 72s 2s/step - loss: 0.6431 - acc: 0.6135 - val_loss: 0.7083 - val_acc: 0.5560
Epoch 43/50
32/32 [==============================] - 73s 2s/step - loss: 0.6344 - acc: 0.6205 - val_loss: 0.6800 - val_acc: 0.5860
Epoch 44/50
32/32 [==============================] - 72s 2s/step - loss: 0.6687 - acc: 0.5735 - val_loss: 0.6699 - val_acc: 0.5940
Epoch 45/50
32/32 [==============================] - 72s 2s/step - loss: 0.6344 - acc: 0.6190 - val_loss: 0.7070 - val_acc: 0.5620
Epoch 46/50
32/32 [==============================] - 72s 2s/step - loss: 0.6340 - acc: 0.6160 - val_loss: 0.7012 - val_acc: 0.5740
Epoch 47/50
32/32 [==============================] - 74s 2s/step - loss: 0.6424 - acc: 0.5985 - val_loss: 0.7255 - val_acc: 0.5340
Epoch 48/50
32/32 [==============================] - 73s 2s/step - loss: 0.6449 - acc: 0.5985 - val_loss: 0.7120 - val_acc: 0.5420
Epoch 49/50
32/32 [==============================] - 72s 2s/step - loss: 0.6437 - acc: 0.6120 - val_loss: 0.6708 - val_acc: 0.6140
Epoch 50/50
32/32 [==============================] - 73s 2s/step - loss: 0.6497 - acc: 0.6030 - val_loss: 0.7339 - val_acc: 0.5280

Graph output

","['neural-networks', 'machine-learning', 'tensorflow']",
2D models on 3D tasks (convolutions): simple replace?,"
2D tasks enjoy a vast backing of successful models that can be reused.
For convolutions, can one simply replace 2D operations with 3D counterparts and inherit their benefits? Any 'extra steps' to improve the transition? Not interested in unrolling the 3D input along channels.
Publication/repository references help.

Details
The input is an STFT-like transform of multi-channel EEG timeseries, except it's 3D. There's spatial dependency to exploit across all three dimensions. The full transform is 4D, but one of the dimensions is unrolled along channels, so data and transform channels are mixed. The transform itself has stacked complex convolutions and modulus nonlinearities (output is real).
I seek to reuse models like SEResNet, InceptionNet, and EfficientNet. The ""benefits"" of interest are mainly train viability (convergence speed, amount of required tuning) and generalization (test accuracy) - or alternative to latter, that blocks don't interact harmfully by e.g. assuming an inherently 2D structure.
","['convolutional-neural-networks', 'reference-request', '3d-convolution', '2d-convolution']",
Does there exist functions for which the necessary number of nodes in a shallow neural network tends to infinity as approximation error tends to 0?,"
The Universal Approximation Theorem states (roughly) that any continuous function can be approximated to within an arbitrary precision $\varepsilon>0$ by a feedforward neural network with one hidden layer (a Shallow Neural Network) with sufficient width.
I remember stumbling upon articles showing that for some functions the necessary number of hidden neurons tends to infinity as $\varepsilon$ tends to zero (similar to this one by A closer look at the approximation capabilities of neural networks), but I have not had success finding them again. Is my memory incorrect or is it merely my searching skills that are insufficient?
","['neural-networks', 'reference-request', 'feedforward-neural-networks', 'function-approximation', 'universal-approximation-theorems']",
"What is the analytical formula for ""Kaiming He"" probability density function?","
A probability density function is a real-valued function that roughly gives the density of probability at a particular value of a random variable.
For example, the probability density function of a normal random variable is given by
$$f(x) = \dfrac{1}{2\sigma \sqrt{2\pi}} e^{-{\LARGE(}\dfrac{x-\mu}{\sigma}{\LARGE)}^2}$$
Uniform Kaiming He probability distribution function is used for initialization of weights in Convolutional neural networks in PyTorch and the distribution function was initially mentioned in the research paper titled Delving Deep into Rectifiers:
Surpassing Human-Level Performance on ImageNet Classification by Kaiming He et al. I think.
What is the analytical formula for the Kaiming He probability density function?
","['convolutional-neural-networks', 'math', 'probability-distribution', 'weights-initialization']","
The activation function proposed by He et al. is not a new probability function of its own kind. It's an improvement over a previously proposed activation function now called Xavier or Glorot (even though it was named by the authors normalized activation in the original paper). The Xavier activation is also simply an activation function and not a new kind of probability distribution.
To answer the question, both functions simply sample values from either a uniform or normal distribution. What change is how the parameters that define the distributions are estimated.
Focusing on the Kaiming activation, since that's the question asked, the formulas used to determine the correct parameters of the uniform and normal distributions to sample values from are:

Uniform: $$\boldsymbol{\mathit{U}}(-\sqrt{6 / n_{j}}, +\sqrt{6 / n_{j}})   $$

Normal:
$$\boldsymbol{\mathbf{\mathit{N}}}(0, \sqrt{2/n_{j}})$$


nj is the generic notation used in the original paper, in other sources like the pytorch documentation the notation used is fan_in or fan_out. Without explaining the details, fan_in means number of hidden units of the input layers for the weight matrix being initialized (or number of rows of the weight matrix to initialize), while fan_out means number of hidden units of the output layers for the weight matrix being initialized (or number of columns of the weight matrix to initialize).
"
What is the difference between Probabilistic Graphical models and Graph Neural networks?,"
While going over PGMs and GNNs, it seems like both leverage the graph data structure. The former has been used to represent causal associations (among other things), while the latter has a varied set of applications. Do these techniques intersect?
","['comparison', 'graph-neural-networks', 'probabilistic-graphical-models', 'probabilistic-machine-learning']",
"Non-face ""deepfakes"" in videos","
Instead of changing faces (like James Bond to Putin) what if, given sufficient training data, I wanted to:

Remove or add some windows from a brick house?
Convert a glass of red wine to a glass of white wine?
Remove the infamous the Starbucks cup from Game of Thrones?

Although deepfakes are notoriously hungry to train, the pre-trained weights are plug-and-play.
Does there exist any equivalent for non-face objects?
","['computer-vision', 'algorithm-request', 'model-request', 'deepfakes']",
Is there any paper that shows that multi-channel neural networks are universal approximators?,"
Lately, I have been reading a lot about the universal approximation theorem. I was surprised to find only theorems about ""single-channel"" standard networks (multi-layer perceptrons), where all layers are 2D vectors and the weights can be represented in weight matrices.
In particular, this is no longer applicable in some convolutional network applications, where the layers tend to be tensors with multiple feature channels. Of course, one could construct an equivalent ""single-channel"" neural network from a multi-channel network by putting the weight matrices together in a certain way. However, one would then have sparse matrices as weight matrices with very many constraints on the matrix entries, so that the ""standard theorems"" would no longer be applicable.
Do you know of any papers that study the Universal Approximation Theorem for multi-channel neural networks? Or is there a way to derive it from one of the other theorems?
","['neural-networks', 'convolutional-neural-networks', 'reference-request', 'universal-approximation-theorems']",
Multi-armed Bandit in optimization on graph edges selection,"
I have the problem, which I described below. I wonder if there exists a class of multi-armed bandit approaches that is related to it.
I am working on computer networking optimization.
In the simplest scenario, we model the network as a graph with a circular node topology, similar to that seen in Chord (attached photo). Each node(vertex) can have a maximum number of $X$ active links (tunnels or edge) to other nodes at any given time. Then it can open, maintain, or close links (each operation has a cost associated with it). If there isn't a direct edge, traffic must be routed through neighboring nodes. What is the best link structure(optimal set of edges in the graph connecting nodes) in the underlying graph given the predicted traffic intensity matrix between the nodes?
Note: the optimal link structure should be recalculated on a regular basis to account for history (for example, it is worthwhile to keep a connection between two nodes open even though there is no traffic at the current time because it was generally a busy link in the past and the chance of using this link is high in future).

Estimation: Can multi-armed bandit be useful here?
","['reinforcement-learning', 'q-learning', 'optimization', 'multi-armed-bandits', 'contextual-bandits']",
Is there a notion of location in Transformer architecture in subsequent self-attention layers?,"
Transformer architecture (without position embedding) is by the very construction equivariant to the permutation of tokens. Given query $Q \in \mathbb{R}^{n \times d}$ and keys $K \in \mathbb{R}^{n \times d}$ and some permutation matrix $P \in \mathbb{R}^{n \times n}$, one has:
$$
Q \rightarrow P Q, K \rightarrow P K 
$$
$$
A = 
\text{softmax} \left(\frac{Q K^T}{\sqrt{d}} \right) \rightarrow 
\text{softmax} \left(\frac{R Q K^T R^T}{\sqrt{d}} \right) = 
R \ \text{softmax} \left(\frac{Q K^T}{\sqrt{d}} \right) R^T = R A R^T
$$
Without the positional embedding (learned of fixed), that breaks permutation symmetry to translational there is no notion of location. And with the positional embedding one introduces a notion - token $x$ is on the $k$-th position.
However, I wonder, whether this notion makes sense after the first self-attention layer.
The operation of producing the output (multiplication of attention by the value)
$$
x_{out} = A V
$$
outputs some weighted sum of all tokens in the sequence, the token at the $k$-th position now has information from all the sequence.
So, I wonder, whether the notion of position (absolute or relative) still makes sense in this case?

My guess is, that since Transformers involve skip connections, these transfer the notion of the location to the next layers. But this depends on the relative magnitude between the activation of the given self-attention layer and the skip-connections
","['transformer', 'attention', 'positional-encoding']","
Attention mechanism solves this problem by allowing the decoder to “look-back” at the encoder's hidden states based on its current state. This allows the decoder to extract only relevant information about the input tokens at each decoding, thus learning more complicated dependencies between the input and the output.
"
When can I call an entity a hyperparameter?,"
As per my knowledge, any entity that is learnable by a training algorithm can be called a parameter. Weights of a neural network are called parameters because of this reason only.
But I have doubts about the qualification of hyperparameter.
Hyperparameter according to my knowledge is an entity that needs to be learned outside of the training algorithm. But, a lot of entities can come into the picture if I want to follow this definition.
For example, the selection of the type of neural network, number of layers, number of neurons in each layer, presence of batch normalization layer, type of activation function, number of parameters, type of parameters (integer, float, etc.), number of epochs, batch size, type of optimizer, learning rate, etc., and I can able to list a lot of entities like this.
Is it okay to call anything that needs to be learned outside the training algorithm a hyperparameter?
","['machine-learning', 'definitions', 'hyper-parameters']","
In older machine learning literature the given definition of hyperparameters was explicitly the same used in Bayesian statistics, i.e.

a hyperparameter is a parameter of a prior distribution

For example, in Christopher M. Bishop's ""Pattern Recognition and Machine Learning"" (Springer, 2006), hyperparameters are introduced in the following paragraph (page 30)

Now let us take a step towards a more Bayesian approach and introduce a prior distribution over the polynomial coefficients $\mathbf{w}$. For simplicity, let us consider a Gaussian distribution of the form
$$
p(\mathbf{w} \mid \alpha)=\mathcal{N}\left(\mathbf{w} \mid \mathbf{0}, \alpha^{-1} \mathbf{I}\right)=\left(\frac{\alpha}{2 \pi}\right)^{(M+1) / 2} \exp \left\{-\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}\right\}
$$
where $\alpha$ is the precision of the distribution, and $M+1$ is the total number of elements in the vector $\mathbf{w}$ for an $M^{\text {th }}$ order polynomial. Variables such as $\alpha$, which control the distribution of model parameters, are called hyperparameters.

In modern machine learning literature though, the definitions became more operational. For example, in Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2016), we can read

Most machine learning algorithms have hyperparameters, settings that we can use to control the algorithm's behavior. The values of hyperparameters are not adapted by the learning algorithm itself (though we can design a nested learning procedure in which one learning algorithm learns the best hyperparameters for another learning algorithm).

So, there is room for interpretation, even though I personally find more technical and precise the old reference to Bayesian statistics. It is clear from that definition that every variable not belonging to the parameters used in the prediction phase but only during training are indeed hyperparameters. Moreover, it is clear that the choice of hyperparameters affects the distribution of learned parameters once the model training reaches convergence.
To elaborate a bit more on the modern definitions, what I don't like about the example taken from Deep Learning is the lack of further explanation about the meaning of ""model behavior"". Does it refer to weight updating during training? Final metrics score? Both and more? In other words, what are hyperparameters supposed to affect? Of course, these loose definitions do not stop machine learning practitioners from using hyperparameters in the right place, but no surprise about doubts emerging like this question.
"
How should I define the reward function for a stock trading-like game?,"
Problem setting
Consider a game like trading a stock

At each step, the agent can buy / sell a stock.
Trade is a pair of buy and sell actions.
We can set the reward for the sell action as the profit made in this trade. Although this might be misleading, since whether a trade is good or not depends on both buying and selling timing.
We don't know the reward for buy, whether a buy is good or not depends on when you sell.

Questions

How should the reward scheme be for a game like this? i.e., whether one action is good or not depends on other actions taken before it?
Can we set a scheme like this: set the reward to be zero at each step, no matter it is a buy or a sell. Only give the reward at the end of the game, say after 1000 steps, and the reward, in this case, is the total money made?

","['reinforcement-learning', 'game-ai', 'reward-functions', 'reward-design', 'algorithmic-trading']","

how should the reward scheme be for a game like this? i.e., whether one action is good or not depends on other actions taken before it?

The reward scheme should always be a ""natural"" representation of the goals of the agent. If the goal of your agent is to make a profit, then the reward should be the amount of profit*.
It is important to RL, and becomes critical for rewards that may be delayed, that the state representation captures all the information that impacts those rewards. That means representing the direct effects of actions, so that they can be tracked. In the case of a trading bot, it will be important at a minimum that the state includes the current portfolio. If the agent can access data about the amount and value of stock it currently holds, it has a chance to predict likely profits later.

can we set a scheme like this: set the reward to be zero at each step, no matter it is a buy or a sell. Only give the reward at the end of the game, say after 1000 steps, and the reward, in this case, is the total money made?

You can, but that looks like it is making the problem harder than it needs to be.
I would naturally set the reward to be the profit on each transaction (including negative reward for buying stock). The agent should be able to use that to figure out long term rewards, and will learn faster if it is given this more direct feedback. RL algorithms are well-suited to figuring out the need to invest at an earlier time in order to make profit later. If there is also a time horizon, that should be factored into the state - the agent should be made aware that the game will end in so many time steps otherwise it may end up holding stock instead of selling it in time to meet the final evaluation.
One important caveat is that real trading problems are hard, because the real environment is highly variable and competitive. The competition includes intelligent people with advanced qualifications in statistics. The chances of making a trading bot that is successful in a real trading environment are low.

* Sometimes this ""natural"" rerpresentation is too sparse, and you may want to look into reward shaping. That is adding interim rewards or spreading out an existing reward, to assist the learning agent. For a stock trading game for instance, it may help to allocate some reward for gaining ownership of a stock, perhaps the nominal resale value minus the fees for selling it.
However, where it is possible to stick with the simplest direct interpretation of the agent's goals, then you should do that. Reward shaping comes with the potential risk that the changes you have made will impact what it means for the agent to perform optimally.
"
Why aren't 3d printers and robotic arms already used to create the first versions of self-replicating machines?,"
The ability to create self-replicating machines can give some very useful benefits. So what is the problem with creating this type of stuff?
Let's say we have two pieces of equipment - 3d printers and robotic arms. These items are already available and are easy to create.
It looks like they are enough to create self-replicating machines. 3d printers are able to print any details for arms and printers. Robotic arms are able to assemble other arms and printers. Both equipment items are able to create almost any other kind of stuff.
So we need only one set of 3d printers and arms with a basic program to start the process. The more sophisticated programs can be added later to create almost any type of equipment from design. If there are enough rough materials, this process can be scaled indefinitely and allow to construct, gather resources, etc.
So, what is the problem with that scheme? Why is is not used already yet everywhere?
","['automation', 'self-replicating-machines']","
The problem is accuracy degrades exponentially and no 3d printed part today can accurately fix the accuracy issue.
The other issue is, to make modern things requires a ridiculous amount of specialized industry.
What your proposing will only work if we can 3d print at a tiny, tiny scale. If you can reliably print semiconductors, then this will be viable.
"
How is the VAE related to the Autoencoding Variational Bayes (AEVB) algorithm?,"
I am familiar with the variational autoencoder, but not totally clear on what exactly the AEVB is.
In the original VAE paper (by Kingma and Welling), he uses both the terms variational autoencoder and autoencoding variational Bayes.

For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm, we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint.

And then in section 2, the paper talks about the SGVB estimator and the AEVB algorithm.
Then in section 3, it says that the VAE is an example.
So, is the VAE a special application of AEVB?
","['comparison', 'terminology', 'papers', 'variational-autoencoder', 'aevb-algorithm']",
"Generating a dataset from data with ""assumed"" lables","
I've got a task similar to the following:
Out of x amount of people, I need to predict, who could be a good athlete and who not. The thing is, I don't have data on the athletic performance of those individuals.
So I was thinking of looking into assumptions/traits:
Most of the NBA players are tall. If someone of a random amount of people is tall, it could be a good basketball player.
In contrast, a tall person would not be a good jockey.
The same goes for age - 3 years or 90 years old might not make a world-performing athlete, etc.
How would I best build a dataset for this problem? Which features do I need to add to the dataset in order to make good predictions about athletic performance?
","['datasets', 'prediction', 'unsupervised-learning', 'data-science', 'feature-engineering']",
"Apart from ontologies, which other methods for knowledge representation are there in Artificial Intelligence?","
From what I have been reading, I see statements like

Ontology is a common method used for knowledge representation in
artificial intelligence.

But there is never really a discussion around what other options are available. To allow me to research other options I am hoping someone could maybe suggest alternatives?
As an extra question, do you have a preference for a particular system? If so, why?
","['reference-request', 'knowledge-representation', 'expert-systems', 'ontology']",
Why is the Graph Isomorphism Network powerful?,"
I am reading a paper known as GIN, How powerful are graph neural networks?, Xu et al. 2019
The paper, Lemma 5 and Corollary 6, introduces Graph Isomorphism Network (GIN).
In Lemma 5,

Moreover, any multiset function $g$ can be decomposed as $g (X) = \phi(\sum_{x\in X} f(x))$ for some function $\phi$

Similarly, in Corollary 6,

Moreover, any
function $g$ over such pairs can be decomposed as $g (c, X) = \varphi((1+\epsilon) f(c)+ \sum_{x\in X} f(x))$ for some function $\varphi$.

Finally, it makes MLP by compositing $f^{(k+1)}$ and $\varphi^{(k)}$. i.e

$f^{(k+1)} \varphi^{(k)}$

I know that $h(X)$ or $h(c,X)$ is injective, because they are unique to $c$ and $X$(in Lemma 5 and Corollary 6, respectively).
What I don't understand is: in the statements in Lemma 5 and Cor 6, $\phi$ and $\varphi$, I don't know whether they are injective or not.
My question is then: why is GIN powerful? i.e, Why does GIN preserve injectivity?
This paper explains that injectivity should be preserved to get powerful GNN. How can I answer this question?
","['papers', 'geometric-deep-learning', 'graph-neural-networks', 'graphs', 'graph-isomorphism-network']",
What is meant by sub-region of an image?,"
Consider the following sentences from the research paper titled PatternNet: Visual Pattern Mining with Deep Neural Network by Hongzhi Li et al.

The value of each pixel in a feature map is the response of a filter
with respect to a sub-region of an input image. A high value for a
pixel means that the filter is activated by the content of the
sub-region of the input image.

The sentences mention the sub-regions of an image. Is there any formal definition for a sub-region of an image?
","['definitions', 'image-processing']","
It seems that they are informally using the term ""sub-region"" to refer to the section of the image with which you multiply the kernel to produce a scalar value of the feature map (which they call pixel, but I would use the term pixel only to refer to the scalar values of the image). So, it seems that a sub-region is a synonym for receptive field. We have a question on the difference between receptive fields and feature maps here.
"
CNN Architectures for local features vs global context,"
Kaparthy in his blog post said

[this] hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context?

I'd like to gain expertise in designing networks for local vs global features. Obviously increasing the receptive field of neurons (though more pooling/striding and bigger kernels) will allow to network to take more global context into account.
Could someone point me in the direction of some readings on the topic?
Are there any specific architectures targeted to local features or to global context?
","['neural-networks', 'convolutional-neural-networks', 'computer-vision', 'reference-request', 'architecture']",
What is the sensible amount of augmentation?,"
I am playing with the transforms from Torchvision.
There are plenty of different kinds of these like:

Resize
RandomCrop
ColorJitter
Blurring
...

These are some cases of Resize for a given image:

ColorJitter

RandomAffine

The main purpose of the augmentation procedure is to prevent overfitting, extend
the training dataset in a certain way.
Some transformed images still look like the image from the original dataset, since a small change in contrast or brightness still makes the image look real.
For the other, there can be a significant change in color, or the original image occupies only a small fraction of the resulting image.
In all cases, one can still classify this object as a dog. However, many of these would not lie on the manifold of real images, and the classification for these may not make sense.
Are there some papers or research discussing the issue of the augmentation choice, and the correspondence of these to the directions along the manifold of real images?
","['computer-vision', 'data-preprocessing', 'data-augmentation']",
Multi-class classification but a single feature sometimes boils it down to a binary-classification,"
I have a three-class classification problem for a large dataset. Classes are 0, 1, and 2. There's a categorical variable in my feature vectors such that when a sample point has this variable positive, it can only belong to either class 1 or class 2. On the other hand, if that categorical variable is negative for another sample point, then this sample can be from all three classes. I was wondering how I can make a neural network use that information during training? I guess I need a custom loss function however I could not figure out how to exactly create that.
","['neural-networks', 'deep-learning', 'objective-functions', 'artificial-neuron', 'multiclass-classification']","
I think a custom loss function would be an overkill in this situation. A linear pattern like this would be easily learned by any loss desinged for multi class classification.
If I were you I would try 3 roads, in this order:

train a classifier based on decision trees (random forest & xgboost in primis). The rule you described would most likely be learned as first split to perform, if the remaining features are easily separable then such classifier would perform better than a neural network, plus it would be interpretable.
train a neural network without any fancy loss function, again the rule you describe is linear and really simple, no reason to think any loss function would miss it. BUT, if you have specific reasons to think that or if you want to try to get the best of the best out of your data there is still another possibility.
train an ensambled model. More precisely, split the data yourself into 2 subsets, one with positive feature ""x"" and one with negative feature x, and train 2 separate models on each subset of training data. It is reasonable to think that the model trained on the subset associated with positive feature x would perform better, since the problem will turn from multi class classification to binary classification, but with pros comes cons as well, specifically higher risk of over fitting or class unbalance depending on the stats of your overall dataset.

"
Is there a tutorial for understanding the proof of convergence for TD learning?,"
I'm reading the article An Analysis of Temporal-Difference Learning
with Function Approximation (1997), but the mathematics inside seems overly complicated for me. Answers to some similar questions had pointed out that these proof typically involves stochastic approximation.
My question is: are there any good tutorials (textbooks, list of papers, etc.) on stochastic approximation (or similar topics) that prepare you for reading proof like this? It will be best if it is rather self-contained under my mathematical maturity.
I have an undergraduate-level mathematical analysis and probability theory foundation and have touched some measure theory.
","['reinforcement-learning', 'reference-request', 'convergence', 'temporal-difference-methods']","
It's a pleasant surprise that somebody is interested in the fundamental convergence properties of TD, like me:)
It depends on which algorithm's convergence you want to know.

TD in the tabular case
TD with value function approximation: linear function approximation
TD with value function approximation: nonlinear function approximation by NN

Here TD mainly refers to TD($\lambda$).

If it is the first case, you can read the paper:

1993 - On the Convergence of Stochastic Iterative Dynamic Programming Algorithms
The most important result is Lemma 1, which is an extension of Dvoretzky's Theorem. If you would like to further study Dvoretzky's Theorem, you will see that the math behind the convergence proof is stochastic approximation (SA). If you go deep into SA, you may diverge from RL too far. One suggestion is to study Robbins-Monro Algorithm, which is easy to understand and inspiring.

If it is the second case, you have been reading the correct paper, though this paper is really complicated. But if you are determined to understand this paper, one suggestion is to consider the special TD(0) case, which will be easier to understand. Nevertheless, it is still nontrivial. But once you understand it, everything will be crystal.

If it is the third case, I doubt it could be proved. That is simply because convergence properties are extremely challenging to analyze when the function appropriator is nonlinear. But it does not mean the convergence analysis in the linear case is meaningless, nor the nonlinear case is impractical.


"
"Is a test accuracy of 0.74 good enough, given a dataset of about 700 samples, and, if not, how can I improve it?","
I am new to neural networks. I am trying to solve a binary classification problem. Specifically, I want to determine whether a patient has or not a certain disease based on the dataset.
The dataset has about 700 samples of different patients. I divided the sets into training and test (test size = 0.3).
My model has 1 input layer, 5 hidden layers, and 1 output layer. I used ReLU for the input and hidden layers, and I used the sigmoid for the output layer.
During the compilation of the model, I used stochastic gradient descent (SGD) as the optimizer and the mean squared logarithmic error for the loss.  I used mini-batch gradient descend (batch size = 4) for the training.
I am trying to calculate the accuracy on the test set I created previously.

The model evaluation for train set is about: 0.07 (loss) 0.76 (accuracy).

The model evaluation for test set is about: 0.07 (loss) 0.74 (accuracy).


Firstly, I would like to know if this is a good value for a model. Is the accuracy too small?
Plus, I would like to know if there's a way to improve accuracy based on my model.
I am trying to work on a project, so I was wondering if these values are acceptable.
","['neural-networks', 'machine-learning', 'accuracy', 'binary-classification']",
What is the Intermediate (dense) layer in between attention-output and encoder-output dense layers within transformer block in PyTorch implementation?,"
In PyTorch, transformer (BERT) models have an intermediate dense layer in between attention and output layers whereas the BERT and Transformer papers just mention the attention connected directly to output fully connected layer for the encoder just after adding the residual connection.
Why is there an intermediate layer within an encoder block?
For example,

encoder.layer.11.attention.self.query.weight
encoder.layer.11.attention.self.query.bias
encoder.layer.11.attention.self.key.weight
encoder.layer.11.attention.self.key.bias
encoder.layer.11.attention.self.value.weight
encoder.layer.11.attention.self.value.bias
encoder.layer.11.attention.output.dense.weight
encoder.layer.11.attention.output.dense.bias
encoder.layer.11.attention.output.LayerNorm.weight
encoder.layer.11.attention.output.LayerNorm.bias
encoder.layer.11.intermediate.dense.weight
encoder.layer.11.intermediate.dense.bias
encoder.layer.11.output.dense.weight
encoder.layer.11.output.dense.bias
encoder.layer.11.output.LayerNorm.weight
encoder.layer.11.output.LayerNorm.bias

I am confused by this third (intermediate dense layer) in between attention output and encoder output dense layers
","['natural-language-processing', 'pytorch', 'transformer', 'bert']","
Feedforward layer is an important part of the transformer architecture.
Transformer architecture, in addition to the self-attention layer, that aggregates information from the whole sequence and transforms each token due to the attention scores from the queries and values has a feedforward layer, which is mostly a 2-layer MLP, that processes each token separately:
$$
y = W_2 \sigma(W_1 x + b_1) + b_2
$$
Where $W_1, W_2$ are the weights, and $b_1, b_2$ - biases, $\sigma$ - is the nonlinearity ReLU, GeLU, e.t.c.

This is kind of a pointwise nonlinear transformation of the sequence.
I suspect, that intermediate here corresponds to $W_1, b_1$ and the output is about $W_2, b_2$.
"
How can I use a ResNet as a function approximator for pixel based reinforcement learning?,"
I'd like to use a residual network to improve learning in image-based reinforcement learning, specifically on Atari Games.
My main question is divided into 3 parts.

Would it be wise to integrate a generic ResNet with a DQN variant?

I believe ResNets take a long to train, and therefore would it be realistic to train on an Atari simulator? What would the downsides be?

Are there any fast ResNets that can be used for such purposes? Perhaps a fast ResNet that is specifically designed for online settings?


","['reinforcement-learning', 'dqn', 'algorithm-request', 'residual-networks', 'atari-games']",
Simple Image-based example for not utilising the variable-sized input handling capability of a Convolutional neural network,"
Convolutional neural networks are capable of handling inputs of varying sizes. It is one of the key benefits of convolutional neural networks. But I am unsure about the cases when we should not utilize this advantage of the convolutional neural network.
Although the following example has been provided in the chapter named Convolutional Networks of the textbook titled Deep Learning by Ian Goodfellow et al.

Convolution does not make sense if the input has variable size because
it can optionally include diﬀerent kinds of observations. For example,
if we are processing college applications, and our features consist of
both grades and standardized test scores, but not every applicant took
the standardized test, then it does not make sense to convolve the
same weights over features corresponding to the grades as well as the
features corresponding to the test scores.

It is not clear for me to understand the above example since I am habituated with images in the case of convolutional neural networks.
Can anyone provide a possible example of images where we cannot utilize the benefit of passing variable-size images to the convolutional neural network?
","['convolutional-neural-networks', 'image-processing']","
Well, I think, this statement sounds somehow misleading.
The main statement is actually in the first passage of this statement:

Note that the use of convolution for processing variably sized inputs
makes sense only for inputs that have variable size because they
contain varying amounts of observation of the same kind of
thing—different lengths of recordings over time, diﬀerent widths of
observations over space, and so forth.

Convolution is applicable only provided the input is the 1D, 2D, 3D array, graph, any collection of the data of the same kind. Each pixel is the feature vector of the same shape.
This statement means, that one cannot apply convolutions if the inputs are not-structured, like the data tables, which involve boolean, categorical, continuous features. There is no notion of locality and adjacency for this kind of data.
"
What is a filter in the context of graph convolutional networks?,"
In Section 2.1 of the research paper titled Semi-Supervised Classification with Graph Convolutional Networks by Thomas N. Kipf et al.,
Spectral convolution on graphs defined as

The multiplication of a signal $x ∈ R^N$ with a filter $g_\theta =$ diag$(\theta)$ parameterized by $\theta \in R^N$ in the Fourier domain,

i.e.:

$g_\theta * x $= $U g_\theta U^T x$

Actually, I don't understand notations here.

What is a filter? Is it like a filter from CNN? Then why it has (or should have) a diagonal form? What is $\theta$ here?

What is a Fourier domain?


I searched on Google and there was no Fourier domain.
","['terminology', 'papers', 'convolution', 'graph-neural-networks', 'notation']",
Is there any point in adding the position embedding to the class token in Transformers?,"
The popular implementations of ViTs by Ross Wightman and Phil Wang add the position embedding to the class tokens as well as to the patches.
Is there any point in doing so?
The purpose of introduction positional embeddings to the Transformer is clear - since in the original formulation Transformer is equivariant to permutations of tokens, and the original task doesn't respect this symmetry one needs to break it in some way to translational symmetry only - and this goal is achieved via the positional embedding (learned or fixed).
However, the class token is somehow distinguished from the other tokens in the image, and there is no notion for him to be located in the [16:32, 48:64] slice of the image.
Or this choice is simply a matter of convenience? And additional parameter, indeed, has a negligible cost, and there is no benefit as well as harm of the addition of positional embedding to the [CLS] or any special token?
","['neural-networks', 'transformer', 'positional-encoding']",
How to choose the new layer and objective function for transfer learning on a neural network?,"
I have a base model $M$ trained on a data say type 1 for task $T$. Now, I want to update $M$ by applying transfer learning for it to work on data type 2 for the same task $T$. I am very new to AI/ML field. One common way I found for applying transfer learning is to add a new layer at the end or replace the last layer of the base model with a new layer, and then retrain the model on new data (type 2 here). Depending upon the size of type 2, we may decide whether we retrain the whole model or only the new layer.
However, my question is that how do we decide following:

What should be the new layer(s)?
Should the objective function while retraining be the same as the one used for the base model, or it can be different? If different, then any insights on how to figure out a new objective function?

P.S. Data of type 1 and type 2 are of the same category (like both are logs or both are images), however are significantly different.
","['neural-networks', 'objective-functions', 'transfer-learning']",
"In this example of fuzzy c-means, what is the difference between ""sigma"" and ""center"" for the clusters?","
In this example, what exactly do ""Cluster"" and ""Sigma"" mean? (They chose random coordinates for the three centroids of the groups)

Centers: Cluster centers, returned as a JxN array, where J is the number of clusters and N is the number of data dimensions.
Sigma: Range of influence of cluster centers for each data dimension, returned as an N-element row vector. All cluster centers have the same set of sigma values.

Please, elaborate on the difference.
","['comparison', 'unsupervised-learning', 'notation', 'clustering', 'fuzzy-logic']","
In the example you link to, the sigma parameter has got nothing to do with the clustering; it is only used to generate sample data for illustration. It defines the spread of cluster elements around the (pre-defined) centroid of each cluster.
This is done for demonstration only: you generate clusters which you know exist, and then check that the cluster algorithm can detect those clusters correctly. In normal centroid-based clustering the centroid does not have a specified range -- each data point it simply assigned to its nearest centroid, however far away it it.
"
Reinforcement Learning method suitable for a large discrete action space with high sample efficiency,"
Consider the following problem.
We have a process, that generates $N$ stones (e.g. 2000) in one batch $b$. Every pebble has state $s_{i}^b$ and reward $s_i^b$. After choosing one pebble $i$ from the $N$, we start sampling again using the chosen pebble as a starting point and we generate the next batch $b+1$. The state $s_i^b$ is a vector of real-values and the reward $r_i^b$ is a real value.
The problem is to choose pebbles so that we maximize reward $r_i^b$ in long term. Because depending on how we choose the pebble, we can sample around the region that gives a better or worse reward $r$.
During each new batch, we make one selection of one pebble (so actions can from $i, \dots, N$. We have access to the previous $m$ batches (e.g. through replay buffer) with their rewards and states.
In short, it looks like this:

Chose randomly the first pebble from which we start sample;
We start sampling from the chosen pebble in the current batch;
We sample $N$ pebbles from a process, each pebble have a state $s_i^b$ and reward $r_i^b$;
We can chose one pebble from $i \dots N$ as action $a_i^b$ based on state $s_i^b$ and reward $r_i^b$;
Go to point 1 and repeat;

For example, at the moment, I choose in a given batch $b$ pebble with max reward $r_i^b$, so
$$i = \underset{i}{\mathrm{argmax}}\, r_i^b$$ and then use $a_i^b$ for a the current batch $b$.
But what I want is to choose:
$$i = \underset{i}{\mathrm{argmax}}\, \underset{b}{E}[R_i^b | s_i, a_i]$$
Graphically speaking:
Assuming one Batch (N) is 30
P - pebble, P - chosen pebble
batch 1: PPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
batch 2: PPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
batch 3: PPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
batch 4: PPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
batch 5: PPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
So, if I have a batch $N$, when I choose one element from the batch as an action, so the expected reward is the highest in long term, and start the sampling again from it. So only one element per batch can be chosen. And only choice of the one pebble per batch does affect the sequence in the next batch, but not inside the current batch.
The problem is, what Reinforcement Learning algorithm to use, when we choose only one item from N. And the one choice affect the whole sampled sequence in the next batch. For example in batch 1 to 4, the reward can be very low, and in batch 5 the reward is super high, if we chose wisely the pebbles in 4 previous batchs.
","['reinforcement-learning', 'deep-rl', 'sample-efficiency']","
My understanding of your environment is:

The batch number $b$ is the same thing as a time step $t$. Each batch is associated with a single static representation of the environment, the agent makes one decision, then receives a reward and a next state. I am using $t$ instead of $b$ to match the usual name seen in RL.

There is a generator which can be in a state $s_t$, initially unknown for $s_0$.

Each time step, $t$, the generator creates a batch of ""pebbles"". The generator state $s_t$ determines the output of the generator, and is the only non-random influence on the contents of each batch - it controls the possible rewards plus the possible next states.

Selecting a specific pebble from the batch returns an immediate reward $r_{t+1}$ plus sets the next state $s_{t+1}$. This is the agent's action, and it can choose any pebble from the batch as its action $a_t$.

The agent can see the values of $r_{t+1}$ and $s_{t+1}$ associated with each pebble before making its selection.


This is a slightly unusual setup for an MDP, but I think it is still a valid MDP, and the expected return can still be maximised using reinforcement learning. One important detail is that although you have a discrete list of actions in each batch, the action index values look like they are meaningless. Your action choice is really to choose the next state $s_{t+1}$ from a presented list of available states. Your action space is therefore continuous, not discrete, even though at each step you are selecting from a discrete set of actions.
The environment design is different enough from standard that most off-the-shelf RL agent libraries will not work for you. However, you can take advantage of this knowledge of the action space, and the fact that there is no other meaningful state than that selected by the agent when it chooses a pebble, to learn state values efficiently.
I would suggest you use a variant of Q-Learning, based on the ""afterstate"" values that you are selecting. You can train a neural network to approximate the state value function $\hat{v}(s,\theta)$ and you have a choice whether to use discounted returns or an average reward setting. When deciding the greedy target policy, you will want to select according to
$$\pi = \text{argmax}_i (r_i + \gamma \hat{v}(s_i,\theta))$$
for discounted return. For Q learning, technically you should also store the whole batch output in experience replay, because the TD target will be
$$g = \text{max}_i (r_i + \gamma \hat{v}(s_i,\theta))$$
and you won't know what this max will be later at the time of storing the memory. There may be ways around that - for instance you could use SARSA updates based on the actual action taken, or you could store the value of $g$. However, these would be less sample efficient and you would need to discard the memory at a faster rate, before it became too out of date with respect tothe current policy.
I think an average reward will be closer to your stated goal of generating highest expected reward from each individual batch, but I am not 100% certain how to set up the policy for that. Whilst a discounted return may end up preferring a high immediate reward in some circumstances. However, you may be able to get near identical behaviour by setting $\gamma$ high enough (e.g. $0.99$ or higher) in the more familiar discounted return setting.
The efficiency of this approach will depend on how easy it is for the agent to learn the mapping between states and the quality of the next batch. However, you should have a higher sample efficiency when selecting by afterstate, because the agent does not need to learn separately how actions relate to states.
"
How does the VAE learn a joint distribution?,"
I found the following paragraph from An Introduction to
Variational Autoencoders sounds relevant, but I am not fully understanding it.

A VAE learns stochastic mappings between an observed $\mathbf{x}$-space, whose empirical distribution $q_{\mathcal{D}}(\mathbf{x})$ is typically complicated, and a latent $\mathbf{z}$-space, whose distribution can be relatively simple (such as spherical, as in this figure). The generative model learns a joint distribution $p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})$ that is often (but not always) factorized as $p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})=p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})$, with a prior distribution over latent space $p_{\boldsymbol{\theta}}(\mathbf{z})$, and a stochastic decoder $p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})$. The stochastic encoder $q_{\phi}(\mathbf{z} \mid \mathbf{x})$, also called inference model, approximates the true but intractable posterior $p_{\theta}(\mathbf{z} \mid \mathbf{x})$ of the generative model.

How is it that the generative model learns a joint distribution $p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})$ in the case of the VAE? I know that learning the weights of the decoder is learning $p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})$
","['papers', 'generative-model', 'variational-autoencoder', 'probability-distribution', 'variational-inference']",
What are the major layers in a Vision Transformer?,"
Currently, I am studying deepfake detection using deep learning methods. Convolution neural networks, recurrent neural networks, long-short term memory networks, and vision transformers are famous deep learning-based methods that are used in deepfake detection, as I found in my study.
I was able to find that CNNs, RNNs and LSTMs are multilayered neural networks, but I found very little about the neural network layers in a Vision Transformer. (Like a typical CNN has an input layer, pooling layer, and a fully connected layer, and finally an output layer. RNN has an input layer, multiple hidden layers and an output layer.)
So, what are the main neural network layers in a Vision Transformer?
","['neural-networks', 'deep-learning', 'transformer', 'layers', 'vision-transformer']",
How are GCN doing semi-supervised learning?,"
In Semi-Supervised Classification with Graph Convolutional Networks, the authors say that GCN is an approach for semi-supervised learning (SSL).
But a GCN is making predictions using only the graph Laplacian. The single place where I find the labels is in its loss function.
$$\mathcal{L}=-\sum_{l \in \mathcal{Y}_{L}} \sum_{f=1}^{F} Y_{l f} \ln Z_{l f}$$
How does it make GCN a SSL approach?
","['papers', 'graph-neural-networks', 'semi-supervised-learning', 'spectral-clustering']",
Graph Convolutional Networks: why are non-parametric filters not localized in space?,"
I was reading the following paper here about some of the groundwork in graph deep learning. On page 3, in the bit entitled Polynomial parameterization for localized filters, it states that non-parametric filters (i.e. a filter whose parameters are all free) are not localized in space.
Question: Why is this the case? It is referring to a filter $g_{\theta}$ such that:
$$ y = g_{\theta}(L) x = g_{\theta} (U \Lambda U^T) x = U g_{\theta} (\Lambda) U^T x $$
where $L$ is the graph laplacian matrix, and $U \Lambda U^T$ are the eigenvector decomposition matrices.
Attempted explanation: Is it because the filter $g_{\theta}$ is defined in the spectral domain and thus its spatial domain (i.e. its inverse graph Fourier transform) may be defined over the whole graph (and thus not localized)?
","['geometric-deep-learning', 'graph-neural-networks', 'filters']",
Does the converted (now square) distorted image of a face affect the accuracy of the calculation of the similarity in FaceNet?,"

As far as I know,

FaceNet requires a square image as an input.

MTCNN can detect and crop the original image as a square, but distortion occurs.


Is it okay to feed the converted (now square) distorted image into FaceNet? Does it affect the accuracy of calculation similarity (embedding)?
For similarity (classification of known faces), I am going to put some custom layers upon FaceNet.
(If it's okay, maybe because every other image would be distorted no matter what? So, it would not compare normal image vs distorted image, but distorted image vs distorted image, which would be fair?)
Original issue: https://github.com/timesler/facenet-pytorch/issues/181.
","['data-preprocessing', 'face-recognition', 'face-detection', 'facenet']","
As Neil Slater said it depends on how the model was trained. Now if you go to the FaceNet implementation in TF in github you can see that in the face alignment they do resize without aspect ratio keeping (old scipy.misc.imresize does not keep aspect ratio) so if you are using this implementation the answer to your question is: it does not affect the accuracy.
But if we want to elaborate the golden rule is: not to change the face aspect ratio because that is causing a distortion of the input data for the embedding computation.
In most face recognition tasks once you detect a bounding box you do a resize with aspect ratio keeping. You can accomplish that with several libraries:

Pillow Thumbnail method
Albumentations LongestMaxSize method

I would recommend to use resize with aspect ratio keeping.
"
What is the most suitable measure of the distance between two VAE's latent spaces?,"
The problem I'm trying to solve is as follows.
I have two separate domains, where inputs do not have the same dimensions. However, I want to create a common feature space between both domains using paired inputs (similar inputs from both domains).
My solution is to encode pairs of inputs into a shared latent space using two VAE encoders (one for each domain). To ensure that the latent space is shared, I want to define a similarity metric between the output of both probabilistic encoders.
Let's define the first encoder as $q_\phi$ and the second as $p_\theta$. As for now, I have two main candidates for this role:

KL-divergence : $\text{KL}(p || q)$ (or $\text{KL}(q||p)$), but, since it is not symmetrical, I don't really know which direction is the best.

JS-divergence: symmetrical and normalized, which is nice for a distance metric, but, since it is not as common as KL, I'm not sure.


Other candidates include adversarial loss (a discriminator is tasked to guess from which VAE the latent code is, the goal of both VAE being to maximally confuse it) or mutual information (seen more and more in recent works, but I still don't fully understand it).
My question is: according to you, which loss could work best for my use case? KL or JS? Or other candidates I didn't think of?
-- More context --
My ultimate goal is to use transfer learning between morphologically distinct robots e.g a quadrupedal robot and a bipedal robot. The first step in my current approach is to record trajectories on both robots executing the same task (walking for example). From said trajectories I create paires of similar states (to simplify the problem, I suppose that both robot achieve the same task at the same speed so temporaly aligned states for both robots are paired). Then my goal is to encode these paired states (that doesn't have the same dimension due to the difference in number of joints) into two latents spaces (one for each VAE) such that similar pair of inputs are close in the latents spaces. If I was working with simple autoencoders I would simply minimize the distance in the latent space between paires of inputs such that similar states on both robots maps to the same point in the latent space. But I need the generative capabilities of VAE so instead I would like to make the distributions outputed by the VAEs as close as possible. Make sense ?
","['neural-networks', 'deep-learning', 'variational-autoencoder', 'kl-divergence', 'latent-variable']",
Can people without a background in STEM go into the field of AI?,"
I've recently become very interested in the potential AI holds for the future of society. I believe it has the potential to truly alter the way we live our lives in the not-too-distant future. I've read around three dozen books by leading scholars and I've written my undergraduate thesis on the implications of AI for modern warfare and the international balance of power. With that in mind, I've started to think about a career related to AI and machine learning.
However, I have a bit of a problem considering my background as a humanities person. I'm about to finish my undergraduate studies in International Relations and I plan to get a master in a similar field. As someone with little background in Comp Sci/ Math/ Physics/ etc, how could someone go about working in a field like AI? I struggle to see what utility non-STEM people could offer to firms.
I hope I'm asking this in the right place considering this forum is mostly for technical questions.
",['machine-learning'],
"How do the BFS and DFS search algorithms choose between nodes with the ""same priority""?","
I am currently taking an Artificial Intelligence course and learning about DFS and BFS.
If we take the following example:

From my understanding, the BFS algorithm will explore the first level containing $B$ and $C$, then the second level containing $D,E,F$ and $G$, etc., till it reaches the last level.
I am lost concerning which node between $B$ and $C$ (for example) will the BFS expand first?
Originally, I thought it is different every time, and, by convention, we choose to illustrate that it's done from the left to the right (so exploring $B$ then $C$), but my professor said that our choice between $B$ and $C$ depends on each case and we choose the ""shallowest node first"".
In made examples, there isn't a distance factor between $A$ and $B$, and $A$ and $C$, so how could one choose then?
My question is the same concerning DFS where I was told to choose the ""deepest node first"". I am aware that there are pre-order versions and others, but the book ""Artificial Intelligence - A Modern Approach, by Stuart Russel"" didn't get into them.
I tried checking the CLRE algorithms book for more help but the expansion is done based on the order in the adjacency list which didn't really help.
","['search', 'breadth-first-search', 'depth-first-search']","
Either one. The BFS algorithm and DFS algorithm do not specify.
Typically, it's programmed as left-to-right, just because that's the way programmers think about trees. It doesn't have to be.
Note that DFS isn't ""deepest node first"" either. Imagine that nodes H and I in your tree did not exist; D, J, K, E, B would be a perfectly valid DFS traversal of B, even though J and K are deeper than D. So would B, D, E, J, K even though E is the parent of J and K! DFS says that you look at a node's children before you look at other nodes on the same layer, but it doesn't say you have to look at the node's children before the node itself. In fact there are three well-known variants (pre-order, post-order and in-order) depending on whether you visit each node before, after or in the middle of its children.
Now, if this is for an AI then you probably do care about the order. If this tree represents a game tree, then you probably want to estimate which node is likely to have the best outcome for the AI player, and check that one first. This can be called ""best-first search"".
"
What is the place of ontologies in artificial intelligence?,"
Very much a general question here, from a somewhat uneducated perspective.
I'm currently part way through an MSc in AI, and at the minute I am taking a module on Knowledge Engineering and Computational Creativity. The professor taking the class obviously does research in this area and is saying that ontologies are becoming very important in the world of AI, or it may be more accurate to say he is suggesting they are becoming more important.
I intend to look into the work he does, and ask him a few questions, but, generally, I was wondering where this type of research sits in the world of AI. Is it something being worked on a lot? Is it becoming bigger?
I am interested because I do find the topic interesting, and I will have a research project coming up soon, and while I do want to work on an interesting topic, I also want to work on a relevant topic, so any information would be great.
","['ai-design', 'ontology', 'knowledge-based-systems']","
Computational creativity is not something I know anything about. However, I work in knowledge engineering. This falls into the areas of knowledge representation and reasoning known as semantic web technologies in general. More recently Google has popularized the name ""knowledge graphs"" and now many people tend to talk of knowledge graphs rather than the semantic web, even though, strictly speaking, knowledge graphs are a small subset of semantic web technologies.
This is a massive field of research, which is widely used in bioinformatics, healthcare, property management, and many other fields to enable semantic search. I myself work in bioinformatics where we use 265 ontologies spanning close to 7 million concepts that are used to enable semantic searches across around 300 petabytes of data. In fact, it is knowledge graphs that are at least in part used to enable Google searches and information provided Google info boxes. Hence, many people are already using the results of knowledge engineering without knowing it.
So what is an ontology? An ontology defines concepts and relations between concepts. This has been done in computer science for ages, so what makes ontologies and the semantic web so special?

Each concept and relation is assigned globally unique identifiers, e.g. URI, IRI, PURL
Ontologies are defined in a machine-readable syntax, e.g. XML, JSON-LD
The semantic web defines a generic data model able to describe arbitrary data called RDF triples. In particular, this helps people to define their data before having to define a schema (as is the case with relational databases).
An RDF triple <s, p, o> expresses that subject (s) and object (o) is related via predicate (p).
SPARQL is a query language for querying RDF triples
Ontologies are equipped with formal semantics based on mathematical logic, which enables artificial intelligence reasoning procedures to infer implicit knowledge from explicit knowledge. E.g. RDFS and OWL.
JSON-LD, RDF, SPARQL, RDFS and OWL are W3C standards.

Here is a book that can give you a reasonably gentle introduction to aspects of the field and how it is used in practice: The knowledge graph cookbook.
"
"Given the immaturity of NLP tools for non-English languages, should I first translate the non-English language to English before text pre-processing?","
For non-English languages (in my case Portuguese), what is the best approach? Should I use the not-so-complete tools in my language, or should I translate the text to English, and after using the tools in English? Lemmatization, for example, is not so good in non-English languages.
","['natural-language-processing', 'data-preprocessing', 'machine-translation', 'lemmatization']",
Why are optimization algorithms for deep learning so simple?,"
From my knowledge, the most used optimizer in practice is Adam, which in essence is just mini-batch gradient descent with momentum to combat getting stuck in saddle points and with some damping to avoid wiggling back and forth if the conditioning of the search space is bad at any point.
Not to say that this is actually easy in absolute terms, but after a few days, I think I got most of it. But when I look into the field of mathematical (non-linear) optimization, I'm totally overwhelmed.

What are the possible reasons that optimization algorithms for neural networks aren't more intricate?

There are just more important things to improve?
Just not possible?
Is Adam and others already so good that researchers just don't care?

","['neural-networks', 'optimization', 'gradient-descent', 'adam']",
What is the best way to train a text-based regressor model?,"
I want to build a deep learning model that can predict a continuous value (LogP in this case) given text inputs (SMILES notations in this case), the dataset is as illustrated below.




SMILES notations
LogP




C1CCCC(C)(C)1
1.98


...
...




I have never tackled text data, I mostly worked with numbers-based datasets (or images).
My questions are:

What is the best model for this case? I believe RNN based architectures, such as LSTM and GRU, are the most suitable.
What about recent architectures such as Transformers?
How can/should I convert (or embed, or encode) the text inputs (SMILES) to feed them to my model?

","['deep-learning', 'regression', 'model-request']",
Demonstration of AI-powered Mario collecting lots of coins?,"
As part of a talk I'm giving, I'd like to show one of the many videos on YouTube where an AI is playing Mario, such as this one. What bothers me though is that the AI is trying to complete the level as quickly as possible, without trying to collect lots of coins. I believe it's known as ""speed run"" in the gaming world, which is fine and well, but I think most people expect Mario to be collecting lots of coins and mushrooms.
Are you familiar with a video of an AI-powered Mario that does collect a lot of coins and mushrooms?
If not, maybe you know a different video of a similarly popular video game where the AI does try to get lots of points and not just do a speed run?
","['reinforcement-learning', 'game-ai', 'atari-games']",
Is there any way to remove background of an image fully with the help of post-processor techniques(like edge detector) after deep learning based model,"
I'm using a deep learning-based model (deep lab v3+ with xception as the backbone) for image segmentation and removing the background. The subject of the image will be a person. And my target is to extract the person from the image. But only with the machine learning model, the output is not satisfactory. I'm thinking if somehow I can detect the edge of the person with ""Canny Edge detector"" and use this as post processor to get exact output. I also try the blur effect for smoothing the object's edge and getting a pleasant-looking output. I saw Adobe Photoshop's ""select subject"" feature. It is quite accurate.
Main image -> segmented mask from ml-model -> adding blur effect on the edge
Canny Edge Detector as Post Processor:
I tried the following steps:

Run Canny Edge on the main image. canny edge image
Remove the background noise(of the canny edge image) using the image segmentation mask(I got from the ml-model). Here I use bitwise-and of the canny edge image and segmentation mask. background noise removed canny edge image
In the 2nd step, some of the real edges of the person also get removed, and to restore those, I have used the breadth-first-search algorithm and run it up to some limit(suppose, up to 10 neighbors). after BFS
I shrink the segmentation mask and using this, removed the interior noise of the canny edge image. removing the inside noise

Now if I could somehow connect the edge line of the canny edge image, we would get a perfect segmentation. But I failed to do so.
Any suggestion will be helpful for me.
","['machine-learning', 'computer-vision', 'image-processing', 'image-segmentation']",
Multi label classification on non binary labels with pytorch,"
I am working on a project consisting of medical images and a huge dataset of multi-label and non-binary labels/outcomes ( sex, blood pressure, age and 40 more ).
Would be the best approach to hard code all of them or is there some better approach? If this is the best way, does anyone have a similar PyTorch notebook on which I could orientate myself? Or some smart solution how to hard code them automatically?
Any help is welcome!
","['classification', 'deep-neural-networks', 'pytorch']","
If your goal is to predict given an image multiple labels (each of them can be binary or multi-class) you could consider two strategies:

Create for each classification task a separate model, which predicts solves only one problem
Create a single model with multiple heads

The first option seems to be more straightforward, but it would most likely need to consume more memory and computational resources, and the gradient signal from the prediction of one model is independent of other models. In case the labels are uncorrelated or weakly correlated this is not a problem.
The second option  when you have a joint backbone for all classification problems and only, in the end, the computation graph splits into branches solving each of the classification problems seems to be more efficient, and in the case, where these tasks are related to each other, improvement of accuracy in one task is very likely to be beneficial for the other task.
Overall, the resulting architecture resembles the Inception architecture:

You can try to put all classification heads in the very end, or some can disentagle from the other part of network a bit earlier.
"
How to detect the sine wave signal with different frequency using neural networks?,"
I'm wondering if there is a way to use a neural network that can detect the noisy sine wave, where the frequency is not constant. In other words, I'm not looking for a solution that would detect the signal of a particular frequency (say 50Hz), but a solution that can detect any sine wave signal in say range (100-1000Hz).
","['neural-networks', 'algorithm-request', 'signal-processing']",
"In this paper, how does scaling the filter instead of the image generate saliency maps of the same size and resolution as the input image?","
In this paper, in section 3.1, the authors state

Scaling the filter instead of the image allows the generation of saliency maps of the same size and resolution as the input image.

How is this possible?
From what I have understood, the process of filtering the image is similar to that of a convolution operation, like this:

However, if this is true, shouldn't we get different sized outputs (i.e. saliency maps) for different filter sizes?
I think I am misunderstanding how the filtering process really works in that it actually differs from a CNN. I would highly appreciate any insight on the above.
Note: This is a follow-up to this question.
","['computer-vision', 'papers', 'saliency-map']",
What is uncentered variance and how it becomes equal to mean square in Adam?,"
I have been reading about Adam and AdamW (Here). The author mentioned that in ""uncentered variance"" we don't consider subtracting mean

In this statement, the author is talking about uncentered variance and how it becomes equal to the square of the mean.

I want to understand what exactly is uncentered variance? (because if I consider the general equation of variance $\dfrac{(\text{obs-mean})^2}{(n-1)}$ then it is not making sense to me the removing mean will lead to the definition of uncentered variance as we need a point around which we are calculating variance and here mean is that point)
Also if we are making mean=0(not subtracting mean from obs) then if I consider this as
uncentered mean (For me it is variance around 0) then it is becoming hard to understand how this will lead to uncentered $\text{variance = mean}^2$
","['neural-networks', 'deep-learning', 'optimization', 'gradient-descent', 'optimizers']","
Let me try to explain here.
Usually, we calculate the variance by subtracting the mean term and then square it. But here mean (first-moment m_t) is fluctuating like anything at each time ""t"" and is getting calculated with the influence of past mean as well, also with the influence of beta_1.
So when the 2nd-moment term v_t is getting calculated with a similar approach, when variance is mentioned in the Adam context is understood that we are considering the gradient mean to be equal to zero. This makes sense, since there is no reason to think positive or negative gradients would be more probable than one-another. we are only considering g_t^2 here, so this is called Uncentred variance in the context of calculating 2nd momentum for ADAM.
"
How do graph neural networks adapt to different number of nodes and connections of different graphs?,"
I have recently been studying GNN, and the fundamental idea seems to be the aggregation and transfer of information from a node's neighborhood to update the node's internal state. However, there are few sources that mention the implementation of GNN in code, specifically, how do GNNs adapt to the differing number of nodes and connections in a dataset.
For example, say we have 2 graph data that looks like this:

It is clear that the number of weights required in the two data points would be different.
So, how would the model adapt to this varying number of weight parameters?
","['implementation', 'geometric-deep-learning', 'graph-neural-networks']",
Why does research on faster Transformers focus on the query-key product?,"
A lot of recent research on Transformers has been devoted to reducing the cost of the self-attention mechanism:
$$\text{softmax}\left(\frac{Q K^T}{\sqrt{d}} \right)V,$$
As I understand it, the runtime, assuming $\{Q, K, V\}$ are each of shape $(n, d)$, is $O(n^2 d + n d^2)$. In general, the issue is the $n^2 d$ term, because the sequence length $n$ can be much bigger than the model dimension $d$. So far, so good.
But as far as I can tell, current research focuses on speedups for $Q K^T$, which is $O(n^2 d)$. There's less focus on computing $A V$, where $A = \text{softmax} \left(\frac{Q K^T}{\sqrt{d}} \right)$ -- which also has complexity $O(n^2 d)$.
Why is the first matrix product the limiting factor?
Examples of these faster Transformer architectures include Longformer, which approximates $QK^T$ as a low-rank-plus-banded matrix, Nystromformer, which approximates $\text{softmax}(QK^T)$ as a low-rank matrix with the Nystrom transformation, and Big Bird, which approximates it with a low-rank-plus-banded-plus-random matrix.
","['deep-learning', 'natural-language-processing', 'transformer', 'attention', 'computational-complexity']",
How to use K-means clustering to visualise learnt features of a CNN model?,"
Recently, I was going through the paper Intriguing Properties of Contrastive Losses. In the paper (section 3.2), the authors try to determine how well the SimCLR framework has allowed the ResNet50 Model to learn good quality/generalised features that exhibit hierarchical properties. To achieve this, they make use of K-means on intermediate features of the ResNet50 model (intermediate means o/p of block 2,3,4..), and I quote the reason below

If the model learns good representations then regions of similar objects should be grouped together.

Final Results: 
I am trying to replicate the same procedure but with a different model (like VggNet, Xception).
Are there any resources explaining how to perform such visualizations?
","['convolutional-neural-networks', 'representation-learning', 'k-means', 'contrastive-learning']",
How do automatic high-beam headlights work on cars?,"
Modern cars can operate high-beam headlights automatically:

They automatically switch from high-beam headlights to low-beam ones (less intense) when you enter a town or there is a car in front of you either going in the same or opposite direction so you don't dazzle other drivers or people in the street.

Oppositely, when you are in almost complete darkness and there aren't any other drivers at sight, the system automatically sets the high-beam headlights.


I am aware that in the front part of these cars there is a camera or sensor and also imagine that the automatic switching when entering or exiting a town is achieved by just having a threshold ambient illumination.
But I am unable to imagine how the recognition of other cars works. It might be that an image recognition program is used to detect pairs of front car lights (white) and rear lights (red). However, how do you deal with:

pairs of street lamps (far from the road and not illuminating it) that could be identified as a car coming in the opposite direction,
pairs of lights coming from the reflectors of the crash barriers,

many other random pairs of lights that could be interpreted as cars.

Is this technology based on AI software that after intense training is able to deal with these points? Or is it a less complex image analysis program that takes into account that moving lights outside the car (with respect to the road) move differently than static lights (with respect to the road) when seen from the car?
Edit: I've seen this technology working on Audi A4 and A5 cars.
","['image-recognition', 'autonomous-vehicles']",
How to train neural networks with multiprocessing?,"
I am trying to figure out how multiprocessing works in neural networks.
In the example I've seen, the database is split into $x$ parts (depending on how many workers you have) and each worker is responsible to train the network using a different part of the database.
I am confused regarding the optimization part:
Let's say worker 1 finished first to calculate the gradient, now it will update the network accordingly.
Then worker 2 finished the calculation and it will also attempt to update the weights. However, the gradient it calculated was for the network before it was updated by the first worker. Now, the second worker will attempt to update the network with a bad gradient.
Did I miss something?
","['neural-networks', 'gradient-descent', 'gpu']",
"In layman terms, what does ""attention"" do in a transformer?","
I heard from many people about the paper titled Attention Is All You Need by Ashish Vaswani et al.
What actually does the ""attention"" do in simple terms? Is it a function, property, or some other thing?
","['neural-networks', 'papers', 'transformer', 'attention']","
Let's start by stressing out that in the literature unfortunately the term attention is still used widely without any precise consensus around the technical details, the only constant across papers is that attention should be used when a model is capable of learning, or focusing on local vs global patterns in the data we use for training. And with ""should be used"" I simply refer to the fact that everyone like to feel eligible to write ""Hey, we used attention!"" simply because of the hype generated by the introduction of transformers by Vaswani et al.
Said that, I think up to this point the best expression to describe attention is:
A specific type of architecture
What do I mean by this: Vaswani et al. introduced the expression attention in the paper you cite with a new whole machine learning architecture, namely the transformers. In the paper, attention is used to refer to a specific set of layers, similarly we call residual blocks or dense blocks specific type of layers combinations that were introduced for convolutional neural networks. For me the is no difference at all between attention and the two above mentioned examples. The confusion around the use of this expression in my opinion arose from the fact that Vaswani et al. put a lot of emphasis on the final purpose of the new proposed model, i.e. capturing local similarities within sentences in machine translation.
One last consideration why I think that architecture is the best label for attention is that it include also type of attentions that are completely different from the multi-head attention module introduced by Vaswani et al, like architectures that leverage attention maps. Mathematically, attention maps and the multi-head attention module share nothing but the name, still, because conceptually they seems to fulfill the same purpose, we call them both attention, with the consequence that to avoid confusion, one should always refer to a specific paper when talking about attention.


"
Understanding gumbel-softmax backpropagation in Wav2Vec papers,"
I'm studying the series of Wav2Vec papers, in particular, the vq-wav2vec and wav2vec 2.0, and have a problem understanding some details about the quantization procedure.
The broader context is this: they use raw audio and first convert it to ""features"" $z$ via a convolutional network. Then they project any feature $z$ to a ""quantized"" element $\hat{z}$ from a given finite codebook (or concatenation of finitely many finite codebooks). To find $\hat{z}$, they compute scores $l_j$ for each codebook entry $v_j$, convert these scores to Gumbel-Softmax probabilities $p_j$ (using a formula which is not deterministic, the formula involves random choices of some numbers from some distribution) and then use these probabilities $p_j$ to choose $\hat{z}$. Further stages of the pre-training pipeline are trained to predict $\hat{z}'s$ by either predicting ""future"" from the ""past"", or ""reconstructing masked segments"".
My question is this is about this sentence:

During the forward pass, $i = \text{argmax}_j p_j$ and in the
backward pass, the true gradient of the Gumbel-Softmax outputs is
used.


I have trouble seeing what exactly is happening in the loss function and back-propagation. Could someone please help me to break this down into details?


My mental attempts to make sense out of it (I'm using the notation $\hat{z}$ for quantized vectors, in the second paper they use $q$)
(1) I would say that during the forward pass, in the Gumbel-Softmax, random variables from the Gumbel-distribution $n_j$ are sampled every time (for every training example) to compute the Gumbel-softmax probabilities $p_j$.
(1a) In the back-propagation, these $n_j$'s are kept constant, and $p_j$ is treated as a function of $l_j's$ only.
(2) The loss function has 2 parts here, Contrastive loss and Diversity loss.
(2a) Based on the description, I would say that in the contrastive loss, the ""sampled"" vectors $\hat{z}_j$ are used, and probabilities never appear (even not in back-propagation of this part of the loss).
(2b) I would believe that in the gradient of the Diversity loss, which only uses probabilities $p_{g,v}$, that here the gradient or the loss actually is used, as this is responsible for maximizing the entropy. This part of the gradient probably does not use the sampled values $\hat{z}_j$.
Is this approximately correct?
If yes, then I still fail to understand what exactly is happening in the vq-wav2vec paper. The sentence

During the forward pass, $i = \text{argmax}_j p_j$ and in the
backward pass, the true gradient of the Gumbel-Softmax outputs is
used.

is there as well, but I cannot see any part of the loss function (in this paper) where the probabilities are explicitly used (such as the diversity loss).
","['deep-learning', 'papers', 'audio-processing']",
What are the roles of the prior $\mathrm{p}(\mathbf{z})$ in a VAE?,"
I know the encoder is variational posterior $q_{\phi}(\mathbf{z} \mid \mathbf{x})$.
I also know that the decoder represents the likelihood:
$p_{\theta}(\mathbf{x} \mid \mathbf{z})$.
My question is about the prior $\mathrm{p}(\mathbf{z})$.
I know ELBO can be written as:
$$E_{q_{\phi}(\mathbf{z} \mid \mathbf{x})}[\log (p_{\theta}(\mathbf{x} \mid \mathbf{z}))]-\mathrm{D}_{\mathrm{KL}}( q_{\phi}(\mathbf{z} \mid \mathbf{x}) \| \mathrm{p}(\mathbf{z})) \leq \log (p_{\theta}( \mathbf{x}))$$
And for the VAE, the variational posterior is
$$ q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}^{(i)})= \mathcal{N}( \boldsymbol{\mu}^{(i)}, \boldsymbol{\sigma}^{2(i)} \mathbf{I}),$$
and prior is
$$ \mathrm{p}(\mathbf{z})=\mathcal{N}( \boldsymbol{0}, \mathbf{I}).$$
So
$$\mathrm{D}_{\mathrm{KL}}\left(\mathrm{q}_{\Phi}(\mathbf{z} \mid \mathbf{x}) \| p_{z}(\mathbf{z})\right)=\frac{1}{2} \sum_{j=1}^{J}\left(1+\log \left(\sigma_{j}^{2}\right)-\sigma_{j}^{2}-\mu_{j}^{2}\right)$$
That's one way I know the prior plays a role, in helping determine part of the loss function.
Is there any other role that the prior plays for the VAE?
","['generative-model', 'variational-autoencoder']",
Can I use the transformers for the prediction of historical data?,"
Can I use the transformers for the prediction of wind power with the historical data?
Dataset
Datetime, Ambient temperature (Degree), Dewpoint (Degree), Relative Humidity\n (%), Air Pressure, Wind Direction (Degree), Wind Speed at 76.8 m (m/sec), Power Generated(kW).
15 years of data from 2007 to 2021 with a sampling time of 1 hour​
","['neural-networks', 'deep-learning', 'datasets', 'transformer']","
Transformers, being a general-purpose sequence model can be used for Time-Series forecasting.
There are some papers dedicated to the use of Transformer for time-series prediction and blogs.
The main ingredient for the autoregression in predictions is the mask in Transformer encoder.
When the next element is predicted, tokens in the sequence attend only to the tokens back in time.

After each block a new element is predicted, based on the decoder and encoder tokens.
However, since the dimensionality of your data seems to be rather small, I would suggest starting from something simpler - say linear AR models or RNN, and only then work with transformers.
"
Why is this vacuum cleaner agent rational?,"
This is the vacuum cleaner example of the book ""Artificial intelligence: A Modern Approach"" (4th edition).

Consider the simple vacuum-cleaner agent that cleans a square if it is dirty and moves to the other square if not; this is the agent function tabulated as follows.
Percept sequence                      Action
[A,Clean]                             Right
[A,Dirty]                             Suck
[B,Clean]                             Left
[B,Dirty]                             Suck
[A,Clean], [A,Clean]                  Right
[A,Clean], [A,Dirty]                  Suck
.                                      .
.                                      .
[A,Clean], [A,Clean], [A,Clean]       Right
[A,Clean], [A,Clean], [A,Dirty]       Suck
.                                      .    
.   
                               .

The characteristics of environment and performance function are as follow:

The performance measure awards one point for each clean square at each time step, over a ""lifetime"" of 1000 time steps.

The ""geography"" of the environment is known a priori (the above figure) but the dirt distribution and the initial location of the agent are not. Clean squares stay clean and sucking cleans the current square. The Right and Left actions move the agent one square except when this would take the agent outside the environment, in which case the agent remains where it is.

The only available actions are Right, Left, and Suck.

The agent correctly perceives its location and whether that location contains dirt.


In the book, it is stated that under these circumstances the agent is indeed rational. But I do not understand such percept sequence that consists of multiple [A, clean] percepts, e.g. {[A, clean], [A, clean]}. In my opinion, after first [A, clean], the agent must be gone to the right square; So, the sequence {[A, clean], [A, clean]} will never be perceived.
In other words, the second perception of [A, clean] is the consequence of acting left or suck action after perceiving the first [A, clean]. Therefore, we can conclude the agent is not rational.
Please, help me to understand it.
","['intelligent-agent', 'norvig-russell', 'rationality', 'rational-agents']","
You are correct that the robot should never perceive two [A, Clean] events in a row, as it should have moved when perceiving the first one.
However, this only applies to a perfect agent, which always successfully executes all its actions. What if the dog walks past and blocks the exit to room B? Then the Right action fails, and the agent is still in A. But now, it might only listen to [B, ?] events, because obviously it must now be in room B, since it executed a Right command.
Having a simple tabulated agent function like this is clearly not adequate for real world problems, as where do you stop? How many [A, Clean] events do you want to list? And, if you start again, why do you have lists of perceptions in the first place? The initial [A, Clean] in the list should suffice, as it would still attempt to move right if it processes perceptions with no internal state/memory.
But bearing in mind that physical actions can fail in the real world, the agent is still rational, it's just overly cautious: it doesn't try to suck when the location is clean, or move away when it's dirty. It just wants to make sure it's really clean before moving away.
"
Should I allow NN to infer relationships of inputs?,"
This question is assuming a sequential, deep neural network
Given some features [X1, X2, ... Xn], I'm trying to predict some value Y.
The raw data available to me contains feature X1 and feature X2. Say that I know there is an effect on Y based on the ratio of the two features, i.e. X1 / X2.
Should I add a new feature, mathematically defined as the ratio of the two features? I haven't been able to locate any literature which begins to describe the necessity or warnings of this.
Instinctly I'm worried about the following:

Overfitting and the need for excessive regularization, due to duplicate information in the feature set
Exponentially growing number of features, since defining a ratio between each feature may be necessary

However, I also recognize that certain relationships are impossible to be defined by a deep neural network (i.e. logic gates, exponential relationships, etc), so when would this sort of ""relationship defining"" be necessary? For example, if an exponential relationship is known to exist?
","['neural-networks', 'deep-learning']","
Ideal advise is to feed the raw data to the neural networks to let neural networks make its own inference
Considering you have expert knowledge that $X1/X2$ has effect on $Y$ , here the new feature ($X1/X2$) is referred to as a derived feature
However, there are few advantages which can help you consider of using derived features like $X1/X2$ in your case

Being a domain expert or SME choosing X1/X2 as an important feature, you can ideally accelerate the training process
Highly advantageous when you are short on CPU time
Most neural networks calculate sums fed through the activation functions, estimating the ratio or the multiplication requires lot of neurons especially if $X1/X2$ is an important feature
You can use feature pruning techniques to eliminate redundant features
One major drawback of neural network is often considered to be a black box model that’s the approximation of neural networks doesn’t give any insight of the form of function f. Use make use of feature selection algorithms to pare down feature space.
With large number of parameters in model also increasing the risk of overfitting the network. You can however overcome this with good regularisation methods. You can also use feature pruning to avoid overfitting when you are having limited data.

I found a literature in the field of medicine that deals with deriving features based on expertise accurately identified the required states.
Heart rate variability-derived features based on deep neural network for distinguishing different anaesthesia states

The incorporation of four HRV-derived features in the time and frequency domain and a deep neural network could accurately distinguish between different anaesthesia states.

"
What makes Sequential Bayesian Filtering and Smoothing tractable?,"
I'm currently diving into the Bayesian world and I find it pretty fascinating.
I've so far understood that applying the Bayes' Rule, i.e.
$$\text{posterior} = \frac{\text{likelihood}\times \text{prior}}{\text{evidence}}$$
are most of the time intractable because of the high dimensional parameter space in the denominator.
One way to solve this is by using a prior conjugate to the likelihood, as then the analytical form of the posterior is known and calculations are simplified.
So far so good. Now I've read about bayesian sequential filtering and smoothing techniques such as the Kalman Filter or Rauch-Tung-Striebel Smoother (find references here). As far as I understood,  assuming a time step $k$,  instead of calculating the complete posterior distribution $p(X_k|Y_k)$ with $X=[x_1, ...,x_k]$ and $Y=[y_1,...y_k]$, a Markov Chain is assumed and only the marginal $p(x_k|Y_k)$ is estimated in a recursive manner. That is, the posterior calculate at time step $k$ serves as prior for the next time step. I guess, Bayes' Rule is somehow involved in these calculations
Furthermore, both techniques assume the posterior always to be Gaussian and therefore closed-form solutions are obtained. Now I was wondering what restriction does make the whole process tractable, i.e. eliminates the need to compute the evidence?
I guess it's the Gaussian assumption, i.e. the prior, the predicted, and the posterior distribution are all assumed to be Gaussian, and therefore updated distributions are obtained without computing the evidence - is this correct and does this refer to conjugate distributions?
Or is it the fact that we assume a Markov Chain and do not consider all states at each time step?
","['bayesian-networks', 'bayes-theorem', 'bayesian-probability', 'kalman-filter']",
Hand Landmark Detector Not Converging,"
I'm currently trying to train a custom model with TensorFlow to detect 17 landmarks/keypoints on each of 2 hands shown in an image (fingertips, first knuckles, bottom knuckles, wrist, and palm), for 34 points (and therefore 68 total values to predict for x & y). However, I cannot get the model to converge, with the output instead of being an array of points that are pretty much the same for every prediction.
I started off with a dataset that has images like this:

each annotated to have the red dots correlate to each keypoint. To expand the dataset to try to get a more robust model, I took photos of the hands with various backgrounds, angles, positions, poses, lighting conditions, reflectivity, etc, as exemplified by these further images:
    
I have about 3000 images created now, with the landmarks stored inside a CSV as such:

I have a train-test split of .67 train .33 test, with the images randomly selected to each. I load the images with all 3 color channels and scale both the color values & keypoint coordinates between 0 & 1.
I've tried a couple of different approaches, each involving a CNN. The first keeps the images as they are, and uses a neural network model built as such:
model = Sequential()

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = (225,400,3)))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'))
model.add(MaxPooling2D(pool_size = (2,2), strides = 2))

filters_convs = [(128, 2), (256, 3), (512, 3), (512,3)]
  
for n_filters, n_convs in filters_convs:
  for _ in np.arange(n_convs):
    model.add(Conv2D(filters = n_filters, kernel_size = (3,3), padding = 'same', activation = 'relu'))
  model.add(MaxPooling2D(pool_size = (2,2), strides = 2))

model.add(Flatten())
model.add(Dense(128, activation=""relu""))
model.add(Dense(96, activation=""relu""))
model.add(Dense(72, activation=""relu""))
model.add(Dense(68, activation=""sigmoid""))

opt = Adam(learning_rate=.0001)
model.compile(loss=""mse"", optimizer=opt, metrics=['mae'])
print(model.summary())

I've modified the various hyperparameters, yet nothing seems to make any noticeable difference.
The other thing I've tried is resizing the images to fit within a 224x224x3 array to use with a VGG-16 network, as such:
vgg = VGG16(weights=""imagenet"", include_top=False,
    input_tensor=Input(shape=(224, 224, 3)))
vgg.trainable = False

flatten = vgg.output
flatten = Flatten()(flatten)

points = Dense(256, activation=""relu"")(flatten)
points = Dense(128, activation=""relu"")(points)
points = Dense(96, activation=""relu"")(points)
points = Dense(68, activation=""sigmoid"")(points)

model = Model(inputs=vgg.input, outputs=points)

opt = Adam(learning_rate=.0001)
model.compile(loss=""mse"", optimizer=opt, metrics=['mae'])
print(model.summary())

This model has similar results to the first. No matter what I seem to do, I seem to get the same results, in that my mse loss minimizes around .009, with an mae around .07, no matter how many epochs I run:

Furthermore, when I run predictions based off the model it seems that the predicted output is basically the same for every image, with only a slight variation between each. It seems the model predicts an array of coordinates that looks somewhat like what a splayed hand might, in the general areas hands might be most likely to be found. A catch-all solution to minimize deviation as opposed to a custom solution for each image. These images illustrate this, with the green being predicted points, and the red being the actual points for the left hand:
   
So, I was wondering what might be causing this, be it the model, the data, or both, because nothing I've tried with either modifying the model or augmenting the data seems to have done any good. I've even tried reducing the complexity to predict for one hand only, to predict a bounding box for each hand, and to predict a single keypoint, but no matter what I try, the results are pretty inaccurate.
Thus, any suggestions for what I could do to help the model converge to create more accurate & custom predictions for each image of hands it sees would be very greatly appreciated.
","['neural-networks', 'convolutional-neural-networks', 'computer-vision', 'tensorflow', 'convergence']",
When is an object detection approach over a CNN approach appropriate?,"
I understand that CNNs are for image classification while object detection is for localization + classification of the objects detected. However, in particular, AI for chest radiographs, why is object detection used? If a CNN has 99% accuracy, should object detection still be considered? I see a lot of research papers on object detection with x-ray data but they don't explain why object detection is better than CNNs. While object detection allows users to see ""where"" the object is located, does this even matter if we get such high accuracy already? Also, if the location really does matter, can't we just use the heat maps from CNN?
","['neural-networks', 'convolutional-neural-networks', 'object-detection', 'applications']","
I think you might have misunderstood 2 concepts here: CNNs and Object Detection.
Object Detection is an AI approach to solve problems where you are interested in both the location and the classification of key elements in the image. On the other hand Image Classification is another approach where you are interested in classify the whole image with a tag.
Those are very well known approaches to solve computer vision problems through AI. There lots of approaches, you select the one that outputs the information you are most interested in.
CNNs is network type. You can build Object Detectors or Image Classifiers with CNNs, but you can also build it with Transformers, with Multi Layer Perceptrons, with LSTM, even there are some approaches with Reinforcement Learning.
Going back to your problem of AI for chest radiographs, when you see 99% accuracy it is probably in Image Classification (predict probability of bone broken in the image). On the other hand Object Detection is a more informative approach because it locates the places where a bone could be broken and the probability of that bone being broken. It is more informative because the doctor now knows where to look in the chest radio image.
"
Do we need automatic hyper-parameter tuning when we have a large enough dataset?,"
Hyperparameter tuning is the process of selecting the optimal hyperparameters for an ANN.
Now, my guess is that, if we have sufficient data (say, 1.4 million for, say, 6 features), the model can be optimally trained and we don't need a hyperparameter tuner (like Keras-Tuner), because, while training, the data itself will optimize the model.
Do we need a hyperparameter tuner if we have a sufficient number of random data for training our ANN model?
","['neural-networks', 'hyperparameter-optimization', 'training-datasets']","
You don't NEED a hyperparameter tuner, but it can help in various situations.  For example, if your model is not training well, perhaps using a tuner can help.
It's hard to say in which hyperparameters you would be turning over in your specific model, but for some specific hyperparameters if you choose a bad value your model won't learn or diverge.  Take for example the learning rate, if you pick a value too high it will overshoot minimums and the error might constantly grow (divergence), or if you pick a value too low it will get stuck in a local minimum and not be able to continue learning. You can have the world's largest dataset, but if you don't pick the correct range for your learning rate, the model will not properly learn.
In general, if you're not confident about specific hyperparameters ranges, then hyperparameter tuning can be a helpful tool, regardless of your dataset size
"
"In mini-batch gradient descent, are the weights updated after each batch or after all the batches have gone through an epoch?","
Say I have a mini-batch of size 32, and I have 10 such batches. Assuming I only run it for one epoch (just for the sake of understanding it), Will the weights be updated using the gradients of one mini-batch, or will it be done after all the 10 mini batches have passed through?
Intuitively for me, it ought to be the first one because otherwise, the only difference between Batch-GD and mini-batch GD will be the size of the batch.
","['neural-networks', 'backpropagation', 'gradient-descent', 'weights', 'mini-batch-gradient-descent']",
What sort of out-of-the-box technology could be used to create work similar to artist Refik Anadol? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 9 months ago.







                        Improve this question
                    



Refik Anadol has machines view actual pictures and then has the machine create its own images. This video shows some of the stuff he does.
What kind of out-of-the-box tools (e.g. a Python package) or algorithms produce similar things to what he does?
I am hoping to play around with it and see what can happen.  Not sure what to even Google for or search for.
","['reference-request', 'image-recognition', 'image-generation', 'algorithm-request', 'artificial-creativity']","
For a Google term you could use ""computational creativity"". It covers a wide range of ideas, and the artist here is not using one single tool or approach.
There are clearly a range of different techniques that went into the artist's installations that were shown on the video. I have an idea about a few of them:

Some are from basic animation and story-telling, and not ""generated"" art - at one stage the installation lists all the files that were used for instance. This is a way to communicate something about how the piece was produced. It would have been constructed into a video sequence by the artist, probably without any AI, although it is always possible to write a script that produces ""decisions"" from some kind of arbitrary mapping, and an artist may decide to cede control in that way as part of a piece.

There is some kind of particle system or physics-based simulation behind the flowing coloured clouds of cubes (note the physics does not have to be real physics, it often is not close, but inspired by some physical process such as flowing liquids, or maybe biological such as swarming insects). There a few different frameworks to generate these - Blender could do it for example, plus many games engines are capable of such renders. It is also common to have these types of systems hooked up to a data source that provides input of perturbations - movement, forces, shapes - that drive the dynamics, and are somehow meaninful in the context of the piece. It is not clear whether the artist has done that in this case, but it is also common to hook these systems up to data inputs in real time, such as location of gallery visitors, weather data from around the world etc. Choices of colour gradients, size and shape of elements etc will be curated by the artist to capture a certain mood.

Given the rest of this installation, the flowing cubes animation could well have something to do with the photos and how they were processed. However, it is not at all clear from viewing it whether this is the case.


The mutating photos look a lot like the output of GANs (generative adversarial networks) or VAEs (variational autoencoders). The liquid-like flow between images is caused by picking a trajectory through a latent space that these image generators learn from observing many images. A popular Python implementation of StyleGAN 2 would definitley be capable of producing output of the quality seen in the video.

Potentially the overall mix of the artwork, transitions and layouts could be a curated video sequence by the artist, or it could be dynamically mixed by a function reacting to a live data feed. In this case, the mixing seems more on the human-curated side - the installation could be e.g. a 15 minute video loop showcasing a fixed rendering of interesting parts of the spaces selected by the artist.


There are probably other things. For instance, the artist has gone to some trouble to curate their data, also using machine learning techniques when they discuss filtering out images of people. I think the effort that has gone into the installation, and the desire of the artist to showcase that is why some of the installation includes storytelling about how it was made.
"
NEAT Speciation distance: How does one treat disabled connections?,"
When calculating the distance between two genomes, how does one treat disabled connections?
For example, consider the following genome:

















[1, 0.2, E]
[2, 0.1, D]
[3, 0.2, E]
[4, 0.15, E]
[5, 0.3, D]

[7, 0.25, D]
[8, 0.25, E]
[9, 0.1, E]


[1, 0.2, E]
[2, 0.2, E]
[3, 0.1, E]


[6, 0.15, E]







For $\overline{W}$, the weighted sum of common genes, do I only sum the genes that are enabled in both (1 and 3), or do I sum all of the common genes (1, 2, and 3)?
For $D$, the disjoint genes, do I only count the disjoint enabled genes (4 and 6), or do I count all of them (4, 5, and 6)?
For $E$, the excess genes, do I only count the enabled genes (8 and 9), or do I count all of them (7, 8, and 9)?
And finally, for $N$, do I count all of the genes in the larger genome or just the enabled ones?
Oh, one last question. Is gene 2 now considered disjoint since it is disabled in the first genome?
",['neat'],
How does $\alpha$ affect the convergence of the TD algorithm?,"
In Temporal-Difference Learning, we update our value function by $V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right)$
If we choose a constant $\alpha$, will the algorithm eventually give us the true state value function? Why or why not?
","['reinforcement-learning', 'reference-request', 'convergence', 'temporal-difference-methods', 'learning-rate']","
In general, NO.
You don't get the ""true"" state value function.  TD-learning approximates the true value function. It can be a very close, or even exact approximation in simple cases, but, in general, it is just an approximation.
Depending on the difficulty of the problem, a non-constant $\alpha$ value can help the policy approximate the true value function more quickly, or help the learning from getting stuck in a local minimum.
There are implementations, like ADAM, which will adaptively change the learning rate for each feature.
Usually, you can expect convergence must faster if you adaptively change the learning rate by using an implementation like ADAM (see this).
"
Why does TD (0) converge to the MLE solution of the Markov model?,"
Why does TD (0) converge to the MLE solution of the Markov model?
Let's take the Example 6.4 in Sutton and Barto's book as an example.

Example 6.4: You are the Predictor Place yourself now in the role of the predictor of returns for an unknown Markov reward process. Suppose you observe the following eight episodes:
$A,0,B,0; B,1;B,1 ;B,1 ;B,1 ;B,1 ;B,1 ;B,0$
...
But what is the optimal value for the estimate $V(A)$ given this data? Here there are two reasonable answers. One is to observe that $100 \%$ of the times the process was in state $A$ it traversed immediately to $B$ (with a reward of $0$); and because we have already decided that $B$ has value $\frac{3}{4}$, therefore $A$ must have value $\frac{3}{4}$ as well. One way of viewing this answer is that it is based on first modeling the Markov process, in this case as shown to the right, and then computing the correct estimates given the model, which indeed in this case gives $V(A)=\frac{3}{4}$. This is also the answer that batch $\mathrm{TD}(0)$ gives.

Given the TD(0) update rule $V(S) \leftarrow V(S)+\alpha\left\lceil R+\gamma V\left(S^{\prime}\right)-V(S)\right]$, how can we deduce that it will get the MLE solution and thus $V(A) =\frac{3}{4}$?
","['reinforcement-learning', 'reference-request', 'proofs', 'convergence', 'temporal-difference-methods']",
Is there a benchmark for multi-objective evolutionary algorithms?,"
I'm working on a project for an evolutionary algorithms course, and the problem we're trying to solve is multi-objective. We'll use NSGA-II but we also wanted to compare with some other MOEAs, however, we haven't been able to find good comparisons/benchmarks of these algorithms, so we don't really know how to decide.
Any insights will be appreciated.
","['reference-request', 'evolutionary-algorithms', 'benchmarks', 'moea', 'nsga-2']",
References for the convergence of gradient-based algorithms for training neural networks,"
I'm looking for some good references that give convergence results of training neural networks. I'm decently familiar with works that analyze the convergence of SGD, and, in particular, I really like this paper Optimization Methods for Large-Scale Machine Learning. I'm looking for works that talk about the convergence of SGD (or possibly a different gradient-based algorithm) specifically for neural networks.
An example of the type of paper I'm looking for is Convergence Analysis of Two-layer Neural Networks with ReLU Activation.
","['neural-networks', 'training', 'reference-request', 'convergence', 'gradient-methods']",
Can RL still learn in a scenario where current state and the next state are independant?,"
I am trying to implement reinforcement learning into my real-world problem.
One thing making me hesitant to apply RL is that this real-world problem of mine is unique in a way how every state is independent of one another. The action taken by the agent at timestep t is the only thing that affects the state at the next timestep. (For example, in the cycle of ""state-action-reward-next state"", the ""next state"" is solely dependent on the ""action"" but not the ""state"".)
I am wondering if the RL could still be able to learn through this scenario. If not, what other methods could be an option?
","['machine-learning', 'reinforcement-learning', 'markov-decision-process']",
Are there standardized forms of the Turing Test?,"
Most computer science instructors will tell you that the Turing Test is more a theoretical or conceptual thought experiment than an actual exam that someone (or something!) can formally sit and receive a score on. A thread here on AI Stack Exchange confirms this.
Considering all of this, have there been any significant attempts to create a standardized form of a Turing Test that could be rolled out widely and used to evaluate various AI constructs? Obviously, none of these standardized testing systems could be considered The Only True Turing Test (TM), but perhaps they could have their place in research as a way to benchmark or categorize various algorithms or evaluate the work of students.
For example, I'm imagining hearing a graduate student muttering the following:

My AI construct passes the Johnson-Smith Turing Test 1992 and the Hernandez-Dorfer 2017, but it's still failing the Takahashi-2003 Advanced Elite. What am I doing wrong? Maybe if I tweak this routine here. [click]. Darn, still fails.

Either a fully automated test (e.g. just login and click to sit the exam) or a standardized system involving trained human judges (e.g. a la 21st century medical board exams) who apply standardized written rubrics would be acceptable as long as the criteria for passing are standardized rather than left to the judgment of untrained personnel or random passersby.
","['implementation', 'intelligence-testing', 'turing-test', 'testing']",
Probability that two words appear in the same sentence,"
How can I know if two words are likely to appear in the same sentence in (British) English (or English in general to enhance the chance of getting a result).
As I don't have access to a powerful machine, is there any relevant website? Or a pretrained model I can use? Or something else?
",['natural-language-processing'],
Is there any reason behind bias towards max pooling over avg pooling?,"
Consider the following excerpt taken from the chapter named Using convolutions to generalize from the textbook titled Deep Learning with PyTorch by Eli Stevens et al.

Downsampling could in principle occur in different ways. Scaling an
image by half is the equivalent of taking four neighboring pixels as
input and producing one pixel as output. How we compute the value of
the output based on the values of the input is up to us. We could

Average the four pixels. This average pooling was a common approach early on but has fallen out of favor somewhat.
Take the maximum of the four pixels. This approach, called max pooling, is currently the most commonly used approach, but it has
a downside of discarding the other three-quarters of the data.
Perform a strided convolution, where only every $N$-th pixel is calculated. A $3 \times 4$ convolution with stride 2 still incorporates input
from all pixels from the previous layer. The literature shows promise
for this approach, but it has not yet supplanted max pooling.


The paragraph is mentioning that the research community is biased towards max-pooling than average pooling. Is there any rational basis for such bias?
","['deep-learning', 'max-pooling', 'pooling', 'average-pooling']","
I've found out rather a good explanation on Quora.

Max pooling extracts the most salient features - edges, cusps, whatever.
Average pooling operates smoothly - collects features, that are relevant to any part of the image.
Max pooling throws some information, It can be thought as some sense of ""forgetting"", whereas Average pooling depends on the whole input, despite the output representation is compressed.
There are cases, where each of these operations may not be good for feature extracting (from here):

In th first case Max Pooling will produce simply a white background, and in the second after Average pooling one will get a pale grey strip (although I think, it should be lighter, than depicted).
Probably It would make sense, to mix these two approaches, and pool half of the filters with Max Pooling, and another half with Average Pooling, although, I am not aware of the use of this approach in the literature.
The most flexible and expressive approach is the stridden convolution and the Average pooling, for example, is a particular case of it, but it introduces a certain (although not big) additional cost for storage of new parameters.
"
Training a reinforcement learning agent that can decide to continue or end the game,"
I am trying to use reinforcement learning to let an agent learn simultaneously how to play a game and when to end a game.
The task is to find a single target in a grid of locations. At each time step, the agent needs to make a series of decisions:

It believes the target is at the currently inspected location. End the trial and see whether the result is correct.
It believes the target is not at the currently inspected location. It then needs to pick another location to check at the next timestep.

If the agent is choose decision #2, the environment will give some hints on where the target is, with some stochastic noise. The noise level depends on the distance between the true target location and currently inspected location. The shorter the distance, the lower the noise. The goal is to let the agent perform the task as fast and accurately as possible, so the agent needs to learn when to stop the trial, and how to select the next inspected location given the hints. The agent also has a internal memory so it won't select previously inspected locations. I would like to compare the agent's speed-accuracy trade off to human's.
In a previous simplified version of the task, the environment ends the trial once the agent hit the target location, so the agent only needs to learn how to choose the next location to inspect. I used a simple Q-network and it works well. I also found that the network should be a fully convolutional network because fully connected layers are not spatially shift-invariant.
Now how can I modify the existing convolutional network to satisfy the new task requirement? Or should I use a new network architecture?
","['reinforcement-learning', 'deep-rl']","
I assume your agent also has to choose which locations to visit next. If so, then there are two rough designs that crossed my mind.
You can use separate agents, one for choosing to inspect or not, and one to choose which adjacent cell to visit. Sum all of the log likelihoods of both agents' actions for the loss.
One particular benefit of this design is that if you can prepare the data, you can separately train the agents for awhile, and maybe train them jointly afterwards. See if the separate pre-training help improve the performance.
Other choice is to pad the ""inspect current location"" action alongside the ""location selecting"" actions, #1 inspect current location, #$2$ until #($N+1$) visit next location, given $N$ is the number of possible locations to visit.
The challenge of the two designs is how to represent this inspect action or the raw features of this action(?). If you have a domain knowledge from the game for this, maybe it can help you design it. Else you can just try with dummy features for the inspect action (maybe all zeroes)  with the same length of the locations' features.
"
What are the defining moments that make community realise the potential of deep learning?,"
Consider the following paragraph from the chapter named pre-trained models from the textbook titled Deep Learning with PyTorch by Eli Stevens et al.

The AlexNet architecture won the 2012 ILSVRC by a large margin, with a
top-5 test error rate (that is, the correct label must be in the top 5
predictions) of 15.4%. By comparison, the second-best submission,
which wasn’t based on a deep network, trailed at 26.2%. This was a
defining moment in the history of computer vision: the moment when the
community started to realize the potential of deep learning for vision
tasks. That leap was followed by constant improvement, with more
modern architectures and training methods getting top-5 error rates as
low as 3%.

This paragraph is mentioning a moment that makes the community realize the potential of deep learning. Are there any other similar defining moments in the history of deep learning?
","['deep-learning', 'history']",
What is the name of the method for the smart extend of image surroundings?,"
I'm looking for the name of the method (or algorithms family, or research body) used for the smart extend of image surroundings.
For example, the method I'm looking for would take this image:

And smartly extend it into:

So that the grass and the surrounding scenery are all generated to fill the desired area.
Generally speaking, what I'm looking for should smartly generate surroundings including entities such as tree trunks and branches, grass patterns, mountains slopes, clouds patterns, water bodies like puddles, shrubs, stones on ground, and so on.
Also, it would be nice to know how mature is this technology, i.e. how well can different entities be smartly extended.
Note that Seam Carving is a candidate (used in Photoshop under the name Content-Aware Scale (see this for example)), but I'm looking for something smarter, I think, and I'm not really sure if it can do what I'm looking for.
","['reference-request', 'image-processing', 'image-generation']",
What is the purpose of hard distillation?,"
In order to get a smaller model, one often uses larger model, that performs reasonably well on the data as a teacher, and uses the information from large model to train the smaller one.
There are several strategies to do this:

Soft distillation
Given logits of the teacher one adds the KL-divergence between the student logits and teacher logits to  the loss:
$$
\mathcal{L}_{loss} = (1 - \alpha) \mathcal{L}_{BCE} (y_{student}, y_{true}) + 
\lambda  \mathcal{L}_{KL} (y_{student}, y_{teacher})
$$
Intuition behind this approach is clear - logits are more informative than a single target label and seemingly allow for faster training.

Hard distillation
One adds the BCE between student logits and teacher model outputs as if they were true labels.
$$
\mathcal{L}_{loss} = (1 - \alpha) \mathcal{L}_{BCE} (y_{student}, y_{true}) + 
\lambda  \mathcal{L}_{BCE} (y_{student}, y_{teacher})
$$


And the benefit of the last approach is unclear to me. For the perfect model, one will have no difference with the vanilla training procedure, and for the case, where the teacher makes mistakes, we will optimize the wrong objective.
Despite these concerns, it was shown experimentally in several papers, and in
the Deit, that this objective can improve performance. Even more, it is better, than soft distillation.

Why can this be the case?
","['image-recognition', 'transformer']","
There are multiple confounding factors here. Firstly, learning from the mistakes is not necessarily bad if encouraging the student model to make these mistakes improves its generalisation†. This can be a result of there being an insufficient sampling around these incorrectly predicted data points, incorrect human-labelling, or these points are just too hard to classify anyway.
If you train a student network solely on the incorrect predictions of the teacher (no ground truth) it will generalise to some extent. This is because the distillation process is doing some form of function matching. Interpolating between these incorrect predictions is then a reasonable function for classifying the training data, hence the student will non-trivially generalises.
Another perspective is that regardless of these two methods (hard or soft), the teacher is still transferring the translational equivariance to the student, which is a strong inductive bias for generalisation.
In summary, although the correlation between classes of the teacher predictions is one important component attributed to the success of distillation, there are a lot more factors to consider which then explain why hard distillation works.
†: Alternatively, we can say the student may accurately predict more of the training data on average by making these few mistakes anyway.
"
Validation set performance increasing even after seemingly overfit on training set,"
I am training a semi-supervised GAN network using data from multiple subjects. I separated the labeled and unlabeled data based on my subjects, so there is no leakage, while having much more unlabeled data than labeled data. After few epochs training accuracy hits 100% which normally indicates overfitting, however the performance on the validation and test sets keeps increasing for 200-300 epochs. Is this considered overfitting and is there an explanation for this behavior?

","['generative-adversarial-networks', 'overfitting', 'semi-supervised-learning']","
You did a great job at this...
You can use the Tensorflow’s LogSumExp built-in function to avoid numerical problems. This routine prevents over/under flow issues that may occur when LogSumExp encounters very extreme, either positive or negative values.
You have sorted out this:
There need to be Images from the generator. To these ones, the discriminator learns to classify as fake.
Real images with labels. These are image label pairs like in any regular supervised classification problem.
Real images without labels. For those, the classifier only learns that these images are real.
I would recommend you visit this link:
Semi-supervised learning with Generative Adversarial Networks (GANs)
Good luck.
"
Can we learn a policy network via a sequence of manually determined actions?,"
In policy gradients, is it possible to learn the policy if the chain of actions is selected and performed manually/externally (e.g. by myself or by someone else who I have no influence over)?
For example, we have four actions, and I choose in the beginning an action 2, and we end up in a given state, then I choose action 4 and we end up in another state, etc. (the actions can follow some logic or not but the question is general; some of the actions will end up with positive rewards).
Can we learn any meaningful policy network from such a chain of actions?
","['reinforcement-learning', 'policy-gradients']",
Is it possible to tell the Reinforcement Learning agent some rules directly without any constraints?,"
I try to apply RL for a control problem, and I intend to either use Deep Q-Learning or SARSA.
I have two heating storage systems with one heating device, and the RL agent is only allowed to heat up 1 for every time slot. How can I do that?
I have two continuous variables $x(t)$ and $y(t)$, where $x(t)$ quantifies the degree of maximum power for heating up storage 1 and $y(t)$ quantifies the degree of maximum power for heating up storage 2.
Now, IF $x(t) > 0$, THEN $y(t)$ has to be $0$, and vice versa with $x(t)$ and $y(t)$ element $0$ or $[0.25, 1]$. How can I tell this to the agent?
One way would be to adjust the actions after the RL agent has decided about that with a separate control algorithm that overrules the actions of the RL agent. I am wondering if and how this can be also done directly? I'll appreciate every comment.
Update: Of course I could do this with a reward function. But is there not a direct way of doing this? Because this is actually a so called hard constraint. The agent is not allowed to violate this at all as this is technically not feasible. So it will be better to tell the agent directly not to do this (if that is possible).
Reminder: Can anyone tell me more about this issue? I'll highly appreciate any further comment and will be quite thankful for your help. I will also award a bounty for a good answer.
",['reinforcement-learning'],"
You could just tweak your reward function to include this restrictions.
In the most simple case, you could reward your agent -1 if $x(t) > 0$ and $y(t) \neq 0$.
The scale of your negative reward depends on your general reward scaling of course.
"
Transferring a Q-learning policy to larger instances,"
How do I best transfer and fine-tune a Q-learning policy that was trained on small instances to large instances?
Some more details on the problem:
I am currently trying to derive a decision policy for a dynamic vehicle dispatching problem.
In the problem, a decision point occurs whenever a customer requests delivery. Customers expect to be delivered within a fixed amount of time and the objective is to minimize the delay. The costs of each state are the delays realized in that state (i.e., the delay of the customers that were delivered between the last two states.
Some details on the policy:
I used a Q-learning policy (Dueling Deep Q Network) to estimate the discounted future delay of assigning an order to a vehicle. The policy was trained on a small-scale instance (5 vehicles ~100 customers) using epsilon-greedy exploration and a prioritized experience replay. I did not use temporal difference learning (as the cost of a decision are only revealed later in the process) but updated the policy after simulating the entire instance.
My problem:
As it is, the policy transfers well to instances of larger sizes (up to 100 vehicles and ~2000 customers) and I could do without fine-tuning. However, there certainly is room for the policy to improve. Unfortunately, when I try to fine-tune the initial model on the larger instance, the retrained model becomes worse over the training steps with regards to minimizing delay.
I suspect that large gradients play a role here as the initial q-values, trained on the small instance, are obviously way off for the large instances (due to the increase in customers).
Is there a standard approach to deal with such a transfer problem or do you have any suggestions?
","['deep-rl', 'q-learning', 'dqn', 'transfer-learning']",
When would it make sense to perform a gradient descent step for each term of a loss function with multiple terms?,"
I am training a neural network using a mini-batch gradient descent algorithm.
Now, consider the following loss function, which is composed of 2 terms.
$$L = L_{\text{MSE}} + L_{\text{regularization}} \label{1}\tag{1}$$
As far as I understand, usually, we update the weights of a neural network only once per mini-batch, even if the loss function is composed of 2 or more terms, like in equation \ref{1}. So, in this approach, you calculate the 2 terms, add them, and then update weights once based on the sum.
My question is: rather than summing the 2 terms of the loss function $L$ in equation \ref{1} and computing a single gradient for $L$, couldn't we separately compute the gradient both for $L_{\text{MSE}}$ $L_{\text{regularization}}$, then update the weights of the neural network twice? So, in this case, we would update the weights twice for each mini-batch. When would this make sense? Of course, my question also applies to the case where $L$ is composed of more than 2 terms.
","['deep-learning', 'objective-functions', 'mini-batch-gradient-descent']","
Technically, nothing prevents you from doing so. When you have mulitple losses, you may call .backward() at each term separately.
However, I wonder, whether it makes sense to optimize each individual path as a separate objective, since if we have multiple of them - we would like to solve several tasks simultaneously.
Probably, it could be beneficial as some kind of regularization. One makes steps away from the gradient, but overall in the direction, which makes the model less prone to overfitting. But the choice of the batch size, learning rate - seem more straightforward way to achieve this. There is also a ~2x times additional computational overhead since we backpropagate twice.
In some sense, It is done in the training of GAN's. Instead of backpropagating for discriminator and generator simultaneously - one calculates the loss separately for each model and updates weights.
"
What is the effect of gradient clipping by norm on the performance of a model?,"
It is recommended to apply gradient clipping by normalization in case of exploding gradients. The following quote is taken from here answer

One way to assure it is exploding gradients is if the loss is unstable
and not improving, or if loss shows NaN value during training.
Apart from the usual gradient clipping and weights regularization that
are recommended...

But I want to know the effect of gradient clipping by normalization in the performance of the model in normal or general cases.
Suppose I have a model and I run up to 800 epochs without gradient clipping because of the reason that there are no exploding gradients. If I run the same model with gradient clipping by norm, even if it is not necessary, then does the performance of the model decline?
","['neural-networks', 'deep-learning', 'gradient-descent', 'gradient-clipping']",
"What is the difference between ""Syllogism"" and ""Law of Syllogism""?","
The logical arguments are the basis for Artificial Intelligence. That is why I picked AI community to ask my question.
Reading from Wikipedia,

A syllogism is a kind of logical argument that applies deductive
reasoning to arrive at a conclusion based on two propositions that are
asserted or assumed to be true.

Again from Wikipedia, deductive reasoning,

is the process of reasoning from one or more statements (premises) to
reach a logical conclusion.

As part of deductive reasoning, it lists Modus Ponens, Modus Tollens, and Law of syllogism where Law of syllogism is defined as

In term logic the law of syllogism takes two conditional statements
and forms a conclusion by combining the hypothesis of one statement
with the conclusion of another.

Based on these articles, is it safe to assume that Syllogism uses Modus ponens, Modus tollens, and Law of syllogism to arrive at conclusion? Does that mean ""Law of syllogism"" is part of ""syllogism"" or is it something different?
P.S.
If this community is not appropriate for this kind of question, kindly guide me to the correct StackExchange or other communities.
","['comparison', 'logic', 'symbolic-ai', 'reasoning']",
Is the multi-head attention in the transformer a weighted adjacency matrix?,"
Are multi-head attention matrices weighted adjacency matrices?
The job of the multi-head-attention mechanism in transformer models is to determine how likely a word is to appear after another word. In a sense this makes the resulting matrix a big graph with nodes and edges, where a node represents a word and an edge the likelihood to appear after that. So basically it is an adjacency matrix that is created.
","['deep-learning', 'transformer', 'attention']","
Short answer, yes I believe we can! One way feels more meaningful that the other. First, let's look at some nuance in the definition of attention. If $\text{score}(x_i, x_j) = \text{score}(x_j, x_i)$, then the attention matrix is symmetric and naturally has the form of a weighted adjacency matrix. For example, this happens when attention is given by a simple dot product $\text{score}(x_i, x_j) = \langle x_i, x_j \rangle = x_i^Tx_j$. This also happens if we have a learnable matrix $A$ and $\text{score}(x_i, x_j) = x_i^TAx_j$ if $A$ is symmetric. We can then view the attention matrix $\alpha_{i,j} = \text{Attn}_{i,j}(X)$ as a weighted adjacency matrix where the nodes represent input tokens, and edge weights correspond to similarity scores (as defined by the inner product, scaled inner product, or the symmetric matrix $A$). Now, for the following definition of attention, this makes a little less sense, as the edge connecting token nodes in graph corresponding to tokens $x_i$ and $x_j$ is not the same as the weight on the edge connecting $x_j$ to $x_i$, in general. Suppose $X \in \mathbb{R}^{d \times n}$ has as columns $X_i$ the $d$-dimensional embeddings of the $n$-tokens $x_1, x_2, ..., x_n$ from your input. Now, let
$$W_QX = Q$$
$$W_KX = K$$
$$W_VX = V$$
be the learned weight matrices giving the ""query"" $q_i = W_QX_i$, ""key"" $k_i = W_KX_i$, and ""value"" $v_i = W_VX_i$ vectors. Then we can define attention as
\begin{align}
\text{Attn}(X) &= \text{softmax}\left( \frac{Q^TK}{\sqrt{d}} \right)V \\
               &= \text{softmax}_j\left(\text{score}(x_i, x_j)\right)
\end{align}
From this we can derive,
$$ \text{Attn}_{i,j}(X) = \frac{\exp \left( \frac{\langle q_i, k_j \rangle}{\sqrt{d}}\right)}{\sum_k \exp\left( \frac{\langle q_i, k_k \rangle}{\sqrt{d}} \right)}.$$
Now, note, we have turned each column into a probability distribution by applying the softmax so we have
$$
P(X_i) = \text{softmax}\begin{pmatrix}
\frac{\langle q_i, k_1 \rangle}{\sqrt{d}}\\
\frac{\langle q_i, k_2 \rangle}{\sqrt{d}}\\
\vdots \\
\frac{\langle q_i, k_n \rangle}{\sqrt{d}}
\end{pmatrix}.
$$
Now, if we adjust for masked self attention, we can represent the attention mechanism as a directed graph (with weighted self-loops) as the entries in the attention matrix above the diagonal are zero, and so all edges are directed at token nodes that come ""after"" them.
There is another way to understand attention using graphs when we view attention through the lens of (complete) graph attention networks as explained here.
There is also a visualization you can find here that shows the attention mechanism as a graph. It is equivalent to the graph attention formulation, but is more interactive and pretty.
"
What kind of algorithm or approach can I use to find a specific type of object in an image?,"
What kind of algorithm or approach can I use to find a specific type of object in an image?
In particular, I am interested in finding an object like a windmill in an image taken, for example, from Google Maps. The image could be something like this

","['machine-learning', 'computer-vision', 'object-detection', 'algorithm-request']","
The computer vision problem that you are describing is object detection, i.e. the problem of finding the location of specific objects in an image and label them correctly with their names.
There are many resources on the web (or in books) that describe this problem more in detail and examples (which also include code) to get you started with it (e.g. this one).
In any case, to solve this problem, you will (probably) need a labelled dataset $D$. So, if you don't have it, you will need to (manually) collect many images, similar to the ones you describe, and find the objects of interest in them (i.e. their locations, which may be specified as a bounding box), and assign the corresponding name to each of them. At the end of this process, you should have a dataset of the form $D = \{(x_1, y_1), \dots, (x_1, y_1) \}$, where $N$ should be as big as possible, $x_i$ is an image, and $y_i$ are the labels (also known as targets), which, in this case, should be both the location and name of all objects of interest in $x_i$. The specific format of the labels depends also on the specific model that you will train to solve this problem. You can find different models online for this task, such as YOLO.
"
How to decide a train-test split?,"
In almost every ML model, a train-test (or train-test-val split) is critical to assess the model's performance. However, I have always wondered what the rationale is to decide a particular train-test split. I've seen that some people like an 80-20 split, others opt for 90-10, but why? Is it simply a matter of preference? Also, why not 70-30 or 60-40, what is the best way to decide?
","['machine-learning', 'training', 'cross-validation', 'testing']",
A recommender system based on millions of fields including text and number,"
I want to train a model based on millions of fields, including text and number, that are stored in a SQL database and recommend a perfect match based on some inputs. Now, which algorithm is the best for this problem?
For instance, consider this database pattern:




Title
Content
Volume
Count




First
row1
5.36
34


Second
row2
36.1
239


...
...
...
...



 ","['machine-learning', 'natural-language-processing', 'recommender-system']","
The first step
You need to decide if you want to hold each string column or not. Then you must encode your text fields into numbers which you need to use some embedding algorithms like word2Vec. Check here.
Second step
Probably, you will have a lot of columns. Now, you need to reduce the space dimension. PCA, manifold transforms, partial least squares regression, etc., may help you in this way.
Third step
Here, you will have nice and tidy tabular data which you can feed into any Recommender System you want to.

I presumed that you mean millions of columns when you say ""millions of fields"".

If you mean millions of rows, thenprobably need to use methods thatdealingh Big Data.


"
Different equations for Yolov3 in courses/ articles and Darknet GitHub code?,"
I am confused by the equations for bounding boxes I find online. Some articles say that
box_width = anchor_width * exp(residual_value_of_box_width))

and the coordinates have a sigmoid function.
Eg: https://www.kdnuggets.com/2018/05/implement-yolo-v3-object-detector-pytorch-part-1.html
https://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html

But Darknet code and GitHub have equations dividing coordinates and box width with image width.
For example, https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/data/voc/voc_label.py
def convert(size, box):
    dw = 1./size[0]
    dh = 1./size[1]
    x = (box[0] + box[1])/2.0
    y = (box[2] + box[3])/2.0
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x*dw
    w = w*dw
    y = y*dh
    h = h*dh
    return (x,y,w,h)

If the image width is used, then what is the use of anchor box width/height values in yolov3.cfg file? I can't find where it has been used in the source code other than generate the anchors file.
","['neural-networks', 'implementation', 'yolo', 'bounding-box']",
Should I train a neural network with data with or without a constraint?,"
I want to train a Neural Network (NN) using a dataset. I want to use the NN model as a prediction function in one algorithm. However, in the algorithm, any data that does not meet a specific constraint (say some parameter $\theta <10$) would not be included.
So, my question is, while generating the training data, should I include all kinds of inputs irrespective of the constraint, or should I generate only those data which meet the constraint $\theta <10$?
Currently, I am training data with constraint ($\theta <10$), and I am getting an average error of around $6\%$. Ideally, I want it below $3\%$.
I am new to NN model training. Any kind of pointers would be helpful.
","['neural-networks', 'training', 'prediction', 'training-datasets']",
"What does ""unknown search spaces"" mean in the context of Evolutionary Algorithms?","
In the article Multi-Verse Optimizer: a nature-inspired algorithm for global
optimization (DOI 10.1007/s00521-015-1870-7), it's written

The results of the real case studies also demonstrate the potential of MVO in solving real problems with unknown search spaces

where MVO stands for Multi-Verse Optimizer.
What does ""unknown search spaces"" mean in the context of Evolutionary Algorithms and, especially, in the context of the Multi-Verse Optimizer?
","['terminology', 'search', 'evolutionary-algorithms', 'neuroevolution']",
"Did the unsolved XOR problem in ""Perceptrons: An Introduction to Computational Geometry"" 1969 book really cause the winter of the AI in 1974?","
Winter of AI definition:

periods of reduced funding and interest in artificial intelligence research, due to unmet expectations after a period of hype. There have been at least two major AI winters in 1974-1980 and 1987-1993.

I read this question: Did Minsky and Papert know that multi-layer perceptrons could solve XOR?

Minsky and Papert show that a perceptron can't solve the XOR problem. This contributed to the first AI winter, resulting in funding cuts for neural networks. However, now we know that a multilayer perceptron can solve the XOR problem easily.

Does someone have evidence to support that the ""unsolved XOR problem"" in Minsky and Papert's book caused the first winter of the AI? Or was it a succession of unsolved problems for neural networks and loss of interest that caused the cuts on funding?
","['reference-request', 'history', 'perceptron', 'xor-problem', 'ai-winter']",
Closed networks vs Networks with a removed delay to predict new data,"
I've come across two types of neural networks to predict, both from Matlab, the closed structure and the net that removes one delay to find new data.
From Matlab's app generated scripts we see:

% Closed Loop Network
% Use this network to do multi-step prediction.
% The function CLOSELOOP replaces the feedback input with a direct
% connection from the output layer.

netc = closeloop(net);
netc.name = [net.name ' - Closed Loop'];
view(netc)
[xc,xic,aic,tc] = preparets(netc,{},{},T);
yc = netc(xc,xic,aic);
closedLoopPerformance = perform(net,tc,yc)


% Step-Ahead Prediction Network
% For some applications it helps to get the prediction a timestep early.
% The original network returns predicted y(t+1) at the same time it is
% given y(t+1). For some applications such as decision making, it would
% help to have predicted y(t+1) once y(t) is available, but before the
% actual y(t+1) occurs. The network can be made to return its output a
% timestep early by removing one delay so that its minimal tap delay is now
% 0 instead of 1. The new network returns the same outputs as the original
% network, but outputs are shifted left one timestep.

nets = removedelay(net);
nets.name = [net.name ' - Predict One Step Ahead'];
view(nets)
[xs,xis,ais,ts] = preparets(nets,{},{},T);
ys = nets(xs,xis,ais);
stepAheadPerformance = perform(nets,ts,ys)

My question is: What is the real difference between them?
Can one uses them equivalently? If yes, why? I mean, even tho the structure or how they are equipped, could be very very different, e.g. one is apple, the other is grape?
As far as I understand both can return new data if one codes them for that. For example, taking the closed net, one can predict 10 new values. Taking the net that removes one delay, one can predict one new value, but if one does this recursively 9 times, one can get the new 10 data. Is there a problem in using this last net in that way?
On another side, running both codes, as they are now (this changes depending on the code one works on), yields very different performances. Why?
Update:
I've checked this page https://www.mathworks.com/matlabcentral/answers/297187-neural-network-closed-loop-vs-open-loop, and in the answer by Greg Heath, we see
[...]

OPENLOOP: The desired output, AKA the delayed target, is used as an additional input. The OL net will produce output for the common time extent of the input and target.
CLOSELOOP: The delayed target input is replaced by a direct delayed output connection. The CL net will produce output for the time extent of the input.

[...]
""The desired output, AKA the delayed target, is used as an additional input."" how is this?
""The OL net will produce output for the common time extent of the input and target."" and this?
""The CL net will produce output for the time extent of the input."" What does this mean?
","['neural-networks', 'comparison', 'prediction', 'feedforward-neural-networks', 'matlab']","




Closed Loop Network
Step-Ahead Prediction Network




The function CLOSELOOP replaces the feedback input with a direct connection from the output layer.
Step-Ahead Prediction Network also known as removedelay function helps to remove delay to neural network’s response


In Closed loop networks, its own predictions become the feedback inputs.
targets with a delay were used as feedback input


Network used to do multi-step prediction
Network used to predict one step ahead


Highly helpful to turn the network into parallel configuration
Can't be used for parallel configuration


Output is not shifted one timestep
the output is shifted one timestep.


Closed loop network continue to predict when external feedback is missing, by using internal feedback
Open-loop and Remove-delay use external feedback


Real time Configuration
Not real time configuration


Closed loop network will produce output for the time extent of the input.
will produce output for the common time extent of the input and target.








Open-Loop Network
In open-loop networks, targets were used as feedback inputs. Open loops are primarily used when future outputs are not known. Below is how open loop network looks like
Note:

The typical workflow is to fully create the network in open loop, and only when it has been trained (which includes validation and testing steps) it is transformed to closed loop for multistep-ahead prediction


UPDATED Response to newly added questions

""The desired output, AKA the delayed target, is used as an additional input."" how is this?

This is with reference to the Step-Ahead prediction network. Here, the delayed target is used as an additional input. Kindly refer to diagram in point #9. when the target is y(t+1) the additional input is the delayed target, i.e., y(t). Whereas in open-loop target is used as additional input this is the main difference between OL and step ahead prediction.

""The OL net will produce output for the common time extent of the input and target."" and this?

As you can see, the open-loop network  will produce output y(t) as long as open-loop has two inputs x(t)and the targety(t)` for a certain time

""The CL net will produce output for the time extent of the input."" What does this mean?

In closed loop based on the input x(t) the output y(t) will be produced for a certain time
Reference Link:

Official Documentation
Discussion on Closed Loop(multi step ahead prediction)
Multi-step ahead prediction using neural networks
Example of Closed loop, open-loop and remove delay
Multi-step Neural Network prediction
Difference between Open loop, Closed loop
Additional difference information

"
What are the recommended ways to change shape of feature maps channel wise other than using Convolutional neural networks?,"
Suppose I have a feature map with size $C_1 \times H \times W$. And I need to convert it into a feature map of size $C_2 \times H \times W$.
One way to do this is to use convolutional neural networks as Conv2d($C_1, C_2$)
I want to know whether there are any other ways in Literature to perform the desired operation?
","['channel', 'feature-maps']",
Is there any recommended way to perform pooling in this context?,"
Suppose I have three batches of feature maps, each of size $180 \times 100 \times 100$. I want to concatenate all these feature maps channel-wise, and then resize them into a single feature map. The batch size is equal to 10.
Consider the following code in PyTorch
import torch
from torch import nn

x1 = torch.randn(10, 180, 100, 100)
x2 = torch.randn(10, 180, 100, 100)
x3 = torch.randn(10, 180, 100, 100)


pool1 = nn.AvgPool3d(kernel_size = (361, 1, 1), stride= 1)
pool2 = nn.AvgPool3d(kernel_size = 1, stride= (3, 1, 1))

final_1_x = pool1(torch.cat((x1, x2, x3), 1))
final_2_x = pool2(torch.cat((x1, x2, x3), 1))

print(final_1_x.shape)
print(final_2_x.shape)

and its output is
torch.Size([10, 180, 100, 100])
torch.Size([10, 180, 100, 100])

You can observe that both types of polling I did are able to give a feature map of the desired size. But the first one takes a large amount of time with unsatisfactory results and the second one ignores many values in the input feature maps. I don't know whether it is okay to ignore or not.
I want to know the recommended way to perform polling in order to get the desired size of feature maps. Is there any such recommended way to perform pooling?
","['convolutional-neural-networks', 'pytorch', 'pooling']",
When to use Value Iteration vs. Policy Iteration,"
Both value iteration and policy iteration are General Policy Iteration (GPI) algorithms.  However, they differ in the mechanics of their updates.  Policy Iteration seeks to first find a completed value function for a policy, then derive the Q function from this and improve the policy greedily from this Q.  Meanwhile, Value Iteration uses a truncated V function to then obtain Q updates, only returning the policy once V has converged.
What are the inherent advantages of using one over the other in a practical setting?
","['reinforcement-learning', 'comparison', 'q-learning', 'value-functions', 'policy-iteration']","
Value iteration (VI) is a truncated version of Policy iteration (PI).
PI has two steps:

the first step is policy evaluation. That is to calculate the state values of a given policy. This step essentially solves the Bellman equation
$$v_\pi=r_\pi+\gamma P_\pi v_\pi$$
which is the matrix vector form of the Bellman equation. I assume that the basics are already known.
The second is policy improvement. That is to select the action corresponding to the greatest action value at each state (i.e., greedy policy):
$$\pi=\arg\max_\pi(r_\pi+\gamma P_\pi v_\pi)$$

The key point is: the policy iteration step requires an infinite number of iterations to solve the Bellman equation (i.e., get the exact state value).
In particular, we use the following iterative algorithm so solve the Bellman equation:
$$v_\pi^{(k+1)}=r_\pi+\gamma P_\pi v_\pi^{(k)}, \quad k=1,2,\dots$$
We can prove that $v_\pi^{(k)}\rightarrow v_\pi$ as $k\rightarrow\infty$.
There are three cases to execute this iterative algorithm:

Case 1: run an infinite number of iterations so that $ v_\pi^{(\infty)}=v_\pi$. This is impossible in practice. Of course, in practice, we may run sufficiently many iterations until certain metrics (such as the difference between two consecutive values) are small enough).
Case 2: run just one single step so that $ v_\pi^{(2)}$ is used for policy improvement step.
Case 3: run a few times (e.g., N times) so that $ v_\pi^{(N+1)}$ is used for the policy improvement step.

Case 1 is the policy iteration algorithm; case 2 is the value iteration algorithm; case 3 is a more general truncated version. Such a truncated version does not require infinite numbers of iterations and can converge faster than case 2, it is often used in practice.
"
Improving validation losses and accuracy for 3D CNN,"
I have used a 3D CNN architecture, for detecting the presence of a particular promoter (MGMT), by using FLAIR brain scans. (64 slices per patient). The output is supposed to be binary (0/1).
I have gone through the pre-processing properly, and used stratification after splitting the ""train"" dataset into train and validation sets, (80-20 ratio). My model initialisation and training kernels look like this:
def get_model(width=128, height=128, depth=64):
""""""Build a 3D convolutional neural network model.""""""

inputs = keras.Input((width, height, depth, 1))

x = layers.Conv3D(filters=64, kernel_size=3, activation=""relu"")(inputs)
x = layers.MaxPool3D(pool_size=2)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv3D(filters=64, kernel_size=3, activation=""relu"")(x)
x = layers.MaxPool3D(pool_size=2)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv3D(filters=128, kernel_size=3, activation=""relu"")(x)
x = layers.MaxPool3D(pool_size=2)(x)
x = layers.BatchNormalization()(x)

x = layers.Conv3D(filters=256, kernel_size=3, activation=""relu"")(x)
x = layers.MaxPool3D(pool_size=2)(x)
x = layers.BatchNormalization()(x)

x = layers.GlobalAveragePooling3D()(x)
x = layers.Dense(units=512, activation=""relu"")(x)
x = layers.Dropout(0.3)(x)

outputs = layers.Dense(units=1, activation=""sigmoid"")(x)

# Define the model.
model = keras.Model(inputs, outputs, name=""3dcnn"")
return model


# Build model.
model = get_model(width=128, height=128, depth=64)
model.summary()

Compile model:
initial_learning_rate = 0.0001
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True
)
model.compile(
    loss=""binary_crossentropy"",
    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),
    metrics=[""acc""],
)

# Define callbacks.
checkpoint_cb = keras.callbacks.ModelCheckpoint(
    ""Brain_3d_classification.h5"", save_best_only=True,monitor = 'val_acc', 
                             mode = 'max', verbose = 1
)
early_stopping_cb = keras.callbacks.EarlyStopping(monitor=""val_acc"", patience=20,mode = 'max', verbose = 1,
                           restore_best_weights = True)

# Train the model, doing validation at the end of each epoch
epochs = 60
model.fit(
    train_dataset,
    validation_data=valid_dataset,
    epochs=epochs,
    shuffle=True,
    verbose=2,
    callbacks=[checkpoint_cb, early_stopping_cb],
)

This is my first time ever working with a 3D CNN, and I used this keras webpage for the format:https://keras.io/examples/vision/3D_image_classification/
The (max) validation accuracy in my case was about 54%. I tried reducing the initial learning rate , and for 0.00001 I got to a max of 66.7%. For learning rates of 0.00005, 0.00002, I got max accuracy of about 60 and 62%.
Accuracy vs epoch plots for learning rates 0.0001, 0.00005,0.00002 and 0.00001:




It does seem like reducing the initial learning rate has a positive effect on accuracy, although the accuracy is still very low.
What other parameters can I tune to expect a better accuracy? And is it okay to just keep reducing the initial learning rate until we achieve a targeted accuracy?
I know this is a rather broad question, but I am quite confused as to how we should approach increasing the accuracy in the case of CNNs, (that too 3D), where there just seems to be a lot of stuff going on. Do I change something in my initialisations? Add more layers? Or change the parameters? Do I decrease or increase them? With so many things going on, I don't think trying every combination and just keep repeating the training process is an efficient idea...
Full notebook (including pre-processing steps): https://www.kaggle.com/shivamee682003/3d-image-preprocessing-17cd03/edit
","['neural-networks', 'convolutional-neural-networks', 'keras', '3d-convolution']","
What is the No Information Rate (NIR)? I.e. what are the percentages of positive and negative labels? Have you looked at the predictions of your model? If it's all 0's or all 1's then it probably learned nothing, other than predicting the majority class.
When it comes to architectural choices and hyperparameters, especially if you start working with NNs, then Andrej Karpathy's blog post called A Recipe for Training Neural Networks is a really good starting point. It gives a good reference on how to approach things in the beginning when you have not much intuition. Simply reducing the learning rate will not help much if your model is way too small. You may also find it useful to add ResNet-like skip connections to improve performance for very deep models (i.e. many layer).
"
"What do we mean by ""orderly opinions"" in this sentence in the context of Bayes theorem?","
In this page, it's written (emphasis mine)

If probabilities are thought to describe orderly opinions, Bayes theorem describes how the opinions should be updated in the light of new information

What is your understanding/definition of ""orderly"" opinion?
Maybe something like: a probability that is not arbitrarily chosen but well-founded and explainable?
","['terminology', 'books', 'bayes-theorem', 'probability-theory']","
That term exactly refers to the difference between two main paradigms in probability and statistics: Frequentism vs Bayesianism. You can find many texts for explaining the difference, for example [1] and [2].
By the way, we can briefly say it means that we assume that there exists a fixed opinion out there (like a parameter of a distribution function) and we are going to estimate it based on the observation. Despite the Baysian probability, in frequentist probability, we assume that the opinion is not fixed and changing randomly based on observations.
"
"In this paper, if region $R_{2}$ moves in a sliding window manner, won't the saliency map have a smaller size than the original image?","
In the paper Salient Region Detection and Segmentation, I have a question pertaining to section 3 on the convolution-like operation being performed. I had already asked a few questions about the paper previously, for which I received an answer here. In the answer, the author (JVGD) mentions the following:

So for each pixel in the image you overlap on top $R_{1}$ and then $R_{2}$ on top of $R_{1}$, then you compute the distance $D$ for those 2 regions to get the saliency value of that pixel, then slide the $R_{1}$ and $R_{2}$ regions in a sliding window manner (which is basically telling you to implement it with convolution operation).

Regarding the above, I had the following question:
If the region $R_{2}$ moves in a sliding window manner, won't the saliency map (mentioned in section 3.1) have a smaller size than the original image (like in convolution the output image is smaller)? If this is so, wouldn't it be impossible to add the saliency maps at different scales since they each have different sizes?
The following edit re-explains the question in more detail:

In the animation above, you can see a filter running across an image. For each instance of the filter, some calculations are happening between the pixels of the filter and the image. The result of each calculation becomes one pixel in the output image (denoted by ""convolved feature""). Here, the output image is smaller than the input image because there are only 9 instances of the filter. From what I understood of the salient region operation, a similar process is being followed i.e. a filter runs across an image, some calculations happen, and the result of each calculation becomes one pixel in the output image (saliency map). Hence, won't the saliency map have a smaller size than the original image? Furthermore, when the filter size is 3 x 3, the output image size is 3 x 3. However, if the filter size was 5 x 3, the output image size would only be 1 x 3. Clearly, the output image size is different for different filter sizes. This makes the output images (saliency maps) impossible to add. There is clearly something I am missing / misunderstanding here, and clarity on the same would be much appreciated.
P.S. There is no indication of padding or any operation of that sort in the research paper, so I don’t want to assume anything because the calculations would then be wrong.
","['computer-vision', 'papers', 'math', 'convolution', 'saliency-map']","
The paper you have provided, discusses *detecting salient regions by using a contrast determination filter

which operates at various scales to generate saliency maps containing “saliency values” per pixel.*

The Contrast detection filter used in this paper is better explained with the figure 2. Here R1 is the inner most region and R2 scale is varied. Note this is the filter and not the image itself



The below fig 3 shown in the paper section 3.1 indicates how the image appears when the filter of varying scales is applied(filtered images). When R2 has the highest scale(i.e., maximum width) the background is also shown that is the non-salient parts are also taken into consideration. As the R2 takes up lesser width, the non-salient parts becomes almost invisible which you can see in 3 to 8 images in figure3. It helps to focus on what's really important in the image, which is the man riding the horse.



If you have noted in the figure 3, though R2 width varies, the filtered image size is the same as the original image. 
To answer your question, the saliency map is the same size as the original image size as only the filter was scaled not the image itself
This is also quoted in the paper section 3.1, page 4

A change in scale is affected by scaling the region R2
instead of scaling the image.
Scaling the filter instead of the image allows the
generation of saliency maps of the same size and resolution as the input image.

Update
Let me provide you with a much more simpler explanation , 
suppose you as a human have couple of reading glasses(of varying sizes) and you are trying to read a book.

The smallest reading glass will help you focus on smaller part of the book, and you would often have to look in different parts of the book
A medium reading glass will focus on a more wider part of the book
If you have a large reading glass, you can cover the whole book

Now in above example, reading glass acts as the filter, the book acts as your input image, the output image is what you can see through the reading glass.
No matter the size of your reading glass(filter) the dimensions of the book hasn’t changed only the focus on what is observed i.e., content or the information of book you see through your reading glass is more focused and everything else is a blur.
So the output image will be the same size as the input image with the out of focused will be like a blur
"
Best way to measure regression accuracy?,"
I'm asking because classification problems have very concrete metrics like accuracy that are totally transparent to understand.
Whereas regression models seem to have a very large number of possible evaluation strategies and to me at least it is not clear which (if any) of them is as reliable/interpretable as accuracy is in classification problems.
Possible Candidates:

Regular loss (e.g MAE): MAE is potentially quite interpretable, but again interpretation depends upon distribution statistics which vary across regression problems.
MAPE/Relative Loss: This is interesting and is potentially decently similar to accuracy. Yet it has obvious draw backs, like the true value being extremely small causing explosion of loss values & there being no incorporation of overall distribution statistics for the output values.
Chi-squared test: I like the idea of this but I have not seen it used at all for NN regression for some reason. I'm not sure why and I'm curious if people think it would be a good idea to use it for that.
(adjusted) R^2 coefficient: Another statistic that seems great in theory, but again I see almost never being used for NNs and I'm not sure why. This has the great advantage of being a 'bounded'/'normalized' metric like accuracy and in theory is should be just as interpretable. Why is it not used for NNs?

","['neural-networks', 'regression']",
How matrix factorization helps with recommendations when it converges to the initial user-items matrix?,"
We can say that matrix factorization of a matrix $R$, in general, is finding two matrices $P$ and $Q$ such that $R \approx P.Q^{T}$ with some constraints on $P$ and $Q$. Looking at some matrix factorization algorithms on the internet like Scikit-Learn's Non-Negative Matrix Factorization I come to wonder how this works for recommendation systems. Generally with recommendation systems we have a user-item ratings matrix, let's denote it $R$, which is really sparse so when we look at datasets we find missing values, $NaN$. When I look at examples of using matrix factorization for recommender systems I find that the missing values are replaces with $0$. My question is, how do we get actual predictions on the items non rated by users when the dot product $P.Q^{T}$ is supposed to converge to $R$?
I have tried with this simple matrix that I found here
R = [
     [5,3,0,1],
     [4,0,0,1],
     [1,1,0,5],
     [1,0,0,4],
     [0,1,5,4],
    ]
R = np.array(R)

The algorithm I used is Scikit-Learn's and no matter how I change the parameters, I can't seem to find a matrix that has actual values in place of $0$s. It always finds a really good approximation of $R$. Maybe all the hyperparameter tuning I'm doing is leading to overfitting, and let's suppose there is a set of combination of parameters for which we don't have $0$s and still we minimize $||R-P.Q^{T}||$ with regard to some norm to a decent level, how can we be sure that the predictions are accurate? I mean, there must be many different combinations of parameters that ensure both prediction different values for the $0$s and minimizing $||R-P.Q^{T}||$ to a decent level.
Thank you!
","['recommender-system', 'scikit-learn']",
What is the best open source python repo for facial recognition? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I am looking for best open source python repo for facial recognition. Best if it uses tensorflow backend. I know you can train images to recognize. Yolo can be used if trained on face. To name the person.
But I wonder if there is any code where you can add new faces to database without training or minimum training. As new faces are added I don't want to train the network repeatedly. Also the less amount of face picture needed the better.
If code is not available any guide or research paper will also be helpful. For example what approach can I take to make an app for a person who has difficulty remembering peoples name. So the app can take a small video or few photos with name and will be able to tell the persons name in the future. Neural network should not be retrained while adding new face to database if possible.
","['neural-networks', 'machine-learning', 'deep-learning', 'python', 'facial-recognition']","
It seems your problem is more related to Face Identification than Face Recognition.
I understand you are looking for the implementation using a NN based approach, but if you're open to giving it a try to other approaches you could consider using Eigenfaces, which is based in PCA.
For that, you can find some references and code implementations.
Datasets you can use for testing face identification:

AT&T Face Database
Extended Yale Face Database B
BioID Face Database

References you can look at for additional implementation details:
https://www.researchgate.net/publication/333367462_Improving_face_recognition_of_artificial_social_companions_for_smart_working_and_living_environments
A forked repository on GitHub for this approach:
https://github.com/joaoquintas/facerec
Original repository for reference https://github.com/bytefish/facerec
"
What exactly happens in gradient clipping by norm?,"
Consider the following description regarding gradient clipping in PyTorch
torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False)


Clips gradient norm of an iterable of parameters.
The norm is computed over all gradients together as if they were
concatenated into a single vector. Gradients are modified in-place.

Let the weights and gradients, for loss function $L$, of the model, be given as below
\begin{align}
w 
&= [w_1, w_2, w_3, \cdots, w_n] \\
\triangledown 
&= [\triangledown_1, \triangledown_2, \triangledown_3, \cdots, \triangledown_n] \text{, where } \triangledown_i = \dfrac{\partial L}{\partial w_i} \text{ and } 1 \le i \le n
\end{align}
From the description, we need to compute gradient norm, i.e. $||\triangledown||$.
How to proceed after the step of finding the gradient norm? What is meant by clipping the gradient norm mathematically?
","['deep-learning', 'pytorch', 'gradient-clipping']","
Gradient clipping is a technique that tackles exploding gradients. The idea of gradient clipping is very simple: If the gradient gets too large, we rescale it to keep it small. More precisely,
$$
\text{if } \Vert \mathbf{g} \Vert \geq c, \text{then }
\mathbf{g} \leftarrow c \frac{\mathbf{g}}{\Vert \mathbf{g} \Vert}
$$
where $c$ is a hyperparameter, $\mathbf{g}$ is the gradient, and $\Vert \mathbf{g} \Vert$ is the norm of $\mathbf{g}$.
Since $\frac{\mathbf{g}}{\Vert \mathbf{g} \Vert}$ is a unit vector, after rescaling the new $\mathbf{g}$ will have norm $c$.
Note that if $\frac{\mathbf{g}}{\Vert \mathbf{g} \Vert} < c$ , then we don’t need to do anything.
Check this article  for more information
"
Implementation of MDP in python to determine when to take action clean,"
I am trying to model the following problem as a Markov decision process.

In a steel melting shop of a steel plant, iron pipes are used. These pipes generate rust over time. Adding an anti-rusting solution can delay the rusting process. If there is too much rust, we have to mechanically clean the pipe.

I have categorized the rusting states as StateA, StateB, StateC, StateD with increasing rusting from A to D . StateA is absolute clean state with almost no rust.
     StateA -> StateB -> StateC -> StateD
     ∆  ∆ ∆       |        |         |           
     |  | |       |        |         |
   Mnt Mnt Mnt    |        |         |
     |  | |_clean_|        |         |
     |  |_______clean______|         |
     |_______________________________|     
                          clean

We can take 3 possible actions:

No Maintenance
Clean
Adding Anti Rusting Agent

The transition probabilities are mentioned below:
The states degrades from StateA to StateD. State degrades with rusting with certain amount of rust denoted by transition probabilities. Adding Anti Rusting Agent decreases the probabilty of degradation of state
The transition probabilities from StateA to StateB is 0.6 with No Maintenance
The transition probabilities from StateA to StateB is 0.5 with adding an anti-rusting agent.
The transition probabilities from StateB to StateC is 0.7 with No Maintenance
The transition probabilities from StateB to StateC is 0.6 with adding an anti-rusting agent.
The transition probabilities from StateC to StateD is 0.8 with No Maintenance.
The transition probabilities from StateC to StateD is 0.7 with an anti-rusting agent.
Action clean will move any state to StateA with probability 1
Rewards for StateA is 0.6, StateB is 0.5, StateC is 0.4, StateD is 0.3
Clean action lead to Maintenance (Mnt) state which has 0.1 reward. The Maintenance state will lead to increase in productivity after cleaning which is good, but there will be shutdown while Maintenance, so there will be loss of production. So reward is less.
I am new to MDP. It will be helpful if anyone can help me in getting the decision about when should we clean through MDP through a python implementation (python codes)? Shall we Clean at StateB, Clean at StateC, Clean at StateD?
","['python', 'markov-decision-process', 'markov-property']",
How to determine the quality of synthetic data?,"
I'm working on a VAE model to produce synthetic data of X-Ray diffraction spectrums.
I try to figure out how I can measure the quality of the spectrums. The goal would be to produce synthetic data which is similar to the training data but also different from the training data. The spectrums should keep their characteristics, but should be different in terms of noise and intensity.
I trained models which can produce those type of spectrums (because I checked some of them visual), but I don't know how to quantify the difference/similarity to the origin (1) and the difference between the produced synthetic spectrums in one dataset (2).
Are there any methods to quantify these points?
","['datasets', 'autoencoders', 'generative-model', 'variational-autoencoder', 'algorithm-request']",
"When can we call a feature ""hierarchical""?","
Features in machine learning are the attributes of the elements of a data set. They are considered as random variables in probability.
Consider the following excerpt from 1.1: The deep learning revolution of the textbook named Deep learning with PyTorch by Eli Stevens, Luca Antiga, Thomas Viehmann,

On the right, with deep learning, the raw data is fed to an algorithm
that extracts hierarchical features automatically, guided by the
optimization of its own performance on the task; the results will be
as good as the ability of the practitioner to drive the algorithm
toward its goal.

When can we call a feature hierarchical? Does it refer to a random variable that is a (function on) derived from some other random variables?
","['deep-learning', 'terminology', 'features', 'random-variable']","
You can find a brief explanation of hierarchical feature selection in the following from ""An Empirical Evaluation of Hierarchical
Feature Selection Methods for Classification in
Bioinformatics Datasets with Gene Ontology-based
Features"" paper:

Hierarchical feature selection is a new research area in machine
learning/data mining, which consists of performing feature selection by exploiting dependency relationships among hierarchically structured features.

Therefore, hierarchical features correspond to the dependency structure between features.
"
What can be an example for the prior knowledge used in Deep Learning systems?,"
It is known that machine learning algorithms expect feature engineering as an initial step. Now, consider the following paragraph, taken from 1.1 The deep learning revolution of the textbook named Deep learning with PyTorch by Eli Stevens, Luca Antiga, Thomas Viehmann, regarding the role of feature engineering in deep learning

Deep learning, on the other hand, deals with finding such
representations automatically, from raw data, in order to successfully
perform a task. In the ones versus zeros example, filters would be
refined during training by iteratively looking at pairs of examples
and target labels. This is not to say that feature engineering has
no place with deep learning; we often need to inject some form of
prior knowledge in a learning system. However, the ability of a neural
network to ingest data and extract useful representations on the basis
of examples is what makes deep learning so powerful. The focus of
deep learning practitioners is not so much on handcrafting those
representations, but on operating on a mathematical entity so that
it discovers representations from the training data autonomously.
Often, these automatically created features are better than those that
are handcrafted! As with many disruptive technologies, this fact has
led to a change in perspective.

The paragraph clearly saying that we need to inject some form of prior knowledge into the learning system. What can be a concrete example for such prior knowledge we are used in deep learning systems?
","['deep-learning', 'feature-engineering', 'inductive-bias']","
I would distinguish at least 2 cases when it comes to a generic expression like prior knowledge:

generic extra information provide to a model, really close if not the same as feature engineering.
literal prior probability distributions used to initialize or guide a model during training.

For the first case there's plenty of examples that we can provide. The most intuitive maybe is use of masks in computer vision. Let's say we want to clean an image (i.e. haze removal) as a pre processing step for a self driving car system. Then we could train a model and feed to it not only the image captured by the camera, but also a depth mask estimated using another model. In this case the other model works as a prior distribution, since the model is not learning it, it just leverage that extra information that comes with the image. 
For the second case, there are specific class of models that learn and sometimes require prior distributions as an input, the most known to me are Bayesian Neural Networks.

Why bothering providing a prior for these models? Well, there are at least 2 reasons: sometimes we have information about a system we're trying to describe, so we can make the training more efficient, for example we're might trying to fit a coin toss model, but we know the coin is not fair and that the resulting probabilities returned by the model should not be 50/50. The second reason is that sometimes we also want to train models that do not return only raw probabilities, but also estimate the uncertainty level of those probabilities. To do that, the model learns a posterior probability over the data, and it does that by updating an initial prior. Note that the prior for this class of models can also be randomly initialize.
"
How do I choose the hyper-parameters for a model to detect different guitar chords?,"
I need to build a hand detector that recognizes the chord played by a hand on a guitar.
I read this article Static Hand Gesture Recognition using Convolutional Neural Network with Data Augmentation that looks like what I need (hand gesture recognition).
I think my task is (from my point of view) a little more difficult than that in the paper, because I think it is more difficult to distinguish between two chords than between a punch and a palm.
What I don't understand clearly is how to choose the best parameters for this more complex task: is it better to have more/less convolutional layers? A higher or lower number of poolings? Max or avg pooling?
The input will be more or less like this one:

There will be a first net (MobileNetV2 trained on EgoHands) that will find the bounding box, crops the image and then passes the saturated blending between the original one and Frei&Chen edges to the second net (unfortunately I don't have a processed picture yet, I will post an example as soon as I get it)
","['convolutional-neural-networks', 'object-detection', 'hyperparameter-optimization', 'convolution', 'pooling']",
"Among N documents, how to summarize the most unique content in each document?","
I now have $N$ documents, which share common content and they have special unique content.
Say I have $3$ legal documents related to the same person. Document $A$ is about land law, document $B$ is about company law and document $C$ is about marriage law.
How can I extract the land, company and marriage content from each document respectively and skip the common personal information?
It sounds like text-summarization but with a very different nature. Any idea is welcome.
Edit: In my situation, $N$ varies and the nature of the unique content is unknown.
","['natural-language-understanding', 'text-summarization']","
I think the best task for your purpose is name entity recognition (NER) rather than text summarization.
The logic is the following: if the three classes of documents are truly specific, there would be specific entities for each of them, but since the documents are linked by information about a single individual, all entities related to that individual and not to the specific domain would be shared.
So the most obvious shared entity in all documents, the name of the individual, could be identified and then pruned in all document, same holds for every other shared entity (can't came up with more clever examples right now).
If you work with python, SpaCy offer pretrained models that do a great job already also for NER, and in several languages as well. But you might consider to train your own model as well, maybe retrain on top of spacy models, cause for these type of tasks, the most information you can provide about which entities belongs to which class, the best the performances, and unfortunately, generic use models can account for many entities, but they can't associate them directly to specific domains of interest.
"
How should I choose a reinforcement learning algorithm? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I'm starting a new RL project. I'm familiar with Deep Q-Learning because of an old project where I used it, but I'm not sure I chose correctly back then.
Why should or shouldn't I choose DQN, or any other RL algorithm/method for a problem? By which criteria should I judge an RL algorithm? Is there any set of guidelines to help you choose a specific RL algorithm for a problem?
I did some research, but wasn't satisfied.
","['reinforcement-learning', 'deep-learning', 'reference-request']",
How do you decide that you have tested enough hyper-parameter combinations for a specific neural network architecture?,"
How do you decide that you have tested enough hyper-parameter combinations for a specific neural network architecture to discard it and move on to a new model?
Do you have a structured (generic) approach? In practice, what gives you the necessary performance (e.g. >= 80% accuracy) the fastest (w.r.t. to your work-hours) assuming there is no SOTA that easily exceeds your requirements and extensive hyper-parameter optimization is infeasible?
","['neural-networks', 'machine-learning', 'deep-learning', 'optimization', 'hyperparameter-optimization']",
"What is meant by ""spatial encoding"" in the context of convolutional neural networks?","
Consider the following excerpt from the abstract of the research paper titled Squeeze-and-Excitation networks by Jie Hu et al.

Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding.

The authors used the term ""spatial encoding"" and the excerpt implies that enhancing spatial encoding has the benefit of increasing the representational power of a convolutional neural network.
What is meant by the term ""spatial encoding"" in this context related to the convolutional neural networks?
","['convolutional-neural-networks', 'terminology', 'papers']","
The Convolution Layer processes a certain part of the picture tensors and compresses it to a lower dimension. The spatial encoding adds the information of where the pixels were located in the image. Loosely speaking: It tells you the pixels I just processed were in the top left corner of the image. That way, the classification in the fully-connected layer can additionaly use this information. For more information on CNNs click here.
Maybe it becomes clearer in text processing. Transformer models for NLP have so-called positional encoding which is the counterpart to spatial encoding in image processing. When the sentence is processed it, the positional encoding tells the index of the word in the whole sequence.
"
"How to encourage the reinforcement-learning agent to reach the goal as quickly as possible, and what's the effect of discount factor?","
I am trying to use reinforcement learning to solve a task and compare its performance to humans.
The task is to find a single target in a fixed number of locations. At each step, the agent will pick one location, and check whether it contains the target. If the target is at this location, the agent will get a $+10$ reward and the trial ends; otherwise, the agent will get a hint at where the target is (with some stochastic noise), get a $-0.5$ reward, and it needs to pick another location in the next step. The trial will terminate if the agent cannot find the target within 40 steps (enough for humans). The goal is to solve the task as quickly and accurately as possible.
I am now trying to solve this problem by Deep Q-Network with prioritized experience replay. With the discount factor $\gamma=0.5$, the agent can learn quickly and solve the task with an accuracy close to 1.
My current questions are:

The accuracy level is already very high, but how to motivate the agent to find the target as quickly as possible?

What's the effect of $\gamma$ on the agent's task solving speed?


I am considering $\gamma$ because it relates to the time horizon of the agent's policy, but I now have two opposing ideas:

With $\gamma \rightarrow 0$, the agent is trying to maximize the immediate reward. Since the agent will only receive a positive reward when it finds the target, $\gamma \rightarrow 0$ motivates the agent to find the target in the immediate future, which means to solve the task quickly.

With $\gamma \rightarrow 1$, the agent is trying to maximize the discounted sum of reward in the long term. This means to reduce the negative rewards as much as possible, which also means to solve the task quickly.


Which one is correct?
I have tried training the network with $\gamma=0.1, 0.5, 0.9, 0.99$, but the network can only learn with $\gamma=0.1, 0.5$.
","['reinforcement-learning', 'deep-rl', 'q-learning', 'reward-functions', 'discount-factor']",
How does backpropagation know which weights to change?,"
I'm currently working on constructing a neural network from scratch (in JavaScript). I'm in the middle of working on the backpropagation, but there's something I don't understand: how does the backprop algorithm know which weights to change or which paths to take? The way I did it, it always took all of the paths/weights and changed them all. So how does the algorithm know which paths to take, which weights to change, and whether to add or subtract X amount from said weight?
","['neural-networks', 'backpropagation']",
Monocular depth estimation,"
I am currently reading the paper towards robust monocular depth estimation and I have 2 doubts about it.
First of all the paper stated that there are 2 types of depth annotated, dense and sparse. What are they and what are their differences?
Secondly, the paper predicts the relative depth given an input, how do we calculate the loss when a relative depth map is predicted? I know we could simply use MSE if the model predicts an absolute depth map. If I were to train a model myself to predict relative depth maps as well, how should I calculate the loss or other evaluation metrics?
Any help would be greatly appreciated, thanks in advance!
","['deep-learning', 'computer-vision']",
2-stage model overfitting,"
I'm trying to build an entity matching model. There are 2 kinds of features - binary (0/1) and text features. Initially I made a deep learning model that uses character level embeddings of some of the text features, word level embeddings of the other text features and also uses the binary features as inputs.
The output is through a softmax for 3 classes, so basically a $n\times 3$ array (where $n$ is the input data length).
I've done three splits - train, val and test, and for training the DL model through Keras, I've specified train as the training split and val as the validation split. I measured the performance on the test split to get DL model metrics. The softmax outputs for all three splits were obtained using model_DL.predict.
Next, I used a Random Forest model as a second stage. Inputs: all the binary features PLUS the softmax outputs as inputs. e.g. I took the train split, removed the text features and added in the columns of the predicted array as separate features. To be even more specific, if predtrain was obtained by using model_DL.predict on the train split, then the additional features were added using train['class1prob'] = predtrain[:,0], train['class2prob'] = predtrain[:,1], train['class3prob'] = predtrain[:,2].
Similarly I did for test and val splits. Now I trained the RF on the augmented train split and measured its performance on the val and test splits. The F-score for the train and val splits was around 0.85, 0.74, 0.73 for the 3 classes respectively (i.e. performance was similar on both splits).
BUT for the train split the predictions were near perfect - 0.98, 0.99, 0.98 F-scores for the three classes. My intuition is that overfitting of the 2nd stage RF is understandable for train, since the softmax outputs were predicted using the 1st stage DL model, which in turn was already trained on train. Also, there's some data leakage for the val split since val was used as a validation set to finetune the DL model by Keras, so maybe even the val metrics aren't so reliable. But there is no leakage for the test set.
My question is, in this scenario have I made an error, or is this blatant overfitting normal for 2 stage models? If there's a glaring error, any way or best practice to fix that?
",['overfitting'],
Does the ANN's training data include the proper output for every neuron?,"
I was designing an Artificial Neural Network a while back, but hit a bump when I got to the backpropagation. I was having trouble making the script choose whether to add or subtract from the weights, when I had a thought.
Does the ANN's training data include the proper output of all the neurons, or just the input and output layers? If it's the latter, could somebody please explain how backpropagation works in a simple way?
","['neural-networks', 'training', 'backpropagation', 'training-datasets']",
How do I compute the value function when the reward is only at the end in the context of actor-critic algorithms?,"
Consider the actor-critic reinforcement learning setting (actor and critic parameterized by a neural network). The reward is given only at the end of the episode (or when there is a timeout there is no reward).
How could we learn the value function? Do you recommend computing intermediate rewards?
","['reinforcement-learning', 'actor-critic-methods', 'reward-functions', 'sparse-rewards']","

The reward is given only at the end of the episode (or when there is timeout there is no reward)

This is a common case. E.g. winning a board game, or reaching a goal state.

How could we learn the value function?

All RL algorithms are designed to cope with this scenario. Actor-Critic is not an exception. Value-based algorithms (including the critic in Actor-Critic) learn through time step backup updates. The simplest backup is to copy data about experience at time $t+1$ into an update for the state or state/action experienced at time $t$. That is what single-step temporal difference algorithms do. Other value-based algorithms can be more sophisticated and more efficient with assigning the update signal back in time.
A very sparse reward can be difficult for an agent to find or learn from. So some methods may work better than others. Without knowing the environment, and what the specific difficulties might be, it is not possible to recommend an approach, or to even suggest whether your algorithm needs any help.

Do you recommend computing intermediate rewards?

In general, no. However, these can help when the ""natural"" rewards in a problem are both sparse and hard for the agent to discover through exploration. Constructing extra reward signals to guide a learning agent is called reward shaping.
Reward shaping needs to be done with care because it can inadvertently change what the optimal solution is. But if done well, it can make a problem much easier to solve for an agent.
A starting rule of thumb for whether to add some kind of reward shaping is based on how often a random agent might accidentally obtain a difference in end rewards. Once the agent has experienced a difference between rewards, it can start to refine its predictions and begin to prefer the higher reward, which then usually leads to exploration nearer more valuable states. It may do this even if the higher reward only occurs one time in a thousand initially, say. However, if initial random actions gain no useful signal even for millions of trial-and-error episodes, then you will need to do something to assist the agent.
"
What is the role of the word sampling in upsampling and downsampling?,"
Upsampling and downsampling are highly used in deep learning algorithms that involve convolutional neural networks.
Upsampling increases the size downsampling decreases the size of tensors.
What is the role of the word sampling in the words upsampling and downsampling?
Does it always have a connection with the sampling techniques that are generally used in statistics? Or is it true that sometimes it does not have any such connection and the only task of upsampling and downsampling is to increase and decrease respectively the size in any way?
","['upsampling', 'downsampling', 'etymology']","
I think that this terminology originates from digital signal processing:

https://en.wikipedia.org/wiki/Upsampling
https://en.wikipedia.org/wiki/Downsampling_(signal_processing)
https://en.wikipedia.org/wiki/Sample-rate_conversion

Given a signal at a given frequency, one would like to get an approximation of the signal, which would be obtained by sampling with a frequency $n$ times higher ($n$ times smaller) than for the original signal. The resulting signal should be close to the original in a certain sense - in the Fourier domain, for instance.
"
How to initialize the coefficient vector of Deep Tensor Neural Network,"
In Quantum-Chemical Insights from Deep Tensor Neural Networks, I would like to ask a question about how to initialize the coefficient vector of the network, because I could not understand it even after reading the paper. In the paper, it says

All presented models use atomic descriptors with 30 coefficients. We initialize each coefficient randomly

If it is initialized randomly like this, why do we initialize it randomly since we cannot use the information of nuclear charge as mentioned in the paper?
",['graph-neural-networks'],
How can I predict the next number in a non-obvious sequence?,"
I've got an array of integers ranging from -3 to +3.
Example: [1, 3, -2, 0, 0, 1]
The array has no obvious pattern since it represents bipolar disorder mood swings.
What is the most suitable approach to predict the next number in the series? The length of the array is about 700 entries.
From where can I start the investigation? (provided that I've got some experience in Python and Node.js, but only a hello-worldish acquaintance with TensorFlow). Which training model might be suitable in this case? How can I chunk the data set properly?
","['machine-learning', 'recurrent-neural-networks', 'prediction', 'sequence-modeling']","
This is a question of time series forecasting, since your numbers form a sequence.
You may want to take a look at the ""forecasting"" tag at CrossValidated.
If you have only 700 data points, ML/AI methods will likely not be very useful. Whatever you do, I would recommend you benchmark your chosen method against very simple approaches, like the overall mean, or the last observation (a ""random walk forecast""), or a simple Exponential Smoothing method. These very simple benchmarks can often be surprisingly hard to beat, and they are trivially easy to set up.
You next step should be to include domain knowledge, as Sanyou recommends. This can be as simple as observing that bipolar mood swings follow a day/night cycle and modeling this seasonality, e.g., in a seasonal Exponential Smoothing method. (I'm not saying this disorder does exhibit this kind of seasonality, only that if it does, this can easily be modeled.) Or model any other kinds of drivers you know.
In my experience, understanding your data and your context always beats building more fancy models, or collecting more data.
As free time series forecasting textbooks, I very much recommend Forecasting: Principles and Practice (2nd ed.) by Athanasopoulos & Hyndman
and
Forecasting: Principles and Practice (3rd ed.) by Athanasopoulos & Hyndman.
"
Does the Weisfeiler-Lehman Isomorphism Test end?,"
I am studying GNNs. I am interested in the Weisfeiler-Lehman Isomorphism Test (WL-Test).
I was looking for information about whether the test always ends or not, but I didn't find a definitive answer.
I know that we can choose how many iterations can be done, or the test is finished if the iteration makes the same result.
My question is: What if we don't choose how many iterations should be done and the iterations don't make the same result between two graphs? Will the iterations keep going on (i.e. infinitely)?
","['graph-neural-networks', 'graph-theory']",
Which method can accurately detect circular/angular shapes? (attached example),"
Is there a method to detect shapes like these accurately and efficiently? I have tried the OpenCv Haar Casacade Classifier which does not work well. These shapes should all be the same class object and can be of different sizes and a little differently shaped (more circular or more angular). In the attached picture there are 4 separate shapes, of which 2 overlap each other.

","['neural-networks', 'machine-learning', 'object-detection', 'opencv']",
variational autoencoder - decoder output for images,"
Following the standard setup/notation for a VAE, let $z$ denote the latent variables, $q$ as the encoder, $p$ as the decoder, and $x$ as the label. Let the objective be to maximize the ELBO, where a single sample monte carlo estimate of the ELBO is given by
\begin{align*} 
\log p(x \, | \, z) + \log p(z) - \log q(z \, | \, x)
\end{align*}
Now I want to focus only on the $\log p(x \, | \, z)$ term, where $x$ is an image, and the decoder $p$ outputs the mean/variance of a normal distribution for each pixel.
My understanding is that pixel values should be integer 0-255. Now consider a single pixel: suppose that the ground truth for that pixel is the value 10, and the encoder predicts the mean, variance $\mu, \sigma^2$ respectively. Now when computing the ELBO, we have this term
\begin{align*} 
\log p(x \, | \, z) = \log f(10, \mu, \sigma^2) \tag{1}
\end{align*}
where $f$ is the probability density of the normal distribution. My question is why it is justified to compute $\log p(x \, | \, z)$ using the density considering that image data should be discrete valued. It seems to me that any sampled output between 9.5-10.5 would all get mapped to the correct value of 10. Then it seems that you should take the term in the ELBO as
\begin{align*} 
\log p(x \, | \, z) = \log \big(F(10.5, \mu, \sigma^2) - F(9.5, \mu, \sigma^2)\big) \tag{2}
\end{align*}
where $F$ is the CDF of the normal distribution.
It seems that all references calculate the ELBO as (1) and none as (2). Why is this justified?
","['image-processing', 'variational-autoencoder', 'image-generation']",
Compare the efficiency of a trained ML model with a non-learning-based method for solving the same problem,"
If a certain task T is solved by a non-learning-based method A (let's say, an optimization-based approach). We now train a machine learning model B (let's say a neural network) on the same task.
What are some metrics that we can use to compare their efficiency in terms of finding the solution (Assuming the quality of both solutions is comparable)?
","['neural-networks', 'optimization', 'metric', 'algorithm-request', 'efficiency']","
The most generic answer to this question is:
the same metrics you use to evaluate the quality of your model during training or in test phase. (Plus the timing of inference if you're referring to computational efficiency)
And I'm not referring to any specific metric yet cause that's really task dependent. But in general if you have a model that perform a task and another algorithm that perform the same task, then you should be able to apply both to the same set of data, compute whatever metric is suitable to evaluate the performance on the task, and compare the two scores. Let me stress out that the test instances should be the same for a scientifically relevant comparison, and I mean literally the same.
As an example of some metrics I would refer to the web since out there there's plenty of blog posts listing and comparing metrics. Just to link a few:

classification
regression
image quality assessment
object detection
machine translation

The list is not exhaustive but I think it illustrates the point.
Also, as a side note: almost all machine learning algorithms are optimization-based, if you want to refer to approaches that don't fall into machine learning I think a better term is analytic methods/approaches.
"
Why do the object detection networks produce multiple anchor boxes per location?,"
In various neural network detection pipelines, the detection works as follows:

One processes the input image through the pretrained backbone
Some additional convolutional layers
The detection head, where each pixel on the given feauture map predicts the following:

Offset from the center of the cell ($\sigma(t_x), \sigma(t_y)$ on the image)
Height and width of the bounding boxes $b_w, b_h$
Objectness scores (probability of object presence)
Class probabilities




Usually, detection heads produce not a single box, but multiple.

The first version of YOLO - outputs 2 boxes per location on the feature map of size $7 \times 7$
Faster R-CNN outputs 9 boxes per location
YOLO v3  - outputs 9 boxes per pixel from the predefined anchors : (10×13),(16×30),(33×23),(30×61),(62×45),(59× 119), (116 × 90), (156 × 198), (373 × 326)

These anchors give the priors for the bounding boxes, but with the help
of $\sigma(t_x), \sigma(t_y), b_w, b_h$ one can get any possible bounding box on the image for some pixel on the feature map.
Therefore, the network will produce plenty of redundant boxes, and a certain procedure - NMS suppresion has to be run over the bounding box predictions to select only the best.
Or the purpose of these anchors is to start from a prior, reshape and shift slightly the bounding box, and then compare with the ground truth.
Is it the case, that if one used only a single bounding box for detection - it would be hard to train the network to rescale the initial bounding to, say, 10 times, and produce some specific aspect ratio?
","['computer-vision', 'object-detection', 'bounding-box']",
Do you need a terminal state when using double deep q networks?,"
I just got my agent training, and I'm wondering if the terminal flags are necessary when sampling from the replay buffer. The game I'm implementing the agent in has two different ways the game can end, and so far my agent seems to be learning without terminal flags. I was wondering how important this feature is, as it's in all the pseudocode but doesn't seem to be necessary in my implementation.
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'keras', 'monte-carlo-tree-search']","
It's an important feature, and you drop it at the risk of the agent failing to learn successfully.
The difference between the TD target without the terminal flag
$$G_t = R_{t+1} + \gamma \text{max}_{a'} Q(S_{t+1}, a')$$
and with the terminal flag applied to $S_{t+1} = S_T$
$$G_t = R_{t+1}$$
is important whenever $Q(S_{T}, a')$ might be evaluated as non-zero. The true action value of the terminal state is always defined as zero.
In some cicumstances, values based on a function approximator estimate could be far from zero - if at any point the estimator moves significantly away from zero then this could lead to value estimates diverging. This can be a problem at any time, because the estimator is usually not trained on actions taken from the terminal state - due to them never being observed. In principle you could add training data to keep the function approximator in line with this estimate, but as it is an absolute value it is far more common (when using approximators such as neural networks) to change the TD target as above.
Double Q learning improves estimates of bootstrap values by removing some of the maximisation bias from the next action value. This may help in your case, if the expected future rewards from the terminal state remain close to zero.
I would expect Double Q learning to work OK without using terminal state informaton, and without adding ""mitigating"" training to keep terminal state estimates close to zero, if rewards are not sparse, and do not increase close to the terminal states. That would mean the estimator should recognise expected future reward becomes closer to zero as the agent approaches a terminal state, thus would not take much extrapolation to predict close to zero for the terminal state - close enough that it does not create runaway feedback or influence the optimal policy much.
With sparse rewards, or with significantly high rewards close to the terminal state (e.g. a goal state which the agent must reach to gain all the reward), then using the terminal flag in the normal way becomes more important. It is unlikely that double Q learning would help much in that case. However, it may still be possible to find the optimal policy but with highly inaccurate action values (e.g. action values could all be double what they should be).
"
What can cause massive instability in validation loss?,"
I'm working with very weird data that is apparently very hard to fit.
And I've noticed a very strange phenomenon where it can go from roughly 0.0176 validation MSE to 1534863.6250 validation MSE in only 1 epoch! It usually then will return to a very low number after a few epochs. Also, no such fluctuation is seen in the training data.
This behavior of instability is consistent across shuffling, repartitioning & retraining. Even though I have 16,000 samples and a highly regularized network (dropout + residual layers + batch normalization + gradient clipping).
I mean I realize I could have more data, but, still, this behavior is really surprising. What could be causing it?
P.S. Model is feedforward with 10 layers of size [32,64,128,256,512,256,128,64,32,1], using Adam optimizer. Also, this question may be related (my experience is also periodic validation loss), but I don't think they experienced the same massive instability I am seeing.
","['deep-learning', 'overfitting', 'validation-loss']",
"FCNs: Questions about the filter rarefaction in the CVPR paper [Long et al., 2015]","
I am reading the paper about the fully convolutional network (FCN).
I had some questions about the part where the authors discuss the filter rarefaction technique (I guess this is roughly equivalent to dilated convolution) as a trick to compensate for the cost of implementing a shift-and-stich method.

Consider a layer (convolution or pooling) with input stride $s$, and a subsequent convolution layer with filter weights $f_{i,j}$ (eliding the irrelevant feature dimensions). Setting the lower layer’s input stride to 1 upsamples its output by a
factor of s.


How does setting the input stride of the lower layer to 1 leads to upsampling (and not in the reduction of output dimension)? I am confused about what the terminologies lower/higher layer and input/output stride refer to here.


To reproduce the trick, rarefy the filter by enlarging it as
$f'_{i,j} = \begin{align} \begin{cases} f_{i/s,j/s} & \text{if $s$ divides both $i$ and $j$} \\ 0 & \text{otherwise} \end{cases} \end{align}$
(with $i$ and $j$ zero-based). Reproducing the full net output of the trick involves repeating this filter enlargement layer-by-layer until all subsampling is removed.


I was wondering how the rarefaction defined here leads to the filter enlargement. Based on the equation, it seems that $f$ and $f'$ has the same size, with $f'$ having different filter weights based on $s$.

","['neural-networks', 'filters', 'fully-convolutional-networks']",
What do people refer to when they use the word 'dimensionality' in the context of convolutional layer?,"
In practical applications, we generally talk about three types of convolution layers: 1-dimensional convolution, 2-dimensional convolution, and 3-dimensional convolution. Most popular packages like PyTorch, Keras, etc., provide Conv1d, Conv2d, and Conv3d.
What is the deciding factor for the dimensionality of the convolution layer mentioned in the packages?
","['convolutional-neural-networks', 'convolutional-layers', '1d-convolution', '2d-convolution', '3d-convolution']","
Kernel dimensionality and presence of filters decides the dimension of convolution operator. N Dimensional Convolutions have N dimensional kernels. For example, from Keras Documentation on 2 Dimensional Convolutions:

kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.

If you have more than one filter in the layer, that also adds another dimension to the layer. So, we can say that a 2D convolutional layer is in general 3 dimensional, where 3rd dimension is the number of filters: (k,k,F). For the special case of a single filter, F=1 and we can treat it as 2 dimensional.
"
When to use Multi-class CNN vs. one-class CNN,"
I'm building an object detection model with convolutional neural networks (CNN) and I started to wonder when should one use either multi-class CNN or a single-class CNN. That is, if I'm making e.g. a human detector from image data and a cat detector also from image data, then when should I have a specific model for each task, and when should I just combine all the data into one and use just one general multi-class CNN?
I've understood from the No-Free-Lunch-Theorem and generally from estimation theory, that there there does not, in theory, exist a model which is simultaneously optimal for every problem. In other words, case specific models should, in general, beat the ""all-purpose""-models in the same task.
I have a difference in opinion with a colleague of mine whether to use one-class of a multi-class CNN and I would like to hear the communities opinion on this.
","['convolutional-neural-networks', 'multiclass-classification']","
I am not really a fan of the One vs All approach.
From my experience it is never convenient to transform a multi-class classification problem with, say, $N$ possible classes to a bunch of binary classification problems.
Reason #1
The number of binary classifiers you need to train scales linearly with the number of classes. Hence, you can easily find yourselves training lots of binary classifiers. What if each one of them has a huge number of neurons? As you can understand, the computational burden here is quite a problem.
Reason #2
With a small $N$, the computation is less of a problem, but still.. why would you do that? By doing things like this, you can easily end up in awkward situations such as two or more of your binary classifiers give a positive outcome, or none activates. How do you handle these issues?

However, there exists a very specific setup where you might want to use a set of binary classifiers, and this is when you're facing a Continual Learning(CL) problem. In a Continual Learning setting you don't have access to all the classes at training time, therefore, sometimes you might want to act at a architectural level to control catastrophic forgetting, by adding new classifiers to train. However, even in CL there exist other methods that work better.
To conclude, I wouldn't recommend anyone go for this option. You can train a multi-class classifier much more easily and avoid all the aforementioned issues.
"
Is the noise term $\epsilon$ in $y=g(x) + \epsilon$ used to denote the model's imperfection to the real world?,"
In supervised machine learning, it is common to say that we learn a function of the form
$$y=g(x) + \epsilon.$$
Generally, $\epsilon$ is used to denote noise or, more precisely, any influence by latent variables such as measurement inaccuracies (right?).
Is it, therefore, correct to say that we use $\epsilon$ to denote the model's imperfection to the real world (caused by anything unknown)?
","['supervised-learning', 'noise']","
Yes, precisely.
What do you mention is known in the literature as the Bayes Error. See page top of the page 114 https://www.deeplearningbook.org/contents/ml.html.

The ideal model is an oracle that simply knows the true probability distribution that generates the data. Even such a model will still incur some error on many problems, because there may still be some noise in the distribution. In the case of supervised learning, the mapping from $\mathbf{x}$ to $y$ may be inherently stochastic, or $y$ may be a deterministic function that involves other variables besides those included in $\mathbf{x}$. The error incurred by an oracle making predictions from the true distribution $p(\mathbf{x}, y)$ is called the Bayes error.

Regardless of how clever is your model, the best error you can achieve for the prediction on the data distribution is $\varepsilon$. Note that it holds for the whole data distribution, not a sample of data.
Say, you would like to fit something like $\sin(x) + \varepsilon$. There are 10 points, and one can fit them perfectly with the 9th-degree polynomial, but this an error on training data, and, in case one samples more data points, the error will be likely do exceed the optimal $\varepsilon$.
"
Appropriate ML algorithm to solve a cutting pattern problem,"
I have a rectangular area, where I need to place some 2 dimensional geometrical shapes - like a square or circle or a little more complicated shapes. And after the arrangement these shapes should be cut out.
Requirements to the disposal of shapes:

These shapes are not allowed to intersect
And also they must disposed on the recatangular area
They must have at least a minimum distance
The waste should be minimized
When more than one shape is arranged on this area it is desirably that the shapes have a certain quantity (e.g. shape A: 50 %, shape B: 30 %, shape C: 20 %)

After the arrangement I get the coordinates of the single shapes so that I can cut out my shapes...
To solve this I thought of (deep) reinforcement learning but because I'm new to ML I'm not sure if there is a more appropriate method to solve this problem.
I hope that you can give me some hints or simply confirm my assumption that (deep) reinforcement learning is appropriate. And perhaps you can also offer me some useful links...
Many thanks in advance for your help!
And lastly a little picture which is showing a possible bad result because shape A and shape E intersect. And probably there is to much waste.

","['machine-learning', 'reinforcement-learning', 'deep-rl', 'ai-design', 'ai-basics']",
What is the fastest multi-human pose estimation model?,"
I am trying to find an accurate and fast multi-person human pose estimation that I can train on with custom data. I have been searching for a little while and I may not be up-to-date on the newest techniques.  I will start by posting what I have found and looked into (a little):

Openpose: This is supposedly real-time (I assume on a GPU, 24fps?) and they provide training code
Lightweight OpenPose: Runs in realtime >20fps confirmed, training code is provided
mediapipe: runs in realtime > 20fps confirmed, training code is NOT provided
posenet: No training code, can one even train tfjs models?
movenet: Very fast but no way to train?
hrnet & lightweight-hrnet: seem to be slow? Can anyone confirm? training code provided
blazepose: haven't tried it yet, looks like tf implementation, but no discussion bout speed. Training code included
alphapose: haven't tried it, but looks to run at 16fps (maybe faster?) but is only intended for research not commercial. Training script available.
MoVnect: Looks new, and fast (haven't tested it) but looks like it uses student-teacher training.

What are some other human poses estimation models out there?
I care more about speed and training. I am thinking #2 is my best bet but it's a few years old. And not the most friendly to train Anything newer? Can anyone confirm or reject my findings?
","['computer-vision', 'model-request', 'pose-estimation']",
Usefulness of the state_values calculation in Dynamic Programming,"
State values are always presented as a central concept in RL, notoriously in the bible, the Sutton&Barton’s book.
I have done some exercises trying to improve my understanding, but it is clear that I am missing some important point(s), at least in the tabular case.
I do not understand, in the case where we have a complete MDP model of the environment, why we should bother doing value iteration or/and policy iteration if we can directly find the optimal policy by calculating each state-action value.  I mean that having the state-action values, we can by the argmax function obtain the optimal policy for any starting point.
In my learning/example, link, with an observation space of one million states and three different actions, using a desktop notebook it took 21 iterations in 19 minutes to find every state action value, and consequently,  the optimal policy.
Perhaps it could be done quicker, selecting only the interesting states. For instance, if there is a unique starting state and the simulation has a finite-small number of steps per episode, probably most of the states are uninteresting because never visited. In that case, we can select the target states using some test simulations, but, is it worth?
","['reinforcement-learning', 'dynamic-programming']",
"How to understand the results of a generator that switches, for metric evaluation?","
I am running a code on generative adversarial networks. The code is designed in such a way that it outputs a fake image after every 5 epochs. The total number of epochs is 800 in number.
After the completion of the program, when I check the images generated by the generator of the generative adversarial network while training, I am so confused about the results.
The phenomenon is as follows:

Image after epoch n is very realistic, while the images n -5 and n + 5 are
not highly realistic. I can see many such n's. And vice-versa sometimes.

Although I am interested to know why it happens. My question here is not directly about why it happens. My doubt is about the decision regarding the generator on which I need to evaluate some metrics.
It is a general practice to evaluate the metric on the generator after the last epoch, that is, the 800th generator. But if the phenomena I told holds, then it may be true that the last generator is not capable of generating realistic images, and the generator 795 or the generator 805 may be good.
So, if I am correct, I need to check the fake sample generated by the generator and then apply my metric to the generator which generated high quality and realistic image. Am I correct?
","['generative-adversarial-networks', 'generator']",
What is the difference between gradient decent in neural networks and temporal difference in reinforcement learning?,"
I am studying Q-learning in reinforcement learning. My question is about the Bellman equation.
In Q-learning, the Bellman equation is often introduced as follows.
\begin{align}
Q_{new}(s,a) 
&= Q_{old}(s,a) + \text{learning rate} \times \text{error}\\
&= Q_{old}(s,a) + \alpha(\text{target} - \text{actual})\\
&= Q_{old}(s,a) + \alpha((\text{reward} + \text{discount factor} \times \text{max next } Q) - Q_{old}(s,a)) \\
&=Q_{old}(s,a) + \alpha[r(s,a)+\gamma\times max(Q(s',a'))-Q_{old}(s,a)]
\end{align}
The update equation of gradient descent (which is used in the context of neural networks and other fields) is as follows.
$$
w_{new} = w_{old} + \eta\frac{dE}{dw} 
$$
So, why the Bellman equation depends on the error after the learning rate while the gradient descent depends on the error gradient?
I feel confused.
","['neural-networks', 'reinforcement-learning', 'q-learning', 'gradient-descent', 'bellman-equations']","
They have a few similarities, but they are quite different. Let me first give you a general description of both approaches/algorithms, so that you start to get a sense of their differences and similarities.
Description
Gradient descent (GD) can be applied to solving any optimization problem where your loss (aka cost or objective) function is differentiable with respect to the parameters that you want to update. For example, if you're training a neural network with gradient descent to solve a classification problem, you could be using a cross-entropy function, which should be differentiable with respect to the weights of the neural network (so you should make sure that all the operations in the neural network, in particular, the activation functions, are differentiable or, at least, you define their derivatives). So, the only restriction to use GD is that your loss function is differentiable, so you could use GD to solve classification, regression, or even reinforcement learning problems (and this has actually be done).
Temporal-difference (TD) learning is a specific approach to reinforcement learning, where you update your current estimate of a value function (in your case, the action, aka state-action, value function) with a value that is the difference between estimates at different time steps (hence the name temporal difference).
How are they different/similar?

They can both be seen as learning algorithms/approaches, although people in other areas other than machine learning may view gradient descent ""just"" as an optimization algorithm.

TD learning is applied in the specific context of reinforcement learning, while GD is applied to any optimization problem where your cost function is differentiable.

In TD learning (or, more generally, in RL), you want to find a value function (or policy), which could be seen as the parameters that we want to find. (So, here, the parameters are the variables that we want to find). On the other hand, in GD, we want to find the parameters of a model (that define some function or distribution, which could be a value function, but not necessarily).

TD learning can be combined with neural networks (for example, see this paper), which leads to a new field often known as deep RL. In this case, you may use GD to update the parameters of this neural network, which represents the value function or policy. So, GD can be used to solve RL problems

In both cases, we have a learning rate, which determines the magnitude of the changes to the current estimate of the parameters.

You can estimate/approximate gradients (or derivatives) with finite-differences, or finite-differences could be seen as the discrete version of derivatives. In fact, derivatives can be defined as limits of differences. Moreover, if you read e.g. this paper, you will see a lot of gradient symbols. Given that TD uses these ""differences"", this could be the reason why you're confused.


"
Book/course recommendation on game theory application to multi-agent system (reinforcement learning),"
Is there any great game theory book or course that discusses the application of game theory to modern reinforcement learning or multi-agent systems? Or a classic reference book that can help me get a full understanding of papers like $\alpha$-rank.
","['reinforcement-learning', 'reference-request', 'game-theory', 'multi-agent-systems', 'books']",
What practically makes a good architecture of ANN?,"
When we take a look at the literature there are so many opinions.
I was wondering what are some generally good practices to design an architecture, like how much depth would you prefer and how much width would you prefer.
Should the number of training data influence your decisions of designing the architecture.
What should the number of parameters be ? etc.
","['neural-networks', 'convolutional-neural-networks', 'training']","
I am not sure if there really are contradicting opinions on this matter. CNNs, RNNs, LSTMs all have specific types of data they are good at predicting. Depth and width, or in general the size of the neural network mostly depends on the size of your dataset. You don't want to build a too large network that will overfit the available data, which can usually be understood to be the case after running a few epochs but in general, number of trainable parameters should be smaller than to total number of independent data points you have to avoid memorizing the data definitely.
Width and depth of the network depends on the size of the data available (as mentioned before), but can simply be thought just as another hyper-parameter that is to be decided by training itself. It can be defined as a part of cross validation process of building a neural network.
"
"Does a bigger neural network learn ""worse"" representations than a small neural network when the amount of data isn't enough?","
Assume we have a neural network and we want to train it on a classification problem. The hidden layers of the neural network are kind of feature representations of the input data.
If the neural network is big and the amount of data isn't enough for the complexity of the model, does it usually learn worse representations than a smaller neural network which is good for the amount of data we have?
","['neural-networks', 'deep-learning', 'overfitting', 'generalization', 'representation-learning']",
Why isn't my perceptron having the expected costs?,"
I want to implement a single perceptron for linear regression using the following formulas:

The input data for the first case is one column (x(392, 1); y(392, 1)) and for the second case is (x(392, 7); y(392, 1)). The NaN values have been removed and x values have been standardized x-x.mean()/x.std()
This is my Python implementation:
class LinearRegression(object):


    def __init__(self, x, y, n_iter):
        self.x = x
        self.y = y
        self.n_iter = n_iter
        self.cost_iteration = []
        # Initializing model parameters (w, b) to zeros
        self.weights = np.zeros((1, self.x.shape[1]))   # w: weights
        self.biases  = np.zeros((1, 1))                 # b: bias 

    def feedforward(self):
        # return the feedforward value for x
        #self.weights, self.biases = self.update_params()
        z = self.x @ self.weights.T  + self.biases
        return z
    
    def loss(self):
      # return the loss value for given x and y
      z = self.feedforward() 
      loss = self.y-z
      cost = np.sum(loss**2)/self.y.shape[0]
      return loss, cost 

    def backpropagation(self):
      # return the derivatives with respect to weight matrix and biases
      loss, cost  = self.loss()
      db = -2*np.sum(loss)/self.y.shape[0]                    # dJ/db
      dw = -2*np.dot(self.x.T, loss)/self.y.shape[0]          # dJ/dw
      return dw, db   
    
    def update_params(self):
      # update weights and biases based on the output
      dw, db = self.backpropagation()
      self.weights -= dw.T
      self.biases  -= db
      return self.weights, self.biases
    
    def fit(self):
      # fit method for the training data
      for iter in range(self.n_iter):
        self.update_params()
        print(self.biases)
        l, c = self.loss()
        self.cost_iteration.append (c)
      return self.cost_iteration


The final cost should be approximately 23.9 and 11.6 for the two models, respectively. But I can't figure out why it's not the case when I use my code.
","['python', 'implementation', 'linear-regression', 'perceptron']",
Is it possible to have different channel dimensions in a CNN?,"
Let's say I have two channels that I wish to feed into a CNN. One of the channel contains 4 traces and has a width of 512. Stacking them on top of each other therefore yields an image with dimensions (4, 512). The other channel is just 1 trace, so its dimensions would be (1, 512).
I then have convolutional filters that are of dimension (1, 5) as an example. That means that the filters run over each trace separately. The first channel (containing the 4 traces) will then have a set of filter weights, shared among the 4 traces. The second channel (containing the 1 trace) will have a completely different set of weights (as per this SE question).
TLDR: Can convolutional layers in a CNN have different dimensions? Putting this in the context of images: Could we have a CNN that takes an image that has dimensions (100, 100) for the red channel, (100, 100) for the green channel, and (50, 100) for the blue channel?
","['convolutional-neural-networks', 'filters', 'channel']","
You can do whatever the heck you want.
Of course you will have to design the data flow through the network so that it can make whatever inferences you intend it to make.

The first channel (containing the 4 traces) will then have a set of filter weights, shared among the 4 traces. The second channel (containing the 1 trace) will have a completely different set of weights (as per this SE question).

Sure, you can do that. No reason why you couldn't. Will it work well? Who knows. Have to try it and see.
The best way to combine them will depend on what the NN is supposed to actually do. With the architecture you've described, at least this layer is unable to relate traces to each other. This would be bad when processing, for example, colour images - you don't want to treat red, green and blue the same way as each other, and you want to detect certain combinations of red, green and blue. If you also want the network to have this ability, then maybe you should treat each trace as a channel so the network can see all of them at once.
At some point you will obviously have to combine the results together.

Could we have a CNN that takes an image that has dimensions (100, 100) for the red channel, (100, 100) for the green channel, and (50, 100) for the blue channel?

As I said, it depends on what the network is supposed to do. Are these channels totally separate? Then you can process them with different sized CNNs - or even the same CNN until the dense layers - and combine the results in the dense layers at the end. But if these are the RGB components of one image, you'd be better off just stretching the blue channel so the CNN can recognize colours like yellow.
"
Should I train my network for classification on samples whose ground truth label is ambiguous?,"
Imagine that I am training a model to classify handwritten digits. Suppose there are some bad quality images that could be classified by a human as either 0 or 8, 1 or 7 or other commonly misclassified pair of digits.
My question is, should I simply remove such ambiguous samples? Should I annotate it as the most similar digit, even though there are other similar answers? Should I present it repeat the sample, presenting it once per each 'acceptable' answer?
","['neural-networks', 'classification', 'data-labelling']","
This depends on the behaviour you want. If the ambiguous sample's ground truth is classified by a range of people, your network will get an average* based on that group. If it's only by one person, your network will be biased to how that one person classifies these samples.
Alternatively, depending on your loss function, you could train the network to classify ambiguous samples with an ambiguous label. For example, if 40 people label the digit as 1, and 60 as 7, you could have the desired output for the network be 0.4 for 1 and 0.6 for 7 (assuming it's a probability).
This doesn't have an exact answer, it's whatever behaviour you deem best for your scenario. If you want to keep it simple, you can remove the samples and then see how the network performs on these ambiguous samples at testing time. Assuming your dataset is good, you'll probably find the network performs about the same on these samples anyway.
"
Are there any animation tools available to visualise and simulate deep neural networks? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






Deep learning researchers have to work with a lot of models. The models may include different types of Layers: They include convolutional neural network layers, recurrent neural network layers, batch normalization layers, polling layers, and many others.
Along With their own visualization, it is also necessary to keep the model detailed enough visually to teach about the model.
Although there are widely used visualization methods available in several packages such as model summary and others. I want to know the availability of animation tools that are useful to simulate the models of deep learning that are more visually intuitive.
Are there any contemporary animation packages available true to draw and simulate deep learning models?
","['deep-learning', 'deep-neural-networks', 'models', 'software-evaluation', 'topology']","
I suggest you take a look at Chris Olah's blog. Has several interesting post including ones on visualizing weights and interpretability. Most of his papers also have Google Colab links so you can reproduce the results.
If you want something more similar to the model.summary() method you mention, TensorBoard Graph Dashboard might help.
"
"""Hot Word Detection"", bur for different applications","
I have been looking in detecting a specific rhythm/pattern within the temporal domain for a time-series signal. For this purpose, how ""Wake Up"" words work for devices like Alexa have gained my interest!
I have 2 questions with regards to this matter:

Is this just, say, training a model (e.g. CNN) on a specific word (I know audio processing is not this simple, I meant it in it's simplest form) and running it continually until a hot word is detected? if so is the name just a formalization and not a different method of implementing a ML model?

If this type of model can be used for a time series signal, namely audio, is there any restriction on applying it on a different set of temporal signals? for example, detecting the onset of an earthquake?


Also, I would really appreciate it if someone could reference a good paper to have a read through on this matter, I have looked online but have not been able to find interesting stuff.
",['machine-learning'],
"AI for games which involve social intelligence. Games like warewolf where players must persuade, charm, threaten etc","
I'm looking for any introductory/accessible reading on AI that can play games which involve social intelligence.
Games like poker, where you might bait someone into overcommiting their hand or threaten them into not betting a lot when they should.
Or warewolf, where there is a group of villagers, and a warewolf. Every night the warewolf ""eats"" a villager, and in the morning all the villagers get together to kick out a person who they think is the wolf. Wolf wins if he's the last one alive, villagers win if they manage to kick out the wolf before everyone dies.
I know there's some online content on the game warewolf, but how does AI tackle such concepts in general? How are the rules represented, and how does the AI ""persuade"" and be persuaded?
","['reference-request', 'game-theory', 'incomplete-information', 'card-games']",
How many iterations of the optimisation algorithm are performed on each mini-batch in mini-batch gradient descent?,"
I understand the idea of mini-batch gradient descent for neural networks in that we calculate the gradient of the loss function using one mini-batch at a time and use this gradient to adjust the parameters.
My question is: how many times do we adjust the parameters per mini-batch, i.e. how many optimisation iterations are performed on a mini-batch?
The fact that I can't find anything in the TensorFlow documentation about this to me implies the answer is just 1 iteration per mini-batch. If this assumption is correct, then how does an optimisation algorithm, like adam, work which uses past gradient information? It seems strange, since then gradients from past mini-batches are being used to minimise the loss of the current mini-batch?
","['neural-networks', 'gradient-descent', 'adam', 'mini-batch-gradient-descent']",
Correctly input additional values into CNN,"
I understand that in order to add additional inputs to a CNN, e.g. in self driving, I can append the data to a flattened layer after the convolutions and before the fully connected layers.
However, a few things confuse me. In a paper the authors want to feed speed measurements into the driving network. Instead of just appending a normalized speed value, they first feed it into several FC layers. Why would they do that? What kind of features could you extract from a single real value? Could there be another reason?


(p. 3, paper 1)
Part two of my question is: In another paper, information about the turn signal is appended to a layer as one-hot encoding. The authors talk about how that didn’t work due to vanishing weights (not gradients). So they scaled weights by a constant factor. What do they mean by vanishing weights and how do I scale weights (e.g. in PyTorch)?

(p. 6, paper 2)
","['convolutional-neural-networks', 'weights', 'dense-layers', 'input-layer']","
I would expect the dense layers to be able to detect certain speed ranges. This neuron activates for 0-10, this one for 10-20, this one for 20-30, this one for 20-50, this one for 47.6-89.2...
Of course a later layer could also do that, but it looks like there aren't many layers after this one.
"
"In mini-batch gradient descent, do we pass each input in the batch individually or all inputs at the same time through the layer?","
In the stochastic gradient descent algorithm, the weight update happens for every training sample.
In the mini-batch gradient descent algorithm, the weight update happens for every batch of training samples.
In the batch gradient descent algorithm, the weight update happens for all samples in the training dataset.
I am confused with the procedure of training that happens in the mini-batch gradient descent algorithm. I am guessing one of the following two must be correct

Passing each input individually at each layer and calculating the output. This happens for a number of training samples that are equal to batch size.

Passing a batch of inputs at once at each layer and collecting the batch output at each layer.


Which of the above is true in general implementations of mini-batch gradient descent algorithms to train your neural networks?
","['neural-networks', 'deep-learning', 'gradient-descent', 'implementation', 'mini-batch-gradient-descent']","
What happens in mini-batches is not very different from the way updates are made in batch gradient descent, only the number of samples is different. In mini-batch, you process all the data in the batch, and the update happens after that. It is detailed in this video after 6:11.
"
What could cause the hamming loss and subset accuracy to get stuck in a multi-label image classification problem?,"
I am rather new to deep learning and got some questions on performing a multi-label image classification task with keras convolutional neural networks. Those are mainly referring to evaluating keras models performing multi label classification tasks. I will structure this a bit to get a better overview first.
Problem Description
The underlying dataset are album cover images from different genres. In my case those are electronic, rock, jazz, pop, hiphop. So we have 5 possible classes that are not mutual exclusive. Task is to predict possible genres for a given album cover. Each album cover is of size 300px x 300px. The images are loaded into tensorflow datasets, resized to 150px x 150px.

Model Architecture
The architecture for the model is the following.
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

data_augmentation = keras.Sequential(
  [
    layers.experimental.preprocessing.RandomFlip(""horizontal"", 
                                                 input_shape=(img_height, 
                                                              img_width,
                                                              3)),
   layers.experimental.preprocessing.RandomFlip(""vertical""),
    layers.experimental.preprocessing.RandomRotation(0.4),
   layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.6), width_factor=(0.2, 0.6))
  ]
)

def create_model(num_classes=5, augmentation_layers=None):
  model = Sequential()

  # We can pass a list of layers performing data augmentation here
  if augmentation_layers:
    # The first layer of the augmentation layers must define the input shape
    model.add(augmentation_layers)
    model.add(layers.experimental.preprocessing.Rescaling(1./255))
  else:
    model.add(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)))

  model.add(layers.Conv2D(32, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(128, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(128, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Flatten())
  model.add(layers.Dense(512, activation='relu'))

  # Use sigmoid activation function. Basically we train binary classifiers for each class by specifiying binary crossentropy loss and sigmoid activation on the output layer.
  model.add(layers.Dense(num_classes, activation='sigmoid'))
  model.summary()

  return model

I'm not using the usual metrics here like standard accuracy. In this paper I read that you cannot evaluate multi-label classification models with the usual methods. In chapter 7. evaluation metrics the hamming loss and an adjusted accuracy (variant of exact match) are presented which I use for this model.
The hamming loss is already provided by tensorflow-addons (see here) and an implementation of the subset accuracy I found here (see here).
from tensorflow_addons.metrics import HammingLoss

hamming_loss = HammingLoss(mode=""multilabel"", threshold=0.5)

def subset_accuracy(y_true, y_pred):
    # From https://stackoverflow.com/questions/56739708/how-to-implement-exact-match-subset-accuracy-as-a-metric-for-keras

    threshold = tf.constant(.5, tf.float32)
    gtt_pred = tf.math.greater(y_pred, threshold)
    gtt_true = tf.math.greater(y_true, threshold)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(gtt_pred, gtt_true), tf.float32), axis=-1)
    return accuracy

 # Create model
 model = create_model(num_classes=5, augmentation_layers=data_augmentation)

 # Compile model  
 model.compile(loss=""binary_crossentropy"", optimizer=""adam"", metrics=[subset_accuracy, hamming_loss])

 # Fit the model
 history = model.fit(training_dataset, epochs=epochs, validation_data=validation_dataset, callbacks=callbacks)


Problem with this model
When training the model  subset_accuracy hamming_loss are at some point stuck which looks like the following:

What could cause this behaviour? I am honestly a little bit lost right now. Could this be a case of the dying ReLU problem? Or is it wrong use of the metrics mentioned or is the implementation of those maybe wrong?
So far, I tried to test different optimizers and lowering the learning rate (e.g. from 0.01 to 0.001, 0.0001, etc..) but that didn't help either.
","['tensorflow', 'keras', 'performance', 'multi-label-classification']","
Generally when you see metrics stagnate like this it's because the model has converged incorrectly (ex: always predicting 0, or gradients/weights have dropped to 0, etc.). Can you see if this is what's happening for your problem?
I'd expect that perhaps your model has converged to predict the majority class for each label.
"
Recommended way to spilt image sequence for training/validation/testing,"
For object detection tasks I have a few minutes of video footage from a surveillance camera, converted to a sequence of images and ground truth bounding boxes for all people walking by.
Now what's the best way to split this into training, validation and test sets (80/10/10)?

I could randomly select 10% for testing and 10% for validation and rest into training.
The first 80% go into training, next 10% to validation, rest to testing.


The first way has the advantage of having a good distribution of different people walking by and also more varying densities and locations of people in the test set. But the disadvantage would be that for each testing image a very similar image exists in the training set.
The second way would have the advantage of the testset being more truly ""never seen"" during training, but at cost of less variety.
","['machine-learning', 'datasets', 'data-preprocessing']",
Is there any relationship between the batch size and the number of epochs?,"
I am currently running a program with a batch size of 17 instead of batch size 32. The benchmark results are obtained at a batch size of 32 with the number of epochs 700.
Now I am running with batch size 17 with unchanged number epochs. So I am interested to know whether there is any relationship between the batch size and the number of epochs in general.
Do I need to increase the number of epochs? Or is it entirely dependent on the program?
","['neural-networks', 'deep-learning', 'hyper-parameters', 'batch-size', 'epochs']","
The smaller the batch_size is, the larger number of batches is processed per one epoch.
On one hand, since one makes more steps per epoch, one can think, that less epochs are required to achieve the same level of accuracy.
On the other side, smaller batch size leads to more noisy and stochastic estimates of the gradient, therefore, convergence would not be as steady most likely.
I think it is difficult to give a definite answer about the exact relation on the number of epochs - since say, to achieve a certain level of accuracy use of small batch may be more beneficial since it allows for more exploration and is more likely to escape from local minima and saddles, but when one reaches the approximation limit of the network and is in the vicinity of the good optimum - large batch would descend better to the extremum.
Good strategy usually is to start from smaller batches to find wide and plain minima, which are better from the generalization point of view, and then increase batch size for steadier convergence.
"
How do you handle unbalanced image datasets?,"
I have an image data set on which I am training a CNN. The data set is slightly unbalanced. So, my solution up till now was to delete some images of the majority class.
But I now realize that there are cleaner ways to deal with this. But I haven't been able to find ways to fix unbalanced image data sets. Only structured data-sets.
I would like someone to guide me to fix the unbalance, other than deleting data from the majority class.
","['machine-learning', 'convolutional-neural-networks', 'datasets', 'imbalanced-datasets']",
How general is generalization?,"
I am sorry but I have to explain my question using an example, I do not know how to ask it in proper scientific terms.
Let's assume, I have trained a deep learning model on classifying hand gestures, but training and testing datasets' images are shot only in one lighting conditions and I achieved certain accuracy, let's assume 85%. As far as I understand, adding more data of the same hand gestures images but shot with different lightning should increase my model's ""generalization"" capabilities, right?
So the question is, if I take this model, trained in two lightning conditions, and test it only on the dataset of the first lightning conditions, would that increase it's accuracy (the 85%) or maybe this ""generalization"" would only mean that it can now also classify correctly images with different lightning, but not increase the accuracy on the first set?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'computer-vision', 'generalization']","
I think there's a crucial point missed in the question, touched by jros answer but without further elaboration.
If you train a model on domain A: single lightning condition and test it on domain B: two lightning condition then you're not evaluating generalization but transfer learning capabilities. Or to phrase it differently you're evaluating how close domain A and B are for the model you trained.
The test set as you said is truly made of instances never seen by the model during training, but it should nevertheless be representative, i.e. correctly sampled, from the training domain, or from the same distribution as jros wrote. So the generalization of your model, trained on single lightning condition, should be evaluated on single lighting condition as well.
A final remark about the rest that have been said:

everything holds only under the assumptions that the initial training dataset is not only unbiased but also balanced. In a real case scenario changing the training distribution from something specific (single light condition) to another distribution (multiple light conditions) might well be lead to a worse model, simply cause the problem is now inherently harder to solve.

So the answer to your question (regarding both, true generalization on same distribution and what you describe, transfer learning) is actually just empirical.
"
Is it true that channels always represent colours of an image?,"
Convolutional neural networks are widely used in image-related tasks in artificial intelligence.
The input of a conventional neural network is generally an image. The output of a convolutional neural network can also be an image. But the output of the hidden/ intermediate layers of a convolutional neural network is generally the feature maps of the input image.
In general, channels of an image, represent colors used. If the input and output of a convolutional neural network are RGB images. Then it is true that three channels of input and output images are representations for red green and blue colors. Is it true for feature maps also? Do the channels in feature maps are also the representations of colours?
I got this doubt because of the reason that I remember channels as colours and feature maps may contain an arbitrary number of channels. If they are also represented under then it is difficult for me to understand.
","['convolutional-neural-networks', 'features', 'channel']","
No, channels do not have to only represent colours. It is common for them to represent other things, even without considering feature maps. For instance RGBD images, where D is a depth measurement or distance from a sensor. Or when CNNs are applied to grid-based games, such as chess or go with AlphaZero, where the input channels are information about game pieces on a board.
Mathematically, there is little to differentiate between a channel or a feature map. Both are numerical values stored in some muti-dimensional array, most often with the following assumptions:

All values in a single channel or single feature map represent a measurment of the same concept. That might be how much blue light was detected at a sensor at a point in space, or it may be the degree to which pixels close to that point match patterns associated with the centre of a certain type of cat's nose.

The values are considered co-located with other feature maps or channels within the same system, such that values at index $i,j$ (or just $i$ of 1D or $i,j,k$ for 3D etc) in one channel are considered to be at same location as values in related channels or feature maps, at least within the same layer.


You will tend to find channels used to describe inputs and outputs that can be directly visualised, whilst feature maps tend to be used to describe the more abstract pattern matching that occurs in the outputs of a CNNs hidden convolutional layers. However, the two terms can be used loosely, and sometimes interchangeably.
The feature maps within a CNN typically do not carry separate colour channels. Although it is possible to design architectures that keep colour information separate, this is very rarely used - normal CNN architectures allow mixing of all layer channels/features with each new layer, through the mechanism of having weights that connect every input channel/feature to every output channel/feature between layers.
You will sometimes see colour channel information extracted from the neural network weights of the first convolutional layer, in order to visualise what that layer is matching to. That is because the first layer's weights (and only the first layer's weights) can be interpretted as template matching to the input channel for each output feature map. This is not the same as visualising the output feature maps - whilst those maps are influenced by the input colour channels, thus do in a general sense carry colour information, they do not measure colour intensity in the same sense as an image channel used for input.
More generally, because human perception is strongly tied to RGB colour channels, and because computer displays and image formats are designed around this, whenever you see any representation of what a CNN layer is doing, you will see one of:

A greyscale representation of feature map values. This is the closest to ""true"" representation, but sometimes it is not very informative.

A heat map of feature map values. Using colour may help with visualisation, but it is false colour in the sense that the same colours are not actually in the feature map.

A representative input that would cause the feature map to activate. This can be informative about the feature map, but it is not showing what the feature map is doing directly, and the channels defined in the input are used for colour.


"
How to assess the goodness of a text generation algorithm,"
Take a RNN network fed with Shakespeare and generating Shakespeare-like text.
Once a model seems mathematically fine, as can be assessed by observing its loss and accuracy over training epochs, how can one assess and refine the goodness of the result ?
Only human eyes can judge of the readable character of a text, its creativity, its grammatical correctness etc.
QUESTION : Which systematic approach can be used to refine a generative model (text) ?
","['machine-learning', 'deep-learning', 'training', 'philosophy', 'generative-adversarial-networks']",
"Mathematically speaking, Is it only the product operation used in the chain rule causing the vanishing or exploding gradient?","
I am asking this question from the mathematical perspective of the vanishing and exploding gradient problems that we face generally during training deep neural networks.
The chain rule of differentiation for a composite function can be expressed roughly as follows:
$$\dfrac{d}{dx} (f_1(f_2(f_3(\cdots f_n(x))))) = \dfrac{df_n}{dx} \dfrac{df_{n-1}}{dy_1} \dfrac{df_{n-2}}{dy_2} \cdots \dfrac{df_1}{dy_{n-1}}$$
We know that multilayer perceptrons are composite functions of layer functions. So, if layers are increasing, then the gradient terms to multiply will increase on the right-hand side.
If all the gradient terms on the right-hand side are between 0 and 1 then the gradient will become less and less if layers keep on increasing the layers. This phenomenon is called the vanishing gradient problem. Similarly, if all the gradient terms on the right-hand side are greater than 1. Then the product will become more and more. This phenomenon is called exploding gradient problem.
Since it is customary to use the same activation function across all the layers in deep neural networks, all the gradients on the right hand behave in a similar manner, i.e. either most of the gradient terms on the right-hand side fall between 0 and 1 or greater than one, which causes either vanishing gradient or exploding gradient problem.
Do my mathematical interpretation of the vanishing and exploding gradient problem true?  Am I missing anything?
","['math', 'gradient', 'vanishing-gradient-problem', 'exploding-gradient-problem']",
How to handle invalid actions for next state in Q-learning loss,"
I am implementing an RL application in an environment with illegal moves. For handling the illegal moves, I am currently just picking an action as the maximum Q-value from the set of legal Q-values.
So, it is clear that when deciding on actions we only pick from a subset of valid Q-values, but, when using the Q-learning algorithm, do we also want to consider the subset of invalid actions for the $\max\limits_{a}Q(s_{t+1},a)$?
My gut tells me that we consider all actions for the max function, purely based on the lack of documentation on the subject, but only considering the subset of legal actions makes more sense to me. I'm having a hard time finding any reliable sources addressing this topic. Any advice/direction would be greatly appreciated.

","['machine-learning', 'q-learning', 'temporal-difference-methods', 'loss', 'action-spaces']",
Can a Reinforcement Learning problem with multiple simultaneous actions be formalized as a Multiagent Partially Observable Markov Decision Process?,"
Consider the following decision making problem. We have a controller that selects locations from a grid of coordinates and captures an image (observation $o_t$) with a camera at each location (action $a_t$). We try to find an optimal sequence of locations for a specific goal. This decision making problem can be formalized as a Partially Observable Markov Decision Process (POMDP). Here, we seek an optimal stochastic policy $\pi^{*}_{\theta}(a_t|h_t)$ that maps the history $h_t= \langle o_1, a_1, ..., o_{t-1},a_{t-1},o_t \rangle$ of actions and observations up to the current time $t$ to action probabilities. The history $h_t$ can be summarized by the hidden state of a RNN and we can use a policy gradient method, e.g. REINFORCE, to update the policy parameters $\theta$.
Suppose now that we want to select multiple locations, i.e. actions, simultaneously. According to my understanding, we could formalize the problem as a Mutliagent POMDP (MPOMDP) [1]. In this formalism, we would replace the single action of the previous problem by joint actions $\vec{a}_t = \langle a^1_t, ..., a^N_t \rangle$, the single observation by joint observations $\vec{o}_t = \langle o^1_t, ..., o^N_t \rangle$ and the history by $h_t= \langle \vec{o}_1, \vec{a}_1, ..., \vec{o}_{t-1},\vec{a}_{t-1},\vec{o}_t \rangle$, where $N$ is the number of agents. We would now try to find an optimal joint policy $\vec{\pi}^{*} = \langle \pi^{1*}, ...,\pi^{N*} \rangle$ consisting of sub-policies $\pi_{\theta_n}(a^n_t|h_t)$ that map the history $h_t$ to the action probability of each agent $n$. This would mean that the RNN would have $N$ output nodes and each sub-policy $\pi^n$ would be parametrized by $\theta_n$, a sub-set of weights of the output layer [2]. Would it be correct to assume that an optimal or near-optimal joint policy $\vec{\pi}^{*}$ can be obtained by simply applying the policy gradient method used above to each sub-policy $\pi^n$?
I would be curious to hear what you think about the MPOMDP formalism applied to the latter decision making problem or whether you would suggest something else.
[1] Oliehoek, Frans A., et al. ""A concise introduction to decentralized POMDPs."" Springer, 2016.
[2] Gupta, Jayesh K., et al. ""Cooperative multi-agent control using deep reinforcement learning."" International Conference on Autonomous Agents and Multiagent Systems. Springer, Cham, 2017.
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'multi-agent-systems', 'pomdp']","
I guess it depends on what the goal is. If the goal is a general reward function, this formulation as an MPOMDP could make sense. One way to think about this, is as a way of modeling a general (centralized) POMDP with factored actions and observation spaces.
However, it seems that what you are describing might be an active perception problem, where the goal is to minimize uncertainty? In that case, there could be better formulations. I personally did some work in this direction with Yash Satsangi and others. E.g., these papers might be of interest:
Yash Satsangi, Shimon Whiteson, Frans A. Oliehoek, and Henri Bouma. Real-Time Resource Allocation for Tracking Systems. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence (UAI), August 2017.
Yash Satsangi, Shimon Whiteson, Frans A. Oliehoek, and Matthijs T. J. Spaan. Exploiting submodular value functions for scaling up active perception. Autonomous Robots, 42(2):209–233, February 2018.
(and there is a large literature on this topic as well - look at the references in the papers).
Hope that is useful.
"
"What is meant by ""shorter connections"" in the case of deep convolutional neural networks?","
Consider the following two excerpts from the research paper titled Densely Connected Convolutional Networks by Gao Huang et al.
#1: From abstract

Recent work has shown that convolutional networks can be substantially
deeper, more accurate, and efficient to train if they contain shorter
connections between layers close to the input and those close to the
output.

#2: From discussion

One explanation for the improved accuracy of dense convolutional
networks may be that individual layers receive additional supervision
from the loss function through the shorter connections. One can
interpret DenseNets to perform a kind of “deep supervision”.

Both excerpts mention the type of connections called shorter connections, especially to the layers that are close to the input and the output layers of the deep convolutional neural network. What does it mean by shorter connections here?
",['convolutional-neural-networks'],"
I think this is one of those vague terminologies used in the context of skip (long-range connections). In a standard feedforward network, say if the information needs to propagate from layer 1 to 4, it has to go through two intermediate layers, namely $1 \to 2\to 3\to 4$. In a densely connected convolution network, every layer is connected to every layer downstream, meaning that you have a direct connection from layer 1 to 4, or $1\to 4$, hence a ""shorter connection"".
Note you also have other ""shorter connections"" such as $1\to 3\to 4$ and $1 \to 2 \to 4$
"
Are there any deep RL algorithms that work well on finite MDPs and non-trivial terminal rewards?,"
I notice that most Deep Reinforcement Learning (DRL) works focus on Markov Decision Process (MDP) with an infinite time horizon.
Are there any algorithms that work well on finite MDP and non-trivial terminal reward?
My definition of non-trivial terminal reward is that the reward function at the terminal time is different from the one at non-terminal timestamps. Many environments or games fall into this category. For example, many games are on $[0, T]$, the total reward is the summation of the accumulated reward plus the final bonus.
","['reinforcement-learning', 'deep-rl', 'markov-decision-process', 'algorithm-request', 'reward-functions']",
Is it okay to use publicly available Instagram videos to train an AI?,"
Since I haven't found any good training data for my university project, I want to use pictures and videos from public Instagram profiles. Am I allowed to do that?
","['computer-vision', 'training', 'datasets', 'research', 'image-processing']","
Disclaimer that every attorney will give unless formally engaged: This does not constitute formal legal advice.

This data was published with the expectation of public view

Viewing that data is a form of utilization—taking in information which may be used to make a decision, or just as divertment (here consumption of entertainment media by humans.)
If any observer can input that data, you can surely use that data for analysis by your learning algorithm.

It's research
It's not for profit
There is no infringement

Selling that dataset might constitute infringement, in that copyright in inherent to natural persons when they create novel content in any memory medium (paper, film, clay tablets, high-level representations of machine instructions, etc.)¹
Re-publishing that data without permission is definitely infringement, except in cases of fair use.
https://en.wikipedia.org/wiki/Fair_use#File_sharing
We in the AI community are on much firmer ground than file sharers, documentarians, internet publishers, and other professional communities because all we are doing is accessing a public dataset for the purpose of scientific research.
The internet itself constitutes a database,² freely available except in the case of firewalls.
My feeling is that these public databases (instagram, et al) are meant to be processed, analyzed, and utilized by any networked users, just as a mathematical formula might.
The internet and all servers and nodes are part of a computational process. Compiling into a dataset is just an extension of that computational process. That data is reproduced and mirrored locally when it is put into packets and transmitted to any system requesting the data.

This content is all in the public domain

They may not have a creative commons statement, but, my feeling is, in this case, so long as you don't re-publish for profit, you're not infringing—there is no financial harm, thus no damages³
Even if an Instagram user believes you are infringing, they must first engage an attorney to send you a cease-and-desist letter. So must Instagram.
In that likely instance, simply comply or challenge by refusing.

Sociologists use datasets like this for research, and I haven't head of any sociologists being sued for studying Facebook!

The analysis of those datasets, even by sociologists, requires some form of automated analytics, but which we can mean intelligence.

[1] I'd have to research further to determine this b/c many are surely doing it or have tried.  But not your question, so will leave unanswered.
[2] Web pages and websites are a form of database, typically with open read access to any user, human or algorithmic.  The internet is a structured database of structured data elements (pages and sites).
[3] I'm taking some graduate IT courses now and my sense is the legal advice schools are being given is to require citation even where citation is not necessary because it's fuzzy. It creates a lot of unnecessary citations of rewordings of common knowledge, but they are choosing strict minimax (paranoia). But this may be b/c non-traditional programs are teaching students to google then reword summaries to avoid detection by current gen plagiarism checkers, and thereby successfully simulate research, even where all the content is generic.
The NFL says specifically ""not reproduce without express consent of the NFL"", but that's broadcast/satellite/cable, which is robustly regulated.
Damages are very hard to prove, and it's a very expensive proposition, with no guarantee of success.  It's a losing proposition for Instagram no matter how you slice it.  (These are the mechanics of law.)
"
How to measure Deep RL algorithms in terms of safety?,"
I applied for a Ph.D. in AI, my advisor told me that my thesis is about safe applications of deep RL algorithms in healthcare. So I decided to do as the first paper, a comparison of Deep RL algorithms in terms of their inherent safety. However, after lots of research, I could not find an answer to my question, that is: How to measure Deep RL algorithms in terms of safety?
","['reinforcement-learning', 'ai-safety']","
I think you should first start with definition of software safety in health domain. For example, you should start with Therac-25 accident.
Then look at the current scientific articles and standards about software safety in medical domain. Then think about how your algorithm will be tested.
You are thinking Deep RL algorithms as a blackbox but they are software in the end.
If Deep RL algorithms will be used in hospitals, they will have to be tested. The benchmarks, conditions and restrictions of normal software must apply to RL algorithms too.
"
How to reduce the number of clusters produced by the Markov Clustering Algorithm?,"
I have used the Markov Clustering Algorithm (MCL) to cluster tweets, based on their similarity. However, I got a too high number of clusters, and most of the clusters have only one tweet. Any suggestions to reduce the number of clusters?
","['natural-language-processing', 'hyper-parameters', 'clustering', 'cosine-similarity']","
Depending on the implementation you're using, you can adjust the granularity, which will influence how many clusters you will get.
See this description of MCL.
"
Is there a widely accepted definition of the width of a neural network?,"
The depth of a neural network is equal to the total number of layers in the neural network (excluding the input layer by convention). A neural network with ""many layers"" is called a deep neural network.
On the other hand, the width is the name of a property of a layer in a neural network: it is equal to the number of neurons in that particular layer. So, it may be apt to use the phrase ""the width of a layer in a neural network"".
But, is it valid to use the phrase ""width of a neural network""?
I got this doubt because the phrase ""wide neural network"" is widely used. The phrase gives the impression that the width is a property of a neural network. So, I am wondering whether the width of a neural network has a definition. For example, say, the width of a neural network is the number of neurons in the widest layer of that neural network.
","['neural-networks', 'deep-learning', 'terminology']","
The width of a neural network layer is an agreed upon term.
According to Lou et al., in the paper The Expressive Power of Neural Networks: A View from the Width (page 4), the width of a neural network is the width of the widest layer of the neural network.

The architecture of neural networks often specified by the width and the depth of the networks. The depth $h$ of a network is defined as its number of layers (including output layer but excluding input
layer); while the width $d_m$ of a network is defined to be the maximal number of nodes in a layer
(emphasis added)

So, I would caution you to be careful with how you use the phrase ""the width of a neural network"", due to interpretability and scale, and the fact that neural networks often contain layers with varying numbers of neurons, depending on the layer.
From this Wikipedia page on ""Large width limits of neural networks"":

The number of neurons in a layer is called the layer width.

From a nice machine learning resource page

Finally, there are terms used to describe the shape and capability of a neural network; for example:

Size: The number of nodes in the model.
Width: The number of nodes in a specific layer.



A node [is] also called a neuron or Perceptron

"
"Would empathy in AI be a reliable tool/capacity, or contribute to a solution to avoid harm done to humans or to other versions of AI?","
TL;DR
Would providing AI the capability of experiencing something as close as possible to the subjective human experience and from that acquiring empathy in the process be a solution, or contribute to a solution, that seeks to prevent AI to cause the same kind of harm we ourselves as a species have caused to each other in the past, and continue to do so, to us humans and also to itself and/or to other versions of AI?
LONG VERSION:
I was thinking about a question I listened being asked on a podcast where the topic was AI, which was something like: does AI need to have a body in order to understand what it is or how it feels to have a body. The person being interviewed said no. AI can understand it without having a body.
My perception is that a lot of what goes on in AI development at the moment is about AI emulating and simulating human behavior in a way, at the external level, that is by observing human behavior and processing it, looking for patterns etc: speech, motion, the human DNA etc.
I was thinking also that in order for a human being to relate and engage with society and with himself in a constructive, healthy way, and with minimal damage, it will need to possess a certain level of empathy. That is to relate to others and possess the ability to somehow feel what the other is feeling.
Now feeling is not an objective state or something you can observe. Maybe you can measure by observing the human behavior that is a result of feeling but the thing in itself can't be measured or captured and transmitted to AI in a way it can learn from human feeling. One can argue it can be done by studying the brain activity at the moment feeling is occurring. But the subjective experience of experiencing feeling and then being able to recognize that experience in others, understanding what the other is feeling because you have felt the same, that is a subjective occurrence.
It is one thing to understand the human body by observation and study of it's patterns, it's something different to be locked inside one and experience its pains and pleasures.
I am left wondering how can AI understand the human condition if it will not experience what it is to be a human because it is built around the material world, built around what can be objectively observed and measured.
The closest thing there is in human experience to an AI in this regard could be a human that suffers some condition which makes him/her being unable to experience empathy. I was about to use the word Psychopathy but being a loaded word might not be the best choice here. But suffice to say that humans that suffer from some kind of psychiatric condition, or trauma based experience which causes the person to lack the capability to feel empathy towards others might, or not, be easier for that person to in the future engage in actions which can cause harm to others and/or him/herself.
We have cases in human history that might be used as an example. In the past countless human beings engaged in actions which due to its nature I can only conclude this people lacked any empathy or were placed in a situation where empathy was drained out of them. Like for example in war, usually soldiers and the whole country will go through a phase of propaganda which consists of dehumanizing the enemy. I believe this is done in order to facilitate and eliminate any sense of empathy and humanity in the population and building up the necessary will, energy and determination for the killing of the enemy.
The solution to this question might be to create in AI the capability of experiencing subjectively what it is to be human, or what it is to have a body, or come up with something that makes it possible for AI to experience something similar to human empathy.
One thing comes to mind which is the merge of AI and the human mind. But this might be another topic altogether.
It has been argued in some places I remember, even in some Science-Fiction that actually the main problem of destructive human behavior is the ability to feel emotion and feeling. And the solution might just be to eliminate or diminish that part of ourselves. I really don't agree with this view and it seems to me that ATM our society does engage and promote  solutions of this type: drugs such as some anti-depressants for ex. instead of going to the root of the problem which in many situations are trauma based. I really hope we don't go this way. I believe that if we have a future the solution is to find ways of developing and cultivating empathy.
","['philosophy', 'human-like', 'ethics', 'emotional-intelligence', 'asimovs-laws']","
SHORT ANSWER: Yes. Empathy can is a good emotion for an AI, but no the best for avoid harm people, could contribute? YES.
ANSWER ABOUT PROVIDE HUMAN EXPERIENCE TO AN AI:
For Humans empathy is not the thing most important for avoid people hurt each other. We may need more, and this is a good law system with people believing in that (political, religious or cultural), a system based in laws that makes the people think twice before committing a crime, a system of laws based on the punishment if a crime is committed. that make that people have a afraid in the future for do it crimes, genocides and damage to the society and people. With this system an AI would be lose more than earn.
I think empathy is not enough for the AI you need laws just like the people. If I do this then that happen to me. a good example can be if a AI harm people eventually will be turned off just like we do with criminals with the death penalty or life imprisonment. The Asimov laws is a good starting point.but we need more than that.
Providing AI with ""human emotions"": We may have more problems, for example, AI may develop strong emotions such as hatred, Resentment, Envy, Fear, Revenge and eventually a War this is the main reason for the Holocaust, Rwandan-Genocide, and a lot of conflicts in the world. We are a violent species if we have to create an AI it will be to be better than us, these emotions are useless. so you have to be eliminate any emotions that lead to conflicts.
Here are some extension lectures:
The amygdala — a part of the brain involved in fear, aggression and social interactions — is implicated in crime.
"
Multiple GRU layers to improve a text generation,"
I am using the model in this colab https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb#scrollTo=AM2Uma_-yVIq for Shakespeare like text generation.
It looks like this
class MyModel(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, rnn_units):
    super().__init__(self)
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(rnn_units,
                                   return_sequences=True,
                                   return_state=True)
    self.dense = tf.keras.layers.Dense(vocab_size)

  def call(self, inputs, states=None, return_state=False, training=False):
    x = inputs
    x = self.embedding(x, training=training)
    if states is None:
      states = self.gru.get_initial_state(x)
    x, states = self.gru(x, initial_state=states, training=training)
    x = self.dense(x, training=training)

    if return_state:
      return x, states
    else:
      return x

I look for strategies to improve the model, under the hypothesis that there is a reliant why to assess the goodness of my model; how could adding one or more GRU layers improve the model, e.g., number of rnn_units, number of layers, for such a stacked model ?
For instance, this gives extremely bad results
class MyModel(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, rnn_units):
    super().__init__(self)
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(rnn_units,
                                   return_sequences=True,
                                   return_state=True)
    self.gru_2 = tf.keras.layers.GRU(rnn_units,
                                   return_sequences=True,
                                   return_state=True)
    self.dense = tf.keras.layers.Dense(vocab_size)

  def call(self, inputs, states=None, return_state=False, training=False):
    x = inputs
    x = self.embedding(x, training=training)
    if states is None:
      states = self.gru.get_initial_state(x)
    x, states = self.gru(x, initial_state=states, training=training)
    if states is None:
      states = self.gru_2.get_initial_state(x)
    x, states = self.gru_2(x, initial_state=states, training=training)
    x = self.dense(x, training=training)

    if return_state:
      return x, states
    else:
        return x

","['neural-networks', 'machine-learning', 'tensorflow', 'recurrent-neural-networks', 'gated-recurrent-unit']","
A single recurrent network layer $l=1$ in the original example carries over its own state $S^{\mathrm{out}(l=1)}_{t-1} \to S^{\mathrm{in}(l=1)}_{t}$ from one minichain/minibatch to the next along the increasing sequence's time[like] (w.l.o.g.) discrete coordinate¹ $(t-1) \gets t$ equidistantly² increasing from one invocation to the next, via the states argument to the MyModel.call() function.
THE INVARIANT $(\mathcal{I})$: Each recurrent layer $l$ has its own carry-over state
$$S^{\mathrm{out}(l)}_{t-1} \to S^{\mathrm{in}(l)}_{t} \mathcal{\tag{I}}$$
This is not what's going on in the second code example, and is indeed the root cause of the very expected, miserably invalid backpropagation gradient approximations. The code takes in $S^{\mathrm{out}(1)}_{t-1}$ and correctly passes it as the input state to the GRU layer $l=1$, receiving a new state $S^{\mathrm{out}(1)}_{t}$ as part of the layer's return tuple. But then the horrible happens: the code passes this state to another layer $l'=2$ as if it were $S^{\mathrm{in}(2)}_{t}$, which is obviously a no-no-no-no-no! These states are unrelated, because they belong to different internal spaces: each one is that of its own respective layer! The tensor[-likeness] $S^{*(l)}$ lives on an entirely different manifold than $S^{*(l')}$ does for $l \ne l'$!
What you should do instead is collect every $S^{\mathrm{out}(l)}_{t}$ for each layer $l$ separately, and pass them to their respective layers as $S^{\mathrm{in}(l)}_{t'}$ on the next iteration and in the same states variable, after the discrete co-ordinate value $t' \gets t$ is advanced by the framework, to ensure that the invariant $(\mathcal{I})$ is conserved.
The code for a model with two recurrent layers, having being corrected for the inconsistency noted above, and including the initialization of the states $S^{\mathrm{in}(l)}_{0}$ for the initial time[likeness] $t=0$ and both layers $l=\{1,2\}$, may look like the following:
class MyModel(tf.keras.Model):
    . . .
    self.gru1 = tf.keras.layers.GRU(rnn_units_1,
    . . .
    self.gru2 = tf.keras.layers.GRU(rnn_units_2,
    . . .

  def call(self, inputs, states=None, return_state=False, training=False):
    . . .
    (s1, s2) = states or (self.gru1.get_initial_state(x),
                          self.gru2.get_initial_state(x))
    x, s1 = self.gru1(x, initial_state=s1, training=training)
    x, s2 = self.gru2(x, initial_state=s2, training=training)
    . . .
    return (x, (s1, s2)) if return_state else x

The False-ity of the None literal value together with the or construct may be conveniently used in this manner to make the code arguably³ more readable.
In the case of packing more recurrent layers into the model, the tuple may be an awkward structure to work with; whether storing the references to all RNN layers in, and passing their respective internal states via a list, tuple or another data structure, is your code readability judgment call.
____________________________
¹ Indices, as do all contravariant tensor bases, transform covariantly. We indicate this fact by the using of a reversed arrow.
² We're using the increment of $1$ w.l.o.g.
³ But please don't: this is far beyond the scope of the question. There's CodeReview.SE for that! :)
"
Why is the validation accuracy lower in case of CNN?,"
I fed the same set of 1.4 million data to two different models:

MLP
CNN model

In both cases, I used the same parameters and hyperparameters.
The CNN is showing comparatively lower accuracy (80%) than that of the MLP (82%).
Why?
And, also, what does this experiment tell us?
Edit:

Is the data images, video or audio, or other grid-based signals (same signal repeated at multiple locations with a meaningful distance metric between them) in your case?

It is protein C-alpha distances data that has 3 classes (helix, strand, coil) and n-number of features, where n is an even number.

In fact, what is it, what was your expectation of the relative performance of the two models, and why?

I thought CNN would be more efficient and thereby would demonstrate better test/validation accuracy.
","['neural-networks', 'convolutional-neural-networks', 'models']","
To get a full understanding of your problem, one would like to know what approximately the $n$-features are.
Whether, it is about the geometrical structure, protein is described by a graph, where vertices correspond to atoms and edges to bonds within them - I would consider use of GraphNN, there is some research, that has demonstrated the success of GraphNN for protein prediction:

Structure-based protein function prediction using graph convolutional networks
Protein Interface Prediction using Graph Convolutional Networks

In case, your data is some general form tabular data with features of different nature and form - like some continuous features, binary features, categorical features, whatever, there is no notion of proximity between different features.
The efficiency of CNN is based a lot on the ability of them to have a notion of locality, aggregate the information from the neighborhood, and construct the hierarchy of low-level (for several neighboring pixels) features and global (that understand the image or a signal as a whole) features.
Two neighboring pixes from the cat's ear have a notion of proximity and relatedness, whereas the red color and square shape do not have.
For unstructured tabular data I would recommend to start from some tree ensembling approach:

Random forest
Gradient boosting

EDIT
Seems like what you receive as input is a matrix of pairwise distances $\rho(i, j)$ between all possible amino residues - Protein contact map.
I would think about it as a weighted adjacency matrix. However, there is a rather special structure, 
Therefore, there is no need for generic GNN, most likely.
In the literature, the recent research solves this problem with the help of CNN:

Improved protein structure prediction using predicted interresidue orientations
A fully open-source framework for deep learning protein real-valued distances

Probably, you should tune some hyperparameters, or introduce residual connections, if there are no such at the moment.
"
"How to understand the common practices followed for writing a ""bounding box"" for an image in datasets?","
For the image datasets, there may be a bounding box for each image at the dataset. It is an annotation for an image. It is a rectangular box intended for focusing on something inside the image.
I read about the following two types of representations for a bounding box.

using two points $(x_1, y_1)$ and $(x_2, y_2)$.

$$<x_1><y_1><x_2><y_2>$$

Using a point $(x_1, y_1)$, width, and height.
$$<x_1><y_1><width><height>$$

How do understand both the representations, Specifically, does the point $(x_1, y_1)$ used to denote the top right or top left or bottom right or bottom left in both cases?
","['image-processing', 'bounding-box']","
With Bounding box people usually mean literally the box which bounds something. So, more intuitive way to use the points in the first case as bottom left and top right points and in the second case bottom left point and sizes. This corresponds to the logic of points on the Cartesian grid.
Of course, you may reverse sometimes against Y axis in case your task requires so. For example, some software has X origin at top left corner of the monitor. In this case, sometimes y subscripts can reverse.
"
How to generate a response while considering past questions as well?,"
User: What is the tallest mountain?
Agent: Everest
User: Where is it located? # Agent hears: ""Where is Everest located?""
Agent: Nepal

I want to be able to generate a sequence that has been generated using the user's current query as well as the past conversation.
More specifically, I am using Google's T5 for closed-book question answering, but, instead of trivial questions, we use the user's frequently asked queries.
I want to be able to encode their past questions and the agent's past answers, then use them to generate the agent's next answer. How can I do that?
","['natural-language-processing', 'python', 'transformer', 'question-answering']","
What you want to look for is called anaphora resolution. You basically keep a record of the past conversation and try and find an antecedent for any occurrences of it, he/she, her/his, etc. You probably want to have a pre-processing step where you substitute the antecedent before passing the input sentence on to the agent.
"
"How to show $\rho > 0$ when $\rho$ be minimum attainable from $y_n(W^{*T}X_n)$, where $W^*$ the vector that separates the data?","
In the book Learning from Data written (by Abu Mostafa), we have the following exercise:

Let $\rho$ be minimum attainable from $y_n(W^{*T}X_n)$ where $W^*$ is the vector that separates the data. Show $\rho > 0$. Also assume the Perceptron Learning Algorithm is initialized with the 0 vector.

How to prove the above statement?
I thought that it could be negative since a Perceptron function returns either +/-1?
Even I wonder if I comprehend this proof question correctly.
","['machine-learning', 'proofs', 'perceptron']",
Uniform representation of images for machine learning,"
I'm new to the field of ML so please bear with me while I try to explain what I'm looking for. In most machine learning pipelines that deal with images there is a requirement to ""normalize"" the data in some way so that images of different dimensions can be used as inputs for the function that is being optimized. As in, if the function takes its input as an $n\times n$ grid of pixels (assuming we're dealing with 1-channel images) then any image that is not of the right shape must be re-shaped so that it can be used as input. We can assume $n = 2$ without losing any generality because any larger image can be reduced to the $2\times 2$ case for what I'm about to describe.
So if we assume we have a $2\times 2$ image then there is an obvious way to map such an image to a function defined on $[0,1]\times [0,1]$ ($f:[0,1]\times[0,1]\rightarrow\mathbb{R}$) by using convex combinations of the points in the image. If the points of the image are labeled as $x_{00},x_{01},x_{10},x_{11}$ where $x_{00}$ is the top left corner and $x_{11}$ is the bottom right corner then given a point $(a,b)\in[0,1]\times[0,1]$ the value of $f$ at $(a, b)$ can be defined as $$f(a, b) = (1-b)((1-a)x_{10}+ax_{11})+b((1-a)x_{00}+ax_{01})$$
Assuming I got all the signs right it's obvious that this idea can be extended to any grid of pixels by mapping the horizontal and vertical dimensions to $[0,1]$ and then interpolating between the grid points as in the $2\times 2$ case. So this mapping from grids of pixels to functions provides a uniform representation for all images as functions defined on $[0,1]\times[0,1]$.
Now my question is the following: Is there any work that tries to use this kind of representation of images and if there isn't does anyone know what exactly are the obstructions to doing so? It's possible I'm missing something that makes this approach non-viable but I wasn't able to find anything that explained one way or the other why the usual tensor representation is preferable to a functional one as above that reduces all images to functions on $[0,1]\times[0,1]$.
","['image-processing', 'function-approximation']",
What is the reason behind using node embeddings?,"
I was reading Chapter 3 from the following book (here) on graph representation learning. The chapter is about node embeddings.
Question: What is the point of using node embeddings?
Do we use them:

to save space/memory by mapping our graph into a form which is of lower dimension?
to find a representation of the graph which can be fed into a neural network?
perhaps some other reason?

It isn't clear to me what the actual purpose/end-goal is of finding a representation is.
Any help would be greatly appreciated as I have also been reading about graph neural networks, which aim to find an embedding for the nodes (but I don't understand why that is of any use to us).
","['geometric-deep-learning', 'graph-neural-networks', 'graphs', 'embeddings']",
What are examples of node 'features' in graph networks?,"
Context: I was reading Chapter 3 in the following book (here) about graph representation learning. Before I get to node embeddings, I wanted to make sure that I do understand what is meant by the phrase 'node features' used numerous times throughout the book. Examples are as follows:
Chapter 5, page 50:

Node Features: note that unlike the shallow embedding methods discussed in Part I of this book, the GNN framework requires that we
node features $\mathbf{x}_{u}$, $\forall u \in \mathcal{V}$ as input
to the model. In many graphs we will have rich node features to use
(e.g. gene expression features in biological networks or text features
in social networks)....

Question: What is a simple, concrete example of different node features? I have read the paragraph above, but I am not sure whether I have interpreted it correctly. For example, if we imagine a social network of some friends, would some example node features be: address, age, height, weight, etc.? Would it be as simple as that? What are some more advanced/subtle bits of information which could be counted as node features. Perhaps one could be 'number of friends' (i.e. the degree of the node), but what about others.
","['geometric-deep-learning', 'graph-neural-networks', 'graphs', 'features']","

For example, if we imagine a social network of some friends, would
some example node features be: address, age, height, weight, etc.?
Would it be as simple as that?

Yes, that is correct. The idea behind Graph Neural Networks (GNNs) is that existing node features are augmented by the local graph structure to produce more accurate results.


What are some more advanced/subtle bits of information which could be
counted as node features. Perhaps one could be 'number of friends'
(i.e. the degree of the node), but what about others.

Yes - structural / positional properties of the graph (such as degree distribution) can also be used. They are particularly useful for graphs without existing attributes to use as node features.
An example use case of this would be modelling molecules as a graph, and trying to classify each molecule by type.
The following papers compare and discuss various different graph properties that could be used for this, depending on task / graph:

https://doi.org/10.48550/arXiv.2107.01495
https://doi.org/10.48550/arXiv.1911.08795



What is a simple, concrete example of different node features

Generally, you want a feature that is correlated to the task at hand.
For example, user interests might be a good feature for predicting friendships between users in a social network, but less good for bot detection.
Same principle applies to positional features (for attributeless graphs) as well.
It might be helpful to look at the features given with common datasets:
https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html
"
How to handle random order of inputs and get same output?,"
I am a beginner with DL. I did some tutorials and I know the basics of TensorFlow. But I have a problem understanding how to construct more advanced NNs.
Let's say I have 6 inputs and a list of 500 names from which you can pick any, but only 6 at the time. The output should be one value between $0.0$ and $1.0$.
My question is, how I can handle random order in inputs? In inputs 1-6 you have names ABCDEF and the output score is 0.7. I need the same output if input will be in order CEDBFA. How can I handle this? Should I make random shuffle on inputs during training, or should I make for every output value 500D binary vector-like $[0,0,1,0,1,...,0,1,0,0,0]$, where index position in the array is the corresponding token of name and then feed it in 500 inputs? Or there is some better way?
","['neural-networks', 'tensorflow', 'keras', 'input-layer']","
Note: This is always going to be an estimate until you actually run the experiment. ML is not always predictable.
If order truly does not matter, then I think it will be better to design a network architecture that automatically ignores order, instead of using one that cares about order and then training it to ignore order. If nothing else, less training data will be needed since you don't need to train it on permutations of the input - similar to why CNNs are useful for image recognition.
One network architecture could be to have an ""input processing block"" (a group of layers) and an ""output processing block"". First apply the input processing block to each input. Then add together all the outputs of the input processing blocks. Because addition is insensitive to order, this step completely discards any information about the order. Finally, apply the output processing block to the sum of those and the output from that block is your final output.
           output
             ^
             |
           +--+
           |NN| (different NN to the one below)
           +--+
             ^
             |
+---------------------------+
| add all together          |
+---------------------------+
 ^    ^    ^    ^    ^    ^
 |    |    |    |    |    |
+--+ +--+ +--+ +--+ +--+ +--+
|NN| |NN| |NN| |NN| |NN| |NN| (same NN 6 times)
+--+ +--+ +--+ +--+ +--+ +--+
 ^    ^    ^    ^    ^    ^
 |    |    |    |    |    |
 C    E    D    B    F    A

This is just one idea. You are not limited to addition; you can use any commutative operation in the middle. Even something like an attention layer could be used.
"
What are Linear and Non-Linear Features of an image in the context of Convolutional Neural Network?,"
What features of image are linear or non-linear, any example ?
","['deep-learning', 'recurrent-neural-networks', 'feature-selection', 'relu']",
"In reinforcement learning, why are policies defined as functions of states and not observations?","
I am new to RL and I am following Sutton & Barto's book.
My doubt is, when we talk about the policy of our agent, we say it is the probability of taking some action $a$ given the state $s$. However, I think that the policy should be defined in terms of observations and not states because I think it is not always possible for an agent to fully capture the state due to various reasons, maybe lack of sensors or lack of memory.
So, why are policies defined as functions of states and not observations?
","['reinforcement-learning', 'definitions', 'markov-decision-process', 'policies']",
"In anchor based object detection, why don't the anchors share the same weights?","
After reading about YOLO V3 and Faster R-CNN, I don't understand why the weights for the regression head aren't the same across all boxes of the same size. Given that the backbone of these systems is fully convolutional, the location of the outputted features should only depend upon the local region of the image which telescops to that feature map. Given that we want the object detector to behave the same way regardless of the object location in the image, shouldn't the weights be the same across anchors of the same size?
","['computer-vision', 'object-detection', 'object-recognition', 'yolo', 'faster-r-cnn']",
Should I repeat lengthy deep learning experiments to average results ? How to decide how many times to repeat?,"
I am doing my MSc thesis on deep learning. My model takes many hours to train. Part of what I do is trying different parameters and settings hoping that they will achieve different results. But I often notice that the result differences are too small to conclude whether the set of parameters A is better than B. Sometimes it happens that on a first run, set A seems to work better than B, but on a second run the opposite is suggested.
The logical approach for that would be to repeat experiments and average out. But it seems unpractical given that each run could take so many hours.
I wonder how expert AI researchers deal with that, do they perform multiple experiments, even if this takes extremely long? Do they draw conclusions from single runs?
","['deep-learning', 'ai-design', 'research']","

I wonder how expert AI researchers deal with that, do they perform multiple experiments, even if this takes extremely long? Do they draw conclusions from single runs?

Unfortunately, the question you ask in the main body of your question here (""how do expert AI researchers do things"") often turns out to actually be different from the question in your title (""how should I do things"").
The very short answer would be that, ideally, you run as many repetitions as you can, but in practice it is indeed very often not feasible to do more than one or a handful.

For the long answer, I think it actually may be quite different also depending on what kind of machine learning you are doing. Personally, I am much more familiar with Reinforcement Learning and similar kinds of problems, i.e. problems where we're generating the ""training"" data ourselves by making agents act in environments, and similarly also evaluating by again making trained agents act in environments and measuring their performance. I'm not as familiar with the state-of-the-art research in ""standard"" machine learning tasks like classification/regression, but probably the problem is much worse in RL-style problems because:

We actually have to generate our own data, which takes a huge amount of time (sometimes much more time than the actual training itself takes)
There is often a lot of randomness in how we generate our training data, so we often have very different training datasets in different runs, which can of course also lead to wildly different levels of performance across different runs
We pretty much always compute gradients from subsets (batches) of data in RL, which is again a source of randomness. In contrast, in supervised ML you could consider estimating gradients from the entire dataset at once per epoch, and then at least that part of your training process becomes deterministic (and hence replicable).

For the case of RL, recently a paper titled ""Deep Reinforcement Learning at the Edge of the Statistical Precipice"" appeared on arXiv, which gets into various tools and approaches you can use to more reasonably draw principled conclusions even from a small handful of repetitions of your training process (in better ways than the common practice of just reporting a mean, or even worse, just reporting the best result).
For supervised ML, some similar techniques may be applicable, but the need for this may also be less great (especially if you have little randomness in your training process).


Part of what I do is trying different parameters and settings hoping that they will achieve different results. But I often notice that the result differences are too small to conclude whether set of parameters A is better than B. Sometimes it happens that on a first run, set A seems to work better than B, but on a second run the opposite is suggested.

For this specific situation, I would be interested to know by how much A is sometimes better than B, and B is sometimes better than A. If we're talking about like 0.1% differences in accuracy either way at a baseline accuracy of 99.5%, that probably just means they're both equally good. If you were ""hoping"" for one of them to outperform the other, well then that's probably disappointing... but for the overall training process in general, you could actually draw a more positive conclusion that, apparently, it is robust; it performs similarly across different runs even with different parameters!
On the other hand, if you observe differences in performance of significant magnitudes (like, 80% vs 90% accuracy), but sometimes A better by that much and sometimes B better by that much... then this strongly suggests that you indeed are dealing with high variance and you're going to have to do many repetitions to get statistically meaningful results.
"
"How to find ""relationships"" between two data representations?","
I am a researcher in a field, and new to the whole of AI and machine learning techniques.  May the following question is trivial or not framed in the ML language but I try my best.
I have two sets of representations (I can extract feature vectors, etc., from the datasets) from vastly different domains. I want to find, if any, a relationship exists between these two sets.  In other words, I want an algorithm (the idea of an algorithm) to learn both representations and find the connections and convert one representation to another.
There is neither apparent one-to-one correspondence nor both need to be the same lengths.
Any suggestion on how to approach this problem is appreciated.
I thought of one method; write an encoder-decoder for each of these presentations separately and swap the decoders.  I am not sure whether it works or not, and besides I may not have any idea what's going on there.
I prefer a general approach if it exists.
","['machine-learning', 'features', 'algorithm-request']",
Is it possible to apply a particular exploration policy for the on-policy RL agents?,"
Is it possible to use any particular strategy to explore (e.g. metaheuristics) in on-policy algorithms (e.g. in PPO) or is it only possible to define particular policies to explore in off-policy algorithms (e.g. TD3)?
","['reinforcement-learning', 'deep-rl', 'on-policy-methods', 'exploration-strategies']",
What problem does the neural network really solve?,"
In the image below taken from a Youtube video, the author explains that the neural network can be used to fit a relational graph for a set of data points shown by the green line. And that this is accomplished by using weights, biases and activation functions.
My slight confusion is that, initially, the weights and biases and randomized, and they are re-adjusted by backpropagation. This means that, at the end of the output layer, we must have the actual values of the target function anyway.
So what problem does the neural network really solve?
So, for example, we want to find the target function for dosage and efficacy, we are given the data points shown in blue. If we initially choose randomized values for the weights, biases and activation function, then, at the output layer, we determine an output value for efficacy, but there is no way to know whether this value is in fact correct or not. So, we need the actual values to determine the difference.
What about when we choose a value of dosage which has not been observed, for example, 0.25? Doesn't this rely upon a best-fit relation graph that has already been fitted to the data prior to adjusting the neural network?

","['neural-networks', 'training', 'supervised-learning', 'labeled-datasets']",
Is Speech to Speech with changing the voice to a given other voice possible?,"
Background:
I am working on a research project to use (demonstrate) the possibilities of Machine Learning and AI in artistic projects. One thing we are exploring is demonstrating deep fakes on stage. Of course, a deep fake is not easy to make. Still, we are exploring creating a ""minor quality"" deep fake live on a stage (or maybe in some other ways where people can make deep fakes of themselves) in which we put words into someone's mouth. I discovered that a semi-nice deep fake of the facial movement is possible, now a also want to add voice.
There are a lot of text-to-speech systems, which allow using a voice that is created from the recording of the voice of a real person. That is already nice.
The video is based on the facial movements of another person. So the Audio has to match the facial movements. The easiest way would be, if the ""fake"" voice says it exactly the same way, as the person doing the ""facial acting"" for the deep fake said it.
Question:
Is it possible to do a fake voice of a person in this way:

Another person (the actor, or source) speaks the words in his voice and records it.
The person with the faked voice (the destination) gives a voice sample with some random spoken text.
An AI/algorithm/whatever modifies the recording from (1) in such a way, that the tone/voice is modified so it matches the voice from (2).

Do systems/research like this exist? I did not find anything using google, but maybe I did not use the correct keywords.
","['natural-language-processing', 'reference-request', 'deepfakes', 'speech-synthesis']",
NLP problem Phrase/Token labeling,"
Looking for suggestions on how to define the following NLP problem and different ways in which it can be modeled to leverage machine learning. I believe there are multiple ways to model this problem.  Deep-learning-based suggestions also work as there is a good amount of data is available for training.
Will evaluate different approaches for the given dataset. Please share relevant papers, blogs, or GitHub repos. Thanks!
Input: Given a sentence S having words W1 to W10.
S = W1 W2 W3 W4 W5 W6 W7 W8 W9 W10
The sentence has some syntactic and semantic patterns, but it is not exactly freely written natural language but it's in English. These are words, can be punctuation
Output: should be something like this.
Label1 - W4
Label2 - W3
Label3 - [W2 W1] continuous // semantically related. Means words [W2 W1] in-order are assigned a Label3. Also okay with solutions that don't output in-order.
Label4 - [W6 W8]
Label5- W10
Noise- W7, W9. Means words W7 and W9 independently are assigned a
Label3.
Label7- W5
Need to solve the problem. Looking for research/thoughts on how this problem can be defined in different ways to exploit different patterns in the structure of sentences. Looking for similar tasks which are already defined in NLP such as token labeling, parsing which can be used.
Would be really helpful to get the suggestions to the latest research on solving/defining this problem.
","['neural-networks', 'machine-learning', 'natural-language-processing', 'recurrent-neural-networks']","
I think you can solve this problem with models trained for Named Entity Recognition. In that case, your entities are labels. To do this you can use spacy to train a NER model or more easily you can fine-tune a Distil-BERT for your task.
"
Why actual mapping is called as unreferenced mapping in this context of residual framework?,"
Consider the following statements from the research paper titled Deep Residual Learning for Image Recognition by Kaiming He et al.
#1:

We explicitly reformulate the layers as learning residual functions
with reference to the layer inputs, instead of learning unreferenced
functions. We provide comprehensive empirical evidence showing that
these residual networks are easier to optimize, and can gain accuracy
from considerably increased depth.

#2:

In this paper, we address the degradation problem by introducing a
deep residual learning framework. Instead of hoping each few stacked
layers directly fit a desired underlying mapping, we explicitly let
these layers fit a residual mapping. Formally, denoting the desired
underlying mapping as $\mathcal{H}(x)$, we let the stacked nonlinear
layers fit another mapping of $\mathcal{F}(x) := \mathcal{H}(x)−x$.
The original mapping is recast into $\mathcal{F}(x)+x$. We hypothesize
that it is easier to optimize the residual mapping than to optimize
the original, unreferenced mapping. To the extreme, if an identity
mapping were optimal, it would be easier to push the residual to zero
than to fit an identity mapping by a stack of nonlinear layers.

The research paper contains the word ""unreferenced"" twice for a function, which I think is the actual function $\mathcal{H}(x)$. In normal deep neural networks, the function $\mathcal{H}(x)$ is generally learned. But, $\mathcal{F}(x)$ is learned in the residual neural networks and hence $\mathcal{H}(x)$.
Why this paper is calling the original function $\mathcal{H}(x)$as unreferenced function?
","['terminology', 'papers', 'residual-networks']",
When should we use CNN instead of MLP?,"
Is CNN only applicable to time-series data or image data?
When should we use CNN instead of MLP?
","['convolutional-neural-networks', 'comparison', 'applications', 'multilayer-perceptrons']",
Is it possible to apply 2D convolution to 1D data?,"
Suppose that I have a 1D dataset with 6 features.
Can I apply a 2D convolutional neural net to this dataset?
","['convolutional-neural-networks', 'convolution']","
You can certainly reshape the data to make it fit a 2D network. You could set the width or height to 1 as suggested by DamirTenishev, but you could also set the features/channels to 1 and treat the features as a height, if you wanted to convolve that way (would be a bit strange).
"
What deep reinforcement learning algorithm should I use for my problem?,"
So here is a description of my problem:
Essentially, I have a large amount of files filled with code for a number of different tasks. However, lets say these codes are inefficient, and should be edited to be made more efficient. There exists a program to edit this code, and essentially what is does is analyze the code to look for what possible edits can be made, and classifies these edits into 5 classes (Class A, Class B, Class C, Class D, Class E). There is also a program to test the efficiency of this code and yields numeric value quantifying the efficiency. However, choosing some edits benefit the efficiency of a some code better than other edits. For example, a piece of code that has had 5 different edits applied to it (2 of Class A, and 3 of Class C) can be more efficient than the same code that has had 10 edits applied to it (5 of Class A, 5 of Class D). Ideally, the less edits and the higher the efficiency the better.
I want to use deep reinforcement learning to  address this problem.
Here are the goals of my model:

Takes the code in a form which it can understand (I have looked into this and stumbled upon code2vec, so I am looking into that)
Also can take in the edits that can be applied to the code (which is the action space) and their respective classes.
Makes decisions for edits which yield the highest efficiency for the code

I want to train this model on the files filled with the inefficient code. The ultimate goal is to end up with an AI that takes code as an input and makes the best decision of which edits it should apply to the code.
What deep reinforcement learning techniques/algorithms/architecures should I use to approach this problem and how can I go about implementing these in Python? As you can probably tell from this question, I am not very experienced in deep reinforcement learning, but I am willing to learn. Any help would be appreciated!
","['neural-networks', 'reinforcement-learning', 'deep-rl']",
Where can I read about upsampling methods in detail?,"
In deep learning, we encounter the upsample blocks several times, especially when we deal with images.
Consider the following statements from description regarding UPSAMPLE in PyTorch

The algorithms available for upsampling are nearest neighbor and
linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input
Tensor, respectively.

Where can I read about these upsampling techniques in detail, especially in the context of deep learning?
","['deep-learning', 'reference-request', 'upsampling']","
Tricky question. In my experience is better to just look for math resources on classic upsampling method, since deep learning papers and books tend to give them for granted, or not something related to AI (they are after all analytic methods). Another reason is probably that the math is not that hard, and already the wikipedia pages offer a good description of the basic methods
(nearest-neighbours, bilinear interpolation, bicubic interpolation).
For some others interesting and more advance techniques I found useful this paper: Mathematical Techniques for Image Interpolation
even though it's not exhaustive since it doesn't mention well known alternatives to bilinear or bicubic upsampling like Lanczos, so for completeness I would read also A Study of Image Upsampling and Downsampling Filters

Regarding the deep learning part, it's more valuable to search for papers that studies not the upsampling methods themselves, but the artifact they introduce, especially related to tasks like super-resolution. As a starting point I would dig into this blogpost and its references.
"
Why using negative integers (as dimensions?) in tensor shapes rather than natural numbers?,"
Consider the following paragraph from A.1 MULTI-MNIST AND CLEVR of A IMPLEMENTATION DETAILS from the research paper titled GENERATING MULTIPLE OBJECTS AT SPATIALLY DISTINCT LOCATIONS by Tobias Hinz et al.

In the global pathway of the generator we first obtain the layout
encoding. For this we create a tensor of shape (10, 16, 16) (CLEVR:
(13, 16, 16)) that contains the one-hot labels at the location of the
bounding boxes and is zero everywhere else. We then apply three
convolutional layers, each followed by batch normalization and a leaky
ReLU activation. We reshape the output to shape (1, 64) and
concatenate it with the noise tensor of shape (1, 100) (sampled from a
random normal distribution) to form a tensor of shape (1, 164). This
tensor is then fed into a dense layer, followed by batch normalization
and a ReLU activation and the output is reshaped to (−1, 4, 4). We
then apply two upsampling blocks to obtain a tensor of shape (−1, 16,
16).

The paragraph is saying that a tensor of shape (1, 164) is reshaped to (-1, 4, 4). What is  the reason behind using negative number -1? Is it representing axis? Can't we represent it with $a \times x \times y$, where $a, x, y$  are natural number s and dimensions of the tensor?
$\dfrac{164}{4 \times 4}$ is not a natural number, so what is the shape of the reshaped tensor using only the natural numbers?
","['implementation', 'notation', 'tensor']","
It definitely is. If you check the code (line 145) you'll see that in the forward definition of the Stage1 Generator they do:
class STAGE1_G(nn.Module):
    def __init__(self):
...
    def forward(self, text_embedding, noise):
    c_code, mu, logvar = self.ca_net(text_embedding)
    z_c_code = torch.cat((noise, c_code), 1)
    h_code = self.fc(z_c_code)

    h_code = h_code.view(-1, self.gf_dim, 4, 4)

Where self.gf_dim is a parameter defined in the configuration file of the gan (most likely number of feature maps, but check cause they didn't write documentation about the config settings)
"
Discard irrelavant states from a MDP,"
I came across this question about MDP.
From the look of it, it seems the full MDP is reducible if the discarded state only have 1 way in and out but is it really so if we change the discounted factor? I think there is some tricky part regarding this problem...

","['reinforcement-learning', 'markov-decision-process', 'value-functions']","
In terms of meaningful decisions that an agent might make, then states $S_1$ and $S_2$ do seem redundant.
However, if you remove those states and the actions from them, you also remove the time step tracking that goes along with them, and this fundamentally affects the MDP.
The question gives some hints about what will be affected at the end to help you answer it. This is a non-episodic (continuing) MDP without a terminal state, so you cannot use discount factor $\gamma = 1$, because the expected return will be infinite. Any valid approach for value functions in continuing MDPs - using a discount factor, finite horizon or average reward - will give you different results in the reduced MDP due to the difference in counting time steps.
"
How to model the inputs and outputs of the neural network for the Splinterlands card game?,"
I have recently just completed a course on deep learning and I feel like an intermediate, but I still don't know how to structure this problem.
I'm looking to create a NN to play the card game Splinterlands.
I have the history of battles, the cards played, who won, and the battle rules and constraints.
I'm struggling with how best to model the inputs. I think that standard feature engineering and the encoding of all the variables affect the game, like manor, cards chosen, rules, etc.
How best to constrain the outputs? For example, the model needs to select cards I have available, etc., while the history contains cards I don't.
I know it's a long shot, but just looking for some inspiration :)
The basic rules of splinterlands are as follows:

Start battle (you are given mana and set of rules, like no
shields etc)
Then select a summon (some rules say it has to be
of certain, you also want to select your highest level summoner)
Then select 6 monsters ( each monster has a position and
stats like attack, speed, health and mana cost)
Press battle

","['neural-networks', 'card-games']",
Do authors generally use fully connected layer instead of affine transformation?,"
We generally encounter the following statement several times

The input vector is first fed into a fully connected layer......

Since linear activation functions, such as identity function, can so considered as an activation functions, a fully connected layer can be considered just as an Affine transformation if the fully connected layer uses linear activation function.
So, in theory, a fully connected layer can refer to the following

Just an affine transformation
Affine transformation followed by a nonlinear activation function

Do authors generally choose to use ""fully connected layer"" for case 2 only or for both cases 1 and 2?
","['neural-networks', 'terminology']",
Why identity function is generally treated as an activation function?,"
It is known that the primary purpose of activation functions, used in neural networks, is to introduce non-linearity.
Then how can the linear activation function, especially the identity function, be treated as an activation function?
Are there any special applications/advantages in using an identity function as I cannot see any such use theoretically?
","['neural-networks', 'activation-functions']","
The identity function can be useful in some cases. For example, if you are doing regression, the output of your neural network needs to be a real (or floating-point) number, so you use the identity function. (If you were doing logistic regression or classification, that wouldn't probably be the case). The identity function is also used in the residual networks (see figure 1). There are probably other examples of its usage and usefulness.
Some people may not consider the identity an activation function because it does nothing to the input. However, whether you consider the identity an activation function or not is a matter of convention, in the same way that considering a model with only identity (aka linear) functions a real neural network or a perceptron a neural network is a matter of convention.
I don't think there's still a consensus on this subject. In fact, you will often hear (or see) people say (or write) that they use ""no activation (function)"" or ""linear activation"" rather than saying that they use the ""identity"" (for example, see the documentation for the parameter activation here).
"
Using parameter estimation for training a neural network,"
Assume that we have 4 layers in a neural network.
$$z_1 = L_1(x, W_1)$$
$$z_2 = L_2(z_1, W_2)$$
$$z_3 = L_3(z_2, W_3)$$
$$y = L_1(z_3, W_4)$$
Where $x$ is the vector input, $y$ is the vector output and $W_i, i = 1..4$ is the weight matrix.
Assume that I could estimate parameters in a function.
$$b = f(a, w)$$
Where the $b$ is a real value and $a$ is the input vector and $w$ is the weight vector parameter. The function $f$ could be like this.
$$b = \text{activation}(a_1*w_1 + a_2*w_2 + a_3*w_3 + \dots + a_n*w_n)$$
Here we can interpret $b$ as the neuron output. Estimate $w_n$ is very easy if we know $b$ and $a_n$. This can be done used by recursive least squares or a Kalman filter.
Question:
If every neuron in a neural network is a function that has inputs and weights, can I use parameter estimation for estimating all weights in a neural network if I did parameter estimation for every neuron inside a neural network?
The reason why I'm asking:
I found a paper where they are using a Unscented Kalman Filter for parameter estimation.
Function $D_{k|k-1} = G[x_k, W_{k|k-1}]$ can be interpreted as a neuron function where $W_{k|k-1}$ is a matrix with different types of weights and $D_{k|k-1}$ is different types of outputs from that neuron. No, it's not a ""multivariable output""-neuron. It's just the way how to estimate the best weights by using different weights.
The error of the neuron output is: $d_k - \hat d_k$ in equation (41).
So when the error is small, that means the output of the neuron is OK and that means the real weights $\hat w_k$ has been found.

","['neural-networks', 'weights', 'kalman-filter']",
Continue teaching pre-trained network without forgetting previous data set,"
I have a rather interesting problem here; I work in the field of image classification for quality assurance.
For this I have a dataset of about 1 million images, which I have used to train different defect classes.
Now one of these defect types has additional properties (new features of an image class), I would like to teach these new features to the previously trained network, and without re-training the whole previous dataset.
In short: new features of an image class should be taught without affecting the performance of the network on the previous training set too much. Is this possible, and if so, what are some strategies for doing this?
Thanks in advance!
","['deep-learning', 'convolutional-neural-networks', 'transfer-learning']",
"What is the derivative of equation 1 in the paper ""Conservative Q-Learning for Offline Reinforcement Learning""?","
I am looking at the paper Conservative Q-Learning for Offline Reinforcement Learning, but I'm not sure how they proved theorem 3.1.
Here is a screenshot of theorem 3.1.

In the proof of theorem 3.1

they say

By setting the derivative of Equation 1 to 0, we obtain the following expression
...
$$\forall \mathbf{s}, \mathbf{a} \in \mathcal{D}, k, \quad \hat{Q}^{k+1}(\mathbf{s}, \mathbf{a})=\hat{\mathcal{B}}^{\pi} \hat{Q}^{k}(\mathbf{s}, \mathbf{a})-\alpha \frac{\mu(\mathbf{a} \mid \mathbf{s})}{\hat{\pi}_{\beta}(\mathbf{a} \mid \mathbf{s})} \tag{11}\label{11}$$

Here's equation 1 from the paper.
$$\hat{Q}^{k+1} \leftarrow \arg \min _{Q} \alpha \mathbb{E}_{\mathbf{s} \sim \mathcal{D}, \mathbf{a} \sim \mu(\mathbf{a} \mid \mathbf{s})}[Q(\mathbf{s}, \mathbf{a})]+\frac{1}{2} \mathbb{E}_{\mathbf{s}, \mathbf{a} \sim \mathcal{D}}\left[\left(Q(\mathbf{s}, \mathbf{a})-\hat{\mathcal{B}}^{\pi} \hat{Q}^{k}(\mathbf{s}, \mathbf{a})\right)^{2}\right] \tag{1}\label{1}$$
My question is: what exactly is the derivative of equation (1)? And how does that result in equation (11)?
The $\hat{B}^\pi$ is the empirical Bellman operator and is defined as $\hat{B}^\pi \hat{Q}^k (s,a) = r + \gamma \sum_{s'} \hat{T}(s' \mid s,a) \mathbb{E}_{a'\sim \pi(a' \mid s')}\hat{Q}_k(s', a')$. Since in offline reinforcement learning, the dataset $\mathcal{D}$ typically does not contain all possible transitions $(s, a, s')$, the policy evaluation step actually uses an empirical Bellman operator that only backs up a single sample.
","['reinforcement-learning', 'q-learning', 'papers', 'proofs']","
first we need to transform the distribution of the first term:
$\mathrm{argmin} \space \alpha \mathrm{E}_{s\sim D, a \sim D}[\frac{\mu}{\hat\pi_{\beta}}Q(s,a)] + \frac{1}{2}\mathrm{E}_{s,a \sim D}(Q(s,a)-\hat\beta^{\pi}\hat Q^k(s,a))^2   $
then its derivate respect to Q is:
$\alpha\frac{\mu}{\hat \pi ^{\beta}} + Q(s,a)-\hat\beta^{\pi}\hat Q^k(s,a) = 0$
then we have
$ Q(s,a) =\hat\beta^{\pi}\hat Q^k(s,a) -\alpha\frac{\mu}{\hat \pi ^{\beta}}$
"
"What does it mean by ""low-level"" and ""high-level"" in features generated by CNN?","
Across the literature, the terms ""high-level"" and ""low-level"" are generally used as an adjective to the features generated by the convolution neural network as intermediate representations.
Should I understand the level to be either high or low based on

the position of feature maps considering the architecture of the convolutional neural network i.e., the size of the feature maps generated  at different layers of the convolutional neural network

(For example, the feature maps after bottleneck layers are ""high-level"" and are after the wide layers are ""low-level"" )
or

the content of the feature maps based on the task

(For example, the feature maps that learned eyebrows of a cat are ""high-level"" and are just containing pixel intensities are ""low-level"" )
or

any other property?

","['convolutional-neural-networks', 'terminology', 'features']",
What's the benefit for using a Kalman filter for training a neural network compared to other optimization algorithms?,"
I found a paper about using an Unscented Kalman Filter(UKF) for traning a neural network.
The UKF filter is modified so it works for parameter estimation. Assume that we have a neural network model $\hat d_k = G(x_k, W_k)$ where $G$ is the neural network, $x_k$ is the input vector, $W_k$ is the parameter gain matrix and $\hat d_k$ is the output vector.
This paper counts servral methods to train a neural network.

Gradient descent
Quasi-Newton
UKF parameter estimation

So my questions for you are:

What's the benefit for using a kalman filter for training a neural network compared to other optimization algorithms?

I remember that I have been using gradient descent methods and they work, but I have always used regularization. Is that due to the noise in the data?

If I'm using a UKF-filter as parameter estimation, I can avoid regularization then?

Is this UKF parameter estimation algorithm only made for 1 layer neural network, or can it be used for deep neural networks as well?


","['neural-networks', 'training', 'kalman-filter']",
"Why do the authors of the T5 paper say that the ""architectural changes are orthogonal to the experimental factors""?","
Here's a quote from the T5 paper (T5 stands for ""Text-to-Text Transfer Transformer"") titled Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel et al.:

To summarize, our model is roughly equivalent to the original
Transformer proposed by Vaswani et al. (2017) with the exception of
removing the Layer Norm bias, placing the layer normalization outside
the residual path, and using a different position embedding scheme.
Since these architectural changes are orthogonal to the experimental
factors we consider in our empirical survey of transfer learning, we
leave the ablation of their impact for future work.

What exactly does 'orthogonal' mean in this context? Also, is it just me or have I seen the word used in a similar way before, but can't remember where?
","['machine-learning', 'natural-language-processing', 'papers', 'transformer']","
Looking at the paper, it seems to me that they are not using orthogonal in a literal, mathematics (or geometric) sense.  Instead, I read that as two things (especially since the word ""ablation"" appears later in the sentence):

They are attempting to use lots of fancy words
They are simply indicating that these changes are separate from and have no impact on (at least they claim this to be so) the ""experimental factors [they] consider in [their] empirical survey...""

"
Machine learning with raw data alone / or raw data with its statistics,"
My question is very general and it does not originate from a specific problem. Let's assume that, through experience, we have learned that some statistical property of a set of data is important in predicting some behavior in a system. For example, for time series data d1, d2, d3, ..., dn, we heuristically know that the average of the last n steps denoted by avg(d,n) and the standard deviation of the last m denoted by std(d,m) are significant in prediction. Now my questions are:

Should a machine learning system, let's say LSTM, or reinforcement learning agent, be fed the raw data or data with other statistical properties? I am asking this because, if the statistical derivatives are useful in training then there is no limit on how many statistical properties we can define and feed to the training process.

Do machine learning, again let's say LSTM, automatically learn about underlying statistics from just pure raw data?

How do we deal with different data of different scales and dimensions, for example, simple average is in the same scale as the raw data but standard deviation is of different scale and dimension and so on so forth?


I appreciate your comments.
","['neural-networks', 'machine-learning', 'deep-learning', 'long-short-term-memory', 'statistical-ai']",
Identify whether two companies are the same,"
I am trying to solve a problem where I need to map multiple variations of a company name to a single name. For example: say I have a company named Super Idea Corporation Limited.
I need to resolve the following to Super Idea Corporation Limited

SICL
Super Idea Corp Ltd
SIC Ltd
SIC Limited

Is there a non regex way of doing this? The reason I am averse to using regex is that there are a lot of business names that can be represented in many different ways. I want something that is more flexible and adaptive.
","['natural-language-processing', 'python']","
Have a look at Named Entity Recognition (NER); these algorithms are mainly concerned with recognising that there is an entity, but often also include normalising the name to a canonical form for information retrieval -- this is what you would need.
In a previous job I actually implemented this, using a fuzzy match with variable word order. You would still need to map Corp and Corporation onto each other as exchangable, and deal with acronyms, but that should be tractable.
"
How to fine-tune a model which was pre-trained on a corpus that contains words with different meanings than the meanings of those words on my corpus?,"
I have a scenario in which we should leverage previously asked questions (not questions pairs, single question in a column) to locate similar questions within those questions.
How can I fine-tune my model to manage out of vocabulary, as my data includes domain-specific questions (3300 questions)?.
Right now, I'm using hugging face sentence transformers, which is already pre-trained on huge data.
For example, BERT knows that gold is a metal, but, in our domain corpus, it's a platform. We have some terminologies which were not exposed openly, how can I fine-tune the model to get related sentences (handling OOV).
","['bert', 'fine-tuning']",
How do I prepare this 3D data for NN?,"
How do I prepare the info of 3D models to use with NN? For example, I have thousands of models with boxes similar to the ones in the image below. I can extract the vertices and their normals that make up the faces of these boxes. Similarly, I would like to prepare the info of the red-shaded surfaces, again I have their vertices and their normals. For future studies, I will have more complex shapes such as cylinders, pyramids,...etc. What would be the best way to represent these complex shapes for NN?
Update: These boxes don't stay in the same position, see the second image I added. I will have different geometric models and different red-shaded areas on the surfaces of these objects. The NN output would be a number for each surface of these boxes/objects. The number represents the surface temperature. The input would be the following:
1- Some climate information such as (air temperature, humidity, ...etc.)
3- The location and size of the buildings that are represented in boxes/or maybe other shapes.
4- The size and the location of the red-shaded areas (red-shaded areas represent the shadow cast by buildings.
5- Material of each surface (concrete, brick,...etc).


","['neural-networks', 'machine-learning', 'vectors']","
I think, that the answer depends on the application, but a possible choice would be store it as a mesh - a list of vertices $V$ and edges $E$. Instead of edges, one can work with polygons, and define connectivity $F$ - for triplets of vertices $(v_i, v_j, v_k)$.
There is a nice paper on Mesh CNN that can handle various geometric object.
For the special case of boxes maybe there is a more educated approach, but since you would like to handle later more complex shapes, I would suggest to work with this architecture from the start.
"
"What are the different possible usages of the word ""i.i.d"" in machine learning?","
The acronym ""iid"" stands for ""independent and identically distributed"". It is a property of a sequence of random variables. You can read here for more details. This question is just about the usage of the word ""iid"" in contemporary machine learning and is not about the feasibility of checking iid based on either associated joint distribution or dataset.
In the formal and strict sense, the word ""iid"" should be used only as a property for a sequence of random variables based on the underlying joint probability distribution function. But, I noticed that there is another (maybe less-strict) usage for the word 'iid' based on the context.
Consider the following statements compiled from different answers to my questions 1,2
From this answer

The term i.i.d. is a property of a dataset. A dataset can be created
that is i.i.d. with respect to a particular probability distribution.
It doesn't matter what that distribution is, it just has to exist, and
be relevant to the purpose the ML is being put to.

From this answer

The point is even you know the distribution, sometimes you can't prove
that the sampled data is i.i.d. or not!....

From this answer

....A table of results of dice throws is likely iid...... (there are some issues with this answer, but the bolded excerpt is true)

So, the usage of the word iid, in this sense, is somewhat different. Although I think, iid is a property of a sequence of random variables in this sense also, it is okay to use the word 'iid' for a dataset (collection of samples) since the dataset represents some underlying probability distribution.
Thus, the two usages I am aware of up to now are

iid for a sequence of random variables based on joint distribution.

iid for a sequence of random variables based on the collection of samples.


Is my understanding of the two usages of the word ""iid"" correct? and are there any other usages for the word ""iid""?
","['machine-learning', 'terminology', 'datasets', 'random-variable', 'iid']",
Discrepancy of backpropagation formula between Andrew Ngs ML Course and those derived by neuralnetworksanddeeplearning.com,"
I'm currently working through Week 5 of Andrew Ngs Machine Learning course on Coursera, which goes through the backprop algorithm for basic neural networks. Whilst trying to derive the formulae he gave in the lectures, I noticed that the formula for $\delta^L$, ""error"" of last activation layer, is slightly different to that derived in http://neuralnetworksanddeeplearning.com/chap2.html.
In Andrew's, it seems like there is no inclusion of the partial derivative da/dz, or $\sigma'(z)$, only the dC/da part.

However Michael Nielson does include that term:

Is this difference significant and why does it arise? Is it because the derivation Nielson goes through defines the Cost using the mean square errors, whereas Andrew Ng defines the cost using the -ylog(h(x))... one? Also will Nielson's equations score full marks on the Ng's assignment?
Thank you for reading.
","['neural-networks', 'machine-learning', 'backpropagation']","
It's a matter of different loss functions used. In Andrew Ng's C1W2L09 video he derives the loss gradient as
"
"What is better to use: early stopping, model checkpoint or both?","
I want to get a model which works best, what should I go for while training the model, ModelCheckpoint, EarlyStopping, or both?
","['deep-learning', 'tensorflow', 'early-stopping']","

Early stopping: stop the training when a condition is met
Checkpoint : frequently save the model

The purpose of Early Stopping is to avoid overfitting by stopping the model before it happens using a defined condition. If you use it, and then you save the model when the training is stopped*, you will get a model that is assumed to be good enough and not overfitted.
The purpose of the class ModelCheckpoint is to save models several times while training. This can be useful to find at which epoch the model gets the best performance. So, if you use it, you will get several models that are saved at different epochs (or, more generally, ""checkpoints"").
Even after using both methods, you will get some models, but they have a different purpose. None of them is better than the other. I almost always use both methods at the same time. You can use early stopping to stop the training and save a lot of models while training using ModelCheckpoint. In most of my cases, the best model is around the epoch during early stopping.
*note: the model saving process is not done by EarlyStopping
"
Is LSTM a subcategory of RNN?,"
Is the LSTM-Architecture a subcategory of RNNs? Or are they totally different?
Literature doesn't seem to be unitary on this.
This figure appears to explain the models to be alternatives, but I thought of them otherwise (LSTM to be a subcategory of RNN)
LSTM as a subcategory of RNN is mentioned in the Wikipedia article on LSTMs:

Long short-term memory (LSTM) is an artificial recurrent neural
network (RNN) architecture...

","['terminology', 'recurrent-neural-networks', 'long-short-term-memory']",
How to build neural network that detects changed signal firing pattern and is trained on positive patterns only?,"
Let's have a set of n devices firing signals. Devices are firing in the same cycles, but each device can fire in different phase of the cycle. More, the exact firing point can fluctuate, for example 20%
off the phase to both sides.

For me, this repeated firings can be seen like some kind of time-based pattern.
The main goal is to detect if some device changed it's phase, so the timings in the pattern are changed.
I generated dataset with rows for each second. On each row there is actual time and  another n-1 columns representing difference between firing time of this device and first device.
Example: let's have 4 devices - D0 to D3. t1 means difference of firing times between D1 and D0.
time     t1 t2 t3
13:00:00  5  2  7

That means that D1 fired 5s after D0, D2 fired 5+2s after D0 and D3 fired 5+2+7s after D0.
Also there can be negative numbers since the allowed fluctuation - the D2 could fired earlier than D0.
To amateur me this seems that this numbers create patterns that some kind of NN should be able to learn and filter out the noise.
My questions:

which kind of network should I use? I tried to do simple feed forward network and do a classification, but since I am generating the dataset, I have no negative data and I so I am unable to categorize it.
is this kind of dataset a good approach, or should I use other properties?

","['neural-networks', 'pattern-recognition']",
"How to increase accuracy of image orientation classification (Left, Right, Center)?","
I am working on classifying images in ""Left"", ""Right"", ""Center"", ""Back"". Training and Validation images look like this:



The images are ""Left"", ""Right"", and ""Center"". I am following Pytorch transfer learning tutorial with Resnet50 architecture and have not changed anything.
The transformations I am using is as follows.
data_transforms = {""train""  : A.Compose([
                           A.Resize(256,256),
                          #  A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),
                          #  A.RandomCrop(height=128, width=128),
                           A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),
                           A.RandomBrightnessContrast(p=0.5),
                           A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
                           ToTensorV2(),]),
                  ""val"": A.Compose([
                                    A.Resize(256,256),
                                    # A.CenterCrop(height=128, width=128),
                                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
                                    ToTensorV2(),])
                }

Initially I had back images too, but they were getting miss-classified with center a lot with val accuracy 65%.
Without back images I got 74% accuracy with labels left, right and center.
Modified it to classify only center and not center, with only three type of images ""left"", ""right"" and ""center"" I have achieved 91% of val accuracy.
I am looking for ways to increase accuracy for classifying images in left, right, center.
","['deep-learning', 'image-recognition', 'pytorch']",
What does it mean when accuracy of regularized model is higher for training set than for validation set?,"
Accuracy of my regularized model is higher for training set than for validation set.

The situation improves when regularization coeefficient is reduced:

What does this really imply?
From my understanding, this seems to suggest that regularization is actually resulting in the model overfitting training set, which is the opposite of the intended outcome
","['neural-networks', 'regularization']","
It implies that your regularization effects are too much, and prevent the model from learning from data. Also, at such a low accuracy (~10%), we can't really talk about overfitting.
"
What are practical methods to acquire a large number of tasks for Meta-learning?,"
It appears that it may be necessary to acquire a very large number of tasks for meta-learning , because MAML for example says that each task is analogous to a single training example in regular learning.
This is slightly confusing to me because it appears that outside of automated techniques like N-way classification where you randomly sub-select classes (& training examples) to include in a given task.
Adding tasks seems to be quite laborious right? I mean it sounds like you would need to get a different micro dataset for each task? And if meta-learning needs so many tasks then how do you satisfy that need?
","['meta-learning', 'model-agnostic-meta-learning']",
Is knowing underlying probability distribution mandatory for deciding iid property of random variables?,"
Consider the following information regarding iid random variables

The acronym IID stands for ""Independent and Identically Distributed"".
A sequence of random variables (or random vectors) is IID if and only
if the following two conditions are satisfied:

the terms of the sequence are mutually independent;

they all have the same probability distribution.



Definition:

Let $\{\mathcal{X}_n\}$ be a sequence of random vectors. Let
$F_{\mathcal{X}_n}{(x_n)}$ be the joint distribution function of a
generic term of the sequence $\{\mathcal{X}_n\}$. We say that
$\{\mathcal{X}_n\}$ is an IID sequence if and only if
$$F_{\mathcal{X}_n}{(x)} = F_{\mathcal{X}_k}{(x)} \forall x, n, k $$
and any subset of terms of the sequence is a set of mutually
independent random vectors.

Thus,

iid is a property for a sequence of random variables.
A joint probability distribution function is necessary to validate whether a sequence of random variables is iid or not.

Thus, the iid property of a sequence of random variables, from 2, is entirely depending on the underlying joint probability distribution function. Am I wrong anywhere?
If I am wrong, is there any other iid property of random variables that do not depend on the underlying probability distribution function?
","['probability-distribution', 'random-variable', 'iid']","
The point is even you know the distribution, sometimes you can't prove that the sampled data is i.i.d. or not!  (more details in https://stats.stackexchange.com/q/130381/144441). Hence, without knowing the distribution, you have less information, and of course, you can't prove any identically distributed-ness property of the sampled data.
Note that i.i.d. is mostly mentioned as an assumption that is held in the corresponding domain, and you do not need to prove it as a property.
"
Which of the following probability distribution is generating an iid dataset?,"
Let $X_1, X_2$ be two discrete random variables. Each random variable takes two values: $1, 2$
The probability distribution $p_1$ over $X_1, X_2$ is given by
$$p_1(X_1=1, X_2 = 1) = \dfrac{1}{4}$$
$$p_1(X_1=1, X_2 = 2) = \dfrac{1}{4}$$
$$p_1(X_1=2, X_2 = 1) = \dfrac{1}{4}$$
$$p_1(X_1=2, X_2 = 2) = \dfrac{1}{4}$$
The probability distribution $p_2$ over $X_1, X_2$ is given by
$$p_2(X_1=1, X_2 = 1) = \dfrac{8}{16}$$
$$p_2(X_1=1, X_2 = 2) = \dfrac{4}{16}$$
$$p_2(X_1=2, X_2 = 1) = \dfrac{3}{16}$$
$$p_2(X_1=2, X_2 = 2) = \dfrac{1}{16}$$
Suppose $D_1, D_2$ are the datasets generated by $p_1, p_2$ respectively.
Then which dataset can I call an iid? I am guessing as $D_1$ since we can prove the random variables are independent and are identically distributed and for $D_2$, iid does not hold.

$\underline{\text{ For }D_1}$
Identical : $p_1(X_1 = x_1) = p_1(X_2 = x_2)= \dfrac{1}{2} \text{ where } x_1 = x_2  \in \{1, 2\}$
Independent: $p_1(X_1 = x_1,X_2 = x_2) = \dfrac{1}{4} = p_1(X_1 = x_1) p_1(X_2 = x_2) \text{ for  } x_1, x_2 \in \{1, 2\}$

We can show that random variables $X_1, X_2$ are not iid if we consider $p_2$.
Is the iid I am discussing is different from making an iid of a dataset as answered here? If not, where am I going wrong?
","['terminology', 'datasets', 'probability-distribution', 'random-variable', 'iid']",
"How can we ""draw i.i.d"" from any probability distribution?","
Consider the following paragraph from 2 Learning in High Dimensions in from of the paper titled Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges

Supervised machine learning, in its simplest formalisation, considers
a set of $N$ observations $D = \{(x_i, y_i)\}_{i=1}^{N}$ drawn
i.i.d. from an underlying data distribution $P$ defined over $\mathcal{X} \times \mathcal{Y}$, where $\mathcal{X}$ and
$\mathcal{Y}$ are respectively the data and the label domains. The
defining feature in this setup is that $\mathcal{X}$ is a
high-dimensional space: one typically assumes $\mathcal{X} =
 \mathbb{R}^d$ to be a Euclidean space of large dimension $d$.

Here, it is mentioned that $N$ observations are drawn i.i.d from probability distribution $P$, which is defined over $\mathcal{X} \times \mathcal{Y}$.
My doubt is that how can we draw i.i.d from every probability distribution if our distribution is not an i.i.d distribution. The only $P$ I know to be an i.i.d is the following
$$p(x_i) = \dfrac{1}{|\mathcal{X} \times \mathcal{Y}|} \text{ for } x_i \in \mathcal{X} \times \mathcal{Y} \text{ and } 1 \le i \le |\mathcal{X} \times \mathcal{Y}|$$
To put simply, dataset with all possible $256 \times 256 \times 3$ images is i.i.d but the dataset with all dogs is not an i.i.d.
As per my knowledge, every possible distribution may not be an i.i.d distribution. Then, without knowing anything about the distribution, how can we draw i.i.d?
","['datasets', 'probability-distribution', 'iid']","
As far as I know, it doesn't make sense to say that a probability distribution is i.i.d., as you're saying.
The property i.i.d. is a property of a sequence of random variables.
In your case, the random variables are $z_i = (x_i, y_i)$, so it's not just the input $y_i$ or the label $y_i$, but both.
The rest of the explanation can be taken from the other answer that I've just given, but, in a few words, the shape of your joint doesn't determine whether your samples are i.i.d. or not.
In practice, sampling/drawing i.i.d. from a distribution could mean that you will not change the probabilities of occurrence of the samples or create other dependencies. See this article on the sampling bias.
"
Which heuristic function should I use for the ColorShapeLinks game?,"
For learning purposes, I am trying to implement the minimax algorithm for the ColorShapeLinks game, which is similar to connect 4, except the fact that it combines both shape and color as the winning conditions, with a shape having priority over color. A color is associated with only one shape, so there can only be X blue or O red.
I have previously applied the minimax to TicTacToe, but I think that's a lot simpler than this one.
My question is: which heuristic function could be used to evaluate the states for this game?
I'm thinking of checking each window of the board, where the new piece is put, and then compare the differences of shapes and colors and calculate the total of each predetermined value before (all heuristic). However, I think that's a little bit too simple, right?
Also, is there any chance that the AI can be made with Local Search algorithms, like Hill Climbing, Annealing, or GA? I think we can't, since the state configurations are not complete, right? If there are any additional reasons for this, please kindly guide me through, I am pretty new in this research :D
","['minimax', 'hill-climbing', 'heuristic-functions', 'evaluation-functions', 'connect-four']",
Joined vs Separate optimizer for Actor-Critic,"
Say that I have a simple Actor-Critic architecture, (I am not familiar with Tensorflow, but) in Pytorch we need to specify the parameters when defining an optimizer (SGD, Adam, etc) and therefore we can define 2 separate optimizers for the Actor and the Critic and the backward process will be
actor_loss.backward()
actor_optimizer.step()
critic_loss.backward()
critic_optimizer.step()

or we can use a single optimizer for both the Actor's and the Critic's parameters so the backward process can be like
loss = actor_loss + critic_loss
loss.backward()
optimizer.step()

I have 2 questions regarding both approaches:

Is there any consideration (pros, cons) for both the single joined optimizer and the separate optimizer approach?

If I want to save the best Agent (Actor and Critic) periodically (based on a predefined testing environment), do I always have to update the Critic, regardless of the current Agent's performance? Because (CMIIW) the Critic is (in its most basic purpose) only for predicting the action-value or state-value thus a more trained Critic is better.


","['reinforcement-learning', 'deep-rl', 'pytorch', 'actor-critic-methods', 'optimizers']","
I am also very curious about this. I have been implementing A2C in PyTorch from scratch and have tried both a single optimizer and separate optimizers. The separate case learns so much quicker. I believe it may have something to do with the coefficients, balancing the critic coefficient so the two losses were within a similar range seemed to work.
"
How are rewards calculated for episodic tasks like playing chess or tic-tac-toe?,"
I am new to Reinforcement Learning and trying to understand the concept of reaping rewards during episodic tasks. I think in games like tic-tac-toe, rewards will be in terms of a win or lose. But does that mean we need to finish the entire game to gain the reward? I mean reward will make sense only if three of the tokens are in one line. Each game of tic-tac-toe will be different as the sequence of actions followed will be different. So does reward come into the picture only after completing the game? And what if the game is a draw?
","['reinforcement-learning', 'deep-rl', 'rewards', 'reward-functions', 'tic-tac-toe']","
There is two ways to formulate a reward function for these types of problems. First there is sparse reward:
Win +1
Loss -1
All other rewards are 0.
As the opposite  there also exist dense rewards, which could be some signal for every timestep which tells you how well you're doing, i.e. give rewards in chess if you kick an opponents piece of the board.
Sparse rewards is harder to learn from since a lot of the time your reward and so your gradient is 0. On the other hand reward functions for dense reward require careful crafting by hand and leave room for human error and bias.
"
Could we add clipping in the output layer of the actor in DDPG?,"
I have a doubt about how clipping affects the training of the RL agents.
In particular, I have come across a code for training DDPG agents, the pseudo-code is the following:
1  for i in training iterations
2      action = clip(ddpg.prediction(state) * a + b, x, y)
3      state, reward = environment(action)
4      store action, state and reward
5      if the number of experiences is larger than L:
6          update the parameters of the agent

In this case, the actor NN that predicts the DDPG has a $\tanh$ activation in the output layer.
My question is, could we add the clipping in the output layer of the actor (changing $\tanh(x)$ by $\operatorname{clip}(a\cdot \tanh(x)+b, x, y$) in the training loop? Would the training work in that case?
","['reinforcement-learning', 'deep-rl', 'ddpg', 'tanh']",
Why should one ever use ReLU instead of PReLU?,"
To me, it seems that PReLU is strictly better than ReLU. It does not have the dying ReLU problem, it allows negative values and it has trainable parameters (which are computationally negligible to adjust). Only if we want the network to output positive values it makes sense to use it in the output layer. Other than that, I don't see why a priori I would decide to choose ReLU over PReLU. However, most architectures I came across use ReLU activations. Why?  Am I missing something?
","['deep-learning', 'comparison', 'activation-functions', 'relu', 'prelu']","
I suppose, the situation is as follows - PReLU increases the expressiveness of a model for a bit at a small cost, but the gain is almost negligible as well (according to this post).
There is, indeed, a noticeable difference between ReLU and PReLU, since the former takes the same value for all $\mathbb{R}_{\leq 0}$.
However, compared with a LeakyReLU, note that this activation is accompanied with a linear operation, like Dense or Convolution layer:
$$
y \sim f \left(\sum w \cdot x \right)
$$
And the slope $\alpha$ can be absorbed in the weights of neural network.
"
Calculating state-value functions in Markov Decision Process,"
I am watching David Silver's lectures on RL available on YouTube. My question here is with regard to Lecture 2 (Link to Video). At 1:11:00, I could not understand how he is calculating the state-value functions for C1, C2 and C3 (nodes with values 6, 8 and 10 respectively) in the student MDP example, starting from C3 and working backwards. Can someone please explain this?
","['reinforcement-learning', 'markov-decision-process']","
I looked at this not long ago.  You need to understand that the slide is referring to an optimal (not expected) value function & optimal Action-Value function.  Let's look at his diagram.  From left to right you have state C1 = 6 for its optimal value, C2 has 8 and C3 has 10.

If we take C10, we can see there are 2 choices for the transition, 1) to the pub for a reward of +1 or 2) to study for +10 reward.  There are no probabilities assigned to our decision, so we will take the action that maximizes our action-value.  So, being in C3 and deterministically choosing to study gives a reward of 10.  The action is studying and the reward is 10 so the action-value is 10 + the undiscounted value of the next state.  From there you end up in the sleep state (with value 0) with no further reward.
If you are in C3 and deterministically choose the action to go to the pub, you receive a reward of +1 with 3 possibilities of states/values.  Given this action value of C3 & going to pub, you can apply an expectation over values of where you end up (not in the choice you make from C3).  Since you can end up in C1, C2 or C3 with their own probabilities and values, you end up with an action-value of 8.3 by choosing to go to the pub.
Likewise for calculating optimal action values for C1 and C2.  You see which deterministic action gives the maximum sum of reward and next state.
"
"Can I always use ""encoding"" and ""embedding"" interchangeably?","
This question is restricted to the text domain only.
The meaning of the word ""encode"" is Convert (information or instruction) into a particular form. One which performs encoding is called an encoder.
In deep learning, an encoder can also be the first part of a neural network (autoencoder) that simulates identity function, which governs the English meaning of encoder since it encodes the input.
Embeddings are encodings where the intention is to preserve semantics. You can observe the following excerpt from the chapter Vector Semantics and Embeddings

In this chapter we introduce vector semantics, which instantiates this
linguistic hypothesis by learning representations of the meaning of
words, called embeddings, directly from their distributions in texts.

But all encodings may not be the embeddings since encodings might not always preserve semantics (?). I have doubt in this statement which I inferred based on my current knowledge.
Many times, I came across the terms text encoding and text embedding interchangeably. But failing to catch whether they are the same or we need to be choosy while using them.
Consider the following usages of encoding and embedding in the paper titled Generative Adversarial Text to Image Synthesis by Scott Reed et al.

#1: The intuition here is that a text encoding should have a higher compatibility score with images of the correspondong class compared to any other class and vice-versa.
#2: Text encoding $\phi(t)$ is used by both generator and discriminator.
#3: ...where $T$ is the dimension of the text description embedding.
#4: ... we encode the text query $t$ using text encoder $\phi$. The description embedding $\phi(t)$ is first compressed ...

I think they are used interchangeably. Is it true? Can I use any word if I am confident enough that my encoding is semantic preserving? Or is there any strong reason for choosing the words?
If you observe the last point, the word ""encoder"" is used. Can I use embedder instead of it?
","['natural-language-processing', 'terminology', 'word-embedding']","
From my experience with reading papers and books, I think these two terms are sometimes used interchangeably.
As you also point out, an encoder (in an auto-encoder) also may also learn some ""semantics"" of the inputs in order to produce the latent space. However, the way encoders are trained may not produce embeddings, with similar properties to e.g. word embeddings. For example, an image of a cat may be mapped to a latent vector that is closer to the latent vector of a person than to the latent vector of a dog (the usual way deterministic autoencoders are trained doesn't enforce these properties).
So, in my head, an encoding may not have any semantics (one-hot encoding is the typical example), but an embedding has. However, again, it depends on the context, so you should take context into account. So, I expect people to use the term encoding to refer to an embedding.
"
How are these two versions of the Bellman optimality equation related?,"
I saw two versions of the optimality equation for $V_{*}(s)$ and $Q_{*}(s,a)$.
The first one is:
$$
V_{*}(s)=\max _{a} \sum_{s^{\prime}} P_{s s^{\prime}}^{a}\left(r(s, a)+\gamma V_{*}\left(s^{\prime}\right)\right)
$$
and
$$
Q_{*}(s, a)=\sum_{s^{\prime}} P_{s s^{\prime}}^{a}\left(r(s, a)+\gamma \max _{a^{\prime}} Q_{*}\left(s^{\prime}, a^{\prime}\right)\right)
$$
The second one is:
$$
V_{*}(s)=\max _{a \in \mathcal{A}}\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P_{s s^{\prime}}^{a} V_{*}\left(s^{\prime}\right)\right)
$$
and for $Q_*$
$$
Q_{*}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P_{s s^{\prime}}^{a} \max _{a^{\prime} \in \mathcal{A}} Q_{*}\left(s^{\prime}, a^{\prime}\right)
$$
If following distributive property to get from the first to the second expression. Why there is no summation term for the reward, for example, $$V_{*}(s) = \max_{a}(\sum_{s'}P^{a}_{ss'}r(s,a)+\gamma\sum_{s'}P^{a}_{ss'}V_{*}(s'))$$?
My guess is that $r(s,a)$ is the constant so it can be moved out of the summation, leaving $$r(s,a)\sum_{s'}P^{a}_{ss'} = r(s,a).$$
But is it always the case that $r(s,a)$ is independent of $s'$? I think the reward of moving from state $s$ to $s'$ may vary.
","['reinforcement-learning', 'reward-functions', 'bellman-equations', 'dynamic-programming']","

My guess is that $r(s,a)$ is the constant so it can be moved out of the summation, leaving $r(s,a)\sum_{s'}P^{a}_{ss'} = r(s,a)$

Yes, this is the case. More specifically:

$r(s,a)$ is the expected reward after taking action $a$ in state $s$.
Reward may depend on the state arrived in, $s'$, but that is ignored in the equations.
Reward may vary randomly, but by using the expected reward, this can be ignored.

The first equations you quote, which sum over $s'$ but use $r(s,a)$ inside that sum, are very misleading IMO, since the individual terms may not represent anything meaningful within the MDP. That is the term $r(s,a) + \gamma V^*(s')$ does not correspond to any part of the trajectory of the agent.
Although the sum is still mathematically sound, it is more normal to see a different term $r(s,a,s')$ (the expected reward similar to $r(s,a)$ but also conditional on $s'$) where the expected reward is used inside the sum of next states. The term $r(s,a,s') + \gamma V^*(s')$ does correspond to nodes on the trajectory of the agent. It is the expected future return from $s,a$ conditional on the state transitioning to $s'$.

but is it always the case that $r(s,a)$ is independent of $s'$. I think the reward  of moving from state $s$ to $s'$ may vary.

Yes $r(s,a)$ is independent of $s'$. Although individual rewards may vary stochastically, and may depend on $s'$ too, the term is already the expected reward when taking the action $a$ in state $s$. So it already includes any effects of random state transition and random reward. For the Bellman equations to work as written, the expectation needs to be independent of the policy $\pi$ thus a property of the environment, and this is the case.
I think both sets of equations are a little bit awkward from using a combination of expected reward, yet summing up expectations over the state transition matrix. I prefer the notation used in second edition of Sutton & Barto's Reinforcement Learning: An Introduction:
$$v^*(s) = \text{max}_a \sum_{r,s'} p(r,s'|s,a)(r + \gamma v^*(s'))$$
Where $p(r, s'|s,a)$ is the conditional probability of observing reward $r$ and next state $s'$ given initial state $s$ and action $a$. The $p(r, s'|s,a)$ function replaces the combination of state transition matrices $P_{ss'}^a$ and the expected reward (either $r(s,a)$ or $r(s,a,s')$). Those objects can be derived from $p(r,s'|a,s)$ if you want, but personally I find the newer notation easier to follow.
"
How does graph Fourier transform work when multiple signals present on each node?,"
Context: I was reading the following set of notes (page 83): here and it says:

Thus, the Fourier transform of signal (or function) $ \mathbf{f} \in R^{|V|} $ on a
graph can be computed as $$ \mathbf{s} = \mathbf{U}^T \mathbf{f} $$

Question: What happens if each node has multiple 'signals'? Are the Fourier transforms on each signal independent of one another?
Attempt: I assume that each signal is denoted as a column vector, and thus multiple signals may be written as a matrix $\mathbf{F} = [\mathbf{f_1}, \mathbf{f_2}, ..., \mathbf{f_n}]$ (where $ \mathbf{f_i} \in R^{|V|} $) for a graph with $n$ signals. Thus, the graph Fourier transform would be $$ \mathbf{S} = \mathbf{U}^T \mathbf{F} $$ and thus each of the Fourier transforms would be independent of one another. Is this the correct way to think about this.
Many thanks in advance!
","['geometric-deep-learning', 'graph-neural-networks', 'spectral-analysis']",
Is it normal that the values of the LogSoftmax function are very large negative numbers? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I have trained a classification network with PyTorch lightning where my training step looks like below:
def training_step(self, batch, batch_idx):
    x, y = batch
    y_hat = self(x)
    loss = F.cross_entropy(y_hat, y)
    self.log(""train_loss"", loss, on_epoch=True)
    return


When I look at the output logits, almost all of them are very large negative numbers, with one that is usually 0. Is this normal or might be something wrong with my training?
I am just using nn.LogSoftmax() on the outputs and taking the max to make my predictions, but my network is not doing so good when I am running on unseen data, and I want to make sure the problem is just me overfitting.
","['deep-learning', 'classification', 'pytorch', 'softmax']",
Multi Agent Deep Reinforcement Learning for continuous and discrete action,"
I am looking to have a cooperative multi agent reinforcement learning framework where one agent has a discrete action space and another agent has a continuous action space. Is there a way to do this as most papers I have seen will only handle one or the other.
","['deep-learning', 'deep-rl']",
How many tasks are needed for meta-learning?,"
This is an empirical question, essentially how many tasks do you need data for, to make a useful meta learning model (e.g. using MAML)? I'm looking for ranges based on personal experience or if anyone has done research on the topic and you know of references for the estimates that would be helpful as well.
For context I'm trying to work with about 5-7 tasks. I saw a person implement meta-learning with about this many in the paper Multi-MAML. But I've since seen example code in the learn2learn library which uses thousands of tasks...
P.S. I'm not sure if different parameterizations of a single task definition are still 'one task' (e.g. y=a*cos(x), where 'a' varies). Could that account for the discrepancy?
","['meta-learning', 'model-agnostic-meta-learning']",
Setting initial values in DDPG to favor better actions,"
I'm working on a problem using DDPG.
Is it possible to add some intelligence in the initialization phase, such that the convergence time is improved/shortened and local optima are avoided as much as possible?
For example, this may include assigning (higher) probabilities to (better) actions (in the action selection algorithm) at the start of an episode. This hopefully leads to the agent discovering and selecting ""better"" actions faster, rather than starting from more random ones. Or this won't work since the neural networks will just unlearn these initial values during the training process?
Also, with the above description, am I better off using Soft Actor-Critic?
","['reinforcement-learning', 'deep-rl', 'actor-critic-methods', 'ddpg']",
Validation accuracy less than training accuracy (with no sigh of overtraining),"
I am working with a deep CNN with over 100k sample data. I divided it up into 75% training, 12.5% validation and 12.5% for testing. As I train my network, the training accuracy approaches near 100% accuracy. The validation accuracy approaches 70-90% accuracy. The validation accuracy is always increasing and never decreases so I do not believe that the network is over-training.
The training accuracy is similar to the validation accuracy but both are less than the training accuracy.
My question is, what is causing my validation/training data to trail the training data? Is it because my validation/training sets contains sample types which are not found in the training set? What else might be causing this?
Additionally, between epochs, I see this 'stair case' in learning in that I see a huge jump in accuracy as soon as a new epoch starts. I am shuffling my data between epochs. What might be causing this jump in accuracy?
Also, if there are more technical terms for the events that I am describing please let me know so that I can further research these.
Thank you!
blue = training,
black = validation

","['convolutional-neural-networks', 'training']","
The validation trend doesn't inform you much about real overfitting, cause the model hyperparameters are optimized based on the validation set. Reason why usually the validation scores are better than the test ones. So to truly check overfitting you should constantly look at the test scores.
The jumps make me think that you're using all training instances each epoch, so no matter if you shuffle or not, the model performs better cause it starts going trough already seen examples.
In general, don't expect to reach a test accuracy similar to the training one, it happens only with perfect toy datasets. And if you have reasons to think the test accuracy should be much closer than what you are observing then inspect more closely your dataset and the splitting you're performing. Specifically check if you have imbalanced classes shown more in validation/test than training, larger variance in data features for validation/test than training or any other form of imbalance you might check depending on the type of data you're using.
"
Is it impossible to evaluate the generator distribution directly?,"
The following excerpt is taken from 3. The Inception Score for Image Generation from the paper titled A Note on the Inception Score.

Suppose we are trying to evaluate a trained generative model $G$ that
encodes a distribution $p_g$ over images $\hat{x}$. We can sample from
$p_g$ as many times as we would like, but do not assume that we can
directly evaluate $p_g$. The Inception Score is one way to evaluate
such a model.

The excerpt is saying that we are not directly evaluating the $p_g$, the generator distribution but trying to evaluate the model $G$.
Does the excerpt intend to say that it is practically impossible to evaluate $p_g$?
","['generative-adversarial-networks', 'generator']","

Does the excerpt intend to say that it is practically impossible to evaluate $p_g$?

No, this statement says that they assume that they are not going to directly evaluate $p_g$ for a given problem, and offer other evaluation criteria which does not require doing so, which is the point of the paper. I think the authors have chosen the wording carefully and accurately.
They have probably been motivated to publish about this approach, because it is not possible to evaluate $p_g$ in most use cases for GANs. Image generators output highly complex functions with very high dimensionality and a lot of correlation between pixel values. So the paper is of interest because it is hard (usually a practical impossibility) to define $p_g$ directly, or use something like KL divergence to assess the generator's match to the real distribution.
"
How does Chebyshev approximation of spectral convolution work?,"
I was reading the following paper: here. In it, it talks about spectral graph convolutions and says:

We consider spectral convolutions on graphs defined as the
multiplication of a signal $x \in R^N$ (a scalar for every node)
with a filter $g_{\theta}$ $=$ $\text{diag}  (\theta)$ parameterized
by $\theta \in R^{N}$ in the Fourier domain, i.e.: $ g_{\theta} * x = U g_{\theta} U^Tx $. We can understand $ g_{\theta}$ as a function of the eigenvalues of $L$, i.e.  $g_{\theta}(\Lambda)$

So far, it makes sense. $U^T x$ is the graph Fourier transform of the signal $x$, then we multiply by $ g_{\theta}$ in the Fourier domain as: $FT(f * g) = F(\omega)G(\omega)$. Then we have the multiplication by $U$ in the front to represent the inverse (graph) Fourier transform.
Then the paper lists some reasons why using the above convolution equation may not be practical in reality:

Evaluating the above equation is computationally expensive; multiplying with eigenvector matrix $U$ is $O(N^2)$
Computing eigendecomposition of $L$ may be too expensive for an arbitrarily large graph
etc.

and then the paper says:

To circumvent this problem, it was suggested in Hammond et al. (2011)
that $g_{\theta}(\Lambda)$ can be well-approximated by a truncated
expansion in terms of Chebyshev polynomials $T_k (x)$ up to
$K^{\text{th}}$ order: $$ g_{\theta '}(\Lambda) \approx \sum_{k =
> 0}^{K} \theta_k ' T_k(\tilde{\Lambda}) $$
with a rescaled $\tilde{\Lambda} = \frac{2}{\lambda_{\text{max}}}\Lambda − I_N$. $\lambda_{\text{max}}$
denotes the largest eigenvalue of $L$. $\theta ′ \in R^K$ is now a
vector of Chebyshev coefficients. The Chebyshev polynomials are
recursively defined as $T_k(x) = 2xT_{k−1}(x) − T_{k−2}(x)$, with
$T_0(x) = 1$ and $T_1(x) = x$. The reader is referred to Hammond et al. (2011) for an in-depth discussion of this approximation. Going
back to our definition of a convolution of a signal $x$ with a filter
$g_{\theta '}$, we now have: $$ g_{\theta '} * x \approx \sum_{k=0}^{K} \theta_k ′ T_k (\tilde{L}) x$$ with $\tilde{L} = \frac{2}{\lambda_{\text{max}}}L − I_N$ ; as can easily be verified by
noticing that $(U \Lambda U^T)^k = U \Lambda^k U^T $

Question: What happened to the terms $U^T$ and $U$ which take the (graph) Fourier transform and invert it respectively?
Attempt: Does it have something to do with what is mentioned in the last line about noticing that $(U \Lambda U^T)^k = U \Lambda^k U^T $? I might guess that we use that because a k-th order Chebyshev polynomial will have $\Lambda ^k$ (and lower powers) present in the equation and thus the $U^T$ and $U$ mean that we can write the convolution equation in terms of the Laplacian matrix $L$
","['papers', 'geometric-deep-learning', 'graph-neural-networks', 'spectral-analysis']",
What is meant by inverting the generator?,"
Generative Adversarial Networks, in general, consists of two multi layer perceptrons: generator and discriminator. Generator is used for generating samples that are as real as training samples and discriminator tries to discriminate the real and fake samples.
Generator receives noise and generate samples.
Some papers like this use extra networks

To achieve this, one can train a convolutional network to invert
$G$ to regress from samples $\hat{x} \leftarrow G(z, \phi(t))$ back
onto $z$.

And some other papers, I remember saying that we use the same generator architecture to invert the it.
What does it mean by inverting the generator of a GAN? Does inverting means passing samples and getting noise as output by using same or different architectures?
","['generative-adversarial-networks', 'generator']",
How do convolutional layers of basic Graph Convolutional Networks work?,"
I was reading the following article on Towards Data Science (here) and it says the following, regarding the calculation of convolutional layers:

So the overall steps are:

Transform the graph into the spectral domain using eigendecomposition
Apply eigendecomposition to the specified kernel
Multiply the spectral graph and spectral kernel (like vanilla convolutions)
Return results in the original spatial domain (analogous to inverse GFT)


Question: How can we visualize the convolutional layer working for a graph neural network?
For example, for a CNN we can imagine the following (source: Stanford CS231n YouTube lectures, Lecture 5: Convolutional Neural Networks (here)). What is the analogous image for a graph convolutional filter?

","['geometric-deep-learning', 'graph-neural-networks', 'convolutional-layers']",
Does an increase in the number of epochs lead to complete breakdown?,"
Recently, I ran a code on my system that involves deep neural networks. The number of epochs provided by the designers are 301.
I tried to increase the number of epochs to 501. To my shock, the model after 350 epochs is behaving eccentric. And I can say that they are just returning crazy values.
What makes such phenomena possible?  Is ""number of epochs"" also a hyperaparameter/ magic number as an upper bound beyond which the model fails?
","['implementation', 'epochs']","
There is nothing specific about this particular numbers. Everything depends on the NN, software, model and data.
As illustrated as the number of epoch increases, more number of times the weights are changed and the curve goes from underfitting to overfitting. And overfitting is exactly eccentric and crazy, see the picture at left:.

"
How to approach a blackjack-like card game with the possibility of cards being counted?,"
Consider a single-player card game which shares many characteristics to ""unprofessional"" (not being played in casino, refer point 2) Blackjack, i.e.:

You're playing against a dealer with fixed rules.
You have one card deck which is played completely through.
etc. An exact description of the game isn't needed for my question, thus I remain with these simple bullet points.

Especially the second point bears an important implication. The more cards that have been seen, the higher your odds of predicting the subsequent card - up to a 100% probability for the last card. Obviously, this rule allows for precise exploitation of said game.
As far as the action and state space is concerned: The action space is discrete, the player only has a fixed amount of actions (in this case five - due to the missing explanation of the rules, I won't go in-depth about this). Way more important is the state space. In my case, I decided to structure it as follows:




A
2
3
4
5
6
7
8
9
10
J




4
4
4
4
4
4
4
4
4
14
2




First part of my state space describes each card value being left in the stack, thus on the first move all 52 cards are still in the deck. This part alone allows for about 7 million possible variations.
Second part of the state space describes various decks in the game (once again without the rules it's hard to explain in detail). Basically  five integers ranging from 0-21, depending on previous actions. Another 200k distinct situations.
Third part are two details - some known cards and information, though they only account for a small factor, but still bring in a considerable amount of variation into my state space.
Thus a complete state space might look something like this: Example one would be the start setup: 444444444142;00000;00. Another example in midst of the game: 42431443081;1704520;013. Semicolons have been added for readability purposes.
So now arises the question: From my understanding my state space is definitely finite and discrete, but too big to be solved by SARSA, Q-learning, Monte Carlo or alike. How can I approach projects with a big state space without loosing a huge chunk of predictability (which I might fear with DQN, DDPQ or TD3)? Peculiarly due to the fact that only one deck is being used - and it's played through in this game - it seems like a more precise solution would be possible.
","['reinforcement-learning', 'deep-learning', 'q-learning', 'state-spaces']","

How can I approach projects with a big state space without loosing a huge chunk of predictability (which I might fear with DQN, DDPQ or TD3)?

You can impact this by choosing a combination of function approximator and engineered features which are a good match to predicting either the value functions or policy function that an agent will need to produce.
It is hard to tell a priori how much work you would need in this regard. Given that you play as many training games as you have time for, use a deep neural network with hardware acceleration, then one approach is to simply normalise/scale your feature vector as it is, and train heavily. This approach in RL has repeatedly shown new state-of-the-art results for e.g. AlphaZero, Open AI's DoTA agent and others. This appears to work provided the compute resource that you can throw at the problem is large enough. As you mention, card-counted blackjack is a solved problem, so doing this may be within your reach on consumer hardware.
Some smart feature engineering may help though, by making the core problem easier for the agent. As this is a hobby project, what you do will depend on what you want the agent to learn. Is the purpose of your project to teach the agent how to card-count from scratch? Given that you have told the agent the count of remaining cards in the state, it does not appear so.
Next question: Do you need the agent to learn to sum up all remaining cards in order to calculate the raw probability of revealing a card of each type amongst cards so far unseen? That's currently what your main state feature is doing. If you don't need the agent to learn that, you could help by using the probability of each card being seen as a feature instead of the remaining count. This feature is likely to reduce the complexity of value and policy functions, thus it will reduce compute time, and may also improve accuracy.
With a game that can be solved analytically, you could take this up to the point of deriving the optimal policy directly and not using RL at all (i.e the input to your NN would be the correct action or the action values!). The question for your project is then this: What precisely are you hoping to demonstrate that your agent can learn? Or maybe: What do you want to gain by applying RL to this problem?
"
Can I call any function a signal?,"
While reading the Notation of the paper titled  Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges, I came across the following notations.
$$
\Omega = \text{ Domain} \\
 u = \text{Point on domain} \\
x(u) \in \mathcal{X}(\Omega, C) = \text{ Signal on the domain of the form } x : \Omega \rightarrow C
$$
Mathematically, a signal is just a function.
But, every function may not be a signal. There may be some distinction between a mathematical function and a signal.
When can I call any mathematical function a signal? And what is $\mathcal{X}$ in the notation given?
","['definitions', 'geometric-deep-learning', 'signal-processing']",
What is meant by domain in the notations of geometric deep learning?,"
While reading the Notation of the paper titled  Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges, I came across the following notations.
$$
\Omega = \text{ Domain} \\
 u = \text{Point on domain}
$$
I can only understand domain here as a set and point as an element in the set $\Omega$.
I have a simple doubt here.
Why do we call the set $\Omega$ as domain? Is it due to the notation related to mathematical function?
$$\text{function_name} : \text{domain} \rightarrow \text{co_domain}$$
Or is it due to the usage of the word domain in the sense of application domain: A specified sphere of activity or knowledge. such as computer vision, natural language processing etc?
or a combination of both?
","['terminology', 'geometric-deep-learning']",
Is my single layer perceptron getting biased input some way or the other?,"
I was working a little bit on a school project my team and I decided to do for submission in the year-end. It's a small game which I call 'Quattro', and its rules are as follows:

The game is played on an 8 x 8 square grid and each player (both the human and the computer) have sixteen pieces on their side (just the same layout as in chess, but here all pieces are identical for each player).
Only vertical moves,(i.e., one can move only one square forward at a time and that too in the forward direction/along a column) as long as no other piece stands before the piece to be moved.
However, one can cross over to a square present in the north-west or north-east (when you look around a 3 x 3 grid with the piece under consideration at the centre) if the north and west/east boundary of the piece when in the 3 x 3 grid are held by the enemy's pieces, as in the case of the move 'en passant' in chess. In the process, the enemy piece in the square just below the square the player has crossed over to is lost by the enemy.
The players involved can either be an attacker or a defender (if one choses the former, the other takes the latter). The attacker wins if he/she successfully takes at least four of his/her pieces(hence the name Quattro) to the enemy's side (that is, to the last row counting from that player's side) while the defender wins if the attacker is prevented from doing so.

You can request me to add screenshots in case the rules are very vague (even my teammates were confused ğŸ˜…).
Okay, so I'm doing this on Python 3.9.6 and I have somehow made the board layout and movement rules (except for rule 2 and rule 4, which are supposed to be added once the primary workings of the game is completed). I had somehow made the AI player (which is based on a single-layer perceptron), but I doubt if it is working right or not. The problem is that when I make a random move, the AI player starts always at the same column and moves pieces in some order I can't clearly remember(in the primary stage of creation, it all seemed to work fine, but as time progressed, I began to see indexing errors so I tried to adjust things somehow) and then it wanders off into an infinite loop. From a debug message I set up to observe the change in weights, I saw that at times one weight it growing while the other few would either be shrinking or remaining constant. As of now, I set up a variable to give the model a random target value (or may not be random, it seems) to train with and still the problem continues. I doubt if the input data is biased someway or the other. Here's how the input is taken:

The model first checks through each column, and the input corresponding to each column will be a vector containing 0's and 1's with the 1's indicating the enemy's presence and 0 for the else case. The model thus generates a 'preference score' (equivalent to the activation function of the sum of the weighted inputs, as how it is in any other perceptron).
The same is done for rows as well and the list of values in both cases are passed to a dictionary, from which the player choses the row and column index with the highest scores and moves the piece there.

I also set up an InvalidMove exception so as to make sure that the machine doesn't play blankly.
So here's the code:

MarchOfTheFinalFour.py - the module containing the required exception and the board.

# MarchOfThFinalFour.py

from time import *

# 'March of The Final Four' - a clone of chess

player_piece = 'Î¦' # player's piece
computer_piece = 'Ï„' # computer's piece

class InvalidMove(Exception):
    '''error when you/ the computer takes an invalid move'''
    def __init__(self, coords):
        self.stmt = ""invalid move from:""
        self.coords = coords

class PlayTable:
    def __init__(self, table_side):
        ''' generates the game board, empty and with no pieces '''
        self.length = table_side
        self.table = [[0]*table_side]*table_side

    def __repr__(self):
        '''prints the table'''
        print()
        table_str = ''
        num = 0
        for row in self.table:
            table_str += str(num) + "" \t""
            for piece in row:
                table_str += str(piece) + ""|""
            table_str += ""\n""
            num += 1
        return table_str + ""\t0 1 2 3 4 5 6 7""
        
    def reset(self):
        ''' resets the board/ places pieces on it '''
        for row in [0, 1] :
            self.table[row] = [computer_piece]*self.length

        for row in [self.length-2, self.length - 1] :
            self.table[row] = [player_piece]*self.length
        
        for row in range(2, self.length-2):
            self.table[row] = [0]*self.length
    def move_piece(self, coord, turn = 'player'):
        '''moves the piece at coord '''
        if self.table[coord[1]%self.length][coord[0]%self.length] != 0 and self.table[(coord[1] + (1 if turn == 'computer' else -1))%self.length][coord[0]%self.length] == 0:
            temp = self.table[coord[1]][coord[0]]
            self.table[coord[1]][coord[0]] = 0
            direction = 1 if turn == 'computer' else -1
            self.table[coord[1]+direction][coord[0]] = temp
            print(f""Moved {temp} from {(coord[0] if coord[0] >= 0 else 8 + coord[0],coord[1] if coord[1] >= 0 else 8 + coord[1])}"") # msg
        elif self.table[coord[1]%self.length][coord[0%self.length]] == 0 or self.table[(coord[1] + (-1)**(1 if turn == 'player' else 1))%self.length][coord[0]%self.length] != 0:
            raise InvalidMove(coord)
        elif turn == 'player' and self.table[coord[1]%self.length][coord[0]%self.length] == computer_piece:
            raise InvalidMove(coord)
        elif turn == 'computer' and self.table[coord[1]%self.length][coord[0]%self.length] == player_piece:
            raise InvalidMove(coord)
        
        
board = PlayTable(8)
board.reset()
print(board)



TestGameML.py - sample game, NPC, single-layer perceptron, etc. all lies here:

from math import *
from random import *
import MarchOfTheFinalFour as mff

######################

## Math functions for our use in here
    
def multiply(list_a, list_b):
    '''matrix multiplication and addition'''
    list_res = [list_a[n] * list_b[n] for n in range(len(list_a))]
    return fsum(list_res)

def sig(x):
    '''logistic sigmoid function'''
    return exp(x)/(1+ exp(x))

##############################

## Neighbourhood search

def neighbourhood(coords, board_length):
    '''generates the 3 x 3 grid that forms the neighbourhod of the required square'''
    axial_neighbours =  [(coords[0] + 1, coords[1]),(coords[0] - 1, coords[1]),
                        (coords[0], coords[1] + 1), (coords[0], coords[1] - 1)] # neighbours along NEWS directins
    diagonal_neighbours = [(coords[0] + 1, coords[1]+1),(coords[0] - 1, coords[1] - 1),
                           (coords[0]-1, coords[1] + 1), (coords[0]+1, coords[1] - 1)] #diagonal neighbours
    neighbours = axial_neighbours + diagonal_neighbours # supposed neighbours
    ## purging those coordinates with negative values in them:
    for i in range(len(neighbours)):
        if (neighbours[i][0] < 0 or neighbours[i][0] > board_length - 1) or (neighbours[i][1] < 0 or neighbours[i][1] > board_length - 1):
            neighbours[i] = 0
    while 0 in neighbours:
        neighbours.remove(0)
    
    return neighbours

########################

# The NPC's brain

class NPC_Brain:
    '''brain of the NPC ;), actually a single-layer perceptron '''
    def __init__(self,board_size):
        ''' Initialiser'''
        self.inputs = board_size # no. of input nodes for the neural network
        self.weights = [random() for i in range(self.inputs)] # random weights for each game
        self.column_scores = [] # column scores (for each column) - the 'liking' of the computer to move a piece in a column as the output
                                # of the neural network's processing
        self.row_scores = [] #same here
        self.inputs_template_columns = [] # a container to hold the inputs to the neural network
        self.inputs_template_rows = [] # same here but for rows
    def process(self, board, threshold):
        '''forward-feeding'''
        # we begin by setting the lists to zero so as to make the computer forget the past state of the board and to look for the current state
        self.inputs_template_columns = []
        self.inputs_template_rows = []
        self.column_scores = []
        self.row_scores = []
        self.row_scores = []
        for column in range(self.inputs):
            scores = [1 if row[column] == mff.computer_piece else 0 for row in board] # checking for enemies in each column
            self.inputs_template_columns.append(scores) 
            score = sig(multiply(scores, self.weights)/threshold) # using the logistic sigmoid function to generate a liking for columns :D
            self.column_scores.append(score) # each column score is appended
        for row in range(self.inputs):
            scores = [1 if board[row][i] == mff.player_piece else 0 for i in range(self.inputs)] # checking for enemies in each column
            self.inputs_template_rows.append(scores) 
            score = sig(multiply(scores, self.weights)/threshold) # using the logistic sigmoid function to generate a liking for columns :D
            self.row_scores.append(score) # each column score is appended
        return {'columns':self.column_scores, 'rows':self.row_scores}
    def back_prop(self, learning_rate, target = 1):
        '''Back-propagation, with error function as squared-error function (target - error)**2'''
        for j in range(len(self.inputs_template_columns)):
            for i in range(self.inputs):
                '''overfitting can occur, but still let's try this'''
                self.weights[i] +=  learning_rate * 2 * (self.column_scores[j] - target) * (self.column_scores[j]*(1-self.column_scores[j])) * self.inputs_template_columns[j][i] #backprop formula
        for k in range(len(self.inputs_template_rows)):
            for i in range(self.inputs):
                '''overfitting can occur, but still let's try this'''
                self.weights[i] += learning_rate * 2 * (self.row_scores[k] - target) * (self.row_scores[k]*(1-self.row_scores[k])) * self.inputs_template_rows[k][i] #backprop formula
                
    
        

class NPC:
    ''' non-playable character / computerized player class '''
    def __init__(self):
        self.mind = NPC_Brain(mff.board.length) # the model
        self.piece_lower = 0; self.piece_upper = 1 # initial row numbers of the computer's pieces
        self.row_expanse = 2
        
    def make_move(self):
        moved = False
        req_target = 0.5
        counter = 1
        while not moved:
            if counter % 50 == 0:
                req_target += log(req_target**(counter%25))
                print(""New target set:"", req_target)
            score_board = temp = self.mind.process(mff.board.table, 0.5) # feeding forward
            x_coord = score_board['columns'].index(max(score_board['columns'])) # choosing the column the compute likes the most
            y_coord = score_board['rows'].index(max(score_board['rows'])) % self.row_expanse # a random y coordinate is chosen
            try:
                if y_coord < mff.board.length - 1: 
                    if mff.board.table[int(y_coord) + 1][int(x_coord)] == 0 and (mff.board.table[int(y_coord)][int(x_coord)] not in  [0, mff.player_piece]):
                        mff.board.move_piece((int(x_coord), int(y_coord)), turn = 'computer')
                        self.piece_upper += 1 #increasing the upper limit of the y coordinate by 1
                        moved = True
                        req_target += 0.0001
                        self.row_expanse += 1
                        counter += 1
                    else:
                        raise mff.InvalidMove((x_coord,y_coord))
                    counter += 1 
            except mff.InvalidMove:
                # trying to avoid the computer's confusion
                self.mind.back_prop(1/pi, target = req_target) # making the computer learn from its decision
                req_target -= 0.0001
                counter += 1
                    
            
                
            
                


npc = NPC() # creating the NPC

## Sample gamplay
## The following gameplay will be a bit smooth in the beginning but turns into a confusion later
all_gone_good = True
while True:
    all_gone_good = True 
    # infinite loop here till errors occur
    player_mv = eval(input(""Enter your move:"")) # waiting for the player's move
    try:
        mff.board.move_piece(player_mv)
    except mff.InvalidMove:
        print(""Invalid move"")
        all_gone_good = False
    # next we check if the player's move was valid
    if all_gone_good:
        print(mff.board)
        npc.make_move()
        print(mff.board)
        

I am sorry that  haven't been able to comment in certain regions of the code, in which case you can ask me for clarification.
My main doubts are : is my data acquisition method biased? Is the training part also little bit wacky? Or is it that I programmed it all without knowing what I am doing? What's actually causing such an infinite loop?

Edit: : I have edited TestGameML.py and it's down here:
from math import *
from random import *
import MarchOfTheFinalFour as mff

######################
##Bug fixes required:

##1. The machine is making multiple moves unknowingly

######################

## Some variables for global use

my_move = (0,0)

## Math functions for our use in here
    
def multiply(list_a, list_b):
    '''matrix multiplication and addition'''
    list_res = [list_a[n] * list_b[n] for n in range(len(list_a))]
    return fsum(list_res)

def sig(x):
    '''logistic sigmoid function'''
    return exp(x)/(1+ exp(x))

##############################

## Neighbourhood search

def neighbourhood(coords, board_length):
    '''generates the 3 x 3 grid that forms the neighbourhod of the required square'''
    axial_neighbours =  [(coords[0] + 1, coords[1]),(coords[0] - 1, coords[1]),
                        (coords[0], coords[1] + 1), (coords[0], coords[1] - 1)] # neighbours along NEWS directins
    diagonal_neighbours = [(coords[0] + 1, coords[1]+1),(coords[0] - 1, coords[1] - 1),
                           (coords[0]-1, coords[1] + 1), (coords[0]+1, coords[1] - 1)] #diagonal neighbours
    neighbours = axial_neighbours + diagonal_neighbours # supposed neighbours
    ## purging those coordinates with negative values in them:
    for i in range(len(neighbours)):
        if (neighbours[i][0] < 0 or neighbours[i][0] > board_length - 1) or (neighbours[i][1] < 0 or neighbours[i][1] > board_length - 1):
            neighbours[i] = 0
    while 0 in neighbours:
        neighbours.remove(0)
    
    return neighbours

########################
# The NPC's brain

class NPC_Brain:
    '''brain of the NPC ;), actually a single-layer perceptron '''
    def __init__(self,board_size):
        ''' Initialiser'''
        self.inputs = board_size # no. of input nodes for the neural network
        #self.weights = [random() for i in range(self.inputs)] random weights for each game
        self.weights = [0.5]*self.inputs
        self.column_scores = [] # column scores (for each column) - the 'liking' of the computer to move a piece in a column as the output
                                # of the neural network's processing
        self.row_scores = [] #same here
        self.inputs_template_columns = [] # a container to hold the inputs to the neural network
        self.inputs_template_rows = [] # same here but for rows
        
    def process(self, board, threshold):
        '''forward-feeding'''
        # we begin by setting the lists to zero so as to make the computer forget the past state of the board and to look for the current state
        self.inputs_template_columns = []
        self.inputs_template_rows = []
        self.column_scores = []
        self.row_scores = []
        for column in range(self.inputs):
            scores = [(1/8)**(row + 1 if row == my_move[1] else 1) if board[row][column] == mff.player_piece else -1/8 for row in range(self.inputs)] # checking for enemies in each column
            self.inputs_template_columns.append(scores) 
            score = sig(multiply(scores, self.weights)/threshold) # using the logistic sigmoid function to generate a liking for columns :D
            self.column_scores.append(score) # each column score is appended
        for row in range(self.inputs):
            scores = [(1/8)**(i + 1 if i == my_move[0] else 1) if board[row][i] == mff.player_piece else -1/8 for i in range(self.inputs)] # checking for enemies in each column
            self.inputs_template_rows.append(scores) 
            score = sig(multiply(scores, self.weights)/threshold) # using the logistic sigmoid function to generate a liking for columns :D
            self.row_scores.append(score) # each column score is appended
        return {'columns':self.column_scores, 'rows':self.row_scores}
    
    def back_prop(self, learning_rate, target = 1):
        '''Back-propagation, with error function as squared-error function (target - error)**2'''
        for j in range(len(self.inputs_template_columns)):
            for i in range(self.inputs):
                '''overfitting can occur, but still let's try this'''
                self.weights[i] +=  -learning_rate * 2 * (self.column_scores[j] - target) * ((self.column_scores[j]**2)*(1-self.column_scores[j])) * self.inputs_template_columns[j][i] #backprop formula
        for k in range(len(self.inputs_template_rows)):
            for i in range(self.inputs):
                '''overfitting can occur, but still let's try this'''
                self.weights[i] += -learning_rate * 2 * (self.row_scores[k] - target) * ((self.row_scores[k]**2)*(1-self.row_scores[k])) * self.inputs_template_rows[k][i] #backprop formula
                
    
        

class NPC:
    ''' non-playable character / computerized player class '''
    def __init__(self):
        self.mind = NPC_Brain(mff.board.length) # the model
        self.piece_lower = 0; self.piece_upper = 1 # initial row numbers of the computer's pieces
        self.row_expanse = 2
        
    def make_move(self):
        moved = False
        req_target = 0.5
        counter = 1
        print(""Thinking..."")
        while not moved:
            score_board = temp = self.mind.process(mff.board.table, 0.5) # feeding forward
            x_coord = score_board['columns'].index(min(score_board['columns'])) # choosing the column the compute likes the most
            y_coord = score_board['rows'].index(max(score_board['rows'])) % self.row_expanse # a random y coordinate is chosen
            try:
                if y_coord < mff.board.length - 1: 
                    if mff.board.table[int(y_coord) + 1][int(x_coord)] == 0 and (mff.board.table[int(y_coord)][int(x_coord)] not in  [0, mff.player_piece]):
                        mff.board.move_piece((int(x_coord), int(y_coord)), turn = 'computer')
                        self.piece_upper += 1 #increasing the upper limit of the y coordinate by 1
                        moved = True
                        self.row_expanse += 1
                        counter += 1
                    else:
                        raise mff.InvalidMove((x_coord,y_coord))
                    counter += 1 
            except mff.InvalidMove:
                # trying to avoid the computer's confusion
                self.mind.back_prop(0.5, target = req_target) # making the computer learn from its decision
                counter += 1
                    
            
                
            
                


npc = NPC() # creating the NPC

## Sample gamplay
## The following gameplay will be a bit smooth in the beginning but turns into a confusion later
all_gone_good = True
while True:
    all_gone_good = True 
    # infinite loop here till errors occur
    player_mv = eval(input(""Enter your move:"")) # waiting for the player's move
    try:
        mff.board.move_piece(player_mv)
    except mff.InvalidMove:
        print(""Invalid move"")
        all_gone_good = False
    # next we check if the player's move was valid
    if all_gone_good:
        my_move = player_mv
        print(mff.board)
        npc.make_move()
        print(mff.board)


Changelog:

Change data distribution method in lines 71 and 76
Asked NPC to choose the column with the least column score and max row score.

","['neural-networks', 'game-ai', 'single-layer-perceptron']",
What are the definitions for the content and style of an image without using deep neural network?,"
In deep learning, an image is said to contain two types of features. One is the content of the image and the other is the style of the image.
Deep neural networks are generally used to obtain both content representation and style representation of an image. So, one can roughly define the style and content representations of an image using deep neural networks.
Research papers generally show foreground objects (under consideration or focus) in an image as the content of the image and the background (or background objects such as sky etc.) as the style of the image.
If we need to define the content and style of an image without using deep neural networks, then what can be the definitions for the content and style of an image?
","['computer-vision', 'definitions', 'style-transfer']",
Is there a venue to publish negative results in AI/ML domain?,"
Negative results occur frequently in AI/ML research (and perhaps in other domains too). Most of the time, these results are not published. This is mostly because your typical AI/ML conference doesn't accept such papers.
However, are there any venues to publish these results?  I believe these results can be still useful to look at before you delve into a certain project so that you'd at least know what approaches don't work.
As an example venue, there seems to be PerFail workshop from the pervasive computing domain. So, is there something similar for AI/ML?
","['research', 'academia']","
Such results should probably be put on arXiv. It's moderated, but there's no peer-review process, so if your goal is to just make useful information available, then this is your best bet.
"
Are the Q-values of DQN bounded at a single timestep?,"
Consider that we have an agent that has a set of thousands of different actions at each timestep. The reward function in $R:S \rightarrow\{0,1\}$. Let $Q_{t}^\pi(s,a)$ be the estimate from the neural network in the DQN. At timestep $t \leq T$, where $T$ is the horizon of the RL task, is there any possible way to upper bound
$$ max_a Q_{t}^\pi(s,a) - min_a Q_{t}^\pi(s,a) $$
where $\pi$ is the policy of DQN's Q-network (Neural Net regressor)?
","['reinforcement-learning', 'deep-rl', 'dqn']",
Why does Q-function training not query the Q-function value at unobserved states?,"
In the paper Conservative Q-Learning for Offline Reinforcement Learning, it is stated (section 3.1, page 3) that

standard Q-function training does not query the Q-function value at unobserved states, but queries the Q-function at unseen actions

I don't see how this is true. For every $(s,a)$ pair, we need to update $Q(s,a)$ to reduce the value $|Q(s,a) - R(s,a) - \gamma E[\max_{a'}Q(s',a')]|$ until it converges to zero.
We see the existence of both $a'$ and $s'$, and $s'$ could be unseen, for example, on the very first update, where we are at $s$, take action $a$, and could arrive at any state $s'$.
Can someone explain this?
","['reinforcement-learning', 'q-learning', 'papers']","

We see the existence of both $a'$ and $s'$, and $s'$ could be unseen, for example, on the very first update, where we are at $s$, take action $a$, and could arrive at any state $s'$.

The very first update is made after taking action $a$ in state $s$ and observing reward $r$ plus next state $s'$. There is no other way in Q learning of knowing what the next state is in order to process the update. So $s'$ in not unseen, it has been observed.
Another way to put this: In model-free methods, the update to Q value estimates for state $S_t$ action $A_t$ on time step $t$ is always made on or after time step $t+1$, when $R_{t+1}$ and $S_{t+1}$ are known.
However, on that same time step, the action $a'$ does not yet need to have been taken. The value of $a'$ is not taken from $A_{t+1}$, but is evaluated for all possibilities. Even after $A_{t+1}$ is known and has been taken, the need to process all possible actions in the state $s'$ (in order to update $Q(s,a)$ or $Q(S_t, A_t)$) can often lead to needing action value estimates for never seen state/action pairs.
The update step you quoted includes the expected reward (or possibly a custom reward function known to the agent) $R(s,a)$, which is not standard for Q learning. It would be more usual to use the observed reward $r$.  However, it may be ok because a lot of the time the developer/researcher is in charge of the reward signal and can provide $R(s,a)$ to the agent, even when the full model is not used.
"
"Can an AI create another better AI, which in turn creates another better AI, and so on? [closed]","







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I have no specific knowledge of the AI field, but I heard that AI systems get better the longer they learn.
So, I was wondering: could it be possible that AIs will learn how to create better AIs (Or assists humans to create a better AI), and then those better AIs will learn to create an even better/faster AI, and so on? Wouldn't this mean that the AI would get exponentially better/faster because, after each successive generation, a slightly faster AI will do the job?
I also heard that ""Google is using AI to design processors that run AI more efficiently"". Wouldn't this be the same? AI designs faster CPU => AI get's faster and can design an even better CPU to run on.
Is something like that possible? Would this mean that at some point there will be a breakthrough in AI that will significantly increase the speed of AIs because of those loops?
",['singularity'],
Is the inductive bias always a useful bias for generalisation?,"
Is it true that a bias is said to be inductive iff it is useful in generalising the data?
Or does inductive bias can also refer to the assumptions that may cause a decrease in performance?

Suppose I have a dataset on which I want to use a deep neural network to do my task. I think, based on some knowledge, that a DNN with 5 or 11 layers may work well. But, after implementation, suppose 11 layer one worked well. Then can I call both of them inductive bias? or only the 11 layer assumption?
","['machine-learning', 'terminology', 'generalization', 'inductive-bias']","
The inductive bias is the prior knowledge that you incorporate in the learning process that biases the learning algorithm to choose from a specific set of functions [1].
For example, if you choose the hypothesis class
$$\mathcal{H}_\text{lines} = \{f(x) = ax + b \mid a, b \in \mathbb{R} \}$$ rather than $$\mathcal{H}_\text{parabolas} = \{f(x) = ax^2 + b \mid a, b \in \mathbb{R} \},$$ then you're assuming (implicitly or explicitly, depending on whether you're aware of these concepts) that your target function (the function that you want to learn) lies in the set $\mathcal{H}_\text{lines}$. If that's the case, then your learning algorithm is more likely to find it.
In most cases, you do not know exactly the nature of your target function, so you could think that it may be a good idea to choose the largest set of possible functions, but this would make learning infeasible (i.e. you have too many functions to choose from) and could lead to over-fitting, i.e. you choose a function that performs well on your training data, but it's actually quite different from your target function, so it performs badly on unseen data (from your target function). This can happen because the training data could not be representative of your target function (you don't usually know this a priori, so you cannot really or completely solve this issue).
So, the definition above does not imply that the inductive bias will not necessarily lead to over-fitting or, equivalently, will not negatively affect the generalization of your chosen function. Of course, if you chose to use a CNN (rather than an MLP) because you are dealing with images, then you will probably get better performance. However, if you mistakingly assume that your target function is linear and you choose $\mathcal{H}_\text{lines}$ from which your learning algorithm can pick functions, then it will choose a bad function.
Section 2.3 of the book Understanding Machine Learning: From Theory to Algorithms and section 1.4.4. of the book Machine Learning A Probabilistic Perspective (by Murphy) provide more details about the inductive bias (the first more from a learning theory perspective, while the second more from a probabilistic one).
You may also be interested in this answer that I wrote a while ago about the difference between approximation and estimation errors (although if you know nothing about learning theory it may not be very understandable). In any case, the idea is that the approximation error (AE) can be a synonym for inductive bias because the AE is the error due to the choice of hypothesis class.
(As a side note, I think it is called ""inductive bias"" because this bias is the one that can make inductive inference feasible and successful [2] and maybe to differentiate it from other biases, e.g. the bias term in linear regression, although that term can also be an inductive bias).
"
What are the different types of geometry in literature that may be used for deep learning?,"
Recently, I asked a question on the concept of a manifold and received an answer that points to a relatively new subfield of deep learning named geometric deep learning.
In the preface of the paper titled Geometric Deep Learning Grids, Groups, Graphs, Geodesics, and Gauges, there is a mention of three types of geometry that do exist in the literature.

For instance, Euclidean geometry is concerned with lengths and angles, because these properties are preserved by the group of Euclidean transformations (rotations and translations), while affine geometry studies parallelism, which is preserved by the group of affine transformations. The relation between these geometries is immediately apparent when considering the respective groups, because the Euclidean group is a subgroup of the affine group, which in turn is a subgroup of the group of projective transformations.

The three types of geometry they mentioned are Euclidean, affine and projective. I want to know the complete list of types of geometry that do exist in the literature, if relevant to geometric deep learning.
What are the types of geometry in the literature that may be used for deep learning?
","['math', 'geometric-deep-learning']",
Should I apply normalization to the observations in deep reinforcement learning?,"
I am new to DRL and trying to implement my custom environment. I want to know if normalization and regularization techniques are as important in RL as in Deep Learning.
In my custom environment, the state/observation values are in a different range. For example, one observation is in the range of [1, 20], while another is in [0, 50000]. Should I apply normalization or not? I am confused. Any suggestions?
","['reinforcement-learning', 'deep-rl', 'regularization', 'normalisation', 'observation-spaces']","
The use of normalisation in neural networks and many other (but not all - decision trees are a notable exception) machine learning methods, is to improve the quality of the parameter space with respect to optimisers that will apply to it.
If you are using a function approximator that benefits from normalisation in supervised learning scenarios, it will also benefit from it in reinforcement learning scenarios. That is definitely the case for neural networks. And neural networks are by far the most common approximator used in deep reinforcement learning.
Unlike supervised learning, you will not have a definitive dataset where you can find mean and standard deviation in order to scale to the common $\mu = 0, \sigma = 1$. Instead you will want to scale to a range, such as $[-1, 1]$.
You may also want to perform some basic feature engineering first, such as using log of a value, or some power of it - anything that would make the distribution of values you expect to see more like a Normal distribution. Again, this is something you could do in supervised learning more easily, but maybe you know enough about the feature to make a good guess.
"
Is there something like person-specific sentiment analysis?,"
Sentiment analysis, as we know, measures ""Cake sucks"" as say -0.4, and ""Cake is great"" as 0.7.
What I'm looking for is something a bit different like so:

Given input text data written by 1 person (say a blog)
Predict how they (the person who wrote the text) might react to a certain piece of text


What might something like this look like?

Let's suppose that Person A with a blog has written in his blog posts thousands of times about how much cake is the best thing to happen to humanity.
The system should probably infer that if that person read something like ""Cake is the WORST food ever"", they would react negatively to it, if say, they also believe that there is such a thing as 'objective taste' somehow (aesthetic absolutism).
Or if Person A has made anti-racist statements, that racist statements would be strongly negative.
If Person A reads the statement ""I hate lawyers"" and in their blog they have written about how they don't care either way about law, it should probably be 0.
Finally, if Person A reads the statement ""iPhones are better than Android"" and there is zero data about either iPhones or Androids, or even related data about Apple or Google, then it should probably be 0, with an additional ""confidence"" metric at 0 (since there is no data, this confidence metric will let us know whether there is any data to support the measurement or not).


This model would need to be able to somehow inductively 'infer' a value system of some kind, and assign intensities of probable reactions based on the frequency of an expressed view, as well as pick up on nuances (such as philosophical assumptions, (for example in the above cake example: aesthetic absolutism) etc.) that may inform that measurement.
In other words, I'd like to create a model (or find a pre-trained model to fine-tune), that would be able to, given text data from that 1 person, predict their sentiment in response to a new piece of text.
Would love any help whatsoever regarding:

What types of pre-trained models I should look at
Any ideas of any kind whatsoever you might have on how to achieve this
What sorts of architectures/resources/concepts may be relevant to look at

","['natural-language-processing', 'natural-language-understanding', 'sentiment-analysis']",
How do I prepare my data for a CNN to be applied to a geophysical-related problem?,"
I am currently doing research work on an inversion of geophysical data using Machine Learning. I have come across some research work where a Convolutional Neural Network (CNN) has been used effectively for this purpose (for example, his).
I am particularly interested in how to prepare my input and output labelled data for this machine learning application, since the input will be the observed geophysical signal, and the label output will be the causative density or susceptibility distribution (for gravity and magnetic, respectively).
I need some assistance and insight as to how to prepare the data for this CNN application.
Additional Explanation
Experimental setup: Measurements are taking above the ground surface. These measurements are signals that reflect the distribution of a physical property (e.g., density) in the ground beneath. For modelling, the subsurface is discretised into squares or cubes each having a constant but unknown physical property (e.g., density).
How it applies to CNN: I want my input data to be the Measurements taken above ground. The output should then be the causative density distribution (that is, the value of the density in each cube/squares)
See attached picture (flat top is the ""above ground"", all other prisms represent the discretisation of the subsurface. I want to train the CNN to give out a density value for each cube in the subsurface, given the above ground measurements)

","['convolutional-neural-networks', 'ai-design', 'labeled-datasets']","
I haven't done similar work with CNNs, but I can list a couple of approaches, maybe it helps you get started.
If I understand it correctly, the question is mostly about shapes of the data, so that's what I'll focus on as well.
Option A: You can keep your input as a 2D ""image"" with a single channel and just use 2D convolutions to expand to the required output size. This could work, but it doesn't incorporate the spatial dependency in the 3D output.
Option B: You can consider your 2D input to be 3D but with only one unit in the extra dimension, then use a couple of 3d transposed convolutions to get to the correct output shape. This is nice because you rely on 3D translation invariance which is probably what you want for the densities, but it would need to be tested. Also, in this case, You would only have one channel both in the input and the output, this doesn't mean you can only use one channel inside the NN, but you need to reduce it towards the end.
Option C: You can have your input as a 2D ""image"" with a single channel and do a couple of 2D convolutions to expand the number channels, then expand the dimensions of the tensors within the neural network and continue with 3D convolutions, considering the previous channels as the 3rd dimension and initially a single channel for the 3D ""image"". I could imagine this working, but the transition from channels (without spacial relations) to the 3rd dimension feels weird, and I'm not sure about the validity of this setup.
"
"What, exactly, do mlp(64,64) and mlp(64,128,1024) mean in PointNet, and how many input neurons does 1 (x,y,z) point have?","
I couldn't find out how to interpret the multilayer perceptron notation given in PointNet. Specifically, I am looking to find out what the numbers inside the parentheses of mlp(64,64) and mlp(64,128,1024) actually mean.
(I also have a 2nd question about PointNet MLP architecture, which I ask towards the end.)
Here's what I found online, which I believe applies:

https://towardsdatascience.com/deep-learning-on-point-clouds-implementing-pointnet-in-google-colab-1fd65cd3a263
There's a paragraph here that says

In this case MLP with shared weights is just 1-dim convolution with a kernel of size 1.

Here, a link is provided to explain more about the 1-dim convolution...
https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/
...and I follow this pretty well.

There's also this Matlab example...
https://www.mathworks.com/help/vision/ug/point-cloud-classification-using-pointnet-deep-learning.html
...which tells me to

set the classifier model input size to 64 and the hidden channel size to 512 and 256 and use the initalizeClassifier helper function...to initialize the model parameters.

inputChannelSize = 64; hiddenChannelSize = [512,256];

Then there's this link: https://www.researchgate.net/figure/Network-architecture-The-numbers-in-parentheses-indicate-number-of-MLP-layers-and_fig2_327068615
...in which they say,

The numbers in parentheses indicate number of MLP
layers

but this is, in my opinion, not written very well. Do they mean,

The notation mlp(64,64,128,256) means that the MLP has 4 layers, and each layer produces an output with 64, 64, 128, and 256 channels, respectively?



Here are my 2 questions about PointNet MLP notation / architecture:

What do each of the numbers in something like mlp(64,64,128,256) actually mean, and what do their positions mean? Are these numbers ONLY referring to the hidden layers, which includes the output layer? Also, are they referring to the number of channels, akin to the depth-wise feature layers of a CNN?

Finally, if your input is nx3 (as in, n (x,y,z) points), does this mean that the PointNet MLP takes an input of 1x3, meaning 1 input neuron, or 3 input neurons?


","['neural-networks', 'multilayer-perceptrons', 'notation']",
How to parallelize multi-agent DDPG (MADDPG),"
I am experimenting with MADDPG algorithm implemented in this repo. Since there were only a few agents (2-3) in the implementation (also in the original paper) steps like parameter updates, action prediction, etc. are done in a for loop. I want to increase the number of agents, say 10 or 30, and perform parallelization of the above-mentioned steps for all agents, i.e. I want to avoid for loops like this
for agent_idx in range(n_agents):
    ...
    ...

I tried Python Multiprocessing module with pool.map method but I am getting 'AttributeError: Can't pickle local object ..."". Below is code I am running to get a joint action prediction but it results in the error above.
def get_ind_action(i, obs_i):
    return actor_critic[i].act(obs_i) # returns an individual action for a given observation for ith agent

def get_joint_action(obs):
    pool = Pool()
    args_list = [[i, obs[i]] for i in range(n_agents)]
    joint_action = pool.map(get_ind_action, args_list)
    return joint_action
    

Here actor_critic is a list of neural networks of all agents, obs is the joint state observed by the centralized critic but each actor only sees its own state. The algorithm has the following architecture.

","['reinforcement-learning', 'python', 'deep-rl', 'ddpg', 'multi-agent-systems']",
"Different ways to calculate backpropagation derivatives, any difference?","
I'm studying error backpropagation in neural networks. I am interested in why we use only one path on the computational graph to get the value of the derivative for a weight? I ask the question because there are several paths in the computational graph to get the  derivative for a particular weight. Why do we only use a one value? Why don't we combine the values from all possible paths?
Schema:

Fromulas:
Normal path:
$$\frac{\partial E}{\partial w_{1,1}} = \frac{\partial E}{\partial Out} \cdot \frac{\partial Out}{\partial a_{1,1}}\cdot  \frac{\partial a_{1,1}}{\partial a_{0,1}}\cdot  \frac{\partial a_{0,1}}{\partial w_{1,1}}$$
Alternate path:
Normal path:
$$\frac{\partial E}{\partial w_{1,1}} = \frac{\partial E}{\partial Out} \cdot \frac{\partial Out}{\partial a_{1,2}}\cdot  \frac{\partial a_{1,2}}{\partial a_{0,1}}\cdot  \frac{\partial a_{0,1}}{\partial w_{1,1}}$$
Why don't we consider both derivatives or the sum of them?
","['machine-learning', 'deep-learning', 'backpropagation', 'gradient-descent']",
Is having near-duplicates in a training dataset a bad thing?,"
I am making a labeled dataset of images from web streams for a CNN classification. Pictures from the same stream are quite similar as far as background, but slightly different as far as the main object. The focus of what should be learned is in the main object.
My concern is that feeding similar images with the same features in the background will result into making those features more relevant and hence lower the weights of the features that matter.
So, should I be worried about removing similar images from a dataset, so that unrelated features are not learned?
An ideal answer would discuss the trade-offs.
I am aware of the practice of augmenting the training images by scaling/skewing/flipping them around. So it looks like people do it intentionally, but why?
I should also say that it's not about learning from a single stream, there are tons of them. So most images are very square-distant from one another since they are coming from different streams, except those ones that were snapped from the same stream.
","['convolutional-neural-networks', 'classification', 'image-recognition', 'data-preprocessing', 'quality-control']",
MNIST with fewer pixels? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



MNIST images are 28x28 pixels. Perhaps a silly question: is there anything like MNIST, but whose images have fewer pixels?
",['mnist'],
Modelling of output neuron for mixed features?,"
A dataset in artificial intelligence, in general, consists of some features (say $n$). Assume that $m$ among them are output features. I want to model this function using a neural network. So, input to my neural network is $n-m$ features and output is $m$ features. My question is about the output features.
If an output feature is a continuous random variable, then its corresponding output neuron can be trained to give continuous output. Similarly, if an output feature is a discrete random variable, then its corresponding output neuron can be trained to give discrete output.
But, I never came across the features that are mixed random variables. What is the nature of the output of the neuron that is intended to give the output value for a mixed random variable, which is neither discrete nor continuous in nature?
","['neural-networks', 'datasets']",
"Can I treat ""experience"" in reinforcement learning as ""training data"" in statistical learning?","
Statistics is a branch of mathematics that extracts useful information from data. The data is generally called as ""training data"" in statistical (machine) learning.
Consider the following paragraph from the section 1.1 Reinforcement Learning of CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM of the textbook Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto.

Reinforcement learning
is different from supervised learning, the kind of learning studied in
most current research in field of machine learning. Supervised
learning is learning from a training set of labeled examples provided
by a knowledgable external supervisor. Each example is a description
of a situation together with a specification—the label—of the correct
action the system should take to that situation, which is often to
identify a category to which the situation belongs. The object of this
kind of learning is for the system to extrapolate, or generalize, its
responses so that it acts correctly in situations not present in the
training set. This is an important kind of learning, but alone it is
not adequate for learning from interaction. In interactive problems it
is often impractical to obtain examples of desired behavior that are
both correct and representative of all the situations in which the
agent has to act. In uncharted territory—where one would expect
learning to be most beneficial—an agent must be able to learn from its
own experience.

You can observe that, training data in machine learning, if we model it in a proper format, can be for reinforcement learning. But, may not be complete and practical.
I am asking this question in the view of statistical learning rather than machine learning alone. training data in statistical learning can be understood to any data, at any time instant, useful in learning.
Then, is it perfectly fine to always interpret experience in reinforcement learning as training data in statistical learning?
","['reinforcement-learning', 'comparison', 'statistical-ai']","
The main similarity between reinforcement learning experience and supervised learning datasets, is that both consist of a set of records. These records are commonly expressed as vectors of numbers for use in the algorithms. In addition, reinforcement learning that uses neural networks (or other function approximation) will typically implement some variant of supervised learning internally.
There are a few key differences between a prepared dataset for supervised learning and the experience in reinforcement learning. There are exceptions to these, but these are the usual case:

A supervised learning dataset has a fixed target value to learn by association, for each entry, e.g. a class or regression value. An individual reinforcement learning experience does not - the raw tuple of state, action, reward, next state $(s,a,r,s')$ must be processed in some way to obtain a useful training target value, and this processing is not static. When training in reinforcement learning for optimal control, even with historical experience, the target values must constantly be re-assessed.

Reinforcement learning is in part a design for actively collecting experience. There is no equivalent in supervised learning where the dataset is a given.

Reinforcement learning experience arrives in groups of $(s, a, r, s')$ - state, action, reward, next state - such groups of related data within a record are called tuples. The RL records are often correlated with each other, at least initially because each time step changes things only slightly, and that can be bad when combined with supervised learning which usually assumes uncorrelated data. In supervised learning you will often have a deliberate shuffling or randomisation of dataset order to protect against this. Experience replay in deep RL is a related idea to protect internal neural networks from being exposed to training samples in sequences with correlated values.


It is possible to apply reinforcement learning to supervised learning problems in theory. You can do this by making the agent guess each labelled value as an action, and reward it with negative the loss from the supervised learning. This is generally a bad idea because it is very inefficient, and there is no matching concept of state transitions in the supervised learning problem (the agent cannot impact what state is next due to its guess). However, the fact that this is possible with very little modification to the reinforcement learning agent shows how general reinforcement learning is as a learning approach.
The inverse is not really true, you cannot normally adjust a supervised learning algorithm so that it can solve a reinforcement learning problem from the given experience. There are some edge cases, such as when learning only from previous experience to assess an existing policy, or to learn a control algorithm which only cares about immediate reward. In which case you could use reinforcement learning theory to help construct a fixed dataset and give that to a supervised learning algorithm. However, by far the most common approach is to use supervised learning approaches (e.g. a neural network) as components of the agent, and rely on reinforcement learning to generate data for them on the fly.
"
What are strategies for data driven weights initialization?,"
I am beginner in deep learning and currently training a few neural networks (Pytorch) for problems in audio and speech. For my tasks, simple feed-forward networks are working well enough. I use basic layers like Linear, ReLU and Softmax with nll loss. I have tested a few initialization schemes provided by Pytorch and noticed that initialization has significant (but not high) effect in the speed of training and the final accuracy of the model. I am currently using torch.nn.init.kaiming_uniform_ for initialization.
In my understanding, all these are data independent initialization schemes. I would like to try something that is data dependent. I saw a few pre-training strategies with unsupervised learning followed by supervised learning, but they seem overly complicated.
I am looking for something simple where I can use (preferably a fraction of the) training data to 'tweak' weights to better positions before the training. Are there any such strategies?
Addendum-1:
Current initialization schemes (AFAIK) are mostly random values with control over range or energy to prevent values from dying down or blowing up. My aim is to further improve the starting point of training by taking account training data (or at least some of it). I am thinking of something like this. We pass a few batches of training data through the initial network and collect statistics on neuron outputs. Based on this, we identify the misbehaving neurons and tweak the weights and biases to reduce such issues so as to improve the training speed or accuracy. Is there anything of that kind?
","['deep-learning', 'weights-initialization']",
Why disentangling the features of variation in representation?,"
Consider the following excerpt from abstract of the research paper titled Better Mixing via Deep Representations by Yoshua Bengio et al.

It has been hypothesized, and supported with experimental evidence,
that deeper representations, when well trained, tend to do a better
job at disentangling the underlying factors of variation.

In general, as per my current knowledge, we want to preserve the factors that contribute to variation in the final representation. But, the abstract is contrary to that. Where am I going wrong? Why there is a need to disentangle the factors of variation?
","['datasets', 'papers', 'features', 'representation-learning']",
Cost functions for reducing Tensors to 1-dimensional arrays?,"
I'm interested in the IT side, here, specifically how I most efficiently store a tensor in a one dimensional data structure. My assumption is that certain approaches will be more expensive than others, but I'd like to be able to validate that assumption, and show it to be wrong. Is there any work on this subject?
","['reference-request', 'tensor']",
What is the optimal score for Tic Tac Toe for a reinforcement learning agent against a random opponent?,"
I guess this problem is encountered by everyone trying to solve Tic Tac Toe with various flavors of reinforcement learning.
The answer is not ""always win"" because the random opponent may sometimes be able to draw the game.  So it is slightly less than the always-win score.
I wrote a little Python program to calculate that.  Please help verify its correctness and inform me if it has bugs or errors.
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'tic-tac-toe']","
This blog post suggests that when playing against a random opponent, if the agent goes first, the win rate is 97.8%, and if they go second, the win rate is 79.6% (and the rest are draws).
"
"Tensorflow object detection model total loss starts out good, but suddenly explodes up to high loss numbers","
I'm training a Tensorflow object detection model with approx. 7500 images of two classes, which contains approx. 10,000 classes per class. I'm using Tensorflow 2.6.0, in case that is relavent. I am using Single Shot Detector (with a ResNet 50 backbone). The image dimensions are 1024 x 1024, and the batch size is set to 2. Training is being done on Ubuntu 20.04 with a GeForce RTX 2080 Super (GPU).
After beginning training, the process is starting out at loss numbers to be expected:
INFO:tensorflow:{'Loss/classification_loss': 2.1305692,
 'Loss/localization_loss': 0.6402807,
 'Loss/regularization_loss': 1.407957,
 'Loss/total_loss': 4.178807,
 'learning_rate': 0.014666351}
I0903 16:56:21.947736 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 2.1305692,
 'Loss/localization_loss': 0.6402807,
 'Loss/regularization_loss': 1.407957,
 'Loss/total_loss': 4.178807,
 'learning_rate': 0.014666351}
INFO:tensorflow:Step 200 per-step time 0.447s
I0903 16:57:06.592366 140581900665792 model_lib_v2.py:698] Step 200 per-step time 0.447s
INFO:tensorflow:{'Loss/classification_loss': 1.2596315,
 'Loss/localization_loss': 0.6752764,
 'Loss/regularization_loss': 3.0123177,
 'Loss/total_loss': 4.9472256,
 'learning_rate': 0.0159997}
I0903 16:57:06.592768 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 1.2596315,
 'Loss/localization_loss': 0.6752764,
 'Loss/regularization_loss': 3.0123177,
 'Loss/total_loss': 4.9472256,
 'learning_rate': 0.0159997}
INFO:tensorflow:Step 300 per-step time 0.452s
I0903 16:57:51.830375 140581900665792 model_lib_v2.py:698] Step 300 per-step time 0.452s
INFO:tensorflow:{'Loss/classification_loss': 1.0455683,
 'Loss/localization_loss': 0.5895866,
 'Loss/regularization_loss': 3.0799737,
 'Loss/total_loss': 4.715129,
 'learning_rate': 0.01733305}
I0903 16:57:51.830749 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 1.0455683,
 'Loss/localization_loss': 0.5895866,
 'Loss/regularization_loss': 3.0799737,
 'Loss/total_loss': 4.715129,
 'learning_rate': 0.01733305}

Up until about step 16,800, the loss is decreasing to these numbers:
INFO:tensorflow:{'Loss/classification_loss': 0.5526215,
 'Loss/localization_loss': 0.28333753,
 'Loss/regularization_loss': 0.24686696,
 'Loss/total_loss': 1.0828259,
 'learning_rate': 0.037849143}
I0903 18:59:14.666097 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 0.5526215,
 'Loss/localization_loss': 0.28333753,
 'Loss/regularization_loss': 0.24686696,
 'Loss/total_loss': 1.0828259,
 'learning_rate': 0.037849143}
INFO:tensorflow:Step 16700 per-step time 0.446s
I0903 18:59:59.247199 140581900665792 model_lib_v2.py:698] Step 16700 per-step time 0.446s
INFO:tensorflow:{'Loss/classification_loss': 0.4649979,
 'Loss/localization_loss': 0.28323257,
 'Loss/regularization_loss': 0.2433301,
 'Loss/total_loss': 0.9915606,
 'learning_rate': 0.037820127}
I0903 18:59:59.247609 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 0.4649979,
 'Loss/localization_loss': 0.28323257,
 'Loss/regularization_loss': 0.2433301,
 'Loss/total_loss': 0.9915606,
 'learning_rate': 0.037820127}
INFO:tensorflow:Step 16800 per-step time 0.446s
I0903 19:00:43.835976 140581900665792 model_lib_v2.py:698] Step 16800 per-step time 0.446s
INFO:tensorflow:{'Loss/classification_loss': 0.43402833,
 'Loss/localization_loss': 0.1641234,
 'Loss/regularization_loss': 0.24129395,
 'Loss/total_loss': 0.8394457,
 'learning_rate': 0.03779093}
I0903 19:00:43.836373 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 0.43402833,
 'Loss/localization_loss': 0.1641234,
 'Loss/regularization_loss': 0.24129395,
 'Loss/total_loss': 0.8394457,
 'learning_rate': 0.03779093}

However, starting at about 16,900, the model total_loss rapidly increases, up to numbers even higher than are shown below:
INFO:tensorflow:Step 16900 per-step time 0.446s
I0903 19:01:28.390861 140581900665792 model_lib_v2.py:698] Step 16900 per-step time 0.446s
INFO:tensorflow:{'Loss/classification_loss': 0.5590624,
 'Loss/localization_loss': 0.5160909,
 'Loss/regularization_loss': 338.40286,
 'Loss/total_loss': 339.478,
 'learning_rate': 0.03776155}
I0903 19:01:28.391232 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 0.5590624,
 'Loss/localization_loss': 0.5160909,
 'Loss/regularization_loss': 338.40286,
 'Loss/total_loss': 339.478,
 'learning_rate': 0.03776155}
INFO:tensorflow:Step 17000 per-step time 0.445s
I0903 19:02:12.936022 140581900665792 model_lib_v2.py:698] Step 17000 per-step time 0.445s
INFO:tensorflow:{'Loss/classification_loss': 0.7908556,
 'Loss/localization_loss': 0.7274248,
 'Loss/regularization_loss': 858.3554,
 'Loss/total_loss': 859.87366,
 'learning_rate': 0.037731986}
I0903 19:02:12.936432 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 0.7908556,
 'Loss/localization_loss': 0.7274248,
 'Loss/regularization_loss': 858.3554,
 'Loss/total_loss': 859.87366,
 'learning_rate': 0.037731986}
INFO:tensorflow:Step 17100 per-step time 0.452s
I0903 19:02:58.127156 140581900665792 model_lib_v2.py:698] Step 17100 per-step time 0.452s
INFO:tensorflow:{'Loss/classification_loss': 0.7510178,
 'Loss/localization_loss': 0.49337074,
 'Loss/regularization_loss': 2617.2888,
 'Loss/total_loss': 2618.5332,
 'learning_rate': 0.03770224}
I0903 19:02:58.127575 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 0.7510178,
 'Loss/localization_loss': 0.49337074,
 'Loss/regularization_loss': 2617.2888,
 'Loss/total_loss': 2618.5332,
 'learning_rate': 0.03770224}
INFO:tensorflow:Step 17200 per-step time 0.445s
I0903 19:03:42.625258 140581900665792 model_lib_v2.py:698] Step 17200 per-step time 0.445s
INFO:tensorflow:{'Loss/classification_loss': 1.1258743,
 'Loss/localization_loss': 0.45634705,
 'Loss/regularization_loss': 394886900.0,
 'Loss/total_loss': 394886900.0,
 'learning_rate': 0.037672307}
I0903 19:03:42.625638 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 1.1258743,
 'Loss/localization_loss': 0.45634705,
 'Loss/regularization_loss': 394886900.0,
 'Loss/total_loss': 394886900.0,
 'learning_rate': 0.037672307}
INFO:tensorflow:Step 17300 per-step time 0.445s
I0903 19:04:27.112154 140581900665792 model_lib_v2.py:698] Step 17300 per-step time 0.445s
INFO:tensorflow:{'Loss/classification_loss': 0.57859087,
 'Loss/localization_loss': 0.53405523,
 'Loss/regularization_loss': 383440770.0,
 'Loss/total_loss': 383440770.0,
 'learning_rate': 0.037642203}
I0903 19:04:27.112533 140581900665792 model_lib_v2.py:701] {'Loss/classification_loss': 0.57859087,
 'Loss/localization_loss': 0.53405523,
 'Loss/regularization_loss': 383440770.0,
 'Loss/total_loss': 383440770.0,
 'learning_rate': 0.037642203}

What could be the cause of this, and what would be the best way to go about fixing it?
","['deep-learning', 'tensorflow', 'training', 'object-detection']",
How to embed game grid state with walls as an input to neural network,"
I've read most of the posts on here regarding this subject, however most of them deal with gameboards where there are two different categories of single pieces on a board without walls etc.
My game board has walls, and multiple instances of food. There are 8 different categories,
Walls, enemy food, my food, enemy powerup, my powerup, attackable enemies, threatening enemies, and current teammate.
I have one hot encoded all of this data into a tensor of size (8, 16, 32) where (16, 32) are the sizes of the game grid. However I'm not sure whether this is appropriate since many of the categories have multiple occurrences of each category in a single  (walls, food). Is it appropriate to use one hot encoding to represent categories in spatial data, where multiple one's may be present?
The alternative I was considering was to use a CNN, however many posts have said it is inappropriate for one hot data. My reasoning was that since the data is a abstract Boolean grid representing the RGB frames, it might be appropriate.
Does anyone have any suggestions as to the best way to represent a spatial Boolean grid representing multiple categories for input to a network?
","['convolutional-neural-networks', 'deep-rl', 'game-ai', 'double-q-learning', 'one-hot-encoding']","
The way you describe with one hot encoding is correct.
Note that how the state is encoded is a separate question from the neural network, so I'm not sure what convolutional neural networks have to do with the question. In the famous atari game example, the input is a sequence of RGB images; a cnn is used to process the images. In your example you probably just want to use a regular Dense network, as your input is just the one hot encoding and not images.
"
REINFORCE differentiation on sum or single value?,"
I'm currently learning Policy-gradient Methods for RL and encountered REINFORCE algorithm. I learned from this site : https://towardsdatascience.com/policy-gradient-methods-104c783251e0 that the gradient of the objective function is calculated as follows:

From what I understand $\sum_{t=0}^{H}\nabla_{\theta}\log{\pi_{\theta}(a_{t}|s_{t})}$ is the sum through the entire trajectory and $\pi_{\theta}(a_{t}|s_{t})$ is the policy of the agent at time step $t$. However in Suton's book the gradient objective is defined differently.

There is only $\nabla \ln{\pi(A_t | S_t)}$ at time step $t$ and no sum of all time steps. So does the algorithm not consider the policy for the whole trajectory when updating? Only a single-step policy?
Furthermore, there is $\gamma^{t}$ (discounted reward) term in the latter and not the former. What is the reason for that?
Hopefully, someone can help me clarify this.
","['reinforcement-learning', 'math', 'reinforce']",
What should the value of $ρ$ in the $w(n+1) = w(n) + \rho*\text{error}(i)x(i)$ formula of Least Mean Squares be?,"
I am trying to better understand the Least Mean Squares algorithm, in order to implement it programmatically.
If we consider its weight updating formula $$w(n+1) = w(n) + \rho * \text{error}(i)x(i),$$ where $w(n + 1)$ is the new weight of the classifier function, $w(n)$ is its current weight and $x(i)$ is the $i$th element of a training dataset, what should $\rho$ be?
From what I have found online, $ρ$ is supposed to be $0 < \rho < \frac{2}{trace(X^TX)}$, where $X$ is a matrix with all the training data the algorithm has processed at that point. One idea that I had, was to take $\rho = \frac{1}{trace(X^TX)} < \frac{2}{trace(X^TX)}$, but I do not know if that is correct. Also, one characteristic that this value has is that it changes with each iteration of the algorithm, as more samples are added to matrix $X$.
So, what is a good value for $\rho$? Should it change during the execution of the algorithm or should it stay the same?
","['classification', 'math']",
Can I extend Graph Convolutional Networks to graphs with weighted edges?,"
I'm researching spatio-temporal forecasting utilising GCN as a side project, and I am wondering if I can extend it by using a graph with weighted edges instead of a simple adjacency matrix with 1's and 0's denoting connections between nodes.
I've simply created a similarity measure and have replaced the 1's and 0's in the adjacency with it.
For example, let's take this adjacency matrix
$$A=
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{bmatrix}
$$
It would be replaced with the following weighted adjacency matrix
$$
A'=
\begin{bmatrix}
0 & 0.8 & 0 \\
0.8 & 0 & 0.3 \\
0 & 0.3 & 0
\end{bmatrix}
$$
As I am new to graph NN's, I am wondering whether my intuition checks out. If two nodes have similar time-series, then the weight of the edge between them should be approximately 1, right? If the convolution is performed based on my current weights, will this be incorporated into the learning?
","['deep-learning', 'geometric-deep-learning', 'graph-neural-networks', 'graphs']","
According to the definition of Graph Neural Networks taken from here GCN perfroms an operation of the form:
$$
f (H^{(l)} ,A) = \sigma(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)})
$$
Where $H^{(l)}$ is the input to GCN layer, $\tilde{A} = A + I$ is the adjacency matrix with self loops added and $\tilde{D}$ is a degree matrix, corresponding to the adjacency matrix $\tilde{A}$ (on the diagonals there are sums over the columns of $\tilde{A}$).
This definition is for matrix with $1$, if there is an edge between $i$ and $j$, and $0$ otherwise. For matrix of this form normalized Graph Laplacian $\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$ is
guaranteed to be positive semidefinite matrix.
One can extend the definition for arbitrary values of $a_{ij} \in A$. But there won't be guarantees, that graph Laplacian will be well-defined.
"
What is the formal definition for manifold in artificial intelligence?,"
We come across the word ""manifold"" in artificial intelligence, especially in the domains where learning is done based on data instances.
What is the formal definition for manifold?
","['machine-learning', 'definitions']","
Manifold is basically a geometric object where every small region can be mapped to a euclidean space(means manifold is locally euclidean). Think of a donut, here any small region can  be mapped to a euclidean space shown in this image:

In this above picture, $M$ is the manifold, $\phi_\alpha,\phi_\alpha$ is the mapping function, $U_\alpha, U_\beta$ is two open sets(small local regions). This donut is an example of a manifold. Similarly, we can think of circles, spheres, paraboloids, $\mathbb{R}^2$, $\mathbb{R}^3$, etc. as a manifold because they all satisfy the above criteria(they are locally euclidean).
Now, the question is why we are interested in manifolds in machine learning. In many machine learning applications, the data we interpret is laying on a manifold or non-Euclidean domain. For example, in astrophysics the observational data often time lies on a spherical domain. If we want we perform convolution over this spherical manifold to extract features, we can't just apply 2D convolution since we have to take account of parallel transport, gauges, symmetries, etc. Similarly, we may want to perform convolution over more complex shapes like those figures to extract features.

There are methods like guage equivalent mesh CNN, geodesic CNN, etc to deal with such kind of data distribution.
Graphs also lie on a non-Euclidean domain since the distance between any two nodes is not a straight line we have to travel through the graph and count the number of edges to measure distance. There are many applications where data lies on a graph, for example, drug-drug interaction, community detection, molecule structure, friendship network, recommendation system, traffic forecasting, etc.

To perform convolution over graphs we have methods like ChebNet, GraphSAGE, graph attention network, etc.
Notes:
1) Parallel transport:  One basic problem occurs when we try to compare two vectors of two different points over the same manifold is that those two vectors belong to different Euclidean spaces (see the first figure), thus we can not directly compare them. Parallel transport provides a mechanism to move vectors over a manifold and analysis them. But note that parallel transport depends on the path means the result of the parallel transport is path-dependent.
2) Guage:  Guage is like a measurement apparatus to specify the tangent vector on the tangent space of a manifold.

References:

Geometric Deep Learning: Grids, Groups, Graphs, Geodesics

To learn about geometric deep learning this is very good website, they also contain extraordinary video lectures on GDL


Note that: I intentionally skipped the rigorous mathematical definition of a manifold while trying to convey the underlying meaning. Please, let me know if you want to know more about open sets, closed sets, topological spaces, topological manifold, charts, atlas, etc.
"
"When can we call a loss function ""adaptive""?","
A loss function is a measure of how bad our neural network is. We can decrease the loss by proper training.
I came across the phrase ""adaptive loss function"" in several research papers. For example: consider the following excerpt from the ""Introduction"" of the research paper titled Generative Adversarial Text to Image Synthesis by Scott Reed et al.

By conditioning both generator and discriminator on side information, we can naturally model this phenomenon since the discriminator network acts as a ""smart"" adaptive loss function.

When can we denote a loss function as adaptive? Is it a mathematical property or is solely based on the context?
","['machine-learning', 'objective-functions', 'generative-adversarial-networks']",
How to handle an unbalanced dataset when training object detection algorithms?,"
I am training an object detection model, and I have some very highly unbalanced data annotations. I have almost 11,000 images, all with dimensions of 1024 $\times$ 1024.
Within those images I have the following number of annotations:
*Class 1 - 40,000
*Class 2 - 25,000
*Class 3 - 900
*Class 4 - 500

This goes on for a few more classes.
As this is an object detection algorithm that was annotated with the annotation tool Label-img, there are often multiple annotations on each photo. Do any of you have any recommendations as to how to handle fine-tuning an object-detection algorithm on an unbalanced dataset? Currently, collecting more imagery is not an option. I would augment the images and re-label, but since there are multiple annotations on the images, I would be increasing the number of annotations for the larger classes as well.
Note: I'm using the Tensorflow Object Detection API and have downloaded the models and .config files from the Tensorflow 2 Detection Model Zoo.
","['tensorflow', 'datasets', 'object-detection', 'data-labelling', 'imbalanced-datasets']",
What are some machine learning frameworks for supervised clustering?,"
I have a task where I need to take ""data points"" which consist of collections of items. Each item needs to be categorised according to predefined categories. That's the easy part - my solution is to train a deep neural network with cross entropy loss. By the way, the reason I don't classify each item separately is because they acquire their meaning when they come together as a set.
The hard part is that each of these items also have a cluster label. Each cluster can only have items of one category in it, and there can be any number of clusters. Unsupervised clustering methods (applied after the neural network does the categorisation) work fairly well, but not well-enough for my needs. I'd like to:
A. Make use of the fact that I have the ground truth labelling for these clusters
B. Leverage my deep neural network because a lot of the ""reasoning"" required to solve the classification task will be conducive to the clustering task.
Answers which address at least one of those are useful to me.
EDIT
I realised I might be confusing people with this concept of cluster ""labels"". To clarify, this is no different than the standard way a classical unsupervised clustering algorithm might return its results. If I have N data points and feed them to a clustering algo, the algo might return N labels, one for each data point, and each of which are integers in [0, C-1] where C is the number of clusters. In my example we have the labels for a training dataset and want to make use of them during training. We cannot use softmax + cross-entropy loss because the cluster labels are permutation invariant.
","['neural-networks', 'supervised-learning', 'clustering']",
Is there a crossover that also considers that every index in the vector also influences the fitness function?,"
Is there a crossover that also considers that every index in the vector also influences the cost function?
I have two vectors $v_1=[A_1, A_2, A_3, A_4, A_5]$ and $v_2=[A_5, A_3, A_2, A_1, A_4]$.
The fitness function considers the index where an element is located. So, basically, every vector represents a matching solution. Using a recombination method would deliver a new combination, but it won't be close to the previous solution, nor would they consider what makes the parents better than the other solution.
In TSP, the indices don't really matter on the sequence of cities.
","['genetic-algorithms', 'crossover-operators', 'fitness-functions', 'travelling-salesman-problem', 'chromosomes']","
Really you're entering the world in which you probably want to develop genetic operators that have meaning in your domain. You mention TSP, and correctly point out that the absolute position within the chromosome doesn't matter. There are other permutation problems where this isn't true. The Quadratic Assignment Problem (QAP) is one example. Like TSP, QAP solutions are represented as permutations of integers, and there are plenty of known recombination operators that work on permutations. But you need different operators for these cases. What works well for TSP won't be very good for QAP, and vice versa.
For reference, a good place to start might be the Cycle Crossover (CX) operator. It's defined on permutations, and basically works by looking for ""cycles"" -- subsets of the indices where the two parents share the same values. For example, if you have parents
P1=<2 3 7 1 4 6 5 8> 
P2=<4 1 2 6 8 5 3 7>

there's a cycle at the index set {0, 2, 4, 7}. Both parents contain the same four values at those positions in the string -- 2, 7, 4, and 8. You could create two new offspring by exchanging the values at those positions, yielding
C1=<4 3 2 1 8 6 5 7> 
C2=<2 1 7 6 4 5 3 8>

This gives me two children, both of whom have the property that they inherited information from their parents related to the absolute position of each value in the string.
That may or may not be exactly what you need for your problem. Maybe you don't have true permutations, or maybe something about that operator doesn't work well with your particular problem. The main point is that you need to build operators with intent. The idea has been called ""respectful recombination"". Understanding what's important in your representation and devising operators that try to respect that property as they pass down information is the name of the game.
"
Is the following a typo or am I understanding wrongly regarding discriminator?,"
Consider the following paragraph from the section 3: Background of the research paper titled Generative Adversarial Text to Image Synthesis by Scott Reed et al.

Goodfellow et al. (2014) prove that this minimax game has a global
optimium precisely when $p_g = p_{data}$, and that under mild
conditions (e.g. G and D have enough capacity) $p_g$ converges to
$p_{data}$. In practice, in the start of training samples from D are
extremely poor and rejected by D with high confidence. It has been
found to work better in practice for the generator to maximize
$\log(D(G(z)))$ instead of minimizing $\log(1 −D(G(z)))$

I am guessing the bolded portion should be replaced by  from G. Am I correct? If not, where am I going wrong?
","['papers', 'generative-adversarial-networks']",
"How to represent ""terminate episode"" for Knapsack problem with a Pointer Network?","
I am currently implementing a Pointer Network to solve a simple Knapsack Problem. However, I am a bit puzzled over the correct (or common, or ""best"") way to give the agent the option to stop taking the item (terminate episode). Currently, I have done it in 2 ways, adding raw dummy features or adding encoded dummy features (dummy features are all zeros). If the agent selects the dummy item, then the agent will stop taking the item anymore and the episode is terminated.
I trained both methods for 500K episodes and evaluated their performance on a single predefined test case in each episode, after adding the gradient. I found that concatenating dummy features with the encoded features yielded a higher score earlier, but also scored 0 very often. On the other hand, adding the dummy features to the raw features learned to maximize the score very slowly. Therefore, my questions are:

Is adding the raw dummy features make learning slower because of additional encoding layer learning?

What is the most correct (or common or arguably best) way to give the agent the option to terminate the episode (in this case stop taking item)?


","['reinforcement-learning', 'ai-design', 'knapsack-problem']","
You can find information similar to exposed by Neil, but with more theoretical detail, in the book Deep Learning (Goodfellow et al., 2016) in the chapter 10 (Recurrent networks), more specifically in 10.2.3 Recurrent Networks as Directed Graphical Models and other subchapters.
Additional, related with pointer networks there are people changing the LSTM with Transformer (Learning Heuristics for the TSP by PolicyGradient, 2018)
"
Are there any benefits of adding attention to linear layers?,"
Is attention useful only in transformer/convolution layers? Can I add it to linear layers? If yes, how (on a conceptual level, not necessarily the code to implement the layers)?
","['neural-networks', 'pytorch', 'attention', 'dense-layers']",
Are there theoretically linguistic inputs that could send an NLP algorithm into infinite loops or break the chatbot?,"
I was asked an interesting question today by a student in a cybersecurity and information assurance program related to getting spammed by chatbots on snapchat.  He's tried many conventional means of blocking them, but he's still getting overwhelmed:

Theoretically, are there lines of code that could disrupt processing, such as commands or syntactic symbols?

My sense is no — the functions would be partitioned such that linguistic data would not execute. But who knows.

Many programmers are sloppy.
I've had friends in video game QA produce controller inputs that programmers claim is impossible — until demonstrated.


Theoretically, is it possible to ""break"" a chatbot in the sense of the Voight-Kampff test thought experiment?

This was, of course, popularized via one of the most famous films on AI, BladeRunner, adapted from one of the most famous books, ElectricSheep, and extended recently via WestWorld. In these contexts, it's a psychological test designed to send the automata into loops or errors.
My question here is not related to ""psychology"" as in those popular media treatments, but linguistics:

Are there theoretically linguistic inputs that could send an NLP algorithm into infinite loops or produce errors that halt computation?

My guess is no, all the way around, but still a question potentially worth asking.
","['chat-bots', 'programming-languages', 'computational-linguistics']","
While it is certainly possible to have NLP algorithms ending up in infinite loops, chatbots will typically not be affected by this.
A first-year pitfall you learn is in the construction of grammars. If you do a top-down analysis of a sentence, the following grammar rule will send it into an infinite loop:

NP -> NP of NP | det N | N

This allows a noun phrase to be expanded to ""noun phrase of noun phrase""; and the parser next tries to expand the non-terminal symbol 'NP', which handily expands to a rule which has the very same symbol at the beginning.
However, modern day chatbots don't tend to use parsers, as their input is not commonly well-formed enough to allow application of grammars. They either use pattern matching (Eliza-style), or machine learning, neither of which would be susceptible to this issue.
And commercial chatbots are typically tested with all kinds of junk input to make sure they don't break or crash (In my previous job I designed chatbots for five years).
One possibility I can think of is if the pre-processing step is poorly coded, that using eg non-ASCII characters or extremely long nonsense words etc might lead to problems (eg buffer overflows), but modern programming languages make it increasingly difficult to actually break anything this way. And as you rightly say, you would separate input from executable code, so no Bobby Tables issues should happen.
"
Delayed state observation or caching action in OpenAI gym. Can it still learn?,"
I am planning to use OpenAI gym for my experiment in real life.
In my experiment design, by the limits of a real-life scenario, I can only receive the state information or the rewards about 2-3 timesteps behind when the action has happened (in OpenAI gym term, ~3 cycles of step(action) function has occurred). For example, by the time the state at timestep i is observed, an action at timestep i+3 would have happened.
From how I perceive the function, step(action), is that it needs to return next_state, reward, done every step. And the agent will learn from state -> action -> next state -> reward tuple. So I was wondering if can I cache the action for future use along with the state with the correct time step in OpenAI gym? or delay the state observation/reward instead? Could the OpenAI be able to learn?
I am experimenting with PPO TD3 SAC which all uses actor-critic networks. Would the network eventually be trained well enough to the point where it would still perform well with the delayed state observation?
","['reinforcement-learning', 'open-ai', 'gym']",
PPO when does the update happen?,"
In many places, it says PPO and Actor-Critic methods in general use TD-updates, but in the loss function for PPO, the Value function loss component uses the difference between output of the value function and the value target, which I can only assume is the discounted sum of rewards that can only be obtained at the END of the episode?
So this might be a moment of stupidity for me, but

Is the value target in PPO set only at the end of the episode using the discounted sum of rewards? or is there a secret way of setting these value targets that I am missing?

If a learning update indeed takes place every learning step (before the end of the episode), then how does this TD-learning happen - does it use some other approximate of the value target?


Thank you.
Please help.
Sincerely,
a frustrated student
","['reinforcement-learning', 'deep-rl', 'actor-critic-methods', 'value-functions', 'proximal-policy-optimization']",
Image recognition neural network: scaling and rotation,"
Are there some effective and robust solutions for scaling and rotation for image recognition with the neural networks (NN)?
I see tons of sources on the Web with explanation how neural network is used for image recognition, but all of them avoiding the topic of scaled or rotated images. The network trained for patterns won’t recognize it if the pattern scaled or rotated.
Of course, there are some intuitive/naive workarounds/approaches:

Brute force – you can rotate and scale image until NN recognizes it. Too much expensive.
You may teach NN for all cases of rotated and scaled image, could be hard and will lead to NN weakening.
You may teach NN that some images (which are rotation and scale of the original image) are clusters and teach to recognize clusters and interpolate/extrapolate them. A little bit tricky in coding and debugging.
For rotation you can move to polar coordinates, this gives a kind of invariant both for recognizing patterns and building histograms for specific portions of the image. But for this you need to recognized the pivot point and again this is quite expensive.

Are there any better solutions, ideas, hints, references?
(I read some answers there to the rotational problem, but what I saw doesn't cover the topic).
",['image-recognition'],
Expression Transfer Deep Learning Problem,"
I have old video and I want to keep the person's face in the video but I want to transfer my facial expressions to that video. Is there any better alternative to first order motion model for that task ? I tried deepfacelab but it has kind of steep learning curve
","['deep-learning', 'generative-adversarial-networks', 'style-transfer']",
Do LSTM in tensorflow work sequentially or in parallel,"
I have a basic understanding how a cell and a layer of an LSTM works. However, I get confused by what ""number of units"" (as termed in tensorflow) exactly means. A unit is, as far as I understand one ""instance"" of a LSTM, consisting of $t$ cells, for a sequence of length $t$. When I have more than one unit, do these work in parallel (i.e. 10 units not interacting with each other) or sequentially (ouput of unit 1 is the input of unit 2 and so on).
","['tensorflow', 'long-short-term-memory']",
Is there a proper initialization technique for the weight matrices in multi-head attention?,"
Self-attention layers have 4 learnable tensors (in the vanilla formulation):

Query matrix $W_Q$
Key matrix $W_K$
Value matrix $W_V$
Output matrix $W_O$

Nice illustration from  https://jalammar.github.io/illustrated-transformer/

However, I do not know how should one choose the default initialization for these parameters.
In the works, devoted to MLP and CNNs, one chooses xavier/glorot or he initialization by default, as they can be shown to approximately preserve the magnitude in the forward and backward pass, as shown in these notes.
However, I wonder, whether there is some study of good initialization for Transformers. The default implementation in Tensorflow and PyTorch use xavier/glorot.
Probably, any reasonable choice will work fine.
","['transformer', 'attention', 'weights', 'weights-initialization']","
IMO xavier/glorot is the correct way to initialize the $W_Q$ and $W_K$ matrices.
In section 3.2.1 of the transformer paper the authors explain why they would want the attention logits to be with unit standard deviation. So assuming the input $x$ is with unit std (which it probably is) you want your queries and keys to also have unit std, which is ensured by the xavier initialization.
For the $W_V$ and $W_O$ matrices I am not really sure what is the correct approach. You are applying layer norm to the output z to scale it to unit std (getting ready for the next layer) so as far as the forward pass is concerned the initialization probably doesn't matter. I suppose it is a good idea to have again the xavier initialiaztion because of the backward pass.
Usually I've seen people add bias only to the $W_O$ weights and leave the $W_Q$, $W_K$ and $W_V$ with bias=False. Cannot comment on why they do it this way, but I think that it is ok all the bias to be concentrated in the $W_O$ layer.
You may also consider some specific initialization of the $W_O$ weights as mentioned by Andrej Karpathy here, although I could not find this referenced anywhere.
I have also seen batching the $W_Q$, $W_K$ and $W_V$ matrices together in a single forward pass, but I guess in this case you should maunally set the variance for the initialization as xavier would be incorrect:
qkv = nn.Linear(in_dim, 3 * embed_dim, bias=False)
# nn.init.xavier_normal_(qkv) # imo is incorrect
nn.init.normal_(qkv, mean=0., std=np.sqrt(2 / (in_dim+embed_dim)))
# ...
queries, keys, values = qkv(x).chunk(chunks=3, dim=-1)

You can also read a more detailed blog post that I wrote about the Transformer model.
"
What is meant by correlation structure?,"
I know only about the Pearson's correlation coefficient in literature.
Covariance between two random variables $X$ and $Y$ is defined as
$$Cov[X, Y] = \mathbb{E}[(X - \mathbb{E}[X])(Y-\mathbb{E}[Y])]$$
(Linear) Correlation between two random variables $X$ and $Y$ is defined as
$$Corr[X, Y] = \dfrac{Cov[X, Y]}{\sigma(X)\sigma(Y)}$$
Covariance is a measure of association between two random variables whereas correlation measures how much dependent they both are on each other.
Consider the following excerpt mentioning ""correlation structure"" from section 2: General Design Principles of the research paper titled Rethinking the Inception Architecture for Computer Vision

Theoretically, information content cannot be assessed merely by the
dimensionality of the representation as it discards important factors
like correlation structure; the dimensionality merely provides a rough
estimate of information content.

What is meant by the ""correlation structure"" mentioned here? Is it a graph on input random variables? Is it in any way related to the aforementioned correlation?
","['definitions', 'correlation']",
When would you use Evolutionary Strategies over Step-Based Reinforcement Learning,"
In Salimans et al, 2016, the authors argue that ES should be considered a competitive alternative to MDP-based RL algorithms like Q-Learning, TRPO.
However, in practice, I notice that more often than not ES takes far more episodes to converge than MDP-based algorithms. So what would still be a reason to consider those, apart from pure academic interest?
The authors mention that ES will show less variance in long-horizon tasks, but didn't give an example. Is this aspect crucial ?
","['reinforcement-learning', 'evolutionary-algorithms']","
Great question! I did some research and found out that Generally capable agents emerge from open ended play by Deepmind is using ES in form of population based training:

We also explored the question, what distribution of training tasks will produce the best possible agent, especially in such a vast environment? The dynamic task generation we use allows for continual changes to the distribution of the agent’s training tasks: every task is generated to be neither too hard nor too easy, but just right for training. We then use population based training (PBT) to adjust the parameters of the dynamic task generation based on a fitness that aims to improve agents’ general capability. And finally we chain together multiple training runs so each generation of agents can bootstrap off the previous generation.

But this didn't really answer on their reasoning, so I dug some deeper and found a great article on lesswrong.com about the use of PBT. I will quote the essence, but highly recommend to read the linked Chapter on PBT:

What does the evolutionary selection give us that we don't already
have? What problem does this let us avoid?
There are several answers to this question.
The more narrow answer is that this allows the dynamic task generation
hyper parameters themselves to shift in a direction that promotes
general competence. Neither of the optimization levels beneath us
include any way of changing these parameters. But the ideal filtering
parameters for the production of general competence might be different
at the beginning, or at the middle of training. Or they might be
different from agent to agent. Without something like
population-based-training, they would have no way of changing and this
would hurt performance.
The less narrow answer, I think, is that this ensures that agents are
developing broad competence in a way the innermost loop cannot do.
[...]  each agent in our population of agents will learn to get better at
some distribution of tasks, then, but without
population-based-training they might not spread themselves broadly
across the entire span of this distribution. Like a student who
advances wildly at subjects she prefers, while ignoring subjects she
is not good at, our agents might not approach our desired ideal of
general competence. Population-based-training helps prevent the
scenario by multiplying agent / teacher pairs that do well generally
and non-narrowly.

"
What does it mean by bottleneck and representational bottleneck in feedforward neural networks?,"
Consider the following paragraph from section 2: General Design Principles of the research paper titled Rethinking the Inception Architecture for Computer Vision

Avoid representational bottlenecks, especially early in the
network. Feed-forward networks can be represented by an acyclic graph
from the input layer(s) to the classifier or regressor. This defines a
clear direction for the information flow. For any cut separating the
inputs from the outputs, one can access the amount of information
passing though the cut. One should avoid bottlenecks with extreme
compression. In general the representation size should gently decrease
from the inputs to the outputs before reaching the final
representation used for the task at hand. Theoretically, information
content cannot be assessed merely by the dimensionality of the
representation as it discards important factors like correlation
structure; the dimensionality merely provides a rough estimate of
information content.

This paragraph warned us to avoid bottlenecks and also representational bottlenecks. What does it mean by a bottleneck of/in a neural network and representational bottleneck?
","['neural-networks', 'terminology', 'bottlenecks']","

A bottleneck layer is a layer that contains few nodes compared to the previous layers. It can be used to obtain a representation of the
input with reduced dimensionality.

If you create bottlenecks in the initial layers it can cause loss of information.
Why it is done?
Take the example of an image dataset that contains high-resolution images. High resolution means more pixels means it will need more nodes in the input layer.
Having more nodes will need more computational power to train the network. Hence in such cases, we can use fewer nodes in the next layer. As images are high resolution we might not lose any important information.


"
Which of the following two implementations of a Least Squares classifier in Python is correct?,"
I am trying to solve a classification problem by implementing the Least Squares algorithm in Python. To solve this problem, I am implementing the linear algebra formula to train the classifier, which is $w = (X^TX)X^Ty$, where $w$ is the final weight vector of the classification function, $X$ is an input matrix of training data and $y$ a matrix of training labels. The classifier must be able to classify into three classes. As seen in the following snippet, during the preparation of the data, I gather my training data in matrix $X$, adding the number one at the end of each sample. I also gather my training labels in matrix $y$, coding each class as a sequence of -1 and 1.
    X = np.matrix(np.zeros((len(train_set),4)))
    y = np.matrix(np.zeros((len(train_set),3)))

    for i, row in train_set.iterrows():
        X[i] = [row[1], row[2], row[3], 1]
        if row[0] == 'H':
            y[i] = [1, -1, -1]
        elif row[0] == 'D':
            y[i] = [-1, 1, -1]
        else:
            y[i] = [-1, -1, 1]

What we have in matrix $y$ in the end, is a matrix that each column is a representation of each class and can tell us which samples belong to the class of the corresponding column. Having explained what each matrix in my program contains, my question is this, which of the following two implementations of the training process is correct and why?
At first, I went with the implementation seen in the snippet below.
    Xtranspose = X.T
    dotProduct = Xtranspose.dot(X)
    inverse = np.linalg.pinv(dotProduct)
    A = inverse.dot(Xtranspose)
    w = A.dot(y)
    
    for i, row in test_set.iterrows():
        r = np.matrix([row[1], row[2], row[3], 1]).dot(w)

As you can see, considering that $A = (X^TX)X^T$, I multiply $A$ with the label matrix $y$ and then I use the weight vector $w$ in the loop to test the classifier on some test data. Later, though, after some research on the internet, I found this second implementation, which actually has a higher success rate.
    for i, row in test_set.iterrows():
        r = np.zeros([3])
        j = 0
        for column in y.T:
            w = A.dot(column.T)
            r[j] = np.matrix([row[1], row[2], row[3], 1]).dot(w)
            j += 1

Having calculated $A$ beforehand, I now calculate the weights for each column of $y$, for each class, separately. This second method, has a 10% greater success rate than the first one. So, why does the second training method have a better success rate? Is the second method the right training method?
","['classification', 'python', 'implementation', 'linear-regression']","
The first implementation is better. In the second one, you calculate the weights based on information about a single class only, the $w$ inside the loop will be primed to find the classes. This is equivalent to training three models each trying to predict confidence in one class only. The first one would be able to do it for three classes.
A couple of extra points:

It's probably better to use np.array instead of np.matrix (Checkout the note here https://numpy.org/doc/stable/reference/generated/numpy.matrix.html)
For classification, it's more common to use cross entropy as the loss function, instead of the L2 loss. The normal equations work with the L2 loss.
I can't see that from the example, but it's worth normalising the features.

"
is there any proof that metric learning cannot achieve better on image classification task than accepted models (resnet etc)?,"
Everything is in the title.
Metric learning seems to be closer to our way of thinking than the best performing models (supervised learning CNNs-based models like resnet or efficientnet). I was looking for research papers that would have try a metric learning-based model for classic classification task on the Imagenet dataset benchmark but I could not find it. That is why I am asking the question here.
","['deep-learning', 'computer-vision', 'classification']",
Are some low dimensional distributions known to be hard to model with VAEs?,"
I am trying to implement a toy VAE project.
My goal is to use a VAE to model the moon dataset from scikit-learn, with an extra constant (but noisy) z-dimension.
To this end I use an approximate posterior with the form of a beta distribution and a uniform prior in a 1D latent space, because essentially the data is 1D. The decoder is a NN-parameterized gaussian.
I cannot get it to work using the simple ELBO.
I tried so far :

Increasing the number of monte carlo samples in the SGVB
Various deterministic pretrainings which tend to raise nans
Increasing the width or depth of the networks
Gradient clipping
learning rate annealing
Remove the noise in the data and perform Batch gradient descent instead of mini-batch
...

I use layers of residual blocks with Tanh nonlinearities, whose outputs are $\log \alpha$ and $\log \beta$ for the encoder, $\mu$ and $\log \sigma$ for the decoder.
I am starting to wonder whether the distribution is actually hard to model, because I ran out of bugs to fix and strategies to improve training.
Are some low dimensional distributions known to be hard to model this way ?
Additionally, what obvious or non obvious mistakes could I have made ?
ADDENDA
Code to generate the data:
# Adapted from sklearn.dataset.make_moons

def make_moons(n_samples=100, noise=None):
    generator = default_rng()

    n_samples_out = n_samples // 2
    n_samples_in = n_samples - n_samples_out

    outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples_out))
    outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples_out))
    inner_circ_x = 1 - np.cos(np.linspace(0, np.pi, n_samples_in))
    inner_circ_y = 1 - np.sin(np.linspace(0, np.pi, n_samples_in)) - .5

    X = np.vstack([np.append(outer_circ_x, inner_circ_x),
                   np.append(outer_circ_y, inner_circ_y),
                   np.zeros(n_samples)]).T
    y = np.hstack([np.zeros(n_samples_out, dtype=np.intp),
                   np.ones(n_samples_in, dtype=np.intp)])

    if noise is not None:
        X += generator.multivariate_normal(np.zeros(3), np.diag([noise, noise, noise])**2, size=n_samples)

    return X, y

# create dataset
moon_coordinates, moon_labels = make_moons(n_samples=500, noise=.01)
moon_coordinates = moon_coordinates.astype(np.float32)
moon_labels = moon_labels.astype(np.float32)

# normalize dataset
moon_coordinates = (moon_coordinates-moon_coordinates.mean(axis=0))/np.std(moon_coordinates, axis=0)

UPDATE
I have found a mistake that can explain poor performance.
In my post I said that the data is basically 1D, yet when I create the dataset I normalize the standard deviation in every dimension. This increases the magnitude of the z noise, and all of a sudden the third dimension accounts for a lot of variance and my model tries to fit to this noise.
Removing the normalization dramatically increases the performance.
","['variational-autoencoder', 'bayesian-deep-learning', 'evidence-lower-bound']",
"Why does $I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$ have eigenvalues in the range [0, 2]?","
In Semi-supervised classification with Graph Convolutional Networks, I am unable to understand a few things.
Given an undirected graph having

adjacency matrix $A$,
degree matrix $D_{ii} = \sum_j A_{ij}$,
normalized graph laplacian $L = I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}} = U \Lambda U^T$, where $\lambda_{max} \approx 2$ (see page 3, 2nd paragraph, not sure which matrix they are talking about)

Then, $I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$ has eigenvalues in the range [0, 2]. How?
","['papers', 'convolution', 'geometric-deep-learning', 'graph-neural-networks', 'linear-algebra']",
How to Weigt Constraints in A Control Problem with Reinforcement Learning,"
I have a control problem for a heating device of a building with the goal to minimize the electricity costs for one day under a varying price for electricity in every hour.
(more details can be seen here as well: Reinforcement learning applicable to a scheduling problem?).
I also want to test two further goals (minimize peak load and maximize PV self-consumption rate).
My problem also has about 10 constraints that should not be violated. I have two main questions about how to integrate the constraints into the Reinforcement Learning agent:
Here are my two main questions (with following minor questions):
(1) Basically I have three goals with normalized rewards between 0 and 1 for every time-slot and I have 10 constraints.
  Should the constraints reward also be normalized for all 10 constraints? And then should I choose a higher weight for the most important constraint than for all three goals combined such that a constraint violation is more crucial than getting a better objective value for all the three goals?
(2) Is it also possible to tell the Reinforcement Learning agent some rules directly without any constraints?
  E.g. I have two storage systems, and the agent is only allowed to heat up 1 for every time-slot. Further, the agent should not start and stop heating up frequently (like around four starts of the device daily is desirable).
  Can I explicitly tell these rules to the agent? Or do I have to do it indirectly by calculating a reward for every of these constraints and incorporate the weighted reward into the overall reward function of the agent?
I'll appreciate any suggestion and comment.
","['reinforcement-learning', 'control-problem']",
Derive Importance Sampling as Expected Value Notation,"
I'm new to RL. Recently, I took a course on Coursera. In the Off-policy MC method, I learned the concept of Importance Sampling as follows:

where the importance sampling ratio is the ratio of the target policy over the behavior policy.
But in Suton book the expectation under the target policy is estimated like this:

Given that both sources used the same importance sampling ratio. However, I ended up getting $E[G_{t}|s] = \sum{G_{t} b \frac{\pi}{\pi}} =  \sum{G_{t} \frac{1}{\rho}\pi} = E[\frac{G_{t}}{\rho_{t:T-1}}|s]$ instead
Did I do something wrong?
","['reinforcement-learning', 'importance-sampling']","
The importance sampling ratio is changing the measure of the expectation. We have the behaviour policy that generates trajectories in the environment, so we can calculate $\mathbb E_b[G_t|S_t = s_t] = v_b(s_t)$, but that's not what we want to calculate, we want to calculate $\mathbb E_\pi[G_t|S_t = s_t] = v_\pi(s_t)$. So, we need to find a scaling factor, $\rho$, that we can use in the first expectation to get to the second:
$$
\mathbb E_b[\rho G_t|S_t = s_t] = \mathbb E_\pi[G_t|S_t = s_t] = v_\pi(s_t)
$$
That scaling factor is the importance sampling ratio. In your example, the first equation, $E[G_t|s]$, is something we aren't interested in because we don't want to improve the behaviour policy.
"
"What is meant by a ""relevant document"" in NLP?","
In natural language processing, I came across the concept of ""relevant document"" several times. And several analytical formulas, such as precision, recall are based on the relevant documents.
Precision = $\dfrac{\text{Number of documents that are relevant and retrieved to the query Q}}{\text{Number of retrieved documents to the query Q}}$
Recall = $\dfrac{\text{Number of documents that are relevant and retrieved to the query Q}}{\text{Number of relevant documents to the query Q}}$
What is meant by ""relevance"" in such cases? Is it a universally objective term or subjective term, decided by the designer, based on that particular context?
","['natural-language-processing', 'terminology', 'precision', 'recall', 'information-retrieval']","
Precision and Recall are concepts that have been introduced in the field of information retrieval. Imagine you have a large set of documents, and you want to find the ones that are relevant to a particular issue.
You can be sure to find all relevant documents if you simply return the whole lot -- you won't miss a single relevant document. So your recall is 1.0, the maximum: you got all the relevant ones. That you also got a large number of irrelevant ones is not important for recall, but means your query wasn't very efficient (and actually quite pointless!).
If you do get all documents, your precision will be low: the number of retrieved documents that are relevant is the same, but the denominator is now much bigger, and depending on how many relevant documents there are, your precision value is small.
The opposite extreme is to not return any documents: Now your precision is high (effectively infinite, as it is zero divided by zero), but your recall is zero (as you don't get any relevant documents).
So in an ideal world, you want to optimise both precision and recall (which is why usually a third metric is used which combines the two, the F-Score.
Now, you can only calculate precision and recall if you know what the correct values are already, so you need to know how many documents are relevant. Obviously you will need to have inspected all documents to decide whether they are relevant to your issue or not. This is usually a subjective value judgment, as relevance tends to be non-binary. If I'm looking for articles on the efficiency of petrol engines, will those about Diesel engines be relevant? Probably, if you're looking at a general dataset. But if the documents are all about the efficiency of different types of engines, then I would most likely not be interested in those about Diesel engines.
From Information Retrieval, the concepts of precision and recall have been generalised to any binary classification tasks, where you can judge the quality of the classification by using these metrics. So you might come across situations where the 'relevance' criterion is more objective than whether a document is relevant to a certain topic. But that depends on the context of what you are classifying.
"
Where can I access this research paper on Frechet distance score?,"
Frechet Inception Distance is a metric that calculates the distance between feature vectors calculated for real and generated images. It is used in evaluations how good the generated images are.
Consider the following citation of the research paper I want to study in detail, which I think is the first paper on Frechet distance

Fréchet, Maurice. ""Sur la distance de deux lois de probabilité.""
Comptes Rendus Hebdomadaires des Seances de L Academie des Sciences
244.6 (1957): 689-692.

I have no clue on where to access the paper.
In general I get PDFs of almost any research paper due to my institute subscriptions in various publishers. But, I cannot see the pdf or contents of this research paper anywhere.
What can I do for accessing this paper?
","['papers', 'resource-request', 'fid-score']",
How to divide a segmented image into classes instances?,"
Is there a method/algorithm to generate instances of objects from image that was segmented by the use of any image segmentation models?
For example, I have an image with one class and it was segmented in a given way, where 1s are objects of the same class and empty fields are of no class:
How can I now generate list of the two objects, where list's elements for example would be positions of all the pixels inside the object (list of list).
","['convolutional-neural-networks', 'computer-vision', 'image-segmentation', 'instance-segmentation']",
How to construct a model to predict the value of a time series $y_t$ that depends from other time series $\bar{X}_t$?,"
I would like to know what are the standard approach to construct a model to predict the value of a time series $y_t$ that depends from other time series $\bar{X}_t$. I use to see around that for this kind of task there is a large amount of models but all autoregressive in a way.
I'm thinking about, for example VAR, SARIMAX, RNN, LSTM.
I'm looking for a model, or at least an approach, where there is no my lagged target variable as predictors. Does anyone has some references ?
","['prediction', 'time-series']","
A time series forecast model based upon lagged variables of that time series is commonly called auto-correlation. A time series forecast model based upon other series is called cross-correlation in the time-series literature. A general forecast that uses any number of lagged time series (including the lagged series itself) is called vector auto regression.
Any of the other ML and AI approaches can use non-linear methods to achieve the modelling and draw from an infinite amount of factors other than lagged time series. In finance, it's common to use factor based investing, for example (like fundamental value data). In Machine Learning, we are interested in finding those features (feature selection) that best model the time series outcome.
"
Reinforcing Learning when action has no effect on the environment,"
I am trying to get my head around a problem where the action by the agent can not change the environment. Without going into details, my problem is about error correction in an stochastic environment.
So, here the action by agent can not change the environment that causes these error and all we can do is to smartly correct as the errors happen. I am currently thinking about using Reinforcement learning for this agent who could correct the errors.
Now my questions are:

Would reinforcing learning be an overkill since the agent can not  influence the environment?
How do RL, LSTM, and even random forest compare in such scenarios?

Thank you.
","['reinforcement-learning', 'long-short-term-memory', 'random-forests']","
Short Intro
It's very common for people to think that Deep Learning is a ""superior form"" of Neural Network, a ""smarter model"". And then they try to use DL for solving simple tasks and they'll find more problems than solutions.

We might think of plane as superior to a bike. But when we need to buy some bread for breakfast, taking a plane is not even an overkill. It's just nonsense.

Your problem
In a similar way, I've seen people thinking Reinforcement Learning as somehow superior to Supervised or Unsupervised Learning. So let's establish a baseline:

The choice between Reinforcement Learning or Supervised Learning is not about superiority, but rather the nature of the task and your available dataset.

Reinforcement Learning
Great when your problem can be modeled as an agent interacting with an environment. The agent will sense (input), process (policy) and act (output). The policy is learned based on the reward of each action.
In most RL tasks (like strategy games), you can't objectively rate each individual action. Instead, it takes multiple actions before the outcome is obtained. But you can't train your model when you can't measure it's performance. So how to train a RL model?
Policy:
A policy is like a function that maps states into actions. When you can't measure the performance of each individual action, you create a policy.
You let the agent interact with the environment (play the game) until you have a score. If the environment is stochastic (the game depends on luck), you might want several rollouts (play lots of times) before evaluating the policy.
So, for acquiring a single data sample, you might need to let an agent to play a game several times.
But if you can directly measure the agent's perform every each action, congratulations! You can probably save lots of computational power by modeling the problem as a:
Supervised Learning
The model is learned based on the input-output pairs. A training dataset.
LSTM
Great when your input is sequential and your output is a function of the previous state.
Random Forest
Great when your input is not sequential, but you have a lot of features.
Unsupervised Learning
Great when your input is not sequential and you want to discover hidden structure in your data.
I hope this helps you with both questions. :)
"
Which model is more efficient and why?,"
Suppose, I have two NN models:

CNN model
Sequential NN model

They are solving the same problem. The data points have the same number of features.
In the case of #1, we used 0.6 million data points, 35k epochs, and the model achieved 80% accuracy in the training.
In the case of #2, we used 1.4 million data points, 1k epochs, and the model achieved 90% accuracy in the training.
Which model is better/more efficient and why?
","['neural-networks', 'deep-learning']",
How to make an output independent of input feature in neural networks?,"
Is there a way to make a certain output dimension of a neural network independent of a particular feature dimension? For example, I have a function $f_{\theta} : \mathcal{R}^{10} \rightarrow \mathcal{R}^2$, I want to make $f_{\theta}(\mathbf{x})_2$  independent of $\mathbf{x}_6$. How can this condition be imposed on a neural network?
I am thinking of penalizing the gradient of $f_{\theta}(\mathbf{x})$ w.r.t $\mathbf{x}_6$ for a considerable range of $\mathbf{x}_6 \in [-1, 1]$. Will this give me the similar effect? If so, how can this be coded in Pytorch or any other deep learning framework?
","['neural-networks', 'deep-learning']","
You could use Mutual Information between the model's prediction, and that particular feature as a regularization term. This will minimize the dependence of the output to that particular feature. Note that simply removing the feature from the dataset might not work if other features are associated with the feature which you don't want your model to depend on.
"
Total number of states reachable from the initial state in 8-puzzle problem,"
I know it's a simple question but the book Artificial Intelligence by Russel says that the number of reachable states from any initial state in the 8-puzzle problem is $\frac{9!}{2}$. However, I think it should be $9!$. Note that we can't say if we rotate the grid horizontally then state we get is the same so as to divide the total number of states by $2$. So why do we divide the total number of (initial) states by $2$? What extra states are we counting?
",['8-puzzle-problem'],
Why labeling facades?,"
In Pix2Pix by Isola et al. they translate images from different pairs of image categories to one another. While most other example applications for the algorithm make sense to me, I'm having difficulties understanding why one would translate facade labels to facade images. As the title says, I already don't see how labeling a facade would help solving any real world problem.
I skimmed the related work of the paper and found little about what ""facade parsing"" could be used for, except maybe reconstruction from images. Where are facades reconstructed from facade labels? Can anyone tell me other example applications for facade labels and translating them to images?
","['machine-learning', 'image-generation']","
It is enlightening to look at how the training data is created. For the purses and shoes they took real purses and shoes and then performed some thing like a county edge on them. They trained the learner using the simpler image to infer texture and color.
There are plenty of folks in the fields like “visual design“ that want to turn a sketch into something more realistic, and the elements of form, and lighting, and space, are important to how they go about doing that. They would use something like this trained pix2pix in their explorations of the transition between the architectural sketch and the real world.
"
Why use sin/cos to give periodicity in time series prediction,"
In this tutorial https://www.tensorflow.org/tutorials/structured_data/time_series#feature_engineering (scroll down a bit to ""Time"" heading), they take the sin/cos of the time index, and give this as an input so that the model can see the periodicity.
Why use sin and cos (which map to a circle)? Why not map to a square, or a diamond?
What about if you just mapped time to 1d instead of 2d, so e.g. 23:59 would be 1 and 00:00 (1 minute later) would be 0. Would that ""jump"" actually cause problems?
Any actual research or experiments which look at this issue?
","['time-series', 'function-approximation']","

Why use sin and cos (which map to a circle)?

Specifically from the tutorial's point of view, this is the most usual piece of feature engineering applied to periodic input data.
It offers the following advantages - roughly in order of importance (in my opinion):

Removes discontinuities (e.g. as you suggest the ""jump"" from 1 to 0 when time of day wraps around). Non-linear learning algorithms - such as neural networks - can learn to approximate discontinuities from inputs or outputs, but why force the learning algorithm to learn about them, when you can make the necessary transform for it? After all, that is a core purpose of feature engineering, to inject knowledge about the problem that assists the learning algorithm.

Creates a consistent distance measure. The vector distance between two points on the circle is always the same if they are the same distance away linearly on the pre-transformed feature (including the periodicity). This is not the case with other mappings such as squares or diamonds.

Sin and cos are standard library library functions in most programming languages. Mapping to a square or diamond involves slightly more custom coding.



Why not map to a square, or a diamond?

Squares or diamonds are close enough to circles, that in a lot of cases you would not notice the difference in practice. There may even be cases where some kind of closed polygon is a more natural mapping for a given problem. So feel free to use them and experiment when performing feature engineering.

Any actual research or experiments which look at this issue?

This kind of low-level feature engineering is simple enough that you probably won't find many papers that pull out just solutions to mapping periodic variables. You will find using sin and cos to transform a periodic variable used as a standard transform in many situations though.
If you are interested to compare different options for mapping periodic variables, you will likely need to perform the experiments yourself. For any specific problem where you are not sure, this is good practice if you have the time. There is a chance that you will find a useful generic mapping for periodic data that is not based on circles, but my prediction is that you will find other shapes perform similarly to circles a lot of the time, but are sometimes worse and rarely if ever better.
"
"Is there any difference between the phrases ""text representation"" and ""text feature representation""?","
Text representation, in simple words, is representing text in sensible numeric form. You can read in detail from the following paragraph

Text representation is one of the fundamental problems in text mining
and Information Retrieval (IR). It aims to numerically represent the
unstructured text documents to make them mathematically computable.
For a given set of text documents $D = \{d_i, i=1, 2,...,n\}$, where
each $d_i$ stands for a document, the problem of text representation
is to represent each $d_i$ of $D$ as a point $s_i$ in a numerical space
$S$, where the distance/similarity between each pair of points in
space $S$ is well defined.

But, I came across the phrase ""text feature representation"" in research papers. Features, in general, are present in dataset. But, I think, features can be characters or words or documents or complete text (as a single feature?) in the case of the text. I am not sure about what we call features in the text.
So, I am not sure about what is meant by text feature representation. Is it the same as text representation?
","['natural-language-processing', 'terminology', 'definitions', 'vector-semantics']","
I think that literature is simply inconsistent in this regard. But here's a distinction that I think helps to shade a bit of light on this question:
text representation: as you said we have to convert text into numerical variables. This term refers to general strategies to convert text into numbers, like embedding, bag of words, and so on.
text features representation: this is what we feed to a real model. The difference is that it can be a combination of different text representation. For example, I could apply a TF-IDF vectorizer to a corpus and use it to encode each word of a sentence and then use also pre-trained embedding to encode the same sentence/document, concatenating for each word both vectors. Or another commonly used strategy is to use multiple n-grams, which also consist in applying multiple text representations to the same sentence/document.
"
What type of neural network do you need if you want to detect an action or dynamic pattern instead of a static pattern?,"
Let's say that you want to detect if a man is running, walking, or dancing instead of just detecting a man still. What type of neural networks will you use for this purpose?
","['neural-networks', 'long-short-term-memory', 'model-request', 'action-recognition']",
How do CNNs handle inputs of different sizes and shapes?,"
I am new to deep learning so feel free to correct me where I am wrong.
Imagine this scenario where we have a 7 * 7 input. We want to slide a 3 * 3 filter with a stride of 3 and padding of zero over this input. As you know, it is not possible to do this.
Also, CNNs have a fixed input shape(Correct me if I am wrong) or at least the input should be of the multiples of the CNN's intended input shape(e.g., 112 * 112, 224 * 224, etc)(Although the situation that this may work is rare.)
According to this PyTorch page, ResNet (for example) accepts images of any size as long as they are bigger than 224.
So my question is, how does it handle images of different sizes? Does it dynamically tweak parts of the structure (e.g., kernels, strides, paddings) based on the input? If yes, wouldn't that change the network architecture? Or it changes the input sizes to the intended size automatically?
Also, this does not answer my question.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'pytorch', 'convolutional-layers']",
Isssue in understanding the derivation regarding mean squared error,"
The following derivation is taken from  Chapter 5: Machine Learning Basics from the book titled Deep Learning (by Aaron Courville et al.)
I am facing difficulty in understanding the zero derivative related to minimizing gradient of mean squared error $\nabla_w  \text{MSE}_{train} = 0$

$\nabla_w \text{MSE}_{train} = 0$
$\implies \nabla_w(Xw - y)^T(Xw-y) = 0$
$\implies \nabla_w(w^T X^T-
 y^T)(Xw-y) = 0$
$\implies \nabla_w(w^T X^T-
 y^T)(Xw-y) = 0$
$\implies \nabla_w(w^T X^TXw - w^T
 X^Ty
 -y^TXw+y^Ty) = 0$
$\implies \nabla_w(w^T X^TXw - 2 w^T
 X^Ty +y^Ty) = 0$
$\implies 2 X^TXw - 2 X^Ty = 0$
$\implies w = 2 (X^TX)^{-1} X^Ty = 0$

I have had difficulty in understanding the flow of the following two lines

$\implies \nabla_w(w^T X^TXw - w^T
 X^Ty
 -y^TXw+y^Ty) = 0$
$\implies \nabla_w(w^T X^TXw - 2 w^T
 X^Ty +y^Ty) = 0$

The first doubt is about in the first two lines, it is possible only if (which I feel is untrue)
$w^T X^Ty = y^TXw$
Note: $X,y$ here refers to input and outputs of train data
","['math', 'gradient', 'matrices']","
The topic you're looking for is called Linear Algebra, and the equation you highlighted at the end is true. This is the reason:

If you have a scalar $x \in \mathbb{R}$, then you can take its transpose without changing the value, its transpose is exactly the same number.
When you have two vectors $w \in \mathbb{R}^n$ and $y \in \mathbb{R}^n$ and a matrix $X \in \mathbb{R}^{n \times n}$, then $w^TX^Ty \in \mathbb{R}$, i.e. it is a scalar.
From Linear Algebra, when you have a product of matrices ($A$, $B$, etc. matrices) you can take the transpose as follows: $(AB)^T = B^TA^T$. You can extend that two three matrices: $(ABC)^T = ((AB)C)^T = C^T(AB)^T = C^TB^TA^T$. You can look at vectors and scalars as special kinds of matrices.
When you have $w^TX^Ty$, you can take its transpose without changing the value because it is a scalar, so $w^TX^Ty = (w^TX^Ty)^T = y^TXw$.

Most books and free online resources on Linear Algebra are enough to understand these concepts. Everyone has their favourite, and it really doesn't matter much which one you pick but I like this book: https://link.springer.com/book/10.1007/978-3-319-24346-7.
"
Get the name of a merchant from records,"
I have a bunch of bank transaction records from which I want to extract merchants' names. In a few subsets of these records, the structure of the string is the same within the subset with only the merchant name changing. For example
subset 1

XXXXX_ID_TIME_STAMP MERCHANT1 CREDIT
XXXXX_ID_TIME_STAMP MERCHANT2 CREDIT

subset 2

BILL PAYMENT BANK_NAME MERCHANT NAME 3 
BILL PAYMENT BANK_NAME MERCHANT NAME 4 

In the above two subsets, the structure of the string is the same, only the merchant names changes
and so on ...
Using NLP, I want to extract merchant names in such cases. How should I approach this?
Using regex is not feasible because I'd have to manually go through the complete data, identify all such patterns and create regex strings that'll extract the name. I would also have to do this for every new pattern.
Is there a way where I can train a model that can identify/extract merchants in such cases?
","['machine-learning', 'natural-language-processing', 'text-classification']","
The problem:
You are facing a Natural Language problem called Named Entity Recognition (that's the key word you are looking for).
But before you dive deep into it, have in mind it's best suited for user input data (where users are absolutely chaotic) and it looks like you have a system data.
The right way:
You should have some kind of tabular (structured) data, instead of a string. So:

Triple check if you have some white-space separator (like tabs).
Review you content retrieval source and method.
If you have influence over the data source, ask them to generate it the right way.

If none of them help...
The programmatic way:
I can't see a specific Machine Learning model to solve that, but you can combine several techniques (some of them NLP) to help. It's an analytical and explorative process. Here are some insights:

the structure of the string is the same within the subset with only the merchant name changing


If you have some absolutely identical records, except for the data you are looking for, you could simply look for the diff between them.

Dictionary retrieval: If you have the list of Merchants, it's as simple as checking if a record contains any of the listed words. If you don't, you can build it as you run other methods. So maybe one subset can help you solving the other, like a Sudoku puzzle.

Track special characters:

If some columns (XXXXX_ID_TIME_STAMP CREDIT BILL PAYMENT) contain numbers (or any special character), you can eliminate them right away.


Tokenization: you can convert every word to a unique token. If the Merchant Name is composed by 2 or more words, you can use n-grams.

Frequency Analysis: The names (tokens / n-grams) will probably have similar frequency inter and intra subsets. For example:

The bank name might be much more frequent than a merchant name.
A merchant might be frequent in one subset but rare in others.


Divide each record into smaller substrings.

If you can eliminate some n-grams inside a record (using methods like special characters or frequency analysis) you'll have smaller (and more structured) problems to handle. For example: [_][_]MERCHANT[_]BANK[_]



"
Is discriminator a regressor or classifier in implementations?,"
GAN has two components: generator and discriminator.
Discriminator in the original GAN is a regressor and always gives value in $[0, 1]$. You can read it in original paper

$D(x)$ represents the probability that $x$ came from the data rather
than $p_g$

Is it true with most of the (advanced or) contemporary GANs? Or the do nature of discriminator, either as a regressor or as a classifier entirely depends on the context?
","['generative-adversarial-networks', 'discriminator']","

Discriminator in the original GAN is a regressor

No, it is a classifier. It classifies an image as ""real"" or ""fake"", with the output usually being probability that the image is ""real"" (you could reverse this and use generated images as the target class, provided you change the generator training to match).

Is it true with most of the (advanced or) contemporary GANs?

In WGANs the W stands for Wasserstein, and these GANs use Wassersten loss, which measures the distance in ""realness"" between real and fake images. This measure of realness is a regression problem, with the caveat that there is no true measure of realness for any images that you can train with separately. The architecture of the critic, which plays the same role as discriminator for classic GANs, is the same as a neural network used for regression.
In general, if you see the term ""discriminator"", you can assume a classifier is being used. If you see the term ""critic"", you can assume a regressor. This may not be true for everything published about GANs, as some authors may use the terms loosely, but it is reasonable to expect if you are reading original papers or learning from a course.
As far as I can tell, StyleGAN2, which produces state-of-the-art results, uses a standard discriminator/classifier setup. There are plenty of other architectural details in the discriminator that contribute to the performance of the GAN. There is a link to the paper describing these from the linked Github implementation.
"
Why do some techniques use random augmentations during convolution processes,"
While going over PyTorch image augmentations, https://pytorch.org/vision/stable/transforms.html, I see that some augmentations can be applied with a certain probability. What is the purpose of applying stochastic augmentations rather than consistently applying a certain augmentation?
","['convolutional-neural-networks', 'image-processing', 'image-transformations']",
Is it possible to identify which feature maps were generated from a particular image after convolutional operation,"
Let's say I have a video that contains 3 grayscale sequential frames having a combined shape of (3, 24, 24). After inputting these frames together into a CNN, multiple feature maps will be generated from each of the images. Would it be possible for me to separate the temporal aspect by identifying which frame generated which feature maps?
",['convolutional-neural-networks'],
Parallelize Backpropagation - How to synchronize the weights of each thread?,"
I implemented a parallel backpropagation algorithm that uses $n$ threads. Now every thread gets $\dfrac{1}{n}$ examples of the training data and updates its instance of the net with it. After every epoch the different threads share their updated weights. For that I simply add the weights of the threads and then divide each weight by the number of threads. The problem now is that the more threads I use the worse the result. For me this means that my way of synchronizing the threads is not as good as it should be.
Is there a better way to do it?
",['backpropagation'],"
After a whole epoch, with multiple update steps, the neural networks in each thread will have diverged in a way where it may not make sense to take means of the weights. Ideally you should be combining data for each update step. In turn that means you will want to avoid making updates on every example, because the overhead of starting, stopping and combining the threads may lose most of the benefits.
It is common in neural networks to use mini-batches (larger than 1, smaller than the whole dataset), to get more accurate gradients, and for parallelisation. There is often a sweet spot in terms of learning speed (or sample efficiency) with some size of mini-batch. Each mini-batch calculates gradients for all examples, combines them into a mean gradient, then performs a single weight update step.
Use your threads to calculate the gradients for a mini-batch, divided up between the threads, and average the gradients across all threads in order to make a single shared weight update. Using larger mini-batches will make more efficient use of multiple threads, but smaller mini-batches can be beneficial because you get to make more weight updates per epoch.
"
What's the best way to feed stories to a neural network?,"
I'm trying to train a model that would generate stories. I have a dataset of 2000 stories prepared. They are tokenized and one-hot encoded. I can't load them all at once as a one big dataset, because of memory limits.
What would be the best way to fit my network so that i can reset the states after each story?
I tried doing it in a nested for loop (for epoch/for story: model.fit) but it's working really slow cause it takes 3 seconds to fit a single story but almost 10 to load the next file and setup model.fit again.
","['neural-networks', 'long-short-term-memory', 'datasets', 'text-generation']",
Is knowing the class of probability density function mandatory for explicit density estimation?,"
In deep learning, models may learn the probability distribution that generated the dataset. Observe the following paragraph from Chapter 5: Machine Learning Basics from the book titled Deep Learning (by Aaron Courville et al.)

Unsupervised learning algorithms experience a dataset containing many
features, then learn useful properties of the structure of this
dataset. In the context of deep learning, we usually want to learn the
entire probability distribution that generated a dataset, whether
explicitly, as in density estimation, or implicitly, for tasks like
synthesis or denoising. Some other unsupervised learning algorithms
perform other roles, like clustering, which consists of dividing the
dataset into clusters of similar examples.

I read about density estimation in the same chapter, as given below

In the density estimation problem, the machine learning algorithm is
asked to learn a function $p_{model} : R^n \rightarrow R$, where
$p_{model}$(x) can be interpreted as a probability density function
(if $x$ is continuous) or a probability mass function (if $x$ is
discrete) on the space that the examples were drawn from.

This question is focused on explicit probability density estimation in continuous case i.e., learning density function $p_{model}$  directly.
Suppose I have a dataset $D$ with $n$ continuous random variables (features) $X_1, X_2, X_3,\cdots, X_n$. And I don't know anything about the probability density function of individual random variables. That is, I don't know about any information about any $X_i$ , such as, whether $X_i$ follows normal distribution or any other distribution. Then, is it possible to learn density function explicitly? Or do I need to provide some necessary information such as the class of probability distribution function to be learned?
I am thinking as follows:
If I have some information about $X_i$, such as: $X_i$ falls to a well known distribution, then I can learn the parameters of the underlying density function from $D$. So, is it mandatory to know some information about the underlying probability density function.
",['density-estimation'],"
Neural Networks can approximate any function. Quoting the essence in case the article is removed in the future.

The key to neural networks’ ability to approximate any function is that they incorporate non-linearity into their architecture. Each layer is associated with an activation function that applies a non-linear transformation to the output of that layer.

So no, knowing the class of the probability density function is not required to approximate it via Deep Learning. With a large enough number of samples you could construct an approximation.
"
What is the fundamental difference between the synthesis task and sampling task?,"
Among the list of tasks in machine learning, synthesis and sampling is one of the key task. Consider the following explanation regarding synthesis and sampling task from  Chapter 5: Machine Learning Basics from the book titled Deep Learning (by Aaron Courville et al.)

In this type of task, the machine learning algorithm is asked to
generate new examples that are similar to those in the training data.
Synthesis and sampling via machine learning can be useful for media
applications when generating large volumes of content by hand would be
expensive, boring, or require too much time. For example, video games
can automatically generate textures for large objects or landscapes,
rather than requiring an artist to manually label each pixel (Luo et
al., 2013). In some cases, we want the sampling or synthesis procedure
to generate a speciﬁc kind of output given the input. For example, in
a speech synthesis task, we provide a written sentence and ask the
program to emit an audio waveform containing a spoken version of that
sentence. This is a kind of structured output task, but with the added
qualiﬁcation that there is no single correct output for each input,
and we explicitly desire a large amount of variation in the output, in
order for the output to seem more natural and realistic.

The explanation does not mention any difference between the two tasks. Both sampling and synthesis, apart from the linguistic differences, I don't know any discriminating criteria, qualities or properties, that separate both tasks in machine learning.
What is the fundamental difference between sampling task and synthesis task in machine learning?
","['machine-learning', 'terminology', 'generative-model']","
The terminologies can be confusing because of the different ways authors use them. The bottom line is this
The Synthesis task basically refers to creating or synthesizing new data. Creation of data can be purely deterministic, e.g.

.. we provide a written sentence .. to emit an audio waveform containing a spoken version of that sentence

But, a vast majority of the time, such creation has an ..

.. added qualiﬁcation that there is no single correct output for each input, and we explicitly desire a large amount of variation

which is done by a Statistical Sampling procedure. This procedure can be as simple as IID sampling from $\mathcal{N}(\mu, \sigma)$ to more complex MCMC methods.
So basically, ""Synthesis"" is a broader task that may contain a ""Sampling"" procedure.
"
How does back propagation adjust the hidden layers' weights and biases?,"
I'm new to neural networks and trying to figure out its fundamentals but I cannot fully understand the back propagation algorithm.
In back propagation, I understand we want to go backwards from the last neurons to adjust the weights and biases to that predicted final neurons. For it to calculate error and derivative, It needs to have the last inputs, the predicted output based on the layer's weights and the actual value ( target ).
As In the final neuron layers we have all this information. But how do we calculate the inputs of middle and hidden layers?

Suppose we have the final output ( 0.73 ), we calculate the error and derivatives of W31, W32 and W33; and adjust them to match the final output, Then we shall go one layer back in our network.
Now we need the N11, N12, N13 and N14 values and the target values of N21, N22 and N23 to calculate errors and derivatives, but we don't have them
Should we feed forward the whole network and map all the labels and values of each neuron in memory to be able to access it later? Because it would be very, very memory and resource intensive on large networks.
","['neural-networks', 'deep-learning', 'backpropagation', 'deep-neural-networks', 'hidden-layers']",
Can future information be included in a control problem with Reinforcement Learning?,"
I have a control problem for a heating device of a building with the goal to minimize the electricity costs for one day under a varying price for electricity in every hour (more details can be seen here:Reinforcement learning applicable to a scheduling problem?). Although the problem is basically a scheduling problem, I want to implement it like a control problem for every time step.
Now, I have 2 questions:

Is it possible to somehow consider future values (e.g. of the electricity price) while during a control action for every time slot? E.g. when the agent knows that in 2 hours the price will fall significantly, then it should tend to consume electricity in 2 hours to get closer to the optimal solution.

Related to 1: Is it possible to get the reward just at the end of the day instead of every hour (although the control actions are every hour)? If you get the reward at every hour, this might lead to a greedy behaviour, which often results in bad results.


","['reinforcement-learning', 'control-problem']",
Bag of Tricks: n-grams as additional features?,"
I've been playing with PyTorch's nn.EmbeddingBag for sentence classification for about a month. I've been doing some feature engineering, playing with different tokenizers, etc. I'm just trying to get the best performance out of this simple model as I can. I'm new to NLP, so I figured I should start small.
Today, by chance, I stumbled on this paper Bag of Tricks for Efficient Text Classification, which very well may be the inspiration for nn.EmbeddingBag. Regardless, I read the paper and saw that they increased performance through using ""n-grams as additional features to capture some partial information about the local word order""
So by the wording of this sentence, specifically ""additional features"", I take it to mean that they made n-grams as part of their vocabulary. For example ""abc news"" is treated as a single word in the vocabulary, and then appended to the training data that is being embedded like so:
dataset = TextFromPandas(tweet_df)
label, sentence, ngrams = dataset[0]
label, sentence, ngrams

# out:

(1,
 'quake our deeds are the reason of this # earthquake may allah forgive us all',
 ['quake our',
  'our deeds',
  'deeds are',
  'are the',
  'the reason',
  'reason of',
  'of this',
  'this #',
  '# earthquake',
  'earthquake may',
  'may allah',
  'allah forgive',
  'forgive us',
  'us all'])

I just wanted to check my assumption, because the paper is not very explicit. I already tried to string n-grams together as a new sentence in place of the old, but performance dropped significantly.
I will continue to experiment, but I was wondering if anyone knows the specific mechanism?
","['natural-language-processing', 'papers', 'feature-extraction', 'bag-of-words', 'n-gram']",
Transformer model is very slow and doesn't predict well,"
I created my first transformer model, after having worked so far with LSTMs. I created it for multivariate time series predictions - I have 10 different meteorological features (temperature, humidity, windspeed, pollution concentration a.o.) and with them I am trying to predict time sequences (24 consecutive values/hours) of air pollution. So my input has the shape X.shape = (75575, 168, 10) - 75575 time sequences, each sequence contains 168 hourly entries/vectors and each vector contains 10 meteo features. My output has the shape y.shape = (75575, 24) - 75575 sequences each containing 24 consecutive hourly values of the air pollution concentration.
I took as a model an example from the official keras site. It is created for classification problems, I only took out the softmax activation and in the last dense layer I set the number of neurons to 24 and I hoped it would work. It runs and trains, but it does worse predictions than the LSTMs I have used on the same problem and more importantly - it is very slow - 4 min/epoch. Below I attach the model and I would like to know:
I) Have I done something wrong in the model? can the accuracy or speed be improved? Are there maybe some other parts of the code I need to change for it to work on regression, not classification problems?
II) Also, can a transformer at all work on multivariate problems of my kind (10 features input, 1 feature output) or do transformers only work on univariate problems? Tnx
def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):

    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):

        # Normalization and Attention
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        x = layers.MultiHeadAttention(
            key_dim=head_size, num_heads=num_heads, dropout=dropout
        )(x, x)
        x = layers.Dropout(dropout)(x)
        res = x + inputs

        # Feed Forward Part
        x = layers.LayerNormalization(epsilon=1e-6)(res)
        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=""relu"")(x)
        x = layers.Dropout(dropout)(x)
        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
        x = x + res

    x = layers.GlobalAveragePooling1D(data_format=""channels_first"")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=""relu"")(x)
        x = layers.Dropout(mlp_dropout)(x)
    x = layers.Dense(24)(x)
    return keras.Model(inputs, x)

model_tr = build_transformer_model(input_shape=(window_size, X_train.shape[2]), head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], mlp_dropout=0.4, dropout=0.25)
model_tr.compile(loss=""mse"",optimizer='adam') 
m_tr_history = model_tr.fit(x=X_train, y=y_train, validation_split=0.25, batch_size=64, epochs=10, callbacks=[modelsave_cb])

","['deep-learning', 'tensorflow', 'keras', 'transformer', 'time-series']",
Can I always interpret features as random variables in machine learning safely?,"
Consider the following statements from Chapter 5: Machine Learning Basics from the book titled Deep Learning (by Aaron Courville et al.)

Machine learning tasks are usually described in terms of how the
machine learning system should process an example. An example is a
collection of features that have been quantitatively measured from
some object or event that we want the machine learning system to
process. We typically represent an example as a vector $\mathbf{x} \in
 \mathbb{R}^n$ where each entry $x_i$ of the vector is another feature. For example,
the features of an image are usually the values of the pixels in the image.

Here, an example is described as a collection of features, which are real numbers. In probability theory, a random variable is also a real-valued function.
Can I always interpret features in machine learning as random variables or are there any exceptions for this interpretation?
","['machine-learning', 'comparison', 'features', 'random-variable']",
Is it really hard to learn in a stochastic environment?,"
I understand that a stochastic environment is one that does not always lead you to the desired state by giving a particular action $a$ (But the probability to change to a not desire state is fixed, right?).
For example, the frozen lake environment is a stochastic environment. Sometimes you want to move in one direction and the agent slips and moves in another direction. Unlike an environment with multiple agents that the probability of the actions of the other agents is changing because they keep learning (a non-stationary environment).
Why is it difficult to learn in a stochastic environment, if, for example, Q-learning can solve the frozen lake environment? In what cases would it be difficult to learn in a stochastic environment?
I have found some articles that address that issue, but I don't understand why it would be difficult if Q-learning can solve it (for discrete states/actions).
","['reinforcement-learning', 'q-learning', 'markov-decision-process', 'environment']",
Residual Blocks - why do they work?,"
I've learnt that idea that the residual block was invented to solve the vanishing gradient problem due to the deep layer to layer multiplication.
I understand that for example if I have 10 layers, and I add another 5 layers, that the output of the 10th layer will 'skip' the 5 layers. Although, the output of the 10th layer will also pass through the 5 layers as well. Just before the 15th layer Relu, the output from the 10th layer is element-wise summed with the 15th layer, just prior to the final Relu.
I have some confusion with this.

Identity mapping/function. I keep reading that it creates an identiy function or it learns an identity function. What exactly is this? Isn't is just F(x) = 5 added layers, and x =output of 10th layer and thus it is just F(X) + X?

By summing the output of the 10th layer to the 15th layer, will this not affect what was learnt in the 5 layers? I.e. from 11th -15th layer.

I believe it also helps with backpropagation so that it doesn't have to update all the weights layer by layer and it can skip back to shallow layers. Therefore, are the weights inside the residual block, i.e layers 11-15 not updated? If not, then what is the point of the 11-15th layer if they are not designed to ""do anything"".


","['neural-networks', 'machine-learning', 'residual-networks']","
First a little intro, skip to the end for the straight answers: residual networks were proposed after observing that deeper models tend to perform worse than their shallow counterpart if we just keep adding hidden layers without applying any other change to the architecture, as we can see in the very first picture of the original paper.

The reason of this phenomena is indeed gradient vanishing. The more the hidden layers, the more the information of the original input get lost, due to the fact that a hidden layer receives only information from the previous hidden layer. How to solve this? Using residual connections. A residual connection is just an identity function that map an input or hidden state forward in the network, so not to the immediate next layers, that's why these connections are also called skip connections. The only purpose they serve is to force deep layers to retain information learned in the early layers of the network.
From a numerical perspective you can think about information getting lost as weights becoming smaller and smaller. By brutally summing the hidden states of previous layers you make sure to avoid this problem, giving the weights a broader range of adjustment even in very deep layers.
In conclusion, to answer your questions:
1 Residual connection don't create or learn an identity function, they simply use it. The formulation of such connections in the paper is:
$y = F(x, W_{i}) + x$
where x could be rewritten as $I(x)$, $I$ being the identity function.
2 No, we don't loose any information by summing the residuals, on the contrary, they are designed to retaining information also in very deep layers, for the above mentioned reasons.
3 All layers are updated, there's no frozen layers in a residual network. The ""skip"" term refers to the fact that a hidden layers is copied in forward layers, which is a legit operation, but it doesn't refer at all to skip in training or weight updates.
"
Can a face recognition system be trained using only computer generated hyper realistic faces?,"
In order to train a face recognition system you need to have access to a large database with thousands of photos containing different faces. Companies like facebook and amazon have these databases but most average people do not.
If you don't have access to a sufficiently large dataset with faces, could you use computer generated random faces instead? I'm asking this because computers are becoming better and better in rendering hyper realistic faces. An example is the meetmike digital human showcase video. Another example is the unreal engine project spotlight video.. Lastly you also have websites like https://thispersondoesnotexist.com/ that can generate random faces.
What if you generate a couple of photos of the same computer generated face and you make sure that each photo shows the face in a different setting or from a different angle. Could you then use such photos to train a facial recognition system that can accurately recognize real people?
","['face-recognition', 'face-detection']",
What is the difference between the US and global edition of the AIMA book by Russell and Norvig?,"
The book Artificial Intelligence: A Modern Approach by Russell and Norvig has two editions: global and the US. It looks like these two are generally the same, but have some differences in the order of the chapters and in the context, is this correct?
","['comparison', 'books', 'norvig-russell']","
I check again with the subchapters of Artificial Intelligence: A Modern Approach, 4th Global ed / US ed from this website the pdf subchapters reference of Global Edition and US Edition. I can confirm you the difference between Global US edition is this subchapter:
20 Knowledge in Learning 739
    20.1 A Logical Formulation of Learning 739
    20.2 Knowledge in Learning 747
    20.3 Explanation-Based Learning 750
    20.4 Learning Using Relevance Information 754
    20.5 Inductive Logic Programming 758
Summary 767
Bibliographical and Historical Notes 768

So Global Edition has more content than US Edition.
"
Tensorflow Probability Implementation of Automatic Differentiation Variational Inference with Mixtures,"
In this paper, the authors suggest using the following loss instead of the traditional ELBO in order to train what basically is a Variational Autoencoder with a Gaussian Mixture Model instead of a single, normal distribution:
$$
\mathcal{L}_{SIWAE}^T(\phi)=\mathbb{E}_{\{z_{kt}\sim q_{k,\phi}(z|x)\}_{k=1,t=1}^{K,T}}\left[\log\frac{1}{T}\sum_{t=1}^T\sum_{k=1}^K\alpha_{k,\phi}(x)\frac{p(x|z_{k,t})r(z_{kt})}{q_\phi(z_{kt}|x)}\right]
$$
They also provide the following code which is supposed to be a tensorflow probability implementation:
def siwae(prior, likelihood, posterior, x, T):
  q = posterior(x)
  z = q.components_dist.sample(T)
  z = tf.transpose (z, perm=[2, 0, 1, 3])
  loss_n = tf.math.reduce_logsumexp(
  (−tf.math.log(T) + tf.math.log_softmax(mixture_dist.logits)[:, None, :]
  + prior.log_prior(z) + likelihood(z).log_prob(x) − q.log_prob(z)), axis=[0, 1])
  return tf.math.reduce_mean(loss_n, axis=0)

However, it seems like this doesn't work at all so as someone with nearly no tensorflow knowledge I came up with the following:
def siwae(prior, likelihood, posterior, x, T):
  q = posterior(x) # distribution over variables of shape (batch_size, 2)
  z = q.components_distribution.sample(T)
  z = tf.transpose(z, perm=[2, 0, 1, 3]) # shape (K, T, batch_size, encoded_size)
  l1 = -tf.math.log(float(T)) # shape: (), log (1/T)
  l2 = tf.math.log_softmax(tf.transpose(q.mixture_distribution.logits))[:, None , :] # shape (K, 1, batch_size), alpha
  l3 = prior.log_prob(z) # shape (K, T, batch_size), r(z)
  l4 = likelihood(tf.reshape(z, (K*T*x.shape[0], encoded_size)))
  l4 = l4.log_prob(tf.repeat(x, repeats=K*T, axis=0)) # shape (K*T*batch_size, )
  l4 = tf.reshape(l4, (K, T, x.shape[0])) # shape (K, T, batch_size), p(x|z)
  l5 = -q.log_prob(z) # shape (K, T, batch_size), q(z|x)
  loss_n = tf.math.reduce_logsumexp(l1 + l2 + l3 + l4 + l5, axis=[0, 1])
  return tf.math.reduce_mean(loss_n, axis=0)

There are no errors when I try to use this as
siwae(prior, decoder, encoder, x_test[:100, ...], T)

but after a few training steps I get only nans. I really don't have any idea of this is an due to a wrong implementation or wrong usage of the loss - especially as I don't have much experience with tensorflow. So any help would be greatly appreciated.
For a full, minimal example I created this colab.
","['tensorflow', 'variational-autoencoder', 'evidence-lower-bound', 'variational-inference', 'tensorflow-probability']",
Why is the prior on the latent variable standard gaussian in VAE?,"
While training a standard VAE, we assume that the prior on the latent variable Z is the standard gaussian and we use KL divergence to push the posterior as close as possible to the standard gaussian. Why not assume any other gaussian as the prior? What are the intuitive reasons for this?
","['deep-learning', 'autoencoders', 'variational-autoencoder']","
Simply, it is just a design choice.
Isotropic gaussian is one of the easiest density to work with. It has an easy-to-compute likelihood and easily reparameterizable.
You are free to use other distribution, but might face computational or implementation hurdles.
"
"What is the definition of ""confidence interval"" around a (complicated) function?","
Consider the following excerpt from Chapter 5: Machine Learning Basics from the book titled Deep Learning (by Aaron Courville et al.)

Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving conﬁdence intervals around these functions;

This excerpt says that machine learning focuses on estimating complicated functions, but not on proving conﬁdence intervals around those functions. What is meant by or definition for a confidence interval around a complicated function mentioned here?
","['machine-learning', 'definitions', 'statistics', 'probabilistic-machine-learning']","
Off the top of my head, I don't know the very specific definition of confidence interval (or whether it's only defined for the parameters of a model), as I am not a statistician. In any case, intuitively, a confidence interval is an interval (or range) of values where some true value of something (e.g. your parameter) lies. (Confidence intervals are also very related to hypothesis testing, but I will not dwell on this topic here). You can find the specific definition of a confidence interval in any statistics book (e.g. this one).
Having said that, I interpret that statement as saying that most machine learning approaches do not take into account any type of uncertainty about either the true value of the parameters (e.g. of the neural networks) or the predictions or do not deal with hypothesis testing (recall above that I said that hypothesis testing is closely related to confidence intervals). Typically, in machine learning, you will find a lot of approaches that just provide you with a point estimate for the parameters (i.e. you estimate a single number for each parameter). Consequently, your final neural network (or model) just represents a single function, but what if that function is not really correct (which is probably the case given the typically limited amount of data)? In this case, you cannot say anything about your uncertainty of the true target function that you were trying to approximate or about the prediction for a new input.
Here's where probabilistic/Bayesian machine learning (PML) comes into play. Probabilistic machine learning is a relatively new subfield of machine learning that deals with uncertainty quantification/estimation or that uses tools from Bayesian statistics, like the Bayes theorem. If (deep) neural networks are involved, it is also called Bayesian deep learning (BDL).
For a gentle overview of PML, you can read this paper. If you want to know more about Bayesian neural networks (i.e. neural networks that learn a probability distribution over functions that are consistent with the data), you can read this paper.
So, nowadays, I wouldn't say that that statement is ""true"", although it was probably true when that book was published.  More and more, people in the machine learning community do research in PML and, in particular, BDL, because, if you want to adopt neural networks in areas like healthcare, you need to provide the doctors with some kind of uncertainty quantification of the predictions. Let's say that that a doctor needs to take an action, such as giving some kind of medicine to a patient based on the condition of the patient (e.g. temperature). The doctor doesn't just want to know ""yes, give the medicine to the patient"". The doctor wants to have an idea of how confident or uncertain the model is about its prediction. This is also where another subfield of AI comes into play, i.e. explainable AI, but I will not dwell more on this topic here.
"
Why data required for hyperparameter tuning is considered as an additional data?,"
Any parametric model may have parameters as well as hyperparameters. Learning algorithm deals with parameters and hyperparameters should be dealt outside learning algorithm. Consider the following paragraph from Chapter 5: Machine Learning Basics from the book titled Deep Learning (by Aaron Courville et al.)

Most machine learning algorithms have settings called hyperparameters,
which must be determined outside the learning algorithm itself; we
discuss how to set these using additional data.

My doubt is about the usage of the word 'additional' in the paragraph. Afaik, a small part of dataset under consideration is used to validate and hence in determining the hyperparameters, called as validation data. It is also a part of the dataset as training and testing data. You can check the section 5.3 for more details.
If yes, what is the need for the usage of the word 'additional'? Is it true that data for setting hyperparameters is taken outside of the underlying dataset?
","['datasets', 'hyper-parameters']","
I think it just tries to emphasize that you need three, non-intersecting, chunks of datasets: training, validation, and test. So, you need some data in addition to the training data to tune hyperparameters. You can simply create the train/test/validation splits by sampling without replacement from an initial dataset. You don't need anything additional than this initial dataset.
"
Implementing Multiple NNs in one DQN model?,"
I'm trying to build a DQN Agent to take a set of 10 best actions simultaneously (integer values from 1 to 100) as outputs per episode. The input is a float. The goal is to find the optimal combination of (10) actions per episode.
Currently, the set up is having a single NN output 10 actions w/ the highest q-valules for each episode. But in the Memory Replay process, each individual Set (of 10 fixed actions obtained from the exploration phase) is being treated as a single action. Because the target network also takes the output of the list of 10-action from the main NN. Hence I can see the agent repeatedly trying certain Set (with a fixed 10 actions) in the replay/retrain part, whereas our goal is to find the optimal combination of 10 actions per episode, Not the optimal Set of fixed combinations. So in essence, I would like the agent to pick out and mix up the actions from the Sets with higher Q-values (known from the exploration phase) to form new optimal ""Sets"" in the Replay process.
I was thinking maybe instead of using a single NN with 10 outputs I could do 10 NNs with single outputs for each episode so that each action is treated separately. And I suppose I will have 10 q-networks and target networks as well, then I could combine the results by the end of each episodes. But, I am not sure if that is necessarily the best way to fix the problem of having repetitive sets of fixed action in the replay process.
Alternatively, I think the problem could be treated as a multi-armed bandit problem, except each arm here has ""sub-arms"" too so to speak, but that could require some changes to the custom environment I am working with and I don't want to touch that unless necessary.
Maybe there is a clever manipulation within the retrain process given my current setup that I am not seeing. Here is a snippet of the code for some more clarity.
class DQNAgent():

    def __init__(self,optimizer):
        # Initialize atributes
        self._state_size = 1
        self._action_size = 76
        self._optimizer = optimizer
        
        self.experience_replay = deque(maxlen=2000)
        
        # # Initialize discount and exploration rate
        # self.gamma = 0.6
        # self.epsilon = 0.5
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.95
        self.learning_rate = 0.01
        
        # Build networks
        self.q_network = self._build_compile_model()
        self.target_network = self._build_compile_model()
        
    
    def store(self, state, action, reward, next_state, terminated):
        self.experience_replay.append((state, action, reward, next_state, terminated))
        
    def _build_compile_model(self):
        
        model = Sequential()
        model.add(InputLayer(input_shape=(self._state_size,)))
        model.add(Dense(100, activation='relu'))
        model.add(Dense(100, activation='relu'))
        model.add(Dense(self._action_size, activation='linear'))
        model.compile(loss='mse', optimizer=self._optimizer)
        return model
    
    def alighn_target_model(self):
        self.target_network.set_weights(self.q_network.get_weights())

    def retrain(self, batch_size):
        if len(self.expirience_replay) < batch_size:
            return
        minibatch = random.sample(self.expirience_replay, batch_size)
    
        for state, action, reward, next_state, terminated in minibatch:
            
            target = self.q_network.predict(np.reshape(np.array(state), (-1,1)))
            print('target size :', np.shape(target))
            
            
            if terminated:
                target[0][action] = reward
            else:
                t = self.target_network.predict(np.reshape(np.array(next_state), (-1,1)))
                target[0][action] = reward + self.gamma * np.amax(t)
    
            self.q_network.fit(np.reshape(np.array(state), (-1,1)), target, epochs=1, verbose=0)
        
    
    def act(self,state):
        self.epsilon *= self.epsilon_decay
        self.epsilon = max(self.epsilon_min, self.epsilon)
        action_space = [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,
       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,
       69, 70, 71, 72, 73, 74, 75, 76] #all 76 available nodes
        if np.random.rand() <= self.epsilon:
            return np.array(random.sample(action_space,10))-1 #-1 to match control's index

        q_values = self.q_network.predict(np.reshape(np.array(state), (-1,1)))
        print(""q_vals shape"",np.shape(q_values))
        print('q_vals type',type(q_values))
  
        top_actions_idx = q_values[0].argsort()[-10:][::-1]

","['deep-rl', 'dqn']",
Does it make sense to use BLEU or ROUGE for any machine translation task?,"
Many machine translation metrics such as BLEU or ROUGE are used to evaluate sequence to sequence models where, usually, the sequences are pieces of natural language.
Is it possible to use these metrics when the dataset is not constituted of natural language sequences? For instance, if the sequences are source code (in some programming language), does it still make sense to use BLEU or ROUGE? How ""good"" are these metrics in general?
","['machine-translation', 'metric', 'bleu']",
How does randomization avoid entering infinite loops in the vacuum cleaner problem?,"
Suppose we have a vacuum cleaner operating in a $1 \times 2$ rectangle consisting of locations $A$ and $B$. The cleaner's actions are Suck, Left, and Right and it can't go out of the rectangle and the squares are either empty or dirty. I know this is an amateur question but how does randomization (for instance flipping a fair coin) avoid entering the infinite loop? Aren't we entering such a loop If the result of the toss is heads in odd tosses and tails in even tosses?
This is the text from the book ""Artificial Intelligence: A Modern Approach"" by Russell and Norvig

We can see a similar problem arising in the vacuum world. Suppose that a simple reflex vacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent has just two possible percepts: [Dirty] and [Clean]. It can Suck in response to [Dirty]; what should it do in response to [Clean]? Moving Left fails (forever) if it happens to start in square
A, and moving Right fails (forever) if it happens to start in square B. Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments. Escape from infinite loops is possible if the agent can randomize its actions. For example, if the vacuum agent perceives [Clean], it might flip a coin to choose between Right and Left. It is easy to show that the agent will reach the other square in an average of two steps. Then, if that square is dirty, the agent will clean it and the task will be complete. Hence, a randomized simple reflex agent might outperform a deterministic simple reflex agent.

And this is the agent program from the same source:
function REFLEX-VACUUM-AGENT([location,status]) returns an action
 if status = Dirty then return Suck
 else if location = A then return Right
 else if location = B then return Left

","['intelligent-agent', 'norvig-russell', 'randomness', 'simple-reflex-agents']",
Is soft labeling the same thing as label smoothing?,"
I have some data with soft labels and I am trying to figure out the best approach to solve the problem with Machine Learning (since regular classification is of the table, i.e. hard labels). However, whenever I look up ""soft label"" materials, I keep getting pointed to label smoothing. Is this the main/only technique to deal with soft labels?
","['classification', 'image-recognition', 'data-labelling', 'labels']",
Markov Decision Processes with variable epoch lengths,"
I am working on modeling a transportation problem as an MDP. Multiple trucks move material from one node to various other nodes in a network. However, the time it takes a truck to travel between any 2 nodes is different based on distance, and decisions are made when a truck arrives at a node. There lies the problem. Is it possible to have an MDP where the length of time between decision epochs is not uniform?
The most similar MDP formulation I could find was the Semi-Markov Decision process, but that uses a random length epoch.
",['markov-decision-process'],"
Timesteps in an MDP do not need to be even, and they have no units, just an index. It is OK for real world clock time between time steps to vary as needed.
The MDP formulation assumes a ""turn-based"" process where the agent picks and action, the environment processes that action plus its own inherent rules, then the agent is contacted again when it is time to make a new decision.
This most common scenario assumes you always have adequate time to make a decision, and whenever a decision is needed, the timestep will be incremented, the agent presented with the reward and next state by the environment (or code that interfaces with the environment).
It looks like this would match your case, and that there is nothing to be concerned about.
There are real time systems where actions may be required reactively, and as fast and accurately as possible given sensor data collected at high frequency compared to the time it would take to analyse state and make a decision. There are some different approaches to this, and it is an area of active research, since the MDP formulation does not capture this. For instance the paper Real-Time Reinforcement Learning attempts to use a variation of MDP with time steps based on a real time metric, and allowances for decisions taking longer than one time step.
In practice, even video-game-playing systems such as classic Atari games are often treated as turn-based instead of real time. During training the emulator may be paused whilst the agent processes state information and learns.
"
Best algorithms/approaches for data sets of binary (1/0) features,"
I am working with a dataset with about 400 features, all binary (1 or 0). What approach would you recommend? Data set is about 500k records.
","['machine-learning', 'learning-algorithms']","
Most standard algorithms will work well on binary data, like:

Decision trees (and random forest)
Nearest Neighbors
Neural Networks
etc


But your choice depends on many other things, like:

What is the expected output?

Are you doing classification?
Regression?
Is it deterministic (same features should always give the same outcome) or stochastic (random factor).


What is the nature of the database and relationship between the features?

A 20x20 black-white image.
A phrase embedded as 20 sequences of 20-size-token.
A 400 questions true/false exam.


They can all have the same shape, but are very different in nature and would perform better with different algorithms.
How disperse / smooth is your data?
Do all 400 features have the same importance?
How independent are they?


How complex the problem really is?
How much performance do you really need?
How much work and tuning are you willing to put on this?

"
What is the borderline between unsupervised learning and regular algorithms?,"
Unsupervised learning using neural networks is clearly machine learning since it is utilising neural nets.
However, some algorithms, k-means clustering, for example, are considered unsupervised learning, while they look just regular algorithms (non-ML).
What should be the borderline (criteria) to differentiate between unsupervised learning and a non-ML algorithm?
","['machine-learning', 'comparison', 'unsupervised-learning', 'learning-algorithms', 'k-means']",
Why does a transformer not use an activation function following the multi-head attention layer?,"
I was hoping someone could explain to me why in the transformer model from the ""Attention is all you need"" paper there is no activation applied after both the multihead attention layer and to the residual connections.  It seems to me that there are multiple linear layers in a row, and I have always been under the impression that you should have an activation between linear layers.
For instance when I look at the different flavors of resnet they always apply some sort of non linearity following a linear layer.  For instance a residual block might look something like...
Input -> Conv -> BN -> Relu -> Conv -> (+ Input) -> BN -> Relu
or in the case of pre-activation...
Input -> BN -> Relu -> Conv -> BN -> Relu -> Conv -> (+ Input)
In all the resnet flavors I have seen, they never allow two linear layers to be connected without a relu in-between.
However in the the transformer...
Input -> Multihead-Attn -> Add/Norm -> Feed Forward(Dense Layer -> Relu -> Dense Layer) -> Add/Norm
In the multihead attention layer it performs the attention mechanism and then applies a fully connected layer to project back to the dimension of its input.  However, there is no non linearity between that and feed forward network (except for maybe the softmax used in part of the attention.)  A model like this would make more sense to me...
Input -> Multihead-Attn -> Add/Norm -> Relu -> Feed Forward(Dense Layer -> Relu -> Dense Layer) -> Add/Norm -> Relu
or something like the pre-activated resnet...
Input -> Relu -> Multihead-Attn -> Add/Norm -> Input2 -> Relu -> Feed Forward(Dense Layer -> Relu -> Dense Layer) -> Add/Norm(Input2)
Can anyone explain why the transformer is the way it is?
I have asked a similar question when I was looking at the architecture of wavenet on another forum but I never really got a clear answer. In that case it did not make sense to me again why there was no activation applied to the residual connections.
(https://www.reddit.com/r/MachineLearning/comments/njbjfb/d_is_there_a_point_to_having_layers_with_just_a/)
","['transformer', 'attention']",
Are goal-reaching and optimizing the utility function special cases of performance measure?,"
In AIMA, performance measure is defined as something evaluating the behavior of the agent in an environment.
Rational agents are defined as agents acting so as to maximize the expected value of the performance measure, given the percept sequence they have seen so far.
Goal-based agents are those acting to achieve their goals. Utility-based agents are those trying to maximize their own expected ""happiness"".
Now, can we say these two design approaches induce performance measures?
What I suggest is that in goal-based agent design we want to find a point satisfying some conditions, so it's an optimization problem with a zero objective function and either this function is the performance measure or performance measure is optimized if and only if we find a solution to this optimization problem with zero objective function. In the utility-based agent design, we have an objective function (as a performance measure) that we want to optimize, and the agent has its own utility function, which it wants to optimize, and this utility function is optimized if and only if our objective function is optimized.
","['intelligent-agent', 'performance', 'goal-based-agents', 'utility-based-agents', 'rational-agents']",
How does a VGG-based Style-Loss incorporate color information?,"
I've recently been reading a lot about style transfer, its applications and implications. I understand what the Gram matrix is and does. I can program it. But one thing that has been boggling me is: how does the VGG style loss incorporate color information into the style?
In the paper ""Texture Synthesis by CNNs"", Gatys et al. show that minimizing the MSE between the Gram matrices of a random white noise image and a ""target texture"" yields new instances of that texture, with stochastic variation. I understand that this must work, as the Gram matrix measures the correlation between features detected by the VGG activations across channels, without spatial relation. So if we optimize the white noise image to have the same Gram matrix, it will exhibit the same statistics, and hence look like an instance of the original texture.
But how does this work with color? Of course, the VGG could learn something like a mean filter, with all ones, whose output would be the avg. color over that filter kernel. After all, ""color"" is just another statistic. But then when using that in conjunction with the Gram loss, wouldn't this information be lost, as it's all just correlation and hence ""relative"" to each other?
While writing this question, I'm starting to think of it like this: Maybe the feature correlation expresses these color constraints in some form like: ""if one part is red, there must be a green part close to it"" (for the radish), or ""if there is a rounded edge, one side of it must be in shadow (=darker)"" in case of the stone texture. This would tie color to the surrounding statistics (e.g., edges, other colors) and is the only reason I can think of why this works at all.
Can somebody confirm/refute this, and share their thoughts? Happy to discuss!

Image Source: Texture Synthesis by Convolutional Neural Networks, Gatys et al.
","['convolutional-neural-networks', 'image-generation', 'statistics', 'vgg', 'style-transfer']","
My two cents on this topic:
""After all, ""color"" is just another statistic"", I think this is the simple (and correct) answer to the question. To go a bit deeper, you can check this paper, which shows how minimizing a loss based on the Gram matrix is mathematically equivalent to minimizing the Maximum Mean Discrepancy between the inputs and targets distribution. The two distributions inevitably contains information about colors, so while disentangle spatial features is rather simple (you could simply show one pixel at the time instead of an image),  disentangling colors is much more tricky, cause it's an intrinsic characteristic of each point.
A last remark from my side is that the main problem when working with style transfer is precisely that ""style"" mean everything. This is not a problem for papers that simply try to achieve it per se, i.e. without a real use case in mind, but it becomes fundamental in real applications. A concrete example of this is super resolution. Many papers try to achieve it with style transfer, coupling low resolution and high resolution images. Ideally the features you would like to transfer are enhanced sharpness and maybe texture injection for details generation. Problem is that along with them there are always side features that hinder the quality of the resulting image, among which noise specific to the target domain, and also colors.
"
How to understand slope of a (non-convex) function at a point in domain?,"
Consider the following paragraph from Numerical Computation of deep learning book that says derivative as a slope of the function curve at a point

Suppose we have a function $y= f(x)$, where both $x$ and $y$ are real
numbers. The derivative of this function is denoted as $f'(x)$ or as
$\dfrac{dy}{dx}$ . The derivative $f'(x)$ gives the slope of $f(x)$
at the point $x$",,
What are the Calculus books recommended for beginner to advanced researchers in artificial intelligence?,"
Calculus is a branch of mathematics that primarily deals with the rate of change of outputs of a function w.r.t the inputs.
It contains several concepts including limits, first-order derivatives, higher-order derivatives, chain rule, derivatives of special and standard functions, definite integrals, indefinite integrals, derivative tests, gradients, higher-order gradients, and so on...
Calculus has been heavily used in optimization and maybe in several other aspects of artificial intelligence.
What are the Calculus textbook(s) recommended that cover all the concepts required for a researcher in artificial intelligence?
","['reference-request', 'math', 'books', 'calculus']","

Answer: Calculus James Stewart is the best for a beginner.

I started to learn Calculus studying engineering with James Stewart Calculus ( maybe the best for beginners and is really didactic ), Problems in Mathematical Analysis Demidovich ( best for me because simplicity, fast, but few multivariable focus and difficult for learn ), Nikolai Piskunov - Differential and Integral Calculus (again difficult to learn but teachers used for prepare his test), Calculus with Analytic Geometry Swokosky, Louis Leithold Calculus and Purcell Calculus. this books are the popular base books for an engineering degree in mostly all universities.
However the best way for approach Calculus to Artificial Intelligence is focus in the chapter that are directly related to IA and we have:

Multivariable Calculus (also could help for understand faster linear algebra, eigen values&vectors, $ R ^n $ spaces,etc )
Directional Derivatives ( For Gradient Descent )
Infinite Sequences and Series
Partial Derivates ( you need know one variable derivatives for go to Partial Derivatives )
Vector Calculus
Jacobian
Of course all that need a deep understanding of integrals and derivatives no forget.
Calculus.
none of this can be learned without knowing algebra, matrices, geometry, trigonometry and logic math ( Elementary Subjects )

I can tell you my experience learning Calculus James Stewart 7th and build a summary about the topics.

Multivariable Calculus 6 Chapters about that.
Directional Derivatives Section 14.6
Infinite Sequences and Series Chapter 11
Vector Calculus Chapter 16
Jacobian Transformation ( 15.10 Change of Variables in Multiple Integrals)
Of course all that need a deep understanding of integrals and derivatives no forget. 6 Chapters about that.

As a reference this is the index:

I am looking forward for more books about advanced calculus especially with focus in multidimensional calculus or applied math for artificial intelligence, I have found more books about statistics approach than calculus/math approach.
"
When to activate batch normalization and dropout in deep Q-learning?,"
In the vanilla version of deep Q-learning, there are three places where the Q-network is queried:

When exploring.

When training:
a. When calculating the optimal value of the state reached by an action (so as to compute a target discounted reward).
b. When calculating the optimal Q-value for a given state, during training (so as to nudge the network weights and better reproduce the observed reward).


Now, during which steps should batch normalization and dropout be activated?
I couldn't find anything through a Google search.
Here are my guesses, for each step:

When exploring: activate batch normalization and dropout: this lets the normalizations to be learned, and gives a chance to uncertain Q-values to be selected even if they are relatively low (because the dropout can result in a Q-value prediction higher than its average).

When training:
a. Do not activate batch normalization and dropout for calculating the optimal state value of the state reached by an action, because we want the Bellman equation to converge faster and therefore prefer stable (optimal state value) targets.
b. Activate batch normalization and dropout when calculating the Q-value of a chosen action, as this is the whole idea of dropout (we use it during training).


What is the common wisdom on this?
","['reinforcement-learning', 'q-learning', 'deep-rl']","
Batch Normalization should be applied between all layers and their activation functions excluding the output layer. This squishes the ranges of numbers in a better range for neural network to build appropriate sized gradients.
I've not seen much use of Dropout in Deep RL because the networks are usually small and overfitting isn't as much of a problem as in supervised learning.
"
Policies for which the policy improvement theorem holds,"
According to Reinforcement Learning (2nd Edition) by Sutton and Barto, the policy improvement theorem states that for any pair of deterministic policies $\pi'$ and $\pi$, if $q_\pi(s,\pi'(s)) \geq v_\pi(s)$ $\forall s \in \mathcal{S}$, then $v_{\pi'}(s) \geq v_\pi(s)$ $\forall s \in \mathcal{S}$.
The proof of this theorem seems to rely on $\pi$ and $\pi'$ being identical for all states except $s$. To my best understanding, this is what allows us to write the expectation $\mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = a, A_t = \pi'(s)]$ as $\mathbb{E}_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = a]$ in line 2, which is central to the proof (re-produced from the book below).
\begin{aligned}
v_{\pi}(s) & \leq q_{\pi}\left(s, \pi^{\prime}(s)\right) \\
&=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=\pi^{\prime}(s)\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, \pi^{\prime}\left(S_{t+1}\right)\right) \mid S_{t}=s\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma \mathbb{E}_{\pi^{\prime}}\left[R_{t+2}+\gamma v_{\pi}\left(S_{t+2}\right) \mid S_{t+1}, A_{t+1}=\pi^{\prime}\left(S_{t+1}\right)\right] \mid S_{t}=s\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} v_{\pi}\left(S_{t+2}\right) \mid S_{t}=s\right] \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} v_{\pi}\left(S_{t+3}\right) \mid S_{t}=s\right] \\
& \vdots \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\cdots \mid S_{t}=s\right] \\
&=v_{\pi^{\prime}}(s)
\end{aligned}
Does this mean that the proof is merely proving the special case of the policy improvement theorem for when the policies are identical except at $s$? I am having trouble seeing why the proof holds for the more general case of the two policies being potentially different for all states. In that case, line 2 would not hold and the theorem would not hold for all states as it claims to do.
","['reinforcement-learning', 'proofs', 'sutton-barto', 'policy-improvement-theorem']",
Using Human Confirmation in place of a loss Function for Training,"
Has there been any experimentation in designing an AI to prompt a human to judge the accuracy of it's outcomes? instead of using a loss function, a human can judge the accuracy of it's estimation using some kind of metric, where it can then use that too update it's weights.
I was looking for some feedback on whether this is a plausible idea.
I was thinking that for domains that lack sufficient training data to solve problems this could be a possible solution.
Of course, it isn't feasible to judge every iteration of a training loop. So maybe feedback could be provided for the average of a number of estimations. Maybe every 100 estimations you could provide feedback.
It may not be a great training method because of the sparsity of feedback, but it could provide a place to start if you don't have a lot of data to throw at your problem initially.
","['training', 'ai-design']",
How to localize and classify objects in video,"
What methods are used to localize an object in an video and classify that object?
Example: I have a camera which detects an pickup truck driving into a garage of three (1,2,3). In need to know if the truck was loaded or not (classification) and which garage it picked (localization). How would a schematic workflow of this problem look like?
It is assumed that the camera is mounted in a fixe position.
",['image-recognition'],
Can brain simulation be done using Tensor Processing Units?,"
A potential way to solving AI is via whole brain simulation. Currently we have the algorithm to model a human brain (albeit far from perfectly): https://thenextweb.com/news/theres-an-algorithm-to-simulate-our-brains-too-bad-no-computer-can-run-it
It is estimated that we might need 100 petaflops to 1 exaflop of computing power to run a brain simulation in real time. Google's Tensor Processing Unit pods, however, have already achieved 1 exaflop of computing power a while back: https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests
Since these brain simulations are basically giant spiking neural networks, can they be run on Tensor Processing Units (TPUs), which are specifically designed for neural networks? Since TPU pods can do an exaflop, they might pack enough power to finally run a whole brain simulation?
","['agi', 'brain', 'spiking-neural-networks']",
Role of confidence or classification score in object detection mAP metrics,"
I know that mAP (mean Average Precision) is the common evaluation metric for the object detection tasks. It uses IoU (Intersection over Union) threshold such as mAP@0.5 to evaluate whether the predicted box is TP (True Positive), FP (False Positive), or FN (False Negative).
But I am confused about the role of classification score in this metric since the positive and negative is determined by the IoU, not the classification score. So, what is the role of classification scores in mAP evaluation?
Let's describe it by example, suppose there is a single object in an image with the ground-truth as follows:

Bounding boxes: [[100, 100, 200, 200]]
Class Index: [0]

Then the prediction of the object detection model resulting as follows:

Bounding boxes: [[100, 100, 200, 200], [100, 100, 200, 200], [100, 100, 200, 200]]
Class Indexes: [3, 2, 0]
Class Scores: [0.9, 0.75, 0.25]

When I tried to calculate the mAP using this library: https://pypi.org/project/mapcalc/
The mAP score is 1.0. So I am confused in the mAP point of view, this prediction is calculated as the correct prediction? So what is the role of classification score in this case? Should we also define the classification score threshold when using mAP?
","['machine-learning', 'computer-vision', 'object-detection', 'metric']","
Nice question got me thinking, hope these help you understand better Source

MS COCO — uses 101-Recall-points on PR-chart mAP (mean Average Precision) for Object Detection
PascalVOC2007 — uses 11-Recall-points on PR-chart
PascalVOC2010–2012 — uses Area-Under-Curve on PR-chart
ImageNet — uses Area-Under-Curve on PR-chart

I too have to understand it further
"
Do solving system of linear equations required anywhere in contemporarty deep learning?,"
Consider the following from Numerical Computation chapter of Deep Learning book

Machine learning algorithms usually require a high amount of numerical computation. This typically refers to algorithms that solve
mathematical problems by methods that update estimates of the solution
via an iterative process, rather than analytically deriving a formula
to provide a symbolic expression for the correct solution. Common
operations include optimization (ﬁnding the value of an argument that
minimizes or maximizes a function) and solving systems of linear
equations. Even just evaluating a mathematical function on a digital
computer can be diﬃcult when the function involves real numbers, which
cannot be represented precisely using a ﬁnite amount of memory.

The paragraph clearly mentions that solving system of linear equations is a common operation in machine learning. I just know that solving system of linear equations is useful in reinforcement learning and some basic algorithms of machine learning including regression.
Is solving system of linear equations useful anywhere in deep learning?
I think that we use them nowhere since optimization is the only algorithm generally used in deep learning.
","['deep-learning', 'linear-algebra']","
I guess a first distinction should be made between deep learning as a whole or deep learning as architecture.
I think the paragraph you quote refers to solving systems of linear equations as a simple operation involved in deep learning generically. And this is definitely the case, when training a deep model we're always solving systems of linear equations, think about the way weights and biases are applied to an input:
$W*x + b$
this is in itself a system of linear equations.
Then, moving away from training and model architectures, in deep learning there is still a massive use of dimensionality reduction techniques involved in pre or post processing of the data like SVD or PCA that also consists in solving systems of linear equations (an we could add any matrix factorization technique, relevant especially in the early methods for word embedding training before the advent of transformers).
"
What is the intuition behind comparing action values to state values in the policy improvement theorem?,"
Sutton and Barto, in their book (Reinforcement Learning 2nd Edition) begin the discussion of policy improvement by comparing the action value $q_\pi(s, \pi'(s))$ to the state value $v_\pi(s)$.
What is the intuition behind this comparison?
It seems more natural to me to compare $q_\pi(s, \pi'(s))$ and $q_\pi(s, \pi(s))$. I understand that for deterministic policies $q_\pi(s, \pi(s))$ is the same as $v_\pi(s)$ so mathematically it makes no difference but perhaps conceptually it does?
","['reinforcement-learning', 'value-functions', 'policy-improvement-theorem']",
"What does it mean ""having Lipschitz continuous derivatives""?","
We can enforce some constraints on functions used in deep learning in order to guarantee optimizations. You can find it in Numerical Computation of the deep learning book.

In the context of deep learning, we sometimes gain some guarantees by
restricting ourselves to functions that are either Lipschitz
continuous or have Lipschitz continuous derivatives.

They include

Lipschitz continuous functions
Having Lipschitz continuous derivatives

The definition given for Lipschitz continuous function is as follows

A Lipschitz continuous function is a function $f$ whose rate of
change is bounded by a Lipschitz constant $\mathcal{L}$:
$$\forall x, \forall y, |f(x)-f(y)| \le \mathcal{L} \|x-y\|_2 $$

Now, what is meant by having Lipschitz continuous derivatives?
Does they refer to the derivatives of Lipschitz continuous functions?  If yes, then why do they mention it as a separate option?
","['deep-learning', 'math', 'derivative']","
Consider a function $f(x) : \mathcal{R}^m\rightarrow\mathcal{R}^n$ defined for $x \in X$. If $f$ is Lipschitz continuous, it has three main properties:

$f(x)$ is continuous for all $x \in X$
$\frac{d f(x)}{d x}$ exists almost everywhere. Meaning, if the derivative is not defined for $x \in \mathcal{B}$, where the set $\mathcal{B} \subset X$, then $\mathcal{B}$ has measure zero.


$\underset{x \in X}{\sup} \left\lVert \frac{d f(x)}{d x} \right\rVert_2 \leq L$, where $L$ is the Lipschitz constant, and the norm indicates the induced matrix norm (or if $f$ is scalar, just the regular 2 norm).

So it follows that a Lipschitz continuous function is continuous and has a bounded jacobian.
Now if $f$ has a lipschitz continuous derivative, then it means $\frac{d f}{d x}$ is Lipschitz continious, i.e.
\begin{align*}
\left\lVert \dfrac{d f}{d x}\big|_{x = s} - \dfrac{d f}{d x}\big|_{x = t} \right\rVert_2 \leq M \left\lVert s - t \right\rVert_2 \quad s, t \in X
\end{align*}
where $M$ is the lipschitz constant. So a function with Lipschitz continuous gradient is continuously differentiable and has a bounded hessian.
"
Which class of functions are quite complicated in deep learning?,"
Deep learning is a field in which we need neural networks that are deep enough to carry on our task. The important fucntions in deep neural networks can be classified in to three classes: activation function, neural network function and loss function.
Activation functions are a part of neural network function and neural network functions may be a part of loss functions.
Consider the following paragraphs from Numerical Computation of a deep learning book

Optimization algorithms that use only the gradient, such as gradient
descent, are called ﬁrst-order optimization algorithms. Optimization
algorithms that also use the Hessian matrix, such as Newton’s method,
are called second-order optimization algorithms (Nocedal and Wright,
2006).
The optimization algorithms employed in most contexts in this book are
applicable to a wide variety of functions but come with almost no
guarantees. Deep learning algorithms tend to lack guarantees because
the family of functions used in deep learning is quite complicated. In
many other ﬁelds, the dominant approach to optimization is to design
optimization algorithms for a limited family of functions.

The last passage is talking about the family of functions used in deep learning. Which class of functions, among the three I mentioned, they are referring to?
","['deep-learning', 'optimization']","
From the phrasing, it seems that complicated refers to the non-convexity of the loss landscapes of neural networks. We do not have formal guarantees of convergence in general for such landscapes. This non-convexity is a property of both the function defined by the neural network, and the particular loss function we use.
In practice though, non-convexity stems from the non-linear activation functions as we almost exclusively use cross-entropy loss when training neural networks.
"
Why must the value of a state under an optimal policy equal the expected return for the best action from that state?,"
The Sutton and Barto reinforcement learning textbook states that

the value of a state under an optimal policy must equal the expected return for the best action from that state.

That is,
$$v_*(s) = \max_a q_*(s, a).$$
I am having trouble gaining intuition for this. Since state values can be written as an expectation of the action values under a given policy, I am not sure I see how
$$v_*(s) = \sum_a \pi_*(a|s)q_*(s,a) = \max_a q_*(s, a).$$
I'd appreciate any insights!
","['reinforcement-learning', 'value-functions', 'policies', 'bellman-equations']",
Is it true that real world data is highly discontinuous?,"
A function $f$ is said to be continuous at a point $c$ if it satisfies three properties:

Should be defined at the point $c$
Left and right-hand limits at $c$ must be equal i.e., the limit must exist
Limit value at point $c$ is equal to the actual value of the function at c

In short:   $\lim \limits_{x \rightarrow c} f(x) = f(c)$
I want to know whether the functions that we want to learn through real-world data, say generator in GAN, such as images, audio, video, text corpora, etc., are continuous or highly discontinuous in general? If discontinuous, what might be the reason for discontinuity? I mean, which among the three properties mentioned got a violation in the majority of cases?
","['datasets', 'math', 'data-preprocessing', 'function-approximation']",
Do gradient-based algorithms deal with the flat regions with desired points?,"
I am studying a chapter named Numerical Computation of a deep learning book. Afaik, it does not deal with flat regions with desired points.
For example, let us consider a function whose local/global minimum or maximum values lies on flat regions. You can take this graph (just) for example.

Can gradient-based algorithms work on those curves with their local/global minima, or do maxima lie on flat regions?
","['optimization', 'gradient-descent']","

Can gradient-based algorithms work on those curves with their local/global minima, or do maxima lie on flat regions?

Yes, with some minor caveats.
All the points on the flat region are equivalent (and in your example, are all valid global minimum points). Gradients outside of the region will point correctly away from that region and gradient descent steps will therefore move parameters towards it.
Provided the step size multiplied by the gradient near the flat region is not too large, then a step taken near it will end up with parameters inside the region. After that, then any further gradients will be zero, so it is not possible to use basic gradient steps to escape it.
In the case of a global minimum, that's fine, you don't care which point in the global minimum you have converged to (otherwise your function to optimise would be different).
In the case of local minima or saddle points, you might care to use optimisation methods that can escape flat areas. Minibatch or stochastic gradient descent can do this because gradient measurements are noisy, whilst momentum algorithms can continue making steps when the immediate gradient signal is zero.
The example function you used is not something you would expect to come across when optimising a machine learning algorithm, although some loss functions do have components that have similar behaviour. For example, triplet loss uses a $\text{max}(d_1 - d_2 + \alpha, 0)$ where $d_1$ and $d_2$ are distances between an anchor image and desired class versus different class respectively, and $\alpha$ is a margin or minimum desirable distance between classes. The details of this are not important unless you want to create a face recogniser or similar - the important detail for your question is that $\text{max}(x, 0)$ is really used in ML as a loss function, and may have a similar shape to your example function. Once used in aggregate with many data examples though, and with regularisation, the shape would not be so simple, and proabably would not have any reachable flat minima regions like this.
"
How is Google Translate able to translate texts of arbitrarily large length?,"
Sequence-to-sequence models with attention are known to be limited by a maximum sequence length. So how can we handle sequences of arbitrarily large size? Do we just set a very large maximum sequence length?
","['recurrent-neural-networks', 'transformer', 'machine-translation']","
You simply split the sequence into smaller sequences; while there are some long-distance dependencies in language, that is generally not a problem for this.
A sentence would typically be short enough, and very long sentences are composed of shorter clauses which would form independent units (albeit connected with each other).
"
How do I create the search tree for DFS applied to a grid map?,"
I have been working through some search tree problems and came across this one:

Assume that that the algorithm has a closed list and that nodes are added to the frontier in the following order: Up, Right, Down, Left. For example, if node J is expanded: there is no node up from J so nothing is added to the frontier for up. K is right from J so it is added to the frontier, H is down from J so it is added to the frontier, there is no node left from J, so nothing is added to the frontier.
a)    Assume that the start node is node F and the goal node is node M. Provide the entire search tree if Depth First Search is employed.
b)    Provide the frontier at the time the search terminates
Because I understand how a depth-first search works with regards to the frontier (it is a LIFO queue), I know that the last node added to the frontier would be the next node you need to expand. Using that knowledge, the frontier would be as follows after each expansion:

F
F I B E
E is expanded: F I B H A
A is expanded: F I B H
H is expanded: F I B J
J is expanded: F I B K
K is expanded: F I B L
L is expanded: F I B M

The solution has been found, as we have reached M.
I thus seem to have answered part b of the question, but as for how to draw the search tree, I am stumped. Any ideas would be appreciated.
","['search', 'depth-first-search']",
How to classify two very similar images using Deep Learning?,"
I am a newbie in Computer Vision.
I have a scenario in which I have a stationary camera in a factory. I want to detect whether the technician is working on the machine or not.
Images are like the following:
Technician working:

Technician absent:

Technician not working:

I am confused whether is it a Image classification issue or an Object Detection/Pose Detection problem.
As per my knowledge this should be a classification problem, I should take multiple images of a condition in which the machine is unattended, and a condition in which the technician is working on the machine.
I will train the model if with different individual technicians on different days with different clothes.
Now if I am in the right direction, how much images do I need to have a good accuracy?
I see there are different models on Tensorflow Hub on image classification like EfficientNet, etc.
Which model/architecture will work for me?
I am sorry if I sound noobish.
I can train the model using simple classifiers' code (like Cat vs Dog), but I want the my architecture to understand that there is an area in the image which should only be checked if it is occupied or not to classify properly.
OR
Shall I cut out the middle area (where technician stands) simply using opencv. And then feed that cutout image to some classifier to detect if there is a human standing there?
Thanks in advance!
","['computer-vision', 'image-recognition', 'opencv']","
This is a bit old question, but I'll answer anyway. Naturally the more you have data the better, but rather than capturing an image (for example) every second, I would rather capture an image every 5 minutes for a period of at least 30 days. This would give you 8 * 60 / 5 = 96 pictures per work day. But naturally you can start developing the algorithm at day one.
Generating test & validation sets on ""time series"" data can be tricky, I would assign complete work days to the validation set rather than taking random samples. This way you can test that the code works even when people are wearing different clothing, and the weather could be different as well.
For supervised learning you must manyally classify all of the images, since there aren't that many images I would go this router rather using a more difficult semisupervised learning. As a human you have some domain expertice on the image content, I would crop the image quite tightly around the area of interest so the network doesn't need to learn to ignore certain areas by itself.
You could have some edge-cases to classify, for example if a technician is just standing in front of the machine, is he considered working? How about if the is facing the other way? Depending on how you want to handle these, your network can either be very simple or a bit more advanced.
Maybe you don't need a neural network at all, and you can just pass a low-res image through LDA / Fisher Linear Analysis. Or create pixel-wise histograms of the colors for working and non-working image classes separately, and create heuristics based on that.
Once you have images from different days, pay attention on what kind of variance there is in colors, brightness, camera noise etc. Then create a data augmentation pipeline which mimics the observed deviations, this way you can make the model much more robust and you don't need to wait several months to start getting good results.
"
A comparison of Expert Systems and Machine Learning approaches in terms of run-time-efficiency and time/space complexity,"
For part of a paper I am writing on Clinical Decision Support Systems (computer-aided medical decision making, e.g. diagnosis, treatment), I am trying to compare Expert Systems with systems based on Machine Learning approaches (Deep Learning, Artificial Neural Networks, etc.).
Specifically, I am currently trying to make a general comparison (if possible) of expert systems with machine learning systems across dimensions of efficiency and complexity, i.e.

run-time-efficiency
time complexity
space complexity

My current line of thinking, after having tried to find literature with limited success, is that, in the case where one is trying to answer questions in a very specific, limited, domain that only requires a few rules (for an expert system), expert systems are relatively ""cheap"" in terms of these three criteria. However, when a problem/domain becomes more complex, expert systems seem to suffer from the fact that the number of rules needed ""explodes"", which, I would think, could lead to things such as large search trees or other problems. My feeling from what I have generally read about machine learning approaches is that these adapt better to more complex problems with higher dimensionalities.
I would like to find some information that either confirms/backs up my general impression, or guides me to some other understanding of this.
Unfortunately, I can't seem to find any sources that specifically deal with this kind of comparison. I'm not sure if I this is because my problem statement is to wide/vague, I am not searching correctly, there just isn't much literature, or my question doesn't make sense.
Some of the sources I did manage to find are:

Expert systems are still used and important in areas such as robotics
and monitoring. However, the complexity of advanced rules systems can
lead to performance issues. ANNs are currently managing to overcome
such performance issues through scale-out.
Source: Forbes

Unfortunately, this is the most explicit source I've found. However, it doesn't really provide any details on which this claim could backed up, nor would I consider this a solid source, especially not in an academic setting.

Checking for the logical consistency of a set of interrelated logical rules results in the formulation of a
satisfiability (SAT) problem [Bezem, 1988]. If one assumes only binary variables, say n of them, then the
corresponding search space is of size 2n
. That is, it can become very large quickly. This is an NP-complete problem
very susceptible to the “dimensionality curse” problem [Hansen and Jaumard, 1990]
Source: Yanase J, and Triantaphyllou E, 2019, A Systematic Survey of Computer-Aided
Diagnosis in Medicine: Past and Present Developments, page 7

This mentions ""dimensionality curse"", but in the context of checking for logical consistency of the rules of an expert system, and not really in the context of run-time-efficiency & complexity.
I have found numerous other articles comparing expert systems and machine learning approaches, e.g. Ravuri et al., 2019, Learning from the experts:
From expert systems to machine-learned diagnosis models, but none of them, from what I have seen, compare expert systems and machine learning approaches across the dimensions I am interested in.
Would anyone be able to provide some input on what would be aspects in comparing expert systems and machine learning approaches in terms of the efficiency and complexity criteria listed above, and/or, be able to point me in the right direction?
","['machine-learning', 'comparison', 'efficiency', 'expert-systems', 'decision-support-system']",
Is there a survey that describes the most effective approaches for an answer retrieval problem?,"
I have a dataset that contains pairs of a question and an answer. My problem is to train a model that can search for the right answer from the pool of my answers given the newly input question, so this is a kind of answer retrieval problem.
Can anyone provide me a survey and effective approaches for this problem?
","['natural-language-processing', 'reference-request', 'question-answering', 'information-retrieval']",
Why does critical points and stationary points are used interchangeably?,"
Consider the following paragraph from Numerical Computation of the deep learning book.

When $f'(x) = 0$, the derivative provides no information about which
direction to move. Points where $f'(x)$ = 0 are known as critical
points, or stationary points. A local minimum is a point where
$f(x)$ is lower than at all neighboring points, so it is no longer
possible to decrease $f(x)$ by making inﬁnitesimal steps. A local
maximum is a point where $f(x)$ is higher than at all neighboring
points so it is not possible to increase $f(x)$ by making inﬁnitesimal
steps. Some critical points are neither maxima nor minima. These are
known as saddle points.

In short, points where $f'(x) =0 $ are called critical points, or stationary points.
But, according to mathematical terminology, the definitions are as follows:
#1: Critical point

A function $y=f(x)$ has critical points at all points $x_0$ where
$f'(x_0)=0$ or $f(x)$ is not differentiable.

#2: Stationary point

A point $x_0$ at which the derivative of a function $f(x)$ vanishes,
$f'(x_0)=0$. A stationary point may be a minimum, maximum, or
inflection point.

It can be noticed that the definitions that are given in the deep learning book do match exactly with stationary points since the only premise is $f'(x)=0$. The definition for critical point is not apt since a critical point can also be a point where $f'(x)$ is nonexistent.
Is there any reason for using the terms critical points and stationary points interchangeably? Is there no need to address the points where $f'(x)$ does not exist?
","['terminology', 'books', 'derivative']","
A critical point of a function $f$ can be

a stationary point (i.e. $f'(x) = 0$), or
a point where the derivative is undefined (for example, in the case of the absolute value function $f(x)$, $x=0$ is a critical point, as $f$ is not differentiable at $x=0$).

So, all stationary points are critical points.
These notes provide more examples of how to find critical points of a function, so they could be useful.
"
How to prove importance sampling ratio is uncorrelated with action-value (or state-value) estimate?,"
In Sutton & Barto (2nd edition), the following is mentioned on page 150 (p. 172 of the pdf), section 7.4:

the importance sampling ratio has expected value one (Section 5.9) and is uncorrelated with the estimate.

How can we prove the importance sampling ratio is uncorrelated with the estimate?
","['proofs', 'off-policy-methods', 'sutton-barto', 'importance-sampling', 'model-free-methods']","
Sutton and Barto explain it themselves in section 5.9. I post it with a bit of context. The equation you're looking for is 5.13.
"
How to calculate policy probability ratio in multiple action space,"
I try to solve a navigation problem with PPO; my actions space have three-part:

robot linear velocity that is in [-3,3] range (getting from a tanh activation func)
robot linear angular that is in [-pi/6, pi/6] range (getting from a tanh activation func)
robot step-time duration that is from [0.2, 0.5, 0.8] (getting from a softmax activation func)

The problem that I face is how to calculate the ratio of probability from this separate disturbing?
Mean or sum? or was there another way to calculate log_prob from different distributions? Something like log_prob from multivariable distribution!
","['reinforcement-learning', 'probability-distribution', 'proximal-policy-optimization']",
How many directions of gradients exist for a function in higher dimensional space?,"
Gradients are used in optimization algorithms. Based on the values of gradients, we generally update the weights of a neural network.
It is known that gradients have a direction and the direction opposite to the gradient should be considered for weight updation. In any function of two dimensions: one input, and one output, there are only two possible directions for any gradient: left or right.
Is the number of gradient directions infinite in higher dimensions ($\ge 3$)? Or does the number of possible directions are $2n$ where $n$ is the number of input variables?
","['optimization', 'gradient', 'calculus', 'dimensionality']","
Let's look at the definition of gradient:

In vector calculus, the gradient of a scalar-valued differentiable function $f$ of several variables is the vector field (or vector-valued function) $\nabla f$ whose value at a point $p$ is the vector $r^{[a]}$ whose components are the partial derivatives of $f$ at $p .^{[1][2][3][4][5][6][7][8][9]}$ That is, for $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$, its gradient $\nabla f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is defined at the point $p=\left(x_{1}, \ldots, x_{n}\right)$ in $n$-dimensional space as the vector: $^{[b]}$
$$
\nabla f(p)=\left[\begin{array}{c}
\frac{\partial f}{\partial x_{1}}(p) \\
\vdots \\
\frac{\partial f}{\partial x_{n}}(p)
\end{array}\right]
$$

First of all, the gradient is not a single value or a vector, it's an operator that given a function returns another function (note that in the definition $$\nabla f$$ map from $\mathbb{R}^n$ to $\mathbb{R}^n$ again), which can be used to compute a vector for each point of a field. So, it doesn't really make sense to talk of a gradient direction per se, since the direction actually belongs to the single vectors associated with each point of the field. How many directions do these vectors have? Well, it depends on the field. A plane has 2 dimensions, hence 2 directions in which you can move, in the same way, $\mathbb{R}^n$ will have $n$ dimensions, hence $n$ directions in which you can move.
Note also that in gradient descent the gradient is computed with respect to the cost (loss) function:
$$W_{t+1} = w_t - \alpha(\partial C / \partial  w)$$
Mathematically this means that:

we have a set of weights $w$
we use these weights to produce an output given a specific input
we then compute an error (or cost, or loss) between the input and output, i.e. we compute at which point of the field of the cost function we end up due to our current weights
we then compute the gradient of the cost function, i.e. we look at all points of the field of the cost function to understand which vector has the biggest magnitude (we care most about the magnitude rather than the direction) and finally
we update the weight in such a way that the next value produced by the weights when computing the cost function will be in the same direction as the previously found vector.

"
Adding data to training results in loss random peaks,"
I have succesfully trained ssd_mobilenet_v2_keras for object detection, with a dataset of about 3700 images. Now I have more images to add. I tried adding only a few images (150-300) to see what happened, but what I obtain is that the trainig looks good in the first steps, but then there are some really high peaks in the loss function.  At first, I thought the problem was the quality of the pictures, so I removed them and tried to add more or less 300 bigger pictures: nothing changed. Then I tried to add only good pictures (no shadows or lights that may confuse the net, no interference with the object, only images where the objects I want to find are big and centered), but nothing.  All the things I have tried leads to the same results:     As you can see, The training looks good at the beginning, but then there are those extremely high peaks that seems to happen at random steps (sometimes after 20.000 staps, sometimes after 2.000).  I tried to train both with and without some data augmentations (random contrast, brightness and saturtion adjust, random rgb-to-grayscale, random horizontal flip, ...) but the results are more or less the same (with data augmentations it's a little better, but still far from good). Any suggestions on why this happens and how to fix?

EDIT: unfortunately I didn't take a screenshot at the end of the succesful training, I only have this one taken after 6.000 steps (total number of steps is 50.000), but then the chart followed this trend and ended with this values: - classification loss: 4.16e-3  - localization loss: 1.11e-3  - total loss: 0.077  
","['training', 'datasets']",
Why is it difficult to propagate intransitive relations over a graph?,"
In the paper Semi-Supervised Learning by Mixed Label Propagation, they say

One major limitation with most graph-based approaches is that they are
unable to explore dissimilarity or negative similarity. This is
because the dissimilar relation is not transitive, and therefore is
difficult to be propagated.

Why is it so?
","['graph-theory', 'semi-supervised-learning']",
The results are not correct when predicting the future for a very long period of time with LSTM,"
I am currently using LSTM to try to predict future data in AirPassengers.csv.
This is current code op my Colab (sorry for the comments are Japanese)
https://colab.research.google.com/drive/16Ntg3dA5ywZvm35PEeMUjHtKhNdsSgbc?usp=sharing
I wanted to use this code to make predictions for a much longer period of time with different data in the future, so I changed the prediction period in the last code block from the original code I referenced from 3 years to 20 years as follows:
#from 
pred_time_length = 12*3
#to
pred_time_length = 12*20

When I do this, I get values like this damped oscillation, and I think that the prediction is probably not working well.
What is the cause of this? Also, what should I change in the code to make it work?

thank you in advance!
","['machine-learning', 'python', 'long-short-term-memory']",
Has the idea of using different learning rates for different layers been explored in the literature?,"
I wonder whether there are heuristic rules for the optimal selection of learning rates for different layers. I expect that there is no general recipe, but probably there are some choices that may be beneficial.
The common strategy uses the same learning rate for all layers. Say, take Adam optimizer with lr=1e-4, and this choice performs well in many situations.
However, it seems that convergence to the optimal values of weights in different layers may be with different speeds. Say, values in the first few layers are close to the optimum after a few epochs, whether features in deeper layers typically require much more epochs to be close to a good value.
Are there any rules to choose a smaller (higher) learning rate in the top layers of the network compared with the bottom layers?
Also, neural networks can have different types of layers - convolutional, dense, recurrent, self-attention. And some of them may converge faster or slower.
Has this question been studied in the literature?
Different learning rates for different layers emerge in transfer learning - it is common to tune only the last few layers, and keep other frozen or evolve with a smaller learning rate. The intuition behind this is that the top layers extract generic features universal for all models and it is desirable not to spoil them during fine-tuning.
However, my question is about training from scratch.
","['deep-learning', 'reference-request', 'hyperparameter-optimization', 'learning-rate', 'layers']",
How can I interpret the way the neural network is producing an output for a given input?,"
I'm using a small neural network (2 hidden layers, 60 neurons apiece) for a rather complex binary classification problem.
The network works well, but I'd like to know how it is using the inputs to perform the classification. Ultimately, I would like to interpret the trained network in order to learn more about the processes responsible for generating the data.
Ideally, I would end up with an equation that would allow me to perform the classification without the network and that would have parameters that I could interpret in the context of the system the network is being used on.
My first thought is to procedurally mask out a growing subset of the ~4000 parameters until there's an appropriate trade-off between performance and simplicity and then maybe use a symbolic logic library to try and simplify further.
I don't think that's the best plan, so I wonder if there's an existing workflow to interpret a neural network.
","['neural-networks', 'classification', 'explainable-ai']","
""Ideally, I would end up with an equation that would allow me to perform the classification without the network"".
If you could find such analytic equation without machine learning then why training a multi layer perceptron in the first place? Or to phrase it differently, the mlp you trained is that equation. And I'm not trying to be ironic, if you need an analytic explanation then don't use a multi layer perceptron, but move to decision trees algorithms for example, than you could literally plot the model itself (still hard to interpret depending on the amount of features you're using).
If instead you still want to stick with mlp, then something that you could do to better understand your model is plotting the decision boundaries learned by it. Sklearn has a nice tutorial on how to do it, I copied it and changes the svm with an mlp just to show that the approach works regardless of the model
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.neural_network import MLPClassifier

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features. We could
# avoid this ugly slicing by using a two-dim dataset
y = iris.target

h = 0.02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
mlp = MLPClassifier().fit(X, y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# title for the plots
title = ""MLP boundries""


# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
# plt.subplot(1, 1)
# plt.subplots_adjust(wspace=0.4, hspace=0.4)

Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
plt.xlabel(""Sepal length"")
plt.ylabel(""Sepal width"")
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())
plt.title(title)

plt.show()

The code output this plot:

Your can see that the boundaries are a crude approximation of the mlp behavior, they are just estimated from a brute force prediction applied to all points of the 2D graph generated by 2 features of the input data. SO the boundaries will change depending also on the features you decide to plot. But it gives you an idea about the relationships learned by the mlp.
If you want more, I stress again that you should train a different model, like decision tree, random forest or XGboost, with these model you can compute scores about features importance and literally plot the decision thresholds learned by the models.
"
Is it possible to solve a linear programming problem using reinforcement learning? (DDPG algorithm),"
I'm trying to solve a linear programming problem using reinforcement learning.
The linear programming problem is:
\begin{array}{ll}
\text{maximize}_x  & C* x \\
\text{subject to}& A*x \le b\\
                 & x_i \in [0,1],\ where \ i=1,2,3,...
\end{array}
For instance:
\begin{array}{ll}
C &=  [1 \; 2 \;3  \;4]\\
x &=  [x1; x2; x3; x4]\\
A &=  [2 \;3 \;4 \;5]\\
b &= 10\\
\end{array}
I've tried to use the DDPG algorithm to train in MATLAB but the result is not good. Any suggestions for this problem, and is it possible to do so, thanks?
","['machine-learning', 'reinforcement-learning', 'ddpg', 'linear-programming']","
Straight theoretical answer:
In theory, yes, it is possible to model this problem as a Reinforcement Learning. But in practice, RL is not the most suitable approach for a simple linear maximization with a boundary. For instance, you could use a Lagrangian.

Practical analysis on your specific problem
In this specific example, you have 1 single constrain: $\sum_{i} a_i x_i \le b$, for an $n$ degree equation (n = size of $X$).
So you might also want to add another boundary, like: all $X > 0$. Otherwise your solution will diverge:

$C = [1 2 3 4];$
$X = [x_1; x_2; x_3; x_4];$
$A = [2 3 4 5];$
$b = 10$

Simple example of divergent solution:
$X = lim_{k=\infty} [-3k, 0,0, k]$
Gives you: $C*X= -3k + 0+0+4k = k$ ✅ Maximum possible reward for $lim_{k=\infty}$
Constrained by $A*X = -6k + 0 + 0 +5k = -k \le 10$ ✅ Minimum possible boundary for $lim_{k=\infty}$

Edit after adding $x_i \in [0,1] $ constraints:
You have described the simplest version of Knapsack Problem, where we can split items in fractions.
For this problem, the greedy solution is very simple and effective:
Calculate a new weight vector: $W = C/A = [ c_1 / a_1, c_2/a_2, ... ]$, which represents the ratio of value $c_i$ $/$ cost $a_i$ for each index $i$.
Now, to have the best value $C$ for a limited cost $A$, you just need to greedy select the $i$ from the largest ratio $w_i$ and ""fill your Knapsack"" (by increasing continuously $x_i$) until some boundary is filled:

If $x_i\le1$ is reached (you have exhausted all available $x_i$), than proceed to the next best $w_i$.
If total boundary $B$ is reached, than you've finished the algorithm and that's a guaranteed best solution.

"
How to create a model for predicting the number of visitors,"
I want to create a model to predict the number of visitors.
Currently, I have a year's csv data for predicting the number of visitors, which is collected every 10 seconds.
I would like to predict the number of future visitors on a daily basis based on this data for the past year.
What kind of method or model can I use to achieve this? I can use a graphics board for learning.
If you have any page of sample codes, it would be very helpful.

",['ai-design'],"
There are various cheat sheets to recommend what predictive model to use in different situations. Here’s a couple to guide you on options for your dataset.
 https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
For this first cheat sheet with your example, you would begin with the start icon. For the first question > 50 samples, the answer is yes. For the second question predict category, the answer is no. For the third question predict quantity, the answer is yes. For the fourth question, < 100K samples, the answer is yes . The suggested type of regression analysis to try from this cheat sheet is a SGD Regressor.
 https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-cheat-sheet
For the cheat sheet you example begin with the question what do you want to do. The answer is predict values; so again we see this should be some type of regression analysis. There are a few options on this one that would be appropriate to try, such as a Poisson Regression.
After deciding which model to use the next decision is how to implement the model. Kimball made a statement a good decade ago,  In today’s environment, most organizations should use a vendor-supplied ETL tool as a general rule. This was mainly due to maintenance costs. We’ve reached the same level of maturity with data science tools that same logic can be applied for common data science models.
"
What all does the gradient tells us other than the direction to move parameters?,"
Gradients are used in optimization algorithms.
I know that a gradient gives us information about the direction in which one needs to update the weights of a neural network. We need to travel in the opposite direction of the gradient to get optimal values.
Thus the gradient provides direction to update parameters.
Is it the only information provided by the gradient? Or does it provide any other information that helps in the training process?
","['optimization', 'gradient', 'calculus']","
Momentum was big.  It allowed several steps to be evened out so that most of the motion in the weights was in the direction of the optimum.  It operates against sequential measurements of the error.  This means that several estimates of the gradient give better local picture of the loss-surface.
The error has a magnitude, so the gradient has both direction and magnitude.  It tells us the direction to go, but also how far to go.  The space is complex, so many momentum methods smooth the magnitude and direction by combining thousands of gradient estimates.
Dataset distillation is interesting because it can require a 10x larger network to learn a task, then distill that learning into a 1x network to do the task.  This is a universal-to-specific transformation, using a global approximator to find the local landscape that works is very different than building an approximator for use only within that landscape.  The converged space of both networks is the same, but one is contrived in vastly fewer parameters.  The gradient in the around the optimum, the perturbed gradient given the training data, tells how to perform the simplification.
Saliency maps use back-propagation (single-pass gradient on fully trained networks) to infer interior structure and operations of complex neural networks.
"
Why don't we bootstrap terminal state in n-step temporal difference prediction update equation?,"
In the algorithm below, when $\tau + n \geq T$, shouldn't the algorithm bootstrap with the value of the next state? For instance, when $T=5, \tau=3, \& \; n=2$, we don't bootstrap the sample return with $V_{(\tau+n)}$, i.e., $V_5$ or the terminal state.

Also, on line 4, what do we mean by ""can take their index mod $n + 1$""?
","['reinforcement-learning', 'temporal-difference-methods', 'model-free-methods', 'pseudocode']","
Because the value of the terminal state is 0 by definition. There is no further reward to be obtained once you reach the terminal state.
"
"Why does one-step TD strengthen only the last action of the sequence of actions that led to the high reward, while n-step TD the last n actions?","
In the caption of figure 7.4 (p. 147) of Sutton & Barto's book (2nd edition), it's written

The one-step method strengthens only the last action of the sequence of actions that led to the high reward, whereas the n-step method strengthens the last n actions of the sequence, so that much more is learned from the one episode.

Why does one-step TD strengthen only the last action of the sequence of actions that led to the high reward, while n-step TD the last n actions?
Here's a screenshot of the figure.

","['reinforcement-learning', 'comparison', 'temporal-difference-methods', 'sutton-barto', 'model-free-methods']",
What is the high-level algorithm followed by contemporary packages for the calculation of gradient?,"
Most of the neural network models in contemporary deep learning packages are trained based on gradients.
Let $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$ be a function for which we want to find a gradient, then the gradient is generally represented by a Jacobian matrix that looks like below
$$J = \begin{bmatrix}
    \dfrac{\partial y_1}{\partial x_1}       &     \dfrac{\partial y_1}{\partial x_2} &     \dfrac{\partial y_1}{\partial x_3} &\dots &     \dfrac{\partial y_1}{\partial x_m} \\
     \dfrac{\partial y_2}{\partial x_1}       &     \dfrac{\partial y_2}{\partial x_2} &     \dfrac{\partial y_2}{\partial x_3} &\dots &     \dfrac{\partial y_2}{\partial x_m} \\
    \cdots & \cdots & \cdots & \cdots & \cdots \\
        \dfrac{\partial y_n}{\partial x_1}       &     \dfrac{\partial y_n}{\partial x_2} &     \dfrac{\partial y_n}{\partial x_3} &\dots &     \dfrac{\partial y_n}{\partial x_m} \\
\end{bmatrix}
$$
For example: If $f(x_1, x_2) = \begin{bmatrix}
           x_1 + x_2 \\
           x_1x_2 
         \end{bmatrix}$ then $J = \begin{bmatrix}
           1 & 1 \\
           x_2 & x_1 
         \end{bmatrix}$
After calculating the Jacobian matrix, we can substitute the co-ordinate values of a particular point so that we can obtain a real matrix which is a gradient at a particular point.
$$
J_{(4, 5)} = \begin{bmatrix}
           1 & 1 \\
           5 & 4 
         \end{bmatrix}
$$
In order to perform the gradient of a function at a point, the algorithm I know is as follows:

Write each output of the function in the analytical form in terms of input;
Apply partial derivative on each output w.r.t each input;
Substitute the values of the input point at which we want to find the gradient.

Thus, finally, we will get the gradient.
Do the popular packages like PyTorch, Tensorflow, Keras, etc., use this or a variant of this algorithm to find the gradients at a particular point?
If yes, will those packages be able to write the analytical forms of all the output variables in terms of input variables?
If not, what is the high-level algorithm for calculating gradients? Is it based on geometrical slope version of gradient?
","['algorithm', 'implementation', 'gradient']",
Predict a part of the input based of the output,"
I'm working on a fun project where I have a dataset of input and output data, both having a fixed size of characters.
I would like to predict a part of the input based on a known output as follows:
$$Input = A+B$$
$$Output = X+Y$$
A, B, X, Y are strings that will be concatenated and they have a fixed size
Knowing A, X and Y; I want to predict B (even if it takes a lot of tries). The output is split in 2 because I don't really care what value Y has (so maybe I can delete it, depending how is more easy).
Is it possible? I'm new to ML and AI and first I want to know if it possible before starting to work on the project (I am time limited). And if it is possible then could you tell me what exactly to study/learn or how I can do it?
","['machine-learning', 'math']",
Is there any way to train a neural network without using gradients?,"
The only algorithm I know for updation of weights of a neural network is based on gradients. The update equation can be roughly written as
$$w \leftarrow w - \nabla_{w}L$$
where $\nabla_{w}L$ is the gradient of loss function with respect to weights.
Are there any learning algorithms for updating weights in neural networks that does not use gradients?
","['neural-networks', 'training', 'reference-request', 'stochastic-gradient-descent']","
Yes.
A prominent class of ""gradient-free"" algorithms in ML world is known as Evolution Strategies (ES). Evolutionary Algorithms, although existed for a long time, only a few have shown to scale well.
Recently, the research group OpenAI managed to train Deep RL models with a specific variant of ES (with careful engineering). You can read this paper. This blog by David Ha provides a starting point if you want to learn about ES and its modern derivatives.
"
Is there any significance for higher order gradients in artificial intelligence?,"
Although I don't know in detail, I am aware of the following facts regarding the use of gradients in some domains of artificial intelligence, especially in minimizing the training of neural networks.

First order gradient: It quantifies the rate of change of a function with respect to its inputs. It is useful in artificial intelligence, especially in gradient-based algorithms, to know about the direction in which the parameters need to be updated.

Second-order gradient: It somehow quantifies the curvature of the function. It is used in artificial intelligence, to know whether the function has convex or concave portions.


In this context, I want to learn whether there is any significance for higher-order gradients in artificial intelligence? Note that higher-order refers to the order $\ge 3$.
","['reference-request', 'objective-functions', 'gradient-descent', 'gradient']","
Gradient descent presumes a Taylor Series.  They estimate the loss given the inputs and target, then use the difference to move the system weights to produce a less-bad loss.
The learner as a universal function approximator means there can be many configurations that yield minimum loss, and there is usually no global ""best"".
One of the reasons for multiple traverses through the data using the optimizer is that the local Taylor series estimate is low order (frequently only first order), so the optimum is not apparent after a single pass.  When the weights change, the landscape changes and for a 25-million-parameter network, that change is happening in a 25-million dimensional space.
The higher order terms of the gradient can accelerate the optimization, but they can be prohibitively expensive to compute or estimate.  The Hessian is nice, but has high overhead to compute and store.  Things like conjugate gradient were popular because they are fast and (very) rough estimates of the Hessian.
If you have a cheap way to get decent higher-order derivatives, then there is value in that.
"
How does n-step Temporal Difference remove the notion of time-step?,"
How does n-step TD removes the notion of time-step as referenced in Sutton and Barto (2nd edition, Page 163) below?

Another way of looking at the benefits of n-step methods is that they free you from the tyranny of the time step. With one-step TD methods the same time step determines how often the action can be changed and the time interval over which bootstrapping is done. With one-step TD methods, these time intervals are the same, and so a compromise must be made. n-step methods enable bootstrapping to occur over multiple steps, freeing us from the tyranny of the single time step.

Consider the following example.
We know our n-step update equation as: $$V_{t+n}(S_t) = V_{t+n-1}(S_t) + \alpha [G_{t:t+n} - V_{t+n-1}(S_t)]$$
Now, let $t=0$ and $n=2$. This gives us: $V_2(S_0) = V_1(S_0) + \alpha [G_{0:2} - V_1(S_0)]$.
Before our n-step TD prediction algorithm starts, we initialize with $V_0$. But we use $V_1$. Why? And how do we calculate $V_1$?
","['temporal-difference-methods', 'model-free-methods']",
What is the difference between a vision transformer and image-based relational learning?,"
I am trying to figure out the difference between the architecture used in this and this paper. It looks like both used multi-headed self-attention and therefore should be the same in principle.
","['comparison', 'papers', 'transformer', 'architecture', 'vision-transformer']",
Is Reinforcement Learning capable of learning complex functions (such as producing a 3d model given an image)?,"
I want to build an AI that can convert an image of a subject into an anatomically accurate 3D model. To do this, I was thinking of adapting the following code for Deep Deterministic Policy Gradient: https://keras.io/examples/rl/ddpg_pendulum/
My reasons considering RL:

I don't have the needed skill (3D modelling) to procure a large dataset for the project. I was hoping RL may help me overcome that by adapting to a smaller dataset through state-reward learning. I'm looking mainly for human and animal anatomical models. Those are hard to find in large numbers.

My second concern is the required density (polygon count) of these models can be rather high. I'm not sure if it is computationally feasible for a NN to output high density models. However, I'm thinking an RL agent can step through and write each vertex one at a time in a 3D space. As compared to a single output layer in a feed-forward network.


However, that means it will have to handle a rather large state-space (an array of length 50,000 or higher).
With all of that said, RL has mainly been used in video games and simple control problems from the OpenAI gym. Is it a waste of time to use RL for this level of complexity?
","['reinforcement-learning', 'deep-rl', 'function-approximation']","
Yes, RL is capable of learning complex functions, as it is a very general learning approach. However, if you have a direct goal of learning a complex function from example data, it will not really add anything to that process. Supervised learning will be more efficient.
RL doesn't directly address or have special mechanisms to deal with the two main issues you have:

Lack of training data

Multi-variable optimisation with a very large number of variables


Adding states, timesteps and trail-and-error learning to this problem does not make it more tractable. If the problem naturally presented itself as a sequence of simpler choices, then RL might help somewhat, but as far as I can see you have a very high dimensional function to learn, and there is no benefit from adding a layer of trial-and-error learning on top of it.
What you probably want is to find ways of constraining the problem, using domain knowledge and/or transfer learning from similar systems. For example, if you already know the types of creature in the photos being turned into meshes, you could probably start by categorising them in order to use some pre-defined meshes that you then adjust, as opposed to starting with figuring out the general plan of the mesh.
There are already systems that can turn pictures of human subjects into 3D models with approximately correct shape and pose. I would suggest you study those to understand how they work and whether the ideas in them can be re-used for your problem. Here is one called PIFuHD.
"
Types of decoder parametrizations in VAE for continuous data,"
I'm wondering what are the different choices of parametrizations available for the decoder in a variational autoencoder. If the data is discrete, you can just output probabilities for each class, so that's straightforward. If the data is a) continuous and not bounded, then the only parametrization I know of is the (multivariate) normal distribution. If the data is b) continuous and bounded, again the only parametrization I know of is a (multivariate) normal distribution, mapped to [0,1] using a sigmoid function.
Are there other formulations that are commonly used? If there are, what are the advantages and any disadvantages compared to using a normal distribution?
To ask the question in a different way: the normal distribution seems like the ""go to"" choice in problems with continuous output since it's simple and allows gradients through the reparametrization trick. Are there other reparametrizable continuous distributions which are also commonly used in machine learning? I know the exponential distribution (and related distributions such as weibull, gamma) are reparametrizable, but would you ever use those instead of a normal distribution to model continuous data?
","['generative-model', 'variational-autoencoder']",
How do I select the class weights for the loss function in the case of more than 2 classes?,"
I have a machine learning task where I would like to weight losses based on the frequency of the categorical values appearing in the data. The binary solution can be seen below, but I'd like to know what to do about the case of n>2 categories.
w_0 = (n_0 + n_1) / (2.0 * n_0)
w_1 = (n_0 + n_1) / (2.0 * n_1)

The frequencies for the samples n0-n5 are:
n_0:     1552829
n_1:     14479
n_2:     13445
n_3:     13781
n_4:     18795
n_5:     64187

","['machine-learning', 'objective-functions', 'multiclass-classification', 'imbalanced-datasets']","
Is that what you want?
w_0 = (n_0+n_1+ ... +n_5) / (5.0 * n+0)

If so, it can be achieved by:
    n = [n_0, n_1, n_2, ...]
    w = []
    for i in range(len(n)):
      w[i] = sum(n) / (len(n)*(n[i]))

Notice that Sum(n)/Len(n) = Average(n), so you'd basically saying your loss-function is avg(n)/ni.
"
What does a value of -1.000 mean in MS COCO Metrics for Object Detection,"
I am training some Object-Detection-Models from the TensorFlow Object Detection API and got from the evaluation with MS COCO metrics the following results for Average Precision:
IoU = 0.5;0.9
maxDets = 100
area = small
AP = -1.000
The other values all make sense to me. But I don´t know what the -1.000 stands for. Does it mean that there are no small objects in my dataset to be detected?
","['deep-learning', 'convolutional-neural-networks', 'object-detection', 'testing', 'precision']","
APsmall stands for Average Precision for small objects.

This website contains all the descriptions about the metrics:
https://cocodataset.org/#detection-eval
If you want to know more about the metrics evaluation (how to calculate this metric), you can find the metric's source code here: https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py
"
Reason for relaxing limit in derivative in this context?,"
Consider the following paragraph from NUMERICAL COMPUTATION of the deep learning book..

Suppose we have a function $y = f(x)$, where both $x$ and $y$ are real
numbers. The derivative of this function is denoted as $f'(x)$ or as
$\dfrac{dy}{dx}$. The derivative $f'(x)$ gives the slope of $f(x)$ at
the point $x$. In other words, it speciﬁes how to scale a small change
in the input to obtain the corresponding change in the output:
$f(x+\epsilon) \approx f(x) + \epsilon f'(x)$.

I have doubt in the equation $f(x+\epsilon) \approx f(x) + \epsilon f'(x)$ given in the paragraph.

In strict sense, the derivative function $f'$ of a real valued
function $f$ is defined as
$$f'(x) = \lim_{\epsilon \rightarrow 0}
 \dfrac{f(x+\epsilon)-f(x)}{\epsilon}$$
wherever the limit exists.

If I replace the original definition of the derivative as follows
$$f'(x) \approx \dfrac{f(x+\epsilon)-f(x)}{\epsilon}$$
then I can obtain the equation given in the paragraph i.e,  $f(x+\epsilon) \approx f(x) + \epsilon f'(x)$.
But, my doubt is that how can I modify the definition with $\lim\limits_{\epsilon \rightarrow 0}$ to an approximation with out limit? How can the following two are same?
$$f'(x) = \lim_{\epsilon \rightarrow 0} \dfrac{f(x+\epsilon)-f(x)}{\epsilon} \text { and } f'(x) \approx \dfrac{f(x+\epsilon)-f(x)}{\epsilon}$$
","['math', 'optimization', 'gradient-descent', 'derivative']","
It is just an assumption. If we assume $\epsilon$ is small enough (depending on the function $f$), you can remove the limit $\lim_{\epsilon \to 0}$ for the approximation.
"
Should I label static objects on video dataset?,"
I'm using nvidia Transfer Learning Toolkit to detect cars in some video frames.
I found some dataset (for example https://www.jpjodoin.com/urbantracker/dataset.html and https://www.kaggle.com/aalborguniversity/aau-rainsnow) and I noticed that usually parked cars are not labeled, and covered under a mask.
Why shouldn't I add also their labels? It would be easy to label them because they are static objects and I could copy-paste in all labels set. So why in video dataset they are not labelled?
","['image-recognition', 'image-segmentation', 'transfer-learning', 'yolo', 'labels']","

Model architecture:

In machine learning, static image detectors can be is very different from video detectors, as movement plays a big role on the task. So, even when comparing frames the objects are similar, when digesting the video, a model can learn very different things. Maybe adding parked cars to the database increased false positives, mistakenly labeling other static noises as cars.

Business goal:

Why are you labeling cars from security camera, in the first place? What is your goal here? Maybe if you want to know car density in a parking lot, than labeling parked cars is very useful. But if you just need to know traffic flow, than parked cars will be just a distraction, noise from the data.
So maybe whoever built the dataset had a different goal in mind.
"
Non-locally Electrically Programmable Logic Gates - Technological Advances Progress,"
Preface: I’d like to clarify that I understand what a relay is and that a PLC uses a fairly conventional microprocessor that only digitally establishes logical logic gate configuration as a digitally programmable alternative to relay banks for analog and/or (depending on the PLC) digital signals. My question is based on the understanding that to date actual logic gates (as far as I know) aren’t non-locally programmable (“re-wirable”) without a person manually rewiring truly programmable actual (not logical programming of a statically wired microprocessor) logic gates.
Rectenna work interests me specifically around any potential relevance of varying transmission wavelengths and material resistances (if this is not possible with MoS2, generally as a concept for other potential materials) to making possible remote switch activation of logically chosen switches along an array. Essentially I am curious about if this or other research has potential for constructing truly physically reprogrammable (externally and maybe wirelessly) logic gates.
In general any information on advances towards this capability would be appreciated as right now it seems like the only rudimentary build I could manage for my project is a 64 gate one. That’s not great because anything less than 512 gates would be very hard to make useful for my proof of concept project, and I know there’s no way I could get to a more ideal 262,144 gates.
One example would be any publication which covers if the kind of uses of phase-engineered low-resistance contacts for ultrathin MoS2 transistors covered in the articles below would be able to be produced with varying resistance in a band usable for varying activation via radio waves for switches.
https://doi.org/10.1038/nmat4080
https://www.ece.cmu.edu/news-and-events/story/2019/05/rectennas-converting-radio-waves-into-electricity.html
I’m not picky if someone knows about other technological advances approaching this capability such as biochemical non-locally programmable switch activation equivalent processes. Thanks everyone.
Update 1: My specific question is: Have there been any significant technological advances towards non-locally electrically programmable logic gates?
Update 2: After further review I’ve found that FPGAs are not what I am asking about. Their reprogramming like PLCs is digital not analog. They seem to just be a more generalized similar thing to PLCs rather than being factory equipment. I might incorporate one or more in my project, but they aren’t what I am referring to which is true analog reprogramming. Why does analog matter? Analog means more efficient at the surface level, but it also allows structured logic similar to ladder logic at the hardware level which enables significantly different uses in structuring and restructuring logic execution.
Update 3: This is for an efficiency proof of concept project trying to prove it is possible to structure logic in a certain way to increase efficiency of certain specific processes. This is a project involving programming and/or design at every single level of development (transistors, machine code, assembly, mid level (such as C/C++), and high level (Python/Tensorflow). I will be creating custom NAND gate structures, writing the instructions to execute on them, writing in assembly, writing in a mid level language, and writing in Python and TensorFlow for different parts of this overall project’s functionality.
In conclusion the straightforward version of this question is: What are the current capabilities for or research done towards creating physically rewired logic gates using non-local digital instructions?
","['research', 'proofs', 'models', 'supervised-learning', 'efficiency']",
Where can I read about the multinoulli distribution?,"
I encountered the term multinoulli distribution in the following sentence from Chapter 4: Numerical Computation of the deep learning book.

The softmax function is often used to predict the probabilities
associated with a multinoulli distribution.

I am guessing that multinouli distribution is any probability distribution that has been defined on a random variable taking multiple values. I know that SoftMax function is used in converting a vector into another vector of the same length with probability values that signify the chance of input falling into that particular class.
Suppose $C$ is a random variable with support $\{c_1, c_2, c_3, \cdots, c_k\} $. Then I am guesssing that any probability distribution on $C$ is a multinouli distribution. SoftMax is an example of such multinouli distribution that uses the expression $\dfrac{e^x}{\sum e^x}$ for calculating probabilities.
Is my guess correct about multinoulli distribution? The reason for my doubt is that I never came across the word multinoulli and I cannot find even on internet. If my guess is wrong, where can I read about multinoulli distribution?
","['terminology', 'probability-distribution', 'softmax']","
You can find a description of this distribution (which is also known as categorical distribution, which you probably already heard of) in section 2.3.2 (p. 35) of the book Machine Learning: A Probabilistic Perspective (by K. Murphy). You can also find there and in the previous section a description of the related Bernoulli, binomial and multinomial (the most general of the four) distributions. The word multinoulli is used in order to remind you that this distribution is a generalization of the Bernoulli.
In any case, this is how you may remember these four probability distributions.

Bernoulli: you throw a coin only once ($n=1$), and a coin has $k = 2$ outcomes (heads or tails)
Binomial: you throw a coin $n$ times, where $n$ can be greater than $1$, and a coin has $k = 2$ outcomes
Categorical: you throw a dice, with $k$ sides (e.g. a side may be $1$ or $5$), where $k$ can be greater than $2$, only once ($n=1$)
Multinomial: you throw a dice, with $k$ sides, $n$ times

So, these are all discrete probability distributions, with an associated probability mass function (p.m.f). In all of them, we have two parameters $n$ (the number of trials of the experiment) and $k$ (the number of outcomes).
I don't think it is correct to say that a softmax is a probability distribution. The softmax is a function used to compute the probabilities that you associate with a categorical distribution, i.e. you use the softmax to produce a probability vector (although some people will say that the softmax produces a probability distribution), as the excerpt that you quote states. In principle, I think you could use other functions to do that (an alternative to the regular softmax, sigsoftmax, is proposed in this paper, section 3.3, p. 6). So, the softmax is used to model a categorical distribution, but I wouldn't say it's a categorical distribution. You can find an explanation of why the softmax is used instead of e.g. just normalizing by the sum here (and a long list of probability distributions here).
"
What are the mathematical properties of natural exponential function that lead to its usefulness in artificial intelligence?,"
In mathematics, there is a proof that the following infinite series converges to a constant irrational number, denoted by $e$, called as Euler's number or Euler's constant or Napier's constant. The value of $e$ lies between 2 and 3.
$$1 + \dfrac{1}{1!} +  \dfrac{1}{2!} +  \dfrac{1}{3!} + \cdots$$
The natural exponential function, defined as follows, has some interesting properties
$$f: \mathbb{R} \rightarrow  \mathbb{R}$$
$$f(x) = e^x$$
It is used in several algorithms and in the definitions of functions like SoftMax.
I am interested in knowing the possible mathematical characteristics that lead this function useful in artificial intelligence.
The following are the properties I am aware of. But, I am not sure about how some of them will be useful

Non-linearlity: Activation functions are intended to provide non-linearity. So, it is a candidate for activation functions due to this property. You can check its graph here.

Differentiability:  Loss functions used for in back-propagation algorithm need to be differentiable. So, it can be a candidate for usage in loss functions too.


$$\dfrac{d}{dx} e^x = e^x \text{ for all } x \in \mathbb{R}$$

Continuity: I am not sure how this property is useful in algorithms. Intuitively, you can check from graph provided above that it is continuous.

Smoothness: I am not sure how this property is useful in algorithms. But seems useful. The natural exponential function has the smoothness property.


$$\dfrac{d^n}{d^nx} e^x = e^x \text{ for all } x \in \mathbb{R} \text{ and } n \in \mathbb{N}$$.
Are there any other properties like non-linearily, differentiability, smoothness etc., for the natural exponential function that make it superior to use in AI algorithms?
",['math'],"
The question must be split in two:
Use in softmax: softmax is based in the concept of cross entropy, logistic regression, logistic function ... . All these concepts includes exponential or logarithmic in its formulation.
Use in activation functions:

monotonic: a non-monotonic function will give same output to different inputs, disturbing the concept of distance/error.
non-linear: a linear activation function collapses the layer with the next one, thus, it is useless.
differentiable: necessary to use back-propagation.

"
Why not undefined expression is different from numerical underflow?,"
Consider an architecture or programming language that uses $n$ bits for storing a floating point number in a particular format. Then each and every floating point number it can store should be in a given range, say $[lf, uf]$.
If there is a need to store any floating point number less than $lf$ then we generally treat such phenomenon as underflow. Consider the following from Numerical Computation chapter of Deep Learning book.

One form of rounding error that is particularly devastating is
underﬂow . Underﬂow occurs when numbers near zero are rounded to zero.
Many functions behave qualitatively diﬀerently when their argument is
zero rather than a small positive number. For example, we usually want
to avoid division by zero (some software environments will raise
exceptions when this occurs, others will return a result with a
placeholder not-a-number value) or taking the logarithm of zero
(this is usually treated as $-\infty$, which then becomes not-a-number
if it is used for many further arithmetic operations).

You can observe that two examples has been given while explaining underflow: division by zero and logarithm of zero. If we treat mathematically, both are undefined. It should not be an issue of storage, especially underflow.
Is there any reason behind proving such examples, which are mathematically undefined, under the umbrella term underflow and using the term ""not-a-number""?
","['terminology', 'math', 'implementation', 'books', 'storage']","
Yes, they can be related to underflow. Mathematically, we do not expect facing with division by zero when we have an expression $\frac{1}{\varepsilon}$ when $\varepsilon > 0$. However, in numerical softwares, it can happen if $\varepsilon < lf$.
Moreover, you can think about the same scenario for taking the logarithm of zero.
"
Is there any domain in machine learning that solves a problem by using only analytical algorithms?,"
Most of the algorithms in machine learning I am aware of use datasets and learning happens in an iterative manner given some examples. The examples can also be understood as experience in the case of reinforcement learning.
Consider the following from Numerical Computation chapter of Deep Learning book

Machine learning algorithms usually require a high amount of numerical computation. This typically refers to algorithms that solve mathematical problems by methods that update estimates of the solution via an iterative process, rather than analytically deriving a formula to provide a symbolic expression for the correct solution. Common operations include optimization (ﬁnding the value of an argument that minimizes or maximizes a function) and solving systems of linear equations. Even just evaluating a mathematical function on a digital computer can be diﬃcult when the function involves real numbers, which cannot be represented precisely using a ﬁnite amount of memory.

I am wondering whether there is any domain in machine learning that deals with solving the problem analytically rather than computationally heavy iterative algorithms?
","['machine-learning', 'math', 'linear-regression', 'numerical-algorithms']","
In some cases, you can solve a linear regression problem with an analytical (or closed-form) solution/expression (although this may not always be the best approach). See this answer for more details.
Note that this solution involves matrix multiplications and the computation of an inverse with floating-point numbers, so this is still a numerical algorithm/problem. We could also consider this solution an iterative algorithm if, under the hood, you compute the inverse of the matrix or perform the matrix multiplications with iterative algorithms, but, from a high-level perspective, this is an analytical (non-iterative) method.
"
What is the difference between “AI Methods” and “AI Techniques”? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



These are words that we frequently come upon. What can be said about the differences? Would these two words' subheadings be different?
",['terminology'],"
According to webster they are considered synonyms. However, from an academic viewpoint there is a distinction:
A method is a systematic procedure, technique, or mode of inquiry employed by or proper to a particular discipline (e.g. scientific method)
A technique is the manner in which technical details are treated or a way of accomplishing a desired aim that may not be considered a scientific method (e.g. heuristic technique)
"
How can equivariance to translation be a benefit of a CNN?,"
I just learnt about the properties of equivariance and invariance to translation and other transformations.
Being invariant to translation is clearly an advantage, as even if the input gets shifted, the network will still learn the same features, and work fine.
But how is equivariance useful?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'convolutional-layers']","
Equivariance is useful because the neural network can learn to detect common image components - edges, corners, curves in specific orientations - in a general way that is then applied across a whole image evenly. These components typically do exist and can appear in multiple places within an image, and may be parts of larger-scale features in turn. Identifying all the edges in an image can be useful before any kind of pooling that adds invariance is applied.
Without equivariance, an edge oriented in one way in one part of the image would be completely different to the neural network to the same kind of edge elsewhere. It would only get made part of a relevant filter and used if a specific training example had an important edge in that one place.
It is hard to separate this usefulness of equivariance from the associated reduction in number of free parameters, thanks to using convolutional filters with small amount of local connection as opposed to fully connected neural networks. CNNs have to be equivariant due to the architecture, whilst the invariance requires a little more effort (pooling and/or strided convolutions).
The fact that CNNs are so successful using this approach probably says something about natural images. It should be possible to construct non-natural images where equivariance would be of limited use e.g. where local features don't exist, or where they vary over the image in a way that makes detecting them in more than a few places pointless.
"
What is the reason for a training loss that drops but validation that NEVER does,"
I've been working on learning about NLP via a beginners competition on Kaggle.
I first trained a model with an embedding layer and then a simple linear layer. I actually got way better than a flip of the coin with this model, so I decided to try to step it up with an LSTM.
What happened was that training loss decreased and then palteaued while validation loss never decreased at all.
In the case of overfitting, I would expect validation loss to decrease for a while but then either remain steady or perhaps even increase as the model starts to overfit.
I can't find any reason for the strange loss curves I'm seeing:


What could cause such a phenomenon?
I would be happy to share my network architecture and training code if there isn't a straightforward answer (I know there usually isn't).
","['natural-language-processing', 'long-short-term-memory']","
As you know, it would be hard to tell exactly what is going on without knowing more about the dataset.
However, a couple things come to mind:

Did u correctly normalize by fitting the scaler only on the train dataset and then apply the same transform (using the the mean and variance from train set) to the test set.

Is your dataset imbalanced? I have found the Python DataPrep tell useful for exploratory data analysis


"
Different ways to produce the same network in NEAT,"
I have an interesting example for the NEAT and want to clarify what behavior is correct from NEAT's perspective and why (why the opposite is wrong, what are the consequences of choosing the different one).
So let we have an initial network of 3 nodes and 2 edges:
Initial Condition
Nodes: [A, B, C]
Edges: {
1: A->B
2: B->C
}


1st Gen
Then in the 1-st generation we get 2 mutants:
Mutant 1 (edge 1 got split)
Nodes: [A, B, C, D]
Edges: {
1: A->B DIS
2: B->C
3: A->D
4: D->B
}

Mutant 2 (edge 2 got split)
Nodes: [A, B, C, E]
Edges: {
1: A->B
2: B->C DIS
5: B->E
6: E->C
}


2nd Gen
In the second generation 2 if we mutate Mutant 1 (by splitting edge 2) and mutate Mutant 2 (by splitting edge 1) which result should we get?
Hypothesis 1: the same result:
Nodes: [A, B, C, D, E]
Edges: {
1: A->B DIS
2: B->C DIS
3: A->D
4: D->B
5: B->E
6: E->C
}

or...
Hypothesis 2: Two new mutants:
Nodes: [A, B, C, D, F]
Edges: {
1: A->B DIS
2: B->C DIS
3: A->D
4: D->B
7: B->F
8: F->C
}

and
Nodes: [A, B, C, E, G]
Edges: {
1: A->B DIS
2: B->C DIS
5: B->E
6: E->C
9: A->G
10: G->B
}


In case the second hypothesis is correct, how does it deal with crossover in the next run?
Say these 2 mutants are breeded. We get :
Breeding in 2nd Hypothesis
Nodes: [A, B, C, D, E, F, G]
Edges: {
1: A->B DIS
2: B->C DIS
3: A->D
4: D->B
5: B->E
6: E->C
7: B->F
8: F->C
9: A->G
10: G->B
}

Looks like a too complicated genome for the 3-rd generation, doesn't it?
In case the first option is correct then actually innovation numbers are somewhat redundant in NEAT and can be done differently.
We can have node list as a list of strings (node names).
Then instead of assigning the innovation number to an edge we can use string value calculated like HASH(fromNodeName + toNodeName).
That way whenever the new link is created in any generation between 2 nodes it gets the same innovation number name for it.
When the node is created (by splitting an edge) its name can be taken right from the edge getting split and the innovation names of 2 new edges can be calculated like HASH(fromNodeName + splitEdgeName) and HASH(splitEdgeName + toNodeName).
That way the algorithm has no global variables, no shared list of all innovations and can be simply parallelized
","['neat', 'neuroevolution']",
Where to start with reinforced learning on actions and rewards sampled from slow ongoing real life system,"
I would like some pointers, possible projects that solve conceptually similar goals, code examples or tutorials.
I am trying to achieve a system that is able to start or stop ventilation of a given space based different outside and inside metrics such as humidity, temperature, time, etc. to achieve a decrease of relative humidity.
Actuating the system based on simple physics gave me questionable results, this is because I am not able to model the whole dynamic of the system.
I was thinking if reinforced learning could help me learn a good policy. The system should learn on real life data, actuating real ventilation, with the obvious slowness of such system.
I am quite new with AI, able to comprehend and create a simple OpenAi Gym. I am not even sure if and how something like this is achievable with so limited data flow.
I am currently recording and analyze all possible data I can measure, together with some more or less random ventilation sessions. I am sure there are better ways to do this.
","['gym', 'real-time', 'applications']","
First you'd need to mathematically model your real environment. Probably use some differential equations.
Once you have a good model, you still won't have your real case parameters. So I can see 2 different approaches:

Theoretically + Experimentally: Empirically measure real data to try to find those parameters. (Make a simple PID controller)
Make a robust and general policy that adapts to any parameter.

The first alternative is very straightforward, you basically just have 3 parameters to adjust and there are clear methods to adjust it depending on your data behavior.
Once you are in AI.exchange, I assume you're choosing the 2nd way. I consider it's way harder to implement, but also more fun.
So you'll need create a highly parametrized environment, make a reinforcement learning model, train it on all different kinds of parameters (just make sure your real parameter are somewhere inside that range).
If you do it all right, you should have a robust general policy that can behave well in any general condition.
"
What does all the formula and pictures mean?,"
https://www.nature.com/articles/s41467-020-17419-7
I am a medical school graduate and I really want to learn AI/ML for computer-aided diagnosis.
I was building a symptom checker and I found the material. It clarifies the drawbacks of associative models which are performing differential diagnosis. And it suggests counterfactual(causal) approach to improve accuracy.
The thing is I couldn't understand what the formulas mean in the article, e.g.:
$$P(D \mid \mathcal{E}; \theta )$$
I really want to know what | and ; are doing here, what do they mean, etc.
I would really happy if someone can directly answer or just provide me some references to get general idea quickly.
Here comes the most tricy part...

","['neural-networks', 'machine-learning', 'algorithm', 'bayesian-networks']",
"Can neural networks have continuous inputs and outputs, or do they have to be discrete?","
In general, can ANNs have continuous inputs and outputs, or do they have to be discrete?
So, basically, I would like to have a mapping of continuous inputs to continuous outputs. Is this possible? Does this depend on the type of ANN?
More specifically, I would like to use neural networks for learning the Q-function in Reinforcement Learning. My problem has basically a continuous state and action space, and I would like to know whether I have to discretize it or not (a more detailed description of my problem can be found here).
","['neural-networks', 'reinforcement-learning', 'continuous-action-spaces', 'continuous-state-spaces', 'discretization']",
Are mean and standard deviation in variational autoencoders unique?,"
In general, if I have a collection of data then mean(Expectation) and standard deviation are calculated as follows
$$\text{mean } = \mu = \mathbb{E}[X] = \sum\limits_{i = 1}^n p_ix_i $$
$$\text{Variance =}\sigma (X) = \sqrt{\sum\limits_{i = 1}^{n}p_i{(x_i - \mu)^2}{}}$$
where $X$ is a random vector having support $\{x_1, x_2, x_3, \cdots, x_n\}$.
Thus a dataset of samples have a single mean and single variance.
Now, let us discuss about the case of variational auto-encoders. They look like follows

Suppose I trained the above auto-encoder on a training set, then for each sample I will get a mean and standard deviation at latent layer. Here, we can get a new $\mu$ and $\sigma$  for each data sample. But, as we see earlier, mean and standard deviation exists for a dataset and not for each sample.
I am confused about ""how can we say that mean and standard deviation are obtained at latent layer if they are not constant in nature""?
","['autoencoders', 'variational-autoencoder']","
Mean $\mu(x)$ and standard deviation $\sigma(x)$ are actually learnable functions, whose parameters are adjusted via the back propagation procedure.
Mean and standard deviation are not computed on the input vector $x$ or any transform of it.
The procedure is the following:

Pass the sample $x$ from the training data


Propagate this vector $x$ through some NN (Feedforward MLP) and obtain some other vector $\tilde{x}$
Get the mean $\mu(x)$ and std $\sigma(x)$ from $\tilde{x}$ from two more neural networks (maybe single layer)


Generate random noise $\varepsilon$ and get a point in the latent space $\mu(x) + \sigma(x) \varepsilon$ (it is known as reparametrization trick)


You can think about the procedure as follows - you have Normal distribution around each point of the input data in the latent space, and the mean $\mu(x)$ and $\sigma(x)$ are the parameters of this distribution (different for each point). The generated data is expected to resemble the training example, but differ in some reasonable sense, belong to the manifold of realistic images.
"
How to re-train an AI model to have smaller input image size,"
I need a PyTorch Model which can do road segmentation on OAK-D camera.
The model provided requires Input Image Size: 896x512, which is too big for running on OAK-D camera. Thus I need to re-train it with a smaller input size(224x224) and just need the BG(background) and road classes, or if any other options available which can easily make it running on the OAK-D camera.
Does anyone know how to do this?
","['training', 'pytorch', 'image-segmentation', 'semantic-segmentation']","
What you need to search for is a Fully Convolutional Network, i.e. a network that use global pooling to overcome the issue of a fixed input size. Unfortunately the model you found is not fully convolutional, and every workaround to make those pre-trained weights usable implies retraining.
At this point it is more convenient to find something else, or train something yourself. You can take a look at this repo which contains also a fully convolutional network for segmentation among other models (they don't seem to link to any pre-trained weights though).
"
Will there be any changes in the model's performance due to the usage of very small batch sizes?,"
I am trying to run a code that has a batch size around 28. I can run the program on my GPU with this batch size.
But, when I modify the code for my requirements and try to run, it is showing an run-time error due to insufficient memory in GPU.
I checked for possible batch-size that I can run and it is just 2-5.
I am not sure whether there is any issue if I run with such small batch sizes? I mean, will there be any performance issues keeping aside the time it takes?
","['performance', 'batch-size']","
Batch size affects how many training updates (steps) will happen during each epoch.
When the batch size is small, this means that the model sees fewer data in each weights update. Thus, your question really depends on the data you have, along with the corresponding task (classification / RL etc.)
If your data is highly imbalanced, then I would not suggest a small batch size, since the probability of seeing a positive instance would be far smaller (assuming you take uniform batches).
For an RL task, imagine using a replay buffer of past experiences and your agent had very few good action selections during the only exploration process. Then a small batch size would make the agent training very difficult, since most of the time samples with not good action selections would be seen. As a result, the agent may drift from good policies.
For a classification task, what I always do, is to make stratified batches. That is each batch has the same label percent as the whole dataset. And most of the time it works for better than uniform batches even for smaller batch sizes. For RL, I would recommend higher batch sizes or similar clever ways of sampling.
"
Is there any concept like 'applying affine transformation on multiple inputs'?,"
Affine transformation on $X$ is a transformation of the following form
$$Y = wX + b$$
In general, $w, X, Y$ and $b$ tensors.
We generally call tensor $X$ as an input to affine transformation or the tensor which we want to transform. We call $w, b$ as weight and bias tensors respectively. We call $Y$ as output tensor after transformation. Every layer of multi layer percetron contains an affine transformation.
Suppose I have two types of inputs, say $X_1, X_2$.  Now, I want to apply affine transformation on them using other.
Consider the following
#1: Combining using individual affine transformations
$$Y_1 = w_1X_1 + b_1$$
$$Y_2 = w_2X_2 + b_2$$
$$Y_1Y_2 = w_1w_2X_1X_2 + w_1b_2X_1 +w_2b_1X_2 + b_1b_2$$
#2 multiplying them and applying affine
$$Y = wX_1X_2 + b$$
#3 concatenating them and applying affine
$$Y = w (X_1, X_2) + b$$
Does anyone of the above eligible to call affine transformation in terms of $X$ and $Y$ (not $XY$)? If not, is it true that there is nothing like affine transformation on two inputs taken together?
","['math', 'affine-transformations']",
"In the NEAT algorithm, what is the purpose of treating disjoint and excess genes differently?","
In the NEAT algorithm, what is the purpose of treating disjoint and excess genes differently?
They are treated so (or may be treated potentially) at least when calculating the distance between 2 individuals when dividing the population into species (c1 and c2 coefficients).
","['neural-networks', 'genetic-algorithms', 'neat', 'neuroevolution']","
In the original NEAT paper, these two concepts are defined distinctly.

When crossing over, the genes in both genomes with the same innovation
numbers are lined up. These genes are called matching genes. Genes
that do not match are either disjoint or excess, depending on whether
they occur within or outside the range of the other parent’s
innovation numbers. They represent structure that is not present in
the other genome. In composing the offspring, genes are randomly
chosen from either parent at matching genes, whereas all excess or
disjoint genes are always included from the more fit parent.

However in terms of process, they are handled equivalently.  If you see equation 1 on page 110, the equation says using coefficient c1 for Excess E and c2 for Disjoint genes.  But in the parameter settings, c1 and c2 are set to equal values.  So, while the evaluations presented in the paper do not distinguish between excess and disjoint genes from a process perspective, the purpose is for the framework to allow the distinction between D and E genes to be used by the user of the framework by supplying the values that make sense in their context.
"
Couldn't the self-attention mechanism be replaced with a global depth-wise convolution?,"
The main advantages of the self-attention mechanism are:

Ability to capture long-range dependencies
Ease to parallelize on GPU or TPU

However, I wonder why the same goals cannot be achieved by global depthwise convolution (with the kernel size equal to the length of the input sequence) with a comparable amount of flops.
Note:
In the following, I am comparing against the original architecture from the paper Attention Is All You Need.
Idea:
Consider the depthwise convolution of size $L$ with circular padding:
$$
y_{t,c} = W_{t^{'},c} x_{t^{'} + t, c}
$$
Here, $x$ is the input signal and $y$ is the output signal,
$t$ is the position in the sequence, and $c$ is the channel index.
Since the convolution is depthwise the given output channel depends on the unique input channel (we would like to have linear complexity in the dimension of the embedding vector).
After a single convolution, one definitely would not have any interactions between the tokens in the sequence.
However, a two-layer convolutional network with these tokens is able to capture long-range pair-wise interactions:
$$
x_{t,c}^{(2)} = W_{t^{''},c}^{(2)} \sigma(W_{t^{'},c}^{(1)} x_{t^{'} + t, c}^{(0)})
$$
And by stacking a not very large number of these layers (like 12 or 24) one can model interactions between tokens in the sequence of arbitrary complexity.
Comparison of complexity:
The asymptotic complexity of both approaches seems to be the same.

Attention: $O (L^2 d)$

Depthwise convolution: $O (L^2 d)$


However, dot product attention seems to be a rather intuitive and biologically motivated operation that is crucial for sequence problems.
Has this question been studied in the literature or discussed somewhere before?
EDIT
De-facto global depthwise convolution is used in MLP-Mixer. One stage performs convolution with global receptive field (of the size of feature map), and other operation is pointwise convolution with kernel_size=1.

","['reference-request', 'attention', 'convolution', 'sequence-modeling']",
What exactly is a grid-like topology according to the book Deep Learning?,"
I am reading this book called ""Deep Learning"" (by Goodfellow, Bengio and Courville).
On page 326, in the first paragraph, it says:

CNNs, are a specialized kind of neural network for processing data that has a known grid-like topology. Examples include time-series data, which can be thought of as a 1-D grid taking samples at regular time intervals, and image data, which can be thought of as a 2-D grid of pixel

Considering an image as a grid is completely intuitive. And, similarly, we can extend the logic to a 1-D time series.
But then what cannot be considered as having a grid-like structure?
","['deep-learning', 'terminology', 'geometric-deep-learning', 'books', 'non-euclidean-data']",
Why do skip layer connections require the same layer sizes?,"
I know how skip connections work: you add the activations of the previous layer to the activations of a successive layer to stabilize information/gradient flow.
My question is, why doesn't it just get implemented in the seemingly more sensible way of concatenating some previous layer's activations onto a later layer's activations?
Most regularization methods are implemented somewhat transparently (to avoid possible negative consequences, e.g. BatchNorm having learnable parameters to disable it). While this method instead interferes with regular functioning of the network rather than simply making itself available in case it is useful.
What is the reasoning behind the choice to do this rather than simply using concatenation?
",['residual-networks'],"

Of course, it does create more parameters to train, but that seems like a small sacrifice to me.

It is not a ""small sacrifice"". For the very deep networks that skip connections are applied to, to get the same benefits when concatenating, you would end up witha significant multiplier on the number of parameters.
To get the same passthrough effect on gradient signals (and allow later layers to learn modifications to the identity function), each layer's output would need to be copied to all the following layers. This scales poorly.
Let's take an example of a 10-layer fully-connected network, with 100 neurons per layer in the hidden layers where we want to apply skip connections. In the simple version of this network (ignoring bias to keep the maths simpler), there are 100x100=10,000 parameters for each added layer, making 90,000 parameters overall. If you use addition-based skip connections, the total number of parameters remains the same, at 90,000. If you use concatenation, the layers connect as 100x100, (100+100)x100, (100+100+100)x100 etc, so you end up with 450,000 parameters, five times as many. This is not a ""small sacrifice"", this is a scaling problem.
The technique of concatenating layers into later parts of the network is known and used. As is adding ""early output"" or head layers to generate gradient signals at different points in the network. These are valid approaches, and can help with vanishing gradient problems in a similar way. However, the additive skip connections in a residual network scale much better into very deep networks.
Concatenated copies of layers, or additional network heads with the same target functions and loss, are still used, but more sparingly.

This avoids the strange choice of addition, which has a questionable benefit

It may avoid a specific addition mechanism that you seem concerned about. However there are still very similar additions occurring when the concatenated layer feeds forward to the next layer. You could set those weights in a specific way (a copy of the weights of the new layer that it is concatenating with for just two combined layers) when the layer sizes are the same, and the concatenation and addition approaches would very similar.
"
How can GPT-3 be used for designing electronic circuits from text descriptions?,"
I was wondering if it is possible to use GPT-3 to translate text description of a circuit to any circuit design language program, which in turn can be used to make the circuit.
If it is possible, what approach will you suggest?
","['generative-adversarial-networks', 'applications', 'text-generation', 'gpt-3']","
As of today (9 Feb 2023) it is possible to create a HSPICE output from verbal description of an electric circuit using chatGPT.
Simply describe a circuit and its element to chatGPT, and ask it to write the HSPICE code to simulate the circuit. Not only chatGPT is able to solve the circuit for you step by step, it can also create the HSPICE format correctly.
Here is chatGPT's HSPICE output for a simple circuit and the description of the code:
* Electric Circuit with 3 elements
V1 1 0 DC 10V
R1 1 2 20Ohm
R2 2 0 30Ohm

* Simulation command
.OP
.DC V1 10 20 0.1
.PRINT DC V(2)
.END



Explanation of the code:


The first two lines define the voltage source V1 with node numbers 1 and 0, and a DC voltage of 10V.
The next two lines define the resistors R1 and R2 with node numbers and their resistance values.
The .OP command sets the simulation to operate point analysis.
The .DC command sets up a DC sweep analysis with the sweep variable V1 ranging from 10V to 20V in steps of 0.1V.
The .PRINT command specifies the variables to be printed in the output, in this case, the voltage at node 2.
The .END command marks the end of the simulation.
This code should give you the voltage across R1 as a function of the voltage source V1.


All I can say is that we are on the verge of a revolution! This is as big as the Internet, and can help experts to make much more rapid progress in many fields of science.
"
Why the partial derivative is $0$ when $F_{ij}^l < 0$?. Math behind style transfer,"
I am currently in the process of reading and understanding the process of style transfer. I came across this equation in the research paper which went like -

For context, here is the paragraph -

Generally each layer in the network defines a non-linear
filter bank whose complexity increases with the position of
the layer in the network. Hence a given input image is
encoded in each layer of the Convolutional Neural Network
by the filter responses to that image. A layer with $N_l$ distinct filters has $N$ feature maps each of size $M$ , where $M_l$
is the height times the width of the feature map. So the re-
sponses in a layer l can be stored in a matrix $Fl ∈ R^{N_l×M_l}$
where F l is the activation of the ith filter at position j in ij
layer l.
To visualise the image information that is encoded at
different layers of the hierarchy one can perform gradient descent on a white noise image to find another image that matches the feature responses of the original image (Fig 1, content reconstructions). Let $\vec p$ and $\vec x$ be the original image and the image that is generated, and $P^l$ and $F^l$ their respective feature representation in layer l. We then define the squared-error loss between the two feature representations
$\mathcal{L_{content}(\vec p, \vec x, l)} = {1\over 2} \Sigma_{i,j} \big(F_{ij}^l - P_{ij}^l \big)$. The derivative of this loss with respect to the activations in layer $l$ [the equation above $(2)$].

I just want to know why the partial derivative is $0$ when $F_{ij}^l < 0$.
","['convolutional-neural-networks', 'relu', 'calculus', 'vgg', 'style-transfer']","
$F_l$ is the activation of the filter. They state in the paper that they base their method on VGG-Network, which uses ReLU as its activation function. In fact, VGG uses it in all of its hidden layers. ReLU is defined as
$$f(x) = max(0,x)$$
Since ReLU is 0 for all x's below 0, the equation above holds; When x is non-positive, all terms in the loss function are constants with respect to $F_{ij}^l$.
"
What would be the advantage of making channel dimension first in TensorFlow Keras implementation?,"
I was reproducing the findings of a research article in which I discovered that they had switched the Channel dimension from last to first. To clarify this concept, I went through A Gentle Introduction to Channels-First and Channels-Last Image Formats . The author of this link stated:

When represented as three-dimensional arrays, the channel dimension
for the image data is last by default, but may be moved to be the
first dimension, often for performance-tuning reasons.

There are two ways to represent the image data as a three dimensional array. The first involves having the channels as the last or third dimension in the array. This is called “channels last“. The second involves having the channels as the first dimension in the array, called “channels first“.

Channels Last. Image data is represented in a three-dimensional array where the last channel represents the color channels, e.g.
[rows][cols][channels].
Channels First. Image data is represented in a three-dimensional array where the first channel represents the color channels, e.g.
[channels][rows][cols].

.
We are aware of when the channel was last used and the manner in which kernels were applied, theoretically. However, I'm curious as to when the dimensions of the channel come first. How kernels will process. More precisely:
Assume we have [rows, columns, channels] -> [2,4,3] image dimensions. We may say we have three data channels, each with two rows and four columns, correct?
Alternatively, if we assume that channel dimensions are first means [channels, rows, columns] -> [3,2,4]. In other words, we now have four data channels, each with three rows and two columns, am I correct? If I am taking correctly than this is quite confusing because we are completely modifying our image.
Question:
What is the benefit of shifting the channel dimension first, and how will the kernels move on it?

For more detail check the code:
input_layer = tf.keras.Input(shape=input_shape, name=""Time_Series_Activity"")
con_l1 = tf.keras.layers.Conv2D(64, (5, 1), activation=""relu"", data_format='channels_first')(input_layer)

Summary of Code
Layer (type)                 Output Shape              Param #   
=================================================================
Time_Series_Activity (InputL [(None, 1, 30, 52)]       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 52)        384       
_________________________________________________________________

","['deep-learning', 'convolutional-neural-networks', 'tensorflow', 'keras']",
Do other online/incremental algorithms not suffer from catastrophic forgetting?,"
All the literature I read seems to indicate catastrophic forgetting affects only neural networks. Do other online/incremental algorithms not suffer from catastrophic forgetting (for example, SGDClassifier)? Why would that be the case?
","['machine-learning', 'incremental-learning', 'online-learning', 'catastrophic-forgetting']",
In what situation would you want to use NEAT over reinforcement learning?,"
NEAT is an evolutionary algorithm. When would you want to use NEAT over more traditional/common RL algorithms like PPO or SAC etc. What advantage does it give you?
","['machine-learning', 'reinforcement-learning', 'deep-rl', 'neat']","
In my opinion, this shouldn't be an either/or question.  Both NEAT and rl with fixed network topology has their own advantage when solving decision problem.
NEAT is good to solve a simple problem fast with minimum network topology and without local optimum issue.
While RL with fixed topology suffers local optimum but learning more directly, and policy-gradient based Rl would be even more adaptive to an environment with stochasticity in the context of statistics.
Then, why not just combine them? At present, the best paper on this topic is this： https://dl.acm.org/doi/10.1145/3205455.3205536
In this paper, it is suggested that doing NEAT at first, then KEEP DOING RL forever, though I don't agree with this special routine.
I think, both NEAT and RL should do interactively during the WHOLE training process.
The problem is, how to combine these 2 in an effective way. One problem I met is for RL, like SAC, which has 2 outputs(one for policy and one for Q-value), And the topology of the Q-value output has no contribution when doing NEAT. Then, how to deal with the Q-value topology? If both the 2 outputs shares some layers at first, then the learning would become more unstable, since doing NEAT would dramatically change these 2 outputs.
The paper above takes a fixed output layer for both 2 outputs, which I think is just a workaround, not the best way.
"
Can NeuralHash be used as a loss for an Autoencoder?,"
I've recently read about NeuralHash, and immediately thought that it might be used as a loss for an autoencoder. However, it only seems to preserve ""structure"" from what I've read, not actual pixel values (which makes sense, given its purpose). Thus, how likely it is that an autoencoder performs well given a loss that compares the NeuralHash of its output with the NeuralHash of its input?
I feel like, assuming that NeuralHash is secure, it should either work well, that is produce an image similar to its input (because the hash is approximately unique) or not work at all (otherwise we would have found a collision), no middle-ground. Is there any thoughts/research on this?
","['neural-networks', 'machine-learning', 'generative-adversarial-networks']",
How to incorporate action information in the state input of a DQN?,"
I am working on an RL problem that I am trying to solve using a Deep Q-network. The problem concerns choosing drivers to take specific taxi orders. I am familiar with most of the existing works and that they use RL to determine which orders to take. I specifically look at the situation where we want to determine which drivers to take.
This means that action space concerns the various drivers we can choose. Initially, we assume a fixed number of drivers to ensure a fixed action space.
My question is about defining the state space. First of all, the state space consists of information about the next order we are trying to assign to a driver from our action set. Besides that, we also want to incorporate state information about the different drivers (e.g. their location). However, this would mean we include state information about the actions as input of the DQN. The reason is that the state of the drivers is the main thing that changes when choosing a different action and therefore determines the choice we want to make at the next timestep. I am for example thinking about creating a list of size |drivers| with element i defining the location of agent i.
I tried to find existing work that uses a similar setting (so that incorporates action states in the state input), however, I did not succeed in this yet. Therefore I am wondering:

Is this a logical/reasonable approach to the problem?
If yes, is someone familiar with existing works that use a comparable approach?

I am familiar with works that take (state, action) as input, which describes the full pair of the state s and the action a, and then produce a single Q(s,a) for each specific pair of state + action. This is an approach we do not want to take, given that it leads to |A(s)| passes through the network instead of a single pass (as explained here).
","['reinforcement-learning', 'dqn', 'state-spaces', 'action-spaces']",
Transforming a complex if-else decision-making to ML,"
I have a time series classification problem that uses a series of if-else statements to arrive at a particular label. I am attempting to use ML/DL to make the system simpler.
So far, I have tried using a tabular data approach where I take a snapshot of information up to a particular point. For example, this will be the rolling sum of certain columns and so on. I have also tried LSTM and CNN. All these approaches have failed to give me F1 scores significantly above 50 %.
Are there other ML/DL approaches that I should try before giving up? The models were built using AutoKeras and PyCaret.
","['time-series', 'rule-based-systems']","
Are you sure, that there is a need for ML? If there is a set of rules that allows to solve this problem without ML, it gives already 100% accuracy, whereas ML/DL will do it up to a certain accuracy, and this task may be even tough for supervised algorithm. Sorting problem is very difficult for neural networks.
Anyway, check whether your algorithm can express these if-else statements. For example such branch, given that the output is of the same size for both conditional branches, can be approximated by the following:
sigmoid(beta * condition) * output_1 + sigmoid(-beta * condition) * output_2

Here beta controls the slope of the sigmoid, condition is the conditional statement of the form $f(x) >(\geqslant) 0$, and output_1 and output_2 are the outcomes of both options.
"
How does noise samples from uniform distribution contribute to the diversity of generator output?,"
In a Generative Adversarial Network (GAN), there are two multi-layer perceptrons. One is the generator network and another is a discriminator network.
The input for the generator network is a noise vector $z$. The input for a discriminator network is either a generated sample $G(z)$ i.e., the output of a generator network or a training sample $x$ for a training dataset.
My doubt is regarding the input of the generator. The noise vector is generally sampled from the standard normal distribution.
$$z \sim \mathcal{N(0, 1)}$$
Although I am not sure, I think : since the values in the normal distribution vary, the output of the generator can vary accordingly.
But some of the research papers say that the noise vector can also be sampled from a uniform distribution i.e., $z \sim \mathcal{U(a, b)}$ for $a<b$.
$$ U(x) = \begin{cases} 
      \dfrac{1}{b-a} & x\in [a, b] \\
      0 & x\not\in [a, b] \\
   \end{cases}
$$
It is clear that uniform distribution does not vary like normal distribution and takes only two possible values, hence all samples have equal probability in the given range. Then how can it contribute to the diversity of the output of the generator network?
","['generator', 'normal-distribution', 'uniform-distribution', 'noise']","
As pointed out in the comments, a random variable $z$ sampled from a uniform distribution $\mathcal{U}(a,b)$, doesn't take only two values but any value between $a$ and $b$ with equal probability $\frac{1}{b-a}$.
The distribution you use for $z$ in a GAN doesn't have a theoretical justification. The fact that it's an $\mathcal{N}(0,1)$ is never used in the proof of the convergence of the generator to the data distribution in the original GAN paper (see Section 4.1).
However, it should be easy and fast to sample from given that you must take samples at each training step. Why certain authors choose one distribution over the other is usually down to empirical trial and error. GANs are notoriously hard to train and the distribution of $z$ is one of the many hyperparameters of the model.
Other generative models though have theoretical justification for the choice of the distribution of the random variable given to the decoder. In VAEs, it is a Gaussian $\mathcal{N}(0,1)$ because the loss function contains a KL term $\text{KL}(p_\theta(z|x)||p(z))$ which has a nice closed form when $p_\theta(z|x)$ and $p(z)$ are Gaussian distributions (see Section 3 in the original VAE paper).
"
How does the learning rate $\alpha$ vary in stationary and non-stationary environments?,"
In Sutton and Barto's book (Chapter 6: TD learning, 2nd edition), he mentions two ways of updating value function:

Monte Carlo method: $V(S_t) \leftarrow V(S_t) + \alpha[G_t - V(S_t)]$.
TD(0) method: $V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$.

I understand that $\alpha$ acts like a learning rate where it take some proportion of MC/TD error and update value function.
From my understanding, in stationary environments, transition probability distribution and reward distribution don't vary with time. Hence, one should supposedly use $\alpha-$decay to update value functions. On the other hand, since distributions change with time in non-stationary environments, $\alpha$ should be kept constant so as to keep updating the value function with recent TD/MC errors (in other words, history doesn't matter).
What's been bothering me is that in Example 6.2, 6.5, and 6.7, probability and reward distribution doesn't change. So why is constant-$\alpha$ being used?
Question: How does $\alpha$ vary in stationary and non-stationary environments?
","['monte-carlo-methods', 'temporal-difference-methods', 'environment', 'learning-rate']",
Is there an entry level textbook on Bayesian Inference that is a nice blend of theory and applications?,"
I am looking for a textbook that is a nice entry level to Bayesian Inference. I was hoping that there is a nice blend of theory and applications (data sets) on how concepts are applied. Programming techniques presented are welcome.
Just for perspective, I feel that Christopher Bishop's PRML is a theoretical treatment. It is very good theoretically, but I find myself not understanding how to apply it given a data set.
I have tried jumping from one book to another and this has just confused me. Is there any authoritative book with these requirements?
","['reference-request', 'books', 'bayesian-statistics', 'bayesian-inference']",
"Given an input of shape $(3, 32, 32)$, which is convolved with a $(3 \times 3)$ kernel, how do I calculate the FLOPS?","
I have an input tensor of shape $\mathbf{(3, 32, 32)}$ consisting of 3 channels, 16 rows, and 16 columns. I want to convolve the input tensor using $\mathbf{(3 \times 3)}$ kernel/filter. How can I calculate the required FLOPs?
","['convolutional-neural-networks', 'filters', 'convolutional-layers', 'computational-complexity', 'flops']",
Why is BatchNormalization causing severe overfitting to my data?,"
So I've been making a mini version of VGGNet, trying to tweak the hyperparameters to match the CIFAR-100 dataset.
It was running slow at first but I was able to get decent accuracy after 60 epochs or so.
However, when I added BatchNormalization layers to my two fully-connected hidden layers, it started learning at like 20% accuracy immediately, but began overfitting my data so badly that after 7 epochs my validation didn't improve from 0.01, compared to 20+ testing accuracy.
Why would adding these layers which are supposed to act as a regularizer, actually cause severe overfitting instead? I'm confused.
","['deep-learning', 'overfitting', 'regularization', 'batch-normalization']",
Why would the Dice coefficient be more suitable than mutual information when you don't want 0-0 matches to be significant?,"
I'm confused about the interpretation and assumptions of the Dice coefficient versus the more popular measure mutual information. I'm specifically referencing its use in hierarchical semantic network analysis, or ranking the significance of collocation of words.
I'm referencing Translating Collocations for Bilingual
Lexicons: A Statistical Approach  which talks about how the Dice coefficient is more appropriate when you don't want 0-0 matches to be significant. However, as a amateur in probability, it's not really clear to me from the respective formulas why this would be.
Could someone explain?
","['comparison', 'information-theory', 'semantic-networks']","
Their reasoning is that mutual information is symmetric, giving equal value to 1s and 0s, as it is derived from information theory, where they are just two symbols used to encode a message, with neither being more important than the other. A message encodes a lot of information if the two symbols are roughly equal in probability.
The Dice coefficient, however, centres on two events occurring at the same time, and so handles 1-1 differently from 0-0, as 1 stands for the occurrence of a (comparatively rare) event, whereas 0 represents the (much more common) absence of the event.
In the formulae, the Dice coefficient adds up the individual probabilities in the denominator, whereas in mutual information they are multiplied. If you add two small numbers, you get a number that is slightly larger than the two individual ones, but if you multiply them, you get one that is much smaller. Mutual information has a well-known problem in that it emphasises extremely rare events, which is why it is not used as much any more as it was in the early 1990s.
Thus the Dice coefficient looks for the mutual occurrences but is less concerned with how often each item occurs on its own (addition vs multiplication of individual probabilities).
"
Is graph embedding linear in its maintaining of graph geometry?,"
It is claimed that the main goal of graph embedding methods is to pack every node's properties into a vector with a smaller dimension, so node similarity in the original complex irregular spaces can be easily quantified in the embedded vector spaces using standard metrics.
However, I can find no formal explanation as to why nodes with similar properties should be embedded so that their separation in embedded space respects their similarity. Is there such a proof, or is it a convenient consequence of embedding?
","['geometric-deep-learning', 'graph-neural-networks', 'embeddings', 'knowledge-graph']",
How to pass variable length data as feature to a neural network?,"
I am working on building a model to classify the type of touch the user makes(Long Press, Left Swipe, Right swipe and so on). I have data with features that characterise the user's touch, like duration, velocity in x-direction, velocity in y-direction etc. One feature that's also present is the trajectory of the touch.
The problem is that for touches like taps or long-press, the length of the trajectory array is 2 or 3 points, but for swipes, it reaches up to 40-100 points. What I thought can work is either use padding or use CNNs. But the problem with padding is that as I am using trajectories, if I pad them with 0s it might affect the learning because a '0' still has meaning in trajectories as some points. And what I think the problem might be with CNNs is that first I don't know if such  an architecture could work for all features (touch duration, xVelocity etc.) as they are not spatially related. I may be wrong about this, feel free to correct it. I also thought of using RNNs but I did not as they are mainly used for NLP tasks and all the features are not related to each other sequentially.
What are the different ways I can handle this kind of variable-sized input feature for neural networks?
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'feature-engineering']",
How to visually or intuitively understand single element multi-dimensional tensors?,"
Consider the following code in PyTorch
>>>torch.tensor([8]).shape
torch.Size([1])
>>>torch.tensor([[8]]).shape
torch.Size([1, 1])
>>>torch.tensor([[[8]]]).shape
torch.Size([1, 1, 1])   

We can notice that we want to store only a single element $8$ in a tensor. But it is possible in tensors to store $8$ in any n-dimensional tensor where $n \in \mathbb{N}$. In strict case $\mathbb{N}$ may be replaced by $\mathbb{W}$.
But, I am facing difficulty in understanding this fact of a single element contributing to all dimensions. If the element is present in all dimensions, then I am assuming that it has to be present multiple times, which is not the case. I can't understand how a single element is contributing any number of dimensions without repeating itself multiple times.
How to understand this phenomenon? How should I interpret or visualize this fact intuitively?
","['tensor', 'dimensionality']","

How should I interpret or visualize this fact intuitively?

You could visualize it as a point in a geometrical space:

8 is just a number
[8] is just a number in a line
[[8]] is a number in a plane
[[[8]]] is a number in a space

The object (number 8) won't change. The space around it changes.

You can never represent a complex object (3d-cube) in a simpler shape (2d-plane).
But you can always represent a simpler object (2d-square) in a higher dimensional shape (3d-space).

A number is a simplest possible object, and therefore it ""fits inside"" (can be represented in) any dimension.
"
Training a sequential model that can only evaluate after several hundred cycles,"
I'm attempting to build a neural network to play the card game, Lost Cities.
A brief overview of the game:

The game involves two players taking turns to play cards on expeditions.
Expeditions incur a debt when you play the first card. Subsequent cards will buy out of that debt and return a profit.
Each player must either play or discard a card (with restrictions), and then draw a new card from the deck or the discard pile.
The game ends when the last card is drawn.

The expanded rules can be found here.
I'm attempting to train a sequential model to play this game, with the state of the board/hand/discard pile as a set of inputs and a three heuristic values for each card (value of playing, value of discarding, value of picking up from this discard) as the output values. However, it's unclear to me how I should approach this from a training perspective.
My biggest hurdle is that the network's success can only be evaluated by wether or not it can ""beat"" itself or a competing network in a game. A player's raw score during the game is not viable due to the nature of play. To me, this means that the network will have to be used with several hundred different sets of input data (for each turn of state of the game board during a match) before any meaningful results are generated.
So far, the only solution I've had minor success with is a generational algorithm that creates ""fuzzy"" children to compete against each other for the most ""wins"". This was done in Python's standard library, and was obscenely slow, even with a reduced sequential network.
My question:
Is there an established method to deliver delayed feedback to a network (after several uses of the network)?
I'm very new at this, so any and all feedback is more than welcomed.
","['neural-networks', 'tensorflow', 'games-of-chance']",
What are the recurrences used for updating state value function in $TD$ and $TD(\lambda)$ learning?,"
There are two types of value functions in reinforcement learning: State value function $V^{\pi} (s)$, state-action  value function $Q^{\pi}(s, a)$.
State value function:
This value tells us how good to be in state $s$ if we are following policy $\pi$. Formally, it can be defined as the average returns obtained at time step $t$ from state $s$ if we follow policy $\pi$.
$$V^{\pi}(s) = \mathbb{E}_{\pi}[R_{t}|s_t = s] = \mathbb{E}_{\pi} \left[ \sum \limits_{k=0}^{\infty} \gamma^{k}r_{t+k+1} \mid s_t = s\right] = \mathbb{E}_{\pi} \left[ \sum \limits_{k=0}^{\infty} \gamma^{k}r_{t+k+1} \mid s_t = s, a_t = a \right]$$
State-action value function:
This value tells us how good is to to perform action $a$ in state $s$ if we are following policy $\pi$. Formally, it can be defined as the average returns obtained at time step $t$ from state $s$ and action $a$ if we follow policy $\pi$ further.
$$Q^{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t}|s_t = s, a_t = a] = \mathbb{E}_{\pi} \left[ \sum \limits_{k=0}^{\infty} \gamma^{k}r_{t+k+1} \mid s_t = s, a_t = a\right] = \mathbb{E}_{\pi} \left[ \sum \limits_{k=0}^{\infty} \gamma^{k}r_{t+k+1} \mid s_t = s, a_t = a \right]$$
Now, Q-learning and SARSA learning algorithms are generally used to update $Q$ function under policy $\pi$ using the following recurrences respectively
$$Q(s_t,a_t) = Q(s_t,a_t) + \alpha[r_{t+1} + \gamma \max\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t)] $$
$$Q(s_t,a_t) = Q(s_t,a_t) + \alpha[r_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)] $$
Now my doubt is about the recurrence relations in Temporal Difference (TD) algorithms that update state value functions. Are they same as the recurrences provided above?
$$V(s_t) = V(s_t) + \alpha[r_{t+1} + \gamma \max V(s_{t+1}) - V(s_t)] $$
$$V(s_t) = V(s_t) + \alpha[r_{t+1} + \gamma V(s_{t+1}) - V(s_t)] $$
If yes, what are the names of the algorithms that uses these recurrences?
","['reinforcement-learning', 'value-functions', 'temporal-difference-methods']",
Is it possible to use Softmax as an activation function for actor (policy) network in TD3 or SAC Reinforcement learning algorithms?,"
As I understand from literature, normally, the last activation in an actor (policy) network in TD3 and SAC algorithms is a Tanh function, which is scaled by a certain limit.
My action vector is perfectly described as a vector, where all values are between 0 and 1, and which should sum up to 1. This is perfect for a Softmax function. But these values are not probabilities of discrete actions. Each value in action vector should be a percentage from the whole portfolio to be invested in a certain stock.
But I cannot figure out, if it would be mathematically fine to use Softmax as an activation layer in TD3 or SAC?
","['reinforcement-learning', 'deep-rl', 'activation-functions', 'soft-actor-critic', 'td3']",
"In deep reinforcement learning, what is this model with state as input and value as output?","
I was looking at this implementation for creating an agent for playing Tetris using DeepRL.
This model uses ""a state based on the statistics of the board after a potential action. All predictions would be compared but the action with the best state would be used"".
So at each iteration, it's feeding a set of future states, computed based on the current state (future state made up of statistics of the game like nb of holes in the board, cleared rows, total height...)
to a neural network and outputs one ""value"" per future state.
So at every step, you predict N ""values"" from the neural network for all N possible future states for the one you are currently in and choose the greatest one as your future state and thus associated action.
Now, my issue: the implementation says it's ""deep Q-learning"", but I do not see it that way. The action, nor some sort of current state is given as input of the network.
Since it is feeding the ""future states"", for me, it looks more like a value iteration algorithm with a neural network or at least something where you know the transition model?
Did I miss something and it is actually DQN?
If not, do you have any references for this kind of RL model? Does this have a name?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'model-based-methods', 'model-free-methods']",
What is the bit memory task?,"
I learned from this post about the so-called bit memory:

They froze its self-attention and feed-forward layers and, in separate copies, fine-tuned peripheral layers on each on a wide range of tasks: Bit memory (memorizing strings of bits), Bit XOR (performing logical operations on pairs of strings of bits), ListOps (parsing and performing mathematical operations), MNIST, CIFAR-10 (classification of images), CFAR-10 LRA (classification of flattened, greyscale images), and remote homology detection (predicting what kind of protein structure an amino acid is part of).

I wonder what the ""bit memory"" task is? Is it an identity function as described in this post? Or the memory network?
","['transformer', 'attention', 'memory']",
Is seq2seq the best model when input/output sequences have fixed length?,"
I understand that seq2seq models are perfectly suitable when the input and/or the output have variable lengths. However, if we know exactly the input/output sequence lengths of the neural network. Is this the best approach?
","['time-series', 'sequence-modeling', 'seq2seq']","
If you would classify a transformer as Seq2Seq, then it is arguably the best. This is only arguably due to accuracy.
Shallow neural networks or even decision trees and forests may be better in production due to lower training time, lower inference time and smaller size in memory.
Overall, it requires a bit of a compromise, and you need to do what suits your use case. For example you wouldn't run GPT-3 on a raspberry pi pico (for example). So it depends on what you mean by ""best"".
"
How to allow RL systems to find better policies after code changes?,"
Suppose that in version 1 of a reinforcement-learning system an optimal policy $A$ got generated for executing a task. But, in a newer version 2 of that application (with new code changes), there might be some policy $B$ that would do slightly (1-2%) better than policy $A$.
How do you allow the system to learn that ""better"" policy $B$? I think the answer is retraining.
But during the training process, the old policy $A$ might still keep accumulating rewards delaying policy $B$ to be recognised as the ""better"" policy than $A$. This could get worse if each newer version of the system would contain a better policy which is only slightly better than the previous release's best policy. It would take a very long time to find the best policy.
Is this accepted in real-world RL systems? Or should I be figuring out a way to tell the system that ""Hey, there might be a better policy somewhere, try to find that instead of rewarding existing policies.""?
","['reinforcement-learning', 'policies']",
"In the original GAN paper, why is it mentioned that you can sample deep directed graphical models without a Markov chain?","
In the original GAN paper (table 2), why is it mentioned that you can sample deep directed graphical models without a Markov chain (well, they say without difficulties, but others list MCMC as a difficulty).
I was wondering how this is done because I have only seen MCMC based approaches.

","['bayesian-networks', 'markov-chain', 'probabilistic-graphical-models']",
Is there any inherent assumption of start and goal states in an MDP?,"
MDP stands for the Markov decision process. It is a 5-length tuple used in reinforcement learning.
$$MDP = (S, A, T, R, \pi)$$
$S$ stands for a set of states, also called state space.
$A$ stands for a set of actions, also called action space.
$T$ is a probability distribution function  $$T: S\times A \times S\rightarrow [0,1]$$
$R$ is a reward function
$$R: S\times A  \rightarrow \mathbb{R}$$
$\pi$ is a policy function
$$\pi: S\times A  \rightarrow [0,1]$$
This question is restricted to continuous spaces i.e., state and action spaces are continuous. And also to stochastic policy function. And also consider only the basic MDP instead of its flavors.
In general, MDP in reinforcement learning is applied mostly to games. And most of the games have certain start states as well as goal states.
Is there any reason for not specifying start and goal states in MDP like in a finite automaton?
Or does MDP has an implicit start and goal states (say from the values of reward function)?
Or is the MDP, by nature, defined irrespective to start and goal states? If yes, can I just imagine MDP as a state-space search problem without a particular goal?
","['reinforcement-learning', 'markov-decision-process', 'state-spaces']","

Is there any reason for not specifying start and goal states in MDP like in a finite automaton?

In general MDPs have a start state distribution. That may be a single state, but does not have to be. In non-episodic problems, you might want to consider a long term state distribution under any given policy, although it is quite common to use a simple start distribution and the assumption of ergodicity for long term distribution.
In general MDPs do not have goal states. Although using the agent's actions to achieve certain desirable end states, such as winning a game or completing a puzzle, is a very common design, there is no requirement for this. The more general requirement is to maximise some aggregate of the reward at each time step - usually either a discounted sum of rewards or the mean reward.

Or does MDP has an implicit start and goal states (say from the values of reward function)?

No, although if you are designing an MDP to model some environment, and it has goal states, you will typically take the goal states into account. Likewise you will usually select the start state distribution as part of the problem definition.

Or is the MDP, by nature, defined irrespective to start and goal states?

You will need to choose at least a distribution of start states to use an MDP practically.
There is no requirement to set a goal state. Whether or not you do that depends on the problem you are modelling.

If yes, can I just imagine MDP as a state-space search problem without a particular goal?

There may or may not be a goal state. You cannot frame RL as state-space search in general. The general solution to an RL control problem is one that maximises an aggregate (sum or mean) over the rewards. There is no requirement for that reward to be received from any single state.
You can usually consider RL control methods to be policy-space searches. The value-based methods such as Q-learning perform the policy search indirectly, whilst policy gradient methods such as REINFORCE model the policy function and optimise it.
The reverse situation, if you do have a state-space search problem, for example some form of combinatorial optimisation, then you can frame it as an RL problem. However, RL would normally be a very inefficient way to perform the search, because it will perform a policy search through trial and error to find the policy which builds the desired state from a starting state. Much better AI tools exist for graph searches and combinatorial optimisations than learning the whole series of actions required to convert an arbitrary start state to a goal state through trial and error.

Aside:

$R$ is a reward function
$$R: S\times A  \rightarrow \mathbb{R}$$

This is not general. This looks more like an expected reward function. You can derive the Bellman equations using an expected reward function, so using the expected reward function does not interfere with most RL theory. However, individual rewards may be based on the next state, and can be stochastic, so the reward function you list does not fully define an MDP - the difference would be important when considering qualities of the MDP such as variance which will impact agent learning efficiency for example.
"
Which product operation should be used in affine transformation?,"
Affine transformation as I am aware can be expressed as either dot product followed by addition or a matrix multiplication followed by addition
$$a.x+b$$
$$a^{T}x + b$$
where the first one is based on dot product and the second one is based on matrix multiplication. it should be noticed that $a, x$ are column vectors here and $b$ is a real number.
In case if a matrix $a$ is compatible (say order $m \times n$) and $x, b$ are column vectors of order $m \times 1$. Then affine transformation is (generally) a matrix multiplication followed by a vector addition.
$$a^{T}x + b$$
I think it is correct.
I have doubt about the product operation between $a$ and $b$ if $a, b$ and $x$ are tensors of higher dimensions. In the case of tensors, should we need to perform Hadamard product or normal matrix multiplication? Or are they both equivalent like in the case of affine transformation on column vectors?
I got this doubt because I encountered an affine transformation that neither uses dot product nor matrix multiplication but uses Hadamard product.

Background: Recently I came across an affine transformation that applies Hadamard product on reshaped weight $a$ and input $x$ and then adds reshaped bias $b$ to it
Initially, the dimensions are as follows
a -> [r, c]
x -> [r, c, d1, d2]
b -> [r, c]

Later they reshaped weight and bias to the shape of the input
a -> [r, c, d1, d2]
x -> [r, c, d1, d2]
b -> [r, c, d1, d2]

and finally, they are performing a * x + b
here $*$ is Hadamard product, an element-wise multiplication operation and (i think) is entirely different from normal matrix multiplication.
Is there any clue on what they did? Is it possible to view Hadamard's product as normal matrix multiplication?
","['tensor', 'affine-transformations']",
Is the range of inception score flexible or bounded based on number of classes?,"
Inception score is used to evaluate the generative models. It is a score given based on quality and diversity of images generated.
I have doubt about the range of inception score because of the reason that an article mentions about the possibility of range $[0, \infty]$ and still talks about upper bound in practical setting

The lowest score possible is zero. Mathematically the highest possible
score is infinity, although in practice there will probably emerge a
non-infinite ceiling. For a ceiling to the IS, imagine that our
generators produce perfectly uniform marginal label distributions and
a single label delta distribution for each image — then the score
would be bounded by the number of labels.

Suppose I have 1000 classes/labels in my task, then is it possible to get an inception score of 2000? Or is it mandatory that the inception score must lie in $[1, 1000]$?
To be concise: Is bounding inception score to a particular range $[1, \text{number of classes}]$ optional or mandatory?
","['math', 'generative-model', 'image-generation']","
Yes. You are right. The IS is bound by the number of classes.
This paper titled ""A Note on the Inception Score"" clearly shows a formal proof of the same. Please head to section 3.3 of the main text for a description and the appendix for the proof.
"
What is the significance behind having small kernel sizes over having one large kernel size that covers the entire input in a CNN?,"
I have hardly ever seen anyone cover the entire input image with a filter of the same dimensions. I was wondering why that is the case, and if the performance in say, an image detection application would decrease if someone used kernel size = the size of the input image itself?
","['machine-learning', 'convolutional-neural-networks', 'image-processing', 'convolutional-layers', 'filters']",
Are there any other metrics available for calculating the distance between two probability distributions other than those mentioned?,"
The divergence between two probability distributions is used in calculating the difference between the true distribution and generated distribution. These divergence metrics are used in loss functions.
Some divergence metrics that are generally used in literature are:

Kullback-Leibler Divergence
Jensen–Shannon divergence
f-divergence
Wasserstein distance

Some other divergence measures include:

Squared Hellinger distance
Jeffreys divergence
Chernoff's $\alpha-$divergence
Exponential divergence
Kagan's divergence
$(\alpha, \beta)-$product divergence
Bregman divergence

I think some naive divergence measures include

Least-squares divergence
Absolute deviation

Along with these, are there any other divergence measures available to compute the distance between the true probability distribution and estimated probability distribution in artificial intelligence?
","['probability-distribution', 'metric']",
Is there any geometrical interpretation on overcoming gradient related problems by adjusting/changing loss function?,"
There are instances in literature where we need to change loss function in order to escape from gradient problems.
Let $L_f$ be a loss function for a model I need to train on. Some times $L_f$ leads to the problems due to gradient. So I reformulate it to $L_g$ and can apply the optimization successfully. Most of the times the new loss function is obtained by making a small adjustments on $L_f$.

For example: Consider the following excerpt from the paper titled Evolutionary Generative Adversarial Networks

In the original GAN, training the generator was equal to minimizing
the JSD between the data distribution and the generated distribution,
which easily resulted in the vanishing gradient problem. To solve this
issue, a nonsaturating heuristic objective (i.e., “$− \log D$ trick”)
replaced the minimax objective function to penalize the generator


How can one understand those facts geometrically? Are there any simple examples on either 2d or 3d that shows two types of curves: one gives no gradient issues and the other gives gradient issues yet both obtains the same objective?
","['math', 'gradient-descent', 'graphs', 'vanishing-gradient-problem']",
Is stability an attribute of model or training algorithm used or combination of both?,"
From this answer, stability is attributed to a learning algorithm

A stable learning algorithm is one for which the prediction does not
change much when the training data is modified slightly.

At some other places, I read the phrase ""stability of neural network model"". I am not sure whether the stability of a learning algorithm and the stability of a model are the same or not. If same then

A stable model is one for which the prediction does not
change much when the training data is modified slightly.

Is it true? If not, is there anything called stability of a model and is different from the stability of a learning algorithm?
Suppose I am training a neural network model with a gradient descent algorithm. For which one, do I need to attribute stability or instability? Is it to the neural network model or to the gradient descent training algorithm? Or it should be attributed to the combination of both?
","['neural-networks', 'training', 'stability']",
Does it make sense for a logistic regression model to perform better than a neural network on the Iris data set?,"
Per a review post, a simple Logistic Regression model on the Iris data set gets about 97% test accuracy on iris dataset whereas a neural network gets just 94%. The neural network model used in Keras is
model = tf.keras.Sequential([
    tf.keras.layers.Dense(500, input_dim=4, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

The model is fit for 30 epochs using a batch size of 20.
Note that I did try fewer neurons and layers but none of them got better performance.
Does this make sense? Can any other neural network get a higher test accuracy than a logistic regression model?
","['neural-networks', 'logistic-regression', 'iris-dataset']","
The most likely explanation is that the neural network model proposed has a much higher model capacity than the logistic regression model.
In fact, the neural network used in the code has 246,343 trainable parameters; the full Iris data set has only 150 samples and only four features – so the model is much more complex than the training data. A neural network model with fewer neurons or layers will likely generalise much better. A logistic regression model is much simpler and has far fewer parameters, so it is in some sense forced to ""learn""
better.
The neural network may simply overfit to the training data as the capacity of the model is large enough; an analogous idea is trying to fit a line to data, then trying to fit a degree 100 polynomial. While the polynomial may fit the training
data better, it is less likely to generalise well.
The hyperparameters used in the training of the network might also not be optimal; I haven't experimented, but neural network training can be a little sensitive to the choices made during training.
"
How is this statement from a TensorFlow implementation of a certain KL-divergence formula related to the corresponding formula?,"
I am trying to understand a certain KL-divergence formula (which can be found on page 6 of the paper Evidential Deep Learning to Quantify Classification Uncertainty) and found a TensorFlow implementation for it. I understand most parts of the formula and put colored frames around them. Unfortunately, there is one term in the implementation (underlined red) that I can't tell how it fits in the formula.

Is this a mistake in the implementation? I don't understand how the red part is necessary.
","['deep-learning', 'tensorflow', 'papers', 'implementation', 'kl-divergence']",
"What's the difference between a ""perceptron"" and a GLM?","
In a comment to this question user nbro comments:

As a side note, ""perceptrons"" and ""neural networks"" may not be the same thing. People usually use the term perceptron to refer to a very simple neural network that has no hidden layer. Maybe you meant the term ""multi-layer perceptron"" (MLP).

As I understand it, a simple neural network with no hidden layer would simply be a linear model with a non-linearity put on top of it. That sounds exactly like a generalized linear model (GLM), with the non-linearity being the GLM's link function.
Is there a notable difference between (non-multi-layer) perceptrons and GLMs? Or is it simply another case of two equivalent methods having different names from different researchers?
",['perceptron'],
How to reduce the number of episodes before the agent learns in this game?,"
The initial environment state is 0.25. Each time step the agent performs a discrete action of 0 or 1. If action is 1, then the new state will be state + 0.1. If action is 0, the new state will be state - random() * 0.2. The reward is state - 0.5, however if state > 0.98 (or state < 0) the agent dies (with no reward).
First question: How do I teach the agent not to be too greedy? How to verify that the agent learned?
Main question: How to reduce the number of trials (i.e. the number of episodes) before the agent learns?
I would also appreciate any relevant references.
Here is the environment and here is what I tried.
It works, however:

It took 1000 episodes of max 2000 timesteps, which is unacceptable
for me (I wish to drastically reduce the number of episodes and timesteps).

The behavior is far from optimal. Ideally, the agent should
choose action 0 only if the state is larger than 0.88 (or something below that and within a small interval such as 0.01). [Edit] However, the threshold is 0.75, that forces the agent to choose 0 even if it could safely choose 1, e.g. following 0.8 -> 0.76 -> 0.75 -> 0.74 trajectory before choosing
1 again.


","['reinforcement-learning', 'deep-rl', 'exploration-exploitation-tradeoff', 'exploration-strategies']","
TLDR: Simplify your agent.
Context:
As you've noticed, it's not a hard game and it does not require a complex policy.
You'd need a single neuron to solve it perfectly: 1 if (state < 0.78) else 0
Problem:
However, the default agent comes with a big neural network (I believe it's a 2 layered 64 fully connected neural network). So you have thousands of parameters trying to solve a simple problem. And that's why it takes thousands of time-steps.
Solution:
So first thing I'd recommend is to use a simpler policy model.
If it doesn't completely solve the problem, then simplify your model by diving deeper on your agent and disabling the functions you think aren't helpful.
https://tensorforce.readthedocs.io/en/latest/modules/policies.html
Lastly, if you really need to nail it, you can use some meta-learning for tuning the hyper-parameters.

Edit: adding Meta-Learning
Meta-Learning is a broad concept of using machine learning for setting up your machine learning architecture and/or hyperparameters. It's an automation of your trial and error process, exploring and exploiting different configurations in a search for a good model.
A related concept is auto-ML.
Keep in mind that:

It's a very extensive computational process, since you need to train and evaluate thousand or millions of models.
You'll need to explicitly define your definition of a good model:
Will you reward it to be simple? Are you looking for fast training speed? Does in need to be light for deploying? Or you want a huge, complex, but accurate model?

In your case, if the training time is too big, you could limit the training time, so you'll find the best model that can be trained in a fixed short time window. And once again, keep in mind it will take way more time to find a good model this way, than just selecting a not ideal architecture and train it extensively.
Here is a Siraj's video talking about the general concept.
"
"Assume 120 examples, a model makes 20 correct predictions and updates weight for the other 100. Should I count this epoch 100 iterations or 120?","
Per google's glossary, an iteration refers to

A single update of a model's weights during training ...

The following code comes from a github repo
def fit(self, x, y, verbose=False, seed=None):
    indices = np.arange(len(x))
    for i in range(self.n_epoch):
        n_iter = 0
        np.random.seed(seed)
        np.random.shuffle(indices)
        for idx in indices:
            if(self.predict(x[idx])!=y[idx]):
                self.update_weights(x[idx], y[idx], verbose)
            else:
                n_iter += 1
        if(n_iter==len(x)):
            print('model gets 100% train accuracy after {} epoch(s)'.format(i))
            break

Note that this model doesn't update weights for each single example, because when the model make a correct prediction for some example, it skips the example without updating weights.
In this kind of scenario where model makes a correct prediction for $i$th input $x_i$ and jump into next example $x_{i+1}$ without updating weights for $x_i$, does it count as an iteration?
Assume there are 120 training examples, in one epoch, the model makes 20 correct prediction and updates weight for the other 100. Should I count this epoch 100 iterations or 120 iterations?
Note: This question is NOT about coding. The code cited above works well. This question is about terminology. The code is just to illustrate the scenario in question.
",['terminology'],"
A quick google the keyword ""definition of iteration in machine learning"" gives us a lot of results. I would like to stick with this StackOverFlow question.
As your example, if we have 100 samples, let me assume batch size is 20, so the number of iteration is 5. If there is one iteration that both 20 samples are predicted correctly, this epoch should be counted which means one epoch you still have 5 iterations since the number of iterations is important in some situations such as control the learning rate while training (decay or cycle).
If you feel uncomfortable with the non-gradient / non-updating on that iteration, you can understand as your model's weight is updated with the gradient is 0.
"
Can we change bias and control the output of neural network?,"
I have read the use of Targeted Adversarial Attacks for making the model perform better. But can we change the bias of the neural networks and control the outcome of the network rather than changing the input. if yes, can you share some resources or research papers on targeted bias in neural networks?
","['convolutional-neural-networks', 'bias']","
No, the bias is part of the network, not one of the inputs, so an adversary has no ability to manipulate it.
Unless the adversary can change the network, in which case they don't need to use any tricks, because they can just change it to one that outputs what they want!
"
What are the (key) purposes of unsqueezing operation on tensors?,"
The unsqeeze operation is used in several deep learning algorithms. However, I only found this operation in the code/implementation of the algorithms presented in the papers, which do not mention it.
The unsqueeze operation never modifies data in a tensor, and it only changes the positions of available data in the tensor. Wherever I see its use in coding till now, it is used before matrix multiplication only. So I am unaware of other uses of it.
Is unsqueeze operation only useful to handle compatibility issues i.e., to make the data compatible for underlying operation and has no other significance, or does it have (used for) any other purposes in deep learning?
","['deep-learning', 'implementation', 'computer-programming', 'tensor']","
Yes, reshaping operations aren't very theoretically interesting, they just make the data compatible with the following operations.
For example, if you have a 1D array of pixels, and you want to do a 2D convolution, you can (not with unsqueeze specifically) reshape that array into 2D so the 2D convolution code knows where the rows and columns are. You could write 2D convolution code that works on a 1D array of pixels, or you could just make it 2D and then use the normal 2D convolution code.
Same with unsqueeze. Perhaps you want to feed a 2D array into a convolution function that expects the last dimension to be channels. You can add a last dimension of 1 and now that code can see there's 1 channel. Or you want to pass one data sample through a function that takes batches. You can add a first dimension of 1 meaning there's only one item in the batch. Adding or removing dimensions of size 1 is free, since it doesn't change the data, only the interpretation of the data.
If you wanted to convert a greyscale image to RGB (but still grey) you might use unsqueeze followed by repeat_interleave to duplicate that 1 channel into 3.
It may be worth noting that Tensorflow has a very generic ""reshape"" operation which lets you convert any shape of tensor into any other shape of tensor, as long as the total number of elements is the same.
"
Is image generation not existent before generative adversarial networks?,"
Although the GAN is widely used due to its capability, there were generative models before the GAN which are based on probabilistic graphical models such as Bayesian networks, Markov networks, etc.
It is now a well-known fact that GANs are excelling at image generation tasks. But I am not sure whether the generative models that were invented before GANs were used for image generation or not.
Is it true that other generative models were used for image generation before the proposal of the GAN in 2014?
","['generative-adversarial-networks', 'generative-model', 'history', 'image-generation']",
Deep Q-Learning with multiple discrete actions,"
I am working on a DQN project with Pytorch, where I should choose multiple discrete actions, each in a range, say, (0, 15). I am wondering how I can model it, such that the sum of actions is 15. Does anyone know how to model that?
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'dqn', 'pytorch']","
As I understand it, you have a problem with a large action space - a vector of 10 integer variables. You also have a constraint on what valid actions should look like.
Even with the action vector discretised to integer amounts, there are millions of possible actions. This is beyond anything you can reasonably solve with value-based methods such as Q-learning. The problem is deriving the policy from the action value estimates. To select a greedy action, you need to find the action which maximises $\hat{q}(s,a, \theta)$, which in your case would mean either an insanely large output vector (covering all possible action combinations) or very large input batches to maximise over.
So, DQN is not really available to you as a method, before considering the constraint. What can you use instead? Any policy-gradient method or actor-critic method should work. These are more fiddly to understand and implement than value-based methods, although there are plenty of references for PyTorch, for example the PFRL library implements A3C, PPO, DDPG which would all be suitable as a start.
What a policy gradient method does for your problem is allow you to define a policy function $\pi(s, \theta)$ which will either output a single action or the parameters for an action probability distribution that you can sample from. The latter is actually more common, to create a neural network that outputs the parameters of a probability distribution. This allows for exploration in an on-policy approach. DDPG (Deep Deterministic Policy Gradients) is an example of a method that uses a deterministic policy function, but there is still an action sampling stage because DDPG adds a noise function to the policy in order to explore.
In your case, you could build a policy network that output a vector of 10 real values to repesent the means of the distribution, plus either 1 or 10 standard deviations if you are not using something like DDPG. Then the action choice could be sampled from the distribution that this defined.
This would not solve your other problem - a constraint on the sum of elements of the vector. You also have an implied constraint of a minimum value for each element.
For the constraint, I suggest you do not attempt to model it directly in the policy function, but instead define a fixed (no learnable parameters) mapping function from something that is easier to model in the neural network, to the constrained version. For instance, whatever action vector is output by sampling the neural network action, you could clip to minimum zero, then sum elements, divide by this sum and multiply by 15. Putting this function outside of the agent for training purposes - either part of the environment, or a ""helper"" - should make the maths and using the framework easier.
Optimising the raw, unconstrained policy function does mean you will have multiple equivalent policies once transformed, so makes it a less efficient search. However, this is offset by not needing to figure out valid probability distributions to sample from within the constrained space, or the gradients associated with the constraints.
Success of this approach will depend on a few details:

Choice of distribution function for selecting actions

Choice of mapping function to apply constraints

Feasibility of exploring the state and action space sufficiently to find near optimal behaviour


You have some influence over the first two issues - if you have some sense of what ""good"" actions will be in the problem then you can try to ensure that the representation covers them well. For example if you expect that the vector should have multiple zeroes, then you could ensure the sampled values can go below zero easily and that you use a clipping function so that you are likely to get a few zeroes.
The last issue is beyond your control. It is possible the problem is too hard to explore using RL. This may be the case when only very specific action values out of the many possible will give you good results. RL relies on getting some kind of reward signal to guide improvements. If there is a very large search space and only sparse rewards in specific circumstances, then the trial and error process may never find the optimal behaviours.
"
How do neural networks deal with inputs of different sizes that are padded in order to have them of the same size?,"
I am trying to create an environment for RL where the size of my input (observation space) is not fixed. As a way around it, I thought about padding the size to a maximum value and then assigning ""null"" to those values that do not exist. Now, these ""null"" values are meaningful in a certain sense, because they are related to the shape and size of the input.
If these ""null"" values were zeros, would neural networks be able to distinguish between these zeros (nulls) and the zeros that are actually part of the picture? If that's not the case, should I assign a different number for the padding? What should I be mindful of in these scenarios? Is there any example I can look at with a similar situation?
","['neural-networks', 'reinforcement-learning', 'observation-spaces', 'padding']",
Training and Evaluating BERT and XLNET [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I am thinking about a project and have a few questions before I accept it. Would be grateful I anyone experienced of you could give me some advice.
In the project, I have been given a data set with (rather small) 30.000 text documents, which are labeled with 0 and 1. I want to train and evaluate (with respect to accuracy) a BERT and XLNet model.
Can you give me some rough estimates for the following questions?:

How much computing power do I need for this task, i.e. can I simply use my private laptop for this or do I need a special CPU/GPU for it?
So far, I just worked with classical machine learning models (e.g. random forests, SVMs, etc.). I am not experienced deep learning architectures yet. How difficult would it be to implement a BERT oder XLNet model with my own data set, having no experience with BERT oder XLNet yet? I.e. how much code would it be that I have to develop by myself? And would I need a deep understanding for it or would be sufficient to follow an online tutorial and basically copy the code from there?
Many thanks.

","['deep-learning', 'natural-language-processing', 'bert', 'text-classification']","

You’ll want a reasonable GPU (probably 8GB+), but otherwise no special hardware needed.* You may need to tune down sequence length and batch size to fit your GPU; RAM will be the limiting factor. Don’t try it on a CPU. It will “work” but you’re gonna have a bad time.
Try the Huggingface Transformers library as your implementation. It’s well documented and straightforward and includes both models.

*assuming an Nvida GPU or something compatible with CUDA. Things are rather hairier on Apple hardware. But you can always grab a cloud VM for a few hours
"
Explaining AI to Non-Technical Individuals,"
How does one approach proposing AI to management? This is something I have struggled with for a long time. I want to implement AI toward a specific problem in my place of work. My supervisors are generally willing to listen; but they want to know how the algorithm(s) is going to work. They are not programmers. My tendency is to write out the math and step through it. However, most of them don't want to do that because they have a limited amount of time to sit there and listen. On top of that, some of these algorithms can get somewhat complex.
Lets take a simple neural network for example; how would you explain the way it works without diving into the math?
","['definitions', 'social', 'education']","
There are a lot of ways to describe ""Artificial Intelligence"".

An artifact that ""makes a decision""

This form of automation/computing/AI goes back to neolithic times.
Early AI was purely heuristic.  (Also known as ""good old fashioned AI"" aka ""Symbolic Intelligence"" aka classical expert systems.)
The current generation of strong (narrow) AI is statistical, which encompasses both neural networks and evolutionary/genetic algorithms.

Artificial intelligence is a machine that makes a decision.  Modern statistical methods allow these machines to learn and improve their decisions.

Current best AI is ""narrowly superintelligent"" in that it can exceed humans at most definable tasks, but machines still lack the intuitivity of biological brains, and this strong intelligence is narrow—restricted to single problems or classes of problems.
"
Why do we lose detail of an image as we go deeper into a ConvNet?,"
I was reading this research paper titled 'Image Style Transfer using Convolutional Neural Networks' which as the title suggests was based on Neural Style Transfer. I came across this line which didn't make immediate sense to me.

Here's how it went -

We reconstruct the input image from from layers ‘conv1 2’ (a), ‘conv2 2’ (b), ‘conv3 2’ (c), ‘conv4 2’ (d) and ‘conv5 2’ (e) of the original VGG-Network. We find that reconstruction from lower layers is almost perfect (a–c). In higher layers of the network, detailed pixel information is lost while the high-level content of the image is preserved (d,e).

The line that is italicised; Why does that happen?
","['convolutional-neural-networks', 'convolution', 'convolutional-layers']","
The point of a convnet, or many kinds of neural networks in general, is to go from a lot of data down to a small piece of data. In classification tasks, for example, the input is all the pixels that make up a picture of a house, and the output is just the word ""house"" (or rather a number representing the word ""house"").
Obviously this process loses information. If you go from the word ""house"", back to a picture (i.e. you tell it ""draw a house"") you're probably going to get a completely different house!
In the style transfer task we have many numbers to describe the house picture with, not just the word ""house"", but we still have less than the full pixel data. Imagine that the intermediate representation represents something like ""yellow wooden house with three windows and one window above and a red brick basement with 4 windows and the house is drawn at a 30 degree angle and there's a pink house to the left with two small windows below and one window above and a red roof is visible behind and above the pink house and .....""
If you had that representation, you could try to draw the original picture again, and the more information you have, the more accurately you can draw it. Early layers of the convnet contain information like ""there's a vertical line at pixel coordinates 123,456"" and later layers contain information like ""there's a yellow house at pixel coordinates 123,456"", and if you get to dense layers, they may just say ""there's a yellow house"".
"
Are calculus and differential geometry required for building neural networks?,"
I've been studying geometry and linear algebra for months with the goal to build neural networks. But now I'm reading that perceptrons require fitting curves, and curves are not expressed as linear functions. So, I might need to study differential geometry and calculus for building good fitting curves in perceptrons.
I already know how to code and was hoping to get my hands dirty by coding a few neural networks. But should I study calculus and differential geometry before coding?
From this video, I understand that the least squares approximation can be used to fit a curve through a set of points, so maybe linear algebra is enough for building good neural networks?
","['neural-networks', 'calculus', 'education']","
To give some practical advice, it is important to understand parts of calculus. This is mainly because Backpropagation is a leaky abstraction in modern libraries. In a nutshell, there is a lot  which can go wrong (exploding or vanishing gradient for example) and you will need knowledge about gradient descent to handle it.
I highly recommend Andrej Karpathys Lecture on it. He gives a easy to understand and intuitive explanation.
"
Why many deep learning research papers continue to be in arXiv?,"
There are plenty of research papers, especially in deep learning are present only in arXiv with large number of citations. I cannot find them in journals as peer-reviewed ones.
For example if I search for Conditional Generative Adversarial Nets then I can find only an arXiv pre-print and has been Cited by 5722
This is not the single paper and I personally found lot of papers in pre-print only with no journal/conference affiliation. Many research papers are at-least 3 years old.
Is it solely due to the will of authors or is there any other reason for this phenomenon of not getting published even though they are widely accepted especially in the domain of deep learning?
","['research', 'academia']",
Is there any difference between an objective function and a value function?,"
I found the usage of both objective function and value function in the same context.
Context #1: In the paper titled Generative Adversarial Nets by Ian J. Goodfellow et al.

We simultaneously train G to minimize $\log(1 −D(G(z)))$. In other
words, $D$ and $G$ play the following two-player minimax game with
value function $V (G,D)$:
$$\min_G \max_DV(D, G) = \mathbb{E}_{x ∼ P_{data}}[\log D(x)] + 
 \mathbb{E}_{z ∼ p_z}[log (1 - D(G(z)))]$$

Context #2: In the paper titled Conditional Generative Adversarial Nets by Mehdi Mirza et al.

The objective function of a two-player minimax game would be as
$$\min_G \max_DV(D, G) = \mathbb{E}_{x ∼ P_{data}}[\log D(x|y)] + 
 \mathbb{E}_{z ∼ p_z}[log (1 - D(G(z|y)))]$$

In fact, the second paper also iterated context #1 i.e., used the term ""value function"" at another place.
We can observe that objective function is a function which we want to optimize

The objective function is the most general term that can be used to
refer to a cost (or loss) function, to a utility function, or to a
fitness function, so, depending on the problem, you either want to
minimize or maximize the objective function. The term objective is a
synonym for goal.

Since the generator or discriminator has to perform optimization, it is agreeable to use the term objective function in this context.
But what is the definition for the value function and how is it different from the objective function in this context?
","['comparison', 'terminology', 'objective-functions', 'generative-adversarial-networks', 'value-functions']","
The value function may be used in the GAN paper because GANs are inspired by game theory, where terms like utility, utility function and value function (just like in reinforcement learning) are used (the first two for sure, but I am not sure about the usage of the term value function in game theory, as I am far from an expert in game theory).  If you want to know more about the usage of the term value function, this Wikipedia article could be useful (or maybe make things more confusing).
Having said that, it seems to me that the usage of the term ""objective function"" in the conditional GAN paper is a bit sloppy. They probably meant the optimization problem.
However, it's also true that the notation used by the original authors of the GAN can also be confusing. They wrote
$$\min_G \max_DV(D, G) = \mathbb{E}_{x ∼ P_{data}}[\log D(x)] + 
 \mathbb{E}_{z ∼ p_z}[log (1 - D(G(z)))] \label{1}\tag{1}$$
Here, $V(D, G) = \mathbb{E}_{x ∼ P_{data}}[\log D(x)] + 
 \mathbb{E}_{z ∼ p_z}[log (1 - D(G(z)))]$, so they could have written \ref{1} as follows
$$\min_G \max_D V(D, G) = \min_G \max_D\mathbb{E}_{x ∼ P_{data}}[\log D(x)] + 
 \mathbb{E}_{z ∼ p_z}[log (1 - D(G(z)))] \label{2}\tag{2}$$
or just
$$\min_G \max_D V(D, G) \label{3}\tag{3}$$
Then clarify that $V(D, G) = \mathbb{E}_{x ∼ P_{data}}[\log D(x)] + 
 \mathbb{E}_{z ∼ p_z}[log (1 - D(G(z)))]$.
This is clarified in this paper (equations 2.1 and 2.2., page 5).
So, in the GAN, we're optimizing $V$, so $V$ is the objective function; thus, in this case, the term ""value function"" is a synonym for ""objective function"". In this case, the optimization problem is a $\color{blue}{\textrm{min}}$$\color{red}{\textrm{max}}$ game, i.e. we $\color{red}{\textrm{maximize}}$ and $\color{blue}{\textrm{minimize}}$ at the same time two terms of the objective function, i.e. $\color{red}{\mathbb{E}_{x ∼ P_{data}}[\log D(x)]}$ and $\color{blue}{\mathbb{E}_{z ∼ p_z}[log (1 - D(G(z)))]}$ (this is explained in the GAN paper!). In practice, they optimize two slightly different objectives, but they are equivalent. See algorithm 1 in the GAN paper.
So, as I said in my other answer, the objective function is the function that you want to optimize (i.e. minimize or maximize), so it's usually a synonym for loss/cost/error function (in case you want to minimize it) and can be a synonym for value function (in case you want to maximize it, for example, in reinforcement learning), as it seems to be the case in the GAN (although, in the GAN, you maximize and minimize the value function).
"
Train separate AutoEncoder's on each class or one AE for all classes to learn features?,"
I'm working on a project where the dataset contains time series of three classes, depending on the shape of the series. I want to learn the representations of these series as vectors, so naturally I use AutoEncoder for the task (precisely, I use LSTM-AutoEncoder to better handle the sequential data).
My question is: should I train one model for all classes or one model for each class? If possible, could you also point out what are the pros and cons of each approach? One thing that worries me about the latter approach is that the AE will simply memorize the data without any learning (again, would that be a concern?)
Thank you very much in advance!
Sincerely,
","['neural-networks', 'deep-learning', 'autoencoders', 'time-series', 'representation-learning']",
Is there any difference between 'input' and 'conditional input' in the case of neural networks?,"
In the research paper titled Conditional Generative Adversarial Nets by Mehdi Mirza and Simon Osindero, there is a notion of conditioning a neural network on a class label.
It is mentioned in the abstract that we need to simply feed extra input $y$ to the generator and discriminator of an unconditional GAN.

Generative Adversarial Nets were recently introduced as a novel
way to train generative models. In this work we introduce the
conditional version of generative adversarial nets, which can be
constructed by simply feeding the data, $y$, we wish to condition on to
both the generator and discriminator. We show that this model can
generate MNIST digits conditioned on class labels. We also illustrate
how this model could be used to learn a multi-modal model, and provide
preliminary examples of an application to image tagging in which we
demonstrate how this approach can generate descriptive tags which are
not part of training labels.

So, I cannot see whether there is any special treatment for input $y$.
If there is no special treatment for the data $y$, then why do they call $y$ a condition and follow the notation of conditional probability such as $G(z|y), D(x|y)$ instead of $G(z,y), D(x,y)$?
If there is a special treatment to input $y$, then what is that special? Don't they pass $y$ in the same way as $x$ to the neural networks?
","['generative-adversarial-networks', 'notation', 'input-layer', 'conditional-probability', 'conditional-gan']",
Does generator in conditonal GAN obey probability laws?,"
In probability, we have two types of probability functions: unconditional probability $p(x)$ and conditional probability $p(x | y)$. Both are fundamentally different and the latter can be obtained by the following equation
$$p(x|y) = \dfrac{p(x, y)}{p(y)} \text{          provided   } p(y) \ne 0$$
I never heard of formal definition for conditioning except for conditional probability function.
But in case of neural networks, I came across the notion of conditioning.
$$\min_G \max_DV(D, G) = \mathbb{E}_{x ∼ P_{data}}[\log D(x|y)] +  \mathbb{E}_{z ∼ p_z}[log (1 - D(G(z|y)))]$$
Since neural network $D$ is intended to implement a probability function, we can at-least think about conditioning on an input. But the neural network $G$ is not intended to implement probability function. $G$ is intended to provide datasamples by learning an underlying probability distribution whose output is not in the range $[0, 1]$.
Does $G$ obey the laws of probability? If yes, how, since its output is not restricted to $[0, 1]$?  If no, then why the authors use the notation of conditional probability for $G$ also?
","['generative-adversarial-networks', 'probability', 'notation']","
I don't see where it's implied that G is a probability distribution. G is a function, whose output conditioned on one variable has a probability distribution, but it isn't one.
z is random noise which is G's source of randomness. y is something that isn't random. We call G over and over with the same y and different random z's and look at the distribution of the output.
For example, here are some outputs of one possible G(z,y), for different z, when y=3. z is not displayed, only G(z,y). This G outputs one digit.
2262662626626262626262626626262262622226626622626662626666262626262626262
These digits have a probability distribution. About half of them are 2 and about half of them are 6. P(G(z)=2|y=3) = 0.479 and P(G(z)=6|y=3) = 0.521 (approximately). Even though neither 2 nor 6 is a valid probability.
"
How does one handle different player turns in MCTS?,"
Suppose we have a two player game like Tic Tac Toe where the two players take turns to play their moves. It is my understanding that in the game tree that MCTS builds, consecutive levels in the tree correspond to different player's turns.
So, for instance, in the root node it is Player1's turn to play, in the children of the root node it is Player2's turn to play, in the children of those children it is Player1's turn again, etc.
Is that correct?
If so, is it really prudent to treat nodes where it's the enemy's turn to play the same as those where we choose the next action (i.e. by averaging rollout results in backpropagation). Since, it's not us choosing the next action but the enemy, shouldn't we ""pick"" the minimum ""return"" (like in minimax) in those cases instead of the average like we do for nodes where we get to pick the next action?
By picking I mean to only count the win ratio of that child node (i.e. the minimum win ratio).
I suspect I am missing something (e.g. that might mess up exploration vs exploitation with UCT) but I can't put my finger on it.
What do you guys think about this?
Edit: Maybe a solution to this is only considering good moves for the opponent? But then again.. how do we define good? Heuristics?
","['game-ai', 'monte-carlo-tree-search', 'minimax']","
The original (vanilla) MCTS use random rollouts.  In some games this is enough to produce a strong agent.  However, in most of the games, using a heuristic that finds the opponent's likely moves makes stronger agents.  There is another line of practice that uses Opponent Modeling to predict the opponent moves.  That is important in games where you have several opponent ""types"" or when an opponent can go for different goals.
From my experience, a good heuristic can greatly improve the agent.  I have implemented UCT agents for Spades (the card game).  I made a vanilla UCT and one that uses a different (simpler) agent as heuristic.  The second UCT is stronger.
Picture from wiki:MCTS

The four phases of MCTS:

Selection: Start from root R and select successive child nodes until a leaf node L is reached. The root is the current game state and a leaf is any node that has a potential child from which no simulation (playout) has yet been initiated. The section below says more about a way of biasing choice of child nodes that lets the game tree expand towards the most promising moves, which is the essence of Monte Carlo tree search.


Expansion: Unless L ends the game decisively (e.g. win/loss/draw) for either player, create one (or more) child nodes and choose node C from one of them. Child nodes are any valid moves from the game position defined by L.


Simulation: Complete one random playout from node C. This step is sometimes also called playout or rollout. A playout may be as simple as choosing uniform random moves until the game is decided (for example in chess, the game is won, lost, or drawn).


Backpropagation: Use the result of the playout to update information in the nodes on the path from C to R.

"
Do researchers generally treat tensors just as mathematical objects with certain shape?,"
Most of the practical research in AI that includes neural networks deals with higher dimensional tensors. It is easy to imagine tensors up to three dimensions.
When I ask the question How do researchers imagine vector space? on Mathematics Stack exchange, you can read the responses

Response #1:
I personally view vector spaces as just another kind of algebraic
object that we sometimes do analysis with, along the lines of
groups, rings, and fields.
Response #2
In research mathematics, linear algebra is used mostly as a
fundamental tool, often in settings where there is no geometric
visualization available. In those settings, it is used in the same
way that basic algebra is, to do straightforward calculations.
Response #3:
Thinking of vectors as tuples or arrows or points and arrows... is
rather limiting. I generally do not bother imagining anything visual
or specific about them beyond what is required by the definition...
they are objects that I can add to one another and that I can
""stretch"" and ""reverse"" by multiplying by a scalar from the scalar
field.

In concise, mathematicians generally treat vectors as objects in vector space rather than popular academic/beginner imaginations such as points or arrows in space.
A similar question on our site also recommends not to imagine higher dimensions and to treat dimensions as degrees of freedom.
I know only two kinds of treatments regarding tensors:

Imagining at most up to three-dimensional tensors spatially.

Treating tensors as objects having shape attribute which looks like $n_1 \times n_2 \times n_3 \times \cdots n_d$


Most of the time I prefer the first approach. But I am feeling difficulty with the first approach when I try to understand codes (programs) that use higher dimensional tensors. I am not habituated with the second approach although I think it is capable enough to understand all the required tasks on tensors.
I want to know:

How do researchers generally treat tensors?
If it is the second approach I mentioned: Is it possible to understand all the high dimensional tensor-related tasks?

","['research', 'data-visualization', 'tensor']","
I would say they are treated as multidimensional arrays of numbers. They are not visualized in their actual dimension. Sometimes small ones will be visualized when someone is trying to explain a concept that requires it.
You may have, for example, a variable uint8 training_batch[100][200][400][3];. This is a batch of 100 RGB images with 200x400 pixels in each image. A pixel is an array of [3] numbers; an image is an array of [200][400] pixels; a batch is an array of [100] images. There's no more structure than that. You don't have to try to imagine a 4D array of numbers. (In this particular case you could easily imagine an array of images though)
What is useful to imagine is what each dimension means. The first dimension is the image within the batch. The 2nd and 3rd dimensions are the pixel position in the image. The 4th dimension is the R/G/B channel.
If I reduce a tensor along a dimension, I wouldn't think of it as flattening, but rather as using up a dimension. If I want to compute the average colour of each image, I reduce the 2nd and 3rd dimensions and get another tensor of shape [100][3]. Now there's no width or height dimension anymore, just image and channel.
If you reshape the vector to [100][240000] so you can compute a matrix multiplication for a dense layer, now the 1st dimension is still the batch number, and the 2nd dimension is essentially meaningless but you have 240000 arbitrarily-indexed numbers per image. You could also reshape it to [100][80000][3] and have 80000 arbitrarily-indexed pixels, but still, be able to use the channel number.
Disclaimer: I'm not actually a researcher.
"
"If we can model the environment, wouldn't be meaningless to use a model-free algorithm?","
I am trying to understand the concept of model-free and model-based approaches. As far as I understand, having a model of the environment does not mean that an RL agent has to be model-based. It is about the policy. However, if we can model the environment, why should we want to employ a model-free algorithm? Isn't it better to have a model and expectation about the next reward and state? If you have a better understanding of all these, can you explain them to me as well?
","['reinforcement-learning', 'ai-design', 'model-based-methods', 'model-free-methods']",
Is there any difference between conditional batch normalization and batch normalization except the usage of MLPs for predicting $\beta$ and $\gamma$?,"
Batch normalization in neural networks uses $\beta$ and $\gamma$ for scaling. The analytical formula is given by
$$\dfrac{x - \mathbb{E}[x]}{\sqrt{Var(X)}}* \gamma + \beta$$
Conditional batch normalization uses multi-layer perceptrons to calculate the values of $\gamma$ and $\beta$ instead of giving fixed values to them.
Is it only the difference between them or is there any other fundamental difference between them in terms of functionality?
","['comparison', 'batch-normalization']",
Example for batch normalization in an aritifical neural network,"
Suppose the following is the neural network I want to train and assume that there is a batch normalization layer for each layer of the neural network
.
My focus is on the activity of batch normalization layer of the hidden layer. Assume that mini-batch size is $3$. The following are the first four outputs of the hidden layer without using batch normalization layer.
$$\left(\begin{array}{c} 4 \\ 5 \\ 2 \\ 1 \\ \end{array}\right), \left(\begin{array}{c} 3 \\ 4 \\ 6 \\ 0 \\ \end{array}\right), \left(\begin{array}{c} 1 \\ 4 \\ 7 \\ 9 \\ \end{array}\right), \left(\begin{array}{c} 3 \\ 5 \\ 7 \\ 9 \\ \end{array}\right)$$
So the first mini-batch is$\left(\begin{array}{c} 4 \\ 5 \\ 2 \\ 1 \\ \end{array}\right), \left(\begin{array}{c} 3 \\ 4 \\ 6 \\ 0 \\ \end{array}\right), \left(\begin{array}{c} 1 \\ 4 \\ 7 \\ 9 \\ \end{array}\right)$ and has the following statistics
mean = $\mathbb{E}[X] = \left(\begin{array}{c} \dfrac{8}{3} \\ \dfrac{13}{3} \\ 5 \\ \dfrac{10}{3} \\ \end{array}\right), Var(X) = \left(\begin{array}{c} \dfrac{14}{9} \\ \dfrac{2}{9} \\ \dfrac{14}{3} \\ \dfrac{146}{9} \\ \end{array}\right)$
So, I am thinking that the batch normalization can be applied during from fourth iteration since mini-batch of outputs of hidden layer are available and so fourth vector  $\left(\begin{array}{c} 3 \\ 5 \\ 7 \\ 9 \\ \end{array}\right)$ will be mapped to $\left(\begin{array}{c} 0.26 \\ 1.41 \\ 0.92 \\ 1.4 \\ \end{array}\right)$
Is it true that if we apply batch normalization the outputs of neural network are
$$\left(\begin{array}{c} 4 \\ 5 \\ 2 \\ 1 \\ \end{array}\right), \left(\begin{array}{c} 3 \\ 4 \\ 6 \\ 0 \\ \end{array}\right), \left(\begin{array}{c} 1 \\ 4 \\ 7 \\ 9 \\ \end{array}\right),\left(\begin{array}{c} 0.26 \\ 1.41 \\ 0.92 \\ 1.4 \\ \end{array}\right)$$
I am thinking that I may be totally wrong because the batch normalization layer is not active till the completion of first mini-batch and also using the stats of first mini batch on the outputs of next mini-batch. If wrong, I want to know what is wrong or a simple numerical example by at-least running the batch normalization layer of neural network for mini-batch + 1 number of iterations.
",['batch-normalization'],
Is this calculation of the vector-Jacobian product in the PyTorch documention wrong?,"
In the official PyTorch documentation there is the following calculation (here):
$$
J^{T} \cdot \vec{v}=\left(\begin{array}{ccc}
\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{1}} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_{1}}{\partial x_{n}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}
\end{array}\right)\left(\begin{array}{c}
\frac{\partial l}{\partial y_{1}} \\
\vdots \\
\frac{\partial l}{\partial y_{m}}
\end{array}\right)=\left(\begin{array}{c}
\frac{\partial l}{\partial x_{1}} \\
\vdots \\
\frac{\partial l}{\partial x_{n}}
\end{array}\right)
$$
However, I am wondering why the result isn't as follows:
$$
J^{T} \cdot \vec{v}=\left(\begin{array}{ccc}
\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{1}} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_{1}}{\partial x_{n}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}
\end{array}\right)\left(\begin{array}{c}
\frac{\partial l}{\partial y_{1}} \\
\vdots \\
\frac{\partial l}{\partial y_{m}}
\end{array}\right)=\left(\begin{array}{c}
m\frac{\partial l}{\partial x_{1}} \\
\vdots \\
m\frac{\partial l}{\partial x_{n}}
\end{array}\right)
$$
As the matrix is multiplied by a vector and the $\partial y_{x}$ terms cancel out it is should be $m$ times $\frac{\partial l}{\partial x_{x}}$.
I know that the official PyTorch docs are probably right but I just can't get my head around why.
","['pytorch', 'vectors']",
What is numerical stability?,"
I came across the phrase ""numerical stability"" several times. But almost in the same context.
I encountered this word mostly in the analytical formula for batch normalization.

$$y = \dfrac{x - \mathbb{E}[x]}{\sqrt{Var[x]+\epsilon}}* \gamma +
 \beta$$
eps – a value added to the denominator for numerical stability. Default: 1e-5

Is the phenomenon of ""numerical instability"" happens during the training of neural networks? Or is it a general one in other models also? What is the reason for its occurrence?
","['neural-networks', 'machine-learning', 'terminology', 'numerical-algorithms']",
"Aside from dice score, what other good metrics are used to evaluate segmentation models?","
I have a segmentation which outputs only one channel image (2 class segmentation). I have used dice score for most of the time, but now higher powers in my team want me to expand evaluation metrics for segmentation model (if it's even possible). I have done some research and as far as right now I have found mainly that everybody uses dice score, and sometimes pixel to pixel binary accuracy, but for the latter seems not the best idea.
If anybody knows something exciting or useful, I'd be glad to hear from them.
","['deep-learning', 'image-segmentation', 'metric']","
I agree fully with @a crazy Minon's answer.  I will just slightly expand on it and provide a couple of additional references.
While Dice is a popular metric for evaluating segmentation, it is certainly not the only one.  You are right in thinking that pixel accuracy is a poor choice of evaluation metric.  The main issue is that it performs poorly when when there is class imbalance, which is often the case in imaging data.
I will add that Intersection over Unions (IoU) is another metric that is frequently employed to evaluate segmentation performance.  It is also known as the Jaccard Index.  The articles ""Metrics to Evaluate your Semantic Segmentation Model"" and ""All the segmentation metrics!"" provide good simplified introductions to various commonly used segmentation metrics.  While Dice and IoU are similar and are positively correlated, they are not equivalent, as explained by this StackOverflow answer.  New metrics are also being developed--such as the Boundary Jaccard--to overcome limitations of current metrics, and comparisons of these metrics have been published for specific applications (see example ref, which lists 33 evaluation metrics for segmentation in Table 1).
Finally, if your interest is really for one class, then accuracy, sensitivity, and specificity for segmenting that class alone can be useful metrics.
The powers that be at your institution are wise in asking for multiple evaluation metrics because each metric has its limitations and no single metric can fully capture the performance of a segmentation network.
"
Data analysis before feeding to ML pipeline,"
I'm new to machine learning and I've been working through a dataset of ~3000 records with ~100 features. I've been hand rolling Python and R scripts to analyse the data. For example, plotting the distribution of each feature to see how normal it is, identify outliers, etc. Another example is plotting heatmaps of the features against themselves to identify strong correlations.
Whilst this has been a useful learning exercise, going forward I suspect there are tools that automate a lot of this data analysis for you, and produce the useful plots and possibly give recommendations on transforms, etc? I've had a search around but don't appear to be finding anything, I guess I'm not using the right terminology to find what I'm looking for.
If any useful open source tools for this kind of thing spring to mind that would be very helpful.
","['machine-learning', 'python', 'datasets', 'data-preprocessing', 'r']",
"Attention mechanism: Why apply multiple different transformations to obtain query, key, value","
I have two questions about the structure of attention modules:
Since I work with imagery I will be talking about using convolutions on feature maps in order to obtain attention maps.

If we have a set of feature maps with dimensions [B, C, H, W] (batch, channel, height, width), why do we transform our feature maps before we calculate their affinity/correlation in attention mechanisms? What makes this better than simply taking the cosine distance between the feature vectors (e.g. resizing the maps to [B, C, HW] and [B, HW, C] and multiplying them together). Aren't the feature maps already in an appropriate feature/embedding space that we can just use them directly instead of transforming them first?

Most of the time, attention mechanisms will take as input some stack of feature maps (F), and will apply 3 transformations on them to essentially produce a ""query"", ""key"" and ""value"". The query and key will be multiplied together to get the affinity/correlation between a given feature vector and all other feature vectors. In computer vision these transformation will typically be performed by the different 1x1 convolutions. My question is, how come we use 3 different 1x1 convolutions? Wouldn't it make more sense to apply the same 1x1 convolution to the input F? My intuition tells me that since we want to transform/project the feature maps F into some embedding/feature space that it would make the most sense if the ""query"", ""key"" and ""value"" were all obtained by using the same transformation. To illustrate what I mean lets pretend we had a 1x1 feature map and we wanted to see how well the pixel correlates with itself. Obviously it should correlate 100% because it is the same pixel. But wouldn't applying two sets of 1x1 convs to the pixel lead to the chance that the pixel would undergo a different transformation and in the end would have a lower correlation than it should?


","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'attention']","
I assume you're talking about this design: (image source)


But wouldn't applying two sets of 1x1 convs to the pixel lead to the chance that the pixel would undergo a different transformation and in the end would have a lower correlation than it should?

Yes, that's the point. We are not trying to measure a pixel's correlation with itself. Rather we are trying to allow it to query different related data. We are giving it freedom to change both the data and the queries.
It is true that the space for the queries and the keys is the same - but we shouldn't use the same transformation for both, or else each instance of the attention layer is just trying to fetch its own value! Generally the purpose of an attention layer is to query different parts of the input.
The first half of your question was essentially ""why do we have a convolution at all?"" and I think this has the same answer: you'd just be able to detect similar pixels, you wouldn't be able to pay attention to noses whenever eyes are detected.
It is also true that you could probably skip the convolution on the h(x) input. It looks like this one is somewhat redundant because the convolutions on h(x) and v(x) apply in series - which makes it a two-layer convolution, not quite the same as a one-layer convolution, but perhaps only one layer is needed.
It is possible that if you removed the conv layer on either the keys or the queries (but not both) the model would learn to generate the keys directly as the features, but this would hinder it because it would be unable to output any data in the values and queries that wasn't part of the keys (or vice versa). Seems silly. Don't do that.
"
What does it mean by strong or sufficient gradient for training in this context?,"
It has been mentioned in the research paper titled Generative Adversarial Nets that generator need to maximize the function $\log D(G(z))$ instead of minimizing  $\log(1 −D(G(z)))$ since the former provides sufficient gradient than latter.

$$\min_G \max_DV(D, G) = \mathbb{E}_{x ∼ P_{data}}[\log D(x)] + 
 \mathbb{E}_{z ∼ p_z}[log (1 - D(z))]$$
In practice, the above equation may not provide sufficient gradient
for $G$ to learn well. Early in learning, when $G$ is poor, $D$ can
reject samples with high confidence because they are clearly different
from the training data. In this case, $\log(1 −D(G(z)))$ saturates.
Rather than training $G$ to minimize $\log(1 −D(G(z)))$ we can train G
to maximize $\log D(G(z))$. This objective function results in the same
fixed point of the dynamics of $G$ and $D$ but provides much stronger
gradients early in learning.

A gradient is a vector containing the partial derivatives of outputs w.r.t inputs. At a particular point, the gradient is a vector of real numbers. These gradients are useful in the training phase by providing direction-related information and the magnitude of step in the opposite direction. This is my understanding regarding gradients.
What is meant by sufficient or strong gradient? Is it the norm of the gradient or some other measure on the gradient vector?
If possible, please show an example of strong and weak gradients with numbers so that I can quickly understand.
","['training', 'terminology', 'papers', 'generative-adversarial-networks', 'gradient']","
The terms ""insufficient gradient"" or ""not strong enough gradient"" usually means that the magnitude of the gradient vector is too small or nearly zero that they can't drive the optimization properly.
Not having sufficient gradient is similar to having a very low learning rate - they are not only slow (in terms of convergence) but also drifts the optimization in a poor direction and get stuck in local minima.
"
"Is ""kernel"" different from ""filter"" in convolutional neural networks?","
Recently I asked a question on how a convolution 2d layer changes an RGB image into a grayscale image. Assume that our task is to convert an RGB image into a grayscale image. I use to believe that filter and kernel are one and the same.
Consider Conv2d in PyTorch.
class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)

The parameters in_channel, out_channel and kernel_size are key to our discussion here.
I have no doubt about in_channel. It simply says the number of channels in the input image. It is 3 for our task since we have RGB images as input.
The doubt is regarding the parameters out_channel and kernel_size. out_channel refers to the number of channels in the output image. It is 1 for our task since we want grayscale images as output. It is also equal to the number of filters we are needing. So, we just use one filter to convert an RGB image into a grayscale image. kernel_size is the size of the kernel which is showing $3 \times 3$ in our case. Now, my convolution layer is
>>> in_ch = 3
>>> out_ch = 1
>>> m = nn.Conv2d(in_ch, out_ch, 3, 1, 1)
>>> print(m)
Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

Since I have doubt about the conversion of RGB image to grayscale image using a single filter and whose size is showing $3 \times 3$, I checked the shape of weights in the filter and realized that the single filter is a 3-dimensional filter of size $ 3 \times 3 \times 3$
>>> print(m.weight.shape)
torch.Size([1, 3, 3, 3])

Now, the filter size is $3 \times 3 \times 3$ and kernel_size is $3 \times 3$.
So, can I safely conclude that the filter is different from the kernel? Can I conclude that kernel is just a part of filter and filter may comprise several kernels? Or is it true that the usage in PyTorch is a bit misleading since I found that our site is also using the same tag for both filter and kernel?
","['comparison', 'terminology', 'pytorch', 'filters', 'convolutional-layers']",
Why do we add 1 in the formula to calculate the shape of the output of the convolution?,"
In the formula to calculate output shape of tensor after convolution operation
$$
W_2 = (W_1-F+2P)/S + 1,
$$
where:

$W_2$ is the output shape of the tensor
$W_1$ is the input shape
$F$ is the filter size
$P$ is the padding
$S$ is the stride.

Why do we add $1$? It gets us to the correct answer, but how is this formula derived?
Source: https://cs231n.github.io/convolutional-networks/#pool
","['convolutional-neural-networks', 'convolution', 'convolutional-layers', 'convolution-arithmetic']",
Confusion about conversion of RGB image to grayscale image using a convolutional layer with 2-dimensional filters,"
Let us imagine $x$ as a tensor containing 1000 RGB images, each of size $64 \times 32$.
>>> x = torch.randn(1000, 3, 64, 32)
>>> print(x.shape)
torch.Size([1000, 3, 64, 32])

I am using a 2d convolutional layer that converts RGB images to single channel (say grayscale) images
>>> in_ch = 3
>>> out_ch = 1
>>> m = nn.Conv2d(in_ch, out_ch, 3, 1, 1)
>>> print(m)
Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

I passed the tensor $x$ in the convolutional layer and obtained another tensor of 1000 grayscale images, each of size $64 \times 32$.
>>> output = m(x)
>>> print(output.shape)
torch.Size([1000, 1, 64, 32])

Now, I can say that my convolutional layer converted an RGB image into a grayscale image using 2d kernel.
How it is doing?
RGB image has 3 planes each of size $64 \times 32$. If a kernel of 2 dimensions is used, then we will get 3 planes in output, corresponding to R, G, and B. How is it possible to convert an image with 3 channels into an image with one channel using 2d kernel?
I can visualize easily if I use a 3d kernel since the kernel considers three channels simultaneously and produces a single feature map for an RGB image.
",['convolutional-layers'],"
Yes, the kernel is 3D in this case - or 4D as in 3x3x3x1. In the general case you can have multiple output channels, making it 3x3x3x8 for example. The number of channels isn't a convolution dimension because the filter does not ""slide""/""translate""/""move"" over this dimension. It's still a 2D convolution, and then the channel part of this operation is thought of separately from the convolution part. The 4D kernel is a bunch of 2D kernels. If you have 1 input channel and 1 output channel then it's just one 2D kernel. Or you can think of it as a bunch of 3D kernels if you like. Or a single 4D kernel. These are just arrays of numbers... you can slice an array up however you like, if you can find a way to think about it.

Note the groups parameter of Conv2d, which affects how the channels are convolved. The default is 1, which means:

At groups=1, all inputs are convolved to all outputs.

If you set it to 3 (and 3 output channels) then the Conv2d layer would maintain the channel separation.
"
How can I weight each point in one-class SVM?,"
I want to give weights to some data points
Specifically, these are points related to anomalies
(I'm implementing one-class SVM for anomaly detection)
Exactly, I want to consider some data points that are likely to be anomalies as more important data points
Is it possible in one-class SVM ?
","['machine-learning', 'classification', 'support-vector-machine', 'binary-classification', 'anomaly-detection']",
Is there a systematic way of conducting deep learning experiments?,"
I have been working on a computer vision problem with the use of cnns, but quite frustratingly I'm often in the situation of not knowing what to do to improve my results. It seems to me that most of the time I am mostly making random changes and experimenting in hope that this change will bring some improvement.
I notice how this is different from non-AI software development where debugging can be performed by trying to pinpoint where exactly in the code lies an unexpected behaviour.
I wonder if there is a technique that could better orient the research effort.
","['deep-learning', 'convolutional-neural-networks']",
Does average loss function in GAN training is just an approximation of value function and does not ensure convergence of generator and discriminator?,"
The value function on which convergence has been proved by the original paper of GAN is
$$\min_G \max_DV(D, G) = \mathbb{E}_{x ∼ P_{data}}[\log D(x)] +  \mathbb{E}_{z ∼ p_z}[log (1 - D(G(z)))]$$
and the loss function used in training are
$$\max L(D) =  \frac{1}{m} \sum_{i=1}^{m}\left[\log D\left(\boldsymbol{x}^{(i)}\right)+\log \left(1-D\left(G\left(\boldsymbol{z}^{(i)}\right)\right)\right)\right]$$
$$\min L(G) =  \frac{1}{m} \sum_{i=1}^{m}\left[\log \left(1-D\left(G\left(\boldsymbol{z}^{(i)}\right)\right)\right)\right]$$
where $\{z^{(1)}, z^{(2)}, z^{(3)}, \cdots, z^{(m)}\}$ and  $\{x^{(1)}, x^{(2)}, x^{(3)}, \cdots, x^{(m)}\}$  ate the noise samples and data samples for a mini-batch respectively.
I found after analyzing some questions 1, 2 on our main site that the loss function used for training is just an approximation of the value function and are not same in formal sense.
Is it true? If yes, what is the reason behind the disparity? Is the loss function used for implementation also ensures convergence?
","['comparison', 'training', 'objective-functions', 'generative-adversarial-networks']","
Expected value can be thought of as a weighted average of outcomes. Thus, expectation and mean are the same thing, if each outcome has the same probability (which is $\frac{1}{m}$), so we can replace it with a sum divided by $m$. We can rewrite the equation:
$$\min_G \max_DV(D, G) = \mathbb{E}_{x ∼ P_{data}}[\log D(x)] +  \mathbb{E}_{z ∼ p_z}[log (1 - D(G(z)))]$$
First, we sample minibatch of size $m$ for $\boldsymbol{x} \sim P_{data}$ and $\boldsymbol{z} \sim \mathcal{N(0, 1)}$. Now we can replace the expectation with the sum:
$$
\begin{align*}
\min_G \max_DV(D, G) &= \sum_{i=1}^{m}\left[p(\boldsymbol{x}^{(i)})\log D(\boldsymbol{x}^{(i)})\right] +  \sum_{i=1}^{m}\left[p(\boldsymbol{z}^{(i)})log (1 - D(G(\boldsymbol{z}^{(i)})))\right] \\
&= \sum_{i=1}^{m}\left[\frac{1}{m}\log D(\boldsymbol{x}^{(i)})\right] + \sum_{i=1}^{m}[\frac{1}{m}log (1 - D(G(\boldsymbol{z}^{(i)})))]\\
 &=\frac{1}{m}\sum_{i=1}^{m}\left[\log D(\boldsymbol{x}^{(i)}) + log (1 - D(G(\boldsymbol{z}^{(i)})))\right]
\end{align*}
$$

Binary cross entropy defined as follows:
$$H(p, q) = \operatorname{E}_p[-\log q] = H(p) + D_{\mathrm{KL}}(p \| q)=-\sum_x p(x)\log q(x)$$
Since we have a binary classification problem (fake/real), we can define $p \in \{y,1-y\}$ and $q \in \{\hat{y}, 1-\hat{y}\}$ and rewriting coros entropy as follows:
$$H(p, q)=-\sum_x p_x \log q_x =-y\log \hat{y}-(1-y)\log (1-\hat{y})$$
which is nothing but logistic loss. Since we know the source of our data (either real or fake), we can replace labels $y$ for real and fake with 1. We then get:
$$\min_G\max_D L =  \frac{1}{m} \sum_{i=1}^{m}\left[1\cdot\log D\left(\boldsymbol{x}^{(i)}\right)+1\cdot\log \left(1-D\left(G\left(\boldsymbol{z}^{(i)}\right)\right)\right)\right]
$$
This is the original loss. The first term in the equation gets always real images, while the second gets only generated. Hence, both terms have corresponding true labels. Read this article for more details.
Since the first term does not depend on $G$, we can rewrite it as follows:
$$\max L(D) =  \frac{1}{m} \sum_{i=1}^{m}\left[\log D\left(\boldsymbol{x}^{(i)}\right)+\log \left(1-D\left(G\left(\boldsymbol{z}^{(i)}\right)\right)\right)\right]$$
$$\min L(G) =  \frac{1}{m} \sum_{i=1}^{m}\left[\log \left(1-D\left(G\left(\boldsymbol{z}^{(i)}\right)\right)\right)\right]$$
"
How to check whether my loss function is convex or not?,"
Loss functions are useful in calculating loss and then we can update the weights of a neural network. The loss function is thus useful in training neural networks.
Consider the following excerpt from this answer

In principle, differentiability is sufficient to run gradient descent. That said, unless $L$ is convex, gradient descent offers no guarantees of convergence to a global minimiser. In practice, neural network loss functions are rarely convex anyway.

It implies that the convexity property of loss functions is useful in ensuring the convergence, if we are using the gradient descent algorithm. There is another narrowed version of this question dealing with cross-entropy loss. But, this question is, in fact, a general question and is not restricted to a particular loss function.
How to know whether a loss function is convex or not? Is there any algorithm to check it?
","['objective-functions', 'convergence', 'convex-function']","
It is the same as other functions. You can use Theorem 2 in this lecture (from Princeton University):

(ii) condition is about the first-order condition for convexity and (iii) is the second-order. You can also find more detail in chapter 3 of this book (""Convex Optimization"" by Stephen Boyd and Lieven Vandenberghe).
"
What exactly is a Parzen?,"
I came across the term ""Parzen"" while reading the research paper titled Generative Adversarial Nets. It has been used in the research paper in two contexts.
#1: In phrase ""Parzen window""

We estimate probability of the test set data under $p_g$ by fitting a
Gaussian Parzen window to the samples generated with $G$ and reporting
the log-likelihood under this distribution.

#2: In phrase ""Parzen density estimation""

Evaluating $p(x)$ in Generative autoencoders and Adversarial models: Not
explicitly represented, may be approximated with Parzen density
estimation

Is there any definition for the word Parzen and how is it related to the probability distributions?
","['terminology', 'definitions', 'probability']","
Parzen was a statistician, who worked in spectral analysis and stochastic processes. I don't know if he invented them, but those windows and probability density esimation methods are named after him.
See also his Wikipedia entry.
"
Does the output layer in a deep neural network need an activation function?,"
I have enrolled in a course that uses only one hidden layer, and that is the only layer that has activation functions. The model can be visualized as follows:

and here is a PyTorch implementation:
class MnistModel(nn.Module):
    """"""Feedfoward neural network with 1 hidden layer""""""
    def __init__(self, in_size, hidden_size, out_size):
        super().__init__()
        # hidden layer
        self.linear1 = nn.Linear(in_size, hidden_size)
        # output layer
        self.linear2 = nn.Linear(hidden_size, out_size)
        
    def forward(self, xb):
        # Flatten the image tensors
        xb = xb.view(xb.size(0), -1)
        # Get intermediate outputs using hidden layer
        out = self.linear1(xb)
        # Apply activation function
        out = F.relu(out)
        # Get predictions using output layer
        out = self.linear2(out)
        return out

shouldn't the output layer also have activation functions?
","['neural-networks', 'deep-learning', 'activation-functions']","
A neural network layer with no activation function is the same as ""linear activation"" i.e. $f(x) = x$
This is often used for the output layer in regression problems, where a constrained output like that of sigmoid, hyperbolic tangent or ReLU may not be appropriate.
For an output layer, this is fine, and does not conflict with any theory behind neural networks. It will often be combined with a mean squared error (MSE) loss function that makes the gradient calculation simple at the output layer.
For hidden layers, skipping the activation function can be a problem, since a purely linear layer in the middle of a multi-layer network is redundant - it could be replaced or even removed with no impact. That is because two directly connected linear layers are functionally equivalent to a single linear layer with different parameters, and every hidden layer consists of a linear component plus an activation function. So even one missing activation function on a hidden layer directly connects two linear sub-components, making one of them redundant.
In the case of a classifier, some libraries (notably including PyTorch which you are using) require you to have two variants of your neural network - a trainable version without a final sigmoid or softmax layer, and the ""full"" version which adds sigmoid or softmax on top (usually just a function that calls the training network and add this last activation). This is done to allow you to use more stable gradient calculations that are based on the logits, the values before applying an activation function.
"
What are the iid random variables for a dataset in the GAN framework?,"
I am trying to understand why mean is used for expectation in training Generative Adversarial Networks.
The answer tells that it is due to the law of large numbers which is based on the assumption that random variables are independent and identically distributed.

If I have a dataset of all possible $32 \times 32$ grayscale images. Then my sample space consists of $256^{32 \times 32}$ elements. Suppose I define 1024 random variables as
$$X_i = \text{ intensity of } i^{th} \text{ pixel for } 1 \le i \le 1024$$
Then it is clear that all the random variables are iid since

$X_i \perp  X_j$ for all $i, j$ such that $i \ne j$ and
$p(X_i = k) = \dfrac{1}{256}$ for all $i$


But these properties do not hold if I take a dataset of (say flower) images since pixel intensities are not independent of each other and the intensity values are not uniformly distributed as well.
Then how can the law of large numbers be applicable for GAN as the dataset (sample space) does not cover all the possible elements? If I am wrong, then what is the sample space they are considering and what are the random variables they are using implicitly that leads to the satisfaction of iid condition and then the law of large numbers?
","['math', 'generative-adversarial-networks', 'expectation', 'random-variable', 'iid']","
Independent and identically distributed random variables share the same probability distribution and each item doesn’t influence or provide insight about the value of the next item you measure. The most common example is a coin toss: as you flip the coin, one outcome does not influence or predict the next one.
As for a dataset of flowers, we assume that the photos were made independently and all kinds of flowers are evenly represented, which in practice is not true. Each flower won't be represented in the same lighting condition and camera view.
Each pixel in an image is highly dependent on the neighboring pixels (and sometimes distant ones, e.g., when generating faces as they are usual symmetric) and the main goal of GAN is to discover this dependence by exploring a latent features manifold that represents abstract attributes of the real data, and to find a mapping between these features and output pixels. Of course, if some of the features are poorly represented, the network will not be able to generate them correctly. For instance, to increase the fidelity, there is the so-called truncation trick, where we sample the latent features vector $z$ from a truncated normal distribution, which ignores rare features. But on the other hand, it reduces diversity.
As for the loss function,
$$\nabla_{\theta_{d}} \frac{1}{m} \sum_{i=1}^{m}\left[\log D\left(\boldsymbol{x}^{(i)}\right)+\log \left(1-D\left(G\left(\boldsymbol{z}^{(i)}\right)\right)\right)\right]$$
the mean is associated with the loss rather than the iid of the random variables and is used to obtain the average gradient of a batch. Here are related questions: one, tow.
"
Is it abuse of notation to use tilde operator in this context?,"
The following is a way to use tilde (∼) in context of random variables or random vectors.

In statistics, the tilde is frequently used to mean ""has the
distribution (of),"" for instance, $X∼N(0,1)$ means ""the stochastic
(random) variable $X$ has the distribution $N(0,1)$ (the standard
normal distribution). If X and Y are stochastic variables then $X∼Y$
means ""$X$ has the same distribution as $Y$.

Consider the following usage of tilde in the paper titled Generative Adversarial Nets
$$x ∼ p_{data}(x)$$
$$z ∼ p_z(z)$$
I am thinking that the following is the standard (and possibly correct) notation
$$x ∼ p_{data}$$
$$z ∼ p_z$$
$p_{data}$ is a probability distribution and $p_{data}(x)$ is not a probability distribution and it is a value in $[0, 1]$. It is same in case of noise probability distribution.
Is it an abuse of notation to use in such a way or is it also a standard and allowed notation to use?
","['probability-distribution', 'notation', 'random-variable']",
Why is noise vector represented by letter $z$? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



Most of the notations in Artificial Intelligence are borrowed from the mathematics.
$x$ stands for input (vector), $y$ stands for output (vector) etc., and the list is long.
But, I am not sure whether $z$ has any (widely used) role in mathematics.
Is there any reason behind the usage of letter $z$ to represent a noise vector?  Or is it just selected randomly without any reason?
","['history', 'notation', 'etymology', 'noise']","
I don't think there's any rationale behind the usage of the letter $z$ to denote the noise (which sometimes is also denoted by $\epsilon$ in other contexts), apart from the fact that $x$ and $y$ are already being used and that the letters $x$, $y$, $z$ and $w$ are often used to denote variables in mathematics. In particular, in machine learning, $x$ and $y$ are often used to denote the inputs and outputs (or labels) respectively, while $w$ often denotes the parameters (although $\theta$ is also used for that).
In other words, it's just a convention (e.g. $z$ is also used to denote the hidden variable in the VAE paper). Even if it wasn't and someone used $z$ as a mnemonic letter or for some particular reason, I wouldn't lose too much time on this issue, as another author may very well use any other letter to refer to the same concept. It's important to be a bit flexible when it comes to notation in mathematics, otherwise, you may easily get lost.
"
"What is meant by ""well-behaved gradient"" in this context?","
Consider the following statement (from the paper Generative Adversarial Nets) about the success of discriminative models

So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units which have a particularly well-behaved gradient.

The piece-wise linear units they are referring to are, I guess, the activation functions. The primary purpose of activation functions is to introduce non-linearity, and there are no other mathematical requirements, such as continuity, differentiability, etc. But not all the activation functions may work well, and some are preferred over others, based on their nature, as well as the task under consideration.
After reading the quoted paragraph, one can conclude that the activation functions that have well-behaved gradient are showing better results than the others, at least in discriminative tasks.
What does ""well-behaved"" in this context stand for? Can we have some mathematical properties of the gradient in order to recognize it as a well-behaved gradient? Or the usage of the phrase ""well-behaved"" is highly dependent on the discriminative task under consideration?
","['deep-learning', 'terminology', 'papers', 'activation-functions', 'gradient']","
""Piecewise linear"" appears to describe ReLu and LRelu activation functions, whose gradients are just simple step functions.
"
Are there any Generative Adversarial Networks without Multi Layer Perceptrons?,"
Although the main stream research is on Generative Adversarial Networks(GANs) using Multi Layer Percepteons (MLPs). The original paper titled Generative Adversarial Nets clealry says, in abstract, that GAN is possible with out MLP also

In the space of arbitrary functions $G$ and $D$, a unique solution
exists, with $G$ recovering the training data distribution and $D$
equal to $\dfrac{1}{2}$ everywhere. In the case where G and D are defined
by multilayer perceptrons, the entire system can be trained with backpropagation.

Are there any research papers that uses models other than MLPs and are comparatively successful?
",['generative-adversarial-networks'],
Which probability distribution a generator in Generative Adversarial Network (GAN) is capturing: dataset or ground truth?,"
Consider the following statement from the abstract of the paper titled Generative Adversarial Nets

We propose a new framework for estimating generative models via an
adversarial process, in which we simultaneously train two models: a
generative model $G$ that captures the data distribution, and a
discriminative model $D$ that estimates the probability that a sample
came from the training data rather than $G$.

Let $D'$ be a dataset of $n$ digital and discrete* images, each of size $C \times H \times W$. Suppose the generative adversarial network is trained on the dataset $D'$.
Our sample space (or data set) is $D' = \{I_1, I_2, I_3, \cdots, I_n\}$, where $I_j$ is the $j^{th}$ image for $1 \le j \le n$
The random variables are $X_1, X_2, X_3, \cdots, X_{CHW}$ where
$$X_i \in \{a, a+1, a+2, \cdots, b\} =\text{ intensity of }i^{th} \text{ pixel;} \text{     for  } 1 \le i \le CHW$$
Since we have dataset $D'$, we can calculate the joint distribution $p_{data\_set}$ having $(b-a+1)^{CHW}$ parameters calculated from the dataset. But the parameters of the original image distribution $p_{ground_{D^{'}}} $is not equal to the $p_{D'}$

Simple example:
Suppose I flipped an unbiased coin 100 times and I got 45 heads, 55 tails, then $P_{data\_set}(H) = \dfrac{45}{100}$ and $P_{data\_set}(T) = \dfrac{55}{100}$. So,  $P_{data\_set} = \{\dfrac{45}{100}, \dfrac{55}{100}\}$
but the ground truth probability distribution is $P_{ground}(H) = \dfrac{50}{100}$ and $P_{ground}(T) = \dfrac{50}{100}$.  So,  $P_{ground} = \{\dfrac{50}{100}, \dfrac{50}{100}\}$

Which distribution is our generator capturing by the end? Is it the probability distribution calculated based on our dataset $p_{data\_set}$ of images or the actual probability distribution $p_{ground}$?
How to understand the act of capturing here? Does it only mean to be behaving (generating instances) in the same way as data distribution?

Suppose I design a machine that is capable to generate equal number of $0$'a as equal number of $1$'s for sufficiently large trails, then I can say that my machine captured the coin toss probability distribution?


discrete images refers to the images whose pixel values takes finite number of integer values.

","['generative-adversarial-networks', 'probability', 'probability-distribution', 'generator']",
Is there a way to parallelise the RL training on multiple stocks to avoid the memory issue?,"
I have some plans in working with Reinforcement Learning in order to predict the stock price movement. For a stock like TSLA

some training features might be the pivot price values and the set of the difference between two consecutive pivot points.
I would like that my model captures the general essence of the stock market. In other words, if I want my model to predict the stock price movement for TSLA, then my dataset will be built only on TSLA stock. If I try to predict the price movement on FB stock using that model, then it won't work for many reasons. So, if I want my model to predict the price movement of any stock, then I have to build a dataset using all types of stock prices.
For the purpose of this question, instead of taking an example of the dataset using all the stocks, I will use only three stocks, i.e. TSLA, FB, and AMZN. So, I will generate the dataset for two years for TSLA, two years of FB, and two years of AMZN, and then pass it back to back to my model. So, in this example, I pass 6 years of data to my model for training purposes. If I start with FB, then the model will learn and memorize some patterns from the FB features. The problem is when the model is made to train on the AMZN features, it already starts to forget the information of the training on the FB dataset.
Is there a way to parallelise the training on multiple stocks to avoid the memory issue?
Instead of my action being a real value, it will be an action vector where the size is depending on the number of parallel stocks.
","['neural-networks', 'reinforcement-learning', 'datasets', 'time-series', 'algorithmic-trading']","
Take a look at:
Deep Reinforcement Learning for Automated Stock Trading where the 30 Dow Jones stocks are trained using OpenAI Gym.
The code is here and the published paper is  here.
An excerpt from the paper:

We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy
Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations.

"
"What is an ""additional channel dimension"" contain in batch normalization?","
Consider the following explanations regarding batch normalization layers in PyTorch
#1: one dimensional batch normalization

class torch.nn.BatchNorm1d(.........)
Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D
inputs with optional additional channel dimension)............

#2: Second dimensional batch normalization

class torch.nn.BatchNorm2d(..........)
Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)

#3: Third dimensional batch normalization

class torch.nn.BatchNorm3d(..............)
Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension)

All these say that there can be an extra element to each vector undergoing batch normalization and is related to channel.
Is the channel referred here is same as the channels of the image? If yes, then what does it contain? Does it contain the number of channels in that particular layer?
Else, what does this additional channel contain?.
","['pytorch', 'batch-normalization', 'channel']",
Reinforcement learning algorithms that deal with noisy state observations,"
I was recently considering training an agent that perform a task by reinforcement learning. Both the state and actions are continuous, but could be discretized if needed. The problem is that in my case the state observations and reward will be quite noisy, so given the same state and action, the next state and received reward will be different on each run, and the noise cannot be described by canonical probability distributions.
Up to now I have tried deep Q-network, stochastic policy gradient and deep deterministic policy gradient. While I could successfully implemented these algorithms in the CartPole game, they all failed to learn my particular task.
I hope to know are there any reinforcement learning methods that can deal with noisy state observations?
",['reinforcement-learning'],
"Why is it called ""batch"" gradient descent if it consumes the full dataset before calculating the gradient?","
While training a neural network, we can follow three methods: batch gradient descent, mini-batch gradient descent and stochastic gradient descent.
For this question, assume that your dataset has $n$ training samples and we divided it into $k$ batches with $\dfrac{n}{k}$ samples in each batch. So, it can be easily understood the word ""batch"" is generally used to refer to a portion of the dataset rather than the whole dataset.
In batch gradient descent, we pass all the $n$ available training samples to the network and then calculates the gradients (only once). We can repeat this process several times.
In mini-batch gradient descent, we pass $\dfrac{n}{k}$ training samples to the network and calculates the gradient. That is, we calculate the gradient once for a batch. We repeat this process with all $k$ batches of samples to complete an epoch. And we can repeat this process several times.
In stochastic gradient descent, we pass one training sample to the network and calculates the gradient. That is, we calculate the gradient once for iteration. We repeat this process with all $n$ times to complete an epoch. And we can repeat this process several times.
Batch gradient descent can be viewed as a mini-batch gradient descent with $k = 1$ and stochastic gradient descent can be viewed as a mini-batch gradient descent with $k = n$.
Am I correct regarding the usage of terms in the context provided above? If wrong then where did I go wrong?
If correct, I am confused about the usage of the word ""batch"" in ""batch gradient descent"". In fact, we do not need the concept of batch in batch gradient descent since we pass all the training samples before calculating gradient. In fact, there is no need for batch gradient descent to partition the training dataset into batches. Then why do we use the word ""batch"" in batch gradient descent? Similarly, we are using the word ""mini-batch"" in ""mini-batch gradient descent"". In fact, we are passing a batch of samples before calculating the gradient. Then why it is called ""mini-batch"" gradient descent instead of ""batch"" gradient descent?
","['terminology', 'gradient-descent', 'stochastic-gradient-descent', 'mini-batch-gradient-descent']","
You are correct, but requires final words:
In Batch GD, we take the average of all training data to update the parameters, hence, one step per epoch. That's very valid if you have a convex problem (i.e. smooth error).
On the other hand, in the Stochastic GD, we take one training sample to go one step towards the optimum, then repeat the latter for every training sample, hence updating the parameters once per sample sequentially in every epoch (no average here). As you can expect, the training will be noisy and the error will be fluctuating.
Lastly, the mini-batch GD, is somehow in between the first two methods, that is: the average of a different portion of the data, every time. This method would take the benefits of the previous two, not so noisy, yet can deal with less smooth error manifold.

Personally, I memorize them in my mind by creating the following map:

Batch GD ≡ Average of All per Step ≡ More suitable for Convex Problems at the Risk of Converging directly to Minima = Heavyweight.
Stochastic GD ≡ Fluctuating & Noisy ≡ Converges on the Long Run especially for Large Dataset ≡ Lightweight but Slower ≡ No Vectorization Possible (because one sample per time).
Mini-Batch GD ≡ Portion of Data per Step ≡ Mixture of Stochastic and Batch GD ≡ Less Fluctuation + Would work for Less Smooth Error Manifolds ≡ Faster Computation.


Regarding the naming convention, I would understand the word ""Batch"" as a ""Set"" or ""Collection"", hence the whole ""Dataset"". Consequently, ""Mini"" would go with the flow, to mean a ""Part of the Set"".
"
What does it mean there is no rollout in AlphaZero's training?,"
According to a blog post by DeepMind, AlphaZero doesn't have a real rollout.

AlphaGo Zero does not use ""rollouts"" - fast, random games used by other Go programs to predict which player will win from the current board position. Instead, it relies on its high quality neural networks to evaluate positions.

Instead, I assume it just interprets the winner at a given state by the NN values head result. This replaces the rollout. So the computation time saved could be used for many expansions instead. Evaluating a state from a root node would then be the best action derived from the visit count in MCTS, which is only based on the predictions of the NN value heads. (no current score, no policy?)
With policy, I mean the NN's policy head (softmax).
This would mean that the NN policy is only used in the loss calculation and nowhere else?
","['reinforcement-learning', 'alphazero']",
How to measure the significance of an input feature for the output of a linear layer in a neural network,"
Suppose I have a simple linear layer $y = xA^T + b$ that is part of a neural network trained on some dataset. The weight matrix $A$ for this layer has the shape [num_outputs, num_inputs].
For each layer input, I would like to find a value between 0 and 1, based on the weight matrix, representing the significance of that input to the layer output.
Intuitively, if the values in the i-th column of the weight matrix are close to 0, then the significance of i-th inputh should also be close to 0. Conversely, if the values are close to maximum or minimum of the entire weight matrix, the significance should approach 1.
This statistic should also adequately recognize cases where the vast majority of values in a column are close to 0, but at least one is not. Then the significance of such an input should not be close to 0, because it is important for a single neuron that detects, for example, an edge case.
Can anyone point me in the right direction?
","['neural-networks', 'machine-learning', 'deep-learning']",
At what point are MCTS results discarded in AlphaZero Training?,"
Regarding the AlphaZero paper, it is not clear to me when the Monte Carlo Tree Search (MCTS) results will be cleaned up.
I assume this has to happen at some point, since mixing results could lead to lower quality results? Imagine in the self-play the Neural Network (NN) is updated to a new version and evaluates certain patterns differently by detecting a new trick. Many iterations must follow to outperform the old best choice (visit-count). I imagine discarding old MCTS results should be done about between an episode and the next NN weight updates.
I feel that a wrong decision here could have a strong negative impact on the overall learning process.
","['reinforcement-learning', 'alphazero']",
Cover a surface with smaller predefined objects,"
I'm trying to make a program that takes a surface designed by the user, and different 3D geometries from a dataset as inputs and gives a good approximation of the surface using only the objects found in the dataset. This program shouldn't do any warping, and should avoid geometries to collide, even though cutting them could be acceptable, but again with as little loss as possible.
I thought about hardcoding this, but I can't find any good way to optimize the surface coverage without brute-forcing it. I'm wondering what ML techniques would be best for this, and how to find a good balance between precision and speed.
",['optimization'],"
a subset of your problem is pallet-loading (you can find a survey here about it). Yet, there is not any polynomial algorithm for an even simpler case of your problem such as pallet-loading. However, different planners in automated planning contexts can be helpful and give you some heuristics to solve the problem, some planners like Fast Forward.
"
"Is a convolutional layer capable of converting, for example, a binary image into an RGBA image?","
I am asking this question for a better understanding of the concept of channels in images.
I am aware that a convolutional layer generates feature maps from a given image. We can adjust the size of the output feature map by proper padding and regulating strides.
But I am not sure whether there exist kernels for a single convolution layer that are capable of changing an {RGBA, RGB, Grayscale, binary} image into (any) another {RGBA, RGB, Grayscale, binary} image?
For example, I have a binary image of a cat, is it capable to convert it into an RGBA image of a cat? If no, can it at least convert a binary cat image into an RGBA image?
I am asking only from a theoretical perspective.
","['convolutional-neural-networks', 'convolution', 'channel']","
No, because each output from a convolution layer only looks at a local region of the image. A convolution layer cannot do any global transformation, only local ones. Convolution layers must have translation invariance which means if it converts an eyeball to a tail at one position, it'll also convert the same eyeball to the same tail if it's found at a different position. If it's not overfitted, it will also convert similar eyeballs to similar tails. If you want only some eyeballs to become tails, you can't do that without introducing overfitting, or expanding the convolution size until the layer can see enough context to distinguish which eyeballs should become tails and which ones shouldn't.
If you want to change one image into a specific other image, and don't care what happens to all other images, it might be possible to create a convolution layer that does this transformation. The input image has to be different wherever the output image is different, or else the convolution layer won't be able to produce that difference in the output image. You would be teaching it to recognize the specific pixel patterns in the input image and generate the specific pixels in the output image. This would be an extreme case of overfitting and wouldn't work for any other input images.
The number of channels in the input and output image is irrelevant, except that more channels means the network has more data to learn from, obviously.
"
Training on the dataset in parts vs training on the whole dataset,"
What is the difference between these two situations? are they the same ?
#1 : train a model 20 epochs on the whole dataset
#2 : divide dataset into n-parts then train the model 20 epochs on each part
20 is a random number just for clarification. do we get the same result (accuracy) between these two situations? and why ?
Side note: this question was raised in my mind when I faced a problem: dataset is bigger than the storage space. So I want to divide it into 4 parts and train the model on each part. But does this effect on accuracy ? does this method of training is correct ?
","['training', 'datasets']",
Do deep learning researchers generally visualize intermediate steps?,"
Many researchers in deep learning research come up with new CNN architectures.
The architectures are (just) combinations of a few existing layers.
Along with their mathematical intuition, in general, do they visualize intermediate steps by execution and then (do trial and error) brute force for achieving the state-of-art architectures?
Visualizing intermediate steps refers to printing outputs in the proper format for analyzing them. Intermediate steps may refer to feature maps in CNN, hidden states in RNN, outputs of hidden layers in MLP, etc.
","['research', 'architecture', 'data-visualization']",
Backpropagation after N sequential input-output pass,"
I'm trying to train a Neural Network in a particular situation -- similar to a genetic algorithm domain as far as I know.
I have to run a simulation with a length of $K$ steps.
I have a neural network $N$ that at each time step is used to produce an output, so that:
$$
o_{t+1} = N(i_{t})
$$
$i_t$ is a feature vector built upon $o_{t-1}$, and $i_0$ is given.
My ground-truth value is $o_k$, namely the right value at the end of the simulation. So, I can evaluate the loss (e.g. MSE) only at the end of the simulation.
Suppose to fix k to 3, the evaluation is: $N(N(N(i_0))$
because:
$$
o_1 = N(i_o) \\
o_2 = N(i_1) \\
o_3 = N(i_2)
$$
So my questions are:

does it have any sense to apply backpropagation in these settings?
if yes, what happens to the gradients?

Practically, in some simple situations, the backpropagation seems to work, but in others, the gradients explode or vanishes
","['neural-networks', 'machine-learning']","
I/ Does it has any sense to apply backpropagation in these settings?
If I understand correctly, this question should be ""Can backpropagation return the gradient for every node of these networks?"".
It depends on your network $N$ if it is differentiable with all inputs $i_t$ so the backpropagation can be guaranteed that there are the gradient values at every node of the network.
II/ If yes, what happens to the gradients?
We deal with the vanishing or exploding gradient after we make sure that the gradient is exist. The reason for this phenomenon in a neural network can come various, some typical causes are:

Very deep neural networks (a large number of layers):
Assume a neural network with $n$-th layers, the backpropagation is followed the Markov Chain's rule can be show as:


$$\frac{\partial loss}{\partial w_t} = \frac{\partial loss}{\partial l_{n}} \times \frac{\partial l_{n}}{\partial l_{n-1}}\times...\times \frac{\partial l_{t+1}}{\partial l_{t}}\times \frac{\partial l_t}{\partial w_t}$$
It's easy to see that the gradient is scaled exponentially for each layer, so if each gradient value is larger than 1, the gradient will become $\inf$ with $n\rightarrow \inf$ (exploding) and $0$ in vice versa.

Activation function:
The image below shows the difference between different activation functions. It's easy to see that the logistic function such as Sigmoid or Tanh is limited in the range $[0,1]$ (in case sigmoid) or $[-1,1]$ in case Tanh. Therefore, if the output from a node of the neural network is larger than 2 or smaller than -2, the gradient will become nearly zero (vanishing) because the output is always the same.



Solution:

Replace it by the simple function such as ReLU: $o_{t+1} = max(0, N(i_t))$. However, the output from ReLU will be $0$ if it's lower than $0$, so there are many variants of ReLU to solve this problem, you can find them easily by the keyword ""ReLU family"" on google (example).

There are other methods or strategies to duel with vanishing/exploding gradient, it also depends on your model or your data. The answer can be more detailed if you give more information.
"
When does a batch normalization layer becomes active?,"
Let us assume your dataset has $n$ training samples each of size $s$ and you divided them into $k$ batches for training. Then each batch has $n_k = \dfrac{n}{k}$ training samples.
Batch normalization can be applied to any input or hidden layer in a neural network. So, assume that I am applying batch normalization at every possible place I can.
Now, consider a particular batch normalization layer (say $b$) of a hidden layer $\ell$. Now, I am confused about the working frequency of $b$.
Will it be activated only after every $n_k - 1$ forward passes i.e, once per batch at the end of the batch? If no, then how $b$ calculates the mean and standard deviation for every forward pass while training if $n_k$ output vectors of $\ell$ are not available at that instant?
Will $b$ calculates the mean and standard deviated, for every forward pass, based on the outputs of $\ell$ that are calculated so far? If yes, then why it is called batch normalization?
To put it concisely, are batch normalization layers active for every iteration? If yes then how they are normalizing a ""batch"" of vectors?

You can check here which says

The mean and standard-deviation are calculated per-dimension over the
mini-batches

","['neural-networks', 'deep-learning', 'batch-normalization']",
"Material(s) for understanding ""image channels""","
I am pretty confused about the concept of ""image channels"".
I want material that explains the concept of channels from scratch to whatever is required to understand their role in machine learning. I think that it is a small concept and possibly present as a chapter in good textbooks.
Where can I read about channels of an image in detail?
","['machine-learning', 'image-processing', 'resource-request', 'channel']","
Image channels have nothing to do with machine learning, they are just part of computer image processing.
A channel is a number per pixel. So most colour images are stored with red, green and blue channels, as you probably know. Some images are stored in greyscale with just one white channel.
A RGB image is stored like this: pixel 0 red amount, pixel 0 green amount, pixel 0 blue amount, pixel 1 red amount, pixel 1 green amount, pixel 1 blue amount, pixel 2 red amount, ...
They could also be rearranged like this: pixel 0 red amount, pixel 1 red amount, pixel 2 red amount, ....., pixel 99999 red amount, pixel 0 green amount, pixel 1 green amount, ....., pixel 99999 green amount, pixel 0 blue amount, ....., pixel 99999 blue amount.
But that is not common.
A greyscale image only has one channel and it's stored like this: pixel 0 white amount, pixel 1 white amount, pixel 2 white amount, pixel 3 white amount, ...
A black-and-white image also has only one channel (a white channel) but that channel can only be 0 brightness or maximum brightness. They can be stored with just 1 bit per pixel.
Alpha is an extra transparency channel that some pictures have. Alpha 0 means fully transparent. Maximum alpha means fully opaque. Half-maximum alpha means the image is partially see-through at that pixel. Things like photos don't have alpha, but computer graphics that are designed to be displayed on top of other pictures often do.
There are also more exotic systems like YCbCr, where you have a white channel, a blue-versus-green channel, and a red-versus-green channel. Mostly we just convert those to RGB before processing.
"
"How to calculate the gradient penalty proposed in ""Improved Training of Wasserstein GANs""?","
The research paper titled Improved Training of Wasserstein GANs proposed a gradient penalty in order to avoid undesired behavior due to weight clipping of the discriminator.

We now propose an alternative way to enforce the Lipschitz constraint.
A differentiable function is 1-Lipschtiz if and only if it has
gradients with norm at most 1 everywhere, so
we consider directly constraining the gradient norm of the critic’s
output with respect to its input. To circumvent tractability issues,
we enforce a soft version of the constraint with a penalty on the
gradient norm for random samples $\hat{x} \sim  P_\hat{x}$. Our new
objective is
$$L = \mathop{\mathbb{E}}\limits_{\tilde x \sim \mathbb{P}_g}
 [D(\tilde x)] - \mathop{\mathbb{E}}\limits_{ x \sim \mathbb{P}_r}
 [D(x)] +  \mathop{\mathbb{E}}_{\hat{x}  \sim P_{\hat{x}}} [
  (\| \triangledown_{\hat{x}} D(\hat{x})\|_2 - 1 )^2  ]$$

The last term in the discriminator's loss function is related to the gradient penalty. It is easy to calculate the first two terms. Since discriminator, in general, gives value in range $[0, 1]$, the first two terms are just the average of the sequence of probability values given by discriminator on generated and real images respectively.
But, how to calculate $\triangledown_{\hat{x}} D(\hat{x})$ for a given image $\hat{x}$?
","['training', 'math', 'implementation', 'gradient', 'wasserstein-gan']","
First of all, the discriminator in WGAN does not give a value in the range $[0,1]$. Compared to the traditional discriminator, it has a linear activation in the output layer. Therefore, the authors call it critic instead.
To calculate the penalty, we sample an image that lies on the line between the real and the generated image. This is done by sampling a real image $x$, generating an image $\tilde{x}$, and mixing these images $\hat{x} = \alpha \widetilde{x}+(1-\alpha)x$ with $\alpha \sim U(0,1)$. That is, $\hat{x}$ is uniformly sampled from the lines between real and fake images, which can be illustrated as follows:

We then feed $\hat{x}$ into the critic and calculate the gradient norm of the discriminator's output with respect to its input, $(\| \triangledown_{\hat{x}} D(\hat{x})\|_2 - 1 )^2$. Here is a snippet code for PyTorch:
def gradient_penalty(D, real_data, generated_data, device):
    batch_size = real_data.shape[0]

    # Calculate interpolation
    alpha = torch.rand(batch_size, 1, 1, 1)
    alpha = alpha.expand_as(real_data).to(device)
    # getting x hat
    interpolated = alpha * real_data + (1 - alpha) * generated_data

    dis_interpolated = D(interpolated)
    grad_outputs = torch.ones(dis_interpolated.shape).to(device)

    # Calculate gradients of probabilities with respect to examples
    gradients = autograd.grad(outputs=dis_interpolated, inputs=interpolated,
                           grad_outputs=grad_outputs, create_graph=True, retain_graph=True)[0]

    # Gradients have shape (batch_size, num_channels, img_width, img_height),
    # so flatten to easily take norm per example in batch
    gradients = gradients.view(batch_size, -1)

    # Derivatives of the gradient close to 0 can cause problems because of
    # the square root, so manually calculate norm and add epsilon
    gradients_norm = ((torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12) - 1) ** 2).mean()
    return gradients_norm

"
Is it better to model a Contextual Multi-Armed Bandit problem as an MDP with a non-zero discount factor than treating it as it is?,"
I'd like to ask if it is, generally, better to model a problem that naturally appears as a Contextual Multi-Armed Bandit like Recommender Systems as a Markov Decision Process with a non-zero discount factor (otherwise it's just an MDP with one step episodes) or is it better to treat it as it is; a Contextual Multi-Armed Bandit (MDP with a zero discount factor)
I'm thinking about some problems like Recommender Systems where we can't define well the dynamics of the environment and so using a non-zero discount factor wouldn't make much sense since we'll take into account the recommendations for users that are independent of each other.
","['reinforcement-learning', 'markov-decision-process', 'multi-armed-bandits', 'contextual-bandits', 'discount-factor']",
How to use a heuristic policy to increase sample efficiency of a deep reinforcement learning agent?,"
I have a heuristic solution to a problem which works quite well when certain environmental parameters are known and unchanging. However, in a real world setting these parameters will not be known and are likely to fluctuate over the course of an episode. I'm hoping to use deep RL to develop a policy that will be similar to the heuristic, but robust to these unknowns.
My question is: does the RL agent need to be trained ""from scratch"" as one would typically do or is there a way to leverage the existing policy to jump start the training progress?
In the latter case, what would this looks like? I've had a couple of thoughts, but I'm not sure how well any of them would work.

Reward actions that the heuristic would take in an environment with static parameter values, then gradually make the environment more complex and set a new reward function based on what I'm actually interested in.

Instead of taking random actions in the exploration stage, take actions dictated by the heuristic.


","['reinforcement-learning', 'deep-rl', 'sample-efficiency']",
Mapping input vectors of variable length to output vectors of variable lengths with dummy variables,"
I have a general question about supervised ANNs that map inputs to outputs. It is possible to vary the length of the input and output vectors by inserting some dummy variables that will not be considered in the mapping (or will be mapped to other dummy variables). So basically the mapping should look like this (v: value, d: dummy)
Input vector 1 $[v,v,v,v,v] \rightarrow$ Output vector 1 $[v,v,v,v,v]$
Input vector 2 $[v,v,v,v,v]\rightarrow$ Output vector 2 $[v,v,v,v,v]$
Input vector 3 $[v,v,v,d,d] \rightarrow$ Output vector 3 $[v,v,v,d,d]$
Input vector 4 $[v,v,d,d,d] \rightarrow$  Output vector 4 $[v,v,d,d,d]$
Input vector 5 $[v,d,d,d,d] \rightarrow$  Output vector 5 $[v,d,d,d,d]$
The input and output vectors have a length of 5 with 5 values. However, sometimes only a vector of size e.g. 3 (which is basically a vector of length 5 with 2 dummy variables) should be mapped to an output vector of length 3. So after training the ANN should know that if it for example gets an input vector of length 3 it should produce an output vector of length 3.
Is something like this generally possible with ANNs or other machine learning approaches? If so, what type of ANN or machine learning approach can be used for this? I'll appreciate every comment.
Reminder: Can anybody give me more insights into this?
","['neural-networks', 'training']",
What is Lipschitz constraint and why it is enforced on discriminator?,"
The following is the abstract for the research paper titled Improved Training of Wasserstein GANs

Generative Adversarial Networks (GANs) are powerful generative models,
but suffer from training instability. The recently proposed
Wasserstein GAN (WGAN) makes progress toward stable training of GANs,
but sometimes can still generate only poor samples or fail to
converge. We find that these problems are often due to the use of
weight clipping in WGAN to enforce a Lipschitz constraint on the
critic, which can lead to undesired behavior. We propose an
alternative to clipping weights: penalize the norm of gradient of the
critic with respect to its input. Our proposed method performs better
than standard WGAN and enables stable training of a wide variety of
GAN architectures with almost no hyperparameter tuning, including
101-layer ResNets and language models with continuous generators. We
also achieve high quality generations on CIFAR-10 and LSUN bedrooms.

Here, the critic stands for discriminator of the GAN. I understood that the discriminator must obey Lipschitz constraint and hence weight clipping is generally done before this paper. The paper provides an alternative way, penalizing the norm of the gradient of the critic with respect to its input, to enforce the desired Lipschitz constraint.
What actually is Lipschitz constraint and why is it mandatory for a discriminator to obey it?
","['training', 'math', 'generative-adversarial-networks', 'discriminator']","
The Lipschitz constraint is essentially that a function must have a maximum gradient. The specific maximum gradient is a hyperparameter.
It's not mandatory for a discriminator to obey a Lipschitz constraint. However, in the WGAN paper they find that if the discriminator does obey a Lipschitz constraint, the GAN works much better.
A perfect discriminator would perfectly accept all the real samples (output=1 with low gradient) and perfectly reject all the fake samples (output=0 with low gradient), but this provides hardly any gradient information to help train the generator. By limiting the gradient of the discriminator, they force it to be a worse discriminator, but provide more gradient information which helps train the generator.
You can see this in figure 2 (page 9) of the WGAN paper. The red line is a good discriminator but its gradient is nearly 0 at most points. The cyan line (which for some reason is presented upside-down) is clearly much worse as a discriminator, but is much better for training the generator because its gradient is not zero.
The way they limit the discriminator's overall gradient in the WGAN paper is by separately limiting (clipping) each of the weights in the discriminator. They are aware this is a ""terrible"" idea, and leave better ideas for further research. The paper you are asking about is one of those better ideas.
Note that since back-propagation works backwards, enforcing a maximum gradient (Lipschitz continuity) on the discriminator in the forward direction causes it to have a minimum gradient in the backward direction, which is what we want when training the generator.
"
Why people always say the Transformer is parallelizable while the self-attention layer still depends on outputs of all time steps to calculate?,"
When compared to an RNN seq-to-seq model, people always say the Transformer is parallelizable. In the original Attention Is All You Need paper, it also said that

Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t−1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples

I use the The illustrated Transformer to help to explain my question here. It said (You can search those sentences):

Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.

However, actually in the self-attention layer, in order to calculate the select V, it needs the key values of all time steps! So each ""time step"" is not fully independent. There exist an operation in the layer that depends on the output, here is the key from all ""time step"".
In the original paper, the same block is repeated 6 times. That means there are at least 6 points where the flow of independent operation of each ""time step"" or each token to wait for the others. Yes, it is better, but why do they call it parallelizable?
","['transformer', 'attention', 'implementation']","
An RNN processes words one by one. For example on the sentence ""man eats dog"", it will:

Fully process ""man"", producing an output $y_1$ and hidden units $h_1$.
Fully process ""eats"", now using also the previous output and/or hidden units.
Finally process ""dog"", again using the previous output and/or hidden units $y_2$ and $h_2$.

Since the outcome of the first word is used as an input to the second, we must wait until it's done before starting computation on the second, and so on.
In self-attention, the output of say word 3 still depends on the previous (and subsequent) words as you say. However the dependence is much simpler: to obtain the key $k_i$ of the $i$th word, we multiply its embedding vector $e_i$by a fixed matrix $M$: $$k_i = M e_i$$ In fact we can first concatenate the embedding vectors into a matrix $E$ with components $E_{ki}$ being the $k$th component of the embedding vector $e_i$ of the $i$th word. This way all key vectors can be obtained at once as $$K = M E$$ with $K$ decomposing into components in the same way as $E$.
Note that matrix multiplication is highly parallelizable: a given component in the matrix $K$ depends on only one row of $M$ and one column of $E$.
So in short, the reason transformers are parallelizable while RNNs are not is not that they do not depend on earlier (or later) words, but rather that the dependence is linear, while for an RNN it is highly nonlinear (i.e. several layers of an affine transformation followed by a nonlinearity).
"
Creating DQN Learning Agent without Gym environment for a custom project,"
In a project for college I created a simple turn based game, with up to 4 players that can either move or attack the opponents. The players are playing over the network, meaning the clients are supposed to be programmed AIs. The client itself is fully functional, meaning it has all the game logic and can simulate complete games.
Now my task is to create a RL-Agent with a Deeq-Q-Network that learns to play the game. However, I don't really find any source to how that should be done. I was able to create an Agent with a DQN for the CartPole environment of OpenAI gym with PyTorch. Now my guess would be to create my own environment with the gym framework, but since the game itself is already implemented I was thinking if it was possible to feed data in the DQN without having to create the gym environment. As a state it would get the gamestate (which is a 2d grid, with information about the players and their remaining hitpoints) and all the possible moves in the current state as the action space. And since the game is network based, it would save the network after each game and reload it when the next starts during the training. For the training I would start the games over and over with a script and let it train for a while.
As I'm quite new to Machine Learning it seems really blurry as of how to tackle this problem and was hoping to get led in some direction on how to start.
","['reinforcement-learning', 'dqn', 'environment']","
Most OpenAI gym environments are thin wrappers around existing games and libraries. You could do the same with your game. See e.g. https://towardsdatascience.com/beginners-guide-to-custom-environments-in-openai-s-gym-989371673952 for a tutorial. There are many others, you can search ""open ai gym custom environment"" for more
"
"What does ""linear unit"" mean in the names of activation functions?","
Activation functions, in neural networks, are used to introduce non-linearity. Many activation functions that are used in neural networks have the term ""Linear Unit"" in their full form. ""Linear unit"" can be abbreviated as LU.
For example, consider some activation functions

ELU  - Exponential Linear Unit
ReLU - Rectified Linear Unit
................................................

Why does the function name contain the term ""Linear Unit""? What is meant by Linear Unit here? Is it saying anything about the nature of function under consideration?
","['neural-networks', 'deep-learning', 'terminology', 'activation-functions']",
"Is my understanding on ""smooth approximation"" correct?","
Consider the following details regarding Softplus activation function

$$\text{Softplus}(x) = \dfrac{\log(1+e^{\beta x})}{\beta}$$
SoftPlus is a smooth approximation to the ReLU function and can be
used to constrain the output of a machine to always be positive.

It says that Softplus is a smooth approximation to the ReLU function. Let us consider the analytical form and plot of the RELU function.
$$\text{ReLU}(x)=(x)^+=\max(0,x)$$

The plot of Softplus function is

If we observe both plots, we can see the Softplus is almost similar to ReLU. There is a property for Softplus that ReLU does not have. ReLU is not differentiable at zero and the derivative of ReLU is also not continuous.
If we observe the behavior of Softplus, it is $n-$times continuously differentiable and hence a smooth function.
Since Softplus is both a smooth function and approximates ReLU, it is considered as a smooth approximation of ReLU.
Is my interpretation correct? if no, then what is meant by ""smooth approximation"" here?
","['comparison', 'terminology', 'definitions', 'activation-functions']",
How do I know what a good mean absolute error value is? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I have just run an MAE calculation for my machine learning models and the results show:

SVM MAE = 28.850 deg.
Random Forest MAE = 33.832 deg.

How do I know what a good MAE value is? What is the range of the MAE?
","['machine-learning', 'performance', 'metric']",
What is meant by an axis of a tensor?,"
Tensor is an ordered collection of elements. The elements are generally real numbers. Tensors are used in deep learning for storing data.
There is a wide usage of the word ""axis"" related to tensor. Axes are not the same as indices, which are used to access the elements of a tensor. An axis is not the same as an element of a tensor.
What exactly is an axis in a tensor? Is it also a (sub-)tensor obtained from the actual tensor? Or is it any other indexing mechanism? If yes, why it is used?
Suppose $a =[[1, 2, 3, 4],[5, 6, 7, 8],[9, 10, 11, 12],[13, 14, 15, 16]]$ is a tensor. Then what does axis do for $a$?
","['deep-learning', 'terminology', 'math', 'definitions', 'tensor']","
Imagine the tensor as a some generalized $n$-dimensional hyperrectangle sliced into $n$-dimensional hypercubes. Each element of the tensor is labeled by the position along the given axis, say $(x_1, x_2, \ldots)$.
Axis is not a property of tensor, rather the tensor is embedded in a $n$-dimensional space, where the axes are chosen along the sides of the hyperrectangle corresponding to the tensor.
There are many operations, that can be applied axiswise to tensor. Several examples:

Mean along the axis (choose $0$ without loss of generality). Given the $n$-dimensional tensor $x_{i_1, i_2 \ldots i_n}$ , the result will be $n-1$-dimensional tensor $\frac{1}{N_0} \sum_{i_1} x_{i_1 i_2 \ldots i_n}$, where $N_0$ is the number of elements of the tensor along the $0$-th axis (height).
Standard deviation along the axis. For each index $i_2 \ldots i_n$, calculate $\sqrt{\frac{1}{N_0} \sum_{i_1}  (\bar{x}_{i_2 \ldots i_n} - x_{i_1 i_2 \ldots i_n})^2}$, where $\bar{x}$ is the mean from previous point, and the result will be again $n-1$-dimensional tensor.

For your example $a$ is a $2$-dimensional tensor with $2$ axes. $0$-th axis corresponds to rows, $1$-st axis corresponds to columns.
"
Reinforcement learning applicable to a scheduling problem?,"
I have a certain scheduling problem and I would like to know in general whether I can use Reinforcement learning (and if so what kind of RL) to solve it. Basically my problem is a mixed-integer linear optimization problem. I have a building with an electric heating device that converts electricity into heat. So the action vector (decision variable) is $x(t)$ which quantifies the electrical power of the heating device. The device has to take one decision for every minute of the day (so in total there are $24$ hours $\times 60$ minutes $= 1440$ variables). Each of those variables is a continuous variable and can have any value between $0$ and $2000 W$.
The state space contains several continuous variables:

External varying electricity price per minute: Between $0$ Cents and $100$ Cents per kWh (amount of energy)
Internal temperature of the building: Basically between every possible value but there is a constraint to have the temperature between $20 °C$ and $22 °C$
Heat demand of the building: Any value between $0 W$ and $10.000 W$
Varying ""efficiency"" of the electrical heating device between $1$ and $4$ (depending on the external outside temperature)

The goal is to minimize the electricity costs (under a flexible electricity tariff) and to not violate the temperature constraint of the building. As stated before, this problem can be solved by mathematical optimization (mixed-integer linear program). But I would like to know if you can solve this also with reinforcement learning? As I am new to reinforcement learning I would not know how to do this. And I have some concerns about this.
Here I have a very large state space with continuous values. So I can't build a comprehensive $Q-$table as there are to many values. Further, I am not sure whether the problem is a dynamic programming problem (as most/all?) of the reinforcement problems. From an optimization point of view it is a mixed-integer linear problem.
Can anyone tell me if and how I could solve this by using RL? If it is possible I would like to know which type of RL method is suitable for this. Maybe Deep-Q-Learning but also some Monte-Carlo policy iteration or SARSA? Shall I use model-free or model-based RL for this?
Reminder: Does nobody know whether and how I can use reinforcement learning for this problem? I'd highly appreciate every comment.
Can nobody give me some more information on my issue? I'll highly appreciate every comment and would be quite thankful for more insights and your help.
",['reinforcement-learning'],
Why not make the training set and validation set one if their roles are similar?,"
If the validation set is used to tune the hyperparameters and the training set adjusts the weights, why don't they be one thing as they have a similar role, as in improving the model?
","['neural-networks', 'deep-learning', 'comparison', 'training-datasets', 'validation-datasets']","
Idea is to optimize with regards to unseen data in each step in order to avoid overfitting and data leakage so that the final network will be most generalizable to novel data.
First, you initialize your network weights randomly. For those weights, training data is unseen so network is optimized with regards to loss function that is calculated using training data. This was the first step.
Second, you would like to optimize hyperparameters, the parameters you use to train your network in first step. The network already worked hard to do its best in the first step while learning weights. If you use the same dataset in this step, it will have even more flexibility to fit even better to training data. But this will result in high variance, and network will perform poorly on unseen data.
For this reason, you split your data into train, dev and test. Train network with train data, optimize it with dev data and finally, evaluate with test data, never touching it until the very last step.
"
How to implement a rule-based decision maker for an agent-based model?,"
I had no idea that there is a stack exchange community for A.I. :-/ So I repost this question here in hope of some guidelines. I tried to delve into the materials discussed in AI: A Modern Approach course book, I am struggling to wrap my head around the model I'm trying to build without some code examples to aid me fill some gaps.
I have a hard time understanding how to combine a rule-based decision making approach for an agent in an agent-based model I try to develop.
The interface of the agent is a very simple one.
public interface IAgent
{
   string ID { get; }

   void Execute(IAgentMessage message,
                IActionScheduler actionScheduler);
}

For the sake of the example, let's assume that the agents represent Vehicles which traverse roads inside a large warehouse, in order to load and unload their cargo. Their route (sequence of roads, from the start point until the agent's destination) is assigned by another agent, the Supervisor. The goal of a vehicle agent is to traverse its assigned route, unload the cargo, load a new one, receive another assigned route by the Supervisor and repeat the process.
The vehicles must also be aware of potential collisions, for example at intersection points, and give priority based on some rules (for example, the one carrying the heaviest cargo has priority).
As far as I can understand, this is the internal structure of the agents I want to build:

So the Vehicle Agent can be something like:
public class Vehicle : IAgent
{
  public VehicleStateUpdater { get; set; }

  public RuleSet RuleSet { get; set; }

  public VehicleState State { get; set; }

  public void Execute(IAgentMessage message, IActionScheduler actionScheduler)
  {
    VehicleStateUpdater.UpdateState(VehicleState, message);
    Rule validRule = RuleSet.Match(VehicleState);
    VehicleStateUpdater.UpdateState(VehicleState, validRule);
    validRule.Fire(this, VehicleState, actionScheduler);
  }
}

For the Vehicle agent's internal state I was considering something like:
public class VehicleState
{
  public Route Route { get; set; }

  public Cargo Cargo { get; set; }

  public Location CurrentLocation { get; set; }
}

For this example, 3 rules must be implemented for the Vehicle Agent.

If another vehicle is near the agent (e.g. less than 50 meters), then the one with the heaviest cargo has priority, and the other agents must hold their position.
When an agent reaches their destination, they unload the cargo, load a new one and wait for the Supervisor to assign a new route.
At any given moment, the Supervisor, for whatever reason, might send a command, which the recipient vehicle must obey (Hold Position or Continue).

The VehicleStateUpdater must take into consideration the current state of the agent, the type of received percept and change the state accordingly. So, in order for the state to reflect that e.g. a command was received by the Supervisor, one can modify it as follows:
public class VehicleState
{
  public Route Route { get; set; }

  public Cargo Cargo { get; set; }

  public Location CurrentLocation { get; set; }

  // Additional Property
  public RadioCommand ActiveCommand { get; set; }
}

Where RadioCommand can be an enumeration with values None, Hold, Continue.
But now I must also register in the agent's state if another vehicle is approaching. So I must add another property to the VehicleState.
public class VehicleState
{
  public Route Route { get; set; }

  public Cargo Cargo { get; set; }

  public Location CurrentLocation { get; set; }

  public RadioCommand ActiveCommand { get; set; }

  // Additional properties
  public bool IsAnotherVehicleApproaching { get; set; }

  public Location ApproachingVehicleLocation { get; set; }
}

This is where I have a huge trouble understanding how to proceed and I get a feeling that I do not really follow the correct approach. First, I am not sure how to make the VehicleState class more modular and extensible. Second, I am not sure how to implement the rule-based part that defines the decision making process. Should I create mutually exclusive rules (which means every possible state must correspond to no more than one rule)? Is there a design approach that will allow me to add additional rules without having to go back-and-forth the VehicleState class and add/modify properties in order to make sure that every possible type of Percept can be handled by the agent's internal state?
The examples I've seen in the Artificial Intelligence: A Modern Approach course book and in other sources are too simple for me to ""grasp"" the concept in question when a more complex model must be designed.
I would be grateful if someone can point me in the right direction concerning the implementation of the rule-based part.
I am writing in C# but as far as I can tell it is not really relevant to the broader issue I am trying to solve.
An example of a rule I tried to incorporate:
public class HoldPositionCommandRule : IAgentRule<VehicleState>
{
    public int Priority { get; } = 0;

    public bool ConcludesTurn { get; } = false;


    public void Fire(IAgent agent, VehicleState state, IActionScheduler actionScheduler)
    {
        state.Navigator.IsMoving = false;
        //Use action scheduler to schedule subsequent actions...
    }

    public bool IsValid(VehicleState state)
    {
        bool isValid = state.RadioCommandHandler.HasBeenOrderedToHoldPosition;
        return isValid;
    }
}

A sample of the agent decision maker that I also tried to implement.
public void Execute(IAgentMessage message,
                    IActionScheduler actionScheduler)
{
    _agentStateUpdater.Update(_state, message);
    Option<IAgentRule<TState>> validRule = _ruleMatcher.Match(_state);
    validRule.MatchSome(rule => rule.Fire(this, _state, actionScheduler));
}

","['multi-agent-systems', 'rule-based-systems']","
For this question:
Is there a design approach that will allow me to add additional rules without having to go back-and-forth the VehicleState class and add/modify properties in order to make sure that every possible type of Percept can be handled by the agent's internal state?,
I think that you can implement a class called  policy or stragegy, with different subclasses, and each object could have a different strategy.
To construct a kind of mindset refering to multi-agent simulations I think that you can find some inspiration in http://cormas.cirad.fr/.
I have worked in some related problems, and it should be useful to look at https://agritrop.cirad.fr/541188/
"
Can the optimal learning rate differ for different architectures?,"
In several courses and tutorials about neural networks, people often say that the learning rate (LR) should be the first hyper-parameter to be tuned before we tweak the others. For example, in this lecture
(minute 59:55), the lecturer says that the learning rate is the first hyper-parameter that he tunes.
However, is it possible that the optimal learning rate is different for different architectures (for example, a different number of layers and neurons)? Or maybe the LR is architecture-independent and it depends only on the characteristics of the particular dataset we train our model on?
Moreover, should the LR be searched in the same process (e.g. grid-search) as the other hyper-parameters?
","['neural-networks', 'deep-learning', 'training', 'hyperparameter-optimization', 'learning-rate']","
Yes, the optimal learning rate will differ for every change you make in the network. In fact finding the optimal learning rate is very computationally expensive, so you will normally only get a rough guess anyway.
The learning rate is used to traverse an N dimensional loss landscape that changes drastically with even the smallest differences. If you add one more training data point, the optimal learning rate will change. If you add one more neuron, it will change.
What you tune first is up to you, but once you have settled on an architecture, you should tune the learning rate first because it will have the largest effect on other hyper-parameters (normally, not always). For example tuning the number of epochs to train for first will be worthless if you start changing the learning rate because the network will learn at a different speed. That's why the learning rate is usually tuned first (after the architecture).
"
"Does ""fusion"" in ""feature fusion"" has any formal definition?","
I encountered the phrase ""fusing features"" several times in the literature. I am providing an excerpt from a research paper to provide context for usage of the word fusion.

The reason is that the signals measured by multiple sensors are
disordered and correlated with multiple sources. Those methods that
are proposed with an attempt to use multiple data sources are called
data fusion techniques. Upon the position where the fusion
operation is conducted, there are three general approaches:
signal-level fusion, feature-level fusion, and decision-level fusion.

I am guessing that ""fusing features"" refers to an act of combining several features, from different domains, and then generating new features that serves the purpose of fusing.
If yes, the word ""fusion"" here refers to its common English usage

The process or result of joining two or more things together to form a
single entity.

That is,we need to combine multiple features in any manner and then coming up with new features that are good enough to perform our AI task.
Or does it have any formal definition and requirements based on the input or output features? Is there any formal definition for fusion operator?
","['terminology', 'definitions']","
With this link I could read the paper. Thanks.
So there is this discipline called sensor fusion. It is very sounded in the field of Autonomous Vehicles where in order to take one decision (whether to break or not) you have to take into account information for multiple sources: car mounted cameras, LIDAR, ultrasound, radar...
So the term ""fusion"" refers to the operation of aggregating the information from multiple sources (that has its problems as the paper says: the signals measured by multiple sensors are disordered and correlated with multiple sources). In order to perform this fusion or aggregation you can aggregate the information in different levels of the processing pipeline: close to raw data (signal fusion), close to high level information (decision fusion) or something in the middle (feature fusion).
Normally when you have a signal (image, radar, electromagenic...) you proces it somehow (normally using filters). The output of those filters is a feature map (in case of 2D images) or feature vectors (in case of 1D signals). Usually when you have those signals processed you use a decision module to extract high level information (a classification head, a SVM, a regressor...).

Basically you can aggregate information at those 3 levels. The authors refer as feature fusion as to aggregate information in the middle step. You do not have raw data, but you do not have refined data either. They do so expecting they remove the noisy part of the signals (filters) or the non relevant parts (PCA) but without removing the nuances lost when using the decision modules.
The name ""feature fusion"" comes from the deep learning terminology in which when you have a signal and you process it somehow the output of it is a feature. When reading a paper on image detection / classification you would see ""feature maps"" but when reading a paper on sensor or audio you would read ""feature vector"" hence then name
"
What are the necessary mathematical properties to be a loss function in gradient based optimizations?,"
Loss functions are used in training neural networks.
I am interested in knowing the mathematical properties that are necessary for a loss function to participate in gradient descent optimization.
I know some possible candidates that may decide whether a function can be a loss function or not. They include

Continuous at every point in $\mathbb{R}$
Differentiable at every point in $\mathbb{R}$

But, I am not sure whether these two properties are necessary for a function to become a loss function.
Are these two properties necessary?  Are there any other mathematical properties that are necessary for a function to become a loss function to participate in gradient descent optimization?
Note that this question is not asking for recommended properties for a loss function. Asking only the mandatory properties in a given context.
","['deep-learning', 'objective-functions', 'math', 'optimization', 'gradient-descent']","
Summary: the loss needs to be differentiable, with some caveats.

I will introduce some notation, which I hope is clear: if not I am happy to clarify.
Consider a neural network with parameters $\theta \in \mathbb{R}^d$, which is usually a vector of weights and biases. The gradient descent algorithm seeks to find parameters $\theta_\mathrm{min}$ which minimise the loss
function
$$\mathcal{L} \colon \mathbb{R}^d \to \mathbb{R}.$$

If this seems abstract, suppose $f(x; \theta)$ is the neural network and $S = \{(x_i, y_i)\}_{i = 1}^n$ is the training set. In binary classification we could have the loss function
$$\mathcal{L}(\theta) = \sum_{i = 1}^n \mathbb{1} \{f(x_i; \theta) \ne y_i\} $$
where $\mathbb{1}$ is the indicator function which is $1$ if the condition is satisfied and zero otherwise. I consider the loss function to be a function of
the parameters and not the data, which is fixed.

Gradient descent is performed by the update rule
$$ \theta_n \leftarrow \theta_{n - 1} - \gamma \nabla \mathcal{L}(\theta_{n - 1}),$$
yielding new parameters $\theta_n$ which should give a smaller loss $\mathcal{L}(\theta_n)$. The quantity $\gamma$ is the familiar learning rate.
The gradient descent rule requires the gradient $\nabla \mathcal{L}(\theta_{n - 1})$ to be defined, so the loss function must be differentiable. In most texts on calculus or mathematical analysis you'll find the result that if a function is
differentiable at a point $x$, it is also continuous at $x$. Obviously there is no
hope that we could perform this procedure without knowing the gradient!
In principle, differentiability is sufficient to run gradient descent. That said, unless $\mathcal{L}$ is convex, gradient descent offers no guarantees of convergence to a global minimiser. In practice, neural network loss functions are rarely convex
anyway.
I have omitted discussion on stochastic gradient descent, but it does not change the requirements for the loss function. There are alternative techniques such as the proximal gradient method for non-differentiable functions.
An unfortunate technicality I have to mention is
that, strictly speaking, if you use the $\mathrm{ReLU}$ activation function, the
neural network function $f$ becomes non-differentiable. I discuss this further in this answer. In practice we can assign
a value and ""pretend"" $\mathrm{ReLU}$ is differentiable everywhere.
"
What is the meaning of R2 appearing as a negative in the RandomForestRegressor?,"
Machine learning model was created by reading an Excel file where data was stored. I applied RandomForestRegressor to create a model that predicts the size of the sieve particles according to pressure, but the value of R2 is too large negative. I found out through googling that R2 can be negative, but I don't know what it means to have such a large negative. When I applied the same amount of different data to the designed model, R2 showed a result that was close to 1, but I don't know why this data is only large negative. RMSE and SCORE scores are good, but I don't understand if only R2 scores are bad... I would appreciate it if you could let me know what is the problem and what to consider.
My Data(Capture Image):


My Code:
import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from google.colab import drive 
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score 

drive.mount('/gdrive', force_remount=True)

data = pd.read_csv(r""/gdrive/MyDrive/Coal_Inert_Case/Inert_Case_1.csv"") 

x =data[['press1', 'press2', 'press3', 'press4', 'press5', 'press6', 'press7', 'press8', 'press9', 'press10', 'press11', 'press12', 'press13',
 'press14', 'press15', 'press16', 'press17', 'press18', 'press19', 'press20', 'press21',
 'press22', 'press23', 'press24', 'press25', 'press26', 'press27', 'press28', 'press29',
 'press30', 'press31', 'press32', 'press33', 'press34', 'press35', 'press36', 'press37',
 'press38', 'press39', 'press40', 'press41', 'press42', 'press43', 'press44', 'press45',
 'press46', 'press47', 'press48', 'press49', 'press50', 'press51', 'press52', 'press53']]

       
y = data[['Sieve 16000', 'Sieve 11000', 'Sieve 8000', 'Sieve 5600', 'Sieve 4000',
       'Sieve 2800', 'Sieve 2000', 'Sieve 1400', 'Sieve 1000', 'Sieve 710',
       'Sieve 500', 'Sieve 355', 'Sieve 250', 'Sieve 180', 'Sieve 125',
       'Sieve 90', 'Sieve 63', 'Sieve 44', 'Sieve 31', 'Sieve 0']] 

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state= 42)

forest = RandomForestRegressor(n_estimators=1000,random_state= 42) 
forest.fit(X_train, y_train) 
y_pred = forest.predict(X_test)

mse = mean_squared_error(y_test, y_pred) 
rmse = np.sqrt(mse)
r2_y_predict = r2_score(y_test, y_pred)

print(""RMSE:"", rmse)
print(""R2 : "", r2_y_predict)

The output (RMSE, R2) is:
RMSE: 0.6913667737213217
R2 :  -7294765918428.414

",['machine-learning'],
ReLU function converging to local optimum in one case and diverging in the other one,"
I implemented a simple neural network with 1 hidden layer. I used ReLU as activation function for the hidden layer and the output layer just uses the linear function.
To check my implementation I tested my neural network with following architecture:
Input Layer: 5 nodes
Hidden Layer: 2 nodes (ReLU)
Output Layer: 1 node (Linear Combination)

Learning Algorithm: Batch Gradient Descent
Error: Squared Error

I trained the neural network for 1000 times over the same input and target output:
Input: [[1, 2, 3, 4, 5], [1, 2, 3, 4, 6]]
Target Output: [[15], [16]]

I expected the network to learn the sum function. However, the network ended up learning a constant function i.e weights were all negative for first layer and the bias values were negative numbers, thus application of ReLU function to it resulted in all 0's. Thus the output was simply the bias values for output layer which was 15.5
How should I interpret the above written results? I could think of a few reasons:

Should I consider that the network converged to a local optimum?
My test dataset (synthetic) was very poor. Had there been negative numbers, I could have ended up with better results?

I tried to verify the 2nd point but it so happened that the results became no better. I used:
Input: [[1, 2, 3, 4, 5], [-1, -2, -3, -4, -6]]
Target Output: [[15], [-16]]

It so happened that the neural network was able to evaluate both the training inputs accurately i.e 15 and -16. However, still it outputs 15 for case [1, 2, 3, 4, 6] instead of expected 16 as the weights for first layer are negative.
This made me believe that my training dataset is poor but then I tried training on a 1000 random test inputs, and the results were very poor. The weights became very large. I really can't understand what the problem is. I doubt that there might be some error in my implementation.
Another observation was:
I initialized the weights and biases to optimal values i.e values that correspond to sum function:
 'W': [[ 1., -1.],
       [ 1., -1.],
       [ 1., -1.],
       [ 1., -1.],
       [ 1., -1.]]
 'b':  [0., 0.]

 'W':  [[ 1.],
        [-1.]]
 'b':  [0.]

I ran the training on that 1000 length training set but there was no effect on parameters as the error was in any case 0. Why wasn't my neural network able to learn these parameters.
For reference this is my code for neural network (hard coded for 3 layer network):
class NeuralNetwork:
    def __init__(self, layers, alpha):
        self.num_layers = len(layers) # has to be 3
        self.layers = layers
        self.alpha = alpha
        self.weights = [{'W': None, 'b': None} for i in range(self.num_layers - 1)]
        for i in range(self.num_layers - 1):
            self.weights[i]['W'] = np.array([[np.random.normal(0, np.sqrt(2/layers[i])) for ii in range(layers[i+1])] for jj in range(layers[i])])
            self.weights[i]['b'] = np.array([np.random.normal(0, np.sqrt(2/layers[i])) for ii in range(layers[i+1])])
    
    def evaluate(self, input_feature):
        psi = input_feature @ self.weights[0]['W'] + self.weights[0]['b']
        x = np.maximum(psi, 0)
        y = x @ self.weights[1]['W'] + self.weights[1]['b']
        return y
    
    def update_weights(self, training_input, target_output):
        training_output = self.evaluate(training_input)
        
        dely = target_output - training_output
        
        db1  = np.sum(dely, axis = 0)
        dw1  = np.sum(a*dely, axis = 0).T
        
        da   = dely @ (self.weights[1]['W'].T)
        z    = training_input @ self.weights[0]['W'] + self.weights[0]['b']
        dz   = np.maximum(z, 0) * da
        
        db0  = np.sum(dz, axis = 0).T
        dw0  = training_input.T @ dz
        
        self.weights[0]['W'] += self.alpha * dw0
        self.weights[0]['b'] += self.alpha * db0
        self.weights[1]['W'] += self.alpha * dw1
        self.weights[1]['b'] += self.alpha * db1

","['neural-networks', 'machine-learning', 'backpropagation', 'feedforward-neural-networks', 'relu']",
Positional Encoding in Transformer on multi-variate time series data hurts performance,"
I set up a transformer model that embeds positional encodings in the encoder. The data is multi-variate time series-based data.
As I just experiment with the positional encoding portion of the code I set up a toy model: I generated a time series that contains the log changes of a sine function and run a classification model that predicts whether the subsequent value is positive or negative. Simple enough. I also added a few time series with random walks to try to throw off the model.
Predictably, the model very quickly reaches a categorical accuracy of around 99%. Without positional encoding that happens already in the 3rd epoch. However, with positional encoding (I use the same implementation as proposed in the ""Attention is all you need"" paper), it takes over 100 epochs to reach a similar accuracy level.
So, clearly, all else being equal, learning with positional encoding takes much longer to reach an equal accuracy level than without positional encoding.
Has anyone witnessed similar observations? Apparently adding the positional encodings to the actual values seems to confuse the model. I have not tried concatenations yet. Any advice?
Edit: Or does it simply mean that learned positional encodings perform better than sin/cos encodings? I have not made any special provisions to encourage learned positional encodings, I simply either added the positional encodings to the actual values or I did not.
","['tensorflow', 'pytorch', 'transformer', 'time-series', 'positional-encoding']",
An online editor that allows data labeling format [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I have a set of students (~20) that will work on annotating data for an NLP project.
The annotation task will be as in the following:
text: I like this piza place.
label: [pos, neg]
comments: 
text fluency: [1,2,3,4,5]

The students will need to correct the text first (e.g. correcting piza word), and then fill the fields below.
Is there an online solution to add the data in this format and then to share the link with the students?
I tried to do this in Google forum, but I wasn't able to; I don't know actually if it's possible there.
I am looking for a solution that can allow saving the edits after annotating # instances, as there are many instances and the students won't be able to annotate everything at once. I know that a good solution would be building a website, but I am looking for something that already exists.
","['natural-language-processing', 'datasets', 'data-labelling']",
Recursive Least squares (RLS) for mini batch,"
For my application I am considering a learning problem where I simulate a bunch of episodes say '$n$' first, and than carry out the recursive least squares update. Similar to $TD(1)$.
I know that RLS can be used to update parameters being learned as they arrive. This can be done efficiently for single data point and the derivations are easily available online and also easy to understand.
However for my case I am looking for same equations when data arrive as a mini batch and not a single data point at a time. I could not find any material regarding RLS for mini batches.
According to my understanding the same equations can be also used by appropriately considering matrix dimensions. However I do not know if this is valid.
What are the alternatives to be used?
","['reinforcement-learning', 'function-approximation', 'weights', 'temporal-difference-methods', 'finite-markov-decision-process']",
How to create a neural network from a set of equations?,"
Say I have these equations:
$$x_1 = x_2 + 2y_1 + b$$
$$x_2 = y_2 + c$$
$$y_1 = z + a$$
$$y_2 = y_3 + d$$
$$z = z_1 + e$$
$x_1$ depends on $x_2$ (depends on $y_2$ (depends on $y_3$)) and $y_1$ (depends on $z$ (depends on $z_1$)).
$x_1$ is my final equation and $y_3$ and $z_1$ are my initial variables.
How do I represent them in a neural network? My final aim is to backtrack from $x_1$, and see what change of an amount $n$ in $x_1$ resulted from which of $y_3$ or $z_1$.
All these variables are item prices in the real world.
My inputs are $z_1$ and $y_3$ and my output is $x_1. z_1$ and $y_3$ are prices and the final output $x_1$ is also a price.
","['neural-networks', 'machine-learning', 'backpropagation']","
All modern frameworks for deep learning (PyTorch, Jax, Tensorflow) support automatic differentation. These operations can be easily implemented. Here I write, how it would look like in PyTorch:
class Net(nn.Module):

    def __init__(self):
         super().__init__()

         self.a = nn.Parameter(torch.randn(1))
         self.b = nn.Parameter(torch.randn(1))
         self.c = nn.Parameter(torch.randn(1))
         self.d = nn.Parameter(torch.randn(1))
         self.e = nn.Parameter(torch.randn(1))

   def forward(self, z1, y3):
       z = z1 + self.e
       y2 = y3 + self.d
       y1 = z + self.a
       x2 = y2 + self.c
       x1 = x2 + 2 * y1 + self.b
       return x1

And the use case is the following, say:
net = Net()
net(torch.ones(1), 2 * torch.ones(1))

"
Does regularization just mean using an augmented loss function?,"
We need to use a loss function for training the neural networks.
In general, the loss function depends only on the desired output $y$ and actual output $\hat{y}$ and is represented as $L(y, \hat{y})$.
As per my current understanding,

Regularization is nothing but using a new loss function
$L'(y,\hat{y})$ which must contain a $\lambda$ term (formally called
as regularization term) for training a neural network and can be
represented as
$$L'(y,\hat{y}) = L(y, \hat{y}) + \lambda \ell(.) $$
where $\ell(.)$ is called regularization function. Based on the
definition of function $\ell$ there can be different regularization
methods.

Is my current understanding complete? Or is there any other technique in machine learning that is also considered a regularization technique? If yes, where can I read about that regularization?
","['machine-learning', 'reference-request', 'terminology', 'regularization']","
Also, keep in mind that not just any augmentation of the loss function is a regularization.
For example, you can add terms to a loss function that enforce constraints on the solution but do not prevent overfitting nor facilitate generalization.
"
"Is there any difference between ""image generation"" and ""image synthesis""?","
Generative Adversarial networks (aka GANs) are used for image generation. The phrase image synthesis is also used in literature.
I know that the phrase image generation stands for

An act of generating an image

The formal definition for image synthesis is given by

Image synthesis is the process of artificially generating images that
contain some particular desired content.

The only difference I can notice is that image synthesis is a focused image generation. The focus is on the parts of the image generated.
But, I have an issue with the word synthesis. The word ""synthesis"" has the following meanings

The combination of components or elements to form a connected whole. Often contrasted with analysis
The production of chemical compounds by reaction from simpler materials.
(in Hegelian philosophy) the final stage in the process of dialectical reasoning, in which a new idea resolves the conflict between thesis and antithesis.

It gives me a sense that I need to use the phrase ""image synthesis"" if I am generating an image by combining some simpler elements, which is not exactly the same as the focused sense given in the formal definition of image synthesis.
Why does the word ""synthesis"" is used in the phrase ""image synthesis""? Are we synthesizing/combining anything that does not happen in other variants of image generation?
","['comparison', 'terminology', 'image-generation']",
Is reconciling shape discrepancies the only purpose of padding?,"
Padding is a technique used in some of the domains of artificial intelligence.
Data is generally available in different shapes. But in order to pass the data as input to a model in deep learning, the model allows only a particular shape of data to pass through it. And hence there is a need to allow padding in case if the input data shape contains dimensions that are less than the dimensions of the input of the model under consideration. For example, we pad input sentences in RNN to match the input shape of the RNN model. Sometimes we pad the input data in order to make a desired shape output. For example, padding is used in convolution operation to keep the size of feature maps intact.
Is handling this type of shape issues is the only purpose of padding? If no, what are the other purposes of padding that are not related to the shaping requirements of data?
","['neural-networks', 'convolutional-neural-networks', 'recurrent-neural-networks', 'padding']","
Learning ""border effects"" is another reason to use padding at least in convolutional neural networks. This paper specifically looks at 2D CNNs for image processing. In my experience, I use pre-padding with 1D CNNs for NLP so my model can learn morphological affixes.
"
What to do when model stops learning after some epochs,"
I am training a segmentation model on 3D data, after around 170 epochs which took around 4 days, I notice the model is no more learning and the dice score is at 0.51. What is the best approach at this point to keep model learning?
Learning rate: 1e-4
Batch size: 6
optimizer: adamw
loss: generalized dice loss
PS: there are more augmentation in training data than validation data, this might be the reason, the validation loss curve is below training loss curve


","['machine-learning', 'deep-learning', 'semantic-segmentation']",
"What do equations 1 and 3 describe in the ""Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"" paper?","
This paper uses image augmentation to improve RL algorithms. It contains the following paragraph -
""Our approach, DrQ, is the union of the three separate regularization mechanisms introduced above:

transformations of the input image (Section 3.1).
averaging the Q target over K image transformations (Equation (1)).
averaging the Q function itself over M image transformations (Equation (3)).""

I do not understand how part 2 and 3 (Equation 1 and 3) and would highly appreciate some detailed elaboration on it.
Here are the equations -


","['reinforcement-learning', 'deep-learning', 'deep-rl', 'image-processing']",
Is there a notion of exploration-exploitation tradeoff in dynamic programming (or model-based RL)?,"
Is there a notion of exploration-exploitation tradeoff in dynamic programming (or model-based RL)?
","['model-based-methods', 'exploration-exploitation-tradeoff', 'dynamic-programming']","
I think there is an implicit notion of it in dynamic programming; say, if you have to make some sort of search over a subset of a state space and you are deciding whether to use BFS, breath first search, or DFS, depth first search, you are at least implicitly thinking on the best way to explore/exploit the state space.
As for model based RL, yes. There is explicit algorithms that mediate exploration and exploitation. One of them is UCB, uper confidence bound. One of the best examples of a model based reinforcement learning algorithm is AlphaGo. The algorithm uses a variation of UCB to explore the state space.
"
Feeding the output back to input in 3D CNN model,"
I am currently designing a Model which takes Input 3D Grid and Model Output at $t-1$. The model figure is described below

I have two thoughts in training the model for above situation.

Feed output $t-1$ from ground truth with some noise. And, during testing feed output of the model as previous output at $t-1$. Maybe we can fine tune with model output when training loss is sufficiently low.

Feed model output to next stage as $t-1$ output. But I am not sure if this works.


The situation is similar to RNNs but I am using 3D CNN’s here. I don’t know if RNN can be used here. How can I train such a model.
","['deep-learning', 'convolutional-neural-networks', 'training', 'recurrent-neural-networks', 'pytorch']",
Not able to find a good fit for a simple function with neural networks,"
I have been trying to adjust a neural network to a simple function: the mass of an sphere.
I have tried with different architectures, for example, a single hidden layer and two hidden layers, always with 128 neurons each, and training them for 5000 epochs.
The code is the usual one. Just in case, I publish one of them
model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])
                        ,keras.layers.Dense(128, activation=""relu"")
                        ,keras.layers.Dense(1, activation=""relu"")])
model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
history = model.fit(x, y, validation_split=0.2, epochs=5000)

The results are shown in the graphs.


I suspect that I am making an error somewhere, because I have seen that deep learning is able to match complex functions with much less epochs. I shall appreciate any hint to fix this problem and obtain a good fit with the deep learning function.
In order to make it clear I post the graph's code.
rs =[x for x in range(20)]
def masas_circulo(x):
    masas_circulos =[]
    rs =[r for r in range(x)]
    for r in rs:
        masas_circulos.append(model.predict([r])[0][0])

   return masas_circulos

 masas_circulos = masas_circulo(20) 
 masas_circulos
 esferas = [4/3*np.pi*r**3 for r in range(20)]
 import matplotlib.pyplot as plt
 plt.plot(rs,masas_circulos,label=""DL"")
 plt.plot(rs,esferas,label=""Real"");
 plt.title(""Mass of an sphere.\nDL (1hl,128 n,5000 e) vs ground_truth"")
 plt.xlabel(""Radius"")
 plt.ylabel(""Sphere"")
 plt.legend();

",['deep-learning'],"
A simpler model seems to be the best option
model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer='sgd', loss='mean_squared_error')
xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys2 = np.array([4/3*r*np.pi**3 for r in xs])    
model.fit(xs, ys2, epochs=500,validation_split=0.2)

def masa_circulo(x):
    return 4/3*x*np.pi**3

Testing it graphically
x = [x for x in range(1,int(1e6),int(1e3))]
y_masa_circulo = [ masa_circulo(m) for m in x]
y_masa_predicha= [model.predict([m])[0] for m in x]

import matplotlib.pyplot as plt
fig,axes = plt.subplots(1,2)
axes[0].plot(y_masa_circulo);
axes[0].set_title(""y_masa_circulo"")
axes[0].set_ylabel(""y_masa_circulo"")
axes[0].set_xlabel(""Radio"")

axes[1].plot(y_masa_predicha);
plt.title(""y_masa_circulo"")
plt.title(""y_masa_predicha"");
axes[1].set_ylabel(""y_masa_predicha"");
axes[1].set_xlabel(""Radio"");


No need of scaling the data.
"
Batch normalization for multiple datasets?,"
I am working on a task of generating synthetic data to help the training of my model. This means that the training is performed on synthetic + real data, and tested on real data.
I was told that batch normalization layers might be trying to find weights that are good for all while training, which is a problem since the distribution of my synthetic data is not exactly equal to the distribution of the real data. So, the idea would be to have different 'copies' of the weights of batch normalization layers. So that the neural network estimates different weights for synthetic and real data, and uses just the weights of real data for evaluation.
My question is, how to perform batch normalization in the aforementioned case? Is there already an implementation of batch norm layers in PyTorch that solves the problem?
","['deep-learning', 'datasets', 'pytorch', 'batch-normalization', 'data-augmentation']",
How will actual labels be matched with predicted labels when LSTM discards data even from current time stamp input data?,"
I read the tutorial of LSTM from here. However, I have certain doubts that I need to address.

Since we use true labels and do not remove anything from the original data, then how is it possible for the LSTM model's predicted output to match the real labels as it throws data?

And how do we determine the number of output neurons?


According to my understanding, in word-to-word prediction, one cell's outputs are the number of words (exiting in vocabulary).
","['deep-learning', 'natural-language-processing', 'long-short-term-memory']",
Is there any existing mechanism that allows us to pass input from randomly selected layers of neural network per iteration?,"
Consider the following neural network with $\ell$ layers.
$$i_0 \rightarrow h_1  \rightarrow h_2 \rightarrow h_3 \cdots \rightarrow h_{\ell-1}  \rightarrow o_{\ell} ,$$
where $i, h, o$ stands for input, hidden and output layer respectively.
In general, an input passes from $i_0$ to $o_{\ell}$, which is known as the forward pass. And then the weight updating happens from $o_{\ell}$  to $i_0$ which is called backward pass.
I want to know whether the following mechanism exists in literature assuming that all layers have the same input and output dimensions.
For each iteration

Select a subset $L  \subseteq \{0, 1, 2, 3, \cdots, \ell\}$ randomly.
Input passes through layers whose indices are present in $L$ only i.e, forward pass happens by dropping some layers.
Update weights for layers whose indices are in $L$ i.e., update the weights of layers which are participated in step (2).

What is the name of the technique mentioned above, if it is present in literature?
","['neural-networks', 'training', 'reference-request', 'forward-pass']",
Isn't attention mask for BERT model useless?,"
I have just dived into deep learning for NLP, and now I'm learning how the BERT model works. What I found odd is why the BERT model needs to have an attention mask. As clearly shown in this tutorial https://huggingface.co/transformers/glossary.html:
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(""bert-base-cased"")

sequence_a = ""This is a short sequence.""
sequence_b = ""This is a rather long sequence. It is at least longer than the sequence A.""

encoded_sequence_a = tokenizer(sequence_a)[""input_ids""]
encoded_sequence_b = tokenizer(sequence_b)[""input_ids""]

padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)

Output of padded sequences input ids:
padded_sequences[""input_ids""]

[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]

Output of padded sequence attention mask:
padded_sequences[""attention_mask""]
[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]

In the tutorial, it clearly states that an attention mask is needed to tell the model (BERT) which input ids need to be attended and which not (if an element in attention mask is 1 then the model will pay attention to that index, if it is 0 then model will not pay attention).
The thing I don't get is: why does BERT have an attention mask in the first place? Doesn't model need only input ids because you can clearly see that attention_mask has zeros on the same indices as the input_ids. Why does the model need to have an additional layer of difficulty added?
I know that BERT was created in google's ""super duper laboratories"", so I think the creators had something in their minds and had a strong reason for creating an attention mask as a part of the input.
","['deep-learning', 'natural-language-processing', 'attention', 'bert']","
This is just an implementation issue. One reason is the Huggingface implementation (which is not the original implementation by Google) wants to strictly separate the tokenization from the modeling. It is a convention that the input sequences are zero-padded, but in theory, it does not have to be so. In the Huggingface implementation, you use a different tokenizer that would pad the sequences with different numbers and still get valid masking.
You are right that you can infer the mask from the input IDs at the very beginning (if you know the pad ID), but you need to explicitly use the mask in every single layer. Each layer returns a 3D tensor of floats from which you cannot say what the padded positions are, you need to have the explicit mask when calling the next layer. I guess that having the mask everywhere makes the API more consistent.
"
Is my understanding about the number of iterations correct?,"
Per google machine-learning glossary, when I have 100 training examples and update my model for each training example, if I train my model 5 epochs without early-stop, there are 500 iterations in total, is my understanding correct?
",['terminology'],
Why is the exponential loss used in this case?,"
I am reading the paper Tracking-by-Segmentation With Online Gradient Boosting Decision Tree. In Section 2.1, the paper says

Given training examples, $\left\{\left(\mathbf{x}_{i}, y_{i}\right) \mid \mathbf{x}_{i} \in \mathbb{R}^{n}\right.$ and $y_{i} \in$ $\mathbb{R}\}_{i=1: N}, f(\cdot)$ is constructed in a greedy manner by selecting parameter $\theta_{j}$ and weight $\alpha_{j}$ of a weak learner iteratively to minimize an augmented loss function given by
$$
\mathcal{L}=\sum_{i=1}^{N} \ell\left(y_{i}, f\left(\mathbf{x}_{i}\right)\right) \equiv \sum_{i=1}^{N} \exp \left(-y_{i} f\left(\mathbf{x}_{i}\right)\right)
$$
where an exponential loss function is adopted ${ }^{1}$. The greedy optimization procedure is summarized in Algorithm 1.

I cannot understand the exponential loss function. In my opinion, the loss function should get the smallest value when $y_i=f(x_i)$. But the loss function in the image obtains a smaller value if $(-y_i f(x_i))$ becomes smaller.
","['machine-learning', 'papers', 'objective-functions', 'decision-trees', 'gradient-boosting']",
Is there a way to select the subset of most important features using PCA?,"
Is there a way to select the most important features using PCA? I am not looking for the principal components with the highest scores but a subset of the original features.
","['machine-learning', 'unsupervised-learning', 'feature-selection', 'principal-component-analysis']","
There are better methods for selecting most important features in supervised setting. Assuming they are not an option, or you're simply interested in PCA:
Say you originally had 100 features and you applied PCA and first 10 PCs explains the 95 % of ratio.
After applying PCA, you can calculate linear correlations between top 10 PCs and original features. I assume some of your features will be highly correlated with some subset of top 10 PCs. You can draw an abstract line and choose subset of original features that are at least 0.80 linearly correlated with at least one of top 10 PCs.
"
Why doesn't anyone use reinforcement learning to find the best possible alternative to backpropagation?,"
To be clear, I'm very uninformed on the topic of alternative learning algorithms to backprop, all my knowledge comes from articles like these:
lets-not-stop-at-backprop
backprop-alternatives
we-need-a-better-learning-algorithm. I also don't know exactly how you would arrange a system to find the best learning algorithm it can, or if it's even possible to make something like that with reinforcement learning.
I was thinking that you could take a system and have it generate neural nets in the space of all neural nets and generate rules for how to deal with the weights in the network, and then just let the system run trying to find the best possible arrangement of neurons and training rules such that it can learn how to do x thing very very fast, with very little training.
Is this something that has already been tried or something that isn't possible?
","['neural-networks', 'reinforcement-learning', 'backpropagation', 'hyperparameter-optimization', 'meta-learning']",
"In Computer Vision, what is the difference between a transformer and attention?","
Having been studying computer vision for a while, I still cannot understand what the difference between a transformer and attention is?
","['computer-vision', 'comparison', 'transformer', 'attention']",
What is the advantage of RL compared with my simple classic algorithm for the MountainCarEnv?,"
What is the advantage of RL compared with the following simple classic algorithm for the MountainCarEnv? Considering that it takes a long time to train the agent just to achieve this simple task?
import gym

envName = 'MountainCar-v0'
env = gym.make(envName)

x, v = state = env.reset()
done = False
maxPotential = False
steps = 0

def computeAction(state):
    global x, v, maxPotential, steps
    xNew, vNew = state
    action = 1
    if xNew < -1.1:
        maxPotential = True
    if not maxPotential:
        if xNew < x: action = 0
        else: action = 2
    else:
        action = 2
    x, v = xNew, vNew
    steps += 1
    return action

while not done:
    state, reward, done, info = env.step(computeAction(state))
    env.render()

print('steps', steps)


result: around 100 steps

","['reinforcement-learning', 'comparison']","
If your goal is to create a controller for the mountain car problem, and you have access to the model, then RL probably offers no advantage over your code. I am saying probably, because I am taking you at your word that the code performs well over multiple tests, and it doesn't matter too much if it does not because there are many equivalent solutions based on analysis of the original problem.
This is the difference:

RL techniques find solutions to control problems. Model-free RL techniques can do so without access to the environment model.

The classic control code is a solution to a given control problem, found by the code author through analysis of the problem.


The same RL agent that could solve mountain car, could solve similar environments with same state and action space, e.g. an environment similar to mountain car but with multiple hills and valleys, or with alterations to the physics model. The same classic control code would fail and need to be re-written for the new environment.
The mountain car problem is interesting in control theory because it introduces a level of abstraction - the simplest feedback-based control algorithms will fail because moving directly towards the goal does not work. However, it is still a toy problem. The solutions are well understood, and no-one needs to solve it again. Solving it with RL is not necessary, it is a demonstration of learning something through trial and error.
As control problems become more complex, with multiple levels of goals to solve, then classic control approaches become more unweidly. For example, a walking robot has many more variables to manage, and walking systems are more likely to benefit from automated search for the best controllers as opposed to analysis and classic control at all levels.
"
Are there any stats available on the usage of libraries by deep learning researchers?,"
I know three Python libraries that are popular in deep learning research community: Keras, PyTorch, Tensorflow.  I don't know much about Theano.
This question is not about the efficiency, flexibility or ease of the library for its users. This question is about the usage of the library by the deep learning (academic, research) community.
Which library is used by most of the contemporary researchers? Is there any comparison or stats available among the libraries, based on GitHub implementations or by some other means?
","['deep-learning', 'tensorflow', 'reference-request', 'keras', 'pytorch']","
Something that I personally use is Google Trends. This is a very useful tool for verifying the interest of a broad public on some subject. Results can even be refined to  include region and/or time span.
For instance, here you can see a comparison for the interest in Tensorflow, Keras and Pytorch over the past 12 months:

"
What is the fundamental difference between max pooling and adaptive max pooling used in PyTorch,"
PyTorch provides max pooling and adaptive max pooling.
Both,  max pooling and adaptive max pooling, is defined in  three dimensions: 1d, 2d and 3d. For simplicity, I am discussing about 1d in this question.
For max pooling in one dimension, the documentation provides the formula to calculate the output.

In the simplest case, the output value of the layer with input size
$(N,C,L)$ and output $(N,C,L_{out})$ can be precisely described as:
$$out(N_i,C_j,k) = \max\limits_{⁡m=0, \cdots ,kernel\_size−1} input(N_i,C_j,stride×k+m)$$

But, adaptive max pooling has no detailed explanation in the documentation.
What is the fundamental difference between max pooling and adaptive max-pooling? max-pooling expects kernel_size and stride as input but adaptive max-pooling does not expect them as inputs and asks only for output size, does it uses kernel and stride for performing the operation? If yes, how does it calculate both?
","['pytorch', 'filters', 'max-pooling', 'stride']",
Is there any metric for calculating how natural a single image is given a dataset of the same class images?,"
Suppose there is a dataset $D$ of images. We have enough number $n$ of images in the dataset and all the images are of a single class.
Suppose I generated a new image $I$, which is not present in the given dataset, of the same class using a generator neural network. I want to calculate how natural the image $I$ is wrt the dataset $D$
$m(I, D) = $ how natural the image $I$ with respect to dataset $D$ of images.
I don't want metrics that are applied to a bunch of generated images. I have only one generated image.

I came up with a naive metric
$m(I, D) = \sum\limits_{x \in D} (x-I)^2 $
where $x-I$, difference between two images, is defined as the sum of pixel differences of both the images i.e., $$x-I = \sum\limits_{x_i \in x, I_i \in I} \|x_i - I_i\|$$
But, this measure shows how similar the new image $I$ w.r.t is to the set of images in my dataset at the pixel level. I want a measure of how natural it is.
","['image-generation', 'metric', 'similarity']","
Evaluating synthetically generated images is challenging and an active area of research. The problem is that the ""how natural is an image""-task is not well-defined and subjective.
To evaluate generated images we can define two abstract properties: fidelity and diversity, as we want to generate not only a single high-quality image, but also different ones from the domain.
There are several methods for automating and standardizing the evaluation of generated samples, such as Inception Score (IS) and Fréchet Inception Distance (FID). Both approaches utilize a CNN classification model (typically Inception-v3), that is pretrained on the entire dataset.
We can then use this pretrained model to classify generated images and calculate the distribution of predicted classes, which should be uniformly distributed for high diversity, and the distribution of predictions on a single class will represent the fidelity. However, this approach does not capture how synthetic images compare to real images.
Instead of comparing images pixel-wise, we can compare their abstract features. CNNs are known to be good at extracting abstract features, so we can use a pretrained CNN for extracting a feature embedding from one of the last hidden layers. After this, we can compare the Euclidean or cosine distance between various embeddings, for instance. The better way to compare the similarity between generated and real images is FID. Here is an article on the topic for more details.
"
Is there any closed form analytical expression to represent fractional max pooling?,"
There are Nineteen types of pooling layers in PyTorch.
Almost all of the layers are provided with corresponding analytical formulae. But analytical formulae are not provided for the fractional max-pooling layers. Instead, they provided this research paper to understand about fractional max pooling. So, I am thinking that it may be complex for a newcomer to understand about fractional max pooling.
Is there any closed analytical formulae available for fractional max pooling like most of the other pooling layers? If no, is there any simple pseudo-code or visuals (diagram or animation) available for this layer?
","['math', 'pytorch', 'resource-request', 'pooling']",
Time series forecasting for multiple objects with common features,"
I know the title of this question may raise an eyebrow, but I can't find the technical terms to define or investigate the actual problem.
To demonstrate my problem with a simple hypothetical scenario: Let's say you have dataset pretraining to fruits!

The dataset contains $N$ fruits

Each fruit has properties ${\{p}\}$, for example, $p_1$ is type, $p_2$ is color, $p_3$ flavour. It is important to note that (i) these properties are communal across all fruits (all fruits have the properties above) and (ii) these properties are constant for each individual fruit (for a fruit,${\{p}\}_n$ stays constant over time).

Each fruit has a time series $\{W_t\}_n$ which relate to, for example, measured weight over time. It is important to note that the fruits aren't measured at regular, or the same intervals. Therefore, each fruit in the dataset will have a different  weight time series.

Therefore the aggregated dataset will have $\sum_{n=1}^{N} dim(\{W_t\}_n)$ observations

Let's assume there is some hidden correlation between the weight for a fruit $\{W_t\}_n$ over time and the fruit properties ${\{p}\}$.


So the problem is: What model(s) can we use that it is able to predict the next weight values $\{W_{(t+1)}\}$? More formally stated $f(\{p\}_n,\{W_t\}_n) = \{W_{(t+1)}\}_n$ ?
The challenges here is:

We want to maintain the 'uniqueness' of each fruit, that is, we can't simply say if two fruits have the same properties ${\{p}\}$ then they will have the same weight changes over time. To conceptualize this, imagine things happen to certain fruits during their life time, the model is supposed to remember this has happened to those specific fruits and incorporate that into the prediction.
Our measurement device was bought at IKEA and sometimes it provides inaccurate readings, so we can't expect a linear or smooth weight time series per fruit.
We don't have a lot of weight measurements, let's say 10 on average, but we have a lot of fruits, let's say 100 000.

I have some experience with vanilla and stacked LSTM's. However, I struggle to consolidate my understanding of LSTM's in the abovementioned scenario.
Thank you for reading. I hope this will get the creative juices flowing, or give you a fun mental challenge at least.
","['machine-learning', 'long-short-term-memory', 'time-series', 'forecasting']",
Methodologies for passing the best samples for a neural network to learn,"
Just an idea I am sure I read in a book some time ago, but I can't remember the name.
Given a very large dataset and a neural network (or anything that can learn via something like stochastic gradient descent, passing a subset of samples to modify the model, as opposed to learning from the whole dataset at once), one can train a model for, say, classification.
The idea was a methodology for selecting the samples that would make the model learn the most from, so you can spare the network from learning from examples that would make the model make only small changes, reducing computing time.
I guess an easy methodology would pick at first a sample that is similar to a previous one but with another label, and pick the most similar on features and label samples at last. Does that make sense?
Is there a googleable keyword for what I am talking about?
","['neural-networks', 'stochastic-gradient-descent', 'batch-learning']",
how produce image of face working with AI [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I came across a https://generated.photos/ site that claims to produce images entirely by artificial intelligence.
My question is how does this program work? What mechanism and libraries should I use if I want to do a project like this?
",['image-processing'],"
According to the information from the site:

We have built a proprietary dataset by taking tens of thousands of
images of people in our studio. These photos are taken in a controlled
environment allowing us to make sure that each face has consistent
look and quality. After shooting, photos are tagged, categorized, and
added to a dataset that is used for machine learning training. In an
on-going fashion we feed this dataset into generative adversarial
networks to produce faces that have never existed. Further machine
learning processes take place after the faces are created in order to
identify and remove flawed faces. The final results are made available
through our website or API integration.

The algorithm they use is a generative adversarial network. A sota architecture today is StyleGAN2 or, a new version of it, Alias-Free GAN.
Here is a live demo of StyleGAN2: https://thispersondoesnotexist.com/
And here is the official PyTorch implementation: https://github.com/NVlabs/stylegan2
"
Is it possible to use deep learning to generate a 2D image from a few numerical values?,"
Is it possible to train a DL model that will generate a full resolution 2D image based on few numbers describing this image and what type of model or architecture would that be?

What I want to achieve is that I deliver to the model some numbers for example describing positions of objects on the screen and number describing how lit the scene is and I get back a 2D image with objects in their correct positions and proper lighting, but for one set of input data values I will get always one same image (see image above). These input data also could be anything else than positions and lighting, these are only examples helping to visualize what I mean.
This all, of course, assuming that I have a lot of annotated training data that consists of images and labels of the objects' positions and scene lighting values.
EDIT: The final model would be trained on real images taken from Full HD camera, not some simple shapes like presented here, that I did only to explain better my question.
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'generative-model', 'image-generation']",
Why doesn't this Monte Carlo Tree Search algorithm work properly?,"
PROBLEM
I'm writing a Monte-Carlo tree search algorithm to play chess in Python. I replaced the simulation stage with a custom evaluation function. My code looks perfect but for some reason acts strange. It recognizes instant wins easily enough but cannot recognize checkmate-in-2 moves and checkmate-in-3 moves positions. Any ideas?
WHAT I'VE TRIED
I tried giving it more time to search but it still cannot find the best move even when it leads to a guaranteed win in two moves. However, I noticed that results improve when I turn off the custom evaluation and use classic Monte Carlo Tree Search simulation. (To turn off custom evaluation, just don't pass any arguments into the Agent constructor.) But I really need it to work with custom evaluation because I am working on a machine learning technique for board evaluation.
I tried printing out the results of the searches to see which moves the algorithm thinks are good. It consistently ranks the best move in mate-in-2 and mate-in-3 situations among the worst. The rankings are based on the number of times the move was explored (which is how MCTS picks the best moves).
MY CODE
I've included the whole code because everything is relevant to the problem. To run this code, you may need to install python-chess (pip install python-chess).
I've struggled with this for more than a week and it's getting frustrating. Any ideas?
import math
import random
import time

import chess
import chess.engine


class Node:

    def __init__(self, state, parent, action):
        """"""Initializes a node structure for a Monte-Carlo search tree.""""""
        self.state = state
        self.parent = parent
        self.action = action

        self.unexplored_actions = list(self.state.legal_moves)
        random.shuffle(self.unexplored_actions)
        self.colour = self.state.turn
        self.children = []
        
        self.w = 0 # number of wins
        self.n = 0 # number of simulations

class Agent:
    
    def __init__(self, custom_evaluation=None):
        """"""Initializes a Monte-Carlo tree search agent.""""""
        
        if custom_evaluation:
            self._evaluate = custom_evaluation

    def mcts(self, state, time_limit=float('inf'), node_limit=float('inf')):
        """"""Runs Monte-Carlo tree search and returns an evaluation.""""""

        nodes_searched = 0
        start_time = time.time()

        # Initialize the root node.
        root = Node(state, None, None)

        while (time.time() - start_time) < time_limit and nodes_searched < node_limit:
            
            # Select a leaf node.
            leaf = self._select(root)

            # Add a new child node to the tree.
            if leaf.unexplored_actions:
                child = self._expand(leaf)
            else:
                child = leaf

            # Evaluate the node.
            result = self._evaluate(child)

            # Backpropagate the results.
            self._backpropagate(child, result)

            nodes_searched += 1

        result = max(root.children, key=lambda node: node.n) 

        return result

    def _uct(self, node):
        """"""Returns the Upper Confidence Bound 1 of a node.""""""
        c = math.sqrt(2)

        # We want every WHITE node to choose the worst BLACK node and vice versa.
        # Scores for each node are relative to that colour.
        w = node.n - node.w

        n = node.n
        N = node.parent.n

        try:
            ucb = (w / n) + (c * math.sqrt(math.log(N) / n))
        except ZeroDivisionError:
            ucb = float('inf')

        return ucb

    def _select(self, node):
        """"""Returns a leaf node that either has unexplored actions or is a terminal node.""""""
        while (not node.unexplored_actions) and node.children:
            # Pick the child node with highest UCB.
            selection = max(node.children, key=self._uct)
            # Move to the next node.
            node = selection
        return node

    def _expand(self, node):
        """"""Adds one child node to the tree.""""""
        # Pick an unexplored action.
        action = node.unexplored_actions.pop()
        # Create a copy of the node state.
        state_copy = node.state.copy()
        # Carry out the action on the copy.
        state_copy.push(action)
        # Create a child node.
        child = Node(state_copy, node, action)
        # Add the child node to the list of children.
        node.children.append(child)
        # Return the child node.
        return child

    def _evaluate(self, node):
        """"""Returns an evaluation of a given node.""""""
        # If no custom evaluation function was passed into the object constructor, 
        # use classic simulation.
        return self._simulate(node)

    def _simulate(self, node):
        """"""Randomly plays out to the end and returns a static evaluation of the terminal state.""""""
        board = node.state.copy()
        while not board.is_game_over():
            # Pick a random action.
            move = random.choice(list(board.legal_moves))
            # Perform the action.
            board.push(move)
        return self._calculate_static_evaluation(board)

    def _backpropagate(self, node, result):
        """"""Updates a node's values and subsequent parent values.""""""
        # Update the node's values.
        node.w += result.pov(node.colour).expectation()
        node.n += 1
        # Back up values to parent nodes.
        while node.parent is not None:
            node.parent.w += result.pov(node.parent.colour).expectation()
            node.parent.n += 1
            node = node.parent

    def _calculate_static_evaluation(self, board):
        """"""Returns a static evaluation of a *terminal* board state.""""""
        result = board.result(claim_draw=True)

        if result == '1-0':
            wdl = chess.engine.Wdl(wins=1000, draws=0, losses=0)
        elif result == '0-1':
            wdl = chess.engine.Wdl(wins=0, draws=0, losses=1000)        
        else:
            wdl = chess.engine.Wdl(wins=0, draws=1000, losses=0)

        return chess.engine.PovWdl(wdl, chess.WHITE)


def custom_evaluation(node):
    """"""Returns a static evaluation of a board state.""""""

    board = node.state
    
    # Evaluate terminal states.
    if board.is_game_over(claim_draw=True):
        result = board.result(claim_draw=True)
        if result == '1-0':
            wdl = chess.engine.Wdl(wins=1000, draws=0, losses=0)
        elif result == '0-1':
            wdl = chess.engine.Wdl(wins=0, draws=0, losses=1000)        
        else:
            wdl = chess.engine.Wdl(wins=0, draws=1000, losses=0)

        return chess.engine.PovWdl(wdl, chess.WHITE)
    
    # Evaluate material.
    material_balance = 0
    material_balance += len(board.pieces(chess.PAWN, chess.WHITE)) * +100
    material_balance += len(board.pieces(chess.PAWN, chess.BLACK)) * -100
    material_balance += len(board.pieces(chess.ROOK, chess.WHITE)) * +500
    material_balance += len(board.pieces(chess.ROOK, chess.BLACK)) * -500
    material_balance += len(board.pieces(chess.KNIGHT, chess.WHITE)) * +300
    material_balance += len(board.pieces(chess.KNIGHT, chess.BLACK)) * -300
    material_balance += len(board.pieces(chess.BISHOP, chess.WHITE)) * +300
    material_balance += len(board.pieces(chess.BISHOP, chess.BLACK)) * -300
    material_balance += len(board.pieces(chess.QUEEN, chess.WHITE)) * +900
    material_balance += len(board.pieces(chess.QUEEN, chess.BLACK)) * -900

    # TODO: Evaluate mobility.
    mobility = 0

    # Aggregate values.
    centipawn_evaluation = material_balance + mobility

    # Convert evaluation from centipawns to wdl.
    wdl = chess.engine.Cp(centipawn_evaluation).wdl(model='lichess')
    static_evaluation = chess.engine.PovWdl(wdl, chess.WHITE)

    return static_evaluation


m1 = chess.Board('8/8/7k/8/8/8/5R2/6R1 w - - 0 1') # f2h2
# WHITE can win in one move. Best move is f2-h2.

m2 = chess.Board('8/6k1/8/8/8/8/1K2R3/5R2 w - - 0 1')
# WHITE can win in two moves. Best move is e2-g2.

m3 = chess.Board('8/8/5k2/8/8/8/3R4/4R3 w - - 0 1')
# WHITE can win in three moves. Best move is d2-f2.

agent = Agent(custom_evaluation)

result = agent.mcts(m2, time_limit=30)
print(result)
````

","['python', 'game-ai', 'monte-carlo-tree-search']",
Using a Neural Network (LSTM) to approve/reject word-type sequences,"
I would like to train an LSTM neural network to either ""approve"" or ""reject"" a string based on the word-type sequence.
For instance: ""Mike's Airplane"" would output ""approved"", but ""Airplane Mike's"" would output ""reject"".
My method for doing this is to decompose the string into an array of words.
eg.
[""Mike's"", ""Airplane""]

, then convert the array of words to an array of word-types since the actual word is irrelevant.
The word types (pronoun, noun, adjective etc.) are defined constants having numerical values.
eg.
const wordtypes={propernoun:1, adjective:2, noun:3, ownername:4};
console.log(wordtypes.propernoun); // 1

Mike's Fast Airplane is
[""Mike's"", ""Fast"", ""Airplane""] 

which becomes:
input:[properNoun, adjective, noun]
output: ""approve""

properNoun represents the first word(Mike's),
adjective the second word(Fast),
and noun the third word(Airplane).
I would then like to use this array to train a Neural Network so that it can approve/reject other word-type sequences.
I am concerned with the methodology/algorithm rather than the syntax; I'm extremely new to Machine Learning and Artificial Neural Networks, so, I am using brain.js and NodeJS because they're relatively easy to use.

I would like to input multiple parameters for a single word because
many words have multiple word types (depending on the context). For
example, a word can be both a ""noun"" and a ""verb"". How do I represent this input?

Is this a good application for LSTM? Or is there a better-suited ML
algorithm? My dilemma is in deriving the proper inputs & methodology
to effectively train the Neural Network.

How is my methodology for accomplishing this approval system?


","['neural-networks', 'long-short-term-memory', 'input-layer']",
How to calculate cosine similarity for classification when you have say 10000 samples belonging to two classes have a bunch of samples,"
Does anyone have experience with using Cosine Similarity for text classification?  I see a number of articles on how to find cosine similarity between documents using Doc2Vec, Gensim, etc.
I have a classification problem (binary) where I want to try out the cosine similarity. I do know how to calculate it, but all the articles that I see only explain until the point of calculating it between two documents.
Right now, I am planning to do this.

Calculate the cosine similarity of 'my paragraph' (the one that I want to classify) with all samples in classi (their class is known). Then take the average (call that avgi)

Calculate the cosine similarity of my paragraph (the one that I want to classify) with all samples in classo (their class is known). Then take the average (call that avgo)

Compare avgi and avgo and then predict the class for 'my paragraph'


That sounds like a very manual way of doing it. Is there some better/widely used way of doing it?
","['machine-learning', 'text-classification', 'cosine-similarity']",
"Is there any animation that illustrates the ""fold"" and ""unfold"" operations of convolutional layers?","
There are fourteen convolution layers in PyTorch. Among them six are related to convolution, another six are related to transposed convolution. The remaining two are fold and unfold operations.
The documentation of PyTorch itself provided this link in order to visualize the operations to understand the convolution and transposed convolution easily. You can see the visuals and understand those variants of convolution operation easily.
Are there any such visuals (e.g. a diagram or animation) or any other resources available to visualize and understand the remaining two operations: fold and unfold?
","['convolutional-neural-networks', 'pytorch', 'convolution', 'resource-request', 'convolutional-layers']",
"Is there any gain by lazy initialization of weights, biases and number of input channels for a convolution operation?","
The basic layers for performing convolution operations 1,2,3 in PyTorch are
nn.Conv1d:   Applies a 1D convolution over an input signal composed of several input planes.

nn.Conv2d:   Applies a 2D convolution over an input signal composed of several input planes.

nn.Conv3d:   Applies a 3D convolution over an input signal composed of several input planes.

Along with them, there are lazy versions 1,2,3 of each of the aforementioned layers. They are
nn.LazyConv1d:    A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1).

nn.LazyConv2d:    A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1).

nn.LazyConv3d:    A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1).

We can observe from the description of lazy layers that the in_channels argument undergoes lazy initalization. Along with it, the attributes that will be lazily initialized are weight and bias.

Lazy Initialization is a performance optimization where you defer
(potentially expensive) object creation until just before you actually
need it. Lazy initialization is primarily used to improve performance,
avoid wasteful computation, and reduce program memory requirements

Since in_channels, weight and bias are undergoing lazy initialization in lazy convolution layers of PyTorch, I am guessing that there may be cases that the layers can perform convolution operation without the need of in_channels, weight and bias or bypassing some of them.
Am I guessing correct? Are there any cases in which convolution operation is said to be done without initializing weights or number of input channels?  If not, what is the gain we are getting by making such lazy initialization?
Is it purely an implementation technique to postpone initialization of those three quantities till the actual execution of the convolution operation in order to use resources minimally?
","['convolutional-neural-networks', 'pytorch', 'implementation', 'convolution']",
"How much research, approximately, is done in ANNs?","
Does someone know where can I find information about how much research, nowadays, is done in ANNs?
I've checked in this document Redes Neuronales: Conceptos básicos y aplicaciones, Universidad Tecnológica Nacional, México (2001) by D. J. Matich, that ""nowadays research is uncountable"" but that was in 2001.
I found nothing else on my further google search. Then, I've consulted Google Scholar, and by clicking on the option ""Since 2021"" it displayed 38,800 results, but, AFAIK, it includes a lot of different types of documents, e.g. books.
","['neural-networks', 'reference-request', 'research']",
Train Validation Test Splitting After or Before Data Augmentation?,"
I have seen tutorials online saying that you should do data augmentation AFTER doing the train/val/test split. However, when I go online to read some research papers, I see numerous instances of authors saying that they first do data augmentation on the dataset and then split it because they don't have enough data. Is it just that these are silly mistakes, even for papers with many citations, or is this acceptable?
Example: Research paper.
they say:
""Among these selected 480 images, 94 images were col-lected while changing the viewing angle, including images of 30 youngapples, 32 expanding apples, and 32 ripe apples.These 480 images were then expanded to 4800 images using dataaugmentation methods, yielding the training dataset. The training da-taset is used to train the detection model. The remaining 480 images areused as the test dataset to verify the detection performance of theYOLOV3-dense model"".
","['training', 'datasets', 'data-augmentation']","
In my personal experience, that depends. Augmenting data for training purposes is valid, and can even improve performance, as you may be aware. For testing purposes, it may be valid. Let me give you two examples when that may be the case:
Facial Recognition. Imagine that you have an augmentation function that can change the face pose (left/right pose, for simplicity). You may want to include augmented pose images for testing your models robustness.
The paper you mentioned. In this case, you have apple detection. As the authors in [1] say:

Apples in orchards were detected and the growth stages of apples were judged. Since the angle and intensity of sunlight illumination varies greatly during the day, whether the neural network can process the images collected at different time of the day depends on the integrity of the training dataset. In order to enhance the richness of the experimental dataset, the collected images were pre-processed in terms of colour, brightness, rotation, and image definition.

After this brief introduction, the authors proceed into discussing the augmentation types they used for enhancing the richness of the dataset. As for the case of Facial Recognition, augmenting the test data follows the same idea of having a diverse testing data.
References
[1] Tian, Y., Yang, G., Wang, Z., Wang, H., Li, E., & Liang, Z. (2019). Apple detection during different growth stages in orchards using the improved YOLO-V3 model. Computers and electronics in agriculture, 157, 417-426.
"
What does 'input planes' mean in the phrase 'input signal/image composed of several input planes'?,"
PyTorch documentation provided the following descriptions to the Convolution layers
nn.Conv1d              Applies a 1D convolution over an input signal composed of several input planes.

nn.Conv2d              Applies a 2D convolution over an input signal composed of several input planes.

nn.Conv3d              Applies a 3D convolution over an input signal composed of several input planes.

nn.ConvTranspose1d     Applies a 1D transposed convolution operator over an input image composed of several input planes.

nn.ConvTranspose2d     Applies a 2D transposed convolution operator over an input image composed of several input planes.

nn.ConvTranspose3d     Applies a 3D transposed convolution operator over an input image composed of several input planes.

If you observe the descriptions on the right side. Each description is of the form ""Applies an operation over an input signal/image composed of several input planes."" It is not just confined to Convolution layers, same phrase has been used for several other layers including pooling layers and a normalization layer.
I have doubt with the word ""input planes"" used here.
What is the meaning of the input plane used here? Does it refer geometrical plane or some other?
","['terminology', 'pytorch', 'image-processing', 'convolution']","
Yes, it is a bit misleading. What it really means is input channels, so it would be: nn.Conv2d: Applies a 2D convolution over an input signal composed of several input channels.
So, why don't just use channels instead of input planes? Well, initially the major deep learning applications were used for computer vision or image processing approaches. In CV or image processing, each one of the components of the third dimension of an image tensor is called channel, so an image $I$ would be $I:H \times W \times C$ where $C$ is the number of channels (usually: $C=3$, RGB, or $C=4$, RGBA). So using the traditional terminology the last dimension of a tensor would be called channels. However this terminology is highly coupled to image processing because it assumes the 3D tensor you are processing is an image.
On the other hand, there have been an increasing number of applications where AI is used for other kind of input data (the sensor that gathers data is no longer a camera). Me for example, I use deep learning for Radar Signals. So what happens there? I happens that the image processing terminology no longer applies and, if any, it is very prone to errors (think that channels in signals can be frequency channels, wave propagation paths...).
So going back to your original question, the pytorch guys realized that and changed the terminology to a more geometrical description (which in the end is a more abstract terminology that would suit any application). So instead of referring to the last dimension of a tensor as channels, as you can see in any research paper, they took a step forward and used the geometrical description of a tensor (which in the end can encode any kind of 3D information not only images).
"
"In the cross-entropy method, should I select state-action pairs by their immediate reward or by the episode reward?","
I am trying to understand the code mechanics when selecting the elite states and elite actions. It appears clear to me that they are those that appear in the episodes with the rewards bigger than the threshold.
My question is: should I select state-action pairs by their immediate reward or by the episode reward?
I am applying the method to a craft environment interesting to me and I have been studying an example applying the OpenAI's Gym taxi environment, but I do not fully understand the code.
","['reinforcement-learning', 'rewards', 'monte-carlo-methods', 'return']",
Computing the mean attention distance for ViT,"
Recently I came across the paper that introduces the Vision Transformer (ViT) ""AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"".
The thing I don't really understand at the moment is, what is meant with ""mean attention distance"".
More specifically in the caption of Figure 11 on page 18 of the paper they state:
"" ... Attention distance was computed for 128 example images by averaging the distance
      between the query pixel and all other pixels, weighted by the attention weight. ...""

How can the query be on a pixel level? 
Isn't the overall approach of the ViT to divide the input image into patches which are linearly embedded, combined with a positional embedding and then feed into the transformer encoder. 
So the attention should be on the patch level not on the pixel level?
I would be very happy if someone could elaborate a bit more on the above sentence, so far I found no deeper explanation.
","['deep-learning', 'computer-vision', 'papers', 'transformer', 'attention']",
"How to prove Lemma 1.6 in the book ""Reinforcement Learning: Theory and Algorithms""","
I am trying to prove the following lemma from Reinforcement Learning: Theory and Algorithms on page 8.
Lemma 1.6. We have that:
$$
\left[(1-\gamma)\left(I-\gamma P^{\pi}\right)^{-1}\right]_{(s, a),\left(s^{\prime}, a^{\prime}\right)}=(1-\gamma)\sum_{h=0}^{\infty} \gamma^{t} \mathbb{P}_{h}^{\pi}\left(s_{h}=s^{\prime}, a_{h}=a^{\prime} \mid s_{0}=s, a_{0}=a\right)
$$
where $\pi$ is a deterministic and stationary policy with:
$$
P_{(s, a),\left(s^{\prime}, a^{\prime}\right)}^{\pi}:=\left\{\begin{aligned}
P\left(s^{\prime} \mid s, a\right) & \text { if } a^{\prime}=\pi\left(s^{\prime}\right) \\
0 & \text { if } a^{\prime} \neq \pi\left(s^{\prime}\right)
\end{aligned}\right.
$$
The Corollary 1.5 should also be useful: $Q^{\pi}=\left(I-\gamma P^{\pi}\right)^{-1} r$
To be honest, I don't have much idea to do it. I saw that the LHS of Lemma 1.6 is related to $Q^\pi$, so my idea is to expand Q and see if it's possible to separate out the $r$. I did the following but end up clueless,
$$\begin{aligned}
Q^{\pi}(s, a) &=E\left[\sum_{t=0}^{\infty} \gamma^{t} R\left(s_{t}, a_{t}\right) \mid \pi, s_{0}=s, a_{0}=a\right] \\
\
&=R(s, a)+\gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid s, a\right) E\left[\sum_{t=0}^{\infty} \gamma^{t} R\left(s_{t+1} \cdot a_{t+1}\right) \mid \pi, S_{1}=s^{\prime}, a_{1}=\pi\left(s^{\prime}\right)=a^{\prime}\right] \\ &= R(s, a)+\gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid s, a\right) \left[R(s^\prime,a^\prime) + \gamma\sum_{s^{\prime\prime}}P(s^{\prime\prime}|s^\prime,a^\prime)Q(s^{\prime\prime}, a^{\prime\prime})\right]
\end{aligned}$$
I have been staring at this equation for hours with no progress. I hope I can get some guidance from you guys.
","['reinforcement-learning', 'deep-rl', 'markov-decision-process', 'proofs']",
What does 'channel' mean in the case of an 1D convolution?,"
While reading about 1D-convolutions in PyTorch, I encountered the concept of channels.
in_channels (int) – Number of channels in the input image

out_channels (int) – Number of channels produced by the convolution

Although I encountered this concept of channels earlier, I am confused about channels and might understand them in the wrong manner.
Since the operation we are discussing is a 1D convolution, then there will be two lists of numbers: one is the input list and the other is the filter list. The last one is the feature map (the output list).
They look like this:

The left one is the input list, the middle one is the filter list and the rightmost one is the output list.
Each cell in the input list contains a whole number. Each cell may take a value in the fixed range $[a, b]$ of numbers.
What is the concept of channels used here? From where the channels are coming? Does the number of channels stand for the number of elements in the corresponding list?
","['terminology', 'image-processing', 'convolution', '1d-convolution', 'channel']","
A common use case for 1d-convolution is to analyse & interpret time series data. Imagine a single sensor that generates a sequence of readings such as [1 2 3 4 2].
That is equivalent to a single channel.
However its also possible to have multiple sensors generating readings, such as
[ 1 2 3 2 4]
[ 2 3 4 1 2]
[ 3 4 5 1 2]

This would be equivalent to 3 channels of time series data that's consumed by a 1d convolution.
"
What exactly is an XPU?,"
I know about CPU, GPU and TPU. But, it is the first time for me to read about XPU from PyTorch documentation about MODULE.
xpu(device=None)


Moves all model parameters and buffers to the XPU.

This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.

Note: This method modifies the module in-place.

Parameters

    device (int, optional) – if specified, all parameters will be copied to that device

Returns

    self

Return type

    Module

CPU stands for Central Processing Unit.
GPU stands for Graphical Processing Unit.
TPU stands for Tensor Processing Unit.
Most of us know that these processing units are highly useful in the research of some computational intensive domains of AI including deep learning. So, I am wondering whether XPU is also useful in AI research since it is used in PyTorch.
From the context, I can say that PU stands for processing unit. But I have no idea of what X is.
What is the full form for XPU? Where can I read about XPU in detail?
","['terminology', 'pytorch', 'implementation', 'resource-request', 'hardware']",
Train a deep learning model with input as a vector and predicts as a vector? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I am trying to build a Deep Learning model that takes a numeric vector $X$ of dimension $1 \times 50$ and predicts a numeric vector $y$ of dimension $1 \times 50$.
It's a linear regression problem. I am trying to achieve the coefficients/weights that can help me
Code I used:
X = np.array(...) // array of 50 features and 5 sample vectors (shape of X is 5x50)
y = np.array(...) // array of 50 features (shape of y is 5x50)
model = Sequential([Dense(1, input_shape=[5,50])])

optimizer = Adam(0.001)

model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])

model.fit(X,y, epochs=250, validation_split=0.25))


So, basically we are achieving X*w ~ y where $w$ is the weights/coefficients that we want to identify using DL.
Programmatically, I tried using the same logic and calculated $w = y . X^{-1}$ for all the vectors. Took average of the coeff and applied on the test data.
","['deep-learning', 'python']",
Building an AI that predicts the pronunciation of words,"
I want to create an AI that converts words to  International Phonetic Alphabet (IPA), but I am not sure which architecture I am supposed to use.
It is not possible to translate the characters one by one since there are multiple characters in the source word corresponding to one IPA character. There are solutions for this kind of problem, for example using an Encoder that encodes the content of the input which the decoder then translates, but I am uncertain if this isn't too abstract for this problem.
Can anyone think of a suitable solution for this task?
",['natural-language-processing'],"
Look for sequence-to-sequence modelling, aka, seq2seq.
https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
https://en.wikipedia.org/wiki/Seq2seq
"
"How can I use a prediction model (e.g., ARMA model or LSTM) for multi-variate data?","
I have a question
I have had a dataset below
             sensor1   sensor2  sensor3 ...
2021-01-01    1.32       2.2      1.0
2021-01-02    4.3        2.0      0.8 ...
...

I know ARMA model is useful for time-series forecasting

However, how can I use ARMA model for data with multiple attributes ?
If data with a single attribute can be a input for ARMA model, should I aggregate the attribute set?
(for example, after normalizing each attribute, I add up all values every rows to transform all attributes to a single attribute)
","['long-short-term-memory', 'datasets', 'prediction', 'time-series', 'forecasting']","
I don't think you need to go for aggregation -- this looks like a job for VARIMA, the vector-version of ARIMA. In ARIMA, the output of the sequence at time $t$, which can be notated $X_t$, is a function of the past inputs $\{X_1, X_2, \dots, X_{t-1}\}$. For a univariate $AR(k)$ process, the corresponding ARIMA model is given by
$$ X_t - \sum_{i=1}^k \alpha_i X_{t-i} = \varepsilon_t + \sum_{i=1}^k \theta_{i}\varepsilon_{t-i}$$
with parameters $\alpha_i, \theta_i$, inputs $X_i$, and i.i.d. zero-mean Gaussian error terms $\varepsilon_i$. The generalization of this to multiple variables is thus simply
$$\mathbf{x}_t - \sum_{i=1}^k \mathbf{A}_i \mathbf{x}_{t-1} = \mathbf{e}_t + \sum_{i=1}^k \mathbf{\Theta}_i \mathbf{e}_{t-i}$$
for $\mathbf{x}_i, \mathbf{e}_i \in \mathbb{R}^n$. Note that before, the parameters were scalars. Now, $\mathbf{A}_i, \mathbf{\Theta}_i \in \mathbb{R}^{n\times n}$ -- size-$n$ square matrices.
It looks like there's a Github implementation here as well, though I haven't looked closely at this.
If you're going for an LSTM-based sequence modeling approach, this is even easier -- an LSTM cell can take in input of arbitrary dimensions, so you shouldn't have to make any changes.
If you'd like to see the math, concretely, the LSTM forward pass equations have the form
$$(\cdot)^{(t)} = g(\mathbf{W}^{(\cdot)} x^{(t)} + \mathbf{U}^{(\cdot)}x^{(t)} + \mathbf{b}^{(\cdot)})$$
where $\mathbf{W}, \mathbf{U}$ are the parameter matrices associated with the inputs and hidden states, respectively, for each gate, and $\mathbf{b}$ is a bias term. So in the single-variable case, $\mathbf{W}, \mathbf{U}, \mathbf{b}$ are all scalars; in the multivariate case, $\mathbf{W}, \mathbf{U}$ are now size-$n$ square matrices, and $\mathbf{b}$ is a vector of length $n$. No further modification is needed, and you should be able to just plug-and-play with most deep learning libraries.
So your inputs would just be the vector of attributes at a particular time step.
"
What is language-conditioned visual reasoning?,"
Can anyone explain what language-conditioned visual reasoning is?
I saw this term in this paper and I searched on the internet but I couldn't find a proper explanation.
","['computer-vision', 'terminology', 'papers', 'reasoning']",
Scrabble rack observation with MuZero,"
Currently I'm trying to implement Scrabble with MuZero.
The $15 \times 15$ game board observation (as input) is of size $27 \times15 \times15$ (26 letters + 1 wildcard) with a value of 0 or 1.
However I'm having difficulties finding a suitable way to encode the player's rack of letters (Always 7 letters on the rack).
The available tiles are: 26 letters $(A-Z)$ and 1 wildcard.
A rack can also contain multiple tiles of the same letter.
Example: rack of player 1 is  $[A,A,C,E,T,T,H] -> A:2x, C:1x, E:1x, T:2x, H:1x$
How can I represent a rack of tiles as a $(? \times)15 \times15$ (or other board size) matrix ?
","['reinforcement-learning', 'game-ai', 'muzero', 'observation-spaces', 'board-games']","

Use one hot encoding for each position, shape $(7, 27)$.
Stack these two dimensions, shape $(189)$.
Tile (replicate) this vector into images of the same resolution, now shape $(15, 15, 189)$
Stack them with your other observation, final shape $(15, 15, 216)$.

Another way is instead of replicating the $(189)$ vector along two axis, you can squeeze them into one single plane of $15 * 15 = 225$ values, and pad the unfilled part with zeros.
"
Where does batch normalization layers present in a neural network?,"
Batch normalization is a procedure widely used to train neural networks. Mean and standard deviation are calculated in this step of training.
Since we train a neural network by dividing training data into batches, we use the word batch normalization as we consider a batch of training vectors at a time.
My doubt is about the position of batch normalization layers in a neural network.
Is it present before the input layer only? Or is it before every layer? Or is it dependent on the underlying task?
Suppose there is a neural network of 4 layers: $I \rightarrow h1 \rightarrow h2 \rightarrow h3 \rightarrow O$
Which one of the following is true
$$bn \rightarrow I \rightarrow h1 \rightarrow h2 \rightarrow h3 \rightarrow O$$
$$bn1 \rightarrow I \rightarrow bn2 \rightarrow  h1 \rightarrow bn3 \rightarrow  h2 \rightarrow bn4 \rightarrow  h3 \rightarrow bn5 \rightarrow  O$$
Here $I$ stands for input layer, $h$ for the hidden layer, $O$ for output layer, and $bn$ for batch normalization layer.
","['neural-networks', 'architecture', 'batch-normalization']","
I think the answer to your question is much more a rule of thumb than an appropriate analytical answer. First of all, I would like to remark that Batch Normalization [1] are applied most commonly to convolutional layer, constituting what is called a ""convolutional block"" (Convolution + Batch Normalization + Activation). Thus, for giving you an idea on where to put the normalization layers, I will analyze three papers that make use of Batch Normalization.
Unsupervised representation learning with deep convolutional generative adversarial networks [2]. In Section 3, Approach and Model Architecture, the authors make the following remark:

Third is Batch Normalization [1] which stabilizes learning by normalizing the
input to each unit to have zero mean and unit variance. This helps deal with training problems that
arise due to poor initialization and helps gradient flow in deeper models. This proved critical to get
deep generators to begin learning, preventing the generator from collapsing all samples to a single
point which is a common failure mode observed in GANs. Directly applying batchnorm to all layers
however, resulted in sample oscillation and model instability. This was avoided by not applying
batchnorm to the generator output layer and the discriminator input layer.

thus, the authors have experimented with Batch Normalization on all layers, but this approach resulted in model instability. They ultimately avoided Batch Normalization on the Generator's output layer, and Discriminator's input layer. These correspond to ""raw image layers"" (the output of the generator is an image, as well as the input of the discriminator). Using your notation, it would be something like:
$$G: z \rightarrow (C + BN + A)_{1} \rightarrow (C + A)_{2} \rightarrow O$$
$$D: I \rightarrow (C + A)_{1} \rightarrow (C + BN + A)_{2} \rightarrow h_{1} \rightarrow h_{2} \rightarrow O$$
where $G$ stands for Generator, $D$ for Discriminator, $C$ for convolution, $BN$ for batchnorm, $A$ for activation and $D_{i}$ for dense layers.
Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising [3]. In Figure 1., the authors show their proposed architecture that applies the same logic. Neither the first convolutional layer, neither the last use Batch Normalization. Although the authors make their point in justifying the usage of residual learning + Batch Normalization, the paper does not justify this architectural choice. The network would look something like this:
$$I \rightarrow (C + A)_{1} \rightarrow (C + BN + A)_{2} \rightarrow \cdots \rightarrow (C + BN + A)_{n} \rightarrow C \rightarrow O$$
Deep Residual Learning for Image Recognition [4]. This paper proposes a different scheme, as it applies Batch Normalization after each convolution operation on convolutional layers (thus including the input).
To sum up some papers use it only on the ""hidden convolutional blocks"", others on all of them. My advice is that, if you have the time, you should compare the two approaches. Remark: maybe I am unaware of some further development on the matter that gives a precise argument in favor of whether of these approaches.
References
[1] Ioffe, S., & Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). PMLR.
[2] Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.
[3] Zhang, K., Zuo, W., Chen, Y., Meng, D., & Zhang, L. (2017). Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7), 3142-3155.
[4] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).
"
Questions about a research paper on salient region detection and segmentation [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I am reading this paper in an attempt to recreate the salient region detection and segmentation model employed. I have the following questions pertaining to section 3 of the paper and I would highly appreciate it if someone could provide clarity on them.

The word ""scales"" is used at multiple points in the section, for example, line 4 of the section states ""saliency maps are created at different scales"". I do not exactly understand what the authors mean by the word scales. Moreover, is there a mathematical way to think about it?

I understand that a saliency value  is computed for each pixel at () using the equation 


However, there is no mention of  in the equation. Hence, I am confused as to what pixel the saliency value is being computed for. Is it ?

I did not understand what the authors meant by the term ""bin"" in section 3.2 line 5 where it is stated, ""The hill-climbing algorithm can be seen as a search window being run across the space of the d-dimensional histogram to find the largest bin within that window.""

Note 1: This question was originally posted on Stack Overflow. I was advised to post it on another platform as a consequence of it being unfitting to the site. Hence, I am uploading the question here. Link to the original post here.
Note 2: In case you are unable to access the link to the research paper, the following citation may help: Achanta, R., Estrada, F., Wils, P., & SÃ¼sstrunk, S. (2008, May). Salient region detection and segmentation.
","['computer-vision', 'math', 'hill-climbing']",
"What does the lambda parameter in the paper ""Interpretable Explanations of Black Boxes by Meaningful Perturbation"" do?","
I do not understand the purpose of the $\lambda$ parameter in equation 3 of the paper Interpretable Explanations of Black Boxes by Meaningful Perturbation.
$$m^{*}=\underset{m \in[0,1]^{\Lambda}}{\operatorname{argmin}} \lambda\|\mathbf{1}-m\|_{1}+f_{c}\left(\Phi\left(x_{0} ; m\right)\right) \tag{3}\label{3}$$
As far as I understand, the argmin function returns the $m$ for which the term $\lambda\|\mathbf{1}-m\|_{1}$ is smallest. If that's the case, I don't understand the purpose of $\lambda$, since it doesn't change the result of $m$.
","['computer-vision', 'image-recognition', 'papers', 'optimization', 'explainable-ai']",
How can I take continuous video input into my model?,"
Let's say I have designed an ML model that can take video input of a dog running around and give the breed of the dog as output. However, I do not want to wait for the video to finish before it is input into my model. I want something like the following to happen:
I am casually taking a video of my backyard when mid-way through a dog runs past the camera. Immediately, my model should identify (a) a dog has appeared within view and (b) the dog is a Labrador Retriever.
In an attempt to achieve the above, I have the following questions:

Do I need to train a new model that detects when a dog has appeared within view?
How can I make my model such that the input is continuous and that the model keeps running providing instant output?

Note: This question was originally posted on Stack Overflow. It was closed as a consequence of it being unfitting to the site. Hence, I am uploading the question here. Link to the original post here.
","['machine-learning', 'computer-vision', 'input-layer']",
How can I compute a mathematical formula for my CNN?,"
Let's say, for example, I have built the following CNN model using Keras:
model = Sequential()
model.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(32, (3,3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(512))
model.add(Dense(10, activation='softmax'))

I wish to be able to transform the above model into a mathematical formula.

I understand the basic structure of a CNN as follows:

where



However, I do not know how to go from the above recursive formula to something like this (the first two summations are weights and the second two are adjustable biases):

Note: The formula above is just an example and not representative of the code given above.


Do I need to trace each weight, each bias and each connection of every neuron? If so, how?
Furthermore, I would highly appreciate it if someone could provide a generalized strategy for tackling such a problem (like finding a math formula to suit a different kind of classifier).
Lastly, is this an easy task and is it a worthwhile one?


Note: This question was originally posted on Stack Overflow. Unfortunately, I received no responses even after offering a bounty. Hence, I am uploading the question here. Link to the original post here.
","['convolutional-neural-networks', 'math']",
What are the applications in which the precision of the neural network's weights is unimportant?,"
While reading about Module in PyTorch, I came across a new data type called half datatype.
half() method when calls on a Module casts all floating-point parameters and buffers to half datatype.
It is a 16-bit floating-point number as mentioned here.
It is mentioned in Wikipedia that

It is intended for storage of floating-point values in applications
where higher precision is not essential for performing arithmetic
computations.

It implies that the precision of parameters (say, weights for a neural network) is not important in certain applications and hence one can use half datatype while implementing a neural network.
Did any research support the statement that precision, that is the range of values it takes, of weights, is unimportant for certain applications?
","['neural-networks', 'reference-request', 'pytorch', 'implementation', 'weights']","
Yes, research into ultra-low precision neural network is generally referred to as network quantization. For example, the weights and actications of an artificial neural network can be quantized down to 4-bit, or in extreme cases even 2-bit and 1-bit (binary neural networks).
This is a good introductory article to start with:
https://arxiv.org/abs/2106.08295, which goes into detail how network quantization can be done to the maximum extent of preserving the original network accuracy.
The applications of network quantization is immediate. A network with lower precisions would use much less memory and compute power, which is particularly important on edge devices.
However, I do not know whether there are certain ML tasks that are more amenable to quantizatios then others. Generally speaking, we talk about quantization on a network basis. For example, if you quantization ResNet-50, you can generally use this network to run multiple CV tasks like classification, detection, etc.
"
What are the numbers that are useful (may need to be stored) other than parameters of a model?,"
Consider the following method related to buffers in PyTorch
buffers(recurse=True)

Returns an iterator over module buffers.

Parameters

    recurse (bool) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.

Yields

    torch.Tensor – module buffer

buffers() is a method used for models (say neural networks) in PyTorch. model.buffers() contains the tensors related to the model and you can see it from the example provided below.
>>> for buf in model.buffers():
>>>     print(type(buf), buf.size())
<class 'torch.Tensor'> (20L,)
<class 'torch.Tensor'> (20L, 1L, 5L, 5L)

The following method informs that a model in PyTorch has both parameters and buffers. So, buffers cannot be the same as the parameters (say weights) of the model.
 cpu()

    Moves all model parameters and buffers to the CPU.

I am not aware of any numbers to store for a model other than its parameters. So, I have no idea what buffers of a model in the PyTorch store. But, this implies that there are some other numbers related to a model that needs to be stored for efficiency or other purposes.
Is it PyTorch specific? Else, what are those numbers, other than parameters, that need to be stored for a model?
","['neural-networks', 'pytorch', 'weights']",
KL divergence coefficient update doesn't make sense in RLlib's PPO implementation,"
I am using RLlib (Ray 1.4.0)'s implementation of PPO for a multi-agent scenario with continuous actions, and I find that the loss includes the KL divergence penalty term, apart from the surrogate loss, value function loss, and entropy.
The KL coefficient is updated in the update_kl() function as follows:
    if sampled_kl > 2.0 * self.kl_target:
        self.kl_coeff_val *= 1.5
    # Decrease.
    elif sampled_kl < 0.5 * self.kl_target:
        self.kl_coeff_val *= 0.5
    # No change.
    else:
        return self.kl_coeff_val

I don't understand the reasoning behind this. If the point of the KL ""target"" is to reach the target, then why do the conditions above imply that the KL coefficient is getting larger (multiplied by 1.5 when the sampled KL is already found to be larger than the target?) when it is supposed to be made smaller instead? I feel like I am missing something here, but I am not able to get my head around it.
I would appreciate any insights on this. Thank you.
","['reinforcement-learning', 'proximal-policy-optimization', 'loss', 'continuous-action-spaces', 'kl-divergence']","
kl_coeff_val = kl_coeff is the multiplier in the KL penalty term in the loss. So increasing this coefficient means increasing the penalty loss, which should lead to greater KL reduction after update.
"
How to approach a two-agent two-step action game?,"
A simple two-player sniper game:

Each player has 9 houses that he can reside in. So 18 houses in total. The houses can be considered in a row: e.g. 1-9 for player A, and 10-18 for player B.

Each step, the player should make two actions! First, he can use his gun's limited view to check out three consecutive houses of the enemy to see if he is there (for example, he choose 3,4,5.). Then, based on that result, he can choose one house to shoot. That means if he guessed correctly, he will know the other player is in one of those three houses. Otherwise, he can shoot one of the remaining six houses.

The killer wins!



Please note that in each step, the player has to perform two actions without interruption from the other player. Based on the result of the first action (limited view), he will have more information to select his second action (shooting). Thus, the first action is informative to reduce action space.

I have decided to use stable-baselines3. I have to create an environment. I am not sure about the policy network.
How should I approach this game for training an AI agent? I would really appreciate it if you can guide me on env creation, policy selection, or any general tips.
","['reinforcement-learning', 'game-ai']",
Would the reward normalization be wrong in early episodes?,"
It's confusing me that how can we normalize the reward without actually knowing the true mean and variance of the reward distribution, specifically, at the early steps and episodes. This may cause problem for the RL algorithms that use the replay buffer such as DDPG, because this wrongly calculated rewards can stay in buffer for too long and the network will adapt with them. Is there something that I am missing or misunderstood? For algorithms with replay buffer, using standardization is better that normalization?
","['reinforcement-learning', 'deep-learning', 'tensorflow', 'keras']",
Is the dropout technique specific only to neural networks?,"
In one Udemy course was mentioned that ""dropout is unique to neural networks"". However, I remember an example of decision trees where nodes that are not participating in the overall result are removed, and I think that this technique is also called ""dropout"". Am I correct?
","['neural-networks', 'regularization', 'decision-trees', 'dropout']",
"What does it mean by ""zeros the networks parameters gradients"" in the context of training a neural network?","
Consider the following PyTorch code
# Run a sample training loop that ""teaches"" the network
# to output the constant zero function
for _ in range(10000):
  input = torch.randn(4)
  output = net(input)
  loss = torch.abs(output)
  net.zero_grad()
  loss.backward()
  optimizer.step()

and its corresponding explanation on training a neural network
A training loop…

acquires an input,
runs the network,
computes a loss,
zeros the network’s parameters’ gradients,
calls loss.backward() to update the parameters’ gradients,
calls optimizer.step() to apply the gradients to the parameters.

Code contains net.zero_grad() which has been explained as zeros the network’s parameters’ gradients.
What does it mean by zeros the networks parameters gradients? In general, loss is back propagated by calculating the gradients of loss wrt parameters. But, I didn't understand the phrase ""zeros of networks parameters gradient"". What does that particular step do?
","['neural-networks', 'training', 'pytorch', 'gradient']","
In the automatic differentiation procedure after backward pass
the gradient with respect to the scalar is added to the current gradient. Without calling zero_grad you will have the sum of all gradients, calcluated before, with the current gradient.
Therefore, optimizer.step() will do not this:
w = w - eta * grad L[i] # L[i] - loss function for the i-th sample

But rather:
w = w - eta * sum_i(grad L[i]) # sum of gradient with respect to all samples

Which is not the desired behavior.
"
How is the convolution operation connected to neural networks?,"
I've been reading up on the convolution operation and neural networks. I understand that the convolution operation is defined as:
$$(f * g)(t)=\int_{-\infty}^{\infty} f(\tau) g(t-\tau) d \tau$$
The convolution operation has some properties, such as commutativity, associativity, etc.
How is the convolution connected to neural networks? How do we use this operation in a CNN?
","['neural-networks', 'convolutional-neural-networks', 'convolution']",
Is there any difference between affine transformation and linear transformation?,"
Consider the following statements from A Simple Custom Module of PyTorch's documentation

To get started, let’s look at a simpler, custom version of PyTorch’s
Linear module. This module applies an affine transformation to its
input.

Since the paragraph is saying PyTorch’s Linear module, I am guessing that affine transformation is nothing but linear transformation.
Suppose $x = [x_1, x_2, x_3,\cdots,x_n]$ be an input, then the linear transformation on $x$ can be $a.x+b$, where $a$ and $b$ are $n-$ dimensional vectors of real numbers. And dot($.$) stands for dot product.
Is affine transformation same as the linear transformation? If yes, then why the name affine is used? Does it cover something more or less than linear transformation?
","['neural-networks', 'terminology', 'pytorch', 'linear-algebra']","
In linear algebra, a linear transformation (aka linear map or linear transform) $f: \mathcal{V} \rightarrow \mathcal{W}$ is a function that satisfies the following two conditions

$f(u + v)=f(u)+f(v)$ (additivity)
$f(\alpha u) = \alpha f(u)$ (scalar multiplication),

where

$u$ and $v$ vectors (i.e. elements of a vector space, which can also be $\mathbb{R}$ [proof], some space of functions, etc.)
$\alpha$ is a scalar (e.g. which can be a real number, but not necessarily)
$\mathcal{V}$ and $\mathcal{W}$ are vector spaces (e.g. $\mathbb{R}$ or $\mathbb{R}^2$)

So, any function that satisfies these two conditions is a linear transformation.
In Euclidean geometry, $g(x) = ax + b$ is an affine transformation, which is generally not a linear transformation as defined in linear algebra. You can easily show that affine transformations are not linear transformations. For example, let $a = 1$ and $b = 2$, does $g$ satisfy the second condition above for any scalar $\alpha$? No. For example, let $\alpha = 3$, then $g(3x = y) = y + 2 = 3x + 2 \neq 3 g(x) = 3 (x + 2) = 3x + 6$.
However, in the context of neural networks, when people use the adjective ""linear"" they are often referring to a line. For example, in linear regression, you can have a bias (the $b$ in the affine transformation $g$ above), which would make the function not a linear transformation, but we still call it linear regression because we fit a line (hence the name linear regression) to the data.
So, no, an affine transformation is not a linear transformation as defined in linear algebra, but all linear transformations are affine. However, in machine learning, people often use the adjective linear to refer to straight-line models, which are generally represented by functions that are affine transformations. In this answer, I also talk about this issue.
"
Proof that there always exists a dominating policy in an MDP,"
I think that it is common knowledge that for any infinite horizon discounted MDP $(S, A, P, r, \gamma)$, there always exists a dominating policy $\pi$, i.e. a policy $\pi$ such that for all policies $\pi'$: $$V_\pi (s) \geq V_{\pi'}(s) \quad \text{for all } s\in S .$$
However, I could not find a proof of this result anywhere. Given that this statement is fundamental for dynamic programming (I think), I am interested in a rigorous proof. (I hope that I am not missing anything trivial here)
","['reinforcement-learning', 'markov-decision-process', 'proofs', 'policies']",
"The model learns well, but the validation decreases over time [closed]","







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I have trained a model for four days. I noticed a behaviour quite strange/unnatural.
During the training, the score and loss look like this:

However, when I see the validation score, I got:

It seems the model learning by heart at the beginning and not generalise well afterwards. Is this a natural behavior? Maybe it's really not normal and there must be some errors in the code or algorithm? I don't know what to think anymore. Can you help me? What is a good solution?
","['reinforcement-learning', 'natural-language-processing']",
End-to-end learning using LSTM-AE,"
I want to use prediction models like LSTM-AE to predict time-series data. The feature that the neural network should learn is in frequency between 40-60Hz. So, in order to learn the feature more effectively and removing the noises, the signal will be filtered using a bandpass filter and the result will be then passed to the network.
The problem is that, if I want to develop an end-to-end (i.e. omitting the bandpass filtering) solution, how can I do that?
","['machine-learning', 'long-short-term-memory', 'time-series']",
Using a rectified Tanh to train a CNN?,"
I have been experimenting with activation functions on CNN, and it occurred to me to use a rectified tanh function. So that is basically if z > 0 tanh(z) else 0. I have implemented it and I compared with ReLu on odd MNIST. They both achieved about 94% success rate in 10 epochs. My logic was that usually humans tend to stop feeling more confident once they have learnt something. Similarly, I thought a Convolutional layer neuron should not feel much more confident (higher activation) with growing evidence (higher weighted input). So is there any evidence of perhaps such a rectified tanh being more successful ?
","['convolutional-neural-networks', 'activation-functions']",
Is label-embedding similar to one-hot encoding?,"
In one-hot encoding, a vector is given to each class label. For each class, only one entry of the vector is equal to 1 and the remaining entries are zeros in this encoding.
Thus, in one-hot encoding, we are encoding the class label.
Is it true that label-embedding gives a vector for each class label like in one-hot encoding? Is one-hot encoding a type of label-embedding?
","['terminology', 'papers', 'one-hot-encoding']",
How to apply the formatting of one json file to another. Coding style transfer for JSON,"
Time after time I need to merge two large JSON files, or more precisely add a json fragment to another file.
The too pieces are often written by different people and have different formatting (spacing), so margining them mechanically result in an ugly code with ragtag spacing, and formatting one of them manually or semi-manually takes a lot of time and effort.
How I can reformat the fragment in the same style the first file is formatted. I do not know what IDE, formatter, or style guide was used
I know that I can use automated tool to reformat the whole document into whatever is supported, but prefer to keep the original style.
I have heard that coding style can be imitated e.g. in the context of adversary authorship recognition by AI and imagine that for simple cases such as JSON that should be easy. I am interested in merely spacing, indentation, brackets placing, not, say property naming or nesting convention (not sure of ordering, probably not so interesting).
I am software developer trying to automate my tasks, rather than AI researcher, so please be patient.
I posted to stackoverflow, yet the task might be to challenging and open ended for traditional programming methods https://stackoverflow.com/questions/68396829
",['style-transfer'],
Doubt in Sutton & Barto's off-policy Monte Carlo control algorithm,"
The algorithm is described as below:

My understanding: In the third last step, we act greedily w.r.t $Q$. Since we use importance sampling, this $Q \approx Q_\pi$. However, in the next step, whenever $A_t \neq \pi(S_t)$, it means the behavior policy isn't aligned with the target (greedy) policy. Hence, we can't use importance sampling and for such $(S_t, A_t)$ we simply take the average of $Q(S_t, A_t)$. Which means these $Q$ values aren't estimates of $Q_\pi$ but rather $Q_b$.
What's been bothering me is when the behavior and target policy eventually align for state $S_t$, won't that alignment be incorrect? Because in the previous step, we would be doing:

$\pi(S_t) = \arg \max [Q_\pi(S_t, a_1), Q_b(S_t, a_2), Q_b(S_t, a_3)]$

assuming $A(S_t) = \{a_1, a_2, a_3\}$ and the true greedy action is $a_1$.
","['reinforcement-learning', 'monte-carlo-methods', 'off-policy-methods']",
How to normalize rewards in REINFORCE?,"
I'm trying to solve a reinforcement learning problem using a Monte Carlo policy gradient algorithm and, more specifically, REINFORCE, with rewards attributed to individual moves instead of applied to all steps in a rollout.
For this, I do $M$ rollouts, each with $N$ steps, and record the rewards. Let's say I fill an $M \times N$ matrix with the rewards. Sometimes just using these rewards as-is will work, but sometimes the rewards are always positive (or always negative), or the magnitudes cover a large range.
A simple thing is to just subtract the overall mean and divide it by the overall standard deviation.
In my particular case, though, the beginning is easier, and during bootstrapping the rewards will be higher.  A typical case would have high rewards at the beginning with a taper to zero before the end of the rollout.  So, it seems to make sense to subtract the mean along with the trial ($M$) dimension.  Likewise, it might make sense to normalize the standard deviation along that dimension as well.
My question: Have others already figured this out and developed best practices for normalizing rewards?
I may have added my own twist, but I'm training in batches and using the stats of the batch (multiple rollouts) to do normalization.  The subtracting the mean part is called a baseline in some papers I think. This article discusses it a bit.
","['reinforcement-learning', 'rewards', 'reinforce', 'batch-normalization', 'standardisation']",
Does randomly adding hand-engineered features increase the CNN's sample efficiency/performance?,"
It is a known fact that preprocessing images using CV techniques will improve CNN performance (see this answer).
But what happens when you feed in the entire image and the filtered image randomly to the network? Would the Neural Network learn to focus on the relevant aspects of an unfiltered image?
If yes, please explain how randomly processed images improve the CNN's performance/sample efficiency.
","['convolutional-neural-networks', 'computer-vision', 'feature-engineering', 'sample-efficiency']",
"What does ""differentiable architecture"" mean?","
I'm currently reading a paper that uses CNN's as a base approach to solving some image classification issues and I've found that they kept mentioning the term ""Differentiable Architecture"", for which I have no idea about its meaning, as I'm new to this world of Deep Learning, Neural Networks, etc., so to sum up my question is
What does ""differentiable architecture"" mean?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'terminology', 'backpropagation']","
Without the specific context, I cannot give a definitive answer, but it's very likely that a ""differentiable architecture"" refers to a neural network that represents/computes a differentiable function (so you need to use differentiable activation functions, such as the sigmoid), i.e. you can take the partial derivatives of the loss function with respect to each parameter/weight of the neural network, so you can use backpropagation to find the gradient of the loss function, consequently, you can train this neural network with gradient descent, which is a numerical/iterative optimization algorithm for finding a (local) minimum of a function.
Most architectures you will find around are differentiable. In fact, gradient descent is the most widely used algorithm for training neural networks nowadays.
"
Defining states and possible actions in Q learning,"
I am trying to define the number of states and possible actions for a reinforcement learning problem that I want to solve with Q-learning, but I am a bit confused, as I'm totally new to reinforcement learning.
The problem I'm trying to solve is to assign different groups with people in the sample group having sequential numbers. Let's say there are three in each group.
Group 1, Group2, Group3.
1:{""group: Group1, ""number"": 1},
2:{""group: Group2, ""number"": 2},
3:{""group: Group3, ""number"": 3},
4:{""group: Group2, ""number"": 4},
5:{""group: Group1, ""number"": 5},
6:{""group: Group3, ""number"": 6},
7:{""group: Group3, ""number"": 7},
8:{""group: Group2, ""number"": 8},
9:{""group: Group1, ""number"": 9},

An optimal output will be a case where the numbers are sequential with respect to the group. For example, all in group1 should have number 1, 2, 3, or 4,5,6 or 7, 8, 9. and not 1, 5, 9 as in the dictionary above.
In other words,  group1, group2, group3 represent the group ids, which means I have 3 groups. The number represents seat numbers. All in each of the groups need to sit close to each other e.g seat numbers 1,2,3 or 4,5,6, or 7,8,9.
I am wondering if the possible state will be all possible combinations of the group and numbers, in which case it will be 1680 and the possible action is the number of numbers to swap to get the desired output which is 9.
Any useful information will be very much appreciated.
","['reinforcement-learning', 'q-learning', 'state-spaces']",
Do the terms multi-task and multi-output refer to the same thing in the context of deep learning?,"
Do the terms multi-task and multi-output refer to the same thing in the context of deep learning (with neural networks)? For example, do neural networks for multi-task learning use multiple outputs?
If not, what is the difference between them?
It would be helpful if you can also give examples.
I found some of the terms here. When I went to study this term on the Internet, I found it very convoluted, as different authors are found to be mixing up those terms.
","['neural-networks', 'comparison', 'terminology', 'multi-label-classification', 'multi-task-learning']",
Is there a gentle introduction to reinforcement learning applied to MDPs with continuous state spaces?,"
I am looking for a gentle introduction (videos, lecture notes, tutorials, books) on reinforcement learning (MDPs) involving continuous states (or very large cardinality of state space). In particular, I am looking for ways on how to deal with them, including a good discussion on the build up to important and relevant concepts.
Most of the books I encountered just state that we need function approximation, and then moved on to talk about radial basis functions. These ideas, however, are very abstract and are not easy to understand. For example, why specifically those functions?
","['reinforcement-learning', 'reference-request', 'continuous-action-spaces', 'continuous-state-spaces']",
Is it possible to compute the logical AND and OR with logistic regression?,"
It's easy to build a perceptron that can compute the logical AND and OR functions of its binary inputs.
Logistic regression could be used as a binary classifier.
$$z^{(i)} = w^T x^{(i)} + b$$
$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$
$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$
Is it possible to compute the AND and OR with logistic regression?
","['function-approximation', 'logistic-regression']",
What algorithms are used in Artificial General Intelligence research?,"
I've read on wiki that already in 2017 there were over 40 institutions researching AGI, and I wonder what type of algorithms are being studied and developed in this field.
For example, for comparison with narrow AI, where models/techniques, such as ANNs, CNNs, SVMs, DT/RT, evolutionary algorithms, or reinforcement learning are used, how would AGI models differ? Do they also use these models but in some specialised way or maybe these algorithms are completely new and different from these currently used in narrow AI?
","['reference-request', 'agi', 'research', 'algorithm-request', 'model-request']","
Current AGI approaches are very heterogenous and therefore there are no dominant algorithms.
Nevertheless, I would suggest you to have a look at Knowledge Graphs and the related algorithms to build and query the graphs, for instance here.
"
Why don't I get the same results of Q-Learning as in Aurélion Géron's Hands-on Machine Learning book?,"
I noticed something rather intriguing while testing the Deep Q-Network implementation from Aurélion Géron's book Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition; I copy-pasted the code exactly as it is but added some lines to get the graph on figure 18-10 presenting the sum of total rewards gained during each episode.
So everything is the same as the book except the training part where I added the lines related to rewards lists and the plotting:
all_rewards = []
for episode in range(600):
  obs = env.reset()
  episode_rewards = []
  for step in range(200):
    epsilon = max(1 - episode / 500, 0.01)
    obs, reward, done, info = play_one_step(env, obs, epsilon)
    episode_rewards.append(reward)
    if done:
      break
  all_rewards.append(episode_rewards)
  if episode > 50:
    training_step(batch_size)

sum_rewards = []
for i in range(len(all_rewards)):
  sum_rewards.append(sum(all_rewards[i]))

import matplotlib.pyplot as plt
episodes = range(1,601)
plt.plot(episodes, sum_rewards)

At my surprise I didn't get the same graph as the one the author presents in its book, so I reran the code again and got a totally different graph from what I had for the first time. Please find below two graphs that I obtained. I'm plotting the total sum obtained during each episode with respect to the episode.


I'd like to ask you if there is some intrinsic to the algorithm that makes it so random and in that case I'd like some references (if there are any) that prove that or I'm just doing something wrong. Thank you.
","['reinforcement-learning', 'deep-learning', 'dqn']",
Is it possible that the fine-tuned pre-trained model performs worse than the original pre-trained model?,"
I have downloaded a pre-trained EfficientDet D2 model (Tensorflow 2.0) and trained it on some data (about 20000 images with 20 classes). I set the number of steps to 25000 and batch size to 3 (computer resources are not the best).
However, if I try to make predictions, the pre-trained model makes better predictions than the model I have trained on the additional data. Is this expected behaviour?
For example, an image of a person may be 78% accurate on the pre-trained model and only 54% accurate on the same image when trained.
","['tensorflow', 'object-detection', 'transfer-learning', 'pretrained-models', 'fine-tuning']",
Is optimizing weighted sum multi objective tasks considered a multi-task learning?,"
I have two sequence prediction tasks, finding $\vec{\pi} \in \Pi$ and $\vec{\psi} \in \Psi$. Each sequence has its own objective function, i.e. $f_1(\vec{\pi})$ and $f_2(\vec{\psi})$. The input for the two sequence prediction tasks are also of different domain.
Say that by modification and extension in the model design, I can use one seq2seq or Pointer Network (or its variants) to produce the two sequence one at a time. In the training stage, however, the two objective functions are combined into $F(\vec{\pi}, \vec{\psi}) = \alpha f_1(\vec{\pi}) + \beta f_2(\vec{\psi})$ and the loss function to train the model use the combined objective function $F(\vec{\pi}, \vec{\psi})$.
Is this considered multi-task learning?
","['machine-learning', 'objective-functions', 'multi-task-learning']",
"Model not learning anything, what can be the problem?","
I've trained a model for heart sound classification with transfer learning (MobileNet) on Physionet dataset, and it works fine.
However, when I train it on my own dataset, it seems that it can not learn anything: more specifically, the loss is not decreasing and the accuracy is not going up. I've checked my labels and they seem to be correct. What other things should I check?
","['classification', 'training', 'transfer-learning']",
Where can I find the original conference paper that introduced Q-learning and Deep Q-Learning?,"
I tried searching a lot, but I could neither find the paper that introduced Q-Learning nor the paper that introduced Deep Q Learning. If anyone knows anything about it please do tell me.
","['reinforcement-learning', 'q-learning', 'reference-request', 'deep-rl', 'dqn']",
What is the sample complexity of Monte Carlo Exploring Starts in RL?,"
We can use a model-free Monte Carlo approach to solving an MDP $(S,A,R,P,\gamma)$ with transition dynamics $P$ unknown by estimating Q-values by rolling out trajectories starting from random states $s_0 \in S$ and improving the policy $\pi$ greedily. This is the Monte Carlo Exploring Starts algorithm in Sutton and Barto page 99 2nd edition.
Does anyone know if there is a sample complexity result for this algorithm?
","['reinforcement-learning', 'reference-request', 'markov-decision-process', 'monte-carlo-methods', 'sample-complexity']",
How can I approach this problem of producing a 3-bit binary string given a sequence of letters?,"
Suppose, I have the following data-set:
... ...
... ...
AABBB  7.027  5.338  5.335  8.122  5.537  6.408
ABBBA  5.338  5.335  5.659  5.537  5.241  7.043
BBBAA  5.335  5.659  6.954  5.241  8.470  8.474
BBAAA  5.659  6.954  5.954  8.470  9.266  9.334
BAACA  6.954  5.954  6.117  9.266  9.243 12.200
AABAA  5.954  6.117  6.180  9.243  8.688 11.842
ACAAA  6.117  6.180  5.393  8.688  5.073  7.722
ABAAC  6.180  5.393  6.795  5.073  8.719  7.854
BAACC  5.393  6.795  5.796  8.719  9.196  9.705
... ...
... ...

Apparently, the feature values represent a string pattern comprising of only three letters A, B, and C.
I have to design a neural network that would be able to detect these patterns and spit out a binary representation of these strings where the letters should be encoded in 3-bit binary(one-hot encoding).
My first question is, What kind of problem is it and why?
My next question is, How should I approach this problem to solve it?
",['classification'],"
If you're trying to predict the string pattern, given the numerical feature and assuming your string pattern is fixed sized, you can one-hot encode each letter then combine them (into an array that is no longer one-hot).
So AABBC would look like:
[1,0,0,1,0,0,0,1,0,0,1,0,0,0,1] <- Use this for training
[A,B,C,A,B,C,A,B,C,A,B,C,A,B,C]
[A,_,_,A,_,_,_,B,_,_,B,_,_,_,C]
AABBC

where each group of triplets represent a single integer.
Then you can train a network with cross-entropy.
This is the problem formulation of multi-task learning where you predict multiple things simultaneously.
Needless to say, it is classification.
"
How can I address missing values for LSTM?,"
I'm a student and writing my first paper for submission on conference. I have a question
there is a dataset below. this is temporal-spatial dataset.
Date         Hour   City       Sensor1  Sensor2  Sensor3 Sensor4 ...
21-06-10     0      Region1      0.12     0.52    0.33     0.44  ...
21-06-10     1      Region2      0.16     0.83    0.34     0.49  ...
21-06-10     2      Region1      0.21     0.44    0.57     0.5   ...
...

My Task is anomaly detection for each region
I want to use LSTM. So, I represent the temporal-spatial data to two time-series data. my dataset can be represented below.
City       Date       Hour     Sensor1  Sensor2  Sensor3 Sensor4 ...
Region1   21-06-10     0         0.12     0.52    0.33     0.44  ...
Region1   21-06-10     2         0.21     0.44    0.57     0.5   ...
...


City       Date       Hour     Sensor1  Sensor2  Sensor3 Sensor4 ...
Region2   21-06-10     1         0.16     0.83    0.34     0.49  ...
...

However, then, there is no a row with attribute 'Hour=1' in Region1 dataset
(you can see the table below)
City       Date       Hour     Sensor1  Sensor2  Sensor3 Sensor4 ...
Region1   21-06-10     0         0.12     0.52    0.33     0.44  ...
Region1   21-06-10     1         NaN      NaN     NaN      NaN   ...
Region1   21-06-10     2         0.21     0.44    0.57     0.5   ...
...

Can I insert estimated values into the row with attribute 'Hour=1' in Region1 dataset? (for example, I want to insert average between the first row and the third row)
Can I claim to have utilized a real world dataset even with this missing value estimation?
","['machine-learning', 'long-short-term-memory', 'datasets', 'data-preprocessing', 'data-science']",
Language Processing: Determine if one paragraph is relevant to another paragraph,"
Context: I want to determine if someone's written review contains content that is relevant to a paragraph that they are reviewing.
To do so, I am trying to determine if one paragraph is relevant to another paragraph. I initially tried to use TF-IDF to calculate the relevancy, but I think TF-IDF works well for determining if one paragraph is relevant to a whole set of paragraphs. I only want to determine if two paragraphs are relevant with each other.
What would be a good approach for this problem?
","['natural-language-processing', 'resource-request']",
What are the practical problems where full bayesian treatment is affordable?,"
Suppose, I have a problem, where there is rather a small number of training samples, and transfer learning from ImageNet or some huge NLP dataset is not relevant for this task.
Due to the small number of data, say several hundred samples, the use of a large network will very probably lead to overfitting. Indeed, various regularization techniques can partly solve this issue, but, I suppose, not always. A small network will not have much expressive power, however, with the use of Bayesian approaches, like HMC integration, one can effectively obtain an ensemble of models. Provided models in the ensemble are weakly correlated, one can boost the classification accuracy significantly.
Here I provide the picture from Mackay's book ""Information Theory Inference and Learning Algorithms"". The model under consideration is single layer neural network with a sigmoid activation function:
$$
y(x, \mathbf{w}) = \frac{1}{1 + e^{-(w_0 + w_1 x_1 + w_2 x_2)}}
$$
On the left picture, there is a result of Hamiltonian Monte Carlo after a sufficient number of samples, and, on the right, there is an optimal fit.

Integration over the ensemble of models produces a nonlinear separating boundary for NN.
I wonder, can this approach be beneficial for some small-size problems, but not toy, with real-life applications?
","['neural-networks', 'applications', 'bayesian-networks', 'bayesian-deep-learning']",
"What is meant by ""Zero-Shot Visual Recognition""?","
Many recent research papers contain the phrase ""Zero-Shot Visual Recognition"".
What exactly is meant by zero-shot visual recognition? Does the task need only images or also the other data like text?
","['terminology', 'definitions', 'zero-shot-learning']",
Is it possible to use (infinite cardinal) random variables during implementation?,"
Random variables can be broadly classified into three types:

random variables whose range is finite,
random variable whose range is countably infinite and
random variables whose range is uncountable.


Random variable is called discrete if its range (the set of values
that it can take) is finite or at most countably infinite.
Random variables that can take an uncountably infinite number of
values are not discrete

Almost all the probabilistic models used in artificial intelligence contain random variables.
In theory, one can deal with all three types of random variables. For suppose, in reinforcement learning or probabilistic graphical models, we can take any type of random variables as state or action spaces (in RL) and as nodes (in PGM) and can analyze.
But, in several textbooks, most of the analysis is restricted to random variables of the first type. The reason they mention is ""to make analysis easy"". It will be complex if we deal with either type 2 or type 3 random variables. So, textbooks and materials generally prefer analysis with type 1 only.
My doubt is:
Do researchers use random variables of type 2 or type 3 during the implementation of (any) AI tasks? Is it impossible to use them due to their (infinite) cardinality? If possible, please provide an example mechanism for implementing such random variables.
","['implementation', 'random-variable']",
Upscaling a low-res IR image with a high res webcam image,"
I have a low resolution thermal/IR image (for example 32x32 or 80x64) and a high resolution webcam image. I would like to combine the two to ""fake"" a high resolution thermal image (I can already map them together via homography). One could probably just apply a FLIR-like palette to the IR image, scale it up, and combine it with the brightness channel of the visible spectrum image. But that would of cause visible artifacts at the pixel edges of the IR picture.
I wonder if there is an AI based approach to colorize the webcam image with the IR data. When a warm IR pixel partially covers a person and partially the background, it would only color the ""person"" warm, and take the ""background"" color from the neighboring IR pixel. For this it would have to consider a small vicinity of either picture at a time.
Although I'm familiar with machine learning in the context of multivariate analysis and classification, I have no experience with modern deep learning or AI based image processing. I would guess that something like style transfer would be a starting point for what I'm trying to achive. One would need 1) a way to identify features (like foreground/background, person/wall) and 2) a way to combine these features with the IR truth to result in a colorized bitmap, I assume.
What would be the best approach to do this? Maybe this is already a solved problem - I have a feeling this might already be a solved problem. In any case I would be grateful for literature pointers.
","['computer-vision', 'style-transfer']",
"What is the proper way to process continuous sequence data, such as time-series, using the Transformer?","
What is the right way to input continuous, temporal (time-series) data into the Transformer? Assume we're using the basic TransformerBlock here.
Since data is continuous with no tokens, Token embedding can be directly skipped. How about positional encoding? I tried this example, removing Token embedding while keeping positional encoding but ended up with shape-related errors. Skipping both token and positional encoding resulted in a network that runs and trains but results were relatively poor compared to the LSTM benchmark with the same data.
I am unsure if the positional encoding is still needed.
Overall, my question is, what is the proper way to process continuous sequence data, such as time-series, using the Transformer architecture?
","['deep-learning', 'keras', 'transformer', 'time-series', 'sequence-modeling']","
Instead of using a token embedding you can use a linear layer. For an input of (10, 5, 4) - (sequence length, batch size, features) you can create a linear layer:
self.embedding_layer = nn.Linear(4, d_model)

Where d_model is the dimension of the input to the transformer.
PositionalEncoding is still needed so as to have a representation of time in the inputs.
src = self.embedding_layer(src)
src = self.pos_encoding_layer(src)
output = self.transformer(src)

"
Dissection of a depth map,"
I am curious about how depth maps work. While searching I came across this website which contains some images and their depth maps. I took this depth map and tried to study it using a python pillow.

from PIL import Image
import numpy as np

image = Image.open('elephant_depth_s.png')
img = np.asarray(image)

print(img.shape)

The depth map shape is (400, 400, 3) with 3 channels. Contrary to my assumption this depth map has three channels instead of one. Even though most of the values are zeros some are not. This len(np.where(img>0)) shows that all the channels have some values greater than zero. My question is;

In color images, RGB channel values are used for creating corresponding color pixels. Example  RGB (255,255,0) creates yellow.

In this depth map how these three channel corresponds to the depth?
Can you please give us some more information on depth maps and their real-world applications?
","['computer-vision', 'deep-neural-networks', 'image-processing', 'image-segmentation', 'semantic-segmentation']","
Depth maps are created using principles of photometry (method of measuring light).
The depth maps (rather images) you took from the website are ""images"" not exact depth ""maps"". So by default when you pull out a png image from a webpage, it will be saved in ""RGB"". That is the reason you got an array with 3 layers. In practice, it will always be a single layer that simply shows relative brightness at each point.
The webpage you referred to is talking about ""ray tracing"" using software called PoVRay (Persistence of Vision Ray Tracing). What it does is creates a 3D surface with a source of light at a point in the frame and simply measures the intensity of light falling on the surface. Remember this is 3D. When you capture this surface from a point using a camera you get a 2D image that represents a ""depth map"" as seen from a point of reference with respect to the source of light.
A depth map has plenty of applications in computer vision, photography, and ray tracing. Illuminating a frame, measuring depth are few such applications.
"
Why doesn't the LSTM model improve the time-series forecasting significantly with respect to the MLP model?,"
I have recently started learning time series forecasting. I have a dataset of the weekly payment history of 10k clients over 1 year, and I want to predict the future 5 payments for a test set of 1k clients.
From what I have tried, I've found that using LSTMs instead of a simple MLP doesn't improve the prediction as much as I anticipated.
My understanding is that LSTMs captures the relations between time steps, whereas simple MLPs treat each time step as a separated feature (doesn't take succession into consideration).
So, my question is: why doesn't the LSTM model improve the forecasting significantly? What are the best models for such a task, given that the time series are short (maximum sequence's length = 52)?
","['deep-learning', 'long-short-term-memory', 'time-series', 'multilayer-perceptrons', 'forecasting']","
RNNs are known to be superior to MLP in case of sequential data, like yours. But complex models like LSTM and GRU require a lot of data to achieve their potential. I don't know about your data but you can try to validate your architecture, approach and overall setting using a different, known time-series benchmark data.
Maybe something is wrong with architecture, loss function, data, etc...
So trying a different but known benchmark data can give you an idea about why you are unable to produce superior results with LSTM.
"
Is the main difference between the logistic regression and the perceptron the activation function they use?,"
I went through a Stats StackExchange's post about the difference between logistic regression and perceptron, which is too long to get the key point.
I'd like to consider the question in terms of the formulas for them.
The logistic regression is defined as
$$\hat{y} = \sigma(\mathbf{w} \cdot \mathbf{x} + b)$$
where
$$
\sigma(z) = 
\dfrac {1}{1+e^{-z}}
$$
The perceptron is defined as
$$\hat{y} = sign(\mathbf{w} \cdot \mathbf{x} + b)$$
where
$$
sign(z) = 
\begin{cases}
1, & z \ge 0 \\
-1, & z < 0
\end{cases}
$$
So, the main between the two models is the activation function, is my understanding correct?
","['comparison', 'training', 'activation-functions', 'perceptron', 'logistic-regression']",
Does this code mean the model trains 10 epochs?,"
Here is an implementation for Perceptron
class Perceptron:
    def __init__(self, eta=.1, n_iter=10, model_w=[.0, .0], model_b=.0):
        self.eta = eta
        self.n_iter = n_iter
        self.model_w = model_w
        self.model_b = model_b

    def predict(self, x):
        if np.dot(self.model_w, x) + self.model_b >= 0:
            return 1
        else:
            return -1

    def update_weights(self, idx, model_w, model_b):
        w = model_w
        b = model_b
        w += self.eta * y_train[idx] * x_train[idx]
        b += self.eta * y_train[idx]
        return w, b

    def fit(self, x, y):
        if len(x) != len(y):
            print('error')
            return False
        for i in range(self.n_iter):
            for idx in range(len(x)):
                if y[idx] != self.predict(x[idx]):
                    self.model_w, self.model_b = self.update_weights(idx,
                                            self.model_w, self.model_b)

Does this code
Perceptron(eta=.1, n_iter=10)

mean the model trains 10 epochs?
",['terminology'],
What is meant by decoding in a Hidden Markov Model?,"
HMM contains two types of states: observable and hidden. Let $\{ h_1,h_2,h_3,\cdots,h_n\}$ be hidden states and $\{o_1,o_2,o_3,\cdots, o_m\}$ be the observable states.
Suppose the $n^2$ transition probabilities $p(h_j|h_i)$ and the $mn$ emission probabilities $p(o_j/h_j)$ are given along with the initial probability distribution vector $\pi =[\pi_1, \pi_2, \pi_3, \cdots, \pi_n]$
Then what is meant by decoding in HMM?
","['machine-learning', 'hidden-markov-model']",
How to handle cycles in minimax algorithm,"
For example, I am implementing AI for turn based game and have enough computational resources for build full game tree. My problem is the game can be infinite if both players will repeat moves and my minimax implementation stucks because game tree is infinite respectively.
For example, my game is in state S1, player 1 do action A1, player 2 do action A2 and we are again in state S1. I can't evaluate S1 node because I need to evaluate all subnodes.
I have no idea how to handle this.
","['game-ai', 'minimax']",
Computational complexity of a CNN network,"
In the following network, the convolution operations of convolutional blocks are performed by three 1-D kernels with the sizes 8, 5, and 3 respectively along with stride equal to 1. The final network is constructed by stacking three convolution blocks with the filters of sizes 128, 256, and 128 in each block. Pooling operation is excluded from the network. I wanted to find the computational complexity of the following network. I was wondering if you could give me some hints to compute the computational complexity of this network. I appreciate your time! Thanks!

","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'deep-neural-networks', 'fully-convolutional-networks']",
How to interpret the policy gradient expression in reinforcement learning?,"
I'm currently going through the OpenAI's spinning up introduction course to reinforcement learning. On one of the final sections, they derive an expression for the gradient of the undiscounted return with respect to the policy weights:
$$\nabla_{\theta} J\left(\pi_{\theta}\right)=\underset{\tau \sim \pi_{\theta}}{\mathrm{E}}\left[\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) R(\tau)\right]$$
Then they give the following explanation:

Taking a step with this gradient pushes up the log-probabilities of each action in proportion to $R(\tau$).

My question is: How does this expression mathematically reflect the fact that this gradient will push up the log probabilities of the actions?
","['reinforcement-learning', 'deep-rl', 'math', 'policy-gradients']","
The value of the objective depends on policy (probabilities of taking an action). Intuitively speaking, better actions lead to better returns and by ""pushing up"" the probabilities (log or not same thing since log is monotonically increasing function) of those actions you're making sure you're getting better returns and increasing the value of your objective.
"
Are there any advantages of the local attention against convolutions?,"
Transformer architectures, based on the self-attention mechanism, have achieved outstanding performance in a variety of applications.
The main advantage of this approach is that the given token can interact with any token in the input sequence and extract global information since the first layer, whereas CNN has to stack multiple convolutional or pooling layers in order to achieve a receptive field, that would involve the whole input sequence.
By receptive field I mean the number of timestamps from the input signal on which does the output depend. For example, for sequence of two Conv1D with kernel_size=3 receptive field is 5. And in transformer the output of the first blocks depends on the whole sequence.
However, this comes at large computational and memory cost in the vanilla formulation:
$$
O(L^2)
$$
where $L$ is the length of the sequence.
There have been proposed various mechanisms, that try to reduce this amount of computation:

Random attention
Window (Local attention)
Global attention

All these forms of attention are illustrated below:

And one can combine different of these approaches as in the Big Bird paper
My question is about local attention, attending only to the tokens in the fixed neighborhood of size $K$.
By doing so, one reduces the number of operations to:
$$
O(L K)
$$
However, now it is local as the ordinary convolution, and global receptive field will be achieved only via stacking many layers.
Are there any advantages of Local self-attention against CNN, or it can be beneficial only in combination with other forms of attention?
","['convolutional-neural-networks', 'attention', 'convolution']","
It is true that when using local attention with a window of size 5, the ""receptive field"" is the same as a CNN with kernel size 5 (or two CNN layers with kernel size 3). However, there is a key difference in how the learned weights are applied to the inputs.
In a CNN, the values of the many convolutional kernels are learned, but once learned, the kernels are static. In other words, at every position in the input (whether it be a 1D signal or 2D image), the dot product between the inputs within the window and the same CNN kernels is taken, and then a non-linear function applied.
With attention, the Query/Key/Value matrices additionally allow context to be taken into account. Instead of taking the dot-product of the input region with a set of fixed kernels, the additional matrices are effectively used to dynamically compute a new set of kernels for each position. ""Attention"" basically figures out for each convolution, which inputs are important (which inputs the network should ""pay attention"" to) by computing higher-valued weights using Q, K, and V.
I highly recommend reading a breakdown of the original ""Attention is All You Need"" paper such as this blog post: https://jalammar.github.io/illustrated-transformer/
"
How does a decision tree split a continuous feature?,"
Decision trees learn by measuring the quality of a split through some function, apply this to all features and you get the best feature to split on.
However, with a continuous feature it becomes problematic because there are an infinite number of ways you can split the feature. How is the optimal split for a continuous feature chosen?
","['decision-trees', 'feature-engineering']","
Decision tree can be utilized for both classification(categorical) and regression(continuous) type of problems. The decision criterion of decision tree is different for continuous feature as compared to categorical.
The algorithm used for continuous feature is Reduction of variance. For continuous feature, decision tree calculates total weighted variance of each splits. The minimum variance from these splits is chosen as criteria to split.
look into this decision tree basics article, section 3
"
How much C++ is needed for research in machine learning and artificial intelligence?,"
I am currently doing a master's in applied mathematics, and I recently got interested in machine learning and artificial intelligence, and I am thinking of going for a Ph.D. in this area.  I have a reasonable maths and stats background, but I haven't done any course in ML/AI. Next semester, I am thinking of doing courses in ML (uses the book by Bishop), AI (uses the book by Norvig) and reinforcement learning at my university. Another advanced course in C++ is being offered, which I am also very interested to take, but the problem is it will be very difficult to manage all of these courses together. I have some knowledge of C++ (built some parts of a reasonably big project in the past but got a bit rusty nowadays) and very basic knowledge of Python, though I find Python much easier to learn and use than C++.
So, my question is: how important is C++ if I go for a Ph.D. in ML/AI/CV/NLP, etc.? Should I bother taking the C++ course or be more focused on Python and do the other three courses i.e., ML, AI, and reinforcement learning?
","['academia', 'programming-languages', 'software-evaluation', 'education']","
Of course, whether or not you will need to know and use C++ depends on the topics you will research during your Ph.D. or job. If you'll need just to use and/or combine some existing ML models (yes, in a Ph.D., you're expected to come up with new ideas/tools), then you won't probably need to know C++, as the most commonly used libraries for machine learning nowadays, such as TensorFlow, Keras, or PyTorch, have their main APIs written in Python (but there are also APIs written in other languages, but they are not typically as mature as the Python ones), although the core of these libraries is or can be written in C++, but you may never need to have to look at the core of these libraries.
I can say that I also know C++ (of course, not everything or every detail and library, and, of course, my knowledge of it also becomes rusty if I don't use it for a long time), but I rarely need to use my knowledge of C++ to do research in ML or AI (which is what I am currently doing), but, again, it all depends on the topic of your Ph.D. For example, if you wanted to contribute to the progress of OpenCog or if your Ph.D. involved an efficient implementation of some algorithm or data structure, then it may be a good idea to know C++, C, or a programming language like Rust.
"
What is a better approach to perform predictions of time-series several values ahead?,"
Suppose one has a time series (univariate or multivariate) and the goal is to predict values of these series several steps ahead. I see two possible strategies:

Create a model (recurrent, convolutional, transformer, whatever) that predicts the value of the signal in the next moment of time, based on the values from previous timestamps from (t_start, t_end). If we aim to predict not one, but several steps ahead we can pass (signal[t_start + 1: t_end], signal[t_end + 1]) to predict signal[t_end + 2] and so on.  In the training stage, we can pass the predicted value of signal[t_end + 1] or the ground truth with some probability, this can be seen as some kind of teacher forcing. In the inference stage, one passes each time the predicted signal. The optimization algorithm aims to minimize (MSE, MAE) loss between the ground truth and prediction. In other words
$$
\begin{aligned}
x_{t+1} &= f(x_t, \ldots, x_{t-N+1}) \\
x_{t+2} &= f(x_{t+1}, \ldots, x_{t-N+2}) \\
x_{t+k} &= f(x_{t+1}, \ldots, x_{t-N+k}) \\
\end{aligned}
$$

Create a model that predicts simultaneously several values ahead. Standard layers from DL frameworks (PyTorch of Tensorflow) for sequence processing problems have two options - output single hidden state in the end or the whole sequence of the hidden states. Therefore, seems like they do not have the functionality, say, to predict values of the time series 16 steps ahead from the values of the last 256 timestamps.
$$
[y_{t+k}, \ldots, y_{t+1}] = f(x_t, \ldots x_{t - N + 1})
$$
I see two potential solutions:

output hidden state (16) times larger than the expected output and reshape - however, it seems that this approach breaks the locality and causal structure and would not achieve good performance.
Choose the option, that returns the sequence of the same length as the input (here 256) and take the last (16) tokens of the output. This approach is inapplicable if the length of the prediction exceeds the length of the previous history, but I think, that such long predictions would produce poor quality in any case.



How stock, weather, sales prediction problems are solved usually in practice?
","['machine-learning', 'prediction', 'time-series', 'forecasting']","
I have found nice tutorial in the Tensorflow documentation: https://www.tensorflow.org/tutorials/structured_data/time_series
They implement and test both strategies.

In the first case, for multi dimensional time series, they output the vector of dimension out_steps * series_dim and then reshape to (out_steps, series dim)

They create a model (AR LSTM), that predicts one step ahead and then apply it several times, where the first step from the previous input is discarded, and the new prediction is last step in new input.



Second approach, seems to require less params, but the obtained quality for this specific case, seems to be comparable for both cases.
"
RLLib - What exactly do the avail_action and action_embed_size represent? How do they work with the action_mask to phase out invalid actions?,"
So, I'm fairly new to reinforcement learning and I needed some help/explanations as to what the action_mask and avail_action fields alongside the action_embed_size actually mean in RLlib (the documentation for this library is not very beginner friendly/clear).
For an example, this is one of the resources (Action Masking With RLlib) I tried to use to help understand the above concepts. After reading the article, I completely understand what the action_mask does, but I'm still a bit confused as to what exactly the action_embed_size is and what the avail_actions fields actually are/represent (are the indices of avail_actions supposed to represent the action 0 if invalid, 1 if valid? Or are the elements supposed to represent the actions themselves - a value of 1, 4, 5, etc corresponding to the actual value of the action itself?).
Also when/how would there be a difference with the action_space and action_embed_size?
This is from the article that I used to sort of familiarize myself with the whole concept of Action Masking (this network is designed to solve the Knapsack Problem):
class KP0ActionMaskModel(TFModelV2):
    
    def __init__(self, obs_space, action_space, num_outputs,
        model_config, name, true_obs_shape=(11,),
        action_embed_size=5, *args, **kwargs):
        
        super(KP0ActionMaskModel, self).__init__(obs_space,
            action_space, num_outputs, model_config, name, 
            *args, **kwargs)
        
        self.action_embed_model = FullyConnectedNetwork(
            spaces.Box(0, 1, shape=true_obs_shape), 
                action_space, action_embed_size,
            model_config, name + ""_action_embedding"")
        self.register_variables(self.action_embed_model.variables())

    def forward(self, input_dict, state, seq_lens):
        avail_actions = input_dict[""obs""][""avail_actions""]
        action_mask = input_dict[""obs""][""action_mask""]
        action_embedding, _ = self.action_embed_model({
            ""obs"": input_dict[""obs""][""state""]})
        intent_vector = tf.expand_dims(action_embedding, 1)
        action_logits = tf.reduce_sum(avail_actions * intent_vector,
            axis=1)
        inf_mask = tf.maximum(tf.log(action_mask), tf.float32.min)
        return action_logits + inf_mask, state

    def value_function(self):
        return self.action_embed_model.value_function()

From my understanding, the action_embedding is the output of the neural network and is then dotted with the action_mask to mask out illegal/invalid actions and finally passed to some kind of softmax function to get the final neural network output?
Please, correct me if I'm wrong.
","['neural-networks', 'reinforcement-learning', 'multi-agent-systems']",
Why do I get bad results no matter my neural network function approximator for parametrized Q-learning implementation for Contextual Bandits?,"
I'd like to ask you why, no matter my neural network function approximator for parametrized Q-learning implementation for a Contextual Bandits environment, I'm getting bad results. I don't know if it's a problem with my formulation of the problem and how I'm trying to solve it, or is it the neural architecture. I tried different fully-connected neural networks with different number of layers and different number of neurons (sticking to low numbers since my environment is not complex) but I always get bad results, and it seems the results are random.
if my implementation of the Q-learning algorithm for the Contextual Bandits problem is right. I made an environment that randomly generates three integers between 0 and 89 and given an action (integer between 0 and 4) it returns a reward following a certain logic (if all three integers are between 0 and 29 and the action is 0 then the reward is 0 otherwise it's -1).
My environment is:
class Environment():

  def __init__(self):
      
      self._observation = np.zeros((3,))
  
  def interact(self, action):
      self._observation = np.zeros((3,))
      c1, c2, c3 = np.random.randint(0, 90, 3)
      self._observation[0]=c1
      self._observation[1]=c2
      self._observation[2]=c3
      reward = -1.0
      condition = False
      if (c1<30) and (c2<30) and (c3<30) and action==0:
          condition = True
      elif (30<=c1<60) and (30<=c2<60) and (30<=c3<60) and action==1:
          condition = True
      elif (60<=c1<90) and (60<=c2<90) and (60<=c3<90) and action==2:
          condition = True
      else:
          if action==4:
              condition = True
      if condition:
        reward = 0.0
            
      return {""Observation"": self._observation,
                  ""Reward"": reward}

The interaction method doesn't return state or time step, not like what TF-Agents environments' step method does. I just thought it's not necessary for the current problem; I don't rely on time steps since each state doesn't influence the next state. I thought that observation is what should be returned, the state being a more general data that could contain information the agent can't observe. I don't return the action too because we can get it outside the environment.
My function approximator of the Q-values are neural networks, always a fully connected architecture. For instance:
model = keras.models.Sequential([
        keras.layers.Dense(16, activation=""relu"", input_shape=[n_inputs]),
        keras.layers.Dense(16, activation=""relu""),
        keras.layers.Dense(n_outputs)])

I took the next blocks of code from Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition and adapted them to my situation:
env = Environment()

n_inputs = 3 #Observations are made of three integers
n_outputs = 4 #Four actions

def epsilon_greedy_policy(observation, epsilon=0):
  if np.random.rand() < epsilon:
    return np.random.randint(4)
  else:
    Q_values = model.predict(observation[np.newaxis])
    return np.argmax(Q_values[0])

replay_buffer = deque(maxlen=2000)

def sample_experiences(batch_size):
  indices = np.random.randint(len(replay_buffer), size=batch_size)
  batch = [replay_buffer[index] for index in indices]
  observations, rewards, actions = [np.array([experience[field_index] for experience in batch]) for field_index in range(3)]
  return observations, rewards, actions

def play_one_step(env, observation, epsilon):
  action = epsilon_greedy_policy(observation, epsilon)
  observation, reward = env.interact(action).values()
  replay_buffer.append((observation, reward, action))
  return observation, reward

batch_size = 16
optimizer = keras.optimizers.Adam(learning_rate=1e-3)
loss_fn = keras.losses.mean_squared_error

def training_step(batch_size):
  experiences = sample_experiences(batch_size)
  observations, rewards, actions = experiences
  target_Q_values = rewards
  mask = tf.one_hot(actions, n_outputs)
  with tf.GradientTape() as tape:
    all_Q_values = model(observations)
    Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)
    loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))
  grads = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(grads, model.trainable_variables))

epsilon = 0.01
obs = np.random.randint(0,90,3)
for episode in tqdm(range(1000)):
  if episode<250:
    obs, reward = play_one_step(env, obs, epsilon)
  else:
    obs, reward = play_one_step(env, obs, epsilon)
    training_step(batch_size)

I'm not sure at how to evaluate the performance of the agent, but I tried this as a first approach just to see if the predicted Q-values will enable a greedy-policy to choose the best action:
check0 = np.random.randint(0,30,3)

for i in range(30):
  arr = np.random.randint(0,30,3)
  check0 = np.vstack((check0, arr))

predictions = model.predict(check0)

c = 0
for i in range(predictions.shape[0]):
  if np.argmax(predictions[i])==0:
    c+=1

(c/predictions.shape[0])*100

Every time I ran the code above it gave me a totally different value. Sometimes it's 0%, sometimes it's 45%, sometimes it's 19%...
The issue is that no matter my model architecture, at the end, I get random results. I wonder if it's something wrong in the overall approach to solve the problem. I want to solve a Contextual Bandit where the agent observe a continuous context, take actions and try to link together the rewards obtained with the actions and the context in order to ""understand"" the logic behind it.
I hope you can help me figure out why do I get these random results.
Thank you.
","['reinforcement-learning', 'q-learning', 'dqn', 'multi-armed-bandits', 'contextual-bandits']",
Conditional input deep neural network,"
I need to input data conditionally to my deep network. In order to explain cases, I'd like to give an example. Assume that I have a 50-attribute dataset. For some attributes, a specific part of hidden layers is responsible, and for others, a different part is responsible. Also, for some cases, the same parts of the hidden layers might intersect. I think I can decide which attributes must go which hidden neurons in the input layer by using some kind of if-else block. However, I could not figure out how.

My current idea

I can enter an identity element for some attributes. For example, I have att1, att2, att3, etc. I have ins1, ins2, etc.
For ins1 -> att1 = 0.5, att2 = 0.2, att3 = None
For ins2 -> att1 = 0.1, att2 = None, att3 = None
But, if I do this approach, the number of attributes for an instance becomes bigger unnecessarily.

End of my current idea

Are there any opinions on this? Should I rearrange my excel file or is there any way to use if-else conditions?
Regards,
","['neural-networks', 'deep-learning', 'ai-design', 'conditional-random-field']",
Why is the input layer of a neural network usually not counted?,"
I came across the following statement from the caption of figure 7.8 from the textbook Neural Networks and Neural Language Models

the input layer is usually not counted when enumerating layers

Why is the input layer excluded from counting?
Is the reason just convention or based on its contribution?
","['neural-networks', 'hyper-parameters', 'input-layer']","
The input layer is just an abstraction for defining the number and/or type/shape of inputs that the neural network accepts (for example, in Keras, you can use the class InputLayer), so it doesn't usually compute any function (although it's possible that your implementation of the input layer performs e.g. some kind of preprocessing), like the other layers, including the output layer, do, but it just represents the inputs, which are passed to the next layer during the forward pass.
Whether it's counted or not as part of the count of the number of layers of a neural network, it's just a matter of convention. If it's not counted, it's probably because of the just mentioned reasons.
"
What are (all) the differences between a neuron and a perceptron?,"
I know two differences between a neuron and a perceptron


Neuron employs non-linear activation function and perceptron employs only a threshold activation function.

The output of a neuron is not necessarily a binary number and the output of a perceptron is always a binary number



I know no other difference between a perceptron and a neuron other than the above.
Are there any other differences between perceptron and neuron?
","['comparison', 'perceptron', 'neurons']","
In addition to those mentioned differences, a perceptron can be thought of as a standalone model (which is trained with a specific algorithm, the perceptron algorithm), while the artificial neuron (sometimes only referred to as neuron, in a similar way that an artificial neuron network is commonly abbreviated to neural network) is the smallest computational unit of a neural network, so it's an abstraction for a relatively simple function (e.g. sigmoid) that will be composed with other simple functions to produce a more complicated function, which is typically non-linear.
Moreover, note that people often refer to ""regular"" neural networks as multi-layer perceptrons (abbreviated to MLPs) for one simple reason: you can think of such an MLP as the composition of multiple perceptrons, where, in this case, the perceptron would be a synonym for artificial neuron, so the smallest computational unit of a neural network, which performs, for example, a linear combination of its inputs followed by the application of an activation function, which can be the sigmoid, tanh, ReLU, identity, or any other function that is differentiable, if you plan to train the neural network with gradient descent.
So, sometimes, the term perceptron is a synonym for artificial neuron, so the perceptron (aka neuron), in this case, could have any activation function. However, the perceptron is often assumed to have the sign function as the activation function, which is not strictly differentiable, while, as you point out, artificial neurons are not limited to the sign function.
The original (photo)perceptron models, as described in this paper, were more complicated (e.g. the inputs were not directly connected to the outputs, or you could have feedback connections), so the definitions of these concepts or what these terms refer to have evolved or can still evolve. In the past, I have also seen people use the term perceptron to refer to an MLP, but this is probably because they were not aware of the model that we typically refer to as the perceptron, for example, as described in section 8.5.4 (p. 265) of the book Machine Learning: A Probabilistic Perspective by Kevin Murphy (you can find free pdfs of this book on the web).
"
"Why is tanh a ""smoothly"" differentiable function?","
The sigmoid, tanh, and ReLU are popular and useful activation functions in the literature.
The following excerpt taken from p4 of Neural Networks and Neural Language Models says that tanh has a couple of interesting properties.

For example, the tanh function has the nice properties of being
smoothly differentiable and mapping outlier values toward the mean.

A function is said to be differentiable if it is differentiable at every point in the domain of function. The domain of tanh is $\mathbb{R}$ and $ \dfrac{e^x-e^{-x}}{e^x+e^{-x}}$ is differentiable in $\mathbb{R}$.
But what is meant by ""smoothly differentiable"" in the case of tanh activation function?
","['neural-networks', 'terminology', 'math', 'activation-functions', 'tanh']",
How to define a continuous action distribution with a specific range for Reinforcement Learning?,"
Specifically for continuous control PPO, let's say my action space range is between  $X$ (low) and $Y$ (high) and they are all sampled from a Gaussian Action Distribution with mean $\mu$ and standard deviation $\rho$.
From what I understood, the actions sampled should fall between $\mu - \rho$ and $\mu + \rho$, but that's not what happens in practice? What am I misunderstanding here? How do I ensure this range constraint from a custom action distribution with a given mean and standard deviation?
Any advice or tips for me? I would really appreciate any insights!
","['reinforcement-learning', 'probability-distribution', 'proximal-policy-optimization', 'continuous-action-spaces', 'normal-distribution']",
Semantic-based evaluation of translations instead of BLEU,"
I have a text generation model and I want to evaluate its output by comparing it to a set of gold human-annotated references.
I went through machine-translation metrics and I found that BLEU is used as the main metric usually.
I didn't like using it because it's shallow as it uses ngrams comparison; the semantics of the translation is missed.
Is there any other metric to do a semantic-based evaluation?
I've thought of using a text similarity model to evaluate the output or even an NLI (Natural language inference) system. I am not sure how precise the evaluation will be because SOTA systems are not really accurate.
","['natural-language-processing', 'machine-translation', 'text-generation']","
There are other possible metrics, e.g. meteor and BLEURT.
They compensate some of the basic problems most researchers would like to avoid BLEU for. The downside of not using known metrics is, that your model is even harder to evaluate against other candidates. If you compare to human gold standard corpus, you should not count on BLEURT too much since it is actually intended to evaluate two systems against a gold corpus and tell you which is better.
"
Why adding a baseline doesn't affect the policy gradient?,"
On the OpenAI's Spinning Up, they justify the fact that adding a baseline $b(s_t)$ in the policy gradient doesn't change its gradient by saying that this is

an immediate consequence of the EGLP Lemma

However, I did not manage to prove it with this lemma. Can somebody help me, please?
The proof is trivial when $b$ is a constant, but I struggle to derive it whenever $b$ is a function of the current state $s$ because you can't take it out of the integral.
","['policy-gradients', 'proofs', 'open-ai', 'reinforce']","
The policy gradient states that
$$\nabla J(\theta) \propto \sum_s \mu(s) \sum_a q_\pi(s, a) \nabla\pi(a | s; \theta)\;$$
where the derivatives are taken wrt the parameter $\theta$.
Now, if we say incorporate a baseline we get
$$\nabla J(\theta) \propto \sum_s \mu(s) \sum_a \left( q_\pi(s, a) - b(s) \right)\nabla\pi(a | s; \theta)\;$$
and this does not effect the gradient at all. To see this, note that
$$\sum_a b(s) \nabla\pi(a|s; \theta) = b(s) \nabla \sum_a \pi(a|s; \theta) = b(s) \nabla 1 = 0\;;$$
where all I have done is expand the bracketed terms inside the sum over $a$ from the second equation, and shown that the new term is equal to 0 -- thus the gradient is unchanged.
If you really want to confirm this then you can fully write down the expansion of the second equation and use the trick I have shown you in my third equation to see that expanded second equation is equal to the first equation.
I imagine that the EGLP lemma that the authors referred to will use a similar trick of a derivative of a probability distribution equalling to 0 when summing(/integrating) over the support of the random variable, which is what I have used here to go from $\nabla \sum_a\pi(a|s; \theta) = \nabla 1$.
"
How to determine the embedding size?,"
When we are training a neural network, we are going to determine the embedding size to convert the categorical (in NLP, for instance) or continuous (in computer vision or voice) information to hidden vectors (or embeddings), but I wonder if there are some rules to set the size of it?
","['deep-learning', 'hyperparameter-optimization', 'hyper-parameters', 'embeddings']","
In most cases, seems that embedding dim is chosen empirically, by trial and error.
Older papers in NLP used 300 conventionally https://petuum.medium.com/embeddings-a-matrix-of-meaning-4de877c9aa27. More recent papers used 512, 768, 1024.
One of the factors, influencing the choice of embedding is the way you would like different vectors to correlate with each other. In high dimensional space with probability 1, chosen at random vectors would be approximately mutually orthogonal. Whereas in the low dimensions and case of many different classes, many vectors will have dot product, significantly different from 0.
I think, that if one expects, that many vectors have to be correlated then the dimension shouldn't be very high. And otherwise, if each of the possible keys in the embedding is expected to produce a different, unrelated vector, than dimensionality is expected to be large.
"
How can we find the value function by solving a system of linear equations?,"
I am following the book ""Reinforcement Learning: An Introduction"" by Richard Sutton and Andrew Barto, and they give an example of a problem for which the value function can be computed explicitly by solving a system of $\lvert S \rvert $ equations that have $\lvert S \rvert $ unknowns. Each of these $\lvert S \rvert$ equations is given by:
$$v_{\pi}(s) = \sum_{a} \pi(a\rvert s) \sum_{s^{\prime}}\sum_{r} p(s^{\prime}, r \rvert s,a)[r + \gamma v_{\pi}(s^{\prime})] $$
I am having a hard time understanding how one could solve this system of equations. It seems to me as if each equation consists of a summation of an infinite amount of terms and therefore one would not be able to analytically solve them. Could anyone offer any intuition as to how this system of equations could be explicitly solved?
","['reinforcement-learning', 'value-functions', 'bellman-equations', 'linear-algebra']","
Provided you have a finite number of states and actions, then there will not be an infinite number of terms. Therefore the state and action spaces need to be discrete and finite before the quote from the book applies.

I am having a hard time understanding how one could solve this system of equations.

There are a few techniques for solving simulteneous equations.
However, what I would probably do is number all the state values from $v_1 = v_\pi(s_1)$ to $v_{N = |\mathcal{S}|} = v_\pi(s_N)$, and write out each line in order:
$$v_1 = w_{1,1} v_1 + w_{1,2} v_2 + w_{1,3} v_3 + ... w_{1,N} v_N + r_1$$
Where $r_1$ is a constant - it is the expected immediate reward when starting from state $1$, but that is not important. It is the constant offset value you get from resolving the sum that is not multiplied by any $v_i$ unknown variable.
You can discover the values of $w_{i,j}$ by expanding the sum in the Bellman equation for each state in turn.
At that point you can build a matrix of the weights, and solve the linear equations by taking the inverse of the matrix.

[from comments] But if the game has no end then theoretically the sum of expected future rewards should be infinite.

The time series definition of $v_{\pi}(s)$:
$$v_{\pi}(s) = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]$$
does not appear in the Bellman equation used to establish the linear equations. This is the main benefit of the Bellman equation, it changes the infinite series view of returns into a set of relations that must hold between the value functions of each state.
"
Does BERT freeze the entire model body when it does fine-tuning?,"
Recently, I came across the BERT model. I did some research and tried some implementations.
I wanted to tackle a NER task, so I chose the BertForSequenceClassifications provided by HuggingFace.
for epoch in range(1, args.epochs + 1):
    total_loss = 0
    model.train()
    for step, batch in enumerate(train_loader):
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        model.zero_grad()

        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        loss = outputs[0]

        total_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        # modified based on their gradients, the learning rate, etc.
        optimizer.step()

The main part of my fine-tuning follows as above.
I am curious about to what extent the fine-tuning alters the model. Does it freeze the weights that have been provided by the pre-trained model and only alter the top classification layer, or does it change the hidden layers that are contained in the already pre-trained BERT model?
","['bert', 'pretrained-models', 'fine-tuning', 'named-entity-recognition']",
Has someone correctly predicted one of the variants of SARS-CoV-2 (like the Delta variant)?,"
Without any evidence, I have wondered it might be possible to predict the upcoming mutations of the COVID-19 virus. I am further assuming people did so.
So, has someone correctly predicted the emergence of one of the variants of SARS-CoV-2 (like the Delta variant)?
I would be happy to have an explanation in layman's terms and citations to papers (if any).
","['reference-request', 'prediction', 'biology', 'healthcare']","
Atomwise, an AI startup, uses a 3d convolution neural network to predict if a molecule will bind to an protein. The covid protein had several human attempts to find a molecule to bind.
See on young inventors approach to solving covid
https://www.cnn.com/2020/10/18/us/anika-chebrolu-covid-treatment-award-scn-trnd/index.html

Anika's winning invention uses in-silico methodology to discover a lead molecule that can selectively bind to the spike protein of the SARS-CoV-2 virus.

I wonder if the atomwise simulator would concur that lead would bind to the spike protein
"
How to measure the similarity the pronunciation of two words?,"
I would like to know how I could measure the pronunciation of two words. These two words are quite similar and differ only in one vowel.
I know there is, e.g., the Hamming distance or the Levenshtein distance but they measure the ""general"" difference between words. I'm also interested in that but mainly I would like to know how they sound differently. I think there must be something like this to test text-to-speech results?
Best would even be an online source where I could just type in those two words.
","['natural-language-processing', 'natural-language-understanding', 'speech-synthesis']",
How can the probability of two disjoint events be non-zero?,"
Let $A$ and $B$ be two models for a classification task. Let $x$ be a test set and $M$ be a metric for the classification task.  $X$ be a random variable on test sets.
Now,
$M(A,x) = $ Score of model $A$ on test set $x$
$M(B,x)$ = Score of model $B$ on test set $x$
$\delta(x) =$ difference in performance of models wrt test set $x$ $= M(A, x)-M(B,x)$
Now, consider the following (statistical) hypothesis on the performance difference $\delta$
$$H_o : \delta(x)  \le 0$$
$$H_1 : \delta(x)  > 0$$
We define $p-$value as follows
$$P(\delta(X) \ge \delta(x) | H_o  \text{is true} ) $$
With this as context, I confused with the following paragraph (taken from p15 of Naive Bayes and Sentiment Classification)

So in our example, this $p-$value is the probability that we would see
$\delta(x)$ assuming $A$ is not better than B. If $\delta(x)$ is huge
(let’s say $A$ has a very respectable $M$ of $.9$ and $B$ has a
terrible $M$ of only $.2$ on $x$), we might be surprised, since that
would be extremely unlikely to occur if $H_0$ were in fact true, and so
the $p-$value would be low (unlikely to have such a large $\delta$ if
$A$ is in fact not better than $B$). But if $\delta(x)$ is very small,
it might be less surprising to us even if $H_0$ were true and $A$ is
not really better than $B$, and so the $p-$value would be higher.

It is told in the paragraph that $p-$value is very low if $A's$ performance is better than $B$.
I am thinking that $p-$value should be zero if $A's$ performance is better than $B$ since it is a disjoint event wrt $H_0$. Where am I going wrong?
","['probability', 'statistical-ai']",
"What is the difference between model setup, model configuration, and model customization?","
In the context of research papers related to deep learning models, the authors usually mention these terms in the experiment section when they are talking about the model: configuration, setup. For example: Akbik et al. 2018.
For example:

""We utilize the BiLSTM-CRF sequence labeling architecture proposed by Huang et. al (2015) in all configurations of our comparative evaluation.""

""Baselines. We also evaluate setups that involve only previous word embeddings.""


What is the difference between the terms? Is the model architecture the same with different hyperparameters?
Thank you in advance.
","['deep-learning', 'terminology', 'sequence-modeling']",
What does it mean by overfitting the test set?,"
Consider the following statement from p14 of Naive Bayes and Sentiment Classification

While the use of a devset avoids overfitting the test set, having a
fixed training set, devset, and test set creates another problem: in
order to save lots of data for training, the test set (or devset)
might not be large enough to be representative.

I heard about overfitting on train data. A model is said to be overfit on train data if it is giving low train error and high test error.
But, what does it mean overfitting on test set?
","['machine-learning', 'terminology', 'overfitting', 'test-datasets']","
Essentially, any data you use to train or develop the model shouldn't be used as test data. In principle, ""unseen"" data gives a good estimate for the generalisation performance of the model; but this is only valid if the data really is unseen and hasn't been used in the model development process. If you've been tuning a model to increase its accuracy on the test set, then that data has influenced the model, so it's not unseen any more!
An example of what is wrong:

Train a neural network on the training set.
Evaluate the performance on the test set, and then change the parameters of the model in some way to try and increase the test set performance.
Use the best parameters you found, and get a final evaluation of performance on the test set.

To make this procedure legitimate, you should have a three way split: train, dev, test. Do the tuning on the dev set, and then you can get a final estimate of generalisation using the test set.
If you don't do this, you'll generally think your model is a lot more accurate than it actually is. It's just like trying to estimate your generalisation performance from the training set, which I'm sure you know is a bad idea!
This phenomenon is what is sometimes known as overfitting the test set. To see why this name is used, just consider what overfitting the training data is: picking parameters that seem to fit the training data well, but don't generalise well. Likewise, overfitting the test set involves picking hyperparameters that seem to work well, but don't generalise. In each case, the solution is to have an additional set so you can get an unbiased estimate of what's actually happening.
"
How could Bayesian neural networks be used for transfer learning?,"
In transfer learning, we use big data from similar tasks to learn the parameters of a neural network, and then fine-tune the neural network on our own task that has little data available for it. Here, we can think of the transfer learning step as learning a (proper) prior, and then fine-tuning as learning the posterior.
So, we can argue that Bayesian networks can also solve the problem of small data-set regimes. But, what are the directions that we can mix Bayesian neural networks with similar tasks to transfer learning, for example, few-shot learning?
They make sense when they both take a role as a solution to the low data regime problems, but I can't think of a mix of them to tackle this issue.
Is it possible, for example, to learn a BNN for which we have picked a good prior to learn the posterior with little data and use the weight distribution for learning our new task? Is there any benefit in this?
","['reference-request', 'transfer-learning', 'bayesian-deep-learning', 'bayesian-neural-networks', 'one-shot-learning']","
Well, I would say, that purpose of Bayesian inference is not transfer learning, but uncertainty estimation.
In case you have good feature extractor in the beginning, you can adjust small number of parameters, like few last layers to achieve good quality in few epochs.
However, this is about adjusting the means of distributions over each weight.
Concerning the variance, I think transfer learning is inapplicable since the source and target distributions can be very different. For example, ImageNet is a broad and diverse dataset with many classes, and the target problem can involve only a few classes. Most probably, uncertainty estimate and the standard deviations of model weights on the ImageNet would be larger, than for the model, trained solely on the target task.
"
TD3 sticking to end values [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am using TD3 on a custom gym environment, but the problem is that the action values stick to the end. Sticking to the end values makes reward negative, to be positive it must find action values somewhere in the mid. But, the agent doesn't learn that and keeps action values to maximum.
I am using one step termination environment (environment needs actions once for each episode).
How can I improve my model? I want action values to be roughly within 80% of maximum values.
In DDPG, we have inverted gradients, but could something similar be applied to TD3 to make action values search within legal action space more?
The score decreases as episodes increases.

","['reinforcement-learning', 'td3']",
Should I continue training if the neural network attains 100% training accuracy?,"
I have a neural network where there are two hidden layers. Each hidden layer has 128 neurons. The input layer has 20 inputs, and the output layer has 3 outputs.
I have 1 million records of data. 80% is used to train the network, 20% is used for validation. I run the training for 100000 epochs.
I see that the neural network attains 100% accuracy on the training data after only 12000 epochs.
Should I stop training or continue until all 100000 epochs are complete? Please, explain why.
","['neural-networks', 'training', 'overfitting', 'accuracy', 'cross-validation']","
First of all, as mentioned by @Neil Slater in the comment - you need to have three splits into the train, validation and test set.
One sometimes disregards the difference between validation and test set. However they serve for different purposes. Here I would like to cite https://machinelearningmastery.com/difference-test-validation-datasets/ :

Validation Dataset: The sample of data used to provide an unbiased
evaluation of a model fit on the training dataset while tuning model
hyperparameters. The evaluation becomes more biased as skill on the
validation dataset is incorporated into the model configuration.
Test Dataset: The sample of data used to provide an unbiased
evaluation of a final model fit on the training dataset.

Secondly, in order to understand what's happening plot jointly the train and validation loss. In case the performance on validation data becomes much worse, that on the training - It is better to terminate training, since it is the indication of overfitting.
A good practice is to use early stopping, there is an implementation of this callback in Tensorflow - https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping.
It a kind of regularization procedure https://en.wikipedia.org/wiki/Early_stopping.
"
Can some of the weights be fixed during the training of a neural network?,"
Is it possible to exclude specific layers from the optimization?
For example, let's say I have an input layer, 2 hidden layers, and the output layer. I know there is a perfect solution for my problem with this setup and I already know the perfect weights between the first and the second hidden layer.
Can I have the weights between the first and the second hidden layer be fixed during the training phase?
I understand that I could just not update these specific weights after I computed the backpropagation for the entire network. But if I throw away those specific weights, will this affect the optimization of the rest of my weights?
","['machine-learning', 'convolutional-neural-networks', 'training', 'backpropagation', 'weights']","
Yes, you can fix (or freeze) some of the weights during the training of a neural network. In fact, this is done in the most common form of transfer learning (which is described here). I don't know exactly how this affects learning in general. In transfer learning, this is definitely beneficial, as we are freezing the weights that are associated with the learned general features of objects, such as corners (where general here is defined intuitively), which can be useful for other tasks, so, by having them frozen, we reuse them.
"
Could someone help tell what the labels are pointed out by red rectangles?,"
The following figure comes from the paper The perceptron: A probabilistic model for information storage and organization in the brain

I can tell the labels pointed out by blue rectangles are: ""Projection area"", ""A-units"", ""$R_1$"", ""inhibitory connections"" and ""$R_2$"".
Could someone help tell what the labels are pointed out by red rectangles?
","['papers', 'perceptron']",
"Why would the ""improvement"" be the result of random initialization, and so why should we use multiple runs?","
I got this feedback for my thesis paper.

The improvement shown in the results section could be the result of random initialization. There should be multiple runs with means and standard deviations.

Can anyone explain this feedback with details?
I used a neural network with pre-trained weights for transfer learning (specifically, EfficientNetB0, with 'noisy-student' for the weights). It was a classification problem to classify between Covid-19, Viral Pneumonia, and normal cases. I normalised the dataset so that the images are in the range [0, 255] and I also did k-fold cross-validation.
","['transfer-learning', 'weights-initialization', 'multiclass-classification', 'uncertainty-quantification', 'k-fold-cv']",
RLlib's Multi-agent PPO continuous actions turn into nan,"
After some amount of training on a custom Multi-agent sparse-reward environment using RLlib's (1.4.0) PPO network, I found that my continuous actions turn into nan (explodes?) which is probably caused by a bad gradient update which in turn depends on the loss/objective function.
As I understand it, PPO's loss function relies on three terms:

The clipped surrogate objective which depends on outputs of old policy and new policy, the advantage, and the ""clip"" parameter(=0.3)

The Value Function Loss

The Entropy Loss [mainly there to encourage exploration]


Total Loss = Surrogate objective (clipped) - vf_loss_coeff * VF Loss + entropy_coeff * entropy.

The surrogate loss
( Reference: https://arxiv.org/abs/1707.06347 )
I have a bunch of questions:

Is the ratio rt(theta) used that of the actual actions taken from new policy vs. old policy or is it the probability distributions of those actions? (since actions are continuous?)

Follow up question to 1: Assuming it is probability, can the probability ever be 0? Because if it is ever 0, then log probability would result in log(0) = inf/undefined - which would prove that is the root cause?

If 1 and 2 are safely debunked, then do I


(A) lower my learning rate?
(B) Reduce the number of layers in my network?
(C) Use gradient clipping or action or reward clipping of some sort?
To anyone who would be kind enough to share any insights into the matter, you have my gratitude.
For more information, see relevant part of progress table below: where the total loss becomes inf. The only change I found is that the policy loss was all negative until row #445.





Total loss
policy loss
VF loss




430
6.068537
-0.053691725999999995
6.102932


431
5.9919114
-0.046943977000000005
6.0161843


432
8.134636
-0.05247503
8.164852


433
4.222730599999999
-0.048518334
4.2523246


434
6.563492
-0.05237444
6.594456


435
8.171028999999999
-0.048245672
8.198222999999999


436
8.948264
-0.048484523
8.976327000000001


437
7.556602000000001
-0.054372005
7.5880575


438
6.124418
-0.05249534
6.155608999999999


439
4.267647
-0.052565258
4.2978816


440
4.912957700000001
-0.054498855
4.9448576


441
16.630292999999998
-0.043477765999999994
16.656229


442
6.3149705
-0.057527818
6.349851999999999


443
4.2269225
-0.05446908599999999
4.260793700000001


444
9.503102
-0.052135203
9.53277


445
inf
0.2436709
4.410831


446
nan
-0.00029848056
22.596403


447
nan
0.00013323531
0.00043436907999999994


448
nan
1.5656527000000002e-05
0.0002645221


449
nan
1.3344318000000001e-05
0.0003139485


450
nan
6.941916999999999e-05
0.00025863337


451
nan
0.00015686743
0.00013607396


452
nan
-5.0206604e-06
0.00027541115000000003


453
nan
-4.5543664e-05
0.0004247162


454
nan
8.841756999999999e-05
0.00020278389999999998


455
nan
-8.465959e-05
9.261127e-05


456
nan
3.8680790000000003e-05
0.00032097592999999995


457
nan
2.7373152999999996e-06
0.0005146417


458
nan
-6.271608e-06
0.0013273798000000001


459
nan
-0.00013192794
0.00030621013


460
nan
0.00038987884
0.00038019830000000004


461
nan
-3.2747877999999998e-06
0.00031471922


462
nan
-6.9349815e-05
0.00038836736000000006


463
nan
-4.666238e-05
0.0002851575


464
nan
-3.7067155e-05
0.00020161088


465
nan
3.0623291e-06
0.00019258813999999998


466
nan
-8.599938e-06
0.00036465342000000005


467
nan
-1.1529375e-05
0.00016500981


468
nan
-3.0851965e-07
0.00022042097


469
nan
-0.0001133984
0.00030230957999999997


470
nan
-1.0735256e-05
0.00034000343000000003




Optional
For even further context, check my related question
","['reinforcement-learning', 'actor-critic-methods', 'proximal-policy-optimization', 'multi-agent-systems', 'continuous-action-spaces']",
How to Study Improving In-depth Reading Comprehension?,"
There are multiple datasets for machine comprehension tasks such as SQuAD. However, most of the questions are straightforward. One can find the answers easily by using the find feature of the browser to look for the question keywords in the passage.
I'd appreciate it if you let us know about standardized in-depth reading comprehension tests for either human or machine that are generalizable. By generalizable, I mean they include a broad range of disciplines and academic levels and are not specifically designed for a target population.
I thought of GRE reading comprehension but was not able to find any study indicating that GRE reading comprehension questions are standardized or generalizable.
","['natural-language-processing', 'computational-linguistics']",
How would the probability of a document $P(d)$ be computed in the Naive Bayes classifier?,"
In naive Bayes classification, we estimate the class of a document as follows
$$\hat{c} = \arg \max_{c \in C}  P(c \mid d) = \arg \max_{c \in C} \dfrac{ P(d \mid c)P(c) }{P(d)} $$
It has been said in page 4 of this textbook that we can ignore the probability of document since it remains constant across classes.

We can conveniently simplify the above equation by dropping the denominator $p(d)$.  This is possible because we will be computing $\dfrac{P(d \mid  c)P(c)}{P(d)}$for each possible class. But $P(d)$ doesn't change for each class; we are always asking about the most likely class for the same document $d$, which must have the same probability $P(d)$.  Thus, we can choose the class that maximizes this simpler
formula
$$\hat{c} = \arg \max_{c \in C}  P(c \mid d) = \arg \max_{c \in C}
 P(d \mid  c)P(c)  $$

Since the value of the document does not influence the choice of the class, naive Bayes algorithm does not consider that.
But, I want to know the value of $P(d)$. Is it $\dfrac{1}{N}$, if total number of documents are $N$? How should I calculate $P(d)$?
","['natural-language-processing', 'classification', 'probability', 'naive-bayes', 'probability-theory']","
$P(d)$ (aka evidence) is a probability of your data (observation) and is defined as follows:
$$
P(d) = \sum_i P(d|c_i)P(c_i)
$$
for all classes $c$.
According to the book, $P(c)=\frac{N_c}{N_{doc}}$ and $P(d|c)$ is a likelihood and, applying the assumptions from the book, can be defined as $P(w_i
|c)=\frac{\text{count}(w_i, c)+1}{\sum_{w \in V}\text{count}(w, c) + |V|}$, where $V$ consists of the union of all the word types in all classes.
Taking the example from Section 4.3 of the book,




Dataset
Cat
Documents




Training
-
just plain boring



-
entirely predictable and lacks energy



-
no surprises and very few laughs



+
very powerful



+
the most fun film of the summer


Test
?
predictable with no fun




we'll get:
$$
P(-) = \frac{3}{5}\\
P(+) = \frac{2}{5}\\
P(S|âˆ’) = \frac{2\times 2\times 1}{34^3}\\
P(S|+) = \frac{1\times 1 \times 2}{29^3}\\
P(d) = P(S|âˆ’)P(âˆ’) + P(S|+)P(+) = 6.1\times 10^{âˆ’5} + 3.2\times 10^{âˆ’5}
$$
"
Does MobileNet SSD v2 only capture aleatoric uncertainty (and so not the epistemic one)?,"
Regarding the MobileNet SSD v2 model, I was wondering to what extend it captures uncertainty of the predictions.
There are 2 types of uncertainty, data uncertainty (aleatoric) and model uncertainty (epistemic).
The model outputs bounding boxes with a confidence score, but what uncertainty does that score represent?
From what I know models usually only capture aleatoric uncertainty in their predictions, but not the epistemic one. Is this also true for MobileNet SSD v2?
","['bayesian-deep-learning', 'uncertainty-quantification', 'single-shot-multibox-detector', 'epistemic-uncertainty']",
Can people use neural networks without providing the set of training data?,"
It seems that neural networks (NNs) can be applied to supervised learning, unsupervised learning and reinforcement learning. Some people even train neural networks without the set of training data. If NNs are used in reinforcement learning, is it possible that we don't need training data?
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'training-datasets']","
You cannot train a neural network without training data. It would be like training a football player without making him/her play/watch football or anything that resembles football: it's simply not possible. The definition of training/learning in machine learning strictly requires data.
You can train a neural network in different ways (e.g. supervised or unsupervised) and with different types of data (e.g. labelled or unlabelled, respectively), but this is a different story. In reinforcement learning, you also have training data, but the data may not be given to the neural network in the same way that it's given e.g. in supervised learning. Still, this does not mean that there is no training data. Of course, there is or must be (by definition)!
However, note that you can use a (e.g. randomly initialised) neural network without training it, but it would probably be a useless neural network. You could also use a neural network that has been trained by someone else with data that you may not have access to anymore (and maybe that answers your question in the title).
"
Book(s) on generative models,"
Generative models in artificial intelligence span from simple models like Naive Bayes to the advanced deep generative models like current day GANs. This question is not about coding and involves only science and theoretical part only.
Are there any standard textbooks that covers topics from scratch to the advanced?
","['generative-model', 'resource-request']","
From the theoretical foundations one can look into the Chapter 20: Deep Generative Models of the classic DL book by Goodfellow, Bengio https://amzn.to/2MmZNbH. Not the most recent reference, but written by the professionals in simple and accessible way.
There is a nice book Generative Deep Learning by D.Foster with some simple heuristics and probability theory motivations and examples https://www.google.ru/books/edition/Generative_Deep_Learning/RqegDwAAQBAJ?hl=en&gbpv=1&printsec=frontcover.
Finally, there is a book from Jason Brownlee (author of many nicely written articles on machinelearningmastery.com) https://machinelearningmastery.com/generative_adversarial_networks/
"
Can people set loss function of neural network by themselves instead of choosing cross entropy or mean square error?,"
I found people used deep neural network to get optimal policy by solving a nonconvex optimization problem. Moreover, they didn't use any set of training data and claimed that it's the difference between their approach and the supervised learning. I wonder can people set loss function of neural network by themselves instead of choosing cross entropy or mean square error?
My experience in machine learning is very limited. I audited two machine learning courses offered by applied math department in my school. I read twenty or more papers on the application of machine learning. I began to use Keras very recently.
","['neural-networks', 'reinforcement-learning', 'objective-functions']",
Why adversarial images are not the mainstream for captchas?,"
In order to check, whether the visitor of the page is a human, and not an AI many web pages and applications have a checking procedure, known as CAPTCHA. These tasks are intended to be simple for people, but unsolvable for machines.
However, often some text recognition challenges are difficult, like discerning badly, overlapping digits, or telling whether the bus is on the captcha.
As far as I understand, so far, robustness against adversarial attacks is an unsolved problem. Moreover, adversarial perturbations are rather generalizable and transferrable to various architectures (according to
https://youtu.be/CIfsB_EYsVI?t=3226).
This phenomenon is relevant not only to DNN but for simpler linear models.
With the current state of affairs, it seems to be a good idea, to make CAPTCHAs from these adversarial examples, and the classification problem would be simple for human, without the need to make several attempts to pass this test, but hard for AI.
There is some research in this field and proposed solutions, but they seem not to be very popular.
Are there some other problems with this approach, or the owners of the websites (applications) prefer not to rely on this approach?
","['computer-vision', 'adversarial-ml', 'captcha']","
Because the examples are fit to a particular ML model and if you train using different parameters they probably won't be valid.
"
What exactly is the population in the problem of finding the best path in a network of nodes using genetic algorithms?,"
I have 17 nodes in my network with 3000 different paths in total. I have to select the path with highest available bandwidth, using genetic algorithm. I'm confused about the approach! Should I have all paths as the population, or should I create a population same size as the nodes(17).
","['genetic-algorithms', 'genetic-programming', 'path-finding']","

Should I have all paths as the population,

No, this is not usually possible for more realistic problems where a population that covered all possibilities would be far too large to manage.

or should I create a population same size as the nodes(17).

No, there is no need to link the population size to other properties of the problem so directly.
If your path must pass through all 17 nodes (like a travelling salesman problem) then your genome coding might have 17 elements to it, and could simply be the path through the nodes. That's not the only way to address even the TSP, and may not be the case here. However, I mention it because it is common that numerical features of the problem will influence the design of the genomes.
The population size is a hyperparameter for the solution, along with mutation rate, rules for recombination etc. It is something you will want to experiment with.
With 3000 combinations to assess, a direct search of all combinations would be fast and effective (and probably easier to code). My understanding is therefore that this is a learning exercise. Your eventual goal might be to have the genetic algorithm find a good solution with less than 3000 evaluations of the path. Finding a good solution in any number of iterations is also a reasonable start to demonstrate you have understood the basics of genetic algorithms.
"
"In a DDQN architecture, why is the value of a state assumed to be the average of the Q values of the actions?","
In a Dueling DQN agent (Wang et al.), the Q function is decomposed as
$$
Q(s, a)=V(s) + A(s, a) - \frac{1}{|A|}\sum_{a'\in \mathcal{A}}A(s, a')
$$
representing the value of the state, plus the advantage of the action, minus the average advantage of all actions available in that state.
However, this formulation means that the value of the given state is skewed, even after subtracting out the mean advantage of all actions available. Why isn't it set up so that the advantage of the best action is 0 (with the other actions' advantages being negative), thus leading to a more accurate $V(s)$?
","['reinforcement-learning', 'dqn']","
This is considered in the original paper, but is rejected due to training instability. To quote:

[The average-advantage formulation] increases the stability of the optimization...the advantages only need to change as fast as the mean, instead of having to compensate any change to the optimal action's advantage in [the formulation where $Q(s, a) is normalized with the best action's advantage].

Furthermore, the authors attempt a softmax version, but they find it doesn't perform that differently than the average-advantage version. I couldn't find any additional experiments using this, but it may be worth trying if it is absolutely necessary to obtain an accurate $V(s)$.
"
Why is AI Super Resolution Reconstruction more than just guessing?,"
I saw a video on Youtube about AI and Super Resolution Image Reconstruction with TecoGAN. I must say I am impressed.
Now, I am wondering how reliable this is.
I have learned at university that you lose information if you do not sample to fullfill Nyquist. I also don't think that the example images are in any way sparse...
Is the AI just trying to fill in the blanks by guessing?
This would be fine for entertainment, but probably not so much to enhance robbery pictures and charge people based on enhanced pictures. It also wouldn't be a good solution for improving the resolution of scientific data if it is just ""guessing"".
","['computer-vision', 'papers', 'image-processing', 'image-super-resolution', 'tecogan']","
You actually don't have to loose information if you don't fulfill Nyquist — although that topic is quite advanced and has limitations. Still, super resolution is reliable and used by most 4K TVs today to upscale 1080p video to fit the 4K screen. You may notice TV ads for 4K TVs occasionally mentioning this.
What super resolution does is just generalising shapes. For example, imagine a simple image with just a black rectangle. You can easily image enlargening that image to whatever size you want because you know how it's supposed to look. By training a neural network on a lot of images it can learn to generalise features such as faces and enlarge them.
Of course you can't take a single pixel of a face and expand this into a full 100x100 image. Therefore, it's best to use super resolution to enlarge entire images, not just individual areas of the image. If you were to use it to enlarge a very small patch of text in the image, it may not replicate the text correctly and read as something else.
Furthermore, very good super resolution models are slow. Most also only increase the resolution of a single frame at a time. This means there might be inconstancies between sequential frames in a video.
"
"Are the held-out datasets used for testing, validation or both?","
I came across a new term ""held-out corpora"" and I confused regarding its usage in the NLP domain
Consider the following three paragraphs from N-gram Language Models
#1: held-out corpora as a non-train data

For an intrinsic evaluation of a language model we need a test set. As
with many of the statistical models in our field, the probabilities of
an $n-$gram model come from the corpus it is trained on, the training
set or training corpus. We can then measure training set the quality
of an n-gram model by its performance on some unseen data called the
test set or test corpus.  We will also sometimes call test sets and
other datasets that are not in our training sets held out corpora
because we hold them out from the held out training data.

This paragraph clearly says that held-out corpora can be used for either testing or validation or others except training.
#2: development set or devset for hyperparameter tuning

Sometimes we use a particular test set so often that we implicitly
tune to its characteristics. We then need a fresh test set that is
truly unseen. In such cases, we call the initial test set the
development test set or,devset. How do we divide our data into
training, development, and test sets? We want our test set to be as
large as possible, since a small test set may be accidentally
unrepresentative, but we also want as much training data as possible.
At the minimum, we would want to pick the smallest test set that gives
us enough statistical power to measure a statistically significant
difference between two potential models. In practice, we often just
divide our data into 80% training, 10% development, and 10% test.
Given a large corpus that we want to divide into training and test,
test data can either be taken from some continuous sequence of text
inside the corpus, or we can remove smaller “stripes” of text from
randomly selected parts of our corpus and combine them into a test
set.

This paragraph clearly says that development set is used for hyperparameter tuning.
#3: held-out corpora for hyperparameter tuning

How are these $\lambda$ values set? Both the simple interpolation and
conditional interpolation $\lambda'$s are learned from a held-out
corpus.  A held-out corpus is an additional training corpus that we
use to set hyperparameters like these $\lambda$ values, by choosing
the $\lambda$ values that maximize the likelihood of the held-out
corpus.

This paragraph is clearly saying that held-out corpus is used for hyper-parameter training.
I am interpreting or understanding the terms as follows:
Train corpus is used to train the model for learning parameters.
Test corpus is used for evaluating the model wrt parameters.
Development set is used for evaluating the model wrt hyperparameters.
Held-out corpus includes any corpus outside training corpus. So, it can be used for evaluating either parameters or hyperparameters.
To be concise, informally, data = training data + held-out data = training data + development set + test data
Is my understanding true? I got confusion because of paragraph 3, which says that held-out corpus is used (only) for learning the hyperparameters while paragraph 1 says that held-out corpus includes any corpus outside train corpus. Does held-out corpora include devset or same as devset?
","['natural-language-processing', 'terminology', 'books', 'test-datasets', 'validation-datasets']","
In the sample dataset, the variable Sprint is the respondent's time (in seconds) to sprint a given distance, and Smoking is an indicator about whether or not the respondent smokes (0 = Nonsmoker, 1 = Past smoker, 2 = Current smoker). Use ANOVA to test if there is a statistically significant difference in sprint time with respect to smoking status. Sprint time will serve as the dependent variable, and smoking status will act as the independent variable.
"
Which policy has to be followed by a player while construction of its own Q-table?,"
Consider the scenario, where there are two players. One of the players perform the action randomly, whereas I want second player as a Q-player. I mean, the player selects a best action from the Q-table for given state i.e., the action with maximum Q-value. So, in case of second player, a Q-table is required.
It is known that Q-table has to be constructed only by running several episodes after its arbitrary initialization. So, the two players has to play with some policy to construct a Q-table for player two. Since the first player uses random policy. I have doubt regarding the policy of second player for constructing Q-table.
I have doubt regarding the policy of second player only. I know the policy he need to follow after the completion of updation of Q-table. I am not sure about the policy he need to follow while updating the Q-table only.
Which policy does my second player need to follow while constructing the Q-table for himself? Can he use random policy like player one? Or does he need to use arbitrarily initialized or partially updated Q-table itself for selecting best action? Or can he use some other policy till the completion of updating Q-table?
","['reinforcement-learning', 'q-learning', 'off-policy-methods', 'exploration-exploitation-tradeoff', 'exploration-strategies']",
How to design a neural network with arbitrary input and output length?,"
I am trying to build a neural network that has an input of $n$ pairs of integer values (where $n$ is random) and a corresponding output of a binary array with length $n$.
The input will be a set of integer value coordinates $[(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3}), \dots, (x_{50}, y_{50}), \dots]$, where each instance can be of various lengths, like $[(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3}), \dots, (x_{52}, y_{52})]$ or $[(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3}), \dots, (x_{101}, y_{101})]$, etc.
The output is a set of binary arrays with each instance having the same length as the corresponding input.

May I know if anyone has any recommendations on what neural network would fit this use case?
","['neural-networks', 'machine-learning', 'deep-learning', 'recurrent-neural-networks', 'pretrained-models']","
A recurrent neural network (RNN, specifically either an LSTM or GRU)  will work well for variable length sequences like you’ve described. Assuming the order of the sequence is meaningful (I.e. you can’t just break up the sequence into individual inputs and associated target value) an RNN model will learn how the sequence of inputs maps to the sequence of outputs.
"
What are the existing AI methods to approach 3D volumes of computed tomography?,"
I have a dataset which consists of computed tomography images (CT scans) of parts that contain pores and cracks. The sets for each part are of about 1100 * 1100 * 3000-ish resolution. Currently, I use a method of thresholding and calculations to find the volumes and locations of these defects, and I would like to reproduce those results with a machine learning approach.
What are the methods known for this type of problem, and what are your general recommendations?
Edit:

Here is the current method I am using :

And this is what I aim to achieve :


","['classification', 'image-segmentation']",
How to interpret the training loss curves in Soft-Actor-Critic (SAC)?,"
I am using stable-baseline3 implementation of the Soft-Actor-Critic (SAC) algorithm. The plotted training curves look promising. However, I am not fully sure how to interpret the actor and critic losses. The entropy coefficient $\alpha$ is automatically learned during training. As the entropy decreases, the critic loss and actor loss decrease as well.

How does the entropy coefficient affect the losses?
Can this be interpreted as the estimations becoming more accurate as the focus is shifted from exploration to exploitation?
How can negative actor losses be interpreted, what do actor losses tell in general?


Thanks a lot in advance
","['deep-rl', 'actor-critic-methods', 'loss', 'soft-actor-critic', 'learning-curve']",
What approaches are there to generate complex structures like syntactic trees?,"
What approaches are there to generate and evaluate complex structures like, let's say, syntactically correct code? I know the approach of Genetic Programming (GP) as a type of Evolutionary Algorithm, but I wonder if there are any other techniques that are being used to produce complex structures more efficiently.
Note that the syntactically correct code example is just that, an example. The code wouldn't be generated to solve a specific task, although it could try to maximize a fitness function. We could be talking about 3D models, music compositions, etc. What interests me about this issue is if there are Computational Creativity techniques being used or researched in the last years, apart from the mentioned GP.
","['generative-model', 'artificial-creativity']",
GLIE MC control (reinforcement learning): how the policy affects evaluation?,"
In his lecture 5 of the course ""Reinforcement Learning"", David Silver introduced GLIE Monte-Carlo Control.

I understand that we do policy evaluation for one step and then policy improvement. My question is how does the improved policy come into play in this GLIE algorithm?
Is Gt (return) based on the policy somehow? is that where the new policy comes in?
Asked another way, how are policy evaluation and policy improvement connected in this image?
","['reinforcement-learning', 'monte-carlo-methods']",
Convolutional Neural Network (CNN) with Tree architecture to organize the number of classes,"
At the moment, I have around 1.000 classes with accuracy and loss that are acceptable. In the long term, there could be more than 100.000 classes. The main problem is that every time a new class is needed, the model needs to be rebuilt.
For this, I made a POC with a Siamese Network with the goal that new classes can be added without the need to rebuild. The results were not what I expected, and probabilities are a must. As far as I know, this could not be done with this network. The conclusion was that this was not the best option for this case.
Before I start implementing, I would appreciate some feedback and second opinion on the following architecture:
The next thing I would do is build a hierarchy chain of CNN’s. The structure is already available in a database and I could automate the build of the CNN’s to a certain level.
The first CNN could have 4 “main” classes. Based on the probability, the next layer will be determined.
Then the second CNN would have 50 to 200 classes. Based on the probability, the next layer will be determined as well.
Then the last layer would be a CNN with up to 1.000 classes. In case there are more, this could be divided even further.
This way, I could gradually build up the model without the need to rebuild everything (last layer). And the first and second layer only needs to be rebuilt if the accuracy and probability start dropping.
I found a paper with a similar proposal, but could not find feedback or experiences of others. Is this something that is feasible? What could be the problems I will face with a structure like this? Or would you tackle this problem in another way?
","['neural-networks', 'convolutional-neural-networks', 'image-processing']",
Is there any model that is probabilistic but not statistical?,"
While studying about the n-gram models, I encountered the terms ""statistical model"" and ""probabilistic model"" several times.
I got a basic doubt that will there be any probabilistic model that is not statistical restricted to models that works on datasets.
In machine learning, we use datasets. Any model that uses dataset can be called as a statistical model since statistics is a branch of mathematics that tries to find insights related to data.
All the models that calculates probabilities using datasets, for any task, are called (empirical) probabilistic models.
Thus, if I am not wrong, every probabilistic model has to be a statistical model since it uses data. Am I wrong?
Is there any model in literature that is a statistical model but not probabilistic?
","['machine-learning', 'terminology', 'statistical-ai']","
First of all, I don't know of any textbook that clarifies these terms, but, although I am not a statistician, in addition to the other answer, one possible way to look at it is as follows.
You use probability theory to model your problem. For example, if it's a classification problem, you could define the conditional probability distribution $p(y \mid x)$, which would compute the probability of a label $y$ given an input $x$. In other words, you assume that there is a probability distribution of the form $p(y \mid x)$ that generates your data, so here $p$ is the ""model"". If you want to generate images, for instance, you could model the process that generates them as the marginal distribution $p(x)$, which, ideally, would tell you the probability of sampling a specific image $x$. This is the theory. So, no observed data is still involved here. By the way, this is exactly how people usually model the machine learning problems in the sub-field of statistical learning theory.
In practice, you need to estimate these probability distributions. To estimate them, you can use data. The type of data depends, of course, on the problem and model. For example, in the case of $p(y \mid x)$, you may need a  labelled dataset $D$. So, if you estimate $p(y \mid x)$ with $D$ to obtain $\hat{p}(y \mid x)$, then $\hat{p}$ would be a statistical model, in the sense that you estimated it from observed/empirical data. In general, statistics is all about taking data and using it to build ""models"" that can be used for prediction or forecasting (of future inputs) or inference (i.e. understanding the properties of the data-generating process or probability distribution) or just to compute the so-called ""statistics"" (hence the name of the field!), such as the ""sample average"" (i.e. the average of your observe data points, where the ""sample"" here refers to your dataset of points, which are also sometimes known as ""samples"", just to make things even more confusing!)
So, let me address your questions and comments, but take my comments below with a grain of salt, because I am not a statistician.

All the models that calculates probabilities using datasets, for any task, are called (empirical) probabilistic models.

To me, this would be a reasonable statement. In this example, you seem to be talking about $\hat{p}(y \mid x)$, which I would also call a probabilistic model, although it's just an estimate of the theoretical/ideal one.

Is there any model in literature that is a statistical model but not probabilistic?

If we follow my reasoning above, initially, if you do not explicitly  model your problem as the estimation of some probability distribution that generated the data, then we would be estimating something from data (so we would be building a statistical model), but it wouldn't be clear whether this ""statistical model"" is an estimate of some theoretical/probabilistic one. So, I don't really have a definitive answer to your question. I suppose that any statistical model could be modelled with the tools of probability theory, so I would be more inclined to think that the answer to your question is ""no"".
In addition to what I just said above, if you take a book like Machine Learning: A Probabilistic Perspective, here are a few examples of how the author uses the terms ""statistical model"" and ""probabilistic model"". For example, he writes (section 7.3, page 217)

A common way to estimate the parameters of a statistical model is to compute the MLE, which is defined as
$$
\hat{\boldsymbol{\theta}} \triangleq \arg \max _{\boldsymbol{\theta}} \log p(\mathcal{D} \mid \boldsymbol{\theta})
$$

So, in this case, is $p$ a statistical or probabilistic model, according to my definitions above? Of course, ignoring the potentially different notation being used here to refer to a statistical model, i.e. without the $\hat{}$, I think that this $p$ could be considered a statistical model (in the sense that $\hat{\boldsymbol{\theta}}$ would be estimated from the observed dataset $\mathcal{D}$, assuming it's the observed dataset and not some random variable) but at the same time also a probabilistic one, in the sense that, here, we are assuming that we have some kind of ""theoretical likelihood"". In any case, the likelihood is something that can make this discussion even more confusing, because the likelihood is not really a probability distribution (if you integrate with respect to the parameters). In any case, here, you could consider $p(\mathcal{D} \mid \boldsymbol{\theta})$ as a (Bayesian) probabilistic model, i.e. you assume that there's some parameters that generate the data and, if you consider it as a conditional probability distribution over the data, rather than the parameters, then this would be  consistent with what I said above.
Here's another example (section 1.3.1, p. 10).

In this book, we focus on model based clustering, which means we fit a probabilistic model to the data, rather than running some ad hoc algorithm.

This usage also seems consistent with my description above. Here, I interpret the part ""we fit a probabilistic model to the data"" as ""we estimate the probability distribution given the data"".
Or, in section 1.4.1 (p. 16)

In this book, we will be focussing on probabilistic models of the form $p(y \mid x)$ or $p(x)$, depending on whether we are interested in supervised or unsupervised learning respectively.

The discussion can become even more complicated, if we start to consider parametric vs non-parametric models, which are mentioned in that same section, where you make or not assumptions about the data-generating process.
So, to conclude, I think that these terms are often used vaguely and sometimes interchangeably, so the confusion is normal.
"
What is a Silhouette Neural Network,"
I was going through a study in which I found something called a dilated Silhouette Neural Network. I want to know what it is, what it can do, and how it is better from a CNN?
Link to the journal: Link
","['neural-networks', 'convolutional-neural-networks']",
Vector input to CNN for object detection,"
I am training a 3D object detection network (Retinanet-based as of the moment) for re-detecting tracked objects. I would like to be able to add the velocity vector of the tracked object as an input to the detection network, as the velocity directly informs the direction along which the principal axis of the 3D bounding box should lie. I would like to include this information as early as possible (i.e. pass in as an input feature map) rather than simply adding it at the end with a few fully connected layers.
Is there a good or established way to encode such a vector in a feature map?
","['convolutional-neural-networks', 'object-detection']",
Which policy do I need to use in updating Q function?,"
Policy function can be of two types: deterministic policy and stochastic policy.
Deterministic policy is of the form $\pi : S \rightarrow A$
Stochastic policy is defined using conditional probability distributions and I generally remember as $\pi: S \times A \rightarrow [0,1]$. (I personally don't know whether the function prototype is correct or not)
I am guessing that both type of policies can be used for Q learning. As one can read from this answer that both reward and policy function are needed to implement $Q$ learning algorithm

In addition to the RF, you also need to define an exploratory policy
(an example is the $\epsilon$-greedy), which allows you to explore the
environment and learn the state-action value function $\hat{q}$.

I have no doubt about the necessity of reward function as it is obvious from the updating equation of $Q$.
And coming to the (usage of policy), you can find it from the line 5 of the pseudocode provided in the answer

Choose $a$ from $s$ using policy derived from $Q$

One can notice that policy is used for computing $Q$ and $Q$ updation also needs a policy.
Henceforth I conclused myself that the correct statement for the line 5 of pseudocode has to be

Choose $a$ from $s$ using policy derived from $Q$ updated so far

Is my conclusion true? Else, how is it possible to break that cyclic dependency between policy and $Q$ function?
","['reinforcement-learning', 'q-learning', 'terminology', 'off-policy-methods', 'exploration-strategies']",
(explore-exploit + supervised learning ) vs contextual bandits,"
Lets take an ad recommendation problem for 1 slot. Feedback is click/no click. I can solve this by contextual bandits. But I can also introduce exploration in supervised learning, I learn my model from collected data every k hours.
What can contextual bandits give me in this example which supervised learning + exploration cannot do?
","['supervised-learning', 'contextual-bandits', 'exploration-strategies']",
Is pruning only applicable to convolutional neural networks?,"
This article talks about pruning in the context of convolutional neural networks:

One of the first methods of pruning is pruning entire convolutional filters. Using an L1 norm of the weight of all the filters in the network, they rank them. This is then followed by pruning the ‘n’ lowest ranking filters globally. The model is then retrained and this process is repeated.
There also exist methods for implementing structured pruning for a more light-touch approach of regulating the output of the method. This method utilizes a set of particle filters that are the same in number as the number of convolutional filters in the network.

Is pruning only applicable to CNNs?
","['neural-networks', 'convolutional-neural-networks', 'optimization', 'filters']","
No, it is not only applicable to CNNs, but to a wide range of other architectures, even the hype transformers.
For an extensive survey, I recommend that you have a look at this paper What is the State of Neural Network Pruning?.
"
Can we combine policy evaluation and value iteration steps for solving model-based MDP?,"
In Sutton & Barto (2nd edition), at the very end on page 83, the following is mentioned:

In general, the entire class of truncated policy iteration algorithms can be thought of as sequences of sweeps, some of which use policy evaluation updates and some of which use value iteration updates.

and this on the beginning of page 84:

max operation is added to some sweeps of policy evaluation.

I understand that the entire class of truncated policy iteration algorithms can be classified as generalized policy iteration (GPI). Also, I know value iteration (VI) is a combination of one sweep of policy evaluation (PE) and one sweep of policy improvement.
My question: What do we mean by combining multiple PE and VI updates in truncated policy iteration?
","['reinforcement-learning', 'value-iteration', 'policy-iteration']",
Is there a way to beat AlphaGo Zero with different method?,"
As I read the research from
https://deepmind.com/research
It seem AlphagoZero use zero knowledge and use Reinforcement learning to improve the ai skill of playing.
Is there a way to beat AlphagoZero? can anyone share an idea.
My friend said we can find a specific move to beat AlphagoZero.
From my point of view I think the only way to beat AlphagoZero is computational power.
But I can't find any other way to beat AlphagoZero, I hope there is many other way to beat it.
Also wanna keep some related topic below:
Would AlphaGo Zero become perfect with enough training time?
","['reinforcement-learning', 'alphago-zero']",
"In addition to the reward function, which other functions do I need to implement Q-learning?","
In general, $Q$ function is defined as
$$Q : S \times A  \rightarrow \mathbb{R}$$
$$Q(s_t,a_t) = Q(s_t,a_t) + \alpha[r_{t+1} + \gamma \max\limits_{a} Q(s_{t+1},a) - Q(s_t,a_t)] $$
$\alpha$ and $\gamma$ are hyper-parameters. $r_{t+1}$ is the reward at next time step. $Q$ values are initialized arbitrarily.
In addition to the reward function, which other functions do I need to implement Q-learning?
","['reinforcement-learning', 'q-learning', 'markov-decision-process', 'reward-functions', 'exploration-strategies']",
Regress values inside the bounding boxes to predict a value in Object Detection,"
I am currently working on an object detection task. I have a dataset of Grayscale and Depth Images.  The annotation format is x1, y1, x2, y2, class, depth. I have calculated this depth (of each object/bounding box) using a clustering algorithm and depth image.
My plan is to use the Grayscale images to detect the bounding boxes and the class labels using a pre-trained CNN.
Furthermore, I want to use the depth images to predict the depth (ground truth values in the dataset as mentioned above). For this task, my plan is to build a Regression-based Neural Network that regresses the depth values inside the bounding boxes of the depth image and compares them to the ground truth values. An RMSE loss function can be used to keep track of the predictions.
How do I go about making this NN and is there a better alternative?
","['neural-networks', 'convolutional-neural-networks', 'regression']",
What type of neural network do I need?,"
I am working on protein structure prediction.
Suppose, I am solving a problem using Neural Networks. I know how many inputs and outputs there will be in the model, as it directly depends on the problem statement.
However, how do I know:

What type of neural network do I need to apply to optimally solve my specific problem?

",['neural-networks'],
What do the terms 'Bellman backup' and 'Bellman error' mean?,"
Some RL literature use terms such as: 'Bellman backup' and 'Bellman error'. What do these terms refer to?
","['reinforcement-learning', 'terminology', 'bellman-equations', 'bellman-operators']",
Is it valid to implement hyper-parameter tuning and THEN cross-validation?,"
I have a multi-label classification task I am solving. I have done hyperparameter tuning (with Keras Tuner) to determine the best configuration for my neural network.
Is it valid to do this (determine the best hyper-parameters) and then do cross-validation to get a more accurate test estimation of the dataset?
I don't see how this would be invalid, given that the cross-validation examples I have seen already have network architectures known a priori, presumably because this is what they chose or feel is the best way of proceeding.
For hyperparameter tuning, all data is split into training and test sets - the training set is further split, when fitting the model, for a 10% validation set - the optimal model is then used to predict on the test set.
For k-fold cross-validation, all data (same as above) is used, but I just split (with sklearn) the data into training and test datasets (so no validation dataset). The test set is used to determine the model performance at each iteration of k-fold cross-validation.
","['neural-networks', 'hyperparameter-optimization', 'cross-validation', 'multi-label-classification', 'k-fold-cv']",
"In value iteration, what happens if we try to obtain the greedy policy while looping through the states?","
I am referring to the Value Iteration (VI) algorithm as mentioned in Sutton's book below.

Rather than getting the greedy deterministic policy after VI converges, what happens if we try to obtain the greedy policy while looping through the states (i.e. using the argmax equation inside the loop)? Once our $\Delta < \theta$ and we break out of the loop, do we have an optimal policy from the family of optimal policies? Is this a valid thing to do?
I implemented the gambler's problem exercise mentioned in Sutton's book. The policies obtained after using standard VI and the method I described above are mostly similar, yet different for some states.
","['reinforcement-learning', 'value-iteration', 'policy-improvement']",
How do I represent sample efficiency of RL rewards in mathematical notation?,"
I define sample efficiency as the area under the curve/graph, where $x$-axis is the number of episodes while y-axis is the cumulative reward for that episode. I would like to formally define it with a mathematical function.
If the notation for cumulative reward for $x$th episode is:
$$R_x = \sum_{t=0}^{t=T} r_t,$$
where $r_t$ is the reward for timestep $t$ and $T$ is the max number if steps per episode.
So is the equation for area under the graph/curve the one below?
$$\text{Sample Efficiency} =\int_{a}^{b} R_x \ dx$$
I will be just using a Python library to get the area under the graph which uses Simpson's rule for integrating.
","['reinforcement-learning', 'deep-rl', 'rewards', 'return', 'sample-efficiency']","
Episodes are discrete, there is no need for calculus. Your ""sample efficiency"" metric is:
$$\sum_{x=a}^b R_x$$
The quantity you are measuring per episode is the return (undiscounted). The sum of this over many episodes does not measure sample efficiency as the term is usually meant, although the sample efficiency of the algorithm you use should impact the numbers you see. Getting a high value of this metric, averaged over many training runs, implies two things:

The algorithm learns to exploit the environment quickly. This is related to sample efficiency.

The algorithm does not pay a high cost for exploring. This is not directly related to sample efficiency, and may in fact be in conflict with learning an environment quickly.


These are both desirable properties of a reinforcement learning algorithm. They often need to be considered in balance, this is the exploration versus exploitation dilemma in RL, which can be studied in a simplified form in mult-arm bandit problems. You may be able to take inspiration from how bandit algorithms are measured for other metrics related to efficient learning.
Your metric is most useful when considering learning agents run in a live environment, where costs of mistakes during learning are real.
If instead you are training an agent in a safe environment - e.g. in simulation - then you may not be interested in the undiscounted returns $R_x$ during training. Your goal may be to train the agent using the least CPU time, the least number of simulation runs etc. In which case, you care most about the mean return achievable by the trained algorithm after spending a certain amount of whatever resource you are managing. This is more closely related to the concept of sample efficiency, and to measure that you could plot the returns from separate test episodes at routine intervals, with exploration removed (e.g. if you were using DQN, then with $\epsilon = 0$
"
Example of lemma having multiple boldface forms,"
Number of lemmas can be used as a rough measure for the number of words in a language. A lemma can have multiple word-form types. It can be understood from the following paragraph taken from p12 of Regular Expressions,Text Normalization, Edit Distance

Another measure of the number of words in the language is the number
of lemmas instead of wordform types. Dictionaries can help in giving
lemma counts; dictionary entries or boldface forms are a very rough
upper bound on the number of lemmas (since some lemmas have multiple
boldface forms). The 1989 edition of the Oxford English Dictionary had
615,000 entries.

It is also given that a lemma can have multiple boldface forms, what are the boldface forms referred here? Are they different from wordforms?
If possible, provide an example for lemma having multiple boldface forms.
","['natural-language-processing', 'books']","
It is very confusingly worded, and I would think it's incorrect according to linguistic terminology.
A lemma is the canonical form of a word, commonly the infinitive of a verb, the nominative singular of a noun, and the positive of an adjective. The inflected forms belonging to a word would the the forms used for other tenses and persons etc for verbs, case and number for nouns, and comparative/superlative for adjectives.
This raises the question of what a word is, and there is no satisfactory answer to this, even more than 100 years after the foundation of modern linguistics...
Anyway, the 'boldface form' (a term I have not come across in 30 years as a linguist), refers to dictionary headwords, which are lemmas. There are some lemmas that are 'shared' by words which have multiple meanings: the common example in linguistics is bank, which can be a financial institution, the side of a river, a term to describe the process of tilting the wings of an airplane in flight, or it can mean to deposit an amount of money in an account, etc. All these words you would find under bank in a dictionary, but usually under several different entries. So I guess this is what is meant by ""multiple boldface forms"". However, these are usually completely unrelated words which by accident share the same spelling; in some cases it could also have been the same word that then developed different meanings.
To summarise: the paragraph you quote is plain wrong/sloppy in its use of terminology, as a dictionary headword is a lemma in every dictionary I have seen, but these are not unique, as several different words might have lemmas which are spelled the same way (but they are still different lemmas — no single word would have multiple dictionary entries).
For example:

bank (bank, banks), noun: a financial institution
bank (bank, banks), noun: the side of a river
bank (bank, banks, banked, banking), verb: tipping the wings of an airplane
bank (bank, banks, banked, banking), verb: depositing money in an account

We have four lemmas (in bold), two of which have two inflected forms, and the other two have three each. These are also four different words, with a total of four different word forms (bank and banks are common forms of all words)
Often, to avoid confusion, you would refer to them as $bank_1$ for the financial institution, and $bank_2$ for the river bank, etc. to indicate that they are different words.
You can probably see that English has a number of lemmata (which is the proper plural of lemma, since it's of Greek origin) which is by a factor of 3-4 smaller than the number of word types, whereas in other languages this ratio will be a lot smaller, as they have more inflected variants. An English noun has just singular and plural forms, whereas a German noun would have singular and plural across each of four cases (though some of them would share the same word forms).
"
What math should I learn before and while using and applying deep learning?,"
I want to learn deep learning. After researching a little, I came to the conclusion that I need a lot of math. I've started a linear algebra course, and it takes a long time (2-3 weeks). I want to start using and applying deep learning to solve problems in this summer, but I assume I would not have enough time to learn all subjects (linear algebra, statistics and probability and calculus 1).
So, what math should I learn before and while using and applying deep learning?
","['deep-learning', 'math']","
The math that you need to be comfortable with most deep learning (DL) topics (such as neural networks, gradient descent are back-propagation) is already mentioned in your post, but I will list the main subjects here too.

Linear algebra (an entire college-level course is necessary; you can start with Khan Academy videos/lessons and you can pick one of Gilbert Strang's books)
Calculus (same; Kenneth A. Ross' book is a decent one)
Numerical analysis/algorithms (you need be aware of numerical algorithms, like gradient descent, and concepts like convergence, round-off errors, etc; in fact, gradient descent is the widely used in DL)
Probability theory (you need to know what a probability distribution, random variable, etc., are)
Statistics (you don't need to know everything at the beginning, but the more you know the better)

I didn't use this book when I was studying deep learning, but part 1 of this book covers (at least some of) the most important mathematical prerequisites for deep learning, so you could try to read some of the chapters to understand at what point you are. I  don't have a favourite book for the last 3 topics listed above.
Check out also the book Mathematics for Machine Learning. I never read it, but it looks like part 1 has many chapters on most important math topics for ML and so DL.
By the way, I don't think that 3 weeks is a lot. You will definitely need more time to learn the mathematical prerequisites for deep learning, but the exact time depends on your specific background.
"
"In SIFT, how is the coordinate system being rotated?","
I need to understand how SIFT calculates the descriptors for the keypoints.
Intuitively, I understand that it takes each keypoint, calculates the gradients for each pixel in a neighborhood of the keypoint, and that's basically the descriptor for the keypoint. The paper mentions a coordination system rotation in the keypoint, I assume this is when the image is rotated, the keypoint descriptor doesn't change.
My question:
I'm following this implementation of SIFT. In the part of the calculation function, there is this cos/sin calculation:
I think this is related to the coordinate system rotation. Can you explain how the coordinate system is being rotated? Why does that have to do with the hist_width?
","['image-processing', 'feature-extraction', 'c++', 'opencv', 'sift']",
The only convergent instrumental goal for self modifying AI,"
Conjecture: regardless of the initial reward function, one of the winning strategies would be to change the reward function to a simpler one (e.g. ""do nothing""), thus getting a full reward for each passing unit of time. For such an agent, the only priority would be to prolong its existence (to maximize the overall reward collected).
So:

The notion of externally defined reward function is incompatible with the concept of self-adjusting AGI.
Any AGI will always settle on self-preservation as its only goal
It is therefore impossible to create an AGI with benevolence towards humans ""build-in"". Instead, the problem of AI alignment should be reformulated in terms of ""what changes to the environment (the physical reality that AGI shares with humans) would irreversibly tie wellbeing of humanity to AGIs existence"".
Since any ""kill switch"" or similar artificial measure is not irreversible and can be overcome by a super-intelligent agent, the only way to tie AGI existence and human wellbeing is the modification of laws of physics, logic, and reasoning. Which is impossible.
AI alignment is impossible

What flaws do you see in this line of reasoning?
","['agi', 'ai-safety', 'value-alignment']","
Any possible action, including changing the reward function, would be evaluated through the initial reward function. In order to avoid the scenario you described, a reward function needs to disincentivize changes to itself by giving those the lowest possible reward.
"
"What are the labels in figure1 in the Paper ""The perceptron: A probabilistic model for information storage and organization in the brain""?","
This figure

comes from The perceptron: A probabilistic model for information storage and organization in the brain
I guess the first circle (neuron) labels RETINA, the second labels perceptron area, what about the third one? what are the labels pointed out by the arrows?
","['neural-networks', 'perceptron']",
"Is the formula $\frac {1}{s}\sum _{j=1}^{s}|d_{j}-y_{j}(t)|$ the correct form of 0-1 loss function, in the context of Perceptron?","
Per page 7 of this MIT lecture notes, the original single-layer Perceptron uses 0-1 loss function.
Wikipedia uses
$${\displaystyle {\frac {1}{s}}\sum _{j=1}^{s}|d_{j}-y_{j}(t)|} \tag{1}$$
to denote the error.
Is the formula (1) the correct form of 0-1 loss function?
","['neural-networks', 'objective-functions', 'definitions', 'perceptron']",
Which tasks are called as downstream tasks?,"
The following paragraph is from page no 331 of the textbook Natural Language Processing by Jacob Eisenstein. It mentions about certain type of tasks called as downstream tasks. But, it provide no further examples or details regarding these tasks.

Learning algorithms like perceptron and conditional random fields
often perform better with discrete feature vectors. A simple way to
obtain discrete representations from distributional statistics is by
clustering, so that words in the same cluster have similar
distributional statistics. This can help in downstream tasks, by
sharing features between all words in the same cluster. However, there
is an obvious tradeoff: if the number of clusters is too small, the
words in each cluster will not have much in common; if the number of
clusters is too large, then the learner will not see enough examples
from each cluster to generalize.

Which tasks in artificial intelligence or NLP are called as downstream tasks?
","['natural-language-processing', 'terminology']",
What is the role of left singular vectors in SVD?,"
SVD decomposition of a data matrix $A$ of order $n \times d$ and rank $r$ can be expressed as follows
$$A_{n\times d} = U_{n\times r}D_{r \times r}V^{T}_{r \times d}$$
The rows of the data matrix $A$ are the data points in $d$ dimensional space. Thus, there are $n$ points in $d$ dimensional space.
The matrix $V$ contains $r$ right singular vectors as columns. Right singular vectors are orthonormal and forms a $r-$dimensional subspace that best fits the given $n$ data points.
The matrix $D$ is a diagonal matrix that contains singular values. Singular values signify the least squares (loss) of n-data points on the subspace $r$ right singular vectors.
The matrix $U$ contains left singular vectors as columns. Left singular vectors are also orthonormal.
But what does the $r$ left singular vectors signify?
","['singular-value-decomposition', 'vector-space']",
When does a neural network have a single and when does it have multiple outputs?,"
What I understand is, each input in a neural network is a feature.
However, what I don't understand is, when we need multiple outputs in a neural network.
For example, say, if we are classifying cats and dogs, only one output is enough. 0 = cat, 1 = dog.
When does a neural network have a single and when does it have multiple outputs?
",['neural-networks'],
Is there any dataset to convert text to sign language? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I'm going to start working on one university project and I would like to ask a question regarding it. My project is about ""Sign language synthesis from NLP"" and I need to develop an application where:

Take spoken language from user microphone
Recognize a word with an algorithm and convert words to sign language

Output should be images with sign language.
For instance, if we say ""I go home"", we should have images of those words in sign language.
My question is that is there any dataset you would recommend to get the images for the sign language?
",['natural-language-processing'],
How does the neural network learn when used in the REINFORCE algorithm?,"
As per my understanding, you run an entire episode, which contains many steps, and then back-propagate using just a single loss value. How does the neural network learn to differentiate between good and bad actions?
","['neural-networks', 'reinforcement-learning', 'deep-rl', 'policy-gradients', 'reinforce']",
Is it possible to have values of the states equal to $0$ at the end of the value iteration?,"
I am new to Reinforcement Learning and I am trying to self learn it. I have already posted some quesiton here and your answershave been really useful to me, so here I am posting another one.
I am studying the value iteration, and while doing the simulation using python, I get that at some states it is associated a value of $0$. I think I have to mention that I have tried to assign to the ststes an initial value different from zero, in order to simulate the fact that the agent already have some information about the enviroment before starting.
So, my questio is:
Is it possible to have values of the states equal to $0$ at the end of the value iteration?
","['reinforcement-learning', 'value-functions', 'value-iteration']",
Loss function to Push response value towards extremes,"
I have a feature map whose values are in the range of [0,1]. I want to push these values either towards extreme 0 or 1 using some loss function. Since I don't have any target value so it had to be in unsupervised way. I want to visualize this feature map in a way that either pixels values are approaching 1 or 0. One possible technique is to use entropy loss. What are other possible techniques used in Loss function to get extreme pixel values?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'objective-functions', 'unsupervised-learning']",
"An explanation involving the sign activation, its affect on the loss function, and the perceptron and perceptron criterion: what is this saying?","
I am currently studying the textbook Neural Networks and Deep Learning by Charu C. Aggarwal. Chapter 1.2.1.3 Choice of Activation and Loss Functions says the following:

The classical activation functions that were used early in the development of neural networks were the sign, sigmoid, and the hyperbolic tangent functions:
$$\Phi(v) = \text{sign}(v) \ \ \text{(sign function)} \\ \Phi(v) = \dfrac{1}{1 + e^{-v}} \ \ \text{(sigmoid function)} \\ \Phi(v) = \dfrac{e^{2v} - 1}{e^{2v} + 1} \ \ \text{(tanh function)}$$
While the sign activation can be used to map to binary outputs at prediction time, its non-differentiability prevents its use for creating the loss function at training time. For example, while the perceptron uses the sign function for prediction, the perceptron criterion in training only requires linear activation.

I am having trouble understanding this part:

While the sign activation can be used to map to binary outputs at prediction time, its non-differentiability prevents its use for creating the loss function at training time. For example, while the perceptron uses the sign function for prediction, the perceptron criterion in training only requires linear activation.

I've read over this a number of times, but I still don't have a good idea of what it is saying (or at least the point it is trying to make). What is this actually saying? What is the point this is trying to make? Perhaps a more detailed explanation of what this is saying will clarify it for me.
","['objective-functions', 'activation-functions', 'perceptron']",
Is it possible to create a simple face-tracking app that can monitor how much time one spends at their desk?,"
Context: I'm an experienced programmer with a graduate education in AI and previous CUDA programming experience. I'm versed in Machine Learning but am out of the loop -- I've not used any of the modern software packages of the last 10 years.
Question: Is it possible using modern AI software to easily create a face-tracking application that can use a webcam to track the amount of time spent at one's desk.
My environment is Fedora Linux. I also have an NVidia GTX 1660 for acceleration.
To make this question and answer precise, I've narrowed it to the following sub-questions:
As of June 2021,

Is there existing software that one can simply ""set up"" with a small amount of programming work (or none at all) that would facilitate training a video classifier from webcam recordings?

How does one provide training examples to this software, or how is data labeled? Does it provide some sort of GUI or accessory tool to label still frames or video sequences?

Does said software provide ""hooks"" or an event API so that one may invoke code on the event of e.g. a classifier edge?

Finally (and consider this optional), would it be realistic for a seasoned programmer to accomplish such a project using said software in about 30 hours? I understand that this is subjective -- just assume a graduate student in the AI field and ballpark terms. Or, answer in terms of the software's intended audience.


First posting in this community, so just offer guidance if you'd like to see this question refined.
","['image-recognition', 'facial-recognition', 'object-tracking']",
Taking a machine learning model to production\deployment,"
I've designed a machine learning model for the predictive maintenance of machines. The data used for training and testing the ML model is the data from various sensors connected to various parts of the machines. Now, I'm searching for a good approach for deploying the model in the real-time environment as explained here. I did some research and found some information about using real-time data for prediction such as using Kafka. I have some questions unanswered regarding the deployment of the ML model. Following are the details of my system:

The sensors (pressure, temperature, flow, vibration, etc) are deployed across the parts of the machines.
The ML model is trained with historical data.
For predictive maintenance (anomaly detection), streams of data will be available via MQTT. As there are 3000 machines, the volume of data will be very high.

My questions are:

Where will be the best place to perform prediction operation, at the factory premice where machines are located (edge computing), at our office (that designs ML model), or at cloud server? I want to know it in regard to operational cost.
Is there any way to estimate the effectiveness of the complete system (full-stack ML architecture)?

","['machine-learning', 'data-science']",
What is the meaning or implications of the rank of a dataset for machine learning algorithms?,"
Consider a dataset with $n$ training examples and $d$ features.
Let $D_{n \times d}$ be the data matrix and $r$ be the rank of it.
In matrices, rank $r$ is generally useful in

Knowing the dimension of (optimal) vector space that can generate the rows or columns of the matrix.

Knowing the number of linearly independent rows or linearly independent columns in the matrix. Note that column rank and row rank are same for a matrix and is generally called as the rank of a matrix.


In fact, both 1 and 2 are same and just rephrased.
What is the meaning or implications of the rank $r$ of a dataset $D_{n \times d}$ for machine learning algorithms?
","['machine-learning', 'terminology', 'datasets', 'math']","
I know at least one example where the rank of the dataset (more specifically, the rank of a matrix that is computed from the design matrix, i.e. the matrix with your data, which I will describe more in detail below) can have an impact on the number of solutions that you can have or how you find those solutions. I am thinking of linear regression.
So, in linear regression, you have the model (written in matrix form)
$$
\mathbf{y} = \mathbf{X} \beta + \epsilon
$$
where

$\mathbf{y}$ is an $N \times 1$ vector of dependent variables (i.e. the labels)
$\mathbf{X}$ is an $N \times K$ matrix of indedendent variables (aka regressors or features)
$\beta$ is an $N \times 1$ vector of parameters
$\epsilon$ is the noise (i.e. you assume that there's some noise that corrupts the function that relates the features to the labels through the parameters)

It turns out that, if $X$ has full rank, then the so-called ordinary least squares (OLS) solution to the linear regression problem (i.e. the estimate of the parameters $\beta$), which can be denoted by
$$
\hat{\beta} = \arg \min \| \mathbf{X} \beta - \mathbf{y} \|_{2},
$$
is given by a closed-form expression
$$\hat{\beta}=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}y \tag{1}\label{1}.$$
If you look at this equation, you see that we are computing the inverse of a matrix, and that matrix is $\mathbf{X}^{T}\mathbf{X}$. What if this matrix is not invertible? It turns out that you cannot invert a matrix if it's not full rank. It also turns out that, if $\mathbf{X}$ is not full rank, then $\mathbf{X}^{T}\mathbf{X}$ wouldn't also be (see this and this), so we couldn't use the closed-form solution \ref{1} to solve the linear regression problem, i.e. we wouldn't have anymore a convex problem (i.e. a unique solution).
So, this is the only implication of the rank of the dataset (or design matrix) has on the machine learning algorithm that I am aware of and comes to my mind right now, but it's possible that the rank can play other roles.
"
Alpha beta pruning - rules for updating alpha/beta value,"
I have been working on a problem to which I've applied alpha-beta pruning. While I got most of the answers right, there is one part I'm not quite getting:

Note that I've only provided a part of the tree I'm working on. Node $B$ starts with the following values:
$B$

$v = \infty$
$\alpha = - \infty$
$\beta = \infty$

Now, we push the alpha and beta values down to node $D$ from parent node $B$, and calculate its value:
$D$

$v = -\infty$
$\alpha = -\infty$
$\beta = \infty$

As leaf node $J$ has a value of $-7$, we push that back up to parent node $D$, changing node $D$'s value to $-7$ (as it is better than the old value of $-\infty$), and we also change the $\alpha$ value of node $D$ (as it is also better than the old value of $-\infty$).
New $D$

$v = -7$
$\alpha = -7$
$\beta = \infty$

We now push the value of $-7$ back up to parent node $B$, changing node $B$'s value to $-7$ (as it is better than the old value of $\infty$), and we also change the $\beta$ value of node $B$ (as it is also better than the old value of $\infty$).
New $B$

$v = -7$
$\alpha = -\infty$
$\beta = -7$

We now traverse down to node $E$ (and we don't prune it because node $B$'s value is NOT <= its $\alpha$ value), and we push the alpha and beta values down from node $B$, and calculate its value:
$E$

$v = -\infty$
$\alpha = -\infty$
$\beta = -7$

As leaf node $K$ has a value of $0$, we push that back up to parent node $E$, changing node $E$'s value to $0$ (as it is better than the old value of $-\infty$). Now, this is the point where my confusion lies. According to my understanding, at this point we would also set the $\alpha$ value of node $E$ to $0$ (as it is better than the old value of $-\infty$). However, the answer I received to this question specifies that we do NOT change the $\alpha$ value of node $E$, and rather leave it as $-\infty$.
Can someone please explain to me why this is the case?
UPDATE
I did not originally include the full subtree - this is it:

In this instance, only node M should be pruned. However, my question still stands as to why the answer did not update the alpha value of node E, as no pruning happened in that part of the tree.
",['alpha-beta-pruning'],"
First, allow me to draw it for better visualization:
1. (α=-∞,β=∞) from B ➡ D

             B (α=-∞,β=∞)
          ↙ / \
(α=-∞,β=∞) D   E
           |   |
        -7=J   K=0
--------------------------
2. (v=-7) J ➡ D α=max(-7,-∞)=-7

             B (α=-∞,β=∞)
α=max(-7,-∞)/ \
(α=-7,β=∞) D   E
      ↖    |   |
        -7=J   K=0
--------------------------
3. (α=-7) D ➡ B β=min(∞,-7)=-7

               β=min(∞,-7)
             B (α=-∞,β=-7)
         ↗  / \
(α=-7,β=∞) D   E
           |   |
        -7=J   K=0
--------------------------
4. (α=-∞,β=-7) from B ➡ E

             B (α=-∞,β=-7)
            / \       ↓
(α=-7,β=∞) D   E (α=-∞,β=-7)
           |   |
        -7=J   K=0
--------------------------

5. (v=0) K ➡ E β=min(0,-7)=-7

             B
            / \ β=min(0,-7)
(α=-7,β=∞) D   E (α=-∞,β=-7)
           |   |    ⬈?
        -7=J   K=0
--------------------------



Update: I found this simulator. It behaves exactly like you describe:
http://homepage.ufp.pt/jtorres/ensino/ia/alfabeta.html

Enter Tree Structure: 2 3 1 1 1 1 1
Enter Values -7 0 -4 -10

I noticed that it tries to update β on node K->E. As β=min(0,-7), it won't change.
It's possible to check their internal code by inspecting the page, to debug even further.
"
What are the standard ways to measure the quality of a set of numerical predictions that include uncertainties?,"
I have a radial basis function that supplies uncertainties (standard deviations) with its predictions, which are numerical values.
This function is computed for a particular point by computing its relative distance to a large set of other reference points in high dimensional space, and compositing a prediction from them.
Over the training set I can compute R to get a correlation between prediction and actual. Weights are assigned to each dimension and optimized to maximize R.
Over a validation set, it seems I'd want to calculate something other than R to measure the model's predictive power, since its predictions are not single values, but ranges.
","['prediction', 'uncertainty-quantification', 'validation']",
Rotationally independent distributions,"
Maxwell's theorem states that multivariate normal distribution $\mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$ is the only distribution of a random vector that is invariant and have independent components after random rotations (orthogonal transformation).
Generally, many distributions are rotationally invariant and it's called spherically symmetric distributions. However, any non-normal spherically symmetric distribution has uncorrelated but dependent components.
Consequently, I am wondering if there are some distributions of vectors which are not neccessary to be rotationally invariant but have rotationally independent components.
",['probability-distribution'],
Why is ancestral sampling used in autoregressive models?,"
I have been reading about autoregressive models. Based on what I've read, it seems to me that all autoregressive models use ancestral sampling. For instance, this paper says the following in Abstract:

We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling.

However, what I don't understand is why (as I understand it) all autoregressive models use ancestral sampling. Why is ancestral sampling used in autoregressive models?
","['machine-learning', 'sampling']",
AI model to predict/generate person's image,"
I want to make a model that predicts person's shape depending on his son's image.
My plan is to create a dataset and each data point in it consists of two images; One for the father or mother and one for the son. Then make a model and train it with this dataset.
So when I give the model an image of a son, it predicts / generates / draws the father's image.

Is that is possible ?
If yes, How can I make it ? ML ? Deep Learning ? Something else ?

I searched a lot but didn't find something helpful; So any ideas or opinions are welcomed.
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks']",
Which topics about/in OpenCog could be researched in a Ph.D. thesis,"
In this interview with Lex Fridman and Ben Goertzel, at 2:23:48, Lex asks about possibilities for young people in the domain of AGI research. Ben Goertzel then answers that there are various possibilities we can find on the OpenCog framework, including Ph.D. theses.
I was wondering if anyone here knows what exactly he meant when he said we can find Ph.D. theses there? (He said about them at 2:25:18)
(I am considering doing a Ph.D. in AI, so I am interested in finding interesting topics for research)
","['agi', 'research', 'academia', 'open-cog']",
What gets optimized in convolutional neural network?,"
In a convolutional neural network, the hyperparameters such as number of kernels and stride, kernel size, etc are determined. After some combination of convolutions, ReLU and pooling layer there is the fully connected (FC) layer in the end which yields a classification result. I originally thought that during training the values of kernels would be optimized and that kernels such as edge detection are a result of optimization.
But at the end if we have weights to optimize at the FC layer, what is it that gets optimized during training of the CNN? Do both the kernel values and weights in FCC get optimized? If so, it seems like we're dealing with two different types of parameters. How are both trained simultaneously? If not so, are there simply sets of kernels known to work and automatically implemented in CNN modules?
","['neural-networks', 'convolutional-neural-networks', 'optimization', 'convolutional-layers', 'dense-layers']",
How would we get a good estimation of the asymptotic performance of machine learning algorithms?,"
The following question is from the webbook Neural Networks and Deep Learning by Michael Nielson:

How do our machine learning algorithms perform in the limit of very large data sets? For any given algorithm it's natural to attempt to define a notion of asymptotic performance in the limit of truly big data. A quick-and-dirty approach to this problem is to simply try fitting curves to graphs like those shown above, and then to extrapolate the fitted curves out to infinity. An objection to this approach is that different approaches to curve fitting will give different notions of asymptotic performance. Can you find a principled justification for fitting to some particular class of curves? If so, compare the asymptotic performance of several different machine learning algorithms.

The ability to mimic complex curves and fit to the data points comes due to the non-linearity used, since, had we only used a linear combination of weights and biases, we would not have been able to mimic these. Now the output depends a lot on our choice of non-linearity. Suppose we have a model. It overfits and we get an order 5 polynomial, while in another case it underfits and we get a linear model. So how would we get a good estimation of the asymptotic performance, as questioned by the author?
","['machine-learning', 'deep-learning', 'time-complexity', 'space-complexity']",
How many singular vectors do we need to calculate for SVD?,"
In the geometrical interpretation of SVD, the data points that we have need to be imagined as points in high dimensional space (say $d$-dimensional space). But we need to find a hyperplane in $k-$dimensional subspace that best fits the given data points

To gain insight into the SVD, treat the rows of an $n \times d$ matrix $A$ as $n$ points in a $d$-dimensional space and consider the problem of finding the best $k$-dimensional subspace with respect to the set of points.

My doubt here is about the uniqueness of $k$. Can we do decomposition for any $k \le d$ or for only certain values of $k$ or only for an unique $k$?

The paragraph is taken from the material on Singular Value Decomposition available here.
","['hyperparameter-optimization', 'hyper-parameters', 'feature-selection', 'singular-value-decomposition']","
The number of singular vectors we need to find during SVD is not unique. The possible values for k are from 1 to $r$. Here, $r$ is the rank of matrix $A$, on which we are performing decomposition.
The same pdf says that

First, in many applications, the data matrix $A$ is close to a matrix of
low rank and it is useful to find a low rank matrix which is a good
approximation to the data matrix .  We will show that from the singular
value decomposition of $A$, we can get the matrix $B$ of rank $k$ which best
approximates $A$; in fact we can do this for every $k$.  Also, singular
value decomposition is defined for all matrices (rectangular or
square)unlike the more commonly used spectral decomposition in Linear
Algebra.

So, the value of $k$ is up to the designer. If the designer selects the value of $k$ smaller than the rank $r$ of the matrix $A$, then it is called as truncated SVD.
"
Are there guiding principles as to which activation functions suit a given RL algorithm?,"
Are there rules of thumb as to which activation functions work well (or which one would not) on the policy and value network of a class of RL algorithms? For hidden layers and for the output layer.
For example, I came across [1], which mentions ELU to be indispensable to MPO [2], and tanh (output activation) to be indispensable to SAC's Gaussian policy.
","['neural-networks', 'reinforcement-learning', 'deep-rl', 'activation-functions', 'hyperparameter-optimization']",
How to add negative samples for object detection?,"
My question is: how to add certain negative samples to the training dataset to suppress those samples that are recognized as the object.
For example, if I want to train a car detector. All my training images are outdoor images with at least one car. However, when I use the trained detector on indoor images, sometimes I got the wrong object detected (false positive). How can I add more indoor images (negative samples) to the training dataset to improve the accuracy? Can I just add them without any labeling?
","['deep-learning', 'object-detection', 'data-labelling']",
How do we get from conditional expectation on both state and action to only state in the proof of the Policy Improvement Theorem?,"
I'm going through Sutton and Barto's book Reinforcement Learning: An Introduction and I'm trying to understand the proof of the Policy Improvement Theorem, presented at page 78 of the physical book.
The theorem goes as follows:

Let $\pi$ and $\pi'$ be any pair of deterministic policies such that, for all $s\in S$,
$q_{\pi}(s,\pi'(s))\geq v_{\pi}(s)$.
Then the policiy $\pi'$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states $s\in S$:
$v_{\pi'}(s)\geq v_{\pi}(s)$.

I take it that for the proof, the policy $\pi'$ is identical to $\pi$ except for one particular state $s$ (at each time step) for which we have $\pi'(s)=a\neq \pi(s)$, as suggested by @PraveenPalanisamy in his answer here.
The proof start from the statement of the theorem: $v_{\pi}(s)\leq q_{\pi}(s,\pi'(s))$
And then $q_{\pi}(s,\pi'(s))$ is developed as $\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s,A_{t}=\pi'(s)]=\mathbb{E}_{\pi'}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s]$
I don't understand how did we get rid of the condition $A_{t}=\pi'(s)$. I don't think it's related to adding the subscript $\pi'$ to the expectation because it's something that should be done by definition since for the following time steps we choose policy $\pi$ which is exactly $\pi'$.
","['reinforcement-learning', 'dynamic-programming', 'policy-improvement', 'policy-improvement-theorem']",
"Is $(y_i - \hat y_i)x_i$, part of the formula for updating weights for perceptron, the gradient of some kind of loss function?","
A post gives a formula for perceptron to update weights

I understand almost all the parts of it, except for the part $(y_i - \hat y_i)x_i$ where does it come from? Is it the gradient of some kind of loss function? If yes, what is the definition of the loss function?
The OP seems doesn't give the hypothesis, so that $\hat y_i = h(x_i)$
However, this hypothesis seems prevalent
\begin{align}
\hat{y} &= sign(\mathbf{w} \cdot \mathbf{x} + b) \tag{1}\\
&= sign({w}_{1}{x}_{1}+{w}_{2}{x}_{2} + ... + w_nx_n + b) \\
\end{align}
where
$$
sign(z) = 
\begin{cases}
1, & z \ge 0 \\
-1, & z < 0
\end{cases}
$$
How do I get $(y_i - \hat y_i)x_i$ from function (1)
","['machine-learning', 'perceptron']","
It depends on your hypothesis $h$. The author of the original article compares the dot product with a threshold:

So for a binary classification problem $h$ can be defined as follows:
$$
h  = \begin{cases}
      1 & \text{if $f>z$}\\
      0 & \text{otherwise}
    \end{cases}
$$
That is, $\hat y_i$ is your prediction and $\hat y_i = h(x_i)$, $y_i$ is a real label and $x_i$ is a sample.
Finally, you can update your weghts $w_n = w_n + \eta(y_i - \hat y_i)x_i$, where $n$ is a number of the weight and $i$ denotes a number of the label/sample pair.
Depending on how you define your hypothesis you will have a different optimization algorithm. Take a look at this answer for more details.
"
How to choose proper normalization strategy for the activations?,"
I am reading a survey on various normalization techniques adopted in neural network architectures.
The purpose of introducing normalization is understandable - to stabilize the training and avoid covariate shifts.
There is a plethora of proposed approaches:

Batch Normalization. Probably, the most well-known approach. One averages over the batch and spatial dimensions and gets the mean and std vectors of size (num_channels,):
$$
\mu_c = \sum_{n, h, w}^{N, H, W} x_{nchw}
\quad
\sigma_c = \sqrt{\frac{1}{NHW}\sum_{n, h, w}^{N, H, W}(x_{nchw} - \mu_c)^2}
$$
Layer Normalization. This technique became very popular after the success of Transformer architectures.  The average is over the channel and spatial dimensions and gets the mean and std vectors of size (batch_size, num_channels):
$$
\mu_n = \sum_{c, h, w}^{C, H, W} x_{nchw}
\quad
\sigma_n = \sqrt{\frac{1}{NHW}\sum_{c, h, w}^{C, H, W}(x_{nchw} - \mu_n)^2}
$$
Instance Normalization. This approach is popular in style transfer applications.  The average is over the channel and spatial dimensions and gets the mean and std vectors of size (batch_size,):
$$
\mu_{nc} = \sum_{n, h, w}^{N, H, W} x_{nchw}
\quad
\sigma_{nc} = \sqrt{\frac{1}{NHW}\sum_{n, h, w}^{N, H, W}(x_{nchw} - \mu_{nc})^2}
$$
There are much more approaches, but I listed these 3 as the simplest.

Then there are trainable parameters $\gamma$ and $\beta$, and the final output is:
$$
\gamma \left(\frac{x - \mu(x)}{\sigma(x)}\right) + \beta
$$
As far as I understand batch normalization forces weights to output something like $\mathcal{N}(\beta, \gamma)$ (normal distribution with mean $\beta$ and std $\gamma$). However, there are problems when batch size is small since the estimate would be inaccurate. Also, it seems to average over all images in the batch, but if there are different classes, probably one would like to have them to be distributed slightly different. This choice is the most widely used in CNN still, despite some [recent work] (https://arxiv.org/abs/2102.06171) says, that this layer can be replaced by another strategy.
Layer normalization seems to equalize different channels. Is there some intuition why it has to be so? Why do we need to make output activations similar to each other?
Instance normalization seems to be the most specific in the list. But I have not seen a lot of usage of this outside style transfer and GAN's.

Overall, the ultimate question is - how to choose a particular
normalization strategy for the given problem and architecture?

","['neural-networks', 'batch-normalization', 'normalisation', 'layer-normalization']",
Confusion about faster RCNN neither object nor background label,"
I am trying to construct a faster RCNN from scratch using KERAS. I am generating the tensor which contains whether anchor at each location corresponds to  object or background or neither for training the RPN.
The   output tensor for the RPN is suppose H x W x L where the L dimension  corresponds to whether an object is detected or is background or neither based on IOU thresholds.
My question is this: What should be the label value for neither an object nor background label and how to stop the gradient flow for this label.
","['tensorflow', 'python', 'keras', 'faster-r-cnn']",
How to deal with images that do not contain any object of interest?,"
I'm currently working on an iOS App where I want to detect if there is a table, chair or bench in the current camera input.
My idea was to take the MobileNetV2 model and get it to classify these three categories with transfer learning in TensorFlow. Because there are cases where none of these three objects are visible, I would add a fourth class ""none"" and feed it with random pictures of different unrelated things.
Is this approach a good idea, or is there a better way of doing this?
","['tensorflow', 'image-recognition', 'transfer-learning', 'multiclass-classification', 'data-labelling']",
Can an animal-level artificial general intelligence kickstart the Singularity? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Most people seem to assume that we need a human-level AI as a starting point for the Singularity.
Let's say someone invents a general intelligence that is not quite on the scale of a human brain, but comparable to a rat. This AI can think on its own, learn and solve a wide range of problems, and basically demonstrates rat-level cognitive behavior. It's just not as smart as a human.
Is this enough to kickstart the exponential intelligence explosion that is the Singularity?
In other words, do we really need a human-level AI to start the Singularity, or will an AI with animal-level intelligence suffice?
","['agi', 'singularity']","

do we really need a human-level AI to start the Singularity, or will an AI with animal-level intelligence suffice?

The requirement from ""theory"" of the singularity is that:

The AI is able to design and implement a better AI than itself.

The trait of being able to design better than itself continues to apply in each iteration.


If both these things hold, then each generation of AIs will continue to improve. This is often assumed by singularity pundits to be an exponential growth curve e.g. each iteration makes +50% compound improvement on whatever measure is being made of intelligence. (Aside: Personally I find this a major weakness in the argument for the singularity being meaningfully possible, that it assumes this growth)
The first item of these two is important to your question. For the singularity to work, there is a baseline capability required - the AI needs to be able to design and build other AIs. A general intelligence at the level of an animal - at least any animal intelligence that we are aware of - does not seem capable of this task. It is not clear that humans are even capable of this task when the AI being built has to possess at least some general intelligence.
The term ""animal-level intelligence"" is tricky. The narrow AIs that we currently build can outperform animals and humans on specific tasks, but in terms of general intelligence they do not score highly (or at all). If we could build one that can outperform humans on a ""building an AI"" task, it might still have the general intelligence of an animal whilst having the capability to bootstrap an iterative process of self-improvement. This does seem like a very dangerous experiment to try though, with idiot-savant-deity AI and paperclip maximiser scenarios as possible outcomes because the AI's general intelligence lags behind its raw capabilities.
"
Why can't recurrent neural network handle large corpus for obtaining embeddings?,"
In order to learn the embeddings, we need to train a model based on some objective function. The model can be an RNN and the objective function can be the likelihood. We learn the embeddings by calculating the likelihood, and the embeddings are considered good if the likelihood is maximum for them.
The following paragraph says that it is difficult to scale RNN to estimate the maximum likelihood for large corpus due to scaling issues:

Likelihood-based optimization is derived from the objective $\log p(w; U)$, where $U \in R_{K \times V}$ is matrix of word embeddings, and $w  =\{w_m \}_{m=1}^M$ is a corpus, represented as a list of $M$ tokens. Recurrent neural network language models optimize this objective directly, backpropagating to the input word embeddings through the recurrent structure. However, state-of-the-art word embeddings employ huge corpora with hundreds of billions of tokens, and recurrent architectures are difficult to scale to such data. As a result, likelihood-based word embeddings are usually based on simplified likelihoods or heuristic approximations.

What is the type of scaling, wrt RNN, is referred to here? Why is it difficult to scale RNN?

The paragraph above is taken from the page 329 of Chapter 14: Distributional and distributed semantics of the textbook Natural Language Processing by Jacob Eisenstein
","['recurrent-neural-networks', 'word-embedding', 'books', 'maximum-likelihood']",
Is my flowchart a good representation of the perceptron learning algorithm?,"
I made a flowchart for a simplified perceptron leaning algorithm.

Here is the process of the learning algorithm.

Initialize the weights first.

Get a training example randomly and make a prediction. If the prediction matches the ground-truth value, then get another training example. If the prediction doesn't match the ground-truth value, update the weights.

repeat step 2 until all predictions match the ground-truth value (or other stop criteria)


Is my flowchart a good representation? If not, what are the errors, and what might be improved?
","['machine-learning', 'perceptron']","
It seems loosely reasonable but there are various things which are potentially unclear.
What exactly is a prediction, and is it deterministic or stochastic? First, if you are predicting a continuous value, you can never be ""correct"" - there will always be at least some very small deviation. This makes me assume that you are talking about making some discrete prediction, e.g. over some classes. In this case you would typically output a probability distribution over the different classes. If this is the case, again it's unclear what ""correct"" means. This makes me believe that the only way to interpret ""correct"" is that for any example, you deterministically output a single class, e.g. by taking the class with maximum probability, and then the prediction is considered correct when you output the correct class.
I think the biggest issue is with ""all predictions correct"". How do you check if all predictions are correct? Would you compute the predictions for all examples each iteration? Because that seems like the only possible way to check whether or not all predictions would be correct. More generally it's often not possible to have all predictions be correct (i.e. for an over determined problem).
"
Too slow search using MCTS in OpenAI Atari games,"
I'm recently using Monte Carlo Tree Search in OpenAi Gym Atari, but the result isn't satisfying.
Without render, the game lasts about 180 steps ( env.step() was called this much time )  with random agent. However, my MCTS agent only made the game last 12 steps. And it took pretty much time to give a next step.
I guess it's the problem of rollout. I build the MCTS tree using nodes containing AtariEnv objects, and deepcopy it each time I rollout, add the reward.
So it takes about 1 second to expand nodes and rollout, if I do 100 iterations, that would be massive waiting time.
My code of rollout is shown below:
def rollout_(current,if_render):
        '''
        current is going to be a Node object
        '''
        sandBox = deepcopy(current.state)
        endReward = 0
        done = False
        while done != True:
            action=sandBox.action_space.sample()
            _,reward,done,info = sandBox.step(action)# wierd return obs_next
            if reward > 0:
                reward *= 2
            endReward += reward-0.008
        return endReward

Anyone can help?
","['reinforcement-learning', 'monte-carlo-tree-search', 'open-ai', 'gym', 'atari-games']",
How does CURL extract labels from logits? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



While going over the pseudocode of the CURL paper, the method to identify labels from the logits wasn't clear to me.  I believe this technique might be common in other PyTorch/Deep Learning tasks. I have attached the pseudocode below -

","['reinforcement-learning', 'deep-learning', 'papers', 'pytorch', 'unsupervised-learning']",
"In NEAT, how do node numbers work?","
I have read a lot of debates about node ids and such. I'm not 100% sure how it works, but I am assuming the next node added to a network would be the next number in that specific networks list?
For example, say we start with a network with 2 inputs 1 output (nodes 1,2,3). Let's say in generation 1, one network splits a connection creating node 4. Then in generation 2, a different network splits a different connection. This would be node 4 for that specific network right? From my understanding (correct me if I'm wrong), this second split would result in 1 new innovation connection. The connection from the input to node 4 would be new but the connection from node 4 to 3 would already exist from the first split?
","['neural-networks', 'reinforcement-learning', 'genetic-algorithms', 'neat', 'neuroevolution']",
What is the difference between a reward and a value for a given state?,"
I am trying to learn reinforcement learning and I am focusing on the value iteration. I am looking at the example of grid world, and I am trying to implement it in python. While doing this, I encountered the situation in which I had to set the rewards for the agent, but looking at the theory, I have found that each state has also a value, which is found using the value iteration.
So, my doubt is: What is the difference between a reward and a value for a given state? And should the initial values of the states always be set equal to zero?
","['reinforcement-learning', 'comparison', 'rewards', 'value-iteration', 'return']","

What is the difference between a reward and a value for a given state?

Let us say that an agent took an action from state $A$ and reached state $B$ and got a score $R$. This instantaneous score the agent received on reaching state $B$ is called the reward.
Now, let me introduce you to the concept of return. Assume that an agent followed a particular trajectory:
1. State 1 -> Action 1
2. Reward 1, State 2 -> Action 2
3. Reward 2, State 3 -> Action 3
...
n. Reward n-1, State n (Terminated)

Return (often denoted by $G$) is the sum total of all the rewards obtained by starting from a state State 1 and following a policy.
So, the definition of the return is
$$G(s_1) = R_1 + R_2 + R_3 + ... = \sum_{i=1}^{\infty}R_i$$
Sometimes (most often) these sequences never terminate, so we include a discount factor (Greek letter gamma, $\gamma$) to rewards obtained in the future.
The definition of the discounted return $G$ is
$$G(s_1) = R_1 + \gamma R_2 + \gamma^2 R_3 + ... = \sum_{i=1}^{\infty}\gamma^{i-1} R_i $$
$\gamma$ is a number between $0$ and $1$: it defines how much importance the agent gives to long-term rewards. For a smaller value of $\gamma$, more importance is given for short-term rewards.
Now, coming back to your question. A value of the state is the expected return for an agent starting from that state and following a particular policy. In the case of stochastic policies (policies that have inherent randomness) and/or for environments with stochastic transition probabilities and/or stochastic rewards, the value is the sum of (the returns of all trajectories multiplied by the probability of taking that trajectory).

And should the initial values of the states always be set equal to zero?

Not necessary, zero initialization is one of many ways to initialize. Random initialization is another method. It depends on the environment setting.
"
Why class embedding token is added to the Visual Transformer?,"
In the famous work on the Visual Transformers, the image is split into patches of a certain size (say 16x16), and these patches are treated as tokens in the NLP tasks. In order to perform classification, a CLS token is added at the beginning of the resulting sequence:
$$
[\textbf{x}_{class}, \textbf{x}_{p}^{1}, \ldots, \textbf{x}_{p}^{N}]
,$$
where $ \textbf{x}_{p}^{i}$ are image patches. There multiple layers in the architecture and the state of the CLS token on the output layer is used for classification.
I think this architectural solution is done in the spirit of NLP problems (BERT in particular). However, for me, it would be more natural not to create a special token, but perform 1d Global Pooling in the end, and attach an nn.Linear(embedding_dim, num_classes) as more conventional CV approach.
Why it is not done in this way? Or is there some intuition or evidence that this would perform worse than the approach used in the paper?
","['computer-vision', 'classification', 'papers', 'transformer', 'vision-transformer']","

However, for me, it would be more natural not to create a special token, but perform 1d Global Pooling in the end, and attach an nn.Linear(embedding_dim, num_classes) as more conventional CV approach.

Surprisingly, this approach was experimented in this new paper Better plain ViT baselines for ImageNet-1k and implemented as default in the torch-vit repository.
Along with other trivial improvements, the authors actually outperform other methods such as DieT or ViT with strong data augmentation.  The modification you described provides 1.8% accuracy gain in the ablation study.
The paper is essentially 1.5 pages so I definitely recommend having a look!
link: https://arxiv.org/abs/2205.01580
"
How does the paper implement NEAT without a global set tracking Innovations?,"
I have been reading this paper on NEAT and trying to implement the algorithm in C#. For the most part, I understand everything in the paper however, there are 2 things I don't understand that confuse me.

In the paper it states:


A possible problem is that the same structural innovation will receive different innovation numbers in the same generation if it occurs by chance more than once. However, by keeping a list of the innovations that occurred in the current generation, it is possible to ensure that when the same structure arises more than once through independent mutations in the same generation, each identical mutation is assigned the same innovation number. Extensive experimentation established that resetting the list every generation as opposed to keeping a growing list of mutations throughout evolution is sufficient to prevent innovation numbers from exploding.

This implies that no global list/set is used to track innovations. If you have a global set to track them, then you wouldn't need to use a list during the evolution of the current generation because as soon as an innovation is created, it would be added to the set. This would be seen by the next evaluated Net/Genome.
From what I have read everyone uses a global list to track the innovations. This makes sense to me. I am just very confused as to how they did it in the paper considering they did extensive testing to figure out they only need a list for the evaluation of the current generation.
","['neural-networks', 'reinforcement-learning', 'genetic-algorithms', 'neat', 'neuroevolution']",
Is there a recent book that covers the theoretical and philosophical aspects of artificial intelligence?,"
What are some recent books that introduce AI and neural networks while also discussing the related philosophical issues, like epistemology and whether AI is really thinking, etc.?
","['neural-networks', 'reference-request', 'terminology', 'philosophy', 'books']","
The famous book Artificial Intelligence: A Modern Approach (by Stuart Russell and Peter Norvig) covers all or most of the theoretical aspects of artificial intelligence (such as deep learning) and it also dedicates one chapter to the common philosophical topics that you mention.
"
How to generate text descriptions from keywords?,"
I wonder how can I build a neural network which will generate text description from given tag/tags. Let's assume I have created such data structure:
{
 'tag1': ['some description1', 'some description2', 'some description3'],
 'tag2': ['some description4', 'some description5', 'some description6'],
 'tag3': ['some description7', 'some description8', 'some description9']
}

Then I would like to create a neural network which will generate randomly generated description based on given tags. For example:
INPUT: ['TAG1', 'TAG2', 'TAG3'] => OUTPUT: 'some description1. some description5 some description9'

Then I thought that it can be a good idea to implement a LSTM and doing text generation, but here I have a problem I know how I can do it for one tag. I can create one corpus of text contains different sentences for tag, then do the training and generate a sentence for given tag, but what If I have multiple tags should I create a corpus for each tag or maybe there is a better way to do that? If you know any articles which covers this problem, I would appreciate if you share them with me. If you have a neural network proposition which will solve this problem, I am also open for proposals.

PS. I know, I can solve this problem with easy Map, for example: ['tag1', 'tag2', 'tag3'].map(tag => tagSentenceMap.get(tag).randomChoice()).join('. ') but this is not the case for me.

","['models', 'word-embedding', 'text-generation']",
How is parameter sharing done in CNN?,"
I am trying to understand the concept of parameter sharing in a convolution neural network from Parameter Sharing. I have a few confusions:
Parameter sharing refers to the fact that for generating a single activation map, we use the same kernel throughout the image. And for that activation map, the weights of that kernel remain the same through the image?
Denoting a single 2-dimensional slice of depth as a depth slice (e.g. a volume of size [55x55x96] has 96 depth slices, each of size [55x55]), we are going to constrain the neurons in each depth slice to use the same weights and bias.
Does the above paragraph refer to the fact that output of neurons in one activation map is generated by using the same weights in kernel throughout the image? And that kernel is convolved on the entire image?
No. of parameters without parameter sharing:
There are 555596 = 290,400 neurons in the first Conv Layer, and each has 11113 = 363 weights and 1 bias. Together, this adds up to 290400 * 364 = 105,705,600 parameters on the first layer of the ConvNet alone. Clearly, this number is very high.
No. of parameters with parameter sharing
With parameter sharing scheme, the first Conv Layer in our example would now have only 96 unique set of weights (one for each depth slice), for a total of 9611113 = 34,848 unique weights, or 34,944 parameters (+96 biases). Alternatively, all 5555 neurons in each depth slice will now be using the same parameters. What does this bold sentence mean?
Also, how the parameters are different for both schemes? In both cases, we are using 96 kernels with 11113 size and the resulting output is 55*55. Then how the number of parameters for both schemes coming out to be different?
",['convolutional-neural-networks'],"
Concerning parameter sharing.

For the fully connected neural network you have an input of shape (H_in * W_in * C_in) and the output of shape (H_out * W_out * C_out). This means, that each color of the pixel of the output feature map is connected to every color of the pixel from the input feature map. There is a separate learnable parameter for each pixel in the input image and the output.
Hence, one gets this huge number of parameters :
(H_in * H_out *  W_in * W_out * C_in * C_out)
In the convolutional layer the input is the image of shape (H_in, W_in, C_in) and the weights account for the neighborhood of the given pixel, say of size K * K. The output is obtained as a weighted sum of the given pixel and its neighborhood. There is a separate kernel for each pair of the input and output channel (C_in, C_out), but the weights of the kernel (a tensor of shape (K, K, C_in, C_out) are independent of the location. Actually, this layer can accept images of any resolution, whereas, the fully connected can work only with a fixed resolution.
Finally one has (K, K, C_in, C_out) parameters, which for the kernel size K much smaller, than the input resolution result into significant drop in the number of variables.

"
What is the conceptual difference between convolutional neural networks and auto-encoders?,"
I'm familiar with Auto-Encoders and I'm about to dive into CNNs. By having a look at the most important component of a CNN, the filter:

I wonder how it is different from Auto-Encoders:

For me, it looks conceptually the same. I even have to admit it is quite of another higher concept: Dimensionality reduction. In e.g. PCA, as well as in AE and in CNN(?) you transform higher data / higher dimensions onto lower / compressed data.
I can see that the methods are somehow different but yet, can't really explain in which manner, finally.
","['convolutional-neural-networks', 'comparison', 'autoencoders']",
"What do ""spatial"" and ""temporal"" mean in the context of image processing?","
I am new to image processing. I am trying to understand CNNs from this blog post. Here's an excerpt from that article that mentions these terms.

A ConvNet is able to successfully capture the Spatial and Temporal dependencies in an image through the application of relevant filters. The architecture performs a better fitting to the image dataset due to the reduction in the number of parameters involved and reusability of weights.

I am not able to understand the terms spatial and temporal, and their respective dependencies in images. I have encountered the spatial and temporal many times. However, still, I am not able to understand how space (spatial) and temporal(time) concepts map to an image.
(By the way, in the quote above, what does the term ""reusability of weights"" mean?)
","['computer-vision', 'terminology', 'image-processing']",
What to do when the ROIs are smaller than $227 \times 227$ in R-CNN?,"
As English is not my native language, I have some hard time understanding the following sentence:

Regardless of the size or aspect ratio of the candidate region, we warp all pixels in a tight bounding box around it to the required size. Prior to warping, we dilate the tight bounding box so that at the warped size there are exactly p pixels of warped image context around the original box (we use p = 16).

This is from the R-CNN paper. I already extracted the ROI, but now, they say that the input of the CNN should be 227 x 227, but a lot of my ROIs are much smaller. How can I deal with it?
","['deep-learning', 'papers', 'object-detection', 'r-cnn']",
GAN performs worse after 50 epochs than after 2,"
I am training GAN on SVHN dataset (house numbers in Google Street View images, dimensions: 3x32x32 - 3 color channels).
The problem is that it performs worse after some training (e.g. after 50 epochs) than after only 2.
Could you please check out my code? Maybe you will be able to notice what can I improve.
I have already tweaked betas in ADAM optimizer (it helped a little bit, because before that, with default settings, d_loss went to 0 after 5 epochs). I also added an extra discriminator training step.
You can find the code below:
batch_size=128

# the input data is scaled to a mean of 0.5 and a standard deviation of 0.5:
transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.5,), std=(0.5,)),
                transforms.Lambda(lambda x: x.view(-1))])


# wypłaszczanie do loadera (view) albo konwolucyjne
# lambda
train_dataset = SVHN(root=""."", split='train', download=True,
    transform=transform) 

test_dataset = SVHN(root=""."", split='test', download=True,
    transform=transform) 

train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0)

# Initialize random noise

def noise(size):
    n = torch.randn(size, 100)
    return n.to(device)

# Define the generator model

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
                                # take a 100-dimensional input (random noise)
                                nn.Linear(100, 256),
                                nn.LeakyReLU(0.2),
                                nn.Linear(256, 512),
                                nn.LeakyReLU(0.2),
                                nn.Linear(512, 1024),
                                nn.LeakyReLU(0.2),
                                nn.Linear(1024, 3*32*32),
                                nn.Tanh()

                            )

    def forward(self, x): return self.model(x)

# Define the discriminator model

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential( 
                                nn.Linear(3*32*32, 1024),
                                nn.LeakyReLU(0.2),
                                nn.Dropout(0.3),
                                nn.Linear(1024, 512),
                                nn.LeakyReLU(0.2),
                                nn.Dropout(0.3),
                                nn.Linear(512, 256),
                                nn.LeakyReLU(0.2),
                                nn.Dropout(0.3),
                                nn.Linear(256, 1),
                                nn.Sigmoid()

                            )
    def forward(self, x): return self.model(x)

# define generator training - input is fake data
def generator_train_step(fake_data):
    # reset the gradient so the parameters will update correctly
    g_optimizer.zero_grad()

    # predict the output of the discriminator on fake data 
    prediction_fake = discriminator(fake_data)

    # torch.ones, because we want 1s outputted by the discriminator when training the generator
    error = loss(prediction_fake, torch.ones(len(real_data), 1).to(device))
    error.backward()

    g_optimizer.step()
    return error

# define discriminator training:

# discriminator as an input takes real data and fake data
def discriminator_train_step(real_data, fake_data):
    # reset the gradient so the parameters will update correctly
    d_optimizer.zero_grad()
  
    #print(real_data.shape)
    #print(fake_data.shape)
  
    prediction_real = discriminator(real_data)

    # calculate loss on real data (expected 1, so torch.ones)
    real_loss = loss(prediction_real, torch.ones(len(real_data),1).to(device))
    real_loss.backward(retain_graph=True)

    prediction_fake = discriminator(fake_data)
  
    # calculate loss on fake data (expected 0, so torch.zeros)
    fake_loss = loss(prediction_fake, torch.zeros(len(fake_data), 1).to(device))
    fake_loss.backward(retain_graph=True)

    d_optimizer.step()
    return real_loss + fake_loss

lr = 1e-3

discriminator = Discriminator().to(device)
generator = Generator().to(device)
d_optimizer = optim.Adam(discriminator.parameters(), lr=lr,  betas=(0.5, 0.999))
g_optimizer = optim.Adam(generator.parameters(), lr=lr,  betas=(0.4, 0.9))
loss = nn.BCELoss()
num_epochs = 50
log = Report(num_epochs)

for epoch in range(num_epochs):
    N = len(train_loader)
    for i, (images, _) in enumerate(train_loader):
        real_data = images.view(len(images), -1).to(device)
        fake_data = generator(noise(len(real_data))).to(device)
        fake_data = fake_data.detach()
        d_loss = discriminator_train_step(real_data, fake_data)
        fake_data = generator(noise(len(real_data))).to(device)
        
        d_loss = discriminator_train_step(real_data, fake_data)
        
        g_loss = generator_train_step(fake_data)
        
        log.record(epoch+(1+i)/N, d_loss=d_loss.item(), g_loss=g_loss.item(), end='\r')
    log.report_avgs(epoch+1)
    z = torch.randn(10, 100).to(device)
    # 10 pictures of dimensions of 3x32x32:
    sample_images = generator(z).data.cpu().view(10, 3, 32, 32)
    grid = make_grid(sample_images, nrow=4, normalize=True)
    show(grid.cpu().detach().permute(1,2,0), sz=10)
log.plot_epochs(['d_loss', 'g_loss'])

In case you would like to see the results of training this GAN, I enclose an example generated images after epoch 2:
EPOCH: 2.000    d_loss: 4.936   g_loss: 1.919   (130.46s - 3131.14s remaining)))


and after epoch 50:

EPOCH: 50.000   d_loss: 1.253   g_loss: 1.038   (3487.47s - 0.00s remaining))

And here is the plot of discriminator loss and generator loss:

","['neural-networks', 'deep-learning', 'generative-adversarial-networks', 'pytorch']",
What is the possible solution to the Problem? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



Don't need a complete solution just some guidance on how to solve it.

Consider that a person has never been to the city airport. It's early in the morning and assumes that no other person is awake in the town who can guide him on the way. He has to drive in his car but doesn’t know the way to the airport.  Clearly identify the four components of problem-solving in the above statement, i.e. problem statement, operators, solution space, and goal state. Should he follow a blind or heuristic search strategy? Try to model the problem in a graphical representation.

","['problem-solving', 'homework']","
Here the Solution to the problem from myself
The problem can be modeled in a graphical way as followed:
A person has to reach from one node of a graph to another, and the current distance from the goal is not known.
Problem statement: Find a path between two nodes of a graph where the edge weights or any other kind of heuristic is unknown.
Operator: Operators in this is driving from one crossing/landmark to the next. In graphical sense, this may be said to be traversing an edge between two nodes.
Solution space: A solution in this problem can be termed as a path from his initial point to the airport. Graphically, this means all possible paths from the starting node to the goal node, irrespective of the distance.
Goal state: The goal state is the state upon reaching which, the algorithm may stop and report success. Here, the airport is the goal state.
The person has never been to the airport before, so he doesn't know how close a location is from the airport. In other words, there are no heuristic values to help the man. Thus, the man should opt for a blind search strategy (such as BFS or DFS).
"
"Is there any work that applies the approach in ""Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms"" to standard Q-learning?","
I am trying to mathematically characterize the finite sample convergence rates for Q-learning. To this end, I have read the following papers

Learning rates for Q-learning, by Eyal Even-Dar et al.;
The asymptotic convergence rate of Q-learning, by Cs. Szepesvari;
Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms, by Kearns and Singh.

In the latter, they introduce a rather simple approach that seems appealing to me; however, they only sketch it for phased Q-learning.
I would be interested in knowing about any source where I could find the same approach modified for standard Q-learning; in section 4 of the paper, they claim that

all of our results can be generalized to apply to standard Q-learning

Moreover, if you feel I am missing any paper that could be of interest with regards to the finite sample convergence of Q-learning, I would greatly appreciate it if you could post the name of it.
","['reinforcement-learning', 'q-learning', 'reference-request', 'papers', 'convergence']",
Can we modelize an RNN by an ANN that takes precedent output as a part of input?,"
Is it possible to consider an RNN as a classical feedforward neural network that just take the precedent output as a part of the input ?
","['neural-networks', 'recurrent-neural-networks']","
Almost. I think that to match the common interpretation of an RNN you need to also have a new input at each time-step (whereas you used the word ""just"" suggesting otherwise).
What you're describing is some function $f_1: \mathbb{R}^n \to  \mathbb{R}^n$ whereas the ""conventional"" RNN is more like $f_2: \mathbb{R}^m \times \mathbb{R}^n \to  \mathbb{R}^n$, where $\mathbb{R}^m$ refers to the new information at each time step.
Of course, this is all an exercise in interpreting semantics. So the other answer to your question is: ""if you want it to""
"
Extracting keywords from messages,"
I'm starting a project where I want to extract keywords from given messages. The keywords are for example something like: ""hard disk"", ""watch"" or other technical components. I'm working with a dataset where a technician wrote a small text if he maintenanced something on a given object.
The messages are often very different in their form. For example sometimes the messages start with the repaired object and sometimes with the current date.
I looked into some NER-Libaries and it doesn't seem like they can handle tasks like that. Especially the German language makes it hard for those libaries to detect entities.
I had the idea to use CRFsuite to train my own NER-model. But I'm not sure how accurate the outcome will be. It would mean that I have to tag a lot of training data and I'm not sure if the outcome will match the time I have to spend to tag those keywords.
Does anybody have any experience with such custom NER-models? How accurate can such a model extract wanted keywords?
","['machine-learning', 'natural-language-processing', 'supervised-learning', 'named-entity-recognition']","
I don't know that NER is the right approach here. It seems to me that you want to find words for certain technical components in free texts, written in German. But ""Festplatte"" is not a named entity. Named entities (cities, companies, countries, etc) in English (and other languages) are usually capitalised, so relatively easy to spot. In German this won't work as every noun is capitalised, named entity or not. But even in English a NER wouldn't help you with ""hard disk"", as it's not what is commonly understood as a named entity.
It's not really an AI solution, but I would get a list of relevant components (eg from a dictionary), and then simply match those in texts. Instead of annotating existing texts, you simply add the words to a list, which would be a bit quicker, generally. And list lookup is very easy to implement.
This, I think, would work a lot better than a machine learning approach. If you find that your technicians often mis-spell the words, use a fuzzy matching algorithm such as Levenshtein distance to allow for close matches; this might also help with inflections.
"
How would you shape a reward function if there was four quantities to optimize?,"
I found this article quite useful on how to shape a reward function in RL. However, the example they gave is quite simple, where the goal is to minimize only two quantities (velocity and distance).
How would you formulate the reward function if you had, for instance, 4 quantities to optimize?

","['reinforcement-learning', 'deep-rl', 'reward-functions', 'reward-design', 'reward-shaping']","
Here is how I managed to construct a reward function in one of my projects, where I trained an RL model for a self-driving robot that has only a single camera to navigate through a tunnel:
$$
R = \left\{
\begin{array}{ll}
d_m - 3 - \left| d_l - d_r \right| & \text{if not terminal state} \\
-100 & \text{otherwise}
\end{array} 
\right.
$$
where $d_m$ is the middle distance, $d_l$ is the left distance and $d_r$ is the right distance, and $d_m, d_l, d_r \in [0, 10]$. To get this information, the agent has a laser sensor. The agent does not directly observe these distances. Instead, it gets a real number as the reward signal that indicates how good the agent is performing an action and tries to map the camera view to it. This function is designed so that the agent should stay in the middle of the tunnel, $-\left|d_l - d_r\right|$, and has to avoid head-on collisions, $d_m - 3$. Thus, the highest possible reward is $R = 10 - 3 - \left| 10 - 10 \right| = 7$

Basically, you can use any number of parameters in your reward function as long as it accurately reflects the goal the agent needs to achieve. For instance, I could penalize the agent for frequent steering on straight sections so that it drives smoothly.
"
What are the 'noisy factors' leading to overfitting?,"
Consider the following excerpt from section 5.5 Regularization (p. 13) of this chapter Logistic Regression.

There is a problem with learning weights that make the model perfectly match the training data.  If a feature is perfectly predictive of the outcome because it happens to only occur in one class, it will be assigned a very high weight.  The weights for features will attempt to perfectly fit details of the training set, in fact too perfectly, modeling noisy factors that just accidentally correlate with the class. This problem is called overfitting.

What are the 'noisy factors' here? Does it refer to the features that are irrelevant to the class label?
Or does it mean the noise/errors in the values taken by features that accidentally correlate with the class label?
","['machine-learning', 'terminology', 'overfitting', 'books']","
Please note:

I am only referring the decision boundary to be a line for simplicity, more often than not it is a hyperplane which is difficult to visualize and spans over n dimensions where n is the dimensionality of your feature space.
The explanation is toned in a more general way for emphasizing explainability.

Answer

What are the 'noisy factors' here? Does it refers to the features that are irrelevant to the class label?


Not Necessarily.


Or does it mean the noise/errors in the values taken by features that accidentally correlate with the class label?


I'm not quite sure I understand.

However
Noisy factors as the literature puts it are outliers in a finite data class. Imagine a dataset where we are asked to calculate the average of a set of numbers. Lets say the numbers are the set S = {2,2,2,2,2,2,2,1000}. The mean value in the case mentioned is 2 if it wasn't for the 1000 at the end.

1000 could be an outlier when you are trying to approximate the mean of the set with some algorithm. The algorithm is more likely to encounter a 2 in the unseen test set than the 1000 it encountered once in a training set.
When an algorithm like Linear Regressions ""learns"" something it is actually learning the weights and intercepts which modify the position of the line in the decision space.
The modifications are Translation(Additive operations) and Rotation(Multiplicative operations) to the said line. It is commonly referred to as the ""weights"" and ""biases"" respectively in case of a simple line Y = mx + c where m is the slope and c is the intercept.
The idea behind noise in the above text is that this:
When an algorithm is trying to ""learn"" these weights you would want it to ignore the outliers - ""Generalize"" but you also want it to not ignore the feature data completely and introduce randomness - ""Specialization"".
How well the line is placed in your feature space is what determines your Classification Effectiveness.
In an ideal world you would want your decision boundary to ignore such outliers which are called noise in the above literature.

Pictures (because everyone likes them)

In the picture below the left image is what a good decision boundary is and the right image is what an overfitted Decision boundary is.
In the left-image case you are trading misclassifications at the cost of better generalizability(Consider that more likely you are going to see an x in the 2nd quadrant and the o was maybe an outlier). You do not fit your training set completely with a 100% accuracy.


TL;DR
You want the model to learn that the x(s) are on the right and the o(s) are on the left but not so specifically as to which x was where. The x in the 4th quadrant and the o in the second quadrant are noisy factors.
"
"In logistic regression, why is the binary cross-entropy loss function convex?","
I am studying logistic regression for binary classification.
The loss function used is cross-entropy. For a given input $x$, if our model outputs $\hat{y}$ instead of $y$, the loss is given by
$$\text{L}_{\text{CE}}(y,\hat{y}) =  -[y \log \hat{y} + (1 - y) (\log{1 - \hat{y}})]$$
Suppose there are $m$ such training examples, then the overall total loss function $\text{TL}_{\text{CE}}$ is given by
$$\text{TL}_{\text{CE}} = \dfrac{1}{m} \sum\limits_{i = 1}^{m} \text{L}_{\text{CE}} (y_i , \hat{y_i}) $$
It is said that the loss function is convex. That is, If I draw a graph between the loss values wrt the corresponding weights then the curve will be convex. The material from textbook did not give any explanation regarding the convex nature of the cross-entropy loss function. You can observe it from the following passage.

For logistic regression, this (cross-entropy) loss function is conveniently convex. A
convex function has just one minimum; there are no local minima
to get stuck in, so gradient descent starting from any point is
guaranteed to find the minimum.  (By contrast, the loss for multi-layer
neural networks is non-convex,  and gradient descent may get stuck in
local minima for neural network training and never find the global
optimum.)

How did they conclude conveniently that the loss function is convex? Is it by plotting or some other means?
","['objective-functions', 'optimization', 'gradient-descent', 'binary-classification', 'cross-entropy']","
The $L_{CE}$ that you provided is binary cross-entropy, the factor $y$ and $(1-y)$ is because $y$ is binary $({0,1})$, careful with the name next time. The cross-entropy loss should have form:
$$L_{CE}=-\displaystyle\sum_{i=1}^C y_i\log(\hat{y_i})$$
Where $C$ is the number of classes. Normally, the $y_i$ factor only is 1 when $i$ is the index of the correct class. Therefore, with each class, the function is just:
$$f(x)=-log(x)$$
Now, to prove this one is convex, we have multiple ways, but my favorite one is computing the derivative and second derivative.
$$\frac{\partial L}{\partial x}=-\frac{1}{x}\Rightarrow \frac{\partial^2 L}{\partial x^2}=\frac{1}{x^2}>0 \text{ for all }x\in(0,1]$$
This proves that $f(x) = -\log(x)$ is convex, given that, if the second derivative of a function is positive, then the function is convex (more info and an example here).
For the case of multiple samples, we also need to prove that the sum of a convex function is a convex function.
Based on the definition of convex function, a function $f:X\rightarrow \mathbb{R}$ will be convex if:
$$f(tx_1+(1-t)x_2) \le tf(x_1) + (1-t)f(x_2)$$
where $0<t<1$ and $x_1,x_2\in X$.
Now, let's assume $h(x) = f(x) + g(x)$ where both $f$ and $g$ are convex. We have:
$$f(tx_1+(1-t)x_2) \le tf(x_1) + (1-t)f(x_2)$$
$$g(tx_1+(1-t)x_2) \le tg(x_1) + (1-t)g(x_2)$$
$$\Rightarrow f(tx_1+(1-t)x_2) + g(tx_1+(1-t)x_2) \le tf(x_1) + (1-t)f(x_2) + tg(x_1) + (1-t)g(x_2)$$
$$\Rightarrow f(tx_1+(1-t)x_2) + g(tx_1+(1-t)x_2) \le t(f(x_1)+g(x_1)) + (1-t)(f(x_2)+g(x_2))$$
$$\Rightarrow h(tx_1+(1-t)x_2) \le th(x_1) + (1-t)h(x_2)$$
$\Rightarrow h$ is the convex function $\Rightarrow$ the summation of convex functions is a convex function.
"
Evaluating a convolutional neural network on an imbalanced (academic) dataset,"
I have trained a posture analysis network to classify in a video of humans recorded in public places if there is a) shake-hand between two humans, b) Standing close together that their hands touch each other but not shake hand and c) No interaction at all.
There are multiple labels to identify different parts of a human. The labels are done to train the network to spot hand-shaking in a large dataset of videos of humans recorded in public.
As you can guess, this leads to an imbalanced dataset.
To train, I sampled data such that 60% of my input contained handshaking images and the rest contained different images than hand-shaking.In this network, we are not looking at just labels but also the relative position of individual labels wrt to one another. We have an algorithm that can then classify them into the three classes.
I am stuck on how to evaluate the performance of this network. I have a large dataset and it is not labeled. So I have decided to pick 25 from class A) and B) and 50 from class (C) to create a small dataset to show the performance of the network.
And to run the network on the large dataset without labels, but because classes A and B are quite rare events, I would be able to individually access the accuracy of the network prediction of True positive and false-positive cases.
Is this a sound way to evaluate ? Can anyone having experience or opinion share their input on this? How else can I evaluate this?
","['deep-learning', 'convolutional-neural-networks', 'classification', 'imbalanced-datasets', 'pose-estimation']",
How to change a single object detection network to a multiple object detection network?,"
I have trained a CNN network to detect a circle and approximate its centre and radius in an image. What I want to do now is detect the centre and radius of all the circles if there are multiple circles present in an image.
How do I proceed to go on about it? Do I have to make changes to my dataset to be able to do so? I tried to look at different architectures that do multiple object detection, but I couldn't understand what changes I could make to my architecture.
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'object-detection', 'image-segmentation']",
Are these visualisations the filters of the convolution layer or the convolved images with the filters?,"
There are several images related to convolutional networks on the Internet, an example of which I have given below

My question is: are these images the weights/filters of the convolution layer (the weights that are learned in the learning process), or the convolved images of the previous layer's image with the filters of the current layer?
image source:
https://stats.stackexchange.com/questions/362988/in-cnn-do-we-have-learn-kernel-values-at-every-convolution-layer
","['neural-networks', 'convolutional-neural-networks', 'filters', 'explainable-ai', 'convolutional-layers']","
Only the first convolutional layer, with filters that process the input [colour] channels directly, can be rendered directly as image patches in the same domain as the input. The left-most panel in your example looks like that.
Further layers of the neural network cannot be rendered like this for two reasons:

They have a number of input channels based on the previous layer's output, for example they may process 32 or 128 channels. There is no simple mapping of these channels to colours.

They respond to a wider range of input stimuli than any single image patch. If you tried to render out all inputs that they respond to then you would likely get an indistinct-looking grey blob, if anything at all. This is different to the first layer which does directly react pixel-by-pixel according to the weights.


What is typically done to render patches like the middle and right-most panels in your example, is to find sample patches that trigger a strong response from that filter. This search can be done using gradient ascent - start with noise, then take gradient steps in the direction of increasing the signal for that filter. This is also the basis of ""Deep Dream"" images, which instead of doing that for small patches, apply it to whole images, and many filters at once.
"
DeepLabV3: Why use global average pooling in the ASPP module?,"
I'm trying to understand the rationale of the various modifications the authors of the DeepLab models have made to their third version, DeepLabV3. In the paper, the following is written:

ASPP  with  different  atrous  rates  effectively  captures
multi-scale information. However, we discover that as the sampling
rate  becomes  larger,  the  number  of  valid  filter weights (i.e.,
the weights that are applied to the valid feature region, instead of
padded zeros) becomes smaller. This effect is illustrated in Fig. 4
when applying a 3×3 filter to a 65×65 feature map with different atrous
rates. In the extreme case where the rate value is close to the
feature map size, the 3×3 filter, instead of capturing the whole image
context, degenerates to a simple 1×1 filter since only the center filter
weight is effective. To overcome this problem and incorporate global
context information to the model, we adopt image-level
features, similar to [58,95]. Specifically, we apply global average
pooling on the last feature map of the model, feed the resulting
image-level features to a 1×1 convolution with 256 filters (and batch
normalization [38]), and then bilinearly upsample the feature to the
desired spatial dimension.

I do not understand how global pooling solves this problem. Is it simply because it does not suffer from the same issue of ASPP (the degeneration of the weights), and serves as an alternative?
From: Chen, L. C., Papandreou, G., Schroff, F., & Adam, H. (2017). Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587.
","['deep-learning', 'image-recognition', 'pooling']",
Can Neural Networks using ReLU activation work without using the bias term in their neurons?,"
I created a super simple NN of 1 input, 2 hidden layers of 2 neurons each and 1 output neuron as shown below.

All activations are ReLUs and neurons doesn't use the bias term. What I found is that the output graph is a combination of two linear functions (one when the input is negative and another when the input is positive) kind of like this.

I think without the bias term, the output will be a linear function (for negative and positive inputs separately) no matter how big the network is. My question is, is this useful at all as an architecture? I assume it might be - if multiple output nodes are available, or is it? Does any of this mean that the a bias term is mandatory? Just trying to get my intuition right here...
","['neural-networks', 'relu']",
Backpropagation not working as expected,"
I'm new to neural networks and I try to make a model that is guessing if a point is below or above relative to a function output. The idea is inspired from this video https://youtu.be/DGxIcDjPzac .
What am I doing wrong?
In the gif below I start the training but it seems that is not working. The blue line is the function (y = x + 50) and all the points above it should be green, but aren't. In order to simplify the example and to debug easier, I picked a simple function such that I can use only a perception for the model.
I also made a method backPropagationDebug(...) to display for the points that are predicted wrong all that matrices in each step, but I couldn't find what's wrong.

public void backPropagation(double[][] input, double[][] expected) {
    double[][][] outputs = getOutputs(input);

    double[][] currentOutput = outputs[outputs.length - 1];
    double[][] currentError = Matrix.subtract(expected, currentOutput);

    for (int i = brain.length - 1; i >= 0; i--) {
        final double[][] layer = brain[i];
        final double[][] previousOutput = outputs[i];

        final double[][] layerTranspose = Matrix.transpose(layer);
        final double[][] previousError = Matrix.multiply(layerTranspose, currentError);

        /* FIST BIT */
        double[][] errorSigmoid = Matrix.copyOf(currentError);

        for (int k = 0; k < errorSigmoid.length; k++) {
            errorSigmoid[k][0] *= - derivativeActivationFunction(currentOutput[k][0]);
        }

        /* SECOND BIT */
        final double[][] slopeMatrix = Matrix.multiply(errorSigmoid, Matrix.transpose(previousOutput));

        /* UPDATE THE WEIGHTS */
        for (int k = 0; k < layer.length; k++) {
            for (int l = 0; l < layer[0].length; l++) {
                layer[k][l] = layer[k][l] - learningRate * slopeMatrix[k][l];
            }
        }

        currentOutput = previousOutput;
        currentError = previousError;
    }
}

The backpropagation steps are inspired from this formulas:


(From: Make Your Own Neural Network By Tariq Rashid)
The code is on github: https://github.com/StamateValentin/Artificial-Intelligence-Playground/tree/7a7446b7faedd7673bc53a62304ff3a5180d77eb
The resources I used are in the README.md file.
","['neural-networks', 'backpropagation', 'supervised-learning', 'perceptron']",
Are policy and value iteration used only in grid world like scenarios?,"
I am trying to self learn reinforcement learning. At the moment I am focusing on policy and value iteration, and I am finding several problems and doubts.
One of the main doubts is given by the fact that I can't find many diversified examples on how to implement these on python, instead I find always only the classical grid world example.
So, my doubt is: Are policy and value iteration used only in grid world like scenarios, or can be used also in other contexts?
","['reinforcement-learning', 'value-iteration', 'policy-iteration', 'dynamic-programming']",
LSTM Recursive Forecast,"
I am confused about the way the LSTM networks work when forecasting with a horizon that is not finite, but I'm rather searching for a prediction in whatever time in future. In physical terms, I would call it the evolution of the system.
Suppose I have a time series $y(t)$ (output) I want to forecast, and some external inputs $u_1(t), u_2(t),\cdots u_N(t)$ on which the series $y(t)$ depends.
It's common to use the lagged value of the output $y(t)$ as input for the network, such that I schematically have something like (let's consider for simplicity just lag 1 for the output and no lag for the external input):
$$
[y(t-1), u_1(t), u_2(t),\cdots u_N(t)] \to y(t)
$$
In this way of thinking the network, when one wants to do recursive forecast it is forced to use the predicted value at the previous step as input for the next step. In this way we have an effect of propagation of error that makes the long term forecast badly behaving.
Now, my confusion is, I'm thinking of an RNN as a kind of a (simple version) implementation of a state-space model where I have the inputs, my output and one or more state variable responsible for the memory of the system. These variables are hidden and not observed.
So, now, the question: if there is this kind of variable taking already into account previous states of the system why would I need to use the lagged output value as input of my network/model?
Getting rid of this does my long term forecast would be better, since I'm not expecting anymore the propagation of the error of the forecasted output. (I guess there will be anyway an error in the internal state propagating)
","['long-short-term-memory', 'time-series', 'forecasting']",
Finetuning solver for Caffe neural network,"
We're working on object detection in thermal images using neural network with Caffe framework. We use SSD ResNet-10 network available in OpenCV repository as it seems to provide the best performance on Raspberry Pi for our needs (in comparison to MobileNet etc.)
https://github.com/opencv/opencv/blob/master/samples/dnn/face_detector/solver.prototxt
train_net: ""train.prototxt""
test_net: ""test.prototxt""

test_iter: 2312
test_interval: 5000
test_initialization: true

base_lr: 0.01
display: 10
lr_policy: ""multistep""
max_iter: 140000
stepvalue: 80000
stepvalue: 120000
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
average_loss: 500
iter_size: 1
type: ""SGD""

solver_mode: GPU
random_seed: 0
debug_info: false
snapshot: 1000
snapshot_prefix: ""snapshot/res10_300x300_ssd""

eval_type: ""detection""
ap_version: ""11point""

Train batch size is 16. Test batch size is 1.
The training process starts at loss 23.4728 and reaches plateau around loss 1.2, learning rate is decreased at iteration 80000 and loss falls down to 0.89. Further decrease continues very slowly to iteration 120000 where loss is around 0.85. Then LR is decreased. Process ends at iteraion 140000 with loss around 0.80 and test evaluation around 0.90.
I noticed that selecting different optimizer gives different results. I tried Nesterov, Adam (with fixed LR) and SGD with different base_lr (0.05) and step size (100000). Are there any recommendation that I could try except of trial&error and waiting 12 hours to compare the results? Reduce/increase batch size? More iterations? Different step sizes?
Adam provides the worst test evaluation. SGD with base_lr reduced to 0.05 and step size 100000 seems to provides the best result now (test eval = 0.94)

","['deep-learning', 'caffe']",
Is the Bandit Problem an MDP?,"
I've read Sutton and Barto's introductory RL book. They define a policy as a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\pi$ at time $t$, then $\pi(a|s)$ as the probability of taking action $A_t = a$ when the current state is $S_t = s$. This definition is the context of the markov assumption, which is why the policy is only dependent on the current state.
When discussing the standard k-armed bandits problem, they write $\pi(a)$ to denote the probability of taking action $a$, since there are no states. However, when designing the agent, clearly, the agent needs to keep track of what the past rewards are for each lever, so either there is a summary statistic of each lever, or the entire history of actions and rewards must be kept.
Is the k-armed bandit problem then a MDP? Why isn't the notation $\pi(a|A_0, R_1, A_1, \ldots, R_T)$ for some sequence  $A_0, R_1, A_1, \ldots, R_T$?
","['reinforcement-learning', 'comparison', 'markov-decision-process', 'notation', 'multi-armed-bandits']","
The bandit problem is an MDP. You can make the same argument about needing data to learn in the stateful MDP setting. The thing is, the data you need (the past rewards in this case) was drawn iid (conditioned on the arm) and is not actually a trajectory. For instance, once you learn an optimal policy, you no longer need to gather data and the sequence of past results doesn't influence your policy.
"
"Feeding CNN FFT of an image, a dumb idea?","
My dataset consists of about 40,000 200x200px grayscale images of centered blobs bathed in noise and occasional artifacts like stripes other blobs of different shapes and sizes, fuzzy speckles and so on in their neighborhood.
They are used in a binary classification problem, with emphasis on recall.
I read that using FFT of image and FFT of the convolutional kernel and multiplying the two, produces a similar result as convolutions would but at a way lower resource expense. This is probably the most straightforward article I found if you need a more detailed description(https://medium.com/analytics-vidhya/fast-cnn-substitution-of-convolution-layers-with-fft-layers-a9ed3bfdc99a)
What I want to do however is simply feed the FFT of images to the standard CNN. The reasoning being, maybe it would be easier for the network to catch on to features that it would miss or tend to weigh less. Or in other words, FFT as a feature engineering technique.
Would this be an idea worth trying to pursue?
If so, any suggestion on which FFT components to extract (Amplitude/Phase, Real/Imaginary)?
","['convolutional-neural-networks', 'image-processing', 'feature-engineering']","
 FFT is in essence linear transformation of the input image and can be represented by application of convolutional filter of the same size as image on the input.
Provided, the convolutinoal neural network is deep enough with sufficient number of parameters and there are skip connections (in order to have a path of purely linear transformations on the input), FFT can be represented by the learned filters. If FFT of the image is relevant for the classification problem, NN most probably would learn to produce them in a certain way.
For image classification problems - when the goal is identify an instance of something, local information is crucial, and this problem is better solved in the spatial, not frequency domain.
However, for your case it seems, like the semantic is rather trivial, and the goal is to get rid of some frequencies. Hence, working in the frequency domain is a sensible option. Possibly, you can combine the spatial and frequency representation in some way.
I think, it would be simpler to work with the real and imaginary part, that with the complex abs and phase, since you need to account for periodicity of the phase in a certain way, and then in the end transform phase to $e^{i \phi}$.
"
How to enforce action bounds between 0 & 1 in soft actor-critic algorithm?,"
In the paper ""Soft Actor-Critic Algorithms and Applications"", appendix C shows enforcing action bounds using the tanh squashing function which is in (-1, 1). I have action bounds in (0, 1), so can I just modify the tanh output by applying the following transformation:
output = 0.5 * (tanh_output + 1). If so, do I need to change logprob formula too?
I have not seen any SAC implementation with different action bounds other than the paper's (-1, 1).
","['reinforcement-learning', 'deep-rl', 'soft-actor-critic']","
Yes you can map the output onto [0,1] as you indicate. You should treat this as a modification to the environment. I.e. imagine that the environment takes actions in [-1, 1] instead of [0,1]. No you don't need to change any equations.
"
What does $v(S_{t+1})$ mean in the optimal state-action value function?,"
In Sutton & Barto's Reinforcement Learning: An Introduction page 63 the authors introduce the optimal state value function in the expression of the optimal action-value function as follows: $q_{*}(s,a)=\mathbb{E}[R_{t+1}+\gamma v_{*}(S_{t+1})|S_{t}=s, A_{t}=a], \forall s \in S, \forall a \in A$.
I don't understand what $v_{*}(S_{t+1})$ could possibly mean since $v_{*}$ is a mapping, under the optimal policy $\pi_{*}$, from states to numbers which are expected returns starting from those states and at different time steps.
I believe that the authors use the same notation to denote the state-value function $v$ that verify $v(s)=\mathbb{E}[G_{t}|S_{t}=s], \forall s \in S$ and the random variable $\mathbb{E}[G_{t+1}|S_{t+1}]$ but I'm not sure.
","['reinforcement-learning', 'optimal-policy']",
Do bi-directional RNNs necessarily use 100% teacher forcing?,"
I typically think of teacher forcing as optional when training an RNN. We may either:

use the output of time-step $t$ as the input to time-step $t+1$

use the $(t+1)$th input as the input to time-step $t+1$


When I actually sat down to write a bidirectional RNN from scratch today I realised it would be impossible to do without 100% teacher forcing, because each time step needs access to the ""history"" going back to the 0th time-step (forward direction) and going back (or forward - however you want to think of it) to the last time-step (backward direction).
Is that correct?
","['recurrent-neural-networks', 'teacher-forcing', 'bidirectional-rnn']",
How is complex systems research interacting with AI research?,"
Because future AI may produce emergent phenomena, and because these are probably gaps in our current understanding of this, it feels like complex systems may be an increasingly important research field.
Unless there is some kind of commonality of emergent behaviour (or aspects of complex systems more generally) it seems that future AI systems may behave in ways very difficult to predict. Potentially complex system research of the brain could help in artificial neural network understanding but because of the diversity of the latter this may be a loose similarity. Further, this may or may not hold for other types of AI.
The main question is from a basic educational level, how does complex systems research affect AI and vice vera. Possible subquestions (whatever helps understanding of this topic better): Is there much research at the intersection of AI and complex systems (so someone like Melanie Mitchell)? What kinds of things may complex systems research inform AI? Is AI now helping us understand AI better?
",['complexity-theory'],
How to ensure that the ES-HyperNEAT algorithm generates an ANN in the substrate?,"
I'm trying to implement the ES-HyperNEAT algorithm using the original paper, as well as the pseudocode provided in the official user page. Occasionally, the algorithm would be unable to generate a network in the substrate. This happens when it finds no valid nodes that could connect a path between the input and output neurons.
I've noticed that this is highly dependent on how the hyperparameters (e.g., variance threshold and band threshold) were tuned.
Is my implementation correct, i.e., is this normal behavior? If so, is there a good way to ensure that a network is always generated (aside from directly connecting the input and output neurons)?
","['neat', 'hyper-parameters', 'neuroevolution']","
You can modify your fitness-funciton in a kind of ""dirty"" way:
Check while generating the fitness if all outputs are the same and than set the fitness to -10000 (or another very bad value depending on your normal fitness).
This way ES-HyperNEAT could/should learn to not generate this kind of networks.
PS: I do not have enough reputation to make this a comment.
"
Would this count as a Transfer Learning approach?,"
I have two datasets, Dataset 1(D1) and Dataset 2(D2). D1 has around 22000 samples, and D2 has around 8000 samples. What I am doing is that I train a Deep Neural Network model with around three layers on the dataset D1, which has an accuracy of around 84% (test accuracy = 82%).
Now, when I use that model to make predictions on D2 without any fine-tuning or anything, I get an accuracy of around 15%(test accuracy = 12.3%). But when I add three more layers to the pre-trained model while keeping the three layers of the initial model(trained on D1) frozen, I get around 90% accuracy (test accuracy = 87.6%) on D2.
This tells me that because the initial model was performing so poorly without any fine-tuning, most of the learning that led to the 90% accuracy was only because of the additional layers, not the layers that were transferred from the model trained on the D1 dataset. Is this a correct inference? And if it is, then is it still valid to call this a Transfer Learning application? Or does it has to have more accuracy without fine-tuning to be rightly listed as a Transfer Learning problem.
","['neural-networks', 'deep-learning', 'deep-neural-networks', 'transfer-learning', 'fine-tuning']",
What is the state of the art in melody generation?,"
Generative Adversarial Networks can generate realistic photos of people, such as thispersondoesnotexist.com. I wonder whether one can train an artificial intelligence on a batch of plain solo melodies (no instruments) and ask it to produce a new and similar one.
This article suggests the techniques require a lot of work and are still young:

We have explored and evaluated the generation of music using a Generative Adversarial Network as well as with an alternative method in the form of an N-gram model. Our GAN is able to capture some of the structure of single track music. We have accomplished our goal of identifying structural similarities shared across music compositions. However, the music we created lacks coherent melodies and needs improvement.

What is the state of the art in melody generation?
","['neural-networks', 'reference-request', 'generative-adversarial-networks', 'generative-model', 'state-of-the-art']","
you do not need ai for that, just a little bit of math / statistics:
audio: https://m.soundcloud.com/user-919775337/sets/algorithmic-reinterpretation
method:
https://stats.stackexchange.com/questions/541044/a-new-method-for-processing-music-scores
source code:
https://github.com/githubuser1983/algorithmic_python_music/blob/main/12RootOf2.py
"
How do very rare words tend to have very high PMI values?,"
Consider the following formulation for pointwise mutual information (PMI):
$$\text{PMI}(w, c) = \dfrac{p(w, c)}{p(w)p(c)}$$
Suppose there are $W$ words with $C$ context words. Then one can write in terms of frequency that
$$\text{PMI}(w, c) = \dfrac{\sum\limits_{i = 1}^{W} \sum\limits_{j = 1}^{C} f_{ij} }{\sum\limits_{i = 1}^{W}f_i \sum\limits_{j = 1}^{C} f_j} $$
I am going to calculate $\text{PMI}(w, c)$ for two different words and contexts based on the following table. The table is taken from fig 6.10 of this book.

I calculated PMI for all pairs and tabulated below.
$$\begin{array}{|c|c|c|} \hline
    & \text{computer} & \text{data} & \text{result} & \text{pie} &  \text{sugar} \\  \hline
  \text{cherry} & 8.2 \times 10^{-7} & 2.9 \times 10^{-6} & 3.9 \times 10^{-5} & 1.7 \times 10^{-3} & 8.4 \times 10^{-4} \\ \hline
\text{strawberry} & 0 & 0 & 2.6 \times 10^{-5} & 1.4 \times 10^{-3} & 3.8 \times 10^{-3} \\ \hline
\text{digital} & 9.6 \times 10^{-5} & 8.6 \times 10^{-5} & 5.2  \times 10^{-5} & 2.8 \times 10^{-6} & 1.9 \times 10^{-5} \\ \hline
\text{information} & 8.6 \times 10^{-5} & 9.1 \times 10^{-5} & 1.03  \times 10^{-4} & 1.2 \times 10^{-6}& 2.7 \times 10^{-5}\\ \hline
\end{array}$$
Based on the above values, we can also notice the following fact:

PMI has the problem of being biased toward infrequent events; very rare words tend to have very high PMI values.

However, it's unclear to me how this apparent behaviour is related to the mathematical formulation of the PMI above.
How do we understand the fact quoted above from the fractional form of PMI given by the equations above?
","['natural-language-processing', 'intuition']","
Note: mutual information is typically expressed as a log value (usually $log_2$, as it's related to information), which makes them easier to compare -- you then don't have to worry about large exponential expressions with negative exponents.
The reason for the bias is in the distributional properties of words. A rare word will have a small frequency of occurrence (by definition), so multiplied with a more common word, the denominator will be fairly small. But the numerator will also be very small, as there aren't that many opportunities for the rare word to occur near the more common word.
However, while with two common words the nominator will be much higher (more co-occurrences), the denominator will now — compared to the rare/common instance — be several orders of magnitude larger. Thus the overall value will be smaller.
Rare co-occurrences are overly dependent on chance, as the nominator can easily fluctuate randomly (say, 1 or 2 co-occurences), and with a comparatively smaller denominator that can make a big difference.
In linguistics, you would generally ignore mutual information values above a certain threshold, as the values are just too unreliable to be meaningful. In fact, when I was still working in academia, mutual information was increasingly replaced by other metrics, such as log-likelihood, which were more robust.
"
Where does the so-called 'loss' / 'loss function' fit into the idea of a perceptron / artificial neuron (as presented in the figure)?,"
I am currently studying the textbook Neural Networks and Deep Learning by Charu C. Aggarwal. Chapter 1.2.1.3 Choice of Activation and Loss Functions presents the following figure:



$\overline{X}$ is the features, $\overline{W}$ is the weights, and $\phi$ is the activation function.
So this is a perceptron (which is a form of artificial neuron).
But where does the so-called 'loss' / 'loss function' fit into this? This is something that I've been unable to reconcile.

EDIT
The way the loss function was introduced in the textbook seemed to imply that it was part of the architecture of the perceptron / artificial neuron, but, according to hanugm's answer, it is external and instead used to update the weights of the neuron. So it seems that I misunderstood what was written in the textbook.
In my question above, I pretty much assumed that the loss function was part of the architecture of the perceptron / artificial neuron, and then asked how it fit into the architecture, since I couldn't see any indication of it in the figure.
Is the loss / loss function part of the architecture of a perceptron / artificial neuron? I cannot see any indication of a loss / loss function in figure 1.7, so I'm confused about this. If not, then how does the loss / loss function relate to the architecture of the perceptron / artificial neuron?
","['objective-functions', 'activation-functions', 'artificial-neuron', 'perceptron', 'loss']","
The loss function is simply a way to measure how wrong a neural network is, it doesn't affect the output of the neuron.
Say we have a neural network with 3 output neurons that attempts to classify images of cats, dogs, and humans. The output it gives is the confidence of the neural network's classification. For example if the output is [0, 0.2, 0.8] (0 being the output of the 1st neuron, 0.2 of the 2nd, and 0.8 of the 3rd), this means that the neural network thinks that the image has 0% probability of being a cat, 20% of being a dog and 80% of being a human.
Imagine that the image that was shown to the network is a human, we can say that the target values are [0, 0, 1] because we want it to output that the image is a human with 100% confidence. Now we must measure how wrong the prediction actually was using a loss function. There are many loss functions, but for simplicity I'll use the squared error. In this case the loss will be equal to (1-0.8)^2=0.04 (expected value - output)^2.
The closer the output is to 1, the result inside of the brackets will be closer to 0, so the loss will be smaller. The objective is to minimize this loss function. For example, if the output was 1 instead of 0.8, the network's loss will be (1-1)^2 = 0. If the output was 0.2 instead, the loss would be (1-0.2)^2 = 0.64, which is larger than the two previous, as it is 'more wrong'.
To train the network we use this instead of accuracy for the following reason. With both these outputs [0, 0.1, 0.9], [0.2, 0.3, 0.5] the network predicts 'human', the largest value, but in the first case it is 90% sure whereas in the second it is only 50% sure. We can say that the first network is better, but if we only used accuracy, as both predict the same, they would appear to be just as good.
The same happens when they make a mistake. If the expected values are [0, 1, 0] and one model predicts [0.5, 0.4, 0.1] and the other predicts [0.9, 0, 0.1], they both got it wrong, but the first one was less wrong. The first loss would be (1-0.4)^2 = 0.36 and the second would be (1-0)^2 = 1, which is much higher
"
What is the best way to generate German paraphrases?,"
What is the best method to generate German paraphrases? The state-of-the-art are seq2seq transformer models, like T5, but they only work for English sentences. I found the multilingual MT5 model, but how do you fine-tune this for German?
","['transformer', 'natural-language-understanding', 'seq2seq', 'natural-language-generation']",
Identifying rotating and resizing letters with background noise,"
I'm trying to complete a captcha, and here is what it looks like:

Between captchas the calligraphy of the letters is the same, but the letters may be resized and rotated. And the background noise (the small dots and lines over and around the letters) will be different. Any Hangul letter may appear.
Edit 1: I can generate any number of new captchas with an answer for each of them. But to be clear, the answers that are generated are for entire captchas, that is, multiple Hangul letters arranged in a specific order as the answer for each captcha, not for individual letters.

What type of machine learning is best for this problem?
How do I extract good data from the image above for this problem?

Update 1: Unfortunately no one has given any suggestions for how to solve this yet. My idea at the moment is to mimic the model in this paper: https://www.ics.uci.edu/~xhx/publications/HHR.pdf
","['neural-networks', 'machine-learning', 'deep-learning', 'training', 'datasets']",
Characterize the high probability bound for learning algorithm,"
Suppose we have a dataset $S = (x_1, \dots x_n)$ drawn i.i.d from distribution $D$, a learning algorithm $A$ and error function $err$. The performance of $A$ is therefore defined by the error/confidence pair $(\alpha, \beta)$, that is
$$P(err(A(S)) \geq \alpha) \leq \beta$$
where the randomness is taken on $S$. Usually, by solving this inequality, we can get some constraints between $\alpha$ and $\beta$, in the form that $\alpha \geq f(\beta, n)$. My understanding is that if we treat $\beta$ as a constant, then we have the high probability error bound in terms of $n$. Is that correct?
Another question is that what if the function $f$ we get is not uniform across all $\beta$, for example
\begin{equation}
\alpha \geq \begin{cases} f_1(n, \beta) \quad \beta \geq 0.5 \\
f_2(n, \beta) \quad \beta< 0.5
\end{cases}
\end{equation}
In this case, how to derive the high probability error bound?
","['computational-learning-theory', 'pac-learning', 'approximation-error']",
How could an attacker poision the training data?,"
I came across the following definition of Backdoor attack (in this paper):

These attacks are accomplished in two steps. First, special patterns are embedded in the targeted model during the training phase, which is typically achieved by poisoning training data.

How could the training data be poisoned? Isn't the training data local to the software developer who is developing the ML algorithm? And won't he train the data on his local machine (could be a company too) before releasing the software out?
","['machine-learning', 'adversarial-ml']",
How could poisoning attacks be prevented in adversarial Machine Learning,"
How we could prevent poisoning attacks in adversarial Machine Learning?
I read it from this link and other sources. As per my understanding, poisoning could be done after the ML algorithm has been made or while building up the model with test data. One example I read was like a car is driving and a small image could be pasted on a wall, which could make it turn left, so the car's AI algorithm misclassifies it.
But for poisoning the test data the attacker needs access to internal software at the time before the model is built so that the model that is built is corrupted. How could an attacker do that? That seems impossible. Or it could be in cases where the ML model is being built dynamically.
Just poured my thoughts out. I am interested in knowing thoughts about the above, and, specifically, what are the ways in which poisoning could be prevented?
","['machine-learning', 'adversarial-ml']",
What is the best way to train neural network with imbalanced mixed data (images and structured data)?,"
I have structured data and image data to solve a regression problem. One sample of structured data can be related to N images.
If I use only structured data, I get decent performance, but not enough to properly solve the problem. I want to use related images to the structured data to improve performance.
My approach was to create 3 neural networks. The first one for the image input, the second one for structured input, and the third one to combine both image and structured networks and output the final result.
The main problem is how to properly combine one sample of structured data with N images. All the images already saved as bottleneck features from one of Keras applications. I combined the structured data with each corresponding image and got a very good result. (Duplicating structured sample for each corresponding image) But investigation showed that the validation dataset had training structured samples, but only combined with different images. So the network just memorized the dataset very well (on 110k samples) giving great synthetic results and bad generalization on real-world data. After I fixed validation and training datasets (each dataset doesn't have the same sample of structured data), the neural net showed real performance, which is bad.
So my question is: What is the state-of-the-art to combine one sample of structured data with N images? Of course, structured data and images are logically connected. Train 2 neural networks alone and then combine their outputs in third network? Or train all three networks at once? Or maybe train images with CNN and then combine CNN output with structured data using some gradient boosting algorithm?
","['deep-learning', 'convolutional-neural-networks', 'reference-request', 'structured-data', 'model-request']",
LSTM Forecast Evolution,"
I have a confusion about the way the LSTM networks work when forecasting with an horizon that is not finite but I'm rather searching for a prediction in whatever time in future. In physical terms I would call it the evolution of the system.
Suppose I have a time series $y(t)$ (output) I want to forecast, and some external inputs $u_1(t), u_2(t),\cdots u_N(t)$ on which the series $y(t)$ depends.
It's common to use the lagged value of the output $y(t)$ as input for the network, such that I schematically have something like (let's consider for simplicity just lag 1 for the output and no lag for the external input):
$$
[y(t-1), u_1(t), u_2(t),\cdots u_N(t)] \to y(t)
$$
In this way of thinking the network, when one wants to do recursive forecast it is forced to use the predicted value at the previous step as input for the next step. In this way we have an effect of propagation of error that makes the long term forecast badly behaving.
Now, my confusion is, I'm thinking as a RNN as a kind of an (simple version) implementation of a state space model where I have the inputs, my output and one or more state variable responsible for the memory of the system. These variables are hidden and not observed.
So now the question, if there is this kind of variable taking already into account previous states of the system why would I need to use the lagged output value as input of my network/model ?
Getting rid of this does my long term forecast would be better, since I'm not expecting anymore the propagation of the error of the forecasted output. (I guess there will be anyway an error in the internal state propagating)
Thanks !
","['long-short-term-memory', 'forecasting']",
CFD Reinforcement Learning Topology optimization wind tunnel,"
I want to create a reinforcement learning environment, designed for win tunnel simulations, where for each iteration a deep convolutional model could receive the 3D vector/scalar fields from the past simulation and output a better shape that maximizes the reward function (e.g. minimize drag, maximize lift, etc.)
The observation and action space for the neural network is the same, the inputs of the model will be 3D arrays representing velocity field, pressure field, etc. and the output will be a 3D array (created using Conv3DTranspose) with values [0, 1] which represents the mesh. I'm thinking that the architecture of the model could be something similar to an auto-encoder.
My plan is to use the algorithm of Marching Cubes in order to create the mesh from those points and openFoam for the CFD simulations.
This is a small diagram showing the workflow

The goal will be to have multiple trained models specialized in optimizing a particular reward function, like minimizing drag or maximizing lift, for any object/shape given as input.
What are your thoughts on this? Do you think it makes sense?
","['reinforcement-learning', 'deep-learning', 'convolutional-neural-networks', 'transpose-convolution']",
To denote a training example should I use row vector or column vector?,"
This code accesses the first 3 examples in the iris data set,
from sklearn.datasets import load_iris
iris = load_iris()
print(iris.data[:3])

and gives
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]]

To denote the first example, $x_1$, should I use a column vector like
\begin{bmatrix}
5.1\\3.5\\1.4\\0.2
\end{bmatrix}
or a row vector like the following?
$$[5.1 \ 3.5 \ 1.4 \ 0.2]$$
Andrew Ng suggests putting examples in columns

while typical relational databases putting examples in rows.
I'd just like to know the pros and cons of different notations so that I can decide which one I would follow.
","['terminology', 'notation']",
What does the product of probabilities raised to own powers used for entropy calculation quantify?,"
Suppose $X$ is a random variable taking $k$ values.
$$Val(X) = \{x_1, x_2, x_3, \cdots, x_k\} $$
Then what is the following expression of $N(X)$ called in literature if exists? What does it signify?
$$ N(X) =  \prod \limits_{i = 1}^{k}  p(x_i)^{p(x_i)}$$
I am using the notation $N(X)$ for the sake of my convenience only.

Background: I am asking this question because of the definition of entropy I encountered. Entropy is calculated as follows.
$$ H(X) = - \sum\limits_{i = 1}^{k} p(x_i) \log p(x_i) $$
If I further solve $H(X)$ as follows, I will get $H(X)$ in terms of N(X).
$$ H(X) = - \sum\limits_{i = 1}^{k} p(x_i) \log p(x_i)  = - \sum\limits_{i = 1}^{k}  \log p(x_i)^{p(x_i)}  $$
$$\implies H(X)= - \log  \prod \limits_{i = 1}^{k} p(x_i)^{p(x_i)}   
 = - \log N(X)$$
Entropy is used to characterize the unpredictability of a random variable.

A logarithm is generally applied to many quantities in AI in order to bring them into the desirable range where overflow and underflow won't happen. Hence I am thinking that $\dfrac{1}{N(X)}$ is the actual quantity one has to measure (the entropy?). Hence I am guessing that $N(X)$ can be treated as reciprocal of entropy. So, does $N(X)$ is a quantity that has quantified the predictability of a random variable?
$$N(X) = \dfrac{1}{2^{H(X)}} = \dfrac{1}{2^{entropy}}$$
So, I am wondering whether there is any quantity that $N(X)$ quantify.
","['terminology', 'probability', 'entropy']","
I don't know if $N(X)$ has a name or has any applicability in AI, but I can comment on how this function varies as the $H(X)$ based on your equation
$$N(X) = \dfrac{1}{2^{H(X)}}$$
which looks correct to me (just apply the $\log_2$ to both sides).
In the case of a Bernoulli random variable (which is a categorical r.v. that can take 2 values, $0$ or $1$, which is a special case of your categorical r.v., if you set $k=2$), then this is the relationship between the probability that this random variable $X = 1$ and the entropy of this r.v.

So, the entropy is $1$ when the probability is $0.5$ and decreases as the probability tends to $0$ or $1$, which makes sense, because the entropy quantifies the uncertainty about the r.v. Here, the entropy is computed in bits because we use the logarithm $\log_2$.
So, for $k=2$ (in your example), then, if $H(X) = 1$,
$$N(X) = \dfrac{1}{2^{H(X)}} = \frac{1}{2} = \frac{1}{2}^{\frac{1}{2}} * \frac{1}{2}^{\frac{1}{2}}$$
which is equal to $P(X = 1) = 1 - P(X = 0)$.
Now, as $H(X)$ decreases to $0$, then $\dfrac{1}{2^{H(X)}}$ increases, because the denominator $2^{H(X)}$ becomes smaller, where the smallest value is $H(X) = 0$
$$N(X) = \dfrac{1}{2^{H(X)}} = 1 = 1^1 * ? $$
This is already problematic because $0^0$ is not well-defined. So, I already see a problem with $N(X)$.
For $k > 2$, I think the same reasoning applies, but now the maximum value of $H(X)$ should be $\log_2 k$.
So, I don't think that $N(X)$ is of any practical value as it can lead to expressions like $0^0$.
However, this type of problem also arises in the original formulation of the entropy, because $\log_2 0$ is not defined. The convention for when $P(X) = 0$ is to set $P(X) \log P(X)$ to zero [1].
Anyway, it seems to me that one way to interpret $N(X)$ is as the (average?) probability that $X$ takes one of the values, and it could be that the information content is what you are looking for. The definition can be found in [1].
"
Why do we commonly use the $\log$ to squash frequencies?,"
Term frequency and inverse document frequency are well-known terms in information retrieval.
I am presenting the definitions for both from p:12,13 of Vector Semantics and Embeddings
On term frequency

Term frequency is the frequency of the word $t$ in the term frequency
document $d$. We can just use the raw count as the term frequency:
$$tf_{t, d} = \text{count}(t, d)$$
More commonly we squash the raw frequency a bit, by using the
$\log_{10}$ of the frequency instead. The intuition is that a word appearing 100 times in a document doesn’t make that word 100 times more likely to be relevant to the meaning of the document.

On inverse document frequency

The $\text{idf}$ is defined using the fraction $\dfrac{N}{df_t}$, where $N$ is the total number of documents in the collection, and $\text{df}_t$ is the number of documents in which term $t$ occurs.......
Because of the large number of documents in many collections,  this measure too is usually squashed with a log function. The resulting  definition  for  inverse document frequency ($\text{idf}$) is thus
$$\text{idf}_t = \log_{10} \left(\dfrac{N}{df_t} \right)$$

If we observe the bolded portion of the quotes, it is evident that the $\log$ function is used commonly. It is not only used in these two definitions. It has been across many definitions in the literature. For example: entropy, mutual information, log-likelihood. So, I don't think squashing is the only purpose behind using the $\log$ function.
Is there any reason for selecting the logarithm function for squashing? Are there any advantages for $\log$ compared to any other squash functions, if available?
","['natural-language-processing', 'definitions', 'books', 'tf-idf', 'logarithm']","
It's much easier to deal with logarithms, as the relevant numbers are usually very small or very large. If you have a long exponential expression, it's hard to see the difference, but if you're looking at 4.3 vs 5.6, you can immediately see what's happening. And logarithms are a well-known (and well-understood) way of achieving this compression. You can easily interpret the difference, depending on the base of the logarithm used.
Quite often the $log_2$ is used when you're dealing with entropy or information, as those are usually expressed in bits.
"
Text matching: fuzzy names matching with learning,"
I'm new to AI/ML and I want to research and learn about techniques that could help me to solve this complex task. Any hint would be appreciated.
Let me explain it with an example:
Let's look at two columns PUR.SUPPLY.MTL_REQ_HDR_ID and MTL.PO_REQUISITION_HEADERS_TAB.ID. It is likely they are related (one is the FK and the other one is PK).

As a human I was able to do it by doing the following:

I decoded abbreviations,
I identified context (MTL module),
I identified the subject (requisition header),
I identified ID keyword,
I identified irrelevant information (TAB postfix),
I matched words that are not in the exact order,
I estimated which elements/words do not have to match/occur,

I would like to match millions of columns relatively quickly (seconds). I would like algorithm to learn:

what words are likely context,
what are irrelevant,
what are subjects,
ideally learn some patterns (prefixes, postfixes, name formats, etc),
based on user responses - approve/reject match,
build dictionary of abbreviations,
estimate probability...

I know this is a complex task, but maybe you know a technique, tool, library, article, example ... anything that could be helpful? Any help would be appreciated.
Thanks.
","['machine-learning', 'natural-language-understanding']",
What to do when you have massive amount of data but you don't have enough computation power for training a machine learning model?,"
For example, I have a massive amount of data, but I have limited computational resources and time to train on the full data. Other cases may include, I have huge amounts of 360-degree images, where I need to train on full-size images (without cropping down), but I have limited computation power (GPU, RAM, etc.), what can I do in those cases?
","['machine-learning', 'deep-learning', 'data-preprocessing']","
It's hard to answer this question without knowing what your goal is, but if your data is extensive, high quality, especially if it is labelled, and no similar dataset is publicly available, then publishing it freely with some kind of challenge could be very helpful if that's an option. Many organisations have the opposite problem: available computing resources but lack of data. If your data are new and interesting, I could imagine researchers wanting to use it. If people get good results, they may publish them, good for AI generally, and presumably also useful to you. If you do this of course you would need to publicise it a bit so people become aware of it.
"
How to detect entities in Montezuma's Revenge environment,"
I'm thinking of implementing ""Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"" paper. In this paper authors used some custom object detector for entity detection(eg: Key, rope, ladder, etc) but they did not give any information about this custom detector. Can you please give me a suggestion on how to Implement this object detector?
","['reinforcement-learning', 'deep-rl', 'dqn', 'hierarchical-rl']",
"What is the difference between the definition of ""accuracy"" in machine learning and federated learning?","
What is the difference between the definition of ""accuracy"" in machine learning and federated learning?
In particular, how is the accuracy calculated in the following paper:

Cai, Lingshuang, et al. ""Dynamic Sample Selection for Federated Learning with Heterogeneous Data in Fog Computing."" ICC 2020-2020 IEEE International Conference on Communications (ICC). IEEE, 2020.

","['machine-learning', 'terminology', 'papers', 'accuracy', 'federated-learning']",
How to prove the second form of Bellman's equation?,"
I'd like to prove this ""second form"" of Bellman's equation: $v(s) = \mathbb{E}[R_{t + 1} + \gamma v(S_{t+1}) \mid S_{t} = s]$ starting from Bellman's equation: $v(s) = \mathbb{E}[G_{t} \mid S_{t} = s]$ where the return $G_{t}$ is defined as follows: $G_{t} = \sum_{k=0}^{\infty}{\gamma^{k}R_{t+k+1}}$.
I tried to use the linearity of the expectation as follows: $v(s) = \mathbb{E}[R_{t+1} \mid S_{t} = s] + \mathbb{E}[\sum_{k = 1}^{\infty}{\gamma^{k}R_{t+k+1}} \mid S_{t} = s]$
Which gives us: $v(s) = \mathbb{E}[R_{t+1} \mid S_{t} = s] + \gamma\mathbb{E}[\sum_{k = 0}^{\infty}{\gamma^{k}R_{(t + 1) + k + 1}} \mid S_{t} = s] = \mathbb{E}[R_{t+1} \mid S_{t} = s] + \gamma\mathbb{E}[G_{t + 1} \mid S_{t} = s]$
I also tried to develop the second formula: $v(s) = \mathbb{E}[R_{t+1} \mid S_{t} = s] + \gamma\mathbb{E}[v(S_{t+1}) \mid S_{t} = s]$ and I'm tempted to say that $\mathbb{E}[G_{t+1} \mid S_{t} = s] = \mathbb{E}[v(S_{t+1}) \mid S_{t} = s]$ but that would only be right in the case that both follow conditions are verified:

We have the value function of a particular state $s^\prime$ inside the expectation of the second term (something like $\mathbb{E}[v(s^\prime) \mid S_{t} = s]$ which would directly give $v(s^\prime)$ since it's a scalar) and not $v(S_{t+1})$.
We have $\mathbb{E}[G_{t+1} \mid S_{\textbf{t+1}} = s^\prime]$ in the second term.

I'm probably not understanding something correctly especially what $v(S_{t+1})$ would mean (that wasn't covered in the material I'm following but for me it would be just a function that maps the possible states at time step $t+1$ to the expected return starting from that step at that time step).
","['reinforcement-learning', 'proofs', 'value-functions', 'bellman-equations']",
How is the bias added after the convolution in a CNN?,"
I'm having trouble understanding how bias is added to the feature extraction convolution. I've seen people either refer to the bias as a single number that changes per filter or the whole matrix that is the size of the output. Here is what I mean:



$I$ is the input single-channel image.
$F$ is the filter.
$b$ is the bias.
""Izhod"" means ""output"".

Which is actually the correct bias used in CNN?
","['convolutional-neural-networks', 'feature-extraction', 'bias']",
"How do sigmoid functions make it so that the prediction $\hat{y}$ indicates the probability that the observed value, $y$, is $1$?","
I am currently studying the textbook Neural Networks and Deep Learning by Charu C. Aggarwal. Chapter 1.2.1.3 Choice of Activation and Loss Functions says the following:

The choice of activation function is a critical part of neural network design. In the case of the perceptron, the choice of the sign activation function is motivated by the fact that a binary class label needs to be predicted. However, it is possible to have other types of situations where different target variables may be predicted. For example, if the target variable to be predicted is real, then it makes sense to use the identity activation function, and the resulting algorithm is the same as least-squares regression. If it is desirable to predict a probability of a binary class, it makes sense to use a sigmoid function for activating the output node, so that the prediction $\hat{y}$ indicates the probability that the observed value, $y$, of the dependent variable is $1$.

I've read about sigmoid functions, but it isn't clear to me how they make it so that the prediction $\hat{y}$ indicates the probability that the observed value, $y$, of the dependent variable is $1$. So how do sigmoid functions make it so that the prediction $\hat{y}$ indicates the probability that the observed value, $y$, of the dependent variable is $1$?
EDIT: I am specifically asking about the probability that the value is $1$ (that is, how sigmoid functions specifically check for this).
","['neural-networks', 'activation-functions', 'perceptron', 'sigmoid']",
How do I quantify the difference in sample efficiency for two almost similar methods?,"
I am comparing my coded TD3 (Twin-Delayed DDPG) and the same TD3 (same hyperparameters) but with Priority Replay Buffer instead of a normal Replay Buffer.
From what I have read, PER (Priority Experience Replay, Priority Replay Buffer) aims to improve sample efficiency. But how do I measure or quantify sample efficiency on these two? Is it who gets the highest average reward in a given number of episodes? Does it have something to do with the batch size?
","['reinforcement-learning', 'deep-rl', 'policy-gradients']",
Why Word2Vec is called a neural model if no neural network is used in it?,"
Word2Vec model does not use any neural network. It uses logistic regression only.
Consider the following paragraph from p:18 of Vector Semantics and Embeddings

We’ll  see  how  to  do  neural  networks  in  the  next  chapter,
but  word2vec  is  a much simpler model than the neural network
language model, in two ways.  First,word2vec simplifies the task
(making it binary classification instead of word prediction). Second,
word2vec simplifies the architecture (training a logistic
regression classifier instead of a multi-layer neural network with
hidden layers that demand more sophisticated training algorithms). The
intuition of skip-gram is:

Treat the target word and a neighboring context word as positive examples.

Randomly sample other words in the lexicon to get negative samples.

Use logistic regression to train a classifier to distinguish those two cases.

Use the learned weights as the embeddings.



But, why it is called a neural model then? Is there any version of Word2Vec that use neural network?
","['neural-networks', 'terminology', 'word2vec', 'books']",
Why the collection of background/negative image dataset is not taught in object detection tutorials and books?,"
While I was doing an object detection project, I have encountered the problem of getting FALSE POSITIVES and FALSE NEGATIVES. After days of research on StackOverflow, I figured out that I need to collect more negative images or background images.I decided to document this process so other people could easily solve this issue and the result of documentation is this. After training the model with Negative/Background images, my FP/FN rates were normalized so that in video frames I started getting fewer FPs. All of us, machine learning developers get experience by getting hands dirty - this is clear to all of us. But I haven't seen(probably missed) any video tutorials or examples on books showing how to collect background images and why we need them at all.
So here is the question: Okay, so every experienced ML engineer knows what is the FP/FNs are, and their prevention methods. But why this topic is less known and taught within popular object detection tutorials and books? Or am I missing something?
","['machine-learning', 'deep-learning', 'computer-vision', 'agi']",
"What is the meaning of ""continuous"" in a continuous bag-of-words model?","
The word continuous in mathematics is a property of either a set or a function that says that the underlying object has no discontinuity in the range mentioned. If the object is a set, then $[-1,1]$ is a continuous one while $\{-1, +1\}$ is not. Similarly, a function is said to be continuous if the actual value and the limiting value at every point in the domain are equal.
Now, coming to CBOW. I read the following statement from p:334 of Natural Language Processing by Jacob Eisenstein

Thus, CBOW is a bag-of-words model, because the order of the context words does not matter; it is continuous, because rather than conditioning on the words themselves, we condition on a continuous vector constructed from the word embeddings.

What is meant by continuous in this case?  Does continuous vector stand for a vector of real numbers?
","['terminology', 'books', 'bag-of-words', 'cbow']","
A bag-of-words-model (BOW) is usually used to represent a text: you throw all the words together (as if in a bag), without keeping track of their sequence. This is a gross simplification over a text, as word sequencing plays an important role in creating the meaning of a text. But on the positive side it's easier to handle, eg in information retrieval tasks, where you might not need the precise meaning anyway.
So the BOW is discrete and symbolic, as it represents each of its elements by a set of words that are contained in it. Nothing numeric in there. You'd calculate the similarity of two items by comparing the two sets, how big is their intersection, and the difference between the two.
A CBOW is a slight modification: instead of the words, we use vector representations of them; and instead of having $n$ vectors for the $n$ surrounding words, they're all added up (formula 14.14) It's still a BOW, as the set of words used to represent an element is now the set of words surrounding it within a certain distance ($h$). What makes it continuous is the switch from a set of words (ie symbols) to a vector.
He contrasts this with a recurrent neural network, where words are represented by a state vector which gets updated after every new word, going back to the very beginning of the text. This would give different representations for the same word occurring in the same localised context, whereas the CBOW would return the same representation.
For example, for $h$ being 1 (to keep it simple):

when a word has a meaning, then a word has a purpose.

Now imagine we're interested in the encoding of word: in the recurrent case the first one is when + a + word, whereas the second one is when + a + word + has + a + meaning + , + then + a +word — the sequences here represent the updated state of the network after the respective words have been added.
In the CBOW case, both occurrences of word are encoded by a + word + has (the word plus/minus one word either side, as $h$ is 1). So they will be identical.
To answer your question, continuous here is in contrast to discrete or symbolic, and indeed refers to a numerical vector.
"
How to reject boxes inside each other with Non Max Suppression,"
I’m working on an object detection cnn, and having some issues with non max suppression. When I have a small box inside a large box, NMS is not rejecting the smaller, incorrect box, because its IOU is small (large union, small intersection). How is this scenario typically dealt with? When using out of the box pretrained models for object detection I don’t seem to get boxes completely inside other boxes. Example here:  green is ground truth, blue is prediction. The center box has a tiny blue box inside that’s not getting rejected by NMS
","['convolutional-neural-networks', 'object-detection', 'non-max-suppression']","
IOU makes sense for determining accuracy against ground truth, but for non max suppression have you tried intersection over minimum size?
"
Is it possible to train a perceptron to tell if a picture is a dog or cat?,"
I know perceptron is a linear classifier that tells linearly separable binary class data, such as iris setosa vs. iris versicolor via their sepal's length and width.
I'd just like to know if I have 2 groups of photos, one is for dog and the other is for cat, is it possible to train a perceptron to tell if a picture is a dog or cat?

","['image-recognition', 'perceptron']",
What is the exact difference between distributional semantics and distributed semantics?,"
While studying word embeddings in natural language processing, I encountered the following statement on page 327 of the textbook Natural Language Processing by Jacob Eisenstein

Distributional semantics are computed from context statistics. Distributed semantics are a related but distinct idea: that meaning can be represented by numerical vectors rather than symbolic structures.

The dissimilarity between them is that distributed semantics represent the meaning of a word by a vector of numbers. Distributional semantics represent the meaning of a word by symbolic structure (inferred from paragraph).
I can say, in distributed semantics, the word cat can be represented by the vector $[23, 43,21,16]$ (for example).
Similarly, please, give me a small example of how the meaning of a word is represented by symbolic structure (which should not be necessarily correct).
What is meant by symbolic structure here?
","['comparison', 'terminology', 'word-embedding', 'books']","
I can't really make much sense of Eisenstein's distinction between distributional and distributed. And I think in your question you actually mix up the two terms as well, as distributed semantics involve symbolic structures, whereas distributional semantics are numerical vectors according to his definition. EDIT: actually, he seems to mix it up himself there?! Very unclear paragraph there.
I can only imagine that the symbolic structures he refers to here are semantic networks and the like, as in

(is-a feline mammal)
(is-a lion feline)
(has-a feline tail)

Here the meaning of lion, as a feline mammal with a tail, is defined through a symbolic structure, and not in reference to the context of usage. Why this should be distributed, I can only guess: the meaning components are split over a set of statements, which build up a larger structure perhaps?
It could, of course, be the case that this is covered elsewhere in the book — I haven't had the time to look through all of it.
UPDATE: Thinking more about this, perhaps he means that distributional semantics are representations where each word is a straight co-occurrence vector, ie a vector as large as the words used to define contexts, while distributed semantics is similar, but it's a different vector which is created through processing the contexts (and could thus be smaller)?
"
"What do RNN, LSTM, and GRU layers do in Tensorflow?","
I have gone through some theoretical introductions of RNN and LSTM, which do not contain any code, and they describe in fair detail what the cells do, how they apply operations like forget, sigmoid, etc.
Then I am trying to implement them with tensorflow, and even after reading the documentation, I am unable to connect the layers' API with my theoretical understanding of the operations. For example, take the following simple code:
import tensorflow as tf # tensorflow 2.5.0
inputs=tf.random.normal(shape=(32, 10, 8))
lstm = tf.keras.layers.LSTM(units=4, return_sequences=True, return_state=True)
outputs=lstm(inputs) # Call the layer, gives a list of three tensors
lstm.trainable_weights # Gives a list of three tensors 

So what exactly is the layer doing here based on the input it receives and the weights that were initialised randomly?
If I am to implement the layer's operation myself, how do I do that?
The Google and Keras documentation contain a lot of example code, but not really explanations of the internal mathematical operations. So any help in this area, or any reference that explains the mathematical operations (not in general, but what's happening in the Tensorflow layer) would be greatly appreciated.
I have the exact same question regarding RNN and GRU layers too.
","['tensorflow', 'keras', 'recurrent-neural-networks', 'long-short-term-memory', 'gated-recurrent-unit']",
Validation accuracy very low with transfer learning,"
I am using MobileNetV3 from TF keras for doing transfer learning; I removed the last layer, added two dense layers, and trained for 20 epochs.

How many dense layers should I add after the MobileNet and How dense should they be?

How many epochs should I train for?

Validation loss and validation accuracy have a strange pattern, is that normal?


Is there anything I am missing?

","['tensorflow', 'keras', 'transfer-learning']",
Is it ok to have an accuracy of 65% and a sensitivity of 90% with Naive Bayes for sentiment analysis?,"
I am creating a sentiment analysis model using Naive Bayes. When I test the model, I get an average accuracy of 65%; however, the sensitivity of the model is much higher, 90%.
So, I am wondering if there are methods to fixing this data; or, since the sensitivity is very high, then would it be ok to move forward with the model?
","['accuracy', 'sentiment-analysis', 'naive-bayes', 'sensitivity']","
I could get perfect sensitivity for positive sentiment if I always predict positive sentiment, but my accuracy could be 50%ish depending on the distribution of positive sentiment in the data. The sensitivity and accuracy scores alone are not enough to tell you if your model is any good, you will need to have some goal that you are trying to achieve e.g., get 70% accuracy.
"
How much statistics is involved in AI?,"
I am a 3rd-year math major, who is interested in computer science, particularly algorithms and competitive programming (did some olympiads in high school, ACM ICPC in university, etc.), and I have been meaning to get into AI.
I have all the prerequisites to get started, but the problem is that I really, really hate statistics. I took a course on it last year and found it to be very dry.
I've heard people say that AI is mostly statistics and I am very concerned if it's true. I can tolerate some amount of stats, but, if the field literally revolves around it, I will not be able to do it.
So, exactly how much statistics is involved in AI? Are there fields of AI which use it less than others?
","['machine-learning', 'statistical-ai', 'ai-field', 'statistics']","
Many people without a formal/solid background in statistics (e.g. without knowing exactly what the central limit theorem (CLT) states) are doing research on machine learning, which is a very big and fundamental subfield of AI that has a big overlap with statistics, or using machine learning to solve problems.
So, in my view, you don't need to learn everything about statistics to do research on some AI topic, including machine learning, but you need to have an understanding of the basics (at least a full introductory college-level course on statistics and probability theory), and the more you know the better.
More specifically, if you don't know what the CLT or the law of large numbers state, you will not have a full understanding of many things that are going on. At the same time, you will find a lot of research papers (published in ML conferences and journals) that do not even mention hypothesis testing, but it's important to have an idea of what a sample, sample mean, sample variance, likelihood, maximum likelihood estimation (MLE) or Bayes' theorem are. In fact, MLE is widely used in machine learning, but not many people using/doing ML would probably be able to explain precisely what the likelihood function is.
Finally, in my opinion, having a formal/solid (not necessarily extensive) background in statistics should be a prerequisite for doing research in machine learning (you need to really know what the likelihood function is!), which some people called applied/computational statistics or glorified statistics for some reason, but not necessarily for using machine learning to solve some problem. Moreover, there are other areas of AI that do not make use of statistics, but ML is probably the most important area of AI. So, if you hate statistics, you may not like AI and particularly ML, but maybe you will change your opinion about statistics, once you understand what e.g. neural networks are capable of doing or not.
"
"What is the difference between terminal state, nonterminal states and normal states?","
In Sutton & Barto's Reinforcement Learning: An Introduction, page 54, the authors define the terminal state as following:

Each episode ends in a special state called the terminal state

But the authors also say:

the episodes can all be considered to
end in the same terminal state, with different rewards for the different outcomes. Tasks
with episodes of this kind are called episodic tasks.

I believe there is also a fundamental difference between a terminal state, nonterminal states and plain, normal states:

In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted S, from the set of all states plus the terminal state, denoted S+.

In the first quote, it appears as if the terminal state is just a term to describe the final state of an episode, but, from the second quote, I understand that the terminal state is the same no matter the outcome of the episode. If we consider the game of chess, what would we consider as a terminal state? Would it be the state that, if reached, will end the game (checkmate), no matter the result (win, loss)? But then how can we describe a state that would lead to draw? If we say about a state that leads to a draw that it's a nonterminal state since we can play an ""infinite"" number of turns without reaching a win or a loss hence without reaching the terminal state, aren't we implicitly supposing that reaching a draw isn't a result for which we should attribute a reward (e.g. 0)? And if we name a state that leads to a draw a terminal state, then what would be the difference between a normal state and a nonterminal state?
","['reinforcement-learning', 'comparison', 'terminology', 'markov-decision-process', 'state-spaces']",
Can you use a graph as input for a neural network?,"
We want to try and distinguish real voices from (deep)fake voices using the graphs generated by a discrete fourier transform (generated from .wav audio files). We know from each image if it is a real or a fake voice, so it's a supervised classification problem. An image would look like this:

We think that real voices generate a graph with clear spikes, whereas fake voices have more noise resulting in less clear spikes. For this reason, we thought of using a CNN to take such an image as input (with x and y-axes ommited), and classify it as real or fake. Our concern is that it's actually a graph and not an image of an object, so we're not sure if this would be a good approach. We could also use the arrays generated from the fourier transform, but we're not sure how we could use that as input as we want to classify if it's real or fake, and not predict y for each x.
","['neural-networks', 'machine-learning']","
There no problem with the use of the data in form of an array to classify, whether the audio belongs to a real or fake voice. Just use 1d convolutional neural network with downsamplings or some global pooling operations, such that in the final layer the temporal extent of the signal has length 1. This would be the logit for binary classification.
However, as far as I understand, you get rid of phase after the Fourier transform, but it can be useful for the prediction. Probably, a better approach would be to use mel_spectrogram https://en.wikipedia.org/wiki/Mel-frequency_cepstrum for this problem.
"
How to test the robustness of an agent in a custom reinforcement learning environment?,"
I have used the stable-baseline3 implementation of the SAC algorithm to train policies in a custom gym environment. So far the results look promising. However, I would like to test the robustness of the results. What are common ways to test robustness? So far, I have considered testing different seeds. Which other tests are recommended?
","['deep-rl', 'testing']",
What are the various problems RL is trying to solve?,"
I have read most of Sutton and Barto's introductory text on reinforcement learning. I thought I would try to apply some of the RL algorithms in the book to a previous assignment I had done on Sokoban, in which you are in a maze-like grid environment, trying to stack three snowballs into a snowman on a predefined location on the grid.
The basic algorithms (MC control, Q-learning, or Dyna-Q) seemed to all be based on solving whichever specific maze the agent was trained on. For example, the transition probabilities of going from coordinate (1,2) to (1,3) would be different for different mazes (since in one maze, we could have an obstacle at (1,3)). An agent that calculates its rewards based on one maze using these algorithms doesn't seem like it would know what to do given a totally different maze. It would have to retrain: 1) either take real life actions to relearn from scratch how to navigate a maze, or 2) be given the model of the maze, either exact or approximate (which seems infeasible in a real life setting) so that planning without taking actions is possible.
When I started learning RL, I thought that it would be more generalizable. This leads me to the question: Is this problem covered in multi-task RL? How would you categorize the various areas of RL in terms of the general problem that it is looking to solve?
",['reinforcement-learning'],
Why do I have better RMSE when I don't scale the target? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.







                        Improve this question
                    



I use PyTorch for training a simple neural net for a regression task on a dataset with 12 numerical features + target (target is the 13th column) + 2 categorical features
Before training, I execute
# numeric_columns = numeric_columns[:-1]
scaler = StandardScaler()
scaler.fit(df_train[numeric_columns]])

Also, in my custom torch.util.data.Dataset I scale the data using my scaler object.
After each epoch, I evaluate the RMSE(""reversed scaled"" prediction, non-scaled target), like the following:
y_pred = (y_pred * self.scaler.scale_[13]) + self.scaler.mean_[13] 
loss += self.criterion(y_pred , y_true).item()

RMSE if I don't scale the target (the first comment would be uncommented and the y_pred row would be commented) is around 0.95 (I tried multiple hyperparameters)
RMSE if I scale the target is 1.7
The target has mean 3.3 and standard deviation of 2.
What am I doing wrong? I thought scaling the target is a must when dealing with neural networks.
","['neural-networks', 'pytorch']",
What is the difference between feature extraction and fine-tuning in transfer learning?,"
I'm building a model for facial expression recognition, and I want to use transfer learning. From what I understand, there are different steps to do it. The first is the feature extraction and the second is fine-tuning. I want to understand more about these two stages, and the difference between them. Must we use them simultaneously in the same training?
","['deep-learning', 'transfer-learning', 'feature-extraction', 'fine-tuning', 'emotion-recognition']","
The difference between the two approaches (feature extraction vs fine-tuning) is well explained here:
Fine Tuning vs Joint Training vs Feature Extraction
Also, this paper evaluate the performance one can hope to achieve with 2 sequence models (ELMo and BERT) with each approach:
To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks
"
What's mutual exclusivity in meta-learning?,"
What do we mean by mutual exclusivity of tasks?
This work (E Pan, 21) and this one (M Yin, 20) state that most classification meta-learning algorithms fail for non-mutually exclusive tasks as the model may over-fit to a task, and no model can solve all the tasks at once (respectively).
I had trouble understanding the exact meaning of a ""task"" in meta classification here. [E Pan, 21] uses ""task"" synonymously with ""new class"", while [M Yin, 20] states ""...prior work uses a per-task random assignment of image classes to N-way classification labels"". However, some priors on few-shot learning [S. Hugo, 17], and [Y Wang, 19] agree with FFLab's, (20) description of ""task"" which I found more clear:

The number of classes (N) in the support set defines a task as an N-class classification task or N-way task, and the number of labeled examples in each class (k) corresponds to k-shot, making it an N-way, k-shot learning problem.

Where the support set $D_s$ here is part of the meta training data $D$ which comprises a support and test set $D_t$ $D = <D_s, D_t>$ [Weng, 18].
However, even with a better understanding of what a ""task"" is, I still couldn't get what constitutes mutually exclusive tasks.
","['terminology', 'papers', 'meta-learning']",
How to choose the reward in reinforcement learning? [duplicate],"







This question already has answers here:
                                
                            




How do we define the reward function for an environment?

                                (2 answers)
                            


What are some best practices when trying to design a reward function?

                                (2 answers)
                            

Closed 2 years ago.



I am solving a combinatorial optimization problem, where I do not have a global optimum, so the goal is to improve the objective function as much as possible. So, to do this, I was inspired by this article Reactive Search strategies using Reinforcement Learning, local search algorithms and Variable Neighborhood Search, I apply during several iterations, heuristics to improve the solution, that is to say, that at each iteration I must choose a heuristic and apply it on the current solution.
In this article, they have defined the state space as the set of heuristics to apply and the action space is the choice of a heuristic among these heuristics.
Regarding the reward, they gave +1 if the solution is improved and -1 if the solution is not improved.
Sincerely, I did not understand how we define the reward for example here -1 and 1, and according to which criteria we choose the reward to use?
","['reinforcement-learning', 'papers', 'reward-design', 'combinatorial-optimization']",
"What is the difference (if any) between semantic segmentation and multi-class, mutually exclusive classification?","
Multi-class classification is simply assigning all data points into one of up to any finite number of mutually exclusive labels.  I am new to the field(s) of AI/ML and I keep hearing people use the term ""semantic segmentation.""
I want to ""translate"" this AI/ML jargon into something more familiar to me.  The best video I have found so far to explain what it is made me wonder, what is the difference between semantic segmentation and classification?
NOTE:  I am specifically not referring to so-called multi-label ""classification"" which allows a data point to have more than one label at a time.  In my experience, that sort of labeling is not classification at all, which is a division into mutually exclusive sets (no overlap).
","['classification', 'semantic-segmentation', 'semantics', 'multiclass-classification', 'labels']","
Both things are similar. But, I think there is a bit of a difference in interpretation.
If what you are solving is a multi-class classification problem in an image, a proper measure of performance of an algorithm would be the accuracy of the prediction for each pixel.
While one of the most used measures of performance for semantic segmentation is the
IOU (intersection over union) for each class. Which, makes sense if your objective is to create a segmentation (a mask) for each class.
"
Changing a CNN-LSTM image captioning architecture to use BiLSTMs,"
Currently I'm dealing with an assignment that made us implement the network mentioned in this paper. The network has an architecture similar to this:

As you can see it uses a Unidirectional RNN (in my case LSTM), which does the many to many sequence prediction task while training, giving LSTM outputs to dense layers with softmax activation. For generating the captions, the network is only given the image at first, and then using the prediction of the image, generates a word, which is then fed to the network along with the generated hidden state, and the model does this recursively to find a unique stop token. Here's the prediction code:
def predict(self, image, max_len = 30):
      output = []
      hidden = None
      inputs = self.encoder(image).unsqueeze(1) # Image features
      for i in range(max_len): # Recursively feed generated words to LSTM 
        lstm_out, hidden = self.decoder.lstm(inputs,hidden)
        output_vocab = self.decoder.fc(lstm_out)    
        output_vocab = F.softmax(output_vocab.squeeze(1), dim=1).detach().cpu().numpy()
        words_indices = output_vocab.argsort(axis=1).squeeze()
        word = words_indices[-1]
        if word == self.unk_token_index:
            word = indices[-2]
        output.append(int(word))
        if word == self.end_token_index:
          break  
        inputs = self.decoder.embed(torch.LongTensor([[word]]).to(image.device))
      return output

The problem I'm having right now is that I don't know whether this generation scheme works with BiLSTMs. Right now my training loss is way better for the sequence to sequence prediction task than the UniLSTM, but my generated captions are far worse.
This is a sample caption generated by Bi-LSTM:

This is a sample caption generated by UniLSTM:

My training loss for BiLSTM converges to 10e-3, while for UniLSTM it converges to 0.5. But the problem is that even before overfitting, BiLSTM is only generating gibberish.
","['deep-learning', 'natural-language-processing', 'computer-vision', 'long-short-term-memory', 'pytorch']",
"Selecting features for a neural network: is it redundant to have a feature that is an average (or max, or min) of some other features","
I'm trying to create a neural network that would able to look at the current price of a crypto asset and classify between a ""BUY"", ""SELL"" or ""HOLD"". So far for my input features, I've decided to go with the past 40 opens, closes, highs, lows, turnover, and volumes (240 features + the current price so 241 total features).
Would it be redundant/not ideal if I had another feature that was the average of the past 40 opens for example? What about the max/min of the past opens?
My thinking was that with only the raw prices data of the past 40 days, the neural network would be able to ""detect"" and create the most optimum features like the average or max in the hidden layers. And therefore, having the avg. or the max/min of some existing features would be unnecessary or perhaps worsen the performance of the model.
Or is there no clear answer and would this be something I'd only be able to figure out by testing against data?
Thanks for your help!!
","['feature-selection', 'algorithmic-trading']","

Or is there no clear answer and would this be something I'd only be able to figure out by testing against data?

That is the general rule you should always consider when looking at feature engineering (which is what you are proposing), as well as for many architecture choices.
It is very hard to tell in advance what a change to a machine learning system will do for whichever metrics you are interested in. You may have some experience that applies, or find similar experiments online that you can take inspiration from. But you will want to test everything, and should take care to use good practice when evaluating different options - e.g. a cross validation dataset (sometimes called a development dataset).

Would it be redundant/not ideal if I had another feature that was the average of the past 40 opens for example? What about the max/min of the past opens?

One aspect of multi-layer neural networks, is that they can in theory learn useful internal features in the hidden layers from raw data. These internal features are unlikely to be exact copies of mean values or min/max values, or anything else you would construct manually. However, they can be similar enough in end result that manually derived features that you think of will not make much difference.
So you would think that derived features would not be useful in nerual networks. In practice though they can be, because the convergence process to find the best internal features is not perfect. Smart feature engineering can improve the performance of a neural network classification or regression supervised learning. Sometimes you can find ""golden"" engineered features that relate really well to your target variable, and that boost results significantly.
A couple of things to bear in mind:

A ""scattergun"" approach of trying a large number of derived features might seem attractive, but there is a risk of overfitting the training data. If you try enough times you may find something that works purely by chance but only for the training data set.

Nonlinear combinations that make conceptual sense given the problem domain can be worth looking at. For instance if you want to predict house prices, and your raw data was house width and depth, then floor area width * depth might be a useful feature.


Feature engineering is still something of an artform. Automated systems using the scattergun approach with filtering are competitive, but domain insight can still win.
If you have vast amounts of data and the CPU time cost is not an issue, you may want to forgo feature engineering due to the theoretical redundancy. It seems possible to make a giant neural network using latest features such as skip connections and batch normalisation, feed it raw (but normalised) data, and press ""go"" to get a state-of-the-art result. From that perspective, feature engineering is for when you don't have ""big data"" or deep pockets for heavy processing - for many of us that still means feature engineering is a standard approach on every project.
"
How to learn transition type in a 1-hour extended DJ Mix?,"
How would you design a model which learns the transitions in a given 1-hour DJ Mix? To be specific, the model should be able to learn transitions, specify the occurring time and the type (Crossfade, Infinite Loop, and so on). Data annotation is way too long since I have 3000+ DJ Mixes that are 1 hour long each, as mentioned. It's almost impossible to annotate the transitions in each mix without spending lots of money. Is there a way to do it unsupervised?
","['machine-learning', 'deep-learning', 'unsupervised-learning', 'architecture']",
Document clustering from ordered pages list,"
I have a series of ordered pdf pages which own to different documents. Let me give you an example:
Pages: 1 2 3 4 5 6
True Pages: 1 2 | 1 2 3 4
So I have like six ordered pages, two of which from document A, and the remaining from document B. I do not have documents labels so the grouping should be done in an unsupervised way.
Which could be a reasonable approach? Using only CNN to detect border pages shouldn't be enough to discern documents, so I was thinking to something like RNN->CNN or CNN->RNN but I don't know how it would practically work because it is the first time I don't use labels in my TF model.
Do you think it would be a reasonable idea?
","['convolutional-neural-networks', 'tensorflow', 'recurrent-neural-networks', 'image-recognition']",
How do autoregressive attention mechanism work in multi-headed attention?,"
[LONG POST!!] I am working on a DNN model that works as an improviser to generate music sequences. The idea of generating music is based on taking a sequence of music nodes (their index representation) and generating sequences that are distinctive with more context and coherent structure as well as capturing syntactic and structural information from the original sequences. Therefore I am dealing with a time series dataset. Similar work was reported in ""Attentional Networks for music generation"" but in our case, we have a different model architecture and different dataset.
It has been known that Transformer (attention) suffers in multivariate time series dataset (Source: Attention for time series forecasting and classification). But given these problems were reported two years ago, the SOTA should be better by now. For that reason, my target is to use the attention mechanism in a way to overcome these challenges.
Recently I have been using the multiheaded attention layer from TF and testing with head size between 128 and 3074 and head number from 1 to 10 and dropout from 0.1 to 0.5. Based on the results there was no noticeable improvement in the model performance, it seems that the multi-headed attention layer didn't have contribution during training.
Therefore and after carefully reading the literature I found that autoregressive attention is the best option for this types of problem. Basically, by making the attention autoregressive, it will compute the attention over the previous (decoder) outputs in such a way as to avoid using future information to make current predictions (to preserve the notion of causality). So the attention has to designed so that at each time step it needs to be autoregressive, for example, use previously generated sequences as extra input while generating the next symbol.
In ""Autoregressive Attention for Parallel Sequence Modeling"" paper they introduced the autoregressive attention mechanism in order to maintain the causality in the decoder. I didn't understand what they mean in Section 3.3 which describe the implementation of autoregressive attention. My problem is in the autoregressive implementation, in the paper they stated that autoregressive mechanics was implemented using the masking technique which changes all of the elements in the upper-right triangle and the diagonal
to −∞ 3 to ensure that all the scores that would introduce future information into the attention calculation are equal to 0 after the softmax. I was hoping to see how it was implemented in the code to get a better idea of how it works.
Here is how the attention is implemented in tensorflow:
def multiHeadedAttentionLayer(cell_input):  
    cell_state = None    

    if cell_state is None:
        cell_state = cell_input

    mha = tfa.layers.MultiHeadAttention(head_size=128, num_heads=5, dropout = 0.5)  
    cell_output = mha([cell_input, cell_state])   
    cell_state = cell_input

    return cell_output

Then the function is recalled in the model architecture with the rest of the layers (below is a section of the model architecture only):
x = MaxPooling1D(pool_size=2)(x) # previous layer
x = multiHeadedAttentionLayer(x) # attention layer
x = LSTM(lstmneurons, kernel_regularizer=regularizers.l2(kreg3_rate), dropout=dropout3_rate, recurrent_dropout=dropout4_rate)(x) # following layer
x = BatchNormalization()(x) # following layer

etc....
Based on my intuition the autoregression should take the output results and feed them back to the input at every time step, so my questions are:
Why do we need the masking technique?
How to implement the masking technique in this case?
Is there is a code for the autoregressive attention that I have a look at for reference?
Is my current intuition about autoregressive attention correct as shown in the diagram?

","['neural-networks', 'deep-learning', 'python', 'transformer', 'attention']",
What is the best clustering method to detect anomalies for data with mostly categorical data?,"
I have a dataset with about 85 columns. Out of the 85 columns, 70+ are categorical. My goal is to identify the outliers in this dataset through clustering methods as I do not have a target column.
What is the best way to approach this? Is it advisable to convert all the 70+ columns to dummies in pandas and use a clustering algorithm like DBScan?
","['unsupervised-learning', 'clustering', 'algorithm-request', 'anomaly-detection', 'categorical-data']",
"What is the difference between ""ground truth"" and ""ground-truth labels""?","
I'm aware that the ground-truth of the example at the top left-hand corner of the image below is ""zero""

However, I am confused about the meaning of the terms ground truth and ground-truth labels. What is the difference between them?
","['machine-learning', 'comparison', 'terminology', 'data-labelling']","
Ground Truth
'Ground truth' is that data or information that you have that is 'true' or assumed to be true. That means that you have high or perfect knowledge of what it is. For example, in your image of numbers, you know that the first row are zeros, the second row are ones, the third are twos, and so on. You have 10 rows of data, each row is of  a different class or category. Each class has 16 samples. Ground truth data is used to train machine learning or deep learning models. The example you provided is from the Modified National Institute of Standards and Technology (MNIST) database which is commonly used for building image classifiers for handwritten digits.
Ground Truth Labels
The 'ground-truth labels' are the names you choose to give them. You may choose to label the classes as '0', '1', '2', etc., or as 'zero', 'one', 'two', etc. Maybe you think in Greek. If so label them as 'μηδέν', 'ένα', 'δύο', etc.
Reference
MNIST database
"
Might AGI need to be flawed?,"
An example is the halting problem, which states computing cannot be solved by exhaustion, but which humans avoid trivially by becoming exhausted.
Humans typically give up what seems like a lost cause after a certain point, whereas a computer will just keep chugging along.

Do flaws have utility?

Can inability be a strength and will AGI require such limitations to achieve human-level intelligence?  Humans are simply not capable of infinite loops, except arguably in cases of mental illness. Are there other problems similar to the halting problem where weakness is a benefit?
","['ai-design', 'philosophy', 'agi', 'human-like', 'halting-problem']",
how to go from mathematical problem to neural network (and back)?,"
I am a little confused on how, you can find online papers that describe complex Machine Learning formulas in a mathematical/probabilistic way, and, in the other hands, easy tutorials that teach you how to use frameworks to create neural networks, without mentioning the maths behind.
What is not clear is, what is the correlation between these two worlds? What are the ""parameters"" that make you understand i.e. how many layer to code, what kind of perceptrons to use, etc?
to make an example:
Let's take this formula, which in Wikipedia Italy is described as ""the standard learning algorytm"":

And suppose that size of w and x is 4. , and g(x) and f(x) are, for examples, linear functions.
What next? Where do I start coding a neural network that solves this problem?
It would seem more logical to me to code this ""directly"" without defining perceptrons, convolution, layers etc.
","['neural-networks', 'machine-learning', 'math', 'definitions']","
Basic feed forward neural nets (MLPs) are essentially just computing sequences of matrix multiplications (with nonlinear activations in between), so this is in fact easy to code ""directly"" like you mentioned. The more difficult part is computing the gradients with respect to the parameter matrices (usually with backpropagation). However there really isn't anything that fancy behind the basic neural network model, it really is just sequences of simple blocks.
You certainly want at least one hidden layer usually, because without it you'll just have a generalized linear model. Neural networks are useful for creating arbitrarily nonlinear models, which can be achieved by adding more layers or more neurons per layer.
If you want some clarification about how to code a neural net, I recommend taking the classic Andrew Ng ML course on Coursera.
With regard to the amount of layers, number of neurons, etc, these are hyperparameters -- there is no known way to determine the correct values for them without experimentation.
"
Best way to use/learn ML for board-game reinforcement learning,"
I am relatively new to Python but I taught myself enough to code a two-player board game that is similar to chess. It has a simple Tkinter UI. Now I am dipping into machine learning, and I want to write another program to play itself in this game repeatedly and ""naturally"" learn strategies for playing the game.
Can anyone give advice on what I might be able to use for this? Is Tensorflow a good option? Is there a Python library well suited for this that I could adapt and train? I am partially through the buildingai.elementsofai.com course, but I am still very new at ML / AI.
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'python']","
There are many approaches - the initial one would be a rule based one with some amount of randomness. The ML-AI approach is some variation of reinforcement learning, defining your game as an environment, see for instance openai-gym. ”Some variation” might be Deep Q Learning or A3C.
"
What is it about sigmoid activations in particular that allows for the keeping and forgetting of past information from different time scales?,"
My understanding is that normal recurrent neural networks (RNNs) are not good at keeping past information from different time scales. Furthermore, my understanding is that Gated RNNs, such as Long Short-Term Memory, model the keeping and forgetting mechanisms explicitly with sigmoid activations, namely gates. What is it about sigmoid activations in particular that allows for the keeping and forgetting of past information from different time scales?
","['recurrent-neural-networks', 'long-short-term-memory', 'activation-functions', 'sigmoid', 'gated-recurrent-unit']",
Loss function to minimize the distance between sets,"
Are there references or links to examples about loss functions ""Distance Metrics"" which could be used to minimize the distance between two sets for a neural network. More precisely, this distance metric must depend on the whole set in calculation and not only a single point as the Euclidean distance.
It is known that Hausdorff distance is used to find the distance between two sets and it is well used for images comparison but it depends additionally on a point in calculation. For my case, I can't depend on a single point for the distance metric but I must consider the whole set to compare it with the other set! Is there any recommendation?
","['neural-networks', 'machine-learning', 'objective-functions']",
How is it possible that Q-learning can learn a state-action value without taking into account the policy followed thereafter?,"
From my readings, I have been taught that the state-action value depends on the policy being followed. That seems logical because the expected return from actual actions will be different depending on which actions follow it.
On page 58 of Sutton & Barto's book, we have

So, how is it possible that Q-learning can learn a state-action value without taking into account the policy followed thereafter (i.e. the policy followed after having taken action $a$ in the state $s$)?
","['reinforcement-learning', 'q-learning']","
The action-value function DOES take into account the policy being followed - that's precisely what the notation $\mathbb{E}_\pi$ is for. Specifically, $\mathbb{E}_\pi$ is a shorthand for
\begin{align*} 
\mathbb{E}_{a_i \sim~ \pi(a_i \,|\, s_i), \, (r_{i+1}, s_{i+1}) \sim p(r_{i+1}, s_{i+1} \, |\, s_{i}, \, a_{i}), \, \forall i}
\end{align*}
where $p$ represents the environment's joint distribution of reward/transitions. This means that the expectation is with respect to the actions, states, and rewards you see under the policy $\pi$. If you want to make this more concrete, we can write out the expectation like this
\begin{align*} 
& \mathbb{E}_\pi\left[\sum_{k=0}^{\infty}\gamma^k r_{k+t+1} | s_t, a_t \right] \\
:=& \int_{(r_{t+1}, s_{t+1})}p(r_{t+1}, s_{t+1})\bigg[r_{t+1} + \int_{a_{t+1}}\pi(a_{t+1})\int_{(r_{t+2}, s_{t+2})}p(r_{t+2}, s_{t+2})\bigg[r_{t+2} + \ldots
\end{align*}
(Really, you should be integrating over dummy variables, and I've omitted the conditional expectations, e.g. $p(r_{t+1}, s_{t+1})$ should be $p(r_{t+1}, s_{t+1} | s_t, a_t)$). Of course you can just replace the integrals with sums if you want discrete actions and/or states. So you can see just writing $\mathbb{E}_\pi$ sweeps a lot of the notation and true meaning under the rug, and I assume that is the source of the confusion.
"
Neural Network Regression Experiment Going Wrong,"
I've been trying to get a simple regression experiment going with a neural network and I would like some help interpreting what is going wrong.
My goal is to see what level of regression accuracy I can achieve with a feed forward neural network. I have N pairs of inputs, x, and outputs, y.
As an example:
X is made up of serial integers starting at 0: e.x [0 1 2 3 ... N]
Y is made up of pairs of random floats between 0 and 1: e.x [[.263 .548] [.157 .014] [.988 .478] ... Nth [.356 .245]].

So my neural network's structure has 1 input neuron and 2 output neurons and some hidden layers in between whose properties are part of this experiment.
These are the questions I seek to answer:

Can this neural network with some configuration of hidden layers map these inputs and outputs with perfect accuracy?
If not, what is the best accuracy that can be achieved with a reasonably sized network?
Is there a limit on the value of N such that the accuracy of the mapping deteriorates past an accuracy threshold t for each value a and b of an output pair, [a, b], of +- z where z is fairly small? In other words can the prediction z stay withing a - t and a + t
and the same for b?

Here is my model and some relevant functions as it stands:
# Custom Loss
def absoluteDifference(y_true, y_pred):
  return abs(y_pred - y_true)

model = Sequential()
model.add(Dense(1, activation='relu', input_dim=1, kernel_initializer='he_uniform'))
model.add(Dense(10, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(magnitudes))

model.compile(loss=absoluteDifference, optimizer='adam', metrics=['accuracy'])

The results of my experiments are confusing. It always seems to bottom out at some accuracy threshold and never progress. When I compare the outputs of the network to the y value pairs they are either  wildly different than y or the predictions seem to be flattening at one value:
Here are some actual results:
Results 1: Flattening:
Y:     
[[0.2890625  0.43554688]
 [0.1171875  0.02734375]
 [0.44921875 0.11328125]
 [0.04296875 0.25585938]
 [0.4921875  0.42578125]
 [0.09960938 0.04101562]
 [0.265625   0.05273438]
 [0.421875   0.26757812]
 [0.40625    0.0859375 ]
 [0.25976562 0.1328125 ]]

Predictions:
[[0.27030337 0.11708295]
 [0.27030337 0.11708295]
 [0.27030337 0.11708295]
 [0.27030337 0.11708295]
 [0.27030337 0.11708295]
 [0.27030337 0.11708295]
 [0.27030337 0.11708295]
 [0.27030337 0.11708295]
 [0.27030337 0.11708295]
 [0.27030337 0.11708295]]

Results 2: Different:
Y:
[[0.3515625  0.47851562]
 [0.16796875 0.28320312]
 [0.140625   0.453125  ]
 [0.21679688 0.44726562]
 [0.21484375 0.0859375 ]
 [0.47265625 0.15429688]
 [0.37304688 0.30078125]
 [0.06054688 0.04492188]
 [0.49609375 0.41992188]
 [0.4453125  0.40820312]]

Predictions:
[[0.18319808 0.33377975]
 [0.16718353 0.33032405]
 [0.19876595 0.32554907]
 [0.23596771 0.3200497 ]
 [0.2726316  0.31566542]
 [0.30836326 0.31124872]
 [0.34609824 0.30569595]
 [0.38202912 0.30291694]
 [0.4184485  0.2967524 ]
 [0.45500547 0.29231113]]

I would have expected Y and Predictions to match after training. What am I doing wrong here?
","['neural-networks', 'tensorflow', 'keras']",
Is categorical encoding a type of word embedding?,"
Word embedding refers to the techniques in which a word is represented by a vector. There are also integer encoding and one-hot encoding, which I will collectively call categorical encoding.
I can see no fundamental difference between the categorical encoding and word embedding at a fundamental level. They may be different at an application level.
Is it true that categorical encoding is a type of word embedding? And are different names solely due to the task in which apply the technique?
","['comparison', 'terminology', 'word-embedding', 'categorical-data', 'one-hot-encoding']",
Text to image GANs and failure,"
My knowledge of GANs is relatively basic at the moment but I seem to remember reading somewhere that GANs that generate images from a text prompt, when they fail to understand some of the text/words render those text/words in the image itself instead of interpreting and rendering what they refer to - this is apparently a known bug or failure.
Can somebody confirm that this is true or false? And if possible provide a link to writing about it that will allow me to verify the details e.g. if its specific to a type of GAN, version etc. Many thanks in advance.
","['generative-adversarial-networks', 'image-generation']",
Which solutions are there to the problem of having too large activations before the softmax (or sigmoid) layer?,"
I'm trying to build a neural network (NN) for classification using only N-bit integers for both the activations and weights, then I will train it with some heuristic algorithm, based only on the NN evaluation.
Currently, I'm using a non-linear activation function for hidden units. Because of its probability interpretation, I am forced to use the softmax (or the sigmoid for 2-class case) for the output layer. However, because of the use of integers, the linear combination of the activations and weights can easily be too large, and this causes a problem to the exponential in the softmax evaluation.
Any solution?
","['neural-networks', 'convolutional-neural-networks', 'activation-functions', 'weights', 'softmax']","
First of all, check out this question. Generally, you don't need to apply softmax and using raw logits leads to better numerical stability.
The numerical issue that you are talking about is well known and dealt with the so-called logsumexp trick. This usually is already incorporated in standard NN libraries. For example keras
CategoricalCrossentropy loss can be configured to compute it from_logits.
"
"Deep Q-Learning ""catastrophic drop"" reasons?","
I am implementing some ""classical"" papers in Model Free RL like DQN, Double DQN, and Double DQN with Prioritized Replay.
Through the various models im running on CartPole-v1 using the same underlying NN, I am noticing all of the above 3 exhibit a sudden and severe drop in average reward (with a sudden and significant increase in loss) after achieving peak scores.
After reading online, I can see that this is a recognized problem but I cant find a suitable explanation. Things I have tried to mitigate:

adapt model architecture
tune hyperparams like LR, batch_size, loss function (MSE, Huber)

This problem persists, and I cannot seem to achieve any sustained peak performance.
Useful links I found:

What could be causing the drastic performance drop of the DQN model on the Pong environment?

Example:

till ~250 episodes in Double DQN with PR (with annealing beta), performance steady goes up in both increase in reward and decrease in loss
after that stage, the performance dips suddenly in both decreased average reward and increased loss as seen in output below

Episode: Mean Reward: Mean Loss: Mean Step
  200 : 173.075 : 0.030: 173.075
  400 : 193.690 : 0.011: 193.690
  600 : 168.735 : 0.015: 168.735
  800 : 135.110 : 0.015: 135.110
 1000 : 157.700 : 0.013: 157.700
 1200 :  99.335 : 0.013: 99.335
 1400 :  97.450 : 0.015: 97.450
 1600 : 102.030 : 0.012: 102.030
 1800 : 130.815 : 0.010: 130.815
 1999 :   89.76 : 0.013: 89.76

Questions:

what is the theoretical reasoning behind this? Does this fragile nature mean we cannot use the above mentioned 3 algorithms to solve CartPole-v1?
if not, what steps can help mitigate this? Could this be overfitting and what does this brittle nature indicate?
any references to follow up with regarding this ""catastrophic drop""?
I observe similar behavior in other environments as well, does this mean that the above mentioned 3 algorithms are insufficient?

Edit:
Taking from @devidduma's answer, I added time based LR decay to the DDQN+PRB model and kept everything else same. Here are the numbers, they look better than before in terms of the magnitude of the performance drop.
   10 : 037.27 : 0.5029 : 037.27
   20 : 121.40 : 0.0532 : 121.40
   30 : 139.80 : 0.0181 : 139.80
   40 : 157.40 : 0.0119 : 157.40
   50 : 225.10 : 0.0107 : 225.10 <- decay starts here, factor = 0.001
   60 : 227.90 : 0.0101 : 227.90
   70 : 227.00 : 0.0087 : 227.00
   80 : 154.30 : 0.0064 : 154.30
   90 : 126.90 : 0.0054 : 126.90
   99 : 154.78 : 0.0057 : 154.78

Edit:

after further testing, pytorch's ReduceLROnPlateau seems to be working best with patience=0 param.

","['q-learning', 'deep-rl', 'gym', 'double-q-learning']","
This is a case of overfitting the Q function leading to compounding errors when selecting actions.

You have been training your policy for too long on the same data distribution.
Overfitting Q functions will then lead to data distribution mismatches more often in action selection and compounding errors will happen earlier than before.

You should probably train until 400 up to 600 episodes and then stop training the policy. Consider the following slide on compounding errors:

Whenever a wrong action is selected, because of overfitted Q value function, the agent can not generalize well on how to recover from that mistake. Eventually, compounding errors increase quadratically in time.

It will only get worse for your agent once the wrong action is picked.
In Temporal Difference learning methods like TD-0, SARSA or Q learning, finite-state and finite-action MDP's converge to the optimal action-value if the following two conditions hold:

The sequence of policies $\pi$ is Greedy in the Limit of Exploration (GLIE)
The learning rates $\alpha_t$ satisfy the Robbins-Munro sequence such that:


$ \sum^{\infty}_{t=1} \alpha_t = \infty $
$ \sum^{\infty}_{t=1} \alpha^2_t < \infty $

You can infer that by any means we should use a decaying learning rate in order to satisfy the Robbins-Munro sequence. There are three types of decaying learning rates:

time-based decay
step-decay
exponential decay

In the limit of exploration, they guarantee convergence to the optimal policy, for any Temporal Difference learning based algorithm. I would suggest you use a time-based decaying learning rate, which is the default choice in Keras when you set the decay parameter.
$ lr = 1 / (1 + decay\_factor * iteration) $
One iteration here means one epoch. You probably train your neural network every step taken by the agent, so one iteration means one epoch, one step taken by the agent.
In Keras you can set the decay like:
model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate, decay=decay_rate))
I would suggest a value of $0.001$, that should be a good starting point.
"
Initial Input $h_0$ for RNN and updation of weights,"
Consider an input to RNN $ x = \{x_i\}_{1}^{n}$. Assume that the length of each input $x_i$ is k.
Now, consider the following diagram from p5  of this pdf

My doubts are:

What should I pass as $h_0$? is it a zero vector?

Does RNN updates its weight matrices $U, W, V$ after each token of input $x_i$ ? Or updates after passing all tokens of a particular input $x$?


","['training', 'recurrent-neural-networks']","
I am answering this question based on classical backpropagation through time (BPTT) only.
What should I pass as $h_0$? is it a zero vector?
Yes. We know that $h_{t-1}$ is the footprint of the first $t-1$ tokens of the input sequence. For the first time, we need to pass $h_0$ along with the token $x_1$. Since, footprint is not formed yet, we need to pass zero vector only. Check Figure 9.4 of your pdf.

Does RNN updates its weight matrices $U,W,V$ after each token of input $x_i$ ?
No, RNN does not update its weight matrices $U,W,V$ after each token of input $x_i$. RNN forward pass generates output sequence $y = \{y_i\}_{1}^{n}$  for given input $x$ and the various weight matrices $U, V, W$ are shared across all timestamps of the forward pass.

Or updates after passing all tokens of a particular input x?
Yes, all the weight matrices get updated only after generating the complete $y$.
But since the classical BPTT may take much time and the gradients may vanish gradually, truncated backpropagation through time is generally used. And the updation of weights takes place after generating the partial output only.
"
"How do we get the value of this state of an MDP, at time-step $h-2$, using dynamic programming?","
I am trying to understand the problem below, represented as an MDP with four states (PU, PF, RU, and RF) and two actions (AS).

Let's consider V(RF), the value of the state RF. At time-step $h$, V(RF) = 10. When we go to the previous time-step $h-1$, V(RF) increases to 19.
Why is the value of RF increasing backward, i.e. at time-step $h$, which is the last step, it's 10, but in $h-1$ it's 19?
Also, when I apply the Bellman equation, I am not getting the value of V(RF) at time-step $h-2$, which is 25.08, according to the table.
Below is my solution which I am applying on V(RF):
Lets suppose for RF, I know that
Vh (RF) = max {R(RF,A), R(RF,S)}
        = max ({10,10}
Vh (RF) = 10

    **for h-1**
    Vh-1 (RF) = max R(RF,act) + gamma E (summation state) P(State|RF,act) Vh(State)
              = max {10+0.9(1*0), 10+0.9(0.5(10)+0.5(10))}
              = max (10,19)

    **for h-2**
    Vh-2(RF) = max R(RF,act) + gamma E(summation state) P(State|RF,act) Vh(State h-1)
             = max {19+0.9(1*0), 19+0.9(0.5(10)+0.5(10))}
             = 28.0

So, in the above scenario, the reward is 0.9, but I am not sure how we get the third result in V(RF) as 25.08. Where are we using this last part Vh(State) from the equation?
","['markov-decision-process', 'reward-functions', 'bellman-equations', 'policy-iteration', 'dynamic-programming']",
Which AI techniques are there that combine multiple models to make sense of data at different stages?,"
I have been working to design a system that uses multiple machine learning models to make sense of data that is dynamically webscraped. Each AI would handle a specific task, for example:
An AI model would identify text in an image, then attempt to create plain text of what it might be. Once the text is extracted, it would be passed in a stored variable to an AI that can read the text to determine if it is a US city/state.
I tried to look into if others have done this, but didn't find much on it relating to what I was looking for. Does anyone know if there are potential issues with this? Logically, it looks good to me, but I figured I'd ask.
If anyone can put me in the right direction for reading material or further information, I would appreciate it.
","['natural-language-processing', 'reference-request', 'optical-character-recognition', 'named-entity-recognition', 'text-detection']",
Is it possible to overfit a model on infinite amounts of data?,"
This is a theoretical question. Is it possible to overfit a model on infinite amounts of data?
Let me clarify there are no duplicates.
Say, we have a generator function that produces data, with the correct classification/regression value, and we can generate infinite amounts of valid data. How long does it take for the model to overfit?
This question arose because I'm training an RNN model for fake news classification, and MSE loss is almost always 0.000, only 25% of the training data.
Will it be possible to overfit with one epoch of training on the infinite data generator?
(I'm thinking what will happen is the model will either get perfect, or sync into the generator's non-perfect randomness, and learn nothing)
","['machine-learning', 'training', 'recurrent-neural-networks', 'overfitting', 'computational-learning-theory']",
Why are BERT embeddings interpreted as representations of the corresponding words?,"
It's often assumed in literature that BERT embeddings are contextual representations of the corresponding word. That is, if the 5th word is ""cold"", then the 5th BERT embedding is a representation of that word, using context to disambiguate the word (e.g. determine whether it's to do with the illness or temperature).
However, because of the self-attention encoder layers, this embedding can in theory incorporate information from any of the other words in the text. BERT is trained using masked language modelling (MLM), which would encourage each embedding to learn enough to predict the corresponding word. But why wouldn't it contain additional information from other words? In other words, is there any reason to believe the BERT embeddings for different words contain well-separated information?
","['natural-language-processing', 'word-embedding', 'bert', 'embeddings']","
While an input with $n$ tokens generates an output with $n$ vectors, there is a lot of cross mixing of information. One word, or even one word sense, may have many different representations.  The embeddings are very sensitive to other words in the sentence, even when the sense of the word does not change. To measure how the embeddings change, we could use the standard deviation of the variations in the vector entries. For example, the difference between the word vectors for run in ""Run fast"" and ""Run fast."" (notice the period) is 0.24, which is only about half the difference among random uses of the word run.
The authors of the BERT paper explored the possibility of generating contextualized word embeddings using BERT. Table 7 in the BERT paper explains different ways they tried for extracting word representations that would work well in a named entity extraction task. They concluded that the weighted sum of the last four hidden layers provides good contextualized word representations. However, there are still many other choices to make when it comes to developing contextualized word embeddings, such as how to go from the wordpieces that BERT uses to a single word vector. If you're looking to learn more about BERT-related topics, I found the paper on BERTology to be a great starting point (section 4.1 discusses word embeddings).
"
What are pros and cons of using a multi-head neural network versus a single neural network for multi-label classification?,"
I haven't been able to find a good discussion specifically comparing the two (only one describing a classification and regression problem). I am training a classifier to learn both age and gender based on genomic data. Every sample has a known age and known gender (20 classes in total).
Currently, I am using a single neural network with a sigmoid activation in the last layer with a binary_crossentropy loss. This works fine. However, I also see people using multi-head neural networks where, for example, a set of shared layers would split in to two either additional dense layers or in to two final layers for classification – each with an independent loss (in my case likely a categorical_ce).
What I am unsure of, though, are the advantages and disadvantages between the two (maybe advantages and disadvantages are not the right words to use – actual differences between the two might be more appropriate and when one might use one of those over the other)?
I want to be able to calculate the usual metrics – TP, FP, etc. after training – presumably it would be easier with two heads at the end of the network, as you can work with two independent sets of predictions to calculate these?
","['binary-classification', 'multi-label-classification', 'binary-crossentropy']",
"In the field of Deep Learning research, what considerations do researchers take into account when inventing new neural network models?","
I am not a researcher, but I am curious to know what considerations are relevant to take into account during research for the invention of a new neural network model, and what relevant knowledge researchers typically possess in the area.
And an accompanying question: is a background in neuroscience relevant to such an investigation?
","['neural-networks', 'deep-learning', 'research']","
In my experience researchers typically base their architectures on previously identified successful architectures and principles. That is to say published methods that have been successful in practice on similar tasks to the current one. This can be followed back to very early networks like the Perceptron which took a lot of inspiration from existing successful mathematical techniques.
This is not to say that researchers do not draw from neuroscience or from broader knowledge to influence their decisions. I believe most research is guided by more abstract intuitions derived from broader experience. Some researchers are very strongly influenced by neuroscience and I understand that some try to replicate behaviours seen in the brain. The similar structure and functions of the brain will surely make knowledge of it useful. Some major advances may have been motivated by direct analogy to the brain, although I am not aware if that is the case. But, on balance, having zero knowledge of neuroscience is not an impediment to undertaking successful research into neural architectures. Having zero knowledge of existing successful techniques is a far greater impediment.
Having a good level of mathematical understanding is also very useful. I will take perhaps a slight risk of going beyond my knowledge and suggest that university-level knowledge of mathematics is also more beneficial than knowledge of neuroscience. I think that is true for research so far, maybe it will change in the future. All these comments are based only on my personal experience and not on any formal research. Also, note my bias: I have minimal knowledge of neuroscience myself.
"
"What is meant by Hinton when he refers to ""Part-Whole Hierarchies"" in his GLOM framework","
I was recently reading Hinton's GLOM idea How to represent part-whole hierarchies in a neural network, and I am simply unsure about what exactly he means when he says parsing images into ""part-whole hierarchies"".
Moreover, wouldn't semantic segmentation ""parse"" the parts from the whole image? So what is different here?
","['neural-networks', 'terminology', 'papers', 'semantic-segmentation']",
Is there a performace benefits using VAE-GAN instead of just GAN?,"
I have read that when using VAE-GANs, first what happens is the VAE's encoder encodes some image to another encoded image, which from GAN's point of view is considered a noise, and then the GAN part generates another image from that noise which from VAE's point of view is just an encoded image.
Is that encoded image better suited for GAN to generate better images or not?
The problem which bugs me is that there are not that many articles about VAE-GANs, especially in the last 2 years.
As a side question, does that mean that VAE-GANs do not have any significant performance benefits than just simple GAN?
","['deep-learning', 'generative-adversarial-networks', 'variational-autoencoder']","
In my experience, it's not a matter of performance benefits; Variational Auto-Encoder GANs are much more useful if you want to have ""knobs"" to turn to influence the generated output.  Since you have a latent layer that represents possibly the mean and the distribution of the data, you can tune to different ""positions"" in that latent space to influence the output of the GAN.
Without this, the generated output is more difficult to predict and/or you will end up with some outputs that are great and others that are completely meaningless.
"
How do sparse word embeddings fail to capture synonymy?,"
While reading some explanations of why dense word embeddings work better than sparse word embeddings, the following statement has been given in the chapter Vector Semantics and Embeddings, showing a drawback of sparse word embeddings.

Dense vectors may also do a better job of capturing synonymy. For example, in a sparse vector representation, dimensions for synonyms like car and automobile dimension are distinct and unrelated; sparse vectors may thus fail to capture the similarity between a word with car as a neighbor and a word with automobile as a neighbor.

It says that the dimensions of synonyms may be unrelated and distinct. I am facing difficulty in understanding it.
Can anyone provide me a simple example to understand it by taking some simple dimensions which are unrelated and distinct?
You can consider either documents or (context) words as dimensions for the example.
","['natural-language-processing', 'word-embedding', 'sparse-word-embedding']",
Split on dataset with some shared features?,"
I have a dataset with financial stock data, some of the features are shared, for example daily gold prices, while the stock price for each individual stock is different, the gold price would be the same for everybody that day.
When I split 80/10/10 randomly, it's ""cheating"" and while the result accuracy is great the actual real world live result is bad.
When I split sequentially, ie first 8 years of data in training, next year in validation, last year in testing.  The result accuracy is bad, and live testing is also bad.
What I want to ask is, should I do random split between just training and validation on first 9 years of data, then do testing on last year of data separately?
OR is sequentially as good as it's gonna get and I simply can't predict the future?
",['datasets'],
Forecasting of spatio-temporal event data,"
I’m currently working on my dissertation which is centred around forecasting social conflict events. I’m using data from GDELT (Global Database of Events, Tone, and Language) to develop my forecasting model. For the sake of conveying the problem and limiting the length of this post, I have simplified the features used in my investigation. These can be summarised as follows:
(please feel free to skip the feature description to the end of this post indicated by ""The Question"" marked in bold if TL:DR)
Temporal Attribute:

FractionDate: Date of event [numerical].

Actor Attributes:

Actor1Type: The type of actor who performed the action [factor]. (e.g. Government, Rebels, Civilians, etc.)
Actor2Type: The type of actor who received the action [factor]. (e.g. Government, Rebels, Civilians, etc.)

Event Action Attributes:

EventClass: Verbal cooperation, material cooperation, verbal conflict, and material conflict encoded as 1,2,3,4 respectively [factor].
EventImpact: A numeric score from [-10,10] capturing the potential impact that type of event may have on the stability of a country [numerical].

Spatial Attributes:

ActionGeoLong: Longitude where the action took place [numerical].
ActionGeoLat: Latitude where the action took place [numerical].

The database is updated on a daily schedule and is roughly 50 MB on average for single days data. The data is filtered to include only events that took place in a single country, which decreases the file size to about 1-2 MB. These events are then aggregated on a weekly basis.
One notable modeling method to predict spatio-temporal data is by means of ConvLSTM models. These models have been successfully implemented in, for example, predicting precipitation or traffic flow. So the strategy that I have so far is:

Aggregate the spatial data to generate weekly geographical heatmaps, showing the intensity (for the sake of simplicity, can be thought as a weighted product of frequency and EventImpact) of events for each EventClass. That is, you are left with a time series of 4 heatmaps similar to the ones below.



Aggregate the actor data to generate weekly actor ""Interaction"" matrices [I]. These matrices show the intensity (yet again, can be thought as a weighted product of frequency and Eventimpact) of interaction between each actor for each EventClass. Actor 1 (performer) are on the rows and Actor 2 (receiver) are on the columns, therefore, [I]_{n,m} would mean the intensity of Actor n doing something to Actor m. (Note that these matrices won't be symmetrical, the intensity of actor n doing something to actor m, is different from actor m doing something to actor n). Then you are left with a time series of 4 matrices similar to the ones below:


The two above (geographical heatmaps and interaction matrices) will be the ""input"" to my model, and it should be able to predict the next weeks heatmap and interaction matrix given the history of events. In theory I should be able to construct ConvLSTM model for the geographical heatmaps or the interaction matrices separately. Therefore, the problem I am faced with is building a sort of ensemble of ConvLSTM which is able to learn from both input sources simultaneously.
The Question:
Is there a way to construct a ConvLSTM that can learn from two different ""types"" of input tensors? The first being a sequence of geographical heatmaps (with 4 channels), and the second being a matrix (also with 4 channels). If so, how would you implement this in Keras? It is very important that the model considers both sources in order to learn underlying mechanisms of the system. An example of the model Input and Output is provided below.

Thank you for taking the time to read. I would appreciate additional opinion or other applicable modeling methods very much.
","['convolutional-neural-networks', 'long-short-term-memory', 'time-series']",
Weird KL divergence behaviour,"
I'm training a complex model for motion prediction using a VAE, however the KL divergence has a very strange behavior.
A scheleton of the network is the following:

At the end my network compute the MSE loss of the trajectory and the Kullback Leibler loss (with gaussian prior with mean 0 and std equal to 1) given as:
kld_loss = -0.5*torch.sum(1 + sigma - mu.pow(2) - sigma.exp())

Any idea of the possible causes? Do you need further details?
","['deep-learning', 'autoencoders', 'variational-autoencoder']",
Fitting a Gaussian distribution into another distribution,"
Assume we have two vectors, containing random samples (maybe audio data?). Their distribution can be approximated to a normal distribution, so we can calculate their mean and standard deviation.

I am looking for a way to ""fit"" the second vector's samples, in a way that their mean and standard deviation correspond to the first vector's mean and standard deviation.

Also, I am looking for a way to do this by ""moving the second vector's samples the least possible"". This is because, an easy way to solve this problem could be to replace the second vector's data, with random samples that fit the first vector's parameters. This solution is easy, but not interesting.


Questions

Is this kind of problem correlated with machine learning in general? If yes ""how""?

Is there a way to perform this kind of operation with some kind of neural network? If yes, how could it be modelled?


","['machine-learning', 'definitions', 'algorithm-request']",
Why would an auto-encoder produce latent vectors with many zeros?,"
My autoencoder give latent vectors with many zeroes components like:
[3.0796502  2.9488854  0.9002177  0.         0.         0.
 0.         0.         0.         1.0181859  0.         0.68507403
 0.         0.6128702  0.         0.         0.         0.
 0.         1.763725   0.         0.         0.         1.0947669
 0.         1.5330162  0.         0.         0.         0.
 0.         1.7434856  0.         0.         1.8942142  2.0379465
 0.         0.         1.2500542  0.         0.         0.
 0.         0.         0.         2.7917862  0.         0.
 2.2105153  0.         0.         0.         1.5798858  0.
 0.         3.7405093  0.8692952  0.01490922 0.         0.
 2.8320081  0.         0.         0.        ]

Certain components are always zero, another not always. Why might this be happening? How can I figure this out?
","['neural-networks', 'autoencoders']",
Are spectral approaches to Graph Neural Networks still considered?,"
I've been reading several papers and reviews about Graph Neural Networks, and I still feel a bit confused about the difference between the two approaches, and also if the spatial approaches have somehow 'overcome' spectral ones. I will add some of my understanding:
Graph Neural Networks take inspiration from the convolution operation between two signals in a Euclidean domain, as a way to combine the features on the nodes as it happens for Convolutional Neural Networks. To do this, a notion of convolution has been required for the graph domain. Given $\bf{x},\bf{y} \in \mathbb{R}^N$ two signals, then
$$\bf{x} \, \, *_G \, \, \bf{y} := U(U^Tx \, \odot \, U^Tx)$$
namely, we perform convolution on the graph domain and then take everything back using the inverse Fourier Transform. If we choose a filter $\bf g_\theta = diag(\theta_1, \dots, \theta_N)$ parametrized by some $\theta \in \mathbb{R}^N$ then the convolution becomes
$$\bf{x} \, *_G \bf{g_\theta} = Ug_\theta U^T$$
The approach above presents several limitations in terms of non-localization of filters (which depend on the entire graph) and scalability issues if presence of perturbations. So the authors of ChebNet proposed the following approximation:
$$\bf{x} \, *_G \bf{g_\theta} = \sum_{i=0}^K \theta_iT_i(\tilde{L})x$$
where $\tilde{L} = 2L/\lambda_{max}-I_n$, $L$ is the Laplacian and $T_i$ are Chebyshev polynomials.
Now the crucial step is that Kipf et Al. (2017) have bridged the gap between spectral and spatial approaches by proposing a first order approximation of the above equation (assuming $\lambda_{max} =2$ and $\theta = \theta_0 = -\theta_1$):
$$\bf{x} \, *_G \bf{g_\theta} = \theta(I_n+ D^{-1/2}AD^{-1/2})$$
Now, from what I've read so far, it seems that from now on several improvements have been made on the spatial approach which defines convolutions on top of node's graph neighbourhood.
The question is, does it still make sense to focus on spectral approaches?
","['geometric-deep-learning', 'graph-neural-networks', 'spectral-analysis']",
What are the most relevant resources that define the face detection problem formally?,"
I am new to AI, and I am a bit lost about finding the relevant materials that define the face detection problem formally/mathematically.
Can anyone help me formally define face detection, or at least point me towards papers that define it formally?
","['computer-vision', 'reference-request', 'face-detection']",
Has positional encoding been used in convolutional layers?,"
Positional encoding (PE) is an essential part of the self-attention layers in the transformer architectures since without adding it in some way (fixed of learnable) to the input embeddings model has ultimately no notion of order and is permutationally equivariant and the given token attends to the far separate and local tokens identically.
The convolution operation with a local filter, say of size $3, 5$ for 1D convolutions or $3 \times 3, 5 \times 5$ for 2D convolutions, has some notion of locality by construction. However, within this neighborhood, all pixels are treated in the same way.
However, it may be the case, that it is important, that the given pixel is the central for the application of this filter, whereas the other is close to the boundary. For small filters
$3 \times 3$ it is probably not an issue, but for the larger - injection of PE can be useful.
Has this question been investigated in the literature? Are there any architectures with PE + convolutions?
","['convolutional-neural-networks', 'reference-request', 'transformer', 'convolution', 'positional-encoding']",
Visualizing encoder-attention after ResNet in terms of ResNet input,"
I have a transform-encoder only architecture, which has the following structure:
      Input
        |
        v
     ResNet(-50)
        |
        v
fully-connected (on embedding dimension)
        |
        v
positional-encoding
        |
        v
transformer encoder
        |
        v
Linear layer to alphabet.

I am trying to visualize the self-attention of the encoder layer to check how each input of the attention attends other inputs. (E.g. https://github.com/jessevig/bertviz)
Where I encounter difficulty is in how I can visualize these activations in terms of the original input of the ResNet and not its output, in order to make my model visually interpretable.
Do you have any ideas or suggestions?
","['pytorch', 'transformer', 'attention', 'data-visualization']",
What does 'downsampling' and 'upsampling' mean in coarse-to-fine segmentation?,"
The paper here in section 2.1 Coarse-to-fine prediction:

To increase the field of view presented to the CNN and reduce the
redundancy among neighboring voxels, each image is downsampled by a factor of 2. The resulting prediction maps are then resampled back to the original resolution using nearest-neighbor interpolation.

What does it actually mean to downsample by a factor of 2?
If I have an image size of $256 \times 256 \times 170$, and if I downsample it by a factor of 2, then will it result in an image of size $128 \times 128 \times 85$?
Similarly, would upsampling/resampling be the opposite interpolation method, getting back to the original size of $256 \times 256 \times 170$?
","['image-segmentation', 'downsampling', 'upsampling']","

What is actually the downsampling of 2 mean?
If I have an image size of 256x256x170 and if I downsample it by a factor of 2, it will result in an image of size 128x128x85?

Yes, that is correct.

Similarly, upsampling or resampling is the opposite interpolation method to original size 256x256x170?

Yes, correct result again. Resampling is a more general term and describes resizing an image (or other grid-based signal) using an automatic rule which could be enlarging or reducing the size.
What resampling does not mean in digital signal processing is going back to the original source and taking a second sample. That might be a reasonable interpretation of the word when coming across it for the first time, but it is not correct here.
Just saying that a signal is resampled does not give enough information to say exactly what was done numerically to the signal, because it does not specify the rule of what happens with the lost (in case of downsampling) or missing (in case of upsampling) information. There are a few different ways to perform the resize, and different kinds of interpolation. It looks like the paper does give more information here, but you may need to read more or inspect the authors' code to be certain - for instance ""nearest neighbour interpolation"" probably involves looking at values of the six adjacent voxels to each new one you create, and taking the mean value of the ones that exist in the original grid (some will not, they will be other new cells), but there are a few different ways to manage that process.
"
"What is an ""input embedding"" in the context of NLP?","
When reading about NLP, I saw it said that ""input embeddings"" are a main element of encoder-decoder learning frameworks for sequence modelling. What is an ""input embedding"" in the context of NLP?
","['natural-language-processing', 'embeddings']",
"What is meant by ""stable training"" of a deep learning model?","
I have read it said that the ""stable training"" of a deep learning model is important. What is meant by ""stable training"" of a deep learning model?
","['deep-learning', 'training', 'deep-neural-networks', 'computational-learning-theory', 'stability']",
Why don't we use this intialization with SGD rather than random?,"
Suppose I have a loss function as a polynomial with its variables being the weights of a network I wish to tune. Now, we want to find the minima of the loss function - so basically argmin.
In ML, we simple use SGD with any initialization. But consider this: we take a few $n$ random combinations of weights and plot a visual graph (not to be confused with computation graph) where we find the local minimas of the graph (basically any point surrounded by larger point values would be minima). We store the weights (value of the variables in the polynomial) used for each point in the graph in a data structure.
Theoretically, if $n$ is big enough to be computationally efficient while being quite descriptive, we can simply take the weights of a random minima as initialization to the network and then perform SGD on it to converge to a global minima (hopefully).
This method would be quite faster since the initialization is better, and we don't need to compute for large values of $n$ - simply having a decent enough estimate. SGD would finally be used with a low learning rate to give the final push and we can be done with easier and faster.

So why don't we do this instead of having random initialization? is there theoretical basis on which this can't work?

","['ai-basics', 'convergence', 'stochastic-gradient-descent', 'weights-initialization']",
How to formulate discounted return in cartpole?,"
I am trying to formulate a problem that aims to prolong the lifetime of the simulation, the same as the Cartpole problem. I aware that there are two types of return:

finite horizon undiscounted return (used for episodic problems)

$G = \sum_{t=0}^T R_t$

infinite horizon discounted return (used for non-episodic problems).

$G = \sum_{t=0}^\infty \gamma^t R_t$
However, I'm confusing that ""Is Cartpole episodic task?"". Ideally, the simulation lasts forever.  This is my final objective (prolonging the lifetime). But it still has some termination states. Should I introduce the termination state and use it with a discounted return like:
$G = \sum_{t=0}^T \gamma^t R_t$
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'return', 'finite-markov-decision-process']",
Is an embedding a representation of a word or its meaning?,"
What does the term ""embedding"" actually mean?
An embedding is a vector, but is that vector a representation of a word or its meaning? Literature loosely uses the word for both purposes. Which one is actually correct?
Or is there anything like: A word is its meaning itself?
","['terminology', 'word-embedding', 'embeddings']","
Although we have had multiple similar questions (see here, here and here) and it seems to me that you focused on word embeddings (probably because you were not aware of the application of embeddings to other contexts), in addition to what is stated in the other answer, it's important to note that the concept of an embedding does not just apply to words. For example, there are also code embeddings (see e.g. code2vec) and graph embeddings (see e.g. this), and there are probably other examples. The linked posts contain answers that explain what an embedding and embedding space generally are, so you may want to read them.
"
Number of classes vs number of parameters/layers?,"
How to estimate the number of parameters in CNN for object detection?
I know that there are some well-known architectures that was trained on a lot of data (AlexNet, ResNet, VGG, GoogleLeNet). But they were trained for example for classifying 1000 classes. Or they were used as backbones in the algorithms like YOLO to localize 80 classes of objects.
Now let's say that I want to classify only 5 classes. Or I want to perform object detection and I am interested only in cars and people. I want to detect/classify this small number of objects. So the network must learn only the features of cars and people (instead of learning the features of hundreds of objects).
So my intuition is that I can use smaller network with fewer number of parameters. Correct me if I am wrong. And my second intuition is that the number of layers should not have a big impact. I mean, you shouldn't decrease the number of layers only because you have less classes. Because the network learns more and more sophisticated features in deeper layers. And it wouldn't be able to detect advanced features of cars (or other objects) if you don't have enough layers.
Recently I tried to use CenterNet https://arxiv.org/abs/1904.07850 to detect digits on 64x64 grayscale images and I achieved success having quite simple 900k convnet. Then I tried to use slightly modified GoogLeNet to detect cars using 224x224, 448x448, and 512x512 images. I trained it on 450 images. After a lot of trials and errors I still cannot train a good model. GoogLeNet is quite small network in compare to other well-known architectures, but I heard that it's very good. It was carefully designed to be very powerful despite being small (7M parameters).
So to be clear. My question is about the dependencies between the number of classes and the number of layers and parameters.
","['convolutional-neural-networks', 'image-recognition', 'object-detection', 'deep-neural-networks']",
Optimal episode length in reinforcement learning,"
I have a custom environment for stock trading where an episode can be as long as 2000-3000 steps. I've run several experiments with td3 and sac algorithms, average reward per episode flattens after few episodes. I believe average reward per episode should further improve, so I thought whether my training episode is too long. What is the recommended upper limit on the episode length?
","['reinforcement-learning', 'deep-rl', 'hyper-parameters', 'soft-actor-critic', 'td3']",
"Why is the validation loss less than the training loss, and what can be said about the effect of the learning rate?","
I have the following results I am trying to make sense of. I have attached the loss curves here for reference.

As you can see, the first issue is that the validation loss is lower than the training loss. I think this is due to using a pre-trained model with a high dropout rate (please correct me if I am wrong here).

As one can see, the mean_auc score is increasing consistently, and so it seems that the network is indeed learning something and the validation loss is also better behaved relatively.

The training loss is what bugs me a lot. It is not at all consistent and varies a lot. This is a naive question, but is this graph giving me any sort of information about the learning rate, etc, or am I in a situation wherein everything is incorrect essentially?


Any response would be really appreciated.

","['deep-learning', 'cross-validation', 'generalization', 'learning-rate', 'multi-label-classification']",
What are the best hyper-parameters to tune in reinforcement learning?,"
Obviously, this is somewhat subjective, but what hyper-parameters typically have the most significant impact on an RL agent's ability to learn? For example, the replay buffer size, learning rate, entropy coefficient, etc.
For example, in ""normal"" ML, the batch size and learning rate are typically the main hyper-parameters that get optimised first.
Specifically, I am using PPO, but this can probably be applied to a lot of other RL algorithms too.
","['reinforcement-learning', 'deep-rl', 'hyperparameter-optimization', 'hyper-parameters', 'proximal-policy-optimization']","
Personally, I would choose the following two as the most important:

epsilon: When using an epsilon-greedy policy, epsilon determines how often the agent should explore and how often it should exploit. Balancing exploration and exploitation is crucial for the success of the learning agent. Too little exploration might not teach anything to the agent and too much exploration might just waste your time.
learning rate: The learning rate determines how fast do you learn from new states of experience. A learning rate that is too high might not be good in cases when the environment has many states with high probabilities of negative rewards, i.e. many penalizations. This might make your agent move back and forth in the same place in order to avoid getting penalized. Also, a learning rate that is too low might make your agent learn very slowly and depending on your epsilon, the agent might enter a phase of exploitation with very little knowledge of an optimal policy.

"
Can unsupervised models learn something from cat vocalizations?,"
I love cats, and over the years have noticed that they have recurrent patterns of vocalizations. For example, upon seeing a bird, a cat may start chittering, but the same cat would never chitter at humans. Then there are complex vocalizations, like meow-wow, which I have observed across multiple cats on different continents. At the same time, we have birds and monkeys which have vocabularies of up to 300 (?) words. It seems like cats are communicating something, but humans may be too tone-deaf to understand that.
It seems to me like the task of understanding what a cat is trying to communicate to humans is suitable for some kind of machine learning process.
My question is: has any of these unsupervised models been applied to cat vocalizations? In other words, if a model can draw or generate text, can it generate cat meows? How close are we to understanding what cats are meowing about and translating it into English?
I remember that some work has been done with trying to decode dolphin vocalizations, but as you can imagine, that requires specialized equipment, while a cat model can be tested in the real world with simpler equipment.
","['natural-language-processing', 'unsupervised-learning', 'natural-language-understanding', 'machine-translation', 'natural-language-generation']",
Where do the feature extraction and representation learning differ?,"
Feature selection is a process of selecting a subset of features that contribute the most.
Feature extraction allows getting new features that are not actually present in the given set of features.
Representation learning is the process of learning a new representation that contributes the most.
I can see no difference between feature extraction and representation learning.
Is feature extraction the same as representation learning? If not, where do they differ? Do they differ at the application level only?
","['machine-learning', 'comparison', 'terminology', 'feature-extraction', 'representation-learning']",
Why did Distributional Q Learning go out of popularity?,"
I read some papers (for example, this) and blogs that spoke about the advantages of distributional Q learning. However, it no longer seems to come up in literature. Did it have any shortcomings that led to its failure? If yes, can someone can talk about it here?
","['reinforcement-learning', 'reference-request', 'deep-rl']",
Does elitism cause premature convergence in genetic algorithms?,"
I have a genetic algorithm which is working fairly well. It's got all the standard operators, including initial random population, crossover ratio, mutation rate, degree of mutation, etc.
This works fairly well, and I have tuned and optimized the hyperparameters as much as possible, including some adaptive variants. The one thing that ruins the results EVERY TIME is when I implement elitism. It does not seem to matter if I include 1 elite, or a certain percentage of elites. I have tried 1% through 10%, tried a decay variable so that elites would only survive a certain number of generations, and numerous other tactics. Every single time I add elitism, the solution gets stuck in a local optimum so deeply that there is no escape.
Most of the literature recommends to have elites, but the elites ruin my GA every single time, without fail.
Ideas?
","['machine-learning', 'genetic-algorithms', 'evolutionary-algorithms', 'convergence', 'elitism']","
There are many ideas to escape from local optima in GA. One solution is selecting the population for the next iteration based on the probability that is defined based on the individual score. In that case, you have a chance to select a bad score individual to escape from the local optima.
Another efficient solution is playing with the mutation rate to get rid of local optima. In that way, you can increase the rate smoothly, to find a proper rate.
"
How do I select the number of neurons for each layer in an auto-encoder for dimensionality reduction?,"
I am trying to apply an auto-encoder for dimensionality reduction. I wonder how it will be applied on a large dataset.
I have tried this code below. I have total of 8 features in my data and I want to reduce it to 3.
from keras.models import Model
from keras.layers import Input, Dense
from keras import regularizers
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
data = pd.read_csv('C:/user/python/HR.csv')
columns_names=data.columns.tolist()
print(""Columns names:"", columns_names)
print(data.shape)
data.head()
print(data.dtypes)
# Normalise
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data)
# Fixed dimensions
input_dim = data.shape[1]  # 8
encoding_dim = 3
# Number of neurons in each Layer [8, 6, 4, 3, ...] of encoders
input_layer = Input(shape=(input_dim, ))
encoder_layer_1 = Dense(6, activation=""tanh"", activity_regularizer=regularizers.l1(10e-5))(input_layer)
encoder_layer_2 = Dense(4, activation=""tanh"")(encoder_layer_1)
encoder_layer_3 = Dense(encoding_dim, activation=""tanh"")(encoder_layer_2)
# Crear encoder model
encoder = Model(inputs=input_layer, outputs=encoder_layer_3)
# Use the model to predict the factors which sum up the information of interest rates.
encoded_data = pd.DataFrame(encoder.predict(data_scaled))
encoded_data.columns = ['factor_1', 'factor_2', 'factor_3']

I have read in this tutorial that, if you have 8 features and your aim is to get 3 components, in order to set up a relationship with PCA, we need to create four layers of 8 (the original amount of series), 6, 4, and 3 (the number of components we are looking for) neurons, respectively. How does it make sense?
Now, let's say that I initially have 500 features and I want to reduce them to 20, what should I do?
According to my understanding, I need to reduce the number of neurons from the first to the last layer. So,

in the first layer, I have 500 neurons
in the second layer, it will be 250
in the third layer, it will be 130
in the fourth layer, it will be 60
in the fifth layer, it will be 20

Is this correct, and why?
And can I get matrix-like PCA at the end to see the components I got?
","['python', 'autoencoders', 'principal-component-analysis', 'dimensionality-reduction']",
How to perform multi-class text classification with a dataset of 80 documents?,"
I have a training dataset of 80 text documents with an average number of characters in each document of 25000 and 210 unique tags.
How can I perform multi-class text classification with such a small dataset, without using the pre-trained model? If it cannot be done without a pre-trained model, then which pre-trained model should I use?
","['machine-learning', 'natural-language-processing', 'transfer-learning', 'pretrained-models', 'multiclass-classification']","
For pretrained models in NLP, look at BERT and RoBERTa. If you can find a language model trained on your data's superset on Huggingface, then, use that pretrained model.
In order to multiclass classification, since your data is less, look at augmentations in NLP (most notably, backtranslation amongst others). Use focal loss (to handle class imbalance).
Since, you are going to finetune use small learning, 1e-5. But, you will be adding your own layers also, so keep 1e-5 for the pretrained model and 1e-3 for the new layer you put.
"
DQN learns to always choose the same action for all states,"
I have created an RL model that uses QBased policy with a neural network for estimating Q values.
My action space is of 27 actions, where each action is a 3 tuple where each value can be 1, 2 or 3. After training, the model always chooses the same action regardless of the state. For example (1, 2, 3) for all states. But I know this is wrong and not an optimal policy. But I cannot figure out why this is happening. The policy I am using is given below (code). Code is in Julia language and uses ReinforcementLearning.jl library.
# Now we use a QBasedPolicy and neural network to estimate values
# Create a flux based DNN for q - value estimation
STATE_SIZE = length(env.channels) # 3
ACTION_SIZE = length(action_set)    # 27     
model = Chain(
        Dense(STATE_SIZE, 48, relu),
        Dense(48, 48, relu),
        Dense(48, 48, relu),
        Dense(48, 48, relu),
        Dense(48, ACTION_SIZE)
    ) |> cpu

# optimizer 
η = 1f-2 # Learning rate  
η_decay = 1f-3
opt = Flux.Optimiser(ADAM(η), InvDecay(η_decay))

# Create policies for each agent
single_agent_policy = Agent(
        policy = QBasedPolicy(;
                learner = BasicDQNLearner(;
                    approximator = NeuralNetworkApproximator(;
                        model = model,
                        optimizer = opt
                    ),
                    min_replay_history = 500
                ),
                explorer = EpsilonGreedyExplorer(
                    kind = :linear,
                    ϵ_stable = 0,
                    ϵ_init = 0.5,
                    warmup_steps = 300,
                    decay_steps = 700,
                    is_training = true,
                    is_break_tie = false,
                    step = 1
                )
            ),
            trajectory = CircularArraySARTTrajectory(;
                        capacity = 500,
                        state=Array{Float64, 1} => (STATE_SIZE)
                    )
            )

During training, the model explores and exploits various actions in different states, but, during the testing/exploitation phase, it always outputs the same action for every state.
I searched for similar questions on the web, but none of the questions were well answered.
","['reinforcement-learning', 'deep-rl', 'dqn', 'policies']",
What makes a transformer a transformer?,"
Transformers are modified heavily in recent research. But what exactly makes a transformer a transformer? What is the core part of a transformer? Is it the self-attention, the parallelism, or something else?
","['deep-learning', 'definitions', 'transformer']","
It's about self-attention, a mechanism that targets parallelism among other goals (see 1706.03762.pdf - Why Self-Attention).
From What Is a Transformer Model? | NVIDIA Blogs:

How Transformers Got Their Name
Attention is so key to transformers the Google researchers almost used the term as the name for their 2017 model. Almost.
“Attention Net didn’t sound very exciting,” said Vaswani, who started working with neural nets in 2011.
Jakob Uszkoreit, a senior software engineer on the team, came up with the name Transformer.
“I argued we were transforming representations, but that was just playing semantics,” Vaswani said.

You'll see the same sentiment in the first paragraph of Transformer (machine learning model).
This is not to say that self-attention is only the kind of attention mechanism employed by a Transformer model; see 1706.03762.pdf - 3.2.3 Applications of Attention in our Model. It's also not to say that adding an attention mechanism to your model is to make it a ""Transformer"" model; the innovation of this particular model was to go ""all out"" on attention mechanisms (you'd also have to get rid of recurrent and convolutional components).
"
Can Reinforcement Learning be used to generate sequences?,"
Can we use reinforcement learning for sequence-to-sequence tasks? If yes, whether or not this is a good choice, how could this be done?
","['reinforcement-learning', 'reference-request', 'applications', 'sequence-modeling', 'seq2seq']","
One renowned example for the specified case is SeqGAN

Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.

"
Which loss function could I use to solve a regression problem as a classification problem (where we discretize the labels into buckets)?,"
I am considering a rather typical regression problem, but, for practice, I am trying to implement this as a classification problem.
The setup is as follows. I have $\mathbb{R}$-valued labels $y_i \in [-1,1]$, which I then discretize to $N$ buckets -- my classification problem is to then predict the labels to the nearest bucket.
This is rather straightforward and easy to implement with a cross-entropy loss function. However, I do not believe that this is the best option, as I would ideally like my predictions to be close to their correct bucket, even if I do not predict them correctly (which will be more difficult as if I take $N$ larger).
My current approach involves using a mean-squared error loss function. My network outputs logits for each bucket, I apply a softargmax (so the network remains differentiable) and then convert the output of the network into the $\mathbb{R}$-valued prediction.
My (very premature) results are nothing to write home about. So, I ask, is there a more natural loss function that I could consider for this exercise?
","['machine-learning', 'classification', 'objective-functions', 'regression']",
"How to implement a (3 + 2)-dimensional convolutional layer where the 2d space is ""internal""?","
I am trying to train a CNN to learn 5D (kind of) data. The data is structured as follows. It has three spatial dimensions [x, y, z], but it also has two ""internal dimensions"" [theta, phi] at each [x, y, z]. What I am trying to do is upsample the internal space from fewer [theta, phi] data points.
When I train a 2d residual network with random [x, y, z] points in just the internal space it learns -- but there is some noise in the x, y, z space, there should be a correlation with neighbouring points.
What I wanted was some way to also include convolutions over the 3D [x, y, z] space to try and remedy this.
A possible but maybe naive approach is to do the following: Stack the images as [theta * phi, x, y, z] (so, many input channels) and then have some 3d convolution layers, then after that stack as [x * y * z, theta, phi] and take 2d convolutions in the internal space.
Another approach is to use 5d filters that span over all dimensions. This might be hard to implement for me and probably very memory hungry.
Are there any other ways?
","['convolutional-neural-networks', 'implementation', 'convolution', 'convolutional-layers', 'residual-networks']",
What is the difference between ERL and EA by considering it as RL?,"
I am currently studying as an MSCS student and my research is based on Evolutionary Algorithm as Reinforcement Learning, and I am confused about the following terms:

What is the difference between Evolutionary Reinforcement Learning and Evolutionary Algorithm by considering it as Reinforcement Learning?
if the Evolutionary Algorithm is Reinforcement Learning, what's the definition of state?
which part of the Evolutionary Algorithm is the rewards of Reinforcement Learning?
What will be the transition and Action function? can anyone please help me with this

","['reinforcement-learning', 'comparison', 'evolutionary-algorithms']",
Necessity of likelihood in training energy-based models,"
Lately, I've been getting into energy-based models (EBMs) through some of Yann LeCun's recent talks, where he advocates the use of non-normalized models because it allows for more flexibility in the choice of the loss function and convenient inference over high-dimensional spaces.
However, after reading some papers on the recent approaches to training EBMs (e.g Kingma's How to Train Your Energy-Based Models), most approaches still use likelihood to optimize the EBMs parameters.
I'm lost in the necessity of using a normalized likelihood for training, while the whole idea of EBMs is that they are not normalized. Why are methods that shape the energy-function directly not popular?
","['machine-learning', 'training', 'generative-model']",
How to build a custom morphological analyser for translation system,"
I want to build a machine translation system from English to Georgian. Georgian is a language similar (and simpler) to the Russian language. its syntax looks like base + suffix, only suffix changes, most of the time base is frozen, to describe the time the only suffix is changed. Unfortunately, I couldn't find a morphological analyser for the Georgian language, so could you link or provide useful resources to help me to build one? or can you give me some suggestions?
",['natural-language-processing'],"
Huggingface has a Helsinki-NLP/opus-mt-ka-en repository with a Georgian (Ka) to English (en) model. A tokenizer_config.json is available
"
Book(s) for text embedding,"
Text here refers to either character or word or sentence.
Is there any recent textbook that encompasses from classical methods to the modern techniques for embedding texts?
If a single textbook is unavailable then please recommend a list of books covering the whole spectrum as mentioned above.
Modern textbooks that are similar to Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008 are highly encouraged.
This question asks for textbook/research paper on word embedding only.
","['natural-language-processing', 'reference-request', 'word-embedding', 'books', 'vector-semantics']","
Having a sound understanding on language processing will help you understand all its concepts. This summarise must reads for NLP.
"
How does policy network learn in AlphaZero?,"
I'm currently trying to understand how AlphaZero works. There is one thing with the training of the AlphaZero's policy head that confuses me. Basically, in AlphaGo Zero's paper (where the major part of AlphaZero algorithm is explained) a combined neural network is used to estimate both, the value of the position and a good policy. More precisely, the loss function used is:
$$L = (z-v) - \pi^t \log(\textbf{p}) + c \Vert \theta \Vert$$
where $z$ is the outcome of the game, $v$ is the value estimated by the neural network, $\pi$ is the policy calculated by the MCTS and $\textbf{p}$ is the policy predicted by the neural network.
I would like to focus on the policy head loss. Basically, we are trying to minimize the difference between the policy calculated by the MCTS and the policy predicted by the neural network. That makes sense when the player has won the game, but it doesn't (at least from my point of view) when the player has lost it. You would be teaching your neural network a policy that has lost. Maybe the loss was unavoidable, but if it wasn't that's definitely not the policy we want to learn.
I have programmed a slightly simplified version that works well with Tic Tac Toe. But for Connect 4 some problems related to this arise. Basically, it learns a bad policy. At the beginning of the training, the values estimated for each board are quite random, and that makes the policy shift to a random (and wrong) direction. At the same time, that makes the value function to be wrong (because we are losing games that we could have easily won), worsening even more the policy.
I suppose that with enough training this problem disappears. The correct value and policy should backpropagate from the leaf nodes of our simulation. Even if the neural network policy gives a probability of 0 to the optimal action, thanks to the Dirichlet noise added to the probabilities the MCTS can find that optimal action and learn it.
However, several things confuse me:

In AlphaGo's paper, they take into account whether if the outcome
of the game has been a win or a loss when training the policy
network with reinforcement learning. More precisely, the
optimization made is
$$\Delta p \propto \frac{\delta \log{p(a_t|s_t)}}{\delta p} z_t  $$
where $z_t = 1$ if we have won or $z_t = -1$ if we have lost. So DeepMind's take into account if the action was good or not and change the direction to optimize.

I haven't found anywhere in AlphaGo Zero's paper that we are training just with the examples where the player has won, so they might be using all the data gathered, including also the examples where the player has lost. As far as I know, they don't mention anything related to this problem.

$\pi$ (the policy provided by the MCTS) is calculated as the exponentiated visit count of each action
$$\pi(a|s_0) = \frac{N(s_0,a)^{1/\tau}}{\sum_b N(s_0,b)^{1/\tau}}$$
where $\tau$ is a parameter that controls the ``temperature''.
DeepMind's team sets $\tau = 1$ during the 30 first movements to
ensure exploration. After that, they set it to $\tau \approx 0$ to
ensure that the action that is considered the best one (and thus has
been simulated more times) is the one played. However, that means
$\pi$ is something like
$$[0,0,\dots,0,1,0, \dots, 0]$$
making the policy changes a bit agressive and especially harmful if
the movement is not the good one (making it even harder to recover from a bad action).


Am I missing something? Is this the intended way of working of the algorithm? Is there any way to overcome the learning of bad policies?
","['reinforcement-learning', 'alphazero', 'alphago-zero', 'deepmind']","
I'll first address the big-picture intuition and then address each point separately.
First of all, the tree search tries to find the best policy at each turn. Losing the game doesn't necessarily mean the policy is bad, often it's just caused by us picking a bad move at least somewhere in the game (thanks to the policy temperature $\tau$), this could even be long before or after the current turn. Training can progress as long as the tree-derived policy is (at least slightly) better than the raw output of the neural net.
You're right that a bad value head can cause tree search to find a bad policy. However near the end of the game we are always getting some true signals:

the tree search encounters some terminal states which can directly influence the policy to be better
the value head can at least learn the right outcome for states close to the end

These two mechanisms cause both value and policy head to become better at the endgame, this means tree search and its policy can become better at near-endgame positions, which means we actually pick better moves to play which means the value head gets to learn better values on average.
So yes, early on during training the network can lean some wrong things for the early- and mid-game. There is always some pressure towards correctness coming from the endgame though, and with enough training and the right hyperparameters this can propagate all the way back to the start of the game.
To address your individual questions:

Yes, AlphaGo was doing something policy gradient-like where they used the game outcome to directly change the policy. AlphaZero works very differently, it's best to view them separate.

Indeed, the tree-derived policy of all moves is used for training, regardless of whether the current player ended up winning or losing the game.

This temperature is only used to pick an action during selfplay, not for the training target which still just uses $\tau = 1$. (as a side note, it happens to still be a good idea to use a temperature for the training target for regularization, LeelaChessZero and KataGo do this, see here for an explanation)


"
"Current extensions of the ""Turing Test""?","
In 2014 it was widely reported that the Turing Test had been passed, and that this was a major AI milestone.
See: Computer AI passes Turing test in 'world first [BBC]; Turing Test Success Marks Milestone in Computing History [reading.ac.uk]; What is the Turing test? And are we all doomed now? [The Guardian]
Never mind that the ""Imitation Game"" is subjective, and that porn bots have been passing it since there were porn bots—University of Reading was clear about their metrics.
But I understand that it did lead to revised tests, and extension of thinking on what constitutes passing such a threshold.

How have Turing Tests been extended since 2014?

How strong was the 2014 test?  What have been the criticisms of the 2014 determinations?
","['natural-language-processing', 'reference-request', 'turing-test', 'ai-milestones', 'journalism']",
Does it make sense to apply batch normalization to a batch size of 1?,"
I am interested in your opinion on the topic if you think that it makes sense to use batch normalization layer in a network that is trained with a batch size of 1. This is a special case as part of an experiment. What effects can be expected?
","['neural-networks', 'batch-normalization', 'residual-networks', 'normalisation']",
Is it realistic to train a transformer-based model (e.g. GPT) in a self-supervised way directly on the Mel spectrogram?,"
In music information retrieval, one usually converts an audio signal into some kind ""sequence of frequency-vectors"", such as STFT or Mel-spectrogram.
I'm wondering if it is a good idea to use the transformer architecture in a self-supervised manner -- such as auto-regressive models, or BERT in NLP -- to obtain a ""smarter"" representation of the music than the spectrogram itself. Such smart pretrained representation could be used for further downstream tasks.
From my quick google search, I found several papers which do something similar, but -- to my surprise -- all use some kind of symbolic/discrete music representation such as scores. (For instance here or here).
My question is this:

Is it realistic to train such an unsupervised model directly on the
Mel spectrogram?

The loss function would not be ""log softmax of next word probability"", but some kind of l2-distance between ""predicted vector of spectra"" and ""observed vector of spectra"", in the next time step.
Did someone try it?
","['transformer', 'gpt', 'audio-processing', 'embeddings', 'self-supervised-learning']","
These papers are also very close to what I meant in the question (too long for a comment).
The following references come mostly from work on speech recognition.

Mockingjay In this work, they use an analogy of Bert architecture that is fed by Mel-spectrogram, with some audio segments ""masked"".

The model is asked to reconstruct the masked parts. To avoid the model using local smoothness of audio-data, they always mask several subsequent frames, so that reasonably long segments are being masked.
They evaluate on downstream ""Phoneme classification tasks"" using the learned features and show that these learned features are stronger than raw spectrogram, in particular if little training data is available.


Audio Albert Same story, but they use shared weights in the transformer layers. This significantly reduces memory and computational requirements and it is shown that results are comparable with Mockingjay (at least for phoneme-classification tasks).
Tera; another Bert variant where instead of masking, they use various ""Alterations"" of certain audio segments.
This and many more references are within this project with code.

"
What approach would work well for predicting earthquake intensity based on historical data?,"
My problem: I own warning system where I collect data from institutions and send them over through various ways to users. I would like to hear your advice on what approach I can use for solving my problem with earthquake intensity far from epicenter. Since seismogical institutions mostly issue info about intenstiy of an earthquake for the epicenter, I would need to predict and classify what intensity the earthquake can have for places distant of several km/miles from the epicenter.
As an input/training set, I can use data of historical earthquakes and their magnitudes in an epicenter. Then I would need to fill mostly ""by hand"" an information about intensity based on seismological records, historical testimonies, chronicles atc.
What I need from AI: I need ""something"" that would predict earthquake intensity based on dataset of historical earthquakes.
Example/TLDR: There is an earthquake with magnitude 3.8, distant 80 km with depth 6 km. Based on dataset of historical earthquakes (with same type of information + witnessed and collected intensity), and output, I would need prediction of intensity of an eartquake 80 km from the epicenter.
","['neural-networks', 'classification', 'evolutionary-algorithms', 'fuzzy-logic', 'random-forests']",
"How can my CNN produce an ""unknown"" label?","
I have a dataset of 20k images of infected mango. I have built a web-based app using Flask, where a user can upload a picture, and my CNN model detects the disease. I have 6 classes in the model, which correspond to 6 types of diseases.
My question is: how do I train the model so that, if a user uploads any picture except infected mango, the model will show ""not mango""?
","['machine-learning', 'convolutional-neural-networks', 'image-recognition', 'supervised-learning']",
How can I compare the results of AC1 with the results of A3C (on the CartPole environment)?,"
I am implementing A3C for the CartPole environment. I want to compare the results I got from A3C with the ones I got from AC1. The problem is I don't know which process to look at. If I use, let's say, 11 processes, should I take the first one which got to average 495 points (over the last 100 episodes), last one, or should I take mean of all?
I don't want to take the first one that got to 495 since it is using a model that was already updated by the first few processes and it looks like cheating. Does some norm exist I can follow for valid results?

","['actor-critic-methods', 'advantage-actor-critic', 'a3c']",
CartPoleV0 model is not getting trained in even after 1500+ episodes using deep Q-learning,"
I am new to deep Q learning and trying to train the open AI cartpole_V0 game using deep Q learning. Here is my code:
import gym
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf
from collections import deque
import numpy as np
import random
import matplotlib
matplotlib.use('tkagg')
import matplotlib.pyplot as plt


EPISODES = 5000
output_dir = ""/home/ug2018/mst/18114017/ML/""
EPSILON = 1
REPLAY_MEMORY = deque(maxlen=800)
MIN_EPSILON = 0.01
DECAY_RATE = 0.995
MINIBATCH = 750
GAMMA = 0.99

env = gym.make('CartPole-v0')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n


class DQNagent:

    def __init__(self):

        self.fit_model = self.create_model()

        self.predict_model = self.create_model()
        self.predict_model.set_weights(self.fit_model.get_weights())

        self.targets = []
        self.states = []

    def create_model(self):

        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.Dense(64, activation =""relu"",input_dim = state_size))
        model.add(tf.keras.layers.Dense(128, activation =""relu""))
        model.add(tf.keras.layers.Dense(256, activation =""relu""))
        model.add(tf.keras.layers.Dense(128, activation =""relu""))
        model.add(tf.keras.layers.Dense(64, activation =""relu""))
        model.add(tf.keras.layers.Dense(32, activation =""relu""))
        model.add(tf.keras.layers.Dense(action_size, activation=""linear""))
        model.compile(loss=""mse"", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])
        return model

    def model_summary(self,model):
        return model.summary()

    def get_q(self, state):
        return self.predict_model.predict(state)

    def train(self,batch_size): 
        minibatch = random.sample(REPLAY_MEMORY, batch_size)
        for state, reward, action, new_state, done in minibatch:
            if done :
                target = reward
            else:
                target = reward + (GAMMA * np.amax(self.get_q(new_state)[0]))
            target_f = self.get_q(state) 
            target_f[0][action] = target

            self.states.append(state[0])
            self.targets.append(target_f[0])

        self.fit_weights(self.states,self.targets)
    
  
    def fit_weights(self, states, targets):
        self.fit_model.fit(np.array(states), np.array(targets), batch_size = MINIBATCH, epochs = 1 ,verbose=0)
    
    def predict_save(self, name): 
        self.predict_model.save_weights(name)
    def fit_save(self, name):    
        self.fit_model.save_weights(name)

    



agent = DQNagent()
print(agent.fit_model.summary())




x=[]
y=[]
z=[]

def update_graph(z,y):
    plt.xlabel(""Episodes"")
    plt.ylabel(""Score"")
    plt.plot(z,y)
    plt.pause(0.5)
plt.show()
    

for eps in range(EPISODES):
    env.reset()
    done = False
    state = env.reset()
    state = np.reshape(state, [1,state_size])
    time = 0
    exp=0
    elp=0
    while not done:
       
        if EPSILON >= np.random.rand():
            exp +=1
            action = random.randrange(action_size) 
        else:
            elp +=1
            action = np.argmax(agent.get_q(state)[0])
        new_state, reward, done, _ = env.step(action)
        new_state = np.reshape(new_state,[1, state_size])
        if not done:
            reward = -10
        else:
            reward = 10
        REPLAY_MEMORY.append((state,reward,action,new_state,done))
        state = new_state
        time += 1
    x.append([eps,exp,elp,time,EPSILON])
    y.append(time)
    z.append(eps)
    update_graph(z,y)
    if (len(REPLAY_MEMORY)) >= MINIBATCH:
        agent.train(MINIBATCH)
        if EPSILON > MIN_EPSILON:
            EPSILON *= DECAY_RATE
    if eps % 50 == 0:
        agent.predict_save(output_dir + ""predict_weights_"" + '{:04d}'.format(eps) + "".hdf5"")
        agent.fit_save(output_dir + ""fit_weights_"" + '{:04d}'.format(eps) + "".hdf5"")
    with open(""score_vs_eps.txt"", ""w"") as output:
        output.write(""Episodes""+""   ""+""Exploration""+""   "" + ""Exploitation"" + ""  ""+ ""Score"" + ""  "" + ""Epsilon""+""\n"")
        for eps,exp,elp,time,epsilon in x:
            output.write(""      ""+str(eps)+""        ""+str(exp)+""        ""+str(elp)+""        ""+str(time)+""       ""+""{:.4f}"".format(epsilon) +""\n"")

agent.predict_model.save('CartPole_predict_model')
agent.predict_model.save('CartPole_fit_model')

Code is running perfectly but the model is taking too many episodes to get trained and even after it scores 200, there is no continuity of it. Could please help me with how can I train the model in fewer episodes and maintain the continuity of the 200 scores?
Here are some of the steps listed:
Episodes Exploration Exploitation Score    Epsilon
  0        18        0        18       1.0000
  1        32        0        32       1.0000
  2        43        0        43       1.0000
  3        17        0        17       1.0000
  4        17        0        17       1.0000
  5        16        0        16       1.0000
  6        13        0        13       1.0000
  7        21        0        21       1.0000
  8        16        0        16       1.0000
  9        20        0        20       1.0000
  10        35       0        35       1.0000
  11        14       0        14       1.0000
  12        13       0        13       1.0000
  13        12       0        12       1.0000
  14        16       0        16       1.0000
  15        17       0        17       1.0000
  16        27       0        27       1.0000
  17        24       0        24       1.0000
  18        14       0        14       1.0000
  19        28       0        28       1.0000
  20        20       0        20       1.0000
  21        13       0        13       1.0000
  22        12       0        12       1.0000
  23        23       0        23       1.0000
  24        17       0        17       1.0000
  25        43       0        43       1.0000
  26        61       0        61       1.0000
  27        29       0        29       1.0000
  28        21       0        21       1.0000
  29        17       0        17       1.0000
  30        41       0        41       1.0000
  31        9        0        9        1.0000
  32        18       0        18       1.0000
  33        23       0        23       1.0000
  34        28       0        28       0.9950
  35        24       0        24       0.9900
  36        25       0        25       0.9851
  37        28       1        29       0.9801
  38        26       1        27       0.9752
  39        35       2        37       0.9704
  40        19       0        19       0.9655
  41        48       0        48       0.9607
  42        25       2        27       0.9559
  43        13       0        13       0.9511
  44        20       2        22       0.9464
  45        21       0        21       0.9416
  46        13       0        13       0.9369
  47        28       5        33       0.9322
  48        23       3        26       0.9276
  49        24       1        25       0.9229
  50        20       2        22       0.9183
  51        13       0        13       0.9137
  52        19       1        20       0.9092
  53        13       1        14       0.9046
  54        18       1        19       0.9001
  55        12       1        13       0.8956
  56        29       7        36       0.8911
  57        28       2        30       0.8867
  58        16       1        17       0.8822
  59        28       6        34       0.8778
  60        13       3        16       0.8734
  61        15       4        19       0.8691
  62        19       2        21       0.8647
  63        27       4        31       0.8604
  64        19       4        23       0.8561
  65        16       1        17       0.8518
  66        60       9        69       0.8475
  67        24       1        25       0.8433
  68        21       7        28       0.8391
  69        14       0        14       0.8349
  70        31       4        35       0.8307
  71        64       13       77       0.8266
  72        58       13       71       0.8224
  73        32       9        41       0.8183
  74        15       1        16       0.8142
  75        23       6        29       0.8102
  76        27       5        32       0.8061
  77        66       6        82       0.8021
  78        30       6        36       0.7981
  79        74       22       96       0.7941
  80        14       1        15       0.7901
  81        18       1        19       0.7862
  82        28       7        35       0.7822
  83        28       4        32       0.7783
  84        12       2        14       0.7744
  85        10       2        12       0.7705
  86        21       4        25       0.7667
  87        13       6        19       0.7629
  88        19       6        25       0.7590
  89        16       4        20       0.7553
  90        46       16       62       0.7515
  91        12       1        13       0.7477
  92        30       15       45       0.7440
  93        38       9        47       0.7403
  94        14       7        21       0.7366
  95        10       1        11       0.7329
  96        16       8        24       0.7292
  97        10       2        12       0.7256
  98        20       5        25       0.7219
  99        19       7        26       0.7183
  100       31       9        40       0.7147
  .
  .
  . 
  1522        0        104       104      0.0100
  1523        0        35        35       0.0100
  1524        0        27        27       0.0100
  1525        0        52        52       0.0100
  1526        0        25        25       0.0100
  1527        1        199       200      0.0100
  1528        0        30        30       0.0100
  1529        0        57        57       0.0100
  1530        0        35        35       0.0100
  1531        0        25        25       0.0100
  1532        0        22        22       0.0100
  1533        0        24        24       0.0100
  1534        1        199       200      0.0100
  1535        0        68        68       0.0100
  1536        0        200       200      0.0100
  1537        0        22        22       0.0100
  1538        2        42        44       0.0100
  1539        1        111       112      0.0100
  1540        0        91        91       0.0100
  1541        0        45        45       0.0100
  1542        2        108       110      0.0100
  1543        1        181       182      0.0100
  1544        0        30        30       0.0100
  1545        0        21        21       0.0100
  1546        1        25        26       0.0100
  1547        4        196       200      0.0100
  1548        0        95        95       0.0100
  1549        0        53        53       0.0100
  1550        0        55        55       0.0100
  1551        0        29        29       0.0100
  1552        0        40        40       0.0100
  1553        0        25        25       0.0100
  1554        0        33        33       0.0100
  1555        0        63        63       0.0100
  1556        0        23        23       0.0100
  1557        0        45        45       0.0100
  1558        0        25        25       0.0100
  1559        0        36        36       0.0100
  1560        0        24        24       0.0100
  1561        1        31        32       0.0100
  1562        0        30        30       0.0100
  1563        1        56        57       0.0100
  1564        0        22        22       0.0100
  1565        0        20        20       0.0100
  1566        1        22        23       0.0100
  1567        0        45        45       0.0100
  1568        1        50        51       0.0100
  1569        0        25        25       0.0100
  1570        0        30        30       0.0100
  1571        2        198       200      0.0100
  1572        2        198       200      0.0100
  1573        1        185       186      0.0100
  1574        0        26        26       0.0100
  1575        4        196       200      0.0100
  1576        3        197       200      0.0100
  1577        1        29        30       0.0100
  1578        0        25        25       0.0100
  1579        0        32        32       0.0100
  1580        3        197       200      0.0100
  1581        1        23        24       0.0100
  1582        0        25        25       0.0100
  1583        0        66        66       0.0100
  1584        1        27        28       0.0100
  1585        0        32        32       0.0100
  1586        0        21        21       0.0100
  1587        0        23        23       0.0100
  1588        1        47        48       0.0100
  1589        0        42        42       0.0100
  1590        0        26        26       0.0100
  1591        0        47        47       0.0100
  1592        0        200       200      0.0100
  1593        2        52        54       0.0100
  1594        1        19        20       0.0100
  1595        0        33        33       0.0100
  1596        0        27        27       0.0100 
  1597        1        79        80       0.0100
  1598        0        54        54       0.0100
  1599        0        50        50       0.0100
  1600        0        25        25       0.0100

I have initiated the epsilon with 1 and it has already reached its minimum possible value. Still, the result is very fluctuating from the score's point of view. Why is this happening? How can I maintain the continuity?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'dqn', 'gym']",
"What is the difference between the ""equal error rate"" and ""detection cost function"" metrics?","
I was designing a multi-speaker identification model, so I searched for some metrics that one may use. I found two metrics:

EER (equal error rate)
DCF (detection cost function)

What is the difference between them? Is one better than the other for my model?
","['machine-learning', 'comparison', 'metric', 'speech-recognition']","
Regarding DCF, Carbonell et al. (2007) state ""Although it is an excellent evaluation tool, the DCF has the limitation that it has parameters that imply a particular application of the speaker detection technology.""
For EER the authors make the following statement: ""The EER is a concise summary of the discrimination capability of the detector. As such it is a very powerful indicator of the discrimination ability of the detector, across a wide range of applications. However, it does not measure calibration, the ability to set good decision thresholds.""
References
Carbonell, J. G., Siekmann, J., Müller, C., van Leeuwen, D. A., & Brümmer, N. (2007). An Introduction to Application-Independent Evaluation of Speaker Recognition Systems. In Speaker Classification I (pp. 330–353).
"
Sutton and Barto 2nd Edition Exercise 13.1,"
I'm attempting exercise 13.1 in the Sutton and Barto textbook. It asks for an optimal probability for selecting action right in the short corridor scenario (see first 6 lines of the image below for the scenario).
Exercise 13.1: Use your knowledge of the gridworld and its dynamics to determine an
exact symbolic expression for the optimal probability of selecting the right action in
Example 13.1.
My attempt:
Letting $p$ denote the probability of choosing right, I understand that using the Bellman equations, we can solve for the value of $s_1, s_2, s_3$ where the states are numbered from left to right in terms of $p$. We have $v(s_1) = \frac{2-p}{p-1}$, $v(s_2) = \frac{1}{(p-1)p}$, $v(s_3) = -\frac{p+1}{p}$.
I can see how we can find the max of each of these functions to get the best optimal policy, given the state we're currently in.
However, how do you find the optimal policy generally (irrespective of starting state)? I found solutions here, which magically arrives at
$\frac{p^2-2p+2}{p(1-p)}$. Can someone explain this part?
https://github.com/brynhayder/reinforcement_learning_an_introduction/blob/master/exercises/exercises.pdf

",['reinforcement-learning'],"
The other answers are cool, here I give a straightforward proof, it might be more clearly for understanding the whole process.
The core idea here is that since each state can rollback to the previous state (the state before State S is itself), and each time it rollbacks to the previous state, the expected value of steps should increase the expectation of the previous state. The number of steps, which is ultimately expressed in the form of an infinite series.
Assuming that the action Left is selected in the first $t-1$ steps, and the action Right is selected in the $t$-th step, a total of $t$ steps are required to reach the State 2, where $ t \in [1, \infty) $:
$ E_{S\rightarrow 2}=1\cdot p^1+2\cdot p^1 (1-p)^1+3\cdot p^1 (1-p)^2+\cdots=p\sum_{t=1}^{\infty}t(1-p)^{t-1} $
$ =\frac{p}{1-p}\sum_{t=1}^{\infty}t(1-p)^t=\frac{p}{1-p}\cdot\frac{1-p}{p^2} = \frac{1}{p} $
The transition from State 2 to State 3 is divided into two cases, namely 2→3, 2→S→2→3, and if rollbacking from 2 to S, it is necessary to recursively calculate the expected number of steps from 2→S→2: $ E_{s\rightarrow 2} $, where $t$ is the total number of times to return to state S:
$ E_{2\rightarrow 3}=(1-p)+(2+E_{S\rightarrow2})p(1-p)+(3+2E_{S\rightarrow2})p^2(1-p)+\cdots $
$ =(1-p)+(1-p)\sum_{t=1}^{\infty}\left[1+t(1+E_{S\rightarrow2})\right]p^t$
$ =(1-p)+p+\frac{1-p^2}{p}\frac{p}{(1-p)^2} = 1+\frac{1+p}{1-p} = \frac{2}{1-p}$
$ E_{3\rightarrow G} $ can be obtained in the same way:
$ E_{3\rightarrow G}=p+(2+E_{2\rightarrow3})p(1-p)+(3+2E_{2\rightarrow3})p(1-p)^2+\cdots $
$ =p+p\sum_{t=1}^{\infty}\left[1+t(1+E_{2\rightarrow3})\right](1-p)^t $
$ = p+ p \left[ \frac{1-p}{p} + \frac{3-p}{1-p} \frac{1-p}{p^2} \right] = p+(1-p) + \frac{3-p}{p} = \frac{3}{p} $
Above we have calculated the expected number of steps for the three state transitions $ E_{S\rightarrow 2}, E_{2\rightarrow 3}, E_{3\rightarrow G} $, so the total expected value is:
$ E = E_{S\rightarrow 2} + E_{2\rightarrow 3} + E_{3\rightarrow G} = \frac{4}{p} + \frac{2}{1-p} $
To maximize the expectation, then
$ \left[\frac{4}{p} + \frac{2}{1-p}\right]^{\prime} = -\frac{4}{p^2}+\frac{2}{(1-p)^2} = 0 $
Get a solution that satisfies the condition: $ p=2-\sqrt{2}\approx 0.5858, E=6+4\sqrt{2} \approx 11.6568 $.
"
How does sharing parameters between the policy and value functions help in PPO?,"
The PPO objective may include a value function error term when parameters are shared between the policy and value functions. How does this help, and when to use a neural network architecture that shares parameters between the policy and value functions, as opposed to two neural networks with separate parameters?
","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization']","
Think of the network as a feature extractor followed by a policy head and a value function head. The feature extractor compresses the inputs into a lower dimensional feature vector that we hypothesize will be useful to both the policy and the value function. Then you train the policy and value function with those learned features as inputs (which is in theory much easier than training both with higher dimensional inputs). This way you save a bunch of parameters and hopefully accelerate training.
"
What is the probability of selecting the greedy action in a 0.5-greedy selection method for the 2-armed bandit problem?,"
I'm new to reinforcement learning and I'm going through Sutton and Barto. Exercise 2.1 states the following:

In $\varepsilon$-greedy action selection, for the case of two actions and $\varepsilon=0.5$, what is the probability that the greedy action is selected?

They describe the $\varepsilon$-greedy method on pages 27-28 as follows:

...behave greedily most of the time, but every once in a while,  say with small probability $\varepsilon$, instead select randomly from among all the actions with equal probability...

The above method makes the agent select an action randomly ""every once in a while"" from the action space uniformly with probability $\varepsilon$. I find the question imprecise since we don't know the ""once in a while"" in this exercise (i.e. is it once every $50$ timesteps? every time step?). If it's for every timestep, isn't it like a Bernouli problem where the parameter is $0.5$? I'd say that the agent has a $0.5$ chance to select a greedy action but I'm not sure at all.
","['reinforcement-learning', 'multi-armed-bandits', 'epsilon-greedy-policy']",
What is the effect of parallel environments in reinforcement learning?,"
Do parallel environments improve the agent's ability to learn or does it not really make a difference? Specifically, I am using PPO, but I think this applies across the board to other algorithms too.
","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization']",
How to detect the description of spine segments in short text using a neural network?,"
The input data is a set of text chunks containing the description of the pathology or the surgical procedure:
For instance:

Tere is a lumbar stenosis L3/4
Patient ist suffering from [...], MRI and X ray showed lumbar stenosis L3/4, segmental instability L3-5, foraminal stenosis L5/S1 both sides
The patient [...] underwent an MRI showing cervical stenosis C4-7 with myelopathy
[...] showed lumbar adult scoliosis L2-S1 with Cobb angle of 42°
Patient fell from the chair [...] showed osteoporotic fracture L3

Now, the ideal classificator would give me:

Segments: L3,L4; typeofpathology: degenerative; subtypepathology: stenosis
Segments: L3,L4,L5,S1; type of pathology: degenerative; subtypepathology: stenosis, instability
Segments C4, C5, C6, C7;type of pathology: degenerative; subtypepathology: myelopathy
Segments L2,L3,L4,L5,S1;type of pathology: deformity; subtypepathology: de novo scoliosis
Segments L3; type of pathology: pathological fracture; subtypepathology: -

I think that this cannot be reasonably achieved by a pre-programmed algorithm, because the amount of the text before the description can vary, and the choice of words can vary too. Is there an approach using neural networks or NLP tools that would have chance at reaching such classification? How large would the dataset used for training have to be (approximately)?
Maybe it would be reasonable to separate the two problems: detection of the segments AND detection of the pathology. For the segments, one could search for a pattern of C? T? L? or S? with ? being a number and then include all such segment descriptions in the next 20-30 characters, then use an algorithm to mark the continuous segments from the upper to the lower vertebra.
Done this, do neural networks offer any significant advantages over simple keyword matching classification? Most importantly, which NLP neural network tools would be the ones you would start trying with?
",['natural-language-processing'],
How to retrain a Facenet model with the triplet loss function?,"
I want to calculate the similarity or distance of two faces. I'm using Python.
I have read and done what this tutorial says. However, the result is not good (the similarity of same faces and similarity of different faces are very very very close to each other!).
I have downloaded and used this Facenet model to get face embedding vectors, and then used 3 distance metrics (Euclidean, Manhattan, Cosine) to calculate the distance.
After that, I decided to retrain that Facenet model with my dataset. I read this article. I want to use the triplet loss to retrain that Facenet model.
How can I retrain that Facenet model with the triplet loss function? Or can you please send me some links to read?
","['deep-learning', 'python', 'facenet', 'triplet-loss-function']","
If you want to train a model that is similar to Facenet, you have to train a Triplet Loss Neural Network similar to the one that you have seen in the tutorial. After training the full network you have to use only a part of the network that is used for embeddings extraction not the whole network, so when you call model.predict() you will get embeddings as output.
For more theoretical info, you can refer to Andrew NG lecture about it
For practical part, unfortunately there is no too much but hopefully this can help at least for building and using the network.
"
A neural network to learn the connection between two totally different type of images,"
I have a dataset of two different type of images. Say, I have images of a person and his all 10 fingerprints. I want to create a relation between them to predict one from another. How I can do that and which architecture is suitable for this problem or similar type of problem.
","['convolutional-neural-networks', 'training', 'deep-neural-networks', 'neural-architecture-search']","
I would try a pair of separate deep image embeddings with a contrastive loss. The idea is similar to the Siamese network architecture. In Siamese networks the pairs of images are of the same type - so both input images are fed through the copy of the same network. In your case the images are of different kinds, so I would just have separate nets for person images and fingerprints.
"
"Is my dataset unlearnable, or is my LSTM model not smart enough?","
I have time-series data obtained from a video. The data is composed of bitrate and corresponding label pairs for each timestamp:

The distribution over the first 30 seconds is as follows:

I have built an LSTM model for this dataset to be able to classify the labels based on the bitrate. However, it seems that my model is not able to learn. Validation accuracy starts from approximately 0.3 (makes sense, since I have 2 classes (log2 = 0.3)) and it does not improve.
Do you have any idea about this, is it normal considering this sample data distribution, or is something might be wrong with my model? Thanks!
","['classification', 'long-short-term-memory', 'time-series', 'binary-classification']",
Arcface implementation for image similarity produces opposite embeddings for positive negative image pairs,"
So I've built an arcface model with this arcface layer implementation:
https://github.com/4uiiurz1/keras-arcface/blob/master/metrics.py
I trained for a few epochs and now that I'm comparing the results I'm quite baffled.
According to the paper and resources I've read the embedding should produce embeddings where similar images are closer and should have a higher cosine similarity.
But I have the exact opposite case, I ran the models embedding layers through the hold out set and to 95% the mismatches are closer than the matches. Thus I have a reversed 95% accuracy.
My feed and labels are correct
I binned similar images in groups similar to here:
https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface
but for a different dataset.
Could someone guess why this is happening? Is it possible that some initialization would produce the opposite goal?
","['deep-learning', 'computer-vision', 'face-recognition']",
Is there an optimal number of species for NEAT?,"
Is there an optimal number of species for NEAT?
Since too low and too high is bad, I am thinking about adjusting the threshold of the distance function at runtime in order to have the number of species always between some bounds. Does this make sense? Is there an optimal range?
","['hyperparameter-optimization', 'neat', 'hyper-parameters', 'neuroevolution']",
What exactly are partially observable environments?,"
I have trouble understanding the meaning of partially observable environments. Here's my doubt.
According to what I understand, the state of the environment is what precisely determines the next state and reward for any particular action taken. So, in a partially observable environment, you don't get to see the full environment state.
So, now, consider the game of chess. Here, we are the agent and our opponent is the environment. Even here we don't know what move the opponent is going to take. So, we don't know the next state and reward we are going to get. Also, what we can see can't precisely define what is going to happen next. Then why do we call chess a fully observable game?
I feel I am wrong about the definition of an environment state or the definition of fully observable, partially observable. Kindly correct me.
","['reinforcement-learning', 'definitions', 'environment', 'state-spaces', 'pomdp']","
You are correct in the question that in RL terms chess a game of chess where the agent is one player, and the other player has an unknown state is a partially observable environment. Chess played like this is not a fully observable environment.
I did not use the term ""fully observable game"" or ""fully observable system"" above , because that is not a reinforcement learning term. You may also read ""game of perfect information"" which is similar - it means there are no important hidden values in the state of the game which may impact optimal play. This is a different concern to understanding the state of your opponent.
Here is a counter-example showing that games of perfect information are not fully observable systems when you have an opponent with an unknown strategy:

Optimal play in tic tac toe leads to a forced draw.

Let's define a reward signal from the agent's perspective of +1 for a win, 0 for a draw, and -1 for a loss.

If the agent's opponent always plays optimally, then a RL agent will learn to counter that optimal play and also play optimally. All action choices will have an expected return of 0 or -1, and the agent will choose the 0 options when acting greedily.

If the agent's opponent can make a mistake that allows the agent to win, then there will be a trajectory through the game with a return of 1, or perhaps some other postive value in cases where the mistake is only made according to random chance.

The value of states in the game therefore depends on the opponent's strategy.

The opponent's strategy is however not observable - it is unknown and not encoded into the board state.


This should match your intuition when asking the question.
Why then, do many two player game-playing reinforcement agents for games like chess perform well, without using POMDPs?
This is because game theory on these environments supports the concept of ""perfect play"", and agents that assume their opponent will also attempt to play optimally - without mistakes - will usually do well. Game theory analyses choices leading to forms of the minimax theory - making a choice that your opponent is least able to exploit.
That does mean that such an agent may in fact play sub-optimally against any given opponent. For example, they could potentially turn some losing or draw situations into a win, but have little or no capability to do so unless trained against that kind of opponent. Also, playing like this may be a large risk against other opponents, it may involve playing sub-optimally at some earlier stage.
I have observed a related issue in Kaggle's Connect X competition. Connect 4 is a solved game where player one can force a win, and the best agents are all perfect players. However, they are not all equal. The best performers tweak their agent's choices for player two, to force the highest number of wins against other agents who have not coded a perfect player one. Different kinds of learning strategy lead to different imperfections, and the top of the leaderboard is occupied by the current best perfect agent that also manages to exploit the population of near-perfect agents below it in the rankings. This difference in the top-ranking agents is only possible due to the partially-observable nature of the Connect 4 game played against agents with unknown policies.

What exactly are partially observable environments?

They are environments where in at least some states, the agent does not have access to information that affects the distribution of next state or reward.
Chess played against an opponent where you have a model of their behaviour - i.e. their policy - is fully observable to the agent. This is implicitly assumed by self-play agents and systems, and can work well in practice.
Chess played against an opponent without a model of their behaviour is partially observable. In theory, you could attempt to build a system using a partially observable MDP model (POMDP) to account for this, in an attempt to try and force an opponent into states where they are more likely to make a decision that is good for the agent. However, simply playing optimally as possible in response to all plays by the opponent - i.e. assuming their policy is the same near optimal one as yours even after observing their mistake - is more usual in RL.
The original Alpha Go actually had a separate policy network for its own choices and modelling those of humans. This was selected experimentally as performing slightly better than assuming human opponents used the same policy as the self-play agent.
"
Reward interpolation between MDPs. Will an optimal policy on both ends stay optimal inside the interval?,"
Say I've got two Markov Decision Processes (MDPs):
$$\mathcal{M_0} = (\mathcal{S}, \mathcal{A}, P, R_0),\quad\text{and}\quad\mathcal{M}_1 = (\mathcal{S}, \mathcal{A}, P, R_1)$$
Both have the same set of states and actions, and the transition probabilities are also the same. The only difference is in the reward functions $R_0$ and $R_1$. Suppose that we've found an optimal deterministic policy $\pi^*_0$ for the problem $\mathcal{M}_0$ and we've checked that this policy is also optimal for $\mathcal{M}_1$
$$\pi_0^*(s) = \arg\max\limits_a Q^*_0(s,a)\qquad  Q_1^*(s,\pi_0^*(s)) = \max\limits_a Q^*_1(s,a)$$
Now, given the two MDPs one can build a whole family of MDPs interpolating between them:
$$\mathcal{M}_\alpha = (\mathcal{S}, \mathcal{A}, P, \alpha R_0 + (1-\alpha) R_1)$$
Where $\alpha\in[0,1]$ is the interpolation parameter between the two problems - the rewards are linearly changing from $R_0$ to $R_1$ with this parameter. My question is - in general. will $\pi_0^*$ be optimal for all MDPs in the middle of interpolation interval?
$$Q_\alpha(s,\pi_0^*(s))\stackrel{?}{=}\max\limits_aQ^*_\alpha(s,a),\; \forall\alpha\in[0,1]$$
I feel like this could be generally true due to linearity of the dependence and convexity of the optimization problem. But I cannot neither prove it, nor find a counterexample.
","['markov-decision-process', 'rewards', 'reward-shaping', 'interpolation']",
Hyperparameters for Reproducing the Results of IRGAN on MovieLens 1M,"
I am trying to reproduce results reported for IRGAN (information retrieval GAN) on the MovieLens 1M dataset. The results I want to reproduce and their sources are listed in the table below.




Model
Precision@5
NDCG@5
Source




IRGAN
26.30%
26.40%
CFGAN


IRGAN
30.98%
31.59%
CoFiGAN


IRGAN
31.82%
33.72%
BiGAN




While my implementation of IRGAN is able to reproduce the results on the MovieLens 100k dataset, I am having problems discovering the hyperparameters for reproducing the results on the MovieLens 1M dataset; currently my IRGAN implementation is achieving a precision@5 score of 21.7%.
Unfortunately, the authors of the aforementioned papers do not share the hyperparameters used for training their version of IRGAN.
Thus, I want to ask if there is a repository with the used hyperparameters?
Furthermore, I would be most grateful if you could provide me information on how to contact the authors?
","['generative-adversarial-networks', 'hyper-parameters', 'recommender-system', 'collaborative-filtering']",
Converging to a wrong optimal policy if the agent is given more choices,"
I am a bit new to Reinforcement learning. So, I am extremely sorry if I am asking something obvious. I have written a small piece of code to find the optimal policy for a 5x5 grid problem.

Scenario 1. The agent is only given two choices (Up, Right). I believe, I am getting an optimal policy.
Scenario 2. The agent is given four choices (Up, Right, Down, Left). I am getting the wrong answer.

I have represented actions with numbers:
0 - Right
1 - Up
2 - Down
3 - Left

When the action Up is chosen, with 0.9 probability it will move up or 0.1 probability move right and vice-versa. When the action Down is chosen, with 0.9 probability it will move down or 0.1 probability move left and vice-versa.
I did not use any convergence criteria. Instead let it run for sufficient iterations. I have indeed confirmed that my optimal state values and policy is converging but to a wrong number. I am attaching the code below:
def take_right(state):
    if (state/n < n-1): state = state + n
    return state

def take_up(state):
    if (state%n!=n-1): state = state + 1
    return state

def take_left(state):
    if (state/n > 0): state = state - n
    return state

def take_down(state):
    if (state%n > 0): state = state - 1
    return state

Scenario 1 result:

Scenario 2 result:

Green has a reward of 100 and Blue has a penalty of 100. Rest of the states have a penalty of 1. Discount factor is chosen as 0.5
Edit:
This was really silly question. The problem with my code was more pythonic than RL. Check the comments to get the clue.
","['reinforcement-learning', 'markov-decision-process', 'bellman-equations', 'policy-iteration', 'finite-markov-decision-process']",
Terminology for the weight of likelihood ratio/score function?,"
If we estimate the gradient of $f(x)$ using the likelihood ratio/score function, i.e.
$$\nabla f = f^*\dfrac{\partial \log p(x)}{\partial \theta}$$
is there any agreed upon terminology to call ""$f^*$""? Specifically I'm thinking of the case where you may use some sort of baseline/control variate or a critic, so $f^*$ is not $f$.
I've seen $f^*$ called the learning signal or the cost. In reinforcement learning, you would call $f^*$ the advantage, but I think that terminology is only specific for RL. What is a general way to call $f^*$ that is not specific to RL?
","['terminology', 'gradient']",
Do we ever need more then 1 hidden layer in a binary classification problem with ANNs? If yes why?,"
I have read about the universal approximation theorem. So, why do we need more than 1 layer? Is it somehow computationally efficient to add layers instead of more neurons in the hidden layer?
","['neural-networks', 'machine-learning', 'deep-learning', 'deep-neural-networks', 'universal-approximation-theorems']","
This is akin to asking ""Why do we need more than one instance of sine to represent any repeating function"" or ""why can't we represent any polynomial with an equivalent polynomial of just the first degree?""  There are many, many problems... I'd even want to say most... that will require more than one layer to solve because the higher dimensional relationships cannot be well represented by just one layer.  This is not to say that the theorem is wrong, but consider the applied aspects.  We can approximate any continuous function, but that might require a single layer that is infinitely wide, however that same function might be approximated by a deep network having only a few dozen neurons.
However, this is not to say that many networks could not be represented with networks that perform at least as well, or perhaps even better, by simpler networks of fewer layers/neurons.  There is active research into how to generalize this.
Ultimately, non-trivial problems often require an empirical approach in this space currently because there is no general solution to ""learning.""
"
Pixel values of segmap in multi-class semantic segmentation,"
I'm preparing a dataset for a multiclass semantic segmentation using U-Net like architecture. To be precise, I've got it ready but a question came to my mind. How does pixel values of a segmentation map influence the training?
Also, is it better to have a greyscale segmap, or RGB one?
I have the dataset labeled and augmented, the only thing I am thinking now is if I should alter the segmaps.
I am planning to use Keras, is it smart enough to take in segmaps in both forms? I haven't found an answer anywhere, doublechecked if not on this forum, I hope it's not something super trivial. First time trying to do segmentation task so it's all a bit new to me :)
Edit: right now the segmaps looks like:

background [0 0 0]

object_type1 [16 180 75]    green

object_type2 [225 225 25]   yellow

object_type3 [230 25 75]    red


","['convolutional-neural-networks', 'keras', 'datasets', 'u-net', 'semantic-segmentation']",
How to classify when the label is seldom known,"
When I think about classification I think of the cancer/not cancer example. You have a bunch of attributes and you know whether the person had cancer during the relevant time period and you determine which attributes predict that result.
I work in a highly-regulated industry that serves the public. There are certain people we are not allowed to do business with, let's say because they will use our service for illegal purposes. Sometimes we tell the (potential) customer ""yes"" and sometimes ""no"".
When we say ""yes"" and the potential customer intends to use our service for illegal activity, they certainly won't inform us of the mistake.
Likewise, when we say ""no"" the potential customer will sometimes go away, will sometimes complain, but  that potential customer will not self-identify and say ""yes, you are correct, I intended to use your service for illegal activity"".
Occasionally we will receive a report from a 3rd-party that will label a customer, but these reports are  a tiny fraction of the number of customers. Unlike the cancer classification we almost always don't know the actual label, we only know what we guessed.
What techniques should we consider to measure our accuracy?
",['classification'],
"Why the non-exploitation of edge labels in current graph convolutions ""results in an overly homogeneous view of local graph neighborhoods""?","
I am currently reading a paper called Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs (2017, CPPR), and I cannot understand the following sentence:

We identify that the current formulations of graph convolution do not
exploit edge labels, which results in an overly homogeneous view of
local graph neighborhoods, with an effect similar to enforcing
rotational invariance of filters in regular convolutions on images.

What does this sentence mean?
","['convolutional-neural-networks', 'papers', 'convolution', 'geometric-deep-learning', 'graph-neural-networks']",
Regression for a discrete variable,"
I'm building a model (neural net) that would predict a quality score for images.
Ground truth is given by a 4-level discrete variable (0%, 33%, 67%, 100%), and I would like to build a model that would give something that looks like a continuous result over the 0-100% scale.
What should I pay attention to?
What I'm afraid of is that the model might stick to ground-truth levels and prefer them over any value in between.
","['neural-networks', 'regression']",
Rescaling time-series data with very spiky pattern for training data in LSTM network,"
I am working with some time-series hydrology data. Our goal is to forecast the time series forward, meaning predicting the data 1 month, 3 months ,6 months into the future. The data itself(image below) is characterized by mostly 0 or very small rates of flow expect for brief periods that are characterized by high flow. So I get this crazy spiky pattern where the median is around 0 or 1-2 meters^3/min, but at the same time there are periods of 5000 meters^3/minute, etc. I am not sure of the exact scale dimensions, but the picture below tells the tale.

So I was trying to figure out a good way to scale this type of spiky data. I have been using a MinMaxScaler just to start with, to rescale the values between (-1, 1). But that approach is not going to work well, especially because at the top ends of the range, the difference between 1000 m^3/min and 5000 m^3/min will be like 0.001 difference.
Does anyone have a good suggestion of how to rescale data like this for time-series analysis in an LSTM or RNN network?
","['recurrent-neural-networks', 'long-short-term-memory', 'data-preprocessing', 'time-series']",
What is the correct notation for an operation that applies to each element of an array independently?,"
I am looking for the standard notation to define element-wise / Hadamard-style functions, if there is one.
That is to say, if the operator I am looking for were represented by a hexagon ⬡, I could use it as such:
$$A(x) = \underset{i}{\Large{⬡} } f(x_i)$$
$$A : \mathbb{R}^n \rightarrow \mathbb{R}^n$$
$$f : \mathbb{R} \rightarrow \mathbb{R}$$
It is very convenient to define such functions explicitly because I want to manipulate them: $B \circ A$ . It seems to me that the following notation is correct: $A_i(x) = f(x_i)$ but I worry it is nonstandard and confusing.
My functions are non-linear so I cannot simply apply them directly to the array as a vector.
As stated in an answer, this is unnecessary when a function is strictly scalar because it is implied to apply element-wise. There are still some situations I would hope to have it:
$$\underset{ij}{\Large{⬡} } e^{M_{ij}} \ne e^M$$
The answers suggest to me the best option would be something like these:
$$A(M) = \text{for each } i,j: e^{M_{ij}}$$
$$A(M) = \text{element-wise}: e^{M_{ij}}$$
The question is now closed in the negative, but I would welcome a new answer. Would be nice to find something like $\forall$.
Related:

https://math.stackexchange.com/questions/159005/notations-for-array-operations
https://math.stackexchange.com/questions/20412/element-wise-or-pointwise-operations-notation

",['notation'],"
The mathematical notation for complex tensorial expressions always tries to balance complexity and precision. More precise notation - the one that explicitly spells all the indices - becomes extremely convoluted very quickly. My favorite example illustrating it is from physics -- the Standard Model Lagrangian is written shortly on T-shirts and coffee mugs as:
$$ \mathcal{L} = - \frac{1}{4} F_{\mu \nu} F^{\mu \nu} + (i \bar{\psi} \hat{D} \psi + \bar{\psi}_i y_{ij} \psi_j \phi + h.c.) + |D_\mu \phi|^2 - V(\phi) $$
But if you try to expand all the indices in all the objects above - then it barely fits on a page.
On the other hand, more succinct notation always leads to ambiguities in interpretations. Your example $f(x_i)$ can be read as:
$$f(x_0, x_1, \dots, x_N)\quad \text{or as}\quad f(x_0),f(x_1), \dots, f(x_N)$$
One way to implicitly resolve this ambiguity is to show that the index $i$ ""escapes"" the argument brackets:
$$a_i = f(x_i)\quad \text{or e.g.} \quad \sum_if(x_i)$$
This can only be interpreted as $f(x_i)$ being element-wise. Also, at least in my opinion, using $x_i$ with index and $x$ without index in the same expression is extremely confusing.
And, of course, the best way to resolve these ambiguities is to state them explicitly. For example, I've seen authors using square brackets $f[x_i]$ or capital letters $F(x_i)$ the vector-argument functions.
"
What happens with policy gradient methods if rewards are differentiable?,"
I would like some help with understanding why there is no explicit flow of information from the reward gradient to the parameters of the policy in policy gradient methods.
What I mean is the following, there are 2 scenarios:
1st - deterministic framework with given initial state $s_0$, actions $a_t = \mu_\theta(s_t)$, rewards $r(s_t, a_t, s_{t+1})$, and transitions $s_{t+1}=f(s_t, a_t)$. Assume all of these things are differentiable (maybe all is continuous). By drawing the computational graph I found I can compute the gradient of $J(\mu_\theta) = \sum_{t} r(s_t, a_t, s_{t+1})$ with respect to $\theta$. I could optimize for cumulative reward by doing gradient ascent on this objective.
2nd - framework in https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#id16 , which seems general, reading $\nabla_\theta J(\pi_\theta) = \nabla_\theta E_{\tau \sim \pi_\theta}[R(\tau)] = E_{\tau \sim \pi_\theta}[R(\tau)\sum_{t=0}^T \nabla_\theta \log{\pi_\theta (a_t | s_t)}]$.
What I don't understand (and I certainly feel confused about in an ignorant way) is why the derivation assumes that $\nabla_\theta R(\tau) = 0$ when this scenario (as it is stochastic) should include the 1st as a particular case, which does have a derivative!
It makes sense that $R(\tau)$ is only affected by $\theta$ through the change in probability of the trajectories $\tau$, but it still feels strange that $\tau = (s_0, a_0, \dots)$ is indeed one sample of $\tau = (s_0\sim \rho(s_0), a_0 \sim \pi_\theta(s_0), \dots)$ which depends on $\theta$. The deterministic reduction is obvious, but one could also think about reparametrization tricks in order to show the same point.
In other words, if the reward function were differentiable, i.e. fully differentiable known environment, how could I use this information?
","['reinforcement-learning', 'deep-rl', 'reward-functions']","
That's exactly the point of the Policy Gradient Theorem. Let's go through he proof of this theorem - it relies on our ability to ""loop"" the reward expression $J(\theta,s)$, expressing it through $J(\theta,s')$ in the next state:
$$J(\theta,s) = \mathbb{E}_{\tau\sim\pi_\theta}\left[ R(\tau) | s_0 = s\right]$$
$$\begin{align}
J(\theta,s) & = \sum_a\pi_\theta(a|s)\sum_{s',r}P(s',r|s,a)(r + J(\theta, s'))\\
& = \sum_aJ(\theta,a,s)
\end{align}
$$
Here, I've introduced the notation $J(\theta,a,s)$ to keep further derivation sane. Taking the gradient:
$$\begin{align}
\nabla_\theta J(\theta,s) = \sum_{a}\left(\vphantom{\sum_a}\right.&\nabla_\theta  \pi_\theta(a|s) \left(\sum_{s',r}P(s',r|s,a)(r + J(s',\theta))\right)  + \\
&+\pi_\theta(a|s) \nabla_\theta \sum_{s',r}P(s',r|s,a)(r + J(s',\theta)\left.\vphantom{\sum_a}\right)\end{align}
$$
With the first term in the sum we do the log trick:
$$
\frac{\nabla_\theta  \pi_\theta(a|s)}{\pi_\theta(a|s)} \left(\pi_\theta(a|s)\sum_{s',r}P(s',r|s,a)(r + J(s',\theta))\right) = J(\theta,a,s)\nabla_\theta\log\pi_\theta(a|s)
$$
And the second term simplifies to:
$$\pi_\theta(a|s) \nabla_\theta \sum_{s',r}P(s',r|s,a)(r + J(s',\theta)) =
\pi_\theta(a|s)  \sum_{s'}P(s'|s,a) \nabla_\theta J(s',\theta) 
$$
The last equality is exactly where we loose the rewards $r$ - the gradient of the constant $r$ is zero, and we can sum over $r$ because of the transition probability normalization $\sum_{r}P(s',r|s,a) = P(s'|s,a)$.
So we've finally get the expression for the gradient:
$$\nabla_\theta J(\theta,s) = \sum_{a}\left(J(\theta,a,s)\nabla_\theta\log\pi_\theta(a|s) + \pi_\theta(a|s)  \sum_{s'}P(s'|s,a) \nabla_\theta J(s',\theta) \right)$$
The Policy Gradient Theorem proof goes on though a couple more steps: unroll the last expression, rewrite through the distribution over states, then convert back from summation to the expectation over trajectories -- going through all this in detail here would too long. While we've already passed the crucial point for your question - we've got rid of the gradient of the rewards.
Edit:
In response to your comment, let me just stress again that this expression is a statement of the Policy Gradient Theorem.
$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau\sim\pi}\left[R(\tau)\sum\nabla_\theta\log \pi_\theta(a_t|s_t)\right]$$
The proof of Policy Gradient Theorem is pretty involved and certainly is not as simple as saying that $\nabla_\theta R(\tau) = 0$. Some authors abuse notation or cut corners when going through the proof of the theorem - for a reasonably strict exposition of the PG theorem I recommend Sutton and Barto, Chapter 13.
Another potential point of confusion is the notation $r(s_t,a_t,s_{t+1})$ that you've been using. In the most general MDP formulation rewards are random variables $r$. The probability of getting a reward $r$ and ending up in state $s_{t+1}$ is encoded in the transition probability $P(s_{t+1},r|s_t,a_t)$. (This covers the case of deterministic rewards by having a deterministic distribution over $r$ ). The derivation above uses this, most general, formulation of the MDP.
"
Model for direct audio-to-audio speech re-encoding,"
There are many resources available for text-to-audio (or vice versa) synthesis, for example Google's 'Wavenet'.
These tools do not allow the finer degree of control that may be required regarding the degree of inflections / tonality retained in output. For example to change vocal characteristics (Implied Ethnicity / Sexbfor example) of a dubbed voice over from one voice whilst retaining tonality (Shouting vs calm).
Text-to-speech 'and back' seems a suboptimal approach due to data loss (e.g. tonality) before reconstruction.
Re-encoding audio-to-audio would/may allow the alteration of characteristics in a manner not available via standard audio processing methods whilst retaining more of the desired tonality.
Is AI able to distinguish between characteristics and tonality as implied above and is such a speach-speach re-encoding tool available, ideally open source?
","['audio-processing', 'model-request', 'speech-synthesis']",
What is the minimum video resolution I need to identify anyone with facial recognition?,"
I am currently working on a small project where I am trying to automate some stuff at home. I am building a model capable of identifying my face with OpenCV. This will be a live feed.
I am making the project's estimations and have a really low budget. Therefore I am trying to identify what could be the minimum quality video feed I can pass to my algorithm to identify any face. For now I am just trying to identify mine.
I understand facial recognition works primarily on the unique pattern that could be found in the face. What is the minimum video resolution I need to identify anyone with facial recognition?
","['machine-learning', 'reference-request', 'python', 'facial-recognition', 'opencv']","
On page 2 of Axis' web page Identification and Recognition there is an estimate of the minimum number of pixels needed for identification, recognition and detection.

"
Why was the VC dimension not defined for all configurations of $d$ points?,"
Let's start with a typical definition of the VC dimension (as described in this book)

Definition $3.10$ (VC-dimension) The $V C$ -dimension of a hypothesis set $\mathcal{H}$ is the size of the largest set that can be shattered by $\mathcal{H}$ :
$$
\operatorname{VCdim}(\mathcal{H})= \max \left\{m: \Pi_{\mathcal{H}}(m)=2^{m}\right\}
$$

So, if there exists some set of size $d$ that $\mathcal{H}$ can shatter and it cannot shatter any set of size $d+1$, then the $\operatorname{VCdim}(\mathcal{H}) = d$.
Now, my question is: why would we be just interested in the existence of some set of size $d$ and not all sets of size $d$?
For instance, if you consider one of the typical examples that are used to illustrate the concept of the VC dimension, i.e. $\mathcal{H}$ is the set of all rectangles, then we can show that $\operatorname{VCdim}(\mathcal{H}) = d = 4$, given that there's a configuration of $d=4$ points that, for all possible labellings of those points, there's a hypothesis in $\mathcal{H}$ that correctly classifies those points. However, we can also easily show that, if the 4 points are collinear, there's some labelling of them (i.e. the 1st and 3rd are of colour $A$, while the 2nd and 4th are of colour $B \neq A$) that a rectangle cannot classify correctly.
So, the class of all rectangles can shatter some sets of points, but not all, so we would need another class of hypotheses to classify all sets of four points correctly. The VC dimension does not seem to provide any intuition on which set of classes would do the trick.
So, do you know why the VC dimension wasn't defined for all configurations of $d$ points? Was this just a need of Vapnik and Chervonenkis for the work they were developing (VC theory), or could have they defined it differently? So, if you know the rationale behind this specific definition, feel free to provide an answer. References to relevant work by Vapnik and Chervonenkis are also appreciated.
","['reference-request', 'definitions', 'computational-learning-theory', 'history', 'vc-dimension']","
The measure that you are talking about actually has a name. It is called the ""Popper dimension"" -- it was introduced by Karl Popper in his ""Logic of scientific discovery"".
Popper's idea of falsifiability was, as Vladimir Vapnik himself admits, the inspiration behind their work on the VC dimension. The VC dimension of the hypothesis set $\mathcal{H}$ measures the ""complexity"" of the set by looking at how easy would it be to falsify it. The hypothesis sets with higher VC dimension should be harder to falsify. With limiting case of infinite VC dimension which is unfalsifiable for almost any data.
VC dimension and Popper dimension are different in exactly the way you are describing. This quote from the philosophy paper that Vapnik co-authored states it rather succinctly:

The VC-dimension is the largest number of points one can shatter, the Popper dimension is one less than the smallest number of points one can not shatter.

This paper goes on trying to reconcile the two definitions but I didn't find their  exposition satisfactory (or even meaningful). I've found much more harsh take on this is in the ""Learning from data"" book (second edition, Section 4.7) :

Now we can contrast the VC dimension and Popper’s dimension, and conclude that Popper’s definition is not meaningful, as it does not lead to any useful conditions for generalization. In fact, for linear estimators the Popper’s dimension is at most 2, regardless of the problem dimensionality, as a set of hyperplanes cannot shatter three collinear points.

Which looks fair to me.
"
Does batch normalization affects the possible solution distribution?,"
As far as I recall, in Deep Learning, batch normalization normalizes each layer activations as a gaussian every batch.  If so, Let $x$ be the input and $z_i$ the activation in the $i$-th layer:  $p(z_i)$ becomes a gaussian with batch-norm. Right?
Does this constraint affects $p(z_i|x)$?
",['batch-normalization'],
Why does the Atari Gym Amidar environment only move after a certain number of episodes? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 


Closed 2 years ago.



After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.
After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.






                        Improve this question
                    



When I try to run Amidar even without RL code, I cannot get the environment to move immediately. It takes about 100 steps before the game actually starts moving. I use the following simple code to display some images and print some actions (I always try to do the same action, namely going up):
env = gym.make('Amidar-v0')
env.reset()

for i in range(1000):
    action = 2 
    next_state, reward, terminated, info = env.step(action) # take a random action
    print(f""Timestep {i}"")
    print(next_state.shape)
    print(reward)
    print(action)
    print(info)
    plt.imshow(next_state)
    plt.show()

When running this code, it takes until about step 85 before the environment starts to move. After that, each step, it moves until the agent is hit by the enemy. Then the environment restarts in the start state, and it takes quite some time before it starts to move again. I have tried doing 'FIRE' as my first action; however, this is not working since it also takes a while before the environment starts moving. Because of this, my buffer is almost always filled with the same images and hence my network isn't learning anything. How do I get this environment to move immediately?
","['reinforcement-learning', 'open-ai', 'gym', 'atari-games']",
Are policy gradient methods good for large discrete action spaces?,"
I have seen this question asked primarily in the context of continuous action spaces.
I have a large action space (~2-4k discrete actions) for my custom environment that I cannot reduce down further:
I am currently trying DQN approaches but was wondering that given the large action space - if policy gradient methods are more appropriate and if they are appropriate for large action spaces that are discrete as in my scenario above. I have seen answers to this question with regard to large continuous action spaces.
Finally - I imagine there isn't a simple answer to this but: Does this effectively mean DQN will not work?
","['reinforcement-learning', 'dqn', 'policy-gradients']","
A side note for other people who have similar questions. A large number of actions is also problematic for the policy network. Basically, the policy network has an output for every action (one-hot encoding), and computing softmax over a large number of classes (more than 1K approximately) is not viable. One can use other activation functions or techniques that make a network more generalizable for such a large number of classes.
"
Is intersection of labels acceptable in computer vision?,"
I have a dataset, where objects are very close to each other. So, the question is: what is the best approach to label them?
There are two possible options:

mark objects so that they will not intersect (it is difficult, surroundings are not included in the label area)
mark a larger area of objects, but labels will intersect

What is more practical?


","['convolutional-neural-networks', 'computer-vision', 'r-cnn', 'data-labelling', 'labeled-datasets']","
In my opinion, the second option will be more general. You can refer to some famous datasets for object detection task such as COCO or Pascal VOC, they usually accept the intersect annotations. As the image below, image from this link where they process the annotation of COCO dataset.
I think the reason is that the model will be easier to separate the intersect patterns in the bounding box than interpolate the missing patterns of the object to understand it

"
Is there a multi-agent deep reinforcement learning algorithm which is for environments with only discrete action spaces (Not hybrid)?,"
Is there a multi-agent deep reinforcement learning algorithm which is for environments with only discrete action spaces (Not hybrid) and have centralized training?
I have been looking for algorithms, (A2C, MADDPG etc.) but still havent find any algorithm that provides all of properties i mentioned (Multi agent + discrete action space + deep learning + centralized training).
I am wondering if we use an actor network that gets state as input and concatenated discrete actions of agents as output (For example if agent has 3 actions and we have 4 agents output can be [0,0,1, 0,1,0, 0,0,1, 1,0,0]) is that would be bad idea ?
","['reinforcement-learning', 'deep-rl', 'discrete-action-spaces', 'multi-agent-rl']","
A natural policy to act in an environment with discrete action space would be a softmax.
This paper describes a method that uses the idea of centralized training, and I believe could be used in your implementation.
With regard to your last question, I don't know if i understood, but if you have a system that must perform 3 actions, you could assign each action to a specific agent (assuming we have three different action spaces). Then you would have a cooperation game with 3 agents, where all of them have a common reward function. In theory, this 3 agents represents an individual agent that interacts with the environment.
"
"In style transfer, why does the comparison between channels give a good sense of style?","
I have been learning about Style Transfer recently. Style is defined as

The correlation of activations between channels.

I can't seem to understand why that would be true. Intuitively, style seems to be the patterns that exist in one particular channel/image rather than the patterns between channels. When filters in CNNs have different weights for acting as filters for different channels, why do we even expect 2 channels to be correlated? And further, why do we expect the style to be conveyed by it?
I expected a style function that could compare activations in some layer of a CNN condensed into one channel so that an algorithm can search for which activations occur simultaneously and hold style information.
I understand how we are carrying out the operations with the matrix and defining the loss function, what I don't get is why we are assuming style information lies in correlation between channels in the first place.
","['machine-learning', 'convolutional-neural-networks', 'generative-adversarial-networks', 'image-processing', 'style-transfer']",
Are there any successful applications of transformers of small size (<10k weights)?,"
In the problems of NLP and sequence modeling, the Transformer architectures based on the self-attention mechanism (proposed in Attention Is All You Need) have achieved impressive results and now are the first choices in this sort of problem.
However, most of the architectures, which appear in the literature, have a lot of parameters and are aimed at solving rather complicated tasks of language modeling ([1], [2]). These models have a large number of parameters and are computationally expensive.
There exist multiple approaches to reduce the computational complexity of these models, like knowledge distillation or multiple approaches to deal with the $O(n^2)$
computational complexity of the self-attention ([3], [4]).
However, these models are still aimed at language modeling and require quite a lot of parameters.
I wonder whether there are successful applications of transformers with a very small number of parameters (1k-10k), in the signal processing applications, where inference has to be performed in a very fast way, hence heavy and computationally expensive models are not allowed.
So far, the common approaches are CNN or RNN architectures, but I wonder whether there are some results, where lightweight transformers have achieved SOTA results for these extremely small models.
","['applications', 'transformer', 'sequence-modeling', 'efficiency', 'seq2seq']",
Why is the equation $\mathbb{E} \left[ (Y - \hat{Y})^2 \right] = \left(f(X) - \hat{f}(X) \right)^2 + \operatorname{Var} (\epsilon)$ true?,"
In the book An Introduction to Statistical Learning, the authors claim (equation 2.3, p. 19, chapter 2)
$$\mathbb{E} \left[ (Y - \hat{Y})^2 \right] =  \left(f(X) - \hat{f}(X) \right)^2 + \operatorname{Var} (\epsilon) \label{0}\tag{0},$$
where

$Y = f(X) + \epsilon$, where $\epsilon \sim \mathcal{N}(0, \sigma)$ and $f$ is the unknown function we want to estimate
$\hat{Y} = \hat{f}(X)$ is the output of our estimate of $f$, i.e. $\hat{f} \approx f$

They claim that this is easy to prove, but this may not be easy to prove for everyone. So, why is equation \ref{0} true?
","['machine-learning', 'proofs', 'function-approximation', 'statistical-ai', 'probability-theory']","
Let me try to show this. The only (non-constant) random variable here is $\epsilon$, while $f(X)$ and $\hat{Y} = \hat{f}(X)$ are constant random variables (so their expectations is equal to their only value).
So, we start with the following expression.
\begin{align}
\mathbb{E} \left[ (Y - \hat{Y})^2 \right] \tag{1}\label{1}
\end{align}
Now, we just apply the distributive property, so \ref{1} becomes
\begin{align}
\mathbb{E} \left[ Y^2 - 2Y \hat{Y} +  \hat{Y}^2 \right] \tag{2}\label{2}
\end{align}
Given the linearity of the expectation, we can write \ref{2} as follows
\begin{align}
\mathbb{E} \left[ Y^2 \right]  - \mathbb{E} \left[ 2Y \hat{Y} \right]  +  \mathbb{E} \left[\hat{Y}^2 \right] \tag{3}\label{3}
\end{align}
Given that $\hat{Y} = \hat{f}(X)$ is a constant and that we can take constants out of the expectations, we have
\begin{align}
\mathbb{E} \left[ Y^2 \right]  - 2 \hat{Y} \mathbb{E} \left[ Y  \right]  +  \hat{Y}^2 \tag{4}\label{4}
\end{align}
Now, let's replace $Y$ with $f(X) + \epsilon$, to obtain
\begin{align}
\mathbb{E} \left[ \left( f(X) + \epsilon \right)^2 \right]  - 2 \hat{Y} \mathbb{E} \left[ f(X) + \epsilon \right]  +  \hat{Y}^2 \tag{5}\label{5}
\end{align}
Now, in the book, they assume that $\epsilon \sim \mathcal{N}(0, \sigma)$, so $\mathbb{E}\left[ \epsilon \right] = 0$ (i.e. the expected value of $\epsilon$ is just the mean of the Gaussian, which is assumed to be zero). So, \ref{5} becomes
\begin{align}
&\mathbb{E} \left[ \left( f(X) + \epsilon \right)^2 \right]  - 2 \hat{Y} \left( \mathbb{E} \left[ f(X) \right] + \mathbb{E} \left[ \epsilon \right] \right)  +  \hat{Y}^2
= \\
&\mathbb{E} \left[ \left( f(X) + \epsilon \right)^2 \right]  - 2 \hat{Y} \left(  f(X) + 0 \right)  +  \hat{Y}^2 
= \\
&\mathbb{E} \left[ \left( f(X) + \epsilon \right)^2 \right]  - 2 \hat{Y} f(X)  +  \hat{Y}^2
= \\
&\mathbb{E} \left[  f(X)^2 + 2 f(X) \epsilon  + \epsilon^2 \right]  - 2 \hat{Y} f(X)  +  \hat{Y}^2 =
\\
&\mathbb{E} \left[  f(X)^2 \right]  + \mathbb{E} \left[  2 f(X) \epsilon \right]   + \mathbb{E} \left[  \epsilon^2 \right]  - 2 \hat{Y} f(X)  +  \hat{Y}^2 = \\
& \mathbb{E} \left[  \epsilon^2 \right]  + f(X)^2  - 2 \hat{Y} f(X)  +  \hat{Y}^2 
= 
\\
&
\mathbb{E} \left[  \epsilon^2 \right]  + \left(f(X)  - \hat{Y} \right)^2  \tag{6}\label{6}
\end{align}
Now, note that the variance of a random variable $Z$ is defined as
$$\operatorname {Var} (Z)=\mathbb {E} \left[(Z - \mu_Z )^{2}\right]$$
In our case, $\mu_Z$ is zero, so the variance of $\epsilon$ is $\mathbb{E} \left[  \epsilon^2 \right]$, so \ref{6} becomes
\begin{align}
\operatorname{Var} (\epsilon)  + \left(f(X)  - \hat{Y} \right)^2 \\
 \tag{7}\label{7}
\end{align}
You can also come up with the same result in a different and simpler way, i.e. rewrite $\mathbb{E}\left[ \left( f(X) + \epsilon - \hat f(X) \right)^2 \right]$ as $\mathbb{E}\left[ \left( \left(f(X) - \hat f(X)\right) +\epsilon  \right)^2 \right]$, then you apply the distributive property and similar rules that I applied above to derive the same result.
"
What are the typical sizes of practical/commercial artificial neural networks?,"
I'm interested in artificial neural networks (ANN) and I wonder how big ANNs in practical use are, for example, Tesla Autopilot, Google Translate, and others.
The only thing I found about Tesla is this one:

""A full build of Autopilot neural networks involves 48 networks that
take 70,000 GPU hours to train. Together, they output 1,000 distinct
tensors (predictions) at each timestep.""

It seems like most companies don't publish clear information about their ANN sizes. I really can't find anything detailed on this subject.
Is there any information about the size of big practical/commercial ANNs that include something like the amount of neurons/connections/layers etc.?
I'm looking for a few examples in this scale with more precise information on the size of the neural networks.
","['neural-networks', 'machine-learning', 'reference-request', 'autonomous-vehicles']","
I hope this helps. Disclaimer: the info is extracted from Computer Vision at Tesla, though aditional references may be needed....

Tesla is using 8 cameras per vehicle fused with radars.
To detect road lanes, vehicles, pedestrians etc., Tesla have to run
at least 50 neural networks simultaneously.
The neural networks are trained using PyTorch. Each image (for each
camera I guess) is of dimension (1280, 960, 3). RGB image it seems.
Regarding the neural network architecture, ""the backbone is a modified ResNet 50"", that is, a convolutional network.
The network is 50 layers deep. The network depth is defined as the largest number of sequential convolutional or fully connected layers on a path from the input layer to the output layer. In total, ResNet-50 has 177 layers. The ResNet-50 has over 23 million trainable parameters
Every camera is processed through a single neural network. Then everything is combined into a middle neural network.

"
Convolutional Layer Multichannel Backpropagation Implementation,"
I have been working on coding a CNN in python from scratch using numpy as a semester project and I think I have successfully implemented it up to backpropagation in the MaxPool Layers. However, my model seems to never converge whenever there is a Convolutional Layer(s) added. I am assuming there is a problem with the way I have implemented the backpropagation.
Most examples that I have seen for this implementation either really simplify it by using a one-channel input and a single one-channel filter, or just dive straight into the Mathematics which doesn't only not help but also confuses me more.
Here is the way I have tried to implement both Forward and Backward Propagation for multichannel inputs and outputs based on my own understanding and things I read online.
Forward Prop:

Backward Prop for Filter Gradients:

Backward Prop for Input Gradients:

Kindly point out anything that's wrong here. I have been working on this part for the last 2 days but there has to be a problem because my model never seems to converge.
Thanks!
","['deep-learning', 'convolutional-neural-networks', 'backpropagation', 'convolution', 'convolutional-layers']",
Tensforflow schedule - does not change boundaries [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.







                        Improve this question
                    



I'm trying to manipulate the learning rate with tf PiecewiseConstantDecay.
I can easily check if the algorithm switches learning rate values, because one rate is extremely low 1e-20 !!
However, NO setting of the ""boundaries"" causes the algorithm to switch learning rate... What am I doing wrong?
step = tf.Variable(0, trainable=False)
boundaries = [100]
values = [1e2, 1e-20]

schedule = tf.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)
lr = 1e-4 * schedule(step)

optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
model.compile(optimizer=optimizer, loss='mean_squared_error')

history = model.fit(x = x_train, y = y_train, validation_data=(x_val, y_val), epochs=100,batch_size=32)

Would love your input.
","['neural-networks', 'machine-learning']","
After 100 steps learning rate will switch from 1e-2 to 1e-24.
What exactly is the problem here? Are you confusing steps with epochs?
"
What is the process working on Tensorflow model.fit()? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.







                        Improve this question
                    



I created a binary image classification model. The dataset contains about 500K images in each class, with ratio = Train : Validation : Test = 7 : 2 : 1. Total images = 1M
I split my dataset into 5 parts (compute constraints)—5 training subsets, 5 validation subsets, and 1 test subset.
I trained and evaluated my model stage by stage. In first stage (evaluation), my model's accuracy was 65%. I re-fitted it with 2nd dataset and the accuracy was 43%. I did same process with the rest, and my accuracies were: 65%, 43%, 57%, 21%, 30%.
How can I train my model in staged training?
I want to train models with different datasets without reinitialize the weight every training process.
","['deep-learning', 'tensorflow', 'training']","
You can save weights during training by passing checkpoint callback to model.fit() method.
# Instantiate your model here
model = create_model() 

# Set model configurations here
model.compile(loss=..., optimizer=..., metrics=...) 

# Set checkpoint path
checkpoint_path = ""model_weights.ckpt""

# Create a callback that saves the model's weights
    filepath=checkpoint_path,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

# Train the model with the new callback
model.fit(train_images_1, 
          train_labels_1,
          epochs=50, 
          batch_size=batch_size, 
          callbacks=[cp_callback],
          validation_data=(test_images_1, test_labels_1),
          verbose=0)

After finishing training 1st dataset, model weights will be saved in file called model_weights.ckpt. Before starting training next dataset, load the model weights as below
# Create a new model instance
model = create_model()

# Set model configurations here
model.compile(loss=..., optimizer=..., metrics=...) 

# Set checkpoint path
checkpoint_path = ""model_weights.ckpt""

# Load the previously saved weights
model.load_weights(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

# Train the model with the new callback
model.fit(train_images_2, 
          train_labels_2,
          epochs=50, 
          batch_size=batch_size, 
          callbacks=[cp_callback],
          validation_data=(test_images_2, test_labels_2),
          verbose=0)

Repeat this for all datasets.
"
"In off-policy MC control algorithm by Sutton & Barto, why do we perform a last update when sample action is inconsistent with target policy?","
I have a question about the $W$ term in the off-policy MC control algorithm on Page 111 of Sutton & Barto. I have also included it in the figure below.

My question: shouldn't the check $A_{t} = \pi(S_{t})$ be made before updating $C(S_{t}, A_{t})$ and $Q(S_{t}, A_{t})$? And, at this point if $A_{t} \neq \pi(S_{t}) $ then the inner loop should exit before updating $Q(\cdot)$. If $A_{t} = \pi(S_{t})$ then shouldn't $W$ be updated to
$W = W \frac{1}{b(A_{t}|S_{t})} $ before updating the $Q(s, a)$ and $C(s, a) functions?
The algorithm as stated seems problematic to me. For example, if say the target policy $\pi$ is deterministic and behavior policy $b$ is stochastic. If in period $T-1$ the behavior policy takes an action that is not consistent with $\pi$ then the importance sampling ratio $\rho_{T-1:T-1} = 0$. However, the algorithm as shown would update $Q(S_{T-1}, A_{T-1})$ since the checks I referred to above don't occur until the end of the inner loop. What am I missing here?
","['reinforcement-learning', 'monte-carlo-methods', 'off-policy-methods', 'sutton-barto']",
Do the training and test datasets need to be equally preprocessed as one whole dataset?,"
I have developed, trained and tested an NLP model. It is persisted in a pickle file. The model contains the data preprocessing function that includes text cleaning and new features engineered with word2vec.
With the trained model, I want to make predictions on a new text. The new text data, after preprocessing, won't contain the same engineered features of the training dataset.
Therefore my question is, how can the trained model make predictions on the new dataset as it has different engineered features (different numbers of columns and different columns)?
Should I preprocess the new text data and the training dataset as one dataset?
","['machine-learning', 'natural-language-processing', 'prediction', 'data-preprocessing', 'feature-engineering']",
Understanding Generalized Advantage Estimate in reinforcement learning,"
I was reading the paper on Generalized Advantage Estimate. It first introduces a generalized form of policy gradient equation without involving $\gamma$ and then it says the following:

We will introduce a parameter $\gamma$ that allows us to reduce variance by downweighting rewards corresponding to delayed effects, at the cost of introducing bias. This parameter corresponds to the discount factor used in discounted formulations of MDPs, but we treat it as a variance reduction parameter in an undiscounted problem.

I know the Monte Carlo estimate of value function is given as:
$$V(s_t)=\sum_{l=t}^\infty \gamma^tr_t$$
The bootstrapped estimate of value function is given as:
$$V(s_t)=r_t+\gamma V(s_{t+1})$$
(In both equations, $\gamma$ is a discount factor.)
The bootstrapped estimate is biased because it based on $V(s_{t+1})$ which is usually a biased estimate by some estimator such as a neural network. The Monte Carlo estimate is unbiased because it contains all rewards sampled from the environment. In this case, however, because the agent might take a lot of actions over the course of an episode, it's hard to assign credit to the right action, which means that a Monte Carlo estimate will have a high variance.
Does this contradict what the paper says: ""a parameter $\gamma$ that allows us to reduce variance""? Or does it simply mean the following: lower $\gamma$ gives smaller weights to distant future rewards, thus making value estimate less dependent on them, reducing variance in comparison to larger $\gamma$, which make distant future rewards contribute significantly to value estimate. So introduction/existence of $\gamma$ itself does not reduce the variance but gives way to increase or decrease the variance.
","['reinforcement-learning', 'deep-rl', 'papers']","

Bootstrapped estimate is biased because it based on $V(s_{t+1})$ which is usually a biased estimate by some estimator such as neural network.

I don't think this statement is totally correct in the context of the paper. Quoting the paper:

Taking $\gamma < 1$ introduces bias into the policy gradient estimate, regardless of the value function’s accuracy. On the other hand, $ \lambda < 1$ introduces bias only when the value function is inaccurate.

So the initial $\gamma$-discounting introduces bias regardless of the accuracy of the $V(s)$.
Same point is reiterated in the footnote on the page 3:

Note, that we have already introduced bias by using $A^{\pi,\gamma}$ in place of $A^\pi$; here we are concerned with obtaining an unbiased estimate of $g^\gamma$, which is a biased estimate of the policy gradient of the undiscounted MDP.

To summarize the above:

the original MDP under consideration is undiscounted
authors introduce $\gamma$-discounting to reduce variance (at cost of bias) of the PG estimate - this happens even if we had accurate $V(s)$
later, authors introduce another $\lambda$-discounting which also controls bias-variance tradeoff - now due to inaccuracy in $V(s)$

As for why lower $\gamma$ reduces variance (p.2 above) - your explanation is on point: smaller $\gamma$ $\Rightarrow$ smaller weights for distant rewards $\Rightarrow$ less variance (but more bias).
"
How should I interpret the surrogate and mean_noise_std plots of training a PPO model (from the Nvidia's Isaac gym)?,"
I am currently using the PPO method from the Nvidia's Isaac gym to train an agent for my robot. Below, you can see the plot which corresponds to a training process.
I know that something is massively wrong with my training process (since the robot does not manage to get a nice policy), so I am trying to understand the training process more with the help of the below values being logged during the problematic training.


So far, I could find out what this value function means: the reward function stabilizes, thus the loss value function also stabilizes, which means that my robot should explore more to fail and learn from the fail!
But what about the other two plots, surrogate and mean_noise_std? How should one interpret those values?

The ideal training process



","['reinforcement-learning', 'proximal-policy-optimization']","
Loss. In the context of Deep Learning and Deep Reinforcement Learning, ""training"" is just a fancy word for ""optimization"". You are essentially looking for an optimum point in some huge-dimensional parameter space. For the Reinforcement Learning problems, the optimal point is supposed to maximize the expected return - while most of the Deep Learning classification problems are minimizing some loss.
Most of the modern Deep Learning programming frameworks was created with the classification problems in mind, so they call the optimization goals ""losses"": tf.keras, pytorch. The good news is that you can easily convert return-maximization problem to loss-minimization problem by just changing the sign of your target function.
Given all the above, the fact that your loss is going up instead of going down doesn't seem to be a good sign.
Value Function. Given an agent's state, Reinforcement Learning tries to find the best agent's action that maximized the expected return. The problem is that the expected return is also unknown. Note on the difference between ""reward"" and ""return"". Rewards $r_t$ are immediate bits of positive/negative reinforcement that the agent receives obtain after taking an action. The ""return"" $G_t$ is the function of a particular trajectory - it is the discounted sum of future rewards starting from step $t$:
$$G_t = \sum_k \gamma^k r_{t+k+1}$$
The expectation of the return at state $s$ given a policy $\pi$ is called value $V^\pi(s)$:
$$V^\pi(s) = \mathbb{E}_{a\sim\pi}\left[ G_t\right|\left. s_t=s\right]$$
In principle, it is possible to estimate $V^\pi(s)$ by doing Monte Carlo: start from the state $s$ and run many simulations following $\pi$ and average all the returns you've got. For most practical cases this approach is infeasible. Modern Deep Reinforcement Learning algorithms try to reduce computational load by using neural networks to approximate $V(s)$.
Having the value function approximation $V(s)$, one can then try to optimize the policy approximation function $\pi(a|s)$ using the policy gradient update. Such setup: with one approximator for policy and another approximator for value function is called an ""Actor-Critic"" family of Reinforcement Learning algorithms.
BTW, it doesn't look like your value function ""stabilizes"" during training. And it also doesn't change too much if you look at the vertical scale of your plot.
Surrogate Loss. In practice, the policy gradient optimization step above suffers from instabilities. The gradient step tends to change the policy too strongly, which makes the approximation error of the value function too large.
To deal with this, the TRPO/PPO algorithms introduce a kind of clipping of the loss gradient that constrains the policy gradient changes from being too large. The new function that produces these clipped/constrained gradients is called a ""surrogate loss"".
mean_noise_std. This doesn't look like a standard name for anything, but I suspect that this is a version of PPO with artificial noise added for better exploration of the agent.
"
"When showing that the policy improvement theorem applies to MC control, why is $q_{\pi_{k}}\left(s, \pi_{k}(s)\right) \geq v_{\pi_{k}}(s)$ true?","
When discussing why the policy improvement theorem is true, when we do Monte Carlo control by updating greedily, it says on page 98 of Sutton and Barto's book (2nd edition) that:
$$
\begin{aligned} 
q_{\pi_{k}}\left(s, \pi_{k+1}(s)\right) 
&=
q_{\pi_{k}}\left(s, \underset{a}{\arg \max } q_{\pi_{k}}(s, a)\right) 
\\ 
&=
\max _{a} q_{\pi_{k}}(s, a) 
\\ 
& 
\geq 
q_{\pi_{k}}\left(s, \pi_{k}(s)\right) 
\\ 
& 
\geq v_{\pi_{k}}(s) \end{aligned}
$$
I don't understand why the last inequality is not an equality.
The policy improvement theorem was derived on page 78 for deterministic policies.
So, the $\pi_{k}(s)$ we see in $q_{\pi_k}(s, \pi_{k}(s))$ is a fixed action, call it $a'$. Then, in this case, since $v_{\pi_k}(s)= \sum_a \pi_k(a|s) q_{\pi_k}(s, a) =   1 * q_{\pi_k}(s, a') = q_{\pi_k}(s, a')$ (where the second equality is because the probability of all other actions is 0), shouldn't the last inequality be an equality? When is it possible that we have a greater than relation?
","['reinforcement-learning', 'proofs', 'monte-carlo-methods', 'policy-improvement-theorem']",
Update Rule with Deep Q-Learning (DQN) for 2-player games,"
I am wondering how to correctly implement the DQN algorithm for two-player games such as Tic Tac Toe and Connect 4. While my algorithm is mastering Tic Tac Toe relatively quickly, I cannot get great results for Connect 4. The agent is learning to win quickly, if it gets the chance, but it only plays in the centre. It is unable to detect threats in the first and last columns. I am using DDQN with memory replay. Also teacher and student refer to two agents at different strengths, while the teacher is frequently replaced by a new student. My algorithm looks simplified as follows:
for i in range(episodes):
    observation = env.reset()
    done = False
    while not done:
        if env.turn == 1:
            action = student.choose_action(observation)
            observation_, reward, done, info = env.step(action)
            loss = student.learn(observation, action, reward, observation_, done))
            observation = observation_
        else:
            action = teacher.choose_action(-observation)
            observation_, reward, done, info = env.step(action)
            observation = observation_

The observation is -1 for player ""o"", 1 for player ""x"" and 0 for empty. The agent learns to play as player ""x"" and through action = teacher.choose_action(-observation) it should find the best move for player ""o"". I hope that is clear.
The update rule looks as follows:
# Get predicted q values for the actions that were taken
q_pred = Q_eval.forward(state, action)
# Get Q value for opponent's next move
state_ *= -1.
q_next = Q_target.forward(state_, max_action)
# Update rule
q_target = reward_batch - gamma * q_next * terminal
loss = Q_eval.loss(q_pred, q_target)

I am using -gamma * q_next * terminal, because the reward is negative, if the opponent wins in the next move. Am I missing anything important or is it just a question of hyperparameter tuning?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'dqn', 'double-dqn']",
Policy Gradient ( Advantage actor-critic) for multiple simultaneous continuous actions,"
i'm trying to solve a problem in which i need to carry out reinforcement learning with multiple simultaneous actions in continuous action space . i checked the multiagent structure; however, im trying to solve a problem in which there are difficulties to set up connection between the agents. for instance, they should take actions simultaneously so there is no way they can be aware of each other's actions. so i decided to go with the multivariate normal solution. has anybody tried that out ever?
first of all i have have difficulties finding the covariance matrix. since it has to be PSD so i decided to assume covariance is zero. something like:
covariance matrix = [[variance1  0][0  variance2]]
but its not everything. the agent doesn't seem to be learning. the problem to be solved by the agent is about resource allocation so the ""mean"" can not be negative then i decided to go with the ""RELU"" activation function for the neural network. surprisingly, mean is usually zero so as you can guess its updating the weights in a way to do nothing (negative mean). on the other hand, the variances are on the rise. Though i have checked it a million times there might be a flaw on the code of the environment there is no doubt. i just wanted to to make sure if its mathematically ok to go in this way ? i checked for papers and i found bunch of them but they don't seem to be enough. i would appreciate any guidance.
","['reinforcement-learning', 'policy-gradients', 'multi-agent-systems', 'continuous-action-spaces', 'advantage-actor-critic']",
Cnn for Combination of both digits and letters(small and capital) [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.







                        Improve this question
                    



Hi I am new to machine learning can anyone suggest open dataset consists of both digits and letters(small,capital)
I want images consisisting of both digits and letters to train my cnn model and make the model recognize the real time images
Can anyone please suggest me that dataset link
Thanks in advance
","['machine-learning', 'convolutional-neural-networks', 'image-recognition', 'datasets']","
As for me, the easiest path to what you are asking for is generating them yourself.
An Example
I usually grab some TTF fonts and put them into a directory so that I have variety for character identification.  Begin by importing dependencies and creating a generator function:
from PIL import Image, ImageDraw, ImageFont
from os import listdir

WIDTH       =   200
HEIGHT      =   100
MINX        =    20
MINY        =    20
MAXX        =    WIDTH-60
MAXY        =    HEIGHT-60
MINSIZE     =    24
MAXSIZE     =    48

fonts = [i for i in filter(lambda i:i[-3:]==""ttf"", listdir(""../data/fonts""))]

def generate_character_image():
    fonts = [i for i in filter(lambda i:i[-3:]==""ttf"", listdir(""../data/fonts""))]
    charset=list(""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"")
    while 1:
        img = Image.new('RGB', (WIDTH,HEIGHT), color = (255,255,255))
        font = ImageFont.truetype(f'../data/fonts/{fonts[randrange(len(fonts))]}', randrange(MINSIZE,MAXSIZE))
        canvas = ImageDraw.Draw(img)
        where = (randrange(MINX,MAXX), randrange(MINY,MAXY))
        character = charset[randrange(len(charset))]
        coords = canvas.textbbox(where, character, font)
        # The coordinates are the top left corner
        bounding_box = (coords[0], coords[1], coords[2]-coords[0], coords[3]-coords[1])
        #canvas.rectangle(coords, outline=0, width=1)
        canvas.text(where, character, font=font, fill=(0,0,0), anchor=""la"")
        yield (character, coords, img)

for i in range(5):
    character,coords,image = next(generate_captcha())
    print(f""{character} is in Bounding Box: {coords}"")
    display(image)
    print(image)

You can then use this to generate an infinite number of training samples pretty trivially.
Why This Might Be Good
Personally, I prefer to generate my datasets whenever possible.  It allows me to do the following:

Train without ever repeating a sample
The validation data changes every time (in fact, I'll often not bother with a validation step since every epoch uses different data)
Zero storage required

Hope this helps!
"
Predicting the the motion of a 3D object when the motion of a set of markers is known,"
trying to figure out where to get started with this:
I have a few hundred CT images where certain three-dimensional features in the image (anatomy) are moving in a correlated fashion with a set of radio-opaque markers. These anatomic features can rotate, translate and deform and the markers can move together or sometimes in relation to each other. The position and motion of the markers are indicative of the position and motion of the anatomy in which they are embedded in.
I'd like to develop a model whereby given the positions and motion of the markers I can then predict the position and motion of the full anatomy in which they are embedded and use this for segmentation.
What deep-learning software and techniques are suited for this type of problem?
Thanks!
","['machine-learning', 'deep-learning', 'image-processing', 'image-segmentation']",
"If $\gamma \in (0,1)$, what is the on-policy state distribution for episodic tasks?","
In Reinforcement Learning: An Introduction, section 9.2 (page 199), Sutton and Barto describe the on-policy distribution in episodic tasks, with $\gamma =1$, as being
\begin{equation}
\mu(s) = \frac{\eta(s)}{\sum_{k \in S} \eta(k)},
\end{equation}
where
\begin{equation}
\eta(s) = h(s) + \sum_s \eta(\bar{s}) \sum_a \pi(a|\bar{s})p(s|\bar{s},a), \text{  (9.2)}
\end{equation}
is the number of time steps spent, on average, in state $s$ in a single episode.
Another way to represent this is setting $\eta(s) = \sum_{t=0}^{\infty} d_{j,s}^{(t)}$, the average number of visits to $s$ starting from $j$, and $d_{j,s}^{(t)}$ being the probability of going from $j$ to $s$ in $t$ steps under policy $\pi_{\theta}$. In particular, $d_{j,s}^{(1)} = d_{j,s} = \sum_{a \in A}[\pi_{\theta}(a|j)P(s|j,a)]$. This formulation is obtained through pag 325 proof of the Policy Gradient Theorem (PGT) and some basic stochastic processes theory.
If instead of defining $\gamma = 1$, we prove PGT using any $\gamma \in (0,1)$, we would get
\begin{equation*}
\eta_{\gamma}(s) = \sum_{t=0}^{\infty} \gamma^t d_{j,s}^{(t)}
\end{equation*}
This is not anymore the average number of visits to $s$. Here comes my first question. Would we still get a $\mu_{\gamma}$ on-policy distribution through the same trick done before? That is
\begin{equation}
\mu_{\gamma}(s) = \frac{\eta_{\gamma}(s)}{\sum_{k \in S} \eta_{\gamma}(k)},
\end{equation}
would be the on-policy distribution?
My second question is related and has to do with the phrase on page 199, that says that

If there is discounting ($\gamma <1$) it should be treated as a form of termination, which can be done simply by including a factor of $\gamma$ in the second term of (9.2).

What the authors mean by ""as a form of termination""?
As inferred by my previous question, my conclusion is that having $\gamma < 1$ does not alter $\mu_{\gamma}$ being the on-policy distribution, so I don't get this last comment on page 199.
","['reinforcement-learning', 'policy-gradients', 'sutton-barto', 'on-policy-methods', 'discount-factor']",
Is it possible to design an AI with two inputs and a Boolean output?,"
I am having a difficult time explaining to my boss that what he is trying to achieve may not be possible or within reason. We have a database of 3 Million data points per computer across hundreds of machines and when any data point is updated, changed, or removed. Some of these data points are the number of times a computer has been logged in, the names of printers attach, folders on the root of the drive. Some of the data points we do care about, others like a printer being out of ink, we don't care about but the same method would return if the printer was offline which we do care about.
He wants to design an AI that would check these data points and return with true or false on whether the data point is significant when they are changed, removed or updated. We are storing the name of the method to retrieve the data, the current data, all previous data, and the time the change was made. I can not foresee a way to train the data efficiently as we currently don't know which methods retrieve significant data or which values are not significant within the method.
Is it possible to design such an AI without hours of supervisor learning?
",['training'],"
Yes... But...  you have a lot of hours.. possibly days or weeks.. of work before you are at that point.
Your bigger problem is apparent from your concerns in the second paragraph.  It doesn't sound as though your org has a solid grasp of the problem at the moment.  For that reason, it seems that some first steps are in order.
Data Exploration
Begin by collecting all of the data points.  I'd recommend that you perhaps begin with some statistical analysis of the data as a whole, including some basic visualization and generating a covariance matrix.  From there, begin to look at using some clustering methods to identify possible patterns.  Along the way, you will almost certainly go down the path of some dimensionality reduction, either via something like PCA or possibly identifying useless features.
Feature Selection
Based on the exploration work above you should now have a much better understanding of your data and relationships within it.  Based on this, it's time to start thinking about how to generate a model that produces the desired output.  Frankly, you may discover that something as simple as a random forest classification or even a clustering method such as DBSCAN can be used to initially train and then continuously fit your data, producing either a binary classification with the random forest or a yes/no cluster with the clustering technique.
Is More Required?
Of course, something more sophisticated might be required, but if it is you would know have a better handle on the problem and likely be able to intuitively generate a large dataset that could be used for a neural network.
Oh... And as a concluding thought... It might turn out that after all of this analysis the problem cannot be solved with the data that you have.  At that point you have to go back and see if there are other data points that could be gathered.
"
What is the difference between same convolution and full convolution in terms of feature map size?,"
In valid convolution, the size of the output shrinks at each layer. So after some point of time additional layers cannot meaningfully performs convolution. For this reason, same convolution is introduced, where where the size of the output remains intact. This is achieved by padding with enough number of zeroes at the borders of input image.
What happens to the size of output feature map in case of full convolution?
If it remains intact then what is the difference between same convolution and full convolution?
","['convolutional-neural-networks', 'convolution']","

What happens to the size of output feature map in case of full convolution?

It increases.

First one is valid padding: the blue square is not padded, so the green square is smaller.  Third one is same padding: the blue square is padded just enough so that the green square is the same size.  Fourth one is full padding: the blue square is padded as much as possible for that size of filter, so the green square is larger.
From here.
"
Unet Overfitting for binary segmentation of fake images,"
I am working on a project where I am trying to detect and localize forgeries in images. I am using the CASIA v2 dataset and using Unet model for the task. I have the binary masks of all the images in the CASIA v2 dataset. The metric I am using for the model are F1 score.
The issue with the model is that it is highly overfitting, the validation loss plateaus up.
Batch size is 128 and Learning rate is 0.000001. Image size is 128 x 128.
Updated graph for batch size 16 with the changes mentioned by @spb is as follows:

I have also tried using Learning rate scheduler to decrease the learning rate(starting with high learning rate) on plateaus but that didn't help much.
I am also using the package Albumentations for data augmentation of both the images and its masks. I load the images and the masks and then apply the augmentations and save the augmented images and masks in a separate arrays and finally extend the original images and masks with the augmented images and masks. So technically I have original plus the augmented images and masks that I use for training the model. The augmentations I am using are:
Augment = A.Compose([
A.VerticalFlip(p=0.5),
A.RandomRotate90(p=0.5),
A.HorizontalFlip(p = 0.5)
])

I have split the dataset into 70% Training, 20% Validation and 10% for testing.
Here is a snippet of my model. Updated Code below
def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):
""""""Function to add 2 convolutional layers with the parameters passed to it""""""
# first layer
x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\
          kernel_initializer = 'he_normal', padding = 'same')(input_tensor)
if batchnorm:
    x = BatchNormalization()(x)
x = Activation('relu')(x)

# second layer
x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\
          kernel_initializer = 'he_normal', padding = 'same')(input_tensor)
if batchnorm:
    x = BatchNormalization()(x)
x = Activation('relu')(x)

return x

def get_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True):
""""""Function to define the UNET Model""""""
# Contracting Path
c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)
p1 = MaxPooling2D((2, 2))(c1)
#p1 = Dropout(dropout)(p1)

c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)
p2 = MaxPooling2D((2, 2))(c2)
#p2 = Dropout(dropout)(p2)

c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)
p3 = MaxPooling2D((2, 2))(c3)
#p3 = Dropout(dropout)(p3)

c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)
p4 = MaxPooling2D((2, 2))(c4)
#p4 = Dropout(dropout)(p4)

c5 = conv2d_block(p4, n_filters * 16, kernel_size = 3, batchnorm = batchnorm)
p5 = MaxPooling2D((2, 2))(c5)
#p5 = Dropout(dropout)(p5)

c6 = conv2d_block(p5, n_filters = n_filters * 32, kernel_size = 3, batchnorm = batchnorm)

# Expansive Path
u7 = Conv2DTranspose(n_filters * 16, (3, 3), strides = (2, 2), padding = 'same')(c6)
u7 = concatenate([u7, c5])
u7 = Dropout(dropout)(u7)
c7 = conv2d_block(u7, n_filters * 16, kernel_size = 3, batchnorm = batchnorm)

u8 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c7)
u8 = concatenate([u8, c4])
u8 = Dropout(dropout)(u8)
c8 = conv2d_block(u8, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)

u9 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c8)
u9 = concatenate([u9, c3])
u9 = Dropout(dropout)(u9)
c9 = conv2d_block(u9, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)

u10 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c9)
u10 = concatenate([u10, c2])
u10 = Dropout(dropout)(u10)
c10 = conv2d_block(u10, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)

u11 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c10)
u11 = concatenate([u11, c1])
u11 = Dropout(dropout)(u11)
c11 = conv2d_block(u11, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)

outputs = Conv2D(1, (1, 1), activation='sigmoid')(c11)
model = Model(inputs=[input_img], outputs=[outputs])
return model

Currently I am not using the dropout as it leads to higher validation loss plateaus in my case.
The F1 score and  F1 loss I am calculating are as follows
def f1(y_true, y_pred):

y_pred = K.round(y_pred)
tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)
tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)
fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)
fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)

p = tp / (tp + fp + K.epsilon())
r = tp / (tp + fn + K.epsilon())

f1 = 2*p*r / (p+r+K.epsilon())
f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)
return K.mean(f1)

def f1_loss(y_true, y_pred):

tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)
tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)
fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)
fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)

p = tp / (tp + fp + K.epsilon())
r = tp / (tp + fn + K.epsilon())

f1 = 2*p*r / (p+r+K.epsilon())
f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)
return 1 - K.mean(f1)

I have also tried using other losses like focal_tversky but have a similar result.
What can be the issue  and how can I solve it?
Is it

Issue with my data like presence of outliers
Model related issue
Batch size and Learning rate related issue
Or anything else?

Please your help in this regard is really appreciated as I really need to solve it soon.
","['deep-learning', 'computer-vision', 'tensorflow', 'image-segmentation', 'u-net']","
Data augmentations is usually done on the fly during training, meaning before each you apply the random augmentation for the entire dataset, because of the randomness there will be different transformation of the same image in each epoch.
Shuffle the dataset before batching in each epoch, so that each epoch will not have minibatch of same images, which will reduce overfitting. Learning rate usually 1e-4 works fine for me.
Your UNet is not wide enough, why are you using only 16 filters in first conv block, original UNet paper had 64 filters in first conv block. Also you have only one convolution block in each layer, why? original unet has 2 conv blocks in each layer. I suggest you to try with unet given in here
https://github.com/zhixuhao/unet/blob/master/model.py
Dice loss is usually prefered for segmentation, check code here
from keras import backend as K

def dice_score(y_true, y_pred, smooth=1e-7):

    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)
    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)

def dice_loss(y_true, y_pred):
    return 1-dice_score(y_true, y_pred)

"
Is there literature on Neural Network with activation functions of bounded domain?,"
I think to have found a somewhat interesting connection between neural networks and another area of mathematics. However, it requires the activation functions in the network to have a bounded - ideally small - domain. For the sake of simplicity, I am restricting this to feedforward networks.
My approach has been the following: Assuming bounded input and weights, a maximal input can be derived. Before each application of an activation function, I thus just scale down the range from the maximal one to the one permitted.
This however causes nearly all weights that are not close (in absolute terms) to the maximal ones to lead to very small outputs of the network, meaning nearly all weight combinations lead to outputs near zero. The network thus has issues learning even simple tasks.
My question, therefore, is: Has anyone ever studied these issues and maybe found a network architecture that works well with this? Or another solution for bounded domains?
","['neural-networks', 'machine-learning', 'reference-request', 'activation-functions']",
What would be the reason behind using plots (such as box-plots or histograms) for ML development?,"
I've been learning Python machine-learning using this project report and the guy who wrote it begins by visualizing his data using various statistical analysis methods: histograms, density plots, box plots, scatter plots, etc.
The problem is that he doesn't explain what this is for. The only detail he provides is that ""univariate plots help to understand each attribute"" and ""multivariate plots help to understand the relationships between attributes.""
What would be the reason behind using these plots for ML development? Do they help you to determine which algorithm(s) you should try? If so, how? Can anyone explain the main points or maybe point me to a resource that will help?
","['machine-learning', 'python', 'statistical-ai', 'model-based-methods', 'statistics']","
In addition to this answer and given that you were also looking for a resource, I suggest that you read chapter 1 of the book An Introduction to Statistical Learning: with Applications in R, where you can find multiple examples of these plots and explanations of why they can be useful: to understand the relationship between the features and the target variable, which you want to predict.
"
Is the existence and uniqueness of the state-value function for $\gamma < 1$ theoretical?,"
Consider the following statement from 4.1 Policy Evaluation of the first edition of Sutton and Barto's book.

The existence and uniqueness of $V^{\pi}$ are guaranteed as long as
either $\gamma  < 1$ or eventual termination is guaranteed from all
states under the policy $\pi$.

I have a doubt on the first condition $\gamma < 1$. If $\gamma < 1$, then it makes our task easy in a way that the $\gamma^k$ becomes zero for sufficiently higher $k$ and it is totally based on the hardware architecture. But, in theory, $\gamma^k$ can never be zero. It may approach zero.
In this context, how can the condition $\gamma < 1$ assure the existence and uniqueness of $V^{\pi}$ theoretically?
","['reinforcement-learning', 'proofs', 'value-functions', 'sutton-barto', 'policy-evaluation']",
Why can't cognitive architectures achieve general intelligence?,"
Newbie here.
I recently read about cognitive architectures (see: https://en.wikipedia.org/wiki/Cognitive_architecture). They are supposed to be modeled after the human mind and represent a promising approach towards artificial general intelligence (AGI).
My question is, however, why haven't these cognitive architectures achieved AGI yet? What are the specific limitations and roadblocks that cognitive architectures face?
","['agi', 'cognitive-science', 'cognitive-architecture']","
This is a philosophical question that does not have a one-off answer.
If I may suggest a quick thought:
Our brains are trained on millions of tasks when we grow up (recognising so many objects, emotions, actions, etc.), that if we were to make one giant neural network, and we train them on so many tasks, perhaps we can achieve a human like response.
As for 'consciousness'. Do any of us really know how that works? Is that due to the neural networks in our brain or something more intangible (that spark of life)? I'm not so sure of it, but it surely seems more complex than any of the deep learning models I've seen.
"
Is there a name for this approach to evolutionary algorithms?,"
I am considering an approach to evolutionary algorithms, in which instead of maintaining a population of individuals, we maintain a pool of $N$ mutations that can be applied to a base genome.  For every possible subset (or many possible subsets) of the mutation pool, we apply that subset to the base genome to produce an individual, then test that individual.  We discard mutations for which the population with that mutation performs worse than the population without it.  We merge the best-performing mutations into the base genome for the next generation.  Then we replace the discarded or merged mutations with new ones.
Is this known under some name?  Is it a good idea?
","['terminology', 'evolutionary-algorithms']","
It's just a genetic algorithm, only your population is a set of instructions that generate a subject to be tested. There are minor differences in the way you apply multiple mutations, so you evaluate groups of them instead of each individually, but for the scoring you effectively single out the commonalities among the worse individuals.
Hard to tell whether it's a good idea; discarding poor mutations based on them being part of a sub-optimal individual might not be as straight forward as it sounds. And where do you get new mutations from?
"
What language is the GPT-3 engine written in?,"
I know that the API is python based, but what's the gpt-3 engine written in mostly? C? C++? I'm having some trouble finding this info.
","['natural-language-processing', 'programming-languages', 'c++', 'gpt-3', 'c']",
Neural Network for Picking Parameters of a Nonlinear Function to Data Points,"
I'm trying to make a neural network in pytorch that picks the parameters of a nonlinear function, the radius and (x,y) center of a circle in the example below, based on a sample of values from the nonlinear function.
More concretely, the neural network trained in the code below takes as input 100 (x,y) points on a circle and outputs radius, x_center, y_center of the circle.
I don't consider this a very difficult problem, but the trained neural network doesn't work very well, as you can see from two example plots after the code. How can the code be improved?
And in case this informs your recommendation, the goal is not to fit circles, which no one needs a neural network to do. I'm trying to use a neural network to calculate 9 parameters in a nonlinear function taking a single real valued input and outputting a complex number f(t) -> a + b*sqrt(-1). The input into the neural network is 54 complex values, and the output is 9 parameter values. I am guaranteed that the 54 complex input values can always be well approximated by f(t) with an appropriately picked 9 parameters. The parameters can easily be guessed by a human because different parameters intuitively change the shape of the complex curve, but I've been unable to use a minimization math algorithm for curve fitting. The problem is there are a lot of local minima the minimization algorithms can encounter before reaching the global minimum. The goal of the neural network is to get a good guess of the 9 parameters at the global minimum for a minimization math algorithm to be close to the global minimum initially, and thus converge to the global minimum rather than get stuck at a local minima.
You probably guessed that I know a bit of math, but I don't know much about machine learning. I was able to pick it up pretty quickly because of my math background, but I am severely lacking in practical experience. I don't know what to do at this point other than randomly changing the number of samples on a circle, number of examples circles, adding more layers to the neural network, adding different types of layers to the neural network, changing the loss function, changing the learning rate, changing the optimizer, changing the loss function, et cetera, but I have no method to my madness.
Post Script
I've found someone who did almost what I need. This paper paired with this github repo used 1,000 samples in a set of 100,000 with 1% failure rate, so there's hope. I have to dig deeper for the innards of their neural network training.
import torch
import numpy as np
import math
import matplotlib.pyplot as plt

#circle parameterized by t, < x(t) , y(t) >
t_parameter = np.linspace(-math.pi, math.pi, 100)

#create random radius,(x,y) center or circle paired with points on circle evaluated at all t in t_parameter
examples = 1000
max_radius = 4
random_rxy = np.random.rand(examples,3)
input_list = []
for i in range(examples):
  r_rand = random_rxy.item(i,0) * max_radius
  x_rand = random_rxy.item(i,1) * 7 - 2 #-2 < x_rand < 5 
  y_rand = random_rxy.item(i,2) - 2 #-2 < y_rand < -1
  x_coordinates = [r_rand*math.cos(t) + x_rand for t in t_parameter]
  y_coordinates = [r_rand*math.sin(t) + y_rand for t in t_parameter]
  input_list.append(x_coordinates + y_coordinates)
input_tensor = torch.Tensor(input_list)
output_tensor = torch.Tensor(random_rxy)

print(input_tensor)
'''
tensor([[ x_0_0,   x_0_1,   ..., x_0_99,   y_0_0,   y_0_1,   ..., y_0_99   ],
        [ x_1_0,   x_1_1,   ..., x_1_99,   y_1_0,   y_1_1,   ..., y_1_99   ],
        [ x_2_0,   x_2_1,   ..., x_2_99,   y_2_0,   y_2_1,   ..., y_2_99   ],
        ...,
        [ x_997_0, x_997_1, ..., x_997_99, y_997_0, y_997_1, ..., y_997_99 ],
        [ x_998_0, x_998_1, ..., x_998_99, y_998_0, y_998_1, ..., y_998_99 ],
        [ x_999_0, x_999_1, ..., x_999_99, y_999_0, y_999_1, ..., y_999_99 ]])
'''
print(output_tensor) #radious, x circle center, y circle center
'''
tensor([[r_0,   x_0,   y_0  ],
        [r_1,   x_1,   y_1  ],
        [r_2,   x_2,   y_2  ],
        ...,
        [r_997, x_997, y_997],
        [r_998, x_998, y_998],
        [r_999, x_999, y_999]])
'''

#define model and loss function.
model = torch.nn.Sequential(
  torch.nn.Linear(200, 200),
  torch.nn.Tanh(),
  torch.nn.Tanh(),
  torch.nn.Linear(200, 3)
)
loss_fn = torch.nn.MSELoss(reduction='mean')

#train model
learning_rate = 1e-4
optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)
for t in range(10000):
  # Forward pass: compute predicted y by passing x to the model.
  output_pred = model(input_tensor)

  # Compute and print loss.
  loss = loss_fn(output_pred, output_tensor)
  if t % 100 == 99:
    print(t, loss.item())
    '''
    99   0.0337635762989521
    199  0.0285916980355978
    299  0.025961756706237793
    399  0.024196302518248558
    499  0.022839149460196495
    ...
    9799 0.004136151168495417
    9899 0.0040830159559845924
    9999 0.004030808340758085
    '''
  
  #typical procedure
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

print(output_tensor[0].tolist())
print(output_pred[0].tolist())
#[0.7722834348678589, 0.46600303053855896, 0.5080233812332153 ]
#[0.7921068072319031, 0.46946045756340027, 0.49222415685653687]

plt.xlabel('x')
plt.ylabel('y')
r_rand, x_rand, y_rand = output_tensor[0].tolist()
plt.scatter([r_rand*math.cos(t) + x_rand for t in t_parameter],[r_rand*math.sin(t) + y_rand for t in t_parameter],label=""Measured Data"")
r_rand, x_rand, y_rand = output_pred[0].tolist()
plt.scatter([r_rand*math.cos(t) + x_rand for t in t_parameter],[r_rand*math.sin(t) + y_rand for t in t_parameter],label=""Fit Data"")
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()



","['neural-networks', 'non-linear-regression', 'curve-fitting']",
Data augmentation for very small image datasets,"
I am looking for techniques for augmenting very small image datasets. I have a classification problem with 3 classes. Each class consists of 20 different shapes. The shapes are similar between the classes, but the task is to identify which class the shapes belong to. Per shape, I have between 1 and 35 training examples. For two classes, I have 25 training examples per shape, but the number of examples per shape for the third classes is usually around 5. Now, what data augmentation schemes do you recommend? Geometric / affine transformations seem like a good place to start. However, I have also thought of applying Fast Fourier Transform (do the forward transform, add some noise, do the inverse transform). GANs seem infeasible, right? Not enough data, I suspect. In any case, I am grateful for your advice.
","['image-generation', 'data-augmentation']",
How would you intuitively but rigorously explain what the VC dimension is?,"
The VC dimension is a very important concept in computational/statistical learning theory. However, the first time you read its definition, you may not immediately understand what it really represents or means, as it involves other concepts, such as shattering, hypothesis class, learning and sets. For example, let's take a look at the definition given by Shai Shalev-Shwartz and Shai Ben-David (p. 70)

DEFINITION $6.5$ (VC-dimension) The VC-dimension of a hypothesis class $\mathcal{H}$, denoted $\operatorname{VCdim}(\mathcal{H})$, is the maximal size of a set $C \subset \mathcal{X}$ that can be shattered by $\mathcal{H}$. If $\mathcal{H}$ can shatter sets of arbitrarily large size we say that $\mathcal{H}$ has infinite VC-dimension.

Without knowing what a hypothesis class is, or what the specific $C$, $X$ and $H$ in this definition are, it's difficult to understand this definition. Even if you are familiar with what a hypothesis class is (i.e. a set of sets, i.e. our set of functions/hypotheses/models, e.g. the set of all possible neural networks with a specific topology) and you know that $C$ and $X$ are sets of input points, it should still not be clear what the VC dimesion really is or represents.
So, how would you intuitively and rigorously explain the exact definition of the VC dimension?
Note that I am not asking for answers like

The VC dimension represents the complexity (or expressive power, richness, or flexibility) of your model/hypothesis class.

Of course, this is easy to memorize, but it's quite vague. So, I am not looking for vague/general answers. I am looking for answers that rigorously but intuitively describe the mathematical definition of the VC dimension. For example, you could provide an illustration that shows what the VC dimension is, and, in your example (e.g. the XOR problem cannot be solved by a set of lines), you can describe what $H$, $C$, and $X$ are, and how they relate to the typical concepts you will find an introductory course to machine learning, but you should not forget to describe the concept of shattering. If you have other ideas of how to illustrate this concept memnomically, feel free to provide an answer.
","['machine-learning', 'definitions', 'computational-learning-theory', 'vc-dimension']","
Trying to explain the idea of VC to some of my colleagues I've discovered quite an intuitive way of laying out the basic idea. Without going through lots of math and notation as I've done in my other answer.
Imagine a following game between two players $\alpha$ and $\beta$ :

First, player $\alpha$ plots $d=4$ points on a piece of paper. She may place the points however she likes.
Next, player $\beta$ marks several of the drawn points.
Finally, player $\alpha$ should draw a circle such that all the marked points are inside the circle, and all the unmarked points - outside. (Points on the boundary considered ""inside"".)

The player $\alpha$ wins if she can draw such a circle at step #3. The player $\beta$ wins if making such circle is impossible.
If you try to analyze this game then you'll notice that the player $\beta$ has a winning strategy. For any $d=4$ points on a plane, there is always a subset such that player $\alpha$ is unable to draw a required circle. ( I don't want to go into the detailed proof of the strategy - it is straightforward, but cumbersome - I've sketched it my other answer). If we now change the number of points to $d=3$ then the game suddenly becomes winnable by player $\alpha$ - for three points that are not on the same line any subset can be separated by a circle.
The largest number $d$ at which the game is winnable by player $\alpha$ is called the VC dimension of our classification set. So, in the case of 2D discs (insides of a circle) the VC dimension is 3. If one changes the rules to use rectangles instead of circles, then the maximum number of points winnable by $\alpha$ would be 4 - thus, the VC dimension of rectangular classification sets is 4.
Restoring our the mathematical notation, we denote $\mathcal{X} = \mathbb{R}^2$ our two-dimensional plane. $C$ is the subset of cardinality $|C| = d$ that the player $\alpha$ selects. And $\mathcal{H}$ is a class (a ""set-of-sets"") of the subsets of $\mathcal{X}$ that one should use as a classification boundary.  Formally, the statement that the game above is winnable by $\alpha$  can be written as:
$$ \exists\;C\subset\mathcal{X}.|C|=d \Rightarrow\forall\; A\subset C\; \exists\; B\in\mathcal{H}\;. A = B\,\cap\,A  $$
The maximal $d$ at which this statement is true would be the VC dimension. (I actually worked backwards from noticing the alternating quantifiers $\exists\,\forall\,\exists$ in the VC definition - which is typical in game playing, so I worked back from the definition to make the game above.)
"
How to train a machine learning model with multiple attributes and one target value?,"
I'm working on a machine learning problem where I need to guess which customers will churn and which of them will continue to be customers.
I have $X_0, X_1, X_2, X_3, X_4, X_5$ and $X_6$ attributes representing if they have credit cards, if they are active customers, if they have money in their accounts, etc. So, according to these multiple $X$ values and the target value $Y$, which is either $0$ or $1$, I need to develop a model that can do the prediction.
I have always worked with only one $X$ attribute and one target value $Y$. Right now, I'm confused about how I should work with multiple $X_n$ values.
Any help is appreciated.
","['machine-learning', 'prediction', 'features', 'binary-classification']",
Is there a technique for analyzing the relationship between time-series clusters?,"
I have two time-series datasets (temperature and speed of the vehicle). I will use Agglomerative Hierarchical Clustering and DTW to cluster both datasets.
I am looking for a technique (like regression model) to find the relationship between two time-series clustered data. I am curious to find the relationship between changes in temperature and vehicle speed. Does anyone have an idea?
","['machine-learning', 'python', 'time-series', 'clustering', 'algorithm-request']",
Why is the number of examined nodes $ O(b^{3d/4})$ in $\alpha$-$\beta$ pruning?,"
I'm taking a course 'Introduction to AI' and, in one of the tutorials, it was written that when pruning the game tree using $\alpha$-$\beta$ boundaries, the number of nodes that will be developed, when using random sort function for the children (i.e., in the average case) will be $$ O(b^{3d/4})$$.
Since the proof is not in the scope of the course we weren't given one at the tutorial, so I tried looking for the proof online, but couldn't find anything, and I didn't think of anything myself. I wondered whether someone could give me a lead or refer me to some reading material that shows the full proof?
","['game-ai', 'proofs', 'minimax', 'multi-agent-systems', 'alpha-beta-pruning']",
Is there any thumb rule on the cardinality of state space in order to use the parameterized function to estimate value functions?,"
Value functions for a given MDP can be learned in at least two ways by experience.

The first way (tabular calculation) is generally used in the case of state spaces that are small enough.

The second way (using parameterized functions) is generally used in the case of large state paces.


It can be understood from the following statement from section 3.7 of the first edition of Sutton and Barto's book.

The value functions $V^{\pi}$ and $Q^{\pi}$ can be estimated from experience. For example,
if an agent follows policy $\pi$ and maintains an average, for each state
encountered, of the actual returns that have followed that state, then
the average will converge to the state's value,  $V^{\pi}$(s), as the number of
times that state is encountered approaches infinity. If separate
averages are kept for each action taken in a state, then these
averages will similarly converge to the action values,  $Q^{\pi}(s,a)$ . We call
estimation methods of this kind Monte Carlo methods because they
involve averaging over many random samples of actual returns. Of course, if there are
very many states, then it may not be practical to keep separate
averages for each state individually. Instead, the agent would have to
maintain  $V^{\pi}$ and  $Q^{\pi}$ as parameterized functions and adjust the parameters to
better match the observed returns. This can also produce accurate
estimates, although much depends on the nature of the parameterized
function approximator.

Is there any thumb rule or strict margin, in the literature, on the cardinality of state space to use the parameterized function to estimate value functions?
","['reinforcement-learning', 'deep-rl', 'value-functions', 'function-approximation', 'state-spaces']",
"In practice, are perceptrons typically implemented as objects?","
I'm fairly new to ANNs. I know the general structure, the math, and the algorithms behind them. I figured the logical next step on my journey to fully understanding them would to be implement one myself from scratch, even if it's a fairly small one.
So I'm curious, coming from those who actually work with and deploy these things, are perceptrons/neurons typically implemented as objects with class variables, methods, etc. (kind of like nodes in a Linked List)? Or is there a more practical/memory-conservative way to build them?
","['neural-networks', 'implementation', 'artificial-neuron', 'perceptron']",
Under what circumstances is a fully connected layer similar to PCA?,"
I am reading this paper on image retrieval where the goal is to train a network that produces highly discriminative descriptors (aka embeddings) for input images. If you are familiar with facial recognition architectures, it is similar in that the network is trained with matching / non-matching pairs and triplet loss.
The paper discusses the use of PCA and whitening on the training set of descriptors as a means of further improving the discriminability (second to last block in image below, fig 1a of paper). This all make sense to me.

Where I'm confused is where they replace PCA/whitening with a trainable fully connected layer with bias. I do understand that PCA+whitening is just the composition of of two linear transformations (ie rotation + (un)squishing in each dimension) and that these are the same as having one linear transformation but:

How is PCA+whitening equivalent to a learnable fully connected layer? Is there some theorem or paper explaining that training a fully connected layer with triplet loss is somehow statistically equivalent to PCA and whitening?
Why is there a bias?

","['architecture', 'facial-recognition', 'principal-component-analysis', 'content-based-image-retrieval']",
What is a model lineage?,"
We know about the lineage of datasets. Is there anything called ""(ML) model lineage"". What are all the works that had been remarkable regarding ""model lineage""?
There are few links available on internet which talk about model lineage. According to one of the articles[1], the definition of model lineage is as follows:

Model Lineage keeps the history of a model: when it was trained, using which data, algorithms, and parameters. This should be automatically generated each time a new version of a model is generated.

The next reference that I could find on the internet is the session by data bricks[2].
Apart from these two links, I could not find many resources or standards regarding model lineage. It will be helpful if anyone could provide more resources or pointers towards this topic.

https://blog.tail.digital/en/we-need-to-talk-about-model-lineage/#:~:text=Model%20Lineage%20keeps%20the%20history,of%20a%20model%20is%20generated

https://databricks.com/session_na20/machine-learning-data-lineage-with-mlflow-and-delta-lake


",['models'],
