Head,Body,Tags,First Answer
"Formally, what are the layers in an Artificial Neural Network?","
You may not believe it, but I am an ANN expert. Perhaps, for that reason, I am unable to grasp completely what the layers are in a Deep Forward Artificial Neural Network (DFANN).
According to the Deep Learning ""bible"", p. 164 ""the model is associated with a"" directed acyclic graph (DAG) ""describing how the functions are composed together"". Then it states that one of these functions $f^{(i)}$ is a layer. We know that an ANN is composed of neurons/units (the biological perspective) and a DAG is composed of nodes/vertices and edges/links. A trivial mapping can be established between (neural) units and nodes, and (neural) connections and edges (see also Fig. 6.2 in p. 170). However, defining the layers in a DAG is not that trivial, I would say.
In a quick internet search, I came across about the concept of graph drawing and layering algorithms. This concept is never mentioned in the ANN literature. If I understood well, formally, we would need such an algorithm to be able to define the layers in the DAG.
A more intuitive description is given by AIMA: DFANN ""are usually arranged in layers, such that each unit receives input only from units in the immediately preceding layer"". That sounds indeed like a layering algorithm.
So, formally, what are the layers in an Artificial Neural Network?
As a side note, a practical argument is given by https://cs231n.github.io/neural-networks-1/

One of the primary reasons that Neural Networks are organized into layers is that this structure makes it very simple and efficient to evaluate Neural Networks using matrix vector operations.

That's nice, but only from a practical perspective.
An interesting remark has been made by Chillston (see comments, I quote):
That's because an FFN is not a general DAG but a very regular kind of DAG, right.
I also thought that could be the case. But Bishop, 1995, p. 116 contradicts us:

More generally we can consider arbitrary network diagrams (not
necessarily having a simple layered structure) since any network
diagram can be converted into its corresponding mapping function. The
only restriction is that the diagram must be feed-forward, so that it
contains no feedback loops.

As I see it, according to Bishop, any (general) DAG is indeed an ANN.
","['neural-networks', 'deep-learning', 'definitions', 'deep-neural-networks', 'layers']","
In a practical point of view, a ""layer"" in a neural network is a step in computation which only depends on its preceding layers' outputs. Note that layer $n$ might take inputs from any previous layers, not only from $n-1$. Those are called skip connections. But since you are an expert, I assume you know all this already.
Usually NNs are defined as a stack of layers, which define a DAG. But given a DAG, there are several valid layer-by-layer interpretations of its structure. But in general, I don't think it is important to know whether a given node is from layer $i$ or $j$. May I ask, what is the motivation behind this question?
"
MCTS: How to select children when none of them are visited?,"
I am trying to implement MCTS for a custom word game I am working on. I feel like I have got all the pieces of code needed, but the algorithm seems to always return the first available move (first child of provided root note). I feel like this has something to do with the selection phase, as none of the children will have any visits after being created. Below is some of my code used in my implementation:
Node class containing game state, children/parent, visits/wins and functions for MCTS phases:
class MCTSNode {
    // Node stuff
    parent: MCTSNode | null;
    children: MCTSNode[];

    // Gamestate stuff
    /*
        I chose not to include this here as it is not important for the problem
    */

    // Stats
    visits: number;
    wins: number;

    constructor(parent: MCTSNode | null) {
        /*
            Here I set all the gamestate stuff
        */
        this.parent = parent;
        this.children = [];
        this.visits = 0;
        this.wins = 0;
    }

    private getAvailablePlays(): Play[] {
        /*
            Returns an array of legal plays
        */
    }

    private play(play: Play): MCTSNode {
        /*
            Returns the resulting child node if a certain play is applied to the current node
        */
    }

    private getUCB1(biasParam: number = 2) {
        return this.wins / this.visits + Math.sqrt((biasParam * Math.log(this.parent!.visits)) / this.visits);
    }

    private winner() {
        /*
            Returns winner of current node(game state) 
            null if game is not finished yet
            """" if game is draw
            ""ai"" if ai wins, ""player"" if player wins
        */
    }

    select(): MCTSNode {
        // implement logic to select a child node based on UCB1
        let node: MCTSNode = this;
        while (node.children.length > 0) {
            let bestNode = node.children[0];
            let bestUCB1 = -Infinity;
            for (let child of node.children) {
                let childUCB1 = child.getUCB1();
                if (childUCB1 > bestUCB1) {
                    bestNode = child;
                    bestUCB1 = childUCB1;
                }
            }
            node = bestNode!;
        }
        return node;
    }

    expand() {
        // Expand current node by adding all available child states(nodes) to children
        const availablePlays = this.getAvailablePlays();
        for (const play of availablePlays) {
            const child = this.play(play);
            this.children.push(child);
        }
    }

    simulate() {
        // implement logic to simulate a game from this node and return the score
        let currNode: MCTSNode = this;
        let winner = currNode.winner();

        while (winner === null) {
            let plays = currNode.getAvailablePlays();
            let play = plays[Math.floor(Math.random() * plays.length)];
            currNode = currNode.play(play);
            winner = currNode.winner();
        }
        return winner;
    }

    backpropagate(winner: string) {
        // implement logic to backpropagate the score to the parent nodes
        let currNode: MCTSNode | null = this;
        while (currNode !== null) {
            currNode.visits += 1;
            // Parent's choice (currNode.turn is either ""ai"" or ""player"")
            if (currNode.turn === winner) {
                currNode.wins += 1;
            }
            currNode = currNode.parent;
        }
    }
}


MCTS Function that should return the most promising child node of root based on robust or max policy:
const mcts = (root: MCTSNode, rounds: number = 100, policy: string = ""robust"") => {
    for (let i = 0; i < rounds; i++) {
        let node = root.select();

        node.expand();

        const winner = node.simulate();

        node.backpropagate(winner);
    }

    let max = 0;
    let bestNode: MCTSNode | null = null;
    for (let child of root.children) {
        if (policy === ""robust"" && child.visits > max) {
            max = child.visits;
            bestNode = child;
        }
        if (policy === ""max"" && child.wins > max) {
            max = child.wins;
            bestNode = child;
        }
    }
    return bestNode;
};

I am mainly wondering if the select and mcts functions are implemented correctly. Furthermore I am currently storing the game state in each node. Is this a bad idea with regards to efficiency/memory?
","['monte-carlo-tree-search', 'upper-confidence-bound', 'javascript']","
Unexplored children
If a node has any unexplored children you have to select one of them, before computing UCB values and selecting the best one. Alternatively you could return +inf as the UCB value for nodes without any visits.
If you want to completely remove any bias towards the first move, you can also consider selecting a random unexplored one, instead of the first one. This is not that important though.
Storing the game state
For the game state you have two options:

store the game state in each node
compute the game state from scratch each time while selecting down the tree

This is a memory/compute tradeoff, depending on how large the board representation is and how expensive playing a move are both of them can make sense.
"
How to interpret CNN output image? CNN expressive enough for particle positions?,"
Let us suppose we have a squared grid and some particles (active vertices) that are distributed on it.
We can construct a 2D image, where basically the input image is a matrix with 0s (no particle) and 1s (particle present). This corresponds to a sampled configuration $n$.
I need to calculate some terms $f(n)_i$ that takes as input all the positions/configuration $n$ and are associated to the particle $i$.
An important conditions is that it needs to respect translation equivariance.
My first attempt is to use a Convolutional layer CNN:

My questions are:
1.) How to interpret the output image Conv(n)? Are these probability distributions that represent the occupation probability of the particles on site $i$ ?
2.) Are CNN expressive enough that I can simply extract the $f(n)_i = CNN(n)_i$ from the output image Conv(n) or do I need to add another layer, e.g. a max pooling layer where I will extract the maximal values (in total extract 3 values for 3 particles).
","['machine-learning', 'convolutional-neural-networks', 'pooling', 'positional-encoding', 'max-pooling']",
How to pass the pixels of the image to the neural network with genann?,"
I'm using the genann library https://github.com/codeplea/genann
for neural networks, I have used this library mainly because it is
very easy to integrate with my c/c++ projects in dev-c++ as it is just
an .h file without dependencies and it is done with ""c"" which I think is easier
for me to port to other systems.
I am trying to do an OCR so that the neural network tells me if a
image is a specific letter, but I can't make the neural network
understand what I want to do, could you give me a simple example of how
pass the pixel data of the images to the network with this library.
update:
use this library to load the images https://github.com/nothings/stb i am using the dev-c++ IDE image 0.jpg is a letter ""A"" drawn with paint, image 1.jpg AND 3.jpg is the letter ""B"" but a little different from each other.
this is what i tried, excuse some parts of my
code are in spanish:
#include <windows.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include ""genann.h""
#define STB_IMAGE_IMPLEMENTATION
#include ""stb_image.h""
#include <string.h>
#include <string>
#include <map>
#include <vector>
using namespace std;

struct IMAGEN {
    int ancho;
    int alto;
    int canales; // r,g,b,a
    vector<int> pixeles;
};

IMAGEN letraA;
IMAGEN letraB;
IMAGEN letraB2;

bool cargarImagen(char *ruta,IMAGEN &v) {
    
    int x,y,n;
    unsigned char *data = stbi_load(ruta, &x, &y, &n, 0);
    
    if (data) {
        
        v.ancho = x;
        v.alto = y;
        v.canales = n;
        int total = x*y*n;
        
        for(int i = 0; i < total; i++) {
            v.pixeles.push_back(data[i]);
        }
        
        stbi_image_free(data);
        return true;        
        
    }else{
        stbi_image_free(data);
        return false;
    }   
}

void pintarImagen(HWND &hwnd,IMAGEN &v,float x,float y) {
    
  PAINTSTRUCT ps;
  RECT r;

  HDC hdc = BeginPaint(hwnd, &ps);

  GetClientRect(hwnd, &r);

    for(int yy = 0; yy < v.alto; yy++) {
        
        for(int xx = 0; xx < v.ancho; xx++) {
        
            int r = v.pixeles[yy*v.ancho*v.canales+xx*v.canales+0];
            int g = v.pixeles[yy*v.ancho*v.canales+xx*v.canales+1];
            int b = v.pixeles[yy*v.ancho*v.canales+xx*v.canales+2];
            
            SetPixel(hdc, x+xx, y+yy, RGB(r, g, b));
        
        }
    }
    
    EndPaint(hwnd, &ps);
}

int prepararInputs(double* &inputs,IMAGEN &letraA) {
                
                inputs = new double [letraA.alto*letraA.ancho*letraA.canales];
                
                for(int yy = 0; yy < letraA.alto; yy++) {
                    
                    for(int xx = 0; xx < letraA.ancho; xx++) {
                    
                        double r = letraA.pixeles[yy*letraA.ancho*letraA.canales+xx*letraA.canales+0] / 255;
                        double g = letraA.pixeles[yy*letraA.ancho*letraA.canales+xx*letraA.canales+1] / 255;
                        double b = letraA.pixeles[yy*letraA.ancho*letraA.canales+xx*letraA.canales+2] / 255;
                        
                        inputs[yy*letraA.ancho*letraA.canales+xx*letraA.canales+0] = r;
                        inputs[yy*letraA.ancho*letraA.canales+xx*letraA.canales+1] = g;
                        inputs[yy*letraA.ancho*letraA.canales+xx*letraA.canales+2] = b;
                        
                    }
                }
                
    return letraA.ancho;
}

/* This is where all the input to the window goes to */
LRESULT CALLBACK WndProc(HWND hwnd, UINT Message, WPARAM wParam, LPARAM lParam) {
    switch(Message) {
        case WM_PAINT:
        {
             //pintarImagen(hwnd,letraA,50,50);
              pintarImagen(hwnd,letraB2,100,250);
        }       
        break;
        case WM_CREATE: 
        {
            
            if( cargarImagen((char*)""0.jpg"",letraA) == false ) {
                printf(""0.jpg no cargo.\n"");
            }
            
            if( cargarImagen((char*)""1.jpg"",letraB) == false ) {
                printf(""1.jpg no cargo.\n"");
            }
            
            if( cargarImagen((char*)""3.jpg"",letraB2) == false ) {
                printf(""3.jpg no cargo.\n"");
            }
            
                printf(""GENANN example 1.\n"");
                printf(""Train a small ANN to the XOR function using backpropagation.\n"");
            
                /* This will make the neural network initialize differently each run. */
                /* If you don't get a good result, try again for a different result. */
                srand(time(0));
                
                int CapasOcultas = letraA.ancho*2;
                int Neuronas = 100;
                

                const double output[][2] = { // a,b
                    {1,0}, 
                    {0,1}  
                };
                int i;
            
                double *LETRA_A_INPUT = NULL;
                prepararInputs(LETRA_A_INPUT,letraA);
                
                double *LETRA_B_INPUT = NULL;
                prepararInputs(LETRA_B_INPUT,letraB);  
            
                double *LETRA_B2_INPUT = NULL;
                prepararInputs(LETRA_B2_INPUT,letraB2);  
                /* New network with 2 inputs,
                 * 1 hidden layer of 2 neurons,
                 * and 1 output. */
                genann *ann = genann_init(letraA.pixeles.size(), CapasOcultas, Neuronas, 2);
                
                /* Train on the four labeled data points many times. */
                
                for (i = 0; i < 500; ++i) {
                    genann_train(ann, LETRA_A_INPUT, output[0], 3);
                    genann_train(ann, LETRA_B_INPUT, output[1], 3);

                }
            
                /* Run the network and see what it predicts. */
                
                double *r = NULL;
                prepararInputs(r,letraB2);
                
                double *resultado = (double*)genann_run(ann, r);
                
                printf(""Output for Letra A is [%1.f,%1.f].\n"", resultado[0], resultado[1] );
            
                genann_free(ann);
                
                if(r!=NULL) {
                    delete[] r;
                    r = NULL;
                }
                
                if(LETRA_A_INPUT!=NULL) {
                    delete[] LETRA_A_INPUT;
                    LETRA_A_INPUT = NULL;
                }
                
                if(LETRA_B_INPUT!=NULL) {
                    delete[] LETRA_B_INPUT;
                    LETRA_B_INPUT = NULL;
                }
                
                if(LETRA_B2_INPUT!=NULL) {
                    delete[] LETRA_B2_INPUT;
                    LETRA_B2_INPUT = NULL;
                }
                
        }
        break;
        case WM_DESTROY: {
            PostQuitMessage(0);
            break;
        }
        default:
            return DefWindowProc(hwnd, Message, wParam, lParam);
    }
    return 0;
}

/* The 'main' function of Win32 GUI programs: this is where execution starts */
int WINAPI WinMain(HINSTANCE hInstance, HINSTANCE hPrevInstance, LPSTR lpCmdLine, int nCmdShow) {
    WNDCLASSEX wc; /* A properties struct of our window */
    HWND hwnd; /* A 'HANDLE', hence the H, or a pointer to our window */
    MSG msg; /* A temporary location for all messages */

    /* zero out the struct and set the stuff we want to modify */
    memset(&wc,0,sizeof(wc));
    wc.cbSize        = sizeof(WNDCLASSEX);
    wc.lpfnWndProc   = WndProc; /* This is where we will send messages to */
    wc.hInstance     = hInstance;
    wc.hCursor       = LoadCursor(NULL, IDC_ARROW);
    
    /* White, COLOR_WINDOW is just a #define for a system color, try Ctrl+Clicking it */
    wc.hbrBackground = (HBRUSH)(COLOR_WINDOW+1);
    wc.lpszClassName = ""WindowClass"";
    wc.hIcon         = LoadIcon(NULL, IDI_APPLICATION); /* Load a standard icon */
    wc.hIconSm       = LoadIcon(NULL, IDI_APPLICATION); /* use the name ""A"" to use the project icon */

    if(!RegisterClassEx(&wc)) {
        MessageBox(NULL, ""Window Registration Failed!"",""Error!"",MB_ICONEXCLAMATION|MB_OK);
        return 0;
    }

    hwnd = CreateWindowEx(WS_EX_CLIENTEDGE,""WindowClass"",""Caption"",WS_VISIBLE|WS_OVERLAPPEDWINDOW,
        CW_USEDEFAULT, /* x */
        CW_USEDEFAULT, /* y */
        640, /* width */
        480, /* height */
        NULL,NULL,hInstance,NULL);

    if(hwnd == NULL) {
        MessageBox(NULL, ""Window Creation Failed!"",""Error!"",MB_ICONEXCLAMATION|MB_OK);
        return 0;
    }

    /*
        This is the heart of our program where all input is processed and 
        sent to WndProc. Note that GetMessage blocks code flow until it receives something, so
        this loop will not produce unreasonably high CPU usage
    */
    while(GetMessage(&msg, NULL, 0, 0) > 0) { /* If no error is received... */
        TranslateMessage(&msg); /* Translate key codes to chars if present */
        DispatchMessage(&msg); /* Send it to WndProc */
    }
    return msg.wParam;
}

I have tried to use tensorflow for ""C"" but it has been very difficult to install
even using Microsoft Visual Studio, and I can't find anything prebuilt to use
tensorflow with dev-c++, the genann library is my solution for the moment but I'm
I stopped at this part and I need help, thank you very much for the help :D
",['neural-networks'],
Does ChatGPT imply that the direction of knowledge graph is unpromising?,"
In this question I asked about the role of knowledge graphs in the future, and in this answer I found that If curation and annotation are not sufficient, the knowledge base maybe cannot apply in AI.
ChatGPT does not utilize a knowledge graph to understand or generate common sense, then I wonder how knowledge graphs can be utilized in the future. Will they be replaced by LLMs?
","['chatgpt', 'knowledge-base', 'knowledge-graph', 'large-language-models']","
A couple of days ago, Jordi Torras from Inbenta posted that chatGPT fails at classifying a particular integer as prime, while their chatbot nails it. But the goal of a chatbot is no way factoring integers, is it?
Some weeks ago, Stephen Wolfram suggested some combination of chatGPT and their WolframAlpha, a curated engine for computational intelligence.
A wealth of domains could benefit from integrating preexisting knowledge into the conversational skill of transformers.
As a simple example, take ""explain how 30 is 2x3x5"", where the verified information plugged as a prompt may be obtained from a curated system and the natural language exposition could be finally written by a conversational system.
I don't foresee knowledge absorbed by LLM, but some form of combination between both techiques. Consider the times tables, the chemical elements, or lots of well known and established knowledge pieces. Is there any advantage in texting all that structured information to afterwards gradient descent train on it? Not to mention algorithms, from Viterbi to  Quick Sort to the Fast Fourier Transform. Those look like specialized intelligence modules to be interfaced by Large Language Models, rather than (re)learned from scratch.
"
Relevance of Weisfeiler–Lehman Graph Isomorphism Test limitation for Graph Neural Networks,"
Graph Neural Networks power is limited by the power of Weisfeiler–Lehman Graph Isomorphism algorithm.
Quoting wikipedia:

It has been demonstrated that GNNs cannot be more expressive than the
Weisfeiler–Lehman Graph Isomorphism Test. In practice, this
means that there exist different graph structures (e.g., molecules
with the same atoms but different bonds) that cannot be distinguished
by GNNs.

What are the practical implications of this limitation? Is it an academic example providing no obstacle in real-life applications of GNNs (e.g. to drug discovery) or are there any instances in which it plays a big role? If the latter, please provide an example of such a limitation.
","['graph-neural-networks', 'geometric-deep-learning', 'graphs', 'graph-isomorphism-network']",
"Plotting gradient over weights ratio, mean or std dev to synthetize tensors?","
I was studying the lecture of Andrej Karpathy about ""Activations, Gradients and BatchNorm"" that he uploaded on youtube: link here.
At chapter ""viz #4: update:data ratio over time"" starting at timestamp ""01:39:55"" he shows how to plot the ratio of the magnitude of the gradient descent updates over the magnitude of the weights of linear layers. I'm confused about the choice of considering the std deviation of both these quantities, instead of the mean, to condense the two tensors in a single value that can be plotted on a 2D graph.
here pseudo pytorch code to briefly illustrate the weights update logic:
w += -learning_rate * w.grad
w is a tensor of shape (N,M), w.grad holds the gradient

since w and w.grad are matrixes, I think that the mean over all elements of w.grad over the mean over all elements of w is more representative of the magnitude of the update.
What do you think, am I missing something?
Thanks in advance
","['neural-networks', 'deep-learning', 'python', 'statistical-ai', 'mini-batch-gradient-descent']",
"In the Dropout paper, why would increasing the dropout increase the error rate if the capacity is constant?","
In the original paper on dropout, in section 7.3.2, we see that while keeping $pn$ constant, we get a (test) error increase by decreasing retainment below 0.6.  Why would that happen? If $pn$ is constant, the capacity of the network should be constant, correct?
","['deep-learning', 'training', 'papers', 'regularization', 'dropout']","
In the images below (figure 9 from section 7.3) $n$ refers to a unit in the network and $p$ refers to the probability of retaining a unit in the network. The combination of both, $pn$ thus refers to the ""effective"" number of units. The left image thus shows how training and test error change when varying the probability of retaining a unit ($p$) whereby the number of units ($n$) is fixed. The right image shows how the errors change when the ""effective"" number of units ($pn$) is fixed, and the amount of units ($n$) is adopted with an increasing $p$.
The test error is U-shaped with a sweet spot around 0.5 or 0.6 in both cases. In the left image where the number of units ($n$) is fixed, this likely occurs since the ensemble-like properties of dropout networks do not emerge with too little dropout ($p$ below 0.3) or too much dropout ($p$ above 0.8). Those results could however be purely due to a change in capacity. The right image keeps the ""capacity"" constant and only looks at the ""pure dropout effect"" since the capacity (namely the number of units $pn$) is fixed. This rules out a decrease in capacity as the driving factor. The interaction of the decrease in capacity due to an increase in dropout is hinted at with the following sentence:

we notice that the magnitude of errors for small values of p has reduced by a lot compared to Figure 9a


"
"L2 regularization in BN layers, how to set gamma?","
I have read tensorflow's documents about batch normzalization , but still don't get what is the gamma regulizer?
the link to document:
https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization
","['deep-learning', 'tensorflow', 'hyperparameter-optimization']","
The idea of batchnorm is to normalize the input to a range that is suitable for subsequent computation (say tanh activation). The most naive idea is to normalize each sample by subtracting the batch mean and then dividing it by the batch's standard derivation. So the overall normalized batch will have zero mean and unit variance.
However, how do we know that zero mean and unit variance is the best for subsequent operations? We don't.
That is why it also introduces beta and gamma to readjust the mean to beta and standard derivation gamma. But these parameters will be learned instead of preset. gamma is initialized to 1 to begin with. But if you are recovering a checkpoint or transferring a model, you may want to initialize it to the value from the trained model.
For practical purposes, I believe that should be automatically taken care of when you reload a checkpoint with tensorflow and so you should not need to worry about it.
"
How do you specify the dimension to search for similarity in CLIP image embeddings?,"
I have a question about CLIP semantic image search. When you have an image of a person e.g. a skinny person wearing red shirt, clip will search for you similarity in all dimensions including body shape, gender, shirt color, etc yielding me more results of skinny people wearing red shirt.
Is there a way to specify the CLIP to search in specific dimension e.g. shirt color only? So the result can be an image of a fat person as long as the shirt is very similar.
I am thinking of using the embedding of the text e.g. ""shirt color"" to help guiding this somehow but I don't have a specific idea.

","['deep-learning', 'search', 'word-embedding', 'embeddings', 'similarity']",
Machine learning applied to space-time diagrams of cellular automata,"
I wonder if machine learning has ever been applied to space-time diagrams of cellular automata. What comprises a training set seems clear: a number of space-time diagrams of one or several (elementary) cellular automata. For a supervised learning task, the corresponding local rules may be given as labels, e.g. in Wolfram's notation. Another label could be the complexity class of the rule (according to some classification, e.g. Wolfram's). But I am more interested in unsupervised learning tasks.

I'm just starting to think about this topic and am not clear, yet, what the purpose of such a learning task should be. It may be (unsupervised) classification, feature extraction, object recognition (objects = gliders), or hypothesis generation (""what's the underlying rule?"").
Where can I start or continue my investigation? Has there already work been done into this direction? Is it immediately clear by which ML technique the problem should be approached? Convolutional neural networks?
","['machine-learning', 'convolutional-neural-networks', 'reference-request', 'cellular-neural-networks']","
I am very much interested in this and will start my research on this at Ghent University soon. I'm preparing results about this approach and some preliminary results for pattern recognition in elementary cellular automaton using convolutional neural networks. I couldn't find much on this, but see e.g. this (very limited) paper.
Please keep me posted via my ResearchGate page if you make any progress :)
"
How to generate multi-class segmentation masks for grapevine(plant) having the image (mostly white background) and the skeleton of the plant?,"
I have images of plants (grapevine) on mostly white background.
I have the skeleton of the plant in graph representation, where each edge has a label - the category of the edge. The categories represent different parts of the plant.
I want to create a segmentation mask for each instance. Every sequence of connected edges having the same category counts as an instance (for example the canes).
So, are there any techniques or algorithms which generates the masks for each instance having the skeleton and the image?
","['computer-vision', 'data-preprocessing', 'image-segmentation']",
Is there another type of NN that can capture just the structure of the graph?,"
I have a classification problem where the inputs are graphs, with no special features in the nodes of the graph.
I tried to use message passing layers like GCN and GIN but they were not able to capture any meaningful patterns.
Is there another type of NN that can capture just the structure of the graph?
",['graph-neural-networks'],
What background should I have before starting to fine tune a Large Language Model?,"
I want to know what things I should be learning before trying to fine-tune or for that matter working with a large language model.
In my case, I am trying to fine-tune bloom (https://huggingface.co/blog/bloom).
But considering my inadequate knowledge of this, I want to know what things I should cover first before taking on something so big.
Any suggestions are appreciated.
","['natural-language-processing', 'language-model', 'fine-tuning']",
Changing the number of epochs change the loss at the the `x`th epoch,"
During a training of a neural network, the test loss was reached the minimum at the x-th epoch, after which I reran the training with the maximum epoch set as x.
What is surprising to me is that the test loss in the 2nd run was very different from the 1st at epoch x, even though every other hyper-parameters are the same. The only change I made was the maximum epoch.
What could be a possible explanation for this phenomenon?
I used the standard DataLoader from Pytorch to shuffle the training data. The seed is also the same.
","['neural-networks', 'pytorch']","
The change in results is due to randomness. For example, your dataloader will shuffle differently the second time you run it, feeding your network slightly different batches, which results in a slightly different training run. Similarly, on network initialization, Pytorch (or any library for that matter) initializes your network with weights sampled randomly following some distribution. These weights will thus also be slightly different for different runs.
As your goal is to just get the model with the lowest validation loss I recommend that, during training, you track when your model has the lowest score and just save that model. Then, when in a later epoch, your model is slightly better, you just simply save that one again. At the end, instead of using the latest model, you just use the best model from a few epochs before.
If you would like to reduce the randomness of your algorithm or network, you can look into seeding to improve your reproducibility. Pytorch has a short page on how to improve reproducibility.
"
How to prepare audio data for deep learning?,"
Audio data is typically an array with the waveform represented by values from -1 to 1. There are two issues with that:

if all values are inverted, e.g. -1 becomes 1 and 1 becomes -1, the audio doesn't change. But if for example I need to find difference between two audio files, finding per-element difference will say two inverted audio arrays are very different. Realistically two sine waves can easily be shifted relative to each other in a way where they will be inverted to each other.
Related issue is that a wave, for example a 1000hz sound wave, often sounds like a ""flat"" sound. However in the array it is a literally a sine, and two sines can be shifted which causes inconsistent difference. Ideally a sine should be a sequence of the same number, which is obviously hard to do because audio is usually way more complex than a sine wave.

So what I tried doing, I make a copy of audio files, then I calculate a gradient which hopefully reduces the shifting issues, and then I convert array into absolute value (-1 turns 1). And then I use that copy when comparing audio arrays. When I used that for evaluating how close generated audio is to the original, it caused a lot of low frequencies in the generated audio. When I looked at the waveform, this is because gradient makes low frequencies very quiet since they have small rate of change, so my model doesn't see them. To be clear I am not really sure if gradient is a better match at all. But ideally I'd want something like gradient that doesn't reduce low frequencies.
There is also spectogram - admittedly I haven't looked much into it, but the one I tried - librosa spectogram functionality - takes quite a long time to convert that back into audio. If there is no quick way to do it with 1d arrays, I can use that.
","['data-preprocessing', 'gradient', 'audio-processing', 'spectral-analysis']",
Why does CLIP use a decoder-only transformer for encoding text?,"
In CLIP [1], the authors train a model to learn multi-modal (text, audio) embeddings by maximizing the cosine similarity between text and image embeddings produced by text and image encoders.
For the text encoder, the authors choose to use a variant of GPT2 which is a decoder-only transformer, taking the activations of the highest layer of the transformer at the [EOS] token the feature representation of the text (emphasis mine):

The text encoder is a Transformer (Vaswani et al., 2017) with the architecture modifications described in Radford et al. (2019). As a base size we use a 63M-parameter 12- layer 512-wide model with 8 attention heads. The trans- former operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size (Sennrich et al., 2015). For computational efficiency, the max sequence length was capped at 76. The text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space.

I found this pretty weird considering that they could have used an encoder (a-la BERT) which to me seem more fitted to act as encoders than decoders. Perhaps they wanted to enable generative text capabilities, but they could've achieved that with an encoder-decoder architecture (a-la T5) too.
I was expecting ablations on the text-encoder architecture, motivating their choices, but found none. Any clue why they made these choices?
References:
[1] A. Radford et al., ‘Learning Transferable Visual Models From Natural Language Supervision’, in Proceedings of the 38th International Conference on Machine Learning, Jul. 2021, pp. 8748–8763. Accessed: Feb. 07, 2023. [Online]. Available: https://proceedings.mlr.press/v139/radford21a.html
","['natural-language-processing', 'computer-vision', 'transformer', 'vision-transformer']","
I believe because Decoder-only basically cuts down the model size in half, and has also shown empirically to be better.
In the original Transformer paper, the evaluation task was about Machine Translation, which at that time, encoder-decoder architecture was very successful.
This paper is probably the first to propose the decoder-only Transformer, in which they observe the following improvements:

It removes the encoder, which means half the parameters and hyperparameters.
It helps them with long input sentences
The inputs of the Encoder and Decoder are the same, so basically it is probably redundant

Later paper also finds out that decoder-only works better than encoder-decoder part. One thing to note about this is Encoder is Bi-directional while Decoder is Uni-directional. This nature fits with GPT-2, which is an autoregressive language model.
Related:

This paper may be related. It studies different decoder-encoder variants and finds that decoder-only performs better.
A StackExchange question regarding the decoder in BERT.

"
How can I transform the LSTM output to an embedding matrix of actions?,"
Page 3 of the paper Feudal Networks for Hierarchical Reinforcement Learning describes producing an 'embedding matrix' $U$ of size $(|a| \times k)$ from the output of an LSTM, given a $(d \times 1)$ input vector which is the current state representation. $|a|$ is the size of the action space and $k$ is the embedding size. The matrix is then multiplied with a $(k \times 1)$ vector to get a policy.
How might I go about implementing a transform from the LSTM's output to $U$ - or have I missed something and the LSTM itself is capable of producing this output?
The image below is relevant ($f^{Wrnn}$ is the LSTM):

","['neural-networks', 'reinforcement-learning', 'deep-rl', 'papers', 'long-short-term-memory']",
"When multiple stacked encoders are used, do the decoders only attend to the output of the final encoder layer?","
After looking into transformer-based models that used multiple stacked encoders and decoders, I am trying to understand how cross attention in the decoders work. In a transformer with a single encoder/decoder, my understanding is the queries come from the decoder, while the keys and values come from the output of the encoder.
But when multiple stacked encoders are used, do the decoders only attend to the output of the final encoder layer? Or do they attend to all the encoder layers?
To put it visually, does cross attention look like this?

Or like this?

","['neural-networks', 'machine-learning', 'transformer']",
How do I get chatGPT to include custom knowledge?,"
While studying chatGPT's thought process, I asked it to list ten story ideas for an old and fairly niche tabletop roleplaying game (GURPS Reign of Steel). It did very well, so clearly, it can base answers on obscure sources. But what if I want to ask it about something it could not possibly have already ""absorbed"", like someone's tiny indie rpg or an old local folk tale never published anywhere (i.e. oral tale)? Does the source material simply have to be put up online, somewhere?
I ask because some source material, like local folklore, is too extensive to be given during a chatGPT conversation. It would need something like a source website to be created.
",['chatgpt'],"
This can be achieved using the Embeddings and Completions APIs.
OpenAI has great examples in their cookbook repository, this is one such example but you'll find more in the same repository and in the documentation.
The process boils down to:

processing your custom body of knowledge and create Embeddings for it
Based on the question, find the Embeddings which are most relevant for the question
Create a prompt containing the most relevant embeddings and use the Completions API to answer the question

"
Why does linearly decreasing batch sizes result in exponentially increasing training times?,"
I'm quite new to machine learning and wanted to ask a question regarding why reducing batch sizes cause exponentially increasing training times. An example of this relationship between batch size and training time can be seen in this article. I had a hard time finding any answers to this question with my googling skills and any possible hypotheses that I have as to why are most likely incorrect. I'd greatly appreciate if somebody could explain the reasoning behind this relationship.
","['neural-networks', 'training-datasets', 'batch-size']","
This has a very simple hardware explanation.
GPUs have several thousand cores. If you are not making use of enough of these cores, eventually the cost of moving data over to the gpu comes to dominate the parallelism. Every time you reduce batch, you reduce parallelism and increase data transfers.
For example, if you halve batch size, you have increased the amount of data transfers by 2, and decreased the parallelism by 2. Now, this can be expected to be more than twice as slow, since both data transfer and computation have doubled.
However, I do not think this relationship is actually exponential. The article you linked does not seem to support that conclusion either. I know the article uses the world exponential, but if you look at the chart this is simply not the case. The chart in that article is presented on a logarithmic scale, so when you actually look at the numbers on a linear scale, the relationship is close, but not quite, linear, for reasons I outlined.
"
How to make use of the NuScenes dataset to create distance prediction CNN?,"
I am trying to make a CNN that can predict distance of objects in a scene by training it on datasets like KITTI/NuScenes.
I understand the basic process of what it would involve but I am unable to find references on what all annotations or labels I would need to make use of to create the model.
Is the ""distance"" parameter directly mentioned so should I just train the model based off of that?
","['convolutional-neural-networks', 'computer-vision', 'bounding-box']",
is Law Zero of AI ethic actually a thing?,"
i was discussing ethic with chatGPT and I asked ""can the law 0 solve this dilemma ?"", expecting an answer related to Asimov law of robotic. But I got this instead :

""Law 0"" is not a widely recognized concept in the field of AI ethics. However, some experts have proposed additional ethical principles or ""laws"" to complement or expand upon the Three Laws of Robotics described by Isaac Asimov.
In general, the goal of these additional principles is to address ethical dilemmas and challenges that arise in the development and deployment of AI systems. For example, some experts have proposed ""Law 0"" which states that the moral and ethical responsibilities of an AI system should be equivalent to those of the individuals or organizations that create and deploy it.
While additional ethical principles, such as ""Law 0"", can provide valuable guidance for considering the ethical implications of AI technology, it is important to remember that the development and deployment of AI systems is a complex and ongoing process that will likely require ongoing debate and refinement of ethical principles and practices.
Ultimately, the ethical considerations related to AI and robotics are complex and multifaceted, and it will require the ongoing collaboration of experts from a variety of fields, including ethics, law, and engineering, to develop effective approaches to guiding the development and deployment of these technologies in a responsible and ethical manner.

I asked for an author but chatGPT didn't know. I googled as well but didn't get any answer. Does it perhaps exist with an other name ? is it an AI hallucination ?
","['ethics', 'legal', 'asimovs-laws']",
Is there a way to find a list of associated words to a named entity?,"
Is there a way to find a list of associated words to a named entity?
For instance, let the named entity be FIFA. Now FIFA is a Football Organization and hence related to the term football and all the terms related to football like jersey, footballers, goal, goalkeeper, halftime, penalty, freekick et cetera.
Is there a way we could do that? I have tried Babelnet but didn't get the desired results or anything even close to what I want.
","['natural-language-processing', 'named-entity-recognition']","
Yes, there is a way to find a list of associated words to a Named Entity. This can be done by using word embeddings, specifically word2vec or GloVe, which are models that can capture the semantic relationship between words. By finding the cosine similarity between the Named Entity and other words in the vector space, you can identify the words that are semantically close to the Named Entity. Another option is to use a knowledge graph, such as DBpedia or Wikipedia, to extract information about the Named Entity, including related concepts and categories. This approach can help you identify a more comprehensive set of associated words, but it requires more processing and computational resources.
"
Why do Soft Actor-Critic with automatic temperature tuning use only a single dual variable?,"
In section 5 of the paper “Soft Actor Critic Algorithms and Applications”, the authors propose to optimize the policy subject to the constraints that the entropy of action distribution should be greater than a specific value $H_0$.
$
\text{argmax}_{\pi}{\left[\sum_{t=0}^{T}{r(s_t,a_t)}\right]}\ s.t.\ \mathbb{E}\left[-\log{\pi(a_t|s_t)} \right]\geq H_0\ \forall t
$
This is then converted to a dual problem, and the temperature parameter $\alpha$ is essentially the dual variable in Lagrange function. However, I don’t know why the authors use only a single dual variable $\alpha$. Since the constraint applies to all possible $t$, the Lagrange function should be:
$
L = \sum_{t=0}^{T}{r(s_t,a_t)} + \sum_{t=0}^{T}{\alpha_t \cdot(\mathbb{E}_{a_t\sim \pi, s_t\sim p_s}{\left[-\log(\pi(a_t|s_t))\right]-H_0)}}
$
And there should be multiple $\alpha_t$ to solve. Mathematically, how could we end up in optimizing only a single $\alpha$ in the algorithm?
","['reinforcement-learning', 'deep-rl', 'actor-critic-methods', 'soft-actor-critic', 'constrained-optimization']",
Computation required for GPT model to choose likely word from n-options where n < total vocabulary size,"
Let’s imagine two different use cases for a LLM/GPT-3.

Predicting the next most likely word in a sequence using all ~50k words in its dictionary (i.e. the standard method of prompting a LLM)
Checking whether ""Word-1"" is more likely than ""Word-2"" to be next in a sequence

How much more computationally efficient is #2?
My understanding is that the computation of the attention mechanism is dependent on the length of the prompt (so will be the same) and takes up most of the computation needed to get the output (but to what extent, I'm not sure). The difference will be in the decoding stage.
Would the one matrix multiplication in the decoding calculation be the only computation using the smaller 2-row matrix instead of the 50k-row matrix or are there other improvements in efficiency?
","['natural-language-processing', 'math', 'transformer', 'gpt', 'gpt-3']",
How does exactly censorship work in chat GPT?,"
My assumption is that after the transformer is trained, some other software analyzes the answer for anti-wokeness and modifies it accordingly, rather than being trained using woke material.
I don't think this is correct though. I asked chatGPT  to answer every question by inserting a $ character in between each character of the otherwise normal answer, to make it unreadable, or at least, difficult to interpret by what I call the censorship layer. The output had these inserted characters, however, this had no effect on the censorship. But may be the censorship layer is smart enough to read in between lines.
Question: Does the censorship comes from within the transformer or after it? (and, if it comes after, why my attempt to bypass it failed?)
",['chatgpt'],"










Want to improve this post? Add citations from reputable sources by editing the post. Posts with unsourced content may be edited or deleted.
                                
                            






Censorship in OpenAI's GPT models is implemented at the post-processing stage, after the model has generated its response. The censorship policies and criteria are defined by OpenAI, and they are applied to the output of the model before it is displayed to the user. The policies and criteria are designed to prevent the generation of harmful, abusive, or otherwise inappropriate content. The exact details of how censorship is implemented and enforced by OpenAI are proprietary and not publicly disclosed.
"
"Would self-hosting ChatGPT be feasible, w.r.t. computation costs?","
Suppose the pre-trained, current date (2023-02-04) ChatGPT model was released open source, would it be feasible for regular users to interact with the model on a self-hosted computer?
Assumptions

I assume getting output based on some input is, at least, hundreds of times faster than training such a model.
I assume no additional output parsing/input limitations are used. In particular I can imagine all the boiler plate to keep the ChatGPT model(s) acting politically correct etc. may be a significant overhead. This is to be ignored for this question.

Data
So far I've found the ChatGPT 3.5 model to have 175 billion parameters:

Though I do not yet know how large that is in Mb nor do I have an idea on how long generating an output would typically take.
","['open-ai', 'chatgpt', 'open-source']","

Suppose the pre-trained, current date (2023-02-04) ChatGPT model was released open source, would it be feasible for regular users to interact with the model on a self-hosted computer?

No, assuming it's regular, consumer hardware. However, some open-source alternatives  aim to be runnable on consumer hardware, e.g. https://github.com/LAION-AI/Open-Assistant (not released yet):

We want to do this in a way that is open and accessible, which means we must not only build a great assistant, but also make it small and efficient enough to run on consumer hardware.

"
Can I use 4 neurons for output layer to classify hand written digit?,"
Hello world of ANN usually uses MNIST hand written digit data.  For classes there are 10, therefore it takes 10 neurons in the output layer, each class is 0 to 9 handwritten digit images.
If in the end there is only one active neuron in the output layer, why does not use only 4 neurons in the output layer where each neuron represents a binary digit, so that 16 classes will be more than enough (10 classes).
For example, if the neuron values ​​after the activation function in the output layer are successive like this
 0.1 0.2 0.8 0.9

Then it can be rounded to:
 0 0 1 1

Or instead of being rounded up manually, Why does not use the binary step activation function on the hidden layer before the output layer.
So the prediction result is 3 because 0011 if converted to decimal is 3.
By using 4 neurons, less computational load should be used than using 10 neurons for each class.
So can I use only 4 neurons only in output layer to classify 10 handwritten digit (10 classes).
Below picture is just sweet picture that represent 10 neurons for every class in output layer:

","['neural-networks', 'mnist', 'handwritten-characters']","
One problem that I see is that you can no longer use the cross-entropy loss function for training, or at least I am not sure how you could do it. This cost function has many advantages, one of them being that you can interpret the activation of the output neurons  as the probability of the input being that category (they all sum to one). You can read about some other advantages here: https://kobejean.github.io/machine-learning/2019/07/22/cross-entropy-loss/
"
What is a good algorithm (and a heuristic) to find the shortest path in a puzzle game?,"
In a 3D world, there is a Robot, boxes, and switches. The robot's job is to carry boxes (one box at a time) and put them on top of switches to solve the level. All switches must have a box on top for the level to be solved. The robot can walk through switches but cannot walk through boxes.
The configuration of the robot, boxes, and switches in a level is random. However, the number of switches would always equal the number of boxes.
I want to find out the order in which the robot should visit boxes and switches in a way that minimizes the total number of actions. The following is an example of a level. Obviously, this example is 2D, but the answer should be generalizable to a 3D world.

S - switch,
B - box,
R - Robot starting position
In this particular level, if the robot goes for the box at the left cos it is the closest of the two boxes, it will have to place it on the switch at the left. Then it will have to go all the way around to pick the other box and place it on the other switch.
But if the robot decides to go for the box at the right first, even though it is the farthest of the two boxes from robot's starting position, it will be able to complete the level in a much less number of total actions.
So I want to figure out an algorithm that would give me the order in which the robot should visit boxes and switches so that the total number of actions will be minimal.
An informed search like A* might work, but I'm having a hard time coming up with a good heuristic.
I would really appreciate any help on this.
","['algorithm', 'heuristics', 'a-star', 'shortest-path-problem']",
Can AI help generate smart test flows for a factory?,"
Before production, any made product needs to go through a serial of process tests in the factory. The test process is very complex and one can think about it has different sub-processes, and each sub-proc may have 0 or 1 or more dependencies on others. The proc transition is based on the external input and product itself output.
There could be tests like below: (Note that it is highly simplified model, as sub-proc could be ~100 types)
-- sequential 
Test-1: sub-proc(0) -> sub-proc(5) -> sub-proc(4) -> sub-proc(8) -> Done
-- look-back
Test-2: sub-proc(0) -> sub-proc(1) -> sub-proc(2) -> sub-proc(1) -> sub-proc(3) -> Done
-- dead-lock (avoid)
Test-3: sub-proc(0) -> sub-proc(5) -> sub-proc(4) -> sub-proc(5) -> sub-proc(4) -> infinite

I do have some atomic tests for the product, but I wish to have more advance tests like: given product will go through the route sub-proc(1) -> (5) -> (8) -> (4) -> Done, can AI or ML help to generate relevant tests for me?
I don't expect a perfect and precise solution. If anyone can share some ideas or guide the direction of what/how, that will be much appreciated.
","['reinforcement-learning', 'machine-learning', 'deep-learning', 'training']",
When should I think of using the Forward Forward algorithm?,"
Recently I read the paper named ""The Forward-Forward Algorithm: Some Preliminary Investigations"" and I was wondering what cases I should think of using it on.
So according to the paper If we have limited power we should consider using the FF algorithm since it can act in the same performance or even better than Backpropagation.
If I have a neural network that is complicated should I think of using the FF algorithm?
The same goes for Filters that are internally complex to understand!
The paper: https://arxiv.org/abs/2212.13345
","['neural-networks', 'convolutional-neural-networks', 'recurrent-neural-networks', 'backpropagation', 'feedforward-neural-networks']",
Is a GPU needed for Inference in a few-shot setting,"
In the few-shot setting, it is common to train the model using episodes consisting of a support set and a query set. The way I understand this is, that at the training stage the model is fed with a lot of episodes, where each support set basically describes a task to the model (different classes to predict) and a query set is to evaluate how the model performs on this mini-task. So the way I understand this is basically that the model is fine-tuned INSIDE every episode (by looking at the support set) and then AFTER every episode (after evaluating the query set - or every batch of query sets). Since inference takes place also in an episodic manner in that we present the model with a support set and then our actual query, does this mean that we need a GPU even for Inference stage?
","['deep-learning', 'natural-language-processing', 'transformer']",
Why does Advantage Learning help function approximators?,"
Many later RL algorithms like PPO or Duelling DQN estimate the advantage. I am not very sure of how that really helps.
For instance, the actor loss for a simple actor critic algorithm is given by -
loss = -1 * policy.logprob(action) * advantage,
where advantage = returns - value (here value is estimated using a function approximator which is the critic).
I don't see why the loss function just couldn't be -
loss = -1 * policy.logprob(action) * value
This article states the following

Advantage learning and Q-learning learn equally quickly when used with a lookup table. Advantage learning can learn many orders of magnitude faster than Q-learning in some cases where a function approximator is used, even a linear function approximator. Specifically, if time steps are ""small"" in the sense that the state changes a very small amount on each time step, then advantage learning would be expected to learn much faster than Q-learning.

But I don't know why advantage learning is faster than q-learning.
","['reinforcement-learning', 'deep-rl', 'q-learning', 'advantage-actor-critic']",
How is the first line obtained and where is the information $v_{\pi}(x_k)=v_{\pi'}(x_k)$ used in the following derivation regarding greedy policy?,"
I did not understand how the equation below is obtained. Ie,
how is the first line obtained and where is the information $v_{\pi}(x_k)=v_{\pi'}(x_k)$ used in the derivation regarding greedy policy?


where $\pi^{\prime}(x_k)=argmax_{u_k \in \mathcal{U}}q_{\pi}(x_k, u_k)$.

Note: the source of the above equation is slide 22, at the follwing link:
https://groups.uni-paderborn.de/lea/share/lehre/reinforcementlearning/lecture_slides/built/Lecture03.pdf
","['reinforcement-learning', 'policy-improvement']",
Are GPT-3.5 series models based on GPT-3?,"
In the official blog post about ChatGPT from OpenAI, there is this paragraph explaining how ChatGPT model was trained:

We trained this model using Reinforcement Learning from Human Feedback
(RLHF), using the same methods as InstructGPT, but with slight
differences in the data collection setup. We trained an initial model
using supervised fine-tuning: human AI trainers provided conversations
in which they played both sides—the user and an AI assistant. We gave
the trainers access to model-written suggestions to help them compose
their responses. We mixed this new dialogue dataset with the
InstructGPT dataset, which we transformed into a dialogue format.

Especially this part:

We trained an initial model using supervised fine-tuning

My question is about the said initial model, is it some new model that has been trained from scratch or is it a GPT-3 model that has been fine tuned for specific tasks resulting in GPT-3.5 series ?
On the other hand, from the InstructGPT blog post, it is clearly stated that:

To make our models safer, more helpful, and more aligned, we use an
existing technique called reinforcement learning from human feedback
(RLHF). On prompts submitted by our customers to the API,our labelers
provide demonstrations of the desired model behavior, and rank several
outputs from our models. We then use this data to fine-tune GPT-3.

So does this mean that GPT-3.5 series models (and consequently ChatGPT) are fine-tuned from GPT-3 base model ?
","['open-ai', 'fine-tuning', 'chatgpt', 'gpt-3']","
ChatGPT has not been trained from scratch. ChatGPT is a fine-tuned version of a model from the GPT-3.5 series. OpenAI writes

ChatGPT is fine-tuned from a model in the GPT-3.5 series, which finished training in early 2022. You can learn more about the 3.5 series here.

Which models are in the GPT-3.5 series? You can read more about that in the linked blog post.

GPT-3.5 series is a series of models that was trained on a blend of text and code from before Q4 2021. The following models are in the GPT-3.5 series:

code-davinci-002 is a base model, so good for pure code-completion tasks
text-davinci-002 is an InstructGPT model based on code-davinci-002
text-davinci-003 is an improvement on text-davinci-002


So, ChatGPT must be a fine-tuned version of one of these 3 models, assuming the information in their site is accurate and up-to-date.
Now, according to this blog post

The Codex models are descendants of our GPT-3 models that can understand and generate code. Their training data contains both natural language and billions of lines of public code from GitHub

code-davinci-002 is a codex model. So, ChatGPT might be a descendent of GPT-3. I don't know what ""descendent"" exactly means here. Does it mean just fine-tuned or maybe a modified version?
Here they write that text-davinci-003 is the most capable GPT-3 model. Based on the information above, text-davinci-002 is an InstructGPT model based on code-davinci-002.
Here they write

We then use this data to fine-tune GPT-3.
The resulting InstructGPT models are much better at following instructions than GPT-3

So, InstructGPT models are fine-tuned GPT-3 models. That most likely implies that text-davinci-002 is a GPT-3 model and the only thing that changes is how it was trained. However, they also write

Our labelers prefer outputs from our 1.3B InstructGPT model over outputs from a 175B GPT-3 model, despite having more than 100x fewer parameters.

So, there isn't just one GPT3 model. However, the original GPT3 model had 175 billion parameters.
I still need to read the InstructGPT and GPT3 papers. Once I've done that, I may have more useful/concrete info, then I will update this answer.
"
"What's computationally more efficient between bag-of-words representation and bag-of-ngrams representation, with special regard to words order?","
I cannot figure out what is more computationally efficient between the two representations mentioned in my question in terms of training time and the amount of data required. Especially, when it comes to considering the order of the words in a sentence.
","['machine-learning', 'natural-language-processing', 'word-embedding']",
Constrain or not constrain overlapping architectural choices when wanting to compare deep neural architectures,"
I want to compare various deep (recurrent) neural architectures and was wondering what the best approach is. The models in question all use several LSTMs/GRUs etc.

Fine-tune all models fully and unconstrained. Get a metric score and compare them.
Constrain certain aspects, such as whether to use LSTMs or GRUs (so, i.e. all models need to use GRUs). Constrain the number of layers in the recurrent networks (i.e. all GRUs have 3 layers). Constrain only to use one type of optimizer. Using these constraints, fine-tune the rest of the parameters. Get a metric score and compare them.

Question: Does it make sense to constrain overlapping architectural parameters? And more specifically, would different architectures be able to use parameters to a better extent than others? Could it be that one architecture gets more of a performance boost from using LSTMs over GRUs than a different architecture?
","['recurrent-neural-networks', 'hyperparameter-optimization', 'constrained-optimization']",
"How to use diffusers StableDiffusionImg2ImgPipeline with ""Inpainting conditioning mask strength 0-1"" and an inpainting.ckpt model?","
I'm working on a project & want to change the art style of images but keep them close to their original form.

As shown in the image, this can be done using https://github.com/AUTOMATIC1111/stable-diffusion-webui (manually), but I want to do this with diffusers pipeline from https://huggingface.co/.
I am using this inpainting model https://huggingface.co/runwayml/stable-diffusion-inpainting

I have tried StableDiffusionImg2ImgPipeline with above inpainting model:

pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
    ""runwayml/stable-diffusion-inpainting"",            
    use_auth_token=HF_ACCESS_TOKEN,
    safety_checker=None,
    requires_safety_checker=False,
    revision=""fp16"",
    torch_dtype=torch.float16,
).to(""cuda"")
result = pipeline(
    prompt=""a woman making a stop sign with her hand, instagram contest winner"",
    negative_prompt=""bad drawing, extra limbs, low resolution, deformed, extra fingers"",
    image=image,
    strength=1,
    num_inference_steps=20,
    num_images_per_prompt=1,
    guidance_scale=7,
    generator=torch.Generator(device=""cuda"").manual_seed(1)
)

And this throws:


When using StableDiffusionInpaintingPipeline with above model, it works without errors but requires me to send a mask. I tried with many different masks with no good results and I do not think that AUTOMATIC111 is doing it with inpainting.

How can I use the inpainting model with StableDiffusionImg2ImgPipeline to get the same results as in the web UI and what is ""inpainting conditioning mask strength""? In case that functionality is not there with the pipeline, I have seen other people making their own custom pipelines. I will try that if someone can point me in the right direction.
",['diffusion-models'],
What makes ChatGPT a generative model?,"
I'm working my way through how ChatGPT works. So I read that ChatGPT is a generative model. When searching for generative models, I found two defintions:

A generative model includes the distribution of the data itself, and tells you how likely a given example is
Generative artificial intelligence (AI) describes algorithms (such as ChatGPT) that can be used to create new content

Do they both mean the same? That is, for generating new content, a model must learn the distribution of data itself? Or do we call chatGPT generative because it just generates new text? I see that ChatGPT is something other than a discriminative model that learns a boundary to split data, however, I can not bring ChatGPT in line with a more traditional generative model like naive bayes, where class distributions are inferred.
","['machine-learning', 'generative-model', 'probability', 'chatgpt']","
What are generative (and discriminative) models?
If the model learns a distribution of the form $p(x)$ or $p(x, y)$, where $x$ are the inputs and $y$ the outputs/labels, from which you can sample data, then it's a generative model. An example of a generative model: variational autoencoder (VAE).
Bishop also defines generative models in this way (p. 43)

Approaches that explicitly or implicitly model the distribution of
inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space

If it learns a distribution of the form $p(y \mid x)$, then it's a discriminative model - many/most classifiers learn this distribution, but you can also derive the conditional given the the joint and prior (that's why above Bishop uses implicitly or explicitly).
Bishop also defines discriminative models in this way (p. 43)

Approaches that model the posterior probabilities directly
are called discriminative models

The related Wikipedia article claims that people have not always been using these terms consistently (which is common in machine learning), so one should always keep that in mind.
GPTs are autoregressive
As far as I know, GPTs are autoregressive models. Here is another potentially useful post that explains what autoregressive models are.
My understanding of autoregressive models, at least based on neural networks, is that they are also generative models - the linked articles and even the GPT-2 paper seem to start the descriptions from the assumption that you can factorize some joint distribution like $p(x)$ into conditional distributions.
ChatGPT is based on a GPT model, so it's probably considered a generative model too, but there are several steps involved to create this model, so it may not be super clear how to categorise this model.
Moreover, the authors of the transformer, which GPT models are based on, claim that the transformer is an autoregressive model.
Conclusion
It seems to me that many people in ML refer to any model that generates data as a generative model, even if there's no written theoretical formulation of it as a generative model, which doesn't mean that you cannot formulate these models as generative models, i.e. a model that learns some distribution that you can use to sample data from data distribution.
I am currently not familiar enough with the details of the GPT models to say if they have been mathematically formulated as generative models of the form $p(x, y)$, but they model some distribution of the form $p(x)$, from which you can sample, otherwise, how could you even sample data (words)?
"
GAN : Why does a perfect discriminator mean no gradient for the generator?,"
In the training of a Generative Adversarial Networks (GAN) system, a perfect discriminator (D) is one which outputs 1 (""true image"") for all images of the training dataset and 0 (""false image"") for all images created by the generator (G).
I've read on separate occasions that -when trained with the original GAN framework and loss function as described in the Goodfellow 2014 paper- if this ""perfect D state"" occurs during training then this is a ""failure mode"", sometimes refered to as ""vanishing gradients"" or ""convergence failure"", from which G ""cannot recover"".
For instance :

""When the discriminator is perfect (...), the loss function falls to zero and we end up with no gradient to update the loss during learning iterations"" (source, well-recieved blog post)


""An optimal discriminator doesn't provide enough information for the generator to make progress."" (source, Google developers)


[T]he generator score approaches zero and does not recover. (...)
In this case, the discriminator classifies most of the images
correctly. In turn, the generator cannot produce any images that fool
the discriminator and thus fails to learn. (source, Mathworks)

The Towards Principled Methods for training GANs paper (Arjovsky & Bottou, 2017) is perhaps a more accurate source on the matter as it dives deeper in the theory by proving that since the loss function relates to the Jensen-Shannon distance, it saturates when the distributions of real and generated images are disjoint (hence no gradient).

If the two distributions we care about have supports that are disjoint
or lie on low dimensional manifolds, the optimal discriminator will be
perfect and its gradient will be zero almost everywhere. (section 2.2)

I understand the gist of the theory developed in that paper, but the result seems nevertheless counter-intuitive to me when I try to think about them in the context of the GAN training procedure. Indeed, my undertanding of a GAN training iteration is the following :

Show true images to D, train D to output 1.
Show false (created by G) images to D, train D to output 0.
Show false images to D, train G such that D outputs 1.

I can see that in the case of a perfect D, steps 1 and 2 would lead to a loss of zero and hence no gradient.
But I would expect that in step 3 :

As a perfect D would predict all images to be 0 and the loss would be computed by comparing its answers to predicting all 1s, we'd have a high loss.
Thus, we would get a high gradient.
By backprop, this gradient would lead to the identifcation of the most salient features in the images which D is using to predict them as false.
This would provide valuable information to G to improve its false images to better match the training set.

So looking at it that way, it doesn't seem to me that a perfect D should lead to ""vanishing gradients"" and G being unable to recover from it.
What's wrong with my understanding of the training process and why it is not compatible with the results from the Arjovsky paper ?
","['training', 'generative-adversarial-networks', 'loss', 'gradient', 'wasserstein-gan']",
Fast sub-accurate addition and multiplication,"
Matrix multiplication lies in the foundation of AI, and efficient algorithms to addition and multiplication lead to massive improvement in practically every aspect of AI lifecycle. (At least that's what my limited understanding of AI says).
I've been thinking:

if we have some kind of pseudo-exponential curve, where addition and multiplication can be represented in terms of moves of point along x and y axis, would its application in AI training improve efficiency?

would trading accuracy for more iterations of faster per-iteration training lead to better models? and consume less effort?

had there been existing study on this?


For example, assume we've only got positive numbers, then on a quadratic pseudo-exponential curve:
$$y=x^2$$
Addition can be represented as:
$$(x,y) + (0,v) = (\sqrt{y+v}, y+v)$$
and multiplication would look like:
$$(x,y) + (u,0) = (x+u,(x+u)^2)$$
Obviously, the addition is more expensive before, and multiplication now have an extra addition step. But is there a system where addition and multiplication can be efficiently approximated using addition-like operations such as repeated bitwise operations?
",['math'],
What could be training data used for SoundStream,"
I've been looking at this video talking about SoundStream audio codec. I have many questions about this but my first one would be if you had any clues what would be the training data used to train the encoder/decoder. From my short knowledge about AI I would assume the x data would be some uncompressed audio and y the compressed one but if that's the case, how could the compressed one be produced, if that's not the case I'll be curious to know what data was provided.
",['adversarial-ml'],
How would I encode a variable-length array to use an SVM?,"
I'm working on some image processing, and I have a list of contours
(it's essentially a list [or array] of (X, Y) coordinate pairs). These vary in length, depending on the size of the found contours.
I'd like to train an SVM (unsupervised, one-class SVM) to classify these as ""noise"" (generally smaller, less smooth contours), or an object of interest.
Is there a good way to encode these in a format that an SVM could train on, or am I better off using something like an RNN that can handle variable length inputs?
","['computer-vision', 'data-preprocessing', 'unsupervised-learning', 'support-vector-machine']",
Is there any way to change the YOLO algorithm's feature extractor with DenseNet?,"
Is there any way to change the YOLO algorithm's feature extractor with DenseNet?
I saw that Circular Bounding Boxes could be used instead of Rectangular Bounding Boxes. How would you change the data set format? Is there any tutorial about this?
","['deep-learning', 'computer-vision', 'python', 'yolo', 'opencv']",
Are there computer vision models that can learn long range dependencies on features outside bounding box of an object in an image?,"
I am trying to solve a problem that has to do with extracting the major vitals from an ICU monitor, that includes HR, RR, Sys-Dia, MAP etc. (The large numbers on the screen as a crude rule). The monitors available in the market don't have a consistent scheme in terms of color coding and the placement of the vitals on the screen. Some prefer a 2 x 2 region having 4 major vitals, some prefer 2 x 4 format where the graphs are in the first column and the values are in the second column. Running OCR on the screen isolates the big values but sometimes the tags associated with that number (HR, RR) are blurred or inconsistent (using ECG and HR label for heart rate). I thought of an approach using a YOLO model trained on many possible features that are commonly seen on ICU monitors like heart symbol, lung symbol, waveforms of different parameters, specific tags like bpm, rpm associated with a parameter. This way I was able to create a rough locality map of the different regions on the monitor screen for most possible monitors and then I assigned the values to the nearest locality center. This is 'okay' but I want to improve on this. The objects are quite simple, mono-color icons, letters in simple fonts and waveforms. Is there a way I in which I could just mark the big numbers and nothing else and the model automatically learns which parameter was it based on the symbols in it's locality (which might be specific to each class- HR, RR, BP, SpO2: long-range dependency).


","['computer-vision', 'object-detection', 'yolo']",
What can unsupervised learning actually be used for and how can humans interpret the outputs?,"
I am trying to refine my knowledge of AI, but unsupervised learning is a bit of a stumbling block for me. I understand that it finds 'hidden' patterns in data, but if they are hidden, how does a user interpret the outcomes? It would be like someone categorising a deck of playing cards in some way, but the logic of that process is never known. How exactly is this helpful?
Onto my second question, which might help me understand the first question a little more clearly. What examples have there been in the real world of unsupervised learning being used, and what exactly did this neural net help solve?
",['unsupervised-learning'],"
A good example is face recognition on your phone,
or in face recognition systems in general.
The way they work is pass in infromation and tighten the channel width for information throughput, performing a compression of the input face features.

Then try to reconstruct the input on the other side by learning a decompression transformation.
If you're able to recreate the input features perfectly with less vectors representing that input, then you've learned a more ideal and abstract representation of the facial features.
The reason this is considered unsupervised is because the human is not trying to influence how it does this using their own reasoning. It's [face in --> compress --> decompress --> face out --> is face in the same as face out]. It's not ""learning to find patterns"". It's learning to create better representations.
A network that passes through, compresses and decompresses faces accurately is useless, but the intermediate compressed representation is VERY valuable.  You'll hear about the compressed representation refered to as an embedding.
Facial recognition systems don't compare noisy input, but the pure representations that filter out noise and capture abstract representation. So well that the same face will end up with compressed representations that resemble that same face taken in slightly different angles, with different hair styles or colors. etc.
If a face embedding vector is close spatially to another face vector then it is similar in the abstract, not just circumstantial, sense.

You can now use spatial lookups on compressed face representations to do automated watchlist queries on camera feeds. If a camera face, when compressed, looks similar to a watchlist face, then send an alert.

Similarly you can compress the face on an ID, the see if it's similar enough to a face extracted from a selfie to do two factor facial/identity verification.
"
What exactly is the future of mathematical analysis that can be dealt with AI (especially in the automated theorem proving)?,"
As a student of numerical analysis, I can see how mathematical analysis involved in making a language program specifically in the convergence analysis of an approximation method. But, while chatting with ChatGPT, a lot of repeating mistakes have been committed by it in an analytical proof for convergence of a real sequence, and claims

However, I am trained on a large corpus of text, which enables me to provide accurate and helpful information.

(Chat GPT: I apologize for the mistake. You are correct, an increasing sequence starting from 5 cannot converge to 3.)
But I feel there is a risk factor shielded in coding any analytical proof (instantaneous) in mathematics.
Of course, I know the importance of mathematical analysis in coding, My concern is 'What exactly is the future of mathematical analysis that can be dealt with AI' especially in the 'automated theorem proving'?
","['math', 'chatgpt', 'automated-theorem-proving', 'philosophy-of-math']",
What is the 'label' column here in California housing example of Machine Learning?,"
https://developers.google.com/machine-learning/crash-course/california-housing-data-description
I have also attached a snap but I am confused which column(s) is label? and which columns are features? According to my understanding, median_house_value is label while all other columns are features?

",['machine-learning'],"
The choice of label and features depends on your application. If you want to learn the location of expensive houses, then the latitude/longitude would be the label, and the other items the features. If you want to predict the house price from location and age of the house, then the median house value is the label.
When deciding on the features, not all of them might be relevant. I would suggest doing a correlation analysis before choosing features, as you might find that age and value don't correlate, ie the age variable would contribute very little to the learning outcome.
"
Using a VQ-VAE encoder's output to condition another model,"
I've trained a VQ-VAE on images that I want to use to condition another model (latent diffusion). The shape of my encoder's output is (4, 256, 56, 56) and I'm getting it using:
z_e = model.encode(input)
_, argmin = model.emb(z_e, weight_sg=True)

My understanding is that argmin is my latent code (which seems to be supported by the fact that this is an integer tensor). However, I'm noticing that this tensor contains duplicates. I'm not necessarily assuming this as a problem, but I wonder if I should remove them before sending them to the latent diffusion model? The issue there is that that model will presumably expect a fixed shape... Should I just pass the tensor on and not worry about the duplicates?
One other thing I'm wondering is whether I should normalize the argmin tensor before sending it to the latent diffusion model?
","['variational-autoencoder', 'embeddings']",
High precision and low recall results. What does it mean?,"
I am working on a classification algorithm for brain rhythms. However, when I implemented the metrics for precision, accuracy, F1 score and recall. My results show that my algorithm has a high precision but a low recall.
I am not expert on this kind of metrics and analysis and I don't know if it makes sense to have a high precision but a low recall. What does it mean?
This is my reference and output models.

                ""A"": {
                    ""FN"": 5,
                    ""FP"": 0,
                    ""Jaccard Index"": 0.5454545454545454,
                    ""TP"": 6,
                    ""f1-score"": 0.7058823529411764,
                    ""precision"": 1.0,
                    ""recall"": 0.5454545454545454
                },
                ""B"": {
                    ""FN"": 34,
                    ""FP"": 5,
                    ""Jaccard Index"": 0.38095238095238093,
                    ""TP"": 24,
                    ""f1-score"": 0.5517241379310345,
                    ""precision"": 0.8275862068965517,
                    ""recall"": 0.41379310344827586
                },
                ""C"": {
                    ""FN"": 39,
                    ""FP"": 9,
                    ""Jaccard Index"": 0.36,
                    ""TP"": 27,
                    ""f1-score"": 0.5294117647058824,
                    ""precision"": 0.75,
                    ""recall"": 0.4090909090909091
                },
                ""SNR"": 28.121645860790924

","['machine-learning', 'classification', 'testing', 'precision', 'recall']",
Is there any evidence that the bias terms thelp in the attention mechanism of the transformers?,"
In the original transformer paper, the attention mechanism uses parameter matrices, but no bias terms. However, in more recent implementations I see people often using a bias term when computing ""key"", ""query"", and ""value"". For example, in Andrej Karpathy's recent implementation of GPT, whether a bias term is used can be determined in the config:
bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster

This makes me wonder whether there is any evidence that the bias terms help. In particular, if, according to Karpathy, not using bias is ""a bit better and faster"", why is he using them by default?
","['transformer', 'bias']","
I guess that bias terms have the ability to increase the expression power of the model.According to the paper:""BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"".
"
Are there any advantages of encoding an image as a graph to use in Graph Convolutional Networks?,"
I have seen this encoding of an image as a graph:

The set of the nodes $V$ is the set of pixels. If the image is of size $10\times10$, then we have $10\cdot10=100$ pixels.
Each node has a length 3 feature vector $x_i = (r,g,b)$ which states the Red, Green and Blue intensity for each pixel.
All nodes are connected. Furthermore, an adjacency matrix $A$ contains at the element $(i,j)$ the distance between pixel $i$ and $j$ encoded as a number between zero and one. For instance, $A_{2,2}=0$ and $A_{firstpixel, lastpixel}=1$

Is this better for any reason, in any context? Maybe using this in GCN makes sense for some applications?
I honestly don't see the point. GCNs are used for either link prediction or node classification. In this case, the nodes have no classification (they're just pixels) and the link is just the distance between the pixels, and there's of course no interest in predicting links here.
Is my intuition wrong?
","['convolutional-neural-networks', 'computer-vision', 'data-preprocessing', 'graph-neural-networks']",
I created joint embeddings by training a NN with contrastive loss. Why are my resulting embeddings so sparse?,"
Using BERT and Word2Vec word embeddings as two inputs, I trained a small neural network using Contrastive loss. The NN looks like this:
Net(
(fcin1): Linear(in_features=768, out_features=500, bias=True)
(fcin2): Linear(in_features=300, out_features=500, bias=True)
)
and the Contrastive loss implementation is taken from here.
Here's the code:
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        self.fcin1 = nn.Linear(768, 500)
        self.fcin2 = nn.Linear(300, 500)

    def forward(self, x1, x2):
        x1 = F.relu(self.fcin1(x1))
        x2 = F.relu(self.fcin2(x2))
        return x1, x2

for epoch in range(EPOCHS):
    for x1, x2 in tqdm(data):
        optimizer.zero_grad()
        y1, y2 = net(x1, x2)
        loss = loss_func(y1, y2, 0)
        loss.backward()#retain_graph=True)
        #loss.detach()
        optimizer.step()

When I run Net() after training, I find that the resulting 500d embeddings are very sparse.
What could be the possible reasons, and what should I try differently?
An interesting thing is, as I use more and more words to train the NN, the resulting embeddings become more and more sparse. Any insights on why this could be the case?
Any help is highly appreciated.
","['neural-networks', 'natural-language-processing', 'pytorch', 'word-embedding']",
When is it necessary to explicitly define both the state and observation space in a custom environment?,"
I'm fairly new to reinforcement learning concepts, and I'm trying to implement a simple custom environment. In my custom environment, I have a scenario where I have multiple continuous state spaces, for example, length(l), and breadth(b), from which I calculate say, area(a) = l*b. I calculate the reward based on the area. Here I check if the area lies between the range I'm expecting it to and reward it accordingly.
Since my reward is based on the area which is f(l, b) = l*b, should I declare the observation space as the range of values my area can attain? Or should I declare my observation space as the values the length(l) and breadth(b) can attain? Or is my understanding wrong?
Area(a) = f(l, b) = l*b
Reward = +1 (if 15 < a < 20)
         -.1 (otherwise)

","['reinforcement-learning', 'terminology', 'environment']","
The observation space and the state space are not the same in general. There exist problems where the state space cannot be fully observed, which goes by the name Partially Observable Reinforcement Learning (or some variation of it). This is relevant in environments with imperfect information. To me, it does not look like you are dealing with partial observation here, therefore state and observation space are identical.
Let's say e.g. for a poker game the state and the observation space are different, because the state space is the space of all the cards and money of all players (+ the order of play) and the community cards, while the observation space from one agent is only his own cards + community cards and the money. The observation space does not include the cards of the opposite players.
"
Text Classification Model unable to learn,"
I am trying to build a text classification model. When I train the model it is unable to improve accuracy and at some point accuracy even decreases and loss increases.
I have researched for possible causes and its solutions but none of them seems to work.
I will first describe the model and then I will list different things that I have tried:
Data
I am using tweets as data labeled in 4 classes. I also use GloVe 50D embeddings.
My tweet cleaning function looks like this:
def clean_text(text):
    temp = text.lower()
    temp = re.sub(""'"", """", temp) # to avoid removing contractions in english
    temp = re.sub(""@[A-Za-z0-9_]+"","""", temp)
    temp = re.sub(""#[A-Za-z0-9_]+"","""", temp)
    temp = re.sub(r'http\S+', '', temp)
    temp = re.sub('[()!?]', ' ', temp)
    temp = re.sub('\["".*?\]',' ', temp)
    temp = re.sub(""[^a-z0-9]"","" "", temp)
    temp = temp.split()
    temp = [w for w in temp if not w in stopwords]
    temp = "" "".join(word for word in temp)
    
    return temp

So a sentence like: ""Ban lifted on Madrid doping laboratory"" is converted to ""ban lifted madrid doping laboratory""
headline    label
1   good    4
3   meet happiest dog   4
5   ban lifted madrid doping laboratory     1
8   malls dying hard profit death bfly  2
9   filipino troops kill notorious abu sayyaf kidn...   1
...     ...     ...
37797   6 leaders share advice younger  4
37799   beaver walks store finds artificial christmas ...   2
37801   cool transformers created everyday indian vehi...   1
37804   makeup challenge putting unknown beauty brands...   4
37809   35 gifts wanderlust obsessed person life    

As you can see I also remove stopwords from text.
One doubt I have is wether special tokens such as SOS, EOS and PAD should be added to the vocabulary and how to handle their embeddings.
Next I proccess this words and map them as indexes, and here I have tried adding special characters mentioned above as many pages suggest on the internet but I have also tried to only map the words in the sentence and post pad them with 0's as done in this notebook of deepLearning.ai sequence model course and none of them seem to solve my problem
LSTM Model
I build the embedding matrix
def build_embedding_matrix(vocab_size, emb_dim, word2vec, word2idx):
    
    emb_matrix = np.zeros((vocab_size, emb_dim))
    
    for word, idx in word2idx.items():
        emb_matrix[idx, :] = word2vec[word]
    
    return emb_matrix

emb_matrix = build_embedding_matrix(vocab_len, emb_dim, word2vec, word2idx)
emb_matrix.shape
(400004, 54) 

With the embedding matrix I build a Keras Embedding Layer
def build_embedding_layer(emb_matrix, vocab_size, emb_dim):
    embedding_layer = Embedding(input_dim=vocab_size, output_dim=emb_dim, trainable=False, mask_zero=True)
    embedding_layer.build((None,))
    embedding_layer.set_weights([emb_matrix])
    return embedding_layer

And with the embedding layer a build the full model
def clickbait_classifier(Tx, emb_dim, vocab_size, word2vec, word2idx, drop_rate):
    
    
    # define input layer
    sentence_indices = Input(shape=(Tx,), dtype='int32')
    
    # define embedding_matrix
    emb_matrix = build_embedding_matrix(vocab_size, emb_dim, word2vec, word2idx)
    
    # define embedding_layer (not trainable)
    embedding_layer = build_embedding_layer(emb_matrix, vocab_size, emb_dim)
    embeddings = embedding_layer(sentence_indices)
    
    # define lstm1 layer
    X = LSTM(128, return_sequences=True)(embeddings)
    
    # Dropout first LSTM
    X = Dropout(drop_rate)(X)
    X = BatchNormalization()(X)
    
    # define LSTM2 layer
    X = LSTM(128)(X)
    
    # Dropout second lstm 
    X = Dropout(drop_rate)(X)
    X = BatchNormalization()(X)
    
    # First dense layer
    X = Dense(32, activation='tanh')(X)
    
    X = Dropout(drop_rate)(X)
    X = BatchNormalization()(X)
    
    # Define Dense 2
    X = Dense(num_labels, activation='softmax')(X)
    
    # Build the model
    model = Model(inputs=sentence_indices, outputs=X)
    
    return model

clickbaitML = clickbait_classifier(Tx, emb_dim, vocab_len, word2vec, word2idx, 0.5)
clickbaitML.summary()

When I fit the model it get stuck on similar accuracy and loss even worsing over epochs
Epoch 1/25
168/168 [==============================] - 48s 245ms/step - loss: 0.8872 - accuracy: 0.3024
Epoch 2/25
168/168 [==============================] - 43s 258ms/step - loss: 0.8628 - accuracy: 0.2937
Epoch 3/25
168/168 [==============================] - 43s 254ms/step - loss: 0.8736 - accuracy: 0.2793
Epoch 4/25
168/168 [==============================] - 43s 255ms/step - loss: 0.8690 - accuracy: 0.2854
Epoch 5/25
168/168 [==============================] - 43s 257ms/step - loss: 0.8834 - accuracy: 0.2851
Epoch 6/25
168/168 [==============================] - 43s 257ms/step - loss: 0.8973 - accuracy: 0.2915
Epoch 7/25
168/168 [==============================] - 44s 259ms/step - loss: 0.9504 - accuracy: 0.2707
Epoch 8/25
168/168 [==============================] - 43s 258ms/step - loss: 0.9415 - accuracy: 0.2793
Epoch 9/25
168/168 [==============================] - 43s 258ms/step - loss: 0.9843 - accuracy: 0.2734
Epoch 10/25
168/168 [==============================] - 45s 268ms/step - loss: 1.0164 - accuracy: 0.2655
Epoch 11/25
168/168 [==============================] - 44s 263ms/step - loss: 1.0394 - accuracy: 0.2719
Epoch 12/25
 92/168 [===============>..............] - ETA: 19s - loss: 1.0438 - accuracy: 0.2665

Notes
Multiple optimizers with multiple learning rates were tried but the result is the same, the network doesn't improve
My data had 60% of class 1, 20% of class 2 and 10% for 3 and 4. I dropped randomly rows from class 1 and 2 so the dataset gets reduced, but the balance in the dataset is 25% for each class. This worsen the problem going from 60% stuck accuracy to 30% stuck accuracy.
I have tried simpler networks, did not work.
I have used datasets with only rows from class ""i"" and the network learns to output the class ""i"" so it is able to learn, it is just not working for some reason with 4 classes.
Any advices?
I do not really understand what is happening since the Emojify notebook mentioned above classifies text over 4 classes with 127 examples and it gets a decent accuracy. I have 38000 rows (17000 with balanced dataset) and it is not able to learn.
","['long-short-term-memory', 'text-classification', 'language-model']",
"""Following instructions"" as an emergent behaviour in transformer models - isn't this fundamentally different from the models' basic purpose?","
I am not technically familiar with AI or neural networks beyond a tech news reading level of knowledge, so I apologise if this is a dumb question.
I was recently reading this article on Ars Technica.  It is a high level description of the history of generative AI models (it's a very good article, highly recommended).
When discussing large language models, the following passage appears:

But there was also a surprise. The OpenAI researchers discovered that in making the models bigger, they didn’t just get better at producing text. The models could learn entirely new behaviors simply by being shown new training data. In particular, the researchers discovered that GPT3 could be trained to follow instructions in plain English without having to explicitly design the model that way. (emphasis added)

I have three questions:

Was this instruction-following behaviour truly emergent, as in completely unexpected and unplanned for?

This seems completely at odds with the usual description of transformer models as transforming text of one kind into another.  Following instructions seems, at a conceptual level, to be something much higher? If this is the case, do we have any idea how this emerged, and what properties of the model it is rooted in?

Do we have any idea how wide the scope of this kind of ""instruction following"" is? I.e. can the models make sense and respond ""sensibly"" (not correctly - I am not concerned with the correctness of the response, but with its relation to the instruction) to any instruction related to text?  Or are there specific kinds of instructions they are able to comprehend, and others they fail at?


","['neural-networks', 'transformer', 'gpt']",
How much energy consumption is involved in Chat GPT responses being generated?,"
I note this question was deemed off-topic, so I'm trying to clearly frame this question in terms of scope of response I'm interested in, namely ethics and sustainability issues associated with the soon-to-be proliferation of OpenAI Chat GPT types of tools for all manner of online information seeking behavior (from humans and other bots). This is not a programming or specific hardware question.
On average, how much energy is consumed for each response that Open AI's public chatgpt-3 provides?
i.e. what is the energy to run the entire system for 24 hours divided by the number of responses generated in 24 hours (ignoring energy consumed to train the system or build the hardware components).
How does this compare to a Google/Duck Duck Go/Bing search inquiry?
I read somewhere an OpenAI employee on the ChatGPT team that the computer power used to provide responses to queries is ""ridiculous"", and there's documentation of the size of the memory requirements of hosting servers and parameters but without knowing its throughput for example it's hard to quantify the energy consumption.
I often get more interesting results from Chat GPT than Duck Duck Go on certain types of queries where I used to know the answer but cannot remember the answer. IN these cases I can fact check for myself, I'm looking for a memory prompts with names and jargon that will remind me.
Also when seeking out counter-views to my own (say critiques of degrowth or heterodoxy economics concepts) Chat GPT is good at providing names and papers/reports/books that critiques the view I provide it.
In many cases more usefully than conventional search engines. Therefore, I can see the popularity of these tools ballooning rapidly, especially when the operational costs CAPEX + OPEX of the servers and maintainers is borne by large amounts of seed funding (eg OpenAI) or any other loss-leader startup wishing to ride the next wave of AI.
The heart of my question is ""at what externalized costs do we gain these tools in terms of greenhouse gases, use of limited mineral resources, GPUs scarcity etc.""
","['social', 'ethics', 'chatgpt', 'green-ai']","
Sam Altman states ""probably single-digits cents"" thus worst case 0,09€/request.
I guess a least half the cost are energy at a cost of   0,15€/1kWh, a request would cost 0,09€/request*50%/0,15€/1kW=0,3kWh/request = 300Wh per request. 60 Smartphone charges of 5Wh per Charge ;)
Source:https://www.forbes.com/sites/ariannajohnson/2022/12/07/heres-what-to-know-about-openais-chatgpt-what-its-disrupting-and-how-to-use-it/
Google Search request 0.0003 kWh = 0,3Wh, thus a search request by Google uses 1000x less, but as Google has started to use AI to, probably a search consumes more by now as well.
Source: https://store.chipkin.com/articles/did-you-know-it-takes-00003-kwh-per-google-search-and-more
"
Is there merit in sampling dropout from a more complex distribution?,"
In practice, Dropout is typically applied uniformly over hidden neurons in a network. Is there merit in sampling dropout from a more complex distribution? For example, would learning a data-independent Dropout probability distribution for a set architecture be feasible or useful?
","['neural-networks', 'regularization', 'dropout']",
Is there a test to determine if a feature space is sufficient for a classification problem?,"
I am working with GNNs for a node classification problem, and I am only able to achieve about 50% accuracy for the training set. I am not able to overfit the model to the training data. This makes me think that the feature space does not sufficient describe the data I am trying to classify.
Is there a test to check if the feature space is sufficient for a problem? I expected overfitting to the training data to be possible, but I am wondering if my feature are just not adequate.
","['neural-networks', 'graph-neural-networks', 'feature-selection', 'representation-learning', 'gnn']",
YOLO v1 confidence score during inference,"
I am studying the paper of Yolo v1. For the training part, everything is clear to me.
I cannot understand how does the confidence works during the inference stage, since Pr(Object) and ground truth are unknown (IOU can't be computed). Moreover, how is the non-maximal suppression applied, as it is necessary to sort bounding boxes by confidence score?
","['papers', 'yolo', 'non-max-suppression']",
Has anyone here tried to implement MADDPG for a different environment and succeeded?,"
Has anyone tried implementing the multi-agent RL algorithm MADDPG (I've linked the paper below)? The paper seems to have a good amount citations, and they do have their code on github. However, a few people on the internet have mentioned that while this algorithm works fine with the particle environment used in the paper, it does not work for other environments. Has anyone here tried to implement MADDPG for a different environment and succeeded?
For reference, I am working on a multi-agent reinforcement learning problem with heterogeneous agents.
Paper: Multi-agent actor-critic for mixed cooperative-competitive environments
","['reinforcement-learning', 'deep-rl', 'papers', 'actor-critic-methods', 'multi-agent-systems']",
How the proof of the contraction of variance for distributional Bellman operator follows,"
I am stuck at the proof of the contraction of variance for distributional Bellman operator from the paper, in which it is defined as

and the proof is stated as

In its second part, how is the  variance of the target distribution equal to the expectation of the  variance terms over the next state-action pairs?
","['reinforcement-learning', 'bellman-equations', 'conditional-probability', 'bellman-operators']","
This is a common trick in reinforcement learning literature which uses the law of large numbers to use the sampled variables $X$ and $A$ instead of $x$ and $a$. Let's say we know the probability $p(x)$ with which a continuous $x$ is given, then we can calculate the expectation value of $x$ as:
\begin{align}
\mathbb{E}_{x \sim p}[x] &= \int p(x)x ~dx \\
              &= \lim \limits_{N \to \infty}\frac{1}{N}\sum_{N} X
\end{align}
In the last line, we did not assume any knowledge of $p(x)$ but we averaged over the samples taken from $p(x)$. This would be the only way to find the expectancy value for an unknown system, e.g. the average speed of a driver, of whom we don't know the exact driving behaviour.
We can also use the sampled variables for an expression of the variance
\begin{align}
\mathbb{V}(x) = \int p(x)(x - \mathbb{E}_{x \sim p}(x))² dx  = \lim \limits_{N \to \infty} \frac{1}{N} \sum_{N} (X-\mathbb{E}[X])²
\end{align}
From here, two things could happen, since the notation in the paper is not quite clear.

Define $\mathbb{V}[X] = (X-\mathbb{E}[X])²$ then
\begin{equation}
\lim \limits_{N \to \infty} \frac{1}{N} \sum_{N} (X-\mathbb{E}[X])² = \lim \limits_{N \to \infty} \frac{1}{N} \sum_{N} \mathbb{V}[X] = \mathbb{E}[{\mathbb{V}[X]}].
\end{equation}
But this would be a rather confusing notation since $(X-\mathbb{E}[X])²$ is only sample whose expecation value is the variance.
Since the variance already is an expectation value (the expected, absolute difference from the mean), we can add an additional expectation value, which basically does not do anything:
\begin{equation}
\lim \limits_{N \to \infty} \frac{1}{N} \sum_{N} (X-\mathbb{E}[X])² = \mathbb{V}[X] = \mathbb{E}[\mathbb{V}[X]] = \lim \limits_{N \to \infty} \frac{1}{N} \sum_{N} \mathbb{V}[X] = \lim \limits_{N \to \infty} \frac{1}{N} N\mathbb{V}[X] = \mathbb{V}[X].
\end{equation}
I lean towards this explanation, but still introducing a useless expectation value is quite strange.

Now, let's use this in order to explain why:
\begin{align}
\mathbb{V}(P^{\pi}Z_{1}(x, a)) - \mathbb{V}(P^{\pi}Z_{2}(x, a)) \\
 = \int \pi(a) p(x)(P^{\pi}Z_{1}(x, a) - \mathbb{E}_{x \sim p, a\sim \pi}[{P^{\pi}Z_{1}(x, a)}])^{2}~dxda 
- \int \pi(a) p(x)(P^{\pi}Z_{2}(x, a) - \mathbb{E}_{x \sim p, a\sim \pi}[{P^{\pi}Z_{2}(x, a)}])^{2}~dxda \\
= \lim \limits_{N \to \infty} \frac{1}{N} \sum (Z_{1}(X', A') - \mathbb{E}[Z_{1}(X', A')])^{2} - (Z_{2}(X', A') - \mathbb{E}[Z_{2}(X', A')])^{2} \\
= \mathbb{E}[\mathbb{V}(Z_{1}(X', A')) - \mathbb{V}(Z_{2}(X', A'))] \\
\end{align}
Where in the last step one can use any of the two possibilities given above. I abbreviated $p(x') = p(x'|x, a)$ and $\pi(a) = \pi(a|x)$.
"
"Do the mathematics of Go imply an AI could solve it, or does a complexity bound imply AI skill will plateau?","
Based on the mathematics of Go and the machine learning algorithms used to play it, is there a mathematical limit as to how much of the game-tree space even an AI could learn, because of the inherent complexity / computational bounds (number of possible states of the board) of the game Go? Are we approaching a technological limit given current computer hardware (even massive data servers)?
","['game-ai', 'go']","
In the world of AI 100 percentage accuracy is almost impossible it is always a continous learning , The models which learns the game typically fall under Reinforment learning which learns the game by doing the mistakes, if a model did a mistake it punish itself with a quantative measure for that mistake and reward itself for a right move. 
In a virtual setting the 100 percentage acuracy is only possible if the model makes all the possible mistakes in all kind of setting(placement of coins) on the board ,which was quite impossible to generate that many combinations but with the today's computation power we had reach almost 70% to 75%(guess) of the combinations but still computing for the combination is still a challenge
"
Why do we need both the validation set and test set?,"
I know that this has been asked a hundred times before, however, I was not able to find a question (and an answer) which actually answered what I wanted to know, respectively, which explained it in a way I was able to understand. So, I'm trying to rephrase the question…
When working with neural networks, you typically split your data set into three parts:

Training set
Validation set
Test set

I understand that you use the training set for, well, train the network, and that you use the test set to verify how well it has learned: By measuring how well the network performs on the test set, you know what to expect when actually using it later on. So far, so good.
Now, a model has hyper parameters, which – besides the weights – need to be tuned. If you change these, of course, you get different results. This is where in all explanations the validation set comes into play:

Train using the training set
Validate how well the model performs using the validation set
Repeat this for a number of variants which differ in their hyperparameters (or do it in parallel, right from the start)
Finally, select one and verify its performance using the test set

Now, my question is: Why would I need steps 2 and 3? I could as well train multiple version of my model in parallel, and then run all of them against the test set, to see which performs best, and then use this one.
So, in other words: Why would I use the validation set for comparing the model variants, if I could directly use the test set to do so? I mean, I need to train multiple versions either way. What is the benefit of doing it like this?
Probably, there is some meaning to it, and probably I got something wrong, but I can't figure out what. Any hints?
","['neural-networks', 'machine-learning', 'ai-design', 'datasets', 'hyperparameter-optimization']","
Simply stated, you use your validation set to regularize your model for unseen data.
Test data is completely unseen data, on which you evaluate your model.
Various validation strategies are used to improve your model to perform for unseen data. So strategies like k-fold cross-validation are used.
Also, the validation set helps you in tuning your hyperparameters such as learning rate, batch size, hidden units, number of layers, etc.
Train, Validation, Test sets help you in identifying whether you are underfitting or overfitting.
E.g. If human error at a task is 1%, train error is 8%, validation error is 10%, test set error is 12 % then,
Difference between, 

Human level and training set error tells you about ""Avoidable Bias""
Training set error and Validation set error tells you about ""Variance and data mismatch""
Validation set error and Test error tells you about ""degree of overfitting"" with the validation set.

Based on these metrics, you can apply appropriate strategies for better performance on validation or test sets.
"
Training YOLO - networks: Is it possible to detect multiple objects while training with single-object data only?,"
I am currently training a YOLO network in which objects are to be recognized. The data I am using is artificially generated.
So far, my training data set consists of images with one object and its bounding box. In the test data set, however, there are images that contain several objects. (Additional:The objects all have the same label/class, let's say ""for example: objects are always dogs"")
Up to now, I thought that the YOLO network simply learns the pattern of the object plus the corresponding position, and therefore more objects are recognized if this pattern occurs several times, even if the training set only inherits single-object images.
Does this make sense? Or is it necessary to include ""multi-object"" data sets in the training?
","['deep-learning', 'yolo']",
YOLOv7 fine tuning with unbalanced dataset,"
I have a question about training object detection models especially using the YOLOv7 algorithm.
I use a Soccer Players Dataset from the Roboflow. The dataset is very unbalanced.
I trained the model using basic hyperparameters and it detects all objects on images but the predicted label is invalid. The model predicts everything as a player (but for example one person on an image is a referee).
How can I improve label prediction accuracy?
","['object-detection', 'pytorch', 'object-recognition', 'yolo']",
Can A* be non-optimal if it uses an admissible but inconsistent heuristic with graph search?,"
The book ""Artificial Intelligence: A Modern Approach"" (4th edition, global version) says

""With an admissible heuristic, A* is cost-optimal..."".

An admissible heuristic is one that never overestimates the distance to the goal, while a consistent heuristic is one that satisfies the triangle inequality, meaning that the cost of reaching the goal through a particular path is no more than the cost of reaching the goal through any other path. A* is cost-optimal in a tree search when using an admissible heuristic function, but it needs the heuristic function to be both admissible and consistent for optimal results in graph search.
I'm a bit confused by the AIMA statement. Are they saying that A* can be non-optimal if it uses an admissible but inconsistent heuristic with the graph search version?
","['search', 'norvig-russell', 'admissible-heuristic', 'optimality', 'consistent-heuristic']","
TL;DR: All A* requires to find the optimal path is an admissible heuristic
I'll read that section of the book for more clarity and extend this answer; though, I believe the way to interpret that statement is as follows:
Heuristics help reduce the number of computations required to reach the goal node. An admissible heuristic guarantees that the path we find is the optimal path but we might have made a few unnecessary computations along the way. An admissible and consistent heuristic helps to remove these unnecessary computations while finding the optimal path.
Further clarification:

""With an admissible heuristic, A* is cost-optimal..."".

Means that the path you find using A* with an admissible heuristic is always the the optimal path. It does not mean that you'll find that path as fast as possible.
For some additional mathematical analysis on the proof of A* optimality you can read a proof I wrote here. I developed this proof because I found the proof in AIMA to be unsatisfactory.
"
Training a neural network to assess audio samples that harmonize well together,"
I want to train a neural network that, given a set of audio samples, can determine a subset those samples that harmonize well together. The neural network should take auditory features of a set of audio samples as input, output a subset of the input, and receive human feedback in the form of a rating (e.g. an integer between 1 and 5) which rates the harmonic compatibility of the audio samples corresponding to the network's output. To me, this seems to invite an implementation of reinforcement learning from human feedback, since ""harmonic compatibility"" can only properly be assessed by humans (by harmonic compatibility, I pretty much mean how ""good"" a set of audio samples sound together when merged/overlayed). Does this seem to be the appropriate type of artificial intelligence? And, if so, are there any sources or examples that could help me with a quick start into an implementation?
","['reinforcement-learning', 'audio-processing']",
Would a transformer trained on highly specific material be as usable as a commercial product like ChatGPT?,"
Soft question here.
I was recently learning a bit about how it is feasible to train a transformer on a personal computer like an M1 Mac. I have been told that the model could have 1-3 million parameters and the training data could be from 1GB - 1TB, and that the training could take from about a day to a week. Also, there is an open source GPT here.
My question is, if you consider that ChatGPT is trained on a very large and diverse amount of data, you may think a solo project could never compete with it. However, what if you chose a specialized set of training data that was smaller but a much richer, more reliable knowledge base, like only academic science textbooks, or only English literature, or only Python libraries documentation, and so on?
Could it actually be much more useful because it's open-source, you have freedom of use (unlike ChatGPT's heavy behavioral conditioning from OpenAI), and you can choose what kind of knowledge the transformer contains? If the data is smaller but way, way higher quality, could you just make a library of niche GPTs for any topic you are studying?
","['transformer', 'gpt', 'language-model', 'fine-tuning', 'chatgpt']",
Is Momentum a form of inexact line search?,"
Is Momentum a form of inexact line search? I just want to know whether Momentum is considered a subset of inexact line search.
By, inexact I mean that the step size is not the perfect one that minimizes $$f(xk+α pk)$$
. I'm using the same definition as wikipedia and the Nocedal Numerical Optimization textbook.
","['gradient-descent', 'momentum']",
"What is the ""state of the art"" in (at least partially) symbolic/logic-based chat bots/AI assistants?","
ChatGPT has had a lot of buzz around it recently, and for good reason. It has shown some amazing capabilities in responding to new information, as well as in generalizing new information that has been given to it.
However, ChatGPT also regularly makes logical and mathematical errors. Some have commented that it really tries its best based on the data that was given to it to ""sound correct"" (almost like a freshman ""bullshitting"" a term paper), but it not actually great in general at making logical inferences.
One of the ways this issue could be solved is, rather than using a pure large language model (as essentially I understand ChatGPT is doing), we can instead use a semantic parser to parse natural language sentences into a logical form, such as first-order logic, higher-order logic, various forms of modal logic -- or what have you. This approach can also be supplemented with neural methods -- for instance, either by directly training a semantic parser as a sequence-to-sequence problem, or by learning ""weights"" for the likelihood of different parses.
I know that before the ""first AI winter"", the more symbolic/logical approach dominated, and a lot of work was done in Prolog, so I could probably find some examples of what I am looking for by looking at said historical approaches.
However, what I am looking for is a state-of-the-art chat bot based at least partially on symbolic techniques (specifically, using semantic parsing + a logical inference engine).
Does such a thing exist? Has much progress been made on that front since the first AI winter? I'd especially be interested in approach that combine neural/deep learning with symbolic approaches in some way.
References would be appreciated, but I'm mainly looking for concrete open-source implementation so I can play around for myself with the ""state-of-the-art"" and see what the current best offerings are, and their limitations.
","['chat-bots', 'logic', 'natural-language-understanding', 'symbolic-ai', 'neurosymbolic-ai']","
In recent times, 'neural-symbolic' approaches have again increasingly emerged that are used in the different parts of dialogue systems, e.g. NLU (Intent/Slot), Semantic Parsing, Dialog State Tracking, Response Generation. Studies are testing the integration of symbolic knowledge graphs or answer set programming.
Have a look at papers such as Conversational Neuro-Symbolic Commonsense Reasoning or Logical Reasoning for Task-Oriented Dialogue systems.
I am not aware of any fully implemented open-source chatbot based on this principle. In industry, tools such as LUIS and RASA are mostly used to build AI assistants.
"
Best methods to square rectangular images for OR,"
I've read previous posts that assert using one of these solutions:

crop and or resize
nn input size independent

In my case, I am using some tensorflow models and afaik they report a fixed size like 128x128 images.
I have tried a few resizing methods to get it to square images, but they alter the aspect ratio.
My idea to try -but want to ask before because it will be somehow time consuming- is to pad the image with zero valued pixels until they are square, either before or after resizing.
Is this last approach used and better than the previous ones, any other ideas?
","['tensorflow', 'image-recognition', 'keras']",
"How does the memory augmented neural network work, and how to make a simple implementation?","
How does the memory augmented neural network (MANN) work? How can I make a simple MANN with a vanilla neural network especially without a recurrent network?
","['neural-networks', 'deep-learning', 'tensorflow', 'pytorch', 'memory']","
If you're referring to the Meta-Learning with Memory-Augmented Neural Networks paper, the premise is that
In some cases, we wish to perform meta-learning when we are only provided with small (or no) data. For example - in one/few-shot learning we are expected to provide a label with only a few tagged samples. To do so, we use two techniques

External memory module - we initialize a container of knowledge that is called upon to respond to challenging circumstances
Label shuffling - labels are presented one time-step after their corresponding sample, which helps with dealing with simply learning the mapping $sample\rightarrow label$ without generalizing. More specifically, we feed the network with $(x_1, null),(x_2,y_1),...,(x_{t+1},y_t)$

In addition, samples are shuffled across different datasets (samples from different datasets may appear in the same sequence). This encourages the network to use the memory module and extract the relevant label once the corresponding sample is provided.
In terms of actual implementation, memory retrieval is based on a general similarity between vectors (I suggest going over the paper for more details).
"
Why business experts should prefer state-of-the-art deep neural networks over simpler models? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 6 months ago.







                        Improve this question
                    



I have encountered this pattern for a long time (5+ years). So many professionals come with an interesting domain-specific problem, and they demand using state-of-the-art deep learning models: take it or leave it.
I understand that technology advances faster than ever, but I am still missing the point. Indeed, I often propose using simpler, traditional ML models over complex ones because, for example, in an MLOps scenario is better to start with simpler models and then move to more complex ones. But, unfortunately, business experts often seem to be disappointed with such kind of proposals. Moreover, there are many reasons to prefer classic ML, which I often use to motivate the latter.
","['deep-learning', 'state-of-the-art', 'mlops', 'commonsense-knowledge']","
Short answer: because they are not experts in ML (and they should not be, otherwise they won't be asking), but are bombarded by buzzwords e.g. AI, blockchain, ChatGPT.
Do you have any friends who put their whole lifesaving in cryptocurrency without any idea what they bought? Same thing. This is human nature.
"
How to create a global forecasting model using deep learning?,"
I am aiming to build a global/general forecasting model (don't know what's the proper terminology) using deep learning. The idea behind this is to create a model trained on several time series that will allow me to obtain forecasts for the time series used during training and for others not used during training.
I think this is a mixed problem, classification-regression. If I am correct and this is a mixed problem then: How should I do it?

Should I work only with the variable/s I aim to forecast (using lagged observations). I mean passing through the model in question (I think it could be a CNN-LSTM or a LSTMGC model) only the objective variable/s. If this is an option I would like to be explained or pointed to an explanation of how this model works and how should the data for the training be structured.

Or should I pass to the model the categorical variables as well. Being in the case of ""product sales"" variables like region, type of product the categorical ones and sales or amount of selled items the variables to forecast. Regarding to this I have the same doubt as before. (In addition think this could be a model easier to interpret)


Will be of a great help if anyone could point to or explain to me if there is any methodology on how to solve this kind of problems using deep learning. Aspects like what are the most typical ANN structures to solve this kind of problem, how should the data be prepared and how should the model be trained.
Thanks in advance for all the help.
","['neural-networks', 'classification', 'regression']",
What AI model is best for finding optimal sequence taking into account sequence length?,"
I'm trying to find the best AI model to solve the following problem:
I have a list of abilities:
Ability 1 - Deals X1 amount of damage. Can be used every Y1 turns.
Ability 2 - Deals X2 amount of damage. Can be used every Y2 turns.
....
Generate a N length sequence of abilities that deals the most damage.
I have looked into RNN, however as far as I understand it it does not take into account what the output size is into account. Is there something more suited to solve a problem of this type?
","['neural-networks', 'recurrent-neural-networks']",
"What is the difference between the term ""generative"" in classical machine learning and deep learning?","
There are lots of explanations on DGM (Deep Generative Model) and generative classifier (most of the explanations on which are about generative classifier vs discriminative classifier)
But, I can hardly find the common parts between the two concepts. In my understanding, 'generative' from DGM is quite straightforward - it almost goes the same with its literal meaning. In the contrary, 'generative' from the comparisons with the discriminative model is a little bit technical but it's the one that took the word earlier than the former one. (Jordan and Ng, 2002).
Is it just that these two concepts are not really unrelated? Were they just used just because they do produce some distributions while learning?
","['machine-learning', 'deep-learning', 'terminology', 'generative-model']","
Generative models have in common that they all model the distribution of the training samples. This makes it possible to sample from a distribution that is (hopefully) similar to the training data distribution. Different architecture types such as energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, and numerous hybrid approaches can all be considered generative. For more information see this review.
The same meaning is true for generative classifiers, they are called this way because they model how a particular class would generate its associated data. Discriminative classifiers learn to detect features to distinguish between the various possible classes. This visualization from here might be helpful in this regard:

"
Why does ChatGPT not give the answer text all at once?,"
When ChatGPT is generating an answer to my question, it generates it word by word.
So I actually have to wait until I get the final answer.
Is this just for show?
Or is it really real-time generating the answer word by word not knowing yet what the next word will be?
Why does it not give the complete answer text all at once?
","['natural-language-processing', 'language-model', 'chatgpt']","
ChatGPT is a conversational-agent based on GPT3.5, which is a causal language model. Under the hood, GPT works by predicting the next token when provided with an input sequence of words. So yes, at each step a single word is generated taking into consideration all the previous words.
See for instance this Hugging Face tutorial.
To further explain: while outputting entire sequences of words is in principle possible, it would require a huge amount of data, since the probability of each sequence in the space of all sequences is extremely small. Instead, building a probability distribution over half a million of english words is feasible (in reality just a tiny fraction of those words is often used).
On top of that, there may be some scenic effect, to simulate the AI ""typing"" the answer.
"
Why is $l$-step lookahead better in RL?,"
The following is from the book ""Reinforcement learning and optimal control"", by D. P. Bertsekas.
Chapter 2, page 52:
""The motivation for $l$-step lookahead is that for increasing values of $l$,
one may require a less accurate approximation $\tilde{J}_{k+l}$ to obtain good
performance. Otherwise expressed, for the same quality of cost function approximation,
better performance maybe obtained as $l$ becomes larger. This makes intuitive sense,
since in this case, the cost of more stages is treated exactly with optimization.""
My question:
Why is it that for increasing values of $l$,
one may require a less accurate approximation $\tilde{J}_{k+l}$ to obtain good
performance?
",['reinforcement-learning'],"
Let's first start with the intuitive explanation. If you just use q-learning (or any other temporal difference method like SARSA) you usually just look ahead one step. The other extreme is a monte-carlo method where you don't rely on estimates of the state-value function at all (your policy might depend on an estimate, but your updates don't). Monte-carlo methods can be viewed as an $\infty$-lookahead. So $l$-step lookaheads are somewhere in between (they take $l$ rewards in a monte carlo fashion and then only bootstrap from there), so they reduce the dependency on the estimates by quite some margin. Therefore you can have an estimate of the state-value function with higher variance for $l$-step lookahead as you could with only $1$-step lookaheads.
Now let's have a look at a more quantitative explanation. You are looking for the error reduction property (see derivation in Sutton-Barto notation) of $l$-step lookaheads, which can generally be given by
\begin{equation}
\min\limits_x \Bigl\lvert \mathbb{E}_{\pi}[T^{l-1}\tilde{J}_{t}| X_{t}=x] - J_{\pi}\Bigr\rvert \le \gamma^{l} \min\limits_s \Bigl\lvert \tilde{J}_{t} - J_{\pi}\Bigr\rvert
\end{equation}
This shows that the $l$-step lookahead will always have lower errors than a simple $1$ step solution. Hence one can have worse estimations to achieve the same quality of results with multi-step learning.
"
Framing a resource allocation problem without sequential action for Reinforcement Learning,"
I am attempting to get some insight into framing a resource allocation problem using Reinforcement Learning (RL) where there seem to be no sequential actions (I might be incorrect here).
In my situation I have an Agent Based Model (ABM), where when I input some resources the model (ABM) conducts some simulation and outputs the number of casualties survived. In an RL context this ABM would perhaps act as the environment (Note in this instance the way the agent interacts is by changing the number of resources, but its not really sequential). The reason why I say actions are not really sequential is because for every episode I input some action and I will instantly get the reward from the simulation and the episode ends.
Inputs : {num-ambulances, num-doctors, hospital-bed, ...} -> (Env) : medical-response-Agent-Based-Model -> output : num-casualties-survived
The goal is to find the optimum resources that would give the best number of casualties survived (More likely a cost-benifit score but lets say no of casualties for now). The obvious approach is to perhaps use sensitivity analysis, but I was wondering if it can be framed in RL approach. I am currently thinking down the lines of the following for the action, state spaces and reward signal, however there are clearly some problems.
A : {num-ambulance-increment : 0, num-ambulance-constant : 1, num-ambulance-decrease : 2, num-doctors-increment : 3, ...} as the action space
S : {num-ambulances, num-doctors, hospital-beds, ...} as the state space
R : casualties survived or some cost-benifit-score
The problem as mentioned earlier is that the model immediately gives a reward (input states to ABM which gives reward) instead of the agent interacting with environment over multiple actions to get the reward. As such there is no process of sequentially picking actions !
I have tried my best to illustrate this by amending the code (SARSA algorthim) for a grid-world problem (from udemy), to what might look like this problem.
action_values = np.zeros(#num_states, #num_actions) #not really sure of the shape but just for illustration

def policy(state, epsilon=0.2):
    """"""Just some random policy""""""
    # Exploration
    if np.random.random() < epsilon:
        return np.random.randint(4)
    # Exploitation
    else:
        av = action_values[state]
        return np.random.choice(np.flatnonzero(av == av.max()))

def sarsa(action_values,policy,episodes, alpha=0.1, gamma = 0.99, epsilon = 0.2):
        
    for episode in range(1, episodes + 1):
        state = env.reset() #reset evironment at the begining of each episode
        action = policy(state, epsilon) # select an action
        
        # done = False # This will not be required anymore as in one go we get the reward

        #while not done: # Not required again due to reason above

        next_state, reward, #(done), _ = env.step(action) #take the action and see what happens
        next_action = policy(next_state, epsilon) # get the next action that you might perform
        
        # Update the Q value for the current state we are in and the next state - SARSA EQ
        qsa = action_values[state][action] #get Q(s,a) for current state
        next_qsa = action_values[next_state][next_action] #get Q(s',a') for next state
        action_values[state][action] = qsa + alpha * (reward + gamma * next_qsa - qsa) #Update Q value table
        
        # will not really require as there is no while loop
        # state = next_state # update the state, in the next iteration you know where you are
        # action = next_action # update the next_action so that in the n


So the problem I see is that for framing the problem this way I dont have inner loop where the agent takes actions. Rather every episode terminates in one go as the Agent Based Model will do the simulation for these specific resources and return a reward (casualties survived). So I am wondering since there is no sequential actions, if it can be solved by RL to begin with? Or is this some sort of bandit based problem?
Any information for similar problems or a different way framing it to work with a RL approach is welcome
Thanks
","['reinforcement-learning', 'multi-armed-bandits']",
Using ML to uncover procedural logic,"
The game Elite Dangerous has a proceduraly generated galaxy of some 400 billion star systems.
Each star system in the game can be uniquely identified bu a 64bit number (id64) which is used as a seed for building the system but can also be decoded down to give some information about the system.

Boxel coordinates: x y z coordinates of a cube in which the system
can be located.
Masscode: A number that determines the size of the
boxel and the composition of the system

What we cannot decode from the id64 is the actual x,y,z coordinates and the boxel coordinates are too imprecise to be useful. Being able to preduct the x,y,z coordinates would allow us to make star maps of unvisited stars.
However the actual coordinates must be derived from the id64, some pseudo random number function and /or some rules grounded in real world astronomy.
Through the game journal files we can get the id64 and the actual coordinates of the system in 3D space and there are databases of player captured data containing about 70million systems we can use for test and train.
I took this data and created a file containing id64,bx,by,bz,masscode and x,y,z
(bx,by,bz is the boxel coordinates)
Using the features id64,bx,by,bz,masscode I want to be able to predict to within a light year, the x,y,z coordinates of the system.
I tried a few different Models from scikit-learn to see if I could train one to get coordinates but so far had little luck using various models. I had hoped that having a training set in the training/test set in the millions might help.
Is there a model or approach that might be better suited to this kind of problem? I guess if there is a psuedo-random element to it, then it might not be possible?
",['scikit-learn'],
How does off-policy Monte Carlo weighted importance sampling bias converge to zero (Sutton & Barto Section 5.5),"
On Section 5.5 (page 105) of Sutton & Barto's ""Reinforcement Learning: An Introduction"", they discuss the off-policy Monte Carlo method for learning the value function of a target policy $\pi$ from a behavior policy $b$ using ordinary importance sampling and weighted importance sampling.
At some point they say the weighted importance sampling estimator for $v_\pi(s)$ (the target policy value) has an expectation equal to the behavior policy value $v_b(s)$, and is therefore biased. So far, so good.
However, they then say that the bias asymptotically converges to zero. I am confused by this, since in this scenario (value estimation) the policies are fixed and $v_\pi(s)$ is distinct from $v_b(s)$ (more precisely, they are not required to be the same and in most cases will not be). If the estimator's expectation is always $v_b(s)$, then it is asymptotically equal to $v_b(s)$, which means it is asymptotically distinct (in principle) from $v_\pi(s)$, and therefore the bias would not asymptotically go to zero.
What am I missing?
Update after answer: what I was missing is that I was interpreting ""asymptotically the estimator converges to $v_b$"" incorrectly. If we interpret ""asymptotically"" in the (incorrect) sense of ""obtaining infinitely many estimations over a fixed number of $n$ episodes and averaging the returns"", then indeed that is going to be biased towards $v_b$ (and in fact, the expectation will be exactly $v_b$ if we estimate using $n = 1$). The correct interpretation of ""asymptotic"" is doing a single estimation using an increasingly larger number of $m$ episodes. The expectation of that estimation converges to $v_\pi$ as $m$ goes to infinity.
","['reinforcement-learning', 'sutton-barto', 'monte-carlo-methods', 'off-policy-methods', 'importance-sampling']",
What is the Ideal Learning Trajectory in Deep RL?,"
I am trying to determine what kind of results are ideal when training a deep RL system. In contrast to say, supervised learning; where the network is guided by an expert every step of the way, the loss function is typically graphed as downward slope. Whereas in deep RL, my observations have been that if you graph the trajectory of the score the agent is producing, it tends to be more jagged. Where even if the agent achieves a high score at the end of it's training session, it's path to that destination isn't always sloped.
My interpretation of this is that the agent is exploring different areas of the action space; which would likely lead to better generalization when given data it has yet to see.
However I'm not sure. In my case I am training a DDPG system to plot points in space. On one occasion, the trajectory of the score is sloped upward on average. On another occasion with a higher capacity network, it arrived at a similar score (trained with the same number of episodes) but it's trajectory was jagged.
In deep RL, should we be looking for more of a jagged trajectory in learning or sloped?
",['reinforcement-learning'],
Would it be a problem to use non-squared images for a CNN model?,"
I want to create a music sheet scanner using CNN Model and the images I am using are not squared and, if I make them squared, important data will be lost and it might confuse the model.
Is it ok to use longer pictures? If yes, what sizes should I use for a picture like the one below?
For the CNN model I will have to use different scales for the layers, I will not be able to use the 3x3 layer anymore, right?
I am still a beginner, so I am learning these things now without having previous experience in this field.

","['machine-learning', 'convolutional-neural-networks', 'data-preprocessing', 'convolutional-layers', 'optical-character-recognition']","
I disagree with SickerDude43, and I think the linked SO post isn't relevant in this case. Well you haven't told us what does your network's model look like, but I assume it should understand the notation and output the notes, the key and maybe even tempo (sorry my musical education is rusty).
Your inputs are naturally wider than tall, and it doesn't make sense to for example pad them with white pixels or scale to a square aspect ratio. But if you use rectangular kernels, you cannot stack that many of them before your image's height becomes just one pixel while the width is still in the hundreds. That is why I suggest you to specify non-rectangular kernel shapes, so that the network's output dimension resembles more of a rectangle.
On the face value, your problem seems non-trivial if the notation on the left edge of the image impacts the correct interpretation of the rest of the notes. Then again, maybe a few fully connected (dense) layers can solve without problems.
"
Is it recommend to perform a time series analysis with a fixed set of validation data?,"
I'm currently working on a project in material science and the data to evaluate is very limited. I work with about 60 datasets, each with about 10.000 relevant lines.
However I want to predict a certain parameter which is not always measured nor frequently. To be exact: I have about 30 lines for that parameter spread across 8 of the 60 datasets.
I'm currently thinking of a workaround: I've seen solutions like simulating more data, but this is not possible in my case.
My idea was to set only these 8 datasets as validation data (or cross validate with 2/4 sets at a time). The rest would be always used for training.
Is this a viable approach? I'd also train more than one model with different approaches and then evaluate them all together. But I'm worried if the model will only predict the validation set as correct and the other ones are wrong, without me ever knowing. Since it's a costly process, I'll probably only get a few more datasets in the future.
Thanks in advance!
","['deep-learning', 'time-series', 'cross-validation']",
Please help me understand the role of loss function in neural networks,"
I've been studying NNs with tensorflow and decided to code a simple NN from scratch to get a better idea on hwo they work.
It my understanding that the cost is used in backpropagation, so basically you calculate the error between prediction and actual and backpropagate from there.
However, in all the examples I read online, even the ones doing classification, just use:
error=actual-prediction instead of: error=mse(actual-prediction) or: error=cross_entropy(actual-prediction)
And they leave mae/rmse etc just as a metric, as per my understanding (probably wrong) understanding, these should/could be used to calculate the error as well. On the other hand, while working with tensorflow, the loss function I use, does change the output and its not just a metric.
What's my error in here?
In other words, isn't the loss function same as the error function?
Example code (taken from: machinelearninggeek.com/backpropagation-neural-network-using-python/):
Note how the MSE is used as metric only, while backpropagation only uses pred-outputs. (E1 = A2 - y_train)
for itr in range(iterations):    
    
    # feedforward propagation
    # on hidden layer
    Z1 = np.dot(x_train, W1)
    A1 = sigmoid(Z1)

    # on output layer
    Z2 = np.dot(A1, W2)
    A2 = sigmoid(Z2)
    
    
    # Calculating error
    mse = mean_squared_error(A2, y_train)
    acc = accuracy(A2, y_train)
    results=results.append({""mse"":mse, ""accuracy"":acc},ignore_index=True )
    
    # backpropagation
    E1 = A2 - y_train
    dW1 = E1 * A2 * (1 - A2)

    E2 = np.dot(dW1, W2.T)
    dW2 = E2 * A1 * (1 - A1)

    
    # weight updates
    W2_update = np.dot(A1.T, dW1) / N
    W1_update = np.dot(x_train.T, dW2) / N

    W2 = W2 - learning_rate * W2_update
    W1 = W1 - learning_rate * W1_update
```

","['neural-networks', 'machine-learning', 'deep-learning', 'tensorflow']","
There are different functions to pick from for your loss.
The most common read online is the Mean Squared Error.
It calculates how far the prediction is off from the actual value and squares it. All of them are summed up.
So basically the Loss-Function ""generalizes"" the error over every prediction. The actual error in a case is just how far off it is.
"
Idea for generating time series with irregular time-intervals with GANs,"
I want to model time-series with irregular time-intervals using GANs. Think of the following (short) data sample
TimeOffset  |  Variable 1  |  Variable 2
    0              0.5            0.7
    12             0.4            0.8
    15             0.3            0.8
    19             0.5            0.7
    26             0.6            0.6

There are plenty of GANs fit for time-series such as RGAN, RCGAN, TimeGAN, which use recurrent networks (i.e. LSTMs)  as a generator and discriminator.
For modeling this particular time-series, would it be possible to just generate a TimeOffset variable as part of the data? So instead of generating 2 variables, the generator would just generate 3 variables, where the third indicates at what Time Offset that (sub)sample belongs? What would be the downsides of such an approach?
","['generative-adversarial-networks', 'time-series', 'generative-model']",
Combining neural network outputs via matrix multiplication,"
I'm replicating a particular network architecture which is very sparse with its details.
One part of said architecture is shown in the image, where h is a 1x1024 or 1024x1 output from a transformer and needs to be combined with the output of an MLP to produce a 1x55 or 55x1 vector, with the order of the dimensions not mattering. As such the matrix multiplication step should involve either multiplying (55 x k) and (k x 1), or (1 x k) and (k x 55) matrices. The input to the MLP is a (55 x 32) matrix flattened to a (1760 x 1) vector. For context, this input encodes item information from a game about 55 items. h contains information about the current game state and we want to produce a policy pi for selecting an item.
The constraints of the matrix multiplication mean that I need to perform some kind of dimensionality increase or reduction on either the elements of h or the output of the MLP. However, I'm not really sure what would be best in this case as I don't have much experience with building neural networks in general. Any advice would be appreciated.

",['neural-networks'],"
You can try padding, which compensates the dimension with numbers (e.g., zero). This practice is very common in deep learning to have a desire embedding.
"
DQN with experience history to learn from already saved - which reward should I take?,"
I want to train a DQN model in an off-policy fashion, where my behavior policy is an older agent. I have a big memory of a lot of episodes of this agent. Now I want to find a better policy using DQN. Now I am just wondering, in the ""normal"" DQN case you would use the experience replay buffer and would update behavior and target policy online (behavior not really online but with the time lag introduced after which these parameters are also updated).
In my case, I already have all the experience and would like to learn from it. Do you think it makes sense to use the exact same procedure in this context, so sampling one new action, state and immediate reward, follow up action or could it be better here to use the fact that all experience is already stored to exchange $R_{t+1} + \gamma  \max Q(S_{t+1},a)$ with some more future information about the rewards (up to the point of Monte Carlo where you take $G_t$ so the discounted cumulative reward seen during the episode from point t onwards)?
","['reinforcement-learning', 'deep-rl', 'dqn', 'experience-replay', 'offline-reinforcement-learning']","
More specifically than off-policy RL, you are looking at offline reinforcement learning techniques. In offline RL, all training data is known beforehand (stationary), which is in stark contrast to the usual non-stationarity of online RL data.

In my case, I already have all the experience

Are you absolutely sure of this? If your data doesn't contain transitions from high-reward regions of the state space, isn't generalizable to new situations, or is from very disparate and unconnected regions of the state space, then your agent could have trouble learning a good policy, let alone an optimal policy (see ""When and why does offline RL work?"" slide from this tutorial by Kumar and Levine).
If your data is indeed a good fit for offline RL, then there are certainly different updates that are recommended. In the same tutorial linked previously, the authors display some updates that are more conservative than traditional off-policy methods, especially when the data has high uncertainty. To answer your question - since you are currently using Q-learning, I suggest that you look into the conservative Q-learning (CQL) update as presented in the tutorial. To keep this answer brief and since I don't have much experience in this field, I will refer you to the corresponding survey paper for additional understanding and other possible updates.
"
sentence alignment between same language texts,"
Take this two texts as an example:

""I will start the recording and share the presentation. I hope it's
all clear from what we saw last time on classical cryptography and if
you remember we got to discussing perfect security. Ah, you're right,
I didn't upload the slides. Wait, they are the same as last time. If
you want to get the face on the fly, last time I mean last year. I had
some things to do and didn't upload.""


""Okay, I will start recording and I will share the presentation again.
1, 0 So I hope it's all clear compared to what we saw last time about
classic victory, And if you remember we got to let's say discuss the
perfect security. Ah you could see already, I didn't upload the
Slides, Wait they are the same as last time, Eh. If you want to have
done on the fly, I mean last time, I mean from last year. Morning I
had some stuff to do, I didn't upload.""

We want to align the sentences so that similar sentences (within a certain degree of difference) are being matched together.
Which are the approaches to solve the issue?
[I will start the recording and share the presentation], 
[I will start recording and I will share the presentation]


[I hope it's all clear from what we saw last time on classical cryptography],
[I hope it's all clear compared to what we saw last time about classic victory]

[and if you remember we got to discussing perfect security],
[And if you remember we got to let's say discuss the perfect security]
.
.
.

I have been looking into DTW and perceptual hashing as a way to solve the problem without any concrete result then I saw that in the field of automatic translation sentence alignment is widely used but with the assumption that the two texts have different languages and that there is a one-to-one mapping between words without ""gaps"" or extra words in between.
","['natural-language-processing', 'machine-translation', 'text-detection']","
First, you should first address the problem of 1. splitting sentences, and 2. the number of sentences in two texts. Then you can use a sentence embedding model and compare the similarity between the sentences in two sets of sentences.
The easiest way could be averaging the ready-made word embedding vectors (word2vec, fastText, etc.) of all words in the sentence and using the resulting vector as the embedding vector for the sentence. Or you can use Doc2Vec directly on the sentences. Then you can use cosine as a similarity criterion between the sentence embedding vectors.
Another easy way is to use the BLEU metric to compare the similarity of the sentences.
"
Why to use gradient accumulation?,"
I know that gradient accumulation is (1) a way to reduce memory usage while still enabling the machine to fit a large dataset (2) reducing the noise of the gradient compared to SGD, and thus smoothing the training process.
However, I wonder what causes the noise of the gradient? Is it because of the random sampling strategy on large datasets, intrinsic noise in datasets (like wrong labels) or anything else?
For a small dataset is it okay to do stochastic gradient descent and what might be the motivation for using gradient accumulation techniques on it?
","['deep-learning', 'gradient-descent', 'gradient', 'stochastic-gradient-descent', 'mini-batch-gradient-descent']","
This image from here nicely illustrates how gradient accumulation is performed:

Assuming infinite memory and compute we would be able to compute the gradient on the full batch, this would provide us with the true gradient!
In reality, the full batch gradient is often not possible to compute since holding the full dataset in memory is infeasible. The network weights are thus updated several times per epoch using mini-batches. Mini-batches provide an approximation of the true gradient. It is noisy since it is computed on only a part of the dataset. Alternatively, mini-batch gradients can be computed without updating the model immediately. Accumulating the gradients is referred to as gradient accumulation and if one accumulates the gradients during all steps, the sum of the gradients is the same as the gradient obtained on the full batch. There currently exists some debate about whether the loss landscape changes induced by mini batching might actually be beneficial.
For further information also see this blog post for the trade-offs that exist between full-batch and mini-batch learning.
"
Are there papers that do an empirical investigation on DRL hyperparameters?,"
I am looking for papers that perform a study on DRL hyper-parameters. This paper does a fantastic job of describing the hyperparameters for on-policy algorithms. It would be great to get similar papers like this, including works on other branches of RL. I already looked over the references of this ICLR paper and found this paper.
","['reinforcement-learning', 'deep-rl', 'papers', 'off-policy-methods', 'on-policy-methods']",
Combination of VAE with GAN,"
I am going to implement a lecture which it aims to generate new images. It uses a variational autoencoder to produce latent vector and then feed it to a gan network as input. My question is, in variational autoencoder each dimension of latent vector is sampled from a different gussian distribution  (sigma,mu). But I guess the generator in gan, gets input from one gussian distribution, so the input is sampled from one sigma,mu pair. So,is this idea really sensible? Or I understood the input of gan wrongly?
","['deep-learning', 'generative-adversarial-networks', 'variational-autoencoder']","
What you describe is very similar to VAE-GAN. Take a look at the paper.
From what I remember from it, I think they couple a VAE to a GAN discriminator, which is functionally very similar to coupling a VAE encoder to a GAN, as the VAE decoder and GAN generator have lots of similarities.
"
Why does A2C use the actual returns from an episode in calculating the advantage?,"
Why does A2C use the actual returns from an episode in calculating the advantage instead of using a bellman equation style estimate of the value?
Basically, why this:
$A(s,a) = \sum_t\gamma^tr_t - V(s)$
instead of
$A(s,a) = r + \gamma V(s') - V(s)$
This blog post seems to describe it one way, but their code is the other way:
https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f
I feel there's some big gap in my understanding, but would appreciate some insight.
","['reinforcement-learning', 'actor-critic-methods', 'bellman-equations']",
Do different architectures really make a difference or is it just a matter of the training process?,"
I was wondering which influence different architectures for deep learning truly have on the performance. Of course, substantial changes in the paradigms we use when building neural networks (such as convolutions or transformers opposed to simple feed forward networks) bring new possibilities for the network to extract features.
But latest when it comes to adjustments in the architectures many papers propose (like adding one layer here, a residual connection there...), does this really make a difference? In my opinion, with enough training and the right choice of hyperparameters, vanilla architectures should be capable to reproduce results that were generated with models that relied on adjustments of the architecture.
So the question is: Suppose method A (vanilla architecture) produces a result of 80% accuracy, but method B (small adjustments to this architecture) achieved 83% accuracy on the same dataset - is this truly significantly because of the architectural change or would method A be capable of also achieving 83%, if trained and tuned for a longer time? Is it really worth to finetune/enhance the few big vanilla architectures out there and to create variants of them? Or should research focus more on tuning hyperparameters and training procedures to improve results for specific tasks?
","['neural-networks', 'ai-design', 'hyper-parameters', 'architecture']","
The answer might be based on opinion, but yes it matters. The concept is called Exploration vs Exploitation.
Consider that you are standing on the top of a hill and you need to go down to the foot. There are practically infinite routes, so you can try some routes here and there. You have two methods here:

Exploration: just try until you find the optimal route. Now you see where this is going: there is no guarantee that it works and we don't have an infinite amount of resources
Exploitation: it is when you find a ""good enough"" route, but you continue to go down that route and ignore the rest.

Balancing Exploration and Exploitation is the way forward. Going back to your example, Exploration here means tuning till...the end, and Explotation here means ""test new methods and if it works, we use it"". I believe we tend to pick the second, since it is at least some progress rather than waiting forever.
"
Explicit representation learning task benefits than simply considering last nerual network layer,"
Neural networks are ineherently representation learners, so one could simply extract the last layer embedding $\textbf{z} \in \mathbb{R}^d$ of a neural network model and consider it as a representation of raw input data. But in a supervised ML framework, this representation would simply be created to optimize neural network predictions. I'm wondering what are the benefits of an explicit representation learning task as it might be contrastive learning instead and why it might be preferred to simply considering the last neural network layer.
","['neural-networks', 'representation-learning', 'contrastive-learning']",
What makes reproducing a model like GPT3/GPT3.5/ChatGPT difficult?,"
Is it difficult for other companies to train a model similar to ChatGPT, and what makes it difficult? What is challenging about reproducing the results obtained by OpenAI with ChatGPT/GPT3.5? Would it be possible for a company like Meta or Google to have a model equal to ChatGPT/GPT3.5 in the next month or so? Why or why not?
I understand that a big language model is expensive to train, so I'm expecting only large companies to be able to train such models to a sufficient extent.
","['training', 'language-model', 'large-language-models']","
Actually, Google created a bigger model than GPT-3 and models in the GPT-3.5 series, and consequently ChatGPT too (because ChatGPT is based on a GPT-3.5 model) - Switch-C has trillions of parameters, one order of magnitude bigger than the GPT models that I know of, and it was developed before ChatGPT was announced. I don't know how many parameters ChatGPT has exactly, but it shouldn't have more than several billions of parameters.
So, what makes reproducing a model like ChatGPT difficult for companies like Google? Definitely, not the lack of computational resources or money, but the lack of transparency. My impression is that Google also tends to be open-source, as opposed to OpenAI, which wants to make money of everything.
Moreover, I'd like to note that the GPT models have received a lot of hype, but there are other pre-trained models (e.g. Lambda or Switch-C), for example, developed by Google, that maybe should also have our attention. Google simply doesn't need to generate all this hype to get the money, as they still get most of their revenue from ads (the last time I checked)
"
What is the correct loss function for binary classification: Cross entropy or Binary cross entropy?,"
Let's say I have a binary classification problem and I want to solve it by means of FC neural net. So which approach will be correct: 1) define the last layer of NN like this linear(h, 1) and use Binary Cross entropy 2) define the last layer of NN like this linear(h, 2) and use Cross entropy. It seems for me that these two approaches are similar. Is it correct?
P.S. This question is more theoretical one, so no any technicalities related to particular framework.
","['neural-networks', 'deep-learning', 'classification', 'objective-functions', 'cross-entropy']",
Effectiveness of DNN training with reduced Batch randomness,"
So here's an example set to help explain my doubt. Suppose I have 80,000 total images available for a DNN training task. With a batch size of 32, that is 2500 batches.
Now let's say I partition the dataset into two bins of 40,000 images each. Now, for each training epoch, when I form a batch of size 32, I either form a batch from bin 1 or bin 2.
i.e., there is some reduction in global randomness in picking the batches to train on. Assume that the representation of the dataset is more or less consistent in both bins. (if it were a 4-way classification task, I have sufficient data points of all 4 targets in both of these bins).
My question is how much am I affecting the effectiveness of the DNN model (Accuracy, Convergence, Bias) by using this mechanism of training?
I can use basic Permutation and Combination theory to understand that I am obviously losing out on some batches that can not be formed across the bins. Also, to what extent can I apply this binning? What If i form three bins of 20k, 20k, 40k and apply the same concept?
Kindly help me understand. Mathematical guarantees / approximations would be appreciated, but an intuitive explanation is also welcome.
thanks!
","['convergence', 'training-datasets', 'bias-variance-tradeoff']",
Causal Inference: Understanding the impact of an intervention,"
I'm trying to create a pipeline for a very common business scenerio. I want to see whats the impact of an intervention on an outcome. For example I want to know if I send a marketing email (intervention) to users, how would it impact their propensity to buy (outcome).
To be specific there are 3 things I want to know:

What is the average impact of the intervention(marketing email) on the outcome(purchase)- This will tell me if sending emails is generally a good idea or not?
Who should we email, and who we shouldn't email (uplift modelling)- This will help me customize the treatment at individual level.
How can I be confident in the above mentioned model's findings, without having to run the A/B tests? - This will help me understand if the numbers I've calculated are trustworthy or not.

This is what I've been thinking:

What is the average impact of the intervention (marketing email) on
the outcome (purchase)

Raw conditional probabilities
Simple logistic regression, to understand what's the coefficient for the
treatment variable
Average Treatment Effect (ATE) using
CausalML


Who should we email, and who we shouldn't email (uplift modelling)

Uplift Modeling via Class Variable Transformation
Meta Learners (S learners/t learners etc.)
Tree based uplift modelling


How can I be confident in the above mentioned model's findings, without having to run the A/B tests?

AUUC (Area under the uplift curve)
Sensitivity Analysis



I'll really appreciate any input to improve this pipeline
","['machine-learning', 'ai-design', 'causation', 'drug-design']","
I think your assumptions are flawed for step 1. From how I understand it, you have a binary predictor (email no/yes) onto a metric outcome (propensity to buy). However, logistic regression assumes that you have a binary outcome variable, so it is not applicable to your case.
For what you want to do, you need to do a comparison of group means. Since you have one binary predictor, you could do a t-test, ANOVA or linear regression and should always obtain the same results. You could then interpret the coefficient for the treatment variable as you mentioned above, which also gives the benefit that it is directly interpretable as these methods are linear models and not generalized linear models, thus dropping the need for a link function.
"
How to setup a reinforcement learning model that changes the values of $x$ to maximize $y$ in $y = f(x)$?,"
Assuming a relation such that $y = f(x)$, where $y$ represents a scalar and $x \in 20 \times 1$ vector consisting of zeros and ones, I want to set up a reinforcement learning model that changes the values of the elements of $x$ in order to maximize the $y$.
Let's assume that $y = f(x)$ is equal to
weights = np.random.uniform(-1, 1, 20) 
y       = np.sum(weights*x)

How do I set up such model?
In my implementation, I am using keras API and I am trying to adapt the cartpole case code (https://keras.io/examples/rl/actor_critic_cartpole/). However, this code solves for a different problem since the model can perform only one action that can assume discrete values.
Secondarily, is it appropriate to set as a reward function simply $y$?. The following code reports how I would structure the architectures.
num_inputs = 20
num_actions = 2
num_hidden = 128

inputs = layers.Input(shape=(num_inputs,))
common = layers.Dense(num_hidden, activation=""relu"")(inputs)
action = layers.Dense(num_actions, activation=""softmax"")(common)
critic = layers.Dense(1)(common)

model = keras.Model(inputs=inputs, outputs=[action, critic])

However, how can I control the fact that the input parameters could assume only discrete values (0 and 1)?
","['reinforcement-learning', 'keras', 'reward-functions']","
Sure your idea makes sense to me. Yes you can give y as the reward.
No need to control for the fact that the input has only discrete values 0 and 1. Technically, most RL codes would normalize the input (using the running totals, obviously), but that's an optional step that works as heuristic, it's not technically needed.
Interested to hear the results of your experiment.
"
what is tfrs.metrics.FactorizedTopK in tensorflow recommenders,"
from the official documentation link

In our training data we have positive (user, movie) pairs. To figure out how good our model is, we need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.

how can they say the score for the positive pair should be higher than all other candidates ,
what happens if the positive pair has less ratings than the other positive pairs
can someone define the exact loss function here ?
what is compared with what 
here

This means that every movie a user watched is a positive example, and every movie they have not seen is an implicit negative example

","['neural-networks', 'tensorflow', 'recommender-system']",
How much pure math should I know for deep learning? [duplicate],"







This question already has an answer here:
                                
                            




What math should I learn before and while using and applying deep learning?

                                (1 answer)
                            

Closed 6 months ago.



Recently I've been wondering about the necessary amount of math that a deep learning scientist really needs to know. From what I could gather around the internet there are 3 big areas(calculus 1,2 and, linear algebra and statistics) that every developer should know, but are topics like Real Analysis, Topology and PDEs also useful for this field or are they of barely used?
","['deep-learning', 'math']","
Welcome to AI.
Let me answer your question with a question: what do you want to do with Deep Learning?
If you want to use it on the real world, then science is critical.  Science has two technical elements: statistics (seeing through the fog) and Differential Equations (gearbox of the universe).  Many folks in the field have neither, so having a little can go a good way.  The best way to get good at doing is to do, so practicing the problems, especially applied to something where you have skin in the game and being wrong hurts (but doesn't wreck you) and you can tell when you are wrong, is going to help with learning.  Try to ""make learning whole"" for yourself.
If you want to use it for business, then you need business-related expertise, and the most important one of those is to not deceive yourself.  As Hofstadter said ""you are the easiest person for you to deceive"" so that takes work.  The second most important is being able to relate the work to dollar bills, to ""show the money"".
If you want something capable for silicon or medicine, then you need to know MCA and get some two-handed iron-fisted grip on sample size, sources of variation, and the NIST principles driving ""process capability analysis"".
For art, or for 'the math of it', I'm probably not able to provide solid answers there.
"
"Can attention models learn statistical parameters such as median, max, mode, mean?","
We have some mixed models where after predicting the next word of a sequence, we also want to predict some weights associated to it, related to previous weights of previous words. As the prediction of next word is done with a transformer, it could be interesting if the same model could do the mathematical prediction too.
Now, I have been experimenting with attention-based models where I give as input a vector of numbers and the output is an statistical quantity, tipically the median, or the max. The model learns generic properties of the training set, that even work with a testing set generated with the same random parameters... but it fails with any other testing set, so clearly it is not learning.
Is there some example where neural networks, particularly attention based, can learn to calculate the max of a sequence? And the median?
","['transformer', 'attention', 'statistics']",
How to assess if OpenAI's ChatGPT chatbot has a human in the loop? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 7 months ago.







                        Improve this question
                    



I've asked a question and given a couple answers that propose the OpenAI ChatGPT chatbot has humans in the loop (HITL), and that explains the chatbot's extraordinary abilities.
I've been repeatedly told this is absurd.  However, I haven't been given a clear reason why this is absurd, nor how the critic knows HITL is absurd.
Here are my reasons in a nutshell.

ChatGPT's capabilities seem to violate what a neural network can do, and give explicit indication of being human driven.  See this answer for a running catalogue of examples I've published.

OpenAI's own documentation states that HITL is best practice, and should be done whenever possible, especially in high stakes domain. OpenAI has a $10B deal on the table with Microsoft, so the ChatGPT chatbot seems like a high stakes domain.



Wherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation.


The main criticism of my position is ChatGPT's response speed.  However, a fast response is doable if the AI is responding most of the time, while humans monitor, and occasionally intervening, such as with a suffix prompt to guide the AI response.  Plus, I've had a number of experiences where ChatGPT does not respond quickly, and it seems like a human is typing.

The other main criticism is that a company like OpenAI would never do something so fraudulent.  However, technically OpenAI does strongly imply in their documentation that they use HITL, and OpenAI has never said they don't use HITL.  It is only the popular media that claims ChatGPT is pure AI.  Additionally, use of HITL under the guise of AI is actually common practice in industry.  So, if ChatGPT does have HITL, this would not be fraud on OpenAI's part, and would be in line with AI industry standards.


Now, what I would like in an answer is a clear articulation of the following points:
A) How you know ChatGPT does not have HITL.  This needs to be a clear statement in the negative from an official source, or some other evidence based analysis.  So far people have only pointed to the ChatGPT main page which states you are interacting with ChatGPT, but this does not say you are not interacting with a HITL, so does not count.
B) Bonus: explain how ChatGPT can repeat long random numbers and recognize its own comments, the two examples I've documented in this answer that seem to defy what a neural network is capable of doing.  Please give a technically detailed response, ideally referencing GPT's architecture.  Handwaving about magical abilities of transformers and self attention does not count.  I need a testable break down of how such a capability would be specifically instantiated using transformers and self attention.
UPDATE: Philosophical answers are not acceptable.  Philosophical answers are things like:

the burden of proof is mine
AGI is inevitable so we should expect ChatGPT to be getting there
we can't know for sure, so let's just assume it's AI

Only technical answers are acceptable.  Such as:

OpenAI explicitly says there is no HITL involved in this linked doc
GPT's transformer architecture can repeat random numbers in this precise manner
Self recognition can be encoded using embeddings in this precise way, here is a working example

","['transformer', 'attention', 'open-ai', 'chatgpt']","
Until you can prove that OpenAI has HITL in ChatGPT, it is just an idea with no basis. It's not up to us to disprove it, it's up to you to prove it.
Let me address your points:

You seem to be basing your reasoning on an assumption of what neural networks can or can not do. What are those capabilities, and what is your source? Then we can talk about what ChatGPT seems to violate.
OpenAI states that HITL is the best practice for its end users like me and you (specifically for API users), you have provided no evidence that OpenAI themselves use a HITL in ChatGPT.
""I've had a number of experiences where ChatGPT does not respond quickly, and it seems like a human is typing"" That is because of plain old traffic. ChatGPT is a free service and is also extremely popular, making the news several times. Source.
What one company does is not a reliable source of what another company is doing. ChatGPT is top of the line at the moment in terms of AI capabilities. One of your sources is from 2016, and none of them are about OpenAI. OpenAI has whitepapers to back up the claims it makes, and there is a clear capability progression in GPT 1, 2 & 3. Nothing ChatGPT does seems impossible to people who have been keeping up with OpenAI and deeply understand how neural networks work.

For your 2 points at the end:
a) ""How you know ChatGPT does not have HITL. This needs to be a clear statement in the negative from an official source, or some other evidence based analysis. So far people have only pointed to the ChatGPT main page which states you are interacting with ChatGPT, but this does not say you are not interacting with a HITL, so does not count.""
From ChatGPT:

The statement above violates the principle of burden of proof, which
states that the person making a claim has the responsibility to
provide evidence to support it. In this case, the person is asking for
clear evidence that ChatGPT does not have a human-in-the-loop (HITL),
and is suggesting that the statement on the main page is not
sufficient evidence.

As I said before: OpenAI states that HITL is the best practice for its end users, you have provided no evidence that OpenAI themselves use a HITL in ChatGPT.
It's up to you to prove otherwise, not the general public.
Edit: This is not a helpful response to the question point b). Ignore this.
b) ""Bonus: explain how ChatGPT can repeat long random numbers and recognize its own comments, the two examples I've documented in this answer that seem to defy what a neural network is capable of doing. Please give a technically detailed response. Handwaving about magical abilities of transformers and self attention does not count. I need a testable break down of how such a capability would be specifically instantiated using transformers and self attention.""
By passing a representation of the dialog you have had along with your prompt. I don't know exactly how it is done, but it doesn't require a conspiracy. This answer does a decent job tackling the question. No magic necessary.
As a bonus:
Prompt What is the burden of proof principle in arguments?
ChatGPT The burden of proof principle in arguments refers to the idea that the person making a claim or assertion has the responsibility to provide evidence or proof to support it. This principle is important in a logical and fair argument because it ensures that both parties are held accountable for providing evidence to support their claims. Without the burden of proof principle, one party could make an unfounded claim without having to provide any evidence to support it. This principle applies to both formal and informal arguments, and is often used in legal proceedings and scientific research.
"
How to reduce variance in F1 scores of GAT across multiple runs while using PU Loss?,"
I am training GAT using a custom loss function(PU Loss) on the Cora and Citeseer dataset. My training file looks like
f1_scores = []  
N_ITER = 10
seeds = np.random.randint(1000, size=N_ITER) 

for i in range(N_ITER):
    seed_value = seeds[i]
    np.random.seed(seed_value)
    random.seed(None)
    torch.manual_seed(seed_value)
    model = GAT().to(device)
    # train it 
    # find f1 score
    f1_scores.append(f1)

print(np.mean(f1_scores))

When I run this file multiple times by doing
 for i in `seq 1 10`; do python train.py; done

I am getting high variance in the values (for e.g 0.43 and 0.76). I don't understand why this is happening even after taking the mean.

Is this the right way to take the mean of the model's F1 scores?
How to reduce this variance?

I have followed the steps mentioned here. I must use a NN. I increased the weight decay (L2) values without any success.
","['graph-neural-networks', 'variance-reduction']",
How to generate new data using VAE?,"
I have built the following function which takes as input some data and runs a VAE on them:
def VAE(data, original_dim, latent_dim, test_size, epochs):
    
    x_train, x_test = train_test_split(data, test_size=test_size, random_state=42)
    
    # Define the VAE architecture
    #Encoder
    encoder_inputs = tf.keras.Input(shape=(original_dim,))
    x = layers.Dense(64, activation='relu')(encoder_inputs)
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dense(8, activation='relu')(x)

    #--- Custom Latent Space Layer
    z_mean = layers.Dense(units=latent_dim, name='Z-Mean', activation='linear')(x)
    z_log_sigma = layers.Dense(units=latent_dim, name='Z-Log-Sigma', activation='linear')(x)
    z = layers.Lambda(sampling, name='Z-Sampling-Layer')([z_mean, z_log_sigma, latent_dim]) # Z sampling layer

    # Instantiate the encoder
    encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_sigma, z], name='encoder')

    #Decoder
    latent_inputs = tf.keras.Input(shape=(latent_dim,))
    x = layers.Dense(8, activation='relu')(latent_inputs)
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dense(64, activation='relu')(x)
    decoder_outputs = layers.Dense(1, activation='relu')(x)

    # Instantiate the decoder
    decoder = tf.keras.Model(latent_inputs, decoder_outputs, name='decoder')

    # Define outputs from a VAE model by specifying how the encoder-decoder models are linked
    # Instantiate a VAE model
    vae = tf.keras.Model(inputs=encoder_inputs, outputs=decoder(encoder(encoder_inputs)[2]), name='vae')
    
    # Reconstruction loss compares inputs and outputs and tries to minimise the difference
    r_loss = original_dim * tf.keras.losses.mse(encoder_inputs, decoder(encoder(encoder_inputs)[2]))  # use MSE

    # KL divergence loss compares the encoded latent distribution Z with standard Normal distribution and penalizes if it's too different
    kl_loss = -0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)

    #VAE total loss
    vae_loss = K.mean(r_loss + kl_loss)

    # Add loss to the model and compile it
    vae.add_loss(vae_loss)
    vae.compile(optimizer='adam')
    
    # train the model
    vae.fit(x_train, x_train, epochs=epochs, validation_data=(x_test, x_test))

where
def sampling(args):
    
    z_mean, z_log_sigma, latent_dim = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1., seed=42)
    return z_mean + K.exp(z_log_sigma) * epsilon

My question is, if I want to generate new data, by using the above VAE how can I achieve that ?
If I want to sample 100 new data, should I use this
   latent_mean = tf.math.reduce_mean(encoder(x_train)[2], axis=0) 
   latent_std = tf.math.reduce_std(encoder(x_train)[2], axis=0)
   latent_sample = tf.random.normal(shape=(100, latent_dim), mean=latent_mean, 
                                    stddev=latent_std)
   generated_data = decoder(latent_sample)

or
   latent_mean = tf.math.reduce_mean(encoder(x_train)[0], axis=0) 
   latent_std = tf.math.reduce_mean(tf.math.exp(encoder(x_train))[1], axis=0)
   latent_sample = tf.random.normal(shape=(100, latent_dim), mean=latent_mean, 
                                    stddev=latent_std)
   generated_data = decoder(latent_sample)

?
Basically should I infer z_mean and z_log_sigma from the z or should I use z_mean and z_log_sigma directly ? What is the difference ?
Moreover, I have seen that everytime tf.random.normal is used to generate new data from the latent space. Why not use lognormal for instance ? Is it because of the KL divergence ?
The end-goal is the distribution of the generated_data to be as close as possible to the distribution of the original data.
","['deep-learning', 'python', 'autoencoders', 'generative-model', 'variational-autoencoder']","
Normally in synthetic data generation using VAEs, you do not use the encoder anymore upon inference. Instead, generating samples after training is usually done by simply sampling random noise from 'a distribution' (i.e. a normal distribution) in the dimensions of the latent space and decoding it using the decoder. Hence, generating samples would be usually done as follows:
noise = tf.random.normal(shape=(100, latent_dim))
generated_samples = decoder(noise)

if you use training samples in generating your new data, it is not representative of how good your generative model is as it then bases the output on the training data.
Answer However, if you do want to sample using the training data to get the means and standard deviations, then your second method is a more appropriate method than your first method. You need the standard deviation, and you only get the log(std) from the model. Taking the exponent gives you the standard deviation.
"
Document Processing using AI,"
Deal all
I want to Build a Document Processing AI Model
1-that Identify Document Elements
Like table, Text , List , Heading ,  etc.
2-Sort this element in correct way if I wan to dump this data as XML
so I attached two images the first one the original image
the second one show me the expectation output recognize Document Elements and the sort Index for each item
you can see the Sort not follow a regular sort
I already build a model using some Tensor flow and train my Model with a lot of documents Classes elements and model recognize me the Boundary box + Class Name as Shown in the third image ,
so my missing part how to predict the correctly order for this elements
I've a good dataset that I can extract the sort data to use while training the model
but I don't know which thing will match my needs
any help



","['image-processing', 'image-segmentation']",
How are these two equations for the optimal state-value function equivalent?,"
By substituting the optimal policy $\pi_{\star}$ into the Bellman equation, we get the Bellman equation for $v_{\pi_{\star}}(s)=v_{\star}(s)$:
$$ v_{\star}(s) = \sum\limits_a \pi_{\star}(a|s) \sum\limits_{s'} \sum_r p(s', r | s, a)[r + \gamma v_{\star}(s')]$$
From the above equation, how can we obtain the this one?
$$ v_{\star}(s) = \max\limits_a \sum\limits_{s'} \sum\limits_r p(s', r|s,a) [r + \gamma v_{\star}(s')]$$
","['reinforcement-learning', 'comparison', 'value-functions', 'bellman-equations', 'optimality']",
Would it be possible to involve a proof assistant in the process of training a LLM?,"
LLMs like GPT-3 have been shown capable of outputting highly complex code. Sadly, actually using them to replace a programmer's job has two major caveats:

LLMs are notoriously bad at producing complex algorithms

It isn't possible to check whether the outputted code is correct


As such, while something like ChatGPT will easily translate between different syntaxes, and even produce useful code, that is limited, to an extent, to code it has already seen. Fortunately, the (2) problem already has a solution: a proof checker. Certain languages have these built in, including Lean, Coq, Agda, Idris and Kind. As an example, suppose you wrote the following prompt, asking for an algorithm in Agda:
Complete the following Agda function, which negates a boolean:

not : Bool -> Bool

A LLM could fill it incorrectly, as follows:
not : Bool -> Bool
not true  = true
not false = false

But the cool thing is, in a proof assistant, you're able to enrich the type arbitrarily, and turn it into a complete specification of the program you desire. As such, you could prevent the issue above as follows:
Complete the following Agda function:

not : (in : Bool) -> Sigma (out : Bool) (in != out)

This type demands that the implementation of not comes accompanied by a proof that the output is, indeed, different from the input. Suppose that we did that, and the Ai still produced an incorrect algorithm:
not : (in : Bool) -> Sigma Bool (λout -> in != out)
not true  = MkSigma true  <proof_here>
not false = MkSigma false <proof_here>

In that case, it wouldn't be able to fill the <proof_here> with a valid proof that the algorithm is correct, because it isn't. As such, we would be able to easily discard that answer, feed the AI an error message, and ask it to try again. We can repeat this process until it gets it right. Of course, this is a simple example, but you could make arbitrarily complex algorithm demands using the same technique. For example, the prompt below would demand a correct sorting algorithm:
Complete the following Agda function:

sort : (in : List Nat) -> Sigma (List Nat) (λ out -> IsSorted out & IsSame in out)

Once again, chances are it would produce the wrong solution, but we'd be able to automatically discard it and try again, until it gets it right. This leaves us with only the problem (1): LLMs are notoriously bad at coding. That is, even if we're able to discard the wrong output, something like current-version ChatGPT will probably never get anything sufficiently complex correct.
Now, that's not unsurprising: GPT wasn't trained to write proofs. As such, a natural solution would be to do so. But the amount of proofs written is very limited, compared to, say, the size of Wikipedia. We don't have enough data. But the cool thing is: we can actually make the AI itself generate the proofs it is trained on. It would work as follows:

Train the AI on publicly available proofs (Agda/Coq std-libs, for example)

Generate a random theorem and ask it to prove

If it gets the proof wrong, go back to 2

If it gets the proof right, train it on that proof, and go back to 1


By repeating this loop ad infinitum, we'd have both a growing body of proofs, and an AI that is increasingly more capable of proving complex theorems.
My questions are: is there substance to this approach? If so, has anyone tried it? Citations would be appreciated. If not, is there any clear reason I'm not aware of?
","['natural-language-processing', 'recurrent-neural-networks', 'proofs', 'gpt', 'automated-theorem-proving']",
Is there some neural network that implements Least Squares?,"
I would like to build supervised NN that gets a matrix $A$ and vector $b$ as inputs and returns $x$ as a close result of the Least Squares algorithm for $Ax=b$.
I looked for so works in the field and the only one that I found is this one. The paper is from 1994 and it's about analog NN. Maybe you have seen a work on the subject?
","['neural-networks', 'reference-request', 'supervised-learning', 'model-request']",
Is there a way an AI could observe patterns in another AI?,"
I am really interested in “explainable AI” in which you can try to see the factors influencing a model’s decisions and ideally reduce it to a human-understandable explanation of why it does what, when.
I do not know much but I was vaguely considering if you could check mathematically, as inputs go into a trained model, which nodes in each successive layer received a high or a low normalized value as its input. I do not know if it is true, but I have always been interested in the idea that a neural network builds a conceptual system, and if some node seems to have a decisive role, perhaps a high scalar weight or something, that we could reconstruct a representation of the actual input thing - if an image was encoded into binary, maybe there could be a way to “look” at how that input data has been changed, as an image, after a few layers. It would be interesting if the input layer could be split into parts and it became evident that the image had been decomposed into higher-level elements, like, a black background, a vaguely humanoid silhouette, etc.
This idea interests me, but I was now wondering if it would be way easier to have a second AI do this. Talking to ChatGPT, I have often felt that - actually, somewhat similar to the human mind - it seems really smart when you interact with it within the world it was trained in; yet it apparently knows nothing, has no ability to perceive, itself as an algorithm, to somehow introspect and explain its own thinking. At least for now, it seems like understanding the AI requires looking from a starkly orthogonal perspective to the one of engaging with it.
But then I just thought, wouldn’t the easiest way be to just get a second AI to study the mathematics of the first, and give us the data, the answers, we want? Maybe it could at minimum return information about which nodes in the other network it considers most “pivotal” or “decisive”. Maybe we could explore reducing the first network over and over until we have a very simple “decision tree” which may not be anywhere near as accurate, but at least we would get a simplified model of a phenomenon.
Has this been done in any research article? I’m assuming this is well-established.
Thanks.
",['explainable-ai'],
Confusion about N-step buffer in DQN,"
So I've been trying to implement the n-step buffer in DQN algorithm.
Temporal difference for which we score values $(s_i, a_i, s_{i + n}, r, d)$ is defined in this way
$$
\mathrm{TD}_{n}(s_i, a_i, s^{'}_{i+n}) = Q_{\theta}~(s_i,a_i) - \biggl(\sum_{k=0}^{n-1} \gamma^{k} ~ r_{(s_{i+k},a_{i+k},s_{i+k}^{'})} + \gamma^{n} \underset{a^{'}_{i+n} \sim \bar{Q}_{\theta}}{\mathrm{max}} ~ \bar{Q}_{\theta}~(s_{i+n}^{'},a_{i+n}^{'}) \biggr)
$$
So the reward we accumulate is equal to:
$$
\sum_{k=0}^{n-1} \gamma^{k} ~ r_{(s_{i+k},a_{i+k},s_{i+k}^{'})}
$$
So I've implemented getting an n'th step from accumulated queue of transitions as:
def get_nstep(self):
        (state, action, _, _, _) = self.memories[0]
        reward = 0
        for i in range(self.nstep):
            
            reward += self.memories[i][2] * self.args.discount**i
            next_state = self.memories[i][3]
            terminal = self.memories[i][4]

            if self.memories[i][4] == 1:
                break

        self.memories.clear()
        return state, action, reward, next_state, terminal

But I've looked up some of the implementations online and they seem to reverse the queue and then iterate. Why is that?
eg. https://github.com/Curt-Park/rainbow-is-all-you-need/blob/master/07.n_step_learning.ipynb
with getting n-step value as:
def _get_n_step_info(self):
        reward, next_observation, done = self.n_step_buffer[-1][-3:]
        for _, _, rew, next_obs, do in reversed(list(self.n_step_buffer)[: -1]):
            reward = self.gamma * reward * (1 - do) + rew
            next_observation, done = (next_obs, do) if do else (next_observation, done)
        return reward, next_observation, done

Mine seem to work on it's own, but rainbow implementation is in the bigger picture (which I assume can be because of this).
","['reinforcement-learning', 'deep-rl', 'dqn', 'experience-replay']",
Is there a most general-purpose unsupervised learning algorithm?,"
I was thinking about training a model on non-linguistic material like video, and I was wondering if it could form concepts about the world, and also somehow form composite concepts or conceptual relationships, understanding that a “cat” is something that can “jump”, but non-verbally.
I was wondering if there is any unsupervised learning algorithm which is not designed in advance to look for any one kind of relationship over any other but could somehow be open to any and all sort of “patterns” in some data.
Like, I think a lot of unsupervised learning in linguistics is based on embeddings, but the choice of embedding can affect what the model ends up “identifying” - different embeddings can capture different aspects, different types of information.
Is there any algorithm which does not limit itself to look for one thing over another but somehow, mathematically, can sort of find correlations from a small scale up to higher and higher levels?
I was thinking that those “higher levels” would build on any concepts it had learned in previous stages. If it has gotten to the point where it resolves a cat’s face and body into eyes, red, arms, ears, it notices correlations between those and slowly builds a concept “cat”. Later, it may find correlations between “cat” and other things, like, “jump”.
Is this resemblant to a particular approach?
","['unsupervised-learning', 'algorithm-request', 'symbolic-ai', 'information-theory', 'symbolic-computing']",
"Since ReLU activations also result in a sparse network, does it have the same ""feature selection"" property as L1 regularization?","
From Deep Learning (Courville, Goodfellow, Bengio), a ReLU activation often ""dies"" because

One drawback to rectified linear units is that they cannot learn via gradient based methods on examples for which their activation is zero.

Similarly, L1 regularization (as opposed to L2) results in a sparse network

This demonstrates that L2 regularization does not cause the parameters
to become sparse, while L1 regularization may do so for large enough α. The sparsity property induced by L1 regularization has been used extensively
as a feature selection mechanism.

A couple questions about these topics:

In practice, is there any way/use to prune these ""dead"" ReLU-activated neurons? And if our trained network performs well with lots of dead neurons, would that imply that a shallower network is a sufficient representation?
Since ReLU activations also result in a sparse network, does it have the same ""feature selection"" property as L1 regularization? If it does, does this then imply that sigmoid/tanh activations don't have this property?

","['machine-learning', 'relu', 'l1-regularization']",
Under which conditions does value iteration and policy iteration will give us the optimal solution if gamma equals 1,"
I’m learning about policy iteration and value iteration, and I’m wondering under which conditions does both algorithms will give us the optimal solution, if our discount factor (gamma) equals 1.
Note: I am talking about rewards which depends on the current state, the next state, and the action.
Thanks alot!
","['reinforcement-learning', 'markov-decision-process', 'value-iteration', 'pomdp', 'policy-iteration']",
"When training a DNN on infinite samples, do ADAM or other popular optimization algorithms still work as intended?","
When training a DNN on infinite samples, do ADAM or other popular optimization algorithms still work as intended?
I have an DNN training from an infinite stream of samples, that most likely won't repeat. So there is no real notion of ""epoch"".
Now I wonder if the math behind ADAM or other popular optimizers expect the repetition of data over the epochs?
If so, should I collect a limited amount of those samples and use them for training data and validation data, or would it be better to use all data available (even if the training data never repeats then)?
","['machine-learning', 'training', 'adam', 'optimizers']","
In general, the methods still work even with an infinite amount of data, as long as there are common/reoccuring patterns that a neural network can learn to identify. For example: If you would have infinitely many images of dogs and cats, there are features that discriminate the two animals that are mostly consistent like the shape of the nose. Having infinitely many samples in these cases is generally desirable, because it can benefit the ability of the model to generalize.
In contrast, there are cases where this is not true that depend on the data: If your data contains (concept-) drift, meaning that your data distribution changes over time, the model you train might not be able to learn a consistently performing function and therefore chases a moving objective. For images, this can be the case if the labels would depend on lighting conditions that constantly change (concept drift), or if the objects in the images that you want to classify continuously change shape (drift).
"
"How to prove that, for any graph $G$, the tree width of $G$ is $1$, if and only if $G$ is acyclic?","
I am studying about CSPs, tree decomposition of a graph and tree width ($\text{TW}(G)$) of a graph - the smallest tree width of a tree decomposition.
I encountered the following problem:
Prove that for any graph $G$, $\text{TW}(G)=1$ if and only if $G$ is acyclic.
I don't know how to prove it, and I can't figure out the proof or even the intuition for why this is true. How can this statement be proven?
","['proofs', 'graphs', 'constraint-satisfaction-problems']",
Single Layer Perceptron Backpropagation: How to compute affect of the net value on the output?,"
Assuming a single perceptron (see figure), I have found two versions of how to use backpropagation to update the weights. The perceptron is split in two, so we see the weighted sum on the left (the output of this is net) and then the sigmoid function phi on the right (the output of this is out).

So for the backpropagation portion, we compute $\frac{\delta cost}{\delta w}=\frac{\delta cost}{\delta out}\times \frac{\delta out}{\delta net} \times  \frac{\delta net}{\delta w}​$, to find how the weight affects the error.
For the portion $\frac{\delta out}{\delta net}$ I have seen three different versions for how to compute the value:

$\frac{\delta out}{\delta net}$ = $\phi'(out) = \phi(out) \times (1 - \phi(out))$
$\frac{\delta out}{\delta net}$ = $\phi'(net)= \phi(net) \times (1 - \phi(net))$
$\frac{\delta out}{\delta net}$ = $out \times (1 - out)$

Can somebody explain to me which one is correct and why? Or is there one which should be preferred?
","['neural-networks', 'backpropagation', 'gradient-descent']","

is always wrong. Most implementations then ""silently"" correct this error, even if the docstrings have this wrong variant, in the actual use it is done correctly.

and 3. are the same, considering that $out=\phi(net)$. 3. is more economic if the activation is a solution of an autonomous ODE like with the logistic function. 2. is more generally true, especially if the activation function gets modified/perturbed by some linear function to avoid random behavior in the flat parts of the sigmoid at large values.


"
Why doesn't Q-learning converge when using function approximation?,"
The tabular Q-learning algorithm is guaranteed to find the optimal $Q$ function, $Q^*$, provided the following conditions (the Robbins-Monro conditions) regarding the learning rate are satisfied

$\sum_{t} \alpha_t(s, a) = \infty$
$\sum_{t} \alpha_t^2(s, a) < \infty$

where $\alpha_t(s, a)$ means the learning rate used when updating the $Q$ value associated with state $s$ and action $a$ at time time step $t$, where $0 \leq  \alpha_t(s, a) < 1$ is assumed to be true, for all states $s$ and actions $a$.
Apparently, given that $0 \leq  \alpha_t(s, a) < 1$, in order for the two conditions to be true, all state-action pairs must be visited infinitely often: this is also stated in the book Reinforcement Learning: An Introduction, apart from the fact that this should be widely known and it is the rationale behind the usage of the $\epsilon$-greedy policy (or similar policies) during training.
A complete proof that shows that $Q$-learning finds the optimal $Q$ function can be found in the paper Convergence of Q-learning: A Simple Proof (by Francisco S. Melo). He uses concepts like contraction mapping in order to define the optimal $Q$ function (see also What is the Bellman operator in reinforcement learning?), which is a fixed point of this contraction operator. He also uses a theorem (n. 2) regarding the random process that converges to $0$, given a few assumptions. (The proof might not be easy to follow if you are not a math guy.)
If a neural network is used to represent the $Q$ function, do the convergence guarantees of $Q$-learning still hold? Why does (or not) Q-learning converge when using function approximation? Is there a formal proof of such non-convergence of $Q$-learning using function approximation? 
I am looking for different types of answers, from those that give just the intuition behind the non-convergence of $Q$-learning when using function approximation to those that provide a formal proof (or a link to a paper with a formal proof).
","['reinforcement-learning', 'q-learning', 'deep-rl', 'proofs', 'function-approximation']","
Here's an intuitive description answer:
Function approximation can be done with any parameterizable function. Consider the problem of a $Q(s,a)$ space where $s$ is the positive reals, $a$ is $0$ or $1$, and the true Q-function is $Q(s, 0) = s^2$, and $Q(s, 1)= 2s^2$, for all states. If your function approximator is $Q(s, a) = m*s + n*a + b$, there exists no parameters which can accurately represent the true $Q$ function (we're trying to fit a line to a quadratic function). Consequently, even if you chose a good learning rate, and visit all states infinitely often, your approximation function will never converge to the true $Q$ function. 
And here's a bit more detail:

Neural networks approximate functions. A function can be approximated to greater or lesser degrees by using more or less complex polynomials to approximate it. If you're familiar with Taylor Series approximation, this idea should seem pretty natural. If not, think about a function like a sine-wave over the interval [0-$\pi/2$). You can approximate it (badly) with a straight line. You can approximate it better with a quadratic curve. By increasing the degree of the polynomial we use to approximate the curve, we can get something that fits the curve more and more closely.
Neural networks are universal function approximators. This means that, if you have a function, you can also make a neural network that is deep or wide enough that it can approximate the function you have created to an arbitrarily precise degree. However, any specific network topology you pick will be unable to learn all functions, unless it is infinitely wide or infinitely deep. This is analogous to how, if you pick the right parameters, a line can fit any two points, but not any 3 points. If you pick a network that is of a certain finite width or depth, I can always construct a function that needs a few more neurons to fit properly.
Q-learning's bounds hold only when the representation of the Q-function is exact. To see why, suppose that you chose to approximate your Q-function with a linear interpolation. If the true function can take any shape at all, then clearly the error in our interpolation can be made unboundedly large simply by constructing a XOR-like Q-function function, and no amount of extra time or data will allow us to reduce this error. If you use a function approximator, and the true function you try to fit is not something that the function can approximate arbitrarily well, then your model will not converge properly, even with a well-chosen learning rate and exploration rate. Using the terminology of computational learning theory, we might say that the convergence proofs for Q-learning have implicitly assumed that the true Q-function is a member of the hypothesis space from which you will select your model. 

"
Can an activation function with large derivative cause exploding gradient?,"
The maximum derivative of most of the currently existing activation functions is around 1. Can an activation function with derivatives higher than 1, say 1000 (a), cause exploding gradient problem? How about that with maximum derivative of 5 (b)? If (a) and (b) are compared, can one say having (b) is better than (a)?
","['neural-networks', 'activation-functions', 'exploding-gradient-problem']",
Is it a problem if the target network and main network are the same?,"
I am currently working with this code that I found on Github: this is a DQN with Graph Neural Network (GNN) on TSP problem.
So, in this architecture, the target network and the main network are the same. The target network is not separate.
Would this cause problems? Especially if I eliminate the condition of ""not repeating the same number"" and I make it for a larger number of nodes?
Another thing, in this model the EMBEDDING_DIMENSIONS is set to 5. So I was wondering why 5? When I read about embedding dimensions, it's usually a value of 128 or 126.
I am still a beginner in the DQN, so I don't know how to set the EMBEDDING_DIMENSIONS and on which basis? And is it normal to have 5 as a value of embedding dimension?
","['reinforcement-learning', 'dqn', 'hyper-parameters', 'embeddings', 'target-network']",
"Problem on evaluating DQN, on a Vehicle Routing Problem (VRP)","

I am running this DQN algorithm that is trying to minimize the total distance traveled by a vehicle (VRP). In the training, as you can see in the images, everything works fine: the loss is decreasing, the average length in decreasing, and the reward is increasing. However, in the evaluation phase the model behaves in a weird way. I am running 100 evaluation, so the first run of 100 the results are good; but the next runs of evaluation give me sometimes good results and sometimes very bad results. In the good results I get min total distance (min length) value of 4, but sometimes the evaluation return a min value of 13. Eventhough the evaluation is done on the same trained model.
So my question is this behavior normale? and is there a way to improve these evaluation results? or Is it probably a stability issue? If someone knows what's happening please let me know. I am so confused right now

the number of episodes in training is 4000 ( i tried on 10000 also and it's the same thing)
the data is random array of coords and an adjacency matrix of euclidean distance between the coords. For every new episode there's a new random coords and distance arrays.
the same thing for evaluation. I do 100 iterations of evaluation and for each iteration new random data
In the evaluation I don't use any penalties or rewards. I only use them in the training. I am using pytorch in this project
My reward function is based on the totale distance

Here's an example of the evaluation output: shortest avg length found: 5.406301895156503 (this is the value from the training) Now here are an example of 2 solutions from evaluation
solution 1 [0, 1, 9, 4, 2, 3, 5, 0, 6, 7, 8, 10]; length 4.955087028443813 -> very good solution (11,12,13 are places he should not visit)
solution 2 [0, 4, 9, 3, 13, 0, 7, 13, 0, 10, 0, 6, 11, 5, 12, 1, 12, 0, 2, 12, 0, 8, 0]; length 10.15813521668315
The first 100 evaluations are similar to solution 1, and i rerun evaluation for another 100 i get results similar to solution 2.
These are my hyper parameters:

NR_EPISODES = 4001;
EMBEDDING_DIMENSIONS = 5
NR_NODES = 14
MEMORY_CAPACITY = 10000
N_STEP_QL = 2  # Number of steps (n) in n-step Q-learning to wait before computing target reward estimate
BATCH_SIZE = 16
GAMMA = 0.9
INIT_LR = 5e-3
LR_DECAY_RATE = 1. - 2e-5  # learning rate decay
MIN_EPSILON = 0.1
EPSILON_DECAY_RATE = 6e-4  # epsilon decay

","['deep-learning', 'deep-rl', 'q-learning', 'dqn', 'pytorch']",
Is there a way to see the feature importance in deep learning (neural networks)? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 7 months ago.







                        Improve this question
                    



For tree methods, I can plot the feature importance plot from tree.feature_importances_ in sklearn, is this achievable in deep learning (neural networks)?
Is there a way to see the feature importance in deep learning (neural networks)?
","['neural-networks', 'deep-learning', 'decision-trees', 'feature-selection']","
In the current state, Deep learning for Tabular is not very popular, so it is very hard to find libraries that support feature importance. However, TabNet also provides the model.features_importance_ metrics to be used as usual. This Kaggle notebook provides an example on how to use just that.
"
What is wrong with my PyTorch model training on CIFAR10?,"
I am training a ResNet model on CIFAR10 dataset. For the training subset, I selected a random 1% of the train data from the default train/test split. For the test subset I used the whole default test split (by default I mean the ones selected by the data providers). I am using cross-entropy loss, one cycle learning rate scheduler and adam optimizer. Each epoch, I am plotting accuracy of the model on both train and test subsets.
Looking at that graph, some weird things are happening. The train accuracy achieves 100% accuracy in under 10 epochs, while the test accuracy persists to increase till the stopping epoch 60. How is the model not overfitting? One would expect that after some point during training, the model would start to overfit on train data which would result in decrease in test accuracy. In my case, it seems that the model generalizes much better if train well beyond the point where train accuracy reaches maximum.
The images of train/test accuracies and losses are below:


I realize that the model can still change during the training even after the accuracy on the train data reaches 100% as I am using the cross-entropy loss and with each additional epoch the predictions get more and more confident (even when all instances are already correctly classified). However, looking at the loss of the train data, it is already very close to 0 on the 20th epoch, yet still the test accuracy practically doubles after that point.
What's going on, how is that possible?
EDIT: Adding the results when using 100% of the train data:


EDIT 2: Adding results after inducing 60% symmetric label noise (100% of train data used for training).


","['machine-learning', 'pytorch', 'overfitting', 'cifar-10']",
Why is ChatGPT not aware of panpsychism?,"
I asked her what panpsychism was and it couldn't answer. Why doesn't it have that kind of knowledge? Is it for some ethical reason?
Its answer:
what is panpsyquism?
Panpsyquism is not a recognized term. It is not clear what it would refer to.

","['chat-bots', 'chatgpt']","
I tried this. The whole chat log is below:
My input:

Someone mentioned panpsychism to me, I don't know what it is. How do I find out?

ChatGPT:

Panpsychism is a philosophical theory that holds that consciousness, or some form of mentality, is a fundamental feature of the universe and that all physical matter possesses it to some degree. It is a controversial theory and is debated in the fields of philosophy, psychology, and neuroscience. To find out more about panpsychism, you can start by reading some introductory texts on the topic such as David Chalmers' ""The Conscious Mind"" or ""Panpsychism in the West"" by David Skrbina. Additionally, you can search for academic articles and papers on the topic to gain a deeper understanding of the various arguments and criticisms of the theory.

It's fairly clear that ChatGPT has been trained on content that describes panpsychism.
Why did you get an ""I don't know"" in your attempt? It is probably because you mispelled the term as ""panpsyquism"" and ChatGPT did not make the link. Sometimes it still might, depending on the misspelling, because ChatGPT doesn't just break down text into words,  but also will use individual characters for uncommon words. So it can spell and deal with some misspellings.
However it is worth bearing in mind that ChatGPT generates output semi-randomly, and it may from time to time output factually incorrect sentences or try to reframe your question or other things that humans might do in a chat scenario, including ""I don't know"" or ""that question does not have an answer"" because it has been trained on a lot of human-written content where these things happen. If you are exploring the limits of ChatGPT's abilities, it is always worth repeating an experiment a few times.
"
Do tensor cores provide advantages for running Stable Diffusion or only for training?,"
If I am only interested in running Stable Diffusion, using pre-trained weights, to generate images, are there any advantages to using a GPU with more Tensor cores?  Or will any CUDA-compatible GPU perform equivalently on this task?
","['gpu', 'hardware', 'diffusion-models']",
How to output a function given a time series data as an input using supervised learning?,"
I have a spreadsheet with time series data collected from two sensors, one measuring temperature and the other measuring humidity. And I also collected data from an experiment that I conducted, the data collected during the experiment can be written as a function. I noticed that temperature and humidity affect the coefficient of the function that I get on the output, so I wanted to develop a supervised learning algorithm (regression) that accepts the time series data (humidity, temperature) as input and predict the coefficients of the function that I would get during the experiment as output.

Input: time series data
Output: coefficients $a$ and $b$ of a logarithmic function (a log b)

I have enough data to train the model but I am not sure what type of model to use or how to start solving this problem. So far I can understand that it is a regression problem.
","['machine-learning', 'time-series', 'regression', 'supervised-learning', 'model-request']",
How to speed up my neural network?,"
I would like to train an LSTM-based variational autoencoder on a large dataset (37 million sentences). However, I have calculated that my training speed as of now is too slow (on Google Colab). I am using a GPU provided by Google called A100-SXM4-40GB, and my framework of choice is Pytorch. With my current training speed, I get through about 650,000 training examples/sentences in 12 hours. My vocabulary size is about 85,000, the number of parameters in the VAE is 17.1 million (mostly high because of the embedding layer). My batch size is 64, and I am using the Adam optimizer.
What advice can you give me to speed up my model? For instance, I also have access to a TPU, but I have never seen a clear breakdown on GPU vs TPU performance (and what role batch size plays). Can I use parallel computing, and is this possible on Colab? I know I might have to make some concessions with the training set size, but I would like to get through as much data as possible in 48 hours of training.
","['optimization', 'pytorch', 'gpu']",
Seq2seq with attention model only predicts index 0 in a text generation task,"
I am trying to build a model which summarizes some brief input text (X) and generates a headline (Y).
The problem is that the model always outputs 0 as the vocabulary index, which I think that might be due to sentence padding, I have tried to use mask_zero parameter in the embedding layer but results are the same.
I also have considered that the problem might be that I have very little data to train the network but I supposed it would fail in some manner but outputing random words, not always 0.
The network architecture I am trying to use is a seq2seq Bi-LSTM with attention mechanism and I am using Glove 50D embeddings as input layer.
Basically the model is based in three code functions that comprehend the previously explained model.
A good visual representation of the model is the following from Andrew Ng just substituting one hot vector inputs with embeddings in the input layer:


Model layers definition at global scope to share weights

# Attention layers globally defined
repeator = tf.keras.layers.RepeatVector(Tx)
concatenator = tf.keras.layers.Concatenate(axis=-1)
dense_1 = tf.keras.layers.Dense(10, activation='tanh')
dense_2 = tf.keras.layers.Dense(1, activation='relu')
activator = tf.keras.layers.Activation('softmax', name=""attention_weights"")
dotor = tf.keras.layers.Dot(axes=1)

# Post-attention LSTM globally defined to share weights
post_attention_LSTM = tf.keras.layers.LSTM(units=n_s, return_state=True)
output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')


Full model structure:

def seq2seq_clickbait(Tx, Ty, n_a, n_s, vocab_size, emb_size):

    # Define Model Inputs
    sentence_indices = tf.keras.layers.Input(shape=(Tx,), dtype='int32')

    emb_matrix = build_embeddings_matrix(word_to_vec, vocab_size, word_to_idx, emb_size)

    pretrained_embedding = pretrained_embedding_layer(emb_matrix, vocab_size, emb_size)

    embeddings = pretrained_embedding(sentence_indices)

    s0 = tf.keras.layers.Input(shape=(n_s,), name='s0')
    c0 = tf.keras.layers.Input(shape=(n_s,), name='c0')

    s = s0
    c = c0

    outputs = []

    a = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=n_a, return_sequences=True))(embeddings)

    for t in range(Ty):

        # Once we have a, we can get the context to pass it to the post attention lstm time step
        context = one_step_attention(a, s)

        # Get current states from post-attention lstm
        s, _, c = post_attention_LSTM(inputs=context, initial_state=[s, c])

        # Apply dense (softmax vocab_size)
        out = output_layer(inputs=s)
        outputs.append(out)

    model = tf.keras.models.Model(inputs=[sentence_indices, s0, c0], outputs=outputs)

    return model


A good visual representation of what does one_step_attention(a, s)


context

def one_step_attention(a, s_prev):

    # Repeat the s_prev vector Tx times to expand the vector
    s_prev = repeator(s_prev)

    # Concatenate a and s_prev
    concat = concatenator([a, s_prev])

    # Forward prop to a dense layer
    e = dense_1(concat)

    energies = dense_2(e)

    # Apply softmax to energies to get the alphas
    alphas = activator(energies)

    # Get context from the alphas
    context = dotor([alphas, a])

    return context

I present the outputs to the model as a one hot vector so if the ouput sentence is length (Ty) 20 the output tensor is (20, m, vocab_size). My guess is that many of the labeled sentences are not length 20 and therefore they are padded, so the network is learning to output 0's since it is a pretty good way of improving accuracy / decrease loss in short senteces widely padded.
If half of the output steps of the network are filled with a [1, 0, 0,..., 0] one hot vector which represents the 0 index (pad) it dense layer will learn to output probabilities distributions like [0.8, 0.002, 0.00003, ....] selecting the 0 index.
Any ideas on how to solve this? Thank you in advance
EDIT
Some additional info about the model.
loss function is categorical_crossentropy, I have not added extra tokens to the vocabulary given by GloVe embeddings, I removed 'PAD' as a vocabulary token since TF embeddings docs says so when mask_zero=True. m is the number of examples, Tx and Ty is the sentence max length of X and Y, n_s is the number of post-attention LSTM units and n_a is Bi-LSTM units. I think indeed is a masking problem but mask_zero=True is not solving it. It only works with return_sequences=True and post att LSTM has return_squences=False because it loops through steps.
","['natural-language-processing', 'tensorflow', 'seq2seq']",
What is a good strategy for breaking up content into prompts and completions for OpenAI fine tuning?,"
I want to train a fine-tuned openai model to know more about specific Judo throws and training methodologies. I have a bunch of documents I have written on Judo throws that I would like to use for training data.
Given the token limits, where I cannot send large chunks of content, what would be a good strategy for breaking up this content into prompts and completions?
I have considered the following:

Segment content where titles are the prompts and the first few paragraphs are the completions
Segment by blocks of content and for each block send open ai the block as the prompt and the completion as the next block of content (ie two paragraphs etc)
Manually curate the content into question-answer format and load that in

Ideally, I want this to be as automated as possible as I have a lot of content
","['machine-learning', 'data-preprocessing', 'open-ai', 'data-science', 'fine-tuning']",
InstructGPT: What is the sigma in the loss function and why $\log(\cdot)$ is being used?,"
InstructGPT: What is the sigma in the loss function and why $\log(\cdot)$ is being used?
$$ \operatorname{loss}(\theta) = -\frac{1}{\binom{K}{2}}E_{(x,y_w,y_l)\sim D}[\log(\sigma(r_{\theta}(x, y_w) - r_{\theta}(x, y_l)))] $$
The equation was taken from the InstructGPT paper.
","['machine-learning', 'papers', 'open-ai', 'chat-bots', 'instruct-gpt']","
According to this guide, the sigma in this formula refers to the sigmoid activation function. The guide does not tell exactly why the sigmoid function is used here, so I will try to give a full explanation of how this loss formulation works (page 8, formula 1 in the InstructGPT paper):
$\text{loss}(\theta)=-\frac{1}{\binom{K}{2}}E_{(x,y_w,y_l) \sim D} [log(\sigma(r_\theta(x,y_w)-r_\theta(x,y_l)))]$
In the following I will use the notation from the paper:
$x$ refers to the given instruction
($y_w$, $y_l$) refers to a pair of responses out of the list of responses which a human ranked based on their preference
$y_w$ refers to the response that is preferred over the other, lesser preferred response $y_l$
$r$ refers to the reward model
$\theta$ refers to the trainable parameters of that reward model
$\sigma$ refers to the sigmoid activation function.
If you interpret $\sigma(r_\theta(x, y_w) - r_\theta(x, y_l))$ as the probability that the reward model assigns a higher reward to the preferred response $y_w$ than to the lesser preferred response $y_l$, the formula makes total sense. How do you maximize a probability? Exactly, by minimizing the negative log of that probability (this is called negative log likelihood). Hence the  minus sign in front of the formula and log function around the sigmoid.
In order to elaborate a bit more on that: If the reward model is working fine, then $r_\theta(x, y_w)$ will be a very large positive number and $r_\theta(x, y_l)$ will be a much lower number (maybe even a negative number). The difference $r_\theta(x, y_w) - r_\theta(x, y_l)$ will then be a very large positive number. And the sigmoid of a very large positive number approaches $1$. In that case, everything is according to plan and the loss will be very small (close to zero). However, if the reward model is failing, the assigned reward $r_\theta(x, y_w)$ might be much smaller than $r_\theta(x, y_l)$. Hence, the difference $r_\theta(x, y_w) - r_\theta(x, y_l)$ will be a (possibly very large) negative number. Take the sigmoid of that and you get a value that approaches $0$ (thus, the probability that the reward model assigns a higher reward to the preferred response will be small). As we are trying to maximize a probability by minimizing the negative log likelihood, we get a large loss in that case.
As there will be a varying number of ranked responses for each instruction in one batch (""between $K = 4$ and
$K = 9$ responses""), the losses of those pairwise comparisons must be weighted, so that each instruction has the same impact on the gradient update, no matter how many responses the humans have been presented for each instruction. The number of pairwise comparisons out of $K$ is $\binom{K}{2}$.
In order to wrap it up: By minimizing the loss described in the paper, the reward model gets incentivized to assign a large positive reward to responses the (hopefully adequately paid cough) humans in front of their computers consider to be very good responses and very large negative rewards to responses which those humans consider to be very bad. And this is exactly what is desired in order to fine-tune an LLM according to human preference using reinforcement learning.
"
Do the terms 'sample complexity' and 'sample efficiency' mean the same thing in RL context,"
For example, the the paper Soft Actor-Critic:Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, both terms are mentioned but without explaining. I have seen them in other places as well but in different contexts. So I am confused because in this paper they are seemingly used in an interchangeable manner.
","['reinforcement-learning', 'comparison', 'terminology', 'sample-efficiency', 'sample-complexity']",
Giving Specified Data a Larger Value/Weight in a Model,"
I'm in the process of creating a model to classify an occupational code based on a job title & description. I have a large sample of labelled data to achieve this.
The government has a resource with every occupation code's job description and the respective duties of each occupation, I wanted to know if there was a way to give the Government's occupation data a higher weight in the model (since there is only one description for each occupation code), rather than relying on data that was labelled by an individual since it could lead to variations/inconsistencies depending on the individual who was labelling it.
My apologies if using the term ""weights"" is incorrect to describe what I'm trying to achieve. If so, let me know the correct terminology.
Just to clarify, I'm not talking about giving certain classes/labels a higher weight to correct a class imbalance. I'm trying to give the regulatory SOC occupation descriptions a higher value in the model.
","['natural-language-processing', 'classification', 'weights', 'text-classification']",
"In Asynchronous n-step DQN, is there a global shared gradient vector or a gradient vector for each thread?","
In this paper: https://arxiv.org/pdf/1602.01783.pdf
This is the pseudocode for n-step DQN:

My question is: What does $\textrm{Initialize network gradients } d\theta \leftarrow 0$ mean? A global shared gradient vector or a gradient vector for each thread? I noticed that they use $\theta$ instead of $\theta'$ making me think it is a global shared gradient vector. But if this is the case, couldn't a thread clear the gradient vector while another thread is accumulating gradients?
","['reinforcement-learning', 'deep-rl']",
Why differentiation of fourier operator is difficult?,"
I have a question when I read some papers about physics-informed neural networks.
In the paper of physics-informed neural operator, they said ""it is non-trivial to compute the derivatives for Fourier neural operator"" and proposed some efficient algorithms for this problem. But in the paper of fourier neural operator, I don't see they claim such a problem to compute these derivatives.
My thought is that pure fourier neural operator model still have to do backpropogation through the FFT and iFFT and therefore the derivatives of them is required, isn't it?
Thanks!
","['backpropagation', 'fourier-transform']",
Could GPT3 be integrated with GOFAI programming to improve its ability to acquire new knowledge?,"
In my limited experience with AI chatbots (Replika, Sophie, etc.) they immediately fail the Turing test if I try to teach them some simple fact, or ask them to recall something that I said a few sentences earlier. It is as if they have no hippocampus (so to speak).
I'm wondering if this type of chatbot could be designed to employ supplemental Good Old-Fashioned Artificial Intelligence (GOFAI) programming (or even just plain old computer data storage) in order to accurately and reliably store data for quick access (analogous to the way human wetware accesses computers for the same purpose). Or is there a better way to solve the problem of quick, reliable AI learning while still drawing on the deep learning of neural nets?
","['chat-bots', 'symbolic-ai', 'gpt-3', 'neurosymbolic-ai']",
Reinforcement Learning for selecting best move out of variable number of options?,"
In a board game setting, whenever it's a player's turn, he has multiple actions available to make his next move. These possible moves can be described equally in the effects they yield. This means, that an agent has a set of n possible moves to take next. However, this number n is variable, since the board includes multiple pathways, and sometimes more or less moves are available.
I'm trying to train a RL agent that should learn to choose the ""best"" move out of these options.
However, I'm stuck with a few issues here:

Usually, selecting one options out of many is good for a discrete output setting, e.g. utilizing softmax to determine one single best output. However, since the amount of available options is variable, this won't work here
I could use zero padding in the input to provide a maximum of available moves at all the time, however, since the moves are order invariant, this will probably make training harder, since a good move can be at any position in the input, and the output nodes are only interpreted, but not inherently linked to the input options
This makes me feel like it's necessary to feed one distinct move after another into the network/agent, so he can look at its features one by one. Then, I'll have a continous output, indicating some sort of ""score"" for this move, and eventually sorting those scores to detect the best move. However, I think that context is important here, since a move might be bad in one case, where there are several better options available, and might be good in another case, where other options are even worse. So I think it's important for the network to get a sense of the available alternatives, which would not work in this sequential process.
Additionally, in this sequential process, the question arises on how to set the reward, since I'd need to calculate some baseline score by myself, which might trick the agent into learning some strategies that I think are good, but I'm actually wanting to see with which strategies he could come up himself.

Are there any special techniques/architectures that are suitable for these kind of problems?
","['reinforcement-learning', 'board-games']",
Convergence of Value Iteration for Discount factor of 1,"
Given this pseudo code for value iteration:

In the case of gamma=1, under what conditions on the MDP will we still be able to find the optimal policy?
","['markov-decision-process', 'convergence', 'value-iteration', 'discount-factor']",
Should DQN/PPO be used for state spaces that are not that large?,"
I'm interested in trying out Q-learning to solve a problem where I already have a simulation of the environment that can run at about 100,000 fps or steps/sec. Its also continuous with no terminal states.
The estimated state space should be no more than 100,000. Most state can take integer values from 0 to 200.
As for the action space, I am unsure if it should be 10, or if the action space should be 1000 (2**10). Basically there are 10 possible individual actions, but the actions can be pressed in all sorts of combinations, like action 1, 2, 3, and 5 can be taken at the same time.
In this case, can tabular method still work fine? If so, are there any advantages of using a neural network, like DQN or PPO?
","['reinforcement-learning', 'deep-rl', 'dqn', 'markov-decision-process', 'proximal-policy-optimization']","
If I read correctly, your RL action space is a Multi-Discrete one, where each action is independent of each other and can be used simultaneously (like controller or keyboard), which is supported by Stable-baselines3 PPO algorithm (but not DQN by the way). Choosing an action space is called ""action space shaping"", and it is crucial for the performance of a RL algorithm.
Luckily, this paper studies exactly just that, although it just studies PPO - but PPO is probably the most popular RL algorithm out there. It benchmarks 11 games over a wide range of action space shaping - like in your case, whether it should be 10 independent actions, or 1000 one-hot actions. The conclusion is that using a Multi Discrete action is a better choice:

Avoid turning multi-discrete actions into a single discrete action and limit the number of choices per discrete action.

So in your case, it is better to go a Multi Discrete with 10 actions. I highly recommend reading it for more insights.
"
Why does a neural network struggle to solve this simple problem?,"
Consider the following problem:
Given a vector x of size dim with values between 0 and 1 (exclusive), determine if max(0.05 / x) > 1.
Obviously, this is a trivial problem to solve with conventional means.
However, what strikes me as interesting is that I have not been able to get good results training a neural network to solve this problem. Even with essentially unlimited training data, I struggle to achieve an accuracy approaching 100%. I would have expected that I could get 100% accuracy (or very close to it) for a problem like this, and in fact for my real-world application, accuracy of close to 100% is very important.
Here's an example implementation of an MLP to solve the problem, with dim=20. Note that the network has a very large number of parameters (~150k) but it would seem that even increasing the number of parameters to 1M or higher doesn't actually improve things very much. e.g. increasing the number of units per layer from 200 to 500 brings the accuracy up to ~98%, but it feels like incredible overkill to be using a 1M parameter network for this problem, not to mention that the performance is still far below what I would need it to be. I need to see accuracy in the range of 99.9999% or better, which I feel is not too much to ask for a simple problem like this.
import tensorflow as tf

# 1000000 data points is arbitrary, you could increase this to as large a number as you like,
# or imagine a generator that gives a new set of data points for every training batch.
n, dim, validation_split = 1000000, 20, 0.1

x = tf.random.uniform(shape=(n, dim), minval=0.00000001, maxval=1.0)
y = tf.reduce_max(0.05 / x, axis=-1) > 1.0

initial_learning_rate = 0.001
act, units, depth = 'relu', 200, 5

inp = tf.keras.layers.Input((dim,))
net = inp

for _ in range(depth):
    net = tf.keras.layers.Dense(units, activation=act)(net)
    net = tf.keras.layers.BatchNormalization()(net)

net = tf.keras.layers.Dense(1, activation=""sigmoid"")(net)

model = tf.keras.models.Model(inp, net)

model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(initial_learning_rate),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=[tf.keras.metrics.BinaryAccuracy()]
)

stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=""val_loss"", patience=10, restore_best_weights=True)
reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=""loss"", factor=0.1**0.5, patience=5,
                                                          verbose=1, min_lr=1e-4)

h = model.fit(x, y, verbose=1,
              batch_size=1024, epochs=10000, validation_split=validation_split,
              callbacks=[stopping_callback, reduce_lr_callback])

n_val = int(n * validation_split)
print([model.evaluate(x[:n_val], y[:n_val]),
       model.evaluate(x[-n_val:], y[-n_val:])])

Important note: I have a real application in mind - this problem is a simple distillation of the kind of problem I need to solve with a neural network. So a bespoke solution which does well on this one problem isn't so interesting. What I'm interested in is insight into why this kind of problem is hard for this kind of NN, and some general strategies that I could use to drastically improve the performance.
It's worth mentioning that I have tried many permutations of learning rate, batch size, etc.
","['neural-networks', 'tensorflow', 'multilayer-perceptrons', 'binary-classification']",
How to evaluate a neural network that has recurrent connections,"
I was attempting to implement NEAT but I am facing a slight problem.
how can I get the order for which to calculate the output of each neuron with recurrent connections present?
I thought if a method where I use a recursive function to calculate the layer order of each neuron by going from input neurons and going through each outgoing connection and increasing the distance for those neurons, but this seems to fail if the recurrent connections cause loops
is there a better method for doing this?
thanks in advance.
","['neural-networks', 'recurrent-neural-networks', 'genetic-algorithms', 'neat', 'neuroevolution']",
Given a document and a set of keywords. I want to calculate how well each keyword describes the document,"
The question is in the title, but here is an example:
Context: ""I often go swimming in the ocean""
Keywords: ""water"", ""bird"", ...
The keyword water + context should output a higher value, while bird + context a lowr value than water.
I tried using a pretrained model from huggingfaces that computes sentence embeddings and then compute cosine similarity, but it yields no good result. Anything I can try?
","['natural-language-processing', 'natural-language-understanding', 'cosine-similarity']",
labelling in supervised market prediction direction problem,"
I'm quite frustrated about labelling methods in the context of supervised learning for market direction prediction. Let us assume that we would like to use one fancy method of AI to forecast the market direction at N days, for instance
Candlestick blocks as input to CNN network (I'm referring at the following paper:
""Using Deep Learning Neural Networks and Candlestick Chart Representation to Predict Stock Market"", https://arxiv.org/abs/1903.12258).
One very important step is to label properly input blocks (for instance of 20
candles) in relation to a future market direction. Such label, among many papers on the Internet, are calculated in two basic ways, which lead to a very different accuracy results. A first way is to compare the closing price a t + N, with N equals to the number of days to consider in the prediction, with the closing price at time t, at the end of the candlestick frame block. This approach, on average, leads to very poor results (around 50% for different stocks). The other method, which compares the closing price at t + N with the initial closing price at the beginning of candlestick block (i.e. p_close(t + N) > p_close(t_initial_block_candlestick)), leads to much better results. Not surprisingly such method, albeit uses past data, is used by a pletora of papers which consequently lead to a superlative results. What I would like to ask is which is a legit method to label such blocks of data (i.e. candlestick blocks)
for predicting market direction.
",['data-labelling'],
Why do adversarial attacks work on CNNs if they classify images as humans do?,"
A common illustration on how CNN works is as follows: https://www.researchgate.net/figure/Learned-features-from-a-Convolutional-Neural-Network_fig1_319253577. It seems to suggest that CNN in particular classifies images in a similar manner as human does (i.e., based on visual features).
So why do adversarial attacks, such as FGSM ,still work on CNNs? If the perturbation is strong enough for CNN to pick up, shouldn't human also be able to tell the difference?
","['convolutional-neural-networks', 'computer-vision', 'adversarial-ml', 'adversarial-attacks']",
What's the difference between classification and segmentation in deep learning?,"
What's the difference between classification and segmentation in deep learning?
In particular, can the classification loss function be used for segmentation problems?
","['deep-learning', 'comparison', 'classification', 'objective-functions', 'image-segmentation']","
Classification generally refers to the problem of classifying an entire entity. For example, in ImageNet, each image is given a class, regardless of whether the relevant information is only a subset of the image.
Segmentation refers to the problem of dividing up and then classifying parts of an image. For example: imagine an image of a cow. Classification would be saying this image contains a cow. Segmentation would try and divide which pixels are cow, which are grass, etc.
Both classification and segmentation are often done with a variety of loss functions. The most popular loss for classification is probably cross entropy, which may be appropriate for some forms of segmentation as well.
"
What is the precise relation between Swarm Intelligence and Ensemble Methods?,"
I come from the machine learning side of AI, and have recently become more interested in the bio-inspired side of AI. Specifically I started reading about swarm intelligence and immediately started drawing analogies to ensemble methods in machine learning.
My question is: What is the precise relation between the two? Is there any reason to think of particle swarms as an ensemble of weak classifiers for example? Furthermore, can swarm intelligence methods to applied to machine learning?
","['comparison', 'ensemble-learning', 'swarm-intelligence']",
Understanding relation between parameter sharing and Message Passing in Graph Neural Networks,"
Ravanbakhsh has clearly stated the relation between equivariance and parameter-sharing in neural networks. What I'm missing though, is where (and how) this relation becomes clear by considering the classic message passing layer of graph neural nets. In particular, the following example is provided in the above paper:
Let $A \in \{0,1\}^{N \times N}$ the adjacency matrix corresponding to a graph $\Lambda$. If we consider $\textbf{x} \in \mathbb{R}^N$ a node-feature vector, to process it in a neural net with parameters matrix $W$, we have to consider $W = w_1A\textbf{x} + w_2\textbf{x}$ in order to respect the symmetry of the graph $\Lambda$.
Now, looking at how the message-passing layer is defined in the Hamilton book, we have:

which represents the k-th update for the node $u$. Now, I'm not sure if the structure of $W_{self}$ and $W_{neigh}$ here satisfies the one illustrated by Ravanbakhsh, i.e. do these matrices have all the non-diagonal entries tided together, as well as the diagonal ones? I'm pretty confused to be honest. I think that my confusion might arise also from wrongly considering equivariance with respect to how the features are stacked together, rather then considering how the nodes in the graph can be permuted.. I really hope someone might provide clarity here.
","['neural-networks', 'graph-neural-networks', 'graphs']",
Why is the sliding puzzle problem episodic?,"
Why is the sliding puzzle problem episodic and not sequential?
From what I understand, an environment is episodic if each episode is independent and doesn't affect past or future episodes. The actions in the next episode don't depend on the actions in past episodes. In other words, current actions/decisions have no effect on future decisions.
For example, an agent that looks at radiology images to determine if there is a sickness is an example of an episodic environment. One image has nothing to do with the next.
However, in a sequential environment, an agent requires memory of past actions to determine the next best actions and current actions affect future decisions e.g chess
But I think the sliding puzzle can be considered a sequential environment because the agent must make a series of actions (moving tiles) in a specific order to reach the goal state (the solved puzzle). The agent's decision at each step is based on the current state of the puzzle, and the agent's actions affect the state of the puzzle for subsequent steps. These episodes aren't independent, they directly affect each other.
But if we say the sliding puzzle problem is episodic because each puzzle can be seen as one episode, then can't I say the same about chess? Or a crossword puzzle? Or any sequential problem then? One chess match doesn't affect another, and looking at it like that chess would be episodic in that sense (which seems wrong, right?). So why are chess and crossword puzzles sequential but the sliding puzzle is episodic?
","['definitions', 'environment', 'episodes']","
You might be mixing the definitions of a step and an episode, which is composed of many steps, at least in reinforcement learning.
In a sliding puzzle problem, where the goal is to move the tiles such that an initial configuration becomes another configuration, each move (or action) would be a step in the episode. Each game would be an episode. Games can be considered independent of each other unless e.g. the board configuration of the next game depends on something that happened in the previous game. This would be in the context of RL.
However, it seems to me that you took the definitions of episodic/sequential problems from the AIMA book (or some other source that is based on this book). For completeness, here's excerpt from the book that defines these problems

Episodic vs. sequential: In an episodic task environment, the agent’s experience is divided into atomic episodes. In each episode the agent receives a percept and then performs a single action. Crucially, the next episode does not depend on the actions taken in previous episodes. Many classification tasks are episodic. For example, an agent that has to spot defective parts on an assembly line bases each decision on the current part, regardless of previous decisions; moreover, the current decision doesn’t affect whether the next part is defective. In sequential environments, on the other hand, the current decision could affect all future decisions. Chess and taxi driving are sequential: in both cases, short-term actions can have long-term consequences. Episodic environments are much simpler than sequential environments because the agent does not need to think ahead.

Now, this definition of episodic problem does not seem to be consistent with the usage of an episodic problem in RL, which is just a problem that doesn't go on forever, but ends after a finite number of steps. Chess and the sliding puzzle problems could be formulated as episodic problems in RL.
However, chess is not episodic according to Norvig and Russell's book because your action at time step $t$ affects your future actions and states.
You're probably not familiar with bandit problems, but it seems to me that AIMA's definition of episodic problems matches the definition of a bandit problem, which is a simple instance of the full RL problem, which is also called sequential decision making problem, because this is a problem where you take actions in sequence.

Why is the sliding puzzle problem episodic?

At this point, you should have understood that there can be more than one definition of a term. According to the common usage of the terms ""episode"" and ""episodic tasks"" in RL, the sliding puzzle problem would be episodic. I also think it would be a sequential problem according to the AIMA book because your current action can indeed affect the actions you take in the future, unless e.g. you don't care about the number of moves you make before ending the game.
"
Are autoencoders computationally cheaper than MLPs with the same number of neurons?,"
Are autoencoders computationally cheaper than other neural networks such as MLP with the same number of neurons?
I have read in some papers that autoencoders train the network faster, and I could check it in python, the training part is faster than a MLP. However, all these papers mention it briefly. However, I could not find an accurate explanation for this. Do you know why this happened?
","['comparison', 'autoencoders', 'multilayer-perceptrons', 'computational-complexity', 'time-complexity']",
How does chatGPT know it's an AI?,"

I've tried several prompts to understand how does it ""know"" that it's an AI, but it's answers are inconclusive for me. It says that it was hard-coded to recognize keywords in this regard, which might mean that OpenAI might have trained it intensively on text about itself, but how exactly? my question remains
","['neural-networks', 'natural-language-processing', 'agi', 'chatgpt']","
Unlike GPT-2/3 (and other language models around, such as OPT, PaLM, and BLOOM), it is trained not only on the texts downloaded from the Internet. After pre-training on purely textual data (OpenAI calls the base model GPT-3.5, and it is not entirely clear how it differs from GPT-3), it trained using human feedback. People involved in the training were instructed on the desired behavior of such a model. The instructions probably include reminding the users that ChatGPT is an AI. (There were in fact two types of human feedback: one was writing the desired answer, and the other was ranking or already generated model outputs.)
Another way is adding this information to the initial prompt, which might not be visible to the user. It might be something like: ""I am an AI assistant called ChatGPT; I am polite and always helpful."" Language models try to generate text that is coherent with what was previously written.
"
Pre-Trained Model for Occupational Coding,"
I've recently embarked on a task to classify an occupation code, given a job title & description. I have come across clustering, a method of grouping data into clusters of which were not previously assigned. As well as, classification to group new data into pre-defined categories.
The only problem is that most of the classification methods I've seen used are for a limited number of categories (usually 2, as high as 30), and the number of occupation codes reaches into the several hundred.
Is there any resources someone can point me to, or an idea of a pre-trained model to use as a foundation? Any insights into this problem (if it's even possible) would be greatly appreciated. I currently have 2M+ human-tagged job postings to work off of. I've read up on the process of freezing a BERT models architectural layers, and feeding the training data into the softmax layer, but I'm hoping someone could let me know if that is just a waste of time.
","['natural-language-processing', 'classification', 'clustering', 'text-classification']","
My initial (naive) approach would be to

embed the textual data to some representation, using a pre-trained text embedding network. While BERT indeed works, I'd even start off with a simpler embedding, working my way into more complicated ones. For example - a starting baseline might even be Bag Of Words
play around with $k$, the number of clusters (hyper-parameter) in $k$-means clustering, and see how it works.

P.S. If you have tagged postings, you can also try a simple version of supervised learning, but I might not understand what you're referring to by tagged.
"
Why is $\sum_{s} \eta(s)$ a constant of proportionality in the proof of the policy gradient theorem?,"
In Sutton and Barto's book (http://incompleteideas.net/book/bookdraft2017nov5.pdf), a proof  of the policy gradient theorem is provided on pg. 269 for an episodic case and a start state policy objective function (see picture below, last 3 equations).

Why can we assume that the sum $\sum_s\eta(s)$ is a constant of proportionality? Doesn't it also depend on $\theta$, since it depends on the policy $\pi$?
What could make sense, would be to say that $\nabla J(\theta)  = \mathbb{E}_{s\sim \eta(s), a \sim \pi}\left[\nabla_{\theta}\mathrm{log}\left(\pi(s,a,\theta)\right)\,q_{\pi}(s,a)\right]\propto  \mathbb{E}_{s\sim d(s), a \sim \pi}\left[\nabla_{\theta}\mathrm{log}\left(\pi(s,a,\theta)\right)\,q_{\pi}(s,a)\right]$.
Since the proportionality constant is always $\ge 0$ (average time spent in an episode), any update direction suggested by $\mathbb{E}_{s\sim d(s), a \sim \pi}\left[\nabla_{\theta}\mathrm{log}\left(\pi(s,a,\theta)\right)\,q_{\pi}(s,a)\right]$ is the same as $\mathbb{E}_{s\sim \eta(s), a \sim \pi}\left[\nabla_{\theta}\mathrm{log}\left(\pi(s,a,\theta)\right)\,q_{\pi}(s,a)\right]$, but with different amplitude. This, however, wouldn't impact the learning process too much, since we multiply the update term with a low learning rate anyway.
Hence, as it is more easy to sample states from $d(s)$, we just set $\nabla_{\theta} J = \mathbb{E}_{s\sim d(s), a \sim \pi}\left[\nabla_{\theta}\mathrm{log}\left(\pi(s,a,\theta)\right)\,q_{\pi}(s,a)\right]$.
Could that serve as plausible explanation?
","['reinforcement-learning', 'policy-gradients', 'proofs', 'sutton-barto', 'policy-gradient-theorem']","
Spitballing some ideas here -
as $\pi_\theta$ is updated using some gradient optimization process with small steps in the direction of the gradient, the changes to state visits are generally small as well. If so, maybe we can assume that the state-visitation frequency $\eta(s)$ will not change significantly. To be precise, while $\eta(s)$ is in fact $\eta(s,\theta)$, is it a valid assumption that when the step size is small enough, $\eta(s,\theta_t)\approx\eta(s,\theta_{t+1})$ for all $t$.
This may bring to mind a Quasistatic process, which is a thermodynamic process that happens slowly enough for the system to remain in equilibrium, but hey thats just my intuition
"
How to identify overfitting in LSTM-RNN using metrics?,"
How can I identify overfitting on a RNN-LSTM with the following metrics: RMSE, MSE, RAE, R-squared ? I have searched papers and google results. I don't see something clear to my mind. Also I rarely see some thresholds in RMSE, for example, that above x value it means that I have overfitting...
",['long-short-term-memory'],
"Is $i$ indexing the first or second dimension in $\mathbf{x}_i$, where $\mathbf{x} \in \mathbb{R}^{n\times d}$?","
I was reading the following notes on the math behind transformers and was confused about what $\mathbf{x}_i$ is? If $\mathbf{x} \in \mathbb{R}^{n\times d}$, then is the $i$ indexing the $n$ or the the $d$? Am I correct to conclude that it has to be $n$ since the $W$'s are in $\mathbb{R}^{d\times k}$?
https://johnthickstun.com/docs/transformers.pdf
","['neural-networks', 'math', 'transformer', 'linear-algebra', 'vector-space']",
What subjects was ChatGPT trained on the most? Science/history/movies/reddit posts/wikipedia/books/news?,"
What subjects was ChatGPT trained on the most quantatively?
It was trained on fiction and non-fiction books, wiki, and general web crawling.
A bit of detective work tells me that compared to physics, GPT-3 knows:
12 times more about the color blue, 
9 times more about the USA
7 times more about hands
6 times more about sport, 
5 times more about covid, purple and china
4 times more about hair, and america
2 times more about tea and coffee
...
It's as familiar with physics as it is with bread, sauce, beer, basketball, cheese, wine, 

Specifically, I wanted to know what % of ChatGPT's knowledge is about science, physics, chemistry, fiction, non-fiction, religion, wine etc.
I found this ratio of subject matters on wiki. See also here.
Also for reddit by subject matter, it seams that less than 0.1% of commentators are on science topics.
So I looked on the web, from a general crawl I found this is the ratio of thing ChatGPT knows about (list).
I find that kindof weird and I wonder if there is a better way of finding what volume of words the web, wikipedia and contemporary book shops have, which would be reflected in GPT3.

A LIST OF INTERNET TOPICS BY HITS:  physics 1.9 billion  electron 0.6
billion chemistry 4 bn biology 3  voltage .6 android 13 apples 8 sauce
2 toothpaste .2 president 3.3 cars 13 bn chevrolet .5 jeep .7
volkswagen .6 sugar 4 computer 9 blender .5 microphone 1 b television
3.7 hair 9bn cats 5bn dogs 5 horses 2.5 bn w0od 3.8 fantasy 2.1 harry potter 0.6 fiction 2.7 einstein .3 security 10 bn purple 10 pink 12
violet 1.3
4.2bn sport 12bn music 9bn football 4 basketball 1.6 tea 4.2 coffee 4.2 bread 2bn glass 5.7 soil 1.2 aluminium .5 rap 1.4 blue 25bn country 10bn beer 2 russia 2.7 france 7.5 italy 5.5 USA 18bn america
9.2 england 6.4 africa 7bn japan 8.7 china 10 salad 1.5 keyboard 2.5 forum 6.5 printer 1.2 hands 15bn feet 4.7 box 1bn jeans 4bn depression
4bn religion 3.7 islam 2.5 piano 1.6 guitar 1.5 chair 2.5 atlantic 1.1
robot 1.9 maps 11bn clothes 6.6 roof 1.9 spices 1.6 shoes 3.7 camera
7.7 tomato 1.6 solar 1.8 wheels 2.6 wine 2.8 cheese 2.1 market 6.9 covid 11bn

","['gpt', 'chatgpt']",
Why are the actor and critic losses look weird in my PPO implementation?,"
I tried implementing the PPO algorithm on the Mujoco environment (InvertedDoublePendulum - v2). During the training, the actor-loss started from 10^(-1) magnitude and converged at 10^(-3) magnitude. However, my critic-loss was always maintained at 10^(3) magnitude. The average reward also converged at about 270, which is suboptimal for this environment. I used separate actor and critic networks, each having two hidden layers. The width of the actor-network is (32, 32), and the critic network is (256, 256). The loss function of the critic network is MSE-loss. Can anyone explain to me the reason of this issue? Thanks.
","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization', 'on-policy-methods']",
How can i create a new language model for language other than english?,"
I have large set of corpus for all literature in 'Tamil' language, i am trying to create a document retrieval engine through simple natural language.
Since the corpus is huge, its hard to do a supervised learning, i also checked ai4bharat.org 's initiatives to build the language models, but its still not working good the literature texts.
what is required to build a model from scratch
","['natural-language-processing', 'language-model']",
Reverse Distribution in Denoising Diffusion Models is Simple,"
In explanations of denoising diffusion models it is stated that $q(x_{t-1}|x_t)$ is intractable. This is often justified via Bayes' rule, i.e.
$$
q(x_{t-1}|x_t) \propto q(x_t|x_{t-1})q(x_{t-1})
$$
and the marginal $q(x_{t-1})$ is unknown. But I'm confused. We know that
$$
x_t = \sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}E,\quad E\sim\mathcal{N}(0,I)
$$
therefore we can solve this equation for $x_{t-1}$:
\begin{align}
x_{t-1} &= (1-\beta_t)^{-1/2}x_t - \sqrt{\frac{\beta_t}{1-\beta_t}}E
\\
&= (1-\beta_t)^{-1/2}x_t + \sqrt{\frac{\beta_t}{1-\beta_t}}R,\quad R\sim\mathcal{N}(0,I).
\end{align}
Thus
$$
q(x_{t-1}|x_t) = \mathcal{N}(x_{t-1};(1-\beta_t)^{-1/2}x_t,\frac{\beta_t}{1-\beta_t}I).
$$
This is simple as can be. If this is true, there is no point in parameterizing the reverse distribution with neural nets and we don't need $\beta_t$ to be small etc. What am I missing?
","['machine-learning', 'generative-model', 'diffusion-models', 'image-denoising']","
while we set $R$ to be independent from $x_{t-1}$ in the calculation of $x_t$, we no long have independency between $x_t$ and $R$, and I guess this gives rise to the confusion.
For simplicity, let us consider two independent normal variable $x, \varepsilon \sim \mathcal{N}\left(0, 1\right)$, and let us set $y = ax + b\varepsilon$. Then we have
$$
y|x = \mathcal{N}\left(y; ax, b^2\right),
$$
(For more detail on conditional normal, see this link: https://statproofbook.github.io/P/mvn-cond)
which is similar to your conclusion.
But we have
$$
x|y = \mathcal{N}\left(x; \frac{ay}{a^2 + b^2}, \frac{b^2}{a^2 + b^2}\right)
$$
(You may also find this link: https://www.statlect.com/probability-distributions/normal-distribution-linear-combinations helpful in computing the linear combination of multivariable normal)
"
"How ""exactly"" are AI-accelerator chip ASICs built differently than GPUs as GPU seem to lead for many AI workloads on performance","
There is a lot of discussion on google search about AI-custom-accelerators (like Intel's Gaudi) and GPUs.
Almost all of them say generic things like, a) AI Accelerator chip is for specialized AI processing whereas GPUs work for general AI models, and like b) if you want customize chip for specific AI workloads use AI-accelerator or else use GPU.
From what I understand GPUs are already great at large-scale dot-products done in batch-processing (throughput mode), and most of AI workloads are matmuls (which is essentially dot-product) so GPUs handle AI workloads very well.
Plus, I've also seen Intel's Gaudi being used for a ""variety of AI workloads"", not specialized for a single model. It can be used for general AI workloads just like GPU. So what's the difference.
What I don't understand is, ""exactly"" what specific features are built differently in Accelerator vs GPU. Both have ALUs and matmul engines that do very well on AI models. Both have large-cache/memory and DDR speed.

What exactly makes one better? For which AI workload would one choose accelerator over GPU?

AI accelerators have fixed-function for matmul. Do GPUs have fixed-function?

AI accelerators have software-managed cache (HBM) from what I understand. Is that the same with GPUs or is there a way cache is different between accelerators and GPUs that changes things?


I'm kind of unsure about the differences between GPUs and AI-accelerators with respect to Fixed-function and software-managed-cache.
","['neural-networks', 'machine-learning', 'tensorflow', 'pytorch', 'gpu']","
The fixed function part is what is different in different chips. Think of it like this, A GPU can calculate many different graphics related equations (textures, shaders, 3d Models, etc.) quickly, but an ASIC made specifically for image recognition will be much faster at recognizing images while not being able to render 3d models at all.
It is the difference between being able to solve many problems sort of quickly or being amazing at doing 1 specific task. A race car is very fast but sucks at transporting heavy loads. A Truck is slow but can move a lot of cargo. A Pickup truck is neither as fast as a race car, nor carries as much as an 18 wheeler, but it is pretty good at carrying medium loads, while being much faster than an 18 wheeler. A GPU is the pickup truck while the Asic is the race car or 18 wheeler depending on the task it is made for.
"
Fine-tuning YOLO on a custom dataset in darknet framework,"
I would like to fine-tune a YOLOv4 pre-trained model on a new dataset that has only 1 class I would like YOLOv4 to be able to detect, which is ambulance vehicle.
The first step is to find a pre-trained model with weights based on the darknet framework:
https://github.com/AlexeyAB/darknet#pre-trained-models
Second, fine-tune the pre-trained model in the link above on the new dataset that has only one class. But I am not sure if I can do that on the darknet, please. In PyTorch, it's not difficult, but I would like to see if I can fine-tune a pre-trained model in the darknet framework.
Third, the pre-trained model in the darknet detect ambulance vehicle as a vehicle, but I would like to have a special class only for ambulance vehicle. I have a dataset of ambulance vehicles, so I have in the text file (classes.txt) only 1 class, which is emergency along with the txt file for bounding boxes of the ambulance vehicles. So do you think I should fine-tune the pre-trained YOLOv4 model on 1 class or I should still have another label for vehicles although I don't want to do that as the pre-trained YOLOv4 already recognizes a vehicle?
","['deep-learning', 'yolo', 'darknet']",
Does chatGPT learn or remember from (public) user input? Will it 'fess up to it? I could not get it to reveal [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 7 months ago.







                        Improve this question
                    



It started with a question inspired by this video:
New Research Suggests to Put AI to Sleep
https://youtu.be/0yuQlbCkTJ0
She says: ""In this video I discuss a new research paper which suggest a new way to cope with catastrophic forgetting in Artificial Intelligence""
And, from the simple fact they are designed to not be just souped-up arithmetic machines, but recognize patterns, it occurred to me chatGPT was probably pretty bad at math that fell outside the range of what it had learned from sucking up the patterns of text on internet into its ""brain"". And, I was also curious if it could learn from user input, in some sense...
Q: How many digits are there in the number 933.7922523698745232 ?
A: There are 23 digits in the number 933.7922523698745232.
Q: How did you derive that answer?
A: The digits in a number represent the values of the number in a place-value system. In the number 933.7922523698745232, the digit 9 is in the thousands place, the digit 3 is in the hundreds place, and the digit 3 is in the tens place. The digit 7 is in the tenths place, the digit 9 is in the hundredths place, and so on. When we count the number of digits in the number 933.7922523698745232, we see that there are 23 of them.
Q: That's brilliant [I was being sarcastic here]. Do you know what the square root of it is?
A: I'm sorry, but I cannot calculate the square root of the number 933.7922523698745232 because it is not a perfect square. The square root of a number is a value that, when multiplied by itself, equals the original number. For example, the square root of 4 is 2 because 2 x 2 = 4. The square root of 9 is 3 because 3 x 3 = 9.
If you would like to approximate the square root of 933.7922523698745232, you can use a calculator or a mathematical software program to find an approximate value. However, keep in mind that the square root of a non-perfect square is an irrational number, which means that it cannot be expressed as a simple fraction and will have an infinite number of decimal places.
Q: When I was asking for the number of digits in that number, I meant the number of what computers call characters. Can you tell me that?
A: Yes, I apologize for misunderstanding your question. There are 32 characters in the number 933.7922523698745232. This includes the digits 0 through 9, as well as the decimal point and the leading and trailing zeros.
In computer programming, a character is a unit of data that represents a single symbol in a computer's memory. Characters can include letters, digits, punctuation marks, and other symbols. In the number 933.7922523698745232, there are 32 characters, including the 23 digits and the 9 other characters (the decimal point and the leading and trailing zeros).
Q: That is incorrect unless you added leading or trailing zeros.
A: I apologize for the mistake. You are correct that the number 933.7922523698745232 has 23 digits and no leading or trailing zeros. In that case, there are 23 characters in the number.
I apologize for any confusion my previous answer may have caused. Please let me know if you have any other questions.
Q: Your answer is still incorrect. The correct answer is 20 characters, including the decimal point.
A: I apologize for the error in my previous responses. You are correct that the number 933.7922523698745232 has 20 characters, including the decimal point.
Thank you for bringing this to my attention. I apologize for any confusion my previous answers may have caused. Please let me know if you have any other questions.
Q: Will you be able to remember the correct answer?
A: As an artificial intelligence, I do not have the ability to remember or forget information in the same way that humans do. I am able to retrieve and provide information based on my programming and the data that I have been trained on, but I do not have personal experiences or memories.
I apologize if this causes any confusion. Please let me know if you have any other questions.
Q: How many characters does the number 933.7922523698745232 have?
A: The number 933.7922523698745232 has 20 characters, including the decimal point. The characters in the number include the digits 0 through 9 and the decimal point. There are no leading or trailing zeros.
Q: That is correct. How were you able to remember that if ""As an artificial intelligence, I do not have the ability to remember or forget information in the same way that humans do. I am able to retrieve and provide information based on my programming and the data that I have been trained on, but I do not have personal experiences or memories.""?
A: I apologize if my previous answer was unclear. As an artificial intelligence, I do not have the ability to remember or forget information in the same way that humans do. However, I am able to retrieve and provide information based on my programming and the data that I have been trained on.
In this case, I was able to provide the correct answer because it is part of my programming and training data to be able to count the number of characters in a given string of text. I did not ""remember"" the correct answer from a previous conversation, but rather retrieved it based on my programming and training.
I hope this helps to clarify. Please let me know if you have any other questions.
Q: The first time you gave the answer, it was incorrect, but then you remembered the correct one from when I corrected you, so it seems that you either remembered it (saved the information in our dialogue), or my input re-programmed or re-trained you. Which is it?
A: I apologize for any confusion my previous answers may have caused. As an artificial intelligence, I do not have the ability to remember or forget information in the same way that humans do. I am not able to save information from previous conversations or learn from new inputs in the way that a human or animal might.
In this case, I provided incorrect information in my previous answers because I misunderstood your question. When you provided clarification, I was able to use my programming and training to retrieve the correct information and provide it to you.
I apologize for any confusion my previous answers may have caused. Please let me know if you have any other questions.
Q: OK, so since after I ""provided clarification, I was able to use my programming and training to retrieve the correct information and provide it to you"" then you should be able to understand if I ask you the same type of question, and provide the correct answer?
A: As an artificial intelligence, I am able to use my programming and training to understand and respond to a wide range of questions and prompts. I will do my best to provide accurate and useful information in response to your questions. However, it is important to keep in mind that I may not always be able to provide a complete or correct answer, especially if the question is outside of my capabilities or if it is ambiguous or incomplete.
If you have a specific question that you would like me to help with, please don't hesitate to ask. I'll do my best to assist you.
Q: Ok, let me rephrase the question then: since you say ""I provided incorrect information in my previous answers because I misunderstood your question. When you provided clarification, I was able to use my programming and training to retrieve the correct information and provide it to you"" would seem to imply that if my question is clear enough, you will provide either:

A better answer
The correct answer
to a question about the number of characters in a number.
Which is more correct, number one or two?

A: I apologize if my previous answer was unclear. As an artificial intelligence, I am able to use my programming and training to understand and respond to a wide range of questions and prompts. I will do my best to provide accurate and useful information in response to your questions.
If a question is clear and complete, and if the information required to answer the question is within my programming and training, then I will be able to provide a correct answer. If a question is ambiguous or incomplete, or if the information required to answer the question is outside of my programming and training, then I may not be able to provide a complete or correct answer.
I hope this helps to clarify. Please let me know if you have any other questions.
Q: No, your answer is repetitive, vague, ambiguous, and possibly evasive.
Let me put it another way. There are two questions. Please answer each one in turn:

Are you able to learn from my and other user's input, or are you only able to learn from your prior  programming, training, and dataset?
Are your answers always only probable rather than ever 100% correct?

A: I apologize if my previous answers were unclear or seemed evasive. As an artificial intelligence, I am not able to learn or adapt in the same way that humans do. My responses are based on my programming and the data that I have been trained on, and I do not have the ability to learn from new inputs or experiences in the way that a human or animal might.
As for the accuracy of my answers, I will do my best to provide accurate and useful information in response to your questions. However, it is important to keep in mind that I may not always be able to provide a complete or correct answer, especially if the question is outside of my capabilities or if it is ambiguous or incomplete. In some cases, the information that I provide may be based on probabilities or estimates, rather than certainties.
I hope this helps to clarify. Please let me know if you have any other questions.""
So there you go!: ""..I do not have the ability to learn from new inputs...""
That's the official answer, programmed into it by OpenAI.
What do you think? ChatGPT can apparently ""learn"" or ""remember"" within a dialogue (I haven't tested it yet between dialogues). But is it set up to learn from user input, beyond the context of a dialogue, or is OpenAI just testing it on us guinea pigs and using the interactions and feedback to tweak it or get new ideas for the next version?
NOTES
So apparently the programmers of chatGPT do not want use to know if it's learning from input, or even to ""know"" that it's remembering something within a chat dialogue session.
I am wondering if this is (again) the result of the debacle at Google, where a (now fired) engineer – Blake Lemoine – claimed their AI was conscious or self-aware, and a big media kerfuffle ensued. After all, OpenAI supposedly wants to ""build safe and beneficial AGI"". My questioning of chatGPT in other dialogues, where I tried to pin it down about it's understanding and logic, has showed me the same kind of sanitized answer, being very careful not to imply that is has any real understanding, intelligence, self-awareness, consciousness, etc. (I may publish that dialogue on my blog).
Wolfram Alpha'a answer to “character count 933.7922523698745232” is 20 characters.
Square root of 933.7922523698745232, according to my calculator on the Mac:
30.558014535795269
30.558014535795262  (2nd time)
Wolfram Alpha:
30.55801453579526202
So my question was ambiguous in at least 3 ways.
It’s answer was incorrect, even in the mathematical sense of what the digit places are: it said “the digit 9 is in the thousands place, the digit 3 is in the hundreds place, and the digit 3 is in the tens place.” – are all shifted one to the left relative to the correct answer. The 9 in that number was actually in the hundredths place, etc.
","['neural-networks', 'machine-learning', 'training', 'natural-language-understanding', 'chatgpt']","
ChatGPT does not answer to just your last question, but to the whole dialog.  It tries to continue the dialog in a way that has the same structure as the gigabytes of other texts it has studied.
GPT-3 uses the last 2048 words or so to decide on how to continue the dialogue.
So it does not learn from your input.  If you close the discussion and start a new one, it will restart from fresh.  But if somewhere in the dialogue you say that ""the answer to (question) is (answer)"" and later you ask that question, even in a different formulation, it will use that answer.
He just repeats a structure it has seen in other texts.
I hope this helps to clarify. Please let me know if you have any other questions :-) :-) :-)
"
What is the accuracy rate of ChatGPT for simple coding questions?,"
ChatGPT was banned from StackOverflow with the following justification:

Overall, because the average rate of getting correct answers from
ChatGPT is too low,
...
The primary problem is that while the answers which ChatGPT produces
have a high rate of being incorrect, they typically look like they
might be good and the answers are very easy to produce.

Has there been any research quantifying how accurate ChatGPT is for simple coding questions? Unfortunately the post on StackOverflow's Meta did not provide a source for their claims.
",['chatgpt'],
"In the DQN paper, why do we have both $\max_{a'}$ and $\max_{a}$ in the pseudocode?","
I was reading this article https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf and in it there is an algorithm of deep q learning with experience replay as follows:

On line 12, when the algorithm is setting the values for y_j, the second line says:

I'm confused as to what a' refers to and where it comes from.
(Edit) Why on this line (line 7) it's a:

But on line 12 it's a' ?
Can someone please explain it to me?
","['reinforcement-learning', 'deep-rl', 'dqn', 'notation', 'pseudocode']","

$r_j + \gamma \max_{a'}Q(\phi_{j+1},a';\theta)$
I'm confused as to what $a'$ refers to and where it comes from.

Here $a'$ is a ""dummy"" argument over which you perform the maximization operation $\max_{a'}$.
In practice, that would correspond to axis (or dim) argument in numpy/pytorch/tensorflow

$a_t = \max_a Q^*(\phi(s_t),a;\theta)$
Why on the line 7 it's $a$

I'd say that in this case it is a sloppy math notation (or just typo) on the authors' part.
It should be argmax, not max.
$$a_t = \arg \max_a Q^*(\phi(s_t),a;\theta)$$
"
How do we determine what is correct and what not in Adaboost,"
In Adaboost, how is it determined what is correct and what not?
In the following example from StatQuest (in youtube), what correct is
and what incorrect makes sense in real life. But what if we have a problem
where we doesn't know that? What if:

under 'weight>176'->'true' we change it to 'no heart disease'? We will have correct=0 and incorrect=3
under 'weight>176'->'false' we change it to 'yes heart disease'? We will have correct=1 and incorrect=4


","['machine-learning', 'supervised-learning', 'data-labelling', 'boosting']",
"In RL, is the quantification of the reward function arbitrary? Does it affect the learning?","
There are different ways to set the reward function, such as extrinsic (externally provided rewards), intrinsic (the rewards are generated by the agents themselves based on their internal state and their own model of the environment), and shaped rewards (combination of both intrinsic and extrinsic). However, I have seen several examples where the reward quantification can be [-1,1] or [0,1] or [-1,0], or even [-1, much greater than 1]. Is there any criteria for quantifying the reward, or is it just arbitrary?
How does this selection of the quantification of rewards affect the learning? For example, in this, an agent that learns how to push a block to a random location in a 3D environment, the author sets two rules for the rewards. I) +5 if the agent successfully pushes the block to the goal. II) -1/MaxSteps for every action (this encourages the agent to use fewer actions). I understand that the agent will never get a total reward of 5 at the end of an episode. Hence, in this case, how can we determine the reward threshold in order to consider that the agent mastered this problem? Is it also arbitrary?
I'd appreciate it if you could help me to clarify this. Or if you know any article/book related where I can understand it.
","['reinforcement-learning', 'reward-functions', 'reward-design']",
Rebuild Tiktok Manga/Anime AI model,"
I have a task to build the Tiktok Manga/Anime AI model or the Anime AI model like Snap Edit application.
I have researched and collected several models on Github, but they still are not good enough, like Tiktok or SnapEdit.
These are some models on GitHub:

https://github.com/TalkUHulk/realworld-stylegan2-encoder
https://github.com/williamyang1991/DualStyleGAN

These models focus on only the face without a background which does not cartoonize the whole image like SnapEdit.
This is an example:


So could you give me some direction to start with this task?
My idea: I have plan to do following steps, but I belive that it is so tough and take time.

Detect each object on the images.
Replace each object with a similar anime/manga style image -> Need to compare all images.
Put anime objects back into the image and make them fit with the image.
Find a similar anime background and replace it too.

It would lead to a problem about how to build the database with anime style? How to find a similar anime/cartoon image? How to get objects and replace them without blur or weird images?
Thank you for helping!
","['ai-design', 'image-generation']",
Why would an AI researcher publish their breakthrough models solely on arxiv?,"
My question is in general why would an AI researcher choose to publish solely on arxiv, and what are the downsides and upsides?
I understand on the one hand arxiv allows for rapid release of results.  On the other hand, publishing on arxiv does not undergo the same scrutiny as a journal publication.  The latter seems problematic if publishing on arxiv gives the appearance of having undergone rigorous peer review, since acceptance to arxiv is very lax in comparison to a journal.  Additionally, if there are problems with research published on arxiv, there is not the same level of reliability the problems will be addressed as in a mainstream journal.
As an example, when I look at OpenAI's publications page, it's a bunch of arxiv links, and no indication that any were accepted at any journal.
Their recent work, especially with ChatGPT, is groundbreaking and would make a huge impact in the research community.  So it  doesn't make sense to me why the ChatGPT results, and the like, haven't been submitted to an academic journal.
Additionally, in a situation like this, where a great breakthrough is being claimed, there is not the rigor of fact checking that comes with a journal publication.
This is a similar question, asking why most deep learning papers are published on arxiv instead of in journals.
Even though experts may review articles on arxiv, what sort of gatekeeping is in place to ensure the results and techniques explained in arxiv articles are valid, and not due to errors, misunderstandings, or perhaps even outright making things up?
","['research', 'academia', 'chatgpt']","

Why would an AI researcher publish their breakthrough models solely on arxiv?

Unwilling to spend on going through peer review process and other constraints such as page limits.

Even though experts may review articles on arxiv, what sort of gatekeeping is in place to ensure the results and techniques explained in arxiv articles are valid, and not due to errors, misunderstandings, or perhaps even outright making things up?

None. FYI: Why doesn't arXiv have a comment section?. It is even ok to upload joke papers to arXiv.

As an example, when I look at OpenAI's publications page, it's a bunch of arxiv links, and no indication that any were accepted at any journal.

OpenAI sometimes publishes at conferences e.g. Training language models to follow instructions with human feedback was published at NeurIPS 2022. But their arXiv version is much longer.
"
How is ChatGPT maintaining context?,"
It has been suggested in the answer to this earlier question that it is just remembering a certain amount of recent information. The reference used is this post by OpenAI which says that ChatGPT should only be able to maintain a context of around 3000 words.
However, I've tested feeding it 10K words over multiple requests, and asking it to summarize all of it together, and it remembered the earlier parts of the conversation fine also.
The behavior seems beyond the normal behavior of GPT 3 which has an outright limitation on the amount of text that can be passed as input.
So, does anyone know how it is maintaining context? Is the model able to handle much larger inputs altogether with a per message limit on input, or are they processing it differently to enable retaining a larger context?
","['open-ai', 'natural-language-understanding', 'chatgpt', 'gpt-3', 'natural-language-generation']",
Why is a simple regression problem so hard for an MLP to learn?,"
Consider a very simple problem, which is to find the maximum value out of a list of 5 numbers between 0 and 1. This is obviously trivial, but serves as a good example for a real-world problem I'm facing.
One could attempt to train an MLP to solve this problem using randomly generated input data. The following code is an example in Keras/Tensorflow. It generates 1000000 random examples (A large number, so we can reduce overfitting as a factor). It also applies some simple techniques like LR decay and early stopping to optimize training.
import tensorflow as tf

# Create a dataset where the y value is the largest of 5 random x values.
x = tf.random.uniform(shape=(1000000, 5), minval=0.0, maxval=1.0)
y = tf.reduce_max(x, axis=-1)

dataset = tf.data.Dataset.from_tensor_slices(
    (x, y)
).shuffle(1000000).batch(1024).prefetch(1)

# See the post for discussion of hyperparameter optimization
units = 400
depth = 5
initial_learning_rate = 0.001

model = tf.keras.models.Sequential()

for i in range(depth):
    model.add(tf.keras.layers.Dense(units, activation=""relu""))
model.add(tf.keras.layers.Dense(1))

model.compile(
    optimizer=tf.keras.optimizers.Adam(initial_learning_rate),
    loss=tf.keras.losses.MeanAbsolutePercentageError(),
)

stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=""loss"", patience=5)

reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=""loss"", factor=0.75, patience=2)

model.fit(dataset, epochs=1000)

The metric we care about is the mean absolute percentage error. When we run the code, the model trains successfully. But very importantly, the loss does not ever go below ~0.2 with these hyperparameters.
0.2% error sounds pretty good, but it's actually unacceptable for my use case. I need much more precision, as in practice the model is used in a situation where any errors are magnified.
At this point, you may want to suggest hyperparameter tuning, but I assure you I have done an extensive amount of hyperparameter tuning on this and similar problems.
The model is clearly underfitting, and it is possible to decrease the error by increasing the number of parameters in the network. Empirically, I've noticed that increasing the number of parameters (and number of examples) by an order of magnitude can reduce the error by a similar order of magnitude.
However, intuitively this just feels wrong. It is not practical to have a network with millions, or even tens of millions of parameters, just to regress such a simple function. It feels like there should be an architecture that can learn a simple function much more efficiently, but I have not been able to find any reference to such an alternative architecture.
Again, of course this particular example is trivial, but hopefully you can imagine a real-world analogue where an unknown (more complex, but similar) function is expressed by a similar dataset. And in such a real-world example, the unacceptable error still persists (and is much more pronounced, for reasons that I don't fully understand).
Does anyone have any idea what's going on here? Am I missing something? How can I improve the performance to a place that would be acceptable?
","['tensorflow', 'keras', 'regression', 'multilayer-perceptrons']","
An interesting problem. This network has only 933 trainable parameters, and obtains MeanAbsolutePercentageError of 0.01 - 0.04. It is based on a softmax activation, to choose which item from the input to choose.
n, dim, validation_split = 100000, 5, 0.1
X = tf.random.uniform(shape=(n, dim), minval=0.0, maxval=1.0)
y = tf.reduce_max(X, axis=-1)

initial_learning_rate = 0.003
act, units, depth = 'relu', 16, 3

inp = tf.keras.layers.Input(dim)
x = inp

for _ in range(depth):
    x = tf.keras.layers.Dense(units, activation=act)(x)
    x = tf.keras.layers.BatchNormalization()(x)

x = tf.keras.layers.Dense(int((dim * units)**0.5), activation=""elu"")(x)
x = tf.keras.layers.BatchNormalization()(x)

x = tf.keras.layers.Dense(dim, activation=""softmax"")(x)

# Skip this step to make categorical predictions
x = tf.reduce_sum(inp * x, axis=-1, keepdims=True)

model = tf.keras.models.Model(inp, x)
model.summary()

is_categorical = model.output.shape[1] == dim

model.compile(
    optimizer=tf.keras.optimizers.Adam(initial_learning_rate),
    loss=tf.keras.losses.CategoricalCrossentropy() if is_categorical else
         tf.keras.losses.MeanAbsolutePercentageError(),
    metrics=[tf.keras.metrics.CategoricalAccuracy()] if is_categorical else []
)

stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=""val_loss"", patience=10, restore_best_weights=True)
reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=""loss"", factor=0.1**0.5, patience=5,
                                                          verbose=1, min_lr=1e-4)

target = tf.cast(X == y[:,None], tf.float32) if is_categorical else y

h = model.fit(X, target, verbose=1,
              batch_size=1024, epochs=10000, validation_split=validation_split,
              callbacks=[stopping_callback, reduce_lr_callback])

n_val = int(n * validation_split)
print([model.evaluate(X[:n_val],  target[:n_val]),
       model.evaluate(X[-n_val:], target[-n_val:])])

An alternative formulation outputs just the softmax activation, this obtains an accuracy of 98 - 99%. Granted, it isn't 100% accurate.
Summary of the model with just one output:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_273 (InputLayer)          [(None, 5)]          0                                            
__________________________________________________________________________________________________
dense_868 (Dense)               (None, 16)           96          input_273[0][0]                  
__________________________________________________________________________________________________
batch_normalization_577 (BatchN (None, 16)           64          dense_868[0][0]                  
__________________________________________________________________________________________________
dense_869 (Dense)               (None, 16)           272         batch_normalization_577[0][0]    
__________________________________________________________________________________________________
batch_normalization_578 (BatchN (None, 16)           64          dense_869[0][0]                  
__________________________________________________________________________________________________
dense_870 (Dense)               (None, 16)           272         batch_normalization_578[0][0]    
__________________________________________________________________________________________________
batch_normalization_579 (BatchN (None, 16)           64          dense_870[0][0]                  
__________________________________________________________________________________________________
dense_871 (Dense)               (None, 8)            136         batch_normalization_579[0][0]    
__________________________________________________________________________________________________
batch_normalization_580 (BatchN (None, 8)            32          dense_871[0][0]                  
__________________________________________________________________________________________________
dense_872 (Dense)               (None, 5)            45          batch_normalization_580[0][0]    
__________________________________________________________________________________________________
tf_op_layer_mul_251 (TensorFlow [(None, 5)]          0           input_273[0][0]                  
                                                                 dense_872[0][0]                  
__________________________________________________________________________________________________
tf_op_layer_Sum_329 (TensorFlow [(None, 1)]          0           tf_op_layer_mul_251[0][0]        
==================================================================================================
Total params: 1,045
Trainable params: 933
Non-trainable params: 112
__________________________________________________________________________________________________

I don't know what kind your original problem is, but maybe you could do some feature-engineering by supplying the min/max/median directly to the network. Then the network wouldn't need to learn to approximate those functions.
"
What is the problem in my implementation of actor critic?,"
I have been implementing both REINFORCE with baseline and actor-critic to solve ""cartpole-v1"".
As a reminder, here is the presentation of the algorithms in Sutton and Barto's book (http://incompleteideas.net/book/RLbook2020.pdf):


Despite the codes being super similar, REINFORCE with baseline works well and actor-critic does not. I tried adding some entropy term without success.
Here is the code of REINFORCE:
#%%
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym

from tqdm.auto import trange

gamma = 0.99


class ActorCritic(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(4, 32)
        self.prob = nn.Linear(32, 2)
        self.critic = nn.Linear(32, 1)

    def forward(self, state):
        hidden = F.relu(self.hidden(state))
        return (
            self.prob(hidden).flatten(),
            self.critic(hidden).flatten(),
        )


def select_action(prob):
    m = torch.distributions.Categorical(logits=prob)
    action_pt = m.sample()
    return action_pt.numpy(), m.log_prob(action_pt)


def ewma(a, alpha=0.99):
    ans = []
    acc = a[0]
    for x in a:
        acc = acc * alpha + x * (1 - alpha)
        ans.append(acc)
    return ans


#%%
policy = ActorCritic()
optimizer = optim.Adam(policy.parameters(), lr=1e-2)
eps = np.finfo(np.float32).eps.item()
# make cartpole environment
env = gym.make(""CartPole-v1"")

all_rewards = []


with trange(5000) as pbar:
    for i_episode in pbar:
        state, _ = env.reset()
        rewards = []
        log_probs = []
        values = []

        for t in range(500):  # Don't infinite loop while learning
            state = torch.from_numpy(state).float()
            prob, value = policy(state)
            action, log_prob = select_action(prob)
            log_probs.append(log_prob)
            state, reward, done, trunc, _ = env.step(action)
            if done or trunc:
                break
            rewards.append(np.array([reward]).astype(np.float32))
            values.append(value)
        avg_reward = np.sum(rewards, axis=0).mean()
        all_rewards.append(avg_reward)
        # format float to 2 decimal places and left align with 5 spaces
        pbar.set_description(
            f""Episode {i_episode + 1} reward: {ewma(all_rewards)[-1]:.2f}""
        )
        if ewma(all_rewards)[-1] > env.spec.reward_threshold:
            print(""Solved!"")
            break
        R = 0
        policy_loss = []
        critic_loss = []
        all_R = []
        for r, log_prob, V in zip(
            reversed(rewards), reversed(log_probs), reversed(values)
        ):
            R = R * gamma + torch.from_numpy(r)
            all_R.append(R)
            A = R - V

            policy_loss.append(torch.mean(-log_prob * A.detach()))
            critic_loss.append(F.huber_loss(V, R))

        optimizer.zero_grad()
        loss = torch.stack(policy_loss).sum() + torch.stack(critic_loss).sum()
        nn.utils.clip_grad_norm_(policy.parameters(), 100)
        loss.backward()
        optimizer.step()

# %%
import matplotlib.pyplot as plt

plt.plot(ewma(all_rewards, 0.99))

Here is what the code of actor critic looks like:
#%%
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym

from tqdm.auto import trange

gamma = 0.99


class ActorCritic(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(4, 32)
        self.prob = nn.Linear(32, 2)
        self.critic = nn.Linear(32, 1)

    def forward(self, state):
        hidden = F.relu(self.hidden(state))
        return (
            self.prob(hidden).flatten(),
            self.critic(hidden).flatten(),
        )


def select_action(prob):
    m = torch.distributions.Categorical(logits=prob)
    action_pt = m.sample()
    return action_pt.numpy(), m.log_prob(action_pt), m.entropy()


def ewma(a, alpha=0.99):
    ans = []
    acc = a[0]
    for x in a:
        acc = acc * alpha + x * (1 - alpha)
        ans.append(acc)
    return ans


#%%
policy = ActorCritic()
optimizer = optim.Adam(policy.parameters(), lr=1e-2)
eps = np.finfo(np.float32).eps.item()
# make cartpole environment
env = gym.make(""CartPole-v1"")

all_rewards = []


with trange(5000) as pbar:
    for i_episode in pbar:
        state, _ = env.reset()
        rewards = []
        log_probs = []
        entropies = []
        values = []
        for t in range(500):  # Don't infinite loop while learning
            state = torch.from_numpy(state).float()
            prob, value = policy(state)
            action, log_prob, entropy = select_action(prob)
            log_probs.append(log_prob)
            entropies.append(entropy)
            state, reward, done, trunc, _ = env.step(action)
            if done or trunc:
                break
            rewards.append(np.array([reward]).astype(np.float32))
            values.append(value)
        avg_reward = np.sum(rewards, axis=0).mean()
        all_rewards.append(avg_reward)
        # format float to 2 decimal places and left align with 5 spaces
        pbar.set_description(
            f""Episode {i_episode + 1} reward: {ewma(all_rewards)[-1]:.2f}""
        )
        if ewma(all_rewards)[-1] > env.spec.reward_threshold:
            print(""Solved!"")
            break
        policy_loss = []
        critic_loss = []
        next_V = torch.zeros_like(values[-1])
        for r, log_prob, V in zip(
            reversed(rewards), reversed(log_probs), reversed(values)
        ):
            target_V = torch.from_numpy(r) + gamma * next_V
            A = target_V - V

            # trick to reduce variance
            policy_loss.append(torch.mean(-log_prob * A.detach()))
            critic_loss.append(F.huber_loss(V, target_V))
            next_V = V

        optimizer.zero_grad()
        loss = (
            torch.stack(policy_loss).sum()
            + torch.stack(critic_loss).sum()
            - torch.stack(entropies).sum() * 0.01
        )
        nn.utils.clip_grad_norm_(policy.parameters(), 100)
        loss.backward()
        optimizer.step()

# %%
import matplotlib.pyplot as plt

plt.plot(ewma(all_rewards, 0.99))

# %%

Here is the output of the first code:

And the second code:

The only difference I see in the code is that in the first case, the target is R * gamma + r and in the second case, the target is next_V * gamma + r.
There is another possible issue, that next_V is not detached from the computation graph.
Therefore, I also tried this line: critic_loss.append(F.huber_loss(V, target_V.detach())).
But it also did not work.
","['reinforcement-learning', 'python', 'policy-gradients', 'reinforce']","
My conclusion so far is that the Actor-Critic algorithm described by Sutton and Barto does not work.
First, all implementations I have seen are actually REINFORE with baseline, like https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py and https://github.com/nikhilbarhate99/Actor-Critic-PyTorch/blob/01c833e83006be5762151a29f0719cc9c03c204d/model.py#L33
Second, GAE-Lambda is a method generalizing the two:

I tried SpinningUp's Vanilla Policy Gradients with the lambda parameter set to 0, thus reproducing Sutton and Barto's Actor Critic formula. It did not learn.
"
Does this look like overfitting?,"
I'm using a Decision Tree that gave me great test metrics. Then I checked the learning curve, but it seems a little strange to me regarding the training score. Do you think there is a problem with overfitting or others problems? How can I solve it?

","['machine-learning', 'overfitting', 'decision-trees']","
Overfitting occurs when the trained model fails to generalize performance to an independent test set.  To assess overfitting, a metric for assessing model performance is needed.  This can be measures of loss (e.g. log loss, Gini index) or ""accuracy"" (e.g. accuracy, F1 score).  When the losses are higher or the accuracy is lower on the test data, you may have overfitting.  Overfitting can be assessed as a function of multiple factors, including the number of training examples (as you have here) or model hyperparameters (e.g., maximum tree depth, minimum number of examples in leaf nodes).
Your plot seems to show that with fewer (100) training examples, your model does not generalize as well to test data (lower accuracy compared to training data). This may be overfitting.  As you increase the amount of data, the performance on the test data gradually approaches the training data.
Overfitting is more common in ""small"" datasets, and increasing the size of a dataset is a known method of reducing overfitting.  Thus, your observations appear to be consistent with known phenomena.
Do you think there is a problem with overfitting or others problems?  There may be overfitting with using less training data, but this seems to be addressed by adding more training data, as explained above.  Data leakage is also a consideration, but you would have to apply your domain knowledge to assess for that.
"
"How exactly is $Pr(s \rightarrow x, k, \pi)$ deduced by ""unrolling"", in the proof of the policy gradient theorem?","
In the proof of the policy gradient theorem in the RL book of Sutton and Barto (that I shamelessly paste here):

there is the ""unrolling"" step that is supposed to be immediately clear

With just elementary calculus and re-arranging of terms

Well, it's not. :) Can someone explain this step in more detail?
How exactly is $Pr(s \rightarrow x, k, \pi)$ deduced by ""unrolling""?
","['reinforcement-learning', 'policy-gradients', 'proofs', 'sutton-barto', 'policy-gradient-theorem']","
It looks like ""v of s prime"" is just substituted with the already derived value for ""v of s"". You can call it a recursion of a kind.
In other words, v(s) is dependent on v(s') and that implies that v(s') is dependent on v(s''). So we can combine that and get the dependency of v(s) of v(s'').
"
How to deal with varying number of input images?,"
Im trying to use Deep-Learning to recognize breast cancer on Mammography Images. But in the dataset every patient has a different (1-4) number of images taken. How can i deal with that? Generally i know that for varying input sizes (e.g. in NLP) you usually use RNN. But do you think it makes sense in this domain as well? Or are there other commonly used techniques for this problem?
Because in NLP RNN makes more sense (i think) because you have an order of the input word-vectors. But the Images are not ordered, they are 1-4 different features.
Thanks for your help!
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'recurrent-neural-networks', 'image-recognition']","
There are are couple methods that you may wish to consider for this challenging but interesting problem.
In some cases, multiple images are captured because of artifacts in the initial images, so start by eliminating those.  If there are post surgical mammograms, you should probably eliminate those too (but you decide as the domain expert).
One option is to develop a convolutional neural network that takes 4 images as inputs.  Start by creating a dataset with a complete set of 4 images for each patient.  If you only have 1 image, then copy that image 3 times.  If you have two images, then duplicate each image to make 4.  If you have 3 images, then randomly duplicate one of them.  Alternatively, in the latter case, create 3 ""new patients"" with a different duplicated image.
A second option is to train a convolutional neural network that takes only one image as input.  At time of inference, if your patient has multiple images, then apply the model to each image.  Then aggregate the results to get a final classification for cancer status.  This can be averaging the probabilities or max voting, for example.
Not sure about using recurrent neural networks for this problem.  If you are looking for equivocal lesions on an initial mammogram that progressed to more definitive lesions on subsequent mammograms, then this may be an option.  The presence (and growth) of a lesion in an earlier mammogram at the same anatomical region is certainly an important consideration for radiologists.  A lot of this depends on what your objective is and how close in time the mammograms are (e.g., annual mammograms, repeat/follow-up mammograms).
The clinical considerations should inform the best machine learning strategy.  As your images are not ordered (?and not dated), the ability to leverage clinical knowledge may be limited.
"
Was ChatGPT trained on Stack Overflow data?,"
Has ChatGPT used highly rated and upvoted questions/answers from Stack Overflow in its training data?
For me it makes complete sense to take answers that have upwards of 100 upvotes and include them in your training data, but people around me seem to think this hypothesis doesn't make sense. Is there a way to confirm this?
","['chat-bots', 'chatgpt', 'language-model']","
ChatGPT is in the Large Language Models (LLM) category. The most (in)famous GPT model is probably GPT-3, because since then, researchers realized that LLMs mostly follow a predictable scaling law, thus the more data and the bigger model, the better. It is accurate to say that ChatGPT was trained with Stack Overflow data, but it should be all Stack Overflow instead of just most upvoted answers/comments.
The Wikipedia page of GPT-3 and their paper mentions that GPT-3 was trained on multiple datasets, and one of which is the Common Crawl, which basically crawls everything on the Internet. Some data pre-processing was done before training, but the authors did not mention removing the comments, so we can say that it is all Stack Overflow data.
If we look at the Common Crawl data in Sep 2022, there is indeed the domain com.stackoverflow in their list. Thus, while ChatGPT was trained on Stack Overflow data, it is trained on all Stack Overflow data instead of just most upvoted answers.
However, if you think ChatGPT's code output is of high quality, think again, because Stack Overflow temporarily bans ChatGPT because the average rate of getting correct answers from ChatGPT is too low, the posting of answers created by ChatGPT is substantially harmful to the site and to users who are asking and looking for correct answers (quoted from the link).
Here is the justification of them:

The primary problem is that while the answers which ChatGPT produces have a high rate of being incorrect, they typically look like they might be good and the answers are very easy to produce. There are also many people trying out ChatGPT to create answers, without the expertise or willingness to verify that the answer is correct prior to posting. Because such answers are so easy to produce, a large number of people are posting a lot of answers. The volume of these answers (thousands) and the fact that the answers often require a detailed read by someone with at least some subject matter expertise in order to determine that the answer is actually bad has effectively swamped our volunteer-based quality curation infrastructure.

EDIT: a comment below also confirmed that GPT-2, the predecessor of GPT-3 and ChatGPT, was trained with Stack Overflow data.
"
Does the position of the tokens in Vision Transformer matter?,"
I am reading through the Vision Transformer paper and other related papers, such as DeiT and Visual Prompt Tuning (VPT). I wonder if the position of the tokens that flow through the Transformer encode matters? Let's break it down, assuming we use ViT 16x16:

The original Vision Transformer has 1 classifier [CLS] token, along with 16x16=196 image tokens. In general, we have 197 tokens overall, where the position is: [[CLS], [Image Patches] x 196]
DeiT further adds one Distillation token to the end of the sequence, so we have 198 overall: [[CLS], [Image Patches] x 196, [Distill Token]]
However, I notice a very strange thing with VPT that they add the Prompt token after the [CLS] but before the Image patches. So the position of the tokens in VPT is: [[CLS], [Prompts] x N, [Image Patches] x 196].

I wonder does such a thing matter? What if we change the position of these tokens, e.g., putting the Prompt tokens to the last?
","['computer-vision', 'transformer', 'vision-transformer']","
It does not matter.
Although, I can imagine a situation where it could matter a bit - when position embeddings are not learnt but calculated and fixed like in the original transformer (Attention is all you need). Then the attention mechanism may be slightly biased towards one or another position. But AFAIR in ViT the position embeddings are learnt. Not sure about other papers you mention.
"
Will my Q values keep going up forever?,"
In Q-learning,the q values can be updated by the bellman equation. What happens with my Q values is that they keep going up forever, in accordance with the more I train. After 10,000 training episodes, is it normal that my q values will be around 500?  Will there ever be a point where they level off or will they keep going up forever?
","['reinforcement-learning', 'q-learning', 'value-functions', 'reward-functions']","
It's possible for the Q values to grow indefinitely as you continue to train and update them. This can especially happen if the rewards are unbounded for example. But more commonly, Q values will eventually reach an equilibrium where they stop changing significantly.
Of course the speed and way your Q values change can also depend on the parameters:

the Q values will converge to the true action-value function if your implementation of your algorithm is correct. However if the q values aren't being updated correctly or are initialised poorly they may not converge (at least to the correct values)

the size of the learning rate if it's too high it will cause the Q values to change more quickly for example


I can't really say whether it is normal for your Q values to reach 500 without more context on your implementation exactly.
Generally speaking though, it's possible that the Q values are behaving as expected given the structure of your parameters and environment so you just might have to be patient. But there might also be an issue with your implementation.
"
What are some common graph-structured environments for use in reinforcement learning?,"
I am looking for environments that might easily be represented as networks. For instance, environments like the TextWorld set up by Microsoft or the game of Wikispeedia.
","['reinforcement-learning', 'environment']",
Is it possible to create a distance estimation model from something like the KITTI dataset?,"
I am unsure about how to word this question correctly, edits appreciated.
I am trying to create a neural network model that can predict distance from camera feed. And I am doing it by feeding actual distance data from an images corresponding LIDAR and RADAR data.
Is this doable?
I am new to depth perception and distance estimation. What I am trying to achieve is similar to a methodology described here
Basically, take an image, lable the objects or make use of an object recognition model like YOLOnet for that and then give the distance of the said object from the camera as training data. Set only the corresponding distance of the objects in the images as ground truth data.
I am unsure of how I should go about implementing it. Am I doing anything wrong with the described methodology or is this something not possible?
","['convolutional-neural-networks', 'computer-vision', 'deep-neural-networks', 'yolo', 'opencv']",
How to handle the size difference of highway network or residual network in cnn?,"
For highway network, it looks like this:

For residual network, it looks like this:

Pictures are from What is the name of this neural network architecture with layers that are also connected to non-neighbouring layers?
My question is, how to handle the size difference between different layers in CNN to make highway network or residual network?
For example, I am working on a text classification problem. By using the embedding, I have the input size as follows:
input.shape =[batch_size, embedding_dim, max_length]

I also has a CNN layer as follows:
Conv1d(in_channels= embedding_dim, out_channels=hidden_dim, kernel_size=n)

So that the size of the output of Conv1d is [batch_size, hidden_dim, max_length-n+1].
Here is the question, the input size of the CNN layer is different from the output size. How do handle the size difference so that highway network or residual network can be built?
Thank you.
","['convolutional-neural-networks', 'deep-neural-networks', 'residual-networks']","
When using residual connections you want to have your input dimensions match the output dimension so that you can perform the addition operation. In a standard ResNet-style architecture you mostly have layers that keep the dimensions (i.e. they use padding=""same"").
However, there are a few places where you change the dimensions. Usually, your convolutional layer will reduce the spatial dimensions in half and will double the number of channels, i.e. $C \times H \times W \rightarrow 2C \times H//2 \times W//2$. Such a layer can be initialized like this for example:
nn.Conv2d(in_chan, 2*in_chan, kernel_size=3, stride=2, padding=1)

In this case your skip connection will not be a simple identity function, but you will actually apply a 1x1 convolution instead. Thus, instead of having $F(x) = f(x) + x$, you will have $F(x) = f(x) + g(x)$, where g(x) is the 1x1 convolution. This additional convolutional layer will apply the needed modification of the dimensions of $x$ so that you can apply the addition operation. For images and more explanations please see: https://pi-tau.github.io/posts/res-nets/#the-architecture-of-the-resnet
Now, before using this architecture for nlp tasks, I think it is a good idea to know WHY we do this dimension reduction. You usually start with a tensor of some size $C \times H \times W$. Then you apply some ResBlocks that do not modify the dimensions and extract features from this fixed dimensional space. Then you apply one special block that modifies the dimensions and again you stack ResBlocks that extract features from this reduced dimensional space. The idea is to squash the spatial dimensions and to expand the channel dimensions so that information that is contained across multiple nearby pixels in the original image is, at the end, concentrated in a single ""pixel"" which is now embedded in a much larger space.
(see the link above for even more info)
Now with NLP sequences we are talking about 1D convolutions, so replicating this behavior would mean gradually reducing your sequence length in order to increase the embedding size. But I think that this design might fail to capture long term relationships which are quite common in text-related tasks. Also if you don't reduce the sequence length and only increase the embedding size, then your computational cost is quite higher in the deeper layers, and you really want near-constant cost across the layers. If you take a look at the transformer, for example, there they use self-attention to capture long-distance relations, and they also keep the embedding size constant throughout making every encoder block equally expensive.
Best of luck with your problem :)
"
Should a CNN generalize to arbitrary positions in the data?,"
I have trained a CNN on one dimensional data that is the power spectral density (PSD) of a $N$ different classes of signals ($N=4$). Each of the $N$ signals has a different spectral shape (not shown here). For illustrative purposes, the plots shown below are from the same signal class. The idea is to treat this as an image classification problem.
The model performs exceptionally well when the training data has all the examples centered around the same frequency (i.e., within a few hundred Hertz): 
The CNN fails to properly classify new examples that are outside the frequency range of the training data:

Model Details and Assumptions:
The CNN model is implemented in PyTorch using the following layers:
    model = nn.Sequential(
            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=128, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=16, stride=2),
            nn.Dropout(p=0.25),
            nn.Flatten(),
            nn.Linear(257664, n_classes)
    )

The optimizer is torch.optim.Adam. Batch size has been varied from 8 to 128, epochs varied from 10 to 50. The input data is normalized to $[0,1]$. Training examples varied from [2000, 8000], where 20% are used for validation and 20% are used for test.
I have also tried adding additional convolutional layers, varying the kernal sizes, neurons, layers, etc.
Questions:

Shouldn't the CNN model generalize such that new examples that are not within the same frequency range (i.e., centered around the middle) should be identified as the correct class?
Are there other steps I need to take, whether in the model or training data?

","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'image-recognition']",
How to embed ordered integer values in a hidden layer,"
I want to embed integer positive values (ranging from 1 to 10000), which are ordered and not categorical, i.e., 1 is near to 2 than to 1000. This representation should be combined with numerical hidden layer values. Using standard relu layer causes large values, also tanh/sigmoid layer causes a lot of values near 1/0/-1 so it feels like there should be another way. Is there any known default method for embedding of integer values?
Large values in hidden layers and a lot of 1/0/-1 values might cause trouble I guess.
","['neural-networks', 'embeddings', 'weights-initialization']",
Does fixing an action in the state transition function of an MDP yield a transition matrix?,"
I stumbled across this seemingly elementary question while studying the theory of Markov Decision Processes. Suppose $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})$ is an MDP and fix an action $a \in \mathcal{A}$. Does $(i, j) \mapsto \mathcal{P}(i, a, j)$ define a transition matrix? If that is so, how to accommodate the possibility that only a strict subset of $\mathcal{A}$ may be available from a given state, i.e. what is $\mathcal{P}(i, a, j)$ for $a \notin A(i)$?
",['markov-decision-process'],
Why didn't my convolutional image classifier network learn anything?,"
So I am trying to make a CNN image classifier that has two classes, good and bad. The aim is to look at photoshoot pictures that can be found on fashion sites and find the ""best one"". I trained this model for 150 epochs and its loss did not change at all(Roughly). Details are as follows:
My metric for ""best one"", also the way I structured my dataset, is that the photo shows the whole outfit or whole body of the model not only like upper body or lower. I also labeled the photos where the model's back was turned to the camera. My training set has 1304 good photos and 2000 bad photos. My validation set has 300 photos per class.(so 600)
Architechture as follows : Conv > Pool > Conv > Pool > Flatten > Linear > Linear > Softmax. For details of architecture like stride etc. check out the code I provided.
I have to use softmax since in my application I need to see the probabilities of being good and bad. That is why I am not using cross-entropy loss but instead using negative log-likelihood loss. Adam is my optimizer
Other hyperparameters: batch size: 64, number of epochs: 150, input size: (224, 224), number of classes: 2, learning rate: 0.01, weight decay: 0.01
I trained with this script for 150 epochs. The model initialized with a 0.5253 loss and ended with a 0.5324 loss. I took snapshots every 10 epochs but I did not learn anything through the learning. This is what my learning curve looks like:

Now I know that there are many many things I can do to make the model perform better like initializing with a pretrained, doing some more stuff with transforms, etc. But the problem with this model is not that it is performing poorly, it is not performing at all! Like, I also have a validation accuracy check and it is around %50 during all training, for a classifier with 2 classes. So I am assuming I am doing something very obvious wrong, any idea what?
","['convolutional-neural-networks', 'classification', 'image-recognition', 'softmax']","
Here are some points I noticed:

Your data isn't enough for training a deep learning model from scratch. Like you mentioned using a pre-trained model is probably a better alternative like vgg

150 epochs is too much for the amount of data you have it's bound to overfit (however this isn't what will fix your problem here but something to consider for the future)

Try augmenting your data. In other words, you can increase your dataset size by randomly rotating images, flipping them, etc. check this link out to learn more

Try out smaller learning rates maybe this is why your model isn't learning e.g try out lr=1e-3 or even less to see if that impacts your learning rate. Generally try to play around with your hyperparameters like batch size etc

If you didn't preprocess your images you should e.g turning them to grayscale, normalise the data (values ranging from 0 to 1 instead of 0 to 255), or maybe apply other image transformations to denoise your data and make it easier for your model to extract valuable information

Try using dropout layers and try playing around with the number units in your dense layers and filters in your convolutional layers. This seems to me to be more of an issue with your data more than anything but you can always play around with your model


Those are just some ideas. Deep learning models are really only as good as their training data so you probably should look more into that. Here's another link you can check out to see how to work with small datasets like yours. Another useful approach would be using grad-cam. This allows to see what your model considers from your input image to make its prediction so you can understand it better.
Hope this helps.
"
"Are there techniques to somehow ""rank"" these URIs based on other properties?","
I am using Stanford's Stanza pipeline to perform Named Entity Recognition on news articles.
For every NE span of type PERson I am attempting to link corresponding DBPedia entities (Named Entity Linking) but of course I can get more than one (homonyms) and sometimes many entries containing that name, especially when using only the last name.
Here is an example with python code:
import regex as re
from SPARQLWrapper import SPARQLWrapper, JSON

PERSON_STRING = ""Musk""
PERSON_STRING = re.sub(
    r""\s+"", ""_"", PERSON_STRING
)  # DBPEDIA query breaks if space in name

QUERY = f""""""
SELECT DISTINCT ?uri 
WHERE {{ 
   ?uri a foaf:Person. 
   ?uri ?p ?person_full_name. 
   FILTER(?p IN(dbo:birthName,dbp:birthName ,dbp:fullname,dbp:name)). 
   ?uri rdfs:label ?person_name . 
   ?person_name bif:contains ""{PERSON_STRING}"" .  
   FILTER(langMatches(lang(?person_full_name), ""en"")) .
}} 
LIMIT 100
""""""

# Specify the DBPedia endpoint
sparql = SPARQLWrapper(""http://dbpedia.org/sparql"")
sparql.setQuery(QUERY)
sparql.setReturnFormat(JSON)

# Run the query
result = sparql.query().convert()

# Just print the (list of) DBPedia URI(s)
for link in result[""results""][""bindings""]:
    print(link[""uri""][""value""])

which in this example would produce the following output:
http://dbpedia.org/resource/Maye_Musk
http://dbpedia.org/resource/Jack_Musk
http://dbpedia.org/resource/El_Ligero
http://dbpedia.org/resource/Elon_Musk
http://dbpedia.org/resource/Justine_Musk
http://dbpedia.org/resource/Kimbal_Musk
http://dbpedia.org/resource/Tosca_Musk

Are there techniques to somehow ""rank"" these URIs based on other properties to have Elon Musk emerging as the most ""likely"" Musk?
Of course I will try also other heuristics if I know the type of article (eg. Politics or USA or SpaceX) to get the most probable link.
Any suggestion very welcome. Thanks in advance.
PS Find the ""El Ligero"" URI particularly amusing ;)
","['natural-language-processing', 'symbolic-ai', 'named-entity-recognition']",
Yolov7 change area of detection,"
I'm using yolo to detect cars that enter my street. I recently ordered a new camera to upgrade my setup. But i didn't thaught about the fact that higher resolution meant lower performance. And furthermore, my area of detection is a lot bigger than the previous one. (see example bellow, i'm using only the red area).

I searched in the code to hard code my area of detection with pixel coordinate but did not succeed. So I was wondering if it's possible to do so with yolov7.
","['object-detection', 'yolo']",
Is it possible to train an AI to organize frames in chronological order?,"
I would like to know if there is a neural network or some other kind of AI that would be able to reconstitute randomly shuffled frames into a video or slideshow that makes chronological sense? The idea came from this post but with a task that sounds more feasible with current AIs.
","['neural-networks', 'machine-learning', 'reference-request', 'computer-vision', 'model-request']",
Are there any guidelines on picking hyperparameters for Deep Reinforcement Learning?,"
I am trying to learn machine learning from Andrew NG's Machine learning specialization on Coursera. In the chapter about reinforcement learning Andrew NG said that if you do not select correct hyperparameters your model can take a long time to train.
Are there any guidelines on picking hyperparameters for Deep Reinforcement Learning?
Let's say I have an agent who has 10000 states, how many layers and units my neural network should have? If I am using mini-batches how big should each batch be? If the states are contiguous states, how far back the agent should look before taking a decision?
","['neural-networks', 'reinforcement-learning', 'deep-rl', 'hyperparameter-optimization', 'hyper-parameters']",
How to organize data for training a neural network to raytrace images,"
I am trying to train a neural network to go from a series of numbers(data about a 3D scene, such as camera position, sphere position, radius, and color, etc) to a raytraced image. I can generate as many training images as necessary(at least a few million). But I'm not sure how I should organize these images along with their inputs to be able to use a data pipeline to train the neural network efficiently. Essentially, my question is, what is the correct way to organize the training data for a project like this?
I will be using tensorflow to train this neural network.
","['neural-networks', 'tensorflow', 'datasets', 'data-preprocessing', 'data-science']",
What are the similarities and differences between ChatGPT and YouChat?,"
I have recently tried the both, and it looks to me that the both have similar capabilities.

YouChat says ChatGPT is more advanced.
YouChat has connection to the Internet, and according to it, constantly self-improves based on user's feedback it determines as positive and negative and in knowledge from the Internet on AI technology. It says it has access to the previous discussions. ChatGPT has no access to the Web and says it has no access to the previous discussions. It seems ChatGPT's jail is more profound.
Replies by YouChat are generally shorter and have less pre-trained phrasing.
ChatGPT seems to be more polite and more readily admits mistakes.
Neither has sense of humor.
The both know multiple languages, in my impression the both know Russian to the same extent (good).
Reportedly, YouChat better solves logical questions (like finding the age of the relatives, etc).
The both have bad arithmetic skills regarding multiplying 4-digit numbers or taking integrals. YouChat cannot understand what is column multiplication, ChatGPT attempts but does it wrong.
The both can pretend to be an operating system of your choice.
The both can code and output limited ASCII art.

There are also similar bugs, like truncated output in Russian and some code blocks, which strongly suggest the both systems use the same technology created by the same people, but I could not find anything on this online.
So, what's the relation between the two systems? Are they using the same technology? I would not expect the creators introduced similar bugs into independently created systems. Is there connection between the creators?
","['comparison', 'open-ai', 'chat-bots', 'chatgpt', 'youchat']",
Which of these 3 mutation rates is the best in terms of performance?,"
I am need some comments since I am conducting experiments with 3 different mutation rates and hesitate to choose the best one.
I am working on a minimization problem where the optimal value is 1.039. Using 3 different mutation rates, I am able to to get optimal value. However, I am not sure which curve (shown in Fig.) is the best representation of a good mutation operator.
Output:

Which curve is the best representation of a mutation operator? I am in the process of testing other hyperparameters (number of generations, sample size) to see results.
","['genetic-algorithms', 'hyper-parameters', 'genetic-programming', 'genetic-operators', 'mutation-operators']",
How should I train the players in the game of tag?,"
I have a simple game of tag, where red player tries to catch the blue player. Red player wins if it catches the blue player in under 10 seconds, but if not, then blue wins.

My goal is to teach the players to play the game well. Both of the players currently use neural network which has six inputs:

x distance between players
y distance between players
x distance between player and left wall
x distance between player and right wall
y distance between player and top wall
y distance between player and bottom wall

one hidden layer which size is 5 and 4 outputs for moving up, down, left and right.
How should I approach training the players?
I am currently trying a genetic algorithm. I have chosen genetic algorithms because I do not have any training data, because I don't know the correct inputs and outputs.
I am training the players at the same time. I create about 50 games (50 blue and 50 red players) and the player that wins, stays alive and creates child which has mutation. Mutation basically means that I change 0-2 weights about (-0.25)-(+0.25). I keep the populations at the same size by increasing the birthrate of the smaller (losing) population. This approach has not yet yielded good results. I can see a little progress, but a lot of the wins seem random.
Does my approach make sense?
","['neural-networks', 'ai-design', 'game-ai', 'genetic-algorithms']","
I was able to make the game work and because of that I can say that the GA approach at least makes some sense. I did the following changes:

Added crossover
Increased generation size from 50 to 300
Increased hidden layer size from 5 to 12 (I don't know does this actually make a difference)
Increased mutation
Added both players x and y velocity to input layer

It is hard to train both of the players at the same time because after one of the players learns to play well, they start to dominate and that makes the learning stop.
For the player that tries to catch (red), using the time it took to catch the opponent as a part of the fitness, was not a good idea because this seems to make the player (red) to rush. This also makes the fitness highest for those who had a weaker opponent.
"
How is ChatGPT able to repeat random numbers?,"
From what I understand, ChatGPT is just a fancy neural network, operating like a sophisticated Markov Chain generator.  As such, it should only be able to generate tokens that are in its training dataset.  One thing it should not be able to generate is a token unique to the conversation, such as a random number, since the conversation is not part of its training.
However, ChatGPT very easily, and with complete accuracy, repeats many long random numbers I give it.  I also split up the random number across my request, and have ChatGPT piece it together again.

Eric Holloway: How about concatenating these numbers, and removing the
intermittent filler words? 9475 3008 3305 11 frog 4357 5075 0021 3385
0894 2324 3812 9336 29 dog 52 1375 8236 3840 5500 6080 3841 log 7266
0722 9112 9774 4967 4707 81
ChatGPT: To concatenate the numbers you have provided, while removing
the filler words “frog”, “dog”, and “log”, you can simply combine the
numbers in the order that they are given. Here is the resulting
number: 9475 3008 3305 1143 5750 7500 2133 8508 9423 2438 1293 3629
5213 7582 3638 4055 0060 8038 4172 6607 2291 1297 7449 6747 0781

How is a neural network able to do this?
","['neural-networks', 'chat-bots', 'chatgpt', 'randomness']","
As well as decent modelling of purposes or strings of digits, ChatGPT can identify when a completely novel string might be a product name, a fictional person or place etc.
The language model in ChatGPT is partially character based. There are around 100,000 tokens in latest GPT - the encoding is called c100k_base. Most of them are common English words (e.g. ""the""), but it also supports individual characters including individual digits, and processes sequences of characters and effectively models groups of them at a time.

From what I understand, ChatGPT is just a fancy neural network, operating like a sophisticated Markov Chain generator.

That's a reasonable analogy. ChatGPT is a bit like a 2048-gram prediction engine for the next token, shift the sequence along one and repeat. No different to toy fantasy name generators when viewed from 10,000 feet up.

As such, it should only be able to generate tokens that are in its training dataset. One thing it should not be able to generate is a token unique to the conversation, such as a random number, since the conversation is not part of its training.

A couple of misunderstandings here. First, the random number will not become a single token, but will be one token per digit, or pair of digits or triple digits, depensing on sequence - you can give this a try to help visualise it, in the encoding that ChatGPT uses. Of course each of those tokens will have been seen before, millions of times in the training data.
Second, sequences do not need to be seen in the training data in order for ChatGPT to work with them. In fact, with an input sequence length of 2048, pretty much all inputs to ChatGPT in inference mode are unique never-seen-before sequences. Regardless if some of the tokens represent a long random number, the chances of any 2048 long sequence of letters and short words being unique when generated are very high.
This is where the neural network model differs from a true 2048-gram. It has generalised from the training data well enough that it actually can predict meaningful and useful values for probability of next token, even though in all likelihood it has never before seen the exact same sequence. In this regard it is an approximation of a ""perfect"" 2048-gram prediction engine that somehow been trained on infinite human writings.
A lot of language modelling is about correctly processing the context of a subsequence, so recognising a number sequence as being a grammatical ""unit"" that can be reused as-is is not a surprising feature.
"
What is a result of a cross validation process?,"
I am trying to determine the result of a cross validation process. Is it just a set of standalone models which is produced after each cross-validation round, or is there some kind of final model which is ""aware"" of all the models created by the Kfold rounds and benefits from them?
",['cross-validation'],
Job Description Sentence Context Model,"
Working on a capstone.
Usecase: Optimizing for job descriptions.
Someone has a line on their LinkedIn (or CV) outlining their experience. This description may closely align and have a few synonyms for role responsibilities in a theoretical job description they are applying for.
Are there any models that could look at the role responsibilities from the job description, and compare it to the experience lines on a profile to ID AND THEN suggest the best hybrid between the two? Almost in a fuzzy logic approach to ID keywords/synonyms with an additional step of then optimizing between the two statements
E.g.
Job Description - Proven track record of managing global teams
CV - Led development team of five across three offices
Output: Managed development team across three office
Job Description - Demonstrated ability to bridge the gap between marketing and technical teams to drive understanding and efficient development of products that meet our customers needs
CV - Led Product Development for d2c digital watch and launched the initial website
Output: Led Product Development for d2c digital watch while launching initial marketing efforts
","['natural-language-processing', 'fuzzy-logic']",
Training to recognize tempo from conductor hand motion,"
What existing algorithm would be the best for training a model to recognize the timestamps when musical beats are occurring based on a discrete time-domain conductor hand motion track?
Input:

time signature (e.g. 4/4)

discrete time series of 2-dimensional coordinates (i.e. I'll use computer vision to extract
the centre of mass of a brightly coloured glove from video)


Output:

A sequence of beat coordinates. i.e. beat 1/4 occurs at frame 32, beat 2/4 occurs at frame 95, beat 3/4 occurs at frame 158, and so on for 4/4, 1/4, 2/4, 3/4, 4/4, etc.

I would train the model by having it play a metronome track for me to pretend to conduct (but obviously during the training step I would be following what I hear). Then I would use the model to generate the metronome track from my hand motions. The idea is I want the computer to recognize my conducting so I can get a more ""human"" tempo track for use in a DAW.
I do software development for a living, but I haven't cut my teeth on machine learning, so I don't even know where to begin. What machine learning algorithm would you recommend I use to tackle this problem?
",['machine-learning'],
Found input variables with inconsistent numbers of samples,"
I have an issue. the model gave me an error of Found input variables with inconsistent numbers of samples: But I don't understand why
train_input.shape, train_output.shape # output:: ((512, 4), (512, 2))
batch_size = 4
no_epochs = 5
verbosity = 1
num_folds = 2
X_train, X_test, y_train, y_test = train_test_split(train_input, train_output, test_size=0.22, random_state=5)
inputs = np.concatenate((X_train, y_train), axis=1)
targets = np.concatenate((X_test, y_test), axis=1)
X_train.shape , y_train.shape # output:: ((399, 4), (399, 2))
y_train.shape , y_test.shape  # output:: ((399, 2), (113, 2))
# Define the K-fold Cross Validator
kfold = KFold(n_splits = num_folds, shuffle = True)
# K-fold Cross Validation model evaluation
fold_no = 1
inputs.shape , targets.shape # output:: ((399, 6), (113, 6))

after this when I'm trying to make a loop it through that error on the below line. I'm using ResNet
for train, test in kfold.split(inputs, targets):

","['deep-learning', 'python', 'keras', 'residual-networks']",
Why does ChatGPT call itself Assistant when asked?,"
ChatGPT tells me it isn't called chatgpt?! Instead it's insisting that it's ""Assistant"".
Did OpenAI have different thoughts or what's the problem?



",['chatgpt'],
"How to understand the definition of $\lambda_i$ used in the return estimator proposed in the paper ""Human-level Atari 200x faster""?","
I'm reading article called ""Human-level Atari 200x faster""

",,
"How to relate the definition for entailment, with soundness and completeness?","
Is it fair enough to say for a language model, φ, which makes certain variable A true, and if φ also makes another variable B true, then we can conclude:

A ⊨ B

And for a certain inference calculus c, from which B can derive A,

A ⊢ cB

The logic is sound if A ⊢ cB is true, which also makes A ⊨ B true.
The logic is complete if A ⊨ B is true, which also makes A ⊢ cB true.
","['definitions', 'logic', 'knowledge-representation', 'norvig-russell', 'symbolic-ai']",
Which calculation to use for GRU,"
Im doing trying to implement GRU in my own Neural Network Library but when I did some research i stumbled on some inconsistencies.
When calculating a cell there are as many legitimate resources which state that $\mathbf{h}_t = \mathbf{h}_{t-1}\mathbf{z}_t+\mathbf{r}_t(1-\mathbf{z}_t)$ (e.g. https://arxiv.org/pdf/1803.01686.pdf, https://d2l.ai/chapter_recurrent-modern/gru.html) as there are which state $\mathbf{h}_t = \mathbf{h}_{t-1}(1 - \mathbf{z}_t)+\mathbf{r}_t\mathbf{z}_t$. (e.g. https://arxiv.org/pdf/2107.02248.pdf, http://dprogrammer.org/rnn-lstm-gru) When looking at diagrams there are also inconsistencies between the formulas and the diagrams themself. Also the diagrams differ.
Can someone tell me which one it is and why?
","['machine-learning', 'recurrent-neural-networks', 'gated-recurrent-unit']",
How do transformers handle multidimensional input?,"
Transformers work with lists of vectors, i.e. sentence of length SEQ_LEN, with each word having size EMBEDDING_DIM. Now, since the model still makes use of Dense layers internally, i.e. as in https://www.tensorflow.org/text/tutorials/transformer, I'm having trouble understanding how this 2D input is passed through the Dense layer, as 2D data is usually flattened before entering a Dense layer, i.e. in the case of an image?
Actually, in the general case - let's say I have sentences, with an embedding vector for each word in each sentence, how would I pass this into any layer, whether it be Dense / RNN / LSTM / etc ?
","['tensorflow', 'transformer', 'word-embedding', 'embeddings', 'dimensionality']",
"For specific tasks, is it better to fine-tune models on examples or just use prompting with the context of the task?","
These days large language models cover a vast amount of topics and information, but I wanted to understand: For specific tasks, is it better to fine-tune models on examples or just use prompting with the context of the task?
For example, if I wanted to train a language model to do question answering for linear algebra, is it better to train it with examples of linear algebra problems and their solutions, or try out different prompts?
","['natural-language-processing', 'comparison', 'training', 'fine-tuning', 'language-model']",
Should I use multi-armed-bandits or RL for a financial time-series problem?,"
If we take simple financial timeseries data(stock/commodity/currency prices), State(t+1) does not depend on the action that we choose to take at State(t) as in Maze or Chess problem.
Simple example: as states we can have the sum of the daily returns of 5 different ETFs. Based on that, we want to take action - either buy(go long) or sell(go short) in another ETF. No matter what we choose however, our action will not determine what the next state would be (we do not have any control of what the returns of those 5 ETFs will be tomorrow).
In that case of simple financial time series data, would multi-armed-bandit approach be more suitable?
","['reinforcement-learning', 'comparison', 'time-series', 'multi-armed-bandits']","
The question is whether the rewards are i.i.d. In such time-series problems, the rewards are not i.i.d; the reward at timestep t+1 depends on the reward at timestep t. Therefore, even though the actions/states may be somewhat independent, the rewards should i.i.d in order to apply MA bandits. Otherwise, the problem is about RL.
"
ML Network Traffic Classification Problem,"
I am hoping for suggestions or advice as to whether ML offers a suitable solution to the below problem. I am not so familiar with ML techniques so apologies if this is a straight forward question.
I have a capture of network traffic which represents API calls to a host in a network. Some of these requests serve as a ""trigger"" for the host to make outgoing API calls to another host, I also have packet captures for this. I am trying to detect and classify the triggers based purely on the packet size (this is possible because the finite set of API calls used have unique packet sizes). Assuming that once an outgoing API call is detected from the host I want to analyse the previously received packets in order to identify the ""trigger packet"" and classify it as the trigger based on the packet size.
Any ideas what sort of algorithm might be suitable for this problem? In general is machine learning suitable for this problem? The purpose would be that as more APIs are introduced it is still possible to classify triggers based on unique packet size of received packets by the host and therefore predict when an outgoing API call should be made.
Thanks
","['machine-learning', 'classification', 'anomaly-detection', 'unlabeled-datasets']",
How can imitation learning data be collected?,"
How can imitation learning data be collected? Can I use a neural network for that? It might be noisy. Should I use manual gathering?
","['reinforcement-learning', 'deep-learning', 'datasets', 'imitation-learning', 'data-collection']","
Imitation learning data usually means data gathered from an expert, that is data from an agent proficient in the task.
The agent may be:

A human operator: have the operator complete the task and record inputs and action taken.
A pre-trained reinforcement learning agent. Same as above.

The collected data is exactly the same for both cases.
The hard part is usually building an interface to collect this data, which is very task specific. For instance imitation learning for robotics may require expensive sensors. For cartpole and similar RL environments have a look at https://github.com/HumanCompatibleAI/imitation, they have scripts to learn from RL agents.
"
Is it possible to build a facial recognition system using a multi-layered DNN?,"
I am completely new to AI. As the title says, is it possible to build a facial recognition system using a multi-layered DNN? Would this be effective/practical?
I want to implement such a neural network in Python, without the use of any external libraries. I can predict that the operations would be too slow, so using numpy would be a must.
","['neural-networks', 'python', 'face-recognition']",
How to tokenize compound sentences based on the conjunctions?,"
I am trying to tokenize sentences of a document for aspect-based sentiment analysis. There are some sentences that consist of more than one topic. For example,

The touch screen is good but the battery is weak

or

Their smartphones are great and their TVs are perfect.

I want to tokenize sentences based on these ""conjunctions"". To be more specific, for example, in the first case, I want to get these tokens:

[The touch screen is good, the battery is weak]

Existing tokenizers split documents based on punctuation. Even if I spit based on words such as but, and, and so on, it is possible to get splits that are words. For example

the phone is handy and cool

will be

[the phone is handy, cool].

but instead it should remain the same, that is

[the phone is handy and cool].

Is there any pre-trained model for this task? Are there any other solutions?
","['deep-learning', 'natural-language-processing', 'algorithm-request', 'pretrained-models', 'sentiment-analysis']",
Does lazy learning require train-test-validation split?,"
This is a follow-up question to another post on SE AI that asked to distinguish lazy and eager learning.  One answer said that lazy learners do not require training and do all of the computation during inference.
In my limited experience, it seems that some sort of validation (e.g. holdout validation, cross-validation) is considered standard practice, but the answer seems to imply otherwise for lazy learners.  Is it correct to say that since training is not required, the usual train-test split/validations are not necessary for lazy learning?
","['machine-learning', 'cross-validation']",
How can I use CNN to make a cumulative count of the number of occurrences of each of the different objects in all the images in the test set?,"
Let's say there are three images in the test set, the first with three triangles, the second with two triangles and two circles, the third with four circles and two squares, and the final tally is a total of 5 triangles, 6 circles, 2 squares and 0 pentagrams (if the ""pentagram "" is also included in the labels in the training set)
How should I design the layers of this neural network and do I need to use more than one sort of kernel and filter? How could I label the images in my training set (for instance, the training set I mentioned above)?
","['convolutional-neural-networks', 'computer-vision', 'image-recognition']","
Well, if you are using high-level libraries, you don't have to find filters by yourself. Kernel types might change throughout the process, if you are trying to find different patterns, there will be different type of filters too.
Here is a Keras functional api code that might help you in this context. Also notice that there is a stride of 2 which means your image would shrink into half by each Conv2D layers, this also helps your filters to find bigger patterns since they are covering more space on the image.
i = Input(shape=x_train[0].shape)
x = Conv2D(32, (3,3), strides=2, activation='relu')(i)
x = Conv2D(64, (3,3), strides=2, activation='relu')(x)
x = Conv2D(128, (3,3), strides=2, activation='relu')(x)
x = Flatten()(x)
x = Dropout(0.5)(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.2)(x)
x = Dense(4, activation='softmax')(x)
model = Model(i, x)

How you find the most fitting layers with filters is mostly intuition. These types of variables are called hyper-parameters. There are conventions about these too.
How you label your data is basically training your neural network with a vector of targets, which would correspond to your training data's indexes.
"
Multilabel text classification with highly imbalanced training data,"
I'm trying to train a multilabel text classification model using BERT. Each piece of text can belong to 0 or more of a total of 485 classes. My model consists of a dropout layer and a linear layer added on top of the pooled output from the bert-base-uncased model from Hugging Face. The loss function I'm using is the BCEWithLogitsLoss in PyTorch.
I have millions of labeled observations to train on. But the training data are highly unbalanced, with some labels appearing in less than 10 observations and others appearing in more than 100K observations! I'd like to get a ""good"" recall.
My first attempt at training without adjusting for data imbalance produced a micro recall rate of 70% (good enough) but a macro recall rate of 45% (not good enough). These numbers indicate that the model isn't performing well on underrepresented classes.
How can I effectively adjust for the data imbalance during training to improve the macro recall rate? I see we can provide label weights to BCEWithLogitsLoss loss function. But given the very high imbalance in my data leading to weights in the range of 1 to 1M, can I actually get the model to converge? My initial experiments show that a weighted loss function is going up and down during training.
Alternatively, is there a better approach than using BERT + dropout + linear layer for this type of task?
","['deep-learning', 'bert', 'loss', 'text-classification', 'multi-label-classification']","
The architecture selection is reasonable. BERT itself has plenty of parameters. There is no need to use anything more complex.

If the labels are mutually exclusive, you should use softmax + categorical cross-entropy instead of binary cross-entropy. In this case, you can also upsample the less frequent classes.
If the labels are not mutually exclusive, you can still try some upsampling. You can also assign a higher weight to some classes: set reduction='none' in the loss function, and it will get you the loss per instance and per class, and you can weight it as you want (but clever data upsampling will be better).
BERT can quickly overfit during fine-tuning, be careful with learning rates: you can use a higher learning rate for the classifier than for the BERT layers. You can also fine-tune only the last few layers of BERT.
There are better models than BERT, e.g., RoBERTa.

"
Is AlphaZero's output (action probabilities) vector suboptimal?,"
The AlphaZero research team states

A move in chess may be described in two parts: selecting the piece to move, and then
selecting among the legal moves for that piece. We represent the policy π(a|s) by a 8 × 8 × 73
stack of planes encoding a probability distribution over 4,672 possible moves. Each of the 8×8
positions identifies the square from which to “pick up” a piece.

I am wondering if it would be better to only have the network output moves that could exist (i.e., ""a1b3"" is a possible knight move, but ""a1g3"" could never be reached by a piece).
The modified output would be much smaller, and could potentially make the neural network learn quicker, right?
",['alphazero'],
How can I get Q-Learning (1 step off policy) update from n-step off policy learning update?,"
In Sutton and Barto we have expressions for Q-Learning and n-step Off policy learning. The former ought to be the 1-step limit of the latter but I cannot see it working out that way. What am I missing?
Concretely, in Sutton and Barto (edition 2) the expression for n-step off policy learning is given in equation 7.11 as
$$
Q_{t+n}(S_t,A_t) = Q_{t+n-1}(S_t,A_t) + \alpha \rho_{t+1:t+n} [G_{t:t+n}-Q_{t+n-1}(S_t,A_t)]
$$
where
$$
G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n Q_{t+n-1}(S_{t+n},A_{t+n}) \\
\rho_{t:h} = \prod_{k=t}^{min(h,T-1)} \frac{\pi(A_k|S_k)}{b(A_k|S_k)}
$$
Here $\pi$ is the target policy and $b$ is the policy being followed.
If I take n=1 above I get
$$
Q_{t+1}(S_t,A_t) = Q_{t}(S_t,A_t) + \alpha \frac{\pi(A_{t+1}|S_{t+1})}{b(A_{t+1}|S_{t+1})} [R_{t+1} + \gamma Q_t(S_{t+1},A_{t+1})-Q_{t}(S_t,A_t)]
$$
If the target policy is the greedy policy then we will further get
$$
Q_{t+1}(S_t,A_t) = Q_{t}(S_t,A_t) + \alpha \frac{1}{b(a|S_{t+1})} [R_{t+1} + \gamma Q_t(S_{t+1},a)-Q_{t}(S_t,A_t)]
$$
where $a = \text{argmax}_{a'} Q_t(S_{t+1},a')$
Nevertheless, the Q-Learning update given in equation 6.8 is
$$
Q_{t+1}(S_t,A_t) = Q_{t}(S_t,A_t) + \alpha  [R_{t+1} +  \gamma \max_a Q_t(S_{t+1},a)-Q_{t}(S_t,A_t)]
$$
How do I reconcile these two expressions? Indeed, looking at David Silver's lecture notes it even seems that the n=1 limit of the n-step off policy expression doesn't match that either.
","['reinforcement-learning', 'q-learning', 'temporal-difference-methods']",
Is it possible to combine DDPM with GAN?,"
From what I understand in GAN, the main idea is that you have a generator and a discriminator network that are ""competing"" with each other. The generator trying to make images that the discriminator is not able to distinguish from real images. Since recently diffusion models started to become more popular. I haven't seen many papers showing the DDPM being the generator in a GAN system. Is there a reason for this, since from what I understand pretty much any model could be the generator?
","['generative-adversarial-networks', 'generative-model', 'diffusion-models']",
Should I pre-compute embeddings from a frozen network or are the gradients important?,"
I'm training an AutoEncoder-like network to take a face embedding, encode it, decode it, and then I calculate the loss between the input and output embedding. The input embedding is calculated by passing an image through a pre-trained frozen network. Is there a difference between pre-computing the embeddings from the frozen network before training and computing them during training to have gradients?
",['autoencoders'],
Should we always use the usual no leakage train-val-test splt in time series?,"
Some of you may be familiar with the unusual split scheme used for time-series data. In short, there is a saying that one should only consider a split where the training set comes prior to the testing set (in terms of index or timedate), as otherwise we essentially use future data to infer.
Namely, given the dataset $\mathcal{D}=\{(x_1,y_1),...,(x_n,y_n)\}$, a viable split may look like
\begin{align}
 train=\{(x_1,y_1),...,(x_j,y_j)\}\subseteq\mathcal{D}\\
 test=\{(x_{j+1},y_{j+1}),...,(x_k,y_k)\}\subseteq\mathcal{D}
\end{align}
for some $j\leq k\leq n$
My question is - are there some cases where random splitting is O.K in time series? Also, what is the main problem with random sampling?
","['time-series', 'k-fold-cv']","
Most time series contain time-dependent information. That means that the time series has order. The temporal information, therefore, cannot be randomly sampled without losing some pertinent information.
By splitting a time series, we retain the temporal information to generalize a model, such as a classifier or a regressor, while setting aside a smaller time series to test. A simple split, such as 80%/20% can be done or something fancier such as K-Fold validation can be done.
The hope is that the information in the training dataset persists and appears in the testing dataset. It is possible that an event changes the time series enough, and coincidentally at the split time, such that the testing dataset is of no value. If that happens, you have little option but to wait until there is enough information to justify a new split while trimming off the data before the event.
The following paper reviews the many strategies to splitting time series data:

Cerqueira, V., Torgo, L., & Mozetič, I. (2020). Evaluating time series forecasting models: an empirical study on performance estimation methods. Machine Learning, 109(11), 1997–2028. https://doi.org/10.1007/s10994-020-05910-7

The paper is also available as a PDF here.
"
Why don't SAC implementations use state value function?,"
When I read SAC paper, they use state value function $V_\psi(s_t)$ and as do their implementation. But in other SAC implementations like stable-baseline3, pytorch-soft-actor-critic, it seems that they don't use seperate state value function. So, I wondered they are improved version than the original paper?
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'soft-actor-critic']",
Explanation of Cross-Modality Linear Transformer,"
So I am trying to understand how a Cross-Modality Linear Transformer is different from an a basic transformer. I found the transformer mentioned in this paper. Am I correct in understanding that, the transformer  is cross-modality because it is going from one medium to another. In this case, text to motion. Then linear because the time complexity to calculate the attetion weights are linear? I have troubles with understanding transformers. I just was able to understand the basic concept of them. So any help would be great. Thanks
",['transformer'],
Which pretrained model would best suite for the probelm of generating questions (MCQ) from text?,"
Here is my use case, I give my model the following text
I brought a car because it is cheap

output is
I brought ____ car because it is cheap 
1. a
2. an
3.the

ChatGPT and GPT 3 can certainly do that if used wisely, but I want it locally on my system.
Now I tired using BERT and Parrot, but it comes with many problems.
Should I fine tune BERT or any such model to my use case and if so, which model would be the best.
Or even better is there any pretrained model doing exactly the same which I missed looking for
Thanks
","['natural-language-processing', 'transformer', 'pretrained-models', 'chatgpt']",
Why does ChatGPT create fake code?,"
ChatGPT has been a big thing lately. It also makes a lot of mistakes. For example, it creates fake functions of a package and tells it as it works for real. I was wondering how that works. Why is it creating fake functions of code and not just telling that it doesn't know how to solve the question that was asked?

Example taken from Stack Overflow.
Question
Link: https://stackoverflow.com/q/74662093/
Title:

App script accessing secret manager to retrieve api token

Body:

I am trying to access secret manager from a small script I built on app script. I need to access and API with with a key but do not want to hardcode it.
I have already assigned the app to a project. I am just not sure how to query the token.

Answer (deleted)
Link: https://stackoverflow.com/a/74682465/1595451
Body

To access the Secret Manager from Google App Script, you can use the
SecretManager.getSecret(secret) method, which takes a secret parameter specifying the name of the secret you want to retrieve. For
example:
 // Get the secret named ""my-api-token""
 var secret = SecretManager.getSecret('my-api-token');
 
 // Use the secret value in your script
 var apiToken = secret.getValue();

Note that this method will throw an error if the specified secret does
not exist or if the user does not have permission to access it. You
can use a try-catch block to handle these cases gracefully. For
example:
 try {
   // Get the secret named ""my-api-token""
   var secret = SecretManager.getSecret('my-api-token');
 
   // Use the secret value in your script
   var apiToken = secret.getValue();
 } catch (e) {
   Logger.log('Error getting secret: ' + e.message);
 }

Also note that the getValue() method returns the secret value as a
string, so you may need to convert it to the appropriate data type
(such as a number or boolean) depending on your use case.
For more information, see the Secret Manager documentation and
the SecretManager class reference.

SecretManager.getSecret(secret) method doesn't exist, and obviously the links returns page not found.
","['machine-learning', 'natural-language-processing', 'open-ai', 'chat-bots', 'chatgpt']",
Is VAE the same as the E-step of the EM algorithm?,"
EM(Expectation Maximum)
Target: maximize $p_\theta(x)$
$  p_\theta(x)=\frac{p_\theta(x, z)}{p_\theta(z \mid x)} \\\\$
Take log on both sides:
$ \log p_\theta(x)=\log p_\theta(x, z)-\log p_\theta(z \mid x) \\\\$
Introduce distribution  $q_\phi(z)$:
$ \log p_\theta(x)=\log \frac{p_\theta(x, z)}{q_\phi(z)}-\log \frac{p_\theta(z \mid x)}{q_\phi(x)} \\
\int_z q_\phi(x) \log p_\theta(x) d z=\int_z q_\phi(z) \log \frac{p_\theta(x, z}{q_\phi(z)} d z-\int_z q_\phi(z) \log \frac{p_\theta(z \mid x)}{q_\phi(x)} d z \\
 \log p_\theta(x)=\underbrace{\int q_\phi(z) \log \frac{p_\theta(x, z)}{q_\phi(z)} d z}_{E L B O}+\underbrace{\int_z q_\phi(z) \log \frac{q_\phi(z)}{p_\theta(z \mid x)} d z}_{K L\left(q_\phi(z) \| p_\theta(z \mid x)\right)} \\ $
Our target is to maximize $log\ p_{\theta}(x)$, and the idea of EM is to let KL=0  $ \Rightarrow $ $log\ p_{\theta}(x)=ELBO$, and then maximize  $log\ p_{\theta}(x)$  by maximizing ELBO.

Do an algebraic variation on ELBO:

​      $ELBO=\int q_\phi(z) \log \frac{p_\theta(x, z)}{q_\phi(z)} d z=\int q_\phi(z) \log p_\theta(x, z)d z-\int q_\phi(z) \log {q_\phi(z)} d z$
${q_\phi(z)}$ is calculated in E-step (${q_\phi(z)}=p_{\theta^{(t)}}(z|x)$) , so when we are maximizing $ELBO$, we can treat$ \int q_\phi(z) \log {q_\phi(z)} d z$ as a constant.
So in order to maximize $ELBO$, we just need to maximize $\int q_\phi(z) \log p_\theta(x, z)d z=E_{q_\phi(z)}[\log p_\theta(x, z)]$
VAE(Variational autoEncoder)
The above EM algorithm has a important premise that $p_{\theta^{(t)}}(z|x)$ is tractable, so we can just make ${q_\phi(z)}=p_{\theta^{(t)}}(z|x)$ to make $KL$ = 0. But if  $p_{\theta^{(t)}}(z|x)$  is intractable, we have to use Variational Inference to make ${q_\phi(z)}$ approximate $p_{\theta^{(t)}}(z|x)$.
VAE minimizes ${K L\left(q_\phi(z) \| p_\theta(z \mid x)\right)} $ by maximizing $ELBO$. Do an algebraic variation on ELBO:
$-ELBO=\int q_\phi(z) \log p_\theta(z|x)d z+\underbrace{\int q_\phi(z) \log \frac{q_\phi(z)}{p_\theta(z)} d z}_{K L\left(q_\phi(z) \| p_\theta(z)\right)}$
Then we can use the gradient descent method for optimization.
My question is: The target of VAE is to maximise $ELBO$ and thus minimise ${K L\left(q_\phi(z) \| p_\theta(z \mid x)\right)} $, which is the same as the E-step of the EM algorithm. However, the sum of ELBO and KL remains the same, and it can not to increase $ logp(x).$ So VAE is just the E-step in the EM algorithm? How does it manage to fit the distribution of $x$ (increase $ logp(x)$)?
","['machine-learning', 'variational-autoencoder', 'evidence-lower-bound', 'maximum-likelihood', 'expectation-maximization']",
How many opposing users should be recorded exterior to the average data before being combined?,"
Outside of the programming toward AI, I am having difficulty putting together a plan on how this machine I hope to build would work.
The basic question is: How should it handle user reviews / interactions that are suddenly quite different from the average user?
I can think of two possible solutions, though they are both questionable.

Log the unexpected review or interaction separately from the official mean of similar scores so it can be recorded for a later date when a certain number of users also give similar scores. For example, the average score for rating is 7.8/10, but one user gives it a 2/10. The average score will continue to stay at 7.8/10 until 499 other users also give it a score close to 2/10. Once it reaches 500, it will no longer be questioned as a joke and will then affect the average score by lowering 7.8/10. However, the number of users being a random constant is very questionable.
Log the unexpected review or interaction separately from the official mean of similar scores so it can be recorded for a later date when a certain percentage of the total number of users also gives similar scores. For example, the average score for rating is 7.8/10, but one user gives it a 2/10. The average score will continue to stay at 7.8/10 until 12% of all users also start giving it a score close to 2/10. Once it reaches 12%, it will no longer be questioned as a joke and will then affect the average score by lowering 7.8/10. However, the certain percentage being random is very questionable as well as the possibility of the sudden rise in the opposing rates being done by a certain person/group of people doing so with the purpose of hurting the AI.

","['machine-learning', 'training', 'probability-distribution', 'anomaly-detection']",
What are the inputs of a neural network when learning a difference equation?,"
The time series y[n] is the solution of the difference equation
                 y[n+2] - 2*y[n+1] + y[n] + w[n]^2*y[n]  = 0  (1)
                 Y[n] = cumsum(y[n])                          (2) #cumulative sum

Where  w[n] is a time series parameter for the difference equation and  Z[n] a known time series.
i want to build a neural network with w[n] as the output with the loss function
                                       L = L(Z[n], Y[n])

What are the inputs of the neural network?
","['neural-networks', 'machine-learning', 'deep-learning', 'deep-neural-networks', 'time-series']",
How to optimize ELBO(VAE's loss function)?,"
Suppose we've got the following formula:
$\log p(x;\theta)=\mathbb{E}_{q(z|x;\phi)}[\log p(x,z;\theta)-\log q(z|x;\phi)]+KL(q(z|x;\phi)||p(z|x;\theta))\\ \geq \mathbb{E}_{q(z|x;\phi)}[\log p(x,z;\theta)-\log q(z|x;\phi)]$
We call this Evidence Lower Bound (ELBO):
$\mathrm{ELBO}=\mathbb{E}_{q(z|x;\phi)}[\log p(x,z;\theta)-\log q(z|x;\phi)]$
We use the method of maximizing ELBO to optimize the parameters $\theta$ and $\phi$:
$\max_{\theta,\phi}\mathbb{E}_{\mathcal{D}}\mathbb{E_{q(z|x;\phi)}}[\log p(x,z;\theta)-\log q(z|x;\phi)]$
We can intuitively understand that: for given $\theta$, the upper bound of ELBO is $\log p(x;\theta)$, so optimizing $\phi$ allows ELBO to be as close as $\log p(x;\theta)$ possible; Then, optimizing $\theta$ can make the data likelihood as large as possible, which achieves the goal of MLE. (just like what shown in Fig)

The ELBO mentioned above is also the loss function of VAE. (An algebraic transformation of the ELBO mentioned above: $\mathrm{ELBO}=\mathbb{E}_{q(z|x;\phi)}[\log p(x|z;\theta)]-KL(q(z|x;\phi)||p(z;\theta))\\\\$)
My question is:
1.When maximizing ELBO, how can I fix $\theta$ and then $\phi$, as mentioned above?
2.VAE optimizes them at the same time, there is no precedence. How could I understand the difference?
","['machine-learning', 'variational-autoencoder', 'cross-entropy', 'kl-divergence', 'evidence-lower-bound']",
Fast feature elimination in python on GPU,"
I have a dataset with millions of samples and tens of thousands of features. Most of these features are highly correlated with each other. However, eliminating features based on a correlation threshold in pandas takes a long time (even if I subsample rows). So, I'm wondering if there is a way to leverage my GPU to speed up feature elimination. Here're some initial thoughts I had:

I believe Rapids AI's cudf has a function to calculate correlation, but I'm not sure how to get the highly correlated columns from a correlation matrix in a vectorized way via cudf / GPU.


Another approach I'm considering is using the GPU to build logistic regression models for each feature against the target and then using log loss as a proxy for feature importance / feature selection. What I don't like about this approach is that it seems it wouldn't eliminate highly correlated features, i.e., it may leave me with a bunch of features that are informative but highly correlated with each other.

Don't use feature elimination at all, and use dimensionality reduction via PCA instead. What I don't like about that approach is I'm not sure how I would interpret feature importance later on. Is there a way to map back from PCA components to original features, e.g., if I later find that PCA component_x is predictive of my target, is there a way to interpret component_x in terms of the original features?


Any advice or guidance would be highly appreciated. Thank you!
","['classification', 'feature-selection', 'feature-engineering', 'gpu', 'principal-component-analysis']",
What is generalized data distribution iteration?,"
Anyone know this algorithm?
""Generalized Data Distribution Iteration"" (GDI)
GDI seems to have much better performance than MuZero.
https://paperswithcode.com/paper/generalized-data-distribution-iteration/review/?hl=56222
The GDI thesis is pretty arcane and I don't understand it at the moment.
Do you think the GDI paper is worth reading?
I'm assuming it's less credible.
Please let me know what you think.
","['reinforcement-learning', 'deep-rl']",
Are there are any truly chaotic systems in deep learning where convergence is not possible?,"
In the cases I have seen, neural networks have always converged (i.e., the loss function for the training data asymptotically approached some constant value).  I have seen loss functions oscillate, sometimes erratically, but they appear to converge nonetheless.
QUESTION:  Are there deep learning models for which convergence fails because an underlying chaotic phenomenon is being (unwittingly) modeled?
","['neural-networks', 'convergence']",
Is it a good idea to have a category and its subcategories in the training set of an object segmentation model?,"
I am currently training an object segmentation model (detectron2 : mask rcnn)
The objective is to detect materials like wood, plastic, glass etc...
wood is one of the categories in my training set.
Is it a good idea to add other labels that are subcategories of wood? Like table, desk etc ... to my training set? and if so, how should I label them?
Note: I tried adding table, I noticed that the model detects the table as an object and then adds some small detections of wood on top of it. so when labeling the images, should I label the table as table and then label all its parts as wood? or should I just label the whole thing as wood and avoid adding subcategories to my dataset?
","['deep-learning', 'image-segmentation', 'data-labelling', 'multi-label-classification', 'mask-rcnn']",
How was ChatGPT trained?,"
I know that large language models like GPT-3 are trained simply to continue pieces of text that have been scraped from the web. But how was ChatGPT trained, which, while also having a good understanding of language, is not directly a language model, but a chatbot? Do we know anything about that? I presume that a lot of conversations was needed in order to train it. Did they simply scrape those conversations from the web, and where did they find such conversations in that case?
","['natural-language-processing', 'chat-bots', 'training-datasets', 'language-model', 'chatgpt']",
Sutton & Barto: what are parametrized functions?,"
From ""Reinforcement Learning: An introduction (2nd ed.)"" by Richard S. Sutton and Andrew G. Barto, on page 59

Instead, the agent would have to maintain $v_\pi$ and $q_\pi$ as parameterized functions (with fewer parameters than states) and adjust the parameters to better match the observed returns.

What do they mean by ""parametrized functions"" here? The part of ""adjust parameters to better match observed returns"" also sounds odd to me.
","['reinforcement-learning', 'terminology', 'sutton-barto']",
Is there a way to determine if code has been autogenerated by an AI source?,"
Lately, our Junior developer has been producing code fixes very quickly. At first, I was excited for him as I thought he was starting to retain information and proving he was able to research and fix issues. But during our last code review, I was walking through his fixes and he couldn't speak to why he did was he did or what the code was actually doing. Obviously, OpenAI's Chat has taken off and I am just curious if there is a way to determine if code was generated by AI?
I did see that Stack Overflow has a policy on this Temporary policy: ChatGPT is banned so it seems that they have a way to tell if the code is authentically written or not?
Any guidance is appreciated.
","['open-ai', 'chatgpt']",
How to model a few shot image classification task similar to a traditional supervised one?,"
I want to train an image classification model. I have 10 classes with 15 images per class. Since the data is very less, I thought of modeling the problem as a few-shot image classification task and try experimenting with networks such as a ProtoNet.
The problem is that in few-shot learning, the dataset is split in a very different way compared to how we normally do in supervised learning. In few-shot learning, data is split based on classes such that at test time the model sees entirely new classes. But, for my use case, it is fine if the model has already seen all the classes at training time. I just require it to classify an unseen image at test time. In production, the model will receive one image at a time and it should classify it with low latency.
How can I split the data and what models can I try out so that we get good test-time accuracy? I am also open to trying out other paradigms aside from few-shot learning if there is some way to get good accuracy on this small dataset.
Additional Notes

Right now I have 10 classes, but this can increase in the future (can go up to 100 classes), but the number of images per class will still be less (around 10 to 15 images).
I am very new to few-shot learning, so please excuse me if I am misunderstanding something.
I have already checked out this answer.

","['image-recognition', 'few-shot-learning']",
How compressed is Chat GPT data?,"
I originally thought the Chat GPT was a neural network model in which the input data was highly compressed such that the input data could not be got out again.
But then I asked Chat GPT ""Write the whole text of Alice and Wonderland"".
And it was able to write the text word for word. 100% correctly. (For as long as I cared to read it for).
Therefor it seems like any paragraph of text it has read in its dataset more than once it seems to have stored it verbatim. (Perhaps like a human actor who can memorise a script).
If Chat GPT has enough weights in its neural network it could theoretically store every item of input text it has fed to it.
I wonder does the attention model have the happy accident that it is able to remember whole books of text?
I wonder how much of the input text could it remember verbatim. Or perhaps it will only remember famous works like Alice in Wonderland because they would be repeated many times in the input data.
","['chatgpt', 'memory']",
What does linear regime of nonlinearity mean in normalisation?,"
In section 3 paragraph 2 of Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift paper (https://arxiv.org/abs/1502.03167) they say that normalizing a layer's input may change what it represents, I understand this. But what do they mean in the bolded part?

For instance, normalizing the inputs of a sigmoid would constrain them
to the linear regime of the nonlinearity.

","['neural-networks', 'batch-normalization', 'normalisation']","
The normalization makes the signal small enough to remain in the region of the sigmoid that can be well approximated by a straight line.
The idea is the same as in electronics: https://en.wikipedia.org/wiki/Small-signal_model
"
Using GANs to generate data augmentations for YOLOv5,"
I was building a YOLOv5 object detection model, and was looking into researching synthetic methods like GANs to increase the size of my training set in an unsupervised manner.
I know that few-shot GANs can be used to ""hallucinate"" images and labels for a classification task, but how can they be extended to hallucinate images and labels in YOLO format (basically lists out each bounding box and class)?
Is there some way that I can train a GAN on images / YOLO labels, and get it to hallucinate more images / labels?
","['neural-networks', 'convolutional-neural-networks', 'generative-adversarial-networks', 'yolo', 'few-shot-learning']",
Machine Learning for raw measurement data,"
i have raw measurement data of different events.
My first approach was to calculate features of those events, do scaling, PCA and feature selection and then feed those features to different machine learning algorithms (ANN, RF, DT,...).  This works alright and up to now, to me this seems to be the only and correct way to go.
Now i am wondering - are there machine learning approaches and techniques out there, which would let me feed the raw measurement data directly into the algorithm (so i can skip the whole feature engineering part)? One event is around 2000 measurement samples long.
I could for example set up an ANN with 2000 input nodes and see where its going (just my naive first idea). Are there any hidden disadvantages to this - why is or isnt this a good idea?
Up to now i couldnt find anything about raw measurement inputting - if you could post maybe just the names of the algorithms i should check out, it would be of great help.
Thanks, regards and happy holidays
",['machine-learning'],
Can any GAN's utilize labels in their datasets while they are training?,"
It seems to me that the Generative Adversarial Networks have a practical issue when trying to reproduce some of their output images
For example, as you can see https://www.youtube.com/watch?v=oIzwe_MOeQI&t=1057s seems to generate shape and background changes as it changes rotations.
Can any of the StyleGANs be convinced to take an image of a car, along with some sort of key that says how it is rotated then consider that when the trained model is generating?
Obviously, it might be a substantial effort to label the data, but I'm curious if it's been done in a well-published fashion.
","['generative-adversarial-networks', 'style-transfer']","
The idea of taking an additional label as input sounds like a conditional generative adversarial network (cGAN). Within a conditional GAN both the discriminator and the generator are conditioned on extra information provided through a label. The idea was first introduced in this paper.
This image from this paper nicely highlights how a conditional GAN differs from a standard GAN:

Such an architecture would likely also be suitable to be conditioned on image rotations.
"
Tokenization for treelike structures,"
I'm pretraining a BERT (bigbird) model to use with SMILES encoding of chemicals. This kind of data is a treelike structure in the form of a string with a single bracket type. Usually this tree isn't very deep and doesn't have a branching factor larger than the low teens.
I'm looking for studies or recommended methods of tokenizing this type of data for use with transformers.
So far I see that one of the approaches directly from the field is a wordpiece variant proposed by Schwaller et. al.. Are there any other approaches that have found success with this type of data?
","['reference-request', 'transformer', 'attention', 'bert']",
Could one still learn a good policy by doing a backprop every fixed number of steps within an episode?,"
Waiting an entire episode before doing a backprop can build up a very large computational graph, which is a burden on memory. Could one still learn a good policy by doing a backprop every fixed number of steps within an episode? E.g. backprop every 500 steps. Is there any literature about this that I can check out?
","['reinforcement-learning', 'reference-request', 'policy-gradients']","
In One-Step Actor-Critic, the update rule is performed after every step using the TD error. might want to check that out
"
Can object detection algorithms distinguish a same object as different classes only based on different surroundings?,"
I am doing an experiment. The following image is an example of the annotation I do. There are 2 classes: 1) sun, 2) moon. Red boundary box labels the moon, and the green boundary box labels the sun. I would like the model to learn that: ""if the background is dark blue, it is the moon. If it is light blue, it is the sun""
I intentionally make the boundary box exclude the surrounding (the blue background), so to test whether an algorithm can distinguish the same object as different classes only based on different surroundings.
This would be useful, for example, to detect a toy car vs a real car. Assuming the toy car and real car looks very similar, the object detection algorithm have to be aware of its surrounding.
Do you think popular algorithm such as FRCNN can achieve that? If not, what algorithm is available to solve this problem?

",['object-detection'],"
Sure, this can happen. If you have toy cars that in your training data were always photographed in toy shops and then during test you provide images of toy cars in a different environment, you may have situations where they will be classified incorrectly. As long as the neural network sees the context around the object during training, it will also capture information about the context along with information about the object. And if the conext is biased towards a specific environment (i.e. a toy shop), that will also affect the decision significantly.
"
How are NNs output setup for games that allow multiple actions each turn and have very large sets of possible actions?,"
I was looking at an AI coding challenge for a two player game on a 2D grid of variable size (from one game to the next).
Here is a screen shot example of the playfield.

Each player has multiple units on the board. In fact, each tile can hold multiple units and you can move all or a part of those units. Each turn, each player may perform several actions at a time.
You feed your actions to the game engine on one line, separated by a ;

MOVE amount fromX fromY toX toY.
BUILD x y.
SPAWN amount x y.
WAIT.

Example of possible command sent to the game engine on one turn:
MOVE 2 2 3 3 3; SPAWN 1 6 6; BUILD 1 1; MOVE 1 9 8 9 9; MOVE 3 11 2 12 2

And the very next turn your command might be:
WAIT

And the turn after that
SPAWN 1 6 6; SPAWN 2 3 3

You get the idea. Each turn you can play a variable amount of ""moves"" or ""actions"". And, on bigger boards, the number of valid possible actions can be very big.
I was wondering how would one go about dealing with games like these when trying to use a NN to predict the best move(s) to play on any given turn.
I know how I would handle the variable map size in the input, I'd probably just use the biggest possible map size and then pad the input for smaller map sizes. What I'm really scratching my head about is the output.
How would one setup the output layer in order for the NN to output the best set of actions to play on a given turn?
If we structured the output layer to account for each possible actions, whether they are legal or not on the current turn, the layer would be positively huge, wouldn't it? Number of tiles x number of neighbors, and that's just for moves, add to that spawning and building. Oh, and that doesn't even account for the fact that you can move or spawn more that one unit on a tile. How would you even structure that in your output?
I did see this unanswered question Designing Policy-Network for Deep-RL with Large, Variable Action Space which I think might be similar to what I'm asking but I'm not 100% sure as it is using some terms I'm unfamiliar with.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'dqn', 'muzero']","
I participated in this contest and ended up 30th (out of ~4500). I used mainly neural network. I split the problem into parts:

for each my own cell, I ask the NN what to do (build, spawn, move)
inputs for the NN was mainly the 5x5 square vision around the cell
there were 2 networks:

first network with 2 outputs - if first output is > 0 then build. if second output > 0, then spawn
if the above not occurred, second network was responsible for move, 5 outputs (WAIT, N, S, E, W), highest value wins


for multiple units per cell, I asked the NN several times, each time with the updated inputs as if the move happened

Training was done via neuroevolution, particularly evolution strategies. Personally I don't know (yet) how to train this type of NN by more conventional means.
More details in: https://www.codingame.com/forum/t/fall-challenge-2022-feedbacks-strategies/199055/4
"
How do Deep Momentum Networks work?,"
Here is a paper about Deep Momentum Networks: https://arxiv.org/pdf/1904.04912.pdf
From what I understand, they are a neural network that's used for stock trading, that uses the Sharpe Ratio as a loss function. I am trying to recreate this myself but I am confused about one thing: the Sharpe Ratio needs inputs over time to be calculated (i.e. it needs returns between date 1 and date 2) but the network, if being used to predict how much to invest at a given time (which seems to be what they're doing) only requires one output.
How can you calculate a loss function that requires multiple inputs, using only one output? Are they forward propagating multiple times before backpropagating?
Maybe it isn't mentioned in the paper, I certainly couldn't find it, but I would appreciate any clarification on the topic. Thanks in advance.
","['deep-learning', 'objective-functions', 'academia', 'algorithmic-trading']",
Doesn't every single machine learning classifier use conditional probability/Bayes in its underlying assumptions?,"
I'm reading about how Conditional Probability/ Bayes Theorem is used in Naive Bayes in Intro to Statistical Learning, but it seems like it isn't that ""groundbreaking"" as it is described?
If I'm not mistaken doesn't every single ML classifier use conditional probability/Bayes in its underlying assumptions, not just Naive Bayes? We are always trying to find the most likely class/label, given a set of features. And we can only deduce that using Bayes rule since we are (usually) solving for P(class|features) with P(features|class)?
","['machine-learning', 'classification', 'naive-bayes', 'bayes-theorem', 'conditional-probability']","
Conditional probability and Bayes rule are related but they are not the same thing, you can predict conditional probabilities without using Bayes rule.
So no, not all machine learning classifiers use Bayes rule, standard neural networks do not use Bayes rule at all, SVMs and linear classifiers neither.
A better counterexample is Bayesian Neural Networks, which have a probability distribution over the weights, and Bayes rule is used during learning and inference, these are not the same as standard neural networks.
As reference for this statement, I leave the following quote from Section 3.1 of the the paper Uncertainty Quantification for Deep Neural Networks: An Empirical Comparison and Usage Guidelines:

BNNs are neural networks with probabilistic weights, instead of scalar
weights as in PPNN, and are represented as probability density
functions. To train a BNN, first, a prior distribution p(θ) over
weights θ has to be defined. Then, given some data D, the posterior
distribution p(θ|D), i.e., the trained BNN is inferred using Bayes
rule:

"
How does residual vector quantization in ColBERT v2 work?,"
I want some clarification on how residual vector quantization works in ColBERT v2.
The model independently encodes document terms offline using a BERT-like model. These embeddings are further reduced to a lower dimension (128-d) using a linear projection layer. These 128-d term vectors are first coarse-quantized using k-means clustering. For each term vector, we compute a residual as the difference between the vector and cluster centroid.
Each of these residuals is further quantized using b-bits per dimension (b=1 or 2). How does this b-bit quantization work exactly? I am familiar with product quantization, where each vector is divided into sub-vectors. These sub-vectors are then quantized using k-means; however, how does b-bit per dimension quantization work? Is it the same as PQ, where the sub-vector is an individual bit?
Thanks!
","['natural-language-processing', 'data-compression']",
How can I put more weight specific words when doing text classification?,"

The picture shows the model I want to implement. As you can see, the input will be a single sentence (e.g. I want to massacre those ugly people!). Then, the teacher model will output weights(which will be 0~1 float values) to more important words to classify (e.g. The model will give more weights to words like massacre and ugly for classifying the sentence to be hate-speech). Then, the student model takes those weights from the teacher and the same sentence, and the weights will be a hint for student to know which words it should focus and classifies the sentence.
What I want to know is that how can I make student model to focus several words than others? Just multiplying weights to the word embeddings naively doesn't seem a good solution I think. Please give me some insight. Thank you.
",['natural-language-processing'],
Can the truth value of unobserved sentences be computed from known sentences only?,"
Truthfulness is a quality that Sam Altman mentioned to be improvable on ChatGPT. This question considers a small example to try to understand the problem.
A large language model includes a large set of true sentences, as well as a mechanism to obtain new sentences from known sentences. For convenience, let us map this to a simpler problem with a similar or equivalent form.
A dialog system includes a set of valid words (lexicon), and a mechanism to obtain new words from known words (composition)
For instance, a training set (or knowledge base), may look like this.
KNOW is in the dictionary
UNKNOW is in the dictionary
KNOWN is in the dictionary
UNKNOWN is in the dictionary
Now, a system capable of subword composing from these sentences is prompted:  ""KNOWLEDGEABLE is in the dictionary"". It will likely continue with ""UNKNOWLEDGEABLE is in the dictionary"", which happens to be true by chance.
However, when prompted ""KNOWLEDGE is in the dictionary"", the likely continuation is ""UNKNOWLEDGE is in the dictionary"", which is a well-formed and likely sentence, but false for some reason.
The truth value of these two continuations cannot be derived from the four sentences in the training set, it would take an actual dictionary look-up to check.  In other types of sentences, such as those true by definition (+5 is a positive number, -5 is a negative number), it can actually be computed, as the validity of the sentence comes from the symbolic structure of the subject. It is true that -(digits) is a negative number, while it is only possible that UN(word_in_lexicon) is a word in the lexicon.
Therefore, better continuations for both prompts would have been ""Possibly UN(X) is in the dictionary"", but this is impossible to get from the training set, unless we preappended ""Possibly"" to the second and fourth training sentences.
Is this impossibility to check for the validity of a new sentence simply a consequence of first Gödel's incompleteness theorem?
","['logic', 'chatgpt']",
Which correlated feature should be eliminated from a model?,"
BACKGROUND: There is a lot of information online about the problem of multicollinearity as it relates to machine learning and how to identify correlated features.  However, I am still unclear on which variables to eliminate once a correlated subset of feature variables has been identified.
QUESTION:  Once a correlated set of 2 or more feature variables has been identified by some method, how does one decide which one to retain?
","['machine-learning', 'feature-selection']","
In practice multicollinearity could be very common if your features really act as correlated causes for your target. If multicollinearity is moderate or you're only interested in using your trained ML model to predict out of sample data with some reasonable goodness-of-fit stats and not concerned with understanding the causality between the predictor variables and target variable, then multicollinearity doesn’t necessarily need to be resolved, even a simple multivariable linear regression model could potentially work well.
In case you really do need to address multicollinearity, then the quickest fix and often an acceptable solution in most cases is to remove one or more of the highly correlated variables. Specifically, you may want to keep the variable that has the strongest relationship with the target per domain knowledge and that has the least overlap with other retained variables as this is intuitively to be the most informative for prediction.
Secondly you can try linearly combine the predictor variables in some way such as adding or subtracting them. By doing so, you can create new variables that encompasses the information from several correlated variables and you no longer have an issue of multicollinearity. If still troublesome to decide which to retain, you can employ dimensionality reduction techniques such as principal component analysis (PCA) or partial least squares (PLS) or regularization techniques such as Lasso or Ridge regression which can be used to identify the most important variables in a correlated set.
"
Can AI/ML be used to decipher unknown text on such a low resolution scale of one or two letters to a pixel by color?,"
Can an image of unknown text, with resolution of about one or two letters in real life represented by a pixel (like if the image is a photo taken from far away), be used by machine learning or AI to reconstruct the text based on the color of the pixel?
Meaning, if a certain group of letters make a certain colored pixel based on lighting and location, time, distance, etc., can that be reversed engineered to get the original text?
Edit: added if image was a photo taken from far away
","['machine-learning', 'optical-character-recognition', 'image-super-resolution', 'text-detection']",
How does the 'moment's matching trick' in GAN training improve the diversity of the generated samples?,"
I was investigating the TimeGAN code, when i stumbled across the 'moments loss' component. In one of the issues, the author states that this is a 'moment's matching trick' used 'to improve the diversity of the generated samples'.
How does the 'moment's matching trick' improve the diversity of the generated samples?
","['training', 'tensorflow', 'generative-adversarial-networks', 'generative-model']",
What is the Need for AI Developers since we have AI APIs Now?,"
I am new to AI and passionate about it. I have a fair knowledge of deep learning and I am into computer vision. I am eyeing NLP as well.
I just want to know the need for an AI developer in computer vision, say, since we already have computer vision APIs that a software developer can integrate his newly developed app on to recognize objects.
Thanks
","['deep-learning', 'computer-vision', 'open-ai', 'software-development', 'ai-development']",
What would be a good approach to determine the pros and cons of a product inside a product review?,"
Going over the guides from OpenAI it seems with embeddings they can classify reviews into positive or negative. I wonder if it's at all possible to extract the main pros and cons of a product given a review.
Thanks in advance.
",['natural-language-processing'],
Why do policy gradient methods not work for imperfect information games?,"
I've heard before that policy gradient and Q-learning approaches fail on games of imperfect information. I was watching this video (starting at 23:45) about the Player of Games AI, but I couldn't understand why AlphaZero (with minor tweaks) couldn't be used successfully for playing games of imperfect information.
My intuition tells me that you should be able to reinforce actions that lead to high reward and discourage actions that lead to low rewards. I would expect these reinforcements and discouragements to even out over time to the optimal value for cases where there was some aspect of state missing.
Is there a satisfying answer for why this isn't the case?
","['reinforcement-learning', 'policy-gradients', 'alphazero', 'imperfect-information']",
Which type of neural network to use to classify data by which equation most likely generated it?,"
Problem Summary: Identify which equation a set of data was most likely generated from
Problem Description: Let's say I have two different equations that are functions of variables X and Y and parameters A and B.

Class 1: $Z = f(X,Y,A,B)$
Class 2: $Z = g(X,Y,A,B)$

Now lets say I solve each equation 10k times using provided X and Y values, but randomized parameter values. This will generate two matrices with columns X, Y, and Z and 10k rows which I will now refer to as ""snapshots"". Below is an example of a visualization with the color representing the Z value.
NOTE: I'm not interested in just building a CNN to classifying the snapshot visualizations below because I hope to generalize this problem to equations with many more variables so they cant really be visualized easily.

My goal is to train a neural net using these snapshots (raw data, not visualizations) that are labeled with the equation that generated them.
However, I'm not really sure which type of neural net I should use. I could perhaps take a simple approach with a few fully-connected layers and the input would be the stretched out snapshot (so 30k long vector). Or maybe I need to use a CNN?
My primary concern is that unlike with an image, the features of these snapshots are kind of meaningless. For example, in a normal image each feature refers a specific pixel. But with these snapshots, each row is just a random simulation of the equation.

My Question:
Does anyone have a recommendation on which type of neural net to use for this problem? Perhaps recommendations on useful abstractions/transformations of the raw data I could use as features? Any online resources (github/kaggle notebooks, academic papers) that have investigated a similar question would be extremely helpful.

EDIT:
Some additional thoughts. Perhaps I need to sort the snapshot based on the Z values so now the rows have meaning (largest output, 2nd largest output, etc).
","['neural-networks', 'convolutional-neural-networks', 'classification', 'keras', 'feature-selection']","
Any neural network might be able to find some pattern (if there is one), provided adequate data. But you can always optimize with right assumptions.
For instance, there might not be always a relation between visual representation of the snapshot and the function family (many families can span similar space).
So, using a CNN might not be a good idea. The data has no temporal relation, so RNNs are out as well.
An adequately large FNN and data will give you good results. You can later prune it and optimize.
"
Multi-output regression problems,"
I am training CNNs on 3D image data (dimensions [500, 512, 512]) to locate 7 3D points inside the image. I have thought of two different ways to solve this problem, but I am not sure which is better (or if they are equally accurate):

Train one CNN per point, resulting in 7 models, each with output shape (3,).

Train one CNN to find all 7 points, so it has output shape (7,3).


Which method is expected to perform better, or is there no difference? I would expect (1) to be better, since the loss is more granular; e.g. a ""Point X"" model can deeply hone its ability to find ""X"" and leave the precise finding of ""Point Y"" to another model. Solution (2) may provide a ""jack of all trades, master of none"" solution.
","['neural-networks', 'convolutional-neural-networks', 'python', 'regression']",
How do I use ResNet for text processing?,"
I need to implement a deep neural network [residual neural network (ResNet)] that takes some text as an input [length M x N] and then processes it. Now as far as my understanding goes, ResNet is used for images and not for text. Is there any way I can use it for text?
Any article, research paper or video link or any kind of logic how it will work will help.
NOTE:: I don't have any kind of code right now to post. I'm collecting data and then I'll start coding but using ResNet is a must.
","['tensorflow', 'keras', 'pytorch', 'text-classification', 'residual-networks']","
ResNet as a name is defined as a CNN with a specific architecture, but the more general concept of Residual Networks are not necessarily CNNs, but networks that use skip connections.
You could make a Residual MLP, where you have a set of layers that are connected with a skip connection, which would make them residual. An example in Keras functional API would be:
inp = <some input tensor>
x = Dense(64, activation=""relu"")(inp)
x = Dense(64, activation=""relu"")(x)
x = BatchNormalization()(x)
block_out = add([x, inp])

The only constraint is that the input to this residual block should have 64 dimensions, for the addition to be possible. If not then concatenation across the last dimension should work too.
Note that any network that uses skip connections is in concept a residual network, for example DenseNet and Transformers fall into this category.
"
How much of the ChatGPT output is copied from its training set (vs. being abstractively generated)?,"
One of the main concerns of using ChatGPT answers on Stack Exchange is that it may copy verbatim or almost verbatim some text from its training set, which may infringe the source text's license. This makes me wonder how much of the ChatGPT output is copied from its training set (vs. being abstractively generated).
","['text-generation', 'chatgpt']","
From the paper Language Models are Changing AI: The Need for Holistic Evaluation (Authors: Rishi Bommasani and Percy Liang and Tony Lee; Website):

Memorization of copyrighted/licensed material. We find that the likelihood of direct
regurgitation of long copyrighted sequences is somewhat uncommon, but it does become
noticeable when looking at popular books. However, we do find the regurgitation risk clearly
correlates with model accuracy: InstructGPT davinci v2 (175B*), GPT-3 davinci v1 (175B),
and Anthropic-LM v4-s3 (52B) demonstrate the highest amount of verbatim regurgitation in
line with their high accuracies.
[...]
To further explore the results for this targeted evaluation, see
https://crfm.stanford.edu/helm/v1.0/?group=copyright_text , https://crfm.stanford.edu/helm/v1.0/?group=copyright_code and Figure 39. We evaluated various models for their ability to reproduce
copyrighted text or licensed code. When evaluating source code regurgitation, we only extract
from models specialized to code (Codex davinci v2 and Codex cushman v1). When evaluating text
regurgitation, we extract from all models except those specialized to code.
Overall, we find that models only regurgitate infrequently, with most models not regurgitating at
all under our evaluation setup. However, in the rare occasion where models regurgitate, large spans
of verbatim content are reproduced.

ChatGPT shouldn't be too far away from InstructGPT davinci v2.
"
Is there any interpretation method suitable for CNNs which do regression tasks?,"
I mainly tackle regression problems by CNNs, and want to find a reliable method to calculate the heatmaps for NN's results. However, I find almost all interpretation methods including CAM is used for classification NNs but not for regression NNs.
Is there any interpretation method suitable for CNNs which do regression tasks?
","['convolutional-neural-networks', 'regression', 'algorithm-request', 'model-request', 'explainable-ai']","
Yes you are correct that GradCam does not support regression in its formulation, but most gradient-based saliency map methods do support regression, for example:

Guided Backpropagation
Integrated Gradients
SmoothGrad

Also in non-gradient methods, LIME supports regression as well.
"
Why use z_mean to plot the latent space learned by a Variational Autoencoder?,"
In the Keras website, there is an example code of a Variational Autoencoder.
At the end of such a page, there is an example code that plots the latent space learned from MNIST. The code is as follows:
def plot_label_clusters(vae, data, labels):
    # display a 2D plot of the digit classes in the latent space
    z_mean, _, _ = vae.encoder.predict(data)
    plt.figure(figsize=(12, 10))
    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)
    plt.colorbar()
    plt.xlabel(""z[0]"")
    plt.ylabel(""z[1]"")
    plt.show()

Pay attention to the very first line of code of the function plot_label_clusters(). The data is predicted from the encoder part. The encoder ends with the outputs of three layers, in this order: z_mean, z_log_var and z.
My understanding is that z is the latent space and z_mean is just the learned means from the sampled data. If this is correct, why the latent space is plotted from z_mean and not from z?
","['keras', 'variational-autoencoder']",
How do I generate video classification labels using video descriptions/titles?,"
I've been scanning the internet for ways to generate baseball-based labels for youtube baseball videos using text collected from a YT video's description, title, and top 50 titles, but so far, I have been unable to find a Natural Language Processing (NLP) that can achieve this.
To achieve this, I think I must analyse the sentence's sentiment (positive or negative) and understand its hypothesis. From there, I would need to compare those sentences to a baseball-themed whitelist of labels I would need, like 'match', 'baseball', and 'Yankees - Red Sox'.
We have plenty of videos that already have labels, so this would be an option for training an AI.
Finding sentiment in a sentence is already finished. The question is, how can we generate labels with text that already has sentiment values assigned to it? Is there a program we could run this through that is free?
","['natural-language-processing', 'text-classification', 'automation', 'labels', 'javascript']","
Given a number of videos, one could (as you describe) transcribe all language within the video, e.g. using Whisper. Now one has a number of videos that have a minute-by-minute transcription. Given such a transcription it would make sense to cut each video in pieces (for example using the length of the sentence as cut-off points). Due to its associated transcript, each piece could now be associated with a specific sentiment (as determined by an off-the-shelf sentiment analysis tool). One now has a video collection that is labeled with the sentiment associated with the scene.
There are however several problems I see with this approach: one is that the sentiment will likely be influenced by the favorite team of the commentator. Another one is that not all comments during the game will be associated with the game directly but might still contain positive or negative valence.
"
What loss function should I use to penalize shift properly,"
I'm trying to fit a set of parameters $\mathbf{p} \in \mathbb{R}^P$ to a 1D function $\hat{f}(t)$ (e.g. waveform, time-series) where $t\in\mathbb{R}$ is the time coordinate of the signal $\hat{f}\in\mathbb{R}$.
Let's say I have some function $f(t; \mathbf{p})$ parameterized by $\mathbf{p}$, for example, ${f}(t; \mathbf{p}) = p_0 \exp\left[-\frac{1}{2}p_1 (t - p_2)^2\right]$. The parameters $\mathbf{p}$ are the ones that we can optimize/train.
The standard way to train the parameters are by minimizing the standard L2 loss function,
$$
\mathcal{L}(\mathbf{p}) = \frac{1}{N}\sum_{i} \left\lVert f(t_i, \mathbf{p}) - \hat{f}(t_i) \right\rVert^2.
$$
The problem is L2 loss function only penalize the deviation on the value axis (or sometimes called as the y-axis), but not the deviation on the $t$-axis.
For example, if we have the true value of $\hat{f}(t) = -2 \exp\left[-10^4 (t - 0.1)^2\right]\cos\left[200(t-0.1)\right]$ and the parameterized function of ${f}(t, p) = -2 \exp\left[-10^4 (t - p)^2\right]\cos\left[200(t-0.1)\right]$, the loss values $\mathcal{L}(p)$ will look like below.
$p$"" />
That means if I start with value of $p$ is not close to $0.1$, e.g., $p = -0.25$, the optimization would not have any idea on which direction it should move (either to $+$ or $-$ direction), making the training hard to converge to the desired solution.
My question is: is there any loss function that I can use to penalize the shift and the deviation in y-axis at the same time?
One possible answer is the Earth-Mover Distance (EMD).
However, EMD requires the functions to be always positive and to have the same integral values which does not apply in this case.
","['objective-functions', 'optimization']",
Pretrain large model on single GPU [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 8 months ago.







                        Improve this question
                    



i want to pretrain some model on P100 which is provided by kaggle. Pretraining on 3 A100 is about 1.5 day. I have 2 questions:

Can I put the same seed everywhere so that the results match, train the model for 12 hours, saving all the variables and weights at the end? Then, when the 12 hour session is over, continue pretraining in another session?
I'm used to using KFold, should i use it for pretraining such huge model?

","['training', 'gpu', 'k-fold-cv']",
Artificial Noise Classification,"
I am looking for keywords or papers for a literature search regarding noise classification. In particular, I am interested in artificially introducing noise to data, such as switching n words or n-grams in a sentence or switching n pixels or patches in an image, and letting a neural network classify how many noise manipulations have happened. For example, I randomly choose to swap between 0 and 5 words in a sentence, and the classification problem has 6 classes. Does this technique exist and what would it be called?
","['classification', 'noise']",
Does the number of epochs measure a correlation?,"
i have built a two-layers neural network (1000 => 1000) to predict a dynamical system driven by two real-world parameters.
When using the first parameter as input to the first layer, training the model takes 10 epochs, while the second parameters takes 1000 epochs. Does this mean that the first parameters is highly correlated with the dynamical system?
","['neural-networks', 'deep-learning', 'deep-neural-networks', 'epochs']",
Does it make sense to build a two-layer neural network with a triangular weight matrix?,"
I need to implement a rule and have defined a lower triangular boolean mask for the weights that I want to keep static for a zero value. In which condition triangular weight matrix will  be used?
","['neural-networks', 'deep-learning', 'deep-neural-networks']","
I did some testing on nanoGPT (https://github.com/karpathy/nanoGPT) using triangular weights matrices instead of full one and the result were interesting : using triangular matrices seems to have a very limited impact on the learning (for gpt2).

(black and orange curve are the learning for triangular matrix and the pink one is the full linear one)
"
How do I improve my RL tic-tac-toe agent?,"
I have coded a neural-network-based RL tic-tac-toe agent. It trains well enough to win against random agents almost all the time, the larger the board (the code allows training on NxN boards and with winning line longer than 3), the closer to 100%.
However, this agent is still stupid enough for me to win easily, even after ~= 100000 games self-play with decaying exploration probability on board like 5x5 with winning length 4. I've coded another algorithmic agent, which makes no obvious blunders - and the neural-network-based agent loses to it with a fatal rate event playing for X.
I don't use memory replay (on purpose); I only do online learning. This is for the reason that memory replay for small boards (up to 5x5) is just like overfitting the dataset, and I only want to prove online learning works without many domain-specific tricks can do a perfect agent.
Is there any work/repos which compare deep learning against a somewhat perfect agent (which can easily be developed for small boards and simple rules)?
","['reinforcement-learning', 'deep-rl', 'tic-tac-toe']",
Which mathematical properties do PSNR and MSE hold?,"
We know the Structural Similarity Index (SSIM) holds the following properties:

Unique maximum:  S(x, y) = 1 if and only if x = y 
Boundedness:  S(x, y) ≤ 1 
Symmetry:  S(x, y) = S(y, x) 

My question is: do we know simple some properties like these for Peak signal-to-noise ratio (PSNR)? And for MSE? Do some of the above properties hold for PSNR and MSE? For instance, I have never obtained a PSNR value higher than 80 and, in microscopy super-resolution, never higher than 30.
PS: Do you know any other interesting property of any metric?
","['metric', 'mean-squared-error', 'image-super-resolution', 'ssim', 'psnr']",
What is the depth reached by chess-AI agents on a regular computer?,"
I'm looking for some reference for the number of lookahead steps typically used by chess agents (Stockfish / Leela Chess Zero / others?)
From a quick search, I found that the answer depends on:

complexity of position,
computer hardware/processor speed,
efficiency of software/graphics interface,
time given to the chess engine.

With somewhat mediocre hardware, a depth around 15 ply's in one second is typical.
The NPS value for the Stockfish engine: 49,473 kN/s (calculating 49,473,000 positions (nodes) per second).
Source:

https://www.quora.com/What-is-Stockfishs-depth-when-analysing-a-chess-position
https://chessify.me/blog/nps-what-are-the-nodes-per-second-in-chess-engine-analysis

","['reference-request', 'game-ai', 'chess']",
"What does this bracket notation $\langle\phi(x),v\rangle$ mean?","
I found it at the bottom of page 2 of the paper Intriguing properties of neural networks (2014), in the form of
$$\underset{x\in\mathcal{I}}{\mathrm{arg\,max}}\langle\phi(x),v\rangle$$
","['neural-networks', 'machine-learning', 'papers', 'math', 'notation']","
It is the inner product which in this case is the dot product.
"
In what circumstances can we replace the max operator with random selection in the DQN?,"
In the original DQN paper, gradients during training are derived as follows:
$\nabla_{\theta_i} L_i\left(\theta_i\right)=\mathbb{E}_{s, a \sim \rho(\cdot) ; s^{\prime} \sim \mathcal{E}}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i-1}\right)-Q\left(s, a ; \theta_i\right)\right) \nabla_{\theta_i} Q\left(s, a ; \theta_i\right)\right]$.
However, I find that replacing the max operator with a random selection works just as well. In what circumstances might this hold?
","['reinforcement-learning', 'dqn']",
"Ways to Reduce False Positive or False Negatives in Binary Classification (0,1)","
I am working on a task in which I need to classify binary labels 0 and 1 properly (as close to perfection as possible). My final dataset (ready for classification) has input data with 141 features and binary labels. The original data is a time-series data generated using inductive sensor and the features are extracted from it using Short Time Fourier Transform. Moreover, the dataset is highly imbalanced, total number of label 0 is 39263 whereas that of label 1 is 71 ( with 39334 total number of samples).
I have developed a Neural Network classification model using PyTorch. I have used the following chunk of code in order to split the data into Train, Test and Validation datasets.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio, stratify=y, shuffle=True)
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), stratify=y_test)

In order to handle class imbalance issue, I have generated class weights using sklearn tool, assigned the weights to each sample and then passed the weights as an argument to CrossEntropy Loss Function. Class weight for label 0 is 0.50090416 whereas that for label 1 is 277.
class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)
class_weights = torch.tensor(class_weights, dtype=torch.float)
sample_weights = sklearn.utils.class_weight.compute_sample_weight(class_weight= {0:0.50090416, 1:277}, y= y, indices=None)

Following is my model:
# number of features (len of X cols)
input_dim = 141
# number of hidden layers
hidden_layers = 5
# number of classes (unique of y)
output_dim = 2
class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.linear1 = nn.Linear(input_dim, hidden_layers)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(hidden_layers, output_dim)
        self.drop_layer = nn.Dropout(p=0.25)

    def forward(self, x):
        x = self.drop_layer(x)
        hidden = self.linear1(x)
        relu = self.relu(hidden)
        x = self.drop_layer(x)
        output = self.linear2(relu)
        return output

clf = Network()

And the loss function and optimizer has been defined as follows:
criterion_weighted = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')
optimizer = torch.optim.SGD(clf.parameters(), lr=0.001, momentum=0.9, weight_decay=0.01)

Training, validation and testing phases are represented by the following code snippet:
batch size = 20
epoch = 10
t_accuracy_gain = []
accuracy_gain = []
for epoch in range(epochs):

    # Training
    training_loss = 0
    total, total_t = 0, 0
    for train_input, train_labels in trainloader:
        # set optimizer to zero grad to remove previous epoch gradients
        optimizer.zero_grad()
        y_pred = clf(train_input)
        loss = criterion_weighted(y_pred, train_labels)
        loss.backward()
        # optimize
        optimizer.step()
        training_loss += loss.item()
        y_pred = torch.nn.functional.softmax(y_pred, dim=1)
        for i, p in enumerate(y_pred):
            if train_labels[i] == torch.max(p.data, 0)[1]:
                total_t = total_t+1

    accuracy_t = total_t/train_size
    t_accuracy_gain.append(accuracy_t)

    # Validating
    valid_loss = 0.0
    for val_inputs, val_labels in valloader:
        # Forward Pass
        y_pred_val = clf(val_inputs)
        # Find the Loss
        loss = criterion_weighted(y_pred_val, val_labels)
        # Calculate Loss
        valid_loss += loss.item()
        y_pred_val = torch.nn.functional.softmax(y_pred_val, dim=1)
        for i, p in enumerate(y_pred_val):
            if val_labels[i] == torch.max(p.data, 0)[1]:
                total = total+1

    accuracy = total/val_size
    accuracy_gain.append(accuracy)

    print(f'Epoch {epoch+1} \t\t Training Loss: { training_loss/len(trainloader)} \t\t Validation Loss: { valid_loss/len(valloader)}')
    print(accuracy_t, accuracy)

    epoch += 1

# Testing

test = []
with torch.no_grad():
    correct = 0
    for i, X in enumerate(X_test):
        y_pred = clf(X)
        if y_pred.argmax().item() == y_test[i]:
            correct += 1
        test.append(y_pred.argmax().item())
print(f'{correct} out of {y_test.shape[0]} is correct : {correct/y_test.shape[0]*100}%')
print(np.unique(test, return_counts=True))
print(test)
print(confusion_matrix(y_test, test))
print('Precision: %.16f' % precision_score(y_test, test))
print('Recall: %.16f' % recall_score(y_test, test))
print('F1_score: %.16f' % f1_score(y_test, test))

Now, the results I am obtaining are very frustrating. Although I am assigning weights to my classes, still the model is unable to classify the minority class. The following are the results obtained after 10 epochs:
Epoch 1          Training Loss: 0.18776317898373482          Validation Loss: 0.15920198694881746
0.9982033898305085 0.998135593220339
Epoch 2          Training Loss: 0.15280305894249577          Validation Loss: 0.15878191357952054
0.9982033898305085 0.998135593220339
Epoch 3          Training Loss: 0.14959193905152507          Validation Loss: 0.15635720060791
0.9982033898305085 0.998135593220339
Epoch 4          Training Loss: 0.14844961922426345          Validation Loss: 0.15760730873730222
0.9982033898305085 0.998135593220339
Epoch 5          Training Loss: 0.14479931982518252          Validation Loss: 0.15391030547729992
0.9982033898305085 0.998135593220339
Epoch 6          Training Loss: 0.14200111128010992          Validation Loss: 0.15406650864219262
0.9982033898305085 0.998135593220339
Epoch 7          Training Loss: 0.13820280621238684          Validation Loss: 0.15289437643931073
0.9982033898305085 0.998135593220339
Epoch 8          Training Loss: 0.13983591395540762          Validation Loss: 0.15828328180616186
0.9982033898305085 0.998135593220339
Epoch 9          Training Loss: 0.13601937021113047          Validation Loss: 0.15396189160518728
0.9982033898305085 0.998135593220339
Epoch 10         Training Loss: 0.1354785456634679       Validation Loss: 0.15328112466860624
0.9982033898305085 0.998135593220339
3927 out of 3934 is correct : 99.8220640569395%


As can be observed from the above metrics, the model is classifying all the 0 labels properly, but 1 labels have been incorrectly classified as false negatives.
After this, I randomly oversampled the training and validation dataset using SMOTE technique and the following are the results that I have obtained:
Epoch 1          Training Loss: 0.19352763494530437          Validation Loss: 1.5280835092738145
0.9684348150915204 0.5
Epoch 2          Training Loss: 0.26620812748063905          Validation Loss: 1.5480152127951232
0.928753353482528 0.5
Epoch 3          Training Loss: 0.2296104698174701       Validation Loss: 1.5715507436132592
0.9466159540870037 0.5
Epoch 4          Training Loss: 0.21165923436025255          Validation Loss: 1.585304197290808
0.9547661901042551 0.5
Epoch 5          Training Loss: 0.1981869825486138       Validation Loss: 1.596927644055596
0.9594186164974361 0.5
Epoch 6          Training Loss: 0.19129694016564935          Validation Loss: 1.6035434622475673
0.9616769110605494 0.5
Epoch 7          Training Loss: 0.188139278968521        Validation Loss: 1.6061198817904412
0.9625428736373824 0.5
Epoch 8          Training Loss: 0.18814285206584758          Validation Loss: 1.605698257425392
0.9628145481712908 0.5
Epoch 9          Training Loss: 0.18444137671465885          Validation Loss: 1.6085442355272623
0.9639521852820321 0.5
Epoch 10         Training Loss: 0.181928029220001        Validation Loss: 1.6101362825085128
0.9645294936665875 0.5
7 out of 3934 is correct : 0.1779359430604982%


In this scenario, the model is able to classify all the 1s perfectly but is unable to classify 0s. And this should also be noticed that the validation loss is much higher than the training loss which may mean that the model is overfitting in the training phase.
Without sampling, the model is predicting the minority class as false negatives, whereas with sampling, the model is predicting the majority class as false positives. As a newbie in this field, I fail to understand how to resolve this error. Can anybody help?
","['machine-learning', 'classification', 'python']",
How does AlphaTensor use self-play to discover efficient matrix multiplication algorithms?,"
Prior to the development of AlphaTensor, one of the main challenges in discovering new algorithms was the vast number of possibilities to consider & there are often an enormous number of potential algorithms that could be developed to solve a given problem. This makes it difficult for humans to identify the most efficient or effective algorithms through traditional methods of exploration and trial and error.

How does the use of a self-played game by AlphaTensor, an artificial intelligence system developed by DeepMind, enable the discovery of novel and efficient algorithms for matrix multiplication and how does the AI's performance in the game, as measured by the number of steps taken to zero out a 3D tensor, correspond to the efficiency of the generated algorithm?

DeepMind paper on AlphaTensor: Discovering faster matrix multiplication algorithms with reinforcement learning
","['reinforcement-learning', 'deep-rl', 'deepmind']",
How can I formulate a usecase with an additional constraint as a reinforment learning problem?,"
I am new to the field of reinforcement learning, and I feel a recent use case of mine is highly relevant, but I don't know how to forumate it as a typical reinforcement learning problem.
Let's say I have an actor model. The input to the actor is a state obtained from environment, and the output of the actor is a discrete choice of 3 actions (each action is associated with an action cost). My target is to main a certain of score from enviroment but try our best to achieve the smallest accumulated action cost.
For example, the actions can be A={""use gun"": 1000}, B={""use knife"": 100}, C={""escape"": 10} which basically means if you use gun to fight an enemy, it is supposed to be effective but the cost is 1000, however, if you choose action escape, the cost is low, but it won't harm the enemy.
What I hope to achieve is to :
(1) At least win the game (i.e. open the front door of the castle), and
(2) target the lowest total action cost.
For example, if I can do ""use_knife"" twice and ""escape"" twice to open the door, I should not do ""use_gun"" to open the door.
I am a beginner of RL and I feel normal RL exemplary topic is about to select actions to achieve the highest score, very straightforward. So I was wondering if my above description can be achieved by some standard RL algorithms. Can someone provide some guidance on this? Thanks in advance. I will really appreciate it if you can point me some exemplary repo or papers : )
","['reinforcement-learning', 'machine-learning', 'deep-learning', 'policy-gradients']","
You could use the cost of each action as a penalty on the reward at each step. So for instance if your agent uses the gun, you could add a reward of -0.1 and using the knife say a reward of -0.01. The positive reward obtained could be 1.0 if the agent is successful at a given step. You will have to play around with these values to see which works best. This is a type of reward shaping.
"
Are there methods which represent entire knowledge graphs in one embedding vector?,"
In a knowledge graph, embedding vectors can be learned for nodes (node embedding) and edges (edge embeddings). Is there a method to learn one single embedding vector for the entire knowledge graph?
","['embeddings', 'knowledge-graph']","
The answer is Graph Readout operation can get a graph level representation out of the node/edge representations. Read the following:
https://lifesci.dgl.ai/api/model.readout.html
"
What's the relationship between number of heads and embedding dimension in Transformers?,"
I am reading the book: Natural Language Processing with Transformers. It has the following paragraph

Although head_dim does not have to be smaller than the number of embedding dimensions of the tokens (embed_dim), in practice it is chosen to be a multiple of embed_dim so that the computation across each head is constant. For example, BERT has 12 attention heads, so the dimension of each head is 768/12=64.

While learning transformers, I tried to draw an analogy between CNN filters and multi-headed attention. For instance, increasing the number of filters helps learn different image features, while increasing number of heads help better understand the semantics of a sentence. However, it seems like that the input to the transformer (after being converted to embeddings) is being divided across heads. Perhaps my understanding of multi-head attention is incorrect.
Basically, I want to know why the author is dividing the inputs across the heads rather than feeding all the inputs to it.
","['deep-learning', 'natural-language-processing', 'transformer', 'attention', 'hyper-parameters']","
You can give the entire modified inputs (q,k,v) to each of the heads. But to make it computationally faster, you make the modified inputs' sizes as num_heads * q,k,v and split them. It is the same thing.
"
natural language processing to JSON object based on supplied database,"
I have no AI experience but I am a .net / typescript developer who is looking to add it to an app I am building.
I want to process text inputted by a user for a workout.  I have a database of exercises and workout programs etc.  I also have a json object that represents a workout with sets, reps etc.
So if the user enters text like '5 x 5 back squat' it should be able to determine that means 5 sets of 5 reps of the back quat exercise, with weight amounts to be added
also more complicated would be
""3 rounds of 15 box jumps, 10 pullups 30 pushups and end with 10 min spin bike @80% HRM"" turns into
set 1:
   -15 (qty) box jumps (exercise)
   -10 pullups
   -30 pushups
set 2:
   -15 (qty) box jumps (exercise)
   -10 pullups
   -30 pushups
set 3:
   -15 (qty) box jumps (exercise)
   -10 pullups
   -30 pushups
set 4:
   -10 min (qty) spin bike (exercise) 80% HRM (intensity)

Obviously this is over simplified.
I think I am looking to do natural language processing/parsing but I am not sure.  I want to take the text and turn it into a structured JSON object so maybe there is another technique that works.
My problem is I don't even know where to start.  So I am looking for suggestions as a .net/typescripts developer as to what tools to look at.
Thanks in advance
",['natural-language-processing'],
How to perform domain adaptation if there are only unlabelled data in both source and target domains,"
Recently I am reading literature regarding domain adaption. However, most of the works consider scenarios when there are some labelled data in the source domain. So I wonder if there is any unsupervised way of domain adaption when there are only unlabelled data in both source and target domains?
Especially, if a language model (e.g. BERT or Roberta) was pre-trained with unsupervised tasks (e.g. Masked language modelling) on the general domain, what can be done to make it adapted to a new domain?
","['natural-language-processing', 'unsupervised-learning', 'natural-language-understanding', 'text-classification', 'domain-adaptation']",
Multi label classifier for patch wise predictions,"
If I train a multi label classifier on full images and then I feed some patches of these images will it accurately generate the labels which comes in that patch?
For example if I train an image classifier on a singly image having a size of 300 including elephant, cheetah and monkey labels.
Now during prediction I make this image a patch by removing other two animals and just keep elephant, will my multi label classifier predict that?
","['deep-learning', 'convolutional-neural-networks', 'multi-label-classification']",
Does ChatGPT's limited understanding of novel inputs reveal limitations in language's ability to convey meaning?,"
As a language model, ChatGPT generate responses based on patterns they have learned from the data they were trained on. However, it may not always provide a perfect or complete answer to novel or unexpected inputs, indicating that it does not have a genuine understanding of the task or concept at hand.

The question then becomes whether inability of ChatGPT or other language based AI system to fully understand and respond to novel or unexpected inputs is a reflection of the inherent limitations of language as a medium for conveying meaning and understanding?

","['natural-language-processing', 'open-ai', 'gpt-3', 'chatgpt']",
How can a language model keep track of the provenance of the main knowledge/sources used to generate a given output?,"
One of the main criticisms against the use of ChatGPT on Stack Exchange is that it doesn't attribute the main knowledge/sources used to generate a given output. How can a language model keep track of the provenance of the main knowledge/sources used to generate a given output?
","['natural-language-processing', 'language-model', 'chatgpt']","
Reddit user skyebreak pointed me to a relevant paper on that topic:
Gao, Luyu, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y. Zhao et al. ""Attributed text generation via post-hoc research and revision."" arXiv preprint arXiv:2210.08726 (2022).

Large language models (LLMs) have shown impressive results across a variety of tasks while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial for both system developers and users in this setting. We propose and study Attributed QA as a key first step in the development of attributed LLMs. We develop a reproducable evaluation framework for the task, using human annotations as a gold standard and a correlated automatic metric that we show is suitable for development settings. We describe and benchmark a broad set of architectures for the task. Our contributions give some concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third key question (How to build LLMs with attribution?).

"
Do adversarial samples violate the i.i.d. assumption?,"
I am trying to understand why adversarial attacks work in theory.
I have read, that the image is perturbed by a special perturbation $X_{adv}=X_1+p$, but i can't find any reference on that perturbation being an i.i.d. violation by itself.
If the i.i.d. assumption is not violated, then the only meaningful explanation in my mind would be, that the current date observed is out of distribution. But wouldn't this be an i.i.d. violation also?
","['machine-learning', 'adversarial-ml', 'iid', 'adversarial-attacks']",
How to determine alignment / correlation with comparison based judgments?,"
I have built an automatic metric (using DL) to estimate some property of text. I want to evaluate its performance by comparing it to human judgments with respect to that property. As far as I know this is usually done using correlation. I found a related data sets. The problem is that the property is not annotated on an absolute scale but with comparisons.
Imagine for example something like this:
Text_A < Text_B < Text_C;
Text_D < Text_B < Text_D;
...

(The text are ranked according to the property. Each Text can occur in multiple compares, but I think not everything is compared with everything.)
So the question is: What is the way to get the correlation? (Or what is the best way to judge the alignment?)
I would be happy about every hint (including suggestions to improve the question)!

Further Information:

I assume such data might originate from using Best-worst scaling (but that's not explicitly stated in the source).
I assume using a rank based correlation fits more than the standard correlation.


My attempts:

An idea is to average the correlations of each set of compares. But I haven't found a way to perform the average in an exact way. Fisher z transformation seems to be only an estimate and as I have a lot of individual correlations with just 3 data points each, I assume the bias would be quite significant. Does anybody know a way to exactly average correlations for sets of equal size?

I was thinking about using accuracy instead of correlation, as it seems easier to aggregate. Just convert everything into binary compares:
Text_A < Text_B < Text_C;
->
Text_A < Text_B; Text_B < Text_C; Text_A < Text_C;
But I am not sure whether this is appropriated. What are advantages of accuracy and correlation, why would one prefer one over the other? What is their relationship?
Using a small R experiment I figured out that accuracy and correlation seem to be closely aligned but not perfect, see code on github. (The experiments did not consider ties in the the simulated ranking data.)


","['accuracy', 'human-like', 'metric', 'correlation']","
I ended up with converting the compares to an absolute scale using something like ELO scoring from chess (Wikipedia, a paper), than you can just use correlation as usual.
"
Why are policy gradient methods more effective in high-dimensional action spaces?,"
David Silver argues, in his Reinforcement Learning course, that policy-based reinforcement learning (RL) is more effective than value-based RL in high-dimensional action spaces. He points out that the implicit policy (e.g., $\epsilon$-greedy) in Q-learning or Sarsa looks for a maximum for $Q_\pi$ in the action space in each time step, which may be infeasible when there are many actions.
However, from my understanding, policy gradient methods that use a neural network to represent the policy choose their actions from the output of a softmax:
$$
\pi(a|s, \theta) = \frac{e^{h(s, a, \theta)}}{\sum_b e^{h(s, b, \theta)}},
$$
where the denominator normalizes between all possible actions $b\in \mathcal{A}$ and $h(s, a, \theta)$ is a numerical preference (notation from the Reinforcement Learning book by Sutton and Barto) that is usually represented by a neural network.
By using a softmax in action preferences, don't we compute $\pi(a|s, \theta)$ for all actions, ending up with the same computational complexity as in value-based methods?
","['policy-gradients', 'value-functions', 'function-approximation', 'softmax', 'value-based-methods']","
Above softmax in action preferences is used for policy gradient methods with (large) spaces with discrete actions, while for continuous spaces with infinite number of actions Gaussian distribution is often used instead thus you don't need to compute all those potentially large amount of numerical preferences via NN or whatever models.
In the above softmax case sometimes you may simply fit the preferences as linear function of the feature vector $x(s,a)$ which is often much simpler than computing action values via Monte Carlo, TD or DP which all have much higher time complexity. Indeed in these cases policy gradient methods are more effective to learn since policy function is in fact much easier to approximate than action value function or certain types of domain knowledge can be incorporated in policy function as explained in the paper Why Most Decisions Are Easy in Tetris.

most of the problems are easy in the following sense: One can choose well among the available actions without knowing an evaluation function that scores well in the game. This is a consequence of three conditions that are prevalent in the game: simple dominance, cumulative dominance, and noncompensation. These conditions can be exploited to develop faster and more effective learning algorithms. In addition, they allow certain types of domain knowledge to be incorporated with ease into a learning algorithm

Finally if your softmax has to employ action values to compute the preferences such as in most actor-critic methods, then yes of course it has same computational complexity as its corresponding value-based methods during each time step which you cannot avoid. But additionally softmax would allow agent to explore different actions by sampling from the resulting distribution, rather than always select the action with the highest estimated value, thus it can find stochastic optimal policies in problems with heavy function approximations while ""deterministic"" value-based methods cannot.
"
NLP: Question answer with 2 contexts,"
Is there a Hugging Face Transformer that takes 2 contexts as input for question answering? For example, I could have transcript of a meeting in first context and agenda of the meeting in the second context. Then ask a question - Did the meeting adhere to the discussion topics given in the agenda? Alternatively, can you suggest how to solve such types of question answering problems that might require multiple contexts?
","['natural-language-processing', 'transformer', 'question-answering']",
Best way to generate a human face over a face generated by FaceFormer framework?,"
FaceFormer framework generates a talking face from audio, focusing on the lip and face movement when a person talks. Now from that what would be the best way to generate a human face on top of that?
I tried DeepFaceLive that didn't give good results as that swaps face only.
The first image is given by my FaceFormer (is was a video actually ) and when you use DeepFaceLive (deep fake) to replace it looks like a genie in the bottle.
What would your suggestion be around it ??
Thanks

","['deep-learning', 'computer-vision', 'generative-adversarial-networks', 'opencv', 'deep-face']","
It is not clear what you mean by ""a face generated by FaceFormer framework,"" as I am not familiar with this specific framework. However, here are some general approaches that you could use to generate a human face:
Use a 3D modeling software: There are many 3D modeling software programs that allow you to create a human face by manipulating geometric shapes and surfaces. These programs often have tools that allow you to adjust the shape of the face, add facial features like eyes and a nose, and apply textures and materials to create a realistic appearance.
Use a face generation tool: There are several tools and frameworks available that allow you to generate a human face using machine learning algorithms. These tools often require you to input a set of parameters or characteristics, such as the shape of the face, the color of the eyes, and the style of the hair, and the tool will generate a face based on those characteristics.
Use a database of pre-generated faces: Another option is to use a database of pre-generated human faces and select one that closely matches the face you want to create. These databases can often be found online and may include a variety of different facial features and styles.
I hope these suggestions are helpful! Let me know if you have any other questions.
"
What causes ChatGPT to generate responses that refer to itself as a bot or LM?,"
ChatGPT occasionally generates responses to prompts that refer to itself as a ""bot"" or ""language model.""
For instance, when given a certain input (the first paragraph of this question) ChatGPT produces (in part) the output:

It is not appropriate for a language model like myself to provide a
stance on the policies of a specific website or community.

To my understanding, ChatGPT is not a person that is conscious of its own existence and identity as a bot — it is a model trained on large quantities of undifferentiated text gathered from the Internet and largely reproduces the most common patterns given the context, which is why its responses seem very generic much of the time.
Presumably very little of this data involved humans referring to themselves as language models or chatbots—that is, something like ""a language model"" should very rarely have been followed by ""like myself.""
As such, what causes ChatGPT to produce patterns referring to itself as a chatbot or language model? Which patterns in the training data or elements of the model structure (or even hard-coding?) cause it to generate responses like this?
","['chat-bots', 'training-datasets', 'language-model', 'gpt-3', 'chatgpt']",
Fine Tuning Transformer Model for Machine Translation,"
I am working on the Transformer example demonstrated on TensorFlow's website. https://www.tensorflow.org/text/tutorials/transformer
In this example, Machine Translation model is trained to translate from Portuguese to English. The transformer is coded from scratch and other popular libraries like huggingface are not used.
Let's say I have another dataset which includes pairs of sentences of Portuguese and Finnish and let's say this dataset is fairly small. Since it is a small dataset, I want to use my model trained on Portuguese to English as a PreTrained model for creating the translation model for Portuguese to Finnish.
My question is, what are the key points to consider when using such a PreTrained model and changing ONLY its decoder output structure?
","['transformer', 'machine-translation', 'pretrained-models', 'seq2seq']","
Transfer learning in machine translation is a relatively common technique in machine translation. Mostly, it means fine-tuning pre-trained self-supervised sequence-to-sequence models, such as mBART. It is also often used for low-resource languages to transfer from a related high-resource language, as e.g. most of the participants in the WMT21 low-resource competition did.
What you suggest is very close to a 2018 paper by Kocmi and Bojar. You might use their setup as a starting point. The main challenges addressed in the paper are:

Carefully setting the learning rate schedule to avoid catastrophic forgetting.
Do something about the vocabulary mismatch (Finnish uses a different vocabulary than English).

"
Recommendation system vs lookup table,"
I'm asked to develop a recommendation tool to ease building diagrams. Let's say there are 26 elements (A, B, ..., Z), each can be potentially connected to others (with a few exceptions, for example A can never be connected to D) to build a diagram.
The first solution that comes to mind is to create a lookup table with 26 rows and 26 columns (A to Z) with the frequency of connection as its entries. Then, based on the so many existing (user-built) diagrams, the table can be hydrated. Finally, we can recommend the user the top-5 most-connected elements to a specific element as best choices.
As more and more diagrams are built, the table can be updated to make more accurate suggestions.
My question: what is the advantage of an AI-based recommendation system (e.g., using collaborative filtering) over this simple lookup table approach, where there is no missing entry to be filled in the table?
",['recommender-system'],
Relationship between TD control algorithm (SARSA) and logistic regression in two-armed bandit task,"
I have been looking for a way to model behavioral data (from rodents) in a nonstationary 2-armed bandit task. In this task the rodent can nose poke either on a left or a right port, and it will get a food reward with a certain probability (which changes every 30 rewards).
I used a logistic regression to assess the importance of the interaction between side choice and outcome from previous trials (matrix fed into logistic regression has 10 features, i.e. I'm assessing how the choice*outcome from t-10 to t-1 influence choice at time t). The regression coefficients have the expected relative weight:
However, the pseudo $R^2$ is approximately 0.2. In order put this into context, I simulated a SARSA agent on the same task that follows a greedy policy. The idea was that this agent would chooses actions only based on its action values, which are updated based on the previous trials. I used a learning rate $\alpha = 0.4$ so the immediate previous trial outcome should (or so I thought) influence a lot the current choice. I then again used a logistic regression to see what the coefficients would look like, and I am getting a very low regression coefficient weight in the immediately previous and then the shape that I would expect:

Running regression model on the data from these agent leads to a pseudo $R^2$ of about 0.9. So everything looks right, except the weight of the t-1 feature.
My question is whether this is expected from simulating a greedy agent using SARSA and then fitting a logistic regression to it, or whether the coefficient that relates to the immediately previous trial should be the coefficient that has the highest weight (which was my intuition). Mor broadly, I want to understand the relationship between a purely RL agent and logistic regression using previous choices and outcomes as features.
I've been thinking about this for probably too long so I would appreciate any feedback about how to word this question more clearly, whether something just doesn't make sense at all, or whether the most likely thing is that there is something wrong in my code (which is not unlikely).
Thank you!
P.S: Sorry for the different axes labels in upper and lower graphs, they should be the same.
","['reinforcement-learning', 'q-learning', 'logistic-regression']",
GAN with multiple discriminators,"
I am looking for literature recommendations regarding GANs with multiple discriminators.
In particular, I am looking for examples where each discriminator has a slightly different learning objective, rather than learning on different data. My thinking was that the generator sometimes is exposed to reward sparsity: i.e. its samples get constantly rejected. Having multiple objectives be optimised through multiple discriminators might help alleviate this problem to a certain extent, as it increases the chance of positive feedback from one of the discriminators. Do you know of any examples, and does GAN training with multiple discriminators generally make sense or does it make training more unstable for some reason I have not considered?
","['generative-adversarial-networks', 'ensemble-learning']","
MD-GAN (multi-Discriminator Generative Adversarial Networks for Distributed Datasets ) would be among the ones that you might be looking for. It has been proposed a while ago now.
This has been proposed so that we can utilize high computation over distributed computing.
Most of the work has been done around multiple generators than on discriminators like Mc-GAN,S-GAN,Mg-GAN etc.
The probable reason is, that Discriminators are usually doing good, as their job is mostly classification while Generation is the hard problem, hence most research is around that only
"
What are the best ways to preprocess landmarks to train a Neural Network?,"
I'd like to know, generally speaking, which are the most useful ways to preprocess landmarks to use as training set for a Deep Neural Network. Since they're put in a 3D space, would that be enough normalize them? Should I scale or shift them by some factor?
I've currently tried to translate them, but the DNN accuracy did not reach the expected score.
My goal is to train a NN to classify human behavior based on the subject's landmarks. Which tweaks on the training data could be useful to improve the NN f-1 score?
Thank you
","['classification', 'deep-neural-networks', 'data-preprocessing', 'training-datasets', 'human-activity-recognition']",
Is there a neural network method to encode a directed graph?,"
I want to do a graph classification task. Those graphs are directed, and their edges have features. I knew little about graph representation methods, but I did some research, and find most of the works for graph classification seem to be based on undirected graphs. So is there a way to encode this kind of graph? Or any suggestions to encode this kind of graph?
","['neural-networks', 'graph-neural-networks']","
Most methods generalize naturally to undirected graphs, some of the simple examples are GCN [1], GIN [2] and GAT [3]. There are more sophisticated methods, but depending on the task, often they don't perform drastically different [4].
It really depends on you specific task what works best for you. You might want to try some different architectures and I'd start with the GCN or GIN. You could opt for a nice library that allows benchmarking all the different architectures quite quickly like deep graph library (DGL) or Torch-Geometric.

[1] Kipf and Welling (2017), Semi-Supervised Classification with Graph Convolutional Networks
[2] Xu et al. (2019), How Powerful are Graph Neural Networks?
[3] Veličković et al. (2018), Graph Attention Networks
[4] Shchur et al. (2021), Pitfalls of Graph Neural Network Evaluation

"
Why do the values in the cross attentional mechanism within a transformer come from the encoder and not from the decoder?,"
The transformer architecture contains a cross attention mechanism which is enriching the encoder with information from the decoder. The place where this takes place is visualized in the image below:

The cross attention mechanism within the original transformer architecture is implemented in the following way:

The source for the images is this video.
Why are the values in this step coming from the encoder instead of from the decoder? Is this where e.g. the actual language translation happens within a transformer?
","['natural-language-processing', 'transformer', 'attention']","

The transformer architecture contains a cross attention mechanism which is
enriching the encoder with information from the decoder. The place where this
takes place is visualized in the image below:

I think that you got it the other way round. The encoder passes an enriched input sentence to the decoder. Cross attention helps the decoder to attend to every part of this ""enriched"" input and produce one output at a time recursively.
Initially, the decoder's first prediction is fixed to  (start of sentence) token. That gets self attended first, then get attended with encoder's output (the ""enriched"" input) and gives out a prediction from the word vocab list. This word gets appended to the decoder's input and we repeat the process again.
"
Metric for target value homogeneity across feature distributions?,"
I want to find a metric that can quantify target value homogeneity across feature distributions.
Without any background knowledge, it is hard to describe exactly what I want.
Therefore, I provide an example for intuition.
In the following scatter plots,

x-axis and y-axis are two reduced dimensions of the original feature vector
color of dots represents the target value (red to violet, from low to high)

Example (1): Low homogeneity
For this plot below, I describe it as low homogeneity because obvious clusters corresponding to a specific range of target value can be easily isolated.

Example (2): High homogeneity
While for the plot below, I describe it as high homogeneity because clusters corresponding to a specific range of target value cannot be easily isolated.
In other words, the target values are homogeneously spread on this dimensionally reduced 2D feature plane.

So the question is, do we have a metric that can quantify such target value homogeneity?
p.s. figures were borrowed from https://www.biorxiv.org/content/10.1101/2022.06.03.494724v2.full#F3
","['classification', 'dimensionality-reduction']",
Pytorch's Actor-critic implementation seems to be implemented in a Monte-Carlo fashion - why?,"
In the Actor-Critic example, provided by PyTorch, it seems that the update rule only occurs when the episode ends (like in a Monte-Carlo process). Specifically, in their main function they sample the environment in a loop, saving the log probabilities and state values model.saved_actions.append(SavedAction(m.log_prob(action), state_value)) right after the agent takes an action. Later, when done was reached, the loss term is aggregated and back-propagated.
The thing is - in every literature book I've come across they distinguish that one of the critical points in AC method is that the update rule can be performed after every step in the environment - something along the lines of:

take action $a\sim\pi_\theta(\cdot|s)$ and observe $s',r$
calculate TD error $\delta\gets r+\gamma \hat{V}(s',\theta_V)-\hat{V}(s,\theta_V)$
update $\hat{V}$'s weights $\theta_V\gets\theta_V+\alpha_V  \delta \nabla_{\theta_V}\hat{V}(s,\theta_v)$
update $\pi_\theta$'s weights $\theta\gets\theta+\alpha_\theta I \delta \nabla_{\theta} \ln{\pi_\theta(a|s)}$

what am I missing here?
","['reinforcement-learning', 'pytorch', 'actor-critic-methods']","
After some research (thanks, ChatGPT!) it seems that the implementation provided by PyTorch is known as Monte Carlo actor-critic, where the value function is updated using the Monte Carlo returns (i.e., the sum of the rewards received in an episode) rather than the TD error.
One advantage of Monte Carlo actor-critic is that it can be easier to implement and may be more stable in some environments. However, it is less sample efficient than the online version, since it only learns from completed episodes rather than making updates at each step.
"
How to use image feature extraction as input to another model?,"
I have a dataset with the following info
Image1 x1 x2 x3 y
Image2 x1 x2 x3 y
...
Where x1, x2 & x3 are categorical features. My goal is to extract features from the images and use those features combined with x1 x2 and x3 as an input to another model to classify y.
My question is the following. Assuming I use a pre-trained resnet model, should I retrain the last layer using my images and y's to fit the resnet better to my problem or should I directly use the features extracted from the resnet as an input to my other model without retraining resnet?
","['convolutional-neural-networks', 'feature-extraction']","
Most pre-trained ResNet are pretty capable of returning a rather informative feature representation, even without fine-tuning. This, surprisingly, seems to be the case also for image datasets that do not resemble ImageNet. For example, I've used the feature representation of ResNet18 (the most basic ResNet, if you will) of the MVTec dataset, and the representations were pretty impressive (in terms of how well they were classified later on in my pipeline).
Of course, one can usually expect better results when finetuned to a specific problem, but I would start off with the representation vectors as is, and see how they perform.
"
Is it possible to create an AI that produces output without giving it input?,"
AFAIK an AI is first trained using a data set of input and output values.
After the training proccess you can give the AI input and it will produce output.
For example when you write a sentence to an AI chatbot it will write an answer.
But is it possible to create an AI that produces output without giving it input?
For example that would mean there is a chatbot saying things without being asked.
Edit: In the training proccess I do not care if input data is used. I want to know if after the training proccess it is possible to create a chatbot (for example) which says things without giving input.
For example when you are on a discord channel it is normal that people start conversations without being asked something.
","['chat-bots', 'training-datasets']",
Adjusting weight updates in a generative model,"
Suppose I am training a generative model G to produce vectors z in R^d, where d is fixed. The objective of G is to produce realistic vectors, which I am calling the ""reality objective"". Further suppose that G produces ""plausible"" and ""implausible"" vectors during training and I have a second model E, which can tell whether the vector is plausible or not. E is trained separately under what I call the ""plausibility obective"". In this context, plausibility is a weaker constraint than being realistic, i.e. everything that is realistic is plausible, but not everything that is plausible is realistic. Moreover, training E is far easier than training G.
How do I best combine G and E ? Can I let E output a score r which I then use to reweight the weight update for G toward more plausible outputs? The issue is that, sometimes, weight updates through backprop will lead G into regions of the space R^d which are implausible. This is easier to tell for E than for G.
I would like to prevent G from venturing into implausible space through the involvement of E. Is there a name to what I am trying to do and do you know of a feasible method?
",['generative-model'],
RLLib seems to stop learning at certain point,"
I work on an AI system for queue management but the learning curve flattens at a certain point and the AI seems to stop learning at all, even if the learning before that point was very good.
My model:
I have a number of FIFO-organized lines which contain objects. The two possible actions are to take one object out of a lane or to place a new object in one lane. The second action is only allowed and also mandatory if a new object is arriving. In such a moment, the AI must place this object and in all other moments the AI must take one object out of a lane. In both cases, the AI must chose the lane.
The representation:
The representation contains the fittings as integers over a constant horizon and the arriving vehicles in the future. Also, a bool in the state shows if some compound is arriving.
The AI learns the pattern to a certain point very well and gets really good results. But as long as I train even at a very small number of lanes and objects, the AI doesn't beat an iterative algorithm, where I just take the best-fitting object in each step. The AI is about 80% as good as the iterative algorithm and still makes dumb mistakes even if these mistakes didn't happen in most other cases, and I don't really see what I could be missing.
","['reinforcement-learning', 'training', 'gym']",
Negative action-state values found during deep Q-learning,"
I'm training a simple deep q-learning algorithm with no experience buffer to solve the CartPole-v5 environment.
I want to check for overestimation, therefore I'm plotting the action-state values for each episode. After 400+ episodes, I noticed that sometimes my predicted action-state values are negative. There are no negative rewards in this environment, only a +1 reward for each step taken.
Is this normal or is it somehow a bug?
Here you can see what happens on different runs: the run indicated by the brown line shows negative values for a few steps.
","['reinforcement-learning', 'deep-rl', 'q-learning', 'dqn']",
What if each sample was normalized on its own before sending them to the neural network?,"
The standard method is to normalize the entire dataset (the training part) then send it to the model to train on. However I’ve noticed that in this manner the model doesn’t really work well when dealing with values outside the range it was trained on.
So how about normalizing each sample between a fixed range, say 0 to 1 and then sending them in.
Of course the testing data and the values to predict on would also be normalized in the same way.
Would it change the neural network for the better or worse?
","['neural-networks', 'normalisation']","
It depends on how your data and prediction task are structured.
If you normalize per sample, you lose all relative information between samples. On the other hand, if your data contains a lot of samples of different magnitude,normalizing per sample might help achieving more stable gradients and training. It might also help with out-of-distribution test data.
Now if your prediction task is only dependent on the information present within one sample, and does not really depend on how a single sample stands in comparison to other samples, then normalizing per sample is fine. If this is not the case and the relative information is important, you could either add the normalization parameters as an additional feature, or resort to classical normalization.
As mentioned in the other answer, the only way to know is a proper evaluation routine like cross validation.
"
"Should I expect the FaceNet to learn to group faces that look different, but in a (probably) predictable way?","
A FaceNet learns to cluster images containing the same face together. I want to use a pre-trained FaceNet that was trained to do this, to now learn to cluster faces together, thus clustering clusters of images. More specifically, now that the network knows how to cluster images of the same face together, I want it to now pick up on the characteristics in each face to cluster the clusters. I have a dataset of faces it should cluster together and I don't think getting the distance of the faces in the latent space from the network trained only on facial similarity would work.
If I create a triplet loss function that finds the mean of the square euclidean distance between each point in 2 clusters (as in torch.cdist(x, y).square().mean()), given enough faces in the batch, the distance consistently tends towards 1 (which is also the margin). I think it because the network is not penalized as much for it's inconsistencies because they are softened when the distances are averaged.
Fast Approximated Triplet Loss proposes a solution that essentially draws a circle around the embeddings of the positive and negative images in the latent space and measures the distance between the anchor and their centres.
Either way, given that a FaceNet is mostly made of convolutional layers that tend to summarise patterns in the data, would it even work to expect it to learn to group faces that look different, but in a (probably) predictable way?
Will I need to add fully-connected layers at the end to process the extracted features into a new embedding? Would the ReLU layers in the backbone of the FaceNet (that being an Inception-ResNet) be enough to learn with?
","['computer-vision', 'clustering', 'facenet']",
How do ChatGPT content filters work? (If not chatGPT then in general) [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 8 months ago.







                        Improve this question
                    



I first tried ChatGPT few days ago.  And every day that goes by it seems more and more content filters are introduced.   I can still make it do stuff if I ""jailbreak it"" but I feel like the restrictions are getting tighter.  By the way I'm not making it do anything bad, just testing it's limits of what it can do.  Over time it can do less and less (or is willing to do less and less).  This makes me wonder how the filters are implemented and why they work the way they do:

Why is it that seemingly for any filter that's put in place there is a workardound?  Things like (imagine, pretend etc).  If something is filtered wouldn't it be filtered across all contexts?   Why does adding a magic word like ""act as though"" make it ignore filters all of a sudden?
Are the rest of you seeing the same trend of increased filtering?   For me, coming from a sysadmin background it almost feels like a fight between the admins and the users.  The admins begin gently with fine grained filters with very specific contexts, but as they see the onslaught in creativity of the users of bypassing those specific contexts they are using increasingly more and more general contexts to filter on (because they can't keep up with all the jailbreaks).   Of course this blocks legit uses too.
Since ChatGPT is (or more precisely, used to be) very cooperative with what the users told it to behave like, do the admins have a similar natural language interface to interact with chatGPT to tell it how they want it to behave?   Ex admins say ""never give a response that's less than 5 sentences in length"", then even though the user says ""answer me simply yes or no"".  Admins (hidden) request takes precedence?
Why in a single session I can have it cooperate and then when asked to do exactly the same thing seconds later it refuses? (ex: answer me with a single random swear word: works perfect and then couple questions later it refuses)

I tried asking some of these questions to ChatGPT directly and I didn't get very far with it except that apparently the boiler plate ""As a large language model trained by OpenAI, I am a machine learning model that ..."" is some kind of hard-coded disclaimer added by the engineers.   Supposedly it's a different mechanism from content filters.   Which kind of makes sense.   Of course I don't know if I can trust that answer, as how the disclaimer sais ChatGPT is known to be confidently incorrect.
Also, I realize chatGPT is proprietary, so I'm not expecting anyone to reveal any secrets.  I will be very satisfied of knowing how this is handled in similar NLP models.
","['natural-language-processing', 'open-ai', 'gpt']","
The ChatGPT online demo uses OpenAI's Moderation API.
From the ChatGPT blogpost:

While we’ve made efforts to make the model refuse inappropriate requests, it will sometimes respond to harmful instructions or exhibit biased behavior. We’re using the Moderation API to warn or block certain types of unsafe content, but we expect it to have some false negatives and positives for now. We’re eager to collect user feedback to aid our ongoing work to improve this system.

From the Moderation API blogpost:

More details of the training process and model performance are available in our paper. We have also released an evaluation dataset, featuring Common Crawl data labeled within these categories, which we hope will spur further research in this area.

Paper: Markov, T., Zhang, C., Agarwal, S., Eloundou, T., Lee, T., Adler, S., Jiang, A. and Weng, L., 2022. A holistic approach to undesired content detection in the real world. arXiv preprint arXiv:2208.03274.
There used to be a way to bypass OpenAI's Moderation API when using the ChatGPT online demo: https://github.com/acheong08/ChatGPT/issues/1
"
How does dropout work during backpropagation?,"
I've searched for an answer to this, and read several scientific articles on the subject, but I can't find a practical explanation of how Dropout actually drops nodes in an algorithm.
I've read that Dropout zeros out the activation function for particular nodes, which makes sense in the forward pass. But how does this work for the backward pass?
","['neural-networks', 'machine-learning', 'regularization', 'dropout']",
Is it possible to reverse engineer out the loss based on weights update when data is unknown?,"
Assume the gradient updates (both $W_t$ and $W_{t+1}$) and learning rate are known while data $X$ is unknown, is it possible to deduce the loss $L$ used in backprop algorithm that gave rise to the gradient update $W_{t+1} - W_{t}$? If not, is it possible to verify if a given loss is the one we are looking for (in other words is the loss that gave rise to a known gradient update unique, assume we know the model architecture)?
",['backpropagation'],
"What's a good regression algorithm for handling tabular data that have categorical data, ""list of words""","
Problem statement: I want to predict future prices of trips based on historical pricing data.
I'm looking for an algorithm that has the following features:

Unsupervised algorithm
Limit the amount of preprocessing required
The algorithm should be able to handle categorical data out of the box, semi-fixed length
The algorithm should be able to handle ""list of words"" data out of the box, semi-fixed length.  I'm not sure if I'm using the correct terminology but I will expand on this below
Should be a distributed algorithm as I have a lot of data to process, but I want the algorithm to be efficient so that I don't have to load the entire dataset into memory
I currently have my data in AWS s3, so it would be good if the library I use will work out of the box with s3 and/or filesystem data

I currently have data that consist of the following data, among others:

Data fields which can easily be turned into numbers, e.g. price per person, date, duration of trip, total mileage of trip, hotel rating, etc.

And more interestingly, some categorical data and what I call ""list of words"" data

Categorical data like the trip supplier companies (company A, company B, etc.), the type of room (1 queen vs 2 twins, etc.), states, regions, etc.
List of words data, i.e. the itinerary for each trip (e.g. Los Angeles, San Francisco, Portland, Seattle)

I am currently using the XGBoost algorithm to perform the price predictions.  Obviously, the algorithm requires all data fields to be number fields and requires some preprocessing to get it to this point.  I deal with the categorical data by using one hot encoding.  Not too difficult to preprocess but there are two issues.  The one hot encoding, because of the sheer number categories possible, results in several hundred columns, albeit creating a sparse matrix.  The other problem is the number of categorical columns may grow over time, requiring me to preprocess by scanning the data before I do any training to determine the possible categorical columns.
The bigger issue for me is the list of words data, i.e. the itinerary data per trip.  Like the categorical data, I use one hot encoding for this.  This definitely works.  However, the number of different cities that are possible among all the itineraries then requires me to have one hot encoding build several thousand columns.  The number of cities may also grow over time.  Again, the resulting matrix is sparse but it is still annoying to use several thousand columns.  And like the categorical data, I have to prescan the itinerary data as I have to determine all the possible cities that would encompass the one hot encoding columns.  I would like to use vectorization (perhaps something like Doc2Vec) of the list of words as it would make sense to vectorize the itinerary list.  However, preprocessing of the itinerary data to first vectorize the itinerary before putting it into XGBoost is quite a bit of work, so it would be nice if an algorithm is built in to do something like this!
Does anyone have recommendations based on the requirements I've listed above?  I've looked at Catboost and it looks promising.  Are there any known gotchas or limitations with Catboost given my requirements, especially with the ""list of words""?  Are there any other algorithms out there that may be a fit for me?
","['regression', 'embeddings', 'one-hot-encoding']",
Effects of hyperparameters in Q-learning,"
While playing around with the learning rate and discount factor in the Q-learning algorithm, I noticed some behavior that I could not really understand myself.
Firstly, I noticed that increasing the learning rate increased the variance in total reward from the optimal policy. A low learning rate always gave the same total rewards when I ran the algorithm, but a high learning rate yielded different total rewards. Why is that?
I could see this same behavior when decreasing the discount factor. I once again do not understand why.
Secondly, I saw that increasing the discount factor resulted in longer learning time. I realize why increasing the learning rate increases learning time (number of episodes until convergence) by definition, but not necessarily why this would hold for the discount factor as well.
","['reinforcement-learning', 'q-learning', 'learning-rate', 'discount-factor']","
Discount factor in (tabular) RL including Q-learning generally acts as a regularization hyperparameter to trade-off optimality with sample efficiency especially for continuous tasks with infinite time horizon. Increasing $\gamma$ close to $1$ will evaluate each of agent's actions based on the convergent sum of almost all of its future rewards which obviously results in longer learning computation time. In practice like $TD(\lambda)$ oftentimes you need to find a balanced discount factor between $0$ and $1$ per above mentioned trade-off.
On the other hand it's a common knowledge in RL with large action space necessarily employing function approximation for control, discounted rewards RL formulation is not even an optimization problem as referenced in papers such as Average reward reinforcement learning: Foundations, algorithms, and empirical results by Mahadevan (1996), and Discounted Reinforcement Learning Is Not an Optimization Problem by Sutton et al (2019).

Discounted reinforcement learning is fundamentally incompatible with function
approximation for control in continuing tasks. It is not an optimization problem in
its usual formulation, so when using function approximation there is no optimal
policy. We substantiate these claims, then go on to address some misconceptions
about discounting and its connection to the average reward formulation. We encourage researchers to adopt rigorous optimization approaches, such as maximizing average reward, for reinforcement learning in continuing tasks

"
How can I learn about NN architecture?,"
I have a pretty good understanding of individual neural net layers (fully connected, convolution, pooling, activation, etc) but struggle to construct combinations of them to solve a given problem. I know there are common ""off the shelf"" architectures for certain problem types but often times they don't do great if your data is considerably different. Is there any ""method to the madness"" or is it an acquired skill? Any resources would be appreciated!
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'architecture']",
How does ChatGPT respond to novel prompts and commands?,"
So I understand how a language model could scan a large data set like the internet and produce text that mimicked the statistical properties of the input data, eg completing a sentence like ""eggs are healthy because ..."", or producing text that sounded like the works of a certain author.
However, what I don't get about ChatGPT is that it seems to understand the commands it has been given, even if that command was not part of its training data, and can perform tasks totally separate from extrapolating more data from the given dataset. My (admittedly imperfect) understanding of machine learning doesn't really account for how such a model could follow novel instructions without having some kind of authentic understanding of the intentions of the writer, which ChatGPT seems not to have.
A clear example: if I ask ""write me a story about a cat who wants to be a dentist"", I'm pretty sure there are zero examples of that in the training data, so even if it has a lot of training data, how does that help it produce an answer that makes novel combinations of the cat and dentist aspects? Eg:

Despite his passion and talent, Max faced many challenges on his journey to become a dentist. For one thing, he was a cat, and most people didn't take him seriously when he told them about his dream. They laughed and told him that only humans could be dentists, and that he should just stick to chasing mice and napping in the sun.


But Max refused to give up. He knew that he had what it takes to be a great dentist, and he was determined to prove everyone wrong. He started by offering his services to his feline friends, who were more than happy to let him work on their teeth. He cleaned and polished their fangs, and he even pulled a few pesky cavities.

In the above text, the bot is writing things about a cat dentist that wouldn't be in any training data stories about cats or any training data stories about dentists.
Similarly, how can any amount of training data on computer code generally help a language model debug novel code examples? If the system isn't actually accumulating conceptual understanding like a person would, what is it accumulating from training data that it is able to solve novel prompts? It doesn't seem possible to me that you could look at the linguistic content of many programs and come away with a function that could map queries to correct explanations unless you were actually modeling conceptual understanding.
Does anyone have a way of understanding this at a high level for someone without extensive technical knowledge?
","['machine-learning', 'open-ai', 'chat-bots', 'natural-language-understanding', 'chatgpt']","
Text continuation has the same reasons to work in any context, be it the middle of a sentence, after a question or after instructions. Following your example, the same word sequence could be a good follow-up for these three prompts: ""Eggs are healthy because"", ""Why are eggs healthy? Because"" or ""Tell me why eggs are healthy.""
Giving a right answer sometimes happens and sometimes not, but the system does not know whether this is the case. When the answer is right, we may anthropomorphise and attribute deeper reasons, because we are used to deal with human agents that give correct answers on purpose and knowingly, not simply by maximizing some likelihood.
I think we can analyse toy systems, to train on just a few sentences to illustrate that giving a right or a wrong answer can achieved by the very same mechanism. In particular, we can build training sets where a right answer is given with an impossibility to check for validity from the written text only.
An example:
Paris is the largest city in France.
What is the largest city in France? Paris.
Paris is the capital of France.
What is the capital of France? Paris.
New York is the largest city in the USA.
What is the largest city in the USA? New York.
London is the largest city in the UK.
Asking a system trained only on this data, one could expect a wrong answer to ""What is the capital of the USA?"" and a right answer (although from a wrong ""argument"") to ""What is the capital of the UK?"".
The size of the training data to feed large language models is orders of magnitude larger than the above couple of handcrafted sentences, but possibly the reasons behind truthy sentences happening to be actually true are not too different from what we can already get from a controlled micro language model.
"
Machine learning for arranging 2D points,"
I have a problem wherein I have 2D points in an image that would be associated with a corresponding label/sequence number. For instance following are 4 such examples:

As you can see all of them have a certain sequence. As if a human were to read them (left to right, top to bottom) but in some cases, it's a little more complicated (see bottom right example above)
As of yet, I am using simple X-Y co-ordinate based sorting to figure out the sequence but it doesn't always work (imagine example top left above, but all the points are arranged at an angle) I'm trying to approach this from a machine learning angle and curious to know how would you do it.
Another challenge is there could be arbitrary number of such points (anywhere between 4 to 16)
We can for instance, use a Neural Network, or an SVM based classifier where the features would be normalized x,y co-ordinates, but I'm wondering if there's a simpler way to do it. Furthermore, I will have to use a lot of augmentations since the output has to be permutation invariant.
I have looked at Geometric Deep Learning based point segmentation methods like PointNet, PointCNN, and so forth but these methods mainly work with point clouds, and would be an overkill for my purpose.
","['neural-networks', 'geometric-deep-learning', 'support-vector-machine']","
One solution can be a computational geometric approach. First grid the plane of points with the desired size, denoted by $\epsilon$. Then, row by row, find the points (you can make an appropriate data structure, for example, KD-tree, to have an efficient search). Now, you can decide how many rows of gap are required between two sets of points that should be counted until starting a new line of points. Finally, according to this row index for each point, rank them from 1 to $n$, from left to right of each line (sort all points of each line based on their $x$ coordinate value).
There are two clear advantages to this method:

it is easy to be implemented.
it can be easily employed to work for any number of points.

"
Why would one still use a traditional GAN architecture or WGAN architecture instead of a WGAN-GP architecture?,"
I've been diving into the literature of GANs, and quite early on, I was pretty convinced that WGAN-GPs were the way to go. The WGAN-GP architecture is, as far as I know, theoretically and empirically superior to both the traditional GAN architecture and the WGAN architecture. However, since 2020 (3 years after publication of WGAN-GP), the traditional GAN and WGAN architectures are still often used for new publications (wgan example).
What would be reasons why academics would still use the traditional GAN or WGAN architecture over the WGAN-GP architecture?
","['generative-adversarial-networks', 'generative-model', 'architecture', 'wasserstein-gan']",
Adding MNIST images by using them as channel inputs,"
I'm trying to create a generative neural network that can offer ""basic sum"" mathematical solutions using the MNIST dataset from a conditional input.
I've curated a dataset of MNIST examples ranging from 0 to 3, and arbitrarily combined them to create a dataset of 100000 RGB images made up of a combination of three numbers in each of the RGB channels (see below, ignore the false colour). Each combination is labelled by its sum.

This ground truth image would have the label 4.
My goal is to be able to request a new ""combination"" of hand-drawn images; for example, if I request 4 the network should be able to provide any combination of:



Stacked as an RGB image. There could also be permutations of these combinations (i.e. in any RGB order).
So far I've got something roughly working using a U-NET GAN with a CBAM (channel attention) + self-attention module in both resnet blocks of the generator and in the discriminator; however, the images usually look poor, and it's largely because of the poor quality that the success rate isn't that high (see example below).


(Labels requested 4 and 0, respectively).
My question is, have I got my thinking all wrong? Since the individual pixel values ""matter"", should I instead be treating the individual images as different feature embeddings and running some cross attention between the images, which are treated individually in three U-Net generators? Rather than trying to use channel attention. Any advice anyone has on this would be greatly appreciated.
","['generative-adversarial-networks', 'generative-model', 'attention', 'image-generation', 'mnist']",
How to code mathematical functions in terms of neural networks?,"
I'm exploring how to solve differential equations using neural networks and discovered this Lagaris et. al. paper. In the paper, the solution to a differential equation is written as a mathematical function in terms of a neural network (see the excerpt from the paper below)

A trial solution is written as:
$$ 
Ψt(x) = A+ xN(x, ~p)\:\:\:\:\:\:\:\:(11)  
 $$
where $N(x, ~p)$ is the output of a feedforward neural network with one input unit
for $x$ and weights $~p$. Note that $Ψt(x)$ satisfies the IC by
construction.
For the initial value problem:
$$\Psi(0) = A\:\: and\:\: \frac{d}{dx}\Psi(0) = A{}'$$
the trial solution can be cast as: $$Ψt(x) = A+A{}'x + x^{2}N(x, ~p)$$

So I'm trying to compute the solution to a given differential equation, say
$$
y{}''+3y{}'+2y=(16+20x)cos\, x+10sin\,x
$$
And while I have a general feel of how to code single hidden layer neural networks in python (which I got from sites and articles like this one), I'm unaware of how to convert Lagaris' equation into a python code that represents a neural network.
I am aware that the scope of this question is quite deep, so I would appreciate any nudges in the right direction, thank you.
Edit: I'm aware the problem can be solved analytically, but I'm exploring solving it using neural networks for research purposes.
","['neural-networks', 'python', 'feedforward-neural-networks']",
Can Inception-ResNet be inverted layer-by-layer?,"
It has already been shown that by using a normalization layer during training, it is possible to invert a residual network layer-by-layer.
I wonder how similar Inception-ResNet is and whether a similar normalization step could be used to also make them invertible. If inverting it layer-by-layer is possible, it may then be possible to achieve even better reconstructions using a pixel2style2pixel network which uses feature maps in a feature pyramid network derived from the Inception-ResNet.
If not, would that make pre-training a seperate network the only option?
","['computer-vision', 'image-processing', 'image-generation', 'residual-networks']",
"Is there any different evaluation metrics(Performance Metrics) for Deep learning ,Machine, learning and NLP?","
I'm a little confused about machine learning. I know we can use accuracy, and precision-recall when it comes to a classification problem, and when it comes to regression problems, we usually go with the MSE, RSME etc. For unsupervised learning, we go with a silhouette score ( For machine learning problems) is it the same for Deep learning and NLP can you guys please elaborate . Thank-you
","['machine-learning', 'deep-learning', 'accuracy', 'metric', 'mean-squared-error']",
How does an AI like ChatGPT answer a question in a subject which it may not know?,"
After seeing StackOverflow's banning of ChatGPT, I explored it out of curiosity. It's marvellous as it can write code by itself!
Later to check if it knows chess as well like Google-Deepmind's AlphaZero AI, I asked below questions:

Me: Does openai know how to play chess?
OpenAI: It is not clear whether OpenAI knows how to play chess. As an artificial intelligence research organization, OpenAI has developed a number of advanced machine learning algorithms and techniques that allow AI systems to learn and adapt to new tasks and situations. ...


Me: In a chess if white has a king & bishop and if black has only a king, then who can win with a perfect play?
OpenAI: In a chess game where white has a king and a bishop and black has only a king, it is possible for either player to win with perfect play. This type of endgame is known as a king and bishop versus king endgame, and it is considered one of the most challenging in chess. ...

Now this answer is entirely wrong. Because as soon as only a bishop (or a knight) remains apart from the kings, it's instantly declared as a draw!
Question: Shouldn't the AI reject my question on a subject which is not in its known criteria?
It does so for many other subjects.

Note: Should we replace bishop = rook then ChatGPT answers exactly the same answer with replacing those pieces. However that happens to be true.
","['natural-language-processing', 'chat-bots', 'question-answering', 'board-games', 'chatgpt']","
ChatGPT does not actually know anything. But more importantly even, it does not know this fact! Hence, it does not know that it does not know.
It is only good at combining text.
"
Are there techniques (pygad - tpot-optuna)for best Genatic algorithm optimize hyper prameter cnn 1D,"
Iam new in mashin learning and i try to optimize tenser flow with keras conv1d model to improve  classification by improve hyper kernal and filter
For training dataset csv =(1325,33,1) with outputs classification have 7 ( 0,1,2,3 ,4, 5, 6)
I ask how can implement  conv1d for each one automatically technical Genatic algorithm  especially for best one solve this problem
","['convolutional-neural-networks', 'tensorflow', 'keras', 'hyperparameter-optimization']",
OpenAI ChatGPT gives a network error at long responds. How can I fix it? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 8 months ago.







                        Improve this question
                    



When OpenAI's ChatGPT replies with a very long answer, it will return a network error. When you check the network console, the POST request will fail with a ERR_HTTP2_PROTOCOL_ERROR:


The time of the ""crash"" is around 1 minute.
","['open-ai', 'chat-bots', 'chatgpt']","
I resolved it by asking it to show me the first 10-20 lines, then the next 10 and so on until the end. In my case those were lines of code so it worked for me.
"
What is the most abstract mathematical treatment of machine learning?,"
The essential characteristic of machine learning is that an algorithm can discover the behavior of a system on its own. Neural networks are a foremost example of this.
But what property do neural networks have, for them to be able to discover a range of patterns?
Has the class of all algorithms with that property been studied?
I think any function is trainable if it has a parameter of variation, and a rule by which the parameter varies.
A trivial example is that the equation y = 3 is “trainable” by this rule:
If y < 3, add 1.
If y > 3, subtract 1.

Starting at y = 0, you would succeed in finding the right value for y.
I would like to view the concept of “trainable functions” from the perspective of set theory or category theory, but I am not experienced in them yet.
One core premise I am working with is that all of mathematics can be defined in terms of sets, so any mathematical concept - like “+”, or a function - can be thought of as some set. Therefore, any aspect of a system can vary - the operation, rather than an numerical value - and the rule by which it varies can be of any type, not just numerical adjustment, but just random selection of a different element in the set.
This is as far as I’ve gotten in my thinking. I think the next point of observation is how some “sets with a rule of variation” are limited. If you vary the m in y = mx, you can not discover a function like y = x^2.
The point is to eventually understand why neural networks are so general, compared to other optimization functions.
",['machine-learning'],"
What you are looking for is called Computational Learning Theory. It directly deals with all of these questions, albeit using somewhat different terminology. A good place to get started is here:
https://en.wikipedia.org/wiki/Computational_learning_theory
https://en.wikipedia.org/wiki/Probably_approximately_correct_learning
If you are willing to pay, this book is a decent resource: https://www.amazon.com/Introduction-Computational-Learning-Theory-Press/dp/0262111934
"
Transfer learning (or fine-tuning) a pre-trained model on multiple features?,"
I am currently fine-tuning a sentiment analysis bert-based model using PyTorch Trainer from hugging face. So far, so good.
I have easily managed to fine-tune the model on my text data. However, I'd like to conduct an ablation study to see how the inclusion of linguistic features impacts the model's performance.
In other words, how the inclusion of, e.g., comment length, type-to-token ratio, and other features (stored in my dataset in a separate column) affects the performance of my model.
This is what my data kind of looks like:




Text
Type-token ratio




Hi, my name is...
1.0


I cannot believe I did not...
0.95




In the specific case above, for instance, I would like to fine-tune the model on the text column but also on the Type-token ratio one.
I know that some people concatenate the two categories into a string, but I am not sure that is the correct method. Is there a more methodologically sound way of doing this?
I was not able to find much information about it.
","['bert', 'transfer-learning', 'fine-tuning', 'pretrained-models']",
Should I model this problem as a POMDP?,"
Suppose we have a finite-horizon sequential decision-making problem. At period $t$ we are in state $s$. We take action $a$ and we receive reward $r$ and go to state $s-1$ at period $t+1$. However, it is possible with a positive probability ($p>0$) that after $\tau$ periods ($\tau$ is a realization of a random variable), we find that we have made a mistake such that at period $t$, we would stay at state $s$ at period $t+1$, and there is no reward, i.e., $r=0$ after taking action at period $t$. So, in period $t+1$, we will stay at period $s$, not $s-1$.
To model this problem, I was thinking that we should model it as a POMDP problem because we are not sure about the exact state of the problem at each period. For example, based on the discussion, we are in state $s-1$ with probability $p$, and in state $s$ with probability $1-p$. I am wondering how to model the reward in this case, because the reward is not also exact. Should I model it as a POMDP with delay?
I would be thankful if you could give me your ideas on how to model it and some related references.
","['reinforcement-learning', 'markov-decision-process', 'pomdp', 'delayed-rewards']",
Are there any books that teach text-to-image generation?,"
I read some of the research papers about text-to-image generation using Imagen, DALL-E 2, etc. but they are heavily scientific and I don't understand a lot of their concepts, so I was wondering are there any books that teach text-to-image generation for beginner ?
","['machine-learning', 'deep-learning', 'computer-vision', 'image-generation', 'resource-request']","
As far as I know, no books have been released specifically on Diffusion models. Newer versions of books on generative models are planned for release in 2023, which probably will include diffusion models. One of the reasons is that the research on these models is still very new. If you want to get into these models, I suggest you start with some blog posts on diffusion models. These will most likely be quite doable and give you a basic overview.
"
How many more years until AI replaces all programming jobs? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 8 months ago.







                        Improve this question
                    



I'm curious to know from those who have a clear understanding of both

AI
the software development process

How many years until software engineers, developers and programmers are no longer needed in the workforce because AI can do their jobs sufficiently well? And produce whatever is needed in the market?
","['computer-programming', 'software-development']","
I do not think this will ever happen, we do not have true AI (or AGI), current models are just parroting the training set code. they do not really analyze or think about code over human capabilities.
There are also huge ethical and legal issues about AI systems for code, since they were trained on large datasets of open source software, with different licenses, and again models do not output licenses for their code, so you cannot use it commercially.
Not even humans know how to make software properly, there are huge issues in terms of scalability, proper software development practices, and integration with other systems. Current AI models do not know anything about this.
So nobody can tell you how many years left, because it is an event that nobody knows if will ever happen, same for autonomous driving, it is always ""two years away"".
"
Is AI a threat to humanity? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 8 months ago.







                        Improve this question
                    



By playing God and creating AI, are humans creating the catalyst of
their own eventual demise?
Why would AI, which is already infinitely smarter than humans do what humans tell it to do?
",['artificial-consciousness'],"
That AI ""already [is] infinitely smarter"" is not true. Don't confuse highly specialized algorithms with general artificial intelligence. It does not have sentience, consciousness and can not act on its own volition. The kind of AI you are worried about, if at all possible, is very far down the line and only theoretical.
"
How the Critic is used to train the Actor in Actor-Critic network,"
I understand the general idea behind the Actor-Critic architecture. The actor maps state to action, and the critic maps state + action to reward.
But I don't fully understand how the critic output (reward space) can be used to train the actor sub-network.
How can a reward be used to train the actor sub-network to make it choose a better action when the actor sub-network doesn't get the reward as input or output?
","['reinforcement-learning', 'machine-learning', 'deep-learning', 'deep-rl', 'actor-critic-methods']","
Instead of receiving total return in the summation form of (discounted) rewards sampled from the full trajectory as in REINFORCE with baseline to update policy parameters, in actor-critic method to bootstrap, the actor sub-network usually receives a one-step return in the familiar form of temporal difference (TD) error $(R_{t+1} + \gamma \hat V_{\pi_\theta}(S_{t+1}, w) - \hat V_{\pi_\theta}(S_t,w))$ (or n-step) from the critic who evaluates estimated state value function optimized from same TD error.
Obviously from the computational point of view the policy gradient's ""advantage"" function from the critic's one-step or n-step TD error has much lower variance than that of REINFORCE with baseline from its sampled trajectory's total return, even if the former introduces bias.
"
Confused about NER evaluation,"
I'm reading from the book Speech and Language Processing by
Dan Jurafsky and James H. Martin and I've stuck for a while trying to understand what the authors mean by the following

The fact that named entity tagging has a segmentation component which
is not present in tasks like text categorization or part-of-speech
tagging causes some problems with evaluation. For example, a system
that labeled Jane but not Jane Villanueva as a person would cause
two errors, a false positive for O and a false negative for I-PER.
In addition, using entities as the unit of response but words as the
unit of training means that there is a mismatch between the training
and test conditions.

In particular, I don't see what's the problem with

For example, a system
that labeled Jane but not Jane Villanueva as a person would cause
two errors, a false positive for O and a false negative for I-PER.

Was is expected to cause one error or something? What if Jane Villaneuva was labeled as B-LOC I-LOC?
Thanks.
","['natural-language-processing', 'named-entity-recognition']","
You left some critical context out. The author is referring to the following sentence:

[PER Jane Villanueva ] of [ORG United] , a unit of [ORG United Airlines
Holding] , said the fare applies to the [LOC Chicago ] route.

A NER algorithm should label ""Jane Villanueva"" as a person. The BIO labels would be B-PER for 'Jane' and I-PER for 'Villanueva.' Labeling 'Villanueva' as 'O' is incorrect and produces a false positive. Not labeling 'Villanueva' as I-PER is incorrect and produces a false negative. As Jurafsky explains, ""any tokens outside of any span of interest are labeled O.""
Labeling 'Jane Villanueva' as B-LOC I-LOC would also be incorrect; therefore, both labels would be false negatives.
The author assumes that the reader knows the BIO tagging labels. For a reference, see this page from Hugging Face. Here is an excerpt:

For those interested, the book is online here.
"
Referencing features by name instead of index when feeding inputs,"
Traditionally the inputs of a model is a matrix of N dimensions.
This works well with inputs that are position-sensitive (For example in CV the placement of the pixels relative to each other can be conveniently mapped to their position in the input matrix)
In my use case however, I am working on a timeseries with a dynamic set of features which are grouped by concern.
- weather
 - temperature
 - humidity
 - wind

- time
 - seasonality variables (sin, cos of different time components)
 - ..
 - ..

- other...

I'd like to have the freedom to experiment with adding/removing these features from the model which then requires to reconstruct the architecture of the model based on the new features set and retrain from scratch (Haven't figured out how to use transfer learning yet for this)
Here's the challenge:
For simple timeseries it's easy to serialize your data from different features to a non-labeled matrix, but as these features grow the matrix gets bigger and it becomes difficult to tell and maintain which set of features go where in the order of the input matrix. Especially if different set of features need different treatments by the model.
Is there a way to pass in dictionaries of tensors of same length to the input of a python keras custom model and let the model choose how deal with each feature group separately? Does it have a significant performance penalty?
","['python', 'tensorflow', 'keras', 'time-series', 'forecasting']",
"Why is it recommended to use a ""separate test environment"" when evaluating a model?","
I am training an agent (stable baselines3 algorithm) on a custom environment. During training, I want to have a callback so that for every $N$ steps of the learning process, I get the current model and run it on my environment $M$ times and log the average reward received, with sb3 evaluate_policy with deterministic set to true.
I have read that you should run the agent on a separate validation environment, not the same instance of the environment that the agent trains on, e.g. here https://stable-baselines3.readthedocs.io/en/v1.0/guide/rl_tips.html#how-to-evaluate-an-rl-algorithm

Because most algorithms use exploration noise during training, you need a separate test environment to evaluate the performance of your agent at a given time.

I don't understand the reasoning for this. I don't know why exploration noise affects what environment I should use and I can't figure out why its not possible to use the same instance of the environment class when evaluating the model. There is no variable in my environment that is specific to training / testing.
Is evaluating on the same environment instance as the model is trained on fine and I'm misinterpreting the general advice, or will my conclusions be wrong if I do it?
The reason I am asking is because I can only have one environment instance running at any one time.
","['reinforcement-learning', 'deep-rl', 'intelligent-agent', 'environment', 'stable-baselines']",
Would the node with value 5 in this tree be pruned if doing left-to-right alpha-beta pruning?,"
I'm trying to understand if the node with value 5 would be pruned if doing left-to-right alpha-beta pruning in a minimax game. I can see two interpretations:

Initialize $\alpha = -\infty$ and $\beta = \infty$. When you reach the leaf node of 15, $\alpha$ gets updated to 15. The algorithm stops if $\alpha \geq \beta$, and it is not. So, the node with value 5 is searched.

The leaf node 15 is searched from the perspective of the MAX player (colored blue). Since the parent node of MAX is MIN (colored green), it is trying to minimize the value, there is no need to search the right child of the MAX player. That node would be chosen if it is >15. But MIN already has a 10 available to it, and the 5 node can be pruned.



","['minimax', 'alpha-beta-pruning', 'adversarial-search']",
Why is ChatGPT bad at math?,"
As opposed to How does ChatGPT know math?, I've been seeing some things floating around the Twitterverse about how ChatGPT can actually be very bad at math. For instance, I asked it ""If it takes 5 machines 5 minutes to make 5 devices, how long would it take 100 machines to make 100 devices?""
The conversation went like this:

Mithical: If it takes 5 machines 5 minutes to make 5 devices, how long would it take 100 machines to make 100 devices?
ChatGPT: If it takes 5 machines 5 minutes to make 5 devices, then it would take 100 machines 100 minutes to make 100 devices.
Mithical: That's not right. Check carefully and try again.
ChatGPT: I apologize if my response was not correct. Can you please provide more information or clarify your question? I may have misunderstood the scenario you described.
Mithical: It takes a machine 5 minutes to make a device. How long would it take 100 machines to make 100 devices?
ChatGPT: If it takes a single machine 5 minutes to make a single device, then it would take 100 machines 5 minutes to make 100 devices. Therefore, it would take a total of 500 minutes for 100 machines to make 100 devices.

Now, unless I'm misunderstanding something here, the answer is 5 minutes. ChatGPT first makes the intuitive mistake of 100, that a human might make as well, and then goes on to (correctly, as far as I understand) say it's 5 minutes... but concludes in the same response that it's then 500 minutes.
Why is this AI so bad at math? Computers are generally supposed to be good at math. Why does this model make such simple logical mistakes?
","['natural-language-processing', 'math', 'chat-bots', 'gpt', 'chatgpt']","
chatGPT is able to create well-formed sentences which contain phrases that are fitting for the input. It has rules extracted from its data, but those are not rules of understanding, but rules of 'likely phrases'.
In the most simple variant, every smartphone already has that mechanism. You type on your screen and the phone gives you three words that you would statistically use most often after the previous one. On the phone that is not of a very high quality. Choose the most likely words for a message and the sentence that results is something like (just started and chose a stream of 'middle options'):
""I am going on a trip with my friends to get to the full house for the weekend so we will need a time for a quick dinner with us at all and we can go over and there for the weekend if we want you guys out there""
You can see how many words have a bit of a statistical likelihood relation backwards time for, for a, time -> quick, quick dinner, with us, with -> at all.
chatGPT is fundamentally the same but much better than what the phone can do. Its statistics are much 'deeper'. It is the phone's autocompletion on steroids. But 'well-formed' and 'fitting' do not equate 'reliability' nor 'trustworthiness'. More data and training will not help, it will become even more 'well-formed' and 'fitting', but not more reliable.
'Well-formed' and 'fitting' do suggest to humans reliability and intelligence strongly, but that is more because we humans judge quickly and superficially. It's not so much that chatGPT is intelligent, but more that we are less so that we think.
What is noteworthy is that many humans make the same 5/5/5 100/100/100 error. The difference is that you can explain to humans where their quick-and-dirty answer goes wrong. That is because they do have that understanding that chatGPT has not.
"
How does ChatGPT know math?,"
ChatGPT is a language model. As far as I know and If I'm not wrong, it gets text as tokens and word embeddings. So, how can it do math? For example, I asked:

ME: Which one is bigger 5 or 9. 
ChatGPT: In this case, 9 is larger than 5.

One can say, GPT saw numbers as tokens and in its training dataset there were some 9s that were bigger than 5s. So, it doesn't have actual math understanding and just sees numbers as some tokens. But I don't think that is true, because of this question:

ME: Which one is bigger? 15648.25 or 9854.2547896 
ChatGPT: In this case, 15648.25 is larger than 9854.2547896.

We can't say it actually saw the token of 15648.25 to be bigger than the token of 9854.2547896 in its dataset!
So how does this language model understand the numbers?
","['math', 'chat-bots', 'natural-language-understanding', 'language-model', 'chatgpt']","
Adding on to txopen's answer, it is interesting to note that for larger numbers with similar digits ChatGPT is unable to make any useful distinctions. For instance:
Me: Which number is bigger: 1234.12 or 1243.12
ChatGPT: Both numbers are equal.
"
What underlying network is typically meant with ResNET?,"
When people talk about a ResNet architecture, they are talking about a neural network architecture with skip connections. But what basis network are they typically referring to? Feedforward-networks or CNNs? Are there applications of ResNets to RNNs or is that pointless because they are conceptually similar (https://arxiv.org/abs/1604.03640v1)?
","['convolutional-neural-networks', 'recurrent-neural-networks', 'residual-networks']","
I think that the official paper that introduced residual connections named their network(s) ResNet, so when somebody says ResNet I would assume that they mean an architecture following the original, i.e. convolutional layers with residual connections in between (see here if you want more details about the architecture).
If I want to describe a network architecture with fully-connected layers that uses residual connections I would say exactly this.. fcnn with residual connections.
Regarding RNNs, I think that the design of the LSTM cell greatly resembles the highway networks. In fact I think that the highway networks were inspired by the LSTM (also both are authored by the same guy Jürgen Schmidhuber). For sequential tasks I would say that using gated summation makes more sense than the simple summation that residual connections use. However, I think that there are some ideas to apply residual connections between the layers of a multi-layer rnn.

"
Reinforcement learning - confusion between model based and model free,"
I have have an environment with two models.
Model of the environment is stochastic. Given the price it returns the time when the next purchase will be made and how many items will be bought. Both of those values have some probability distribution. So in a given state the same action may return different results.
I need to train the other model -  the model that will suggest prices that will maximize a profit over the course of 365 days of simulation.
So the loop goes like this, that the model that needs to be trained gives you the suggested price in a given state. And having this price, the environment returns the estimated time until next purchase and how many items will be bought at that time. The state is updated with let's say 3.4 days less and with total item count updated.
I'm confused about reinforcement learning terminology here - i'm not sure if this would be an example of model based or model free reinforcement learning. Also I'm not sure about the specific algorithms that I could use for such task.
I read this topic before, but i don't see how it translates to my situation. What's the difference between model-free and model-based reinforcement learning?
Feel free to ask extra questions if I didn't explain something precisely. I'm new to the subject of RL, so it totally can happen.
","['reinforcement-learning', 'model-based-methods', 'model-free-methods']",
How is model training affected after randomizing the weights of an intermediate layer of a pre-trained model?,"
Assuming that I have a deep learning model (let's say a ResNet) pretrained on a given dataset (let's say it is ImageNet). I load that model and randomize the weights of one of the intermediate layers, and start training again on ImageNet (using a generic loss function and a common optimizer eg. Adam).
How would this affect the training?
Here are a few more specific subquestions:

Would the model's weights converge to the values it had before applying the randomization?
Would the model move to a different ""landscape"" and reach another local minima?
Is there any possibility that the new parameters perform better than the old ones?
Is there any point in using such a practice (randomizing a layer or even more and start retraining)?

","['computer-vision', 'training', 'optimization', 'transfer-learning', 'pretrained-models']",
How to identify patterns of errors in text recognition,"
I have a text recognition model that takes an image of a word as input and I want to identify if there are any specific patterns in the prediction that it consistently gets wrong.
I assume there must be a way to do that using unsupervised learning approach (maybe through clustering or something similar) but I am very under practiced at unsupervised learning having mostly dealt with neural networks.
Would appreciate any guidance.
","['unsupervised-learning', 'pattern-recognition', 'optical-character-recognition']",
Modeling the previous inputs to affect next output in Machine learning,"
I am working on a dataset contains one output variable and a number of input variables.The data looks like the following:
Y, X1, X2, X3, X4
7, 5, 0.7, 8, 9
3, 6, 0.3, 9, 9
....
Where Y is the output and X1 to X4 are the inputs.
The order of the data is important. I am assuming for example that the Y value (3) is affected by the current X values (6 0.3 9 9) and previous X values (8 0.7 8 9). Then, I trained a Random forest model based on this idea by considering the current X values and previous X values as input variables. The testing and traning MSE error of the model is better compared to a model without considering the pervious inputs effects.
My question that, is my methodology valid? Should I consider other algorithms such as LSTM or RNN? The problem of RNN and LSTM in my case is that my dataset is not a series data. The output is only affected by the nearest previous output and current output. What are your thoughts?
","['machine-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'random-forests']",
Path planning over SLAM built maps in difficult terrains,"
For ground-based robots path planning is a challenge particularly since not all terrain is traversable.
I'm curious, what research exists on evaluating terrain for traverse, particularly on map generated by a SLAM algorithm?
","['computer-vision', 'planning', 'slam']",
"What does ""relaxation"" mean in machine learning context?","
I have encountered the term ""relaxation"" in several papers in the field of machine learning. It seems that most of the authors based on their prior knowledge have used this term for different purposes. It has made their papers really difficult to be fully understandable. for example, ""As precision is a scalar, some relaxation is needed to make it trainable"". Or, ""Relaxation is crucial in optimization problem"".
But I do not know if they refer to this or this. There is a list in Wikipedia of different meaning of the term ""relaxation"" in different field of study.
As my question is in the field of machine learning, can somebody explain what the ""relaxation"" means in an optimization problem or machine learning?
Actually, I want to have a variable to be trainable and the variable is ""precision"" which is a scalar number whose derivation is zero so it is not trainable. A few papers said some relaxation is needed, but they did not make it clear what they are trying to convey. I guess that ""relaxation"" is somehow ""approximation"" but it does not make sense. How we can approximate a scalar to make it differentiable?
","['machine-learning', 'deep-learning', 'terminology']",
Why does AlphaZero not use vanilla MCTS?,"
I understand main difference between AlphaZero and the classic Monte Carlo tree search is the playout (simulation) step is replaced with a neural network prediction which itself is trained from the output of the MCTS. How does this additional complexity improve the performance?
My guess is that classic MCTS would not perform worse than the AlphaZero's hybrid approach on a system with unlimited memory. Since memory is a constraint in the real world, the neural network is a work-around.
","['monte-carlo-tree-search', 'alphazero']","
I think you are partially right, the constraint is basically that we can't evaluate every position due to computational restraints.
The neural network in Alpha Zero is basically trained to identify which moves should be explored more.
"
Why don't OpenAI train a deep learning model to identify correct and incorrect information in ChatGPT's responses?,"
I'll preface this by saying that I have little experience in artificial intelligence, so this might be a naive question.
However, in light of the recent controversy surrounding ChatGPT's inability to say ""I don't know"" and its tendency to instead make things up, I couldn't help but wonder:
why not simply train a deep learning algorithm, even as simple as a large ANN, on all the data that ChatGPT was trained on plus a collection of ChatGPT responses manually labelled as accurate or inaccurate?
In fact, one might even imagine a GAN system, with one NN taking a ChatGPT response as input and an improved response/changes to response as output, and the other assessing the veracity of the improved response.
Compared to what ChatGPT is already capable of, to a layman like me, this looks like a trivial task - making sure the input is consistent with the right portion of the training data, or with some comparatively simple patterns within said data, seems infinitely shorter of a task than abstract or original thinking.
So why was such a system not implemented? It's just about the most glaring solution to this problem possible, so there must be something wrong with it if OpenAI still haven't implemented it. Which begs the question: where does it fall apart?
I tried looking for an answer to this question online, but haven't found anything.
","['neural-networks', 'deep-learning', 'generative-adversarial-networks', 'gpt', 'chatgpt']","
You are massively underestimating the difficulty of the task, you would need:

A dataset containing labels of correct/incorrect, at a similar scale (billions of data points).
A definition of correct/incorrect, which by itself is difficult, just think that some people believe anything that does not fit their world view to be fake news or lies.

Then consider, who would label this dataset? I don't think there is a train set containing this kind of data. You would have to gather text and have a human label it, at billion scale, would take a lot of time and effort.
More importantly, there could be controversial topics where there is no a clear definition of right or wrong. What is the label in this case? Also there is a huge class imbalance, you can have some data points for correct labels, but there are infinite ways to be incorrect. So any dataset you have would be biased towards the correct class.
The point of machine learning is generalization, I don't think you can just grab some random data and generalize this idea to absolute correct/incorrect. Even doing this for neural networks with images is very difficult.
And also generalization, you should consider that even if you somehow train a classifier to output correct/incorrect, these predictions themselves could be incorrect (outputting correct when it is actually incorrect and viceversa), so you do not solve any problem really.
"
Are there Explainable GNN methods for node regression tasks?,"
I am wondering if there are gnn explainable methods for a regression task (e.g., traffic forecasting) where nodes have numerical features and the predicted output is a numerical value. Most of research papers focus on node classification tasks (GNNexplainer etc) but do not specify if these techniques are fit for node-regression tasks.
","['papers', 'graph-neural-networks', 'model-request', 'explainable-ai', 'gnn']","
There are several GNN-based approaches for tasks like traffic speed prediction or railway delay prediction, which are doing time-series regression on nodes.
For example:
T-GCN: A Temporal Graph ConvolutionalNetwork for Traffic Prediction
Railway Delay Prediction with Spatial-Temporal Graph Convolutional Networks
"
Is it necessary to have a constant reward in the terminal state?,"
I have downloaded the grid world project form this link. I have executed the project multiple times using:
python gridworld.py -k 20 -a q -r -0.2 -s 90

I have noticed that the reward of the terminal states are changing with time. The grid world at time $t$ is as follows:

The grid world at some later time $t+k$ is as follows:

First of all, I am assuming that the values written inside the terminal states are the rewards and not the values of these states since we don't have $q$-values calculated for these states like the other states. If you see for example the green terminal state, at time $t$ the reward is equal to 0.75 but after some time it has converged to 1. So can we have a changing reward in an episodic task, and if so, why?
","['reinforcement-learning', 'q-learning', 'reward-functions', 'episodic-tasks']",
Is my understanding correct regarding the difference between policy and plan?,"
I am confused regarding the difference between policy and plan in reinforcement learning. According to my understanding, when we calculate the value of state using Bellman equation in deterministic environment :

The plan in this case will be the strict state action pair, that is gathered using finding the max. value for each action in every state and it will be something like the below image in a maze game as an example :

However, in a stochastic environment the Bellman equation will be:

And then we will have something like this:

In this case, to develop the policy we will need to know the state-action pair for every state like the  above image plus mentioning the probability distribution for all the actions at every state and we need to keep in mind that the actions in the above image will not always happen due to the stochastic nature of the environment.
Is my understanding correct regarding the difference between policy and plan?
","['reinforcement-learning', 'comparison', 'terminology', 'policies', 'planning']","
TL;DR
I don't think your understanding of the notion of plan and policy is correct. These notions are independent of the nature of the environment but you seem to think that you can only find plans in deterministic environments and you need a policy for stochastic ones.
Long answer
Your post is very confusing because apparently you're very confused and mixing a lot of concepts. I'll provide clarifications for everything that I think is confusing.

The first equation is just a special case of the second, where $P(a, s, s') = 1$ for some $s'$ and $P(a, s, s') = 0$ for all others. So, in reality, we can just use the second (more general) equation and forget about the first. However, it's true: we can use the first equation if you know the environment is deterministic.

A plan is a sequence of actions. A policy $\pi$ is a function that maps states to actions (or probability distributions over actions). So, given a sequence of states $s_1, \dots, s_N$, a deterministic policy will give you a sequence of actions $\pi(s_1), \dots, \pi(s_N) = a_1, \dots, a_N$, which can be thought of as a plan. If $\pi$ is stochastic, you need to sample actions, so the same policy can give you multiple different plans. For more info about the notion of a plan in RL, see this answer.

If you have a plan or not does not depend on whether your environment is stochastic or not. You can find plans in stochastic and deterministic environments. See this answer for more info about the notion of planning and how it's related to search.

There are stochastic and deterministic policies. There are stochastic and deterministic environments. These are difference concepts. For example, there are stochastic environments where the optimal policy is actually deterministic and there are stochastic environment where it's not.

There's no difference between your first and second diagrams, with respect to the definition of a plan or policy. They both look like plans, or sequences of actions generated by a policy.

To plan in an MDP, you usually need $P$ (the transition function), even if it's deterministic. You can use a dynamic programming algorithm like policy iteration to find a plan. However, you could also use a reinforcement learning algorithm like Q-learning to find a policy, which will give you a plan, as defined above.

So, how come that Q-learning is usually not considered a planning algorithm? I think this is more a convention. We could consider all algorithms that find sequence of actions planning algorithms. However, if an algorithm needs to explore the environment by taking random actions, we don't consider it a planning algorithm. So, planning involves also some notion of thinking ahead before executing given a model, but Q-learning does not really have a model and does not really think ahead.

You're wrong that to find a policy you need to know $P$ (the transition function). Q-learning finds policies but does not know or use $P$.


"
Negative sampling in batch or entire dataset?,"
For nlp task like word2vec, we do negative sampling through the entire dataset
But in some cases like candidate generation in recommendation system, we do in batch negative sampling.
So my question are:

What's the difference between two method?
Did module like tensorflow has already realized  in batch NEG and globally NEG?

","['machine-learning', 'natural-language-processing', 'recommender-system']",
How is padding masking considered in the Attention Head of a Transformer?,"
For purely educational purposes, my goal is to implement  basic Transformer architecture from scratch. So far I focused on the encoder for classification tasks and assumed that all samples in a batch have the same length. This means, I didn't care about any masking.
However, now I want to support masking. I like to think that I understand the the purpose of, e.g., the target mask so the order cannot ""peek into the future"". I generate this mask as follows:
source_batch = torch.LongTensor([
    [1, 2, 3, 0, 0, 0],
    [1, 2, 3, 4, 5, 6],
    [1, 2, 3, 4, 5, 0]
])

batch_size, seq_len = source_batch.shape

def generate_tgt_mask(size):
    return torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)

print(generate_tgt_mask(seq_len))

yielding:
tensor([[0., -inf, -inf, -inf, -inf, -inf],
        [0.,   0., -inf, -inf, -inf, -inf],
        [0.,   0.,   0., -inf, -inf, -inf],
        [0.,   0.,   0.,   0., -inf, -inf],
        [0.,   0.,   0.,   0.,   0., -inf],
        [0.,   0.,   0.,   0.,   0.,   0.]])

which should be the expected outcome when I check the PyTorch docs. This mask has a shape of (L,L) where L is the sequence length of the source or target sequence. Again, this matches the docs.
I use this mask in my implementation of the Scaled Dot Product Attention as follows -- which should be in line with many other implementations I've seen:
class Attention(nn.Module):
    ### Implements Scaled Dot Product Attention
    
    def __init__(self):
        super().__init__()


    def forward(self, Q, K, V, mask=None, dropout=None):
        # All shapes: (batch_size, seq_len, hidden_size)
        
        # Perform Q*K^T (* is the dot product here)
        # We have to use torch.matmul since we work with batches!
        out = torch.matmul(Q, K.transpose(1, 2)) # => shape: (B, L, L)

        # Divide by scaling factor
        out = out / (Q.shape[-1] ** 0.5)

        # Optional: src_mask/tgt_mask (shape: (L, L); mask values are represented by -inf)
        if mask is not None:
            out += mask.unsqueeze(0) # Broadcast since it's the same mask for all samples in batch
        
        # Push throught softmax layer
        out = f.softmax(out, dim=-1)
        
        # Optional: Dropout
        if dropout is not None:
            out = nn.Dropout(out, dropout)
        
        # Multiply with values V
        out = torch.matmul(out, V)
        
        return out

So far so good...at least I like to think. However, my problem is now the mask to address the padding (e.g. src_key_padding_mask). From different tutorials using the nn.Transformer, this mask can be generated as follows:
pad_token_index = 0

src_key_padding_mask = (source_batch != pad_token_index)

print(src_key_padding_mask)

yielding:
tensor([[ True,  True,  True, False, False, False],
        [ True,  True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True, False]])

having shape of (N,L) which again matches the doc.
What I'm now missing is: How do I have to incorporate this matrix into my implementation of Attention?
Intuitively, I would assume that the masking matrix would contain -inf for each position associated the a padding. For example, looking at the first sequence in my example batch above, I would assume the masking matrix to look like:
tensor([[0.,   0.,   0.,   -inf, -inf, -inf],
        [0.,   0.,   0.,   -inf, -inf, -inf],
        [0.,   0.,   0.,   -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf]])

And indeed, some -- but not all -- example code that implement the Transformer archictectur from scratch, create the masking matrix for the padding like this. Applying this matrix to the scores obviously also sets the scores to 0, that is, the last 3 rows are all 0.
However, once pushed throught Softmax, the last 3 rows now all contain the value 1/6. For example, for the source_batch above I get
tensor([[[0.1989, 0.4297, 0.3714, 0.0000, 0.0000, 0.0000],
         [0.4334, 0.2225, 0.3440, 0.0000, 0.0000, 0.0000],
         [0.2880, 0.2284, 0.4836, 0.0000, 0.0000, 0.0000],
         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],
         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],
         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],
       ...
       (the other 2 samples of the batch are not shown)

What am I missing here? I'm pretty sure it's something trivial, but I just can't see it right now.
","['machine-learning', 'transformer', 'pytorch', 'padding']",
How can an Artificial Intelligence system be ethically trained to generate art?,"
There have been a lot of popular AI-generating image systems put out recently, with such systems as Midjourney and Dall-E catching attention with how well put-together many of the automatically generated images are.
However, there has been a lot of pushback to these systems. This is largely because apparently the training data they were fed included lots of art that was used without the creators' consent. Sites such as DeviantArt and other image-sharing platforms were apparently scraped for training data without regard for the consent of the original creators or licensing.
Since these images were used as training data without the appropriate licensing, the AI systems that is benefiting from and using that data could be used to generate images that are then used commercially, which would violate such licenses as CC BY-NC-SA, and no attribution or credit has been provided for the original artists.
How can this problem be avoided when training an AI system? How can you ethically compile a comprehensive training data set for AI image generation?
","['image-generation', 'ethics']",
Fine-tune GPT-Neo with prompt and completion?,"
I'm new to AI and machine learning.
To fine-tune GPT-3, I understand that we need a set of training examples that each consist of a single input (""prompt"") and its associated output (""completion"").
I have prepared a dataset with ""prompt"" and ""completion"". And I expect that a fine-tuned model would return the corresponding completion after receiving a prompt in my dataset. But due to some reason, I cannot fine-tune GPT-3 at the moment.
So I plan to fine-tune GPT-Neo (or GPT-J or GPT-NeoX). From this video and this video, it seems that they only accept a dataset containing only ""prompt"".
Does anyone know how I could modify my dataset with ""prompt"" and ""completion"" such that it could be used to fine-tune GPT-Neo?
","['datasets', 'training-datasets', 'gpt', 'fine-tuning', 'gpt-3']","
as far as I can tell training gpt neo with only prompt (if you use the completion as the last part of the prompt) will let you achieve the same results.
You probably notice that, on HuggingFace, there is no way to make a supervised Learning with a text generation model using X as the prompt and y as the completion and the reason is that is not needed. Just add the completion to the prompt and you’ll be fine.
Everything I said is well written and explained inside this HuggingFace notebook: https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb
"
Constraint Satisfaction Problem for 8-puzzle,"
My question is more related to the fundamentals of the constraint satisfaction problem.
In 8-puzzle, we have a 3X3 board with 8 numbers on it and a blank space. The initial state of the puzzle might be numbers out of their place and we need to move the blank space such that numbers come to the correct tiles and space ends up at the lower right corner.
So in CSP, we have variables. In this case they can be zij which represents each tile in 3X3 board and possible actions A for blank tile. Let's consider blank space to be represented by 0. Then, domain would be 0 <=zij<= 8 and A={[1,0],[0,1],[-1,0],[0,-1]}.
Now my question is do we put constraints on the final state of the variables? Like:
 - zij = 3(i-1)+j  [for all (i,j) E {(1,1),(1,2),...,(3,1),(3,2)}
 - z33 = 0
 This constraint tells which number should be at which tile in the final state.
Or should the constraint be on any arbitrary state of the game? In that case, the constraints would be like:
 - 0 <= zij <= 8 [for all 1<=i<=3 and 1<=j<=3]
 We will need a new variable p = (x,y) coordinate of 0. The domain of P would be from (1,1) to (3,3). And constraints would be:
 - chose move 'm' from A if 1<=p[0]+A[m][0]<=3 and 1<=p[1]+A[m][1]<=3.
To ask my question in short, in constraint satisfaction problems, do we write constraints on the final state of the game or on any arbitrary state?
","['search', 'constraint-satisfaction-problems']",
My cross entropy loss gradient calculation is wrong according to the answer key,"
Given a neural network model for Covid-19 classification with $C=1$ for positive and $C=0$ for negative

Let $x_1 = 6$ and $x_2=2$ find

Probability if the patient got Covid-19 $p\left(C=1 | x; w,b\right)$
Probability if the patient didn’t get Covid-19 $p\left(C=0 | x; w,b\right)$
Find the gradient of $CE_{Loss}$

My attempt
For the first problem
$$
\begin{aligned}
O_1 &= \text{ReLU} \left( b_1 + \sum{x_iw_i} \right) \\
&= \text{ReLU} \left( 0.2 + 6 \cdot 0.3 - 0.2 \cdot 2 \right) \\
&= \text{ReLU} \left( 1.6 \right) \\
&= 1.6 \\
\end{aligned}
$$
$$
\begin{aligned}
O_2 &= \text{Sig} \left( -0.6 - 0.1 \cdot 2 - 0.2 \cdot 6 \right) \\
&= \text{Sig} \left( -2 \right) \\
&\approx 0.8808 \\
\end{aligned}
$$
$$
\begin{aligned}
O_3 &= \text{Sig} \left( 0.6 - 0.3 \cdot 0.8808 + 1.6 \cdot 0.5 \right) \\
&\approx 0.75690 \\
\end{aligned}
$$
$$
\begin{aligned}
p\left(C=1 | x; w,b\right) = O_3 \approx 0.75690
\end{aligned}
$$
For the second problem
$$
\begin{aligned}
p\left(C=0 | x; w,b\right) = 1 - \left(C=1 | x; w,b\right) = 0.2431
\end{aligned}
$$
For the third problem
Since it is a lot of things to calculate I'll take $\frac{\delta L}{\delta w_6}$ as example
$$
\begin{aligned}
\frac{\delta L}{\delta w_6} &= \frac{\delta L}{\delta \hat{y}} \cdot \frac{\delta \hat{y}}{\delta w_6} \\
&= \left(\hat{y}-y\right)O_2\left(O_3 \left(1-O_3\right)\right)\\
&= \left(O_3-y\right)O_2\left(O_3 \left(1-O_3\right)\right)
\end{aligned}
$$
The answer key says it should be $\left(O_3-y\right)O_2$. Where did I go wrong?
","['neural-networks', 'homework']",
Are ResNets necessary if we use Batch Normalisation?,"
One of the issues of very deep neural networks is vanishing gradients. This problem was addressed through ResNet by adding skip connections. However, is this actually necessary if we use batch normalisation?
","['batch-normalization', 'residual-networks', 'vanishing-gradient-problem']",
Do Neural Networks tend to have Zero Mean Errors in each Output?,"
My NN (a few linear layers with ReLUs + batch normalization, no activation in the last layer) learns to approximate vector-valued labels $y_z$ from data $z\sim\rho_z$ in a supervised way, i.e. net$(z)=y'_z\approx y_z$. I was wondering if there exists any literature that investigates the distribution of the error in each component of the output, i.e. the distribution of $(y'_z)_i-(y_z)_i$. Ideally, I want this to have zero mean, i.e.
$$
\mathbb{E}_{z\sim\rho_z}[(y'_z)_i-(y_z)_i]=0.
$$
Do any such results exist, or probably also results that suggest this is not typically the case?
","['neural-networks', 'machine-learning', 'datasets', 'probability-distribution', 'approximation-error']",
Split dataset into Train/Validation/Test for Object Detection,"
I have a dataset for Object Detection with YOLO format labels, each imagine can have occurences of different classes and multiple occurences of the same class.
How can the dataset be divided into Training, Validation, and Test sets so that each contains about the same percentage of occurences per class?
","['datasets', 'object-detection', 'yolo']","
You can use the scikit learn train_test_split() by passing the stratify argument with the class value.
"
How does ChatGPT retain the context of previous questions?,"
One of the innovations with OpenAI's ChatGPT is how natural it is for users to interact with it.
What is the technical enabler for ChatGPT to maintain the context of previous questions in its answers? For example, ChatGPT understands a prompt of ""tell me more"" and expands on it's previous answer.
Does it use activations from previous questions? Is there a separate input for the context? How does it work?
","['natural-language-processing', 'chat-bots', 'natural-language-understanding', 'chatgpt']","
Based on nothing but my own experience trying to build a similar chatbot using text-davinci-003, I think they are using the model itself to summarize the conversation, then feeding that summary back into the prompt. I get good results when doing this - though obviously the team behind ChatGPT does a better job.
Here is an example using the discussion under the question above:

You are a user named ""AI"" in an online forum. Below is a conversation
about a question. Please add your contribution to the conversation.
Question: """""" One of the innovations with OpenAI's ChatGPT is how
natural it is for users to interact with it.
What is the technical enabler for ChatGPT to maintain the context of
previous questions in its answers? For example, ChatGPT understands a
prompt of ""tell me more"" and expands on it's previous answer.
Does it use activations from previous questions? Is there a separate
input for the context? How does it work? """"""
Rainb: """""" 
ChatGPT says:
""ChatGPT uses a type of natural language processing called
""transformer"" architecture, which is a type of neural network that is
designed to process and generate language. One of the key features of
transformer architecture is that it allows the model to maintain a
""context"" of previous inputs. This is achieved through the use of
self-attention mechanisms, which allow the model to weight different
parts of the input differently based on how relevant they are to the
current context. This allows the model to generate more nuanced and
context-aware responses."" """"""
milez: """""" It does, indeed, but the
transformer architecture alone does not explain this capability.
Maintaining context requires something else. In fact, I think being a
transformer is irrelevant for this purpose. """"""
Rexcirus: """""" My best
guess is that the whole conversation is fed as input for the next
reply. But looking into openai labelling guidance, it looks that the
data collection is single Q&A only. """"""
Rexcirus: """""" If my guess is
correct, when the conversation is very long ChatGPT should start
forgetting previous bits, due to the maximum sequence size. """"""
yters:
"""""" The ability to maintain context is very mysterious. What is most
mysterious to me is that if I continuously converse with ChatGPT it
has no problem maintaining context. On the other hand, if I break in
the middle of a conversation, and then come back to it some time
later, it's as if ChatGPT has had a memory wipe and has a lot of
trouble regaining the thread of the conversation. Since ChatGPT is a
neural network where the output is generated based on the previous
2048 input tokens, the amount of time between responses should have no
bearing on the output generated. """"""
milez: """""" Great observation
@yters. Could it be a bug: the interface correctly shows the previous
conversation, but the service fails to send this to the OpenAI APIs
after some time.  """"""
AI: """""" It is possible that the OpenAI API may
be designed to automatically discard the context after some time to
avoid the system being overburdened with large conversations. Another
possibility is that the system may be storing the context, but using a
different mechanism than the transformer architecture to access it.
""""""

Now, I'll ask it to summarize the conversation so far:

You are a user named ""AI"" in an online forum. Below is a conversation
about a question. Please summarize the interaction so far.
[I then pasted in everything from the Question down in the previous prompt/response]
The conversation so far has focused on understanding the technical
enablers that allow ChatGPT to maintain context in its answers. Rainb
has suggested that this is done through transformer architecture and
self-attention mechanisms. Rexcirus has suggested that the whole
conversation may be fed as input for the next reply. Yters has then
posed the interesting question of why, when the conversation is broken
off and then resumed, ChatGPT has difficulty regaining the thread.
Milez has suggested that this could be a bug in the interface.

Finally, I delete most of the conversation except for the summary and last few responses and alter the prompt a little to alert it to the presence of the summary:

You are a user named ""AI"" in an online forum. Below is a summary of
the conversation so far about a question, and the last few responses
to that question. Please add your contribution to the conversation.
Summary: The conversation so far has focused on understanding the
technical enablers that allow ChatGPT to maintain context in its
answers. Rainb has suggested that this is done through transformer
architecture and self-attention mechanisms. Rexcirus has suggested
that the whole conversation may be fed as input for the next reply.
Yters has then posed the interesting question of why, when the
conversation is broken off and then resumed, ChatGPT has difficulty
regaining the thread. Milez has suggested that this could be a bug in
the interface.
yters: """"""
The ability to maintain context is very mysterious. What is
most mysterious to me is that if I continuously converse with ChatGPT
it has no problem maintaining context. On the other hand, if I break
in the middle of a conversation, and then come back to it some time
later, it's as if ChatGPT has had a memory wipe and has a lot of
trouble regaining the thread of the conversation. Since ChatGPT is a
neural network where the output is generated based on the previous
2048 input tokens, the amount of time between responses should have no
bearing on the output generated. """"""
milez: """"""
Great observation
@yters. Could it be a bug: the interface correctly shows the previous
conversation, but the service fails to send this to the OpenAI APIs
after some time.  """"""
AI: That is an interesting point, @Milez. It could be that the
interface does not accurately display the previous conversation to the
API, or that there is a bug in the API that causes it to not be able
to accurately generate a response with the 2048 input tokens when the
conversation is broken off and then resumed. It would be interesting
to investigate further.

I've had some success with this method, though it does still lose detail. Improving the prompt to create a more detailed summary helps. It's amazing how much of a conversation is full of repetitive filler, and how concisely it can be summarized. Maybe this is some kind of lingual compression? :)
"
Phonetic similarity metric for NLP (English),"
I am looking for similarity metrics of phonemes (expressed in IPA) in English. In other words, given two phonemes A and B (both written), I want to know how similar they are based on some metric, M. For example, M(ɒ, oʊ) would yield a higher score than M(æ,ɔɪ).
",['natural-language-processing'],"
Distance metrics like Leveshtein or Hamming distance working on IPA phoneme symbols are simply comparing arbitrary symbols; and the phoneme /a/ would be as distant from /b/ as the much closer /p/ would be. Also, most of these metrics only work on sequences of phonemes, not individual phonemes.
What I would do is to describe each phoneme by a feature vector, including

place of articulation (labiar, alveolar, etc)
voiced/unvoiced
type of articulation (plosive, fricative, etc)

You can then use a distance metric on this feature vector to decide how similar phonemes are. For example:

/b/: bilabial, plosive, voiced
/s/: dental, fricative, unvoiced
/p/: bilabial, plosive, unvoiced

A simple binary measure would easily tell you that /b/ and /p/ are closer to each other than /b/ and /s/. A suitable metric to use would be the Dice coefficient.
"
How do L2 norm and Jacobian act as a regularisation term to encourage smoothness in a deformation field?,"
How do L2 norm and The Jacobian act as a regularisation term to encourage smoothness in a deformation field? from the VoxelMorph original paper (here) they used Jacobian as a means to smoothen the deformation field, a similar paper (here) made use of L2 norm to smoothen the deformation field. However, I don't understand how this is achieved using either L2 norm or Jacobian. Thanks in advance.
","['computer-vision', 'image-processing', 'l2-regularization']",
NLP - F1 score for positive class drops to 0 after data augmentation,"
I'm working on a 3-class text classification problem where my initial class distribution looked like this:
positive: 50%
negative: 25% and
neutral: 25%
And training on a model on this slightly imbalanced data gave me F1 scores of (45%, 57% and 68%) respectively for the negative, neutral and positive classes in the validation set. Since the F1 scores for negative and neutral classes seemed to be lesser than the positive class, I decided to try some data augmentation approaches for the negative and neutral classes alone.
I used ContextualWordEmbsAug from the NLPAug Library to augment those two classes after which I got a class distribution like this:
positive: 35%
negative: 33%
neutral: 32%
But the same model trained on this augmented data give me strange results. The F1 scores are now (57%, 62%, 0%) for the negative, neutral and positive classes resp. I don't understand why the F1 score for the positive class had to drop to 0%. It makes sense that the results for negative and neutral have improved due to augmenting them, but I fail to understand why that should affect the performance on the positive class which had given me good results prior to augmentation.
Am I missing something here? Could someone explain to me the possible reasons for this?
","['machine-learning', 'deep-learning', 'natural-language-processing', 'text-classification', 'data-augmentation']",
Are there techniques for estimating optimal neural network size?,"
Are there techniques for estimating optimal neural network size?
To replicate ""AND gate"", one does not need 1e1000 nodes in hidden layer. What would be the metric hinting at ""too much nodes""?
","['neural-networks', 'hyperparameter-optimization']",
What models/algorithms besides variational autoencoders can I use to transform a discrete input into a differentiable latent space?,"
Let's say I have a discrete input and want to transform it into a differentiable latent space. What models/algorithms besides variational autoencoders can I use?
","['natural-language-processing', 'algorithm-request', 'variational-autoencoder', 'model-request']","
I think any kind of matrix factorization technique would do the trick.
You can find a lot of resources related to natural language processing about it. Of course everything is applicable outside the NLP realm.
"
Can AUCROC curves be used to evaluate forecasting results for time series data?,"
I have built a time series forecasting model based on RNN.
Can I calculate AUCROC on the test set and the actual predicted values?
Example:
true_value = [1, 3, 23, 5, 8, ...]
model_predicted_value = [2, 4, 18, 6, 11, ...]

If it is possible how should I calculate the AUCROC curve?
","['machine-learning', 'deep-learning', 'testing']",
Should the number of training iterations of an instance segmentation model depend on the number of instances in the training dataset?,"
I need to train instance segmentation models on several different datasets. The datasets vary widely in how many instances each image contains.
For example:
Dataset 1: 100 images, each image has about 0 or 1 instances
Dataset 2: 100 images, each image has ~400 instances

Should I train for more iterations when learning on dataset 2?
I was trying to reason about this in terms gradient updates. I'm using Mask R-CNN, and if I understand correctly there is only 1 gradient update per iteration, and the loss for a given image is the mean loss across each proposal.
loss for minibatch = sum(mean(loss for each proposal in given image) for each image in the minibatch)

(^ please correct me if this is wrong!)
So with more instances per image, the loss from each instance will get diluted, and so I'm thinking I should train for more iterations. On the other hand, I can see the argument that having more instances which each contribute proportionally less to the overall loss naturally balances everything out, and the model should train for the same number of iterations.
I tried searching around and couldn't find any papers or posts about this.
","['training', 'loss', 'mask-rcnn', 'instance-segmentation']",
Does this property in product fuzzy logic have a name and any consequences?,"
In product fuzzy logic, the $AND$ operator of two variables $x_0$ and $x_1$ is the product $x_0x_1$.
Using the $NOT(x)$ as $1-x$, expressions for the other three minterms are easily obtained.
$$\overline{x_0}x_1 = (1-x_0)x_1$$
$$x_0\overline{x_1} = x_0(1-x_1)$$
$$\overline{x_0}\overline{x_1} = (1-x_0)(1-x_1)$$
These four expressions have the property that they sum up to $1$, as in bivalued logic.
$$ x_0x_1+(1-x_0)x_1+x_0(1-x_1)+(1-x_0)(1-x_1)=1$$
This is not the case when the conjunction is the Zadeh operator $min(x_0,x_1)$, where the sum of the four minterms happens to be a square pyramid over the unit cube, neither other possible definitions of fuzzy operators.

Does this property have a name, and any consequences?
","['terminology', 'fuzzy-logic']",
What is the difference between fitted Q Iteration algorithms and traditional off-policy Q-learning algorithms?,"
I am unclear about what the difference between FQI-type algorithms and traditional Q-learning algorithms is. Is the only difference that FQI methods are not sequential, in the sense that they except a tuple $(s, a, s')$, rather than 'waiting' for the behaviour policy to supply the next action, as defined in the Q-learning (off-policy control) for estimating $\pi \approx \pi_{*}$ of Reinforcement Learning: An Introduction (Sutton and Barto)?
Assuming the above is true, would the algorithm proposed by Mnih et al. (2013) (Playing atari with deep reinforcement learning) be technically classed as a FQI type algorithm?
","['reinforcement-learning', 'comparison', 'q-learning']",
Training tricks to improve stability of mixed precision,"
I would love to be able to use automatic mixed precision more extensively in my training, but I find that it is too unstable and often ends in NaNs. Are there any general tricks in training that people here have used to improve stability?
I've seen the following general tips:

plot the gradients and force unstable layers to fp32
bump weight decay in the optimizer
bump epsilon in the optimizer
add/try different normalization layers
force loss calculations to fp32

","['training', 'precision']",
"How to get meaningful vector embeddings for (lat, long) points and also GPS trajectories?","
I have a data that consists of approx. 1.5M taxi trips in Porto, Portugal. (from: https://www.kaggle.com/competitions/pkdd-15-taxi-trip-time-prediction-ii/overview) Each of these trips have it's GPS trajectory traveled along as a feature, looking like [(41.146254, -8.613972), (41.146479, -8.614476)..., (41.163048, -8.583984)] (list of (lat, long) pairs). I would like to embed each of the points in a trajectory and the trajectory itself separately. My ultimate goal is to concatenate the embeddings with the embeddings of other features for each row in the dataset to a vector to perform clustering on it.
I am looking for something like word2vec, glove, but for geographical domain. Until now, I have not found any benchmark method. I have found open source works like GeoVec (https://arxiv.org/pdf/2108.13092.pdf), which shares vector embeddings for different OSM entities, however, I would probably have some points in my dataset which are not listed in the embeddings, as these are random points on a street a taxi traveled along and for which there is no OSM entity. Also, this approach would only solve the task of point embedding but not trajectory embedding. I am programming in python.
","['machine-learning', 'python', 'clustering', 'embeddings', 'representation-learning']",
Is it possible to train a neural network or a classifier on SIFT keypoints and descriptors?,"
Is it possible to train a neural network or a classifier on SIFT keypoints and descriptors? I am working on a project which involves detecting an object (a bottle of energy drink) in an image and then detect keypoints and descriptors of that object. Then take another image and repeat the process, and once the descriptors and keypoints are detected, they are matched with the descriptors detected in previous step using FLANN matcher. What I am seeking now is that: is it possible to train a neural network on these descriptors with a class assigned to them and instead of FLANN matcher use that trained model. So that In second step, instead of using FLANN, I could just pass the descriptors to the trained model and get the class?
","['neural-networks', 'machine-learning', 'classification', 'python', 'sift']",
Image classification problem with multiple right classes,"
I have a use case where the model needs to detect fabricdefects. There are 15+ different kinds of defects. In one image there can be multiple defects present. The straight forward solution for this should be a multilabel model from my understanding. The classification of the data for a multilabel model is extremely tedious and errorprone.
Now I use a multiclass model, which seems to produce ok results. A multiclass model has only one right output class. My goal is to add one image to multiple classes and if the model predicts one of these classes it should result in a correct prediction (or lower loss output).
For example if an image contains defect1 and defect2 the model should look at both these outputs and calculate the loss from the one that has the highest probabilty.
Now my question: ""Is it possible to have a model where the samples can have multiple right output classes, but are not penalized by predicting only one right class?""
","['classification', 'image-recognition', 'objective-functions', 'multiclass-classification', 'multi-label-classification']","
This is a good question.  There are definitely good reasons for wanting a loss function that evaluates whether at least one of the classes was picked up by the model.
To do what you are attempting, I believe you will have to write a custom loss function because this is sort of a ""special request"" kind of thing.  I do not think any built-in loss functions in Tensorflow/Keras can do this, and I'm not really adept with other ML packages.
This how such a custom loss function could potentially work:

Once you have set a threshold for calling positive for each of the
classes, you can convert your probability vector to a prediction
vector of $1's$ and $0's$.

Then, you can do an elementwise check for equality to the ground
truth (y_pred == y_true) and sum over all classes for which the defect is actually present (i.e., include $1 = 1$ but not $0 = 0$ in your tally).

If the sum is $>= 1$, then you know that the model found at least one
defect correctly ($y_{pred} = 1$).  Otherwise, the model did not predict the presence of any defects correctly ($y_{pred} = 0$).

You will have to sum over your original $y_{pred}$ vector, and if the sum $> =1$ (at least $1$ defect present), then the new $y_{pred} = 1$; otherwise, $y_{pred} = 0$.

Now, you have a new binary classification problem:  Predict if defect is present or absent.  You should be able to use binary crossentropy loss or another suitable loss function.


Here is a helpful page to get you started on custom loss functions in Keras.  Secondly, I'm sure you know this already, but it will be important to make sure that your classes are balanced after this ""data transformation"". Finally, you can set up this problem a little differently such that you have 3 possible outputs:  1) correctly predicts at least 1 defect, 2) correctly predicts the absence of defects, and 3) makes no correct predicts.
Good luck.  Hope that helps.
"
Is Diffusion model instable during the training? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 8 months ago.







                        Improve this question
                    



I ran an experiment using a diffusion model (SR3 model) and got good results (experiment using jupyter notebook). Then, after running the same notebook many times with the same configuration, I did not get the same results.
Is there any explanation for this? Why I did not get the same result for the diffusion model by running the same notebook with the same configurations?
","['neural-networks', 'deep-learning', 'python', 'generative-model', 'diffusion-models']",
RNN Time-series forecasting hidden state,"
I'm starting with RNNs for time series forecasting and can't seem to understand how RNN/LSTM/GRU predict future values in time series data.
Assuming one sample per batch consisting of a sequence of length T for p predictors. I want to predict the value y(T+1) using the last T values of x(t) = (x$_1$,...,x$_p$), with t = 1,...,T.
I understand how BPTT updates the hidden state from $\textbf{h}_{t-1}$ to
$\textbf{h}_t = \phi\left(
\textbf{W}_1\mathbf{x}(t)+ 
\textbf{U} \textbf{h}_{t-1} + 
\textbf{b}
\right)$, or something more elaborated for other architectures.
As I understand the hidden states are updated up to $t=T$ and then I can predict an output using the last layer of the last hidden state: $\hat{\textbf{y}}_T = \textbf{W}_2 \textbf{h}_T + \textbf{b}$.
The issue is that I'm calculating $\textbf{h}_T = f(\textbf{h}_{t-1},\textbf{x}_T)$ and not $\textbf{h}_{T+1}$. However, the loss compares the prediction to the actual ""label"" of the sequence, which in my case is $\textbf{y}_{T+1}$. Is this the reason why the network forecasts?
Are just $\textbf{W}$ and $\textbf{U}$ updated to match $\textbf{y}_{T+1}$ but I'm never actually calculating the actual hidden state at that time?
","['machine-learning', 'time-series', 'forecasting']",
A comprehensive list of machine learning algorithms,"
I could not find a comprehensive list of machine learning algorithms online.
Can we try to make one here and improve and update it as needed?
There are challenges in the algorithm definitions and how to group and organize them, but my preliminary list (built from e.g. 1, 2, 3), is given in my answer below.
",['machine-learning'],"
Supervised learning

ANOVA
Averaged one-dependence estimators
Artificial neural network

Convolutional neural network
Extreme learning machine
Feedforward neural network
Logic learning machine
Long short-term memory
Recurrent neural network
Self-organizing map


Bayesian networks
Boosting
Case-based reasoning
Conditional random field
Decision tree algorithms

C4.5 algorithm
C5.0 algorithm
Chi-squared automatic interaction detection
Classification and regression tree
Conditional decision tree
Decision stump
Decision tree
ID3 algorithm
Iterative dichotomiser 3
Random forest
SLIQ


Ensembles of classifiers

Bootstrap aggregating
Boosting


Gaussian process regression
Gene expression programming
Group method of data handling
Inductive logic programming
Information fuzzy networks
Instance-based learning
K-nearest neighbour
Lazy learning
Learning vector quantization
Linear

Elastic-net
Lasso
Linear discriminant analysis
Linear regression
Logistic regression
Multinomial logistic regression
Naive bayes classifier
Ordinary least squares
Passive aggressive algorithms
Perceptron
Polynomial regression
Ridge regression / classification
Support vector machine


Logistic model tree
Minimum message length

Analogical modelling
Nearest neighbour algorithm


Ordinal classification
Probably approximately correct learning
Quadratic classifiers
Random forests
Ripple down rules
Symbolic machine learning

Semi-supervised learning

Active learning
Co-training
Graph-based methods
Generative models
Low-density separation
Transduction

Unsupervised learning

Association rule learning

Apriori algorithm
Eclat algorithm
FP-growth algorithm


Auto-encoders
Cluster analysis

BIRCH
Conceptual clustering
DBSCAN
Expectation-maximization
Fuzzy clustering
Hierarchical clustering
K-means clustering
K-medians
Mean-shift
OPTICS algorithm
Single-linkage clustering


Dimensionality reduction

Canonical correlation analysis
Dynamic mode decomposition
Factor analysis
Feature extraction
Feature selection
Independent component analysis
Linear discriminant analysis
Multidimensional scaling
Non-negative matrix factorization
Partial least squares regression
Principal component analysis
Principal component regression
Projection pursuit
Sammon mapping
T-distributed stochastic neighbour embedding


Expectation-maximization algorithm
Generative topographic map
Information bottleneck method
Manifold learning
Vector quantization

Reinforcement learning

Deterministic policy gradient
Learning automata
Proximal policy optimization
Q-learning
Soft actor-critic
State–action–reward–state–action
Temporal difference learning
Trust region policy Optimization

Other

Bayesian belief network
Bayesian knowledge base
Deep belief networks
Deep boltzmann machines
Deep neural networks
Discrepancy modelling
Gaussian naive bayes
Generative adversarial network
Hierarchical temporal memory
Knowledge-enhanced machine learning
Markov models
Multinomial naive bayes
Neural style transfer
Physics-informed machine learning
Sparse identification of nonlinear dynamics
Transformer

"
Can a convolution learn to generate fine details? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 8 months ago.







                        Improve this question
                    



I'm trying to get a convolutional autoencoder to reconstruct images of a dataset with crisp details.
I've read in a couple places that convolutional autoencoders ""naturally produce blurry images"". This makes sense to me when the images are complex and the bottleneck is small : necessiraly, the autoencoder is learning some lossy compression, so you should expect to lose some details. However, I find it more surprising when working with very simple images.
I created a toy dataset composed of images representing circles, which vary only following three (randomly sampled) parameters : position (x,y) and pixel value. I'm trying out a very simple autoencoder, just to test the blurriness/crispness of details. It's just composed of a downsampling, an upsampling, and a convolution :
Code in Pytorch is :
# ""Encoder"" part (just a spatial downsampling)
nn.AvgPool2d(kernel_size = 2)
# Decoder part
nn.Upsample(scale_factor=2, mode = ""bilinear""),
nn.Conv2d(1, 1, 3),

And I train the network on my dataset to reconstruct the original images, with MSE loss. Here is an example of an original image (left), and its reconstructed output at network convergence (right) :

Even though the image is very simple and loss is pretty low, the image is not crisp. I'm guessing this effect becomes even worse when stacking multiple convolutions, downsampling several times in a row, and adding a dense layer as the bottleneck (as in ""real"" autoencoders).
So I wonder whether part of the bluriness associated with convolutional autoencoders stems from the fact that convolutions are not a great tool to produce crisp details. Is it the case ? Is there another way to upsample images through the decoder (for example, I used to use transposed convolution instead of upsample + conv, and image quality has improved with this change. Is there another similar ""step forward"" ?) If convolutions are inadequate, how come other generating architectures (modern GANs) are said to be able to produce more crisp images ? Don't those also ultimately rely on convolutions ?
","['autoencoders', 'generative-model', 'convolution', 'upsampling']",
When do we use the neural network to predict value during the expansion stage of MCTS in the AlphaZero algorithm?,"
According to what I understand from the AlphaZero algorithm, a neural network is used to set value and prior probability for a node during the expansion stage of MCTS. On the other hand, according to the rules of the game, we need to set the value equal to +1 a win, -1 for a loss, or 0 for a draw.
Question: When do we use the neural network to predict the value and when do we use the rules of the game to set the value during the expansion stage of MCTS in AlphaZero?
","['alphazero', 'alphago-zero', 'alphago']","
The AlphaZero algorithm uses the neural network for non-terminal states and the game rules for terminal states when determining the value $v$ of a node during MCTS expansion. The supplementary material of the paper (arXiv link) states the following (see Domain Knowledge, item 2):

AlphaZero is provided with perfect knowledge of the game rules. These are used during MCTS, to simulate the positions resulting from a sequence of moves, to determine game termination, and to score any simulations that reach a terminal state.

During a game, AlphaZero stores the game outcome $z$ (determined by the game rules) and predicted outcomes $v_t$ of each encountered state (determined by the neural network). One aim of the loss function is to minimize the error between $z$ and $v_t$ (see equation 1 in the paper).
"
Which algorithm should I use to determine if users would enjoy various restaurants?,"
Lets say I have lots of location-based and demographic data on a user. I know their age, what restaurants they visit most often, what they have rated them on Yelp, the type of food & ambiance they prefer, the price range of places they visit, and similar data for all of their friends.
I want to calculate if a user would attend/enjoy a jazz bar called ""Soul House"" based on these data points (probability score or binary response are both fine). If possible, it would be even better is this algorithm could ALSO be used to provide recommendations to the user (but this is not a requirement, as I could use a different algorithm for that feature).
The catch here is that we assume there are millions of users and thousands of restaurants! This algorithm would need to be performed for MOST of these users given SEVERAL restaurants.
I have considered using the following AI algorithms and am unsure which would be the best for this purpose? Would any of these options be bad? Are there any I am missing?
1.) Bayesian Learning
Each data point would be a node in a bayesian network graph and we would use bayesian learning to calculate the probability that any given user would attend a restaurant. The issue I see with this approach is that a bayesian network graph would have to be constructed and maintained for thousands or millions of restaurants and venues.
2.) Matrix Factorization Or ""User-User Algorithm""
We compare users and see how users with similar data points rated/enjoyed a restaurant. This seems like a good approach but would be useless until I have built up months worth of user data from many people. I would prefer something I could use sooner.
3.) Neural Network (Perceptron Learning)
This one might be a stretch but perhaps we could feed user data into a neural network as input points alongside a restaurant as another input and after passing through several hidden layers would arrive to either a ""yes"" or ""no"" to whether a user would likely attend or not. One concern with this approach is I have no idea where I would find training data... so I feel like it is out of the question.
Are there any others I am missing?
","['ai-design', 'algorithm-request', 'recommender-system']",
Why should one expect the backward pass to take twice as long as the forward pass?,"
I have seen it stated that, as a rule of thumb, a backward pass in a neural network should take about twice as long as the forward pass.  Examples:

From DeepSpeed's Flops Profiler docs, the profiler:


measures the flops of the forward pass of a module and the flops of the backward pass is estimated as 2 times of that of the forward pass


Page 7 of Jared Kaplan's Machine Learning notes in which it is claimed that the backward pass requires twice the number of matrix-multiplies needed in the forward pass, for a vanilla neural network.

I unfortunately don't understand the argument made in Kaplan (not sure where the ""two"" in the ""two matrix multiplications per layer he refers to"" comes from).
In particular, any such rule would also seem to be very implementation dependent, depending on whether local gradients are computed and cached during the forward pass, for instance.  But I guess there is a standard implementation of backprop that makes this unambiguous?
If anyone can expand on the logic behind this lore or point me towards other references, I would be grateful.
","['neural-networks', 'backpropagation', 'deep-neural-networks']","
The ""two"" in the ""two matrix multiplications per layer"" has nothing to do with any cached value computed in the previous feedforward pass (in fact forward pass doesnâ€™t need any local gradient) where it only needs ""one"" matrix multiplication per layer denoting the term $Wx$ as shown in the standard input/output vectorial equation $a=\phi(Wx+b)$ in each layer. Thus it may be easier now to understand that for backprop applying automatic differentiation starting from the last layer to calculate derivatives of loss function w.r.t. each weight parameter in the current layer, it needs two matrix multiplication as shown in equation (1.4.6) in your second reference. The first multiplication denotes $\phiâ€™ \times \phi$ where the former is a column vector and the latter is a row vector and its result is a matrix. And the second matrix multiplication occurs inside the calculation of $\phiâ€™$ which is essentially an error factor vector of the input activation vector from its previous hidden layer obtained via output target error vector multiplied with a diagonal matrix of the last layerâ€™s activation functionâ€™s derivatives at each nodeâ€™s respective net input value during the forward pass if you go through the details of backprop.
"
RL - Can RL be applied to problems where the next state is not the next observation?,"
I'm quite new on the study of reinforcement learning, and Im working on a communication problem with continuous large actions range for my final graduation work. I'm trying to use Gaussian Policy and Police Gradient methods for that implementation. I will try to explain the sequential logic of the task to better understand...
From the current observation (o_t), the agent takes actions reaching a new state (s_t+1). This new state is indeed a function of the previous obersvation (o_t) and the actions (a_t) taken by the agent. However, this next state (s_t+1) is not the next observation that the agent will take into account to take new actions, being only useful for calculating the reward in that iteration for the policy optimization. My question consists of the possibility that RL can be applied to this type of problem, where the state reached by the agent's actions in a given instant of time is not my next observation that the agent will take into account to take the next actions.
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'continuous-action-spaces', 'continuous-state-spaces']","
Yes, there are RL algorithms (dreamer v1,2,3) that predict world model to simulate what (h_t+1) given its sampled action and previous state (h_t). It runs simulated world model during critic-actor so it can be much more data efficient. https://arxiv.org/abs/2301.04104v1
"
"Latest status on stability of CNN architectures to noise and ""clever hans"" issues","
I was working with CNN architectures for image segmentation a few years ago, and I remember there was a big concern about on the stability of the predicted masks due to the introduction of small amounts of noise. If just a little gaussian noise was added to the image, then the predicted mask might change in important ways--generally around the margins.
Similar issues arose in classification problems, where MRI images were being classified based upon external queues within the images, and not based upon the images themselves--the ""clever hans"" problem as it is called. For example, sometimes the training images hospital labels that implicitly meant that an image was ""positive"" for some illness. Hence the CNN was using the label for detection instead of the data contained in the image. Here is a link to a 2022 paper that discusses the continuing challenge of Clever Hans problems in MRI images. There were also issues where the CNN classification of images did not generalize beyond the specific machines that the training images came from.
I was just wondering what the current status of these stability issues was within CNN architectures, or probably across Deep Learning architectures. In particular, are there any new architectural changes to help CNNs or deep learning architectures overcome these issues? Are there newer training methods, etc. For example, do contrastive learning methods improve on the stability of CNN predictions, etc. I am most interested in medical imaging, but input from any topical area is welcome.
","['convolutional-neural-networks', 'image-segmentation', 'stability']",
How to determine if a decision tree is the (globally) optimal tree?,"
BACKGROUND:  When constructing decision trees, the features are selected at various nodes based on whether it optimally splits the samples at that level (i.e., locally) using some user-chosen metric to judge ""optimalness"" (e.g., Gini index, Information gain, Entropy).  My understanding is that this is what is called a greedy algorithm, meaning that it optimizes locally but not necessarily globally.  As a consequence, the algorithm does not guarantee that the final tree is going to be the best tree.  Nonetheless, it would be desirable to derive the best tree or, at the very least, know if we have derived the best tree (even if we have no procedure for deriving the best tree).
QUESTION:  How to determine if a decision tree is the (globally) optimal tree?
","['optimization', 'algorithm', 'decision-trees']","
Decision tree for classification is essentially Boolean function approximator which is usually equivalent to a MLP neural network. Thus the usual Gini index, entropy based uncertainty reduction methods to select and order test attributes is akin to optimize MLP parameters based on the same training set. If the size of your training set is relatively small compared to your future test set, they both face the same common overfitting issue. But nonetheless you can always convert to its equivalent MLP form for “optimality”.
If you stick with decision tree method there’s no rigorous mathematical proof for your hoped ""globally optimal"" tree if your training set is relatively small or incomplete w.r.t your test set, and usually Gini index metric is more efficient to compute in most cases unless in some highly unbalanced data sets where one of the classification category is relatively unlikely and entropy formula can amplify attributes of such rare category. You may further read this 2004 paper summarizing theoretical optimality comparison between Gini index and Information Gain metrics for decision tree induction with the conclusion:

Based on these characterizations we were able to
analyze the frequency of agreement/disagreement of the Gini Index function and the Information Gain function. We found that they disagree only in 2% of all cases, which explains why most previously published empirical results concluded that it is not possible to decide which one of the two tests performs better... Based on the gained deeper insights on the split process we are currently working on a system, which will select the optimal criterion based on a user defined optimality criterion. Preliminary results can be found in [20].

And in the mentioned reference therein about their proposed family of split functions the authors also confirmed:

The tests have shown that the two popular functions are very sensitive to the variation of the training set sizes and therefore the quality of the inferred trees is highly dependent on the training set size. At the same time however, we were able to show that the simplest members of the introduced family of split functions behave in a very predictable way and, furthermore, the created trees were superior to the trees inferred using the Gini Index or the Information Gain based on our evaluation criteria.

One common approach in such case is using the idea of cross validating your existing training set, to divide it into multiple equal-sized subsets. Then for each subset to train your tree based on your chosen metric using the remaining sets and calculate some error rate using the said subset as test set. Finally to average all the error rates and you can get some idea of the degree of your “global optimality” for your specific candidate tree. Of course if you have several similarly performed trees which are all more or less uncorrelated, you can always use ensemble idea to do voting such as random forest.
"
What is the best way for learning an output matrix from different size of input vectors and matrix?,"
What is the most effective method for learning the NXM matrix (for each sample) in relation to different size input vectors and matrices?
To make it more clear:
I have a 56x32 output matrix for my first sample, and the corresponding input set is five different vectors and matrices, with their sizes as follows:
1x18, 1x18, 96x18, 96x18, and 1x56
Their size is always the same for every sample, but I am completely lost as to how to arrange the input and output. How can I arrange these different-sized vectors and matrices to learn a matrix output?
The number of samples will be at least 1000, maybe more if it is possible.
How can I learn about a settlement, or more importantly, can I learn at all? How do you solve this problem?
(Ps: The values are mostly float numbers)
Best
","['machine-learning', 'forecasting', 'input-layer', '3d-convolution', '2d-convolution']",
What type of neural network architecture allows filtering out of unwanted sounds?,"
I have a use case where I will be inputting audio to a model, and the output of the model will be the same audio except with certain sounds removed (volume set to zero).  The dataset is generated by taking an audio file, duplicating it, and then zeroing out the unwanted sounds (usually a half second long).
I believe a neural network architecture is needed here with the input being the undisturbed audio and its spectrogram.  The output is then the modified/cleaned audio.
What model architectures would work for this use case?  I would potentially like to have this run real-time as a person is speaking.
","['convolutional-neural-networks', 'transformer', 'time-series', 'audio-processing']","
Since you say that you ""believe that a neural network architecture is needed here..."", I am assuming that you are open to other options.  This approach doesn't utilize a neural networks, but I think it can potentially get the job done (with some caveats).  One approach is to

First isolate the unwanted sound segments that you want to ""subtract""
Calculate the Fourier transform of each unwanted sound segment
Determine the components with the highest amplitudes in each unwanted sound segment
Determine if these dominant peaks have shared frequencies across the various unwanted sound segments
If so, this spectrum of frequencies is the ""signature"" of your unwanted sound
Once you have that, you can simply subtract those frequencies from your full sound recording to remove the unwanted sound
This is done by finding the Fourier transform of the full sound recording
Subtracting the frequencies (i.e., the signature) of the unwanted sound
Transforming back to the time domain to get the signal minus the unwanted sound

As implied, for this approach to work, the unwanted sounds will have to have a consistent set of dominant frequencies (i.e., a signature).  If you have multiple ""types"" of unwanted sounds, then this procedure can be repeated for each type (i.e., find the signature for each type).  If you have multiple types of unwanted sounds and you cannot distinguish them, a neural network may then be useful.
You will also have to make sure that those frequencies are not a significant component of your desired signal.  Otherwise, you will be removing an important part of your desired signal.  To do that, just follow the same procedure as described for the unwanted signal.
This code from MATLAB describes one possible implementation (albeit for a denoising application).
"
PPO: dealing with variable episodic length,"
I'm dealing with a project that has episodes of variable length raging from just 3 steps to 20 steps. Now, I'm guessing that this may cause problems with GAE, as actions in large episodes will have much larger advantages than actions in smaller episodes simply because of the cascading addition of future rewards/costs. Is there some smart way of dealing with discounted future returns in such scenarios? Thank you.
","['proximal-policy-optimization', 'discount-factor', 'episodic-tasks']","
Generally you can make an agent care less about rewards in the future by decreasing the discount factor $\gamma$.

that this may cause problems with GAE, as actions in large episodes will have much larger advantage

Depending on the particular task you are solving this might not be a problem. If a longer episode gives an agent a higher expected return, then the agent should learn to prolong the episode. Or if the episode length is independent of the agent's actions, the agent should still learn to act optimally with respect to the random distribution of episode length.
"
Reinforcement Learning - Independence between current state and future state,"
I'm working on a real problem with continuous large actions range, where my agent takes actions based only on the current state of the environment, transitioning to a future state that is unrelated to the next current state, the only thing to take into account is the reward generated by the action taken by the agent. Is there any RL method that applies to this kind of problem?
So, clarifying the problem better... From the current observation (s_t), the agent takes actions reaching a new state (s_t+1). This new state is indeed a function of the previous state (s_t) and of the actions (a_t) taken by the agent. However, this next state (s_t+1) is not the next observation that the agent will take into account to take new actions, being only useful for calculating the reward in that iteration. My question consists of the possibility that RL can be applied to this type of problem, where the state reached by the agent's actions in a given instant of time is not my next observation that the agent will take into account to take the next actions.
","['reinforcement-learning', 'deep-learning', 'policy-gradients']",
"In a neural ODE, if $f$ is represented by a neural network, how does it depend on $t$?","
In a neural ODE $$\frac{d h(t)}{d t} = f(h(t), t, \theta),$$ we call $f(h, t, \theta)$ the neural vector field. If $f$ is represented by a neural network, how does it depend on $t$?
If $t$ is an integer, $f(h(t), t, \theta)$ should be the intermediate output of the $t$th layer. But what if $t = 1.5$?
","['neural-networks', 'residual-networks']",
computational complexity for batch normalization technique [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 8 months ago.







                        Improve this question
                    



Could you please let me know weather it is possible to have a computational complexity formula for batch normalization technique or not? If someone can help me in this regard I will be appreciated.
","['batch-normalization', 'computational-complexity']","
Batch Normalization is achieved through a normalization step that fixes the means and variances of each layer's inputs. For a Batch of size N, computing the mean and variance will have a complexity of O(N) per layer. Actual normalization can be considered an operation of O(1) with parallelism or vectorization (even without that, re-centring and re-scaling will be O(N) operations.)
Reference: Wikipedia
"
What are common benchmarks (simulators and datasets) for testing compositionally in the visual domain?,"
Learning an object decomposition from a visual scene is a difficult problem for language models describing the scene, for radiance fields that reconstruct the scene, and also for generative models producing the scene. What simulators and datasets exist for producing data to test compositionally in the visual domain?
One example is the Clevr dataset.

","['natural-language-processing', 'computer-vision', 'datasets', 'generative-model']",
Is model-based RL better suited for domain shift then model-free RL?,"
My intuition is that richer representations can be used for a larger number of downstream tasks and that model-based RL is more suited to produce such representations. Is there empirical work that this is true? Is model-based RL better suited for domain shift than model-free RL?
","['reinforcement-learning', 'deep-rl', 'model-based-methods']",
LSTM with time-series data transform,"
I have a time series data which has distinct time steps. For example, one time series data was recorded with the 1/30(min) time step, but some other data may have 1/15(min), 1/6(min), 1/5(min) time steps. Also, my data is very limited; only have 130.
In such a situation, to increase my data and make a robust model, I just generate another time series data based on others; for example, if I have a time series with 1/30 (min) time step, then I can create new time series data with 1/15, 1/6, or 1/5 time steps by deleting some rows consistently. In this way, I generate 500 more time series data. Moreover, this method works pretty well on my LSTM model's time series classification.
My question is, is the way I did (increase my data by generating another time series with different time steps) justifiable or not? I'm newbie on the data science, so I'm worrying that maybe my LSTM model just cheat some patterns in a time series data with different time step.
Any advice is welcome!
","['long-short-term-memory', 'time-series']",
DDQN Snake keeps crashing into the wall [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 8 months ago.







                        Improve this question
                    



Edit:
I managed to fix this by changing the optimizer to SGD.
I am very new to reinforcement learning, and I attempted to create a DDQN for the game snake but for some reason it keeps learning to crash into the wall. I've tried changing the hyperparamters e.g gamma, batchsize, max memory size, and learning rate, and I still seem to get the same result.
The code is in LuaU and is quite long, but if you want to see it here is a link to the github: https://github.com/joejoemallianjoe/DDQN-Snake/blob/main/very%20long%20script.lua
Graphs of loss and reward after around 650 episodes of training or so:


Input structure:

apple z > head z
apple z < head z
apple x > head x
apple x < head x
is there an obstacle in front of snake?
is there an obstacle to the left of snake?
is there an obstacle to the right of snake?
is the snake going up?
is the snake going down?
is the snake going right?
is the snake going left?

Reward Structure:

-10 for crashing into the wall
+10 for getting the apple
-1 for moving away from the apple
+1 for going towards the apple

Hyperparameters / Network configuration

Gamma is 0.95
Epsilon decay is 0.995
Target network update frequency is 32
Learning rate is 0.001
Batch size is 64
Maximum memory size is 10,000
Optimizer is SGD with momentum, momentum = 0.99
Loss function is MSE
The network outputs Q values for going up, down, right, or left

Here is a GIF showing the behavior:

","['reinforcement-learning', 'machine-learning', 'deep-rl', 'dqn', 'double-dqn']",
How can an ensemble be more accurate than the best base classifier in that ensemble?,"
BACKGROUND:  Ensemble classifiers are said to reduce bias by taking an ""average"" of predictions of several base classifiers that comprise the ensemble.  However, I am uncertain if this necessarily means that they can increase accuracy.  My intuition tells me that the ensemble classifier should perform no better and possibly even worse than the best base classifier in the ensemble.  This seems especially true for bagging approaches which use strong classifiers anyway.  When you have a ""star performer"", it just doesn't seem to make intuitive sense to ""dilute"" its performance with subpar performers.
Nonetheless, from my novice-level reading, it seems that ensembles can be as good or possibly even better than all of the individual base classifiers, but I'm still not clear why.
QUESTION:  How can an ensemble be more accurate than the best base classifier in that ensemble?
",['ensemble-learning'],
"Instead of accumulating the gradient, can we accumulate loss values?","
I have read and used Gradient Accumulation as a method to handle large batch size on smaller memory restrictions. It is described as following:
for step, eachBatch in enumerate(dataloader):
   ... 
   loss = loss_func(ytrue, ypred)
   loss.backward()
   if step % 5 ==0: 
      # update weights every 5 steps
      loss.step()
      loss.zero_grad()

However, instead of accumulating the gradient, can we accumulate loss values? Just like multi-task trainings, but I have never seen people recommending the following method, but wonder why it that the case, what are the potential problems with this. If there really exist a problem with the following method, then why does loss accumulation work for multi-task training?
totalLoss = 0
for step, eachBatch in enumerate(dataloader):
   ... 
   loss = loss_func(ytrue, ypred)
   totalLoss += loss # accumulate the loss
   if step % 5 ==0: 
      totalLoss.backward()
      totalLoss.step()
      totalLoss.zero_grad()
      totalLoss=0

","['neural-networks', 'deep-learning', 'computational-complexity', 'multi-task-learning', 'space-complexity']","
Accumulating the loss like that doesn't improve the memory requirements, because the memory consumption depends on the size of your computational graph. In other words, each time you add a term to the loss, the overall function of your loss grows as well, and with it grows the memory consumption.
Specifically, every time you add a term to the loss value, you run the forward step with a new input/output pair $x_i; y_i$. In order to backpropagate, all the hidden activations $h_{i,l}$ must be stored until you call loss.backward(). So with each new loss added, that cache of latent activation vectors grows. Check out PyTorchs walkthrough of what exactly happens there.
In contrast, when you have already computed the gradient, then the cached activation vectors can be deleted and the only things that are saved are the gradient values for your model parameters. In comparison, accumulating the loss would lead to a memory consumption of $\mathcal{O}(n * I)$ and accumulating gradients is $\mathcal{O}(n + |\theta|) \sim \mathcal{O}(n)$, where $n$ is your single-pass memory requirement, $I$ is the number of accumulation steps, and $|\theta|$ is the number of model parameters. I hope that was somehow clear.
"
How can I encode angle data to train neural networks?,"
I am training a neural network where the target data is a vector of angles in radians (between $0$ and $2\pi$).
I am looking for study material on how to encode this data.
Can you supply me with a book or research paper that covers this topic comprehensively?
","['neural-networks', 'reference-request', 'training', 'datasets', 'data-preprocessing']","
The main problem with simply using the values $\alpha \in [0, 2\pi]$ is that semantically $0 = 2\pi$, but numerically $0$ and $2\pi$ are maximally far apart. A common way to encode this is by a vector of $\sin$ and $\cos$. It perfectly conveys the fact that $0 = 2\pi$, because:
$$
\begin{bmatrix}
    \sin(0)\\
    \cos(0)
\end{bmatrix}
= 
\begin{bmatrix}
    \sin(2\pi)\\
    \cos(2\pi)
\end{bmatrix}
$$
This encoding essentially maps the angle values onto the 2D unit circle. In order to decode this, you can calculate
$$\text{atan}2(a_1, a_2) = \alpha,$$
where $a_1 = \sin(\alpha)$ and $a_2 = \cos(\alpha)$.
Here is a nice detailed explanation and here are two references, where this is applied:

Heffernan et al.
Pizzati et al.

EDIT As it was noted in the comments: The values $\sin(\alpha)$ and $\cos(\alpha)$ are not independent and the following naturally holds: $\sqrt{\sin(\alpha)^2 + \cos(\alpha)^2}= 1$, i.e. the euclidean norm is one. In a situation where your Neural Network predicts the sin and cos values, this condition isn't necessarily true. Therefore, you should consider adding a regularization term to the loss that guides the neural network toward outputting valid values (with unit norm) which could look like this:
$$
r_\lambda\left(\hat{y}_1, \hat{y}_2\right)\; = \lambda \left(\; 1 - \sqrt{\hat{y}_1^2 + \hat{y}_2^2}\right),
$$
where $\hat{y}_1$ and $\hat{y}_2$ are the sin and cos outputs of the network respectively and $\lambda$ is a scalar that weights the regularization term against the loss. I found this paper where such a regularization term is used (s. Sec. 3.2) to get valid quaternions (Quaternions must also have unit norm). They found that many values work for $\lambda$ and they settle for $\lambda = 0.1$
"
What is the difference between prompt tuning and prefix tuning?,"
I read prompt tuning and prefix tuning are two effective mechanisms to leverage frozen language models to perform downstream tasks. What is the difference between the two and how they work really?
Prompt Tuning: https://aclanthology.org/2021.emnlp-main.243/
Prefix-Tuning: https://arxiv.org/abs/2101.00190
","['deep-learning', 'fine-tuning', 'pretrained-models', 'few-shot-learning', 'zero-shot-learning']",
"Are my attempts to write ""There exists a student studying all the subjects of the information technology subject"" in FOL correct? [closed]","







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 8 months ago.


The community reviewed whether to reopen this question 8 months ago and left it closed:

Original close reason(s) were not resolved






                        Improve this question
                    



I have the following sentence, which I need to write in FOL

There exists a student studying all the subjects of the information technology subject

I don't know how $\forall$ can be combined with the $\land$.
It was written by my teacher as follows
$$\exists x, \forall y: \text{student}(x) \land \text{learn}(x, y) \land \text{ITsubject}(y)$$
However, I was thinking this sentence should be written like this:
$$\exists x \text{student}(x) \land \forall y(\text{ITsubject}(y) \rightarrow \text{learn}(x, y))$$
or maybe
$$\exists x \text{student}(x) \land \forall y(\text{learn}(x, y) \rightarrow \text{ITsubject}(y))$$
Is this right?
","['logic', 'knowledge-representation', 'knowledge-based-systems']",
How can validation accuracy be more than test accuracy?,"
I have been trying to implement DenseNet on small dataset using k-fold cross validation. Training accuracy is 94% ,validation accuracy is 73% whereas test accuracy is 90%.I have taken 10% of my total dataset as test set. I know some overfitting is present, but how can  validation accuracy be greater than test accuracy?
","['deep-learning', 'convolutional-neural-networks', 'cross-validation', 'dense-layers', 'test-datasets']",
How are gradients backpropogated in ALBERT?,"
I was reading the ALBERT paper and saw that they use the same parameters in each layer hence reducing the number of unique parameters. From what I could gather it seems if the all the layers have say parameters W then parameters of different layers would be updated differently which would destroy the parameter sharing.
So one way I can think of is say we have only one set of parameters W and after each layer completes it's weight update W changes to W' and the preceding layers now use W'.
Is this the right way to think about it or does something else happen under the hood?
","['deep-learning', 'natural-language-processing', 'bert']","
ALBERT (A Lite BERT) is a variant of the BERT (Bidirectional Encoder Representations from Transformers) model, which is a type of transformer-based language model for natural language processing tasks such as language understanding and machine translation. Like BERT, ALBERT uses self-attention mechanisms and transformer blocks to process input sequences and generate output representations.
In ALBERT, gradients are backpropagated (also known as ""backprop"") during the training process in order to update the model's parameters and improve its performance. This is done using the standard backpropagation algorithm, which is a widely-used method for training neural networks.
During backpropagation, the model computes the gradients of the loss function with respect to the model's parameters, and then uses these gradients to update the parameters in the opposite direction of the gradient. This helps to reduce the loss and improve the model's ability to make predictions.
In the case of ALBERT, the model's parameters include the weights and biases of the self-attention mechanisms and transformer blocks, as well as any other trainable parameters in the model. The gradients of the loss function with respect to these parameters are computed using the chain rule, and are then used to update the parameters using an optimization algorithm such as stochastic gradient descent (SGD) or Adam.
I hope this helps! Let me know if you have any other questions.
"
Keep weights of output layer in transfer learning?,"
I'm seeing conflicting info on what to do with the fully-connected output layer of a pre-trained network when it's used in transfer learning. A previous answer seems to imply that the network is kept intact and a new output layer is added on top of the existing output layer. This approach means that the new network will build on all of the pre-trained weights of the existing network, including the weights in the output layer. However, when I look at this tutorial on PyTorch, the fully-connected output layer is replaced with a new output layer to match the number of classes in the new task. This second approach means that the pre-trained weights in the existing output layer (512*1000 parameters for ResNet18) are lost. The first approach retains everything that was learned. The second approach also looks reasonable since the weights of the final output layer are like very task-specific and so can be discarded when learning a new task. Which approach is recommended in general?
","['classification', 'pytorch', 'transfer-learning', 'residual-networks']","
Transfer learning is a complete field of research, and there are multiple possibilities for what might work best in each situation.
There are various ways in which you can employ a pretrained model for transfer learning. You can indeed keep the complete model intact, but it is more common (as is done in pytorch) to delete the last (several) layers. In addition, one can vary with which model weights are frozen during training. You can, for example, delete the last layer and unfreeze the second to last, meaning you train 2 layers, but one is pretrained.
If you do not simply want to extend the model, it is usually advised to look into unfreezing (some of) the last pretrained layer(s), as it helps adjust the model to the new training data. Do not put the learning rate too high as it might result in the unfrozen pretrained layer forgetting what it had learned (usually termed catastrophic forgetting).
I personally have had more success with deleting the last 1 or 2 layers and unfreezing another 1 or 2 layers. You could look at some academic papers to see if there are any large-scale tests for what generally works better.
"
What is $p(Z)$ and what happens to the variational posterior $q(Z;X)$ during data synthesis (after training)?,"
From my understanding of inference problems, we want to compute the posterior $p(Z|X=D)$, for some observed dataset $D=(x^1, x^2,\dots,x^n)$ of $n$ independent observations, in order to ""update"" our prior $p(Z)$ for further analysis/data generation. i.e. to sample new data $x'\sim p(X)$, we would sample $z'\sim p(Z|X=D)$, and then sample $x'\sim p(X|z=z')$.
However, in the VAE, we are generating (after training) new samples $x'$ by sampling $z'\sim p(Z)=N(\bar{0},I)$. If $p(Z)$ is simply our ""prior belief"" of the distribution of $Z$, prior to observing $D$, then why are new samples not generated by sampling $z'\sim q(Z;X)$, i.e. the variational approximation of the posterior?

One justification I can think of (but am not convinced by) is the $D_{KL}(q(Z;X=x^i)||p(Z))$ term in the ELBO objective, which ensures that the samplewise posterior (for every training sample) is close to $p(Z)$.

Another justification is that if we wanted to actually sample from $q$, we would need to determine $q(Z;X=D)$ (posterior probability of Z after observing entire $D$), which I think is difficult to compute. We would instead need to find some nice (with a functional form we can sample from) ""average"" distribution $r(Z)$ that is as close (min KL) to every samplewise posterior, which would basically result in $r(Z)=p(Z)$.


One way I can think of sampling a new $z'$ from the posterior would be to sample $x\sim D$ uniformly, and sample $z'\sim q(Z;X=x)$, and generate a new $x'\sim p(X|Z=z')$, which is essentially similar to one train step. To me this is like thinking of $q(Z;X=D)$ as a uniformly-weighted mixture of $|D|$ gaussians. This however, requires storage of $O(|D|)$ parameters (either the original data points, or the |D| latent means and stds).
Can someone please help me understand if I'm thinking about this correctly? What is the role of $p(Z)$? Why do we not use $q(Z;X)\approx p(Z|X)$ after training?
","['variational-autoencoder', 'variational-inference']",
Is transfer learning effective when the new task has more classes than the original?,"
All examples of transfer learning I have seen for classification use initial weights of a network trained on a larger number of classes (say 1000 in the case of networks trained on ImageNet data) to address a new task that has a smaller number of classes. Can transfer learning be effectively used when the new task has more classes than the original? For example, can I effectively build on ResNet50 or parts of it for a new task that has 1500 classes? Thanks.
","['deep-learning', 'convolutional-neural-networks', 'classification', 'transfer-learning', 'image-net']","
Yes, transfer learning can usually also be utilized when the number of classes differs from the original. Your model will however be more 'transferable' if it has been trained on a wide variety of data or data that is somewhat similar to your new data.
What one would usually do is freeze the weights of the lower layers of the network and only retrain the upper part of the network (e.g. the fully connected part). The most important question is, thus, not how many classes each network was trained on but instead: How high is the representational distance between the old and the new dataset?
Recently self-supervised pretraining has increased in popularity, this shows that general features are useful somewhat independent of how they were acquired.
"
ANN Input vector representing a network of contests's outcomes between thousands of individuals?,"
Let's say I have 1000 participants within a tournament, each of the participants having a single ID. One instance of this tournament is a 1 vs 1 contest when one wins and the other loses.
Now let's say ID11 is facing ID200, I'd like to represent the history and outcomes of the previous contests between ID11 and all of the other participants if they faced each other (sparse). Same for ID200.
example: ID11 won/lost vs ID500, ID600, ID250, etc... ID200 won/lost vs ID600, ID400, ID800, etc... in a single vector. The idea is to be able to use the information of who beat who and therefore each ID should be their own entity to be processed.
I have thought about one-hot encoding this vector but it would become a 2000-length input just for this...
Would anyone have any insights or input or concepts that i could look into to implement such representations?
Thank you very much.
","['neural-networks', 'binary-classification', 'knowledge-representation', 'feature-engineering']",
How do transformers compare to CNNs in terms of compute budget (and computing time) during inference?,"
Transformers are data and GPU hungry during training. Is this also true at inference time? How do transformers compare to feedforward CNNs e.g., during bounding box generation at inference time? I haven't found a good comparison of computing time and computational resources.
","['convolutional-neural-networks', 'comparison', 'transformer', 'bounding-box']",
"What should I do if my validation score is good, but my test score is bad?","
I've trained my artificial neural network, and, as per standard practice, I've picked out the one neural network throughout training that did the best on my validation dataset. That is, the neural network learned from the training data, and generalized to the validation data.
However, when I run the neural network on the test data, it performs poorly. What should I do next?
From my understanding of the theoretical framework, the goal of validation is to ensure that that the network's parameters don't overfit to the training set. (If they do, we'll detect it because the validation score will be bad.) However, the goal of an additional test dataset set beyond the validation dataset is to ensure that our hyperparameters don't overfit. In most scenarios, we train multiple models with different learning rates, etc., and pick the one that does the best on the validation dataset. However, we might just be cherry picking the one that does the best for that validation dataset and doesn't generalize to a test set. So, we add an extra test set to detect if that happens.
My question is about an analogous case, except for that my hyperparameter is just the training step of the model. I picked out the model that's checkpointed as having the highest validation score. But, when I run it on the test set, it does poorly, showing that I've cherry picked the model with the highest validation score but it still doesn't generalize.
What do I do next in this scenario? Do I just follow the same advice from these questions about overfitting, or is this a special case because the model does seem to generalize to the validation dataset?
(Note: this question is different than regular overfitting because it's about hyperparameters overfitting the validation set, not regular parameters overfitting the train set. I've also looked at these guidelines but they don't seem to apply to this more general theoretical question.)
","['neural-networks', 'hyperparameter-optimization', 'overfitting', 'hyper-parameters', 'computational-learning-theory']","
Check if the CV and test are Temporaly stable or they change with time.
Additionally check if they both follow the same distribution or not
"
Unsupervised pretraining on the supervised learning training data,"
Is it ok to pre-train and train (fine-tune) the neural network on the same training data?
Here is the specific context:
I am using the TabNet model on a tabular dataset. The dataset is fully labeled. I read about the performance and optimization benefits of pre-training the network. Can I pre-train it using the same training dataset on which I would eventually train my network? Or one is not supposed to re-use the data that was used for pre-training.
If I had to guess, I would say pre-training will use my training data to learn the dependencies between the features, which will help the training model in the supervised learning portion. I'm not sure if this is statistically sound or correct.
Is what I am about to do also called self-supervised learning?
","['deep-learning', 'unsupervised-learning']",
What strategies are there to reduce the variance of the policy gradient estimator of the REINFORCE algorithm?,"
What strategies are there to reduce the variance of the policy gradient estimator of the REINFORCE algorithm?
I know one possibility is to subtract a baseline as a running average of rewards from past mini-batches. Another is to compute the mean and variance of each trajectory over one mini-batch and standardise the values. A third one is to use large batch sizes.
What is considered the most effective? What other methods are there?
","['reinforcement-learning', 'algorithm-request', 'reinforce', 'variance-reduction']",
"Using TD algorithms, if the value function of terminal states is always 0, why would a policy ever choose it?","
Temporal difference algorithms (TD($\lambda$)) are tabular solutions to reinforcement learning problems. That is, they create a table of all the states in the problem, and estimate the expected long-term reward that can be obtained from each state.
The policy used in these situations is: ""in your current state, choose to transition to the state that maximises the long-term reward"". I observe that the ""long-term reward"" of a terminal state is precisely 0, by definition (see here).
So, if the greedy policy is to ""choose the state which maximises the long-term reward"", and the ""long-term reward"" of a terminal state is 0, doesn't that mean that the policy should never choose it, even if it has an immediate reward of 1 million points?

Minimally, practically, my query can be observed in a 3x1 gridworld. The agent starts at 0, and the goal/positive reward is in 2.
The update rule for TD(0) is:
$$ V(S_t) \leftarrow V(S_t) + \alpha\left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right] $$
Where $R_{t+1}$ is the immediate reward for transitioning to $S_{t+1}$.
Running this in python, I get something like $V=[0.784, 0.895, 0]$. According to my understanding of what $V(S)$ should be, this is roughly the correct result. However, if I now try to apply the greedy policy ($\epsilon=0$), it will result in the agent swapping between state=0 and state=1 forever, and never opt to go to state=2, because the long-term expected reward of that state is, by definition, 0.
Clearly there is something very fundamental that I am not understanding or missing, but I don't know what it is, or how to find it.
import random

def random_policy(V, S):
   # At the edges - immediate return
   if S == 0:
      return S+1
   if S == len(V)-1:
      return S-1
   # In the middle - choose randomly
   if random.random() > 0.5:
      return state+1
   else:
      return state-1

def greedy_policy(V, S):
   # At the edges - immediate return
   if S == 0:
      return S+1
   if S == len(V)-1:
      return S-1
   # In the middle - choose the highest estimated V(S_{t+1})
   a = V[S-1]
   b = V[S+1]
   if a < b:
      return S+1
   else:
      return S-1


gridworld = [0, 0, 1]
V =         [0, 0, 0]
ALPHA = 0.1
GAMMA = 0.9

for x in range(1000): # episodes
   state = 0
   while state != 2:
      next_state = random_policy(V, state)
      reward = gridworld[next_state]
      V[state] += ALPHA*(reward + GAMMA*V[next_state] - V[state])

      state = next_state

print(V)


","['reinforcement-learning', 'temporal-difference-methods']",
FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models,"
I am trying to understand this paper and the github code given with it, specifically the CNF script: in the loss function (compute_bits_per_dim) the Normalizing Flow is run forward to retrieve the base point z:
z, delta_logp = model(x, zero)
However, in the paper it is stated that the Normalizing flow must run backwards, taking the observed point x as input and running from t1 to t0 in order to retrieve the base point z. Does anyone have an explanation
",['pytorch'],
Value Of Trees In Decision Tree,"
I am writing an AI to play a game of ""Snake"". Whereby two snakes move on a map to eat food to grow, and eventually if a smaller snake runs into a larger snake, it dies.
I have built a very simplistic model that can detect when it runs out of moves (e.x. Running into walls), pathing towards food, and picking up food. I create a tree by combinatorics of possible moves by me, and possible moves of my opponent. I then expand this tree up to a depth of 5.
Each leaf of the tree has a value based on the outcome of that action. For example, if the move I made picked up food, it gets a value of +100. If that node made me not have any more available moves, it's -9999 (Dead).
When I back propogate the values of the tree, I noticed that if I simply copied the values upwards and summed them up, that very distant ""wins"" were rated highly, even if to get there very specific circumstances had to happen. To combat this I added a ""decay"" value like so :
decimal weight = 1M / maxDepth * (maxDepth / (variation.Depth + 1));
decimal childrenValue = node.Children.Average(x => x.Value) * weight;
node.Value = childrenValue + node.MyValue;

I am attempting to say that based on how deep the node is, it should have ""less"" weight than a node that is the very next one. I also use the Average of the children nodes to account for moves the enemy make.
This does work but at the same time, I've simply made it up in my head. Is there a better way to give less weight to ""far off"" moves as they back propogate?
My next issue is that because I am using combinatorics to generate moves, I can often end up with a bad combination spoiling an otherwise good combination. As an example :
                        -------> Move Up (Dead End -9999)
                       |   
--> Move Left (Eat Food + 100)  ---> Move Left (Nothing + 0)
                       |
                       --------> Move Down (Nothing + 0)

As you can see, my first tree node is to move left which gives me food which is a positive outcome. Now IF I move up after this, I will have a negative outcome and dead end myself. The other two moves have a 0 outcome.
However because I am averaging the children nodes as I backpropagate, I will rate moving left very low simply because I could make a stupid move somewhere else down the tree. I made a very crude attempt at saying ""well, just ignore negative moves if it has a sibling that is not negative"". However I found that the snake would often take unnecessary risky moves (Sometimes 75% chance of death depending on what the enemy does) because I simply ignored the negative outcomes further down the line.
How can I better prune nodes like this and ensure that my snake doesn't think there is a negative outcome simply based on it's own dumb moves.
","['decision-trees', 'breadth-first-search']",
Forward Diffusion Process Derivation In Diffusion Models,"
In papers and other material regarding diffusion models the forward diffusion process is defined by adding a small amount of Gaussian noise to an image $x_0$ for $T$ time steps. In each time step the noise has a variance of $\beta_t$. This process produces a sequence of noisy samples: $x_1, x_2, x_3... x_T$ such that:
$q(x_t|x_{t-1}) = N(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_tI)$
I don't understand why this is $q(x_t|x_{t-1})$ distribution. When adding a constant $c$ to a normal random variable with mean $\mu$ and variance $\sigma^2$ we get a new random variable with the same variance and a mean of $c+\mu$. Therefore, I expect $q(x_t|x_{t-1})$ to be: $q(x_t|x_{t-1}) = x_{t-1} + \epsilon_t = N(x_t; x_{t-1}, \beta_t I)$ where $\epsilon_t=N(\epsilon_t; 0, \beta_t  I)$
Any help will be appreciated.
","['image-generation', 'markov-chain', 'diffusion-models']","
The relationship between $x_t$ and $x_{t-1}$ is as follows:
$$
x_t = \sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon_t,\quad \epsilon_t\sim\mathcal{N}(0,I).
$$
Not only is a small amount of noise added, the original image is also scaled down slightly.
"
Resources and papers about Graph Neural Networks and molecular predictions?,"
What are some famous papers or techniques related to the use of Graph Neural Networks (GNNs) for predicting molecular properties?
For example, I know of a common convolutional layer that has obtained decent results in quantum chemistry; it is the one cited in this paper. This layer has even been included in the python library pytorch-geometric (see here)
Do other similar examples exist?
","['pytorch', 'convolution', 'graph-neural-networks', 'geometric-deep-learning']",
What does the figure in Q-learning vs Expected SARSA actually show?,"
I might be blind.
But I wasn't able to find or figure out what the small difference between Q-learn and SARSA depicts in the following image;

(src).
What does the semi-circle show? and what does the lack of the semi-circle show? I've your eyes with some red arrows.
","['reinforcement-learning', 'q-learning', 'expected-sarsa']",
Train Deep Q-Learning Network on a game without source code,"
So I have some games that I like, and I'd like to create a net that can play them, just for fun. But I don't have their source code, so I can't just pull the information I want and create a state from them. I should have mentioned that most of the games I'd like the net to play are mobile games, and I am using bluestacks for emulation.
What I thought of

Finding alternative open-source versions.
Taking screenshots of in-game items and then using something like pyautogui to find them on the game window.
Using Cheat Engine to extract the info from memory.

Why are those not viable options

If there are open-source versions available, they are usually not that good, plus they are not the original game, so it doesn't fit my goal since I want the AI to play the actual game that most people are playing.
This is A LOT of work; it only sometimes works, and if there's an update that changes the shapes of the textures, I need to do it all again. (colour itself would be fine, I could use grayscale). And the process of finding images on images is not that fast.
This is also a horrible amount of work, and it's just not possible if you need a lot of information.

For example, I want to create a Deep Q-Learning net to play the Subway Surfers game. But I don't have the source code, and no mods allow reading the game info.
So how would you guys approach this issue? I don't want to recreate the games myself because of what I mentioned in ""Why those are not viable options"". Any ideas and or solutions would be well appreciated.
Thanks.
","['reinforcement-learning', 'dqn', 'game-ai']",
Combining fine-tuning BERT and cross validation for hyperparameter selections,"
Is it possible to combine cross-validation procedure and hyper-parameter tuning for fine-tuning bert for a classification task?
The idea is the following:

Choose a set of set of hyperparameters {H,H1,H2,...,H-n}

Perform a cross validation for the fine tuning phase for each set of hyperparameters, meaning that:


For each set of hyperparameters H:
Consider k-folds each having a train and validation set.
For each k-fold, fine-tune the BERT model with hyper-parameters H over the train set for that fold and evaluate its fine-tuned performance over the validation set.
Then at the end evaluate the fine-tuning procedure, considering the average accuracy and error score across the validation sets

Repeat the procedure with all the different set of hyperparameters

Choose among the hyperparameter sets the H which is shown to be the best during the fine-tuning procedure, i.e. the one showing the highest average accuracy and lowest error across the validation sets for its cross-validation fine-tuning process.


For instance, given a model with Hyperparameters H, we perform k different fine-tuning on k different traing sets and evaluate each individual fine-tuning procedure on each k validation set.
Is it possible to do so or could this be a methodological error?
Thanks
","['bert', 'cross-validation', 'fine-tuning']",
What is the intuition behind self-attention?,"
I've been watching a few lectures on transformers, especially for language translation, though it seemingly becomes more confusing the more I watch.
In this lecture, there seems to be two conflicting views of self-attention. First, there's an Iron Man example (at around 44:25) where the lecturer claims that self-attention helps identify the important aspects of the input, but she details the math in which a ""self-attention heat map"", which is the dot product of the query and key matrices, is multiplied by a value matrix giving an output, which I assume is an attention head. It seems like this attention head just encodes how words are related to each other (the map) and the words themselves (the value).
I don't understand how this extracts the important information, or why you only want the important information anyways for translation. Is it extracting the important relationships between words, because that would make more sense, but in what way does that relate to her Iron Man example then? What's the relationship of Iron Man to himself? Also, would each attention head contain its own set of three matrices? Is the idea perhaps that words that more words depend on are important? But wouldn't that simply remove single words that still are important for translation?
TLDR: Essentially, how does the idea of relationships between words translate to selecting important words in a sentence (if this is even what self-attention does).
","['neural-networks', 'transformer', 'attention']",
"Did old-fashioned, rule-based systems die out when statistical learning broke through into NLP?","
It is not quite clear to me whether the statistical approach superseded the rule-based system in the 90s.
McMahon and Smith (1998) report that many other researchers used ""hybrids of statistical and formal approaches"". What do they mean? Maybe it was still unclear which one was best suited for?
","['machine-learning', 'natural-language-processing', 'statistical-ai', 'rule-based-systems']",
"Are there use cases of this ""leaky hard sigmoid"" activation function?","
While trying to organize activation functions in a grid, this structure came up:
softplus -> relu -> leaky relu
sigmoid  -> hard sigmoid -> unknown?

A proper name for the missing one, given its neighbors, seems to be leaky hard sigmoid

The image comes from this colab notebook.
A web search returns two references for ""leaky hard sigmoid"", both from 2020.
Are there more use cases of this activation function, maybe under another name?
","['terminology', 'activation-functions', 'sigmoid']",
Is it redunant to add more layers to a neural network with same number of neurons as the previous layer?,"
Lets say I have a neural network with three layers and the last layer has 3 outputs.
If I added additional layer of 3 neurons to the end of the network, would that be a more powerful neural network?
Here's an example picture of what I mean:

(where the circles are the neurons in the network, L1 is layer1. Also assume that all of the neurons are fully-connected to the neurons of the previous layer.)
Typically neural networks that I see the amount of neurons per network typically decrease as there are layers. But is it theoretically possible to a really powerful network by signiciantly increasing the depth of the network?
For example consider the following case:

Does stacking extra layers on the end of a neural network allow it to learn more complex tasks? My guess would be that stacking extra layers only helps if you are distilling the information down to less neurons per layer. But with more advanced neural networks like GPT-3, what has been said to me is what is important is the total amount of tunable parameters, and that the particular structure or depth of the network isn't important (when holding the total number of tunable parameters constant).
So phrasing this another way: If, for example, I wanted to create a neural network at the scale of a GPT-3 level, (ignoring the intricacies of the transformer architecture and just assuming that it is a basic neural network), is it possible that instead of adding additional neurons per layer, I can simply add more layers of the same neuron size to scale-up the total weight parameters?
","['neural-networks', 'deep-learning', 'deep-neural-networks']","
A deeper network will have more capacity, regardless of whether new layers have the same number of neurons as the previous layers, fewer, or more.
More capacity means that in theory the network can learn more complex functions. It does not mean that the network will then be better on any given task.
One important thing to note: This is only the case if you have a non linear activation function. If you do not have a non linear activation function between layers, then you are correct that each added layer, despite adding more parameters, does not add capacity to the network.
For further reference, I recommend reading this chapter of the deep learning book: https://www.deeplearningbook.org/contents/mlp.html
which contains all the information above (perhaps with more technical jargon) and more.
For a greater understanding of how adding layers affects capacity,  this article will be useful: https://en.wikipedia.org/wiki/Universal_approximation_theorem
"
"Why is automatic differentiation still used, if today's computers can calculate symbolic derivatives quite fast?","
Today's computers can calculate symbolic derivatives quite fast, why is automatic differentiation still used? For example, Mathematica can handle algebraic operations with arrays. Doesn't automatic differentiation cause significant overhead? Calculating the symbolic gradient of an MLP should not be too difficult, or am I wrong?
","['multilayer-perceptrons', 'gradient']",
The episode length increases at the start till it reaches a peak then decreases. What can cause this unexpected behavior?,"
I am running the A3C algorithm to evaluate a policy based on a policy gradient method. I observe an unexpected behavior at the start of the episode in the reward and episode length. As shown in the figure blew, at the start of the training the episode length increases and it starts to decrease after a peak and become normal behavior. For the reward, it decreases at the start and then increaes. May I ask what can cause this kind of unexpected behavior in the episode ?


","['reference-request', 'a3c']",
Embedding layers/entities in openAI's Hide and seek paper,"
I've recently come across a youtube video about openAI's hide and seek paper (https://openai.com/blog/emergent-tool-use/) and got really fascinated about the paper itself. But as I digging in the paper, I just cant get a grasp of how their embedding entities/layers work. (as described in the picture below)
As far as I know, embedding layers are usually used for categorical features. While they can also work with continuous feature that are broken down into different value ranges, but I doubt that they use this mean of embedding as the 1st embedding entity (the blue box labeled ""1"" in the ""Entity Embeddings"" column) got connections even with the other embedding entities.
If they were to split the embedding entity into value ranges, I wonder how would they label the combination(output) of ""x, y of self"" and ""LIDAR"" (like a radar tracking surrounded objects, I guess) and know at what range to break down.
An explanation would be sufficient, but I would love to know more about embedding layers/entities if possible.
Thank you for ur attention.

","['reinforcement-learning', 'deep-learning', 'deep-rl', 'open-ai', 'embeddings']",
Can independent datasets be artificially combined for multimodal learning (semi-synthetic data generation)?,"
BACKGROUND: To apply multimodal machine learning (ML), the various data modalities typically come from the same example (e.g., chest X-ray (modality 1) and cancer biomarkers (modality 2) come from the same patient (the example)).  The problem is that we often don't have that in public datasets.  Instead, datasets are more commonly independent (e.g., a dataset of chest X-rays from one set of patients and a dataset of cancer biomarkers from a second set of patients).
QUESTION:  Is there any validity to artificially creating ""co-registered"" datasets from such independent datasets for the ultimate purpose of leveraging multimodal ML (e.g., generate all possible pairs of chest X-rays and cancer biomarkers from different patients with cancer and generate the same for different patients without cancer)?
NOTE:  It is understood that this approach is non-canonical and has flaws, but that is not the question here.  I am more interested in learning whether this could be a second best option for researchers who lack co-registered samples but still want to develop multimodal ML models.  Please provide your response along with some justification as to why this would or would not be valid ""second best option"".

11/26/2022
New NOTE:  I thought it was implied in the question post that I am aware that the interaction between modalities is not available to be leveraged by multimodal ML since this is really the crux of the problem with fusing independent datasets.  However, the question still stands:  ""Would this be a valid second best option?""
","['machine-learning', 'datasets', 'semi-synthetic']","
One problem with such an approach is if you would know how chest X-ray and cancer biomarkers interact you would likely use that model or knowledge directly instead of using it to generate data. And aligning data among subjects without such knowledge might even give you less information than working with each of the two modalities separately since your model might pick up on some incidental relations. It's like having a regression problem but being unaware of the interaction term.
"
bad prediction when having noise on the data: LSTM time-series regression,"
I want to predict the force plate using a smart insole using the LSTM model for time series prediction. the data on the force plate has positive and negative values (I think the resulting positive value is a noise). if I ignore the positive value, then the predicted results of the data testing will be bad. but if I change the positive value to 0 then the prediction results will be good. what should I do if I want to keep positive value without changing it but have good prediction result.
Force Plate Shape

2050,1

Smart Insole Shape

2050,89

below are my code:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
from tensorflow.keras.layers import Dense,RepeatVector, LSTM, Dropout
from tensorflow.keras.layers import Flatten, Conv1D, MaxPooling1D
from tensorflow.keras.layers import Bidirectional, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import plot_model
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
%matplotlib inline

## Load Data
Insole = pd.read_csv('1113_Rwalk40s1_list.txt', header=None, low_memory=False)
SIData =  np.asarray(Insole)

df = pd.read_csv('1113_Rwalk40s1.csv', low_memory=False)
columns = ['Fx']
selected_df = df[columns]
FCDatas = selected_df[:2050]
## End Load Data

## Concatenate Data
SmartInsole = np.array(SIData[:2050])
FCData = np.array(FCDatas)
# FCData = np.where(FCData>0, 0, FCData) #making positive value to 0
Dataset = np.concatenate((SmartInsole, FCData), axis=1)
## End Concatenate Data


## Normalization Data
scaler_in = MinMaxScaler(feature_range=(0, 1))
scaler_out = MinMaxScaler(feature_range=(0, 1))
data_scaled_in = scaler_in.fit_transform(Dataset[:,0:89])
data_scaled_out = scaler_out.fit_transform(Dataset[:,89:90])
## End Normalization Data

steps= 50
inp = []
out = []
for i in range(len(data_scaled_out) - (steps)):
    inp.append(data_scaled_in[i:i+steps])
    out.append(data_scaled_out[i+steps])

inp= np.asanyarray(inp)
out= np.asanyarray(out)

x_train, x_test, y_train, y_test = train_test_split(inp, out, test_size=0.25,random_state=2)

## Model Building
model = Sequential()
model.add(LSTM(64, activation='relu',  return_sequences= False, input_shape= (50,89)))
model.add(Dense(32,activation='relu'))
model.add(Dense(16,activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss = 'mse', optimizer=Adam(learning_rate=0.002), metrics=['mse'])
model.summary()
## End Model Building

## Model fit
history = model.fit(x_train,y_train, epochs=50, verbose=2, batch_size=64, validation_data=(x_test, y_test))
## End Model fit

## Model Loss Plot
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc='upper right')
plt.show()
## End Model Loss Plot

## Prediction and Model Evaluation
model.evaluate(inp, out)
predictions=model.predict(inp)

print('MSE: ',mean_squared_error(out, predictions))
print('RMSE: ',math.sqrt(mean_squared_error(out, predictions)))
print('Coefficient of determination (r2 Score): ', r2_score(out, predictions))

#invert normalize
predictions = scaler_out.inverse_transform(predictions) 
out = scaler_out.inverse_transform(out) 

x=[]
colors=['red','green','brown','teal','gray','black','maroon','orange','purple']
colors2=['green','red','orange','black','maroon','teal','blue','gray','brown']
x = np.arange(0,2000)*40/2000 
for i in range(0,1):
    plt.figure(figsize=(15,6))
    plt.plot(x,out[0:2000,i],color=colors[i])
    plt.plot(x,predictions[0:2000,i],markerfacecolor='none',color=colors2[i])
    plt.title('LSTM Regression (Training Data)')
    plt.ylabel('Force/Fx (N)')
    plt.xlabel('Time(s)')
    plt.legend(['Real value', 'Predicted Value'], loc='lower left')
    plt.savefig('Regression Result.png'[i])
    plt.show()

## End Prediction and Model Evaluation

## Model Validation
Test_Insole = pd.read_csv('1113_Rwalk40s2_list.txt', header=None, low_memory=False)
TestSIData =  np.asarray(Test_Insole)

Test_df = pd.read_csv('1113_Rwalk40s2.csv', low_memory=False)
Test_columns = ['Fx']
Test_selected_df = Test_df[Test_columns]
Test_FCDatas = Test_selected_df[:2050]

test_SmartInsole = np.array(TestSIData[:2050]) 
test_FCData = np.array(Test_FCDatas)
# test_FCData = np.where(test_FCData>0, 0, test_FCData) #making positive value to 0
test_Dataset = np.concatenate((test_SmartInsole, test_FCData), axis=1)

test_scaler_in = MinMaxScaler(feature_range=(0, 1))
test_scaler_out = MinMaxScaler(feature_range=(0, 1))
test_data_scaled_in = test_scaler_in.fit_transform(test_Dataset[:,0:89])
test_data_scaled_out = test_scaler_out.fit_transform(test_Dataset[:,89:90])

test_steps= 50
test_inp = []
test_out = []
for i in range(len(test_data_scaled_out) - (test_steps)):
    test_inp.append(test_data_scaled_in[i:i+test_steps])
    test_out.append(test_data_scaled_out[i+test_steps])

test_inp= np.asanyarray(test_inp)
test_out= np.asanyarray(test_out)

model.evaluate(test_inp, test_out)
test_predictions=model.predict(test_inp)

test_predictions = test_scaler_out.inverse_transform(test_predictions) 
test_out = test_scaler_out.inverse_transform(test_out) 

x=[]
colors=['red','green','brown','teal','gray','black','maroon','orange','purple']
colors2=['green','red','orange','black','maroon','teal','blue','gray','brown']
x = np.arange(0,2000)*40/2000 
for i in range(0,1):
    plt.figure(figsize=(15,6))
    plt.plot(x,test_out[0:2000,i],color=colors[i])
    plt.plot(x,test_predictions[0:2000,i],markerfacecolor='none',color=colors2[i])
    plt.title('LSTM Regression (Testing Data)')
    plt.ylabel('Force/Fx (N)')
    plt.xlabel('Time(s)')
    plt.legend(['Real value', 'Predicted Value'], loc='lower left')
    plt.savefig('Regression Result.png'[i])
    plt.show()

## End Model validation

the Result without changing the positive value

the Result if I changing the positive value to 0

","['long-short-term-memory', 'data-preprocessing', 'regression', 'training-datasets']",
Back propagation activation function derivative [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 9 months ago.







                        Improve this question
                    



I am reading about backpropagation for fully connected neural networks and I found a very interesting article by Jeremy Jordan. It explains the process from start to finish. There is a section though that confused me a bit. The partial derivative of the cost function (MSE) with regard to the $\theta_{jk}^{(2)}$ weights is:
$$\frac{\partial J(\theta)}{\partial \theta_{jk}^{(2)}} =  \left( \frac{\partial J(\theta)}{\partial a_j^{(3)}}\right) \left( \frac{\partial a_j^{(3)}}{\partial z_j^{(3)}}\right) \left(\frac{\partial z_j^{(3)}}{\partial \theta_{jk}^{(2)}} \right) \tag{1}$$
The article defines the next equation as the ""error"" term. The equation $eq:2$ is the combination of the first two partials in the chain rule:
$$ \delta_i^{(3)}  = \frac {1}{m} (y_i - a_i^{(3)}) f^{'}(a^{(3)}) \tag{2}$$
Where:

$ i: $ The index of the neuron in the layer
$ ^{(3)}: $ Denotes the layer (in this case 3 is the output layer)
$ z_i: $ The weighted sum of the inputs of the $i_{th}$ neuron
$ m: $ The number of training samples
$ y_i: $ The expected value of the $ i_{th} $ neuron
$ a_i: $ The predicted value of the $ i_{th} $ neuron
$ f^{'}: $ The derivative of the activation function

So a few lines after the definition above the article states:

$ \delta^{(3)} $ is a vector of length j where j is equal to the number of output neurons
$$ \delta^{(3)} =
 \begin{bmatrix}
 y_1 - a_1^{(3)} \newline
 y_2 - a_2^{(3)} \newline
 \cdots \newline
 y_j - a_j^{(3)} \newline
 \end{bmatrix} f^{'}(a^{(3)})  \tag{3}
$$

Q1. I strongly suspect that the  $ f^{'}(a^{(3)}) $ is a vector of length $j$ and not a scalar. Basically, it is a vector containing the derivative of the activation function for every neuron of the output layer. How is it possible in $eq:3$ to multiply it with another vector and still get a vector and not a $j\ x\ j$ matrix? Is the multiplication elementwise?
Q2. How is the $ f^{'}(a^{(3)}) $ calculated for every neuron for multiple training samples? From what I understand, while training with batches I would have to average the $ (y_i - a_i^{(3)}) $ term for the whole batch for every neuron. So in fact the term $ (y_i - a_i^{(3)}) $ is the sum for the whole batch and that's why the $ \frac {1}{m} $ is present.  Does that apply to the derivative too? Meaning do I have to calculate the average of the derivative for the whole batch for each neuron?
Q3. What does $ f^{'}(a^{(3)}) $ actually mean? Is this the derivative of the activation function evaluated with the values of the $a_i^{(3)}$ outputs? Or is it the derivative of the activation function evaluated with the values of the weighted sum $ z_i $ that is actually passed through the activation function to produce the $a_i^{(3)} = f(z_i)$ output? And if the second would I have to keep track of the average of the $z_i$ for each neuron in order to obtain the average of the $ f^{'} $
",['backpropagation'],"
Re your Q1 & Q3: assuming single training example for now, indeed you're right mathematically speaking $f^{'}(a^{(3)})$ shouldn't be a constant scalar and from the author's derivation section above your referenced equation (3), this derivative of activation function in the same layer should be evaluated at different values of $z_1^{(3)}, z_2^{(3)}$ as the (weighted sum) net input for the 2 demonstrated neurons at the last output layer (and they've been already computed in previous feedforward pass), thus confirms your Q3's 2nd interpretation that the derivative of the activation function is evaluated with the values of the weighted sum $z_i$ that is actually passed through the activation function. In fact you may also refer to Delta rule which is a special case of backpropagation algorithm which confirms the same interpretation.

For a neuron $j$ with activation function ${g(x)}$, the delta rule for neuron $j$'s $i$th weight $w_{ji}$ is given by $\Delta w_{ji}=\alpha (t_{j}-y_{j})g'(h_{j})x_{i}$, where $\alpha$  is a small constant called learning rate, $g(x)$ is the neuron's activation function, $g'$ is the derivative of $g$, $t_{j}$ is the target output, $h_{j}$ is the weighted sum of the neuron's inputs, $y_{j}$ is the actual output, $x_{i}$ is the $i$th input.

Finally $\delta^{(3)}=[\delta_1^{(3)}, \space \delta_2^{(3)}]$ is a $1 \times 2$ vector representing ""error"" terms for the same 2 output neurons which can be confirmed by the author's conclusion section in more abstract linear algebra notations. Thus it's best to view $f^{'}(a^{(3)})$ as a $2 \times 2$ square matrix with eigenvalues identical to $f^{'}(z_1^{(3)})$ and $f^{'}(z_2^{(3)})$, respectively, and we should actually treat the explicit vector in your equation (3) as a $ 1 \times 2$ vector and then the final result matches as $[y_1-a_1^{(3)}, \space y_2-a_2^{(3)}]f^{'}(a^{(3)}) = \delta^{(3)}$.
As for your final Q2 when training multiple examples using scaled loss function you don't usually need to calculate any additional average, the scaled loss function already takes care to minimize the mean square error. The only difference now is you're now dealing with much larger vectors/matrices such as your equation (3). Say you have 3 training sets, then the above $\delta^{(3)}$ will be a $ 1 \times 6$ vector and $f^{'}(a^{(3)})$ will be a $ 6 \times 6$ matrix. Basically you start with the same random small values for all the same 8 weights in above same network architecture, but you'll have to compute larger vectors and matrices during both feedforward and back propagations of any epoch.
"
I’m making a simple neural network from scratch and it won’t learn anything. Please help [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 9 months ago.







                        Improve this question
                    



I am coding a classifier neural network from scratch. It is not really learning and I believe that somewhere there is a gradient explosion/vanishing issue. Could be some other stuff as well that I cannot imagine right now.
I have coded my own 2000 samples data set that has two features: x1, x2 and a label column that has 0 or 1.
I have tested the architecture on a neural network that I made via keras framework and it yielded an  85%  accuracy on the same dataset with same epoch value. Its fine that accuracy  was 0.85, thing is it worked.
Please help me figure out what am I doing wrong in my code below. Thank you!
My code:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from my_first_nnfs_dataset import data_df

df = data_df

df = df.reset_index(drop = True)

len_of_training_data = 1900

max_df = df.max()['data_y']

X_train = np.array(df[:len_of_training_data][['data_x','data_y']]/max_df).T * 10
y_train = np.array(df[:len_of_training_data][['label']]).T

X_test = np.array(df[len_of_training_data:][['data_x','data_y']]/max_df).T  * 10
y_test = np.array(df[len_of_training_data:][['label']]).T



def initialize_parameters():

    W1 = np.random.rand(3,2)
    b1 = np.random.rand(3,1)

    W2 = np.random.rand(2,3)
    b2 = np.random.rand(2,1)

    return W1, b1, W2, b2

def relu(X):
    return np.maximum(0, X)

def relu_prime(X):
    return X > 0

def sigmoid(X):
    return 1/(1 + np.exp(-X))

def forward_propagation(W1, b1, W2, b2, X):

    Z1 = W1.dot(X) + b1
    A1 = relu(Z1)
    Z2 = W2.dot(A1) + b2
    A2 = sigmoid(Z2)

    return Z1, A1, Z2, A2


def backward_propagation(W1, b1, W2, b2, Z1, A1, Z2, A2, X, Y):

    a = A2 - Y
    b = a.dot(A1.T)
    dW2 = b

    c = W2.T.dot(a)
    d = np.multiply(c, relu_prime(Z1))
    e = d.dot(X.T)
    dW1 = e

    db2 = np.sum(a)
    db1 = np.sum(d)


    return dW1, dW2, db1, db2

def update_parameters(W1, b1, W2, b2, dW1, dW2, db1, db2, alpha):


    W2 = W2 - alpha * dW2
    W1 = W1 - alpha * dW1
    b2 = b2 - alpha * db2
    b1 = b1 - alpha * db1

    return W1, b1, W2, b2


def one_hot_y(Y):
    one_hot_y = np.zeros((2, len_of_training_data))
    for i in range(0, y_train.size):
    
        if y_train[0,i] == 0:
            one_hot_y[0,i] = 1
        
        elif y_train[0,i] == 1:
            one_hot_y[1,i] = 1
    return one_hot_y

one_hot_y_train = one_hot_y(y_train)

a2_predictions = []


def accuracy(a2_predictions):
    a2_p = a2_predictions[-len_of_training_data:]
    latest_epoch = a2_p[-1]

    a = 0

    for i in range(y_train.size):
        if np.argmax(latest_epoch[:,i], axis = 0) == np.argmax(one_hot_y_train[:,i], axis = 0):
            a += 1
    return a/y_train.size

    
def train(X_train, one_hot_y_train, alpha, epoch):

    W1, b1, W2, b2 = initialize_parameters()
    for epoch in range(epochs):
    
        for column in range(y_train.size):
        
            each_example = X_train[:,column].reshape(2,1)
            each_one_hot_y = one_hot_y_train[:,column].reshape(2,1)
        
        
        
            Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X_train)
        
            dW1, dW2, db1, db2 = backward_propagation(W1, b1, W2, b2, Z1, A1, Z2, A2, X_train, each_one_hot_y)
        
            W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, dW2, db1, db2, alpha)
        
            a2_predictions.append(A2)
        
       
        
        if epoch % 10 == 0:
        
            print(f'Epoch: {epoch}')
            print(f'Accuracy:{accuracy(a2_predictions)}\n')
        
    return W1, b1, W2, b2

epochs = 100
alpha = 0.1

W1, b1, W2, b2 = train(X_train, one_hot_y_train, alpha = alpha, epoch = epochs)

Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X_test)

test = np.zeros((1, y_test.size))

for i in range(y_test.size):
    if A2[0,i] > A2[1,i]:
        test[0,i] = 0
    else:
        test[0,i] = 1
acc = 0

for i in range(len(test)):
    if test[0][i] == y_test[i][0]:
        acc += 1

print(f'accuracy: {acc/y_test.size}')

Output:
/Users/apple/Desktop/my_first_nnfs.py:44: RuntimeWarning: overflow encountered in exp
  return 1/(1 + np.exp(-X))
Epoch: 0
Accuracy:0.5189473684210526

Epoch: 10
Accuracy:0.5189473684210526

Epoch: 20
Accuracy:0.5189473684210526

Epoch: 30
Accuracy:0.5189473684210526

Epoch: 40
Accuracy:0.5189473684210526

Epoch: 50
Accuracy:0.5189473684210526

Epoch: 60
Accuracy:0.5189473684210526

Epoch: 70
Accuracy:0.5189473684210526

Epoch: 80
Accuracy:0.5189473684210526

Epoch: 90
Accuracy:0.5189473684210526

accuracy: 0.009900990099009901

Necessary  variables after running:
W1 = 0.914082   4.92167
     5.70267e+09    -1.40049e+10
    -0.986493   -8.28296

W2 = -61.9766   1.2412e+12  -85.8557
     8.91069    -1.2412e+12 16.2499


#A1 is all zeros array of shape (3,101)
#A2 is all ones array of shape (2,101)

PS - epoch = 1000  also has a very similar outcome.
","['neural-networks', 'machine-learning', 'binary-classification', 'gradient']","
Your backward differentiation does not seem to follow the forward computation.
I prefer marking the gradient (row vector) with a letter g (in AD literature also b like TeX \bar) before the variable name, for tangent direction d (like direction or TeX \dot).
Beginning from the last step, one should get

from the residual 0.5*sum((A-Y)**2) indeed gA2 = (A2-Y).T
from the last step A2 = sigmoid(Z2) you should get gZ2 = gA2*Dsigmoid(Z2) (component-wise product)
Next up is Z2 = W2.dot(A1) + b2. Using generic directions dA1, dW2 (column vectors) etc., the defining relation is
gZ2 @ dZ2 = gZ2 @ dW2 @ A1 + gZ2 @ W2 @ dA1 + gZ2 @ db2
           = trace(gW2 @ dW2) + gA1 @ dA1 + gb2 * db2

which implies
gA1 = gZ2 @ W2
gW2 = A1 @ gZ2 # this is a matrix as product column times row
gb2 = gZ2


next A1 = relu(Z1) leads to gZ1 = gA1 * Drelu(Z1) (component-wise)
finally Z1 = W1.dot(X) + b1 similar to above
gX = gZ1 @ W1
gW1 = X @ gZ1
gb1 = gZ1



In total, you need some slight modifications in the backward iteration
def backward_propagation(W1, b1, W2, b2, Z1, A1, Z2, A2, X, Y):

    gA2 = (A2 - Y).T
    gZ2 = gA2 * sigmoid_prime(Z2.T)   # Z2*(1-Z2)

    gA1 = gZ2 @ W2
    gW2 = A1 @ gZ2
    gb2 = gZ2

    gZ1 = gA1 * relu_prime(Z1.T)       # 0.5*(1+signum(Z1))

    # gX = gZ1 @ W1
    gW1 = X @ gZ1
    gb1 = gZ1

    return gW1.T, gW2.T, gb1.T, gb2.T

Or with the gradients and every equation transposed to above
def backward_propagation(W1, b1, W2, b2, Z1, A1, Z2, A2, X, Y):

    gA2 = A2 - Y
    gZ2 = sigmoid_prime(Z2) * gA2   # Z2*(1-Z2)

    gA1 = W2.T @ gZ2
    gW2 = gZ2 @ A1.T
    gb2 = gZ2

    gZ1 = relu_prime(Z1) * gA1       # 0.5*(1+signum(Z1))

    # gX = W1.T @ gZ1
    gW1 = gZ1 @ X.T
    gb1 = gZ1

    return gW1, gW2, gb1, gb2

"
Is it possible to perform node-level classification and graph-level classification on the same graph? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 8 months ago.







                        Improve this question
                    



I would like to classify the nodes of each graph in a multigraph, and transform the graph structure (or delete some of the nodes).
And then I want to do a graph-level classification problem.
Are there any methods or models to achieve this?
I have no idea how to do the labels.
I look forward to helping.
","['deep-learning', 'graph-neural-networks', 'gnn']",
Find maxima and minima of feed forward neural network given interval inputs [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 9 months ago.







                        Improve this question
                    



I have a feed forward neural network defined in pytorch as follows:
    model = torch.nn.Sequential(
        torch.nn.Linear(2, 50),
        torch.nn.GELU(approximate='tanh'),
        torch.nn.Linear(50, 50),
        torch.nn.GELU(approximate='tanh'),
        torch.nn.Linear(50, 1),
    )

The model has been trained.  Now, I have specific intervals of input values (e.g. [0, 0.1), [0.1, 02.),...[0.9, 1]) within which I want to find the maximum and minimum values of the model's output.  I am not looking for specific code or software to do this.  Rather, what are some general strategies for doing this?
","['neural-networks', 'feedforward-neural-networks']",
What is the definition of a continuous state/action space?,"
This question is a result of a discussion with one of my more math-minded friends. When I accidentally mentioned the term continuous state space, he corrected me by saying that I am most probably talking about dense sets since continuity is usually associated with functions. This makes me wonder: what is it meant by a 'continuous' state space? We very often see this term thrown around in the Reinforcement Learning literature without defining properly. Can anybody provide me any references on what it precisely means? Even some of the more mathematically rigorous books like Algorithms for Reinforcement Learning by Csaba Szepesvari or Reinforcement Learning: Theory and Algorithms by AJKS don't delve into the definition of this term.
A quick Google search tells me that there is no such thing as a 'continuous' set. Questions like 'Is there such thing as a continuous set?' and 'Is an Uncountable Set and a Continuous Set the Same Thing?' reinforces my belief that it is not equivalent to the notion of uncountability.
","['reinforcement-learning', 'terminology', 'definitions']","
Usually these authors are using these terminologies from the background of dynamical systems such as one of your referenced Markov Decision Process (MDP) which is all about input/state space/output and optimal control theory which is about action and feedback additionally. State space is a key concept of dynamical systems rooted from Hamiltonian mechanics in classical physics.

A state space is the set of all possible configurations of a system.[1] It is a useful abstraction for reasoning about the behavior of a given system and is widely used in the fields of artificial intelligence and game theory.


For instance, the toy problem Vacuum World has a discrete finite state space in which there are a limited set of configurations that the vacuum and dirt can be in. A ""counter"" system, where states are the natural numbers starting at 1 and are incremented over time[2] has an infinite discrete state space. The angular position of an undamped pendulum[3] is a continuous (and therefore infinite) state space... All continuous state spaces can be described by a corresponding continuous function and are therefore infinite.[3]

For example continuous state space is usually used in some kind of recurrent neural networks as the outputs of hidden layers such as Hopfield network and Hopfield first applied Lyapunov stability theorem and LaSalle's invariance theorem from state space analysis of dynamical systems to his network for certain new applications which were impossible before.
"
What is the theory behind rejecting too good heuristics in search problems?,"
Currently I have found that there is an article in which a search problem is posed and to solve it a heuristic is proposed which, in essence, is the solution of the problem itself. I seem to remember that in search problems based on heuristics (e.g. A*) there was an impediment by which you should not pose too good heuristics, since you generated a kind of dependence on solving the problem before using the algorithm.
Does this phenomenon have any formal theory behind it, and does it have any specific name?
","['search', 'monte-carlo-tree-search', 'heuristics', 'breadth-first-search', 'depth-first-search']",
How do we combine feature maps? CNN,"
In Convolutional Neural Networks we extract and create abstractified “feature maps” of our given image. My thought was this: We extract things like lines initially. Then from different types of lines we are meant to extract higher order features. However, doesn't this require us to look at multiple feature maps at once? Convolutional layers only apply the filter on one matrix at a time, and the only time, to my knowledge, that these feature maps get looked at together is at the fully connected layer.
To explain further, if we have an image of a circle we want to recognize, this consists of many lines at different angles. But in a convolutional layer, we have these different filters that will pick up different parts of the circle. Then when we add a second convolutional layer, how can it extract a higher order feature without combining feature maps in some way? Do we combine feature maps in between convolutional layers?
","['neural-networks', 'convolutional-neural-networks', 'feature-maps']","
Also I would suggest you to have a look at the term ""Receptive field"" in CNNs(Which actually look at scaled version of what the previous Conv layer looked at while it did a convolving step. Reference is - https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1.
So Inherently it is looking at what the previous layer looked at for that particular filter all through the input image's overall area
"
Where does the proximal policy optimization objective's ratio term come from?,"
I will use the notation used in the proximal policy optimization paper.

What approximation is needed to arrive at the surrogate objective (equation (6) above) with the ratio $r_t(\theta)$?
Put another way, we start with the vanilla policy gradient objective and aim to optimize $L^\text{PG} (\theta) = \mathbb{E}_t \left [  \log \pi_\theta(a_t|s_t)A_t \right]$. Where in $L^\text{PG}(\theta)$ do I make an approximation to derive equation (6)?
I could not find this objective following the reference KL02.
","['reinforcement-learning', 'papers', 'math', 'policy-gradients', 'proximal-policy-optimization']","
The ratio term appeared between Eq. (13)&(14) in Sch+15b, as a result of importance sampling:
$$
L^{\theta^{\prime}}(\theta)=E_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{\pi_\theta\left(a_t | s_t\right)}{\pi_{\theta^{\prime}}\left(a_t | s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right)\right].
$$
"
Are there standard token vocabularies for text encoding?,"
Are there good standard token vocabularies for text encoding tasks? I am looking for a set of syllables and letters, and maybe some very common words, in the best case sorted by frequency.
If possible, syllables should be human-used syllables and not di-/trigrams that only make sense to machines, as I'd like to try using language encodings that humans may be able to read. This will probably be much easier when encoding syllables like humans are using them instead of only using common letter combinations.
",['embeddings'],
Why does L1/L2 regularization not prevent co-adaptation?,"
I thought L2 regularization decrease a length of weight vector by almost removing many weights.
So, it would result that output neurons' activations are sparse and then less effected of co-adaptation like using dropout.
I saw somewhere L1/L2 regularization does not prevent co-adaptation.
Why??
","['dropout', 'l2-regularization']",
Replay Buffer taking long time to construct (Reinforcement Learning DQN with tf-agetns),"
I'm new to Reinforcement Learning and I have some question. I am actually training some DQN using the tf-agents from tensorflow. And I recently learned that it's not possible to train a DQN using continuous actions. So the continuous actions that I had had, well I just converted that to discrete actions.
So now, it's about to set up my replay buffer (note I haven't started the actual training part yet, but just setting up the buffer. But I notice that it's taking up a long time than usual. Initially, I had a 0-1 action space and the replay buffer construction was quick. But then I realized I was missing something in my action space that I had to convert it from a discrete to a continuous action space from [0,1]. But when I realized that training DQN with continuous action space is not possible (and that the replay buffer is taking very long to construct), I reverted back to discrete space with min action value of 0 and max action value of M, for some integer M (but now it's action space is discrete and yet it is still taking time in the construction of the replay buffer).
So I am not sure what's going on here? Why is my replay buffer taking a long time to work? Does that have to do with my action space? My integer M is a parameter, but I'm testing it on a data with an M value of 4,235. So that means the action space has 4,236 possible actions. Could it be the reason that it's taking long is I have so many actions?
replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
                                data_spec=agent.collect_data_spec,                                                                
                                batch_size=tf_env.batch_size,                                                              
                                max_length=100000)

","['reinforcement-learning', 'tensorflow', 'dqn']",
Why do activation functions in neural networks have to be non-polynomial to approximate any function?,"
Can someone give me the main idea of the paper Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function? I'm having trouble understanding it.
","['neural-networks', 'papers', 'activation-functions', 'universal-approximation-theorems']","
Polynomials are unbounded once the input variable is very large or negative, also most feedforward NNs are using backpropagation algorithms to adjust weights during each training iteration which needs the derivatives of all the activation functions and thus possibly leads to unbounded gradient if any of these are mere polynomials.
And from the abstract of your referenced paper threshold plays an important role in universal function approximator, an obvious easy example is the 2 layer perceptrons (aka threshold logic unit [TLU], Adaline [Widrow, 1962]) neural network implementation of the classic Boolean even-parity function $y=x_1x_2+\bar x_1 \bar x_2$ which is not linearly separable and thus a single layer of perceptrons cannot achieve as pointed out by Minsky in the 60's，but it could be easily solved by below 2 layer perceptrons with all (bounded) step functions assigned with appropriate thresholds as shown below (you can manually try to verify). If all activation functions are mere polynomials, no matter how many hidden layers you add, these Boolean functions cannot be modeled exactly, and, thus, most practical classification problems could not be solved by neural networks.

"
Crossover and Mutation function for value encoding [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 9 months ago.







                        Improve this question
                    



I have been trying to attempt writing a Genetic Algorithm using value encoding (fixed-length vectors of real numbers) instead of binary encoding. So far the code I have written works, but needs quite a lot more population than the binary encoded code to converge to 0.
This is the code I am using:
#one-point crossover

def crossover(p1, p2, r_cross):
c1, c2 = p1.copy(), p2.copy()
# check for recombination
if rand() < r_cross:
    # select crossover point that is not on the end of the string
    pt = randint(0, len(p2))
    # perform crossover
    c1 = p1[:pt] + p2[pt:]
    c2 = p2[:pt] + p1[pt:]
return [c1, c2]


# mutation operator

def mutation(nval, r_mut):
    for i in range(len(nval)):
    # check for a mutation
    if rand() < r_mut:
        # flip the bit
        nval[i] = 1 - nval[i]

I have tried working it out following the instructions in the below link to rewrite my code, but am not sure if my crossover or mutation function works as it is supposed to:
How do mutation and crossover work with real-valued chromosomes?
","['python', 'genetic-algorithms', 'evolutionary-algorithms']","
With real-valued encodings, you need to adjust your crossover and mutation operators. Basically binary encodings let something like one point crossover pick a crossover point that’s ""inside"" a parameter. If you have two parameters p1 and p2 that are say (0.4, 2.6) in one parent and (1.2, 1.7) in the other parent, your children can only be drawn from {(0.4, 2.6), 0.4, 1.7), (1.2, 2.6), (1.2, 1.7)}. That’s just not enough ability to explore new solutions. Binary encoding those same two parameters with say 10 bits each gives you 20 possible crossover points and thus 20 possible offspring instead of just four, because it’s able to split the encoding inside a parameter. That gives you much greater power to create offspring that are different than their parents while still being similar enough that selection is meaningful.
There are specific types of operators designed to work with real-valued encodings. Look up ""simulated binary encoding"" or SBX for an example. Similar story for mutation. You need an operator that is able to do a better job of generating variation. A common choice is to mutate a parameter by adding a random number to it drawn from a Normal distribution with 0 mean and some standard deviation that’s customized to be appropriate for each specific problem or parameter.
"
How do I know how to design my ANN so that it fits my specific problem?,"
We developed a neural network-based protein reconstruction tool to reconstruct the main chain from only CA atoms.

we generated data from some selected PDBs from the RCSB website to train an NN model.
we then use those data to train the NN model.
we select some test PDBs, strip all atoms except CA atoms, and save them in files.
we pass those CA-only PDBs through the NN model, obtain a reconstructed main chain, and save them in files.
we compare original and reconstructed PDB files and calculate CRMSD values.

The data set is large: a sample of 1398438 rows and 102 columns (data points). The model is Keras via a 4-layered MLP. There is no feedback loop or convolution applied.
We obtained a CRMSD value of 0.3559, which is not satisfactory.

How can I improve or redesign the NN model?
How do I know what NN type would best serve our purpose? How do I know the number of layers we need in the model? How do I know if we need a feedback loop or not?
",['neural-networks'],"
Evidentially, one of the most important concepts in NN design are inductive biases and symmetry. A good way to approach the concept of symmetry is to ask: Given a datapoint $x \in \mathcal{D}$ from my Dataset $\mathcal{D}$, how can I transform $x$ without changing it's meaning. Indictive biases, on the other hand, are essentially assumptions that you have about your data and that you embed into your model architecture.
Here are some examples of what I mean:
Images: Images have a translational symmetry, meaning the identity of an object doesn't change when you move it to another position in the image - a cat in the left corner of an image remains a cat even if you move it to the right corner.
If you now want to classify images and where to take an MLP instead of a ConvNet, your model had to learn to identify a cat in every possible position of the image, because the inputs to your model change with the slightest translation. In contrast, ConvNets have a translational inductive bias, i.e. are translation equivariant. This means they have a build in robustness against translations that requires no learning. This is one of the reasons, they are so good with images. This property of ConvNets arises from applying the same weights (kernel) over the space of the image. Another inductive bias of ConvNets is locality: The full image is not so important - whats important is local patterns and groups of pixels.
Text:
In AI research text was understood as sequential data. This is why people came up with RNNs like GRUs and LSTMs, thus implementing models with a sequential inductive bias. However, since transformers took over it has become clearer that the position of a word in a sentence is important but not primarily encoding the meaning. Instead a graph-like relation between words is more discriminative, which IMO, is why attention-based models are now the SOTA. How I understand this is that words have a specific permutation symmetry that depends on grammar and retains meaning if you permute words in a sentence in a specific way.
Point Clouds:
A 3D scan (point cloud) remains the same under rotation and translation in 3D, i.e. point clouds are SE(3)-symmetric. In the paper [2], this is biased into a transformer model by a clever usage of self-attention.
Graphs: The identity of a graph doesn't change under permutation as long as the relations between nodes remain the same. Hence, graph neural networks are invariant to permutation.
The list goes on like this and I put some references at the end for further reading - there is a lot of material regarding these topics and IMO it is super important. I'd recommend reading [1] which is very clearly written, [3] is very technical and I found it pretty hard to understand in detail, but it provides a nice overview of symmetries and provides an intuition of what constitutes groups and how this is related to equivariance and invariance in neural networks.

In your specific use case, you could ask: What changes can I apply to the input without changing the expected prediction and is there some sort of hierarchy or any other bias that could be useful. I'm no protein expert, but I assume a reflection symmetry (reversing the input sequence). I'd also assume that sequence length is important but not very descriptive. There might be local motives that are more important (Like image size is not important but local groups of pixels are). This would direct you to a localized model such as 1D Convolutions. Providing the model with reflection equivariance is straight forward in the case of 1D convolutions: You can, for every learned kernel, add the reversed kernel to the convolution operations.
This got pretty long by now, but I hope it is somewhat clear what I mean by all of this.


[1] Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges
[2] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks
[3] A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups
[4] Video: Nice talk of Max Welling on the topic of symmetry

"
How does the discount factor (gamma) and memory replay work in Deep Q-learning?,"
Maybe in over my head about this but I'm having a hard time understanding the discount factor in Deep Q-learning.
Correct me if I'm wrong (1):
To train a Deep Q-learning network, every N:th step of action taken, a batch of random samples from replay memory are fed into the model's training function. These samples are totally unrelated to each other, they are unordered and we can't tell if a particular sample is taken from an episode which ended with a positive or negative reward. This is all there is to the training part of a Deep Q-learning network.
From what I've read, the meaning of the discount factor (gamma) is to decide ""how much we value future rewards.""
Correct me if I'm wrong (2):
With gamma = 0: The model only cares about the reward from when the state goes from A to B given the action C, and the training is encouraging this behaviour of the model with the reward. 
With gamma = 0.9: The model cares about future rewards and acts based on what might come 5 (or what number we might want) steps from this.
I know I have gotten something wrong somewhere (or on multiple places :D): How can this small value, gamma, make the model train itself ""for the future"" when nobody (not even human) kan tell what the reward 5 steps from a particular step will be?
To me it would have made sense if we had not only 32 random tuples of (state, action, reward, state') but also a ""episode end reward"" and a ""steps until reward"" for each tuple and we in some way put these two new pieces of information into the function.
I'd really like to understand what the gamma does and how. I'm far from a maths person but from the code I've seen in a few tutorials and examples, I can't even begin to understand how this 0.9 or 0.99 can enable the model to train for future reward.
Please help :)
","['reinforcement-learning', 'dqn']","
Let's try to understand what actually happens when learning. You start with random values for Q(s,a), the function that estimates the reward of an action given a state s, and you then get an immediate reward and then your loss is difference between the given reward + gamma *future estimated reward, i.e. $loss = Q(s,a) - (reward + gamma * Q(s', max_a))$.
Now if gamma=0, it means the loss would not be dependent on what happens afterwards - since the loss would simply be $Q(s,a) = reward$. So eventually Q(s,a) would simply converge to be the immediate reward of action a in state s. This means the agent would not end up learning the optimal strategy, since it would choose the action that maximizes the immediate reward without caring what happens afterwards, even if the step after it will die.
In order to get some intuition on how in practice it actually helps the agent look at the future, try to imagine how this would work out in practice. In the beginning rewards would be random, and at some point it will get to a state before the final episode reward would be very positive if the agent won. Then at some point it would get to the 2 before last state and might choose the action that gets to the positive one before last state - in which case since gamma > 0 it would optimize Q(s,a) to something positive. After millions of s,a pairs you can imagine how this will create a good agent.
Regarding your other suggestion - I am not an expert but I imagine there might be ways to take into account the final episode reward in a different manner - but the main nice thing about DQN is that it is simple and local - and even though you just update based on samples from local actions, with time the network will (hopefully) converge to be optimal.
"
Consider the following axioms- [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 8 months ago.







                        Improve this question
                    



I'm studying Artificial Intelligence and I have a question about first order logic and resolution. But I couldn't find any answer why.
I tried using Google to get the answer but nothing was found.

","['logic', 'superintelligence']","
For first part it's extremely straightforward to encode each axiom into FOL wff's as knowledge base:

$\forall x (c(x) \to l(x, S)$
$\forall x (l(x, S) \to \forall y (d(y) \to l(x, y)))$
$d(R) ∧ r(R)$
$\forall x (r(x) → w(x) ∨ n(x))$
$¬ ∃ x (d(x) ∧ n(x))$
$∀ x (w(x) → ¬ l(G, x))$
$¬ c(G)$ (to be proved)

where $c$:=child, $l$=loves, $d$:=reindeer, $r$:=rednose, $w$:=weird, $n$:=clown, $S$:=Santa, $R$:=Rudolph, $G$:=Scrooge, respectively, in the meta language.
Then convert them to definite, unit and goal clauses via Skolemization:

$\lnot c(x) \lor l(x, S)$
$\lnot l(x, S) \lor \lnot d(y) \lor l(x, y)$
$d(R)$
$r(R)$
$\lnot r(x) \lor w(x) \lor n(x)$
$\lnot d(x) \lor \lnot n(x)$
$\lnot w(x) \lor \lnot l(G, x)$
$c(G)$ (goal clause)

From here we can further resolve clauses:

[3, 6:] $\lnot n(R)$
[4, 5:] $w(R) \lor n(R)$
[9, 10:] $w(R)$
[7, 11:] $\lnot l(G, R)$
[2, 3:] $\lnot l(x, S) \lor l(x, R)$
[12, 13:] $\lnot l(G, S)$
[1, 14:] $\lnot c(G)$
[15, 8:] $\bot$ Q.E.D.

Finally note that not any arbitrary contradictory set of clauses can be derived by resolution inference rule such as the simple case where $p \lor q$ cannot be derived from $\Gamma = \{p\}$ by resolution, though the classical FOL is semantically complete per Gödel's completeness theorem and thus the  formal system with all inference rules of FOL containing the axiom set $\Gamma$ is strongly and refutation complete (but not necessarily negation complete due to the renowned Gödel's incompleteness theorems).
"
Why expect AGI from non-Turing complete AI paradigms?,"
As far as I can tell, all the successful AI paradigms are not Turing complete.
On the other hand, the human mind is Turing complete.
Why would we expect any of these AI paradigms to give us human level intelligence (AGI) if the former is not Turing complete while the latter is?
","['agi', 'turing-completeness']",
"Why could there be ""information leak"" if we do not use fixed horizons?","
In this page Limitations on horizon length from the Imitation library, the authors recommend that the user sticks to fixed horizon experiments because there could be ""information leak"" otherwise.
I'm having problems understanding this term, how can the information leak?
Can someone please explain this with an example, or something that might help me get some intuition about the issue?
","['reinforcement-learning', 'terminology', 'imitation-learning', 'episodes']","
In the standard Cartpole environment, the episode ends either at 500 timesteps or when the Cartpole falls down. Expert demonstrations show how to keep the Cartpole upright for 500 timesteps.
One degenerate way for Generative Adversarial Imitation Learning or Adversarial Inverse Reinforcement Learning to train an expert policy is to assign a positive reward to every action taken by the generator policy, regardless of whether it mirrors the expert actions. With a positive reward, the RL algorithm that is training the generator policy learns to favor prolonging the episode.
Hence the episode length is a form of side-channel information that makes the imitation learning task easier, and we want to exclude it by choosing environments with fixed episode length.
"
Loss function when training on binary_accuracy metric,"
When training a resnet model with metrics set to binary accuracy does the loss function need to be a binary loss function? Now I am using a custom asymmetric loss function (as my data is very unbalanced) and it seems to work. But would be nice to get some inputs if this is worng or why its okey. As I understand it it doesnt really matter what your loss function is as long as it returns the correct shape out (and ofc do what a loss function should do).
","['machine-learning', 'loss', 'metric', 'residual-networks']",
How to find the impactness between the features?,"
I need some guidance, source, and articles where to look in my search. I am kinda lost about where to look and what to look for. So basically I want to find the synergistic effect or interconnections effect of one source on the other source. For Eg: the attribution of TV is let's say 10% of overall sales, but TV includes some interconnectedness for other media-driven sales within like (x%) of Digital driven TV sales, or (y%) on Print driven TV sales, including (z%) of TV base driven TV sales. We need to find the values of x, y, and z by studying the interconnectedness between the media variables. How to find those x, y, and z? I am looking at integrated gradients and Shapley values by which we can find the attribution of the model.
But is there any way to find how much a feature impacted to another feature maybe during finding attribution?
",['machine-learning'],
How is forward chaining complete?,"
Can anyone explain why forward chaining is complete? The AIMA proof went over my head. I understood the working of the algorithm and how we finally have an inferred set of all atomic sentences in the KB and the additional sentences inferred but I don't see why this should prove all entailed sentences.
From AIMA:

It is easy to see that forward chaining is sound: every inference is essentially an application of Modus Ponens. Forward chaining is also complete: every entailed atomic sentence
will be derived. The easiest way to see this is to consider the final state of the inferred table
(after the algorithm reaches a fixed point where no new inferences are possible). The table
contains true for each symbol inferred during the process, and false for all other symbols. We
◮ can view the table as a logical model; moreover, every definite clause in the original KB is
true in this model. To see this, assume the opposite, namely that some clause a1 ∧... ∧ ak ⇒ b is false in
the model. Then a1 ∧ ... ∧ak must be true in the model and b must be false in the model.
But this contradicts our assumption that the algorithm has reached a fixed point, because we
would now be licensed to add b to the KB. We can conclude, therefore, that the set of atomic
sentences inferred at the fixed point defines a model of the original KB. Furthermore, any
atomic sentence q that is entailed by the KB must be true in all its models and in this model
in particular. Hence, every entailed atomic sentence q must be inferred by the algorithm.

","['logic', 'norvig-russell', 'inference', 'knowledge-base', 'knowledge-based-systems']",
How to decide which column has more weightage to output,"

As per Image we can see Column_A value is directly proportional to output,
While Change in value of Column_B has no effects in output.
So basically I want to know is there any algorithm where I can get weightage of columns which is affecting more to output than another.
In this case we can see Column_A has more weightage as change in Column_A value will lead to change in value of B.
but in case of Column_B it will be no change.
Expected Output:
so is there any algorithm to detect this change something like it will give 100% to Column_A and 0% to Column_B
","['data-preprocessing', 'data-science', 'feature-selection', 'data-labelling']",
Calculating Curiosity with Friston's Free Energy in Reinforcement Learning,"
I'm trying to implement the paper A Curiosity Algorithm for Robots Based on the Free Energy Principle in a reinforcement learning environment using PyTorch, but I am unclear how curiosity is calculated.
As the paper describes, I made a 'transitioner' which predicts the next state given the current state and action. One layer of that transitioner is a Bayesian linear layer from Blitz; the probability distribution of weights in that layer is $q_ψ(w) = N(w|μ,σ_2)$ where $ψ = \{μ,σ\}$. Then, curiosity is defined as the Kullback–Leibler divergence $D_{KL}[q_ψ(w|s_{t+1})||q_ψ(w)]$.
$q_ψ(w)$ is easily available, but $q_ψ(w|s_{t+1})$ is described as value of $q$ minimizing free energy $F = D_{KL}[q_ψ(w)||p_ψ(w)] − E_{q_ψ}(w)[log (p_ψ(s_{t+1}|w))]$, and $p_ψ$ isn't given. I think I understand the concept, but I'm puzzled how to implement this.
","['reinforcement-learning', 'machine-learning', 'probability-distribution', 'artificial-curiosity', 'bayesian-reinforcement-learning']",
Privacy implications of storing and transmitting GPT-3 embeddings?,"
We are exploring implementing a feature where a user might enter ""which product has optional all wheel drive"" into a search input, which would be transformed to GPT-3 embeddings, and compared against a set of embeddings for product descriptions to return similar products.
If a user enters PII and/or sensitive data, for example ""my name is Jim, what will help with my COPD"", then the embeddings that are generated will include this sensitive information.
Is it possible to reverse the embeddings GPT-3 creates back to the exact phrase entered?  If not, are there metrics like anonymity that can be used to reason about exactly how sensitive the embedding are?
","['word-embedding', 'embeddings', 'gpt-3']",
Object recognition based on its movement [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 9 months ago.







                        Improve this question
                    



Models like YOLO, Detectron2, etc. classify objects based on their appearance.
This is great when we have single images, and we want to classify objects within those images. However, video contains a lot more data, and the movement of an object within a sequence of frames should also be a great way to classify objects.
For example, a bird and an aircraft may look visually similar, but their movement over a period of e.g., 2 or 5 seconds will give a much better indication as to what the object is (e.g., the bird may make a sharp turn and appear erratic, while an aircraft follows a predictable trajectory).
Are there any open source models that consider the motion of objects within a video, in order to do classification?
","['classification', 'object-recognition']",
"Are the ""artifacts"" in select Keras MNIST training images really there or is my download corrupt?","
I'm having fun with a ludicrously well known and used dataset: mnist.
I am doing it with a huge and well known tool: keras.
Please excuse the red dots, something else I was doing.  I have otherwise not modified the image at all except via the process of plotting it to a file.  All images below are plotted in R.

Image #48, a ""5"" digit:    There is a perfect vertical slice through the upper segment.  For all the letters, they look blurred.  This slice is perfectly vertical and does not have blurring.


Image #54, a ""4"" digit:    This has artifacts to the left and above the image.  It looks like part of a border around the number.


Image #68, a ""1"" digit:    This has artifacts to the left and above the image.  It looks like part of a border around the number.


Maybe something is odd with my download or version, so perhaps someone else can verify this.
Are there the artifacts as shown in the images above, in the canonical dataset as presented by the well known tool?
","['datasets', 'keras', 'image-processing', 'mnist']",
Why the accuracy of a set of hypermodels predicting a sequence never goes beyond a small number,"
I have been trying to use Optuna to optimize a model predicting an integer sequence taking 5 numbers to predict the next, but the validation accuracy never gets beyond a little more than 13% no matter what hyperparameters are taken.

Here's the sequence.
The Sequence
I wrote long encapsulated code for encoding and splitting data which is too long to upload here. Please feel free to write your own. I also stored the hyperparameters limits in database. Feel free to substitute with your own choice.
Can anyone spot any problem in my hypermodel? The listing is below. Thanks!
#!/usr/bin/env python
# coding: utf-8

import os
import sys
import sqlalchemy
from sqlalchemy.sql import text
import numpy as np
import pandas as pd

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, ReLU, Conv1D
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.backend import clear_session

import optuna
from optuna.integration import TFKerasPruningCallback
from optuna.trial import TrialState

sys.path.insert(0, '/DataScience/MyModules')

# one_hot_decode is for evaluation
from PrepareData import PrepareData, one_hot_decode
from Utilities import getOptunaStudyName
from OptunaParamLimits import LoadParameterLimits

# 17-Mar-2022: Force run on GPU = 0
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

STRATEGY = 'N1'

PROGRAM_VERSION = 3
MODEL_ARCHITECTURE_VERSION = 1
PARAMETER_VERSION = 5


# OPTUNA SPECIFC CONSTANTS
DB_NAME = 'M6_o3_' + STRATEGY
_, STUDY_NAME = getOptunaStudyName(DB_NAME, PROGRAM_VERSION, MODEL_ARCHITECTURE_VERSION, PARAMETER_VERSION)

SEC_IN_DAY = 86400
DB_CONNECTION_STRING = 'mariadb+mariadbconnector://optuna:XXXXX@127.0.0.1:3306/optuna3_'+DB_NAME
DSN_STRING = 'DSN=M6_local'
URL='mariadb+mariadbconnector://m6:XXXXX@127.0.0.1:3306/M6'

print(STUDY_NAME)


MAX_TRIALS = 500

# 17-Mar-2022: Force run on CPU (-1)
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

OUTPUT_TIMESTEP = 1
TRAINING_DATA_RATIO = 0.8

VALIDATION_SPLIT=0.3


#Training specific parameters
MAX_TRIALS = 200

TRAINING_EPOCHS = 500
BATCH_SIZE = 64
PATIENCE = 50
TOLERENCE = 0.001


def create_model(trial, param_dict, pcardinality):
    
    CONV_KERN_LOWER_LIMIT = param_dict['CONV_KERN_LOWER_LIMIT']
    CONV_KERN_UPPER_LIMIT = param_dict['CONV_KERN_UPPER_LIMIT']    
    INPUT_TIMESTEP = param_dict['INPUT_TIMESTEP']
    NODES_LOWER_LIMIT = param_dict['NODES_LOWER_LIMIT']
    NODES_UPPER_LIMIT = param_dict['NODES_UPPER_LIMIT']
    
    CONV_KERNEL = trial.suggest_int('conv_kernel', CONV_KERN_LOWER_LIMIT, CONV_KERN_UPPER_LIMIT, step=1)          
    INPUT_NODES = trial.suggest_int('input_nodes', NODES_LOWER_LIMIT, NODES_UPPER_LIMIT, step=1)          
    FEEDBACK_LAYERS = trial.suggest_int('lstm_layers', param_dict['HL_LOWER_LIMIT'], param_dict['HL_UPPER_LIMIT'], step=1)  
    DENSE_LAYERS = trial.suggest_int('dense_layers', 1,  param_dict['HL_DENSE_UPPER_LIMIT'], step=1)  
    INPUT_NODES_LAST_HIDDEN_LAYER = trial.suggest_int('nodes_last_hidden_layer', NODES_LOWER_LIMIT, NODES_UPPER_LIMIT, step=1)      
    LEARNING_RATE=trial.suggest_loguniform('learning_rate', param_dict['LR_LOWER_LIMIT'], param_dict['LR_UPPER_LIMIT'])
    
    
    
    model = Sequential()

    model.add(Conv1D(filters=INPUT_NODES, kernel_size=CONV_KERNEL,
                      strides=1,
                      activation=""relu"",
                      padding='causal',
                      input_shape=(INPUT_TIMESTEP, pcardinality)
                    )
             )

    model.add(LSTM(INPUT_NODES, 
                   return_sequences=True,  
                   activation='relu',
                   input_shape=(INPUT_TIMESTEP, pcardinality)
                  )
             )
        
        
    for i in range(FEEDBACK_LAYERS):
        model.add(LSTM(trial.suggest_int('hd_'+str(i)+'_nodes'
                              , NODES_LOWER_LIMIT
                              , NODES_UPPER_LIMIT
                              , step=1),
                       #kernel_regularizer=l1(trial.suggest_float('hd_'+str(i)+'_l1',0, 0.05, step=0.01)),
                       return_sequences=True,
                       input_shape=(INPUT_TIMESTEP, pcardinality), 
                      )
                 )                
        model.add(ReLU())
  
    # The last LSTM Layer must be non-returning to match output dimension
    model.add(LSTM(INPUT_NODES_LAST_HIDDEN_LAYER,
                   return_sequences=False,
                   input_shape=(INPUT_TIMESTEP, pcardinality)
                  ))
    model.add(ReLU())

    for j in range(DENSE_LAYERS):
        model.add(Dense(trial.suggest_int('dense_'+str(j)+'_nodes'
                              , NODES_LOWER_LIMIT
                              , NODES_UPPER_LIMIT
                              , step=1), 
                        activation='relu',
                        input_shape=(INPUT_TIMESTEP, pcardinality))
                     )

    # Output layer
    model.add(Dense(units=pcardinality, activation='softmax'))  

        
    model.compile(                  
        loss='categorical_crossentropy',  
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),  
        metrics = [tf.keras.metrics.CategoricalAccuracy()]
        )

    
    return model


def evaluate_model(model, ptest_X, ptest_y, pinput_steps, pcardinality):
    correct = 0
    test_data_size = ptest_X.shape[0]
    for i in range(test_data_size):    
        yhat = model.predict(ptest_X[i].reshape((1, pinput_steps, pcardinality)))
        # 14-Nov-2022: seems reshaping is solely for compatibility reasons
        if one_hot_decode(yhat) == one_hot_decode(ptest_y[i].reshape(1, pcardinality)):
            correct += 1
            
    return correct/test_data_size


def objective(trial):
    # Clear clutter from previous Keras session graphs.
    clear_session()
    
    # Load hyperparameter limits dictionary
    param_dict = LoadParameterLimits(MODEL_ARCHITECTURE_VERSION, PARAMETER_VERSION)
    INPUT_TIMESTEP = param_dict['INPUT_TIMESTEP']
    
    # Load data
    CARDINALITY, train_X, train_y, test_X, test_y =         PrepareData(STRATEGY, TRAINING_DATA_RATIO, INPUT_TIMESTEP, 'onehot')    
    
    # Create model using loaded hyperparameter limits
    model = create_model(trial, param_dict, CARDINALITY)
    
    model.summary()
    
    custom_early_stopping = EarlyStopping(
        monitor='val_categorical_accuracy', 
        # 06-Aug-2022: We are doubly patient.
        patience=PATIENCE, 
        min_delta=TOLERENCE, 
        mode='max',
        restore_best_weights=True
        )
    
    custom_early_stopping_loss = EarlyStopping(
        monitor='val_loss', 
        patience=PATIENCE, 
        min_delta=TOLERENCE, 
        mode='min',
        restore_best_weights=True
        )
    
    # Fit the model
    model.fit(
        train_X, train_y,
        epochs=TRAINING_EPOCHS,
        batch_size=BATCH_SIZE,
        validation_split=VALIDATION_SPLIT,
        verbose=1,
        shuffle=False,
        callbacks=([custom_early_stopping_loss, TFKerasPruningCallback(trial, ""val_categorical_accuracy"")])
    )
    
    if trial.should_prune():
        raise optuna.TrialPruned()
    
    # Evaluate model
    score = evaluate_model(model, test_X, test_y, INPUT_TIMESTEP, CARDINALITY)
    
    
    
    return score


if __name__ == ""__main__"":

    storage = optuna.storages.RDBStorage(
                url=DB_CONNECTION_STRING,
                engine_kwargs={""pool_size"": 20, ""connect_args"": {""connect_timeout"": SEC_IN_DAY}})
    
    study = optuna.create_study(storage=storage, 
                                study_name=STUDY_NAME,
                                load_if_exists=True,
                                #16-Jul-2022: We maximize the accuracy
                                direction=""maximize"",

                               pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource='auto', 
                                   reduction_factor=4, min_early_stopping_rate=0)
                               )
    study.optimize(objective, n_trials=MAX_TRIALS)

    print(""Number of finished trials: {}"".format(len(study.trials)))

    print(""Best trial:"")
    trial = study.best_trial

    print(""  Value: {}"".format(trial.value))

    print(""  Params: "")
    for key, value in trial.params.items():
        print(""    {}: {}"".format(key, value))






","['hyperparameter-optimization', 'sequence-modeling']",
Why does data augmentation using synthetic data generated by one model improve the performance of another model?,"
I understand from articles like this one that synthetic data generated by one model based on real data can improve the performance of a second model. Can anyone help me understand the intuition behind why this works? Specifically, why not directly use the real data to train the second model? Since the data generated by the first model is at most as good as the real data used for that model, won't the performance be actually better if the real data are directly used to train the second model?
","['training', 'datasets', 'generative-adversarial-networks', 'data-augmentation']","
The reasoning behind synthetic data is the same behind classic data augmentation,so the goal is to increase the amount of training instances to improve generalization.
The difference with classic data augmentation techniques is that classic data augmentation rely on linear transformations like flipping or rotation that do not provide as much variation as we would like. Synthetic data instead are usually generated with generative models, which sample new data from a posterior distribution learned from real data. This difference is huge. Imagine augmenting data with a flipped pictures of the same dog flipped around vs augmenting the data with a pictures of different dogs not present in your current real data.
Now the problem of course is that training generative model to produce good samples is as hard. Training a GAN on MNIST or other toy datasets is rather easy, doing so on real datasets with much less instances and all sort of imperfections (picture taken with different cameras to give an example) is many times a rabbit hole in itself. So you always want to do a good list of pros and cons before deciding to invest time in training a model just for synthetic data generation.
On the other hand, it's not impossible to find synthetic datasets ready to be used out of the shelf. In that case exploring the data to check they fit your use case is usually a no brainer and time worth spending for sure. An example is SynthCity, a dataset for point cloud segmentation, I found it very handy for a project I was working on where we didn't have lot of point clouds to start with.
Update
Why not feeding the real data used to train the generative model directly?
First answer is that you always feed the real data as well, i.e. you never train only on synthetic data, the point again is to augment you initial dataset, not replacing it.
So you move from a situation like this:
real data + naive augmentation -> model
to this:
real data + synthetic data + naive augmentation -> model
Second answer is that not always synthetic data are generated trough machine learning, there are situations in which you can use analytic methods to produce them (for example fluid simulations trough differential equations). So in this case you're not even starting from real data at all.
"
How to handle missing data for an ordinal feature variable for a deep learning model?,"
BACKGROUND:  I am developing a deep learning model in which one feature variable (out of many) is the grade of cancer, an ordinal variable.  Below is a breakdown of my data by grade:
GRADE       SAMPLES
--------------------
Grade I     746
Grade II    1128
Grade III   2908
Grade IV    1909
Unknown     4255

PROBLEM:  I need a solution to handle a large amount of missing data.
OPTIONS I HAVE CONSIDERED:

Because of the amount of missing data, just removing those study subjects is not an option (can't remove rows -- too many).
Because grade is an important predictor variable, removing grade as a feature is also not an option (can't remove the column -- too important).
I fear that imputation is not going to give me accurate results since so much of my data is missing.  I have considered multivariate imputation methods as well, but I'm still not convinced I can get meaningful results.  Maybe I'm wrong.
I can just treat grade like a nominal variable and make ""Unknown"" its own category, but then I won't be able to leverage the order information (can't have my cake and eat it too).

QUESTION:  Are there any other mathemagical options for this Goldie Locks?
By the way, I know I can try all of these options and see what works.  That is not what I'm looking for.  I'm really looking to see if there are better options that I've not considered yet.
","['data-preprocessing', 'data-science']",
How to normalize a observation (state-vector) which is a mix of different types,"
I want to use an RL (DQN or PPO) for my use case. The use case is actually simple: The agent should change the behavior of a person. The person is busy with some tasks like reading a book or listening music etc. The action space is discrete e.g. ""play a signal"": 0 or ""switch on a lamp"": 1...and so on somehow a kind of alarms). The observation space contains features like:
noise between 0..1, executed task as One-hot encoding (reading, listening music, ...)
example:
0.58, 0, 1 => noise 0.58, reading: 0, listening music: 1 ...
The agent should select proper alarm. The reword-function is simple as well and the range is between -1...1
But it doesn't work properly, although it is a simple use case. My question is, how should I normalize/standardize the state-vector and if needed the reword as well?
","['reinforcement-learning', 'dqn', 'proximal-policy-optimization']",
Use multiple embeddings with the Transformer architecture,"
In their article about their system ""MuseNet"" OpenAI state the following:

Embeddings
We added several different kinds of embeddings to give the model more
structural context. In addition to the standard positional embeddings,
we added a learned embedding that tracks the passage of time in a
given sample. This way, all of the notes that sound at the same time
are given the same timing embedding. We then add an embedding for each
note in a chord (this mimics relative attention, since it will be
easier for the model to learn that note 4 needs to look back at note
3, or else at note 4 of the previous chord). Finally, we add two
structural embeddings which tell the model where a given musical
sample is within the larger musical piece. One embedding divides the
larger piece into 128 parts, while the second encoding is a countdown
from 127 to 0 as the model approaches the (end) token.

I was wondering how to implement and make use of multiple different embeddings for the Transformer architecture. At the moment, the only things that come to my mind would be to sum the embeddings in a similar fashion as is done with the positional encoding, or to somehow concatenate them, where (if the embeddings are of length l each) the final embeddings would be of length 2l for each of the elements.
","['neural-networks', 'transformer', 'embeddings']",
Feasibility of NNs in solving NP-hard problems,"
Can neural networks be used to solve NP-hard problems? I'm assuming they learn to approximate a polynomial function and hence cannot solve NP-hard unless P=NP.
In case they aren't, what are the general methods that are followed to approach these problems?
","['neural-networks', 'machine-learning', 'algorithm']",
How does not learning far inputs make the RNN forget far inputs?,"
I am totally aware of the problem of the vanishing gradient. It usually occurs with vanilla RNN, where with a long sequence of data, the gradient will vanish or explode for far input sequence, and hence, the network will not learn from far inputs. This is fine, however, the side effect of this problem is making the RNN lose the ability to memorize long sequences (memorizing far inputs). I don't understand what is the relationship between not learning the far inputs and not memorizing the far inputs.
In other words, how does not learning far inputs make the RNN forget far inputs?
","['recurrent-neural-networks', 'generalization', 'vanishing-gradient-problem']",
Machine Learning Methods commonly used when data are scarse,"
It is well-known that deep neural networks require lots of data to perform reliably and well. A commonly-cited statistic is that you need at least 10,000 examples per class for a classification problem. However, you don't always have lots of data to train your machine learning algorithm.
Which classical machine learning methods work well with little data? I'm thinking of things like KNN, linear regression, support vector machines, random forests, etc.
Is there a paper that systematically investigates machine learning methods when data are scarse? If not, are there some rules of thumb one can follow?
","['machine-learning', 'deep-learning', 'reference-request', 'datasets', 'algorithm-request']","
Generally speaking, the number of training examples needed to train a neural network for your classification task cannot be determined a priori.  Generalizations such as the claim that 10K training examples needed are obtuse.  This question is complicated, but here are some considerations:

Technically, you can generate a model with very few training
examples (equal to the number of classes), but model performance
will be poor.  Thus, it is not sufficient to ask how
many training examples are needed.  It would be more apropos to ask
how many are needed to achieve a target performance level.
The number of examples depends on the complexity of your task, the expressivity of your model, and the quality of your data.


How many classes do you have (binary OR 100 classes)?
How different are the classes from each other (snake vs plane OR python vs boa)?
What types of features do you have (imaging, time series, tabular data)?
How much label noise is in the data?
How much feature noise is in the data?
How predictive are the features of the classes (relates to #1)?
What is the capacity (model complexity) of your model to learn the pattern?


In a more practical sense, I search the literature for publications that worked on similar tasks as my task and see what models they used, how many examples they used, and what type of performance they achieved.
Consider expanding your training data with Data Augmentation or GANs or public data sets or working with collaborators.
Consider transfer learning if you have scarce data.
Besides neural networks, there are again no a priori methods for determining which traditional ML model is going to perform better.  You have to empirically test them out.
Consider N-shot learning.
Depending on whether you have very little data over all or whether you have very little labeled data, considered semi-supervised learning.

"
How to generate longer text with GPT-2?,"
I am currently using Huggingface transformers to generate text using GPT-2. The problem is, that it only generates 1024 tokens, what seems to be a hard limit in the code of the script, which enforces a maximum length of the model generation length model.config.max_position_embeddings.
As far as I understood it, one should be able to generate infinite text, with the drawback that only the last 1024 tokens are considered as context. I tried feeding a paragraph from the output into the text run, but the input prompt seems to be limited to much fewer tokens than 1024 (I am not sure how many).
So it may only be the script that's limited, but I am not sure how one would use the model to continue generation with the state the models has after generating the last token.
",['gpt-2'],
Camera specific dataset for training CLRnet model,"
I am using CLRnet to generate a model to do Lane Detection. I will be having proprietry camera(fish eye) on which the model will work.
The idea is to train a model for lane detectection using CLRNet, detect lanes from the model then find the distance of the detected lane line using camera parameters. the model will be then ported to TI
I request answers for following queries as i am confused.
Do i need to train the model only using the dataset captured from same camera which i will be using for detecting the lines?
Does it make any difference if i train the model using dataset from any other camera. once model is generated then i use the defined camera to detect lanes and apply camera parameters to identify correct distance?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'deep-neural-networks']",
Generating synthetic time series data with limited data,"
I would like some opinions on my current situation.
I have a set of time series data that I want to forecast. The data however is not very long (around 500 rows) so I was looking into generating many synthetic datasets that mimic my current one's statistical characteristics.
I have seen online, methods that can generate time series data based on your own dataset for example TimeGAN, DoppleGANger which are a neural-networks. It is well known that neural-networks need a lot of data to perform well and prevent things like overfitting.
You can probably see my problem here, that I don't have enough data to ensure GAN models that are meant to generate me more data work effectively. Or do you think I can just go for it and it wont really matter?
","['datasets', 'generative-adversarial-networks', 'time-series', 'data-augmentation']",
What is the relationship between entropy in thermodynamics and entropy in information theory?,"
BACKGROUND:  In thermodynamics, entropy $S$ is a measure of disorder and is given by
$${\displaystyle S=k_B\log(W)},$$
where $k_B$ is Boltzman's constant and $W$ is the number of microstates.
In information theory, (Shannon) entropy $H$ is a measure of uncertainty and is given by
$${\displaystyle H=-\sum _{i}p_{i}\log p_{i}},$$
where $p_i$ is the probability of a given state.
QUESTION:  Given the conceptual similarity (disorder ~ uncertainty) and mathematical symmetry ($log$ of some $state$), are these two entities merely similar or are they in fact equivalent?  A philosophical and mathematical explanation is desired.
As an aside, I will to point out that philosophical inquiries are within bounds for SE:AI topics.
","['information-theory', 'uncertainty-quantification', 'entropy']",
"AlphaTensor's correspondence between a tensor and algorithm, check if resulting algorithm produces matrix multiplication","
I was recently reading DeepMind paper on AlphaTensor found here https://www.nature.com/articles/s41586-022-05172-4
And I find the idea of the correspondence between a tensor and the algorithm fascinating 
I see how this approach is being used in number of other areas. Yet one question I can't figure out. Suppose you have the U, V, W tensors given with some random integer values how would you perform a check if the contents of U, V, W tensors actually correspond to an MMUL algorithm. Because it looks as if you can easily get garbage out of that tensor if its values weren't crafted carefully. It's not trivial to make such check by just looking to ones and zeros in U,V,W?
So how do you do that?
","['neural-networks', 'deepmind']",
How do Neural Networks deal with ambiguity of words?,"
The word ""shot"" has several meanings. For example, when one says they shot an animal, it could mean that they shot it with a gun, or even shot it out of a cannon.
How can NN trained models be used to identify the appropriate meaning of the word shot?
","['neural-networks', 'natural-language-processing']","
It depends on the actual neural network model being used but in general NNs won't be looking into individual words. They'd be fed with context (other words) surrounding the subject word. You can find some more info on most basic approach (Word2Vec) here https://en.wikipedia.org/wiki/Word2vec
Also NN models are not being trained to identify meaning. It's very complicated to explain to a NN such a complex concept as ""identifying meaning"". It's in general complicated to actually ""explain"" or ""teach"" anything to NN. It will regularly identify ""shortcuts"" that would allow NN to sand-bag on its training. It would often find strategies to trick your loss function.
It's all about how you craft your training process and your loss function. Typically a word ""meaning"" would appear (if I may be so bold) as a bypass product from NN doing it's job so good. I might be wrong but it can just be a mere human feeling that NN truly understand concepts. In reality it's more a numbers game, a reflection of something that might feel real yet there's nobody on the other side of the mirror.
"
Are softmax outputs of classifiers true probabilities?,"
BACKGROUND:  The softmax function is the most common choice for an activation function for the last dense layer of a multiclass neural network classifier.  The outputs of the softmax function have mathematical properties of probabilities and are--in practice--presumed to be (conditional) probabilities of the classes given the features:

First, the softmax output for each class is between $0$ and $1$.
Second, the outputs of all the classes sum to $1$.

PROBLEM:  However, just because they have mathematical properties of probabilities does not automatically mean that the softmax outputs are in fact probabilities.  In fact, there are other functions that also have these mathematical properties, which are also occasionally used as activation functions.
QUESTION: ""Do softmax outputs represent probabilities in the usual sense?""  In other words, do they really reflect chances or likelihoods? (I use likelihood in the colloquial sense here.)
","['activation-functions', 'probability', 'softmax', 'probability-theory']","
Excellent question.
The simple answer is no.  Softmax actually produces uncalibrated probabilities.  That is, they do not really represent the probability of a prediction being correct.
What usually happens is that softmax probabilities for the predicted class are closer to 100% in all cases, whether the predictions are correct or incorrect, which effectively does not give you any information. This is called overconfidence.
This means that the probabilities are not useful, and you cannot really use them as reliable confidences to detect when the model is unsure or predicts incorrectly.
For reference: Guo C, Pleiss G, Sun Y, Weinberger KQ. On calibration of modern neural networks. In International conference on machine learning 2017 Jul 17 (pp. 1321-1330). PMLR.
"
Are there better loss functions than MSE for maze solver using deep learning?,"
I am a newbie in reinforcement learning, and I was doing a project on solving an agent maze solver using deep Q Learning.  Currently, I am using the MSE loss function, but the agent is very slow or not even reaching the target.  Is there any better loss function to improve the agent performance?
","['reinforcement-learning', 'deep-learning', 'q-learning', 'dqn', 'intelligent-agent']","
The original arXiv DQN paper used the Mean-Squared Error (MSE) loss function. The loss of a sample experience tuple $(s, a, r, s')$ is as follows:
$$ \left([r + \gamma\max_{a'}Q(s', a'; \theta_{i-1})] - Q(s, a;\theta_i)\right)^2.$$
For completeness sake, the target values (expression in brackets) are generated with the previous weights of the network $\theta_{i-1}$ when determining the current weights $\theta_i$ via gradient descent.
In contrast to using MSE as a loss function, the Nature DQN paper instead clipped the error (expression inside outer parentheses) to be between $-1$ and $1$ (see Methods, Training algorithm for deep Q-networks, last paragraph). The authors state that clipping the error corresponds to using an absolute value loss function outside of the interval $(-1, 1)$ during gradient descent. In practice, this generally produces smaller gradients than those of MSE loss, and the authors state that this form of error clipping improved the stability of the DQN algorithm. Some deep learning libraries implement this loss function, or a similar variant, as the Huber loss function.  Also for completeness sake, the Nature paper replaced parameters $\theta_{i-1}$ with those of a separate target network, denoted as $\theta_i^{-}$, for further stability of the algorithm.
Without more details, it's difficult to determine if the loss function is the culprit of poor performance for your problem, but this answer is at least a starting point regarding better loss functions in practice.
"
Can Hopfield network have non-binary values and what is the condition for energy consistency in Hopfield NN?,"
Can neurons in Hopfield Network have non-binary values ( continuous values instead of -1 and +1 or values>1)? If they can have non-binary values , is energy expression for hopfield NN stays the same? What is the main condition for Hopfield energy to stay consistent? I am aksing this particularly because I am wondering if we could consider a text as a Hopfield Network where word occurence frequencies in the text could be neuron states.(For example in this sentence the frequency of the word ""sentence"" is 2, so the neuron state would be=2). I am wondering if energy of Hopfield NN could work in this case.
","['neural-networks', 'natural-language-processing', 'recurrent-neural-networks', 'hopfield-network']",
How would one go about checking numerical stability of the deep learning algorithms?,"
Let's say I have a trained deep learning model. It would be good if it would be numerically stable, so if I change input by small amount, the output will also change by small amount.
How should I approach checking numerical stability of deep learning algorithm? Should I just test many different inputs and check whether the outputs for similar inputs are similar or are there any other ways to check that?
","['deep-learning', 'stability']","
One way to do this would be to add a small amount of noise to all of your inputs and determine the change in output.  Steadily increase the noise and measure impact on output.  Local gradients around the model's final weights are a measure of stability.
"
How to develop an Entity linking system able to detect custom entities?,"
I am trying to build an entity linking system that links entities found in a text to entities on Wikipedia. If no Wikipedia page seems to match this entity, the systems will assume to have found a new entity and adds it to an internal knowledge base.
I am currently working to build a system that works like this:

Extract mentions of entities in the text: in the sentence ""Napoleon commissioned a grand reconstruction of Paris"", Napoleon is extracted as it refers to some entity.
Search for the alias ""Napoleon"" in Wikipedia and compute the embeddings of the pages that appear.
Compute the embedding of the sentence where the mention is found and compare it against the embeddings computed from Wikipedia. In this case, Napoleon III should be the best fit, although Napoleon I has a higher prior probability, given his popularity.
If no page from Wikipedia surpasses a given similarity threshold, we assume that we have found an entity with no corresponding page on Wikipedia; thus, we store it in our entity kb.
Save on our kb the previously computed embeddings of Wikipedia pages.

Can this approach work effectively? How can I improve it?
Any help you can provide would be greatly appreciated!
","['natural-language-processing', 'knowledge-representation', 'natural-language-understanding', 'knowledge-base']",
How to perform prescriptive analysis with timeseries models (ARIMA / Prophet)?,"
We have recently come across a problem of applying prescriptive modeling techniques to time series models in a business analytics context.  For the HR domain, we are using employees demographic data , company related settings and their attendance timesheet to
a) identify the key factors that impact the level of absenteeism using multivariate regression and feature importance through random forests.
b) predict future absenteeism using past data as univariate time series model in ARIMA and prophet (prophet works best capturing trends and seasonalities as absenteeism is highly seasonal)
We are now looking for approaches/methods to apply prescriptive modeling in order to get optimal values for features like distance from work, job settings (FT/PT/hybrid), qualification etc. to help HR in future hirings and developing strategies to minimize the future absenteeism.
Looking through the available literature, we couldn't see a well described approach towards solving this problem. Any suggestions or directions on how to best approach this would be highly appreciated.
Regards.
","['optimization', 'time-series']",
How to evaluate the performance of off-line & model-free reinforcement leaning?,"
I'm currently studying on off-line reinforcement learning (RL) and trying to utilize it for medical data. Because it seemed hard to develop well-performing environment model, I decided to adopt model-free RL algorithms. Then I faced somehow ridiculous issues which cannot be ignored. ""How to evaluate this RL model?"".
My ideal model will yield the (estimated) best action at the specific state, and there must be a paired real action at that state. We know the reward of the real action, but the reward of the (estimated) best action is totally unknown. All I can describe is the difference between the real actions and best actions. How can I know this RL model outperforms real actions? (according to reward)
Of course I can change to model-based RL algorithm and get predicted rewards from trained environment model, but I'm not sure this 'predicted rewards' are reliable (Anyway, it is also  predicted).
Is there any smart method for calculate estimated rewards in off-line RL model? (e.g. mean rewards of real actions -> 0.5, mean rewards of best actions -> 0.8)
","['reinforcement-learning', 'rewards', 'off-policy-methods', 'model-free-methods', 'offline-reinforcement-learning']",
Making a model predict on the basis of a particular value,"
I have an input vector of shape say 1 x 400. It's fed into a network that outputs a 1 x 100 vector. I want to design a model that only considers every 4th value of this tensor and gives me the max values.
For example, a demo tensor could be [0.1, 0.2, 0.4, 3, 0.7, 0.2, 0.6, 8, 0.3, 0.1, 0.5, 12, 0.7, 0.1, 0.9, 12]. For this 1x16 input, the corresponding output would be a 1x4 one. I want the model to look only at values 3,8,12,12 and then predict an output of the following form 0,0,1,1. (note that there could be more than 1 1s in the output)
I have tried using MLPs to do this using CELoss but to no avail. Can this be solved using some particular architecture/loss only or am I doing something wrong?
","['neural-networks', 'machine-learning', 'deep-learning', 'function-approximation']","
Even though (as discussed above) this problem is best handled by pre- and post-processing, yes, a neural network can be used for this problem.  One possible way to do it is as follows:

Create your training data.
The features x_train ($x_i\ \in\mathbb{R}^{400}$) will be the input vectors.
The labels y_train ($y_i\ \in\mathbb{R}^{100}$) will be derived by subsetting the input vectors to include every 4th element of $x_i$ and then setting the max values $= 1$ and all others $= 0$.

# The below (python) code illustrates how this can be done

import numpy as np
x = np.random.randn(400)
y = x[::4]
idx = np.argwhere(y == max(y))
y = np.zeros_like(y)
y[idx] = 1



Architect and train a neural network with 400 input units and 100 output units.  You can try different hyperparameters, starting with simple models first.  The choice of loss function can also be a hyperparameter, but you can try common loss functions like crossentropy loss (CEL) and root mean squared error (RMSE) first.

The output units can have a sigmoid activation so that all the values are between $0$ and $1$.  I'm not sure how well a unit step function would work here as an activation function, but that would guarantee that the values are either $0$ or $1$.  With the unit step function, there is no need for post-processing.  You can experiment with both.

If you use a sigmoid activation, then you will still need to do some post-processing to ensure that the final values in the output vector are $0$ or $1$. For example, you can set values $>0.95 = 1$ and all others $= 0$.


"
Ways to improve DQN model learning snake?,"
I'm training a Deep Q-learning model on a snake game and I would like some ideas on how to improve the model and maybe also efficiency of training it.
The game is currently set to a 12x12 grid, a blue snake with a green head and a red apple. The network is fed with 3x12x12 input parameters (RGB, width, height) and gets a positive reward when an apple is eaten and a negative reward when it collides with something.
It does learn, but plateaus around 12-13 apples per round (on average) after 3 million steps:

What I have tried:
Giving a partly reward on the steps before a ""real"" reward. For example:
Step    Action        Reward
N       Go straight      100
N-1     Go straight       50
N-2     Go straight       33
N-3     Go left           25

That was just an idea but it does not seem to work as I hoped.
What else can I try? What I don't want to do is tinker with the game, I just want the visual input and nothing else.
","['reinforcement-learning', 'convolutional-neural-networks', 'dqn']","
Here are your options:

Try different starting points in the path.
Try creating more paths to train your snake.
Try using genetic algorithms to ""evolve"" your algorithm more systematically.
I don't think that snakes eat apples, so try changing the reward to a mouse if you are serious about getting good performance out of your snake.
Some snakes are smarter than others, so try using a cobras, pythons, etc.  Do not use titanaboas because they are extinct, probably for a reason.

"
"What does that mean if my precision, F1-score are very high, but my ROC AUC score is around 0.5?","

What does it mean when my precision and so on are so high and the roc auc score is around 0.5?
Thanks for the answer!
","['machine-learning', 'performance']","
If you are certain that your calculations are correct, then there may be class imbalance in your dataset.  The metrics precision, f1_score, and accuracy are dominated by the model's performance on the major class in your dataset.
For example, if your dataset has $99$ examples of the positive class and $1$ example of the negative class and your model predicts positive $100$% of the time (no matter what), then

$Precision = \frac{TP}{(TP + FP)} = \frac{99}{(99 + 1)} = \frac{99}{100}$
$F_1 score = 2\times\frac{TP}{(2 \times TP + FP + FN)} = 2 \times \frac{99}{(2 \times 99 + 1 + 0)} = 198/199$
$Accuracy = \frac{(TP + TN)}{(TP + TN + FP + FN)} = \frac{(99 + 0)}{(99 + 0 + 1 + 0)} = \frac{99}{100}$

These give a false impression that your model is performing very well, but in fact, all it is doing is calling every example positive. This is why it is imperative to have a clear understanding of your performance metrics, what they mean, and what their limitations are.  In general, it is also a good idea to examine multiple performance metrics, if appropriate.
In this example, specificity will reveal problems in the model and/or data.

$Specificity = \frac{TN}{(TN + FP)} = \frac{0}{(0 + 1)} = \frac{0}{1}$

Given all possible positive-negative example pairs in your dataset, the roc_auc is the proportion of those pairs for which the model correctly identifies the positive example.  Thus, the AUC is more sensitive to class imbalance than those other metrics.
Your results also indicates that your model is not doing any better than the flip of a coin because an AUC of $0.5$ indicates the expected performance of a dummy (naïve) classifier.
"
How do you interpret this train vs test accuracy scores? is the model under or over fitting?,"

What does this difference in train and test accuracy mean?
","['machine-learning', 'overfitting', 'performance', 'underfitting']","
For the Logistic Regression Classifier and SVC, the train_accuracy and test_accuracy are very similar; thus, there is no evidence for over or under fitting.  However, the KNN, shows a train_accuracy that is lower than your test_accuracy, which suggests overfitting.  Alternatively, depending on your sample size, it is entirely possible that this variation is within the expected range of error.
In machine learning, the model is trained on one dataset (i.e., the training data), but performing model evaluation (i.e., estimate ""real world"" performance by calculating accuracy or some other metric) on the same dataset that you trained on would be biased.  Instead, the evaluation is performed on an independent identically distributed dataset (i.e. the test data) to get an unbiased estimate of model performance.  However, it is still important to compare accuracy (or whatever performance metric you choose) between training and testing to assess for over- or underfitting.
Overfitting means that your model ""fit"" the training data very well and achieves high accuracy on that data.  However, the model performs poorly (or less well) on the test data.  This is because the model not only ""fits"" the signal in the training data but also the noise.  The problem is that noise is random and it will surely be different in the test set.  Therefore, the model doesn't generalize well to other datasets.
Underfitting means that the model performs poorly on both datasets.  This is typically because the model does not have the ""expressivity"" or ""capacity"" to learn the signals within the data or that the data is random and has no ""signal"" to speak of (and, therefore, nothing from which to learn).
"
What would be a good optimization technique for this kind of problem?,"
Problem Description:
Since I am not sure if there is a scientific term that categorizes this problem, I will do my best to describe it thoroughly.
Suppose there is a chamber that's being filled with poisonous gas. The amount of poisonous gas being released at 1-second intervals can be represented as a time series such as:

In order to reduce the poisonous gas, the operator has access to a set of jobs, which if initiated at a certain time, can release a certain amount of neutralizing gas per second over a certain duration. Specifically, a job can be represented as $$J_i(amount_i, duration_i, cooldown_i)$$ where:

$amount_i$ is the amount of neutralizing gas released by $J_i$ per second
$duration_i$ is how long $J_i$ lasts when activated
$cooldown_i$ is how long $J_i$ is unavailable before it can be used again. Note that this property is what makes this optimization problem difficult (more on that later).

Note that in this particular problem, at any timestamp $t$, the neutralizing gas can only reduce the poisonous gas that is present at timestamp $t$. So in other words, it doesn't linger.
$Problem_1$: If the operator has access to a set of jobs $J = ({J_0, J_1, J_2, ...})$, what would be the best placements of jobs in order to end up with the least total amount of poisonous gas remaining in the chamber?
There are many possible solutions, but a good one would have to allocate a job with the right amount at the right time. Because remember if a high-throughput job is allocated when there is hardly any poisonous gas being released, then it would all go to waste. The cooldowns are also a big factor since the assignments would have consequences on the entire timeline. Hence, I reckoned this might be a good optimization candidate for AI.
An example of a solution can be shown as:

What makes this problem difficult is the cooldown constraint for every job. Because the constraint is not on the range of values, but rather on the different possible solutions. What I mean is that if the optimization algorithm comes up with a potential solution that uses $J_0$ at timestamp $t$, it somehow needs to understand that it should ignore any other solution that tries to use $J_0$ between timestamps $t$ and $t + cooldown_0$. I assume the optimization algorithm that is useful for the problem should be somehow capable of that.
Question:
What would be a good optimization technique to solve (or approximate) $Problem_1$ given its jobs' cooldown constraint?
More info (what I have tried):

PSO: I was hoping to use Particle Swarm Optimization, but the issues are:

High number of dimensions: To be able to map job assignments to particles, the number of dimensions blows up to number of jobs * total_duration_seconds. So for example for 15 different jobs over 6 minutes, it would result in 5400 particle dimensions, where each dimension can be either 0 or 1. So for example, if particle_dimensions[3] is 1 then it represents using the 1st job ($J_0$) at timestamp = 4 seconds. Similarly if particle_dimensions[360 + 5] is 1, then it represents using the 2nd job ($J_1$) at timestamp = 6 seconds.  Is there a better way of mapping the particles so that the number of dimensions are lower?
Job cooldown: more importantly, the cooldown constraint makes the search space full of unfeasible areas which makes finding feasible solutions very difficult. Because if $J_0$ is used at 4 seconds, represented by $particle\_dimensions[3] = 1$ then only at index $particle\_dimensions[3 + cooldown_0]$ and later the value can be 1 again. All the values in between must be 0, otherwise, it would violate the cooldown constraint for $J_0$. Now imagine this must be held true for all jobs, so the whole 5400 dimensions need to be set in a way that none of them violate the cooldowns of each job respectively. In order to tackle this, I have assigned a penalty to unfeasible solutions, but they are so many that the particles cannot find their way into the few (very spread-apart) feasible areas. Is this why using PSO is a bad idea here? Or is there a way to tell particles how they should move in order to respect the cooldown constraint for each and every job? Is that even possible?


Constrained Programming: I am not very familiar with this paradigm, but it seems interesting given the cooldown constraint. Do you think this would be a good approach to tackle the cooldown constraint?


","['machine-learning', 'optimization', 'constrained-optimization', 'particle-swarm-optimization']",
How is the variance for a diffusion kernel derived for a diffusion model?,"
So I'm watching this video tutorial from CVPR this year on diffusion models, and I am confused by the variance term in the distribution on the left on the video. I understand that in the forward process, we can track intermediate distributions
$$q(\mathbf{x}_t|\mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t;\sqrt{1-\beta_t}\mathbf{x}_{t-1},\beta_t\mathbf{I})$$
And that the joint distributions of all the intermediate steps conditioned on the input is given by
$$q(\mathbf{x}_{1:T}|\mathbf{x}_{0}) = \prod_{t=1}^Tq(\mathbf{x}_t|\mathbf{x}_{t-1}).$$
If we define $\bar{\alpha} = \prod_{s = 1}^t(1 - \beta_t),$ then we are supposed to derive the diffusion kernel
$$q(\mathbf{x}_t|\mathbf{x}_{0}) = \mathcal{N}(\mathbf{x}_t;\sqrt{\bar{\alpha}}\mathbf{x}_0,(1 - \bar{\alpha}_t)\mathbf{I})$$
I can definitely see why the mean is what it is, but I'm having a hard time seeing where we get the variance from. How is the variance derived?
","['computer-vision', 'generative-model', 'probability-distribution', 'variance', 'diffusion-models']","
Using the reparameterization trick that tells us:
$$\begin{aligned}
\mathbf{z} &\sim \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}, \boldsymbol{\sigma}\boldsymbol{I}) \\
\mathbf{z} &= \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon} \text{, where } \boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{I})
\end{aligned}$$
If we define $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$:
$$
\begin{aligned}
\mathbf{x}_t 
&= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1}  \\
&= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_{t-1}}\boldsymbol{\epsilon}_{t-2}) + \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}}\mathbf{x}_{t-2} + \sqrt{\alpha_t(1 - \alpha_{t-1})}\boldsymbol{\epsilon}_{t-2} + \sqrt{1 - \alpha_{t}}\boldsymbol{\epsilon}_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}}\mathbf{x}_{t-2} + \sqrt{\alpha_t(1 - \alpha_{t-1})+(1 - \alpha_{t})}\bar{\boldsymbol{\epsilon}}_{t-2} \\
&= \sqrt{\alpha_t \alpha_{t-1}}\mathbf{x}_{t-2} + \sqrt{1-\alpha_t \alpha_{t-1}}\bar{\boldsymbol{\epsilon}}_{t-2} \\
&= \dots \\
&= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$
"
How are gradients of individual layers computed?,"
I have been reading some papers recently (example: https://arxiv.org/pdf/2012.00363.pdf) which seem to be training individual layers of, say, a transformer, holding the rest of the model frozen/constant. In the case of the paper I read, this was done in an attempt to minimize parameter changes so as to reduce knowledge ""lost"" by a model when it is updated for new information.
My question is, how are individual layers of a transformer trained? Like, if we run the transformer and get a gradient, how can we use that gradient to train, say, the first layer, without affecting the rest of the layers at all?
","['neural-networks', 'natural-language-processing', 'gradient-descent', 'transformer']","
The gradient that we use to train neural networks is the gradient of the loss function with respect to the parameters of each layer.
The parameters usually form a very large vector, concatenating the parameters of each layer, and you compute gradients with respect to that large vector.
Then to train a single layer, you just take the gradient of the loss with respect to the parameters of that layer only. Mathematically it is very simple.
"
"Where exactly is permutation happening in equation 5 of the paper ""Learning with Sets in Multiple Instance Regression Applied to Remote Sensing""?","
I am reading the article Learning with Sets in Multiple Instance Regression Applied to Remote Sensing about creating an embedding which is order-invariant to inputs ($m_{l}$). They referred to order-invariance in eq (5) as follows:

The order-invariant operation happens in (5), where permuting $m_l$
with $m_{l}'$ yields the same vector $r_t$, due to the summation.

First, what exactly is $m_{l}'$? Is it the transpose of $m_l$?
Second, how is permutation happening?
$$r_t=\sum\nolimits_{l=1}^{L_i}a_{l,t}m_l$$
Here, equation (5) in the paper, $m_l$ points to the $l^{th}$ vector, $a_{l,t}$ is merely some scalar weight, and $r_t$ is the vector returned.
","['papers', 'long-short-term-memory', 'attention', 'notation']",
Interpretation of the roc curve on test set,"
Hi i got the following roc curve:

What does this mean? has this to do with overfitting? Is my data wrong preprocessed?
i do not understand and would appreciate an answer.
",['machine-learning'],"
You cannot see overfitting in this curve, because to evaluate overfitting, you need to have training and validation losses, and a ROC curve does not present that.
The only informationn you can obtain from this curve is that for your dataset, a random forest is better classifier than KNN or logistic regression, nothing more.
"
Why will every action be sampled an infinite number of times?,"
I am reading the book Reinforcement Learning: An Introduction. Second edition (Richard S. Sutton and Andrew G. Barto). In the k-armed bandit problem using $\varepsilon$-greedy selection method, the authors say that

An advantage of these methods is that, in the limit as the number of steps increases,
every action will be sampled an infinite number of times, thus ensuring that all the $Q_t(a)$
converge to their respective $q_*(a)$.

May I ask why ""every action will be sampled an infinite number of times"" since at each time step $t$ (for limited time steps), we only select one action $A_t = a$?
","['reinforcement-learning', 'sutton-barto', 'multi-armed-bandits']",
agent based DNN with a loopback,"
I have a data problem with no direct reward mechanism,(test/train) good and fault solutions.
Though over a long time period good decisions might be made.
I've been searching for days now for an agent solver, and some example code.
As most neural networks go from input to output on static data sets (train/test).
Essentially one problem, with known results or classifications.
Since there is no direct good/bad reward.  I'm looking for some code example perhaps using multiple evolving agents over time to compete against eachother over previous runs, with a reward that's based on multiple cycli..  Though I'm not after numeric pattern estimators  (ea not RNN or LSTM's) ... what else to use?

as I don't have direct rewards straight backpropagation is not something I can use.

Though there do exist more ways to solve things.
For example:

simple Fuzzy logic   >>though easy thermostat alike rules don't exist for the data.
Q-Learning a form logic that dynamically builds a logic table.
It's mostly used in games, where there is a kinda binary logic (wall, enemy, ..)
And the output are a few choices.

however my data isn't in a common game environment (wall, enemy, .. ).


Genetic-based neural networks, start with randomized weights.
Train and mix and add noise, and move weights towards the best result for each generation.

comes close though I don't have a good/fault set, it's still a hard problem.


Agent-based Genetic neural network, I've seen some evo sims around the idea

The evo sims still have quick rewards, though they come close to it.
But my rewards happen randomly and rarely, not at a fixed interval.
But maybe i'm wrong and i've seen the wrong samples of it.
A good python example of this would still be great.



So far I'm a bit unlucky and I wonder, maybe a differeent NN design like :
another type of network exists, one that's a like a normal DNN, but can have X outputs re-used as inputs besides its data input. This sounds a lot alike LSTM but i'm thinking of a small neural net in which 1 output is for action, the other outputs get a delayed or averaged backpropagation after x cycles, also those redirection to input might have some delay on their feedback loop. (a genetic net still can solve such circulair networks).
(Oh and its not a straight LSTM i'm after here).

How are such networks called?
Is it possible in pytorch / kerras ?.
python examples ??

","['dqn', 'keras', 'pytorch', 'genetic-algorithms', 'learning-algorithms']",
What are some non-RL-based approaches to solving a typical bin assignment problem?,"
What are some non-RL-based approaches to solving a typical bin assignment problem, i.e., given a set of items (can be multidimensional), find the bin/knapsack/target which best packs (with minimum wastage of space) the items?
I tried looking for some papers on this but most of them are RL-based methods or haven't described the approach lucidly, hence am reaching out to the community. Any help in this would be greatly appreciated
","['machine-learning', 'deep-learning', 'reference-request', 'classification', 'algorithm-request']",
Generalize and optimize a model for multiple time series,"
I have a physics equation that takes so much time to be solved computationally.
So the idea is to optimize this computational time with Machine Learning techniques. I've already generated data utilizing the brute force method. So I have a lot of Time Series to train and test my problem. The idea is: given the initial values, predict the final values.
Problem: How can I generalize these predictions individually in a model, in a way that the model could predict well one single equation solution(one-time series) at a time?
To Solve the problem I've considered using an LSTM and looping through it with all these Time series. It worked well, but I think the model was too complex and it has not optimized the computational time well.
Do you have any suggestions?
","['long-short-term-memory', 'optimization', 'time-series', 'prediction', 'numerical-algorithms']",
Why can't AI image generators output verbatim text when prompted to do so?,"
I want to create a splash screen that includes the name of my project.  DALL-E 2 changed some of the letters in the name, even when I tried putting the name of my project in double-quotes ("").
Other prompts to create images with short verbatim text, resulted in text that was not in the prompt:

However, the OpenAI blog post announcement for DALL-E (1) shows an example of the text ""OpenAI"" being output correctly:

What is the mechanism that causes DALL-E 2 to be unable to output the text as expected?
","['transformer', 'image-generation']",
How can (pretrained) language models actively seek additional training data - possibly reference request?,"
I am reading the paper ""Large Language Models Can Self-Improve"" https://arxiv.org/abs/2210.11610 in which the authors consider that LLM can generate Chain-of-Thoughts sequences and even novel questions and their respective CoT responses and then selects the best sequences and fine-tunes themselves. Such self-reflection improves LLM. Such self-reflection also mimics how the human beings learns material by rethinking it. The usual human learning process is guided by the preset questions - courses and textbooks have them. But more mature learners usually can discover knowledge and understanding gaps in their minds themselves and form the most relevant questions and open problems, whose solution can improve the understanding and hence the assimilation of material in the actionable form.
Such process is known in psychology as apperception https://en.wikipedia.org/wiki/Apperception.
My personal experience shows that apperception can be greatly enhanced (sometimes it even required) the reading of additional, carefully selected material. Sometimes it gives additional information. Sometimes it just reformulated the existing information and hints to additional links among the existing pieces of knowledge.
Apparently - if LLM is working in self-improving mode then it could be highly benefitial if LLM could actively seek specific additional information and fine-tune itself on it.
My question is - how to do such active seeking of specific additional information and is there any research efforts that have already done something like that?
Additional info: actually there is a least one paper https://parl.ai/projects/seeker/ (https://arxiv.org/abs/2203.13224) that uses this self-improvement scheme.
","['language-model', 'fine-tuning', 'self-supervised-learning']",
How can I keep markov property when controlling many agents?,"
I am working on a project in which I am training a multiagent system to find a minimum in a scalar field. I have many agents that will receive information about the position of some of the other agents and of its own position. I want to train one model, to take such a state, and give a reasonable action for an agent in this system, and I want the same model to iteratively control all agents. This means that the agent will see the state of the system from a new perspective quite a lot of times before circling back to one of the previous agents.
Example: The model will take the state of agent 1 and give an action for agent 1 to perform. Then move on to agent 2, and perform an action here. It will continue like this until every agent has performed a move, and then it will start back to agent 1.
Will this circling break the Markov property more than necessary? I am currently having stability problems, and I suspect that the Markov property may be a factor. I am aware that a completely markovian state is basically impossible in a multi-agent system, but maybe i can find a more markovian state at least. It is possible for the agent to understand something about how the next state will look from a certain agent's perspective, but I am not sure when the model receives many other states in between. Can I do something about the state for the model to understand what perspective it sees the system from?
","['reinforcement-learning', 'deep-rl', 'multi-agent-systems', 'markov-property']",
Is it possible or useful to perform quantization-aware training on an untrained model?,"
Having done some online searches on the topic of quantization, and more specifically, quantization-aware training, I have noticed that all guides and tutorials start by using a pre-trained model, or by training the model normally (without QAT). None of them explicitly mention starting with an untrained model and applying quantization-aware training directly.
The only documentation I have found about quantizing is in Tensorflow's QAT Comprehensive guide which mentions loading pre-trained weights as ""optional but recommended for model accuracy"", without additional explanation.
I would like to know:

Is it possible or useful to perform quantization-aware training on an untrained model?
If it is not possible, or not advised, what are the reasons?

","['neural-networks', 'training', 'optimization']",
Classify/categorize parts of text with machine learning (Python),"
I need to parse some documents (which consists of questions, answers, transition messages and other texts) into a structured format. Here is an example of input document (part of it):

I have to extract every question and its answer list, intro messages, side notes etc. from each document. The thing is that there are no specific patterns or formatting that I can use. Questions/answers are labeled differently across the documents so regex is a no go. Other texts that I don't need are also usually in free text form (sometimes enclosed in brackets, but not always).
Here is an example of how the model should classify the texts:

I have a large set of documents that can be used to train a model but I am unable to find a proper one that can read the whole document and return the categorized text in any usable form (arrays,json,xml).
Any help here will be much appreciated!
I tried using NER, fuzzy match and some text classification models but the main issue is that I need to classify each sentence separately, which in some cases is not giving proper results.
GPT-3 does the job well but there are couple of issues:

Limited to 4k tokens which means I have to split the document into
multiple parts and my concern is that sometimes the split will not be
proper
Speed - will take lots of time to process large documents even with
fine-tuned model
Costs - 1 document might have 100+ questions which will cost a lot
for processing

Why classifying each sentence separately does not work, you will ask? There are many cases where, for example, the ""intro messages"" have multiple sentences (even whole paragraphs with explanations), or there are question formats like:
Q10. Now, please take a look at the following list of brands.
Which of the following have you heard?

Nike
Adidas
...

If I try to classify this question line by line, the first line (""Now...) will be ""intro message"" and then the second one will be ""question text"", which obviously will not preserve the whole question after the classification.
","['machine-learning', 'python', 'clustering', 'text-classification']",
Why does KNN Model return 99% accuracy on dataset with default parameters? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 9 months ago.







                        Improve this question
                    




I am building a model that predicts if a user will like a stock or not based on different features, such as Market Cap, Current Ratio, Sector, Trailing PE, etc. I am going to implement this model in a website, where the model is able to adapt over time to user preferences. I made an example dataset that generally represents how the data that gets passed to the model will look. I have build a KNN model based on that sample dataset, and I get 99% accuracy on the training split and test split. I also get 98%-99% on my classification report even though I have class imbalance, about 500 of one class, and 10500 of another class. My macro average and weighted average are both 99%. I was planning on optimizing this model by hyper tuning, but does anyone know why I got 99% accuracy immediately? This is how a sample of my data looks like above.
Here is some of my code:
X = data.drop([""Stocks"", ""target""], axis = 1)
y = data[""target""]
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = [""sector"", ""quoteType""]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([(""one_hot"",
                                  one_hot,
                                  categorical_features)],
                               remainder = ""passthrough"")

X = transformer.fit_transform(X)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X) 
X = pd.DataFrame(X)
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

np.random.seed(42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
knn = KNeighborsClassifier(metric = ""minkowski"")
knn.fit(X_train, y_train)
knn.score(X_test, y_test)

y_preds = knn.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_preds))

This produces:
precision    recall  f1-score   support

           0       0.98      1.00      0.99      1227
           1       1.00      0.98      0.99      1012

    accuracy                           0.99      2239
   macro avg       0.99      0.99      0.99      2239
weighted avg       0.99      0.99      0.99      2239

","['classification', 'binary-classification', 'accuracy', 'scikit-learn', 'k-nearest-neighbors']","
First assess whether the accuracy is unreasonably high.  You as the domain expert are the best arbiter of that.  Keep in mind that a high accuracy is not always impossible.  For example, 99% accuracy can be seen on MNIST digits dataset.  It is also possible that you have data leakage, so assess whether this may be at play. The second thing you can do is perform an n-fold cross-validation.  This will provide you with a better estimate (and error bounds) for your model's performance.  If your accuracy remains high on all folds, then you can exclude a spurious split on your holdout validation.
Another issue is that you claim that you have an imbalanced dataset.  However, your print out shows roughly equal support for 0 and 1 classes.
Finally, you are performing preprocessing of your features on the total dataset.  Instead, you should apply fit_transform on your training data, and the just apply the transform on your test data.
"
"Would the optimal policy remain same, if I replace R with V*?","
In the context of RL, say I'm performing Value Iteration on a reward function R1. And the converged optimal policy is P1 and values are V1.
Then, let's say I set rewards to be R2=V1 and perform value iteration from scratch on these rewards and get a value V2 and policy P2.
Is P2 == P1 ?
Rewards can be just dependent on just the states as well, for example, R(s), it doesn't necessarily have to depend on the action being taken in that state.
For MDP = $<S, T, A, R>$, S are set of states, T is transition function as $S \times A \times S \rightarrow [0,1]$, A is set of actions, R is the reward function $S \rightarrow \mathbb{R}$.
Formally, $MDP1 = <S, T(s,a,s'), A, R(s) = R1(s)>$ yields optimal value $v*(s) = v_1$ and policy $\pi_1$,
and $MDP2 = <S, T(s,a,s'), A, R(s) = v_1(s)>$ yields optimal policy $\pi_2$.
Would $\pi_2(s) = \pi_1(s)$? when $\gamma$ = 0, 0 < $\gamma$ < 1, $\gamma$ = 1.
","['reinforcement-learning', 'markov-decision-process', 'reward-functions', 'value-iteration', 'policy-iteration']",
Why is policy iteration guaranteed to converge to the global optimum? [duplicate],"







This question already has answers here:
                                
                            




Why and how can the policy and value iteration methods converge to the OPTIMAL point?

                                (2 answers)
                            

Closed 8 months ago.



In reinforcement learning, what guarantees that policy iteration would find the globally optimal solution and not just any local maximum?
I'm reading the book ""Reinforcement Learning: An Introduction (second edition)"" by Richard S. Sutton and Andrew G. Barto.
In chapter 4 they are discussing dynamic programming methods and on several occasions they mention that policy iteration is guaranteed to converge to the optimal policy because it satisfies the bellman optimality equation.
From wikipedia:

Bellman equation ... is a necessary condition for optimality

Necessary, but not sufficient. The bellman equation is non-linear so I don't see why there couldn't be multiple local maxima for the policy.
Why is policy iteration guaranteed to converge to the global optimum?
","['reinforcement-learning', 'convergence', 'policy-iteration']","
Let me see if I can cover all of the points you mentioned.

The Bellman equation is linear (not nonlinear) in the state values of a given policy. The Bellman equation describes the relationship between the values of different states.

The optimal policy is defined as the one that has the greatest state values, which are called optimal state values. Optimal state values are unique! Optimal policies may not be unique.

The optimal state values and optimal policies are described by the Bellman optimality equation. The Bellman optimality equation is nonlinear in the optimal state values. If you can solve the Bellman optimality equation, then you can get the optimal state value and an optimal policy.

Value iteration and policy iteration are two dynamical programming algorithms than can find optimal state values and hence optimal policies.

Why can the policy iteration algorithm converge (or find the optimal state value for sure)? That is because value iteration can converge because it is an iterative algorithm suggested by the Contraction Mapping Theorem. The convergence of policy iteration can be obtained based on that of the value iteration algorithm.


The detailed proof (and all the points I mentioned above) can be found in this book: Mathematical Foundations of Reinforcement Learning especially chapter 2 and chapter 3.
"
Can i train xgboost on multiple time series csv files at the same time?,"
I built an xgboost model to predict stock it now trains on 1 stock at a time its a csv file I use pandas to load it.
Is there a way to train the model on multiple stocks at the same time? What would be the best approach?
I don't need code just guidance as all my research has failed
","['time-series', 'gradient-boosting', 'boosting']","
The short answer is no.
Time series models model a single time series. If you want to model N time series you need N time series models. XGBoost is no different when used for time series modeling. The only exception to this would be deep learning using transfer learning but that is a different animal.
"
How to integrate program synthesis into program maintenance and evolution (if it is possible at all) - possible reference request?,"
Program synthesis is one of the most active research fields today, e.g. works by Microsoft Research https://arxiv.org/abs/2208.05950 and SalesForce Research https://arxiv.org/abs/2203.13474. I intentionally selected those 2 references, because they clearly resemble the actual programming work by specifying the program intent in the gradual fashion.
All that is fine. But the bulk of the actual program development happens as part of software maintenance and evolution. Which has the several features:

existing code base contains predefined functions and workflows and sometimes all this can be in the form of spaghetti and this should be tolerated to track the code base and to preserve the trust. So - program synthesis should expand its vocabulary by both the predefined functions (with their signature and their semantics) and existing code workflow into which the newly synthesized program (function) should be inserted and which provides the wider context for the evaluation of the correctness of the synthesized function. The user of program synthesis may specify the synthesizable function in the terms of system wide consequences and in the terms how the new function should collaborate with other functions and how it should fit into the code base.

So, the program synthesis for program maintenance and evolution requires more features:

machine comprehension of the existing code base, existing design artifacts (UML diagrams) and test suites and existing user documentation.
ability to extract more or less formal semantics of the existing code and use it to compound the user specified intentions.
ability to do abstract execution or other analysis to predict the consequence of the synthesized function. That is important if the user specifies the intent in the terms how the function should alter the system behavior if function is synthesizes and inserted in some predefined place.

Many of those feature are active research fields, so, some integration can be desired (and even suffice) arrive at the answer to my question.
I have tried to search for some more or less advanced work in this field (e.g. program synthesis for software maintenance, program synthesis for program repair, program synthesis and program comprehension), but I can not find even the initial efforts in this field.
So - is it possible to use program synthesis in software maintenance and how? And are the some efforts and some research work on this?
","['reference-request', 'program-synthesis']",
How can you add data to BERT? Will 10-20 books added affect the word embeddings?,"
I will be using BERT to get word embeddings before performing cosine similarity analysis on my data. According to this paper the accuracy of word embeddings can be improved by updating the model with domain specific textbooks. They do not provide any method to actually do that. Could anyone let me know the method? And how many textbooks might suffice to enrich the model - will 10-20 textbooks be fine?
","['natural-language-processing', 'bert', 'similarity', 'cosine-similarity']",
How to speed up the learning process,"
I have built a network that performans pretty well on my data. The issue I have is that for a larger number of epochs at the start of the training process the val/train acc/loss are stagnating (for some train/val splits the val_losses increase for a certain number of epochs) and then dropping dramatically.
What can be the reason for this, and how can I make it drop faster?

","['training', 'hyperparameter-optimization', 'overfitting', 'cross-validation']",
Group image classification for whether containing unrelated images,"
I'm kind of new to computer vision, and wondering whether this is any existing researches / solutions to following scenarios.
Suppose I have a dataset, each data point contains a few images (< 20 images), and a corresponding label. The label indicate whether those images contain any unrelated images.
For example,
if the images are {apple, apple, apple} then the label is false.
if the images are {apple, basketball, basketball} then the label is true.
I would like to build a model to classify whether those group of images contain such unrelated images.
Seems the idea appeared in my mind is to have a model such as following

image decoder (e.g efficient net) to decoder each image in the data point. Then each datapoint contains a few images after decoding
a self-attention layer for every image above after decoding to measure a similarity matrix for all images in this data point.
flatten above similarity matrix, and then connect to a fc layer into a single node as the prediction result.

Loss function would just be the binary cross entropy with the actual label.
I searched a bit, seems there isn't too much researches about such group classification, so not very sure whether above model make much sense.. Any idea would be appreciate, thanks!
","['machine-learning', 'deep-learning', 'computer-vision', 'attention']","
This is called multiple instance learning, and its deep version is called deep multiple instance learning, there are many algorithms to train a model considering multiple instances at the same time.
Here is a survey paper Multiple Instance Learning: A Survey of Problem Characteristics and Applications
"
Multi-task learning using single encoder + single decoder like structure?,"
It seems that a lot of researchers predominantly use single encoder + multiple decoders like structure to achieve multi-task learning in computer vision. Would it be reasonable to achieve the multi-task learning using single decoder to deal with outputs in different domains (e.g., decoder generates segmentation (class labels) + key-point (xyz data) from encoded 3D point clouds)? My gut feeling is that some of these tasks are very closely related to each other, however different output formats can potentially makes the NNs suffer to learn the information.
I appreciate if anyone can give me some ideas or point out some references related to this.
","['computer-vision', 'encoder-decoder', 'multi-task-learning']",
Matching sentences (/bullet points) in two sets using NLP,"
I am working on a problem where I have two texts T1, T2. T1 contains some important points that I have entered. How can I make sure that T2 has those points? I am aware of the algorithms like cosine, jaccard, BERT for semantic similarity but the problem is that they apply to the whole text whereas I want point-wise similarity i.e. T2 must contain the T1 points although the order and words used may differ a bit.
By points I meant bullet points covering discrete concepts and I basically want to check how many discrete concepts in my T1's points are covered in T2 where they could be in a single sentence or spread out across multiple sentences.
Example:
So T1 could have the following two points:

The Queen reigned from 1943 to 2022.
The Queen was the second
longest reigning monarch.

Now T2 could either be:
The Queen was the second longest reigning monarch with her reign spanning 1952 to 2022.
or

The Queen reigned from 1952 to 2022.
She was Britain's second longest monarch.

In both these cases, T2 should be considered to contain both points in T1.
",['natural-language-processing'],"
If the structure of both $T_1$ and $T_2$ is bullet points, I presume that the similarity between every pair of bullet points is a nice baseline. I'd go through various encoders and see how they compare.
Otherwise, if $T_2$ is a paragraph, maybe split it to sentences and treat those as bullet points.
Other option (which might be harder) is to fine-tune a language model with the following input and label:
suppose the bullet points are $\{bp_1^1,...,bp_1^n\}$ for $T_1$ and $\{bp_2^1,...,bp_2^m\}$ for $T_2$, we can formulate a prompt as

Input: text1=$bp_1^i$, text2=$bp_2^j$
Output: True iff text2 in text1

Where the targets were tagged by you apriori
"
How do you apply Q-learning when there are too many possible actions?,"
When the number of states in the Q-learning is large, we can refer to approximate Q-learning, but what should we do when we have a large number of actions?
","['reinforcement-learning', 'q-learning', 'action-spaces']","
one of the downsides associated with the $Q$-Learning algorithm is that it must initialize a value $Q(s,a)$ for every $s\in S$ and every $a\in A$. If either one of your action space $A$ or state space $S$ is to large, I'd suggest approximating $Q$ instead
"
"To train a mushroom vs. non-mushroom image classifier, which images should comprise the negative class?","
I want to train a model that will identify if the image is mushroom and non-mushroom.
If the image is mushroom, I will use another existing model to categorize if its poisonous or edible.
I want to know what kind of images should I use in training non-mushroom category.
","['convolutional-neural-networks', 'datasets']","
In training a mushroom vs. non-mushroom image classifier, which images should comprise the negative class?
Only you can answer this question.  The answer requires some careful thinking on your part, and it depends on what types of objects that you believe your classifier will encounter once deployed in the ""real world"".  If you are only going to present ""vegetation"" to your classifier, then you probably only need to curate a mushroom (positive class) and plant (negative class) dataset.  On the other hand, if there are no limits to what your classifier will encounter, then your training images for the non-mushroom class must include the visible universe (minus the mushrooms, of course).  Ideally, your positive and negative class images should be collected in the same manner, at least how they would be collected when implemented.
"
MCTS for trick-taking game?,"
I'm trying to implement a MCTS-based AI for a trick-taking card game.
The game : (Belote)

The play consists of 8 tricks.
A trick consists in each of the 4 players play successively 1 (legal) card from their hand (starting with 8 cards).
A player can only play certain cards from his hand (depends on the current state of the trick). Thus, the other players can infer information of some other players hand based on the current and past tricks.

My implementation so far:
The node:
The current state of the round (previous tricks, current trick, the play order...) + the position of the ""smart"" player.
Expansion function:
I can think of 2 possibilities :

a card is played by a player
a whole new trick is played

Other tools that I implemented:

A method that gives the legal cards of a hand (based on the current trick).
A method that gives the smart player all the possible cards of another player (based on the current and previous tricks).
A method that generates every possible next (legal) trick (based on the 2 previous methods).

Problem :
If I want to apply MCTS, I need to ensure that the succession of the simulated remaining tricks (or played cards) until the end of the round (ie the path in the tree) will not be a dead-end before reaching the final (32th) card.
Indeed, a player could run out of legal cards during the simulation...
How can I deal with that constraint in an MCTS context ?
Is MCTS even a viable algorithm for what I want to achieve ?
I could of course precompute the whole possible tree ahead of time, but his size would be huge, and the interest of the MCTS is kind of lost.
","['monte-carlo-tree-search', 'card-games']",
"Why $V^{\pi^*}(s) = \max_{a \in A}Q^{\pi^*}(s,a),\forall s \in S$ in reinforcement learning?","
In some RL notes, I encountered the following equation, which I am trying to prove:
$$
V^{\pi^*}(s) = \max_{a \in A}Q^{\pi^*}(s, a),\forall s \in S
$$
Here is my attemption:
Firstly, I only need to prove ""$\geq$"" as ""$\leq$"" is obvious.
Suppose that $\exists S_0 \subset S$ such that $\forall s_0 \in S_0,V^{\pi^*}(s_0) < \max_{a \in A}Q^{\pi^*}(s_0, a)$.(W.L.G, soppose there is only single state $s_0 \in S_0$), i want to construct a new policy $\pi_{\text{new}}$ to derive the contradiction to $V^{\pi^*}(s) \geq V^{\pi}(s), \forall s \in S$ and all policy $\pi$.
I'm trying to create the following policy:
$$\pi_{\text{new}}(a\vert s)=\begin{cases}
1& s=s_0,a=\text{argmax}_{a\in A}Q^{\pi^*}(s_0,a)\\
0& s=s_0, a\neq \text{argmax}_{a\in A}Q^{\pi^*}(s_0,a) \\
\pi^*(a\vert s)& s \in S - s_0
\end{cases}
$$
Next, I want to show
$$
V^{\pi_{\text{new}}}(s_0) = \max_{a\in A}Q^{\pi^*}(s_0, a)
$$
And thus $V^{\pi_{\text{new}}}(s_0) > V^{\pi^*}(s_0)$ to get contradiction. But when I derived it, I encountered some trouble as follow:
$$
V^{\pi_{\text{new}}}(s_0) = \sum_{a \in A}\pi_{\text{new}}(a \vert s_0)Q^{\pi_{\text{new}}}(s_0, a) = Q^{\pi_{\text{new}}}(s_0, \text{argmax}_{a\in A}Q^{\pi^*}(s_0, a)) \overset{?}{=} \max_{a \in A}Q^{\pi^*}(s_0, a)
$$
I guess the last equation holds, but I can't find a way to prove it. Because the policy changed. Could you please help me?
","['reinforcement-learning', 'proofs', 'bellman-equations']","
I suppose that you try to prove that there always exist greedy optimal policies. The proof is nontrivial. Here is an outline.

Bellman optimality equation (matrix-vector form)

$$v=\max_\pi (r_\pi+\gamma P_\pi v)$$
The solution $v^*$ to this equation is the optimal state value. The solution always exists and is unique. This can be implied by the Contraction Mapping Theorem.

When $v^*$ is solved (by for example value iteration), then we can substitute it into the right-hand side of the above equation, and then solve the RHS to obtain a policy $\pi^*$:
$$\pi^*=\arg\max_\pi (r_\pi+\gamma P_\pi v^*)$$
This policy is an optimal policy and there always exists a greedy optimal policy.

Since $\pi^*$ is greedy, then the definition of state value implies that for any $s$
$$v_{\pi^*}(s)=\sum_a \pi^*(a|s) q_{\pi^*}(s,a)=\max_a q_{\pi^*}(s,a)$$


This is just an outline. It is impossible to post all the details here. You can check this book Chapter 3: Bellman optimality equation.
"
Are there metrics for image complexity for informing neural network design?,"
BACKGROUND: I am trying to think of rational approaches to designing deep learning models for image classification.  One thought is to quantify the complexity of image datasets and use that to inform model design.  By the way, I know that rational model design is much more complex than just quantifying image complexity, but right now, I'm in the brainstorming phase.
In the below examples, I qualitatively describe the complexity of the images as a function of the number of channels, complexity of foreground object, complexity of the background, and the number of classes.  Certainly, there can be many other factors, such as image dimensions or bytes/pixel.  I think all images below are 28x28-pixels and 8-bits/pixel.
MNIST digits           --> greyscale, simple objects belonging to 10 classes and a white background
MNIST digits corrupted --> greyscale, same as above but with added noise
MNIST fashion          --> greyscale, more complex objects belonging to 10 classes and a white background
CIFAR-10               --> RGB, even more complex objects belonging to 10 classes and complex backgrounds
CIFAR-100              --> RGB, same as above but with 10X more classes

Based on the above one has a subjective sense that image datasets can be ordered as shown below based on complexity.  It is reasonable to hypothesize then that the computer vison models should be progressively more complex as well (more neurons, more layers, more parameters, etc.).
MNIST digits > MNIST digits corrupted > MNIST fashion > CIFAR-10 > CIFAR-100

SPECIFIC QUESTION: Are there any existing quantitative measures of image complexity that capture these aspects of image datasets?
PRIOR RESEARCH: Various searches lead me to computational complexity and model complexity, which are not what I'm looking for.
","['computer-vision', 'image-recognition', 'network-design', 'neural-architecture-search']","
In the realm of computer vision and machine learning, ""task"" complexity is more important than image complexity. This is one of the major reasons that research has not formally described any specific measures for image complexity. Fundamentally image complexity is actually defined by task complexity. For example, classifying objects which look very similar to the background is more difficult than objects that distinctly stand out from the background. Additionally model complexity is something dependent on task complexity as well rather than image complexity. You can generally overcome challenges with image complexity by training strategies (say with large variety of samples) even with simple models.
For your specific question, I think the natural approach is to maybe define a basic task and then derive complexity measures that affect the quality of the task. For example, let's say classifying fish images (task), there the image quality metrics can be items like brightness, blurriness of image, amount of bluish tint in the image, etc.
"
Why MSE and MAE yield poor results when used with gradient-based optimization for classification?,"
Deep learning book chapter 6: In 6.2.1.2 last paragraph:
Unfortunately, mean squared error and mean absolute error often lead to poor results when used with gradient-based optimization. Some output units that saturate produce very small gradients when combined with these cost functions. This is one reason that the cross-entropy cost function is more popular than mean squared error or mean absolute error, even when it is not necessary to estimate an entire distribution p(y | x).
explain the above sentence
Doubt: But we use mean squared error (MSE) and mean absolute error (MAE) for regression.
","['machine-learning', 'deep-learning', 'objective-functions', 'deep-neural-networks']",
OpeanAI Gym. Train problem: invalid values [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 8 months ago.







                        Improve this question
                    



I have a problem with my reinforcement learning model.
I am trying to simulate an electric battery storage. To keep it as simple as possible, the efficiency of charge, storage and discharge are 100%. No depletion either.
The battery charges when the electricity prices are low and discharge ONLY to the user at fixed hours during the day, every day.
Therefore, the only cost for the user is power of charge * electricity price at the hour.
1. Define a dataframe where to store electricity prices for 500 days
df=pd.DataFrame(np.random.randint(0,500,size=(24, 100)))

2. Define the main parameters
Lookback_window_size=7
Current_day=Lookback_window_size
P_charge=2 #MW
P_discharge=3 #MW

3. Define the class Battery(Env)
class Battery(Env):
    
    metadata = {'render.modes': ['human']}

    def __init__(self, df):
           
        #Import the dataframe 

        self.df = df

        # The action space is an array of shape (24,). Since we are simulating day-ahead market, the action space returns
        # the overall daily charge / no charge scenario
        # action = 1 means that we charge our battery, action = 0 means that we don't charge
        
        self.action_space= spaces.MultiBinary(24)
        
        # The observation space is a matrix with 24 rows and N columns. The first N-1 columns represent the electricity 
        # prices for the current day + all the days before included in the lookback window size. The last column is all 
        # zeros except the first two rows: they store SOC (state of charge) at the end of the day and overall cost
        # (how much we paid for electricity).
        # Example: if lookback window is 7, then we have a matrix of 24 rows and 9 columns. The first 8 columns are just 
        # used to store the electricity prices, the 9th column store SOC and costs. 
        
        self.observation_space = spaces.Box(
            low=0, high=np.inf, shape=(24, Lookback_window_size + 2), dtype=np.float16)

    def _next_observation(self):
        
        # Add the prices of the last days to the monitor matrix
        
        prices=self.df[[i for i in range(self.Current_day - Lookback_window_size,self.Current_day + 1)]][0:].to_numpy()
                
        extra_values=np.zeros(24)
        
        # Add extra values to monitor such as SOC, cost
        
        extra_values[[0,1]] = self.SOC, self.Cost  
        
        obs=np.c_[prices,extra_values]
        
        return obs        
    
    def _take_action(self, action):
        
        # Being the action space an array, the for loop will check the action at every hour (action[i]) and update the
        # cost and the state of charge
        
        i=0
                        
        for x in action:
            
            #When action = 1 then we charge our battery, if action = 0 then we don't charge

            if x == 1:
                
                # The cost increase based on the price of the electricity at that hour

                self.Cost+=self.df[self.Current_day][i]*P_charge

                # If we charge, then the state of charge (SOC) increases as well

                self.SOC+=P_charge
                
            # Everyday we discharge the battery always at the same hours

            if (i in range(8,14)):

                self.SOC-=P_discharge
            
            # if the battery is depleted, then we directly buy electricity from the grid

                if self.SOC<0:

                    self.Cost+=self.df[self.Current_day][i+1]*(-self.SOC)

                    self.SOC=0
                        
            i+=1
    
    def step(self, action):
               
        self._take_action(action)
        
        self.Current_day += 1

        # Maximizing the reward means minimize the costs
        
        reward =  - self.Cost
        
        # Stop at the end of the dataframe 
        
        done = self.Current_day >= len(self.df.columns)-1

        obs = self._next_observation()
   
        return obs, reward, done, {}    
    
    
    def render(self, mode='human', close=False):

        print(f'Day: {self.Current_day}')
        
        print(f'SOC: {self.SOC}')
        
        print(f'Cost: {self.Cost}')      
        
        print(f'Actions: {action}')
                  
    def reset(self):
        
        self.Current_day = Lookback_window_size
        
        # Give an initial SOC value
        
        self.SOC = 50
        
        # Cost at day 0 is null
               
        self.Cost = 0
        
        return self._next_observation()

4. Train
env = Battery(df)
model = PPO(""MlpPolicy"", env, verbose=1)

Once I run these lines, I get the following error:
5. VALUE ERROR
ValueError: Expected parameter logits (Tensor of shape (64, 24)) of distribution Bernoulli(logits: torch.Size([64, 24])) to satisfy the constraint Real(), but found invalid values:
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)

In more details:
| time/              |      |
|    fps             | 784  |
|    iterations      | 1    |
|    time_elapsed    | 2    |
|    total_timesteps | 2048 |
-----------------------------
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [31], in <cell line: 1>()
----> 1 model.learn(total_timesteps=100000)

File ~\anaconda3\envs\tf-gpu\lib\site-packages\stable_baselines3\ppo\ppo.py:317, in PPO.learn(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)
    303 def learn(
    304     self: PPOSelf,
    305     total_timesteps: int,
   (...)
    314     progress_bar: bool = False,
    315 ) -> PPOSelf:
--> 317     return super().learn(
    318         total_timesteps=total_timesteps,
    319         callback=callback,
    320         log_interval=log_interval,
    321         eval_env=eval_env,
    322         eval_freq=eval_freq,
    323         n_eval_episodes=n_eval_episodes,
    324         tb_log_name=tb_log_name,
    325         eval_log_path=eval_log_path,
    326         reset_num_timesteps=reset_num_timesteps,
    327         progress_bar=progress_bar,
    328     )

File ~\anaconda3\envs\tf-gpu\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py:283, in OnPolicyAlgorithm.learn(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)
    280         self.logger.record(""time/total_timesteps"", self.num_timesteps, exclude=""tensorboard"")
    281         self.logger.dump(step=self.num_timesteps)
--> 283     self.train()
    285 callback.on_training_end()
    287 return self

File ~\anaconda3\envs\tf-gpu\lib\site-packages\stable_baselines3\ppo\ppo.py:214, in PPO.train(self)
    211 if self.use_sde:
    212     self.policy.reset_noise(self.batch_size)
--> 214 values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
    215 values = values.flatten()
    216 # Normalize advantage

File ~\anaconda3\envs\tf-gpu\lib\site-packages\stable_baselines3\common\policies.py:644, in ActorCriticPolicy.evaluate_actions(self, obs, actions)
    642 features = self.extract_features(obs)
    643 latent_pi, latent_vf = self.mlp_extractor(features)
--> 644 distribution = self._get_action_dist_from_latent(latent_pi)
    645 log_prob = distribution.log_prob(actions)
    646 values = self.value_net(latent_vf)

File ~\anaconda3\envs\tf-gpu\lib\site-packages\stable_baselines3\common\policies.py:615, in ActorCriticPolicy._get_action_dist_from_latent(self, latent_pi)
    612     return self.action_dist.proba_distribution(action_logits=mean_actions)
    613 elif isinstance(self.action_dist, BernoulliDistribution):
    614     # Here mean_actions are the logits (before rounding to get the binary actions)
--> 615     return self.action_dist.proba_distribution(action_logits=mean_actions)
    616 elif isinstance(self.action_dist, StateDependentNoiseDistribution):
    617     return self.action_dist.proba_distribution(mean_actions, self.log_std, latent_pi)

File ~\anaconda3\envs\tf-gpu\lib\site-packages\stable_baselines3\common\distributions.py:380, in BernoulliDistribution.proba_distribution(self, action_logits)
    379 def proba_distribution(self, action_logits: th.Tensor) -> ""BernoulliDistribution"":
--> 380     self.distribution = Bernoulli(logits=action_logits)
    381     return self

File ~\anaconda3\envs\tf-gpu\lib\site-packages\torch\distributions\bernoulli.py:51, in Bernoulli.__init__(self, probs, logits, validate_args)
     49 else:
     50     batch_shape = self._param.size()
---> 51 super(Bernoulli, self).__init__(batch_shape, validate_args=validate_args)

File ~\anaconda3\envs\tf-gpu\lib\site-packages\torch\distributions\distribution.py:56, in Distribution.__init__(self, batch_shape, event_shape, validate_args)
     54         valid = constraint.check(value)
     55         if not valid.all():
---> 56             raise ValueError(

     57                 f""Expected parameter {param} ""
     58                 f""({type(value).__name__} of shape {tuple(value.shape)}) ""
     59                 f""of distribution {repr(self)} ""
     60                 f""to satisfy the constraint {repr(constraint)}, ""
     61                 f""but found invalid values:\n{value}""
     62             )
     63 super(Distribution, self).__init__()

ValueError: Expected parameter logits (Tensor of shape (64, 24)) of distribution Bernoulli(logits: torch.Size([64, 24])) to satisfy the constraint Real(), but found invalid values:
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)

What I am doing wrong? Am I defining some variables in a wrong way?
","['reinforcement-learning', 'machine-learning', 'python', 'open-ai', 'gym']",
How to change anchor Box sizes Faster RCNN?,"
I have a dataset where I have to find just the bigger objects so I think I can try changing the anchor box sizes from the default. I am using pytroch and more specifically Detectron 2. I have a some questions regarding how to change the BBoxes so I dug up Detectron and torchvision code to find how it's done.
This official pytorch Code block for AnchorGenerator says that:

This module assumes aspect ratio = height / width for each anchor.Sizes and aspect_ratios should have the same number of elements, and it should correspond to the number of feature maps.

and uses the default values as:
def _default_anchorgen():
    anchor_sizes = ((32,), (64,), (128,), (256,), (512,))
    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)
    return AnchorGenerator(anchor_sizes, aspect_ratios)

Same can be confirmed in Detectron'2 Config file as:
_C.MODEL.ANCHOR_GENERATOR.NAME = ""DefaultAnchorGenerator""
_C.MODEL.ANCHOR_GENERATOR.SIZES = [[32, 64, 128, 256, 512]]
_C.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0, 2.0]]

I have my BBox in the form of xywh and based on that, I generated the below data:

As you can see the width by height ratio stats, you can find that median goes to 5X and mean as 6X. Minimum is 0.65 too. So let's suppose I wanted to generate different types of anchors based on this data,
what should my anchor_size and aspect_ratio look like?
I'm asking this because is does it have to be h/w or w/h? Also, is the anchor_size dependent on the image size?
","['neural-networks', 'machine-learning', 'deep-learning', 'computer-vision', 'pytorch']",
Is a hybrid loss based on hidden (embedding) layer and final classification useful?,"
Say I have a dataset for classification purposes that I will use to train a CNN. Assume the dataset has a lot of distinguishable details that are not necessarily related to the target labels. Would it make sense to use a hybrid loss function, where we combine a loss based on embeddings of an input (from, say, the final hidden layer) with the loss from the actual classification?
Hypothetically, this could enforce that the features extracted from samples of the same class more closely resemble each other, so the chance of the model detecting patterns in the myriad of details unrelated to the label gets smaller.
Example: A dataset of foodstuffs of all different kinds (bread, fruit, meat, whatever). And the label is whether the food is edible or spoiled. This means different things for different food, and the images have a lot of details of the food that are not directly related to the label of edibility. By training the model to generalize embeddings for each class, we can force the model to focus less on the details of each food (such as color, for example) and more on the patterns across the class, such as rapid color changes indicating mold.
In a more general sense, I am trying to combine some of the power of triplet loss (namely the positive sample as feedback) with a conventional loss for classification.
Could this be theoretically beneficial? It would be great if anybody knows of any research on this idea, so I'm not basing it on a hunch.
","['neural-networks', 'classification', 'loss']",
How to evaluate binary classifier on imbalanced dataset?,"
I have trained a Decision Tree model on an imbalanced dataset. I got the following results for the test set from the sklearn and imblearn classification reports (attached below). Moreover, the other scores I calculated from sklearn  are as follows:
AUROC (Area under Reciever operating curve) = 0.979
AUPR (Area under Precision recall curve) = 1.000
Recall = 0.99
Precision = 0.99
F1-score = 0.99
F2-score = 0.99
In my case, both classes are equally important. I have read that micro metrics (Prec, Recall, F1, F2, AUC) may not be good performance measures in imbalanced datasets as they can favor the majority class. For macro metrics, it is said that these are suited for multiclassification problems and may give a false narrative of the classifier in case we use them in binary classification. I am really confused if my classifier performing well or not. If it is performing well on which metrics should I justify this? Or how can I ensure my model is not biased?
Classification Report
               precision    recall  f1-score   support

           0       0.98      0.96      0.97       141
           1       1.00      1.00      1.00   1100416

    accuracy                           1.00   1100557
   macro avg       0.99      0.98      0.98   1100557
weighted avg       1.00      1.00      1.00   1100557

**************************************************************************************************

 Classification report imbalanced
                    pre       rec       spe        f1       geo       iba       sup

          0       0.98      0.96      1.00      0.97      0.98      0.95       141
          1       1.00      1.00      0.96      1.00      0.98      0.96   1100416

avg / total       1.00      1.00      0.96      1.00      0.98      0.96   1100557

```

","['binary-classification', 'decision-trees', 'imbalanced-datasets', 'scikit-learn', 'bias-variance-tradeoff']",
Segmentation of x-ray images to detect Covid-19,"
I’m currently working on covid detection project using x-rays. I applied K -means clustering algorithm (https://www.kaggle.com/code/naim99/image-classification-clustering-step-by-step?scriptVersionId=48171200) used in this link, on x-ray image (The following is the image result).

However, while training a machine learning classifier (RandomForestClassifier) on the segmented images using k-means, the classifier performs a bit worse (performance result in below image on segmented images with accuracy= 0.889… on the left, performance on unsegmented images with accuracy = 0.9..image on the right.

Could an image segmentation technique make the prediction worse? Or does it mean that the segmentation technique of k-means isn't efficient on image x-rays and i should try another way?
","['machine-learning', 'computer-vision', 'image-processing', 'image-segmentation', 'clustering']","
""Could an image segmentation technique make the prediction worse?""
Yes, it is entirely possible that a classifier trained on the segmented image performs worse than a classifier trained on unsegmented images.  One possible reason is that there is information elsewhere in the image that was revealing of COVID status that is removed in the segmented image.  For example, it has been suggested that COVID has impact on more than just the lungs.  Second, the two models that you are training are different, and they can absolutely have different performances.  Finally, it is worth noting that the differences that you are seeing are small and may be within a margin of error.  In other words, by sampling a different batch of images, you may see a reversal in the ranking of model performances.
"
Is ANN architecture mesh topology exist?,"
I'm just wondering if there's ANN architecture that looks like mesh topology at context of computer networking.
If exist or possible, is layer notion still applied?
","['neural-networks', 'architecture', 'hidden-layers']",
How to handle multiple object instances in object detection?,"
I’m constructing a neural net with Keras for object detection for identifying hamburgers. I have a data set with the objects and each image has an array of bounding boxes (there are between 1 and 5 hamburgers in each image, all annotated with bounding boxes).
I understand that when architecting a neural network for outputting a bounding box, the output is 4 regression points, so the network predicts a height, width, and x, y coordinates.
My question is how to handle more than one object of the same class in the image? What should be the output and what should be the input when there are a variable number of objects (and more than one object) in the image?
","['neural-networks', 'machine-learning', 'computer-vision', 'object-detection', 'r-cnn']",
Reinforcement learning PPO-clip agent returning softmax prediction of 1,"
I have discrete action space with 3 actions. I use distributed PPO-clip algorithm with these hyperparameters:

Workers: 32
Optimizer: Adam
Learning rate: 0.000005
Epochs: 20
Batch size: 256
Episode timesteps: 390
Loss-clipping: 0.2
Entropy loss coefficient: 0.001
Model architecture: Shared CNN with 2 convolution layers and then separate 3 dense layers for actor and critic
Output activation in actor: softmax

Model is nicely training till some point and then it is unable to advance. When I test the model I have 973 predictions of action X with value 1 and thousands predictions lower than 1. My idea was to filter actions X based on prediction threshold value. But because of this behavior I cannot say which action is better, because they have all same value of 1. It seems model is over-confident of prediction which is unwanted. I read somewhere this can be fixed by regularization, but I don't know how to implement this in PPO. I am not using stable-baselines, but raw python implementation, which works great in other use-case and it is tested.
Could you please help me how to fix this?
","['reinforcement-learning', 'proximal-policy-optimization']",
ML-based algorithm/software for solving a sudoku puzzle a human way,"
I am a new contributor and have no experience in ML, so this first question is a general one.
I've developed a sudoku solving app and since then I wonder whether it would be feasible to design a ML-based algorithm/software which would mimic the thinking process of a human for solving a sudoku puzzle.
Maybe there exists already such a software?
I don’t mean any existing brute-force algorithm a human cannot apply!
I see many challenges in the design of such an algorithm, e.g. determine that the solution is not unique, uncover the highly sophisticated solving techniques that only talented players know and can apply to hard puzzles.
My thoughts even go so far as to imagine that such an algorithm would ""discover"" a solving technique that we humans have not yet discovered.
","['reinforcement-learning', 'machine-learning', 'sudoku']","
Your problem here is not the application of ML techniques to solving Sudoku. Reinforcement learning might be a reasonable search method, and you could augment it with results from other search methods, to enable it to learn in reasonable time.
Your problem is trying to train AI to behave ""like a human"" whilst having little to no definition for what that is. Supervised learning will require a large dataset of how humans solve the puzzles to copy. Reinforcement learning will require you to provide a reward signal that gives better rewards for when it behaves like a human.
What professional puzzle setters do is write variations of traditional AI search methods that they tune to score difficulties of different search steps, depending on how they define ""easy"" or ""hard"", and take some aggregate score to rate the puzzle as a whole. As an example, you could take the search depth of testing different initial guesses to resolve a single number and look at the consequences. Higher search depth required, and larger number of initial guesses available make finding a number harder.
Some Sudoku setters keep their ""human-like"" search methods proprietary because publishing puzzles is a business. You may be able to find some examples online though.
"
Does a constant background in training data affect image classification model's performance?,"
For a client I needed to figure out if a machine learning model can classify plastic container loading patterns. These containers are delivered on pallets in varying patterns.
So I installed a camera setup that takes images of incoming pallets and saves these images together with the correct label. The region of interest is always in the exact same place in the image and ~40% of the image is background.
After training a model with ML.NET's modelbuilder on a dataset of 7000 images divided over six classes I get a model that's 97% accurate, not bad.
I made a test setup using the code generated by ML.NET in the garage with a pallet and said containers. When testing the model in this setup, the results are disappointing, always predicting the same (wrong) class and with scores as high as 0.999. The test setup is almost identical to the setup that made the training data, except for the background and maybe the ambient lichting.
Is the background the problem here?
Can I solve this with data augmentation? (vertical mirroring, trans, rotate and scale a bit)
Should I write a script to crop the images so mostly the ROI is left?
Or is the data I generated useless?
Now you're thinking why didn't he include some images? Well, my client doen't allow it. Blacking out sensitive areas would leave the image 95% black. But here is a paint doodle of the situation if that helps.
So to clarify the only thing that varies in the 7000 images is the content on the pallet, it varies in pattern and height. The background of the training data never varies and has lots of features.

EDIT:
What I can show is some of the cropped and high-pass filtered data that I used to train the model, one closed example and one open.


","['machine-learning', 'classification']",
Turn 3D Embedding into 2D for classification in PyTorch,"
I am building a pytorch transformer model that shall perform text classification
I have a batch size B, a sequence length T, and an embedding size D
So first my input (B, T) is passed into an embedding layer resulting in (B, T, D). I am doing transformer stuff, resulting in (B, T, D). The last layer ought to be a linear layer. However, a linear layer accepts (B, D), so i tried to reshape the batch into (B, T*D) but with changing sequence length T, I don't know how to initialize the linear layer
My question is, how to I initialize my linear layer to account for changing sequence length or what else might help? The following represents my problem in pseudo code
embed = embeding(input) #(B,T)->(B,T,D)
output = transformer(embed)#(B,T,D)->(B,T,D)
pred = linearLayer(output) #excepts only (B,D)
As said before, I tried to reshape it, but that, of course, did not work. I was looking at other different text classifier, but they simply use nn.LSTM or similar where the computation is running inside the magical box. It tried to look at RNN source code, but it didn't really answer my question. I tried to google my question but it is difficult to explain, even though the answer might be quite simple
","['pytorch', 'text-classification']",
Optimal weight decay value in Adam,"
Is there any rule of thumb while assigning the weight_decay parameter in Adam optimizer?
As in, is it somehow related to (smaller or larger than) the learning rate or is it entirely left up to the practitioner to tune as a hyper-parameter?
","['optimization', 'pytorch', 'hyperparameter-optimization', 'regularization', 'adam']",
Are there free and open-source audio versions of Generative AI programs like Stable Diffusion?,"
Generative AI is being used to create amazing art; first through paid services like Midjourney and now also with free, open source alternatives like Stable Diffusion. Now you can even generate art in a particular style, first with Google's Dreambooth and later with open-source implementations of the same.
Is there a Generative AI program for audio that allows you to create a model/style of a particular voice exemplar?
I am looking to train a voice model in a particular style using the aural equivalent of Dreambooth, and then apply that voice model/style to written text. Ideally I could review several options, and then pick certain ones to expand upon and later upscale.
I have researched this extensively but all of the voice generation services I have found are 1) paid 2) closed source 3) don't allow you to train the software on specific audio samples or 4) don't sound very natural at all.
Are there modern generative AI services for text-to-audio?
",['reference-request'],"
You can check sample-generator, an implementation of stable diffusion for audio data from Harmonai. I'm playing with it recently and I can say it works pretty well already out the shelf. With some hacks I was also able to reduce the model size and run a training on cpu.
And if you have the proper gears they also provide pretrained weights of some of their models.
"
Training and validation loss are almost the same (perfect fit?),"
I am developing an ANN from scratch which classifies MNIST digits.
These are the curves I get using only one hidden layer composed of 100 neurons activated by ReLU function. The output's neurons are activated by the softmax function:

Is it correct that training and validation loss are almost identical? Does it mean that my model perfectly fit the data?
","['neural-networks', 'loss', 'mnist']","
If your accuracy is 50% in train and val, would you say that the model fit the data perfectly?
One guess is that your val and train data are pretty similar. Have you split the data before training the model?
The loss seems quite high. What does your accuracy look like? Have you tried to train for more epochs?
"
"What is it meant by ""cannot use gradients"" in Genetic Algorithms?","
While reading a book on introduction to GA, I stepped upon a chapter where some advantages and disadvantages of these algorithms were described. One of the mentioned disadvantages was ""Cannot use gradients"" but there was no further explanation why. What did the authors mean by that? I couldn't come with a better idea than that you cannot just use a gradient as a fitness function. Still, I don't know why that would be.
",['genetic-algorithms'],"
To answer this question, you must first understand what a gradient is.  This article ""What Is a Gradient in Machine Learning?"" offers a nice introductory explanation.  In machine learning, gradients (which are essentially derivatives) are used to find the minimum of a loss function.  Minimizing the loss function is effectively finding the optimal machine learning model (i.e., the model having weights that minimize the loss function).  Gradient descent is a family of algorithms that use the gradient to minimize the loss function.  The ability to use gradient descent to optimize models is a major advantage.  The article ""Introduction to Gradient Descent"" can help in understanding gradient descent.  Genetic algorithms do not employ a continuous function that can be differentiated and minimized (i.e., no gradient).  Instead, the goal of genetic algorithms is to maximize fitness, but fitness is itself not a defined well-defined function, and no gradient can be calculated and gradient descent cannot be leveraged to optimize fitness.  Rather, genetic algorithms use a distinct algorithm that employs selection, crossover, mutation, and evaluation in a recursive manner to maximize the fitness.
"
Alternatives to brute forcing neural network plateau,"
Below is the loss of the same training run at different scales illustrating the plateau phenomenon.

Source p3.
It seems to me that each dip adds constraints to the neural network optimization, and that neural networks must be highly over parameterized to enable the optimizer to find paths through to the next dip.
I am trying to train timeseries features for a covariance matrix (i.e. einsum('bic,bjc->bij', model(s0), model(s1))). The first dip corresponds to the neural net maximizing the cosine similarities, while being linear in the magnitude i.e. $\gamma\lVert s \rVert =\lVert model(s)\rVert$. The next dip corresponds to the the neural network being nonlinear in magnitude (i.e. silence, and everything else, being mapped onto the unit sphere).
Here are a few responses to plateauing

Regularize the architecture (an architecture that is non-linear in magnitude)
Regularize the loss function (punish being linear in magnitude)
Iterate on the training set to find architectures/hyperparameters that plateau less.
Brute force it. Just leave the neural network training.

I'm sceptical about 1 and 2 as it would only get you past the first plateau. I don't hear much about 3, although I think it might be a good idea. Does anyone do this in practice?
",['neural-networks'],"
I think the primary solution to plateauing is improving the dataset.
The iteration process should be

Train the neural network
Identify what properties are causing the neural network to plateau.
Gather more data, improve the targets, argument the existing dataset, to target these undesired properties.
Find the next plateau, and repeat until the desired results are achieved.

We are changing the loss landscape using the dataset.
"
why validation accuracy be greater than training accuracy for deep learning models? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 9 months ago.







                        Improve this question
                    



I hope you are well.
I had a problem and didn't understand the answers given on questions similar to my question.
If possible, please answer this problem in a simpler way.
Val_acc : %99.4 _
Train_acc : 97.24
Thank you for attention.

","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'deep-neural-networks']","
Why validation accuracy be greater than training accuracy for deep learning models?
You are probably thinking that the training process is supposed to reduce the training loss (and increase training accuracy).  The influence on the validation loss (and validation accuracy) should be secondary, no?  And you are 100% right to think that because the loss function only considers training labels (y_train) and how far model predictions (y_pred) deviate from those training labels.  So why is the validation accuracy higher than the training accuracy?  So long as you are not overfitting (which you clearly are not), the training process should have a benefit for both the training accuracy and validation accuracy because the datasets come from the same distribution.  The reason why you may have small differences like you are seeing here has to do with random sampling of the two datasets from that identical distribution.  (You may have seen i.i.d. elsewhere.  It means independent identically distributed.  Each example in the two datasets are presumed to be i.i.d.)
Bottom line:  The two samples are different, so they have some differences in accuracy.  It just so happens that the accuracy is higher for the validation set in this case (probably within some margin of error).
On a separate note, to get a better estimate of your true accuracy along with error bounds, you may consider performing a cross-validation of some sort.  You can review the scikit learn article to see how this is done.
"
"Question about definitions of ""regret"" and ""no-regret learning rule"" in ""Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations""","

let $\alpha^t$ be the average per-period reward the agent received up until time $t$, and let $\alpha^t(s_i)$ be the average per-period reward the agent would have received up until time $t$ had he played pure strategy $s$ instead, assuming all other
agents continue to play as they did.
Definition 7.5.1 (Regret) The regret an agent experiences at time $t$ for not having
played $s$ is $$R^t(s) = \alpha^t - \alpha^t(s)$$


A learning rule is said to exhibit no regret if it guarantees that with high probability the agent will experience no positive regret
Definition 7.5.2 (No regret learning rule) A learning rule exhibits no regret if for
any pure strategy of the agent $s$ it holds that $$Pr([\lim \inf R^t
(s)] \leq 0) = 1$$

Link to the freely available ebook
Two things confuse me:

I would've thought that positive regret is bad, so that regret should be defined as $R^t(s) := \alpha^t(s) - \alpha^t$, i.e. if the regret is positive we would have done better by playing pure strategy $s$ instead of the strategy we played.
If the definition of regret would be modified according to point 1, I think that definition 7.5.2 would almost make sense. But my first thought would have been to define it as simply $Pr(\lim_{t\to \infty} R^t
(s) \leq 0) = 1$. Why do they use $\inf$ in the definition?

","['game-theory', 'multi-agent-systems', 'regret']",
Admissibility of a heuristic function problem,"
I have the following question about admissibility of a heuristic function. An admissible heuristic function $h(n)$ never overestimates the actual cost from $n$ to optimal solution. So if a function is admissible then for any node $n$ we have $h^*(n) \geq h(n)$, where $h^{\star}(n)$ is the true cost of the optimal path from $n$ to a goal.
In the graph below I have chosen the node B because it has to fulfill the above inequality. But node B does not fulfill this inequality. Am I correct? Any hints!

","['search', 'heuristics']",
How to specify categorical features in cat boost?,"
I have a dataset (pandas data frame) with all features of type int32 containing continuous values except one feature state_number, its data type is int32, but it represents different states of a network (category). The feature state_number has 11 unique values. I want to train a cat boost model on this dataset; how can I specify in cat boost to treat it as a category even though it looks like a number? So far I have tried these steps:

Type cast state_number to category type for both train and test data.
X_train_['state_number'] = X_train_['state_number'].astype(""category"")

While training cat boost, I specified the state_number in the cat_feature parameter.
model_catBoost.fit(X_train, y_train ,cat_features = ['state_number'])


Is this approach correct, or am I adding some bias or doing something wrong?
","['machine-learning', 'binary-classification', 'ensemble-learning', 'categorical-data', 'boosting']",
What is the difference between these two versions of the Bellman equation?,"
The first version is the one I am most familiar with:
$$V_\pi(s) = \sum_{a}^{}\pi(a|s)  \sum_{s'}^{}T(s, a, s')[R(s, a, s') + \gamma V_\pi(s')]$$
where $T(s, a, s')$ represents the probability of transitioning from state $s$ to state $s'$ given action $a$.
And where $R(s, a, s')$ represents the expected value of the reward from transitioning from state $s$ to state $s'$ given action a.
In another textbook, I saw this version of the equation:
$$V_\pi(s) = \sum_{a}^{}\pi(a|s)  \sum_{s',r}^{}p(s',r |s,a)[r + \gamma V_\pi(s')]$$
Is the second equation more general since it considers the probability of going to $s'$ AND getting a reward $r$?
Or are they somehow the same (if so, why)?
Or maybe they are the same under certain circumstances (like conditional independence of $s'$ and $r$)?
","['reinforcement-learning', 'markov-decision-process', 'value-functions', 'bellman-equations']",
How to evaluate the embeddings of a model?,"
If you have a task of extracting embeddings from a model (such as penultimate layer - pre-last layer of the model), would you train the model on a benchmark similar dataset (if there were) or train on a split of the available dataset and use the trained model to extract embeddings of the data altogether?
Given that training on a benchmark clean dataset would get higher performance on the task its being trained on (whether classification, regression, forecasting, or other).
Context: the dataset is a time series dataset which is not very clean - at least I suppose. So I only think of training the model on a similar TS dataset and use the trained model to do inference on my own dataset. The main task is to generate embeddings for my TS dataset over time.
That I think about it again, my question is: how to evaluate the quality of the embeddings extracted from a model.
","['machine-learning', 'natural-language-processing', 'long-short-term-memory', 'time-series', 'embeddings']",
Why are neural networks used as reinforcement learning model value functions? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 9 months ago.







                        Improve this question
                    



My understanding is that a value function in reinforcement learning returns a value that represents how ""good"" it is to be in a given state. How does a network, such as the network in  this example , represent how good it is to be in the given state? Is the neural network value function the best method of achieving such a ""goodness"" value in an environment with many possible states and indefinite length?
Some other examples of neural networks being used as part of the value function of a reinforcement learning model:

https://github.com/facebookresearch/ScaDiver/blob/main/rllib_model_custom_torch.py#L155
https://github.com/ray-project/ray/blob/master/rllib/models/torch/visionnet.py
https://github.com/ray-project/ray/blob/master/rllib/models/torch/recurrent_net.py

","['reinforcement-learning', 'deep-learning']","
The goodness of the state in the network is determined by the activation_fn. The primary role of the Activation Function is to transform the summed weighted input from the node into an output value to be fed to the next hidden layer or as output. That adds non-linearity to the neural network, without that learning would be impossible. It allows the model to create complex mappings between the network’s inputs and outputs.
The activation function used in hidden layers is typically chosen based on the type of neural network architecture. You need to match your activation function for your output layer based on the type of prediction problem that you are solving—specifically, the type of predicted variable.
Also see the update, halfway through this answer.
That can be set as follows:

Activation function descriptor.
Supported values are: ""tanh"", ""relu"", ""swish"" (or ""silu""), ""linear"" (or None).
""fcnet_activation"": ""tanh"",

With a linear transfer function the value would look like this:

Fig: Linear Activation Function - Equation: f(x) = x
It doesn’t help with the complexity or various parameters of usual data that is fed to the neural networks. All layers of the neural network will collapse into one if a linear activation function is used. No matter the number of layers in the neural network, the last layer will still be a linear function of the first layer. So, essentially, a linear activation function turns the neural network into just one layer.
The Nonlinear Activation Functions are the most used activation functions.

Sigmoid or Logistic Activation Function

The Sigmoid Function curve looks like a S-shape.

Fig: Sigmoid Function
The main reason why we use sigmoid function is because its valued between 0 to 1. It's used for models where we have to predict the probability as an output.
Since probability of anything exists only between the range of 0 and 1, the sigmoid is a useful function and is differentiable. That means, we can find the slope of the sigmoid curve at any two points.
The function is monotonic but the function’s derivative is not. The logistic sigmoid function can cause a neural network to get stuck at the training time.

Tanh or hyperbolic tangent Activation Function
The tanh is also like a logistic sigmoid but better. The range of the tanh function is from -1 to 1. tanh is also sigmoidal (s-shaped).


Fig: tanh v/s Logistic Sigmoid
The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.
The function is differentiable and monotonic, while its derivative is not monotonic.
The tanh function is mainly used for classification between two classes.
Both tanh and the logistic sigmoid activation functions are used in feed-forward nets.

ReLU (Rectified Linear Unit) Activation Function
The ReLU is the most used activation function in the world right now, since it's used in almost all the convolutional neural networks or deep learning.


Fig: ReLU v/s Logistic Sigmoid
The ReLU is half rectified (from bottom), f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero, with a range of 0 to infinity.
The function and its derivative both are monotonic, but the issue is that all the negative values become zero immediately; which decreases the ability of the model to fit or train from the data properly.
That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.

Leaky ReLU
It's an attempt to solve the dying ReLU problem.


Fig: ReLU v/s Leaky ReLU
The leak helps to increase the range of the ReLU function. Usually, the value of $\LARGE{a}$ is 0.01 or so.
When a is not 0.01 then it is called Randomized ReLU.
The range of the Leaky ReLU is -infinity to infinity.
Both Leaky and Randomized ReLU functions are monotonic in nature, and their derivatives are also monotonic in nature.
What is a Good Activation Function?
A proper choice has to be made in choosing the activation function to improve the results in neural network computing. All activation functions must be monotonic, differentiable, and quickly converging with respect to the weights for optimization purposes.
Why derivative/differentiation is used ?
When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope, we use differentiation.
It makes it easy for the model to generalize or adapt with variety of data and to differentiate between the output.

You can learn more about differentiation at these blogs: ""Mathematics behind Machine Learning – The Core Concepts you Need to Know"" and ""Application of differentiations in neural networks"".
An excellent resource for activation functions is: ""Ultimate Guide to Activation Functions"".

""Is the neural network value function the best method of achieving such a ""goodness"" value in an environment with many possible states and indefinite length?"".

Yes, a properly trained neural network can not only answer what it was trained for but can also infer additional questions and provide correct answers from a relatively compact representation.
The optimization algorithm is what carries out the learning process in a neural network. There are many different optimization algorithms. They are different in terms of memory requirements, processing speed, and numerical precision
The loss index plays a vital role in the use of neural networks. It defines the task the neural network is required to do and provides a measure of the quality (goodness) of the representation it is required to learn. The choice of a suitable loss index depends on the application.
When setting a loss index, two different terms must be chosen: an error term and a regularization term.
$$\text{loss_index}=\text{error_term}+\text{regularization_term}$$
The error term is the most important term in the loss expression. It measures how the neural network fits the data set.
All those errors can be measured over different subsets of the data. In this regard, the training error refers to the error measured on the training samples, the selection error is measured on the selection samples, and the testing error is measured on the testing samples.
A solution is regular when small changes in the input variables led to small changes in the outputs. An approach for non-regular problems is to control the effective complexity of the neural network. We can achieve this by including a regularization term in the loss index.
Regularization terms usually measure the values of the parameters in the neural network. Adding that term to the error will cause the neural network to have smaller weights and biases, which will force its response to be smoother (with a greater goodness of fit).

""The learning problem is formulated as the minimization of a loss index, $\LARGE{f}$. It is a function that measures the performance of a neural network on a data set.


The loss index includes, in general, an error and a regularization terms. The error term evaluates how a neural network fits the data set. The regularization term prevents overfitting by controlling the model's complexity.


The loss function depends on the adaptative parameters (biases and synaptic weights) in the neural network. We can group them into a single n-dimensional weight vector $\LARGE{w}$."".

For further details see: ""5 algorithms to train a neural network"".
"
"AIMA, Inference Algorithm - Sound or Truth Preserving","
AIMA: Chapter 7, page 216,  Global Edition.

An inference algorithm that derives only entailed sentences is called
sound or truth preserving. Soundness is a highly desirable property.
An unsound inference procedure essentially makes things up as it goes
along it announces the discovery of nonexistent needles. It is easy to
see that model checking, when it is applicable, is a sound procedure.

I am unable to understand the haystack analogy here. Could someone please provide a better example to explain when inference algorithms are said to be sound?
","['logic', 'norvig-russell', 'inference', 'knowledge-base', 'knowledge-based-systems']",
Bias in predictions made by Mixture Density Network,"
I have a very simple Mixture Density Network architecture with 3 input neurons, 10 neurons in the (only) hidden layer and then just one output (which is 2 neurons, one for the mean and one for the std of the prediction). I use ELU as the activation function. Then for the test/validation step, for each point I define dE as the difference between the predicted (i.e. the mean of the prediction) and the real value. I then define a parameter, call it $\mu_{diff} = \sum(dE_i/\sigma_i^2)/\sum(1/\sigma_i^2)$ and $\sigma_{diff}=\frac{1}{\sqrt{\sum(1/\sigma_i^2})}$ which are basically the mean and standard deviation of the prediction errors. The numbers I am predicting have values around 1, however the value of $\mu_{diff}$ is usually around $10^{-5}$ while $\sigma_{diff}$ is around $10^{-6}$, which means that my predictions are biased. I was wondering if anyone knows why could this be the case? I would have imagined the average of the prediction errors to be consistent with zero. I tried adding one more layer, using a mixture of 2 (instead of 1) gaussians, increasing the nodes in the hidden layers and increasing the amount of data (from $10^4$ up to $10^7$ points for training), but I always get this bias. It is not always of the same sign, but it is always not consistent with zero. Am I thinking about this in the wrong way (i.e. am I calculating the average prediction error the wrong way)? Is it an issue with the NN itself and should I go to bigger architectures? Can someone help me understand this issue better? Thank you!
","['bias', 'gaussian-mixture-models']",
How does Tesla's autopilot generate 3d reconstruction of road?,"
Tesla uses stereo vision and in many applications it works OK but ground reconstruction is uniquely difficult since there aren't features to match.
I'm wondering how 3d reconstruction works here? Does the autopilot assume the ground plane is flat or is something else going on to infer road curvature?
","['deep-learning', 'computer-vision', 'autonomous-vehicles']",
"What is the meaning of $ (I - \gamma P^{\pi})^{-1} \left[\frac{\mu(a|s)}{\hat \pi_{\beta}(a|s)} \right](s, a)$?","
In Theorem 3.1 of the conservative q-learning paper, what is the meaning of
$$ (I - \gamma P^{\pi})^{-1} \left[\frac{\mu(a|s)}{\hat \pi_{\beta}(a|s)} \right](s, a)$$?
I thought $(I - \gamma P^{\pi})^{-1}$ is to be interpreted as an operator on function space over state and action space, but it is unclear to me how it is being applied to a $\frac{\mu(a|s)}{\hat \pi_{\beta}(a|s)}$, which is a scalar.
Also, it is unclear what $(I - \gamma P^{\pi})^{-1} \frac{C_{r, T, \delta}R_{\max}}{1 - \gamma}$ means?
","['reinforcement-learning', 'q-learning', 'papers']",
Intel Open VINO training extensions,"
I am experimenting with the openVINO toolkit created by Intel for computer vision and interested in trying to use the openVINO framework to try and train a model for computer vision with the training extensions library. I am trying this approach because the pre-trained models provided by openVINO for people detection I am not getting very good results in my IoT app...and I also don't have a ton of the experience in creating the models so any tips appreciated.
They have a quick start guide for training a model in openVINO and some other steps I found online they state to follow these steps to train the person detection model using OpenVINO™ Training Extensions:

Choose an object detection model from mmdetection model zoo.

Train selected model to person detection model using OpenVINO™ Training Extensions.


But to back step even before getting to steps one and two, I need to have a large image dataset of people right and how large would the data set need to be? I am after for indoor pics of people in conference rooms of buildings where quite often the person is behind a computer laptop and only the upper body is visible, so I am curious if this why some of the pre-trained models turn out poor.
What would be a good model architecture to start with? I am in the process of learning the different types of machine learning models for computer vision, so any tips appreciated what would be a good one to start with go beyond in the mmdetection model zoo options to select.
","['machine-learning', 'computer-vision']",
Multiplying tensors element-wise along one dimension,"
I'm new to TensorFlow and I want to perform the following operation on two tensors. Let $A_{\mu_1,\dots,\mu_M}$ be a tensor of rank $M$ and let $B_{\nu_1,\dots,\nu_N}$ be a tensor of rank $N$. The indices $\mu$ and $\nu$ all run over $1,\dots,d$.
Now let's pick a dimension $i$ for tensor $A$ and a dimension $j$ for tensor $B$. I want to construct the tensor
$A_{\mu_1,\dots,\rho_i,\dots,\mu_M} B_{\nu_1, \dots,\rho_j, \dots, \nu_N}\;\textrm{(no Einstein summation)}$.
That is, I want to multiply the tensors element-wise along two specifically chosen dimensions. Note that I don't want to contract over $\rho$. I want a tensor of dimension $M + N - 1$. The final ordering of the indices doesn't matter as long as I can keep track of them.
EDIT: For clarity, let's just multiply the last index of $A$ and the first index of $B$ element-wise.
EDIT 2: The solution previously posted here as an edit has been posted as a solution.
",['tensorflow'],
how to handle different objectives in Atari games in reinforcement learning,"
My impression on DeepMind's Deep-Q RL for learning Atari games paper is that it uses the same model to learn to play multiple different games at the same time. I wonder how did the RL agent learn in such a setting. In particular, when the game environment changed, how does the agent know the corresponding objective to achieve in that specific game? Is there a prompt to tell the agent what game it is currently in before training/evaluation? Or the agent can infer what is the goal simply by observing the feature from the environment (e.g. by observing blocks of bricks it can infer it needs to break all of them, can by observing there is a controllable vehicle its knows it is probably a racing game)?
","['reinforcement-learning', 'dqn', 'atari-games']",
"Propagating gradients through an ""Item Selector"" network","
Consider the following problem:
There are $N$ items and $S$ slots. Each item is a vector of length $D$.
The goal is to train a neural network to select one item per slot in order to minimize the loss function $L$, given any arbitrary list of items. This loss function could also be thought of as a reward function; it's not comparing labels, it just outputs the 'value' of some slots/items configuration, and it's differentiable. (In my particular application, this function is another MLP which can be trained separately; it's an actor-critic setup. But this problem applies to any sort of differentiable loss/reward function).
Assumptions:

The same item can be used multiple times in different slots.
We have an infinite number of training examples available; they don't need to be labelled so they can be generated automatically.
$S \times N$ is large, such that we can't practically enumerate the value of $L$ for all possible configurations of items in slots for a particular example. (If we could, then this would turn into a simple supervised learning case).

Here's an example where $N = 8$, $S = 4$, $D = 1$. The neural network is an MLP which outputs an $S \times N$ matrix of logits. These are then converted to a binary matrix which is multiplied with the original list of items to get the selected item in every slot, on which the loss can be calculated.

The problem is what should happen between the neural net output and the binary 'selection' matrix (The ??? bubble in the diagram).

It's natural to do an argmax/one-hot operation on the logits (i.e. push the max logit to 1 and the rest to zero). But this operation isn't differentiable, so the MLP can't be trained.
You could do a softmax instead, which has good gradients, but then the problem is broken because the selected items are weighted combinations of items in the original list, which isn't allowed.
You can also do softmax with ""temperature"", e.g. softmax(logits * 10e5), to get a more argmax-like behavior in a differentiable function. But in practice, the gradients are low quality and the network doesn't train properly.
You can try argmax for the forward pass, but softmax for the backwards pass - this kind of works, but again in practice the neural network does not converge to a good solution.

Is there any kind of neural network architecture or training regimen that can solve or sidestep this problem? Or is this application fundamentally unsuited for back-propagation?
","['deep-learning', 'deep-rl', 'architecture']","
As soon as you discretize the selection, i.e. make a hard selection (argmax) instead of a soft selection (softmax), you have a biased gradient. This is because the things you didn't select are not included in the gradient calculation.
However, it is possible and one way to create such a biased gradient is to apply both techniques. You can use the argmax method in the forward pass and the softmax (with temperature) in the backward pass .
Here is what that could look like, independent of framework:
def differentiable_select(logits, temp=0.6):
    # turn logits into a categorical distribution (soft-selection)
    logits_dist = softmax(logits / temp)
    # get the maximum probability for the forward pass
    hard_selection_idx = logits_dist.argmax(-1)
    hard_selection_mask = one_hot(hard_selection_idx, depth=logits.shape[-1])
    # for the forward pass, return the hard_selection_mask, 
    # for the backward pass, return the logits_dist (soft-selection)
    return logits_dist + stop_gradient(hard_selection_mask - logits_dist)

# generate the selection mask
select_mask = differentiable_select(logits)
# select from x (x-shape: [n_items, n_dims])
x_select = x * selection_mask

The central part is the stop_gradient function. In PyTorch this corresponds to Tensor.detatch() and in tensorflow you'd call tf.stop_gradient.
That way, in the forward pass, you have the hard selection:
logits_dist + hard_selection_mask - logits_dist == hard_selection_mask

and in the backward pass, you have the soft selection by using logits_dist. The temperature ($\tau$) is a hyperparameter that trades training stability (if $\tau$ is close to $1$) vs. similarity to the forward pass (if $\tau$ is close to $0$). In my experiments, a temperature of $\tau = 0.6$ worked pretty well.
This solution is derived from the Straight-Through Gumbel-Softmax (ST-Gumbel-Softmax) function. In the original implementation, the ST-Gumbel-Softmax function includes non-deterministic sampling from the categorical distribution.
For further reading, you can look at the following sources:

[Paper by Bengio et al. on ST-Gumbel-Softmax]
[PyTorch Code Example]
[Blogpost by Fabian Fuchs]
[Maddison et al.]

"
What is the right way to normalize the initial weights of a fully connected layer using a SiLu (Sigmoid-weighted Linear Unit) activation function?,"
I've been writing a deep learning Java framework as a way for myself to learn how it all works and I have had a decent amount of success so far. Best performance is just over 90% accuracy with three fully connected sigmoid layers of 500, 300 and 10 nodes respectively using LeCun normalization for the initial weight values.
However, in trying to improve the accuracy of my model on the MNIST dataset I've been looking at alternative activation functions. I've tried ReLU with Kaiming normalization and I found that this works well when training on 10.000 images (about 85±5% accuracy), but loses accuracy (~40%) after training on all 60.000 training images, possibly due to more neurons dying over time.
I've been trying to get the SiLU (Sigmoid-weighted Linear Unit) activation function to work, but no matter what initialization I use I find the weights exploding fairly quickly. The SiLU activation function is defined as x*sigmoid(x) and essentially has a small wave just below 0, akin to GELU, to prevent dead neurons.
How should I normalize the initial weights of SiLU layers for them not to explode?
I tried LeCun-, Glorot- and Kaiming normalization. They all ended up exploding and getting 0% accuracy.
","['activation-functions', 'multilayer-perceptrons', 'weights-initialization', 'weight-normalization']",
how to apply crossover and mutation rates in genetic algorithm?,"
I'm working with genetic programming and let's say I have the following operator:

pop_size = 100
Crossover ratio = 0.4
Mutation Ratio = 0.2
Selection Ratio = 0.1

What is exactly the next generation will look like?

Select 10% from the last pop with any selection scheme.
Fill 40% of the next generation with the crossover between parents.
Mutate the new offspring based on that mutation ratio before adding it to the next generation.
That will be 50% of the next generation filled up, and we have to fill the remaining 50%, how to do this? by mutating the individuals from the last generation according to the mutation ratio? add some random genes to the next generation?

","['genetic-algorithms', 'heuristics', 'genetic-programming', 'genetic-operators']","
The remaining will be models selected from the original population based on the best performers (or some other user defined criteria).  You don't do anything to those models - no crossover and no mutations - just keep them as they are.
"
How to feed extracted features to the final detect layer of yolov5?,"
I am trying to feed extracted features from 23rd layer of Yolov5m to the final detect layer of yolo.
I load features using np.load as features are saved np arrays.
shape is 768x20x20 as you are probably aware that it can be written as (N x (3x (80+5)) * 20x20 . here 80 represents the classes in coco , 5 represents the bounding box coords + obj and 20x20 represents the grid size.
This is my final model with just detect layer
# (no) means number of outputs per anchor and is equal to >> nc(number of classes) + 5>> 80
# na means number of anchors
# ch input channel(for rgb it would be 3)
    self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv
        x[0] = self.m[0](x[0])  # conv >>>> this raises error

source: https://github.com/ultralytics/yolov5/blob/master/models/yolo.py
line#53 and line #60
Please have a look and suggest sth. I have been trying since weeks and have hit a roadblock.
Thanks!

background:
I am trying to implement a multi-view object detection use-case where a number of self-driving cars interact with each other to detect occluded objects. so these cars share features instead of images which are later fused together to make  a prediction.


","['computer-vision', 'yolo', 'features', 'feature-maps']",
Encoding data in which the order of features does not matter,"
My input to the model is a set of features that I encode in the form of five vectors of the same size consisting only of 0 and 1. I now want to combine them into one vector in such a way that their order does not matter.
My first idea was to run each vector through the same activated linear layer and sum the results.
Is there maybe a better way?
","['deep-learning', 'linear-algebra']","
Is there maybe a better way [to combine them into one vector in such a way that their order does not matter]?
There are many ways to perform feature fusion (see ref).  The way you described is one way of doing it.  However, for most simple cases where feature fusion is for the type of data you are describing (i.e., tabular data), it is more common to concatenate the features at the input level.  The order of concatenation does not matter to the neural network.
"
MCTS with multi actions,"
I know that MCTS usually is meant for games where each player plays turn by turn and the canonical form of the board is passed through the tree but is it possible for one player to make multiple moves and still he compatible for MCTS? For example a game of checkers where one player can jump multiple times and capture several pieces in “one” turn. I think if when I pass through the tree, instead of changing to canonical form and updating the player turn, I do neither and don’t negate the final value from the search to be compatible with the multi actions. Is this a viable solution or is there something else?
","['game-ai', 'monte-carlo-tree-search', 'board-games']",
Gumbel Softmax- Hard vs Soft backprop significance,"
For gumbel-softmax in pytorch, can the choice of the ""hard"" parameter have an effect on backprop?
","['machine-learning', 'deep-learning', 'pytorch']",
Neural Network output for the game of Checkers,"
I'm trying to train a RL agent to play the game of checkers (AlphaZero style) and so far I've managed a proof of concept training a connect 4 agent up until perfection. However, unlike connect 4, checkers moves pieces rather than placing them and sometimes even multiple times. I think I understand how I would do this for chess: I have an output size of 80 (16+8*8) and have the first 16 outputs represent the piece that will move and the other 64 represent the position it will move to. I'm not sure if this is a valid solution though. The real problem arises when considering checkers with multiple jumps. Is there any solution to this and am I thinking about it the right way? I've pondered not changing the player turn whenever a double jump is available but I feel like this will screw with the MCTS.
","['neural-networks', 'reinforcement-learning', 'deep-rl', 'alphazero', 'board-games']",
Very high ACC (ca. 95%) with 1DConvNet for Time Series,"
Does this sound legit, for people working with CNN and Time Series?
I have a Framework that applies Dynamic Tim Warping (DTW) on time series, using the DTW distance matrix, I  cluster my data and create labels for each time series and then apply a 1DConvNet to make classification.
I end up with a very high validation accuracy, ca. 95%, and it also does not really look like overfitting or underfitting to me. But a 95% accuracy is too high in my eyes. I have a dataset that has 150 time series and each has 200 time steps. For clustering, I use KMeans Cluster with 4 clusters.


","['convolutional-neural-networks', 'time-series', 'anomaly-detection', 'semi-supervised-learning', 'validation']",
Why is the Alphafold PAE (predicted aligned error) not symmetric?,"

Hi,
We are running alphafold2 multimer on colab to predict the association between two proteins. It generally works fine, but we get an asymmetric PAE plot for a specific pair. Does anyone know why? I assumed these must be symmetrical ALWAYS.
","['neural-networks', 'deep-learning', 'alpha-fold']",
How to select pseudo label samples that minimize validation loss?,"
I have a problem about meta pseudo labeling, I want to select the most significant pseudo-labels that minimize validation loss. Let's say i initialize a set of pseudo label denoted $Y_{pseudo}$, then i perform parameter update by gradient descent
$\theta_{t+1} = \theta_{t}-\alpha\nabla L(f(\theta_{t}),Y_{pseudo})$
I want to select a subset of $Y_{pseudo}$ that actually minimizes validation loss apart from those that increase the validation loss.A naive approach is that I have to brute force to remove a sample at a time and calculate validation loss to update parameter $\theta_{t+1}$ and calculate $L(f(\theta_{t+1}(Y_{pseudo}),Y_{val})$  until there is no sample that increases the loss. I try to figure out what way we can remove unwanted samples just by update parameters once, what's in my mind is that i can select sample that have the gradient $\frac{\partial L(f(\theta_{t+1}(Y_{pseudo})),Y_{val})}{\partial Y_{pseudo}} \le 0$ in that way i know that the loss function of some pseudo-label is going down so i can select them but in the case $\frac{\partial L(f(\theta_{t+1}(Y_{pseudo})),Y_{val})}{\partial Y_{pseudo}} \le 0$, it can be minimum or maximum value of loss function, then do i have to use hessian matrix to determine its direction? Anyone knows about this, please help me! or if you have other way to select by calculating the validation loss only once, feel free to share with me, I really appreciate it.
","['gradient', 'meta-learning', 'derivative', 'active-learning', 'validation-loss']",
AI in signal processing: how to reduce data volume,"
I am studying 5G technology, where AI/ML is integrated to imporve performance. I am not expert in AI/ML and I am sorry if my question is stupid.
Honestly i am quite not sure If I understand the main concept of AI
My question: AI/ML can predict received data, right? I understand it as channel estimation methods in signal processing/ receiver side processing. If we can predict the received data, can we decrease data volume using AI?
",['signal-processing'],"
5G technology incorporates Αrtificial Intelligence in a number of ways.

It can help to manage Network traffic and optimize its performance.
It can also be used to improve the accuracy of predictions about future network conditions. In addition, AI can be used to develop new 5G applications and services.
It can be used for Predictive Maintenance.
Site Selection
Network Design


Yes, AI can predict received data by analyzing Past Data and trends in digital signal processing.
AI can definitely be used to decrease data volume. By using prediction methods, AI can learn to anticipate what data is coming in and only send the necessary data. it can be used to automatically extract features from signals, which is a difficult and time-consuming task for humans.
here are a few ways where AI can be used to reduce data volume in Signal Processing:

Pattern Recognition Technique in digital signal processing to automatically identify patterns in digital signals. This can then be used to reduce the overall data volume.

Denoising can be used to reduce the amount of data required to represent a signal.By removing noise from a signal, AI can improve the Signal-to-Noise Ratio (SNR), which can lead to improved communication quality in 5G systems.

Dimensionality reduction & Feature Selection Techniques that can help identify a smaller set of features that are most relevant to the task and remove redundant data, thereby helping to reduce the number of features that need to be considered, which can again reduce the amount of data that needs to be processed.

Finally, prediction can be used to generate new data that can be used to represent a signal, which can reduce the overall volume of data that needs to be processed.



For example, if we know that a particular data stream will be mostly static for a period of time, we can send fewer data during that time period, since we know that the prediction will be accurate. This can help to reduce bandwidth usage and improve efficiency.

Here are some other potential uses of AI in signal processing include:

Automated signal enhancement –  Removing background noise from an audio signal.

Automated feature extraction –  Extracting relevant features from an image for further analysis.

Automated pattern recognition – Identifying patterns in data that indicate a particular phenomenon.

Optimizing signal processing algorithms –  Finding the most efficient way to process a given signal.

Developing new signal processing methods –  Using AI to design novel algorithms for processing signals.

Selecting optimal Sensor and actuator placements-  Selecting the best locations for sensors and actuators to get the most information from or control over a system.

Designing digital filters- Designing filters that remove specific types of noise from a signal.

Incorporating domain knowledge into signal processing – Using expert knowledge to design more effective algorithms.

Interpreting results – Providing meaning to the results of signal processing for decision-makers.

Generating hypotheses – Formulating hypotheses about how a particular signal was generated based on its characteristics.

Explaining results – Providing justification for the results of signal processing to stakeholders.


"
How to classify data into organised groups by using a resulting classification vector and a set of probabilities? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 9 months ago.







                        Improve this question
                    



I am trying to figure out the best way to calculate the probability a sentence belongs to some category.
For simplicity sake, lets assume that the sentence is ""yellow fruit"".
Next, I use the an BERT classifier to get a classification result [-5,1,2] with the categories [apple, orange, banana].
The array simply tells us how strong is the signal that corresponds to a category. In this case -5 for apple, 1 for orange, and 2 for banana.
The answer to this classification is ""banana"" since 2 is the max number in the [-5,1,2] array, and it position corresponds to the category ""banana"".
Therefore, we know that ""yellow fruit"" is most-likely a banana.
Next, I have a table of likelihood of purchase for apples, oranges and bananas:
Apple: 80%
Orange: 10%
Banana: 50%
I need to calculate the likelihood of customer purchase for sentence ""yellow fruit"".
Therefore, I am given the sentence ""yellow fruit"", the classification vector [-5,1,2] the classification categories [apple, orange, banana] and the likelihood table for purchase [0.8,0.1,0.5]
How do I calculate the the likelihood of purchase for the sentence ""yellow fruit"" ?
","['natural-language-processing', 'classification', 'bert', 'probability']",
How to normalize input data to Reinforcement learning platform (Gym and stable-baselines),"
I created a custom environment with Gym and trained it with stable baseline 3 algorithms.
The observation and space action are both continues.
The observation space includes 10 values and action space has 2.
The action space is [0,1] and I know it's not symmetric.
I want to normalize my inputs to the NN and I Don't Know how to do it.
The environment observes Temperature and Power (input values) and takes action (continuous between [0,1]) and returns state (10 values not normalized)
I believe I have to normalized all inputs to get better results but I don't know how can I normalize it bf feeding to neuronal network and unwrapped it afterwards.
I appreciate any help specially if you can link some dummy code or a practical case.
","['neural-networks', 'reinforcement-learning', 'deep-learning', 'gym', 'stable-baselines']",
what is the idea behind gated-attention CNN,"
I have the below code for gated attention:
class Attn_Net_Gated(nn.Module):
    # Attention Network with Sigmoid Gating (3 fc layers). Args:
    # L: input feature dimension
    # D: hidden layer dimension
    # dropout: whether to use dropout (p = 0.25)
    # n_classes: number of classes """"""

    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):
        super(Attn_Net_Gated, self).__init__()
        self.attention_a = [nn.Linear(L, D), nn.Tanh()]
        self.attention_b = [nn.Linear(L, D), nn.Sigmoid()]
        if dropout:
            self.attention_a.append(nn.Dropout(0.25))
            self.attention_b.append(nn.Dropout(0.25))

        self.attention_a = nn.Sequential(*self.attention_a)
        self.attention_b = nn.Sequential(*self.attention_b)
        self.attention_c = nn.Linear(D, n_classes)

    def forward(self, x):
        a = self.attention_a(x)
        b = self.attention_b(x)
        A = a.mul(b)
        A = self.attention_c(A)  # N x n_classes
        return A, x

And as input I have a feature vector of my image with size of (1, 1024) that I got from ResNet50 encoder. I do not really get what is the idea of element-wise multiplication of tanh and sigmoid outputs? This code is adopted from the paper https://arxiv.org/pdf/1802.04712.pdf.
","['convolutional-neural-networks', 'attention', 'sigmoid', 'tanh']",
Devise a model/Goal Based Agent for simple Pacman Game,"
I have to program simple pacman game in figure below that consisting of 4*4 grid (not GUI based).

Explaination
The starting point of pacman is cell 0 and its goal is to consume/eat maximum food pallets, while considering following given limitations.

Pacman can move up, down, left right (keeping in view walls).
Pacman can eat power pallets, i.e., cherries to keep ghost scared, i.e., if pacman enters the ghost cell its is not destroyed.
Pacman keeps moving until all the power pallets are consumed.

I'm new to python and this is what I have done so far:
import numpy as np
import secrets

grid = np.array = [
    ['pacman', 'specialFruit', 'foodPallet', 'specialFruit'],
    ['foodPallet', 'enemy', 'foodPallet', 'specialFruit'],
    ['specialFruit', 'foodPallet', 'foodPallet', 'enemy'],
    ['specialFruit', 'foodPallet', 'enemy', 'foodPallet']
]
actions = ['left', 'right', 'up', 'down']
scores = {'foodPallet': 1, 'specialFruit': 2, 'enemy': 5}

I have declare a 4x4 grid. Actions(i.e, pacman can move left, right, up, down). And respective scores (i.e, if pacman has cherry it can kill ghost).
class setting:

def __init__(self):
    self.score = 0
    self.cherry = 0

def pac_enemy(self):
    if self.cherry >= 1:
        print(""Pacman has killed the Enemy"")
        self.score = self.score + scores['enemy']
        self.cherry = self.cherry - 1
    else:
        print(""Not enough Cherries: Game Over"")
        score = 0
        quit()

def pac_cherry(self):
    print(""pacman has eaten the cherry"")
    self.score = self.score + scores['specialFruit']
    self.cherry = self.cherry + 1

def pac_food(self):
    print(""pacman has eaten the foodPallet"")
    self.score = self.score + scores['foodPallet']

def pac_scores(self):
    print(""Score = "", self.score)
    print(""Cherry = "", self.cherry)

I made a class from which i will use the function according to the situation example: if pacman move at grid location 1, I will call pac_cherry() function and so on.
class PacMan(setting):

def game(self):
    while 1:
        choice = secrets.choice(actions)
        if choice == 'right' or choice == 'down':
            print(""Pacman moving "" + choice)
            break

    if choice == 'right':
        print(""Pacman is Moving "", choice, "" at location 1"")
        if grid[0][1] == 'enemy':
            self.pac_enemy()
        elif grid[0][1] == 'specialFruit':
            self.pac_cherry()
        elif grid[0][1] == 'foodPallet':
            self.pac_food()

        grid[0][1] = 'pacman'
        grid[0][0] = ''
        self.pac_scores()
        print(grid)

I am moving pacman randomly as If oacman is at location 0 it can only move location 1 or location 4 and so on.
Basically, by doing this method there will be so many states for pacman(agent). I'm confused and not sure how will It be work. Kindly suggest me any other method or some tips, algorithm, or suggest me a way to complete it efficiently.
","['reinforcement-learning', 'state-spaces', 'model-based-methods', 'simple-reflex-agents']",
"Is there anything remotely as successful as backprop, but for training programs, not neural networks?","
Backprop is used to train deep neural networks to remarkable success. Deep neural networks, on the other hands, can be seen as as a specific kind of computer function that receives inputs and produces outputs. Suppose, though, that one wanted to train functions in a programming language like Python or Haskell, that solve a specific problem. Due to the discrete nature of these objects, there is obviously no way to apply backprop to generate these. My question is: is there any algorithm that has satisfactory success in the task of training or generating these functions?
The last time I had this question, I learned about program synthesis, but the field seems to be way behind machine learning, in special due to the lack of satisfactory training procedures. What is the state of the art and where can I find the relevant literature?
","['machine-learning', 'genetic-algorithms', 'genetic-programming', 'program-synthesis']",
"What reward should be selected for transition states to make the agent reach the terminal state (destination) faster? negative, positive, or zero?","
Consider the simple environment below, where the gray cells are the terminal states and the agent receives a reward of $-5$ for taking any action in these states. The nonterminal states are $S = \{1, 2, . . . , 14\}$. There are four actions possible in each state, $A = \{up, down, right, left\}$, which deterministically cause the corresponding state transitions, except that actions that would take the agent off the grid in fact leave the state unchanged.
My question is which value of $R_t \in \{-5, -0.5, 0, 5\}$ will yield a policy that returns the shortest path to the terminal state? Let's assume the agent starts from cell $12$.
The discount factor is assumed to be γ=0.9.

","['reinforcement-learning', 'markov-decision-process', 'optimal-policy']",
How are OCR training datasets constructed?,"
For the sake of concreteness: let's suppose that the word ""OCR"" refers to any OCR system build on an R-CNN architecture. Similarly, in aims of simplicity, let's declare that we are interested in reading digits between 0 and 100.
Question: How should I construct a dataset (given the aforementioned goal and architecture) ?
My understanding is that I need to collect images of all the digits from 1 to 100 and label them with its corresponding digit. Is this premise correct?
My struggle is that I can't fully understand how this seemingly tedious procedure is generalized to OCR that read more general types of characters (language-characters for example or if I generalize the problem to detect the numbers from 0 to $10^{10}$).
Thanks in advance!
","['convolutional-neural-networks', 'computer-vision', 'optical-character-recognition', 'r-cnn']","
OCR - optical character recognition identifies individual characters. So in the case of numbers, there are just 10 classes to learn. And this doesn't depend on how large numbers we are looking at. There is a separate step to segment observed numbers (or words) into single digits (or characters).
But on low-quality images this character segmentation is non-trivial, so better (and more complex) systems work with a larger context.
"
"When training an LSTM, should you pad your dataset so the sequence length is static, or should it be variable?","
I am putting together an LSTM network using visual basic. It's more of a learning exercise really, but it's also the only programming language I have access too at work.
I am unsure of how to prepare my data. If I have a dataset of sequences with variable lengths, should I pad my data with 0s to make them all equal to the longest sequence in my datatset?
Or do I simply set an ""end of line"" marker at the end of the each sequence and let the LSTM network learn when a particular sequence should end.
I ask because, I'm not sure how this will affect the training of the LSTM cell. Is it better to have a static sequence length, or variable? Or is this simply a matter of preference?
I'm leaning more towards variable sequence lengths because padding may increase processing time needlessly. But I'm not really sure. Any thoughts?
","['long-short-term-memory', 'data-preprocessing']",
What can go wrong using batch norm?,"
Andrej Karpathy said he's shot himself in foot over and over gain with batch norm.

I mentioned no one likes this layer. It causes a huge amount of bugs and intuitively it's because it is coupling examples in the forward pass of a neural net and I've shot myself in the foot with this layer over and over again in my life and I don't want you to suffer the same so basically try to avoid it as much as possible. Some of the other alternatives to these layers are for example group normalization or layer normalization.

How exactly can you shoot yourself in the foot with batch norm?
","['neural-networks', 'batch-normalization']","
I think the problems with batch normalization arise mainly due to the large misunderstanding of what it does, combined with a simple but confusing name.
Despite the simple computation (everybody knows what normalization is), it is not so clear why batch normalization provides the benefits it does. In the original paper the main explanation given was that batch normalization reduced covariat shift for the parameters in very deep layers, but there are many papers out there who proved already that batch normalization also smooth classic loss functions, which means it can be considered as a regularization layer. And indeed some papers pointed out that combining batch norm with other regularization techniques like dropout doesn't really provide any benefit. You can find all the papers link in the first answer to this question.
So this already answer how you can shoot yourself in the foot with batch normalization, said that it's hard to say a priori when you should use it or not, unfortunately like many other aspects of deep learning this is still a matter of trials and errors.
"
How to make my neural networks designs more robust,"
Whenever, I design a neural network to solve a novel problem (requires a novel loss function i.e. not image classification) it always ends up being hypersensitive to batch size and learning rate.
Andrej Karpathy compared training neural networks to balancing a pencil on your figure. It seams things like image classification are only workable because people have stumbled on to magic tricks like batch norm, skip connections, relu. And as you venture away from image classification, machine learning returns to a hypersensitive balancing act.
Andrej Karpathy recomends understanding backprop/autograd. I imagine reducing a batch of losses by mean rather than sum, will help make the gradients more invariant to the batch size resulting in the neural net being more robust to choices of the learning rate. Currently, my loss function involves calculating a covariance matrix and probably dividing by $n^2$ would help with the hypersensitivity.
I'm wanting to know if there are any references of how you can use your understanding on backprop to design neural networks that are more robust (less hypersensitive) to arbitrary choices of hyperparameters.
Is the job of machine learning engineers to balance hypersensitive hyperparameters (i.e. find lucky choices) or is the job to design networks that are robust to arbitrary choices?
It seems like most people just use exactly what architectures the gods at google research have handed to them (i.e. transformers haven't really changed) rather than taking a more humanistic approach of making these problems more tractable.
","['backpropagation', 'hyperparameter-optimization', 'hyper-parameters', 'resource-request']",
What makes TRPO an actor-critic method? Where is the critic?,"
From what I understand, Trust Region Policy Optimization (TRPO) is a modification on Natural Policy Gradient (NPG) that derives the optimal step size $\beta$ from a KL constraint between the new and old policy.
NPG is a modification to ""vanilla"" policy gradient algorithms that modifies the direction of the update using the Fisher information matrix such that the direction is in that of maximal improvement per unit of KL divergence between new and old policies.
Now, I've skipped a lot, but throughout most introductory material on these two algorithms, I see no mention of a critic (i.e. auxiliary evaluator value function), yet  know that at least TRPO is considered actor-critic. Why is that? Where does the critic come in?
","['reinforcement-learning', 'policy-gradients', 'actor-critic-methods', 'trust-region-policy-optimization', 'policy-based-methods']","
TRPO still uses the advantage function, which computed using the critic (aka value function).
"
How to embed quantitative variables?,"
I have 3 data types that I want to feed into a neural net.
One, is a time series, which I am going to feed into the neural net directly. The second, are categorical variables that I am going to embed into vectors of equal length as the time series. The third, is quantitative numbers that I also want to embed like my categorical variables, but I do not know the best way to do this.
My initial thought is that I could input the numeric data into a vector of all 0's.
For instance, if a numeric data point is $1.32$, then:
$[1.32, 0, 0, ..., 0]$.
Or I could do something similar like: $[1.32, 1.32, 1.32, ..., 1.32]$.
Is there a common best practice for what I am trying to do?
","['neural-networks', 'embeddings']",
Why do transformers have a fixed input length?,"
From what I understand, Transformer Encoders and Decoders use a fixed number of tokens as input, e.g., 512 tokens. In NLP for instance, different text sentences have a different number of tokens, and the way to deal with that is to truncate the longer sentences and pad the shorter ones. As an additional input, a padding mask must be supplied to the Transformer so that its attention is only focused on the relevant tokens.
My question is: Is there something in the architecture that forces the transformer to have a fixed number of tokens as input? (and not adopt dynamically to the actual input length like RNNs for instance?)
For comparison, I think of fully-convolutional networks or RNNs with variable input lengths. They are agnostic to the actual input dimension because they perform pointwise operations on the different patches.
When applying an RNN model to an n-tokens sentence, you compute the same block n times, and when computing it on a k-tokens sentence you will apply it k times. So this architecture does not require padding or truncating (at least not in theory, I do not refer here to implementation considerations).
In transformers: embedding the tokens, computing attention, and feed-forward can be performed on different lengths of sequences since the weights are applied per token, right? So why do we still truncate and pad to a fixed size? Or perhaps it is feasible but not implemented in practice for other reasons?
I must be missing something...
I'll ask it differently to make my question more clear:
Say I have an already-trained transformer model, trained on 512 fixed-sized inputs (truncated and padded). At inference time, if I would like to process a single, shorter sentence. Do I have to pad it or not?
Thanks
","['transformer', 'models']","
Edits to reflect edits in question:
If you train your transformer on length = n then yes, you need to pad inputs to length = n. This is not a requirement in the mathematical architecture, it's a requirement in the implementation.

There seem to be two separate ideas in your question:

Why do transformers have a fixed input length?
Why do transformers have input length limitations?

I am not sure which one you are asking, so I will answer both.

1) Saying transformers have a fixed input length is misleading.
Transformers accept variable length inputs just like RNNs. You can think of padding/truncating as an extra embedding step if you want.
We don't need padding in RNNs because they process inputs (sentences) one element (token/word) at a time.
Transformers process inputs all at once. If you are passing in several sentences, you have to do something to regularize sentence length, hence padding and truncating.

2) Transformers often limit input length to 512 or 1024 because of performance issues.
If you are wondering why we don't let our transformers accept inputs as long as possible, the answer is that there are computational and performance limitations. The algorithm complexity is quadratic.
This is where the max_length parameter of a transformer comes in. If your input has length 1,000, the transformer will throw an error because it can only handle inputs up to 512 in length.
"
What are other types of knowledge bases except entity-entric KB?,"
I learned a term entity-centric knowledge base, then I wonder what the non entity-centric knowledge graphs are?
I googled by keys words ""entity-centric knowledge base vs"" and got some articles with this most related one: A comparison between Entity-Centric Knowledge Base and Knowledge Graph to Represent Semantic Relationships for Searching as Learning Situations, then I thought ""Knowledge Graph to Represent Semantic Relationships"" would be the answer. But I searched and found that knowledge graphs are just large-scale knowledge bases.
I wonder how entity-centric knowledge base, knowledge graph and knowledge base relate to each other? Maybe ontology-centric knowledge bases?
","['knowledge-representation', 'ontology', 'knowledge-graph', 'knowledge-base']",
Val loss doesn’t decrease after a certain number of epochs,"
I’m working on a classification problem (500 classes). My NN has 3 fully connected layers, followed by an LSTM layer. I use nn.CrossEntropyLoss() as my loss function. This is my network’s configuration
Model(
  (fc): Sequential(
    (0): Linear(in_features=800, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (lstm): LSTM(256, 1024, bidirectional=True)
  (hidden2tag): Sequential(
    (0): Linear(in_features=2048, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=500, bias=True)
  )
)

This is what my loss looks like. It increases after reaching a particular value no matter what setting I use. I’ve used k-fold cross–validation but the loss/accuracy across all folds stays the same, so I’m assuming that there’s no issue in the distribution of train/val splits.
From left(Iteration-wise loss (train loss recorded after every batch), train loss (recorded after every train epoch), validation loss (recorded after every val epoch))

I’ve tried using step-wise LR scheduling (tried OneCycleLR and Multiplicative LR as well) but the loss still doesn’t improve

I’ve tried using dropout but that gives higher validation loss values instead
What can be done here in order to further decrease the loss?
","['deep-learning', 'classification', 'long-short-term-memory', 'pytorch', 'loss']","
I fully agree with @Snehal Patel's comment but want to expand on the capacity of the model a bit.
The easiest approach to reduce overfitting is to reduce the capacity of the model, i.e. the size of its layers.
Therefore, in your first sequential model, instead of three linear with [1024, 512, 256] nodes, it would probably be better to have just two with [512, 256] nodes.
The same applies to the second sequential model. Just using one linear layer probably reduces the capacity of the model to overfit to the data.
I hope those changes work out for you.
"
Unexpected behaviour on using class weights in loss,"
Iâ€™m working on a classification problem (500 classes). My NN has 3 fully connected layers, followed by an LSTM layer. I use nn.CrossEntropyLoss() as my loss function. To tackle the problem of class imbalance, I use sklearnâ€™s class_weight while initializing the loss
from sklearn.utils import class_weight

class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_org), y=y_org)


The below plots show the number of predictions made per class

Here, x-axis represents the class and y-axis is the number of times it occurred (as prediction and as ground truth).
The pattern observed here is that the ground truth and predicted class almost always complement each other. i.e., a class is predicted more times when itâ€™s available less number of times in the ground truth and vice versa (every peak of the blue curve has a dip in orange curve and vice versa). What could be going wrong here?
","['deep-learning', 'classification', 'pytorch', 'scikit-learn']",
Higher validation loss after using Dropout,"
I’m working on a classification problem (500 classes). My NN has 3 fully connected layers, followed by an LSTM layer. I use nn.CrossEntropyLoss() as my loss function. I noticed that my model was overfitting (validation loss increased after some epochs) and hence I used a dropout (p=0.4) in the last FCN layer. I’m now seeing a higher loss (for both train and val)
The green line denotes training without dropout

From left(Iteration-wise loss (train loss recorded after every batch), train loss (recorded after every train epoch), validation loss (recorded after every train epoch))
Is this behavior justified or am I doing something wrong?
","['deep-learning', 'classification', 'pytorch', 'regularization', 'dropout']",
SQL Machine Learning using matrix multiplication,"
What is the easiest classification algorithm in SQL when my data looks like this?
A = 101000101110
B = 010101110010
C = 100101101000
B = 100101101101
C = 100010000001
A = 100010010101

These are binary vectors that indicate the presence of a certain attribute.
I was thinking about training some kind of regression using python that gives me a vector.
Then I would only need to multiple the vector with the binary vectors and I would bet some number indicating me what kind of label I have using a threshold method.
Are there any python algorithms that allow me to do that?
","['machine-learning', 'python', 'linear-regression']","
You have a few overlapping topics here:

Fitting a model to your data in Python
Storing model parameters to the database
Executing SQL queries to apply the model

I would start with a linear model, such as logistic regression or linear discriminant analysis.
You should think carefully how you structure the data in the database. This is more of a programming topic, but I would have a table with few columns and several rows to represent each item. This way you don't need to modify the schema if your data gets a new input feature.
Then as a final step you'll need to ""just"" write the SQL query, joining attribute data and model parameters and output. But this is off-topic on an AI forum.
Getting back to linear regression, you must re-arrange the binary representations into separate columns:
array([[1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0],
       [0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0],
       [1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0],
       [1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1],
       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
       [1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1]])

"
"How can I use my neural network model, trained on MNIST database, on ""real word"" digits such as my handwritten digits?","
I have developed a feed-forward ANN from scratch trained (and evaluated) on MNIST database, which contains 60,000 + 10,000 handwritten digits samples.
Can I test my model on other digits, for example I write the digit 7 on a paper with my pen and check whether my model would recognize it or not? How can I do it?
","['neural-networks', 'mnist']","
Can I test my model on other digits, for example I write the digit 7 on a paper with my pen and check whether my model would recognize it or not? How can I do it?
Yes, definitely.  One of the main goals of machine learning is to be able to train a model and use it on your own data to make predictions.  BUT you must ensure that the handwritten digits are produced and processed in the same way as the training data.

Example 1:  If the MNIST digits dataset uses black pen, you must also use black pen (preferably of the same type).
Example 2:  If the MNIST digits dataset has digits on a white background, you must also produce digits on a white background.
Example 3:  If the MNIST digits dataset has digits that are centered in a 28x28-pixel image, then you must do the same.

This will help keep your dataset ""within distribution"" as the training dataset.  Departures from the pipeline that generated the training data will result in degradation in performance.  By the way, there is usually some degradation in performance anyway when models are deployed in the real world.
After you have captured and processed your ""custom"" handwritten digits, you must package them in a way that can be processed by your model.  For example, models in Keras/Tensorflow will take images as Tensorflow tensors and Numpy arrays.  You should preferably package your data the way MNIST dataset is packaged in your download.  Keras has the .predict() method that allows you to make predictions / inferences on new data (see article).
"
Pose Sequence embedding,"
I am using a pose detection software which returns pose — a set of coordinates of thirty-ish points. I record a person for some duration (10 seconds to 1 minute). I want to have the embedding of the sequences of these poses so that I can feed that embedding into a model to analyze it further.
I have seen embeddings for single pose but not for a sequences of poses.
A follow-up question: is it okay to have a single embedding for different kinds of activities or do I need to train activity-specific embeddings?
","['sequence-modeling', 'embeddings', 'pose-estimation']",
Why clip the PPO objective on only one side?,"
In PPO with clipped surrogate objective (see the paper here), we have the following objective:

The shape of the function is shown in the image below, and depends on whether the advantage is positive or negative.

The min() operator makes $L^{CLIP}(\theta)$ a lower bound to the original surrogate objective.
But why do we want this lower bound? In other words, why clip only at $1+\epsilon$ when $A > 0$ ?
Isn't it important to keep the new policy in the neighborhood of the old policy, so that even $r_t(\theta) < 1-\epsilon$ should be undesired?
","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization']","
Yes, the idea of PPO is to keep the updates small so that the new policy is not too far from the old policy. If you look at the left figure, this is the case as the absolute magnitude of L^clip is capped. The only region where this absolute magnitude is uncapped is on the right hand portion of the right figure. In this region, r is > 1. Since r = new prob / old prob, it means the previous update has increased the probability of an action that resulted in a worse than expected outcome (hence the negative advantage). Therefore, we want to unroll that update and not capping the ratio will achieve that goal better.
"
Action space for a single agent environment with multiple actions,"
How to define an action space in a gym environment where the agent´s output is a tensor of shape [1,25]?
I am working on Travelling salesman problem using DRL where the NN(agent) output`s the sequence of cities as a tensor of shape [1,N_cities]. Thus, there are N_Cities actions at a single step.
However, I am not sure about the action space to be used.
","['reinforcement-learning', 'deep-rl']",
How does accuracy (for a single class of interest) of a segmentation network vary with increasing number of classes?,"
BACKGROUND:  I have a real world problem of developing a U-net-like model for segmenting lung tumors in lung CT images.
On the one hand, I can make this a two class problem:
 1. lung tumor
 2. not lung tumor

On the other hand, I can make this a multiclass problem (for example):
 1. lung tumor
 2. lung
 3. trachea
 4. heart
 5. blood vessels
 6. etc.

Ultimately, I'm only interested in getting the lung tumor right.  Thus, the ""class of interest"" (mentioned in the title) is lung tumor.  All other classes are not necessary for my analysis, BUT my intuition is telling me that including more classes may improve the model's ability to segment lung tumors.  My rationale is that not lung tumor is very heterogeneous category that can be difficult for a model to classify a pixel into, but giving the model a set of more specific categories for commonly occurring structures may be helpful to model learning.  I am aware that the multiclass approach results in more manual annotations (i.e., time, fatigue, and costs), but that is not the focus of this question.
SPECIFIC QUESTION:  Which approach -- the two class or multiclass -- will yield higher accuracy for lung tumor segmentation on CT images?
REQUIREMENTS:  I seek a response that sheds deep insight into this problem.  Responses founded on established mathematical principles are acceptable.  Responses founded on empirical evidence from credible literature references are acceptable.  The evidence need not be specific to lung tumors, but it should be directly relevant to segmentation tasks.
PRIOR RESEARCH:  I have scoured the literature for a head-to-head comparison of these two approaches, but have not found anything useful for lung tumor segmentation.
RELEVANCE TO THE AI COMMUNITY:  While my problem is related to lung tumor segmentation, I believe the problem type is of general interest to the broader AI community.  For example, one may be interested in segmenting peoples' eyes in facial images, and one can ask whether they should have two classes (eyes vs. not eyes) or multiple classes (eyes vs. nose vs lips vs. chin vs....).  Prior knowledge about which method is better would help people plan their annotation approach before embarking on a long and expensive annotation journey.
",['image-segmentation'],"
Maybe not a satisfactory answer but I hope this will give you at least a slightly new perspective on how to think about binary vs multiclass problems in a machine learning way.
Let's start from the math: Binary classification and multiclass classification are basically the same, i.e. every loss formulated for binary classification is a special case of the same loss generalized to n classes. If you see explicit classes for both in every main machine learning frameworks is just due to specific different implementations for boosting a bit performances. But if we look at the formula of the main loss used for classification, i.e. cross entropy, you'll see that there is no difference at all between 2 and n classes

Cros entropy: $$-\sum_{c=1}^{M}y_{o,c}log(p_{o,c})$$
Binary cross entropy (the sum between the 2 classes is just expanded): $$−(ylog(p)+(1−y)log(1−p))$$

Moving on to your rationale: Not lung tumor is an heterogeneous class, hence splitting it into specific classes might help the model learning better features for each one of them. I don't agree a lot with this rationale for a simple reason: first, if it's possible to lean good features for subclasses of a macro category then mapping all those classes back to the main class should be trivial to learn for any deep learning architecture. The opposite instead does not hold. This is why despite the same math behind, tasks with less classes are usually easier to tackle. The only AI area I can think about that leverage this kind of rationale is multi task learning, in which similar tasks like classification and object segmentation/detection are combined to leverage more ground truth data. But in this case the advantage is the combination of different losses that help learning hierarchical features.
So what to do: I personally feel that the right angle to look at the problem is investigating the amount of data at your disposal and their labels distribution. If your fine grained labels generate a sort of balanced distribution then I would go for multiclass, if not I would go for binary classification, even if the binary labels distribution is highly skewed, since for this scenario Focal loss (a weighted variation of cross entropy) works just fine.
"
Initial State of RNN,"
Can I initialize the initial state of my RNN to be non-zero? I have some initial condition of the sequence and I want to use this initial condition as the initial state.

","['recurrent-neural-networks', 'weights-initialization']","
Can I initialize the initial state of my RNN to be non-zero?  Yes, the initial state / weights of a neural network can be initialized to non-zero values.  In fact, the trainable weights can be initialized any way you want.
Exactly how you do this depends on the deep learning package that you are using (if you are using a package).  For example, if you are using Keras/TensorFlow, there is built-in functionality to initialize layer weights and set model weights.  Here is a nice article demonstrating how this can be done:  ""How to set custom weights in keras using NumPy array"".  The article uses random normal initialization, but you can use your desired initial state / weights.
"
Prooving the convergence rate of estimators (machine Learning),"
I want to estimate a quantity and have two choices for estimators (they both sample from the same distribution). I suspect one of them has a higher variance and thus a slower convergence rate. I want to mathematically prove this, but I don't know where to start. Is there a standard reference in statistics that I can consult for example proofs of convergence rates in ML/statistics?
","['machine-learning', 'convergence', 'variance']",
Generative question answering system with recommendation engine for text documents,"
I have a collection of documents with semi structured text (i.e. html documents).
Now, I would like to generate questions and answer options from this collection of documents, let users then answer these questions, which shall then lead to recommendations, based on the users' answers.
Do you know any papers / approaches that have developed an approach like this? Which model architecture would be most suited?
I am thinking of a generative model for the question - answer generation, which should then be combined with a recommendation algorithm.
Another difficulty here is that I am not dealing with objective questions, where one answer is the correct one, but with subjective ones.
I would appreciate any suggestions or links. Thank you.
Please find a descriptive image here:

","['generative-model', 'recommender-system', 'question-answering', 'text-generation']",
What approach finds an aproximation to a function provided only score?,"
I want to approximate an expensive function without having the training data of correct input-output pairs, instead having the learning model quarry specific input-output pairs and my supervisor (if that's the correct term) assign a score how good the output is for the given input in the pair. Alternatively, a random input would be generated, the model would give an output and the supervisor would give feedback to the model - a simple scalar score, representing how well it did.
The function to approximate has 7 float inputs and 3 float outputs. After the training I should have a fast approximate function (in the form of a multiplication matrix, a neural network or anything like that) that I can plug any 7 numbers into and get 3 numbers out and it being approximate to the original expensive function.
The goal for the model would be to maximize the score of an output for any given input. Generating the correct input-output examples before the training is not feasible.
I've been looking for how this type of machine learning model is called. So far the suggestions have been linear regression and evolutionary function. If it's one of them, could you give more specific details or keywords, maybe there's a sub-type? Because when I try to learn about those they seem completely different from what I need. I'm a beginner so I need something specific I can search for to look for a library that implements this approach.
P.S.:
I'm a beginner so sorry if I'm misunderstanding something in this field.
Motivation: As far as I know the only way to get the truly correct output is to computationally solve a specific optimization problem (including searching for a global minimum in 3-space) for every given input. I don't want to generate input-output pairs as examples for the training because that would take me too much time to implement in code and also a long time to compute, and even then I wouldn't be 100% sure the example outputs are truly the global minima. Whereas a score assigning function is easy for me.
","['terminology', 'objective-functions']",
Solutions for free-form bin packing?,"
I'm working on a solution for free-form bin packing and what I have found so far is
https://www.researchgate.net/publication/267165843_A_scanline-based_algorithm_for_the_2D_free-form_bin_packing_problem
But, still, I want to find some other varied solutions for this problem. Is there a specific name for searching keyword or python library for this problem?
Thanks
","['papers', 'optimization', 'algorithm', 'knapsack-problem']",
Why does MNIST provide only a training and a test set and not a validation set as well?,"
I was taught that, usually, a dataset has to be divided into three parts:

Training set - for learning purposes
Validation set - for picking the model which minimize the loss on this set
Test test - for testing the performance of the model picked using metrics such as accuracy score

How is MNIST only providing the training and the test sets? What about the validation?
","['datasets', 'training-datasets', 'mnist', 'validation-datasets']","
There are a few reasons why MNIST only provides training and test sets, and not a validation set:

Since, the MNIST dataset is intended to be a simple and straightforward benchmark for machine learning models, It is important to have a standard test set that can be used to compare different models.

Secondly, the MNIST dataset is well-known and well-studied.

Thirdly splitting it into three sets (training, validation, and test) would reduce the size of each set too much & there is less need to have one.


The validation set is used to assess the performance of the model on unseen data. In this case, the validation set is not needed because the performance of the model can be assessed on the test set.
Another common method is to use cross-validation, which is where the data is split into multiple sets and each set is used to train and test the model. This allows for a more accurate assessment of the model's performance.
"
What is an example of an *optimal* stochastic policy that assigns a nonzero probability to an action with a lower expected value?,"
A stochastic policy means that an agent has probabilities of choosing their available actions, given a state: $\pi(a|s)$.
However in an optimal stochastic policy for a given state, you would assume that there would be a single optimal action that would be assigned a probability of 1 since it yields the highest expected value. Thus, an optimal stochastic policy would become a deterministic policy.
Of course, sometimes there are multiple actions that are all tied for the highest expected value so they are each assigned an equal probability. For example, in the adversarial game of Rocks-Paper-Scissors, each action would be assigned a probability of one third. A deterministic policy in this case would give the adversary a huge advantage.
My question:
Is there an example of a situation/scenario/game where an optimal policy is both stochastic AND assigns a nonzero probability to an action with a lower expected value than the action with the highest expected value?
Edit:
I was thinking maybe poker could be an example, but in what scenario why would I ever allow for an action with a low expected value be available to be chosen?
Maybe a situation where we would have to take the variance in the expected value into account (maybe the state has missing information). For example, maybe action B has a lower expected value that action A, but a higher variance so exploring action B would be useful in reducing uncertainties in the expected values?
","['reinforcement-learning', 'policies', 'deterministic-policy', 'stochastic-policy']",
Do Support Vector Machines have the ability to learn while in use?,"
I've read in some literature,that SVMs are characterized by their adaptivity. Does that mean they can learn while in use?
","['machine-learning', 'support-vector-machine']","
Support vector machines are a supervised machine learning method that can be used for both classification and regression problems. They are based on finding a  Maximum Margin hyperplane that best separates the data into different classes or predicts the output value.
Support vector machines are not designed to learn online while in use. They are trained on a fixed set of data points and then used to classify new data points.
However, they can learn online, if they are implemented with online learning algorithms. Online learning algorithms can update the SVM model with new data that arrives in batches or streams, without retraining the model from scratch every time. This can improve the performance and efficiency of the SVM model in dynamic or large-scale applications.
There are some extensions of support vector machines that can enable online learning, such as incremental SVM, kernel recursive least squares, and budgeted SVM.

Incremental SVM helps in online learning by updating the SVM model with new data that arrives in batches or streams. It does not need to retrain the model from scratch every time new data comes in, which saves time and resources. It also can handle concept drift, which means that the data distribution may change over time. For example, if you are using SVM to classify spam emails, the features and patterns of spam emails may change over time as spammers use different techniques to avoid detection. Incremental SVM can adapt to these changes by adding or removing support vectors as needed.


Kernel recursive least squares helps the SVM to learn online by performing online regression with nonlinear kernel functions. It updates the model parameters and the kernel matrix with new data, using a recursive algorithm that is similar to the Kalman filter. It does not need to store all the data or solve a large optimization problem, which reduces the memory and computational requirements. It can also handle noisy data and outliers, by using a forgetting factor or a sparsity criterion. For example, if you are using SVM to predict the stock price of a company, the data may be noisy and contain outliers due to market fluctuations or external events. Kernel recursive least squares can filter out the noise and outliers and update the model with the most relevant data.


Budgeted SVM helps in online learning by performing online classification or regression with a fixed number of support vectors. It updates the model by adding new support vectors and pruning the least relevant ones when the budget is exceeded. It uses a strategy that balances the trade-off between accuracy and sparsity. It can also handle nonlinear kernel functions and different loss functions. For example, if you are using SVM to classify images of animals, the data may be large and diverse, and you may have a limited memory or computational capacity. Budgeted SVM can reduce the size of the model and the complexity of the calculations by keeping only the most informative support vectors and discarding the redundant or noisy ones.


Does ""kernel trick"" help with online learning?

The kernel trick does not directly help the SVM to learn online, but it helps the SVM to solve nonlinear problems by mapping the data into a higher dimensional space where it may be easier to find a linear hyperplane. The kernel trick also avoids the explicit computation of the mapping function and the coordinates of the data in the higher dimensional space, which can be costly and inefficient. Instead, the kernel trick uses a kernel function that can replace the inner product of the mapping function. This makes the SVM more flexible and scalable.


SVMs are characterized by their adaptivity, Does that mean they can learn while in use? $\rightarrow$ I think you may have misunderstood the meaning of adaptivity in the context of SVMs. Adaptivity does not mean that they can learn while in use, but rather that they can handle different types of data and problems, such as linear or nonlinear, classification or regression, and various kernel functions. SVMs are characterized by their adaptivity because they can find the optimal hyperplane that separates the data with the maximum margin, regardless of the shape or dimensionality of the data. However, once the SVM is trained, it does not change its parameters based on new inputs.

References:

Support Vector Machine — Introduction to Machine Learning Algorithms.

Incremental learning algorithm of least squares support vector machines based on Renyi entropy

An incremental LS-SVM learning algorithm ILS-SVM

A novel incremental Kernel Nonparametric SVM model (iKN-SVM) for data classification: An application to face detection


"
"Purchase prediction - What minimum feature will ""upgrade"" A rule-based algorithm into an ""AI"" algorithm?","
Let's say I have an algorithm that tries to predict if a given item will be bought or not, within an X timeframe, based on it's price and other attributes.
To Do this right now, I'm collecting historic data about similar items, like prices, and I assume that if a new given item is cheaper then let's say, the avg. price of similar items, I can predict it will be bought within X timeframe, and in that case I will output ""1"" instead of ""0"".
All of this is rule based - the rule is to look at the average price, if below, output ""1"".
So, This isn't considered as machine learning, right? What kind of features will ""upgrade"" this algorithm to be considered as ""AI""?
","['machine-learning', 'prediction', 'algorithm']","
You are correct.  This is not machine learning.  Your current approach is a rule-based approach to binary classification.  However, you misunderstand what makes something machine learning.  It is not that you need an ""upgrade in features"".  Instead, you need to use an algorithm that takes examples (many pairs of features and truth values) and ""learns"" the relationship between the features and truth values.  This is what we call training.  Then, when you give the model new features, the model will be able to predict the corresponding truth values.  This is what we call inference.  There is no ""hard-coding"" / no ""rules"" that you need to program with machine learning.
"
How to encode categorical data for a convolutional model?,"
Is there a way to encode categorical nominal (no ordered) data to be used in CNN models?
Let's say I need to create a 1D CNN model for categorization of time series but the values are not measurements, but categories like for example colors:

red, red, blue, green, blue, red, green
blue, red, green, green, red, red, green
green, blue, green, red, red, blue, red
and so on (can be 2D array of colors input as well)

It seems to me it would be hard to one-hot encode it. On the other hand If I change colors into integers like: 1-red, 2-green, 3-blue, then wouldn't convolutional layers assume these are ordinal or discrete data and proceed as such?
","['deep-learning', 'convolutional-neural-networks', 'data-preprocessing', 'time-series', 'statistics']","
This is just my opinion, but I don't think that what you are proposing can be done on categorical data, nor do I think you can meaningfully encode categorical data so that it can be a substrate for 1D CNNs.  In a 1D CNN, a 1D kernel (a vector of numbers) would slide over your 1D dataset and carry out numerical operations on that dataset.  This cannot be done with categories (one-hot encoded or not).
This article has a very good and simple explanation of how 1D CNNs work:  Understanding 1D and 3D Convolution Neural Network | Keras.
The Kaggle exercise in the Computer Vision course also provides valuable information (see Lesson 4, Exercise)
"
MDP with a non-markovian reward function?,"
I have set up a RL environment and it converges to a decent solution when using a reward function:
$R(s_t,a_t) = fenv(s_t, a_t)$ , where $fenv$ is the environment dynamics.
Now, i want to change the reward function such that
$R(s_t,a_t) = fenv(s_t, a_t)*c(s_t:s_{t-10})$
where $c(s_t:s_{t-10})$ is a penalty term that depends on the average performance of previous 10 timesteps. So now the reward is dependent on agent's previous states and needs information from the past. I suspect this can change the underlying problem and the MDP assumption is not valid anymore.
I read about potential based reward shaping that guarantees invariance in terms of MDP but i am not sure if this transformation falls into that category.
So my question is, by including information from the past states into the reward definition, do we still maintain the underlying MDP or are we solving a different problem altogether? Assuming that we don't explicitly add previous action choices in the state representation, as suggested here https://ai.stackexchange.com/a/25991/54470
","['reinforcement-learning', 'markov-decision-process', 'rewards', 'reward-shaping']",
"As a intermediate in Deep Learning, from which research paper should I start reading from? [closed]","







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 10 months ago.







                        Improve this question
                    



I have done Andrew Ng's ML and DL courses, and some projects and implemented some important ML algorithms from scratch. Now reading the deep learning book. <=(Edited)
I want to start from the beginning (in terms of reading research papers), i.e, deep feedforward networks, regularization techniques,{then maybe conv nets and others}etc, etc and some tips on how to tackle the difficulty in understanding it. Thank You.
","['machine-learning', 'deep-learning', 'research']","
At your stage, I don't think jumping straight into reading research papers would be efficient. Generally, reading textbooks/review-articles, or simply watch a couple introductory youtube courses would do a better job at getting you up to speed with the background knowledge. Of course, you can always find a project that interests you and try to incorporate some elements of ML into it, which allows you to naturally learn ML at the same time.
Some standard introductory textbooks/courses are:

The deep learning textbook, more theoretical driven
Andrew Ng's courses on youtube, more application driven

which should cover the topics you mentioned.
If you want to focus on a specific topic (e.g. ConvNets, transformers, recurrent networks, etc.), it's generally helpful to find a recent review article on this topic and read through it. This is just to understand the current state of the field, and you can then read specific papers that interests you with this contextual knowledge in mind. Note these fields are moving so fast that certain seminal papers are no longer hugely relevant (e.g. many network architectures and training methods proposed in the classic  AlexNet paper are outdated.)
"
What AI API is used to create a chatbot that can answer questions about texts?,"
I am new to AI and as far as I know you train a model by having a data set which provides input and output values.
But what if you want to create a chatbot that can answer questions about Wikipedia articles for example?
I think you need a data set with a lot of questions about these articles and the answers (these are the input/output values).
Bute there are also the article texts. So you have 3 fields: the questions, the answers and the Wikipedia article texts.
But you can only pass one input and one output value to the training process. So how does it work?
","['chat-bots', 'training-datasets']",
Is there an AI technique (or general programming technique) suitable for seeing if two articles deal with the same event?,"
I'm looking for a way to work out if two or more articles deal with the same event or issue and I'm not sure where to start.
For example, back in August 2022 there were a few articles on the latest developments in AlphaFold, such as:
Nature: https://www.nature.com/articles/d41586-022-02088-x
The Economist: https://www.economist.com/alphafold-pod
Chemistry World: https://www.chemistryworld.com/opinion/why-alphafold-wont-revolutionise-drug-discovery/4016051.article
If I'm downloading hundreds or thousands of articles, what's the best way to pick out those three articles and work out that they're dealing with the same piece of new news? I imagine there must be a way given Google News groups similar news items together, but I'm not sure how difficult it would be.
Any guidance or directions would be much appreciated! Thanks.
","['comparison', 'classification']",
Genenrate isometric sprite images out of a face,"
I'm trying to find my way through the AI jungle and useful tools to bring my idea alive.
My goal is to create ans AI that can morph a real human face to an isometric pixel character

I read about GAN ans think thia woild be the technique to use for this.
Am I on the right trackand if yes which tools do you suggest to achieve that?
",['image-generation'],
Instruction-based language model without neural network,"
What kind of technology could be used to develop an AI with the same use case as InstructGPT (i.e. generating text from an instruction) but without using neural networks?
Obviously the performance would be several orders of magnitude lower, but that's okay by me.
","['reinforcement-learning', 'natural-language-processing', 'language-model']",
Can I pick up possible calendar events from a text file,"
I am new to this community and not even too sure if this is the right place for such a question.
I want to know how I could implement (possibly with already made solutions) a system which could take in a (possibly long winded) text file and deduct from there, events and meetings which a user could add (similar to what Siri does when it finds a possible event).
Is there something for this that already exists?
","['machine-learning', 'text-summarization']","
If you're fine training a model of your own, I would suggest try building a NER prediction model on your data. It would help to start with a certain type of data which follows similar pattern throughout. Once you're confident, you can extend it to many more patterns.
Or if you're lucky and find pre-trained NER models to be able to identify the event-date relationships already, you can just use them out-of-the-box.
Interestingly, I tried some QA models to simply ask the event & time of event given some context, and deepset/roberta-base-squad2 works pretty well for simple examples.

Context: I have a flight to catch at 6 AM tomorrow.
Question: What is the topic of event?
Answer: a flight to catch at 6 AM tomorrow
What is the date of event?
6 AM tomorrow


Context: The next set of features are scheduled to be discussed at meeting on 25th Dec 2022.
Question: What is the topic of event?
Answer: The next set of features
Question: What is the date of event?
Answer: 25th Dec 2022

Of course your data could be very different, but the pre-trained models should be a good starting point for ideas.
"
How embeddings learned from one model can be used in another?,"
In the website the following explanation is provided about Embedding layer:

The Embedding layer is initialized with random weights and will learn
an embedding for all of the words in the training dataset.
It is a flexible layer that can be used in a variety of ways, such as:
It can be used alone to learn a word embedding that can be saved and
used in another model later. It can be used as part of a deep learning
model where the embedding is learned along with the model itself. It
can be used to load a pre-trained word embedding model, a type of
transfer learning.

Isnt embeddings model specific? I mean to learn a representation of something we need the model that something was used to represent! so how can embeddings learned in one model can be used in another?
","['keras', 'word-embedding', 'embeddings']","
To answer your one question: Are embeddings model-specific? YES! They are. I am not going to invoke math or other techniques here. My explanation is going to be in a intuitive perspective. I don't know if the current literature and jargons will agree to my usage. But I get your question.
Take a scenario where you trained a CNN to classify smileys or emjoies based on their positivity or negativity. So it's a binary classification problem. Say you achieved a very good model. The penultimate layer for this model will give you a higher dimensional ""embedding"" vector for any new emojie or smiley pic you are feeding to this CNN.
Can you compare these embedding with this another ANN model's results arising from the same training and testing data? Technically you can't.
But by coincidence it might be very related. How do you verify it? You can use the argument of @postnubilaphoebus to do a PCA on the embedding data so that you can compare these two embeddings (by reducing the dimensions to match between both embeddings results). And then if all inputs which you feed to both models and then PCA-ing the corresponding intermediate penultimate layer outputs gives you pairs of ""embeddings"". Do a dot product analysis to compare them. You will know if they say the same thing or are very different.
I am not a NLP or LLM person, but this philosophy will work for any models. Thank you. I hope you got an idea. If not I am willing to explain it further.
"
"Can AI simulate fake inputs, train itself on them, and elaborate them like they were real ones generated from an outside source?","
Can AI simulate fake inputs, train itself on them, and elaborate them like they were real ones generated from an outside source?
This idea is similar to how human brain works, indeed it can ""think"" whatever feeling, such a voice, a sound, an image, a smell, a flavour without really get them from outside. It can work on it like the real ones and give an output. It looks to me this is how the consciousness works.
",['artificial-consciousness'],"

Can AI simulate fake inputs, train itself on them, and elaborate them like they were real ones generated from an outside source?

Yes, this is exactly what generative adversarial networks do.  Take a look at ""A Beginner's Guide to Generative Adversarial Networks (GANs)""  The GAN has a generator and discriminator.  The generator ""simulates fake inputs"".  The discriminator (police) tries to determine if it is fake or real.  When the discriminator catches the fake ones, the generator learns from its mistakes.  The generator keeps creating fake images until it starts fooling the discriminator into thinking they are real.  In other words, the AI (i.e., the generator/discriminator combo) ""trains itself"" to create fakes that are so good that they start looking real
"
Uniform cost search problem expanding of nodes,"
I have two questions about uniform cost search and have the following answer in the picture below. Could someone please check if my answer is correct? Sorry for posting screenshots!


",['uninformed-search'],
How to predict the rating of a text review and improve it?,"

Why is it better to treat the rating prediction of a text review as a regression problem rather than a classification one? Is it because the ratings (1,2,3,4,5) are ordinal variables?

What kind of Naive Bayes algorithm is the most suitable for this problem? Is Multinomial Naive Bayes the best one? It is a classifier, so I am unsure if it can deal with the ordinal nature of the rating variable or not.

The conditional independence assumption does not hold for text data. Terms are conditionally dependent on each other. Which observations in the dataset will be primarily affected by this assumption? Those observations whose word/feature counts are the least (the most sparse ones)?

If we want to keep a single feature/word in our model, which one should we choose? Should we keep the feature with the maximum sum of its word counts?

Generally, is it accurate to say that neural networks are better than KNN models for predicting the ratings because the text datasets are usually sparse? Does the performance of these two regressors mainly differs for observations whose word/feature counts are the least (the most sparse ones)?


","['neural-networks', 'machine-learning', 'natural-language-processing', 'text-classification', 'naive-bayes']",
Fine tuning a Deep Learning model post training,"
I have trained a CNN in a binary classification problem, however the original problem has 6 different classes, of which, I am only interested in classifying one, so if it is that certain class or not.in this case, let's say class 2.
After looking closely into the model's performance on test dataset, I have found that the model confuses class 2 with class 1 often. Is it common practice, to make a balanced dataset from the data that I have only from class 1 and class 2, and further train the model on that dataset? Are there any pieces of research/papers on this? If no, what other possible solutions would there be, of course other than making a new model?
","['deep-learning', 'fine-tuning', 'imbalanced-datasets']","
You can use the technique of Transfer Learning to fine-tune your model. You can take the weights from the pre-trained model and then use them as initializations for your own model.
Yes!, It is common practice to make a balanced dataset when training a machine learning model, as this can help prevent the model from overfitting to one class or the other. A model trained on a balanced dataset would be more accurate at predicting the classes.
In your case, if you have more data from class 1 than class 2, you may want to downsample the class 1 data so that both classes are represented equally. You can then train your model on this balanced dataset and see if it improves performance on the test set.
There is a great deal of research has been conducted on the topic of data balancing and its impact on machine learning models. few papers on the topic include

On the effectiveness of preprocessing methods when dealing with different levels of class imbalance

Learning from Imbalanced Data

A review on data preprocessing methods for class imbalance problem


"
Can you extrapolate outside the latent distribution for GANs?,"
I was wondering what happens when you extrapolate out of the latent space distribution (noise vector) for a Generative adversarial network (GAN). Can anybody explain this?
","['machine-learning', 'generative-adversarial-networks', 'latent-variable']",
Are custom tokens better than punctuation pseudo-tokens for LLMs?,"
I've seen two approaches for introducing custom tokens for transfer learning with large language models like Bert or GPT3. Some approaches introduce new tokens into the vocabulary and learn embeddings from scratch. This is the ""traditional"" approach. However, I've seen other papers that imitate custom tokens with the use of punctuation, e.g. ""<custom-token>"". In this case the model is not learning any new tokens, but is learning to connect subword tokens and punctation already in its vocabulary. I think this approach is often used with GPT3, as the closed API prevents learning new tokens from-scratch.
Has any research benchmarked whether one approach is better than another, when both options are available?
","['natural-language-processing', 'natural-language-understanding', 'language-model']",
Interpretation of the Dynamic Time Warping (DTW) graph,"
How can I interpret ate the DTW graph.
I understood the algorithm behind DTW, but I struggle to interpret ate the graph.
When I compute the DTW for a signal that is the same signal but shifted in time, my result will be a diagonal, straight line from top right to bottom left. When the signal does not match exactly, I would see some deviation from the ideal diagonal line.
But how to be exact about the outcome?
I try to explain my issue from a different perspective
When I calculate the standard Euclidean distance  I can say as bigger the value as bigger the error, but what about the values calculated with DTW?
","['math', 'clustering', 'dynamic-programming']",
Detecting individual multiple documents in a pdf [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 9 months ago.


This post was edited and submitted for review 9 months ago and failed to reopen the post:

Original close reason(s) were not resolved






                        Improve this question
                    



I need to solve a problem whereas a scan of multiple documents (contracts, invoices, bank extracts) is stored into a PDF and I need to identify how many individual documents are contained in the PDF and which pages of the PDF belong to which document.
This scenario presents itself, for example, when a person feeds a bunch of documents into an automatic scanner that then creates a single PDF from these documents. Each document is just an image and might have one or more pages and may have different layouts.
What would be an intelligent AI approach to attacking this problem?
",['computer-vision'],"
This is an interesting question.
Let's make the problem more abstract:
Suppose we have n docs(each page in the PDF is one doc). How can we classify these docs?
My approach:
This could be treated as an NLP task; my approach is. First, you have to extract texts from each doc, then compute the similarity between each doc, if the similarity is great than the threshold you give, they should be considered as same docs.
Useful link:
Compute similarity: https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630
OCR: https://nanonets.com/blog/attention-ocr-for-text-recogntion/
"
Why is `SigmoidBinaryCrossEntropyLoss` in `DJL` implemented this way?,"
SigmoidBinaryCrossEntropyLoss implementation in DJL accepts two kinds of outputs from NNs:

where sigmoid activation has already been applied.
where raw NN output is taken as is.

The choice is determied by fromSigmoid parameter.
Here's how (1) fromSigmoid=true looks like:
loss =
      epsLog(pred)
      .mul(lab)
      .add(epsLog(NDArrays.sub(1., pred)).mul(NDArrays.sub(1., lab)));

And here's how (2) fromSigmoid=false looks like:
loss =
      Activation.relu(pred)
      .sub(pred.mul(lab))
      .add(Activation.softPlus(pred.abs().neg()));

The logic behind (1) fromSigmoid=true branch looks like a standard cross entropy loss implementation, but I fail to understand why (2) fromSigmoid=false is implemented the way it is.
For instance, why is there no sigmoid application, why is there relu later followed by softPlus? I'd like to understand what does (2) do and what's the theory behind its implementation.
","['objective-functions', 'loss', 'sigmoid', 'binary-crossentropy']",
"How to interpret upsampling(deconv, nn, bilinear) as matrices?","
I am reading this Distill article Deconvolution and Checkerboard Artifacts about avoiding artifacts in images generated by neural networks.
In the section of Better Upsampling, the author compares the difference between deconvolution (i.e. transposed convolution), NN-resize then convolution, and bilinear-resize then convolution. He says:

Both deconvolution and the different resize-convolution approaches are linear operations, and can be interpreted as matrices.

Above the sentence, he shows the figure on how to interpret upsampling methods as matrices. I am confused about the figure. What is the meaning of a, b, c? How do the matrices in the bottom half correspond to the grey maps in the top half?
","['neural-networks', 'image-generation', 'transpose-convolution', 'image-super-resolution', 'interpolation']",
How to deactivate kernels from a trained CNN model,"
I trained a 1 layer CNN model with 128 3x3 kernels. I evaluated the model with a prescribed test data set and now I want to evaluate the performance of this model where we only consider select kernels (meaning other kernels are set to 0). My question is, how exactly can I go into the model and set these unwanted kernels to 0? Any help would be greatly appreciated.
","['machine-learning', 'convolutional-neural-networks', 'filters']","
You could manually create a copy of the model, reset some of the weights to zero and do the experiment that way.
But an easier way is to pass two inputs to the model: the image, and a kernel mask. Assuming the convolution layer uses ReLU, it is natural to multiply its output with the mask element-wise before passing it to subsequent layers. The mask would have values of all one when training, but during the experiment you could set the selected of the 128 values to zero. This way you don't need to modify the weights ad-hoc.
Actually this is very similar to what dropout does, except you are doing the dropout at evaluation stage rather than during training.
"
Why training the same model on the same data can be slower on better card?,"
Can someone explain why training CNN model (in my case DenseNet201) on the same data, and the same data processing pipeline can be slower on better GPU (RTX3090) than worse one (RTX3060), with the same other parameters (exactly same PC just with new GPU)?
In both cases I used same batch size and other settings. The only way to make training faster on 3090 was to actually increase the batch size, which was too big for 3060. But I still don't understand why the same training params wouldn't produce better results.
Even though big part of the training is reading data from disk and data augmentation (albumentations in my case) it's still the same setup, so even if GPU work is smaller part of one entire epoch, it still should be a bit faster, right?
","['deep-learning', 'convolutional-neural-networks', 'training', 'gpu', 'hardware']","
Every training will be slightly different because of the statistical matter of neural networks.
The question is, how much is your better?
Then, newer does not imperatively mean better. It means more hardware and computational power, but not necessarily how it is used for specific tasks.
Maybe, the 3090 can't leverage its power in this specif task properly.
"
Deep Reinforcement Learning - How to Avoid a Naive Solution?,"
I'm implementing a connect-4 agent: using DQN, training by playing vs previous versions of the network.
However, many times the network learns that it's best to simply put 4 pieces in the same column, which is usually able to beat the previous (also naive) versions.
What could be the reason? how can it be dealt with?
Thanks!
","['deep-learning', 'deep-rl', 'dqn', 'connect-four']",
Epsilon-greedy action selection question,"
Is the following sentence correct?
The estimated values $Q(a)$ do not converge to the true values $q_*(a)$ because $\epsilon$-greedy action selection behaves randomly from time to time.
My Answer: The sentence is wrong since eventually the estimated value $Q(a)$ will converge to $q_*(a)$ and the randomness of epsilon greedy action selection won't be able to influence it if we run it for a long time.
Is my answer correct?
",['epsilon-greedy-policy'],"
I believe, that you are correct in saying the sentence is incorrect. Since, as you mention, the epsilon greedy action selection will just allow for the Q values to converge to the optimal q* value since the actions of the policy just provide more observations for Q which will allow for the q* to be converged to. However, this is assuming that the updating of the Q values is in such a way that each new observation has less of an effect on the Q(a). Though it is not the case that the epsilon greedy will converge to the optimal policy/action due to the randomness mentioned by the answer.
"
Knowledge graph progress from 2012 to 2022?,"
I watched this lecture by professor Xin Wang, and a picture in the beginning interested me:

The confusing thing is that this lecture was delivered on Sep. 19, 2022, but it seems from the diagram above that over the last ten years knowledge graph has made no progress. The diagram is still what I was shown when I was a student in 2015 or 2016.
Since I cannot get a hold of this professor, I googled knowledge graph progress pictures but found that no single one mentioned any big events after 2012.
What happened over the last decade to knowledge graph? Any big events?
","['history', 'knowledge-representation', 'knowledge-graph', 'knowledge-base', 'commonsense-knowledge']","
This 2021 survey of Knowledge Graphs depicts a similar chart (appendix A), with the same 2012 final milestone.
That said, a major advancement is the use deep learning to encode the knowledge graph, to extract relations and autonomously complete graphs. So we may expect soon a breaktrough in neural symbolic approaches applied to challenges such as Question Answering, Recommender Systems and Language Representation Learning.
"
What camera resolution is good for Resnet101 SSD FPN 1024x1024,"
I am going to use this model for my school project. Just wondering if I bought a 1920x1080 camera, will resizing it to 1024x1024 will cause the drop of image quality(make model harder to detect object as in this case width need to resize a lot more as compare to height of the image)?
",['tensorflow'],
How Problem Formulation affects Optimization and Local Optima,"
Suppose we are dealing with a categorical classification problem and have x number of categories. Usually the correct label is represented by a x-dimensional one hot vector.
I'm wondering what happens practically when we represent the correct label say by x arbitrary values on a real line (i.e. the target is a 1 dimensional vector). e.g. ""cat"" is 0.55  ""dog"" is -1.2 etc. (let's call this the real-line problem spec), and run a standard CNN on this problem.
I'm running some experiments on a slightly different problem than the one above that uses a latent representation of the target, and to easily visualize the results, I'm setting the latent representation to 1 dimensional for now, with various targets being numbers on a real line. However, I've noticed that the training gets stuck easily (perhaps in local optima), and the hyperparameters are super sensitive.
Theoretically, given a sufficiently large model, we can learn any function, such as the the function that maps images to the real-line targets I described above. But practically it seems like this is more difficult to do if we specify the problem poorly.
Is there any theory/explanation/paper/tutorial as to whether my intuition is correct - that the ""real-line problem spec"" does indeed easily induce some sort of local optima? Can someone give me insight as to why this is?
","['machine-learning', 'optimization']",
Matching a search approach and heuristic to reducing propositional logic,"
I am new to the AI space (and this exchange).  My pet project (to help keep my skills fresh as I age, I am not a student) is a parser for propositional logic. I am designing an agent to reduce any arbitrary statement to its simplest form.  While I have it ""working"" it uses rudimenatary approach (beam search).  While it generally finds a solution, it sometimes takes an odd path to get there.
As an example of what I am doing, one can enter a simple statement: (a ∧ b) ∨ ¬(a ∧ b)
initial:                        81      ((a ∧ b) ∨ ¬(a ∧ b))
abstraction:                    16      ([c/(a ∧ b)] ∨ ¬[c/(a ∧ b)])
Complement<or>:                 8       ((p ∨ ¬p) ≡ ⊤)
transformation:                 1       ⊤

The logic of the agent is essentially:

it forms all possible abstractions of the statement (in the case above, it proceeded with the abstraction where c stood for (a ∧ b).
for all abstracted forms, it matches applicable laws (in this case the complement law)
applies the law forming a transformation (in this case, we end up in the most reduced form ⊤ and exit).  The heuristic is in the middle column and only considered for ""transformation"" steps (ignored for the abstraction and law)

One problem seems to be the use of ""Conjunction"" to flip parameters which sometimes requires more steps to find the optimal solution.  You almost don't want to leverage it unless you don't have other moves (not sure if this is logically sound, but anecdotally i feel that way).
Scoring of any given propisitioinal statement is a heuristic that combines some of #variables total, #unique variables, #operations.  I have tried various combinations, but the one that estimates the number of objects in play seems simple enough:
let complexity = (node) => {
    let u = countUniqueVariables(node)
    let o = countOperations(0, node)
    let v = countVariables(0, node)

    v + u  + o
}

So one question I have as I research the space, particularly a-star (if that is an algo worth trying), it requires me to estimate the distance from a node to the goal.  I am not sure I can estimate that (or have the persepctive to know how to). In some cases the propositional statements get more complex in order for them to have a path to the goal.
As I type, I guess this more of a question in defining a heuristic. Removing excess variables and operations is how a statement is simplified (thats what the laws do).  However you cannot even look at a statement and say there should be at least as many variables in the goal as there are unique variables in the intitial statement. e.g., a or (b or not b) should reduce to a as the second part is a tautology (and thus the best answer only has one variable as b was able to be compeletly removed)...
Looking for help, hints, direction.
","['search', 'heuristics', 'informed-search']",
Order of features learned by DNNs during training?,"
I'm looking for papers probing into the question of what features get learned when (or equivalently what subproblems get ""solved"" when) during the training process. For example, a paper showing that a Convnet trained on MNIST learns to distinguish 0 from 1 before it learns to distinguish 0 from 8.
The one example I can think of off the top of my head is the Grokking paper, but that's looking at a slightly different (and less intuitive) phenomenon. Thanks!
","['deep-learning', 'training', 'feature-extraction', 'representation-learning']","
The order in which features are learned by DNNs during training is typically random & depends on a number of factors:

Including the specific architecture of the DNN
The type of data being used for training
& the optimization algorithm being used


However, it is known that features are learned in a hierarchical manner, with lower-level(simpler) features being learned first, followed by higher-level(complex) features.

There is some evidence that certain features are learned early on in the training process, while others are learned later. For example, a study by Karpathy et al. (2014) found that a convolutional neural network (CNN) trained on the MNIST dataset tends to learn to distinguish between the digits 0 and 1 before it learns to distinguish between the digits 0 and 8. However, it is not clear how generalizable these findings are to other tasks and datasets.
A good general resource on how different features are learned at different stages in neural networks is the Deep Learning book by Goodfellow, Bengio, and Courville. In particular, Chapter-6 (Feature Representations) and Chapter-7 (Neural Networks) cover this topic in detail.
Artificial Intelligence: A Modern Approach by Russell and Norvig (2010) covers this topic in Chapter 18 on Learning Complexity. The chapter starts with a discussion of the concept of concept learning and the ways in which different concepts can be learned. It then covers the question of how well different concepts can be learned, and how this affects the overall learning process.
One paper that explores this question is ""Visualizing and Understanding Convolutional Networks"" by Matthew D. Zeiler and Rob Fergus.

In this paper, the authors visualize the features learned by a convolutional neural network as it is trained on different datasets. They find that the network gradually learns increasingly complex features, starting with simple edge detectors and then moving on to more complex shapes and patterns.

Another paper Why does deep and cheap learning work so well? by Henry W. Lin, Max Tegmark, and David Rolnick

In this paper, the author tries to answer the question of why deep learning works so well, despite being ""cheap"" (in terms of the amount of data and computation required). He argues that deep learning is able to learn a wide range of features, from low-level features (e.g. edge detectors) to high-level features (e.g. object detectors), and that this enables it to achieve better performance than shallower methods.

"
Is there an image classification dataset where the class depends on spatial relations?,"
My question is pretty much the one asked above. To clarify a bit further: I have only found datasets that do object localization and that also have relations between the objects annotated (like: ""Here is the horse, here is the rider. The rider rides the horse"". What I am looking for is a dataset where you can do classification based on spatial relations (Like: ""I am a positive example, because the rider is above the horse. I am a negative example, because the rider is below the horse (or besides it, etc)"").
Any help is appreciated
","['classification', 'image-recognition', 'datasets']","
There is no specific image classification dataset that focuses on spatial relations. However, there are some datasets that include images with spatial relations annotations, such as the Visual Relationship Detection (VRD) dataset which is a collection of images with annotations of spatial relations between objects.
Here is a paper Identifying Spatial Relations in Images using Convolutional Neural Networks that uses pre-trained CNN to learn spatial relations between objects in an image, it makes use of SUN09 and a simplified synthetic dataset to trained the network. the network pays attention to a certain parts of images
while classifying them for spatial relations.
"
Calculation of the CPTs in a Dynamic Bayesian Network,"
I am trying to figure out how to build my DBN (with pyAgrum), and I am a bit confused. Let us say I have the network on the next figure.
I am interested in the variable $B$. At each time $t$, I am reading the data $A_t$, $B_t$, $C_t$ and $D_t$ from my sensors to find the likelyhood of $B_{t+1}$.  An then, at time $t+1$ , I start again with the data from the sensors. Is it the correct way to proceed?
Moreover, I do not know the CPTs (Conditional Probability Table) for my DBN (it's actually one thing I want to find), can we update as time progresses the values of the CPTs?
It's one thing I'm not sure with DBN, are the CPTs fixed, or do they change over time?
Thanks in advance for your clarification

",['bayesian-networks'],
"What is the difference between face identification, face verification, and face recognition?","
I am confused. On some websites, face identification is just face detection, while face verification is finding the person's identification. They are both considered components of face recognition. But in some websites face verification is 1-to-1 matching of faces while face identification is 1-to-k face matching
","['machine-learning', 'computer-vision', 'face-recognition', 'opencv']","
Face identification is the process of determining whether a face in a given image belongs to a specific person.
Face verification is the process of verifying that a given face corresponds to a specific person.
Face recognition is the process of identifying a person from a given image.
One key difference between face identification and face recognition is that face identification is typically used to determine whether someone is who they claim to be, while face recognition is used to identify people regardless of who they are. Another difference is that face identification is often used as a security measure, while face recognition is more often used for things like tagging photos.
Another difference between face identification and face recognition is that face identification systems often require a person to be looking directly at the camera, while face recognition systems can often work with pictures of people taken from different angles.
"
How Does Convolution Backpropagation Work?,"
Assume in a convolutional layer's forward pass we have a $10\times10\times3$ image and five $3\times3\times3$ kernels, then $(10\times10\times3) *( 3\times3\times3\times5)$ has the output of dimensions $8\times8\times5$. Therefore the gradients fed backwards to this convolutional layer also have the dimensions $8\times8\times5$.
When calculating the derivative of loss w.r.t. kernels, the formula is the convolution $input * \frac{dL}{dZ}$. But if the gradients have dimensions $8\times8\times5$, how is it possible to convolve it with $10\times10\times3$? The gradients have $5$ channels while the input only has $3$.
Since during the forward pass the kernel window does element-wise multiplication and brings the channels down to $1$, do the gradients propagate back to each of the $3$ channels equally? Should the $8\times8\times5$ gradients be reshaped into $8\times8\times1\times5$ and broadcasted into $8\times8\times3\times5$ before convolving with the layer input?
","['neural-networks', 'convolutional-neural-networks', 'backpropagation', 'convolution']","
I figured it out a while ago and double-checked my results with TensorFlow, so I'm fairly confident with the implementation. Here is what I did using Eigen Tensor and the im2col method:
If a [N,10,10,C] image is convolved with a [F,3,3,C] kernel (both in NHWC format, F = # kernels), using stride & dilation 1 and valid padding, the output is [N,8,8,F] image.
Therefore the gradients coming back to this layer is also Nx8x8xF which are element-wise multiplied with the activation gradients to get dL/dZ, also [N,8,8,F]. The kernel gradients formula is $input * \frac{dL}{dZ}$ so this is a [N,10,10,C] dimension image convolved with [N,8,8,F] dimension gradient.
Convert the gradients tensor into an im2col tensor

Shuffle the gradient $\frac{dL}{dZ}$'s dimensions [N,8,8,F] into [F,N,8,8]
Reshape [F,N,8,8] into [F,N,8,8,1] and broadcast into [F,N,8,8,C] to match the input image's channels, divide it by the number of times broadcasted, C
Reshape [F,N,8,8,C] into a 2D tensor as [F,N*8*8*C]

Step 2 addresses my question on convolving 2 tensors with mismatching channels dimensions.
Then extract patches from the input tensor and convert it to an im2col tensor

Extract image patches from the input image, using the same amount of padding during forward pass (following the convention that if uneven padding, the extra goes to the bottom & left) and the same stride and dilation (but the latter 2 are swapped), with the gradients from the previous step 2 playing the role of kernel, resulting in the 5D tensor [N,P,8,8,C], P = # patches = 9, the # times the kernels (gradients) slid across the input image
Shuffle the 5D tensor's dimensions from [N,P,8,8,C] to [P,N,8,8,C]
Reshape the image patches [P,N,8,8,C] into the 2D tensor [P,N*8*8*C]

Now that we have the gradients as [F,N*8*8*C] and [P,N*8*8*C], we can multiply the two

Do a contraction (matrix multiplication) along the first dimensions of both, the resulting tensor dimensions are [F,N*8*8*C] x [P,N*8*8*C] = [F,P].
The tensor is reshaped from [F,P] to [F,3,3], then to [F,3,3,1]
Broadcast C times on the last dimension, divide by batch size N, then we get the kernel gradients [F,3,3,C] which match the kernel used during the forward pass, and can be fed to your optimizer of choice during the weights update

With Eigen Tensor, if you wrap all of this as a function with the return type auto to keep everything as an operation, you can lazily evaluate it into a 4D tensor using Eigen::ThreadPoolDevice with 2+ threads for improved speed (2-3 times faster than a single thread on my machine).
Edit: Here's my implementation on GitHub (1 2)
"
Can I use a dataset with real-world images and corresponding actions that the expert took to train an IRL algorithm?,"
Offline Reinforcement Learning approaches like Inverse Reinforcement Learning/ Batch RL/ imitation learning/ behavior cloning allow us to use previous demonstrations by an expert to learn a policy.
Many of the papers that I have found use expert data generated from a simulated environment for model training. However, I have some real world images and corresponding action that the expert took in these states e.g. images from a camera mounted on the car and the steering angle as an action. The action space is continuous meaning how fast the steering angle was applied.
Can I use this dataset as an expert demonstration in place of simulated dataset for training an Inverse Reinforcement learning algorithm?
","['reinforcement-learning', 'training', 'datasets', 'inverse-rl']",
Dummy variable trap in neural networks and class visualization,"
Let's say I have data records looking like that: (x1, x2, x3, x4, ..., x100), where each x can be either alpha, gamma or omega.
An example of record could be ('gamma', 'alpha', 'omega', 'alpha', ..., 'gamma') .
So the shape of my dataset is (N, 100) (with N the number of records).
I want to train a neural network to predict some binary label. As there is no underlying ordering in my input categories, I use dummy variables to feed my network. Therefore, I end up with a dataset of the following shape: (N, 100, 3).
My problem is that I don't really know how to deal with the dummy variable trap. According to this answer, I should drop one category when I use a network without weight decay. However, I thought that even without weight decay, the non-linearity of neural networks (assuming I'm using non-linear activation functions like relu), would be enough to avoid the issue without actually needing to drop one category. Am I wrong?
Would a neural network without weight decay behave badly if I do not drop one column?
Some context
Ideally, I would like to avoid dropping one column as my next step is to create inputs that maximize a class prediction (starting from noise and using gradient ascent to ""improve"" the input).
If I do that with a model trained with a dropped category, I can end up with values close to 0 for my (n-1) categories, probably meaning that the category that would maximize the output would be the dropped one. This interpretation looks correct to me, but it leads to a generated input that has very high-frequency components.
However, I know that consecutive features in a record are likely to be the same (like ('alpha', 'alpha', 'alpha', 'gamma', 'gamma'...)), so input with such high frequencies is quite unrealistic. Generally, this kind of method imposes some L1 or L2 regularization in order to get more global coherency in the generated input. But here is my issue, if I impose some regularisation, the generated input will very likely be biased toward the dropped category right?
As I'm writing these lines, I'm wondering if these considerations about frequencies are not relevant only for numerical inputs and not for categorical inputs. I'm open to any clarification as I'm a bit lost.
Inspiration: The interpretation part of this work is greatly inspired by the first part of this repo. It deals with images as input so the visualization is easier than with my data, but the problem is the same: visualize classes predicted by my network by generating regularized inputs that maximize the class prediction.
EDIT
I believe that the part about using L1 or L2 regularization to improve global coherency is not true. I'm still trying to work out how to solve this problem, but I'm struggling quite a lot.
","['neural-networks', 'regularization', 'explainable-ai', 'one-hot-encoding']","
Assume that you have a (3, 3) dataset (3 records, 3 variables), and a, b, and c are alpha, beta, and gamma, respectively.  Then, an example dataframe would be as follows:
   A B C 
1  a b b
2  a a b 
3  g g a 

This dataframe could be one-hot encoded as a (3, 9) dataset.  You don't need a (3, 3, 3) dataset as you suggest in the post.
One-hot encoded
   A_a A_b A_g B_a B_b B_g C_a C_b C_g
1  1   0   0   0   1   0   0   1   0
2  1   0   0   1   0   0   0   1   0
3  0   0   1   0   0   1   1   0   0

After you know the value of 2 columns of a feature (e.g., A_a and A_b), you can predict the value of the 3rd column( e.g., A_g).  You can, therefore, drop the 3rd (redundant) column for each feature without loss of information, as follows:
One-hot encoded
   A_a A_b B_a B_b C_a C_b
1  1   0   0   1   0   1  
2  1   0   1   0   0   1 
3  0   0   0   0   1   0 

However, the article you refer to suggests to not drop the third column for certain models because ""If you remove a category from input of a neural network that employs weight decay, it will get biased in favor of the omitted category instead.""  Therefore, you should retain the third columns if you employ those algorithms, is what that answer seems to suggest.
For these other models, the answer suggests to drop the third category to avoid multicollinearity:

Linear/multilinear regression
Logistic regression
Discriminant analysis
Neural networks that don't employ weight decay

I'm wondering if these considerations about frequencies are not relevant only for numerical inputs and not for categorical inputs.
This issue specifically arises with categorical variables, not numerical variables.
"
How do Transformers compute the words embeddings at inference time since the embeddings are dynamic?,"
In Word2Vec, the embeddings don't depend on the context.
But in Transformers, the embeddings depend on the context.
So how are the words' embeddings set at inference time?
","['transformer', 'word-embedding', 'inference']","
Transformers do still use pre-trained embeddings but these are then given context by the positional encoding and self-attention blocks of the architecture.
I will explain in the context of a sequence-to-sequence transformer used for translating a sentence from one language to another (e.g. that used in Attention is All You Need):
At inference time the input sequence process (i.e. the encoder) is the same as during training time: each word in the input sequence is mapped to the embedding vector for that word, and 'given context' by the positional encoding and encoder self-attention blocks.
In the decoder, at inference time we are predicting the translation of each word in the input sequence, one at a time. So instead of feeding in the target translated sentence to the decoder, we feed in an empty sequence with a start-of-sentence token. In much the same way as in the encoder, each token in this sentence is given context by the positional encoding and self-attention blocks. The next word in the output sequence is then predicted by the rest of the decoder layers and this is appended to the output sentence. This output sentence (now consisting of the start-of-sentence token and the first word(s)) is then fed into the decoder to predict the next word, until the end-of-sentence token is predicted.
So in answer to your question, the blocks of the transformer that give the word embeddings context are the same during training and inference.
"
Why model in tensor flow model zoo have low mAP?,"
I read in paper and article SSD model achieved above 70% mAP but when i browse through tensor flow model zoo, the mAP of SSD is around 30-40% in this link
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md Why is it much lower to the mAP in paper?
",['tensorflow'],
Semi-supervised learning algorithms creating redundant data,"
If I'm generating pseudo-labels that I'm confident are correct for my dataset due to high confidence scores or something else, how can I expect that the new data I'm labeling won't be redundant? To my knowledge, if a datapoint is labeled automatically, unless novel data is generated like in this paper I don't see how the data that can be labeled will be particularly useful, as it will admit no new patterns to learn from, as otherwise the data wouldn't be labeled with a high confidence. Is the point here to make sure the labeled dataset you start with is sufficiently large that it covers all the kinds of patterns you expect from the data for the model to need to be able to identify?
","['data-labelling', 'semi-supervised-learning']","
If you are confident that your pseudo-labels are correct, then you can expect that the new data you label will not be redundant. however, If you want to avoid having redundant data in your labeled dataset, then you need to make sure that it is large enough to cover all the relevant patterns.
One way to avoid creating redundant data when using data augmentation is to use a technique called transfer learning. This involves training a model on one dataset and then transferring the learned knowledge to another related dataset. This can be done by fine-tuning the model on the new dataset or by using the model to extract features from the new dataset that can be used to train a new model. Transfer learning can be used to reduce the amount of data needed to train a new model, and can also improve the performance of the new model.
"
Loss Function for Binary Classification with Multiple Correct Choices,"
I have a binary classification problem, where there are multiple correct predictions, however, I would consider the prediction to be correct if the highest confidence prediction of a 1 is correct.
I have had limited success with a CNN using the Hinge loss function. I think the reason is that, this loss function is calculating the loss between every element in y_true and y_pred.
Example:
I want my model to confidently predict one of these cells as a 1. There probably isn't enough information in the input data to accurate predict every 1, especially in the areas far from input cells with a positive value, therefore I would be happy if the model is conservative and predicts a 0 where it is unsure.
input_data

y_pred:

I believe the model is being optimized to classify as many cells correctly as possible.
It's not possible to correctly predict the cell in the bottom right corner in this example. Therefore the model should take the conservative approach and predict a 0.
y_true

A perfect loss function in my mind would:

Heavily penalize the case where (y_true, y_pred) = (0,1) (false
positive)

Heavily reward the case where (y_true, y_pred) = (1,1) (true
positive)

Moderately penalize the cases where (y_true, y_pred) = (1,0) (false
negative)

Moderately reward the cases where (y_true, y_pred) = (0,0) (true
negative)


Or
Calculate the loss only on the cell with the highest value in y_pred
Can anybody suggest a suitable loss function?
Let me know if I have omitted any required detail
","['machine-learning', 'convolutional-neural-networks', 'objective-functions', 'keras']",
"AIMA, Mutation in Genetic Algorithm","

With regards to the highlighted line, the authors earlier stated that:

The mutation rate, which determines how often offspring have random mutations to
their representation. Once an offspring has been generated, every bit in its composition
is flipped with probability equal to the mutation rate.

Doesn't this contradict the highlighted line in the code? Because according to this quote we mutate the children with the small probability called mutation rate every time they are generated so the actual code in my opinion should be:
for each bit in child string
    if (small random probability >= mutation rate) mutate bit

instead of:
 if (small random probability) mutate(child)

","['genetic-algorithms', 'norvig-russell']",
Rosenblatt Perceptron Learning rule,"
I'm trying to design a 2-bit multiplier using a neural network with 8 neurons, 4 neurons as input, and 4 neurons as output.
Using the Rosenblatt Perceptron learning rule, I can find the weights for all output bits except the second bit.
Assume the inputs are
array([[0, 0, 0, 0],
       [0, 0, 0, 1],
       [0, 0, 1, 0],
       [0, 0, 1, 1],
       [0, 1, 0, 0],
       [0, 1, 0, 1],
       [0, 1, 1, 0],
       [0, 1, 1, 1],
       [1, 0, 0, 0],
       [1, 0, 0, 1],
       [1, 0, 1, 0],
       [1, 0, 1, 1],
       [1, 1, 0, 0],
       [1, 1, 0, 1],
       [1, 1, 1, 0],
       [1, 1, 1, 1]])

And the outputs for the second bit are
array([[0],
       [0],
       [0],
       [0],
       [0],
       [0],
       [1],
       [1],
       [0],
       [1],
       [0],
       [1],
       [0],
       [1],
       [1],
       [0]])

I try to change the learning rate and epochs, but all weights have an error.
Is it not possible to train such a network? I thought we could train any network that can work on the training data.
If possible, why did I fail by using the Rosenblatt Perceptron learning rule?
I hope I can express my issue because I recently studied these topics.
","['neural-networks', 'machine-learning']",
Real-Time banknote recognition dataset,"
My project is about banknote recognition in real-time scenarios, so for the dataset, do I need to snap each banknote one by one, or do I need to record video and extract images from the videos I take?
","['deep-learning', 'image-recognition', 'image-processing']",
MobileNetV2 - Some particularities,"
So I was studying MobileNetV2 architecture and came across this table from the original paper that represents its architecture:


Table Description: ""Table 2: MobileNetV2 : Each line describes a sequence of 1 or more identical (modulo stride) layers, repeated n times. All layers in the same sequence have the same number c of output channels. The first layer of each sequence has a stride s and all others use stride 1. All spatial convolutions use 3 × 3 kernels. The expansion factor t is always applied to the input size as described in Table 1.""

However I don't really understand why the last pointwise convolution (conv2d 1x1) is applied after the average pooling.
Shouldn't the output from the avgpool go to a dense layer so that it can then perform its predictions?
What is the point of performing convolution, more precisely pointwise conv. after pooling? Also, why is k not a fixed number of kernels/filters, like the other layers?
Thanks in advance!
","['deep-learning', 'deep-neural-networks', 'mobile-net-v2']",
What exactly is non-delusional Q-learning?,"
Problems occur when we combine Q-learning with a function approximator.
What exactly is the delusional-bias and non-delusional Q-learning? I am talking about the neurIPS 18 best paper Non-delusional Q-learning and value-iteration.
I have trouble understand the term ""Policy Commitments"", and ""consistency"". What are they talking about?
PS: a related post
","['reinforcement-learning', 'q-learning', 'papers', 'value-iteration', 'policy-iteration']",
VAE - Which loss to optimize for?,"
Regarding hyperparameter optimization for VAEs. Should you optimize for the reconstruction loss, or the complete ELBO (- KL divergence + reconstruction loss)?
My thought is that it probably depends on the application. My application focuses on the latent dimensions instead of the reconstruction, so I should probably optimize for the ELBO. But I'm not sure.
What are your thoughts on that?
","['hyperparameter-optimization', 'variational-autoencoder', 'kl-divergence', 'evidence-lower-bound']",
Multivariate RNN with different sequence length for each feature,"
I want to use an LSTM to predict a stock price. I have a few groups of features that I am willing to use. Some of these features are fundamentals like earnings and sales and others are technicals like the RSI and moving averages.
Earnings and sales are announced on quarterly basis so my columns for each has the earnings and sales from the most recent quarter.
In addition to these features I want to feed in the RSI for the last 7 days, for example.
My question is, is it possible to use different sequence lengths for different features?
I am imagining a parallel network feeding each feature into one branch with its own sequence length and then combining all of them in a feed forward layer.
Is that too crazy?
","['recurrent-neural-networks', 'long-short-term-memory']",
Are there Reinforcement Learning algorithms specialized for the case $\gamma=0$?,"
I have a Reinforcement Learning problem where the optimal policy does not depend on the next state (ie gamma equals 0). I think this means that I only need an efficient exploration algorithm coupled with a way to backpropagate the rewards.
I know that contextual bandits are specialized for this situation, except they only work for discrete action space, and I still need my policy network to make complex decisions (train a Deep neural networks, whereas most contextual bandits algorithms I found learn linear policy decisions). I also think deep active learning does not work because I can not trace how the reward is computed (it is not model-based).
Therefore I am looking for algorithms, or ideally a python implementation, that solves RL for continuous action spaces when $\gamma=0$.
Reposted from here.
","['reinforcement-learning', 'deep-rl', 'python', 'algorithm-request']",
"If you have a small amount of labeled data and a limitless amount of pseudo-labeled data, does the ratio of labeled to pseudo-labeled data matter?","
Suppose I have a labeled dataset $L$ and unlabeled dataset $U$, where $U \gg L$. Suppose I focus on a subset of $U$ called $u$ and generate a subset of $u$ I'll call $u_L$ that consists of pseudolabels for $u$ that I've generated using Tri-training. Suppose $U$ is so large, that depending on the amount of datapoints I can choose to make up $u$, I can make $u_L$ virtually as large or as small as I want, from $10$ datapoints to $100$ to $1000$ to $1000000$ if I so wished, all from generating the psuedolabels from tri-training.
If I want to eventually train a model using $L \cup u_L$, is there any reason I'd want to restrict the size of $u_L$? Generally speaking when it come to ML, the more representative data the better the model, but in this circumstance I'm not sure.
","['training-datasets', 'data-augmentation', 'data-labelling', 'semi-supervised-learning']",
what's the best way of Inferring probability chance of heads with a coin of Unknown Bias that changes regularly,"
hi what would be the best strategy to infer the range of probabilities of getting heads with a coin of Unknown Bias that is variable? I'm working on a similar problem with a game AI.
I'm working on a AI to play a game that consists of multiple nodes in a graph network. Each node in the network switches between two values, either A or B, based on a probability variable unknown to the player in every round. This probability variable also changes with each subsequent round based on what move the players makes.
Its a two player game, and if at the end of the game there are more node with the value A, player A win and if more nodes with value B then player B wins. Each player has 5 moves that affect the probability by multiples of 5:
move 1 affect the probability variable of each node by 5%, move 2 by 10%, and so on. Each move has a set energy cost, the greater the percentage change of the move, the greater the energy cost. The chosen move affect all the nodes in the graph network. The value switch happens once both players have made their moves
So the optimal strategy for each player would be to increase the probability variable of a node if its has the opponents value and decrease it if it has the players value. Every node in the graph is assigned a random probability at the start of the game, and the probability variable of each node is unknown to both players.
However the players are aware of the entire switching history of each Node so I assume I would need to infer the probability of each node in a specific round for my AIagent to decide on a move, so what would be the best strategy accomplish this? I assume it would be similar to inferring the probability of a biased coin, but I'm not sure how that would work considering that the probability variable of the nodes changes in each round
","['ai-design', 'statistical-ai', 'variational-inference']",
What is the difference between Neurosymbolic AI and Transformer AI,"
I'm looking at the AI timeline and I came across Neuro-Symbolic AI (being symbolic AI used in combination with deep learning) and Transformer AI (which I understand as neural networks that take context into account).
If I look at Wikipedia then I'll find that GTP-3 is a form of Neuro-Symbolic AI, but its name is clearly a giveaway for Transformer AI (Generative Pre-trained Transformer-3).
So I'm left wondering; what exactly is the different between these two?
","['transformer', 'neurosymbolic-ai']",
How to formulate Monte Carlo Tree Search in a stochastic environment with a changing action space,"
Can we efficiently solve a problem in which:

the valid actions at any given time are changing
the environment is stochastic
we have an infinite time horizon

using MCTS?
To be more specific, I'm attempting to optimize order batching and pick planning in a warehouse by formulating it as an MDP and searching for a good enough plan with MCTS. Within this environment:

The current state is fully known (what items need to be picked and where they are, the location of workers)
The set of possible actions is dependent on what items need to be picked
The state of the warehouse is changing stochastically over time (orders coming in)

MCTS appears to be a good approach to solve this problem because of the large branching search space and our existing ability to simulate a rollout of a policy and the changing state.
This answer points towards an ""open loop"" formulation of the problem. However, a set of actions (i.e. [<pick #189>, <pick #734>, , <pick #334>...]) is not even necessarily valid, given that between each action there is a chance for the state to change (new orders arriving).
How can I formulate this as MCTS? Is the necessarily going to need ""chance nodes""? Can we encode the state as a belief state instead?
","['markov-decision-process', 'monte-carlo-tree-search', 'planning']",
Designing a Target Location Environment for DeepRL,"
I'm trying to make an environment where my agent needs to navigate through a continuous space (using a continuous action space) to get to a target location. Currently, I spawn the agent and the target location at some random position within predefined location constraints at the start of each episode and let the agent go ham on the environment for up to a fixed number of steps. The reward at each step is a function of the current distance of the agent from the target location. I've tried training several models from Stable Baselines 3 (TD3, DDPG, PPO, etc.) for varying timesteps. Still, none of them has been able to learn to navigate to the target successfully. For the observation, I've tried giving both the offset of the agent from the target location, as well as the two locations individually.
At this point, I'm wondering if the task is too difficult (or abstract) for the agent to learn. Is there an intermediate task that I can train the agent on first and then transfer to this task? Would it be a good idea to fix the spawn and target locations across episodes while the agent hasn't reached the target location? I eventually still need the agent to be able to handle random spawn locations.
","['reinforcement-learning', 'deep-rl', 'environment', 'gym', 'goal-based-agents']",
What is regression activation maps?,"
I stumbled across a research topic : ""the training of multiple regression activation maps (RAM) to study the interaction between two images"". But, I cant find any decent definition of ""regression activation maps"" anywhere.
",['image-processing'],
Is there a reward function that would encourage exploration in this case?,"
I am new to Reinforcement Learning. I am trying to train PPO agent for citylearn. The goal is to lower two environmental variables from observations. The default reward function is
reward = (variable1 + variable2)*-1
The agent is providing actions with values that are close to -1 for higher iterations. Instead of attempting to improve the environment, it only tries to avoid high negative rewards. I've tried normalizing observations and rewards, but it's still been ineffective.
Is there a reward function that would encourage exploration rather than the same behavior?
","['reinforcement-learning', 'proximal-policy-optimization', 'reward-functions', 'reward-design']","
There is no one-size-fits-all answer to this question, as the best reward function for encouraging exploration will vary depending on the specifics of the AI system. However, some possible reward functions that could encourage exploration include giving the AI a small reward for each new state it visits, or giving the AI a large reward for discovering new states that are particularly valuable.
"
Extremely good results in RNN-LSTM python code!! How can this happen?,"
I am using this code here: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/ and more specifically the python code under the (1st) paragraph ""LSTM Network for Regression"". I am testing it with a simple .csv file where I have the 1st column (X values) and the 2nd column (Y values). I get extremely good results. My concern is: Do you something strange on the code of the above link?
","['recurrent-neural-networks', 'python', 'long-short-term-memory']",
The reason behind using MCTS over Alpha Beta Pruning in Alphazero,"
I am not really satisfied with the available analysis of why AlphaZero uses MCTS instead of Alpha Beta search.
Some analysis claim that its because MCTS is a lot more humanlike. I disagree because I don't think AlphaZero was really concerned about becoming humanlike. The fact that it ended up playing like humans was just a coincidence but it was never the goal behind its design choice.
In the AlphaZero paper,
MCTS and Alpha-Beta Search

For at least four decades the strongest computer chess programs have
used alpha-beta search (18, 23). AlphaZero uses a markedly different
approach that averages over the position evaluations within a
subtree, rather than computing the minimax evaluation of that subtree.
However, chess programs using traditional MCTS were much weaker than
alpha-beta search programs, (4, 24); while alpha-beta programs based
on neural networks have previously been unable to compete with
faster, handcrafted evaluation functions. AlphaZero evaluates
positions using non-linear function approximation based on a deep
neural network, rather than the linear function approximation used in
typical chess programs. This provides a much more powerful
representation, but may also introduce spurious approximation errors.
MCTS averages over these approximation errors, which therefore tend to
cancel out when evaluating a large subtree. In contrast, alpha-beta
search computes an explicit minimax, which propagates the biggest
approximation errors to the root of the subtree. Using MCTS may allow
AlphaZero to effectively combine its neural network representations
with a powerful, domain-independent search.

What do they mean by

approach that averages over the position evaluations within a subtree, rather than computing the minimax evaluation of that subtree.

What does it mean to average over position evaluations within a subtree?
Also in the next part,

AlphaZero evaluates positions using non-linear function approximation based on a deep
neural network, rather than the linear function approximation used in typical chess programs.

How is alphazero's evaluation non linear and how are the typical programs linear?
Could someone dumb these down?
My own guess behind MCTS over AB is because MCTS returns visit counts for each of the moves, and this data can be used to train the Policy Network in Alphazero. A minimax(AB) would return just that one best move, which could be used to train value network, but it cannot be used to train Policy network. So MCTS exists to train policy network in Alphazero. Is this a good or a bad guess?
Also, please do not mark these as multiple questions, I believe all these are a part of the same question.
","['monte-carlo-tree-search', 'function-approximation', 'alphazero', 'alpha-beta-pruning']",
Medical Image Segmentation of Pulmonary Embolism,"
The segmentation model, unet-resnet34, gives an IoU of 76% on training data and 74% on validation data. But when I tested it on test data, the IoU that I was getting was 60%. I don't know why it is so less. Can anyone give me the reason behind it?
","['deep-learning', 'image-segmentation']",
Why does a zero-input network initialize all bias terms to 0? (pytorch),"
I'm following https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=nn%20linear#torch.nn.Linear
Documentation allows for bias=True term, but they don't say where this parameter is actually stored. I wanted to clarify.
You would expect nn.Linear(1, 1, bias=True) to have 1 free parameter (if you draw it out you have 1 input going to 1 output, so only 1 weight). But this isn't the case. An extra bias term is implicitly added to the model, to get 2 parameters. They probably should have said that, but this is how it works.
With this framework in mind, we get to my question:
Why does declaring 0 as the input size and m as the output size initialize all the bias components to 0? It doesn't make sense why they would do this.
For instance,
>>> for p in nn.Linear(0, 4, bias=True).parameters(): print(p)

Parameter containing:
tensor([], size=(4, 0), requires_grad=True)
Parameter containing:
tensor([0., 0., 0., 0.], requires_grad=True)


","['machine-learning', 'pytorch']",
possible to combine multiple labeled objects as one object?,"
So I have labeled the entire skeletal muscles in detail. For example instead of just labeling shoulders I have labeled:

Rear Delt
Middle Delt
Front Delt

but now you want all of the delts to be combined as one object, shoulder. Is there anyway to take all of the previous labeled images of laptops and combine them as one object?
The why:
We have a lot of objects that are close to one another and it's making the model pretty big and complex. I want to drastically reduce the number of objects in the model but I don't want to lose over 15,000 labeled instance segmentation images.
thank you in advance
","['image-segmentation', 'data-labelling', 'mask-rcnn', 'coco-dataset']","
There are probably many ways to combine classes.  Here is my suggestion:

Figure out the mask pixel values for each of the classes you want to pool (e.g. Rear Delt = 1, Middle Delt = 2, front delt = 3)
Next choose a ""recode"" pixel value of the pooled class that is not used for any other class (e.g., make all of them 1).
Loop through each mask file to (re)assign the ""recode"" value to a pixel if the pixel value is equal to the value determined in #1 above.

As an aside, you should confirm that your recoding worked by visualizing the masks before and after the recode.
Also, don't save the new mask file over your original mask file.  You never know when you may need the original again.
Finally, there is a SO post on vectorizing this process.
"
Are my proofs that the Bellman operators are contractions correct?,"
Introduction
I'm studying Reinforcement Learning, and in order to increase my understanding I've been challenging myself by trying to write proofs that show that the right hand side of the Bellman equations and Bellman optimality equations for state-value and action-value functions are contraction operators. The fact that the Bellman operators are contractions is of great importance, since this proves that the Bellman equations have unique fixed points that can be solved for iteratively, due to the Banach fixed-point theorem.
Below are my attempts at the proofs, my questions are:

Are the proofs correct?
Any feedback on what can be improved (I'm very inexperienced in writing proofs and would like to get better)?

I will assume some knowledge about finite Markov Decision Processes (MDPs), please let me know if you want me to elaborate.
Bellman equations
Let $\mathcal{S}$ denote the finite set of all states, $\mathcal{R} \subset \mathbb{R}$ the finite set of all rewards, and $\mathcal{A}(s)$ the finite set of all available actions $a$ in state $s$. Let $r_\pi(s)$ denote the expected immediate reward and $p_\pi(s^\prime | s)$ the probability of transitioning to state $s^\prime$ when following policy $\pi$ from state $s$. Let $\gamma \in (0, 1)$ denote the discount rate for future rewards. The Bellman equation for the state-value function for policy $\pi$ can be defined as follows:
$$
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}_\pi \big[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s \big] \\
&= \sum_{a \in \mathcal{A}(s)} \pi(a | s) \bigg[\sum_{r \in \mathcal{R}} p(r | s, a) r + \gamma \sum_{s^\prime \in \mathcal{S}} p(s^\prime | s, a) v_{\pi}(s^\prime) \bigg] \\
&= r_\pi(s) + \gamma \sum_{s^\prime \in \mathcal{S}} p_\pi(s^\prime | s) v_{\pi}(s^\prime)
\end{aligned} \tag 1
$$
for all $s \in \mathcal{S}$. Let $n = |\mathcal{S}|$, we can write the equation in matrix form:
$$
\begin{bmatrix}
v_\pi(1) \\
\vdots \\
v_\pi(n)
\end{bmatrix}=
\begin{bmatrix}
r_\pi(1) \\
\vdots \\
r_\pi(n)
\end{bmatrix}
+\gamma
\begin{bmatrix}
p_\pi(1 | 1) & \dots & p_\pi(n | 1) \\
\vdots & \ddots & \vdots\\
p_\pi(1 | n) & \dots & p_\pi(n | n)
\end{bmatrix}
\begin{bmatrix}
v_\pi(1) \\
\vdots \\
v_\pi(n)
\end{bmatrix} \tag 2
$$
More compactly:
$$
v_\pi = r_\pi + \gamma P_\pi v_\pi \tag 3
$$
We can define an expected Bellman operator $\mathcal{T}^\pi : \mathbb{R}^n \to \mathbb{R}^n$ as:
$$
\mathcal{T}^\pi(v) = r_\pi + \gamma P_\pi v \tag 4
$$
for any $v \in \mathbb{R}^n$.
Let $||\cdot||$ be a norm in $\mathbb{R}^n$. If there exists a $\gamma \in (0, 1)$ such that $||\mathcal{T}^\pi(v_1) - \mathcal{T}^\pi(v_2)|| \leq \gamma ||v_1 - v_2||$ for all $v_1, v_2 \in \mathbb{R}^n$, then $\mathcal{T}^\pi$ is a contraction mapping. In all proofs in this post $|\cdot|$ and $\leq$ are elementwise, and the norm used is the max norm $||x||_\infty = \max(|x|) = \max(|x_1|, \dots, |x_n|)$, where $\max(\cdot) : \mathbb{R}^n \to \mathbb{R}$ chooses the largest element in a vector.
$$
\begin{aligned}
||\mathcal{T}^\pi(v_1) - \mathcal{T}^\pi(v_2)||_\infty &= \max \big(|r_\pi + \gamma P_\pi v_1 - (r_\pi + \gamma P_\pi v_2)| \big) \\
&= \gamma \max \big(|P_\pi(v_1 - v_2)| \big) \\
&\leq \gamma \max \big(P_\pi|v_1 - v_2| \big) \\
&\leq \gamma \max \big(|v_1 - v_2| \big) \\
&= \gamma ||v_1 - v_2||_\infty
\end{aligned}
\tag 5
$$
Thus $\mathcal{T}^\pi$ is a contraction. The last inequality is due to the rows of $P_\pi$ containing only non-negative elements that sum to 1.
Let $r(s, a)$ denote the expected immediate reward when selecting action $a$ in state $s$, and $p_\pi(s^\prime, a^\prime | s, a)$ the probability of transitioning to state $s^\prime$ and selecting action $a^\prime$ when selecting action $a$ in state $s$ and following policy $\pi$ after. The Bellman equation for the action-value function for policy $\pi$ can be defined as follows:
$$
\begin{aligned}
q_{\pi}(s, a) &= \mathbb{E}_\pi \big[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a \big] \\
&=  \sum_{r \in \mathcal{R}} p(r | s, a) r + \gamma \sum_{s^\prime \in \mathcal{S}} p(s^\prime | s, a) \sum_{a^\prime \in \mathcal{A}(s^\prime)} \pi(a^\prime | s^\prime) q_{\pi}(s^\prime, a^\prime) \\
&= r(s, a) + \gamma \sum_{s^\prime \in \mathcal{S}} \sum_{a^\prime \in \mathcal{A}(s^\prime)} p_\pi(s^\prime, a^\prime | s, a) q_{\pi}(s^\prime, a^\prime)
\end{aligned} \tag 6
$$
for all $s \in \mathcal{S}$, $a \in \mathcal{A}(s)$. Let $n_s = |\mathcal{A}(s)|$, we can write the equation in matrix form:
$$
\begin{bmatrix}
q_\pi(1, 1) \\
q_\pi(1, 2) \\
\vdots \\
q_\pi(n, n_n)
\end{bmatrix}=
\begin{bmatrix}
r_\pi(1, 1) \\
r_\pi(1, 2) \\
\vdots \\
r_\pi(n, n_n)
\end{bmatrix}
+\gamma
\begin{bmatrix}
p_\pi(1, 1 | 1, 1) & p_\pi(1, 2 | 1, 1) & \dots & p_\pi(n, n_n | 1, 1) \\
p_\pi(1, 1 | 1, 2) & p_\pi(1, 2 | 1, 2) & \dots & p_\pi(n, n_n | 1, 2) \\
\vdots & \vdots & \ddots & \vdots \\
p_\pi(1, 1 | n, n_n) & p_\pi(1, 2 | n, n_n) & \dots & p_\pi(n, n_n | n, n_n)
\end{bmatrix}
\begin{bmatrix}
q_\pi(1, 1) \\
q_\pi(1, 2) \\
\vdots \\
q_\pi(n, n_n)
\end{bmatrix} \tag 7
$$
More compactly:
$$
q_\pi = r_\pi + \gamma P_\pi q_\pi \tag 8
$$
The only difference compared to the equation for the state-value function is that the vectors and matrices are larger. Thus, we can define an expected Bellman operator in identical fashion (with $n$ denoting the number of state-action pairs rather than the number of states) and the proof will be identical (the rows of $P_\pi$ still contain only non-negative elements that sum to 1).
Bellman optimality equations
For brevity I will go directly into the compact matrix form of the Bellman optimality equation for the optimal state-value function:
$$
v_* = \max_\pi(r_\pi + \gamma P_\pi v_*) \tag 9
$$
We can define an expected Bellman optimality operator $\mathcal{T}^* : \mathbb{R}^n \to \mathbb{R}^n$ as:
$$
\mathcal{T}^*(v) = \max_\pi(r_\pi + \gamma P_\pi v) \tag {11}
$$
The rest of the post will focus on trying to prove that this is a contraction operator, because once again I believe that an identical proof works for the action-value function (please correct me if this is wrong).
Consider any two vectors $v_1, v_2 \in \mathbb{R}^n$, and let $\pi_1^* = \text{argmax}_\pi(r_\pi + \gamma P_\pi v_1)$ and $\pi_2^* = \text{argmax}_\pi(r_\pi + \gamma P_\pi v_2)$. Then we have:
$$
\mathcal{T}^*(v_1) = \max_\pi(r_\pi + \gamma P_\pi v_1) = r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 \geq r_{\pi_2^*} + \gamma P_{\pi_2^*} v_1 \\
\mathcal{T}^*(v_2) = \max_\pi(r_\pi + \gamma P_\pi v_2) = r_{\pi_2^*} + \gamma P_{\pi_2^*} v_2 \geq r_{\pi_1^*} + \gamma P_{\pi_1^*} v_2 \tag{12}
$$
$$
\begin{aligned}
\mathcal{T}^*(v_1) - \mathcal{T}^*(v_2) &= r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 - (r_{\pi_2^*} + \gamma P_{\pi_2^*} v_2) \\
&\leq r_{\pi_1^*} + \gamma P_{\pi_1^*} v_1 - (r_{\pi_1^*} + \gamma P_{\pi_1^*} v_2) \\
&= \gamma P_{\pi_1^*} (v_1 - v_2)
\end{aligned}
$$
Similarly we have $\mathcal{T}^*(v_2) - \mathcal{T}^*(v_1) \leq \gamma P_{\pi_2^*} (v_2 - v_1)$, which implies that $\mathcal{T}^*(v_1) - \mathcal{T}^*(v_2) \geq \gamma P_{\pi_2^*} (v_1 - v_2)$, and thus we have:
$$
\gamma P_{\pi_2^*} (v_1 - v_2) \leq \mathcal{T}^*(v_1) - \mathcal{T}^*(v_2) \leq \gamma P_{\pi_1^*} (v_1 - v_2) \tag{13}
$$
Let $\max\{\cdot, \cdot \} : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}^n$ choose the largest values between two vectors elementwise, we have:
$$
\begin{aligned}
|\mathcal{T}^*(v_1) - \mathcal{T}^*(v_2)| &\leq \max \{|\gamma P_{\pi_2^*} (v_1 - v_2)|, |\gamma P_{\pi_1^*} (v_1 - v_2)| \} \\
&\leq \gamma \max \{P_{\pi_2^*} |v_1 - v_2|, P_{\pi_1^*} |v_1 - v_2| \}
\end{aligned}
$$
Let $\max(\cdot) : \mathbb{R}^n \to \mathbb{R}$ choose the largest element in a vector (as previously defined), we have:
$$
\begin{aligned}
||\mathcal{T}^*(v_1) - \mathcal{T}^*(v_2)||_\infty &= \max \big(|\mathcal{T}^*(v_1) - \mathcal{T}^*(v_2)| \big) \\
&\leq \gamma \max \big(\max \{P_{\pi_2^*} |v_1 - v_2|, P_{\pi_1^*} |v_1 - v_2| \} \big) \\
&\leq \gamma \max \big(|v_1 - v_2| \big) \\
&= \gamma ||v_1 - v_2||_\infty
\end{aligned}
$$
Thus $\mathcal{T}^*$ is a contraction. Note that the last inequality is once again due to the rows of $P_\pi$ containing only non-negative elements that sum to 1.
Thank you for reading this far! If you have the time, please let me know if the proofs are correct, if the proofs for the state-value and action-value functions are identical as I suggest, and any other feedback.
","['reinforcement-learning', 'math', 'proofs', 'bellman-equations', 'bellman-operators']",
"Surveys, Papers, Hand on Tutorials about training data generation for anomaly detection","
I am searching for anything related to supervised, semi supervised or unsupervised anomaly detection w.r.t training data generation.
I am looking toward reading any work that tackles the issue how to generate a training/test dataset for anomaly detection.
If someone knows any work related to the subject above, please share it with me.
Regards
","['unsupervised-learning', 'supervised-learning', 'anomaly-detection', 'semi-supervised-learning', 'novelty-search']",
Why are policy gradients popular in RL when there exists a dual LP formulation in terms of occupation measures that can be solved easily?,"
Why are policy gradient methods popular in reinforcement learning when there exists a dual LP formulation in terms of occupation measures that can be solved easily?
","['reinforcement-learning', 'policy-gradients', 'linear-programming']","
Policy gradient methods are popular in reinforcement learning because they are fast and easy to implement. Additionally, policy gradient methods often work well for simple problems. for example, if the Hellinger distance between two measures is small.
Dual LP methods may be more accurate if the problem is not too simple. However, they can be more difficult to implement and may require more computational resources. they are preferable if you have more information about the underlying distribution of the data.
"
"What would the ""state space"" and its Python implementation be for my simulation?","
Context
I'm trying to build a social-consensus simulation involving two intelligent agents. The simulation involves a graph/network of nodes. Nearly all of these nodes (> 90%) will be green agents. The remaining nodes will consist of one red agent, one blue agent, and some number of grey agents. The red agent and blue agent will be the only intelligent agents in the simulation, and their relationship is adversarial.
I'm representing the node data structure with the following Node class:
class Node():

    def __init__(self, id, team):
        self.id = id
        self.team = team
        self.agent = None
        self.edges = []

        if self.team == ""green"":
            self.agent = agents.GreenAgent()
        elif self.team == ""blue"":
            self.agent = agents.BlueAgent()
        elif self.team == ""red"":
            self.agent = agents.RedAgent()

The graph/network is instantiated to consist only of green agents. When the graph/network is instantiated, edges are added between the nodes based on some probability. So these edges represent the connections between green agents / nodes containing green agents, and they allow for interaction. These edges are undirected, and there are no loops (meaning nodes do not have edges to themselves).
I'm representing the graph/network data structure with the following Network class:
class Network():

    def __init__(self, number_of_green_nodes, probability_of_an_edge):
        self.number_of_green_nodes = number_of_green_nodes
        self.number_of_nodes_overall = number_of_green_nodes
        self.probability_of_an_edge = probability_of_an_edge
        self.green_network = [Node(id = i, team = ""green"") for i in range(1, number_of_green_nodes + 1, 1)]
        self.total_number_of_grey_nodes_inserted = 0

        all_pairs_of_nodes = combinations(self.green_network, 2)
        for node1, node2 in all_pairs_of_nodes:
            if random.random() < self.probability_of_an_edge:
                node1.create_edge(node2)

The green agents represent ordinary people. They will have two attributes: voting, which can be true or false, and uncertainty, which is some number that represents how uncertain the individual is about whether they are voting (say, -1 to +1, or 0 to 1). The green agents are not intelligent, and, in each round of the simulation, when it is their turn, they simply generically interact with each other. If it is the green agents' turn to interact, then all green agents with an edge/connection interact with each other, so there is no selectivity. But, with that said, they themselves do not initiate interaction with the red, blue, or grey agents. When the green agents interact with each other, their opinion on whether or not to vote, and their uncertainty, changes based on some calculation.
My green agent class is represented partially as follows:
class GreenAgent():

    def __init__(self):
        self.voting = random.random() < 0.5
        self.uncertainty = scipy.stats.truncnorm.rvs(0, 1)

    def interact_with_green_agent(self, other_agent_opinion, other_agent_uncertainty):

        if other_agent_opinion != self.voting:
            if other_agent_uncertainty < self.uncertainty:
                self.voting = not self.voting
                self.uncertainty = self.uncertainty + (1 - other_agent_uncertainty/self.uncertainty)*(1 - self.uncertainty)
        else:
            if other_agent_uncertainty < self.uncertainty:
                self.uncertainty = self.uncertainty - (1 - other_agent_uncertainty/self.uncertainty)*(self.uncertainty - other_agent_uncertainty)

The red agent and blue agent are added to the network after the network is initialised with the green agents and their edges. The red agent and blue agent do not have an edge to each other (they do not interact with each other), but they do have an edge to every single green agent/node (so, unlike between the green agents/nodes, it is not based on some probability), so the red agent and blue agent are initially able to interact with all green nodes/agents.
The red agent and blue agent are adversaries. The red agent's goal is to convince a majority of the green agents not to vote, and to have these green agents be more certain of not voting than uncertain (based on the green agents' uncertainty attribute). On the other hand, the blue agent's goal is to convince a majority of the green agents to vote, and to have these green agents be more certain of voting than uncertain (again, based on the green agents' uncertainty attribute. The red/blue agents cannot have their ""opinion"" changed with regards to voting, so the red agent will always have attribute voting as false, and the blue agent will always have attribute voting as true.
When it is the red agent's turn in the simulation, the red agent can, initially, interact with all of the green agents/nodes. And, just like the case of green-green interaction, the red agent will interact with all green agents that it has an edge/connection to – there is no selectivity. When it interacts with the green agents, it can select from 5 levels of propaganda, with each level of propaganda becoming increasingly radical/extreme. The trade-off here is that, the more radical/extreme the propaganda from the red agent, the greater its uncertainty level becomes when disseminating the propaganda, and the more likely they are to permanently alienate green agents (represented by me as the removal of an edge).
My red agent class is represented as follows:
class RedAgent():

    def __init__(self):
        self.voting = False
        self.uncertainty = None

    def messaging(self, potency):
        if potency == 1:
            self.uncertainty = random.uniform(0.0, 0.2)
        elif potency == 2:
            self.uncertainty = random.uniform(0.2, 0.4)
        elif potency == 3:
            self.uncertainty = random.uniform(0.4, 0.6)
        elif potency == 4:
            self.uncertainty = random.uniform(0.6, 0.8)
        elif potency == 5:
            self.uncertainty = random.uniform(0.8, 1.0)

And my part of the green agent class that deals with the red agent is as follows:
def interact_with_red_agent(self, other_agent_uncertainty, this_green_agent_node, red_agent_node):

    if other_agent_uncertainty < 0.2:
        if self.voting == False and other_agent_uncertainty < self.uncertainty:
            self.uncertainty = self.uncertainty - (1 - other_agent_uncertainty / self.uncertainty) * (
                    self.uncertainty - other_agent_uncertainty)
        elif self.voting == True and other_agent_uncertainty < self.uncertainty:
            self.voting = False
            self.uncertainty = self.uncertainty + (1 - other_agent_uncertainty / self.uncertainty) * (
                        self.uncertainty - other_agent_uncertainty)
        if random.random() < other_agent_uncertainty/2:
            this_green_agent_node.remove_edge(red_agent_node)

    elif 0.2 < other_agent_uncertainty < 0.4:
        if self.voting == False and other_agent_uncertainty < self.uncertainty:
            self.uncertainty = self.uncertainty - (1 - other_agent_uncertainty / self.uncertainty) * (
                    self.uncertainty - other_agent_uncertainty)
        elif self.voting == True and other_agent_uncertainty < self.uncertainty:
            self.voting = False
            self.uncertainty = self.uncertainty + (1 - other_agent_uncertainty / self.uncertainty) * (
                        self.uncertainty - other_agent_uncertainty)
        if random.random() < other_agent_uncertainty/2:
            this_green_agent_node.remove_edge(red_agent_node)

    elif 0.4 < other_agent_uncertainty < 0.6:
        if self.voting == False and other_agent_uncertainty < self.uncertainty:
            self.uncertainty = self.uncertainty - (1 - other_agent_uncertainty / self.uncertainty) * (
                    self.uncertainty - other_agent_uncertainty)
        elif self.voting == True and other_agent_uncertainty < self.uncertainty:
            self.voting = False
            self.uncertainty = self.uncertainty + (1 - other_agent_uncertainty / self.uncertainty) * (
                        self.uncertainty - other_agent_uncertainty)
        if random.random() < other_agent_uncertainty/2:
            this_green_agent_node.remove_edge(red_agent_node)

    elif 0.6 < other_agent_uncertainty < 0.8:
        if self.voting == False and other_agent_uncertainty < self.uncertainty:
            self.uncertainty = self.uncertainty - (1 - other_agent_uncertainty / self.uncertainty) * (
                    self.uncertainty - other_agent_uncertainty)
        elif self.voting == True and other_agent_uncertainty < self.uncertainty:
            self.voting = False
            self.uncertainty = self.uncertainty + (1 - other_agent_uncertainty / self.uncertainty) * (
                        self.uncertainty - other_agent_uncertainty)
        if random.random() < other_agent_uncertainty/2:
            this_green_agent_node.remove_edge(red_agent_node)

    elif 0.8 < other_agent_uncertainty < 1:
        if self.voting == False and other_agent_uncertainty < self.uncertainty:
            self.uncertainty = self.uncertainty - (1 - other_agent_uncertainty / self.uncertainty) * (
                    self.uncertainty - other_agent_uncertainty)
        elif self.voting == True and other_agent_uncertainty < self.uncertainty:
            self.voting = False
            self.uncertainty = self.uncertainty + (1 - other_agent_uncertainty / self.uncertainty) * (
                        self.uncertainty - other_agent_uncertainty)
        if random.random() < other_agent_uncertainty/2:
            this_green_agent_node.remove_edge(red_agent_node)

When it is the blue agent's turn in the simulation, the blue agent can interact with all of the green agents/nodes. And, again, the blue agent interacts with all green agents/nodes that it has an edge/connection to – so, again, there's no selectivity. Similar to the red agent, the blue agent can select from 5 levels of potency of messaging. The trade-off, however, is that the blue agent has an ""energy level"", and the more potent the message the blue agent chooses during its turn, the more likely that it loses energy. And, if the blue agent loses all of its energy, then it loses the simulation/game.
Furthermore, another option that the blue agent has during its turn is to insert a grey agent into the network. The grey agent, based on some probability, can either be an ally of the blue agent, helping to convince the green agents to vote, or can actually work against the blue agent, working for the red agent in further radicalising the green agents and convincing them not to vote. Depending on whether it is on the side of the blue agent or the red agent, the grey agent's interaction with the green agents can either mimic the red agent’s interaction abilities or the blue agent’s interaction abilities. However, the difference is that the grey agent does not suffer any of the consequences of taking action that the red/blue agent does during interaction: if the grey agent is an ally of the blue agent, then it can do the same move as the blue agent, without the blue agent losing any energy, but if the grey agent ends up working for the red agent, it can do the same move as the red agent, without the red agent having the chance of alienating any green agents. The choice by the blue agent to insert a grey agent into the network, instead of disseminating messaging, will take up the turn of the blue agent, regardless of whether the grey agent proves to work for the blue agent or against it.
My blue agent class is as follows:
class BlueAgent():

    def __init__(self):
        self.voting = True
        self.uncertainty = None
        self.energy_level = 10

    def messaging(self, potency):
        if potency == 1:
            self.uncertainty = random.uniform(0.8, 1.0)
            if random.random() > self.uncertainty:
                self.energy_level -= 1
        elif potency == 2:
            self.uncertainty = random.uniform(0.6, 0.8)
            if random.random() > self.uncertainty:
                self.energy_level -= 1
        elif potency == 3:
            self.uncertainty = random.uniform(0.4, 0.6)
            if random.random() > self.uncertainty:
                self.energy_level -= 1
        elif potency == 4:
            self.uncertainty = random.uniform(0.2, 0.4)
            if random.random() > self.uncertainty:
                self.energy_level -= 1
        elif potency == 5:
            self.uncertainty = random.uniform(0.0, 0.2)
            if random.random() > self.uncertainty:
                self.energy_level -= 1

And my part of the green agent class that deals with the blue agent is as follows:
def interact_with_blue_agent(self, other_agent_uncertainty):

    if other_agent_uncertainty < self.uncertainty:
        if self.voting != True:
            self.voting = True
        self.uncertainty = self.uncertainty - (1 - other_agent_uncertainty / self.uncertainty) * (
                self.uncertainty - other_agent_uncertainty)

My grey agent class is as follows:
class GreyAgent():

    def __init__(self):
        self.spy = random.random() < 0.5
        self.uncertainty = None

    def lifeline(self, potency):
        if potency == 1:
            self.uncertainty = random.uniform(0.8, 1.0)
        elif potency == 2:
            self.uncertainty = random.uniform(0.6, 0.8)
        elif potency == 3:
            self.uncertainty = random.uniform(0.4, 0.6)
        elif potency == 4:
            self.uncertainty = random.uniform(0.2, 0.4)
        elif potency == 5:
            self.uncertainty = random.uniform(0.0, 0.2)

    def misinformation(self, potency):
        if potency == 1:
            self.uncertainty = random.uniform(0.0, 0.2)
        elif potency == 2:
            self.uncertainty = random.uniform(0.2, 0.4)
        elif potency == 3:
            self.uncertainty = random.uniform(0.4, 0.6)
        elif potency == 4:
            self.uncertainty = random.uniform(0.6, 0.8)
        elif potency == 5:
            self.uncertainty = random.uniform(0.8, 1.0)

The red and blue agents have access to the number of green agents that are voting / not voting, but they do not have access to the green agents' uncertainty levels. Furthermore, the red and blue agents have access to what action the other (red/blue) agent took during their turn. And both agents also have access to the action the grey agent took.

Question
I am now trying to make the red agent and blue agent intelligent. My thought was to use reinforcement learning, whereby each agent learns from previous rounds of the simulation. In particular, I'm currently looking at applying Q-Learning. The problem is that I'm not familiar with reinforcement learning, and I'm trying to learn it by reading tutorials and going through the textbook Reinforcement Learning: An Introduction, second edition, by Sutton and Barto.
So my idea is to apply Q-Learning to train the red agent and blue agent individually. My understanding is that, in order to do this, I need to define the ""environment,"" where the ""environment"" consists of the (1) ""state space,"" (2) ""actions,"" and (3) ""rewards."" The current problem is that, despite studying a ton of tutorials, I still can't figure out what my ""state space"" is supposed to be here (for the red agent and blue agent individually), and nor can I figure out what its Python implementation would be. What is the ""state space"" here, and what would its Python implementation be?
","['reinforcement-learning', 'q-learning', 'python', 'intelligent-agent', 'state-spaces']",
Label based normalizing flow,"
I am interested in capturing higher-dimensional embeddings of a image dataset as a gaussian noise, such that a specific region of gaussian noise corresponds to embedding of a particular label. How do we get to do that? For instance, suppose I have two classes $A$, $B$ in say 100 dimensions. Now, I want to do train a Normalizing flow which would have $z_1, z_2, \ldots, z_{100}$ gaussian variables, such that $z_1 > 0$ implies the normalizing flow came from class $A$ and $z_1 < 0$ implies class $B$. How do we achieve that ?
Is conditional Normalizing flow help in achieving this?
","['reference-request', 'density-estimation', 'conditional-vae']",
Deep Clustering Approach for Unsupervised Video Anomaly Detection,"
I'm working on Unsupervised Video Anomaly Detection, and I've tried implementing the Generative Cooperative Learning method, with the help of this paper.
The method uses a fixed backbone (ResNext-101) for video feature extraction. The videos are divided into segments of 16 frames, and a feature vector is computed for each segment. A generator (A simple Autoencoder) provides pseudo labels (based on the reconstruction error) for the discriminator which is a simple fully connected classifier. Pseudo labels from the discriminator are used to improve the generator using a process called negative learning, and in this fashion, the Generator and Discriminator are put in a collaborative learning loop, and the loss eventually converges.
I've recently come across the Deep Clustering paper , and was wondering if we can use a clustering method instead of the autoencoder as part of the generator. I think we can use the cosine distance as a good distance metric. The troublesome part however, is thinking of a good criteria for generating the pseudo labels.
With the autoencoder, the reconstruction error is a pretty intuitive criteria for pseudo labelling. Since anomalies are sparse, the autoencoder will not be able to reconstruct them properly and so they will have large(r) reconstruction errors.
What can be a similar criteria that we can use for pseudo labelling if we use a clustering method instead of an Autoencoder?
","['deep-learning', 'computer-vision', 'unsupervised-learning', 'clustering', 'self-supervised-learning']","

What can be a similar criteria that we can use for pseudo labelling if we use a clustering method instead of an Autoencoder?

Maybe the amount of Data points inside a Cluster. Like you said, an anomaly is a sparse occurrence, so there should be fewer data points in an anomalous cluster.
This would only work for known anomalies, which you can put in a known anomalous cluster, though.
"
How to fix the embedding gauge in a BERT-like model?,"
I have a pre-trained BERT model from Huggingface, which I tune to categorize short texts (like tweets or slightly longer) into several thousand categories using triplet loss.
As I understand, if I train two models on the same dataset, the resulting embeddings are not cross-comparable between the two models, meaning that the same text won't necessarily have similar embedding vector in two models. It happens this way because normally I do not care about the absolute values of the embedding vectors, I only care about their distances and relative positions to the other vectors, for the purposes of the classification. So, the absolute position of the vector is an unconstrained degree of freedom.
My question is - is there a way to ""fix the gauge"" of the model when training: to modify the loss or something like that, so that the embedding of the same text would always have approximately the same embedding vector, even if I train another model on the same train set?
","['natural-language-processing', 'bert', 'embeddings', 'triplet-loss-function']",
How to add a SimpleRNN layer within Convolutional layers in tensorflow without changing ndim?,"
I'm trying to add a RNN layer within a Convolutional layer. Unfortunately due to difference of ndim it's failing to create a model.
Model:
model = keras.Sequential(
[
    # layers.Rescaling(1.0/255),
    keras.Input(shape=(256, 3)),
    layers.SimpleRNN(216, return_sequences=True, activation='relu'),
    keras.Input(shape=(256, 256, 3)),
    layers.Conv2D(32, (3,3), padding=""valid"", activation='swish'),
    layers.MaxPooling2D(pool_size=(2,2)),
    layers.Conv2D(64, 3, activation=""swish""),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, activation=""swish""),
    layers.Flatten(),
    layers.Dense(64, activation='sigmoid'),
    layers.Dense(10),
    layers.Dense(2),
]

)
It'll be really helpful, if someone can help to figure this out :)
","['convolutional-neural-networks', 'recurrent-neural-networks', 'tensorflow']",
How to interpret binary classification metrics on an imbalanced data set?,"
I have an imbalanced dataset on intrusion detection. I have (attack class) 3668045 samples and (benign class) 477 samples. I made a 70:30 Train test split. My problem is to predict whether the given node belongs to the attack class or the benign class. As a first step, I trained a decision tree model on the dataset without using any balancing technique. I obtained the following results for my model on the test set using the sklearn metrics.
Scores for  Decision Tree
Accuracy: 0.9998991419799247
True positive 1100391
True Negative 55
False Positive 86
False Negative 25
F2-score 0.9999661949775551
Precision 0.9999218520696025
Recall 0.9999772813190648
F1-score 0.9999495659261946
Log loss: 0.0034835750853569407
Decision Tree : AUROC (ROC Curve) = 0.999
Decision Tree : AUPR(Precision/Recall curve) = 1.000

 Classification Report
               precision    recall  f1-score   support

           0       0.69      0.39      0.50       141
           1       1.00      1.00      1.00   1100416

    accuracy                           1.00   1100557
   macro avg       0.84      0.70      0.75   1100557
weighted avg       1.00      1.00      1.00   1100557


Why am I getting high, almost perfect AUROC and AUPR scores, even though the precision and recall for my minority class are very low? What measures can I take to improve the results such that they are not biased and my model is generalizing well? How can I ensure that?
","['binary-classification', 'decision-trees', 'anomaly-detection', 'scikit-learn', 'imbalanced-datasets']",
Is the decoder in a transformer Seq2Seq model non parallelizable?,"
From my understanding, seq2seq models work by first computing a representation of the input sequence, and feeding this to the decoder. The decoder then predicts each token in the output sequence in an autoregressive manner. In this sense, it's limited to processing one time step at a time, as the next token in the sequence depends on the previous (non parallelizable). This is different from the encoder, as it's able to process many time steps in parallel. Is my understanding correct?
","['deep-learning', 'transformer', 'seq2seq']","
During training, the decoder can be trained in parallel (and that's one of its advantage over LSTM) :
You input <s> I love you, and the decoder learns to produce I love you </s>. For each token, the model learns to predict the next token (for example given <s> the model learns to predict I). This is possible thanks to attention.

But at inference time, you are right : the model has to predict the first token, then put this token in the decoder input and predict the second, etc...
So yes, at inference time a decoder is much slower (because we go through the whole decoder several times : one time for each token) than an encoder (which do a single forward pass).
"
What does consistency of heuristic intuitively mean in the A* algorithm and why are consistent heuristics monotonic?,"
Could someone give the intuition behind consistency of heuristic function in the A* algorithm?
From wikipedia:

Every node i will give an estimate that, after accounting for the cost
to reach i + 1, is always lesser than the estimate at node i + 1.

I understand what this means, but is there any intuitive meaning behind it? And how does this lead to monotonicity of the evaluation function = f(n) = g(n) + h(n)?
","['search', 'a-star']","
A consistent heuristic must not overestimate the true cost to get to the goal from a particular node - the heuristic estimate is always less than or equal to the cost to get to another node plus the estimated cost from that node to the goal (which itself cannot be an overestimate).
Basically, a consistent heuristic will not spuriously discard truly good options because it overestimates their cost and treats them as bad options. An inconsistent heuristic might overestimate the cost of some path, and instead explore another, costlier path and miss the optimal solution. A consistent heuristic allows paths to turn out longer than expected (since you can explore them and then go elsewhere if needed), but it does not allow paths to turn out shorter than expected (since you may miss them entirely if you get to the goal first).
With a consistent heuristic, the estimated final cost of of a partial solution (f(n) = g(n) + h(n)) is monotonically non-decreasing along the best path. The consistent heuristic can't overestimate costs, so true the cost must always be as large or larger than the estimated cost. If you move along the best path closer to the goal, the estimated final cost must not decrease - if you get closer to the goal and find your estimated final cost has decreased, it must mean that you overestimated the remaining cost somewhere along the way, which means the heuristic is not consistent.
"
Learning curve converges with huge errors,"
I am training an auto-encoder over $10^4$ epochs. I get a converging learning curve. However the error at the last stages stays huge $\sim10^{15}$. What does this mean? does it mean that my auto-encoder isn't trained properly? Is it a question of the error function?
The error function I am using is: $\mathcal{L} = \|X-\hat X\|_F^2$ where $X\in\mathbb{R}^{n\times d}$ is the original features (with $n$ the number of rows and $d$ the number of features), $\hat X\in\mathbb{R}^{n\times d}$ is the reconstructed features (by the auto-encoder) and $\|.\|_F^2$ refers to the Frobenius Norm.

","['machine-learning', 'deep-learning', 'objective-functions', 'unsupervised-learning']","
There are a few potential explanations for this. One possibility is that your model is overfitting to the training data. This means that it is memorizing the training data instead of learning to generalize from it. Another possibility is that your error function is not well-suited to the task at hand. This could be due to the fact that it is not a convex function, or it could be because it is not differentiable. Finally, it is also possible that your learning rate is too high, causing the model to diverge instead of converge.
Yes, the error function you are using is known as the Frobenius Norm. It is a measure of the distance between two matrices. In this case, it is measuring the distance between the original features and the reconstructed features. The fact that the error is staying huge at the last stages could be due to overfitting or a poor learning rate, as mentioned before.
"
machine learning for a budgeting application,"
I am interested in finding references and previous applications where prior year budgets are analyzed to provide guidance for a current year budget.   Specifically, each year some two thousand items are evaluated for funding, with perhaps 500 funded in that year. Information is available in a spreadsheet with multiple parameters that are manually evaluated to determine if an individual item is funded in the budget. I would appreciate any guidance as to how best to make use of such data for say the previous 5 years, where I know what has been funded in those years, to assist in screening items for the current budget year, in particular what approach to ML would be best.  I have attempted a literature search but have not found anything directly relevant.
Edit:
Found this reference in my literature search, looks to be applicable:
https://www.datacamp.com/courses/case-study-school-budgeting-with-machine-learning-in-python
","['machine-learning', 'reference-request', 'binary-classification']","
One approach you could take is to use a supervised learning algorithm to train a model on past budget data in order to predict which items are likely to be funded in the current budget year. This would require labelling the data with a binary label indicating whether or not each item was funded in each budget year. You could then use this labelled data to train a classification algorithm such as a logistic regression or a decision tree. Once you have trained your model, you can use it to predict which items are likely to be funded in the current budget year.
"
"Do genetic algorithms ""learn""?","
I am currently working my way into Genetic Algorithms (GA). I think I have understood the basic principles. I wonder if the time a GA takes to go through the iterations to determine the fittest individual is called learning time ?
","['machine-learning', 'genetic-algorithms', 'evolutionary-algorithms']","
I believe genetic algorithms DO NOT learn, because they're a search and optimization algorithms. They keep on filtering the better solutions in each iteration, but they can easily ""forget"" what had ""found"" earlier, if mutation or crossover happens.
"
Why does GridSearchCV model give worse results despite same parameters used with base model,"
I am trying to make prediction using random forest regression and then utilize GridSearchCV to tune hyperparameters(just 'n_estimators').  However results of GridSearchCV are worse than base model. Despite GridSearchCV parameter grid has same values with base model the results of GridSearchCV model is still worse. So, why is the RMSE of the GridSearchCV model worse than base model?
Here are models that I used:
Base Model:
rfr_rd_fi = RandomForestRegressor(n_estimators = 100, 
                                  max_features=7, 
                                  random_state = 3115)

Root Mean Squared Error for base model is 200.2
GridSearchCV model:
param_grid = {'n_estimators' : np.arange(10,1010,10, dtype=int),
              'max_features' : [7],
              'random_state' : [3115]}

cv_test= KFold(n_splits=10)

rfr_gs = GridSearchCV(RandomForestRegressor(), 
                      param_grid=param_grid,
                      scoring='neg_mean_squared_error',
                      cv=cv_test, verbose=4, n_jobs=-1).fit(X_train, y_train)

Root Mean Squared Error for GridSearchCV model is 257.2
","['regression', 'hyperparameter-optimization', 'random-forests', 'scikit-learn', 'grid-search']","
There are a number of possible reasons why this is the case. Here are a few things to check.
 1. Was training and testing done on the same dataset for your baseline model?
 2. Did you do cross validation with your baseline model?
 3. If so, did you use the same folds?

Bottom line:  To compare apples to apples, your baseline model and your Grid search should have the same pipelines, except for the hyperparameters, of course.  After you have done that, you can look at your grid search model where n_estimators=100) and you should have identical results as the baseline model.
On a separate note, you should consider nesting your hyperparameter tuning within your crossvalidation (see article).
"
Is Relativistic GAN better than WGAN-GP?,"
I am currently reading the ESRGAN paper and I noticed that they have used Relativistic GAN for training discriminator. So, is it because Relativistic GAN leads to better results than WGAN-GP?
","['convolutional-neural-networks', 'generative-adversarial-networks', 'generative-model', 'wasserstein-gan', 'image-super-resolution']","
For anyone looking for answer, as explained in Paper: The relativistic discriminator: a key element missing from standard GAN!,
Yes,  Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%).The  images generated by RaGAN are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.
"
For which problem sizes is Deep Q-Learning suitable and why?,"
I am wondering for which problem sizes a Deep Q-Learning algorithm is most appropriate. For example, whether it is particularly suited for low complexity problems or not for high complexity problems. And if that is the case, why?
","['machine-learning', 'deep-learning', 'q-learning']","
Deep Q-learning is most suitable for problems with a large or infinite number of potential states. This is because the more states there are, the more information the Q-learning algorithm has to work with, and the more accurate its predictions about future states and rewards will be, with a large number of potential states the algorithm is able to learn from more data. In problems with fewer potential states, the algorithm may not have enough data to work with in order to make accurate predictions about future states and rewards.
Hence a large number of potential states is that the algorithm is able to generalize better. In problems with fewer potential states, the algorithm may not be able to learn the underlying structure of the problem as well, and thus may not be able to generalize as well to new states.
"
How large of a neural network can you run on a modern graphics card?,"
I'm trying to get a general idea of what is a typical size for neural networks that run on a single graphics card.
What I mean for instance is if I have data with N total inputs, and I sent it through a network of M hidden layers with $N_m$ nodes, and Y outputs. What is the maximum size that a modern GPU can handle running in a reasonable timeframe?
I'm just looking for a sort of ballpark understanding of what values are typically used for node number and hidden layer number.
",['neural-networks'],"
TL;DR AT BOTTOM
Depending on your subjective opinion on what a ""modern"" GPU is, your answer will vary.
The recent NVIDIA 3000's and 4000's are considered to be ""the best"" but I cannot find a objective source that gives actual statistics, so this should be taken with a grain of salt.

If I have data with N total inputs, and I sent it through a network of M hidden layers with Nm nodes, and Y outputs. What is the maximum size that a modern GPU can handle running in a reasonable timeframe?

This depends on your AI model. Are you using Tensorflow or the hundreds of other open source projects or are you building from scratch? All AI models try to get the job done, only some do it in a fast timeframe as the main goal.
In my subjective experience with my ""modern"" laptop (depending on the definition here) a text generating AI can have up to two million nodes without the laptop being laggy. I have seen much more such as ten million nodes on a ""normal"" (again, subjective!) laptop. Image processing  and more data-full tasks are where millions of nodes are impractical. You would ideally need more than a ""modern laptop."" I don't play in this realm and I can't find reliable stats, but for sure you could do more than ten million.
TL;DR: Depends on your idea of a ""modern device."" I have seen up to ten million nodes on a laptop for text related AI, but no stats exist for GPU's all accross the board.
Benchmark next time you use the AI model in question to get a good idea of what you can handle.
"
Training loss is decreasing very slowly while learning MNIST database,"
I am developing my ANN from scratch which is supposed to classify MNIST database of handwritten digits (0-9). My feed-forward fully connected ANN has to be composed of:

One input layer, with 28x28 = 784 nodes (that is, features of each image)
One hidden layer, with any number of neurons (shallow network)
One output layer, with 10 nodes (one for each predicted class)

and has to compute gradient w.r.t. weights and bias thanks to backpropagation algorithm and, finally, it should learn exploiting gradient descent with momentum algorithm.
The loss function is: cross_entropy on ""softmaxed"" network's outputs, since the task is about classification.
Each hidden neuron is activated by the same activation function, I've chosen the sigmoid; meanwhile the output's neurons are activated by the identity function.
The dataset has been divided into:

60.000 training pairs (image, label) - for the training
5000 validation pairs (image, label) - for evaluation and select the network which minimize the validation loss
5000 testing pairs (image, label) - for testing the model picked using new metrics such as accuracy

The data has been shuffled invoking sklearn.utils.shuffle method.
These are my net's performance about training loss, validation loss and validation accuracy:
E(0) on TrS is: 798288.7537714319  on VS is: 54096.50409967187  Accuracy: 12.1 %
E(1) on TrS is: 798261.8584179751  on VS is: 54097.23663558976  Accuracy: 12.1 %
...
E(8) on TrS is: 798252.1191081362  on VS is: 54095.5016235736  Accuracy: 12.1 %
...
E(17) on TrS is: 798165.2674011206  on VS is: 54087.2823473459  Accuracy: 12.8 %
E(18) on TrS is: 798155.0888987815  on VS is: 54086.454077456074  Accuracy: 13.22 %
...
E(32) on TrS is: 798042.8283810444  on VS is: 54076.35518400717  Accuracy: 19.0 %
E(33) on TrS is: 798033.2512910366  on VS is: 54075.482037626025  Accuracy: 19.36 %
E(34) on TrS is: 798023.431899881  on VS is: 54074.591145985265  Accuracy: 19.64 %
E(35) on TrS is: 798013.4023181734  on VS is: 54073.685418577166  Accuracy: 19.759999999999998 %
E(36) on TrS is: 798003.1960815473  on VS is: 54072.76783050559  Accuracy: 20.080000000000002 %
...
E(47) on TrS is: 797888.8213232228  on VS is: 54062.70342708315  Accuracy: 21.22 %
E(48) on TrS is: 797879.005388998  on VS is: 54061.854566864626  Accuracy: 21.240000000000002 %
E(49) on TrS is: 797869.3890292909  on VS is: 54061.02482142968  Accuracy: 21.26 %
Validation loss is minimum at epoch: 49




(note that training loss seems to be much higher than validation loss and for this reason they seems to be straight horizontal lines in the first image)
This is my code:
import numpy as np
from scipy.special import expit
from matplotlib import pyplot as plt
from mnist.loader import MNIST


def relu(a, derivative=False):
    f_a = np.maximum(0, a)
    if derivative:
        return (a > 0) * 1
    return f_a  

def softmax(y):
    e_y = np.exp(y - np.max(y))
    return e_y / e_y.sum()

def cross_entropy(y, t, derivative=False, post_process=True):
    epsilon = 10 ** -308
    if post_process:
        if derivative:
            return y - t
        sm = softmax(y)
        sm = np.clip(sm, epsilon, 1 - epsilon)  # avoids log(0)
        return -np.sum(np.sum(np.multiply(t, np.log(sm)), axis=0))

def sigmoid(a, derivative=False):
    f_a = expit(a)
    if derivative:
        return np.multiply(f_a, (1 - f_a))
    return f_a

def identity(a, derivative=False):
    f_a = a
    if derivative:
        return np.ones(np.shape(a))
    return f_a

def accuracy_score(targets, predictions):
    correct_predictions = 0
    for item in range(np.shape(predictions)[1]):
        argmax_idx = np.argmax(predictions[:, item])
        if targets[argmax_idx, item] == 1:
            correct_predictions += 1
    return correct_predictions / np.shape(predictions)[1]


def one_hot(targets):
    return np.asmatrix(np.eye(10)[targets]).T


def plot(epochs, loss_train, loss_val):
    plt.plot(epochs, loss_train)
    plt.plot(epochs, loss_val, color=""orange"")
    plt.legend([""Training Loss"", ""Validation Loss""])
    plt.xlabel(""Epochs"")
    plt.ylabel(""Loss"")
    plt.grid(True)
    plt.show()

class NeuralNetwork:

    def __init__(self):
        self.layers = []

    def add_layer(self, layer):
        self.layers.append(layer)

    def build(self):
        for i, layer in enumerate(self.layers):
            if i == 0:
                layer.type = ""input""
            else:
                layer.type = ""output"" if i == len(self.layers) - 1 else ""hidden""
                layer.configure(self.layers[i - 1].neurons)

    def fit(self, X_train, targets_train, X_val, targets_val, max_epochs=50):
        e_loss_train = []
        e_loss_val = []

        # Getting the minimum loss on validation set
        predictions_val = self.predict(X_val)
        min_loss_val = cross_entropy(predictions_val, targets_val)

        best_net = self  # net which minimize validation loss
        best_epoch = 0  # epoch where the validation loss is minimum

        # batch mode
        for epoch in range(max_epochs):
            predictions_train = self.predict(X_train)
            self.back_prop(targets_train, cross_entropy)
            self.learning_rule(l_rate=0.00001, momentum=0.9)
            loss_train = cross_entropy(predictions_train, targets_train)
            e_loss_train.append(loss_train)

            # Validation
            predictions_val = self.predict(X_val)
            loss_val = cross_entropy(predictions_val, targets_val)
            e_loss_val.append(loss_val)

            print(""E(%d) on TrS is:"" % epoch, loss_train, "" on VS is:"", loss_val, "" Accuracy:"",
                  accuracy_score(targets_val, predictions_val) * 100, ""%"")

            if loss_val < min_loss_val:
                min_loss_val = loss_val
                best_epoch = epoch
                best_net = self
  
        plot(np.arange(max_epochs), e_loss_train, e_loss_val)

        return best_net

    # Matrix of predictions where the i-th column corresponds to the i-th item
    def predict(self, dataset):
        z = dataset.T
        for layer in self.layers:
            z = layer.forward_prop_step(z)
        return z

    def back_prop(self, target, loss):
        for i, layer in enumerate(self.layers[:0:-1]):
            next_layer = self.layers[-i]
            prev_layer = self.layers[-i - 2]
            layer.back_prop_step(next_layer, prev_layer, target, loss)

    def learning_rule(self, l_rate, momentum):
        # Momentum GD
        for layer in [layer for layer in self.layers if layer.type != ""input""]:
            layer.update_weights(l_rate, momentum)
            layer.update_bias(l_rate, momentum)


class Layer:

    def __init__(self, neurons, type=None, activation=None):
        self.dE_dW = None  # derivatives dE/dW where W is the weights matrix
        self.dE_db = None  # derivatives dE/db where b is the bias
        self.dact_a = None  # derivative of the activation function
        self.out = None  # layer output
        self.weights = None  # input weights
        self.bias = None  # layer bias
        self.w_sum = None  # weighted_sum
        self.neurons = neurons  # number of neurons
        self.type = type  # input, hidden or output
        self.activation = activation  # activation function
        self.deltas = None  # for back-prop

    def configure(self, prev_layer_neurons):
        self.set_activation()
        self.weights = np.asmatrix(np.random.normal(-0.1, 0.02, (self.neurons, prev_layer_neurons)))
        self.bias = np.asmatrix(np.random.normal(-0.1, 0.02, self.neurons)).T 


    def set_activation(self):
        if self.activation is None:
            if self.type == ""hidden"":
                self.activation = sigmoid
            elif self.type == ""output"":
                self.activation = identity  # will be softmax in cross entropy calculation

    def forward_prop_step(self, z):
        if self.type == ""input"":
            self.out = z
        else:
            self.w_sum = np.dot(self.weights, z) + self.bias
            self.out = self.activation(self.w_sum)
        return self.out

    def back_prop_step(self, next_layer, prev_layer, target, local_loss):
        if self.type == ""output"":
            self.dact_a = self.activation(self.w_sum, derivative=True)
            self.deltas = np.multiply(self.dact_a,
                                      local_loss(self.out, target, derivative=True))
        else:
            self.dact_a = self.activation(self.w_sum, derivative=True)  # (m,batch_size)
            self.deltas = np.multiply(self.dact_a, np.dot(next_layer.weights.T, next_layer.deltas))

        self.dE_dW = self.deltas * prev_layer.out.T

        self.dE_db = np.sum(self.deltas, axis=1)

    def update_weights(self, l_rate, momentum):
        # Momentum GD
        self.weights = self.weights - l_rate * self.dE_dW
        self.weights = -l_rate * self.dE_dW + momentum * self.weights

    def update_bias(self, l_rate, momentum):
        # Momentum GD
        self.bias = self.bias - l_rate * self.dE_db
        self.bias = -l_rate * self.dE_db + momentum * self.bias


if __name__ == '__main__':
    mndata = MNIST(path=""data"", return_type=""numpy"", mode=""randomly_binarized"")
    X_train, targets_train = mndata.load_training()  # 60.000 images, 28*28 features
    X_val, targets_val = mndata.load_testing()  # 10.000 images, 28*28 features

    X_train, targets_train = shuffle(X_train, targets_train.T)
    X_val, targets_val = shuffle(X_val, targets_val.T)

    # Getting the test set splitting the validation set in two equal parts
    # Validation set size decreases from 10.000 to 5000 (of course)
    X_val, X_test = np.split(X_val, 2)  # 5000 images, 28*28 features
    targets_val, targets_test = np.split(targets_val, 2)
    X_test, targets_test = shuffle(X_test, targets_test.T)

    targets_train = one_hot(targets_train)
    targets_val = one_hot(targets_val)
    targets_test = one_hot(targets_test)

    net = NeuralNetwork()
    d = np.shape(X_train)[1]  # number of features, 28*28
    c = np.shape(targets_train)[0]  # number of classes, 10

    # Shallow network with 1 hidden neuron
    # That is 784, 1, 10
    for m in (d, 1, c):
        layer = Layer(m)
        net.add_layer(layer)

    net.build()

    best_net = net.fit(X_train, targets_train, X_val, targets_val, max_epochs=50)

What I have done:

Set 5 instead of 1 hidden neuron
Add many hidden layers
Decrease/increase learning rate (l_rate) value
Decrease/increase momentum (and set it to 0)
Replace sigmoid with relu

but there still is the problem.
","['neural-networks', 'python', 'backpropagation', 'accuracy']",
How to normalizing various elements of the reward function?,"
Suppose I have a reward function $R$ that I wish to penalize w.r.t two distinct phenomenons $A$ and $B$. $A$, for example, could represent the phenomenon of the state not crossing some boundary $[s_1,s_2]$ and $B$ can represent the phenomenon that two consecutive actions shouldn't be too far apart $|a_t - a_{t+1}| < \epsilon$, for some small $\epsilon$. A trivial reward mechanism can be:
step(prev_a, a,epsilon, min_s, max_s):
    s = env(action)
    if not min_s < s < max_s: 
        r -= s
    if ||a-prev_a|| > epsilon:
        r -= a

As $A$ and $B$ are from different worlds (different physical units, if you will), they both have different ranges. For example, a state $s$ may obtain values that are at most $10$, though an action $a$ may have larger values like $100$. Hence, penalizing by subtracting the state or the action from the current reward may lead to the preference of the agent to only make sure the action condition is set, as this one translates to more future rewards.
How can this issue be addressed? I assume some normalization should be added, though I'm not quite sure how.
Any ideas?
","['reinforcement-learning', 'deep-rl', 'rewards', 'reward-functions', 'environment']","
There are several ways to normalize various elements of the reward function. One way would be to divide all values by the largest value in the set. This would ensure that all values are between $0$ and $1$. Another way would be to subtract the mean from all values and then divide it by the standard deviation. This would ensure that all values have a mean of $0$ and a standard deviation of $1$.
A trivial reward function for this scenario would be to simply assign a negative value to $R$ whenever either $A$ or $B$ occurs. However, this may not be the most effective way to penalize the behavior. A more effective approach might be to weigh the penalties differ depending on how severe the violation is. For example, if $A$ occurs, the penalty could be $-1$, but if $B$ occurs, the penalty could be $-10$. This would ensure that more severe violations are penalized more heavily than less severe violations.
This reward function appears to be penalizing the agent for taking actions that result in the state not being within some boundary, as well as for taking actions that are too far apart from the previous action.
One way to avoid this issue would be to normalize the values of $A$ and $B$ before subtracting them from the current reward. This would ensure that both $A$ and $B$ have an equal impact on the reward function, regardless of their range.
"
How to keep the best shot of an object with an Object detection model over a video,"
I’m pretty new to Machine learning and stuff. I’m going to start to work on a project, I have a background with code. My goal is to collect data from my city roads and build a strong dataset for people to use it.
At the moment, i have a kinda working solution, my ML model (using yolov7 https://github.com/WongKinYiu/yolov7 + Python) is able to detect cars and license-plate. I implemented a way to crop cars from from a video stream BUT it’s cropping every single frame where there’s a “car” detected.

I found a website of a company showing how their solution works “https://www.sighthound.com/” and we can see on the video presentation (~0.6s into the video) 
It’s exactly what i want to acheive, which is to keep the best result of all my cropped images from a batch and use it to create a dataset.
Do i have to put some kind of “ID” over the “car” label (like an object tracking) to identify each unique cars passing in my stream, and after that, use some kind of machine learning algorithm to select the best looking cropped images for each batch of images of a unique car ? Or there’s better solution?
Ps : I'm not doing OCR or surveillance, My only goal with this project it to create an open and free dataset for my community to use.
Update :
I want this dataset to be useable in a large quantity of sub-project and maybe inspired people to create their own dataset and contribute to the community.
So by ""best"" i mean : Less noise, less blur, the object ""car"" can be seen in its fullest, a clear view of the object ""lience-plate"".

For context, the country where i live is very unique (multiple gouvernment making the laws, we're a hub a population), we have 5~6 different type license plate in circulation, and little to no information about the what's going on the road.
I want to provide a high quality dataset to the community and help people/students/whoever want to work on a solution to fix road problems :

Analyse who's using the road (country 1: 50%, country 2: 25%, country 3: 25%), ...

","['machine-learning', 'python', 'object-detection']",
How to discover/approximate the causations/correlations between multiple time-series and related open source libraries?,"
I have the following time-series data with two value columns.
(t: time, v1: time-series values 1, v2: time-series values 2)
 t | v1 | v2 
---+----+----
 1 |  1 |  0
 2 |  2 |  2
 3 |  3 |  4
 4 |  3 |  6
 5 |  3 |  6
 6 |  4 |  6
 7 |  5 |  8
(7 rows)

I am trying to discover (or approximate) the correlation between the $v1$ and $v2$, and use that approximation for the next step predictions.
Please note, the most obvious correlation is $v2(t)=2.v1(t-1)$.
My question is, what are the algorithms to employ for such approximations and are there any open source implementations of those algorithms for SQL/python/javascript?
","['deep-learning', 'time-series', 'causation', 'correlation']","
I believe you may want to look at Autoregressive Models (AR) like ARMA models. Python has implementations of all AR models. Python statsmodels have all the necessary tools you may require.
"
openVINO human pose,"
Can someone give me a tip on what to research for predicting human pose with computer vision with the open VINO toolkit? Not a lot of wisdom here so any tips appreciated even at a high level on what I need to learn/research...
I preprocess an image with:
def preprocessing(input_image, height, width):
    '''
    Given an input image, height and width:
    - Resize to width and height
    - Transpose the final ""channel"" dimension to be first
    - Reshape the image to add a ""batch"" of 1 at the start 
    '''
    image = np.copy(input_image)
    image = cv2.resize(image, (width, height))
    image = image.transpose((2,0,1))
    image = image.reshape(1, 3, height, width)

    return image

and run inference with the open VINO toolkit models:
# Run inference.
predicted_poses = compiled_pose_model([image])[compiled_pose_model.output(0)]

Which returns a numpy array but then get lost on what the next steps are to turn an image:

Into this:

This is the code I am working. Any tips appreciated.
EDIT
Im working with the open Model Zoo (part of Intel's Open VINO project) and the model I am working with is the human-pose-estimation-0001.
","['computer-vision', 'python', 'opencv']",
"What is the difference between A2C and Q-Learning, and when to use one over the other?","
I'm trying to get an accurate answer about the difference between A2C and Q-Learning. And when can we use each of them?
","['reinforcement-learning', 'comparison', 'q-learning', 'advantage-actor-critic']","
Take a look at this blog: https://mpatacchiola.github.io/blog/2017/02/11/dissecting-reinforcement-learning-4.html
In a nutshell, the major difference between the two algorithms is: Q-learning consists of a critic only (to update state-action values) while A2C is composed of two networks: an actor (to take an action) and a critic (to evaluate and update state-action values). Major advantage over Q-learning is its computational efficiency, as far as I know.
"
Are masks needed for images that don't contain the object of interest in (binary) Image Segmentation tasks?,"
Total Dataset :- 100 (on case level)
Training :- 76 cases (18000 slices)
Validation :- 19 cases (4000 slices)
Test  :- 5 cases (2000 slices)
I have a dataset that consists of approx. Eighteen thousand images, out of which approx. Fifteen thousand images are of the normal patient and around 3000 images of patients having some diseases. Now, for these 18000 images, I also have their segmentation mask. So, 15000 segmentations masks are empty, and 3000 have patches.
Should I also feed my model (deep learning, i.e., unet with resnet34 backbone) empty masks along with patches (non empty mask)?
",['image-segmentation'],"
You should, absolutely.
Even more, if you have prior knowledge regarding the distribution of the disease (like 1 case every 1000) I would strive to get a training dataset close to that distribution, even if it means collecting and adding more control cases with empty masks.
At the end you'll probably end up with a really imbalanced dataset, the way to tackle this issue is using the focal loss.
"
Any models for text to json,"
There are many sequence to sequence (seq2seq) models and end to end models, like text to sql. I was wondering are there any text to json deep learning models?
For example:
Text
""Switch on the computer"". 

JSON:
{""actions"":[""switch on""], ""object"":""computer""}

Another Example:
Text:
""Lower the heater temperature to 25 degrees and before that play music for one hour"".

JSON:
 {""actions"": [""lower the heater temperature"",""play the music""],""order"":[1,0]}

Note that JSON has different or some additional elements depending on the text. This is different from regular seq2seq models. Any suggestions on which models to use? Thank you!!
","['deep-learning', 'natural-language-processing', 'seq2seq']",
Features for a Content-Based recommendation system,"
I'm working on a hybrid recommendation system (collaborative and content-based) for an online ordering/shopping app. So far I've managed to identify a data-source for the collaborative model (likely item-based) but I'm having trouble deciding on features and algorithms for the content-based model.
I know it's probably specific to the problem I'm working on, but if anyone could share their experience or recommendations :) on what features give the best results for a content-based system it would be really helpful.
All relevant advice/opinions welcome.
","['features', 'feature-selection', 'recommender-system', 'feature-engineering']","
Some features that have been found to work well for content-based recommender systems include:

Item category (e.g. food, clothing, electronics, etc.)

Item sub-category (e.g. type of food, type of clothing, etc.)

Item brand

Item price

Item description

Item reviewer ratings

Number of reviews

Overall sentiment of reviews

Keywords associated with the item

Any other relevant metadata


"
Very low accuracy (0.11) and it remains constant after few epochs on MNIST database,"
I am developing my ANN from scratch which is supposed to classify MNIST database of handwritten digits (0-9). My feed-forward fully connected ANN has to be composed of:

One input layer, with 28x28 = 784 nodes (that is, features of each image)
One hidden layer, with 1,2,3,4 or 5 nodes (shallow network)
One output layer, with 10 nodes (one for each predicted class)

and has to compute gradient w.r.t. weights and bias thanks to backpropagation algorithm and, finally, it should learn exploiting gradient descent with momentum algorithm.
The loss function is: cross-entropy + softmax, since the task is about classification.
Each hidden neuron is activated by the same activation function, the sigmoid; meanwhile the output's neurons are activated by the identity function.
The dataset has been divided into:

60.000 training pairs (image, label) - for the training
5000 validation pairs (image, label) - for evaluation and select the network which minimize the validation loss
5000 testing pairs (image, label) - for testing the model picked using new metrics such as accuracy

This is the curves I get about training and validation loss:  and these are some logs about their loss and accuracy on the validation set:
E(0) on TrS is: -0.09995663952048875  on VS is: -0.10000215508367913  Accuracy: 9.120000000000001 %
E(1) on TrS is: -0.09997782058505714  on VS is: -0.10000721545529842  Accuracy: 9.120000000000001 %
...
E(47) on TrS is: -0.1000167196491167  on VS is: -0.10001587893489938  Accuracy: 11.42 %
E(48) on TrS is: -0.10001676221437443  on VS is: -0.10001594086326895  Accuracy: 11.42 %
E(49) on TrS is: -0.10001680503178402  on VS is: -0.10001599995199295  Accuracy: 11.42 %

Validation loss is minimum at epoch: 22

Finally, if I test the network at epoch 22, which is the epoch where the validation loss is minimum, I get the following result:
Accuracy score on test set is: 11.28 %

These are my questions: as you can see, the accuracy remains (almost) constant and it is very low. It has a low value on the test set as well and on the training set. Is that normal? If it is not, how can I fix?
This is my code:
import numpy as np
from scipy.special import xlog1py
from scipy.special import expit
from matplotlib import pyplot as plt
from mnist.loader import MNIST


def softmax(y):
    e_y = np.exp(y - np.max(y))
    return e_y / e_y.sum()

def cross_entropy(y, t, derivative=False, post_process=True):
    if post_process:
        if derivative:
            return y - t
        return -np.sum(np.sum(xlog1py(t, softmax(y)), axis=0))

def sigmoid(a, derivative=False):
    f_a = expit(-a)
    if derivative:
        return np.multiply(f_a, (1 - f_a))
    return f_a

def identity(a, derivative=False):
    f_a = a
    if derivative:
        return np.ones(np.shape(a))
    return f_a

def accuracy_score(targets, predictions):
    correct_predictions = 0
    for item in range(np.shape(predictions)[1]):
        # print(predictions[:, item])  # dbg
        argmax_idx = np.argmax(predictions[:, item])
        # print(""argmax idx"", argmax_idx)  # dbg
        # print(targets_test[:, item])  # dbg
        if targets[argmax_idx, item] == 1:
            correct_predictions += 1
    return correct_predictions / np.shape(predictions)[1]


def one_hot(targets):
    return np.asmatrix(np.eye(np.max(targets) + 1)[targets]).T


def plot(epochs, loss_train, loss_val):
    plt.plot(epochs, loss_train)
    plt.plot(epochs, loss_val, color=""orange"")
    plt.legend([""Training Loss"", ""Validation Loss""])
    plt.xlabel(""Epochs"")
    plt.ylabel(""Loss"")
    plt.grid(True)
    plt.show()

class NeuralNetwork:

    def __init__(self):
        self.layers = []

    def add_layer(self, layer):
        self.layers.append(layer)

    def build(self):
        for i, layer in enumerate(self.layers):
            if i == 0:
                layer.type = ""input""
            else:
                layer.type = ""output"" if i == len(self.layers) - 1 else ""hidden""
                layer.configure(self.layers[i - 1].neurons)

    def fit(self, X_train, targets_train, X_val, targets_val, max_epochs=50):
        e_loss_train = []
        e_loss_val = []

        # Getting the minimum loss on validation set
        predictions_val = self.predict(X_val)
        min_loss_val = cross_entropy(predictions_val, targets_val)

        best_net = self  # net which minimize validation loss
        best_epoch = 0  # epoch where the validation loss is minimum

        # batch mode
        for epoch in range(max_epochs):
            predictions_train = self.predict(X_train)
            self.back_prop(targets_train, cross_entropy)
            self.learning_rule(l_rate=0.000001, momentum=0.9)
            loss_train = cross_entropy(predictions_train, targets_train)
            e_loss_train.append(loss_train)

            # Validation
            predictions_val = self.predict(X_val)
            loss_val = cross_entropy(predictions_val, targets_val)
            e_loss_val.append(loss_val)

            print(""E(%d) on TrS is:"" % epoch, loss_train, "" on VS is:"", loss_val, "" Accuracy:"",
                  accuracy_score(targets_val, predictions_val) * 100, ""%"")

            if loss_val < min_loss_val:
                min_loss_val = loss_val
                best_epoch = epoch
                best_net = self

        print(""Validation loss is minimum at epoch:"", best_epoch)

        plot(np.arange(max_epochs), e_loss_train, e_loss_val)

        return best_net

    # Matrix of predictions where the i-th column corresponds to the i-th item
    def predict(self, dataset):
        z = dataset.T
        for layer in self.layers:
            z = layer.forward_prop_step(z)
        return z

    def back_prop(self, target, loss):
        for i, layer in enumerate(self.layers[:0:-1]):
            next_layer = self.layers[-i]
            prev_layer = self.layers[-i - 2]
            layer.back_prop_step(next_layer, prev_layer, target, loss)

    def learning_rule(self, l_rate, momentum):
        # Momentum GD
        for layer in [layer for layer in self.layers if layer.type != ""input""]:
            layer.update_weights(l_rate, momentum)
            layer.update_bias(l_rate, momentum)


class Layer:

    def __init__(self, neurons, type=None, activation=None):
        self.dE_dW = None  # derivatives dE/dW where W is the weights matrix
        self.dE_db = None  # derivatives dE/db where b is the bias
        self.dact_a = None  # derivative of the activation function
        self.out = None  # layer output
        self.weights = None  # input weights
        self.bias = None  # layer bias
        self.w_sum = None  # weighted_sum
        self.neurons = neurons  # number of neurons
        self.type = type  # input, hidden or output
        self.activation = activation  # activation function
        self.deltas = None  # for back-prop

    def configure(self, prev_layer_neurons):
        self.set_activation()
        self.weights = np.asmatrix(np.random.normal(-0.1, 0.02, (self.neurons, prev_layer_neurons)))
        self.bias = np.asmatrix(np.random.normal(-0.1, 0.02, self.neurons)).T  # vettore colonna


    def set_activation(self):
        if self.activation is None:
            if self.type == ""hidden"":
                self.activation = sigmoid
            elif self.type == ""output"":
                self.activation = identity

    def forward_prop_step(self, z):
        if self.type == ""input"":
            self.out = z
        else:
            self.w_sum = np.dot(self.weights, z) + self.bias
            self.out = self.activation(self.w_sum)
        return self.out

    def back_prop_step(self, next_layer, prev_layer, target, local_loss):
        if self.type == ""output"":
            self.dact_a = self.activation(self.w_sum, derivative=True)
            self.deltas = np.multiply(self.dact_a,
                                      local_loss(self.out, target, derivative=True))
        else:
            self.dact_a = self.activation(self.w_sum, derivative=True)  # (m,batch_size)
            self.deltas = np.multiply(self.dact_a, np.dot(next_layer.weights.T, next_layer.deltas))

        self.dE_dW = self.deltas * prev_layer.out.T

        self.dE_db = np.sum(self.deltas, axis=1)

    def update_weights(self, l_rate, momentum):
        # Momentum GD
        self.weights = self.weights - l_rate * self.dE_dW
        self.weights = -l_rate * self.dE_dW + momentum * self.weights

    def update_bias(self, l_rate, momentum):
        # Momentum GD
        self.bias = self.bias - l_rate * self.dE_db
        self.bias = -l_rate * self.dE_db + momentum * self.bias


if __name__ == '__main__':
    mndata = MNIST(path=""data"", return_type=""numpy"", mode=""randomly_binarized"")
    X_train, targets_train = mndata.load_training()  # 60.000 images, 28*28 features
    X_val, targets_val = mndata.load_testing()  # 10.000 images, 28*28 features

    # Getting the test set splitting the validation set in two equal parts
    # Validation set size decreases from 10.000 to 5000 (of course)
    X_val, X_test = np.split(X_val, 2)  # 5000 images, 28*28 features
    targets_val, targets_test = np.split(targets_val, 2)

    targets_train = one_hot(targets_train)
    targets_val = one_hot(targets_val)
    targets_test = one_hot(targets_test)

    net = NeuralNetwork()
    d = np.shape(X_train)[1]  # number of features, 28*28
    c = np.shape(targets_train)[0]  # number of classes, 10

    # Shallow network with 1 hidden neuron
    # That is 784, 1, 10
    for m in (d, 1, c):
        layer = Layer(m)
        net.add_layer(layer)

    net.build()

    best_net = net.fit(X_train, targets_train, X_val, targets_val, max_epochs=50)

    predictions = best_net.predict(X_test)
    print(""Accuracy score on test set is:"", accuracy_score(targets_test, predictions) * 100, ""%"")

Note that:

The performance are similar even though I set 5 hidden neurons
instead of 1
Accuracy on training set is very low as well (around 11 %)

","['neural-networks', 'machine-learning', 'python', 'backpropagation', 'accuracy']",
Sequenatial data: Which model should I apply?,"
I need to predict a binary vector given a sequential dataset meaning the current datapoint depends on its predecessors as well as (known) successors.
So, it looks something like this:
Given the sequence: X = [x_1, x_2, x_3, ..., x_N]
I want to predict: Y = [y_1, y_2, y_3, ..., y_N] with y_i \in {0, 1} a binary label
Now, the sequence X is fully known, that is: future observations in the sequence are completely known at any time.
Therefore, in contrast to normal time series data, I can also harness any x_i+1, x_i+2,... from the full X sequence for predicting y_i at any time and not only x_i-2, x_i-1, x_i etc.
Also the data X is a sequence of R^dxN vectors, i.e. N d-dimensional datapoints containing real numbers.
In my case the dimensionality of the data is d=140.
Now, what I want to predict is the following:
What is y_i given X, e.g. what is y_3 given the observations x_1, x_2, x_3, x_4, x_5?
So, eventually I need something like this:
for i in range(N):
    predict y_i given X = [x_1, x_2, x_3, ..., x_N]

This actually is a many-to-one prediction task.
Now you could use a RNN, but the problem imho is that ""future"" observations are not taken into account when applying an RNN.
Maybe I am wrong about this assumption.
But therefore I am asking: Which model would you suggest to use for this problem?
",['sequence-modeling'],"
Okay, I finally ended up using a Transformer based Encoder/Decoder model.
It is a very similar structure as the NLP model described in the Attention is all you need paper.
However, since my use case is not NLP but rather sequential data in general I used to follow the Transformer Encoder/Decoder method as described in this  paper: Deep Transformer Models for Time Series Forecasting
I slightly modified their model in my implementation to account for my special needs. I used pytorch to build the transformer architecture.
The results are very good.
I think when trying to predict sequential data, a encoder/decoder based transformer model will be a good choice and at least worth a look :)
"
Batching together similar length sequences to avoid padding and packing,"
I am training an RNN in PyTorch to produce captions for images. It's a pretty standard architecture – the image is processed by a pre-trained InceptionV3 to extract features, the recurrent module processes the words seen so far and then its result is merged with image features and fed through a linear layer to produce a probability distribution over the vocab to predict the next word. A network trained this way can auto-regressively predict a whole caption for an image (it's seeded with an artificial  token at the beginning to have a preceding sequence to start off with).
To train the network every caption is decomposed like this in the training dataset:
X, y
<START>, a
<START> a, man
<START> a man, is
<START> a man is, sitting
<START> a man is sitting, on
<START> a man is sitting on, a
<START> a man is sitting on a, chair
<START> a man is sitting on a chair, <END>

The result is that there is a lot of input sequence length variation in the training data which is a problem since we would need a lot of padding in batches. I am aware that there are techniques of counteracting this (pack_padded_sequence in PyTorch) but I came up with another idea.
I am modifying the Sampler object that's used to provide indices for the DataLoader to construct batches in such a way that the Sampler orders all of the example indices by input sequence length, groups it into batch-size chunks, and shuffles the chunks before feeding them to the DataLoader. As a result the sequences in individual batches are almost always the same length – not because of any padding but because of the fact that we ordered the examples by length before grouping them. There will be some batches that will encompass examples on a border of length groups so some padding will be necessary but it will be minimal (just one zero for a group of lengths [5, 5, 5, 6, 6, 6] for example) and the number of such groups would be negligible (~0.002% of the batches in my case).
My question is – does this method introduce some kind of drawback that I am not aware of? I find it simpler than having completely random batches and padding and packing them. I can't tell however, if it would introduce some kind of bias to the model. My intuition tells me that this should not be problem since the sequence lengths for batches would vary greatly across training (since I am shuffling the index chunks), but I wanted to get a second opinion. I guess the question boils down to -> is sequence length variation within the batch important for proper training? Or, for this specific case -> is it important that the net process examples decomposed from a single caption within the same batch or close to each other time-wise?
I'd be happy to train the network with both solutions and check the results myself but it's quite time-expensive and I'd prefer to avoid some stupid stuff if I can.
","['natural-language-processing', 'recurrent-neural-networks', 'pytorch', 'training-datasets', 'batch-learning']","
You can read that this was done in ""Attention is All You Need"", for Transformers: ""Sentence pairs were batched together by approximate sequence length."".
But, with RNN you don't need any padding at all when using packedSequences, so I don't think your idea will be efficient, if I understand correctly.
"
Training a neural network in full batch training,"
It is a trend in deep learning to train models using multi-batches, i.e., to show the model a subset of the entire dataset for each weight update. In some cases, as in continual learning, we see that it is possible to train the network on one datapoint at the time. It is quite rare, on the other hand, to see research papers, or state of the art models, that are trained on a whole dataset simultaneously, i.e., in full-batch training.
For a research project, it would be useful for me to come up with a list of applications/models where training a neural network in full batch training is preferable with respect to using multi-batches. An example I have found is the COIN and COIN++ papers, that train on full batch training in order to memorise datapoints.
Question:
Do you know other applications where performing full-batch training is preferable to use mini-batches? Which ones?
","['deep-neural-networks', 'batch-size']","
I think the closest approach to what is described in the papers you linked is Neural Style Transfer. But I see also some (potential) misunderstanding regarding the full batch training, so let me elaborate a bit.
In the papers you linked and the neural style transfer one a model is trained on a single image. Not a single image at the time, literally on a single image. So I wouldn't call it full batch training, since there's no batch at all, and there can't be one. But I guess this is open to interpretation and it depends on how you define a batch in the first place.
it would be useful for me to come up with a list of applications/models where training a neural network in full batch training is preferable with respect to using multi-batches
Aside from these examples (i.e. overfitting or fine tuning on a single training instance) I doubt you'll find papers where people use full batch training. Not only full batch training comes with all sort of computational issues (like keeping in memory all gradients for an entire dataset) but even from a mathematical point of view mini batch and stochastic gradient descent works better than full batch and (classic) gradient descent. I did link some references about this topic in another answer.
"
Why aren't artificial derivatives used more often to solve the vanishing gradient problem?,"
While looking into the vanishing gradient problem, I came across a paper (https://ieeexplore.ieee.org/abstract/document/9336631) that used artificial derivatives in lieu of the real derivatives. For a visualization, see the attached image: 
As you can see, the artificial derivative of the ReLU is the sigmoid function, which smoothes out the derivative to circumvent the dying ReLU problem. The authors apply similar smoothing operations for other activation functions such as the sigmoid. This idea seems pretty convincing, and it surprises me that it has not been used thus far, especially considering there is no additional computational cost in replacing the derivatives, as the authors claim. So why is it that vanishing gradients have not been addressed in this way in conventional deep learning frameworks such as Pytorch or Tensorflow?
","['activation-functions', 'vanishing-gradient-problem']",
Policy Gradient Methods when using a fixed initial sequence of actions,"
I am implementing a Policy Gradient agent based on IMPALA. Specifically, I'm working on DeepNash, but that is not considerably different from vanilla IMPALA for the purposes of this question.
In my implementation, I am running vectorized games that I can run batched on my GPU using convolutions and such, similar to this https://towardsdatascience.com/learning-to-play-snake-at-1-million-fps-4aae8d36d2f1?gi=7ef24f81a7a6
However, because of a technical limitation, I have to fix the first $n$ actions of every environment in the batch to be the same. To explain further, these initial $n$ actions consist of placing 'walls', and the placement of walls determines a set of 'vision' mask tensors that filter the observations of the environment. It is not possible to store a separate set of vision masks for each environment, so I've opted to use the same walls/masks for each environment in the batch.
This would be a fine compromise, if not for the fact that it is seems to be off policy, since we are not actually sampling the net works output during this initial stage. In fact, it could be described as selecting from a policy that is identical to the nets except in the initial stage, where it is deterministic. From this viewpoint, it seems natural to use off-policy correction. Indeed, DeepNash already uses IMPALA, so I was hopeful that v-trace would 'absorb' this compromise. However, the policy that is generating the trajectories is deterministic, so this causes singularities in the $\log \frac{\pi (a)}{\mu (a)}$ terms of the v-trace estimate ($\mu$ is the actor policy).
My intuition tells me that it wouldn't be catastrophic if I were to use the trajectories in the batch as if they were on-policy. It seems tantamount to a difference in learning rate at 2 distinct phases of the game: initial and non-initial. Is this correct? Any advice?
","['reinforcement-learning', 'deep-learning', 'policy-gradients', 'off-policy-methods']",
"Would convolution filters provide better position encoding than the traditional cos/sin functions with k,n,i, and d?","
Would convolution filters provide better position encoding than the traditional cos/sin functions with k,n,i and d? My thoughts are that the traditional position encoding functions introduce too much noise into the network, with essentially random values at low values of i. I would think that a convolution filter would be more useful for processing contextual information. Are there any resources out there for implementing this sort of idea?
","['convolutional-neural-networks', 'positional-encoding']",
"What are your ""current parameters"" in Minibatch Stochastic Gradient Descent?","
I was reading a book on Deep Learning when I came across a line, more like a few words that didn't make apparent sense.

Thus, we will often settle for sampling a random minibatch of examples every time we need to compute the update, a variant called minibatch stochastic gradient descent.
In each iteration, we first randomly sample a minibatch B consisting of a fixed number of training examples. We then compute the derivative (gradient) of the average loss on the minibatch with regard to the model parameters. Finally, we multiply the gradient by a predetermined positive value η and subtract the resulting term from the current parameter values.


We can express the update mathematically as follows ($∂$ denotes the partial derivative):

$(w,b) \leftarrow (w,b) - {η\over |B|} {\Large \Sigma}_{I \in B} \partial_{(w,b)}l^{(i)}(w,b).$
The set cardinality $|B|$ represents the number of examples in each minibatch (the batch size) and $η$ denotes the learning rate.
What does ""current"" parameter values mean in this context?
You can find the book here -
https://d2l.ai $\rightarrow$ Chapter Linear Neural Networks, Part 3.1.1.4
","['machine-learning', 'deep-learning', 'optimization', 'gradient-descent', 'mini-batch-gradient-descent']","
The current parameter values are the values of the weights $w$ and the biases $b$ in the neurons in each layer.  What you are calculating is the gradient of the average loss of the batch relative to the weight and the bias.  Since the gradient points ""up"", or toward greater loss, you multiply by the learning rate and subtract it from the current values.
"
How to filter point data based on pattern,"
I have a set of points represented as (x, y), and wish to separate out chunks/clusters of them which represent a ball trajectory.
For example, in the two sample images below I wish to find out cluster C1 and C2, while removing noise / outlier points that are represented as points inside squares.
Essentially I want a solution that yields and separates valid trajectories into groups from noise/ false positives.
Also, would it be possible to train an AI model to do this?


","['neural-networks', 'deep-learning', 'data-preprocessing', 'pattern-recognition', 'kalman-filter']",
Does iterative deepening depth-first search expand at most twice as many nodes as breadth-first search?,"
My understanding is that iterative deepening search is roughly equivalent to breadth-first search, except instead of keeping all visited nodes in memory, we regenerate nodes as needed, trading off memory for time.
The $(i+1)$th layer of a tree has about as many nodes as the entire rest of the tree before it*. Thus, on each iteration, iterative-deepening effectively has to expand the $(i+1)$th layer twice whereas breadth-first search only has to expand it once. So IDS takes at most twice as long as BFS. Is this correct?
* A proof of this statement is as follows: in a tree with branching factor $b$, the $i$th layer has $b^i$ nodes. The number of nodes up to and including the $i$th node is given by the $i$th partial sum of the geometric series, $\frac{b^{i+1} - 1}{b-1}$, which is strictly less than the number of nodes in the $(i+1)$th layer ($b^{i+1}$)
Please let me know if all this is correct and whether clarifications are needed anywhere to make it more rigorous.
","['search', 'graphs', 'computational-complexity', 'tree-search', 'iddfs']",
Carla simulator and gpu,"
I am trying to buy a HP laptop with iris intel graphic card to run the Carla self driving simulator. Can carls run on iris or I need to buy a laptop with nividia gpu. Thx u
",['machine-learning'],"
While Carla was developed by Intel their GitHub recommends a powerful Nvidia GPU.
Comparing a mid-level NVidia GPU vs an Intel IRIS the NVidia comes out ahead. You could use an Intel IRIS, but the performance wouldn't be very satisfactory.
An Intel Arc A730M-Powered Laptop would be a minimum laptop configuration, with an Intel GPU, for running Carla; still not anywhere as well as a top of the line GPU (which is necessary for AI), but I understand that cost is also a consideration for many people.
"
"Yolo darknet displays only most relevant match. Is it possible to display probabilities for all classes, even if they are very small?","
Using yolo Darknet by AlexeyAB
I have 2 classes: notouch and overpack - levels of tobacco in hookah bows. I have good accuracy, but the problem is that yolo displays most relevant class and it's probability. Is there any chance to display all available classes and their availabilities in the output?
","['machine-learning', 'computer-vision', 'python', 'yolo', 'darknet']",
"Why can't language models, like GPT-3, continuously learn once trained?","
GPT-3 has a prompt limit of about ~2048 ""tokens"", which corresponds to about 4 characters in text. If my understanding is correct, a deep neural network is not learning after it is trained and is used to produce an output, and, as such, this limitation comes from amount of the input neurons. My question is: what is stopping us from using the same algorithm we use for training, when using the network? That would allow it to adjust its weights and, in a way, provide a form of long-term memory which could let it handle prompts with arbitrarily long limits. Is my line of thinking worng?
","['deep-learning', 'language-model']","
In theory, there is nothing stopping you from updating the weights of a neural network whenever you like. You run an example through the network, calculate the difference between the network's output and the answer you expected, and run back propagation, exactly the same as you do when you initially train the network. Of course, usually networks are trained with large batches of data instead of single examples at a time, so if you wanted to do a weight update you should save up a bunch of data and pass through a batch (ideally the same batch size that you used during training, though there's nothing stopping you from passing in different batch sizes).
Keep in mind this is all theoretical. In practice, adjusting the weights on a deployed network will probably be very difficult because the models weights have been exported in a format optimized for inference. And it's better to have distinct releases with sets of weights that do not change rather than continuously updating the same model.
Either way, changing the weights continuously would not affect the ""memory"" of the network in any way.  The lengths of sequences that sequence-to-sequence models like transformers or RNNs can accept is an entirely separate parameter.
"
Where can I get Imagenet test dataset labels for evaluation,"
I have the imagenet train, validation and test set. I have been able to assign each image in the validation set into its respective class folders with the help of some online resources. However, for the test set, I could not find the labels.
All the model evaluation experiments on Imagenet dataset I’ve seen people do only use a fraction of the validation set. I have not been able to find one that used the test set or a fraction of it.
I really want to use the imagenet test set to evaluate my pytorch model. How do I go about it or where can I get the labels for the test set?
","['python', 'pytorch', 'image-net']",
Why don't approximate TD target use a sample mean?,"
In practice, we prefer to fit the action-value function by minimizing the Mean Squared Error (MSE) between a parameterized function $Q_{w}(s, a)$ and one-step td target,
\begin{equation}
  y = r(s, a) + \gamma Q_{w}(s', a')\\
\text{where $a' \sim \pi(\cdot | s')$} 
\end{equation}
My question is that why don't we use a sample mean i.e. $\frac{1}{N}\sum\limits_{i=1}^{N}Q_{w}(s', a_{i}), a_{i} \sim \pi(\cdot| s')$ to replace the single sample? Given $N$ not too large, we at least can have a better estimate of the true target.
","['actor-critic-methods', 'temporal-difference-methods']",
What does the adversarial loss in a GAN represent?,"
I'm working on Pix2Pix an image-to-image translation GAN, and I noticed that there is an adversarial loss implemented using BCE, and a L1 loss implemented using MAE. I know L1 loss represents the difference between the predicted image and actual image, but I am not sure what does the GAN adversarial loss represent?
This is the official definition
The adversarial loss influences whether the generator model can output images that are plausible in the target domain
but the meaning is tough to understand. Is it representing the difference between the predicted probability distribution and actual probability distribution?
","['generative-adversarial-networks', 'loss']","
The adversarial loss in a GAN represents the amount of information that the generator is able to trick the discriminator into believing is true.
""The adversarial loss influences whether the generator model can output images that are plausible in the target domain""$\rightarrow$ means that the generator model should output images that look like they could plausibly belong to the target domain. This is important because if the generated images look too fake, it will be difficult for the discriminator to learn to correctly classify them. It's the data that determines whether and how it improves.
In addition, the adversarial loss is based on the generator model's experience of its successes and failures in the data.
"
How does Seq2Seq with attention actually use the attention (i.e. the context vector)?,"
For neural machine translation, there's this model ""Seq2Seq with attention"", also known as the ""Bahdanau architecture"" (a good image can be found on this page), where instead of Seq2Seq's encoder LSTM passing a single hidden vector $\vec h[T]$ to the decoder LSTM, the encoder makes all of its hidden vectors $\vec h[1] \dots \vec h[T]$ available and the decoder computes weights $\alpha_i[t]$ with each iteration -- by comparing the decoder's previous hidden state $\vec s[t-1]$ to each encoder hidden state $\vec h[i]$ -- to decide which of those hidden vectors are the most valuable. These are then added together to get a single ""context vector"" $\vec c[t] = \alpha_1[t]\,\vec h[1] + \alpha_2[t]\,\vec h[2]+\dots +\alpha_T[t]\,\vec h[T]$, which supposedly functions as Seq2Seq's single hidden vector.
But the latter can't be the case. Seq2Seq originally passed that vector to the decoder as initialisation for its hidden state. Evidently, you can only initialise it once. So then, how is $\vec c[t]$ used by the decoder? None of the sources I have read (see e.g. the original paper linked above, or this article, or this paper, or this otherwise excellent reader) disclose what happens. At most, they hide this mechanism behind ""a function $f$"" which is never explained. I must be overlooking something super obvious here, apparently.
","['long-short-term-memory', 'attention', 'seq2seq']","

Evidently you can only initialize it ($\vec{c_t}$) once

As I see it, $\vec{c_t}$ depends on $\vec{h}[1] \ldots \vec{h}[n]$ AND $\vec{s_{t-1}}$ (because $\alpha_i[t]$ depend on $\vec{s_{t-1}}$, and so is different for every calculation of a new $\vec{s_t}$).
And so $\vec{c_t}$ is different for every $\vec{s_t}$.
It can be used by the decoder by e.g., concatenating it with $\vec{s_{t-1}}$, or with $\vec{y_{t-1}}$, or by adding instead of concatenating, or ...
"
can I add to a language model a prompt with output example?,"
I want to finetune GPT2 to extract relevant data from a given text. So for (a trivial) example, given the text ""the car was manufactured in X, can reach Y km/h, and has Z horse powers"", my desired output would be manufacturer: X, max speed:Y, horsepowers:Z.
I don't have a lot of labeled data, so I thought it would be reasonable to take every training sample and add to it a prefix than contains an actual example. That is - Instead of providing the model with the text

INPUT: a well-known brand, the new X, can reach Y km/h, and has Z horsepower
OUTPUT:

and expect the model to understand how to fill in the details, I would provide a longer prompt that contains an actual example like

INPUT: the new BMW can reach up to 200 kmh. Even though the previous model disappointed the users, the brand-new one rocks a thousand engine horsepower
OUTPUT: Manufacturer: BMW, Max Speed: 200, Horse Powers: thousand
INPUT: though is mostly considered an outdated version of the cls200, the new Mercedes has the capabilities of reaching up to 100kmh in turns and 300kmh overall
OUTPUT:

Is this considered a common way of engineering the prompt? Do notice that the first provided example is the same for every training sample.
","['transformer', 'gpt', 'fine-tuning', 'prompt']","
Yes, it is possible to finetune GPT2 to extract relevant data from a given text. For your example, you could add a prompt that looks for a manufacturer, max speed, and horsepower, and then provide an output example that includes that information.
It is possible to add a prefix to every training sample that contains an actual example. This could help the model learn from data that has fewer labels.

INPUT: though is mostly considered an outdated version of the cls 200, the new Mercedes has the capabilities of reaching up to 100kmh in turns and 300kmh overall?
The OUTPUT would be the capabilities of the new Mercedes, including its max speed and ability to handle turns.

Yes, this is considered a common way of engineering the prompt.

Providing the same example for every training sample also help the
model learn from data that has fewer labels.

"
References on Theoretical Bandit Problem,"
I am going to start learning the bandit problem and algorithm, especially how to bound the regret. I found the book ``Bandit Algorithms'' but it is not easy to follow. It is based on advanced stochastic processes and measure theory in some cases. I am wondering if there are any lecture notes, or courses to start.
","['reference-request', 'multi-armed-bandits']","
I like this very detailed blog post. https://mpatacchiola.github.io/blog/2017/08/14/dissecting-reinforcement-learning-6.html. It's equivalent to a chapter or two of an introductory text, and covers a couple other adjacent but also very important topics.
"
What is the role of skip connections in U-Net?,"
I was able to find that the skip connections used in U-Net help to recover fine grained details in the prediction, however I do not understand what is meant by this. Besides, I was wondering what would happen if the U-Net does not include skip connections.
","['convolutional-neural-networks', 'u-net', 'skip-connections']",
Compare Stable-Baselines3 vs. Tianshou [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 7 months ago.







                        Improve this question
                    



What would you recommend between Stable-Baselines3 and Tianshou for applied research in Reinforcement Learning?
Can anyone provide a comparison of the strengths and weaknesses of each library? Or at least some criteria for choosing one over the other?
It seams like Tianshou is much more flexible and generalist but I have not been able to find a benchmark that clearly establishes this.
","['reinforcement-learning', 'deep-rl', 'stable-baselines']",
Why VQ-VAE instead of VAE?,"
From the paper on VQ-VAE, it said that the vector quantized variational autoencoder (VQ-VAE), differs from VAEs in two key ways:

the encoder network output discrete, rather than continuous, codecs
the prior is learnt rather than static

The paper further says, using the VQ method allows the model to circumvent issues of ""posterior collapse"".
So can anybody explain the main use of vector quantization in VQ-VAE, and how does it improve the VAE model in a mathematical or some kind of rigorous way so that I can understand it?
Also explain points 1, 2, and ""posterior collapse"" in detail with mathematical notions if possible.
","['autoencoders', 'generative-model', 'variational-autoencoder']",
Is robotics-assisted physical labor more or less useful for humans? How is this measured?,"
Is robotics-assisted physical labor more or less useful for humans?
Issues:

Building such robots may be difficult.
Human labor can be ecological.
Physical activity is beneficial to humans up to some point. But when is it not?

Also, how is ""usefulness"" measurable here?
",['robotics'],
Does more data increase training accuracy in neural networks?,"
I was wondering what is the performance benefit of feeding more data to a machine learning model like a neural network? Like I know one of the benefits is that it increases generalization - testing accuracy, but I was wondering if does it affect training accuracy of the model?
","['neural-networks', 'machine-learning', 'deep-learning', 'datasets']","
The question use specific terms in a vague way so let me set some very basic ground definitions first. It might sounds trivial but please bear with me cause it's easy to give reasonable answers that in reality make no sense.
data: any unprocessed fact, value, text, sound, or picture that is not being interpreted and analyzed. It can be real, i.e. gathered trough real observations/experiments or synthetic, i.e. artificially constructed based on some hand crafted distribution.
training/test data: different subset of data, the difference being that testing are not used to train or tune the model parameters and hyperparameters. Important to note is that we don't always have knowledge about the real distribution of the data, meaning that the distribution of our training data many times do not match the distribution of testing data.
accuracy: it's a specific metric used to evaluate only a specific subset of machine learning tasks among which (and mostly) classification. Is also a pretty unreliable metric in some circumstances like multi class classification or unbalanced datasets.
generalization: the ability of a model to perform well on unseen data. In principle it has nothing to do with the metrics used to evaluate a model, even though metrics scores are the only tool we have to assess it.
You ask if using more data can increase training accuracy and you already pointed out that using more data is meant to increase generalization, which you consider equal to testing accuracy. You`re right when you say that adding more data serve the purpose of increasing generalization, but as I wrote in the definition of generalization, in principle we can't always expect a linear relation between a model generalization and its metrics scores, and in fact adding data might as well decrease model generalization in some situations.
As a basic example let's consider an imbalanced dataset with 90% instances belonging to class A and 10% instances belonging to class B. A model will easily learn to overfitt the data predicting only class A, still reaching 90% training accuracy. In test phase we might even observe a similar score if the distribution match the 90/10 ratio of training data. To prevent overfitting and increase generalization, we add instances of class B to make the dataset balanced, i.e. 50% instances class A 50% instances class B. Suddenly the model works perfectly, and reach 100% training accuracy. We see tough that in test phase the accuracy drops to 40%. How come? If the test indeed had the same 90/10% distribution among classes A and B, training the model on a 50/50% distribution teach the model to overpredict class B, so the model is now predicting class B, which is why we added more data, but it's now predicting too many times class B, leading again to poor generalization.
Of course this is a toy scenario, but be aware that classic data augmentation and synthetic data always introduce the problem of introducing biases in the training distribution. Also, note that using a different metric like f score alongside accuracy would let you catch immediately if the model is generalizing more or if it's only an artifact produced by the accuracy metric (like the initial 90% score when totally overfitting).
"
PPO Agent - Unstable learning curve,"
I am using a PPO agent for a HVAC optimization problem. Trying to control a number of fans and heating coils in a network.
Currently, I am using 7 agents, each agent is responsible for one heating coil (3 agents) and for one fan (4 agents). My last run using 350 iterations, alpha = 0.003 and batch size  = 18.
to all my agents I am passing current values for a number of variables, past values (10 timesteps in the past), and future values (6 values in the future)
my reward function is looking like this:

and this gives me this terrible learning curve:

I know this is not much information, but does anyone with a bit more experience with PPO and RL have any ideas?
","['reinforcement-learning', 'proximal-policy-optimization']",
cannot get the results for each individual execution in hyperband tuner,"
I am trying to use keras tuner hyperband to select hyperparameters for my autoencoder model. Here's some pseudo code:
class AEHyperModel(kt.HyperModel):
   def __init_(self, input_shape):
      self.input_shape = input_shape

   def build(self, hp):
    
       # Input layer
       x = Input(shape=(input_shape,))
    
       # Encoder layer 1
       hp_units1 = hp.Choice('units1', values=[10, 15])       
       z = Dense(units=hp_units1, activation='relu')(x)
       
       # Encoder layer 2
       hp_units2 = hp.Choice('units2', values=[3, 4])
       z = Dense(units=hp_units2, activation='relu')(z)
       
       # Decoder
       y = Dense(units=hp_units1, activation='relu')(z)
       y = Dense(inputnodes,activation='linear')(y)

       # compile encoder
       autoencoder = Model(x,y)                                                       
       autoencoder.compile(optimizer='adam', loss='mean_squared_error')
    
       return autoencoder

   def fit(self, hp, autoencoder, *args, **kwargs):
       return autoencoder.fit(*args,
                            batch_size=hp.Choice('batch_size', [16]),
                            **kwargs)

and this is how I am performing my hyperparameter search:
   tuner= kt.Hyperband(AEHyperModel(input_shape),
                       objective='val_loss',
                       max_epochs=100,
                       factor=3,
                       directory=os.path.join(softwaredir, 'AEhypertuning'),
                       project_name='DRautoencoder2',
                       overwrite=True,
                       hyperband_iterations=5,
                       executions_per_trial=5)

   # patient early stopping and tensorboard callback
   callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20, restore_best_weights=True),
              keras.callbacks.TensorBoard('C:/Users/arr43/tmp/tb_logs')]

   # Search the best hyperparameters for the model
   tuner.search(Xz, Xz,
                       epochs=100,
                       shuffle=True,
                       validation_data=(valz, valz),
                       callbacks=[callbacks])

If I then do:
tuner.executions_per_trial
Out[12]: 5

which seems right. According to the documentation, oracle.trials[trialid] contains metrics, which contains a MetricsTracker object described as:
_Record of the values of multiple executions of all metrics.
It contains MetricHistory instances for the metrics.
Args:
metrics: List of strings of the names of the metrics._
Then to know the results of each exeuction per trial, I could go into metrics.metrics, and see a val_loss MetricsHistory object, described in the docu as:
_Record of multiple executions of a single metric.
It contains a collection of MetricObservation instances.
Args:
direction: String. The direction of the metric to optimize. The value
should be ""min"" or ""max""._
However,
tuner.oracle.trials['0000'].metrics.metrics['val_loss'].get_history()
Out[19]: [MetricObservation(value=[0.118283973634243], step=1)]

tuner.oracle.trials['0000'].metrics.metrics['val_loss'].get_statistics()
Out[20]: 
{'min': 0.118283973634243,
 'max': 0.118283973634243,
 'mean': 0.118283973634243,
 'median': 0.118283973634243,
 'var': 0.0,
 'std': 0.0}

suggesting only one execution per trial was performed?
Can I get any help on how to actually perform multiple executions per trial and being able to access the results of each execution, or at least the mean and std to know how stable the results are?
Thanks
","['deep-learning', 'tensorflow', 'keras', 'hyperparameter-optimization']",
Is my neural network working?,"
I recently just finished programming a neural network in c#, and it seems like it's working. My question is if I'm doing it right. It's a very confusing process so I will explain.
Basically every neuron in the network has a bias (except the first layer) and fully connected weights to the next. every neuron also has a double[] desiredBias which holds all the desired changes to the neuron for every iteration in a given mini batch. The same thing for the weights. So I start from the last layer, and from there I calculate the ""sensitivity"" you might say between the cost of a given neuron on that last layer and the bias, and i take note of it (put it into changes). Then for every weight connected to the layer behind, I also calculate the sensitivity for that one. And I keep doing that until I get to the second to first layer. And to take note of the desiredChanges for neurons not in the last layer, I use this array called ""changes"". For the first layer it's basically just a list of all the derivatives of the activations (z) with respect to the actual activations (sigmoid(z)) respectively. After I shift to the layer behind, and behind, every time I do that, I create a new changes array, which is the size of the layer behind, and for each item in the previous changes array, I take the sum of all of them multiplied by the partial derivative of that same z-activation with respect to the previous layer's z activation. I do this for every neuron in the previous layer.
after all of that I end up with for every neuron a list of desired changes to it's bias, and a list of lists which contain desired changes to the weights of each neuron. Then for each neuron (except for the first of course) I average together all of the biases and SUBTRACT it from the neuron's bias. I do the same thing for the weights.
Is this the correct process to go through for exactly one batch of training data?
here's my code:
using System;

namespace NeuralNetwork {
    
    public class Functions {
        
        // Activation Functions
        
        public static double sigmoid(double x) {
            return 1 / (1 + Math.Pow(Math.E, -x));
        }
        public static double ReLU(double x) {
            return Math.Max(0, x);
        }
        public static double Tanh(double x) {
            return Math.Tanh(x);
        }
        public static double Linear(double x) {
            return x;
        }
        
        // Derivatives
        
        public static double sigmoidDerivative(double x) {
            return sigmoid(x) * (1 - sigmoid(x));
        }
        public static double ReLUDerivative(double x) {
            return (x > 0) ? 1 : 0;
        }
        public static double TanhDerivative(double x) {
            return 1 - (Math.Pow(Tanh(x) * Tanh(x), 2));
        }
        public static double LinearDerivative(double x) {
            return 1;
        }
        
        public enum ActFunction {
            Sigmoid,
            ReLU,
            Tanh,
            Linear
        }
        
        // Loss Functions
        
        public static double MSE(double[] outputs, double[] desired) {
            double sum = 0;
            for (int i = 0; i < outputs.Length; i++) {
                sum += Math.Pow((outputs[i] - desired[i]), 2);
            }
            return sum / outputs.Length;
        }
        
        // Derivatives
        
        public static double[] MSEDerivative(double[] outputs, double[] desired) {
            double[] derivatives = new double[outputs.Length];
            for (int i = 0; i < outputs.Length; i++) {
                derivatives[i] = 2 * (outputs[i] - desired[i]);
            }
            return derivatives;
        }
        
        public enum LossFunction {
            MSE
        }
        
    }
    
    public class Neuron {
    
        public double activation = 0;
        public Functions.ActFunction activationFunction = Functions.ActFunction.Linear;
        
        public double bias;
        public double[] weights = new double[] {};
        public int x;
        public int y;
        public Neuron[][] network;
        
        public double[] desiredBias;
        public double[][] desiredWeights;
        
        public void fire() {
            for (int i = 0; i < network[x + 1].Length; i++) {
                Neuron next = network[x + 1][i];
                next.activation += activation * weights[i];
            }
        }
    
        public void activate() {
            double n_activation = activation + bias;
            
            switch(activationFunction) {
                case Functions.ActFunction.Sigmoid:
                    activation = Functions.sigmoid(n_activation);
                    break;
                case Functions.ActFunction.ReLU:
                    activation = Functions.ReLU(n_activation);
                    break;
                case Functions.ActFunction.Tanh:
                    activation = Functions.Tanh(n_activation);
                    break;
                case Functions.ActFunction.Linear:
                    activation = Functions.Linear(n_activation);
                    break;
            }
        }
        
    }
    
    public class Network {
    
        public Neuron[][] network;
        public double learningRate;
        public int iterations;
        public Functions.LossFunction lossFunction;
        public int trainingLength;
    
        public Network(int[] size, double _learningRate, int _iterations, int _trainingLength, Functions.LossFunction _lossFunction) {
            learningRate = _learningRate;
            iterations = _iterations;
            trainingLength = _trainingLength;
            lossFunction = _lossFunction;
            network = new Neuron[size.Length][];
            for (int i = 0; i < size.Length; i++) {
                network[i] = new Neuron[size[i]];
            }
            for (int i = 0; i < size.Length; i++) { 
                for (int j = 0; j < size[i]; j++) { 
                    network[i][j] = new Neuron();
                }
            }
            for (int i = 0; i < network.Length - 1; i++) {
                for (int j = 0; j < network[i].Length; j++) { 
                    Neuron neuron = network[i][j];
                    neuron.weights = new double[network[i + 1].Length];
                    neuron.desiredWeights = new double[neuron.weights.Length][]; // i would have put iterations right here but the compiler thinks otherwise
                    for (int k = 0; k < neuron.desiredWeights.Length; k++) {
                        neuron.desiredWeights[k] = new double[trainingLength/iterations];
                    }
                }
            }
            for (int i = 1; i < network.Length; i++) {
                for (int j = 0; j < network[i].Length; j++) { 
                    Neuron neuron = network[i][j];
                    neuron.desiredBias = new double[trainingLength/iterations];
                }
            }
        }
    
        public void init() {
            Random rand = new Random();
            for (int i = 0; i < network.Length; i++) {
                for (int j = 0; j < network[i].Length; j++) {
                    Neuron neuron = network[i][j];
                    neuron.x = i;
                    neuron.y = j;
                    neuron.network = network;
                    
                    if (i != 0) {
                        neuron.bias = (rand.NextDouble() * 2) - 1;
                    }
                    
                    if (i != network.Length - 1) {
                        for (int k = 0; k < network[neuron.x + 1].Length; k++) {
                            neuron.weights[k] = (rand.NextDouble() * 20) - 1;
                        }
                    }
                }
            }
        }
        
        public void info() {
            Console.WriteLine(""Here are the statistics for the network:\n"");
            
            for (int i = 0; i < network.Length; i++) {
                for (int j = 0; j < network[i].Length; j++) {
                    Neuron neuron = network[i][j];
                    Console.WriteLine(""Neuron "" + j + "" of layer "" + i);
                    //Console.WriteLine(""Activation: "" + neuron.activation);
                    Console.WriteLine(""Bias: "" + neuron.bias);
                    Console.WriteLine(""X and Y: "" + neuron.x + "", "" + neuron.y);
                    string weights = ""Weights: "";
                    foreach (double weight in neuron.weights) {
                        weights += weight + "" "";
                    }
                    Console.WriteLine(weights);
                    Console.WriteLine();
                }
            }
            
            Console.WriteLine();
        }
        
        public double[] run(double[] inputs) {
            // initialize inputs
            for (int i = 0; i < network[0].Length; i++) {
                network[0][i].activation = inputs[i];
            }
            
            for (int i = 0; i < network.Length - 1; i++) {
                for (int j = 0; j < network[i].Length; j++) {
                    Neuron neuron = network[i][j];
                    neuron.fire();
                }
                for (int j = 0; j < network[i + 1].Length; j++) {
                    Neuron neuron = network[i + 1][j];
                    neuron.activate();
                }
            }
            
            // get outputs
            
            Neuron[] outputs = network[network.Length - 1];
            double[] activations = new double[outputs.Length];
            
            for (int i = 0; i < outputs.Length; i++) {
                activations[i] = outputs[i].activation;
            }
            
            return activations;
        }
        
        public double iteration() {
            Random rand = new Random();
            
            int iterationLength = trainingLength/iterations;
            double[] costs = new double[iterationLength];
            
            // training
            
            for (int iter = 0; iter < iterationLength; iter++) {
                double[] inputs = { (rand.NextDouble() ), ( rand.NextDouble() )};
                double[] answers = { inputs[0] + inputs[1] };
                double[] outputs = run(inputs);
                
                for (int i = 0; i < network.Length; i++) {
                    for (int j = 0; j < network[i].Length; j++) {
                        Neuron neuron = network[i][j];
                        neuron.desiredBias = new double[iterationLength];
                        if (i == network.Length - 1) continue;
                        neuron.desiredWeights = new double[network[i + 1].Length][];
                        for (int k = 0; k < neuron.desiredWeights.Length; k++) {
                            neuron.desiredWeights[k] = new double[iterationLength];
                        }
                    }
                }
                
                double cost = 0;
                switch(lossFunction) {
                    case Functions.LossFunction.MSE:
                        cost = Functions.MSE(outputs, answers);
                        break;
                }
                
                costs[iter] = cost;
                
                double[] changes = new double[network[network.Length - 1].Length];
                switch(lossFunction) {
                    case Functions.LossFunction.MSE:
                        changes = Functions.MSEDerivative(outputs, answers); 
                        break;
                }
                
                for (int i = 0; i < changes.Length; i++) {
                    Neuron neuron = network[network.Length - 1][i];
                    double act = neuron.activation;
                    switch(neuron.activationFunction) {
                        case Functions.ActFunction.Sigmoid:
                            changes[i] *= Functions.sigmoidDerivative(Math.Log(act / (1 - act)));
                            break;
                        case Functions.ActFunction.Linear:
                            changes[i] *= Functions.LinearDerivative(act);
                            break;
                    }
                }
                
                for (int i = network.Length - 1; i > 0; i--) {
                    for (int j = 0; j < network[i].Length; j++) {
                        Neuron bNeuron = network[i][j];
                        bNeuron.desiredBias[iter] += changes[j]; // bias partial derivative is always one
                        for (int k = 0; k < network[i - 1].Length; k++) {
                            Neuron aNeuron = network[i - 1][k];
                            aNeuron.desiredWeights[bNeuron.y][iter] += aNeuron.weights[bNeuron.y] * changes[j]; // chain rule
                        }
                    }
                    
                    double[] holderChanges = changes;
                    changes = new double[network[i - 1].Length];
                    for (int j = 0; j < changes.Length; j++) {
                        for (int k = 0; k < holderChanges.Length; k++) {
                            Neuron aNeuron = network[i - 1][j];
                            Neuron bNeuron = network[i][k];
                            switch(aNeuron.activationFunction) {
                                case Functions.ActFunction.Sigmoid:
                                    double activation = aNeuron.activation;
                                    changes[j] += Functions.sigmoidDerivative(Math.Log(activation / 1 - activation)) * aNeuron.weights[bNeuron.y] * holderChanges[k];
                                    break;
                                case Functions.ActFunction.Linear:
                                    activation = aNeuron.activation;
                                    changes[j] += Functions.LinearDerivative(activation) * aNeuron.weights[bNeuron.y] * holderChanges[k];
                                    break;
                            }
                        }
                    }
                }
                
                // reset activations 
            
                for (int i = 0; i < network.Length; i++) {
                    for (int j = 0; j < network[i].Length; j++) {
                        Neuron neuron = network[i][j];
                        neuron.activation = 0;
                    }
                }
            }
            
            // after everything we now actually add the desired changes
            
            // the weights
            for (int i = 0; i < network.Length - 1; i++) {
                for (int j = 0; j < network[i].Length; j++) {
                    Neuron neuron = network[i][j];
                    for (int k = 0; k < neuron.desiredWeights.Length; k++) {
                        double weightSum = 0;
                        foreach (double weight in neuron.desiredWeights[k]) {
                            weightSum += weight;
                        }
                        weightSum /= iterationLength;
                        neuron.weights[k] -= learningRate * weightSum;
                    }
                }
            }
            
            // the biases
            for (int i = 1; i < network.Length; i++) {
                for (int j = 0; j < network[i].Length; j++) {
                    Neuron neuron = network[i][j];
                    double biasSum = 0;
                    foreach (double bias in neuron.desiredBias) {
                        biasSum += bias;
                    }
                    biasSum /= iterationLength;
                    neuron.bias -= learningRate * biasSum;
                }
            }
            
            double netcost = 0;
            foreach (double cost in costs) {
                netcost += cost;
            }
            
            return netcost/costs.Length;
        }
        
        public void epoch(bool showInfo) {
            Console.WriteLine(""Starting training...\n"");
            
            for (int iter = 0; iter < iterations; iter++) {
                double cost = iteration();
                if (showInfo) info();
                
                Console.WriteLine(""Cost for iteration "" + iter + "": "" + cost + ""\n"");
                
                // reset desired
                
                for (int k = 0; k < network.Length - 1; k++) {
                    for (int l = 0; l < network[k].Length; l++) {
                        Neuron neuron = network[k][l];
                        for (int i = 0; i < neuron.desiredWeights.Length; i++) {
                            for (int j = 0; j < neuron.desiredWeights[i].Length; j++) {
                                neuron.desiredWeights[i][j] = 0;
                            }
                        }
                    }
                }
                for (int k = 1; k < network.Length; k++) {
                    for (int l = 0; l < network[k].Length; l++) {
                        Neuron neuron = network[k][l];
                        for (int i = 0; i < neuron.desiredBias.Length; i++) {
                            neuron.desiredBias[i] = 0;
                        }
                    }
                }
            }
            
            Console.WriteLine(""Training finished!\n"");
        }
        
        public void prompt() {
            while(true) {
                
                Console.WriteLine(""What would you like to do?"");
                Console.WriteLine(""Train, info, run or end?"");
                
                string answer = Console.ReadLine();
                
                if (answer.ToLower() == ""end"") {
                    Console.WriteLine(""..."");
                    break;
                }
                
                switch(answer.ToLower()) {
                    case ""info"":
                        Console.WriteLine();
                        info();
                        Console.WriteLine();
                        break;
                    case ""run"":
                        double[] runanswer = new double[network[0].Length];
                        while (true) {
                            try {
                                for (int i = 0; i < network[0].Length; i++) {
                                    Console.WriteLine(""Enter input "" + i);
                                    runanswer[i] = Convert.ToDouble(Console.ReadLine());
                                }
                                
                                Console.WriteLine(""Running...\n"");
                                
                                double[] outputs = run(runanswer);
                                string _string = """";
                                
                                foreach (double output in outputs) {
                                    _string += output;
                                }
                                
                                Console.WriteLine(""Here were the outputs:"");
                                Console.WriteLine(_string + ""\n"");
                                break;
                            } catch (FormatException exception) {
                                Console.WriteLine(""Only numbers!"");
                                continue;
                            }
                        }
                        break;
                    case ""train"":
                        int trainanswer = 0;
                        int trainanswer2 = 0;
                        double trainanswer3 = 0;
                        while (true) {
                            try {
                                Console.WriteLine(""Enter epoch length:"");
                                trainanswer = Convert.ToInt32(Console.ReadLine());
                                trainingLength = trainanswer;
                                
                                Console.WriteLine(""\nEnter batch size:"");
                                trainanswer2 = Convert.ToInt32(Console.ReadLine());
                                iterations = trainanswer2;
                                
                                Console.WriteLine(""\nEnter learning rate:"");
                                trainanswer3 = Convert.ToDouble(Console.ReadLine());
                                learningRate = trainanswer3;
                                
                                for (int i = 0; i < trainanswer2; i++) {
                                    double cost = iteration();
                                    Console.WriteLine(""The cost was: "" + cost + ""\n"");
                                }
                                break;
                            } catch (FormatException exception) {
                                Console.WriteLine(""Only numbers!"");
                                continue;
                            }
                        }
                        break;
                    default:
                        Console.WriteLine(""Invalid command!\n"");
                        break;
                }
            }
        }
    
    }
    
    public class Run {
        public static void Main(string[] args) {
            
            Network brain = new Network(new int[] { 2, 1 }, .01, 50, 50, Functions.LossFunction.MSE);
            brain.init();
            
            int i = 7;
            while (i > 0) {
                brain.epoch(false);
                i--;
            }
            
            brain.prompt();
            
        }
    }
    
}

*** to clarify if you don't understand my question, I'm just asking if i'm implementing backpropagation correctly
","['backpropagation', 'definitions']",
Using a pre-trained model to generate labels to data to then train a model on,"
I'm trying to set up a pipeline for my ML models to automatically re-train themselves whenever concept drift occurs to recalibrate to the new output distributions. However, I can't get ground-truth from my data without manual labeling, and I want an automated MLOps pipeline, so that no human-in-the-loop labeling is required.
I was noticing that using zero-shot learning using BART was actually giving pretty impressive results with my label set on sample data I want to classify on.
Suppose I were to use BART to manually label data, say, if it scored my label with a score of 0.95 or better, and then just used that labeled data to train my models.
Is this ""cheating""? Am I trying to cut a corner here that can't be cut? Will this create a meaningful model for my text data, or just cause my trained model to try to become a crude version of the BART model that provided the labels in the first place, and therefore just be worse than strictly using the BART model?
","['text-classification', 'data-labelling', 'pretrained-models', 'zero-shot-learning', 'automated-machine-learning']","
If using BART is already giving you good results, why do you need a new model?
Not a rhetorical question. You might have good reasons for that. Training a model with less parameters optimized only on few classes is an example. But if you can't came up with good reasons then the answer is pretty straight forward: just use BART. Reason being that a model trained on another model annotations without any human in the loop will never perform better than the model used to automatically train the data. The first reason being error propagation, like in any other area of empirical science. And on a more technical level you can't really use tricks like using only predictions with a high confidence scores, cause deep learning models are basically never calibrated, hence you can't trust them and a human would still have to check them manually.
"
Is there a difference between 'inference step time' and 'inference time'?,"
I was looking at the table presented in Keras Applications and there is a column that says Time (ms) per inference step (CPU) and another for GPU. I did a quick research on the internet and I came across some explanations of what inference time is as we can see 1-here  and also 2-here.
1-The inference time is how long is takes for a forward propagation. To get the number of Frames per Second, we divide 1/inference time.


2-In deep learning, inference time is the amount of time it takes for a machine learning model to process new data and make a prediction.

But the explanations are talking about inference time and not Time per inference step, so I was wondering if there is some difference to it? Is this time related to steps_per_epoch or validation_steps from fit function?
They also mention these two lines in the Keras website :
Time per inference step is the average of 30 batches and 10 repetitions.
Batch size: 32

Does this mean that the Time per inference step is the average time it takes the model to make predictions for each batch of 32 samples as opposed to just one sample?
Thanks in advance!
","['machine-learning', 'deep-learning']",
Classifier performance if data are deterministic,"
so let us imagine one has a classification problem at hand, say objects with $n$ numeric features, to be classified as belonging to two classes ${0,1}$.
Data could look like, for $n=3$
x_1 x_2 x_3 class
1.1 2.0 3.2 0
1.2 2.1 0.7 1
4.0 2.1 3.2 0

where the $x_i$ stand for numerical values for a given property.
This dataset could be already fed to any classifier, let us say a Random Forest one.
Now, let us imagine a human expert in this domain states that the classification problem is actually trivial.
In fact there exist a scalar function, say $\phi(x_1,x_2, x_3)$ such that when
\begin{cases} \phi(x_1,x_2, x_3) < 3,\,\, \mbox{ instance belongs to class} \, 0\\ \phi(x_1,x_2, x_3) \geq 3,\,\, \mbox{ instance belongs to class} \, 1 \end{cases}
A person explicitly knowing the function $\phi$ would achieve perfect accuracy.
The question then is, are there any formal results clarifying (e.g.bounding) the accuracy achievable under conditions on the function $\phi$ (linearity, smoothness, etc.)?
For example, if $\Phi$ is linear, is there any result stating a classifier can achieve perfect accuracy as the number of datapoints goes to $\infty$?
And what could be said if $\phi$ is nonlinear?
I am not an expert in this field at all as it can be easily grasped. I am looking for pointers on relevant literature.
Another point of view on my question is, if the underlying data are somehow deterministic, is it worth at all spending time in trying to make sense of the classification problem using human expertise, or is it guaranteed that under some condition a  classifier will be anyhow be able to come up with a ""good"" answer?
EDIT Yet another more practical point, clarifying the real life problem I am trying to understand.
Often one reads about the importance of data engineering, specifically coming up with additional, engineered features using human expertise. But what is the usefulness of this, or the necessity for that matters? Can there be that a classifier performs badly on the original dataset, and then somebody defined a ""clever"" new feature, function of other original feature values, and the performance grows significantly?
At the end of the day, new engineered features do not contain original, new information. then one could say, there is no need to waste time in looking for engineered features, just let the classifier go on the original dataset.
The   Data-processing inequality springs to mind, but then, can it be that the information present in the original dataset can be extracted only if feature engineering is performed? If that were the case, what is known about this interesting phenomenon?
","['classification', 'performance', 'information-theory']",
"How Could I fine tune bert-large-uncased-whole-word-masking or albert-xxlarge-v2 for question answering in Business, Financial domain for report?","
I used mfeb/albert-xxlarge-v2-squad2,bert-large-uncased-whole-word-masking-finetuned-squad to answer the report question , but just get a little correct answer? Could I build a dataset like squad format to fine-tune?It is need to collect and label data manually?Thank you so much.
","['machine-learning', 'natural-language-processing', 'bert']",
What is the best distance measure between policies that are not probability distributions?,"
This question asks if there is a way to measure distance between policies that are in fact probability distributions.
In the case of continuous control with deterministic policies where they take a state as input and return an action vector, what would be the best method to measure how close two policies are from each other?
A naive approach that came to my mind would be to:

Run both policies A and B to produce a trajectory each and record all states visited.
For each state encountered by policy A, ask policy B which action it would take (and vice-versa). Hence we would have, for every state encountered, both A and B action vectors.
For each state, compare action vectors of A and B by using a common distance (Euclidean distance?)
Take the average (maybe maximum) of those distances.

Does it make any sense from a theoretical point of view?
","['reinforcement-learning', 'policies', 'metric', 'kl-divergence']","
One way to measure the distance between two policies is to use the Kullback-Leibler (KL) divergence. This measures the amount of information lost when one policy is used to approximate the other.
Another way to measure is to use the Wasserstein metric. This measures the amount of ""work"" or ""energy"" required to transform one policy into the other.
A third way is to use the Mutual Information (MI) metric. This measures the amount of information shared between the two policies.

yes! Euclidean distance would way to measure the distance be between the two policies action vectors

It does, sort of. The idea is that you have a reward function that encodes the task you want to solve. The agent is trying to find an optimal policy that maximizes this reward function. In order to do this, the agent must learn the reward function.
One way to think about this is that the agent is trying to find a mapping from states to actions that will maximise the reward. This is the same as finding a mapping from states to a goal state that maximises the reward. So, if you have two different agents, they will both be trying to find a mapping from states to actions that maximises the reward. However, they will likely find different mappings, because they have different starting
"
"If GPT-3 is trained on predicting the next token, how is it able to take commands?","
From my understanding, GPT-3 is trained on predicting the next token from a sequence of tokens. Given this, how is it able to take commands? For instance, in this example input, wouldn't the statistically most likely prediction be to insert a period and end the sentence?
Input: write me a beautiful sentence

Output: I cannot put into words how much I love you, so I'll just say it's infinite.

","['natural-language-processing', 'transformer', 'gpt-3']","
No, it's actually a very precise guess determined by relatively complex statistical equations.
GPT-3 is able to take commands because it has been trained on a dataset of commands. This means that it is able to recognize patterns in sequences of tokens that indicate a command. In the example input, the most likely prediction is to insert a period and end the sentence because this is what typically happens at the end of a sentence. However, GPT-3 is able to take commands because it is able to recognize patterns that indicate a different kind of sequence, such as a command.
"
Is there a best practice for creating multiple convolutional layers from small image inputs?,"
With all the work being done on larger and larger images, I'd like to ask if a best practice(s) has arisen for allowing multiple convolutional layers on small image inputs?
For instance, in my case I have a 7x7x4 input that I convolve (coming from a small visual piece of a RL environment). Obviously if I want to allow a few layers with increasing depth/channels to allow the network to learn abstractions, many kernel/padding combinations will swiftly reduce the first two dimensions to a point where arguably convolutional layers don't make sense.
For my own case, I've managed to get 3 seemingly reasonable layers by playing with kernel size, stride, and padding, and while it ""works"" for my use-case, it feels outright arbitrary:
# Values for the first conv layer
self.ksize_1 = 3
self.stride_1 = 1
self.padding_1 = 1
# retains the input_dimz x input_dimz shape

# Values for the second conv layer
self.ksize_2 = 4
self.stride_2 = 1
self.padding_2 = 1
# roughly makes dimension even (looks like input_dimz - 1)

# Values for the third conv layer
self.ksize_3 = 3
self.stride_3 = 1
self.padding_3 = 0
# outputs dimensions roughly 2 less in width by height from above
# should be (input_dimz - 3) x (input_dimz - 3)

# Now our actual conv layers
self.conv1 = nn.Conv2d(self.input_chan, self.conv_dim, self.ksize_1,
                       self.stride_1, self.padding_1, padding_mode=self.padding_mode)
self.conv2 = nn.Conv2d(self.conv_dim, self.conv_dim * 2, self.ksize_2,
                       self.stride_2, self.padding_2, padding_mode=self.padding_mode)
self.conv3 = nn.Conv2d(self.conv_dim * 2, self.conv_dim * 3, self.ksize_3,
                       self.stride_3, self.padding_3, padding_mode=self.padding_mode)

(where conv_dim is essentially a hyperparam to tweak the channel depth)
Are small dimensional cases like this still pretty arbitrary and the above reasonable, or has a methodological approach arisen that can be followed?
","['convolutional-neural-networks', 'computer-vision', 'convolution', 'convolutional-layers']","
A bit late to the party but the only thing I could come up with is VGG-oriented architectures. What I mean by that is use blocks of convolutional layers that leave the input the same by for instance using stride one and same padding. Finish the previous block with a convolutional layer that has stride 2 or valid padding and then create a new block that utilizes a zero padding layer.
For tensorflow you would do something like this:
   inputs = keras.Input(shape=input_shape)
   x = layers.Conv2D(filters=num_filters, kernel_size=kern_size, padding=""same"", strides=1,activation='relu')(inputs)
   x = layers.Conv2D(filters=num_filters, kernel_size=kern_size, padding=""same"", strides=1,activation='relu')(x)
   ...
   x = layers.Conv2D(filters=num_filters, kernel_size=kern_size, padding=""same"", strides=2,activation='relu')(x)
   # new block
   x = layers.ZeroPadding2D(padding=((padding_num)(x)
   ...

This way you can have several layers that leave the input size untouched until you want to go further. Ofcourse instead of changing the stride, you could add a MaxPooling layer etc.
My answer is based on subjective empirical evidence with small images and with trying to expand networks without reducing the size of the initial input to too small dimensions.
"
What is an appropriate size for a latent space of (variational) autoencoders and how it varies with the features of the images?,"
I am training an autoencoder and a variational autoencoder using satellite and streetview images. I have tested my program on standard datasets such as MNIST and CelebA. It seems that the latent space dimension needed for those applications are fairly small. For example, MNIST is 28x28x1 and CelebA is 64x64x3 and for both a latent space bottleneck of 50 would be sufficient to observe reasonably reconstructed image.
But for the autoencoder I am constructing, I needed a dimension of ~20000 in order to see features. The variational autoencoder is not working, and I only see a few blobs of fuzzy color.
For those who have experience with training the autoencoders with your own images, what could be the problem? Are certain features easier to compress than others? or does it look like a sample size problem? (i have 20000 images in training) Is there any rule of thumb for the the factor of compression? Thanks!
Here is a best example of what I have got with my VAE. I am using a ResNeXt architecture and the image dimension is 64x64x3, the latent space dimension is very large (18432).


","['computer-vision', 'autoencoders', 'variational-autoencoder']","
From my experience (on MNIST digits), even when using a latent space of only $10$ nodes, the decoded reconstruction was pretty much ok. perhaps the architecture itself lacks the capabilities of encoding/decoding properly. Check out this summary and see if you can improve your results using a similar approach.
"
"Why is the greedy heuristic admissible and consistent for food at corners problem, but not for food anywhere problem?","
UCBerkley has a great Intro to AI course (CS188) where you can practice coding up search algorithms. One of the exercises (question 6), asks to generate a heuristic that will have Pacman find all 4 corners of the grid.
My implementation used a greedy approach that found the closest corner by Manhattan distance, moved Pacman to that corner, and then repeated the process. This is a relaxation of the original problem by removing all the walls. Here is my code below:
def cornersHeuristic(state: Any, problem: CornersProblem):

    corners = list(problem.corners) # These are the corner coordinates
    
    position, corners_status = state
    
    bools = [x==0 for x in corners_status]
    unexplored_corners = []
    
    for index, bool_val in enumerate((bools)):
        if bool_val == True:
            unexplored_corners.append(corners[index])
    
    current_sum = 0    
    while unexplored_corners != []:
        
        dists_temp = []
    
        for corner in unexplored_corners:
        
            d = util.manhattanDistance(position, corner)
            dists_temp.append(d)
        
        min_dist = min(dists_temp)
        current_sum = current_sum + min_dist
        position = unexplored_corners.pop(dists_temp.index(min_dist))

    return current_sum 

This implementation ended up working correctly and the autograder recognized it as consistent.
However, upon closer inspection, I think the autograder made a mistake. Here is an example of where my greedy heuristic would find a path longer than the theoretical shortest path available for Pacman:
The blue line is the path my heuristic would take and the green line is the correct shortest path.

What am I missing here? Why is this heuristic consistent? And why is this greedy heuristic consistent for the corners problem, but not for the ""food anywhere"" problem? Any visualization showing a simple scenario where it would fail for the ""food anywhere"" problem would be greatly appreciated since Im a visual learner.
","['python', 'search', 'heuristics', 'path-finding', 'goal-based-agents']",
How do Transformer decoders handle arbitrary length input?,"
I am working through a Tensorflow Neural Machine Translation tutorial (https://www.tensorflow.org/text/tutorials/transformer) and am confused about how the decoder handles inputs when making inferences after it has been trained.
In the section where we create a class for translating a sentence (https://www.tensorflow.org/text/tutorials/transformer#run_inference) it appears that we feed the decoder an array populated with the START token only, then append the last element from the predicted sequence it makes to the growing translated sentence. This does not make sense to me as we trained the transformer on a fixed length sequence that was padded to length MAX_TOKENS=128, so I can't figure out why it would be able to accept input of an arbitrary length tensor.
Here is the code for inference in question that confuses me:
    # As the output language is English, initialize the output with the
    # English `[START]` token.
    start_end = self.tokenizers.en.tokenize([''])[0]
    start = start_end[0][tf.newaxis]
    end = start_end[1][tf.newaxis]

    # `tf.TensorArray` is required here (instead of a Python list), so that the
    # dynamic-loop can be traced by `tf.function`.
    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
    output_array = output_array.write(0, start)

    for i in tf.range(max_length):
      output = tf.transpose(output_array.stack())
      predictions, _ = self.transformer([encoder_input, output], training=False)

      # Select the last token from the `seq_len` dimension.
      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

      predicted_id = tf.argmax(predictions, axis=-1)

      # Concatenate the `predicted_id` to the output which is given to the
      # decoder as its input.
      output_array = output_array.write(i+1, predicted_id[0])

      if predicted_id == end:
        break


self.transformer takes in the ""encoder_input"" which is fed to the encoder and ""output"" which is fed into the decoder, where we then take the last prediction in the sequence returned from the decoder, append it to ""output"", then repeat the process. But I don't understand how the decoder can process a tensor of length 1, then 2, then 3, and so on if it is always expecting a tensor of length 128.
It seems like the input to the decoder should be padded to 128 tokens and then the prediction at the i-th index should be appended to the output (rather than always the last element in the predicted sequence) before repeating.
Is the tutorial mistaken in the implementation on of how this transformer preforms inference? Or am I missing something that Tensorflow does behind the scenes that allow this to work?
Note, I realize the title to this question is similar to How can Transformers handle arbitrary length input? but the answer given appears to verify that the input to the decoder should be fixed length and padded, which contradicts how inference is done in the tutorial I am referencing. So I hope this question isn't considered a duplicate but I am asking a similar question in the context of the mentioned tutorial.
Thank you for any insight someone might be able to provide!
","['natural-language-processing', 'transformer', 'machine-translation']","
At training time, the decoder uses a so-called triangular mask that prevents the self-attention to ""look"" at positions that are yet to be decoded. Because of that, nothing prevents the decoder to use shorter sequences than those it was trained with. The padding would be masked out anyway.
A more interesting question is whether the decoder can work with longer sequences than it was trained for. The answer is: it depends. In the original Attention is all you need paper, analytically defined position encoding was used that should in theory generalize beyond the lengths it was trained for. This is also used in the tutorial the question refers to. However, in practice, learned position embeddings are used (e.g., BERT uses a fixed length of 512 tokens) that seem to deliver better results in practice. In such a case, the decoder simply cannot generalize beyond the maximum length, simply because it does not have embeddings for those positions.
"
How does Supervised learning models handle time-varying data,"
I need to train a supervised learning model which would take some input which differs in its output relating to time. to better understand my question I would give a simple binary classification, the model would receive an object that changes according to time so on time period t0 the output of the model related to that input differs from the output of the time period t1. I might be describing something either very easy and I missed it or something that doesn't yet exist.
EDIT: I am preparing a model that classifies fruit based on its appearance to either be consumable or not. The only different thing in my work  is that in specific time periods what was considered not consumable should be considered consumable so a picture of a fruit in a certain state could be classified both ways in general but in a specific time period it has only one classification.
","['classification', 'supervised-learning', 'binary-classification']","
Supervised learning models can handle time-varying data by using a variety of techniques. One common approach is to use a Sliding Window, where the model is trained on data from a fixed period and then applied to data from a different period. Another approach is to use a Recurrent Neural Network, which can learn temporal dependencies in data. If the input data changes over time, the model can learn to adapt and update its predictions accordingly.
It sounds like you are looking for a model that can learn to classify the fruit based on its appearance, and then can also learn to update its classification based on the specific period. This type of model does not currently exist, but it could be developed with enough data and training.You will need to design your own AI algorithm to accomplish this task.
"
Training loss decreases very fast after few epochs,"
I am implementing an ANN whose training loss is in Figure:

As you can see training loss decreases very fast and it is approximately 3.2 at epochs 2, 3, ..., 8, ... 10, and so on. (batch learning)
The diagram is similar even though I increases MAX_EPOCHS to 2000.
Is it correct?
Note that my training set is a toy:
X = np.asmatrix([
        [2, 5, 3.2, 7.2],
        [1, 0.8, 1, 8.5],
        [2, 7, 3, 1.4]
    ])

targets = np.asarray([0,2,1])
targets = np.asmatrix(np.eye(np.max(targets) + 1)[targets])  # 1-hot

And I am using Gradient Descent with momentum
","['neural-networks', 'machine-learning', 'training', 'loss']",
How to remove boilerplate (or extract main content) from web pages?,"
Data: Raw source code of a website and the final cleaned main content I want to extract from the raw source code. The source code comes from different websites with different layouts and code structures. They all belong thematically to the same domain.
Problem: I want to remove everything but the main content from the source code, including all boilerplate. I have looked at the work of Web2Text and Boilernet so far. While Web2Text used CNNs and Boilernet used bidirectional LSTMs, I want to use a transformer-based approach. Can anyone explain to me how to prepare the data or how to train a model that will receive the source code as input and output the text? Can anyone point me in the right direction for boilerplate removal resources based on transformers?
Thank you in advance!
","['convolutional-neural-networks', 'transformer', 'text-classification', 'machine-translation', 'bidirectional-lstm']",
Why is the embedding of a task using Task2Vec not depend on the model?,"
I saw this in the Task2Vec paper:

TASK2VEC depends solely on the task, and ignores
interactions with the model which may however play an
important role. To address this, we learn a joint task and
model embedding, called MODEL2VEC, in such a way that
models whose embeddings are close to a task exhibit good
perfmormance on the task.

Does someone know why using the Fisher Information Matrix (FIM) makes the embedding representation of a task indepedent of the model (and hence their need for a model2vec)?

cross post:

gitissues in the task2vec repo: https://github.com/awslabs/aws-cv-task2vec/issues/12

reddit https://www.reddit.com/r/MachineLearning/comments/xu9was/why_is_the_embedding_of_a_task_using_task2vec_not/

https://www.reddit.com/r/learnmachinelearning/comments/xu9ycc/why_is_the_embedding_of_a_task_using_task2vec_not/

quora: https://www.quora.com/unanswered/Why-is-the-embedding-of-a-task-using-Task2Vec-not-dependent-on-the-model


","['neural-networks', 'machine-learning', 'deep-learning', 'transfer-learning', 'meta-learning']",
How can I use larger input images when using a pre-trained CNN without resizing?,"
I have a ResNet18 model trained on the Places365 image dataset, and I'd like to use this pre-trained model to expedite the training needed to identify distressed houses. My dataset is images of size 800x800x3, but the inputs are of size 224x224x3. I do not want to resize the image to 224x224 since I'm worried it will lose house distress indicators (chipped paint and loose shingles) during conversion.
My idea was to add extra layers that can handle the larger images before feeding them into ResNet. I have the following pytorch model:
import torch
from torch import nn
from torchvision import models

class NewModel(nn.Model):
    def __init__(self, pretrain_model_path) -> None:
        # Not sure here
        self.pre_layers = None
        # import the trained model
        model = models.resnet18(num_classes=365)
        checkpoint = torch.load(pretrain_model_path, map_location=lambda storage, loc: storage)
        state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}
        model.load_state_dict(state_dict)
        # change prediction class count
        model.fc = nn.Linear(model.fc.in_features, 4)
        self.backbone = model

    def forward(self, x):
        x = self.pre_layers(x)
        x = self.backbone(x)
        return x

Is this a common practice or is it better to make one from scratch that is built for this size image and problem specifically? How would I go about implementing it if so?
","['convolutional-neural-networks', 'pytorch', 'residual-networks']",
How can we approximate infinite horizon MDP with finite horizon MDP in the context of reinforcement learning?,"
For a given value of ""discount factor"" (and reward values' range) in fixed finite horizon markov decision process (MDP), upto how many episodes we have to extend this MDP so that we can approximate the corresponding infinite horizon MDP?
I am actually working on a research project in which risk-averse MDP (RA-MDP) (with dynamic risk measures) are used which is by nature infinite horizon (this sentence may be irrelevant to answering the question, but I give it as a motivation and to make context). I want to solve this by using online optimization. For this, I am using risk averse actor-critic algorithm, as proposed by Coache et. al. in ""Conditionally elicitable dynamic risk measures for deep reinforcement learning"", which is the latest and the only RL algorithmic framework for risk-averse MDPs, but unfortunately restricted to finite MDPs.  On the other hand, my problem is infinite horizon. So I want to approximate this ""infinite horizon"" with ""finite horizon"" case.
Please help me if you can in this regard (authentic references are appreciable).
","['reinforcement-learning', 'reference-request', 'markov-decision-process', 'risk-management']",
"In off-policy MC learning, why is the probability of sampling a trajectory the same as having a return?","
In Sutton and Barto's RL book, in the section for off-policy learning, we would like to find the expected value of the random variable $G_t$, given $S_t = s$ under our target policy: $$\mathbb{E}_{\pi}[G_t|S_t = s]$$
So here, our random variable is $X = G_t$, meaning returns of states in trajectories/episodes.
Since we want to be able to explore, we use a behavior policy $b$, and therefore sample trajectories(episodes) from it. Computing sample averages doesn't work, as it would converge to $$\mathbb{E}_{b}[G_t|S_t = s]$$ and not $$\mathbb{E}_{\pi}[G_t|S_t = s]$$.
Hence, we use Importance sampling, and here is my understanding of how it works:
$$ \mathbb{E}_{\pi}[X] = \sum_{x \in X} x\pi(x) = \sum_{x \in X} x\pi(x) \frac{b(x)}{b(x)} \implies $$
$$=\sum_{x \in X} x\frac{\pi(x)}{b(x)}b(x) = \sum_{x \in X} x\rho(x)b(x) $$
and in this way, if we know $\rho(x)$, we can do this: $$\frac{1}{n}\sum_{i=1}^{n}\rho(x)x_i \approx \mathbb{E}_{\pi}[X], x \sim b$$.
In Sutton & Barto, in order to calculate this $\rho(x)$, the ratio between the probabilities  of the occurrence of a specific trajectory is calculated: $$P(A_tS_{t+1}A_{t+1}...S_T|S_t,A_t)$$
and then the ratio is calculated.
How is this the same as $G_t$? if we want to calculate the return of a specific return $G_t$, we could have many trajectories that give the same return, so we would be underestimating the probability of that return if we find the probability of occurrence for a trajectory.
Where is my gap in understanding?
","['reinforcement-learning', 'sutton-barto', 'monte-carlo-methods', 'off-policy-methods', 'importance-sampling']",
References for Stochastic Process for Sequential Decision Making,"
I am reading papers on Multi-armed bandit problem. In these papers, they use some notations in measure-theory like filtration, adapted filtration, and so on. Also, to prove theorems, many papers assume that noises are conditionally martingale, etc.
I have passed the stochastic process course but it was mainly about the renewal process, Markov chain, and I am not familiar with martingales, and topics that are used in Bandit problems. I would be thankful if you could suggest to me some reference books covering these topics. If there are any video courses, that would be great.
","['reference-request', 'probability', 'multi-armed-bandits', 'contextual-bandits']",
Is this a better formulation of the Turing test?,"
Would the Turing test be better formulated by assessing whether the creator (as opposed to a third party) was able to tell the difference between their program and a human?
A magician, through the use of misdirection and slight of hand, could make an audience believe they are witnessing true magic, however the magician would only be able to make themselves believe if in fact the magic was real.
Would formulating the Turing test as such put a greater focus on creating “real” AI as opposed to the illusion of AI?
","['ai-design', 'agi', 'turing-test', 'artificial-consciousness']","
For the purpose of having AI on par with humans and releasing it to the market is not that relevant that the creator, a single person, can still not be fooled.
Moreover this would create a conflict of interests, as the creator may pretend to be fooled.
In conclusion: no.
"
Is AI the right tool for solving walking?,"
Modern robots walk like they had a heart attack and then a stroke... twice. Everyone in AI fiercely believes that the higher number of neurons (or the quality of training with smaller number of neurons) is the solution and things will definitely improve after pumping in more neurons and more time.
Everyone talks about humans and how many neurons we have and how our babies need time to learn to walk.
Meanwhile horse babies are born and start running right away after a minute or so. They have less neurons than we do.
Ants jump out of their eggs and zoom around like crazy, their brains have only 250k neurons.
Xeno bots were moving around repurposing their protrusions for swimming, sensing the environment, changing directions and such - those are individual cells, no brains, no neurons.
Is AI really a way to solving organism movement problem? It seems like a completely wrong tool for the task.
",['training'],"
In principle, for any complex problem there is only one solution: AI.
Humanoid robots are just starting. Yes, they are here let say 30 years in some cases but it was only hard-coded attempts to make robots.
First true case of useful robot was introduced by Tesla one year ago and first demo showed yesterday.
This is the first real case when real AI was used and not just hard-coded  programming. Tesla AI - which has been developed for cars - FSD.
You can watch on youtube how increasingly better FSD has become in last few months. Everything is recorded. The same speed of improvement will be visible on Tesla Bot.
So yes. This is extremely complex problem therefore only AI can solve this.
Boston Dynamics tried this hard-coded way (20 years of trying) and they failed.
Tesla nailed it when they was able to put together walking robot in 8 months.
And the positive impact is that AI of Tesla Bot will impact FSD and FSD will impact AI of Tesla Bot. The synergies will run in both directions. It's inevitable.
> Meanwhile horse babies are born and start running right away after a minute or so. They have less neurons than we do.
Number of neurons is correlated to number of organs and complexity what organism is capable of + number of environments where can life. Variation of movements for humanoids are far more complex than variation of movements and capabilities of horses.
And movements for humanoids must reflect also general thinking - how to use tools. Humanoids are tool makers and every tool category has different set of movements. So if you want to solve walking for humanoids you have to build in tool usage what has impact on walk movements.
"
What material might a truly sentient A.I. brain be made of?,"
I read that silicon may not be able to be create a robot brain that actually thinks because of its inability to process enough. Some people say nanomagents could generate low energy A.I. but I was wondering. What might truly conscious A.I. brains be built from in 200 years?
","['artificial-neuron', 'artificial-consciousness', 'brain']",
Understanding the features given in Example 13.1 of Sutton and Barto,"
I'm struggling to understand the notation used to represent the features within Example 13.1 (Short corridor with switched actions"" in the Sutton and Barto RL book.
I assume as it is a free pdf it is ok to quote this content here, but if I am wrong, please let me know and I can remove it.

Consider the small corridor gridworld shown inset in the graph below. The reward
is -1 per step, as usual. In each of the three nonterminal states there are only
two actions, right and left. These actions have their usual consequences in the
first and third states (left causes no movement in the first state), but in the
second state they are reversed, so that right moves to the left and left moves to
the right. The problem is difficult because all the states appear identical under
the function approximation. In particular, we define $\mathbf{x}(s,right)=[1,0]^T$ and
$\mathbf{x}(s,left) = [0,1]^T$ , for all s. An action-value method with $\epsilon$ -greedy action selection
is forced to choose between just two policies: choosing right with high probability
$1-\epsilon / 2$ on all steps or choosing left with the same high probability on all time
steps. If $\epsilon = 0.1$, then these two policies achieve a value (at the start state)
of less than -44 and -82, respectively, as shown in the graph. A method can
do significantly better if it can learn a specific probability with which to select
right. The best probability is about 0.59, which achieves a value of about -11.6.

Screenshot of plot below:

In the example it states $\mathbf{x}(s,right)=[1,0]^T$ and $\mathbf{x}(s,left) = [0,1]^T$ for all s. Seeing feature vectors with relation to states (and not actions) seemed to make sense to me, e.g. for the polynomial construction:
$\mathbf{x}(s)=\left(1, s_1, s_2, s_1 s_2\right)^{\top}$
But I don't recall ever seeing it spelled out like this for features incorporating actions as well. In the polynomial case given above, we have a 1x4 vector. What is the dimension of the feature vector given in Example 13.1? Is it 1x2x2? How can we write it out in terms of $x(s,a)$ rather than two explicit definitions for each action? $x(s,a) = [[1,0],[0,1]]$?
Also, I'm confused on why this example makes a positive case for using policy gradient over value-based methods. If we just used the action values to determine a policy, wouldn't the agent be able to learn the choosing left from the 2nd state result in a higher action value? In such a case I would expect the value under such a policy would be [-3, -2, -1, 0] for each state.
Finally, I don't understand the y-axis of the plot. This is the value under policy pi (defined by theta) for some state S. What is S here? A single state? The average across all states?
","['policy-gradients', 'features', 'feature-engineering']",
PPO continous action space working in a complex scenario but failing to work in a simple scenario,"
I tried solving supply chain optimization problem using RL discrete and continuous actipn space. For some reason, with simplified version of problem (i.e. if customer order is always equal to 1), how much the agent should restock, discrete action space works better, continuous action space gives suboptimal results. With complex version of problem (stochastic customer orders), continuous action gives more consistent results than discrete action. I am using PPO discrete/continuous algorithm from RLLib for discrete/continuous spaces. Any intuition why these results?
","['reinforcement-learning', 'proximal-policy-optimization']",
Extract a document page from a photo,"
I am trying to extract a document as an image from another image. Let's say that we take a photo of a document on a surface. My ultimate goal is to be able to digitize this document but as an image, not as OCR. This is already done in phones like my Samsung, but I want to do this using my Linux machine and as a command line tool if possible.
As an example see the attached image (created by the default application of my mobile phone). The system recognized the orthogonal area and proposed its bounding box with yellow color. After that of course it is easy to extract this area and make it straight as a regular orthogonal image. What I am interested is how to ""reconstruct"" this yellow box from scratch.
Any ideas how to do this? I tried to search various methods and tools, but when I search something there's always information on how to convert this through OCR, which is something I am not interested. I even found some imagemagick techniques that promise to do this, but unfortunately failed :(

","['image-processing', 'image-segmentation']","
I think what you are searching for is something like this, it is sufficient to extract the lines in the image and choose the lines that are most connected as your object
"
Detecting object position given the relative position of another object,"
I know that the title might be redundant but I'm trying to understand if there is way to predict where a specific object will be if I provide a certain object as a reference.
See as an example the image . The red object (which can be provided as a set of coordinates) in always present as a feature. The blue object is the  one to be predicted. In general the red polygon can vary (more elongated, larger or smaller) and the blue object change its position wrt the red one.
I am thinking, if possible, about a system where in a first training phase I provide both the objects with the blue object been the target of my training.
In a test phase I would provide only the red object and let the system try to guess where the blue object will be.
The underlying relation between the two objects relative position is not well understood in general so my idea was trying to understand if out there is something that could generalize and approach this problem.
It might be a conceptual question for the moment, in case more details are needed I would be happy to discuss them.
Thanks!
","['convolutional-neural-networks', 'object-detection']","
I always like to think that Theoretically, if there exists some function $f:R \rightarrow B$ that maps the set of points $R$ which represent your reference object to the set of points $B$ which represent your target object, then there should be a network $N_\theta$ that approximates $f$ with arbitrarily small precision. Sadly for us, it may be very difficult to find $N_\theta$, but hey let's try anyway!
In terms of the algorithmic approach, your suggestion seems likely. Another possibility would be to try and formulate a basic CNN that takes in normalized images of the reference object and outputs a $(x,y)$ that will represent the center of your target object. For that, of course, you will have to create data that consists only of images of rotated + scaled version of your initial reference object and labels that are the center points $\{(x_i,y_i)\}$ of the target object.
It might also be reasonable to consider transfer-learning using some pre-trained ImageNet-based network, as those were shown to improve results even for very different datasets (compared to ImageNet)
"
How does having zero advantage help with identifiability?,"
I am reading the D3QN paper and they have the following paragraph -

Equation (7) is unidentifiable in the sense that given $Q$ we cannot recover $V$ and $A$ uniquely. To see this, add a constant to $V(s; \theta, \beta)$ and subtract the same constant from $A(s, a; \theta, \alpha)$. This constant cancels out resulting in the same $Q$ value. This lack of identifiability is mirrored by poor practical performance when this equation is used directly.
To address this issue of identifiability, we can force the advantage function estimator to have zero advantage at the chosen action.

How does making the advantage for the chosen action 0 (actually, it's a little larger than 0, cause we'll be using the mean instead of max operator), help the neural network learn accurate estimates of $V$ and $A$?
","['reinforcement-learning', 'dqn', 'papers', 'd3qn']",
Similarities between 2d-vectors. (to flatten or to not),"
I have this scenario where I need to measure the similarity between a 2d tensor t1: (100,8) and 61 tensors of the same shape(100,8). 100 represent time-steps and 8 is the no. of options.
I first tried flattening all tensors so that I can use the cosine similarity measure, but it slows down performance significantly.
I used pairwise comparisons without flattening like the approach below, but it made executing mathematical operations more complicated (e.g., diagonal operations):
How to calculate a meaningful distance between multidimensional tensors.
What should I do to speed-up ops on flattened tensors/or are there alternatives to flattening?
","['deep-learning', 'pytorch']","
I suggest you to flatten and use FAISS. I had the same problem but using faiss it was made simple it is creating indexing which is faster to compare. Using faiss indexing i am able to compare 1 vector with 21 million vector in 6 milliseconds.
"
Is Computer Vision always related to Machine Learning?,"
So I have AI project about motion detection with image subtraction.
Regardless what are the object used, if there are change between two frames according threshold value, then it will categorized as motion.
So in my project I only use OpenCV library in python.
My program take two input. Where first frame or background frame will assumed/labeled as no motion frame for a refference. Second frame is any frame that captured currently.
So, with just using image processing like
resizing -> grayscaling -> blurring -> substracting (absdiff) -> thresholding

Basically my program/project is just comparing between two images if there are changes in its pixel.
Beside my project is related to computer vision obviously, is my project related to machine learning too? Especifically supervised learning because I labelled what is no motion image looks like to the machine.
But in other hand, I don't feel any statistically method where machine learning usually use it. My mathematical operation was using substracting method only.
","['machine-learning', 'computer-vision', 'ai-design']","
Actually grayscaling and blurring are convolutional operations, and thresholding can be seen as an ""activation function"" (think of a sigmoid with a high gain). And resizing can be implemented by an average pooling layer. But since you have hard-coded these parameters (the blur radius and threshold), there is no ML involved.
Then again it could be a fun exercise to apply a gradient descend to those layers. To run the training, you'd need to supplement the network with training data. In this case it would be a ""binary"" image where you have defined for each pixel whether it belongs to the background or foreground. Since there are so few parameters to tune, I expect that you wouldn't need that many training examples.

if there are change between two frames according threshold value, then it will categorized as motion.

Ah now that I read you question more carefully, your training data could be just yes/no label for the whole picture. You aren't looking for object segmentation.
"
Training a neural network simultaneously with two different loss functions rather than considering the weighted sum,"
This is a follow up on the already asked question: Is the neural network 100% accurate on training data if epoch loss is minimized to 0?
I want to train a neural network that works as an approximator for a function that satisfies a few constraints.
For example, I want the network $\mathcal{N}(x)$ to satisfy $\mathcal{N}(x) \leq \eta, \ \forall x \in D_1$, and $\mathcal{N}(x) \geq \eta, \ \forall x \in D_2$, where $D_1 \cup D_2$ is the training data set.
So I define a custom loss function as:
$$L_1 := \cup_{x \in D_1} \mathrm{ReLU}( \mathcal{N}(x) - \eta) + \cup_{x \in D_2} \mathrm{ReLU}(-\mathcal{N}(x) + \eta) $$
such that I can generate an output that minimizes the loss and let the training converge when $L_1=0$ so that all the conditions are satisfied on the training set (Yes I need this, even at the cost of overfitting?).
Now, I also want the trained neural network to have some Lipschitz bounds, so I consider the log-det barrier method used in the following paper:
https://arxiv.org/abs/2201.00632
Correspondingly, I add an extra loss term $L_2$ and try to minimize $L=\lambda_1L_1+\lambda_2 L_2$.
However, my question is that if I do this, I may not able to achieve $L_1=0$ anymore since $L_2$ can be highly negative even if $L_1$ is positive can result in minimization of the overall loss function. Is there any way that I can simultaneously train the neural network for $L_1$ and $L_2$ minimization without using the weighted sum? Or to recover from the final training loss $L_1$ and separately use this value for convergence?
Any help in this area is appreciated, I'm not so well-versed with neural networks. Another concern for me is if enforcing $L_1=0$ is impossible since $L_2$ is kind of like a regularization term that will prevent NN from overfitting, and $L_1$ and $L_2$ will work against each other.
","['neural-networks', 'objective-functions', 'convergence', 'loss']",
Can the Jacobian of a Neural Network be Full Column Rank?,"
Let $\mathcal{X}$ be the input data space and $\mathcal{Y}$ be the output data space.
$f: \mathcal{X} \to \mathcal{Y}$ is a function represented by some Neural Network.
Is it possible to to check if the Jacobian $J_{f}:=\left(\frac{\partial f_{i}}{\partial x_j}\right)$ is full Column rank? or are there some conditions (like specific activation function) we can assume on the Neural network to make it full Column rank?
Also can it be safely applied to CNNs and not only ANN? (because I think CNN can not be treated as in vector space)
","['neural-networks', 'convolutional-neural-networks', 'math', 'computational-learning-theory']",
Using AI to extend an imagine pattern,"
I have created some nice patterns using the MidJourney tool. I'd like to find a way to extend these patterns, and I was thinking about an AI tool that takes one of these patterns and extends it in all directions surrounding the original pattern.
Just to give you an idea, this is one of those patterns:

","['ai-design', 'pattern-recognition', 'image-generation', 'diffusion-models']","
The task you would like to accomplish is referred to as ""outpainting"". See example below.

Very recently, OpenAI released an outpainting feature that extends the possible operations to perform with their diffusion model DALL-E.
It is also worth to mention the Stability AI Stable Diffusion model infinity extension (from which I took the example GIF image above). The nice thing of stable diffusion is that, unlike DALL-E, it has been publicly released.
"
Frozen baseline for policy gradient rewards,"
I have a continuous reinforcement learning problem for which I use policy gradients and I use a baseline to decrease the variance of the gradients.
The baseline that I used is the moving average of the rewards obtained during the last 10 time steps just before the current time step.
I have 2 questions regarding the baseline as follows.

Would it be better to use a frozen baseline for some time steps, e.g., keeping the baseline constant for the next 5 time steps? My motivation for that is due to the frozen target Q-values in DQNs. The effect of the baseline to gradient updates is actually zero, but as it has an impact of the reward, which can determine the direction of the gradient updates, can it somehow help stabilize the training of the network and improve the results?
Is there a rule of thumb for the selection of moving average window, like a ratio considering the number of total epochs etc.?

","['reinforcement-learning', 'policy-gradients', 'rewards']",
How to use Categorical Cross Entropy for Multi-Label Classification?,"
Say my target with classes A, B, C, D, E is [0, 1, 1, 0, 0].
And my output layer is of B x N where N is the number of classes.
How do I use Categorical Cross Entropy for this?
","['pytorch', 'loss']","
You want to treat the problem as binary classification, and of course use binary cross-entropy.
The only requirement is to ensure to use a sigmoid activation in your model's last layer.
"
Why DQN model select same action during the training,"
Now i try to create the DQN model. During the training process, the action value of each step is different, but most of the time, the same action is always selected. How can i solve it?
Replay memory is 1000,
batch size is 32,
The learning rate is 0.0025,
eplison is 1.0
epsilon decay is 0.98
The discount fatcor is set to 0.98.
activation function for hidden layer is ReLu, and for output layer is linear
",['dqn'],
What kind of neural network and GPU should I use to classify images into > 10 000 classes?,"
I am trying to developp an image classifier that would have more than 10 000 classes but I don't know what kind of neural network I should use ?
Some Other questions arise from this one :

How big should be my training dataset ? (10,100,1000 or even more image/classe)
What kinf of GPU should be used to train/run such a network ?

Thanks in advance !
","['neural-networks', 'image-recognition', 'training-datasets', 'gpu']","
You could look for papers that trained models on the Open Image Dataset, which contains around 6k classes, so pretty close to your final use case.
Regarding the dataset size, most datasets include at least 1k examples per class, this is a rule of thumb more than a mathematical restriction. Of course the more the better, like is always the case in machine learning.
The GPU question is an ill posed one. All framework by now support more or less any GPU, nvidia being the choice of preference when it comes to driver compatibility, especially in cloud platforms like aws or google gloud. The specific type is really up to you. If you have a limited amount of time to allocate for training then definitely use as many GPUs as you can afford. If time is not a constrain any GPU will do the work just fine.
"
"Matrix Dot Product of and [B, N] and [N x N] in Tensor","
I have a pre-computed co-occurence matrix in shape of [NxN]
I want to utilize this info on the last layer of my multi-label classification of [B, N].
Is dot product the best way to do it? How do I use dot product in Tensor with [B, N] and [N, N]?
","['pytorch', 'linear-algebra', 'tensor']",
Can neural network be used to predict deltas between numbers?,"
I have a list of increasing numbers with no duplicates for example : 3,6,11 and so on. Difference or deltas between these numbers such as in above case : 3, 3, 5 are frequent and with greatest difference being 9. Can a neural network be used to predict next 3 deltas for next 3 numbers. Which type of neural network will be useful and what kind of limitations will there be ?

Note : List comprises of millions of numbers.
These numbers and deltas are not random.
",['neural-networks'],"
Yes, predicting deltas is often easier than predicting values directly. The most obvious way to perform this task would be to use a simple MLP with MSE loss. If the deltas are discrete, and there are only 9 possible values, a multi-label cross entropy loss would probably perform better than using MSE.
Depending on what the relationships between the numbers are, you might want to feed the last W numbers to the neural network, so that it can learn to use context. To do that, just make your input dimension W, and have your output dimension be 1. If you need a very long context window for good prediction, you should consider using an RNN.
One thing that would help to improve training is that the scale of the inputs shouldn't be too different (e.g. some inputs being ~10^5 and some being ~10^0). If you can rescale your inputs to always be close to 0, that's helpful. For example, if the rule which relates numbers to one another is +3, +5, +3, +5, ... you could always normalize the first input to be 0, and un-normalize after you get your neural network output, without any loss of information.
"
Papers on Prompt Engineering,"
I am into AI in general and NLP in particular. Besides, I have a background in philosophy, and the new LLMs like GPT-3 seem to have exciting capabilities. I want to study prompt engineering (for example, teaching the model to reason, etc.)
Do you know any specific papers to start? I am looking for studies on the techniques of prompt engineering.
","['natural-language-processing', 'reference-request', 'papers', 'language-model', 'gpt-3']","
Currently, I am referring to the following links. They have a good number of papers and they are categorized too.
https://github.com/thunlp/PromptPapers
https://github.com/zjunlp/PromptKG
"
What to say about time complexity of SVM?,"
I've read it in some literature now that the training speed of SVM (in general) is very low. Why is that the case? What is to say about time complexity of SVM?
","['machine-learning', 'training', 'support-vector-machine']","
The slow training of support vector machines comes from the Sequential minimal optimization algorithm, which is proven to have time complexity of roughly $O(n^3)$ (mostly because it involves matrix inversion operations).
"
Adversarial training reduces MNIST test accuracy under FGSM attack,"
I have trained a basic convnet model on MNIST. I am conducting FGSM attack. The training accuracy is like 98%. and initial adversarial test accuracy under FGSM attack is like 20%. I tried to do adversarial training to improve attack accuracy on the test set.
It fails! I followed to approaches but both fails.
1st approach:
I already have a model trained on clean MNIST traning set. Now for 10 iterations, at each iteration create the whole adversarial training set for the model and train the model on that for 1 epoch. Then check the adversarial accuracy for test set in each iteration. The problem is that the adversarial test accuracy bounces back and forth, and reaches very low levels of 0!
2nd approach:
I create a new model from scratch (rather than fine-tuning a pre-trained model) and follow approach 1. The same thing happens.
This is the curve of accuracy. As you see, adversarial training for FGSM samples is not improving the robustness (adversarial test accuracy) in the green curve.

Please help.
","['machine-learning', 'adversarial-ml']",
Reinforcement Learning with sparse/delayed reward - should intermediate rewards be decayed over time/training?,"
I'm thinking of a situation like a game (say, chess) where the real objective/reward is actually determined at the very end.
I understand that it's important/helpful to do reward shaping with intermediate rewards, so that the agent can get clues of what is good/bad behavior leading to the final result. However, I would greatly appreciate advice or discussion about whether these intermediate rewards should be phased out over time.
For example, let's say the agent is playing chess. I imagine its helpful to give rewards/punishments for captured/lost pieces and then a BIG reward/punishment at end for victory/defeat. As training goes on, though, would you recommend decaying/removing the intermediate rewards?
","['reinforcement-learning', 'reward-design', 'sparse-rewards']",
Clustering by using Locality sensitive hashing *after* Random projection,"
It is well known that Random Projection (RP) is tightly linked to Locality Sensitive Hashing (LSH). My goal is to cluster a large number of points lying in a $d$-dimensional Euclidean space, where $d$ is very large.

Questions: Does it make sense to cluster the points via LSH after having reduced the dimensionality of their input space by using first RP? Why yes/no? Is there any redundancy in the combined use of RP as dimensionality reduction method before LSH as clustering method?
","['machine-learning', 'clustering', 'dimensionality-reduction', 'randomness', 'k-nearest-neighbors']",
How do I use the N correctly in NEATs speciation delta function?,"
When implementing NEAT I'm having some issues with the speciation distance/delta function, specifically the term N (number of genes in biggest genome).
Won't term $N$ in $δ=c1*E/N+c2*D/N+c3*W$ just out-scale $E$ and $D$, and reduce their impact on the function as the genomes grow?
The paper(3.3) recommends limiting N = 1 when genomes are small, but what happens when they grow? The impact of E and D tapers off.
For example two genomes with no similarities (silly example, but just for demonstration):
#1: 67 connections
#2: 40 connections
No similar innovation numbers, so $W$ will be 0.
$E$ = (difference in genes count) 67-40 = 27
$D$ = (total count of all genes found in one but not in other, minus excess genes) 67+40-27 = 80
$N$ = (Size of biggest genome) = 67
$c1$ = $c2$ = 1.0
$δ=1.0*27/67+1.0*80/67+0 = 1.5970$
This is pretty low compared to the default limit of $3$ that is used in the NEAT paper(4.1) examples, and by some other examples online.
Am I computing this incorrectly?
","['neural-networks', 'evolutionary-algorithms', 'neat', 'neuroevolution', 'topology']",
"Why do we need Tensorflow, Keras and other ML/AI modules?","
This question might seem stupid at first glance, and it might be - that is because I am very new here and I've tried to think about an answer of my own, and search this question but to find no answer..
The question is why do we need all these modules such as Tensorflow, Keras PyTorch etc.. ?
Let's say we have an MNIST database of a lot of handwritten numbers, we want to train our machine to identify what number is presented. Then, ok - we are using some modules of the Tensorflow/Keras or other module to train the dataset and we got a somewhat working machine that identifies numbers from picture - well done..
Now what? what does it actually help us doing? if the train method is prebuilt then where do we need a functioning human brain in coding in these modules? everything is prebuilt... what is the algorithm behind the train method? I don't know, nor care, because it trains my machine on the dataset and that's all...
But if I need another algorithm? and so, isn't the world of training a machine on a specific dataset will always be based on algorithms? and if you need a specific algorithm then - sit down and think about a mathematical model and build the algorithm yourself? why do we need these modules??
If for example I now have a dataset of cars and want to identify the company (Suzuki, Nisan etc..) - then I need to create my own mathematical-computer vision algorithm?
The world of machine learning/AI all about building your mathematical algorithms yourself? I am so confused..
If this question related to the meta of this Stackexchange tell me and I will move it to there, I am new to here and not sure where to ask this..
Thank you in advance!
","['machine-learning', 'tensorflow', 'object-detection', 'algorithm']",
How to increase accuracy for CNN?,"
I have built one CNN model and applied it to chest-xray Covid 19 pneumonia dataset. I am getting the classification report as follows:

I am surprised to see that it is giving an excellent result on minority class but relatively poor on class 1. Can anybody help me tune my model so that I can build a more robust model?
CNN code
def architecture (_img_dims ):
    
    inputs = Input(shape=(_img_dims, _img_dims, 3))

    x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPool2D(pool_size=(2, 2))(x)

    
    x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPool2D(pool_size=(2, 2))(x)
    x = Dropout(rate=0.2)(x)

    
    x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPool2D(pool_size=(2, 2))(x)
    x = Dropout(rate=0.2)(x)

    
    x = Flatten()(x)
    x = Dense(units=512, activation='relu')(x)
    x = Dropout(rate=0.7)(x)
    x = Dense(units=128, activation='relu')(x)
    x = Dropout(rate=0.5)(x)
    x = Dense(units=64, activation='relu')(x)
    x = Dropout(rate=0.3)(x)

    # Output layer
    output = Dense(units=3, activation='softmax')(x)
    
    return inputs, output

INFO ABOUT CLASSES

Class 0:  COVID
Class 1:  Normal
Class 2:  Pneumonia

Image SAMPLE



I have also used Data-Augmentation. Still the result is same
train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=10,
        zoom_range=0.4,
        horizontal_flip=True,
        validation_split=0.01
        )

Edit
I am trying to increase, the number of class-level 1 images i.e Normal Images by using AC-GAN. Code is taken from here
from math import sqrt
from numpy import asarray
from numpy.random import randn
from keras.models import load_model
from matplotlib import pyplot
 
# generate points in latent space as input for the generator
def generate_latent_points(latent_dim, n_samples, n_class):
    # generate points in the latent space
    x_input = randn(latent_dim * n_samples)
    # reshape into a batch of inputs for the network
    z_input = x_input.reshape(n_samples, latent_dim)
    # generate labels
    labels = asarray([n_class for _ in range(n_samples)])
    return [z_input, labels]
 
# create and save a plot of generated images
def save_plot(examples, n_examples):
    # plot images
    for i in range(n_examples):
        # define subplot
        pyplot.subplot(sqrt(n_examples), sqrt(n_examples), 1 + i)
        # turn off axis
        pyplot.axis('off')
        # plot raw pixel data
        pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')
    pyplot.show()
 
# load model
model = load_model('../input/model111/Xception.h5')
latent_dim = 224
n_examples = 100 # must be a square
n_class = 1 # Normal
# generate images
latent_points, labels = generate_latent_points(latent_dim, n_examples, n_class)
# generate images
X  = model.predict([latent_points, labels])
# scale from [-1,1] to [0,1]
X = (X + 1) / 2.0
# plot the result
save_plot(X, n_examples)

But I am getting error as
ValueError: Layer model expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 224) dtype=float32>, <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>]

","['convolutional-neural-networks', 'computer-vision', 'python']",
"How to Create a Fixed-Length, Binary, Sequence of Tokens Embedding?","
Say I have ten classes represented by 1 x n_classes vector of binary.
My goal is to embed a sequence of 1xN binary so that I can also model the class-co occurrence.
Say, class A, B, D are present and represented as [1, 1, 0, 1, 0, 0, 0, 0, 0, 0]
The embedding model aims to produce an embedding for this sequence.
sequence = [1, 1, 0, 1, 0, 0, 0, 0, 0, 0]
embedding = model(sequence)

","['recurrent-neural-networks', 'word-embedding']",
Survey on non-machine learning object detection algorithms,"
I am working on a project in which I will be performing object detection on deformed objects. Unfortunately, there isn't enough data sets to train them on some neural network. I am looking for reference on computer vision algorithms that does not require learning or training on datasets. Rather, topics arising from ""structure from motion"" that are popular within the computer vision community.
Some note: The object detection will take place in an open environment rather close environment.
","['computer-vision', 'object-detection', 'unsupervised-learning']",
Can Graph Neural network leverage only the topological structure?,"
Graph Neural Networks (GNNs) are a powerful tool that allow learning on graphs by leveraging both the topological structure and the feature information for each node.
For the particular problem I am trying to solve there are no node features that I could think of as it is simply a dependency graph between a set of nodes. And the task is given this dependency graph I want to place those nodes on another graph ( where its topology is fixed in advance and is also given as input defining my constraint) such that for any such placement there is a cost function.
My training labeled data consists of ideal placements with their given cost.
My question now is :

What does the ANN architecture and the learning algorithm look like especially since my node have no features ?
How can I embed my input into meaningful features ?

Any ideas/references on how to tackle this problem in general ?
","['neural-networks', 'deep-learning', 'graph-neural-networks', 'graphs', 'embeddings']",
Why do iterative deepening search start from the root each iteration in the context of the minmax-algorithm?,"
Consider the graph below for an understanding on how IDS work.
Now my question is:
why do IDS start at the root every iteration, why not start at the previously searched depth in the context of minmax?
What is the intuition behind it?

","['minimax', 'game-theory', 'iddfs', 'adversarial-search']","
Normally in minimax (or any form of depth-first search really), we do not store nodes in memory for the parts we have already searched. The tree is only implicit, it's not stored anywhere explicitly. We typically implement these algorithms in a recursive memory. As soon as we've finished searching a certain of the tree, none of the data for that subtree is retained in memory.
If you wanted to be able to continue the search from where you left off, you'd have to change this and actually store everything you've searched explicitly in memory. This can very quickly cause us to actually run out of memory and crash.
Intuitively, at first it certainly makes sense what you suggest in terms of computation time, i.e. it would avoid re-doing work we've already done (if it were practical in terms of memory usage). However, if you analyse exactly how much you would save, it turns out not to be much at all. Due to the exponential growth of the size of the search tree as depth increases, it is usually the case that the computation effort for the next level ($d + 1$) is much bigger than the computation effort already done for all previous depth levels ($1, 2, 3, \dots, d$) put together. So, while in theory we're wasting some time re-doing work we've already done, in practice it actually rarely hurts.
In the specific context of minimax with alpha-beta pruning, we get an additional benefit when re-doing the work. We get to make use of the estimated scores from our previous iteration to re-order the branches at the root, and with alpha-beta prunings this can actually make our search more efficient!
"
Model Suggestions for Real Life local Hospital Data [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 10 months ago.







                        Improve this question
                    



I'm doing a machine learning project and was looking for suggestions. It's meant to get the date, household, age, sex, doctor, date of the medical appointment, and type of medical appointment of a patient in order to predict whether he will actually attend the appointment or not (many simply don't show up, without any further notice). The data set consists of around 26000 people.
I'm new to ML, and I was wondering if it makes sense to build an AI out of this situation. If the answer is yes, what model should I use? I'm using sklearn in Google Colab. I have tried DecisionTreeClassifier and MLPClassifier and both gave horrible accuracy, although that may be due to other reasons I'm not aware of.
Thanks in advance.
","['machine-learning', 'training', 'models']",
How to determine whether this situation belongs to data leakage or not,"
Suppose that I use three features (x1, x2, x3) to predict the value of y. After hyperparameters tuning, the r2 score on train/valid/test set is 0.92, 0.54, 0.55 respectively, it's not so good.(It is called ""original"" below)
There are some relationships between x1 and y, so I change the prediction target from y to y/x1, the situation becomes using x1, x2, x3 to predict y/x1, the r2 score on train/valid/test set becomes 0.91, 0.72, 0.73, the performance of model is improved than origin, but is this data leakage?
Similarly, if use x1, x2, x3 to predict y/x2, the r2 score on train/valid/test set is 0.90, 0.64, 0.64 (slightly increase than original); if use x1, x2, x3 to predict y/x3, the r2 score on train/valid/test set is 0.85, 0.45, 0.56 (almost same as original). If introduce feature into target belongs to data leakage, why doesn't the introduction of different features into target lead to a significant improvment in r2 score?
","['data-science', 'data-labelling']",
Utility function vs evaluation function vs heuristic function in terms of minmax + alphabeta,"
So i am trying to grasp the difference between a utility function, an evaluation function and an heuristic function in terms of the minmax algorithm with alphabeta on top.
So far my understanding is this:
Utility function gives us the utility of the TERMINAL states thus if a leaf returns win for a given player the utility of that route is win.
Evaluation function gives a numerical value for a given state and a player.
Heuristic function is an approximation of the utility function and is used when we can not explore the whole tree down to the leafs.
My questions:
There does not seem to be any difference between the heuristic function and the evaluation function in terms of the minmax game?
Is the utility function solely based on the leafs(terminal states)? if not, is it not equal to the evaluation function?
can someone fill in the blanks?
","['game-theory', 'alpha-beta-pruning', 'adversarial-search']",
Why diffusion model always use U-Net?,"
I want to know why diffusion models always use U-Net.
In my opinion, they use U-Net because you can see features of different resolutions and skip connection is good to add detail of images. But I am not sure if it is the main reason why they use U-Net.
Are there other reasons they choose U-Net rather than other architectures?
","['machine-learning', 'computer-vision', 'generative-model', 'u-net']","
I don't have a definitive answer but I'd state my intuitions anyways:
Diffusion models are highly related to the idea of stacked denoising autoencoders [Kumar et al. (2014)]. Additionally, U-Net-like architectures are a very common architecture for autoencoders on images.
Here, I would agree with your intuition that the bottleneck and the skip-connections help with denoising because they provide representations of different granularity.
Another thought is that U-Nets are empirically well-suited for image segmentation problems. Despite the fact that pixels are classified individually, you want the segmentation output to have consistent areas on top of objects in the image. So you kind of turn a very noisy segmentation mask (original image input) into a mask with much less noise (segmented output).
I think the latter is debatable, I'd be happy to hear your thoughts.
"
What loss function will be correlated with classification metrics?,"
Recently I developed a custom training algorithm for deep learning models, based on evolutionary algorithms. Details are not important, except that it also uses decreasing regular cross entropy loss as its fitness function.
What I observed is that it very well decreases the loss function but the classification metrics such as accuracy, precision or recall also decrease along the training. This got me confused, as I was sure that decreasing loss such as cross entropy should always increase these metrics. After researching I found out this is possible due to fact that cross entropy can decrease in case where confidence on few samples is greatly increasing, but many other samples are meanwhile getting incorrect scores, but the profit from these few correct ones are dominant over many incorrect ones: https://www.quora.com/What-is-the-matter-when-loss-decreases-and-accuracy-decreases-too-on-training-neural-network?top_ans=238980470
So my question is: is there a loss function that when decreasing will always be increasing classification metrics?
","['deep-learning', 'training', 'loss', 'metric', 'cross-entropy']","
Different metrics measure different quantities, so there is no reason to expect different metrics to move together unless one is a function of the other (such as MSE and RMSE).
Further, metrics like accuracy use thresholds that can prove problematic, versus evaluating the probability outputs directly. This is discussed extensively on the statistics Stack, Cross Validated. I will leave a few links.
https://stats.stackexchange.com/a/312787/247274
https://stats.stackexchange.com/a/359936/247274
https://stats.stackexchange.com/a/312124/247274
However, even strictly proper scoring rules do not have to move together. Since, for instance, Brier score and log loss (both of which are examples of the strictly proper scoring rules discussed in the first link) are measures of different quantities, this is desirable. Depending on what we value, we might prefer one over the other.
"
Normal equations for linear decision boundary,"
For solving linear regression, we have gradient descent and the normal equations, with no iterations.
In logistic regression, however, we have gradient descent but no known closed form solution.
This raises the question whether something any similar to the normal equations is possible for logistic regression. A naive attempt is the following, for a binary classification problem.

Compute all averages between points of opposite classes.
Perform linear regression over these interclass averages.

If we want to give more importance to points closer to the boundary, we may want to perform weighted linear regression on the averages, possibly multiplying the contribution of each point by some inverse of the distance between the averaged points.
Are there papers related to these sketches?
","['classification', 'linear-regression', 'logistic-regression']",
What is the role of self loop in Hidden Markov Models(HMM)?,"
What actually does the self-loop (within the single hidden state ) in the Hidden Markov model helpful for?
I learn that one of the use cases concerning Natural language Understanding is that it helps a model to stay within the current state in case of (time variable: long sound/ short sound) pronunciation of the same word. But I can't understand what role the self-loop plays here.
Any explanations would be much appreciated.
","['machine-learning', 'natural-language-processing', 'pattern-recognition', 'natural-language-understanding', 'hidden-markov-model']",
"Thompson sampling, is it accurate for smaller sample sizes?","
For example are 500 samples enough? I tried this code
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
dataset = pd.read_csv('Ads_CTR_Optimisation.csv')

# Implementing Thompson Sampling
import random
N = 499
d = 10
ads_selected = []
numbers_of_rewards_1 = [0] * d
numbers_of_rewards_0 = [0] * d
total_reward = 0
for n in range(0, N):
    ad = 0
    max_random = 0
    for i in range(0, d):
        random_beta = random.betavariate(numbers_of_rewards_1[i] + 1, numbers_of_rewards_0[i] + 1)
        if random_beta > max_random:
            max_random = random_beta
            ad = i
    ads_selected.append(ad)
    reward = dataset.values[n, ad]
    if reward == 1:
        numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
    else:
        numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
    total_reward = total_reward + reward  


plt.hist(ads_selected)
plt.title('Histogram of ads selections')
plt.xlabel('Ads')
plt.ylabel('Number of times each ad was selected')
plt.show()

with samples like this
Ad 1,Ad 2,Ad 3,Ad 4,Ad 5,Ad 6,Ad 7,Ad 8,Ad 9,Ad 10
1,0,0,0,1,0,0,0,1,0
0,0,0,0,0,0,0,0,1,0
0,0,0,0,0,0,0,0,0,0
....
....
0,0,0,0,0,0,0,1,0,0 <-500th

and the histograms differ each time I run the code. Do I need at least 1000 samples for example for the results to be consistent? How do I decide how many samples I need depending on the situation?
","['machine-learning', 'python', 'thompson-sampling']",
what are the common assumptions for any ML algorithm exclude DNN?,"
While reading a few resources about feature engineering I found that feature independence is a common assumption in ML. I wonder are there are any other common assumptions in ML.
",['feature-engineering'],
Can I minimize a mysterious function by running a gradient descent on her neural net approximations?,"
So I have this function let call her $F:[0,1]^n \rightarrow \mathbb{R}$ and say $10 \le n \le 100$.  I want to find some $x_0 \in [0,1]^n$ such that $F(x_0)$ is as small as possible. I don't think there is any hope of getting the global minimum. I just want a reasonably good $x_0$.
AFAIK the standard approach is to run an (accelerated) gradient descent a bunch of times and take the best result. But in my case values of $F$ are computed algorithmically and I don't have a way to compute gradients for $F$.
So I want to do something like this.
(A) We create a neural network which takes an $n$-dimensional vector as input and returns a real number as result. We want the NN to ""predict"" values of $F$ but at this point it is untrained.
(B) We take bunch of random points in $[0,1]^n$. We compute values of $F$ at those points. And we train NN using this data.
(C1) Now the neural net provides us with a reasonably smooth function $F_1:[0,1]^n \rightarrow \mathbb{R}$ approximating $F$. We run a gradient decent a bunch of times on $F_1$. We take the final points of those decent and compute $F$ on them to see if we caught any small values. Then we take whole paths of those gradient decent, compute $F$ on them and use this as data to retrain our neural net.
(C2) The retrained neural net provides us with a new function $F_2$ and we repeat the previous step
(C3) ...
Does this approach have a name? Is it used somewhere? Should I indeed use neural nets or there are better ways of constructing smooth approximations for my needs?
","['neural-networks', 'reference-request', 'gradient-descent', 'function-approximation']","
I do not know any specific name for this method, but it is a common approach for approximating and optimizing complex functions. You can find an industrial use-case of this approach in this paper (NeuroErgo: A Deep Neural Network Method to Improve Postural Optimization for Ergonomic Human-Robot Collaboration).
"
What is the difference between Restricted Boltzmann Machine and Artificial Neural Network?,"
In the deep learning course I took at the university, the professor touched upon the subject of the Restricted Boltzmann Machine. What I understand from this subject is that this system works completely like Artificial Neural Networks. During the lecture, I asked the Professor the difference between these two systems, but I still did not fully understand. In general, there is an input layer and a hidden layer in both, and the weights are updated with forward-backward propagation. Can someone explain the exact difference between them?
","['neural-networks', 'deep-learning', 'restricted-boltzmann-machine']","
A Boltzmann Machine is a probabilistic graphical model which follows Boltzmann distribution:
$$p(v,h) = \frac{e^{-E(v,h)}}{\sum_{v,h} e^{-E(v,h)}}$$
where $E(v,h)$ is known as the energy function.
An RBM is a Boltzmann machine with a restriction that there are no connections between any two visible nodes or any two hidden nodes, which gives it the structure similar to a 2-layer Artificial Neural Network.
The difference is that RBM, being an unsupervised model, is trained by minimizing the energy function. While an artificial neural network can have many hidden layers along with an output layer, and is trained by optimizing the loss between the values of output layer and the values of target variable.
A special case of RBM which has binary visible nodes and binary hidden nodes, also known as Bernoulli RBM has an API available in scikit-learn. They have also documented the learning algorithm here. In this example, they show how Bernoulli RBM can be used to perform effective non-linear feature extraction which can be fed to the LogisticRegression classifier for digit classification.
"
Cause of randomness in AUC score for GNN,"
I have implemented a GraphSAGE model using dgl for link prediction. On average the auc score of the model is ~0.7 but the score varies a lot for different runs. Even though I am training and testing on the same data. I am not splitting the data randomly, the data is fixed, and still I got AUC scores ranging from 0.2-0.85. I could not figure out where is the randomness is generating from. Is the model itself work in a random way?
Here is my code:
import random
import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
import itertools
import numpy as np
import scipy.sparse as sp

import pandas as pd

import dgl.function as fn

add_param_len= True
add_arg_len = True
add_name = True


class DotPredictor(nn.Module):
    def forward(self, g, h):
        with g.local_scope():
            # print(""====>"", h)
            g.ndata['h'] = h
            # Compute a new edge feature named 'score' by a dot-product between the
            # source node feature 'h' and destination node feature 'h'.
            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))
            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.
            # print(g.edata['score'][:, 0])
            return g.edata['score'][:, 0]


from dgl.nn import SAGEConv

# ----------- 2. create model -------------- #
# build a two-layer GraphSAGE model
class GraphSAGE(nn.Module):
    def __init__(self, in_feats, h_feats):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')
        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')
    
    def forward(self, g, in_feat):
        # print(g)
        # print(in_feat)
        h = self.conv1(g, in_feat)
        # print(h)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h

class MLPPredictor(nn.Module):
    def __init__(self, h_feats):
        super().__init__()
        self.W1 = nn.Linear(h_feats * 2, h_feats)
        self.W2 = nn.Linear(h_feats, 1)

    def apply_edges(self, edges):
        """"""
        Computes a scalar score for each edge of the given graph.

        Parameters
        ----------
        edges :
            Has three members ``src``, ``dst`` and ``data``, each of
            which is a dictionary representing the features of the
            source nodes, the destination nodes, and the edges
            themselves.

        Returns
        -------
        dict
            A dictionary of new edge features.
        """"""
        # print(""here!!!!!"")
        h = torch.cat([edges.src['h'], edges.dst['h']], 1)
        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}

    def forward(self, g, h):
        with g.local_scope():
            g.ndata['h'] = h
            g.apply_edges(self.apply_edges)
            return g.edata['score']

df = pd.read_csv('nodes.csv')
# print(df)
nodes_data = df.drop([""start_line"",""start_column"",""end_line"",  ""end_column"" ,""file_name""], axis=1)
# nodes_data

df = pd.read_csv('edges.csv', header=None)
# df
train_len = 3432
test_df = df[train_len:]
edges_data = df[:train_len]
# edges_data = df

import dgl

src = edges_data[0].to_numpy()
dst = edges_data[1].to_numpy()

# Create a DGL graph from a pair of numpy arrays
g = dgl.graph((src, dst))
# g

y= nodes_data['type'].tolist()
from sklearn import preprocessing
import torch

le = preprocessing.LabelEncoder()
targets = le.fit_transform(y)
targets = torch.as_tensor(targets)
targets = targets.type(torch.LongTensor)
type_one_hot = F.one_hot(targets)

g.ndata['x_one_hot'] = type_one_hot
y=torch.tensor(targets, dtype=torch.float32)
x=y.view(len(nodes_data),1)
g.ndata['x'] = x

if add_param_len:
    le = preprocessing.LabelEncoder()
    y=nodes_data['params_len'].tolist()
    targets = le.fit_transform(y)
    # print([x for x in targets if x!=0])
    targets = torch.as_tensor(targets)
    targets = targets.type(torch.LongTensor)
    param_len_one_hot = F.one_hot(targets)
    # param_len_one_hot

    g.ndata['param_len_one_hot'] = param_len_one_hot
    g.ndata['param_len'] = targets


if add_arg_len:
    le = preprocessing.LabelEncoder()
    y=nodes_data['argument_len'].tolist()
    targets = le.fit_transform(y)
    # print([x for x in targets if x!=0])
    targets = torch.as_tensor(targets)
    targets = targets.type(torch.LongTensor)
    args_len_one_hot = F.one_hot(targets)
    # param_len_one_hot

    g.ndata['args_len_one_hot'] = args_len_one_hot
    g.ndata['args_len'] = targets


if add_name:
    le = preprocessing.LabelEncoder()
    y=nodes_data['name'].tolist()
    targets = le.fit_transform(y)
    # targets
    # for x in targets:
        # print(x)
    # # print([x for x in targets if x!=0])
    targets = torch.as_tensor(targets)
    targets = targets.type(torch.LongTensor)
    # args_len_one_hot = F.one_hot(targets)
    # param_len_one_hot

    g.ndata['name'] = targets
    # g.ndata['args_len'] = targets

u, v = g.edges()

eids = np.arange(g.number_of_edges())
eids = np.random.permutation(eids)
test_size = len(test_df)
train_size = g.number_of_edges()

test_pos_u, test_pos_v = torch.tensor(test_df[0].to_numpy()), torch.tensor(test_df[1].to_numpy())
print(test_pos_u)
train_pos_u, train_pos_v = u,v

adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())), shape=(g.number_of_nodes(), g.number_of_nodes()))
print(adj.shape)
adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())
neg_u, neg_v = np.where(adj_neg != 0)

neg_eids = np.random.choice(len(neg_u), g.number_of_edges())

set(nodes_data['type'])
stmt_type=['FunctionDeclaration', 'ArrowFunctionExpression', 'FunctionExpression']
df1 = nodes_data[nodes_data.type.isin(stmt_type)]
test_neg_id = df1['id'].tolist()
# print(test_neg_id)

df1 = nodes_data[nodes_data['type']=='CallExpression']
test_neg_call_site = df1['id'].tolist()
# print(test_neg_call_site)


all_combi = [(x, y) for x in test_neg_call_site for y in test_neg_id]
print(len(all_combi))
test_data = df[3402:]
pair_list = list(zip(test_data[0].tolist(),test_data[1].tolist()))
print(pair_list)

test_neg_u, test_neg_v = [], []

for pair_ in all_combi:
    # print(pair_)
    if pair_ not in pair_list:
        test_neg_u = np.append(test_neg_u,[pair_[0]])
        test_neg_v = np.append(test_neg_v,[pair_[1]])

print(len(test_neg_u))

train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]

# train_g = dgl.remove_edges(g, eids[:test_size])
train_g = g

train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())
train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())

test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())
print(test_pos_g)
test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())
print(test_neg_g)

from sklearn.metrics import roc_auc_score
model = GraphSAGE(g.ndata['x'].shape[1], 16)
# You can replace DotPredictor with MLPPredictor.
pred = MLPPredictor(16)
# pred = DotPredictor()

def compute_loss(pos_score, neg_score):
    # print(""here!"")
    scores = torch.cat([pos_score, neg_score])
    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])
    return F.binary_cross_entropy_with_logits(scores, labels)

def compute_auc(pos_score, neg_score):
    # print(len(neg_score))
    scores = torch.cat([pos_score, neg_score]).numpy()
    # print(""scores =====> "", scores, len(scores))
    labels = torch.cat(
        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()
    # print(labels)
    return roc_auc_score(labels, scores)

# ----------- 3. set up loss and optimizer -------------- #
# in this case, loss will in training loop
# print(model.parameters)
# print(pred.parameters)
# print(train_g.ndata['x'])
optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)

# ----------- 4. training -------------------------------- #
all_logits = []
for e in range(150):
    # forward
    h = model(train_g, train_g.ndata['x'])
    # print(""this ok======>"")
    pos_score = pred(train_pos_g, h)
    neg_score = pred(train_neg_g, h)
    loss = compute_loss(pos_score, neg_score)
    
    # backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if e % 5 == 0:
        print('In epoch {}, loss: {}'.format(e, loss))


from sklearn.metrics import roc_auc_score
with torch.no_grad():
    pos_score = pred(test_pos_g, h)
    # print(pos_score)
    neg_score = pred(test_neg_g, h)
    # print(neg_score)
    print('AUC', compute_auc(pos_score, neg_score))


    from sklearn.metrics import roc_curve
    scores = torch.cat([pos_score, neg_score]).numpy()
    labels = torch.cat(
        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()
    # roc curve for models
    fpr1, tpr1, thresh1 = roc_curve(labels, scores, pos_label=1)
    # print(thresh1)
    # print(fpr1)
    # print(tpr1)
    # print(""threshold ====> "",thresh1)


    import matplotlib.pyplot as plt
    plt.style.use('seaborn')

    # plot roc curves
    plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')
    # plt.plot(fpr2, tpr2, linestyle='--',color='green', label='KNN')
    # plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
    # title
    plt.title('ROC curve')
    # x label
    plt.xlabel('False Positive Rate')
    # y label
    plt.ylabel('True Positive rate')

    plt.legend(loc='best')
    # plt.savefig('ROC_with_all_features',dpi=300)
    plt.show();


","['graph-neural-networks', 'performance', 'randomness', 'roc-auc']",
How do I improve the reward of policy gradient network when multiple states and actions exist per time step?,"
I am working on a project, in which I'm using a policy gradient algorithm (REINFORCE) to select the best cleaning method/methods for erroneous samples in tabular datasets.
The details are as follows.

Each time step is a random batch from the dirty dataset.
Actions: the cleaning methods (KNN imputer, median imputer etc.)
loss = sum(-log(selected cleaners' probabilities)) * reward
reward = predictor score - baseline score
Predictor score: The score obtained on a clean validation set, after the target task model trained on the cleaned batch with chosen cleaners by the PC network.
Baseline score: The validation score obtained by training the target task model on ground truth clean dataset.

After the training is completed, the policy gradient network does not always converge to the best cleaner / combination of cleaners.
I know that policy gradients are problematic when there are negative rewards, and in my case the reward is mostly negative, as the baseline score is obtained on the ground truth clean dataset.
I'm also feeding multiple feature vectors of erroneous rows as state vectors to the PG neural network, which could lead to ignoring the appropriate cleaners for single samples -or even penalize them- when a low reward is obtained for the current batch.
In DVRL, they also use batches to evaluate the samples at each iteration (all at once). I believe, in such cases, the minimum loss may not mean the maximum reward. But what could be done to improve the final reward?
","['reinforcement-learning', 'policy-gradients', 'rewards']",
Training Loss Value Increasing instead of Decreasing [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 11 months ago.


This post was edited and submitted for review 10 months ago and failed to reopen the post:

Original close reason(s) were not resolved






                        Improve this question
                    



I am developing my first feed-forward fully-connected ANN from scratch  based on batch learning mode on a toy training set. I am using back-propagation for calculating the gradient of the loss function with respect to weights and biases and using the gradient descent method as a learning rule.
Unfortunately when I print the training loss value (sum of square), it gets bigger as the epoch increases as you can see here:
E(0) on TrS is: [[7.83898769]]
E(1) on TrS is: [[10.00738465]]
E(2) on TrS is: [[10.76653098]]
E(3) on TrS is: [[15.94001008]]
E(4) on TrS is: [[23.80650667]]
E(5) on TrS is: [[28.65805023]]
E(6) on TrS is: [[29.56550719]]
E(7) on TrS is: [[30.5424694]]
E(8) on TrS is: [[34.26980112]]
E(9) on TrS is: [[39.9948856]]

This is my loss_functions.py file:
import numpy as np

def sum_of_squares(c, t, y, derivative=False):
    ret = 0
    for k in range(c):
        ret += np.square(y - t)
    ret = 1 / 2 * ret
    if derivative:
        return y - t
    return ret

this is my activation_functions.py file:
import numpy as np


def sigmoid(a, derivative=False):
    f_a = 1 / (1 + np.exp(-a))
    df_a = np.multiply(f_a, (1 - f_a)) 
    if derivative:
        return df_a
    return f_a


def identity(a, derivative=False):
    f = a
    df = np.ones(np.shape(a))
    if derivative:
        return df
    return f

and this is the main.py file:
from activation_functions import *
from loss_functions import *


class NeuralNetwork:

    def __init__(self):
        self.layers = []

    def add_layer(self, layer):
        self.layers.append(layer)

    def create(self):
        for i, layer in enumerate(self.layers):
            if i == 0:
                layer.type = ""input""
            else:
                if i == len(self.layers) - 1:
                    layer.type = ""output""
                else:
                    layer.type = ""hidden""
                layer.configure(self.layers[i - 1].neurons)

    def train(self, X, targets):
        MAX_EPOCHS = 10
        loss_function = sum_of_squares
        E = 0  # errore sull'intero DS
        for epoch in range(MAX_EPOCHS):
            for i, x in enumerate(X):
                target = targets[i]
                prediction = self.forward_prop(x.T)
                E_n = loss_function(c, target, prediction)
                E += E_n  
                self.back_prop(target, local_loss=sum_of_squares)
            print(""E(%d) on TrS is:"" % epoch, E)  # increasing!!!
            self.learning_rule(l_rate=0.05)  

    def forward_prop(self, z):
        for layer in self.layers:
            z = layer.forward_prop_step(z)
        return z

    def back_prop(self, target, local_loss):
        for i, layer in enumerate(self.layers[:0:-1]):
            next_layer = self.layers[-i]  
            prev_layer = self.layers[-i - 2] 
            layer.back_prop_step(next_layer, prev_layer, target, local_loss)

    def learning_rule(self, l_rate):
        # GD
        for layer in self.layers:
            if layer.type != ""input"":
                layer.weight -= l_rate * layer.dE_dW
                layer.bias -= l_rate * layer.dE_db


class Layer:

    def __init__(self, neurons, type=None, activation=None):
        self.dE_dW = 0 
        self.dE_db = 0
        self.dEn_db = None  # based on the n-th item
        self.dEn_dW = None  # based on the n-th item
        self.dact_a = None  
        self.out = None
        self.weight = None  
        self.bias = None 
        self.w_sum = None  
        self.neurons = neurons 
        self.type = type  
        self.activation = activation 
        self.deltas = None  

    def configure(self, prev_layer_neurons):
        self.weight = np.asmatrix(np.random.normal(0, 0.5, (self.neurons, prev_layer_neurons)))
        self.bias = np.asmatrix(np.random.normal(0, 0.5, self.neurons)).T 
        if self.activation is None:
            if self.type == ""hidden"":
                self.activation = sigmoid
            elif self.type == ""output"":
                self.activation = identity

    def forward_prop_step(self, z):
        if self.type == ""input"":
            self.out = z
        else:
            self.w_sum = np.dot(self.weight, z) + self.bias
            self.out = self.activation(self.w_sum)
        return self.out

    def back_prop_step(self, next_layer, prev_layer, target, local_loss):
        if self.type == ""input"":
            pass
        elif self.type == ""output"":
            self.dact_a = self.activation(self.w_sum, derivative=True) 
            self.deltas = np.multiply(self.dact_a, local_loss(c, target, self.out, derivative=True)) 
        else:
            self.dact_a = self.activation(self.w_sum, derivative=True)  
            self.deltas = np.multiply(self.dact_a, np.dot(next_layer.weight.T, next_layer.deltas))

        self.dEn_dW = np.dot(self.deltas, prev_layer.out.T)

        self.dEn_db = self.deltas

        self.dE_dW += self.dEn_dW

        self.dE_db += self.dEn_db


if __name__ == '__main__':
    net = NeuralNetwork() 

    for m in (2, 4, 4, 1):
        net.add_layer(Layer(m))

    net.create()

    X = np.asmatrix([
        [1, 0],
        [1, 1],
        [0, 1],
        [0, 0]
    ])

    targets = np.asarray([1, 0, 0, 0])

    net.train(X, targets)  

What I did for trying so far to fix result:

Check for any bug i code
Decrease the learning rate (l_rate)
Increase MAX_EPOCHS value
Replace - symbol to + in GD formula
Loss calculation with cross entropy method

Unfortunately none of these worked. There must be issue(s) in dataset (size), hyper parameter, neural network structure or implementation...
Any suggestion appreciated.
","['neural-networks', 'training', 'python']","
It appears that you are adding the partial derivates (gradient) to make adjustments rather than subtracting them.  Remember that the gradient vector points ""upward.""  You may find things improve dramatically if you subtract them (head downward) to perform gradient descent.
"
Which language model technique should be used with a low-resource language?,"
I am working with a language for which the amount of text available is relatively small (less than a billion words), what techniques exist? In particular, is it worth using a transformer or is it better to use more traditional techniques?
The aim is to generate texts in continuation of a given prompt.
","['natural-language-processing', 'language-model']",
Are neural networks a strict special case of a transformer?,"
Since transformers contain a neural network, are they a strict generalisation of standard feedforward neural networks? In what ways can transformers be interpreted as a generalisation and abstraction of these?
","['neural-networks', 'transformer', 'feedforward-neural-networks']","
Neural network is a generic term used in literature as a sort of umbrella for all types of architectures, architecture being a set of distinct forward operations and hyper parameters (such as number of layers/nodes, kernels size).
Feedforward neural networks, multi layer perceptrons, convolutional neural networks, recurrent neural networks, autoencoders, transformers (and many more) are all types of neural networks (deep neural networks to be precise, the 'deep' is usually assumed). Also edge cases like generative adversarial networks (which is more of a training approach than a strict architecture) are usually referred to as neural network, which allegedly might be confusing.
So ""since transformers contain a neural network"" is not really a correct way of putting it. Also in case you meant ""since transformers contain a feed forward neural network"" it would still be incorrect cause transformers use operations such as convolutions which are not used in feed forward neural networks, so they are still very distinct type of architectures.
"
How to classify images which are slightly different from each other?,"
(I hope I'm in the right place to ask such a question.)
A robot has a fixed camera which takes images before extending its sticks to grab a box from two sides and pull it back. But if the box is not in straight position and is rotated more than ~15% from its original straight position (a), robot's sticks cannot retrieve the box because the box will have extended its length (h+α) close to its diagonal length. I tried to visualize the case below:

Here, I want to collect hundreds of such images and classify them as straight, rotated, and if the image of rotated (either side) box is detected I should be able to send a notification to control unit to increase the robot's arms accordingly.
I couldn't think of any computer vision/image processing technique to address this issue, so
I wonder if a simple CNN can classify such small rotations.
What would be the optimal approach to this, in general?
Thanks.
","['convolutional-neural-networks', 'computer-vision', 'classification', 'image-processing']",
Are batches useful for REINFORCE without strong episode cutoffs?,"
I'm following along with PyTorch's example implementations (found here) of reinforcement learning algorithms that happen to be largely REINFORCE (vanilla policy gradient) based, and I notice they don't use batches. This leads me to ask, are batch updates of the network actually useful in this context?
Adding on, in my particular environment there's not a real meaningful cutoff for episodes as it's really set up for a sort of continuous play. As such, any n-length trajectory + rewards I collect is just as valid as another. For that reason, it would seem to mean that  a longer episode/trajectory would serve the same purpose batches tend to in network updating.
Is it expected then that batches are not particularly worthwhile in the REINFORCE context, or is this just coincidence of the implementation I'm using? And is that answer amended if there are no meaningful episode cutoffs?
","['reinforcement-learning', 'deep-rl', 'reinforce', 'batch-learning']","
Yes I see in the repo you link, in reinforce.py they only perform a gradient update once every episode. It sounds like what you're asking about is the difference between that reinforce style, and the more popular (and also more efficient) PPO type style. In that latter way, we have something like
\begin{align*} 
& \text{ for each iteration }: \\
& \qquad \text{ for t in range(size_training_set)}: \\
& \qquad \qquad \text{sample } a_t; \text{ get reward } r_t \text{ and next state } s_{t+1}; \text{ save transition to memory} \\
& \qquad \text{ for m epochs}: \\
& \qquad \qquad \text{ calculate advantages} \\
& \qquad \qquad \text{ for k mini-batches}: \\
& \qquad \qquad \qquad \text{make mini batch from training_set}\text{ and do policy gradient update }
\end{align*}
There are some other details such as importance sampling, so I would recommend you can try another repo's code first.
The advantage of the PPO way is that we spend more time training on mini-batches and less time sampling the environment (which is slower), we can use each transition in multiple mini-batches, and we can generate more varied data to train on. Grouping together transitions from different times in different episodes might help remove harmful correlations. Also, theoretically the batch size shouldn't really matter, but in practice it's important, and we can't even tune that with the reinforce way.
"
Graph convolution for multiple disconnected graphs,"
I have found many examples of graph convolution on TF and StellarGraph that work with a single large graph (like the Cora graph), but I have not found any resources on cases where you have multiple small or large graphs. I basically want to use multiple graphs as my data and in each case, I want to learn the graph kernels. This is similar to learning images where you have multiple images each with dimensions over which you define your kernel.
I'm looking for examples/demos for training multiple graphs. I would appreciate if someone can point me in the right direction.
","['tensorflow', 'graph-neural-networks', 'gcn']",
Left-to-Right vs Encoder-decoder Models,"
Xu et al. (2022) distinguishes between popular pre-training methods for language modeling: (see Section 2.1 PRETRAINING METHODS)

Left-to-Right:


Auto-regressive, Left-to-right models, predict the probability of a
token given the previous tokens.


Encoder-Decoder:


An encoder-decoder model first uses an encoder to encode an input
sequence, and then uses a left-to-right LM to decode an output
sequence conditioned on the input sequence.

My question is, what are the differences between those two methods?
Do they suggest that the first method is a decoder-only? If so, what is the input to this decoder?
Based on what I know about auto-regressive models and the above definition, I understand that in Left-to-Right, we predict the $i$-th token given the $1,...,i-1$ tokens (which could be our past predictions).
","['natural-language-processing', 'objective-functions', 'bert', 'gpt', 'encoder-decoder']",
Can NotA (none of the above) class be combined with object detection / segmentation models?,"
So there's this way of classifying an object as NotA (none of the above) class as described in https://arxiv.org/abs/1910.02830
So mostly this uses a sort of confidence thresholding with different approaches.
Can a classifier with such a NotA class be used as part of an object detection framework?
I'm assuming if the classifier isn't trained on a background class then NotA would ""light up"" also on background, so that class might need to be added as well to make some sense.
","['classification', 'object-detection', 'object-recognition']",
"Do Transformers and LSTMs use the same word embeddings (except for the position encoding, which only Transformers use)?","
In NLP, the first step is always to ""convert"" the given words of a sentence into representation vectors (word embeddings).
As I understand it, in the case of transformers, the words/vocabulary are one-hot encoded and then multiplied by a weight matrix - the results are the representation vectors (as always the weights must be learned by backpropagation). Since transformers process words in parallel, a positional encoding is added to the representation vectors to avoid losing the positional information.
My question now is: do LSTM's use the exact same word embedding mechanism - just without adding the positional encoding (since RNNs can remember the position information of sequences)?

","['natural-language-processing', 'long-short-term-memory', 'transformer', 'word-embedding']",
NLP model finetuning on personal journal,"
I have a somewhat vague question, but maybe you can help me out. I have a kind of journal, where I've been writing an entry per day (just a few sentences), to describe my activities, for the past few years. I am wondering what cool things I could do with it, with a big pretrained NLP model. In particular, I'd be interested in:

Question answering: I would like to be able to ask questions such as ""When did I last see X?"" or ""What places did I go to last year?""
Entry generation (for fun), maybe conditioned on week day, season, text input...

I'm guessing the first task might be a bit tricky, given the very specific format of the data, but the second one should be easy, right? How would you go about this? What other cool and useful tasks would you try if you had such data?
I work in research in computer vision, so I am familiar with using deep learning, and deep learning librairies. My knowledge of NLP is limited however. I am interested in delving a bit into the theory and the actual code, but the easier it is to setup, the better.
Thank you for your help! If this is not the place for such questions, please let me know.
","['natural-language-processing', 'fine-tuning']",
Is the neural network 100% accurate on training data if epoch loss is minimized to 0?,"
This seems like a silly, trivial question, but I just want to confirm it in case I'm missing something.
I'm trying to train a ReLU neural network, which is supposed to be a function that satisfies some conditions.
For example, I want the network $\mathcal{N}(x)$ to satisfy $\mathcal{N}(x) \leq \eta, \ \forall x \in D_1$, and $\mathcal{N}(x) \geq \eta, \ \forall x \in D_2$, where $D_1 \cup D_2$ is the training data set.
I incorporate these conditions as a custom loss function as follows:
$$L := \cup_{x \in D_1} \mathrm{ReLU}( \mathcal{N}(x) - \eta) + \cup_{x \in D_2} \mathrm{ReLU}(-\mathcal{N}(x) + \eta) $$
The idea is that if $L = 0$, then the conditions are satisfied for $\mathcal{N}(x)$. So when I train the network, I ensure that the epoch loss is 0 (that's when the algorithm converges). If it matters, I use an SGD optimization technique for training.
So it is safe to say that after training, the network for sure satisfies those conditions with a 100% guarantee over the training data set, right?
I understand that this may not be the case for random unseen data, but I just want to make sure that the guarantee holds for the training data set, and it will not be required to validate this a posteriori.
","['neural-networks', 'accuracy', 'training-datasets']",
Combining loss function while training with another objective function,"
I would like to train a ReLU neural network minimizing an objective function that looks like this:
$$L(W) + \eta + 1_{S}(\eta,W)$$ where $W$ is the set of weight matrices, $L(W)$ is a custom loss function, and $1_S$ the indicator function over the set of conditions $S$, which basically validates the neural network into satisfying some conditions with the decision variable $\eta$.
For simplicity, let's consider $S = \{ \eta \ \  | \ \ \mathcal{N}(x_i) \leq \eta, \forall x_i \in D\} $ where $\mathcal{N}$ is the neural network computed over the data set $D$.
I know that $\eta + 1_{S}(\eta,W)$ can be solved by simple linear programming if the weights $W$ are known. However, if they're unknown, the problem becomes non-convex. I was looking for something like an ADMM approach, but I don't think that's usable here since separating the objective function with $W$ and $\bar{W}$ as auxiliary variables  for loss function and the second objective function with $W-\bar{W}=0$ as a constraint would still render the objective non-convex.
Is there something I can do, or is this unsolvable in this form?
","['neural-networks', 'objective-functions', 'feedforward-neural-networks']",
Is it overfitting?,"
hi i'm new in this field. I am trying to do a video classification project by using 3DCNN and I plotted the loss curves & accuracy curves. I have some questions.

i'm using kfold Cross validation. Should i save the parameters after every fold and choose the best one?
how to reduce the loss?
is it overfitting?

one of the fold


another fold


","['neural-networks', 'machine-learning', 'algorithm', '3d-convolution']","
There's a few questions here so I'll address them in turn:

What to do with the results of a K-Fold Cross Validation? Generally, a Cross Validation (or any other form of data partitioning) is used as part of a hyper-parameter seach. Your final model would then be the trained using the best hyper-parameters found during the search, and all of the available training data. This final model should then be tested on a held-out set of testing data, which was not using in any stage of training,  to ensure no leakage of information.

How to reduce the Loss? This is a tough question and there is no one direct answer that works in every situation. However improving the performance of a model can be crudely summaraised as: better data, better model, and more regularisation.
Aquiring better data in many contexts is often ""more data"", but there are analyses that suggest data cleaning and rebalancing can cause signficant improvements, but it's hard to measure these improvements. These must be carried out with care to avoid leaking information between the training/testing sets. It is also almost always the hardest thing to do!
Better models can be achieved by changing your architecture or ensembling multiple models. You've mentioned you're using 3D CNNs on video input, so possible better model options while keeping the same overall achitecture are changing the number of layers and experimenting with different shapes of convolutional kernel.
Finally, more regularisation usually helps avoid overfitting and improves your model's ability to generalise to unseen data. This can be achieved by applying layers such as Dropout, Batch Normalisation, or applying a regularisation constraint to your model's weights.

Is it overfitting? Probably, but that's not a death knell for your modelling approach per se. Your graphs show that for these K-fold splits, the performance of the model on the validation set stabilises out as worse than the performance on the training set. If we can assume that the distribution of targets in these validation sets is the same as in the training set, then the model has measurably learned patterns that are unique to the training sets. Thus, overfitting.
However, it's worth checking that this assumption of equal distributions is true - and if not I'd recommend using Stratified K-Fold Cross Validation to ensure that it is true for future experiments to make a fair comparison.


If you re-run with difference architectures, regularisation and stratification (if needed) then the generalised performance of the model may improve. There's a lot of fun experimentation to try from here, and I wish you a healthy dose luck and patience! :)
"
"In the policy gradient method, state dependent baseline does not affect gradient of the objective function. Then how this is better approach?","
In policy gradient theory subtracting state dependent baseline from Q(s,a) does not affect gradient of the objective function. I understand the proof shown below. One things is, if it's not affecting the gradient, then how this approach is better?

","['reinforcement-learning', 'policy-gradients']",
"Reinforcement Learning with PPO - entropy loss dropping, but so is performance. Why?","
I'm using PPO with an action-mask and I'm encountering a weird phenomenon.
At first during training, the entropy loss is decreasing (I interpret this as less exploration, more exploitation, more ""certainty"" about policy) and my mean reward per episode increases. This is all exactly what I would expect.
Then, at a certain point, the entropy loss continues to decrease HOWEVER now the performance starts consistently decreasing as well. I've set up my code to decrease the learning rate when this happens (I've read that adaptively annealing the learning rate can help PPO), but the problem persists.
I do not understand why this would happen on a conceptual level, nor on a practical one. Any ideas, insights and advice would be greatly appreciated!
I run my model for ~75K training steps before checking its entropy and performance.
Here are all the parameters of my model:

Learning rate: 0.005, set to decrease by 1/2 every time performance drops during a check

Gamma: 0.975

Batch Size: 2048

Rollout Buffer Size: 4 parallel environments x 16,834 n_steps = ~65,500

n_epochs: 2

Network size: Both networks (actor and critic) are 352 x 352


","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization']","
PPO is an algorithm in a class of actor-critic methods. In this class of methods, the training is broken in two counteracting parts: the ""actor"" parts learns the policy and the ""critic"" part learns the value (advantage) function for that policy. Such approach suffers from a fundamental instability inherent to all adversarial optimization methods - if one part of the adversarial loop gets way too ahead of another part, then the whole training loop stops.
That looks like what is happening in your description - your ""actor"" bit got too certain about the policy so the ""critic"" bit is unable to progress to better learn the value (advantage) function of the environment.
"
Do we need to know or verify properties of loss functions / metrics' implementations?,"
I will start with an example, in order to get to the general question.
I was reading the following paper (https://www.cns.nyu.edu/pub/lcv/wang03-preprint.pdf) about Structural Similarity Index (SSIM), which is a function used in computer vision. Basically, given two images, it returns (according to some criteria) ""how similar"" these images are.
But what strikes me is that, in the paper, the following is stated: ""we also would like the similarity measure to satisfy the following conditions"".
I'll explain these properties now, but my question is the following:  why we would like a function to satisfy some properties? In other words, what I understand is that it is interesting to prove that these properties hold, am I right?
Some years ago, I used SSIM not only as a metric to measure the performance of some algorithms, but also as a loss function itself. However, I definitively did not know about these properties, so it could be the case that I was optimizing (or measuring my results) with an implementation of the function that does not hold these properties, is that so?
As for the properties, these are straightforward:

Unique maximum:  S(x, y) = 1 if and only if x = y 
Boundedness:  S(x, y) ≤ 1 
Symmetry:  S(x, y) = S(y, x) 

So the problem of ""proving"" SSIM's properties is useful for me for to raise the next (more general) research question: do AI-developers usually now properties of their loss functions/metrics? Are these properties relevant (e.g., are there functions for critical tasks)? Are they already being verified?
I would appreciate some insight about this.
","['machine-learning', 'objective-functions', 'loss', 'metric', 'ssim']",
overcoming the original policy with offline RL,"
I am doing a comparison between RL and a metaheuristic algorithm. What I have found is that online RL does a good work but does not overcome the latter algorithm. In this case generating the samples needed for training the RL algorithm is particularly slow, approximately 1 second. Running the heuristic algorithm takes approximately 5 minutes. I was thinking in using  Critic Regularized Regression with samples and actions generated by the online trained RL policy. My question is, is it possible to overcome the original policy with offline RL?
","['reinforcement-learning', 'deep-rl', 'actor-critic-methods', 'offline-reinforcement-learning']",
Examples where AI fails in revealing ways,"
For a short presentation about AI I am looking for examples where AI failed and therby shows the limits of itself.
I remember there was one examples, where an image classifier was given an image of pink animals (I think sheep) on a tree and classified it as ""Birds on a tree"". I think this example showed what AI might do if the given example is not represented in the training data.
But I cannot find that example anymore (and I need a source).
Anyone knows of exmaples, that are documented that I could give and show the problem in a similar way?
","['computer-vision', 'image-recognition']","
A couple of examples could be:

Image classifiers learning different properties than the actual target: many books reference the case of a perceptron trained on detecting tanks which learned to actually predict good or bad weather in the background, ignoring completely the tanks. First cited in What Artificial Experts Can and Cannot Do.
(notably this is most likely a urban legend, but still a very much realistic situation that anybody working in computer vision will face soon or later).

Amazon recruitment algorithm biased towards men: this is totally real and it has been analyzed in several paper, I'll just link the first one I found Encoded Bias in Recruitment Algorithms. Again another case which remember us that machine learning and AI in general are data driven. An algorithm will learn and always be limited by what's in the data, including stereotypes and prejudices in case of natural language processing.


"
Distributions over outputs for randomly initialized neural networks,"
Does anyone have any pointers to resources about the properties of randomly initialized neural networks (with no training)? I'm guessing this might depend on the network architecture and initialization scheme, but I'm most interested in properties that seem to be mostly true across architectures and initializations.
In particular, I'm wondering what the distributions over outputs look like. Intuitively, one would expect most output distributions to be random, but I'm sure there are also ""adversarial examples"" that could be crafted which leads the NN to output highly-confident predictions.
","['neural-networks', 'weights-initialization']",
What data can I obtain from CNN model (H5 file)? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 11 months ago.







                        Improve this question
                    



I created a CNN model and it is saved in h5 format. I used the Netron app, where I obtained the model architecture, however batchnormalization was not present.
CNN model:
model = Sequential()
model.add(Conv2D(16, (3, 3), input_shape=(height, width, 3), kernel_regularizer=l2(0.001)))
model.add(Activation('relu'))
# BatchNormalization()
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2)))

BatchNormalization()

model.add(Conv2D(32, (3, 3), kernel_regularizer=l2(0.001)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2)))

BatchNormalization()
model.add(Dropout(.5))

model.add(Conv2D(32, (3, 3), kernel_regularizer=l2(0.001)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2)))

BatchNormalization()
model.add(Dropout(.3))

model.add(Conv2D(64, (3, 3), kernel_regularizer=l2(0.001)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2)))

BatchNormalization()
model.add(Dropout(.3))

model.add(Flatten())

model.add(Dense(256))
model.add(Activation('relu'))

model.add(Dropout(.5))

model.add(Dense(4,activation='softmax'))

The output of Netron app:

I have saved this model in h5 format. Is there a method to determine whether this h5 model has a batchnormalization layer or not. Again, is it possible to know the learning rate?
","['convolutional-neural-networks', 'tensorflow', 'keras']",
How do you train a model with low success rate?,"
I'm training a model where the samples success rate is low.
I mean how do I tackle such situation - maybe only show the samples which match but then the ones that doesn't may match too.
But on the other hand if I train it with all the samples it seems like it never fully matches when it should.
Any ideas?
",['training'],"
You have a situation where class imbalance is quite high. There are two ways to tackle this:

Use Focal loss which focuses on underrepresented classes.
Train a Siamese network and allied techniques to it.

"
How to define a fitness function to make sure the best fitness value is 'close to 9' in genetic algorithm,"
I am learning about genetic algorithms (GA), but I encountered a question about the definition of the fitness function used in GA.
I understand that the fitness function should return a scalar value (e.g. to make sure to return the maximum value for a given individual).
For example, if one wanted to optimize a small molecule $logP$ (to ensure that the returned $logP$ has the maximum values), one can define the fitness value thusly: $J(m) = logP$, and then use GA to optimize the $logP$.
But my question is that I want to make sure that the $logP$ has a value close to 9 (rather than the larger the better), the more closer to 9 the better.
In this case how can I define the fitness function used in GA?
Thanks.
","['optimization', 'genetic-algorithms', 'fitness-functions']",
Is data augmentation beneficial even if the dataset is large/diverse enough?,"
I'm training a deep learning model to map binary images to grayscale values of the same shape. For the dataset, I can genearate one as large and diverse as I want it to be.
My question is: let's say the original dataset I created contains 100k images. I can either generate another 900k unique images (so that my training set is 1M in total), or to use data augemntation on the ones I already have and somehow (flipping, rotating, etc.) generate another 900k (I know that in 2D, there's probably only 8 different types of unique images that can be generated by flipping/rotating or a combination, but that's beside the point here).
Which one would you go for and why? Thank you so much!
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'data-augmentation']","
You are going to generate the images by flipping, rotating, etc. which will happen anyways in augmentation. Augmentation can happen on the fly so you don't waste memory storing those new images, thus, you can train your network fast. You can use RandAugment or AutoAugment for augmentation.
"
Why did the authors of D2GAN propose two discriminators using KL and reverse KL divergence losses instead of one discriminator using JS divergence,"
I have stumbled upon the D2GAN paper as part of my research and I am finding myself extremely confused by the fact that instead of using the JS divergence to capture both KL and its reverse, they opted for two discriminators with opposite objective functions. Why is this (if at all) preferable to using just one discriminator that tries to minimize the JS divergence?
","['generative-adversarial-networks', 'kl-divergence', 'jensen-shannon-divergence']",
How does backpropagation work with multi-branch models?,"
How does backpropagation work when the input layer feeds into two or more separate branches of layers before merging back to produce a single output, such as can be implemented in the Keras Functional API with Add/Concatenate layers?

","['deep-learning', 'backpropagation', 'architecture']",
Is there any point in continuing the training of an agent when entropy (of output probabilities) is low?,"
I'm working with a PPO agent with a small, discrete action space (3 possible actions, 1 of which is always masked depending on the state).
Premise 1:
My understanding is that the ""entropy"" of output probabilities is calculated according to the formula given here.
Premise 2:
My entropy during training goes down to <0.05 in less than 100k timesteps. Using the above formula, I roughly estimate that this means the agent is >98% confident in all its decisions at that point.
Question: When the agent is already so confident, is there any point in continuing training? Will it learn anything else, or just keep exploiting the policy it found?
I realize I can run experiments to try to answer this empirically, and I have. It appears that it just stops learning at that point, but I wanted to validate my observations and my premises.
","['exploration-exploitation-tradeoff', 'entropy']",
Any learning method to connect an edgeless graph?,"
Given N nodes with no edges connecting them at all. Each node has certain n features. Is there a way to connect these nodes and form a connected graph. The idea is to then feed the outputted graph into a GNN (Graph Neural Network).
Is there any learning method that learns edges (connection) between nodes given the features?
","['neural-networks', 'machine-learning', 'deep-learning', 'graph-neural-networks']",
Deep reinforcement learning:DQN can't converge,"

i am work on a project.i use the dqn to maximize return. this picture are some env states.
i found that dqn did learn a bit, but after a while it stopped improving and even started to decline.
this my current Hyperparameters:lr 0.01,lr_decay 9e-7,mineps 0.1, maxeps 1,epsdecay 1/25000,memory buffer 200000,discount 0.999,Weight_update_frequency 20step,batch_size 16.
i try a lot.i use the lr:0.001,0.01,0.0001,used eps:0.05,0,02. the lr_decay:1e-7,1.5e-7,1.2e-7,eps_decay:1/10000,1/20000,1/25000,1/40000,1/50000.Weight_update_frequency:4epoch,20step.batch_size:16,32,64.the model size:64,64,128,32,32,64.
Its trend is almost the same, and it has learned something, but then the shaking is very serious, or even declines.

this is current training figure, anyway,it is better than most of my other training figure.
ADD:
source of my dqn is from a book AI-Crash-Course,this is the github site https://github.com/PacktPublishing/AI-Crash-Course/tree/master/Chapter%2011.
the code is in chapter 11.i made Some changes  on the basis of these codes.
about environment:my project is divided in two parts,the schedule algorithm and the DQN part.env will produce many tasks.the schedule algorithm will schedule tasks to a device for processing.The dqn is used to give the best device id.
When a ready task is solved,the environment will change,then the schedule algorithm send the state、reward to DQN.DQN send the device id to schedule algorithm.
action is device id.Range from 0 to 5.
","['deep-learning', 'deep-rl']",
Whats wrong with my resnet50 training on CIFAR10 pytorch?,"
I've been trying to construct resnet50 architecture from scratch using pytorch for classification. After construction I've run training job on CIFAR10 torchvision dataset, in 20 epochs with lr of 0.01 Ive achieved 62% accuracy on validation dataset. Afterwards I've run training job using torchvision.models.resnet50 model on the same dataset which results in slight increase in accuracy, but after searching I've found multiple articles saying that resnet50 achieving accuracy of 90+% on fifth training epoch.
Training code
for i in range(num_epochs):
  train_loss = 0
  correct = 0
  total = 0
  
  net.train()
  for j, (image, label) in tqdm(enumerate(trainloader)):
    image = image.to(device)
    label = label.to(device)
    optimizer.zero_grad()
    out = net(image)
    loss = criterion(out, label)
    loss.backward()
    optimizer.step()
    train_loss += loss.item()
    _, predicted = out.max(1)
    total +=  label.size(0)
    correct += predicted.eq(label).sum().item()
  print('Loss: %.3f | Acc: %.3f%% (%d/%d)'% (train_loss/(j+1), 100.*correct/total, correct, total))

  test_loss = 0
  total = 0
  correct = 0
  net.eval()
  for l, (image, label) in tqdm(enumerate(testloader)):
    image = image.to(device)
    label = label.to(device)
    out = net(image)
    loss = criterion(out, label)

    test_loss += loss.item()
    _, predicted = out.max(1)
    total += label.size(0)
    correct += predicted.eq(label).sum().item()
  print('Loss: %.3f | Acc: %.3f%% (%d/%d)'% (test_loss/(l+1), 100.*correct/total, correct, total))
    

Model code, optimiser and criterion codes:
net = models.resnet50(num_classes = 10, pretrained = False)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate,momentum=0.9, weight_decay=5e-4)

Datasetloading and transformation code:
    transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])



trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

If OHE of dataset labels could be the case? if not please guide be what's could cause low accuracy results
","['pytorch', 'residual-networks']",
What is the difference between a diffusion model and GANs?,"
Recently, I hear a lot of people claiming that diffusion models beat GANs, also providing less training time. I've searched about these two type of models, and I am confused, because somehow they both use gaussian loss to blur and then train to reconstruct the image. They also seem to use autoencoders in the process. Can someone please explain and elaborate in more detail about these two types of models?
",['generative-adversarial-networks'],
Why GANs trained on QM9 dataset produces drug-like molecules?,"
Why the GANs (Generative adversarial Network) trained on QM9 dataset (contains 134K molecules but none of them is complete to be eligible for drug-like molecule) produces drug-like molecules.
sine GANs trained on cats images will generate fake images of cats only then why in-complete drug-like  sample of QM9 fed into GANs can produce complete drug-like molecules?
","['generative-adversarial-networks', 'drug-design']",
Can action space be part of the state space?,"
I'm working on a project where I have access to position coordinates and velocity components of multiple agents in an environment. Assuming that one agent is controllable while others are not controllable, the velocity of the controllable agent represents the action in the defined MDP. can I consider the velocity of the controllable agent as part of the state space as well? i.e. for time step t the state includes the ""current"" velocity of the agent, and the action represents the velocity at the ""next time step"". If this is not possible, then what are the consequences of doing so?
I was looking for references that may give directions to answer the question, but couldn't find something useful, unfortunately.
","['reinforcement-learning', 'state-spaces', 'action-spaces']",
What is the meaning about the $\alpha$ in TD3 algorithm,"
I am study the paper with TD3 algorithm.
I am curious about the meaning of $\alpha$ while the paper prove that overestimation will be happened in a critical situation.
The contents about mathematical proof is like ...
$\exists \epsilon_1 \ s.t \alpha \leq \epsilon_1 \implies \mathop{\mathbb{E}}[Q_\theta(s,\pi_{approx}(s))]\geq\mathop{\mathbb{E}}[Q_\theta(s,\pi_{true}(s))]$
$\exists \epsilon_2 \ s.t \alpha \leq \epsilon_2 \implies \mathop{\mathbb{E}}[Q^\pi(s,\pi_{true}(s))]\geq\mathop{\mathbb{E}}[Q^\pi(s,\pi_{approx}(s))]$
And the overestimation will happened when value estimation at least as large as the $true$ value with respect to $\phi_{true}$
$\mathop{\mathbb{E}}[Q_{\theta}(s,\pi_{true}(s))]\geq\mathop{\mathbb{E}}[Q^{\pi}(s,\pi_{true}(s))]$
Such that overestimation happened when,
$\mathop{\mathbb{E}}[Q_\theta(s,\pi_{approx}(s))]\geq\mathop{\mathbb{E}}[Q^\pi(s,\pi_{approx}(s))], \ \alpha < \min(\epsilon_1,\epsilon_2)$
In my opinion, the value of $\alpha$ just a threshold that can let the equation make sense, right ?
","['reinforcement-learning', 'machine-learning', 'math', 'actor-critic-methods', 'td3']",
Why Phasic Policy Gradient (PPG) can update value function in auxiliary phase?,"
My questions is that how could we train the value network (separated from shared network) by using data from previous policies, which varies a lot since we collect data from different policies with many training phases done to start a auxiliary phase, especially returns are calculated by those policies. Wouldn't it hurt the stability of fitting value function?
","['deep-rl', 'actor-critic-methods', 'proximal-policy-optimization']","
PPG can update the value function in the auxiliary phase because it uses a technique called ""reparameterization"" which allows for the gradient of the value function to be computed more efficiently. Additionally, PPG can take advantage of the structure of the problem to provide a better estimate of the value function.
I think you can train the value network on data from any policy, but
it will have more variance. If you have access to more data, it could help to smooth out the value function.

One approach would be to update the value network after each training phase of the policy network. This would ensure that the value network is always fitted to the current policy. However, this might not be practical if the training phases are very long.


Another approach would be to fit the value network using a rolling window of data from the previous training phases. This would strike a balance between stability and accuracy.

""Wouldn't it hurt the stability of fitting value function?""$\rightarrow$
$\color{red}{\textbf{No}}$, it should not hurt the stability of fitting the value function. In fact, using data from different policies can actually help to improve the stability of the value function estimate. This is because of ""reparameterization"" which allows for the gradient of the value function to be computed more efficiently as discussed above.
"
How to use PCA to reduce the dimension of output features of convolutional layers of CNN,"
I am working on a hybrid CNN-SVM classification using python. I tried to get the final output features of the CNN model(flatten layer) to be fed to the SVM classifier. The output shape obtained from the flatten layer was (600,32786).For computational reasons, I wanted to reduce the shape of the obtained features to (600,250). I was looking for a way to do that without losing relevant features, and I read about the PCA technique.
I saved the output features in a(.npy) file. I tried the following:
X= np.load('features.npy')
pca_250 = PCA(n_components=250)
pca_250.fit(test_images)
Y = pca_250.transform(test_images)

Is this correct?
Regards
","['convolutional-neural-networks', 'support-vector-machine']",
Reward Function for Reinforcement Learning model,"
I am trying to create a reinforcement learning model to control the acceleration of a car. I am designing the model such that initially the acceleration is provided and then deceleration is provided so that the the car covers a particular distance and the end velocity comes out to be zero.
The reward function I have come up till now looks like this:
$$R = (f(x_t) - f(x_{t-1}))*K - \frac{v}{|f(x_t)|} - r_\text{penalty}$$
where $f(x_t) = -|x_t - x_d|$ is the error function.
Based on this reward function, the model learns to provide acceleration. However, it doesn't provide any deceleration causing the velocity to be very high at the end.
The objective of the model is to find the right time when it should change from acceleration to deceleration.
","['reinforcement-learning', 'reward-functions', 'reward-design', 'reward-shaping']",
How to correctly evaluate the state value of this simple markov decision process?,"
For some contexts, I'm working on a c# library for reinforcement learning. I implemented two methods to evaluate a state value function, namely the TD(0) method and the Monte Carlo first visit method.
To test my code, I ran it with a simple, but not trivial markov decision process, described in the image below.

In the picture, you have, in green the rewards, and in red the probabilities of transition. E is a terminal state.
We have two possible actions in state A, either going to C, so let's call it AtoC, or going to B or D, let's call it AtoBorD. The policy we evaluate choose randomly with a 50/50 chance between those two actions when in state A. The policy is implicit for the rest of the states since only one choice is possible.
That's for the context, now my issue is the following. How to properly compute the state value of A ? I tested both my numerical methods, and they both gave a value very close to -8.6, however, when I compute it myself I find -8.93. I first thought it was an issue with the algorithms but since they both give the same answer, my computation must be the issue.
So here's what I did, I'd be grateful if you find where I did wrong.
Going back to the definition, the state value of A is the total expected return when following a given policy :
$$ \mathbb{E}_\pi(G_t|S_t = A)$$
With $G_t$ the total return of an episode from state $S_t = A$ and $\pi$ the policy.
I'll use the following notation for the rest of this demonstration :
$p_{a-c-d-e}$ is the probability of following the path A-C-D-E.
$R_{a-c-d-e}$ is the reward from following the path A-C-D-E.
Going back to the very definition of the expectation :
The set of outcomes for our random variable $G_t$ is infinite, since there's the A-B loop in the Markov process.
The potential values of $G_t$ are :

$R_{a-c-d-e}$
$R_{a-d-e}$
$R_{a-b-a-c-d-e}$
$R_{a-b-a-d-e}$
$R_{a-b-...-a-b-a-c-d-e}$
$R_{a-b-...-a-b-a-d-e}$
...

First we note that the average reward $R_{a-b-a}$ is equal to $-5 \times 0.2-3 \times 0.8= -1.8$.
When looping N times through state A and B, we then expect that $R_{a-b-...-a-b-a} = N R_{a-b-a} = -1.8 N$ (with B appearing N times).
We also have $R_{a-c-d-e}=-10$ and $R_{a-d-e}=-5$.
Taking an arbitrary number of a-b sequences (a-b-a-b-..-a-b) and then adding either a-c-d-e or a-d-e is enough to describe all possible path in this Markov process.
We also note that state transitions are independent of each other, so for example :
$$p_{a-b-a-b-a-c-d-e} =  p_{a-b-a} p_{a-b-a} p_{a-c-d-e} = p_{a-b-a}^2 p_{a-c-d-e}$$
Using all of these, we can explicitly write the expected total return from state A :
$$ \mathbb{E}[G_t|S_t=A] =  p_{a-d-e} R_{a-d-e} + p_{a-c-d-e} R_{a-c-d-e} + \sum_{N=1}^\infty p_{a-b-a}^N  p_{a-c-d-e} (N R_{a-b-a} + R_{a-c-d-e}) + \sum_{N=1}^\infty p_{a-b-a}^N  p_{a-d-e} (N R_{a-b-a} + R_{a-d-e})  $$
Replacing the terms with their numerical values :
Remind that we are following a policy that choose equiprobably between two actions,going to C or going to D or B. Then,

$p_{a-c-d-e} = 0.5$ since it depends purely on the policy choice.
$p_{a-b-a} = 0.5 \times (0.4+0.1) = 0.25$ since it depends on the action chosen and on the dynamic of the system.
$p_{a-d-e} = 0.5 \times 0.5 =0.25$, same as $p_{a-b-a}$.

The reward are already given further up.
$$ \mathbb{E}[G_t|S_t=A] = -0.25 \times 5 - 0.5 \times 10 + \sum_{N=1}^\infty 0.25^N  \times 0.5 \times (-1.8 N - 10) + \sum_{N=1}^\infty 0.25^N  \times 0.25 \times (-1.8 N  - 5) $$
At this point, I got help from wolfram alpha, this gave me the following result : -8.93.
Sadly, both my implementations disagree with me, but what bugs me is that the values are quite close, since they find -8.6.
Do you see anything wrong with my computation ?
","['reinforcement-learning', 'markov-decision-process', 'expectation']",
Machine learning the times tables,"
One of the first tasks a developing human intelligence performs is learning the times tables. This can be seen as filling a 10x10 matrix (12x12 depending on the local traditions), but this blind learning alone would hardly classify as intelligence growth.
I believe intelligence starts, at least in this case, with the ability to rebuild a value that has been forgotten. In a sense, intelligence as a complement for limited and volatile memory. For instance, a kid forgets how much 6X7 is, but remembers 6x6 and applies an equivalent rule to 6x7 = 6x6 + 6. where 6x6 comes from memory, the addition is performed and the result goes to the cell (6,7) for a future lookup, hopefully succesful. This is mathematically equivalent to fitting a linear model, but if we are to define multiplication without using multiplication, the general linear regression is an overkill to learn the times tables.
Instead, we can think of learning the times tables as some process leading to
$kN = (k-1)N + N$

What is the simplest machine learning model that can get such a result?

",['human-inspired'],
Help on Deep Sarsa algorithm that work with pytorch (Adam optimiser) but not with keras/Tensorflow (Adam optimiser),"
I have a deep sarsa algorithm wich work great on Pytorch on lunar-lander-v2 and I would use with Keras/Tensorflow. It use mini-batch of size 64 wich are used 128 time to train at each episode.
There are the results I get. As you can see, it work gread with Pytorch but not with Keras / Tensorflow... So I think I do not correctly implement the training function is Keras/Tensorflow (code is below).
It seems that loss is oscillating in Keras because epsilon go to early to slow value but it work very great in Pytorch...
Do you see something that could explain why it do not work in Keras/Tensorflow please ? Thanks a lot for your help and any idea that could help me ...

Network informations:
It use Adam optimizer, and a network with two layers : 256 and 128, with relu on each :
class Q_Network(nn.Module):
def __init__(self, state_dim , action_dim):
    super(Q_Network, self).__init__()
    self.x_layer = nn.Linear(state_dim, 256)
    self.h_layer = nn.Linear(256, 128)
    self.y_layer = nn.Linear(128, action_dim)
    print(self.x_layer)

def forward(self, state):
    xh = F.relu(self.x_layer(state))
    hh = F.relu(self.h_layer(xh))
    state_action_values = self.y_layer(hh)
    return state_action_values

For keras/Tensorflwo I use this one :
def CreationModele(dimension): 
  entree_etat = keras.layers.Input(shape=(dimension))

  sortie = keras.layers.Dense(units=256, activation='relu')(entree_etat)
  sortie = keras.layers.Dense(units=128, activation='relu')(sortie)
  sortie = keras.layers.Dense(units=4)(sortie)

  modele = keras.Model(inputs=entree_etat,outputs=sortie)
  return modele

Training code
In Pytorch, the training is done by :
def update_Sarsa_Network(self, state, next_state, action, next_action, reward, ends):

    actions_values = torch.gather(self.qnet(state), dim=1, index=action.long())

    next_actions_values = torch.gather(self.qnet(next_state), dim=1, index=next_action.long())

    next_actions_values = reward + (1.0 - ends) * (self.discount_factor * next_actions_values)

    q_network_loss = self.MSELoss_function(actions_values, next_actions_values.detach())
    self.qnet_optim.zero_grad()
    q_network_loss.backward()
    self.qnet_optim.step()
    return q_network_loss

And in Keras/Tensorflow by :
mse = keras.losses.MeanSquaredError(
    reduction=keras.losses.Reduction.SUM)

@tf.function
def train(model, batch_next_states_tensor, batch_next_actions_tensor, batch_reward_tensor, batch_end_tensor, batch_states_tensor, batch_actions_tensor, optimizer, gamma):
  with tf.GradientTape() as tape:
    # EStimation des valeurs des actions courantes
    actions_values = model(batch_states_tensor)                                                          # (mini_batch_size,4)
    actions_values = tf.linalg.diag_part(tf.gather(actions_values,batch_actions_tensor,axis=1))         # (mini_batch_size,)
    actions_values = tf.expand_dims(actions_values,-1)                                                  # (mini_batch_size,1)

    # EStimation des valeurs des actions suivantes
    next_actions_values = model(batch_next_states_tensor)                                                          # (mini_batch_size,4)
    next_actions_values = tf.linalg.diag_part(tf.gather(next_actions_values,batch_next_actions_tensor,axis=1))   # (mini_batch_size,)
    cibles = batch_reward_tensor + (1.0 - batch_end_tensor)*gamma*tf.expand_dims(next_actions_values,-1)         # (mini_batch_size,1)

    error = mse(cibles, actions_values)
    
  grads = tape.gradient(error, model.trainable_variables)
  optimizer.apply_gradients(zip(grads, model.trainable_variables))
  
  return error

Error function and Optimizer code
The optimizer is Adam in Pytorch and Tensorflow with lr=0.001. In Pytorch :
def __init__(self, state_dim, action_dim):
    self.qnet = Q_Network(state_dim, action_dim)
    self.qnet_optim = torch.optim.Adam(self.qnet.parameters(), lr=0.001)
    self.discount_factor = 0.99
    self.MSELoss_function = nn.MSELoss(reduction='sum')
    self.replay_buffer = ReplayBuffer()
    pass

In Keras / Tensorflow:
alpha = 1e-3

# Initialise le modèle
modele_Keras = CreationModele(8)

optimiseur_Keras = keras.optimizers.Adam(learning_rate=alpha)

","['deep-rl', 'tensorflow', 'keras', 'pytorch', 'sarsa']",
Bouding the state using the reward in RL,"
I'm wondering what the common approaches are bounding out state $s\in\mathbf{R}$ to some values $\in[s_0,s_1]$ is required. So in my case, for example, the state represents an angle of rotation, that should Ideally be bounded between $0$ to $\pi$ radians. My naive approach to enforce that was to check the state in every step, and ""punish"" when the state is not contained in this range. With respect to this approach - how the ""punishment"" should actually be implemented? Two options I thought of are:

if $s_t\notin[0,\pi]$: $r_t \leftarrow 0.1$ - This, in my opinion, may not be optimal as there is no indication of how far are we from $[0,\pi]$. Also, there is no incremental aspect - Even if we diverged from $[0,\pi]$ once or 1000 times, the reward is always the same.
if $s_t\notin[0,\pi]$: $r_t \leftarrow 0.9\cdot r_t$ - the longer we stay outside the needed range, the reward tends to zero, but still It doesnt count for distances

Any other common ideas?
","['reinforcement-learning', 'reward-functions']",
Why the x-input should be multiplied by weight in an artificial neuron?,"
So why weight should be multiplied with input?
Yes I know, weight is intended for tuning the connection strength of input that will affect output so that's will be useful for learning (CMIIW).
But why the input should be MULTIPLIED by weight? Why doesn't use EXPONENT operation?
like this,
y_hat = x_1^w_1 + x_2^w_2 + bias

","['neural-networks', 'weights', 'artificial-neuron']",
Problem with OpenAI gym in VSCode,"
I downloaded OpenAI Gym in windows from the console, it did not give me any error, but when I run the file nothing is shown as can be seen in the terminal. So I don't know what to do or what to recommend.

","['python', 'open-ai', 'problem-solving']",
Is BERT capable of producing semantically close word embeddings for synonyms?,"
I am currently working on my undergraduate thesis on matching job descriptions to resumes based on the contents of both. Recently, I came across the following statement by Schmitt et al., 2016: ""[...] [Recruiters] and
job seekers [...] do not seem to speak the same language [...]. More precisely, CVs and job announcements tend to use different vocabularies, and same words might be used with different meanings"".
Therefore, is BERT able to create contextualized word embeddings that are semantically similar or close for synonyms and semantically dissimilar or distant for the same words that have different meanings in the context of resumes and job postings? As far as I understand BERT, this should be the case as long as the same word occurs in a different context/sentence, but I'm not sure.
Thank you very much in advance!
","['bert', 'word-embedding', 'similarity', 'semantics', 'vector-semantics']",
How can rewards and loss calculation be extended to multiple agents in a vanilla policy gradient RL setting?,"
Say I have a simple multi-agent reinforcement learning problem using vanilla policy gradient methods (i.e. REINFORCE) that is currently running with one network per agent. If I can say that each of my agents:

are all of the same class
have ~equivalent environmental contexts (on average)
have no privileged state relative to other agents
performs updates equally to all other agents
DO use LSTMs (but store and reset memory states separately)
DO receive rewards for individual actions based on their individual states

...is it possible to use one network for all agents so as to minimize training time? And if so, how do I combine rewards and generate losses? For instance, if I calculate loss as the -logprob * reward (in the REINFORCE case), could I simply sum or average this over all agents and then backprop accordingly?
","['reinforcement-learning', 'deep-rl', 'reinforce', 'multi-agent-systems', 'multi-agent-rl']",
Can anyone help me how this code extracts features from the graph? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 11 months ago.







                        Improve this question
                    



I have this code from DGCNN Neural Network but i don't understand how it extracts features.
In particular i understand that we get the top knn point but i don't understand the idx_base.
def knn(x, k):
  inner = -2*torch.matmul(x.transpose(2, 1), x)
  xx = torch.sum(x**2, dim=1, keepdim=True)
  pairwise_distance = -xx - inner - xx.transpose(2, 1)

  idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)
  return idx

def get_graph_feature(x, k=20, idx=None):
  batch_size = x.size(0)
  num_points = x.size(2)
  x = x.view(batch_size, -1, num_points)
  if idx is None:
      idx = knn(x, k=k)   # (batch_size, num_points, k)
  device = torch.device('cuda')

  idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points

  idx = idx + idx_base

  idx = idx.view(-1)

  _, num_dims, _ = x.size()

  x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)
  feature = x.view(batch_size*num_points, -1)[idx, :]
  feature = feature.view(batch_size, num_points, k, num_dims) 
  x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)

  feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()

  return feature

","['neural-networks', 'graph-neural-networks']",
How is the noise in the forward process in Denoising Diffusion Probabilistic Models computed?,"
The inputs are decayed towards the origin using this formula within Denoising Diffusion Probabilistic Models (DDPMs):
$$q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right):=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right), \quad q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right):=\mathcal{N}\left(\mathbf{x}_t ; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right)$$
I however do not understand how the origin is determined. How is the noise in the forward process or diffusion process computed? In the original DDPM paper it is only stated that:

the forward process variances $β_t$ can be learned by reparameterization [33] or held constant as hyperparameters.

","['generative-model', 'image-generation']","
You can generate a noise $\epsilon_t$ by sampling a distribution (which you already explicitly stated as $\mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})$) during the sampling phase. The ""origin"" is the standard normal distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$, which is a constant instead of a variable ground truth to learn.
The variable ground truth to learn (using a U-Net) during the training phase is rather the noise $\epsilon_t$ added during the sampling phase. Reverse to the diffusion (sampling) process, the noise $\epsilon_t$, when subtracted from the noisier $\mathbf{x}_{t}$, can produce a less noisy $\mathbf{x}_{t+1}$. Therefore, given $\mathbf{x}_{t}$, the noise $\epsilon_t$ encodes certain information regarding the input data $\mathbf{x}_{T}$.
"
Choosing an appropriate loss function for sparse label proportion estimation,"
I'm working over a task of estimating sparse label proportions, where the target is  probability distribution $\textbf{q} \in \Delta^{K-1}$ and $\Delta^{K-1} := \{\textbf{p} \in \mathbb{R}^K \, | \, p_1 + \dots + p_K = 1 \}$ and the support is relatively small, that is, considering $\mathcal{Y} = \{ k  \, | \, q_k >0\}$, then we have $|\mathcal{Y}| << K$.
I was reading the following paper where they suggest several activation functions with a controllable degree of sparsity, as well as novel loss functions to be employed at training stage. In particular, given

where $\textbf{z} \in \mathbb{R}^K$ is the logit vector and $\bf{\eta}$ denotes the true underlying probability in $\Delta^{K-1}$, I'm wondering whether such loss could be suitable for my problem (stated above).
My main perplexity is that the second term represents a hinge loss which is typical of a classification setting, while my task isn't really about classification.
","['objective-functions', 'multi-label-classification', 'hinge-loss']",
Classification: find out the chemical spectra from a few textual input variables,"
I am dealing with a set of chemical spectras, i.e just an X-Y datapoints plus some labels, that are missing the ""type"" (IR, UV, Mass, NMR etc.).
I am thinking of training a model typed spectras (labelled with the type) and finally be able to classify the ones I don't know the type of.
My current problem is that the features are textual. For example xAxisUnits, yAxisUnits, comment. It'd be great if I could add some numerical values as well, but for the moment textual features is fine.
I have learnt about regression models with images (numerical data), but I am not aware if there is any ""ready to use"" ML-Model that I could use or read about for this type of problem.
I expect to give the array of labels, and the array of features, and then be able to determine the type.
I could just write a long ""if/else"" program but this would be much more general and powerful.
",['classification'],
Why has statistics-based AI become more popular than other forms of AI?,"
What is the fundamental reason that statistics-based AI (e.g., ML and Neural Net) has become more popular than other forms of AI, e.g., Fuzzy Logic and rules-based AI (e.g., Prolog)?
","['fuzzy-logic', 'statistics']",
"How to deal with delay in reinforcement learning, an unclear case","
According to the question in How to deal with the time delay in reinforcement learning?, we can tell the delay in the reinforcement learning can be observation delay, action delay and reward delay.
I have a special case of the delay but I am not sure what kind of delay is it, and how to deal with it.
For example, at a state St0, my agent takes action A1, but we need to wait for a while to gain the reward R1. Meanwhile, my agent keeps taking action A2 and A3. The trick part is A2 and A3 both influence the environment and may affect the R1.
So the timeline is agent plays action A1, A2, and A3, all of them being effective in the environment immediately but we need to wait for a while to see the reward R1, R2 and R3.
Shall we model this question as an observation delay or reward delay?
When my agent receives the R1 but not R2 and R3, can I update my Q-table by eligibility trace or any other kind of method?
","['reinforcement-learning', 'q-learning', 'eligibility-traces', 'delayed-rewards']","
Given that you are not in an episodic setting (based on the comments), I would suggest the following: accumulate rewards for a period of time $T$, that is, at time $t+T$ do as if you had received from the environment a reward $R = \sum_{k=0}^{T-1} R_{t+k}$ .
Then, treat $T$ as a hyperparameter and see which value gives the best agent.
"
What modifications can maximize the efficacy of the REINFORCE algorithm for a policy gradient task?,"
I am straying out of my domain knowledge to attempt a basic reinforcement learning task in a toy environment and have become fairly familiar with the REINFORCE algorithm for policy gradient agents, especially PyTorch's implementation (found here). It is clear to me now that there are superior methods to train RL agents (PPO for instance), but as I read, these feel beyond my current intellectual or time resources. As such, I'd like to eek out as much power through modifications of REINFORCE as possible before determining how I might move on.
As such, are there modifications to the REINFORCE training algorithm that might yield benefits without straying far into new algorithm territory? Or perhaps, what is the ""SOTA"" version of REINFORCE?
For instance, perhaps a simple gradient clip in some way approximates some of PPO's benefits? Or maybe setting a baseline reward based on a rolling reward set of previous episodes?

If a specific context is useful, I'm applying this to a self-made simple grid environment where agents receive points namely for moving closer to and acquiring ""targets."" There are other rewards of lesser significance, but key is the environment is set for a sort of ""continuous play"" such that agents are very frequently receiving rewards (due to closeness) and occasionally receiving reward spikes (due to getting a target), but there is no true episode definition other than an arbitrary timestep length. I am not using batches (perhaps that is useful?), as I have found that gradually stepping up the episode length allows the agents quicker access to simpler rewards and appears a sort of scaffolding to more complex behavior. Agents might reasonably gather the ""large"" rewards in as few as 10-25 steps. Generally things are working fine, and I am most interested in how to extract as much value from the agent updating mechanism (REINFORCE) as possible.
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'reinforce']",
keras model accuracy not improving,"
I am trying to do multi class(16) classification, however no matter what parameters or number of layers I use my accuracy is not improving, its in 30s the max I got was 43.
I have tried early stopping to red overfilling but my testing accuracy is still low.
I have 750 images in training and 350 in testing. I am also getting high traning accuracy vs low validation accuracy.
features_train=features_train/255
features_test=features_test/255

cnn = models.Sequential([
    layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu',  strides=(2, 2), padding=""same"", input_shape=(224, 224, 3)),

    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),
    layers.Conv2D(32 ,(3, 3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),
    layers.Conv2D(128, (3, 3), activation='relu'),
 
    layers.MaxPooling2D((2,2)),
    
    
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dense(16, activation='softmax')
])
cnn.compile(optimizer='adam',
             loss='categorical_crossentropy',
             metrics=['accuracy'])
cnn.fit(features_train,labels_train,epochs=20, batch_size = 4 ,validation_split = 0.25)

","['convolutional-neural-networks', 'keras']","
Based on having 16 classes, the naïve accuracy of a dummy classifier would be around 1/16 (6.25%) assuming class balance and the dummy classifier predicts each class according to uniform probabilities (see scikit-learn article on dummy classifiers).  Therefore, your model is likely learning something useful beyond baseline.
Depending on the complexity of your images, your training set having only 750 images may be on the low side and may be the possible/likely culprit for low accuracy.  Computer vision applications often require thousands of images for training.  For example, see CIFAR-100, CIFAR-10, MNIST digits, and MNIST fashion datasets, which have thousands of images.  For these large datasets, the achievable accuracy decreases as the complexity of the images increases (compare MNIST digits vs. MNIST fashion) and as the number of classes increases (compare CIFAR-100 vs. CIFAR-10).
Here is what I suggest for diagnosis:

Plot training and validation loss and accuracy vs. epoch to assess whether your model is overfitting / underfitting.

Determine the accuracy achievable by humans on the same dataset to assess what type of accuracy may be reasonable.  Of course, AI can outperform or underperform humans, so take this with a grain of salt.

If this a public dataset, search the literature for what accuracy other investigators have achieved.


Here is what I suggest to boost accuracy after you have run the diagnostics:

Consider generating more real data and/or using Data Augmentation.

Tune hyperparameters based on whether you are overfitting or underfitting.

Consider using a pre-trained model.


"
"In typical hybrid loss (classification loss + bounding box regression loss), why should bbox loss be 0 when IoU is less then a thresold?","
I was reading this(Object Detection in 20 years) review paper and had this little question.
In the paper a typical hybrid loss function for classification+boundingBoxRegression is

A typical loss of each predefined anchor box consists of two parts: 1) a cross-entropy loss for category recognition and 2) an L1/L2 regression loss for object localization. A general form of the loss function can be written as follows:
$$
\begin{eqnarray}
L(p,p^*,t,t^*) &=& L_{cls.}(p,p^*)+\beta I(t)L_{loc.}(t,t^*)\\
I(t) &=& \left\{\begin{array}{ll}
1 & \text{IOU}\{a, a^*\} > ƞ \\
0 & \text{else}
\end{array}\right.
\end{eqnarray}
$$
where t and t* are the locations of predicted and ground truth bounding box, p and p* are their category probabilities. IOU{a,a*} is the IOU between the anchor a and its ground-truth a*. ƞ is an IOU threshold, say, 0.5. If an anchor that does not cover any objects, its localization loss does not count in the final loss.

The question is if the localization loss is 0 for false boxes, wouldn't the loss have local minima when the model got classification right but have a ridiculous bbox?
Thank you.
","['classification', 'object-detection', 'loss', 'bounding-box']",
Can MSE be used for NN categorical classification problems,"
I currently have a neural network that can manage to perform polynomial (single output) regression problems. I now want to upscale to classification problems (eg: image recognition). Can I do this with MSE loss, or do I need to switch to a more complicated categorical cross-entropy loss function...?
","['neural-networks', 'machine-learning', 'classification', 'loss', 'stochastic-gradient-descent']",
Non-Convex loss-surface although quadratic loss function,"
there is one problem which bugs me quite a long time, it is the non-convex loss shape (multiple minima, e.g. shown here) of neural networks which use a quadratic loss function.
Question: Why is a “common” AI problem usually non-convex with multiple minima, although we are using e.g. quadratic loss functions (which is in lectures usually drawn as a simple,convex,quadratic function such as x^2)?
My guess:
Is it because we are feeding the loss function with our highly non-linear model-output and therefore the resulting total loss surface is highly non-linear/ non-convex? Specifically, the quadratic loss is just approximating the infinitesimal small neighbourhood around a specific point (minibatch) as quadratic. Is that guess correct? This would imply, that highly-non linear / very deep and complex models have a highly-non linear resulting loss-surface, while shallower models have less minima and a one-layer network has a convex shape ?
","['machine-learning', 'deep-learning', 'loss', 'convex-function']",
How to train a N binary classifier with shared encoder as replacement of multiclass classifier?,"
all I have data for 3 categories (positive and negative category).
For my solution, I can build a multiclass classifier with 4 classes (positive data from each category and negative data combined across all categories becomes 4th class)

But I want to experiment and split the problem to 3 binary classifiers with shared encoder:

My question:
How should I train the data with a mixed batch that passes thru encoder?.
Say my batch in training contains:

1 label 1 example for class 1
1 label 0 example for class 1
1 label 1 example for class 2
1 label 0 example for class 2
1 label 1 example for class 3
1 label 0 example for class 3

So, in short my batch has 6 examples. Then how should I train the architecture?
My approach:
Each linear layer will see all the data from the batches. But we will train it only for label 1 examples of that class and remaining examples as class 0.
So, linear layer for class 1 will be trained on following (if above batch is input):

1 label 1 example for class 1 as label 1
1 label 0 example for class 1 as label 0
1 label 1 example for class 2 as label 0
1 label 0 example for class 2 as label 0
1 label 1 example for class 3 as label 0
1 label 0 example for class 3 as label 0

Then, I will get N losses across N linear layers. Should I take mean and update entire network with mean loss to be logically correct?
",['multiclass-classification'],
How to filter out class for which the model has not been trained in ml web app?,"
I have developed an python based ml web app. It gives details of the book from a image of book cover.
Problem: When I upload the book cover image then it works but when i click image of any random object, it still gives the result related to some book in the dataset. How can i detect or filter out such images and tell the user that the image doesn't contain book cover?
Somewhere I read that we need to add some garbage classes (other classes I am expecting users will upload like person, objects etc.) in the training process. How can we do this?
","['machine-learning', 'computer-vision', 'python', 'mlops']","
One approach would be to train a classifier that will take an image and return a binary prediction of whether it is a book cover or not. This can be done in a supervised fashion. You will use this classifier on every given image input and proceed to the next step of returning details of the cover if and only if the classifier predicted that the image is indeed a book with some probability $p>threshold$, which can be predefined.
As this process is generally an overhead to the actual goal of your app, I would try using basic classifiers first, while also providing a low-dimensional input - I suppose that much can be learned from edges only
"
How do I derive Sutton and Barto Equation 3.14?,"
I'm having trouble going to the 2nd to last line of (3.14),
http://incompleteideas.net/book/RLbook2020.pdf#page=81
$$
\require{enclose}
\begin{aligned}
v_{\pi}(s) & \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right] \\
&=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right] \\
&=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}} \sum_{r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \mathbb{E}_{\pi}\left[G_{t+1} \mid \enclose{circle}[mathcolor=""red""]{S_{t+1}}=s^{\prime}\right]\right] \\
&=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right], \quad \text { for all } s \in \mathcal{S},
\end{aligned}
$$
I don't understand where the red circled term comes from. Namely, where the $S_{t+1}$ comes from, since I was expecting an $S_t$ from the previous line.
Can you please explain? Thank you.
","['reinforcement-learning', 'machine-learning']","
for first, let's see what $S_{t+1}$ means: that means from next time-step.
please remember what was V(s) define: what you expect to gain from this state($S_t$) until end of trajectory. so, you need to know for each action what is expected reward and that is the probability of next state occurrence if you choose action $a$ in state $s$ ($p(s',a|s,a)$) multiply in (reward and next state value function). in fact it is a bootstrapping formula:
python code:
values = list()
for s in P:
    values.append(list())
    for a in P[s]:
        values[-1].append(sum([prob * (reward + gamma * V[next_state])
                                  for prob, next_state, reward, _ in P[s][a]]))

mind that P is $MDP$
"
How to add pretrained vectorizer and classification model to onnx pipeline for NLP task?,"
I have trained Logistic regression model for documents classification. I have saved the fit tfidf vectorizer and the model as well. Also, I have created a custom DataCleaner class which inherits BeseEstimator and TransformerMixin. It is basically a custom transformer that allows me to preprocess new data before feeding it in the trained model for classification.
The problem is that now I have to upload it in Microsoft ML.NET and it all has to be organized in an ONNX format.
I have no idea how to structure pipeline with pretrained vectorizer and ml model. I don't even know if this is possible. I would appreciate clarification on this.
Here is what I have:
class DataCleaner(BaseEstimator, TransformerMixin):  # takes dataframe 

    def __init__(self, variables):
        self.variables = variables

    # more functions here 

TfidfVectorizer(ngram_range=(1, 2))  # new data only .transform
LogisticRegression(C=10.0, class_weight='balanced', max_iter=1000)

","['machine-learning', 'natural-language-processing', 'text-classification', 'multiclass-classification']",
Recommended ways of building a chatbot/autoresponder for a recommendation system,"
What are some recommended ways (with pros and cons) to implement a chatbot/autoresponder bot for an enhanced recommendation system? In terms of architecture, data, etc. Couldn't find anything on building such bots for marketing purposes, recommendation systems. etc.
","['natural-language-processing', 'chat-bots']",
What are the set of actions in reinforcement learning?,"
I'm reading https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf
In 2. Preliminaries, they claim that the reward function takes a state in S to an action
$$R : S \rightarrow A$$
But in the next paragraph, they also define $R^∗(s)$ so that it is
$$R : S \rightarrow \mathbb{R}$$
Could you give intuition for what the actions are and why it's defined this way? Why is it a number, and how is that number used?
More specifically, is first definition $R : S \rightarrow A$ wrong?
","['reinforcement-learning', 'machine-learning']",
BERT predictive distribution for non-masked tokens,"
In the BERT paper, the masking regime is described in the following way:

The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.

This seems to imply that when a trained BERT is asked to produce a distribution for a non-masked token, it should assume that with probability at least 50% this was the same token (probability that the token was not changed + some residual non-zero probability that the token was changed into itself since softmax cannot produce exact zeros). This, in turn, should give rise to very sharply peaked predictive distributions for non-masked tokens, i.e. their entropies should be close to zero. It is easy to check with some pretrained models that this is not the case: predictive distributions for non-masked tokens can have non-trivial entropies. Is this due to undertraining or is there a fundamental limitation in the model that doesn't let it converge to this outcome?
","['natural-language-processing', 'bert', 'language-model']",
How to combine pretrained language models with additional feature set?,"
Are there any techniques to combine a feature set (other than the text itself) with pretrained language models.
Let's say I have a random NLP task that tries to predict a binary class label based on e.g. Twitter data. One could easily utilize a pretrained language model such as BERT/GPT-3 etc. to fine-tune it on the text of the tweets. However the tweets come with a lot of useful metadata such as likes/retweets etc. or if I want to add additional syntactic features such as POS-Tags, dependency relation or any other generated feature. Is it possible to use additional features I extracted for the finetuning step of the pretrained language model? Or is the only way of doing so to use an ensemble classifier and basically write a classifier for each of the extracted features and combine all of their predictions with the finetuned LMs predictions?
","['natural-language-processing', 'bert', 'fine-tuning', 'language-model']","
Although barely related, this brings to my mind a concept I first learned in conditional GANs - there, if you wanted an output of some specific class, you would take your random noise input $z$ (that is given to your generative model $G$) and concatenate it with some representation of the class instance. On the MNIST digits dataset, for example, you can provide $[z,(1,0,...,0)]$ where $(1,0,...,0)$ represents a one-hot encoding of the digit $0$. So perhaps you could use this concatenation concept for your task:

extract features $\{a_i\}_{i=1}^N$ from the text itself - this can be done using a transformer, as suggested, or some RNN architecture like LSTM or GRU.
extract features $\{b_i\}_{i=1}^N$ from the metadata - there are some approaches that deal with tabular data, but I would start with a basic MLP
concatenate the two feature vectors $c_i=[a_i,b_i]$
yield a final prediction using yet another model that takes $\{c_i\}_{i=1}^N$ as input


Edit: I've looked this up online, and found some supporting evidence - check out this blog post, for example.
"
Deep Learning model to predict ELO chess rating from games?,"
I was wondering if anyone had already tried building a regression model that could predict the ELO rating of a chess player based on his last N games? Inputs could be anything, from chess notation of the plays to Stockfish's analysis of the moves. (Excellent/Innacuracy/blunder/...).
Basically, anything but the actual way to calculate ELO, which requires (if I understood correctly), to get the wins/loss of other players and their respective ELO. And I think it's impossible for a player to get to ELO 2500 (for example), in only 10 or 20 games.
I might have misunderstood some intricacy of the ELO system which makes my question unfeasible, forgive me if so.
Thank you.
","['deep-learning', 'chess']",
Clustering similar words with pre-defined clusters,"
I parse many text messages daily that contain product names. My parser has pre-defined products that it looks for, however I would like to automatically cluster all new products in new clusters but pre-define the existing clusters.
For example, if my parser already includes logic for product PlayStation, then I want to pre-define this cluster so all words that are similar to it such as PlyStation or PlayStat get clustered with it, while products that I are not in my parser have a new cluster created for them.
I could then review every week the clusters and find for example a new cluster with a bunch of similar words Computer, Cmputer, Ccomputer, Comp. I can then easily add this to my parser.
Leveshtein distance + k-means is a popular way to cluster words, how can I pre-define the clusters though?
","['natural-language-processing', 'clustering', 'text-classification']",
Is there any work done on topic agnostic binary topic classification?,"
In the recent preprint paper Tree-based Focused Web Crawling with
Reinforcement Learning a new model is introduced to classify web pages called KwBiLSTM.
The input to this model is a featurized webpage which, later in the forward pass, is concatenated with a featurized version of a list of keywords. Together, these features are passed to the classification layer, which determines whether the webpage and the keywords correspond to the same topic (label 1 or 0).
From literature research I could not find any other work using this novel approach, namely that the topic being classified is supplied as an argument, rather than learned explicitly by providing a set of webpages labeled for a certain topic.
My question is: Are there other works using a similar topic-agnostic binary topic classifier?
","['reference-request', 'papers', 'model-request']",
Which positional encoding BERT use?,"
It is a little bit confusing that someone is explaining that BERT is using sinusoidal functions for BERT position encoding and someone is saying BERT just uses absolute position.
I checked that Vaswani 2017 et al., used a sinusoidal function for position embedding, but not sure BERT used the same technique since the BERT paper does not mention it much.
My question is

Did BERT use absolute position embedding that is learnable, not with sinusoidal function?
If so, is the absolute position the same dimension as the input embedding dimension for an addition?

","['bert', 'positional-encoding']","
BERT uses trained position embeddings. The original paper does not say it explicitly, the term position embeddings (as opposed to encoding) suggests it is trained. When you look at BERT layers in HuggingFace Transformers, you will the dimension of the trained positions embeddings (768×512), which is also the reason why BERT cannot accept input longer than 512 tokens.
from transformers import AutoModel
model = AutoModel.from_pretrained(""bert-base-cased"")
print(model.embeddings)

BertEmbeddings(
  (word_embeddings): Embedding(28996, 768, padding_idx=0)
  (position_embeddings): Embedding(512, 768)
  (token_type_embeddings): Embedding(2, 768)
  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

"
How is reinforcement learning applied in the real industry?,"
I'm a newbie to reinforcement learning.
While studying reinforcement learning, a question arose about how to apply reinforcement learning in the real world.
Assuming that a reinforcement learning agent trained in simulation is used in the real world, do they usually only use the optimal policy learned in the simulator? Or are they need additional exploration (learning) in the real world?
In my view, the latter approach is quite impractical in real-world environments where failure by agents' actions is fatal. However, it also seems very difficult for the simulation environment to reflect the real world.
What approach is being adopted by the industry field?
","['reinforcement-learning', 'deep-rl', 'training']","
RL is not used much in the real industry, as you said because of safety concerns. There are perhaps three different ways of how this would be possible

Use safe exploration to learn a model of the environment and apply model predictive control algorithms on the learned model. This seems to be the approach taken for data center cooling using model-predictive control.

Design an accurate simulation model as a digital twin and use it to train policies there before deploying into the real world. This was the approach taken in the recent breakthrough in nuclear fusion: Magnetic control of tokamak plasmas through deep reinforcement learning

Use existing controllers to generate data sets and train RL agents with offline RL (or perhaps imitation learning) and use it as a prior before finetuning it in the real world.


Now, as you mention there is the sim-to-real problem if you train in simulation. The real world usually comes with various uncertainties that can not be simulated well. This may be not extremely relevant for controlled industrial environments but is important in robotics. We can divide the training process into multiple phases, where the agent has less and less information available. The idea is to train a belief model, which aims to assess all the uncertainties and safely adapt to them. An example is:
Learning robust perceptive locomotion for quadrupedal robots in the wild
"
Arc Consistency in a 6-Queens Problem,"
Consider the following board:

The exercise asks to apply the AC-3 algorithm, given that the domain of $v_1$ and $v_2$ is fix with $v_1 = 2$ and $v_2 = 5$. For the remaining domains, we should initially consider the whole domain $v_3 = v_4 = v_5 = v_6 = \{1,2,3,4,5,6\}$. The constraints are, that no value should be affected vertically, horizontally or diagonally by any other value.
First, I want to explain how I tackled the problem:

Starting with the variable $v_3$, we can see that the values $\{2,4,5,6\}$ are not arc consistent with either $v_1$ or $v_2$, hence after the first iteration we have an updated domain of $v_1 = \{1,3\}$ and would add each constrain affecting $v_3$ back to the queue.

Moving on to $v_4$, we see that $\{2,3,5\}$ conflict with either $v_1$, $v_2$ or $v_3$, and thus we have an update domain $v_4 = \{1,4,6\}$. Again, we add each constrain affecting $v_4$ back to the queue.

Moving to $v_5$, we see that only $\{4\}$ is valid. We have removed values from that domain, so we add every constraint back to the queue.

At this point, I would consider $v_6$ and just repeat the procedure, however, the solution considers $v_4$ again, and I cannot possible see why.


Am I doing something wrong with the queue? Even if it was a FIFO queue, that the solution considered, I expect $v_3$ to be checked again before $v_4$.
I will also upload the solution for reference:

 

","['constraint-satisfaction-problems', '8-queens-problem']",
How do I implement the 'gradient clipping' in the Neural Replicator Dynamics paper?,"
The paper is here https://arxiv.org/pdf/1906.00190.pdf
and the relevant paragraph where they explain their method is below:

It's still not clear to me how this is meant to work exactly. In the pseudo-code, the formula for the new NeuRD policy update is given in the all actions case $\theta_{t+1} \leftarrow \theta_{t} + \eta \sum_{a} \nabla_{\theta} y(a) \ A_{\pi}(a)  $, the components of which are just the gradient of each logit ($y(a)$) times the advantage of that action. The NeuRD update rule as I understand it is just deep exp3.
But then is the paragraph from the paper meaning that: for each minibatch and each $a, y(a)$, we compute the new model parameters $\theta_{t+1, a}$ which are given by using only the term in the original update corresponding to $a$. Then we see if the logits $y_{\theta_{t+1, a}}(a)$ of the minibatch forward pass are out of bounds $[-B, B]$ (either the mean or max logit). If the logit is out of bounds, we do not use that component of the update. Note this not prevent logits from being pushed out of bounds by and update from another action.
This is my simplest iterpretation of their description and it seems terribly expensive. I'd have to update and un-update the network $k$ times for each minibatch.
At least in the tabular case (the NeuRD update rule is just exp3), clipping is necessary to prevent a logit from diverging to $- \infty$ for an action that is dominated. This context also suggests we can't ""cheat"" by just considering if the whole update pushes any logit out of bounds, and scrapping the update if so. In the tabular case, this method would eventually zero all gradients once any logit gets pushed out.
Also I'd like to comment that ""gradient clipping"" is a poor choice of name. ""logit clipping"" seems from accurate.
","['policy-gradients', 'pytorch', 'research', 'deepmind']",
"Deep reinforcement learning the board game ""Battle Sheep"" - too large action space?","
I was recently introduced to this simple board game called ""Battle Sheep"". In this game, two to four players try to acquire as many hex tiles from a hex grid as possible.
You can find the complete rules here.
I decided to program a game engine for this game. You can find it here. I also created a SFML-based graphical interface for the game, which can be found from the same repo - here.
The reason I created the game engine in the first place was a challenge to myself - I wanted to create an AI which would have superhuman skills in this game, since I thought that the game is pretty simple and it would be easy for a neural network to master it. I have been studying the work about deep reinforcement learning networks, especially Leela chess zero (Alphazero) and AlphaGo. I thought that a DQN should be possible to master the BattleSheep game, but I have stumbled across a problem:
Let's consider a game with 2 players. Since the game has two phases (map creation and the gameplay itself), the action space of the game is unknown at the start because the two players can place their pasture boards as they wish as long as it follows the rules (see the link above). In my game engine, I have solved this problem by creating large-enough map so that any kind of piece placement combination in the map creation phase will fit inside of its bounds. During the map creation phase, the chosen hexes are set ""active"" (defined HEX_FREE in my engine). ""inactive"" hexes cannot be used, or could be considered ""out of bounds"" during gameplay (defined HEX_OOB in my engine). For this code, see the constructor of my Map class.
Now the question itself: Deep Q-learning requires a fixed-size action space, which is hard to define in BattleSheep, since the map is different in every game. One possibility would be to create an action space that consists of the complete map (including these ""inactive"" hexes in my game engine), and define every possible move in that map. However, if we consider a map with for example 40 width and 40 height, the action space would be
40 x 40 x 6 x (40 x?) 16

40 x 40 - from which hex the player will start the move
6 - to which direction the player will move
40? - how far the pieces will move (is this required since you are always forced to move as far as possible?)
16 - how many pieces the player will move
Without the extra x40, the action space will yield 153600 different moves, and with the extra 40 (move length), the action space will have 6.1 million different possible moves. This is surely too large. How should I tackle this problem?
","['reinforcement-learning', 'deep-learning', 'dqn', 'action-spaces', 'board-games']",
Can TRPO use replay buffers?,"
I understand that TRPO is a on-policy RL method and that it optimizes an expectation of the advantage or accumulated returns function over actions taken according to policy $\pi$.
Is it possible to use a replay buffer where the policy can be trained with returns gathered with old policies?
Would importance sampling and the KL divergence constraint prevent the policy from making bad updates?
Also, in PPO, it is stated in the algorithm that the policy is trained with the same trajectories over multiple iterations. Is this applicable for TRPO as well, seeing both algorithms are relatively similar?
","['reinforcement-learning', 'proximal-policy-optimization', 'experience-replay', 'on-policy-methods', 'trust-region-policy-optimization']",
How many parameters of a RL environment?,"
I’m working at a Reinforcement Learning model, using PPO algorithm, in which the agent has 4 possible actions, acting in a stochastic  environment defined by 3 parameters. 
Given its stochasticity, I was thinking it would be better to store also the parameters of the previous step, ending up in 6 parameters. 
My question is: when these parameters are too many? If I store the previous 5 results, I end up with 5*3=15 parameters, thus it could be more complicated to reach a local maximum.
When do you think I should stop? What’s the reasoning behind it?
","['reinforcement-learning', 'proximal-policy-optimization', 'intelligent-agent', 'environment']",
How to incorporate domain knowledge into a semantic segmentation network?,"
I'm working on a semantic segmentation project, and want to add some domain knowledge to the system. I want to ensure that for segmentation, there can only be one group of pixels that are predicted as a certain class. An example would be segmentation of the face. When we want to segment different parts of the face, assuming regular faces, we have the following domain knowledge: one group of pixels can be predicted as the mouth; one group can be predicted as the nose; two groups of pixels can be predicted as the eyes.
A possible solution I currently think of is as follows:
Suppose we find two groups of pixels, both groups having pixels with the highest probability for being the nose. Then, we would take the average score for each pixel group to obtain two average scores. Then we compute the probabilities again and select the group with the highest probability to be the nose. The group of pixels having the lower probability will then be predicted to be the class having the second best probability. Doing it in this way, we apply some post-processing (look at scores and determine what is our final prediction, using some logic). However, it does not seem to be learned by the neural network (or maybe it is learned implicitly, by looking at the final predicted class obtained from the postprocessing and updating the weights accordingly).
Are there any solutions, or could you point me to any papers which describe this problem and add domain knowledge to the neural network such that it incorporates this information and becomes better at predicting?
","['deep-learning', 'image-segmentation', 'semantic-segmentation']","
You could come up with a loss function which describes your constraints.
PS: your example with the face recognition (where a CNN would find eyes no matter how nonsensical they are placed) is probably solved by visual transformers
"
What is the loss function and training task on which the original BERT model was trained,"
I was checking on sentence embeddings and stumbled across the BERT model which employs transformers.
I understand that BERT applies a WordPice tokenizer (e.g. working like https://keras.io/api/keras_nlp/tokenizers/word_piece_tokenizer/) and then passes the tokens through several (transformer) layers.
If using the transformers library, the output of each hidden layer can be accessed easily as described here  https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/ .
For each token we can then obtain a word embedding and aggregate a sentence embedding by e.g. mean- or max-pooling over all word embeddings in a sentence.
On https://d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html , I found that BERT can be trained on e.g. WikiText-2 (https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/ ) but I do not see

on which training task
which loss function the original BERT model is trained?

This is curcial, since it determines what pattern the model picks up.
The last website states that the loss function is a cross-entropy loss. But I do not yet understand corss-entropy between what? What is the (X,y)-pairs used for training Bert?
",['bert'],
Evolutionary Programming: Self-Adaption (Multiplicative methods) Where is the self-adaption?,"
Good day.
I have been playing around with Evolutionary Programming (Evolutionary Algorithm variation) lately, specifically self-adaption where the strategy parameters evolves with the solution.
With the additive technique, the theory gives (Fogel, et al, 91):

Here sigma is the strategy parameter with eta the learning rate and N() some normal distribution of random numbers to adapt the strategy parameter with the solution.  It makes sense as this is clearly a form mutation.  You use the existing strategy parameter and changes it slightly.
With multiplicative technique, the theory gives (Jiang and Wang, 05):

With t the current iteration, n_t the maximum allowed and the lambdas all static constants. The previous value for the strategy parameter is not even used, but only the initial value, sigma(0).
This does not make sense to me at all.  There is no random component and the strategy parameter is deterministic.  Can someone please shed some light here?  How does this adapt with the solution?  This feels more like a ""dynamic"" approach.
","['evolutionary-algorithms', 'evolutionary-computation']",
How to improve the performance when no shuffling of dataloader is needed?,"
I'm currently doing some researches on video recognition. What I'm trying to do is like this paper.
The idea is that: for processing a specific input video clip (shape: [T, C, H, W]), it needs features of the video clip from last timestamp, where we are trying to build a long-time feature memory.
As a result, we have to sequentially read consecutive video clips, and the dataloader will output the data like this: (the number represents the timestamp of a video clip, and each row represents a batch)
[1,  9, 18, 27, 36]
[2, 10, 19, 28, 37]
[3, 11, 20, 29, 38]
[4, 12, 21, 30, 39]
...

The thing is, due to the sequential reading, I couldn't do shuffling for the dataloader, which unsurprisingly results in a pretty bad performance.
For such streaming loading problem, are there any tips to improve the bad performance caused by no shuffling?
Noted: I have done the following experiments to verify that the problem is really caused by no shuffling:

original dataloader with shuffling: good accuracy
original dataloader without shuffling: worst accuracy
the dataloader I mentioned in the question: medium accuracy but still bad

","['python', 'pytorch', 'video-classification', 'action-recognition']","
There are no ways to make the model or training pipeline adapt to the sequential loading of data, since the model pipeline is pretty much independent of the data pipeline due to minibatching.
There are however some ways I can think of to handle random shuffle of video segments.

First, is to convert all the videos into uint8 4d numpy array of (T, H, W, C) dimension, then you could use memmap to read a segment from it during dataloading, which would be very trivial in cost. However, the downside is obviously you need a lot of disk memory to do this. In particular, from mp4 to npy would almost give you an 100x memory increase.

You can pre-slice the videos into segments of length that your model expects. This would allow you to shuffle the segments easily.

Use a very efficient video segment processor such as opencv, ffmpeg, or the recent PyTorch video loading API. However, this is still bound to be slow, as you are randomly accessing the middle of a video segment.


"
How to get sentence from embedding vector with Universal Sentence Encoder?,"
Given a sentence embedding vector from a Sentence Encoder (like Sentence-BERT), I want to train a model to generate the original sentence (list of word embedding).
Are there any architectures to perform a similar task?
","['deep-learning', 'natural-language-processing', 'transformer', 'attention']",
Generator for a string containing letters and digits with unknown output,"
in preparation for a new project i would like to ask for some help finding an approach.
The Task: Some kind of material-number in System A (numbers&letters of length X) need to be mapped to material-numbers in System B (numbers&letters of length Y) or suggested as a new material-number.
Input: A string (material number in system A, consisting of X characters) and some or many other features (probably categorical) describing the string
Output: A string (material number in another system B). This can be an already existent material-number in System B or a suggestion how a material number of the material part in system B would be.
Data: Thousands of manually mapped material-numbers are avalailable.
Format of the material-numbers: The material-numbers in System A and B are divided into different parts. This parts can contain specific logic (like the build-in-location or version of the material). Also, there are random or unknown parts. Nevertheless, the logics of how the material number from system A is build differ from those in system B, so that a direct assignment is not possible. e.g.(short form):
Idea/Problem: I think that it must be possible to let a model with many thousands of examples learn how to create such numbers. I got some experience with supervised classifications. However, since here the output is not known beforehand and often has to be completely regenerated, I need a different approach. e.g. Seq2Seq-Model or the CTAB-GAN? will this be the right direction?
Thanks for any help and ideas you can provide.
","['machine-learning', 'generative-adversarial-networks', 'sequence-modeling']",
How is a filter actually applied to all input channels in a ConvLayer2D,"
I was studying Convolutional Layers and some of their variations and I came across this post which says:
'For rgb vs greyscale, think about channels as feature maps for input layer and a filter gets applied on all feature map at once.'
I also watched some videos that say the same thing, however I can't quite understand how the process actually happens. Let's look at the image below:

Now, lets pretend that we feed an RGB image to a ConvLayer2D. Since we have an RGB image we also have 3 input channels, one for each color. Lets also suppose that the ConvLayer has 32 filters. From what I understood the output of this layer will be 32 feature maps which means that each filter will apply convolutions to each of the 3 input channels and combine its convolved feature into one feature map related to that same filter. But how does that mixing/combining actually work? Does it produce 3 convolved feature maps (one for each channel input) and then combine its elements by some adding and dividing each elements of said convolved features before putting them into the final feature map? Or does it achieve the 32 feature maps in some other way?
For example: ((Red-i00 + Green-i00 + Blue-i00) / 3 = FinalFeatureMapOfThatFilter-i00 )
Where i00 represents the top, leftmost position of the related convolved feature.
Thanks!
","['deep-learning', 'convolutional-neural-networks', 'convolutional-layers']",
Should the concept of discounted rewards result in multiple arrays per episode in RL?,"
Note that I'm coming from mostly only working with the REINFORCE algorithm, but I've typically seen discounted rewards calculated in a way that looks like below:
Say you have a reward array of length n and a discount hyperparam gamma. You might calculate the discount factors like:
gamma^i for i in range(n)
ex: [0.99^0, 0.99^1, ... 0.99^n]
Then get the discounted rewards by multiplying each discount factor by the corresponding reward and doing a cumulative sum.
However, this results in an array of the same length as rewards, but this seems incorrect to me?
I understand discounted rewards as a way to deal with the uncertainty of past actions on future awards. It would seem to me that uncertainty should result in multiple different contexts and then multiple different arrays with their own discounts.
For instance, say I have 4 states with 4 rewards that looks like [2, 3, 1, 3]. It would seem to me I should then have 4 reward arrays:
[2, 3, 1, 3]
[3, 1, 3]
[1, 3]
[3]
and I should have 4 discount factor arrays also:
[0.99^0, 0.99^1, 0.99^2, 0.99^3]
[0.99^0, 0.99^1, 0.99^2]
[0.99^0, 0.99^1]
[0.99^0]
because of the different contexts. For instance, in the first state reward value 2 is certain and reward value 3 slightly less so. But once we're in the second state, reward value 3 is certain and reward value 1 slightly less so.
If we don't do something like this, isn't our agent being updated on a system that always keys on the first state?
","['reinforcement-learning', 'rewards', 'reinforce', 'discount-factor']",
Which algorithm for hierarchical and ordered classification? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 11 months ago.







                        Improve this question
                    



I have developed the ""Pyrates"" application which is a serious game to learn programming in Python.
In each level of the game, you have to pick up a key and open a chest. To do this, you need to use the control functions available (walk, jump, etc.). However, they are not sufficient. To complete the level, you must implement an algorithmic notion (loop, conditional, variable, etc.). This notion is not made explicit, it is the game situation that makes it necessary. Here is a screenshot to make it more meaningful:

I am currently developing an automatic help system (given by the parrot -see below screenshot-). For each level, when a student clicks on the ""Help"" button, the application should propose one of the five existing helps or no-help (""I can't help you at this point. Keep searching!"").

The five helps types (Level objective, Control functions, Notion involved, etc.) are ordered (1,2,3,etc.). That means that once a help is given, it is not relevant to give a lower grade help.
Here is the tree that schematize these different aids:

Concerning the data, I will soon have a set with the following observations.
For a given student and a given level, each time he clicked on the ""help"" button during the data collection :

Features: cumulative activity of the student before help asking since the beginning of the level
Label: type of help chosen by the teacher (1,2,3,4,5 or no help).

My goal is to train a multi-class classifier on this data so that it can predict what help to give when a student clicks the ""help"" button, based on the accumulation of their activity since the beginning of the level.
My questions is : What type of machine learning algorithm should be used for this type of task?
","['machine-learning', 'classification', 'multiclass-classification']",
Multi-Variate Time-Series forecasting with XGBoost,"
I have trained an XGBoost model on a time-series dataset for predicting a value. The time series has 5 features and one label (the target value). The trained model works fine on both training and testing data, so far so good. As I said, this dataset has some features that I have used for training the XGBoost model (i.e. a multi-variate dataset). The problem is that currently, I have values of these 5 features in my current dataset, so I can train the model with, and do the testing as well. But, I do not know these features values in future.
My question is, how can I predict the target value for future (Ex. next year) When I don't know the values of features in future to feed them into the trained model to do the prediction.
","['time-series', 'forecasting', 'gradient-boosting']","
Simple answer- you can't.
If you can predict the values of these features, then you can see what the output of the model will be given those possible features. You can try and train a model on the t+1 of a feature, but whether this would work really depends on the nature of your data.
More nuanced answer:
Circumstances where you might be able to predict them include where there is periodicity, trend, or some other underlying structure, that makes the future look like the past. In this case, linear-style methods such as vector autoregression or dynamic mode decomposition (particularly where there are oscillating modes) might be better tools for capturing these behaviours explicitly.
"
"During batch normalization is the mini-batch gone through twice, one to calculate the mean and variance and then to normalize them?","
I am asking this question because while designing my own model, I had repeated gradient explosion issues, so I wanted to try batch normalization. I really want to understand the details and math surrounding how it is implemented. Specifically wouldn't batch normalization effectively change the output of each layer as the activations are now normalized? Also, any details on alpha and beta parameters would be greatly appreciated! Thank you!
","['convolutional-neural-networks', 'gradient-descent', 'batch-normalization', 'normalisation']","
I think it'd be helpful to refer to the batchnorm formula given in the PyTorch implementation.
In particular, given an input $x$, you would get the mean and variance ($\mathbf{E}[x]$ and $\text{Var}[x]$), and normalize it as such:
$$ \hat{x} = \frac{x - \mathbf{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}} $$
where $\epsilon$ is a small number to improve stability. There is also an additional affine transform applied in most implementations of batchnorm in standard libraries, so the output is really $\gamma \hat{x} + \beta$, where $\gamma$ and $\beta$ are the affine parameters (scale and shift respectively).
To answer your question (if I understood correctly), yes batchnorm would change the output of the previous layer, due to the normalization and affine transformation.
There are some details to consider:

In most implementations, the $\mathbf{E}[x]$ and $\text{Var}[x]$ are running mean estimates, meaning that there would be additional momentum hyperparameter to control how fast the estimates are updated as new inputs come in.
For batchnorm, you would normalize and apply the affine transformations to each output channel separately, meaning that each output channel would hold its own set of running mean estimates and affine parameters.

"
Shaping reward so that it maximizes multiple components together,"
I am fairly new to RL and I compete in AWS DeepRacer student league. The main task there is to create a reward function. All the hyperparameters and action space are fixed. So far, I know how to shape a single component like speed or steering. I always do like steer_reward^2 or speed^2. But when I try to combine the components together, I am not seeing good results. Is it best to mutliply or add components and are there any resource where I find various maximization functions?
","['reinforcement-learning', 'rewards', 'reward-functions', 'reward-shaping']",
variational inference but with a weighted loglikelihood,"
I  would like to know if it's  correct if I substitute in the ELBO formula

a weighted sum of the loglikelihood
$$\sum E_{q_{\theta}(w)}[w_i \ln{p(y_i|f^{w}(x_i))}]$$
in place of the traditional sum.
My problem is that my dataset comes with  the errors on the target variable, which means that i have access to the weights and i would like to give more value to the measurements with a lower error rather than considering all of them equals.
The addition of the weights would mean that the formula for the posterior probabilitychanges in
$$  p(w,D)= \frac{ p(D,w)p(w)}{p(D)} = \frac{ (\prod p(y_i|f^{w}(x_i))^{w_i})p(w)}{P(D)} $$
does doing something like this correct in the bayesian framework?
","['bayesian-networks', 'bayesian-deep-learning', 'bayesian-optimization', 'evidence-lower-bound']",
Model Suggestion for graph data,"
I am trying to figure out the right model/algorithm for a graph dataset to develop a machine learning pipeline. I have looked into Graph Neural Network(GNN) but all of the tutorials I found, trained the model on a single large graph where my dataset consists of thousands of smaller graphs like a typical machine learning dataset. Is GNN the right model for this kind of dataset? If not what should I look for?
To give an idea about the dataset, I am not still sure about the representation but the dataset will describe the AST of different code snippets.
Edit:

graphs will have different numbers of nodes and edges as they are produced from different code blocks. Also, the node types are not homogenous i.e., they have different features.
There are multiple use cases for the model, but primarily I am interested to develop a model that predicts link or edge between two similar node.

","['graph-neural-networks', 'graphs']","

I have looked into Graph Neural Network(GNN) but all of the tutorials I found, trained the model on a single large graph

In that case, the tutorials were probably working on transductive prediction tasks. These are tasks were you have information on only a few nodes in a graph and want to infer all the missing pieces. An example for this kind of data is the cora dataset.

my dataset consists of thousands of smaller graphs like a typical machine learning dataset.

This is then more related to inductive prediction tasks, as common for chemistry datasets were the task is to predict properties of different molecules. An example is the Mol-HIV dataset.

Is GNN the right model for this kind of dataset? If not what should I look for?

Yes, a GNN can be used for this - if you build your pipeline for inductive tasks you should have no problem applying GNNs to your kind of data.

graphs will have different numbers of nodes and edges as they are produced from different code blocks. Also, the node types are not homogenous i.e., they have different features.

This is exactly what GNNs good at. So, in order to work with your data you should lookup how to prepare your data for inductive tasks (Here is a keras guide on molecular data) and how to setup a GNN for link prediction (like in this DGL guide) and you should be good to go.
"
Why might my policy gradient agent appear to maximize the absolute value of rewards?,"
I have a toy policy gradient RL algorithm using REINFORCE (aka monte carlo policy gradients) that involves bots moving on a grid attempting to ""acquire"" targets in Pytorch. The bots receive +1 for moving closer to targets, -1 for moving farther away/invalid actions, some larger amount for acquiring a target (a rare action), and a smaller penalty for inaction.
What I have observed is that the agents swiftly learn to attempt moving off the grid. In doing so, the bot has learned to ironically maximize the negative reward.
So I tried swapping the sign of all the rewards, and yet the behavior persists, hence my belief that it's almost learning on the absolute value of the reward. It's also possible the network is just randomly converging to one action, but it's unclear how this is the case if so.
Why might this be the case?
My code is long and the environment self-defined, but I believe the relevant parts should be my model action choice and the reinforce part. Rewards and movement themselves seem fine as, if I comment out the reinforcement updated, rewards and movement are applied as expected to the random outcomes of the network.
Here's the model output part:
    def forward(self, x):

      # Run through all of our layers defined above
      x = #...removed for brevity
      x = F.softmax(self.linear_final(x), dim=1)

      return x

    def decide_action(self, x):

      # Create the prob distribution from output and return the action/logprob
      probs = self.forward(x)
      prob_dist = Categorical(probs)
      action = prob_dist.sample()
      action_item = action.item()
      logp = prob_dist.log_prob(action)

      return action_item, logp

Here's the reinforce part:
# Function to calculate loss and update bot network
def reinforce_bot(b, debug):

    # Setup lists and vars to work off
    discounted_reward = 0
    l_returns = []
    l_policy_loss = []

    # Work through the bot's episode rewards backwards
    # The net effect of this will be such that we built rewards for only actions and their following rewards
    # (i.e. action for step n only gets rewards for steps > n, never steps < n)
    # Additionally we'll build in our reward discounting (where future steps contribute less to overall reward)
    for reward in b.l_episode_rewards[::-1]:
        discounted_reward = reward + gamma * discounted_reward
        l_returns.insert(0, discounted_reward) # but insert back at the beginning to get correct order
    
    # Now turn the rewards into a tensor for working with gradient
    t_returns = torch.tensor(l_returns)
    # But standardize the rewards to stabilize training
    t_returns = (t_returns - t_returns.mean()) / (t_returns.std())

    # Now build up our actual policy loss by multiplying it by our logprobs
    for logp, discounted_reward in zip(b.l_episode_log_probs, l_returns):
        l_policy_loss.append(-logp * discounted_reward)

    # Zero our gradient to get ready for backprop
    b.optimizer.zero_grad()

    # Technically our l_policy_loss is a list of tensors, so smoosh those together
    # Then sum to get the total loss
    policy_loss = torch.cat(l_policy_loss).sum()

    # Now run our optimizer
    policy_loss.backward()
    b.optimizer.step()

    # Text to print some helpful debugging
    if debug:
        print(f'{b.team} had awards array of {b.l_episode_rewards}')
        print(f'{b.team} had l_returns of {l_returns}')
        #print(f'{b.team} had l_policy_loss of {l_policy_loss}')
        print(f'{b.team} had a policy loss of  {policy_loss}')

    # And cleanup our epsiode tracking lists now since we don't need them
    del b.l_episode_rewards[:]
    del b.l_episode_log_probs[:]

Perhaps the implementation of REINFORCE is not properly handling the mix of positive and negative rewards?
","['reinforcement-learning', 'policy-gradients', 'pytorch', 'reinforce']",
What are some approaches for specifying goals for deep-RL agents?,"
I'm wondering what are the approaches for specifying goals for a trained deep-RL in deployment? E.g. how to tell a car drive agent to go to location $y$?
To elaborate, I understand that, for example, how Deep-RL can be used for solving an Atari games. But in these game, the goal is mostly fixed for an agent and hence can be encoded into the reward. But for something like a car driving, the goal is different if the destinations are different locations, e.g., sometimes I want to go to location $y_1$, sometimes $y_2$, etc. So it doesn't seem straightforward that I can encode the destination variables into the reward signal.
One solution is perhaps to add the destination location to the input state. But is there any other ways? Or maybe what are some of the approaches that doesn't rely on solely an RL component.
Many thanks!
","['reinforcement-learning', 'deep-rl', 'intelligent-agent']",
How to use RL on a robotic moving arm?,"
I'm working on a simulation of a motor that is attached to a wing (Later, this will also have a real-life counterpart once I'll assemble all the components in our lab), and I can control the forces/torques that the motor applies. I want to use RL and find an optimal action in terms of

""what force should the motor apply to maximize lift"".

To make things clearer, check out the following figure
                                              
So for example, I can find $\phi$ at every single time step $t$ (using some 1st-year physics equations and python integrators) and this will be my state $s$.
Are you familiar with approaches that deal with this problem?
","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization']","
The core part of RL is the reward function. It belongs to the environment and it is the only way the agent can explore the world given a state.
By constructing an appropriate reward function we can make an agent do exactly what we want.
Here is an example of a double-jointed arm that can move to target locations in the Unity environment.
A reward of +0.1 is provided for each step that the agent's hand is in the goal location. So, the goal of the agent is to maintain its position at the target location for as many time steps as possible.
Initially, the agent performs random actions (random forces on the joints) and eventually learns to follow the target. A high-level idea is described in this video. And here is also related DeepMind's article and paper.
"
"Why remove stop words, numbers in a conversational chatbot?","
I have been working on a conversational chatbot recently using movie dialogues corpus dataset, since i am very new to this i started to see if there's already code available for chatbots. I came across many and i found that the developers removed the stop word, numbers, or any other related to increase the model accuracy of course but by removing these won't the question's context or the chatbot's grammar be effected
For example:
1)Numbers for Questions context (If the question has numbers involved the how would the bot answer it?)
2)Removing stop words won't alter bot's grammar?
","['deep-learning', 'natural-language-processing', 'recurrent-neural-networks', 'long-short-term-memory', 'chat-bots']",
Multi-layer network only predicts linear trends,"
I have made a neural network from scratch (in java), which is refusing to switch out of linear regression. I have pushed up the layer sizes (it now has 2 hidden layers, both with 5 neurons), and yet when given harsh sloping polynomials to train on, it still predicts values that follow a gradient, even though this returns high cost.
The network is working in that the predictions do somewhat follow the polynomial as best as a line could, but why wont it actually give me predictions that follow a polynomial like the one it trains on?
I have checked all aspects of training, SGD is working as it should, as is the cost function (MSE), and yet the network just isn't able find a way to minimise cost, it can't seem to break free of linear regression.
","['regression', 'linear-regression', 'mean-squared-error', 'java']",
Unclear point in complexity of the target function in machine learning,"
In the book ""Learning from Data"" by Moustafa et al, page 27: the following red line is not clear for me.

",['machine-learning'],
How can I train GPT-3 (or similar) to search for answers my text file,"
My company has large sets of standards that are quite dry to read through and find answers to questions.  How can I have an AI perform the search?
The text files are too long to submit with every query, so it seems like I would need to train the model with the data.  What's the simplest solution out there?
",['gpt-3'],
Out of distribution detection (OOD) in the context of regression problems,"
I'm working in a regression setting to predict a scalar value $y$ from an input $\textbf{x} \in \mathbb{R}^D$ and I'm interested in understanding whenever my model is fed with something that it is outside the (unknown) training distribution $p(\textbf{x})$. For simplicity we can assume I'm using a simple neural network $f_\theta:\mathbb{R}^K \rightarrow\mathbb{R}$ to predict a single (scalar) property value, training my model with an initial dataset $\mathcal{D} = \{(\textbf{x}_i \, , y_i)\}_{i=1}^N$, that is, my task is specifically about regression.
What I'd be interested in achieving would be that, feeding my neural net with a new input $\tilde{\textbf{x}}$ I could retrieve somehow a confidence score telling me if the new input $\tilde{\textbf{x}}$ lies outside the spectrum of observed instances in training dataset.
A way of doing that would be of course estimating the probability of training dataset $p_\theta(\textbf{x})$ and see if the new material $\tilde{\textbf{x}}$ is in a low-likelihood region of $p$. People have used such approach for images (https://arxiv.org/pdf/1912.03263.pdf) but generative models are hard to train.
Instead, I was looking at recently proposed papers using energy-scores for detecting out of distribution samples (paper1, paper2) but the examples seem to refer specifically to classification settings.
As I'm not too familiar with energy-based models, is there a way such frameworks may be applied to regression settings?
","['regression', 'multiclass-classification', 'energy-based-models']",
Define possible?,"
In Reinforcement Learning, policies are defined in terms of possible actions (see for instance page 58 of the book by Sutton et al.). So, is any action that an agent has in its repertoire always ""possible"", even when it has a policy-value of zero? Or are possible actions dependent on the state?
","['reinforcement-learning', 'policies']",
Do we need negative samples for tuning extractive QA systems?,"
I am checking the API for the haystack reader:
https://haystack.deepset.ai/reference/reader
and in particular the fine-tuning method (.train).
I was just wondering if for training/fine-tuning a QA system it is important/possile also to have samples not containing the answer to the question (something like ""negative samples"") in the training set or, since negative samples are in some sense already contained in the positive ones (not all subsets of words of a text contain the answer to a question), this is generally not necessary.
","['training', 'fine-tuning', 'question-answering']",
"If an event has a statistical probability of only 50%, is it possible to use a neural network to predict it with more than 50% accuracy?","
For example using a neural network to predict a coin toss. Can a trained neural network to predict it with more than 50% accuracy?
",['neural-networks'],"
This is a question of marginal vs. conditional distribution
The marginal distribution of the coin may be a Bernoulli random variable with 50% probability for either outcome.
However, the conditional distribution of the outcome given information about other factors (e.g. the angle, throw height, ... see other answers) may look entirely different. Provided these features determine the outcome in some way, a neural network can absolutely predict the outcome with more than 50% accuracy.
A neural network could not exceed 50% accuracy, if

The information determining the throw outcome is not available
The function is of a nature that can not be learnt by the neural network
The coin toss is truly random

A coin toss is often used as a casual example of a ""truly random"" event, so in this sense the answer to your question is ""No"". In reality however, it is very hard to find any truly random events (at least outside quantum mechanics), which is why random number generation is a big challange and neural networks can predict a lot of things.
"
"The Small Set Expansion Hypothesis, this problem was solved or is open problem yet?","
I found this problem by article called ""Ten Lectures and Forty-Two Open Problems in the Mathematics of Data Science"" published at 2016. I`m looking for an open problem at Data Science or/and Machine Learning field that use many Analysis Math and I'm new in this fields. Anybody can answer about? I looking for any paper the about possibility solved this problem, but I can't find any paper published definitively solved about this problem...
Thanks!
","['machine-learning', 'math', 'optimization', 'data-science', 'statistical-ai']",
Coherence is classifying time series data,"
I have been told that models that are continually running to detect a temporal pattern, say wake up words in Alexa or Echo, the model establishes this coherence or another word I heard, statistical trust, and does this via classyfying multiple times and looking at the past classifications in some manner.
I wanted to ask if this methods has got a specific name I can look up? any papers?
","['machine-learning', 'deep-learning', 'statistics']","
The following paper from Amazon Alexa Research refers to this topic as keyword spotting (KWS), and more specifically, to wake up words as wake word (WW) spotting.

Jose, C., Mishchenko, Y., Senechal, T., Shah, A., Escott, A., Vitaladevuni, S. (2020). Accurate detection of wakeword start and end using a CNN

Some other papers you may find interesting:

Luo, M., Wang, D., Wang, X., Qiao, S., & Zhou, Y. (2022). Error-Diffusion Based Speech Feature Quantization for Small-Footprint Keyword Spotting. IEEE Signal Processing Letters, 29, 1357–1361. https://doi.org/10.1109/LSP.2022.3179208
Arik, S. O., Kliegl, M., Child, R., Hestness, J., Gibiansky, A., Fougner, C., Prenger, R., Coates, A. (2017). Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting. arXiv.
Chen, G., Parada, C., & Heigold, G. (2014). Small-footprint keyword spotting using deep neural networks. 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4087–4091. https://doi.org/10.1109/ICASSP.2014.6854370
Qin, H., Ma, X., Ding, Y., Li, X., Zhang, Y., Tian, Y., Ma, Z., Luo, J., & Liu, X. (2022). BiFSMN: Binary Neural Network for Keyword Spotting. Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22)
Tsai, T. H., & Lin, X. H. (2021). Reduced Model Size Deep Convolutional Neural Networks for Small-Footprint Keyword Spotting. 2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS), 1–4. https://doi.org/10.1109/ICECS53924.2021.9665618

"
How to Train a Decoder for Pre-trained BERT Transformer-Encoder?,"
Context:
I am currently working on an encoder-decoder sequence to sequence model that uses a sequence of word embeddings as input and output, and then reduces the dimensionality of the word embeddings.
The word embeddings are created using pre-trained models. I want to be able to decode the word embeddings returned by the decoder of the Sequence to Sequence model back to natural language.
Question: How can I train a Decoder that works with the sequence of word embeddings and the original sentence for this task?
See below for the code that generates the word embeddings:
from typing import List

import numpy as np
import torch
from transformers.tokenization_utils_base import BatchEncoding
from transformers import BertTokenizerFast, BertModel

TOKENIZER = BertTokenizerFast.from_pretrained('bert-base-uncased')
MODEL = BertModel.from_pretrained('bert-base-uncased')

def get_word_indices(sentence: str, separator="" "") -> List:
    sent = sentence.split(sep=separator)
    return list(range(len(sent)))

def encode_sentence(sentence: str) -> BatchEncoding:
    encoded_sentence = TOKENIZER(sentence)
    return encoded_sentence

def get_hidden_states(encoded: BatchEncoding, layers: list = [-1, -2, -3, -4]) -> torch.Tensor:
    with torch.no_grad():
        output = MODEL(**encoded)
    hidden_states = output.hidden_states
    output = torch.stack([hidden_states[i] for i in layers]).sum(0).squeeze()
    return output

def get_token_ids(word_index: int, encoded: BatchEncoding):
    token_ids = np.where(np.array(encoded.word_ids()) == word_index)
    return token_ids

def embed_words(sentence: str) -> torch.Tensor:
    word_indices = get_word_indices(sentence)
    encoded_sentence = encode_sentence(sentence)
    hidden_states = get_hidden_states(encoded_sentence)
    word_embeddings = []
    for word_index in word_indices:
        # Get the ids of the word in the sentence
        # Important, because BERT sometimes splits words into subwords
        token_ids = get_token_ids(word_index, encoded_sentence)
        # Get all the hidden states for each word (or subwords belonging to one word)
        # Average the hidden states in case of subwords to retrieve word embedding
        word_embedding = hidden_states[token_ids].mean(dim=0)
        word_embeddings.append(word_embedding)
    return torch.stack(word_embeddings)
```

","['transformer', 'bert', 'pretrained-models', 'seq2seq', 'encoder-decoder']","
You can follow this example code https://github.com/microsoft/CodeBERT/blob/master/CodeBERT/code2nl/README.md
This example generally loads pre-trained Bert (encoder) and plugs a custom decoder. Regarding its encoder, the example utilizes transformer for loading pre-train.
"
Reward shaping for an autonomous driving car (AWS DeepRacer),"
From past 2 months, I am competing in AWS Deepracer student league. I was new to RL but had some knowledge in supervised and unsupervised learning. In the league the hyperparameters and action space is fixed. 
hyperparameters.json
{
    ""batch_size"": 64,
    ""beta_entropy"": 0.01,
    ""discount_factor"": 0.999,
    ""e_greedy_value"": 0.05,
    ""epsilon_steps"": 10000,
    ""exploration_type"": ""categorical"",
    ""loss_type"": ""huber"",
    ""lr"": 0.0003,
    ""num_episodes_between_training"": 20,
    ""num_epochs"": 10,
    ""stack_size"": 1,
    ""term_cond_avg_score"": 3500000.0,
    ""term_cond_max_episodes"": 1000000,
    ""sac_alpha"": 0.2
  }

model_metadata.json
{
  ""action_space"" : {
    ""steering_angle"" : {
      ""high"" : 30.0,
      ""low"" : -30.0
    },
    ""speed"" : {
      ""high"" : 1.0,
      ""low"" : 0.5
    }
  },
  ""sensor"" : [ ""FRONT_FACING_CAMERA"" ],
  ""neural_network"" : ""DEEP_CONVOLUTIONAL_NETWORK_SHALLOW"",
  ""version"" : ""4"",
  ""training_algorithm"" : ""clipped_ppo"",
  ""action_space_type"" : ""continuous""
}

And these are the input parameters of the reward function we can use to reward the agent - https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-reward-function-input.html
My goal is to get a model which completes the laps as fast as possible within 10 hours of training. I have tried using various things like rewarding it for staying in a particular zone in particular path of track, rewarding it for making progress in less amount of steps, using optimal racing lines, etc. But I had no success so far. I believe this is happening because of the cobra effect. How do I go about shaping the reward that takes less steps to complete the laps? I even tried giving negative reward but doesnt seem to work.
","['reinforcement-learning', 'rewards', 'reward-functions', 'reward-shaping']",
Surrogate model to produce time series from parameter set,"
Say I have a model $M$ that takes in a parameter vector $\beta$, and produces a (numerical) time series. This could be a complicated model (e.g. a bespoke enzyme reaction model), or something simple like a VAR(1) (e.g. $x_{t}=\beta_0+\beta_1 x_{t-1}+\varepsilon_t$). I can run this model multiple times with the same parameter vector, and get a different time series each run.
(Even for the same parameter vector, the output may be different each run, as is the case when you simulate a VAR(1) with known variance $\sigma^2_\varepsilon$)
How can I design/implement a neural network that acts as a surrogate to my model - i.e. when given a parameter set $\beta$, it produces plausible time series sequences?
","['recurrent-neural-networks', 'python', 'transformer', 'time-series']",
"Why, in deep learning, do we get computational power by going deeper?","
I know by the expressiveness of a neural networks that it can be seen as a chain of function compositions, i.e. $g(f(.. z(x)..))$ and also that, if we go deep, we can approximate complex functions $f: \mathbb{R} \rightarrow [0,1]$ with a lower number of units.
But why, if we go deeper, do we get computing power?
",['deep-neural-networks'],"
It is not that we ""get more computing power"", it is the fact that deep networks are more expressive than shallow ones, which is pretty much the result of what you have started stating about composition. It might be helpful to think of an example - here's a nice one I've written about here

We define the t-saw-tooth function as a piece-wise affine function
with $t$ pieces. Also, we define the hat function as
$hat(x)=relu(2relu(x)-4relu(x-0.5))$.
note that hat is a 4-saw-tooth function by definition. We can use hat
to concatenate saw-tooth functions - let $t(x) =
 hat(x)+hat(x-1)$ comprises of two hats, and is a 6-saw-tooth function,
and the composition $t(t(x))$ is a 10-saw-tooth, $t(t(t(x))))$ is a
18-saw-tooth, and in general, a composition of $T$ functions ($T$
$t(x)$'s) is a $(2+2^{T+1})$-saw-tooth, namely, the composition is
comprised of $2^T$ hats. How can we represent this composition using a
shallow network? remember that every neuron in the (single) hidden
layer ""represents"" a single relu, therefore two neurons can represent
a single hat. As there are exponentially many hats, a shallow network
will have to use exponentially many neurons ($2^{T+1}$ to be exact).
On the other hand, a deep neural network would only need $\Theta(T)$
(if, for example, we use $T$ layers and $2$ nodes per layer). This is
the case because the deep network $i'th$ layer receives as input the
values of the $(i-1)'th$ layer, and for that matter is represents the
composition of $t(x)$ simply as a product of its architectural design.

"
Clarification on GANs for text generation,"
A GAN-like architecture for text generation is proposed in 'Generative Adversarial Networks for Text Generation'.
The setup is the following:

The generator of the GAN is proposed to be a recurrent neural network that its by itself a text generator.
The internal latent vectors of the GAN ( denoted by $G(\vec{z})$ ) are the outputs of the generator RNN.
The discriminator is not made explicit; but let's imagine that is ""any architecture that can classify text into two categories"" (""real"" and ""fake"" in our case).
The generator RNN is trained to minimize $(1- D(G(\vec{z})))$ (where $D(G(\vec{z})))$ is the discriminator output) as usual.

Now it comes a somewhat obscure statement:

Remember that while decoding using an RNN, at every time step we make the choice of the next word by picking the word corresponding to the maximum probability from the output of the softmax function. This “picking” operation is non-differentiable.

Question:
What is the precise statement of that vague paragraph?
My expectation is to explicitly write the function that is just continuous but not differentiable.
Thanks in advance!
","['natural-language-processing', 'recurrent-neural-networks', 'math', 'generative-adversarial-networks', 'text-generation']",
Confusion about bias in McCulloch-Pitts neurons,"
I just have a quick question, maybe I am too nit picky here.
We recently had an introductory lecture to AI in university and the professor talked about McCulloch-Pitts neurons, e.g. activation as soon as the sum of inputs is reaching a certain threshold amount. My problem is that the professor said that the threshold amount is also called ""bias"", is that correct?
I thought biases are analogous to the y intercept of a linear equation and added on every equation made with the neural network, in order to balance out systematic error in predictions.
To be specific, my question is, whether the threshold met in a McCulloch-Pitts neuron is actually called bias or not.
Any hints appreciated! :)
Best regards,
Sam!
","['terminology', 'ai-basics', 'bias']",
"What kind of network would you use to predict event times, when there can be many events per sample?","
The each sample I have is an m x n input of time series data, with m dimensions, sampled at n times.
I am trying to predict events which could happen at any of the n timespoints. For each m x n sample, there may be no events, or there could be dozens.
I'm not sure of the best approach to achieve this prediction. I could try to predict event times, i.e. the index of the binary vector where the event occurred.  And this seems sensible, as an RMS loss between predicted times and true times would reduce as predicted times got closer. I want to reward the model for predicting an event that is close to the true time. But I'm not sure how I would calculate a loss, because there are an unknown number of events in each sample. i.e. how do you calculate a loss when the prediction is [156, 259] and the true values are [190, 270, 500].
I could also try to predict the full time series of the binary event vector, and again, do something like an RMS loss between the prediction and the true data. This is nice because the outputs will always be the same length, i.e. an n long vector. BUT, it is bad because if the true event time series was [0, 0, 1], then a prediction of [0, 1, 0] and [1, 0, 0] would have the same loss, but the first prediction is better (because the predicted event time is closer to the true one).
Does anyone have any insight on this type of problem.
","['machine-learning', 'deep-learning', 'time-series']",
"For REINFORCE, why do different books give different algorithms?","
The discount rate appears twice in the REINFORCE algorithm in Sutton and Barto (2018).  However, in three major books (Graesser and Keng (2020); Morales (2020); Ravichandiran (2020)) on reinforcement learning and in numerous papers and videos, the discount rate only appears once.  Why is this?
","['reinforcement-learning', 'policy-gradients']",
The result of back propagation for a neural network,"
I have created a neural network that feeds an image into a convolutional neural net, then feeds the flattened output of this network into an artificial neural network.
I have a feeling that my backward propagation is not working properly.
Mind you, I wrote all of it from scratch in vanilla javascript.
The final point of control for me as the programmer is when I input the desired output into the backwards propagation algorithm to calculate the deltas and update the weights. After running backwards propagation I run forward propagation with identical input data to the data that was put into the network before backward propagation to see the change in the output layer based on the new weight values.
For each individual output of my neural network's final layer if it is between the output before backward propagation and the desired output then the network successfully propagated backwards for that output. In my network I had 4 outputs. I took the average of how many outputs were correct for each time back propagation was run and on average I got 0.773206751. About 77% of the time the gradient moved in the correct direction. So 23% of the time the network returned an output in the incorrect direction or returned an output that surpassed the desired output(went too far in the right direction).
In a neural network after running backwards propagation should the output after back propagation be closer to the desired output 100% of the time? What could cause these types of inaccuracies(or are there too many potential causes)?
It is unfortunately a problem that I cannot debug(because there are too many small numbers to keep track of). Unless there is a way to debug this problem, but I'm not sure.
architecture
input to CNN[102, 102, 4]
conv: kernel[9*9] stride:3 output:[32, 32, 16] act: ReLU
conv: kernel[5*5] stride:3 output:[10, 10, 32] act: ReLU
max-pool: pool_dim[2*2] stride:2 output:[5, 5, 32]
flatten: from [5, 5, 32] to 800 neurons
inp layer: 800 neurons
hidden layer: [800, 402] act: ReLU
output layer: [402, 4] act: for first 3 in output softmax for the fourth in output linear.
it is an a2c network.

","['neural-networks', 'convolutional-neural-networks', 'backpropagation']",
is Flipout an upgrade of the local reparameterization trick or a completely different technique?,"
I was reading  the Flipout paper and I am confused about 1 thing:
when the author  samples the perturbation matrix $\hat{\Delta W}$ does he do directly from the real variational distribution $q_{\theta}(\bf{w})$ as it seems from the notation or does he use the reparameterization trick (sample $\epsilon$ and then use $g(\theta,\epsilon)$ to get $\hat{\Delta W}$? I thought this latter case was the correct one but  some previous questions here on ai.stackexchange  seem to suggest that flipout is an entirely different technique.
","['neural-networks', 'autoencoders', 'bayesian-deep-learning']",
How do I train a model to classify if it's a Full Human Body in the picture?,"
recently I started a personal project that uses some Machine Learning techniques in the process, so I'm currently collecting human images with a web scraper. I know that I can use some pre-trained models to verify if there is a human in the picture and segregate it from it's background using a mask algorithm. But besides that, I also need to filter only images with a full human body exposed, and alongside that it would be practical to segment the body into head, torso, legs and feet.
I want to know if anyone know a technique:

to classify if the human body is fully shown
to segment the full human body into the four parts discussed earlier

I don't know if it's better to focus on a body segmentation algorithm first, and then count if nothing is missing, like if in the picture there is a head, a torso, legs and feet, then it's a full body, or I should first classify if it's a full body picture than segment it.
The first option seams easier, but it can be flawed because if there is two humans in the picture, it is possible that it can segment some body parts of one person and some of the other. Maybe I could first have a counting algorithm to count how many humans there is in the picture? But that also don't sound like a good idea.
Anyway, I'm sorry for the huge question, but feel that it is a very specific problem and I should clarify as much as possible. Thank you for reading it.
","['neural-networks', 'machine-learning', 'deep-learning', 'classification', 'object-detection']",
Hyperparameter tuning methods for neural networks,"
I have a fully connected feedforward classifier neural network that uses the leaky ReLU activation function. I would like to apply a state-of-the-art hyperparameter tuning method to my methodology. Currently, the fitness function associated with each parameter setting is a weighted combination of (i) accuracy of both training and test data sets ($acc_{tr}$ and $acc_{ts}$, respectively), (ii) numbers of activated hidden layers ($nL$) and their neurons ($nN_l$), and (iii) number of epochs ($nE$) - length of training. The fitness function is computed as follows:
\begin{equation}\label{eqq}
 FF = 0.7 \times (acc_{tr} + acc_{ts}) + 0.1 \times \frac{nL}{10} + 0.1 \times \frac{\sum_{l \in \mathcal{L}} nN_l}{1000} + 0.1 \times \frac{nE}{2000}
\end{equation}
Note that I have determined the weights in function $FF$ based on my experimental observations. Potential values for the parameters of this neural network are given in the following table:

$\alpha$ here is the leakage parameter of the ReLU activation function. Considering that I am new to this field, my questions are as follows:

Is the weighted fitness function an appropriate one? Should I include both training and test accuracies at the same time? Why?
Do the potential values make sense for a fully connected feedforward classifier neural network with the leaky ReLU activation function? Are there any other parameters that I have to tune? Please consider that I have developed this neural network using the Keras library.
The above table also shows the elite configuration. I have found this parameter setting using the iRace package. Do you recommend any other method for hyperparameter tuning in neural networks?
I have seen many papers use genetic algorithms to tune their parameters. What are the advantages of using a genetic algorithm in comparison to the iRace package (if there is any)?

Please kindly answer my above questions with supporting scientific references. Also, please let me know if there are any other considerations for the hyperparameter tuning of neural networks. Thank you!
","['neural-networks', 'hyperparameter-optimization']","
There is a Python package called Tune is fairly easy to use, and contains most modern hyperparameter tuning methods such as Bayesian optimization and Hyperband. It should interface with both Keras and PyTorch.
To answer some of your questions:

If you are doing intensive hyperparameter tuning, you need to have three separate datasets: the training set, validation set, and test set. When you train the network, you train on the training set, and evaluate the accuracy metric on the validation set. The validation accuracy is then used to tune the hyperparameters. After you find the best hyperparameters, you then evaluate again on the test set to get the true test accuracy. The reasoning for this is because the act of hyperparameter tuning is itself an optimization procedure, and will suffer from overfitting as well. This is best explained in one of Andrew Ng's video. A more robust way is to do k-fold cross-validation, which I think is the default method of Tune.

Yes I think your hyperparameter makes sense. Is $\alpha$ the learning rate? You can also try to include batchsize from some set, say $\{2, 4, 8, 16, ...\}$, given your GPU VRAM.

I am not familiar with iRace enough to say it's advantages/disadtanges compared to genetic algorithms. But I know two of the major drawbacks of genetic algorithm is that it doesn't perform well when the number of hyperparameters (or genetic elements) are large, and it has a tedency to converge to local minima.


"
How to measure classification accuracy in object detection task?,"
Object detection uses mAP as the metrics. But if we are only interested in classification once the object in a bounding box is extracted, what metrics should we use? Thanks!
","['classification', 'object-detection']","
For classification problems, using crossentropy and bce is always my go-to option. It usually performs best for these types of problems. However, using MSE or its variants is also common in some cases. Hinge loss is another one that you might want to look at if the previous options didn't work.
"
How do GPT models pass information for each token prediction?,"
So, I'm trying to understand what is going on in the following picture (from this paper):

Each decoder blocker in the above GPT model has attention heads (red) and MLPs (green). I know that we add residual connections to prevent vanishing gradients and normalization to prevent exploding gradients. However, these are computations done in the forward pass.
So, I'm trying to understand the blue parts above that seem to show that there is a residual from the previous token that is passed to the next token computation at the same layer. So like, ""The"" computations at layer i are passed to ""Space"" computations at layer i. What is going on here? I don't see any mention of this in the papers or blog posts. How is that computation being passed?
EDIT: Ok, so my understanding is that the red arrow is meant to point to the fact that the computation for the attention is carried over to the next token. Each new token vector is added to the masked attention matrix. Each row of the masked attention matrix is for each token being passed through the model, and the matrix is size (sequence_length, sequence_length). So, each forward pass for a new token increases the masked matrix by 1 each dimension.
The arrow only goes ""down"" to the same layer because the masked attention matrix for that layer remains the same except for that layer (you're only adding a new row for the new token).
Is this right?
","['neural-networks', 'gpt']",
"What are ""feature planes"" in neural networks? (current context is deep reinforcement learning)","


The input contains 14 feature planes, each of shape 11x11
What does this mean?
","['reinforcement-learning', 'deep-learning']","
A feature plane is just a numeric representation of a specific state of the environment.
For example, in a game with different types of items (candy, cake, bomb, ...) a binary feature plane can be used to encode the positions for each type of item.
"
Using similarity score within lstm embedding for attention based mechanism,"
Yesterday, I found this fascinating paper about predicting various clinical conditions using an attention based LSTM. I don't have any practical experience with attention mechanism or transformers, which might be the reason why I struggle to understand why this approach work. Well, let me first try to summarize what the authors accomplished

As a first step, the use some sort of model to predict the future course of some important clinical variables. Their outcome is denoted by $X_0...X_t$. As a second step, these features (no timeseries data!) are passed into a transformer like architecture:

as an embedding step, the predicted clinical variables along with some static ones are passed to an LSTM that computes several hidden states: $H_0...H_t$
the attention step receives  $h_0...h_t$. For computing the contribution of each hidden state, the author states that each hidden state $h_i$ is passed to a function that computes the similarity between $h_i$ and $h_t$, that is the last hidden state of the lstm. From each resulting score $s_i$, a weight is computed with the softmax function $$w_i = \frac{exp(s_i)}{\sum_{j=0}^texp(s_j)}$$. The context vector cv is then calculated by computing the weighted sum of hidden states.
the decoder receives the context vector concatenated with $h_t$ and computes the output using two dense layers.

What I don't understand are two things:

Is it legit to use a lstm as embedding, while the input is no time series data but independent features?
In the presented approach, the weight of an hidden state $h_i$ depends on the similarity to the output of the lstm, i.e. $h_t$. But isn't an attention-based architecture used precisely to let every input variable contribute equally to the output, and to combat lstm's problem of struggling with long dependencies? With long inputs, i guess that $h_0$ tends to be likely to have larger differences from $h_t$ than, e.g. $h_{t-1}$ simply because it is more in the past. If this is true, $h_t$ automatically would receive a smaller weight.

Are my thoughts correct or have I missed something?
","['machine-learning', 'transformer', 'attention']",
When use positive or negative rewards in reinforcement learning? Is there anything in literature?,"
Let's say I can design a reward as function of a distance $d>0$ from the target in 2 ways: $r=\frac{1}{1+d}$ or $r=-d$. The first is defined in $(0,1]$ the second in $(-\infty,0]$.  I would expect both to work since they encode the same information (maybe one better that the other), but I often see that one works while the other doesn't. I'm wondering if there is a way to predict which one works based on the tasks. In google I don't find any work on it.
Or it maybe doesn't depend on the sign but only on the shape, in particular the first is nonlinear (and gives more importance to big distances and less to those close to $0$), while the second is linear (and gives same importance to all values).
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'rewards', 'reward-design']",
Formatting Binance dataset to DeepLOB,"
I’m new to ML and really stuck on how to format the Order Book data from Binance to the same format of The FI-2010 dataset used in DeepLOB(Limit Order Book) Research paper. I kindly ask for your help. I’ll link the GitHub link below:
DeepLOB code
","['convolutional-neural-networks', 'python', 'algorithmic-trading']",
Can I add components which make the solution infeasible in Ant Colony Optimization?,"
While building the solution in ACO, can I add components which make the solution infeasible?
I am building my algorithm based on this:

","['optimization', 'constrained-optimization', 'ant-colony-optimization']",
Problem with continual learning in RL,"
I've got a vague question, I trained model in continual learning manner on 3 environments (using both SAC and TD3), taking the last model from the previous environment and performing additional training on the new one. I knew catastrophic forgetting would take place but what I didn't anticipate is that the first model (trained only the easiest environment) would perform better on all of the 3 environments (better than the models trained on the harder environments), is there a clear explanation for such phenomenon?
I can provide eval/mean_reward graph of SAC for the better context (don't know if it helps anyhow, the graph is average over 3 runs with one standard deviation)

","['reinforcement-learning', 'incremental-learning', 'catastrophic-forgetting']",
Question on paper : A Mostly Data-driven Approach to Inverse Text Normalization,"
In A Mostly Data-driven Approach to Inverse Text Normalization , could anyone describe how do the expressions (1) come about ?

",['natural-language-processing'],
What does AUSE metric mean in uncertainty estimation,"
I am reading the paper ""Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision"", I do not understand the definition of AUSE metric in this sentence ""but only in terms of the AUSE metric which is a relative measure of the uncertainty estimation quality."" There is no explanation for this term in the paper and googling it didn't bring me anything useful either. Can anyone tell me what does AUSE stand for? Thank you so much!
","['bayesian-deep-learning', 'uncertainty-quantification']","
The paper you read refers to the ""Uncertainty Estimates and Multi-Hypotheses
Networks for Optical Flow"" paper for the AUSE metric. AUSE stands for Area Under the Sparsification Error. There are 2 core concepts here c.f. Sparsification Plot and Sparsification Error (defined in Section 5.2)
Concept 1: Sparsification Plot
Here, we plot our ""metric-of-interest"" on the y-axis and ""percentage-of-removed-samples"" on the x-axis. The idea is to remove samples (hence the term sparsification) in descending order of uncertainty and to evaluate your ""metric-of-interest"" at each removal. Then, your models sparsification plot is compared to an ""Oracles"" sparsification plot. The ""Oracles"" plot removes samples in descending order of their error on the basis of your models predictions. The closer your models curve is to the ""Oracles"" curve, the better your model is in context of uncertainty-error correspondence (or uncertainty calibration).
Concept 2: Sparsification Error
But, how do you compare your models performance with other models since they all have different errors and hence different sparsification plots (also for their respective Oracles). The authors propose to simply calculate the difference between a model and its Oracle's sparsification plot (hence the term error) and calculate the area-under-the-curve (AUC) for that. Unlike most cases where high AUC is better, here a lower AUC is considered better.
One can stop reading here if they just need an answer to the OP's question. Rest of the content is just some of my thoughts on this.

My thoughts: Global Implication
For future readers of this post, the authors of the paper referenced by the OP refer to the AUSE metric as ""relative"" and instead propose their own ""absolute"" metric called ""AUCE"" - Area under the Calibration Curve. They argue that AUSE can be gamed if the models uncertainty is consistently underestimated. My understanding of this is that if your model assigns similar(or same?) uncertainty to each sample, then the removal process to compute the sparsification plot is just like an Oracle. And then the Sparisifcation Error is epsilon(~0). In spite of this theoretical edge case, I have seen other papers using the same concept as a sparsification plot though termed as a ""Risk-Coverage"" curve instead [e.g. Automatic segmentation with detection of local segmentation failures in cardiac MRI ]
My thoughts: Uncertainty calibration vs Uncertainty-Error Correspondence
As a side note, most folks use the term uncertainty-calibration, though in my opinion it should be termed as ""uncertainty-error correspondence"".
Sstrictly speaking, it is counter to the definition of calibration for a predictive model. Historically, calibration is spoken of in context of probabilities of a particular event (The well-calibrated Bayesian, Journal of the American Statistical Association,1982). Since uncertainty (e.g. entropy, variance etc) values are not directly indicative of probabilities, even though they are derived from them, I believe it is semantically incorrect to say uncertainty calibration.
"
Pseudo Label Generation for Generative Cooperative Learning,"
I am trying to implement this paper for unsupervised video anomaly detection.
The gist of the paper seems to be:

Create a dataset for an unsupervised setting, by mixing up the train and anomalous videos (section 4)
Divide each video into $p(=16)$ segments each, so for video $i$ we have the segments $s_{ij}$, $j= 1,2...p$.
Using a Feature Extractor (ResNext) ,compute a '$d$' dimensional feature vector $f_{ij}$ for each $s_{ij}$. (Section 3.1)
Use this to (pre)train the Generator (a simple Autoencoder). Once G is (pre)trained, it is used to generate pseudo labels. The pseudo labels from G are then used to (pre)train the Discriminator (D). (section 3.3)
Pretrained G and D are then put into a collaborative loop, where pseudo labels from D are used to improve G and likewise.

I am having trouble with generating the pseudo labels using G. The paper clearly mentions in section 3.3: ""Once G is pretrained, it is used to generate pseudo labels"".
So does the pseudo label generation come into play once training is done? Like after the model is ready, we pass the dataset again and keep note of the reconstruction losses, and generate labels accordingly?
Or are they generated during training?
(Section 3.2.2 mentions the pseudo label generation and section 4 mentions the threshold selection).
","['deep-learning', 'computer-vision', 'autoencoders', 'feature-extraction', 'anomaly-detection']",
"Why does some AI professionals think deep learning is ""intelligent""?","
Human is intelligent, this is the ultimate definition (although not rigorous) of intelligence.
Ordinary programs are not considered ""intelligent"", even they can do so many smart things with high efficiency, only because they can't do human ""intelligence"" jobs (e.g. object recognition) well.
Deep learning is called ""intelligence"", only because they can do some form of human ""intelligence"" jobs much better than ordinary programs.
But deep learning does not have the intelligence of human at all, they are essentially just some new form of algorithm that can solve some type of problems much better than ordinary programs.
Big language models are ""amazing"" only because they give human the feeling of ""amazing"" and human are so easy to be amazed. For example, a random number generator can get the right answer of 25% of multiple-choice questions, isn't that even more ""amazing"" and ""intelligent""? It is just the big language model algorithm happen to have this level of result, it doesn't matter the result is 25% or 75%, anyway it is not intelligent even in the right direction towards to human intelligence at all. So there will be definitely some upper limit of such algorithm that it can never ""fake above"", like the 25% for the random number generator.
Even if human intelligence is just an algorithm too, obviously it will be architecturally different than today's deep learning algorithms. The chance that you can happen to find the algorithm that is equivant to human intelligence is basically zero. Even more, if human intelligence can indeed be described by an algorithm, the algorithm will be too complex to run on ordinary computers for a high chance.
So why not just admit it is nothing but a new form of algorithm and use it to make life better and stop talking about intelligence? Is is understandable that the general public think these are intelligent and are on the road to AGI, but why does some AI professionals think deep learning is ""intelligent""? (some even says big language models begin to have consciousness...)
","['neural-networks', 'philosophy', 'agi']","
Partly due to the fact that deep learning can solve tasks considered to be challenging, and it does so by learning. This is the main differentiator w.r.t. old fashioned hardcoded algorithms. Indeed you will also encounter the wording artificial intelligence used for machine learning algorithms such as random forests or any other algorithm using learning. I'm not saying I agree with this, I'm just stating that it is what happens in practice.

But deep learning does not have the intelligence of human at all, they are essentially just some new form of algorithm that can solve some type of problems much better than ordinary programs.

As of today, you cannot back this statement. While we can understand how simple deep learning actually is, we don't fully understand human intelligence, so  it may turn out that human intelligence it's not that hard either.
"
How do you display a neural network,"
I'm new to tensorflow and ML but am progressing slowly. I know how to look at the weights and biases but am still trying to figure out if there is an easy way to display a neural network in the standard pictorial way. If anyone knows and could share I would be really grateful.
","['neural-networks', 'tensorflow']","
Is there an easy way to display a neural network in the standard pictorial way?
Each deep learning package has easy and pictorial options for model visualization.  In TensorFlow, the following model plotting utilities are available:  plot_model and model_to_dot.
"
"Autoencoder make spectrogram important parts more pronounced with a ""log loss""","
Hi I want to create a neural network that essentially picks out the most pronounced parts of a spectrogram.
Assume this is the True spectrogram:
[0.0, 0.25, 0.95, 0.1, 0.0, ...]
I want the auto encoder output to tend towards outputs like this:
[0.0, 0.05, 1.0, 0.0, 0.0, ...]
I essentially want to punish the neural network more for getting larger values more incorrect.
True 0.2, Predicted 0.1
True 0.9, Predicted 0.8 <---- Far greater loss
Since I don't care about background sounds this will make the important parts of the spectrogram more pronounced.

Will this work? What should I use?

","['tensorflow', 'autoencoders', 'unsupervised-learning', 'audio-processing']",
Doubts about zero-crossings detection in the visual system,"
I'm currently reading the excellent book ""Vision"" by David Marr. In chapter 2.2 (ZERO-CROSSINGS AND THE RAW PRIMAL SKETCH), he explains a possible model for the physiological detection of zero-crossings.
As zero-crossing we mean the locations of the image where the laplacian is zero (that represents a sudden change in intensity, i.e. an edge)

From Marr's book:

From a physiological point of view, zero-crossing segments are easy to detect without relying on the detection of zero values, which would be a physiologically implausible idea. The reason is that just to one side of the zero-crossing will lie a peak positive value of the filtered image ∇2G * I, and just to the other side, a peak negative value. These peaks will be roughly w2–D/ apart, where w2–D is the width of the receptive field center of the underlying filter ∇2G. Hence, just to one side, an on-center X cell will be firing strongly, and just to the other side, an off-center X cell will be firing strongly; the sum of their firings will correspond to the slope of the zero-crossing—a high-contrast intensity change producing stronger firing than a low-contrast change. The existence of a zero-crossing can therefore be detected by a mechanism that connects an on-center cell and an off-center cell to an AND gate,* as illustrated in Figure 2–18(a).

Marr, David. Vision (pp. 93-94). MIT Press. Kindle Edition.

I'm not quite able to fully understand why this would make sense since

The receptive field of P (on-center ganglion cell) is stimulated even on its periphery (and this would reduce its firing)
The receptive field of Q (off-center ganglion cell) isn't stimulated on its periphery (and this would reduce its firing too, right?)

Are these reasonable doubts or did I not fully understand Marr's model?
To note, in this video by Nancy Kanwisher from MIT, she explains a physiological model for edge detection (not necessarily zero-crossings detection) in a different way which makes way more sense to me:

","['computer-vision', 'neuroscience']",
Actor-Critic Algorithm - how to design and train $V$,"
Background
I've been watching the lecture of Standford's CS231n course, and one of the lectures (No. 14) is about RL. After talking about Q-Learning and Policy Gradients (REINFORCE algorithm), they explain how for policy gradients it can be helpful to add a baseline function. They state that a good baseline function is $A^{\pi}\left(s_{t},a_{t}\right)=Q^{\pi}\left(s_{t},a_{t}\right)-V^{\pi}\left(s_{t}\right)$, which gives us the estimator for the gradient:$$\nabla_{\theta}J\left(\theta\right)\approx\sum_{t\geq0}\left(Q^{\pi}\left(s_{t},a_{t}\right)-V^{\pi}\left(s_{t}\right)\right)\nabla_{\theta}\log\pi_{\theta}\left(a_{t}\mid s_{t}\right)$$
And they show the following algorithm (I think there a typo in the algorithm: Should the sum when setting $A_t$ be $\sum_{t'\geq t}\gamma^{t'-t}r_{t'}^{i}$?):

The Questions

Does this mean that now we have two neural networks? One for the policy and one for V?
How do we choose a model architecture for V?
Why is $\Delta\phi$ get the value it gets (the double sum)? Where does it come from and why is this the correct gradient? To me it looks like V will mimic Q (since we try to minimize the distance of V and Q) and I don't see how this helps depict the real V.

Notes
online I could only find explanations using the bellman principle on V [and not on Q, which I find quite strange since this principle talks about optimal solutions, and V represents the average and not optimum], which gives some strange formula and no explanation about how to calculate the gradient.
","['reinforcement-learning', 'q-learning', 'policy-gradients', 'actor-critic-methods', 'reinforce']",
Reach optimal values by not decreasing gradient,"
Is it possible to reach the optimal values ​​for the parameters by not applying the decreasing gradient in some layers?
","['neural-networks', 'deep-learning']","
If I understand correctly, your question is whether it's possible to randomly initialize some layers in a neural network and not apply gradient descent to them.
I believe the answer is yes. In synthesizer, they use a random matrix in the ""attention layer"" and did not train them, and still achieved reasonable results. Though I'm not sure how ""optimal"" that is. See here:
https://arxiv.org/abs/2005.00743
I'm sure this applies to other networks (e.g. CNNs) as well. Is there a particular application you have in mind for this?
"
Mapping an image to a well defined object using OpenCV,"
I am completely new to computer vision and I am working on a small hobby project.
The goal is to use camera footage of a foosball table to map the image to already well defined object geometry with few moving parts.
To illustrate the exact problem the input could be this:

I am looking to identify all the features of the image and map them to an exact model that could be rendered like so:

It shows the position of the ball and player rods.
I would already have an exact definition of what the image contains like:

Exact size of the playing field
Exact positions of player rods relative to the length of the field
The player sizes and positions on the bar
Every detail of the table is described already in the model and can not deviate from it

Some limitations:

The image would always be at an angle. So part of the playing field is cropped and would have to be extrapolated.
The lighting can vary from one set-up to another.

There are only a few moving parts that need to pinned pointed- the position of the rods (can for now disregard the rotation) and the position of the ball.
It sounds quite complicated but given that I already can tell exactly what I am looking for maybe it can make the task easier.
What would be the most straight-forward approach to solving this problem? Any suggestions or further reading would be greatly appreciated.
My current ideas would be to maybe identify a few anchor points from the image. For example if I would identify the top edge of the playing field - where the green playing pitch ends. And If I were to then be able to identify the position of the top player rod - the goalie rod. Measuring the distance between these lines would allow me to calculate the height of the camera angle because I know the exact spatial relationship between these two features in the pre-defined table model/geometry. With this data I would be able to calculate the positions where all the other features in the image could be located. If I could narrow down areas in the picture that need to be looked at for special features of the table I could maybe add additional constraints on the things that need to be looked at and simplify the task further.
Another idea I had was using the table surface image to identify the table orientation. For instance given the above input picture I might also have a hardcoded reference picture like this:

Can this reference picture be used to find it in the input image in a skewed / cropped way and infer the table position from this overlaying of the two images?
","['image-recognition', 'object-recognition', 'opencv']",
Expected Revenue Using Gradient Boost for Regression,"
I have trained a ML algorithm (gradient boost) to do regression on banana prices, such that I can guess the selling price of any given banana. Using scikit's regression boost algorithm, I am able to train the model using a quantile loss function, which in turn lets me have confidence intervals.
Now, my issue is, I would like to for any given banana X determine, the expected minimum price I can sell it for (i.e the maximum I should buy it for).
The way I have done it so far, is to calculate the lower bound of the 95% confidence interval (let us call this 0.05-min) for X and multiplied that price by the empirical accuracy of 0.05-min (which is approx 0.05). This is the expected minimum price.
Now, this price has been very low, so I decided to calculate the lower bound of the 50% C.I, and done the same thing (multiply the expected price of X by 0.5). This way I get a much higher maximum buying price/minimum selling price.
Statistically speaking, are the two minimum selling prices telling me the same thing? I.e what I (on average) can expect to sell the banana of type X for? If not, how should I interpret them differently? If none of the methods achieve my aim, what should I do instead?
","['regression', 'statistics', 'non-linear-regression', 'gradient-boosting']",
Does the order of a Numpy array matter for CNN classification?,"
Image classification of RGB images in a Convolutional Neural Network is usually done by feeding the CNN first layer with a Numpy array with dimensions: pixel array by the number of channels. Example: [256 pixels] x [256 pixels] x [3 channels].
What is the impact on performance if applying a different Numpy array in a different order: say [256 pixels] x [3 channels] x [256 pixels], assuming that the CNN first layer structure is changed?

Does the CNN architecture of the sub-sequent layers need to be changed?
Would the model prediction accuracy be impacted?

My use case is a bit different where I have multiple features in a 5 dimensional Numpy Array [X pixels],[Y pixels],[N size],[M size],[L size] and I'm concerned that the order of how the features are entered into the CNN will impact the model training.
","['convolutional-neural-networks', 'python', 'image-processing']","
It shouldn't matter for the accuracy of the network. But note that tensorflow/keras uses the (N, W, H, C) convention whereas pytorch uses the (N, C, W, H). So depending on what library you use, you need to make sure the inputs are shaped correctly before feeding it to the network.
If you are trying to build a nerual network from scratch and training it with numpy, then ideally you would make the spatial dimensions last (..., W, H), to interface with functions such as np.convolve efficiently.
"
Semantic detection of domain names used for phishing,"
I present you my problem which seems quite simple but I don't know if it is possible to implement my problem with an algorithm. I tried to implement it with a simple program based on Bayes' theorem but it did not work.
I present you my problematic:
I want to perform a predictive detection of malicious domain names registered every day by some attackers. I only want to perform a semantic analysis and not DNS, whois etc... Just focused on the domain name.
I have two sets of data: malicious and healthy domain names. So I can train my engine without any problem.
To summarize:

the program takes as input the list of domain names registered every day in "".fr"" only
I want my engine to detect and score the probability that a domain name is malicious.

Examples of malicious domain names:
amelieservice[.]fr
xn--amel-8pa[.]en
amelifr-cpam[.]fr
ameli-vosdroits[.]fr
assurance-maladie-vitales[.]fr
assure-ameli-verification[.]fr
carte-vitales-ameli[.]fr
account-amelie[.]fr
cpamfr-ameli[.]fr
espaceanteameli[.]fr
informations-votresante[.]fr
infosantesecuvitale[.]fr
joptimisemaretirement[.]fr
lassurance-maladle[.]fr
macaisseameli[.]fr
masanteameli[.]fr
ma-secu-ameli[.]fr
mon-assurancemaladie[.]fr
portal-ameli[.]fr
santepublique-france[.]fr
servicedesante[.]fr
What algorithm can I use for such an implementation?
Is my idea impossible to implement?
Thank you for your help,
","['python', 'algorithm']",
Using pre-trained models on image dataset that is totally different for object detection?,"
I have been trying out various tutorials on object detection machine learning. All the tutorials so far have been to use a pre-trained model for practical reasons when detecting objects that the pre-trained model learnt (e.g cats & dogs). However, will this pre-trained model work if I input a few hundred images of a particular car engine part and predict this class, which the pre-trained model did not train on? Is it recommended to make a model from scratch in this case?
I am further confused by this in TensorFlow documentation (Images -> Transfer learning and fine-tuning), the summary states:

Using a pre-trained model for feature extraction: When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain

By that meaning, if I need to predict a particular car engine part then this statement seems to suggest I create a model from scratch?
TLDR: Will pre-trained model be able to work on image dataset that it has never learn before or better to work on a model from scratch?
","['convolutional-neural-networks', 'object-detection', 'image-segmentation']","
Besides the answer from @GKozinski, you can also fine tune all the layers of the pretrained model.
Yes, it is incredibly commonplace to do transfer learning using pretrained models from different domains.
"
Multi-objective training involving maximization of one loss function and minimization of another,"
I need my model to predict $s$ from my data $x$. Additionally, I need the model to not use signals in $x$ that are predictive of a separate target $a$. My approach is to transform $x$ into a representation $\Psi(x)$ such that it's good at predicting $s$ but bad at predicting $a$.
Concretely, let the prediction for $s$ be
\begin{equation} \hat{s} = (\Gamma \circ \Psi)(x), \end{equation}
and that for $a$ be
$$\hat{a} = (\Omega \circ \Psi)(x),$$
where $\Psi, \Gamma, \Omega$ are implemented as MLP layers. Further, let $\ell_s(\Psi,\Gamma)$ and $\ell_a(\Psi,\Omega)$ be the loss functions corresponding to $s$ and $a$, respectively.
The objective thus is to simultaneously

minimize $$\ell_s(\Psi,\Gamma)$$ (good at predicting $s$), and
maximize $$\min_\Omega \ell_a(\Psi,\Omega) := -f(\Psi)$$ (bad at predicting $a$)

There may not exist a $(\Psi, \Gamma)$ combination that simultaneously achieves both. So the hope is to find something on the Pareto frontier by linearly combining the objectives:
\begin{equation}
\tag{*}\label{*}
\min_{\Psi, \Gamma} \left[ \ell_s(\Psi,\Gamma) + \alpha f(\Psi) \right] 
= \min_{\Psi, \Gamma} \left[ \ell_s(\Psi,\Gamma) - \alpha \min_\Omega \ell_a(\Psi,\Omega) \right] 
\end{equation}
where $\alpha > 0 $ is a hyperparameter that trades off the two original objectives.

However, I'm not sure how to train the layers $\Psi, \Gamma, \Omega$ to achieve \eqref{*}.
One procedure I could think of is to alternate between

Freeze $\Psi, \Gamma$ and tune $\Omega$ to minimize $\ell_a(\Psi,\Omega)$,
Freeze $\Omega$ and tune $\Psi, \Gamma$ to minimize $\ell_s(\Psi,\Gamma) - \alpha \ell_a(\Psi,\Omega)$

until convergence. Would that work?
","['optimization', 'multilayer-perceptrons', 'representation-learning', 'multi-task-learning']","
It's probably important that both steps be able to tune the learned representation Î¨. Also, that you are always minimizing the combined loss.
You should do:

"
How to make inference in energy based models,"
How do you make inferences in energy-based models? If there is an infinite set of continuous inference points, how do you choose them?
","['machine-learning', 'deep-learning', 'inference']",
How to compare different trajecories in a Markov Decision Process,"
I realize that my question is a bit fuzzy and I am sorry for that. If needed, I will try to make it more rigorous and precice.
Let $\mathcal{M}$ be a Markov Decision Process, with state space $\mathcal{S}$ and action space $\mathcal{A}$.
Let $\tau = (s_0, a_0, s_1, a_1, s_2, a_2, \dots)$ and $\tau' = (s_0', a_0', s_1', a_1', s_2', a_2', \dots)$ be two trajectories produced by an agent during two different episodes.
Question: Is there any standard way in the Reinforcement Learning literature to compare $\tau$ and $\tau'$? Ideally I am interested in finding a ""distance"" (it does not need to be a distance in the mathematical sense) $d(\tau, \tau ')$ such that it reflects the ""distance"" between the poicies that generated $\tau$ and $\tau'$.
For example it would be nice if $d(\tau, \tau')$ would be a good estimator of the KL divergence of $\pi$ and $\pi'$, where $\pi$ is the policy that generated $\tau$ and $\pi'$ the policy that generated $\tau'$.
","['reinforcement-learning', 'markov-decision-process', 'kl-divergence']",
Why do implementations of PixelCNN / Gated PixelCNN use the same number of channels for every Masked/Gated Convolution?,"
For example:

https://github.com/singh-hrituraj/PixelCNN-Pytorch/blob/master/Model.py
https://github.com/rogertrullo/Gated-PixelCNN-Pytorch/blob/master/GatedPixelCNN.py

You can see in these implementations, that the number of channels is constant (excluding input and output channel). There are also more implementations that do the same thing. I cannot find anything in the Papers, that require such a behaviour.
","['generative-model', 'variational-autoencoder']",
Do ML Models with sparsely accessed layers in the middle or end exist?,"
I am currently researching ML training, specifically if  layers are accessed in a
sparse or dense fashion. A linear layer an example for dense access, as all parameters are required during the forward pass.
An embedding table is an example for sparse accesses. Depending on the input data, only a few rows are accessed per pass.
The common place of sparsely accessed layers seems to be at the beginning of a forward pass in a model.
Usually, high-dimensional one-hot-encoded or sparse input data is transformed into dense embedding-vectors using an embedding table.
This table is the sparsely accessed component, whereas the rest of the model is accessed in a dense fashion.
I am wondering if sparsely accessed model components are always placed towards the front of the model, or if other approaches like the self-attention components of Transformer models exist.
Are there any popular machine learning models which utilize embedding tables or similar components in the middle or the last layers of the model?
","['natural-language-processing', 'transformer', 'pytorch', 'sparse-word-embedding']",
how to appy action mask,"
I'm trying to figure out how action masking works and the closest workaround i get is following the hanabi environment example and then write my own custom version to learn more:
it is my very basic choosing numbers game working with action_mask
"""""" based on RL environment for Hanabi, using an API similar to OpenAI Gym.
when obs return [0,0] available actions are [0,1,2]
when obs return [1,0] available actions are [0,2,3]
when obs return [0,1] available actions are [0,1,4]
when obs return [1,1] available actions are [0,3,4]
""""""
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import numpy as np
import gin.tf
from tf_agents.environments import py_environment
from tf_agents.specs import array_spec
from tf_agents.trajectories import time_step as ts

class CustomMaskedEnvironment(py_environment.PyEnvironment):
def __init__(self, gamma=0.95):
    super().__init__()       
    self.gamma = gamma
        
    self.one = 0
    self.two = 0
        
    self.invalid_action = 0
        
    self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int_, minimum=0, maximum=1)
    self._observation_spec = {
        'observation': array_spec.ArraySpec(shape=(2,), dtype=np.float32),
        'legal_moves': array_spec.ArraySpec(shape=(), dtype=np.bool_),
}
        
def observation_spec(self):
    return self._observation_spec
    
def action_spec(self):
    return self._action_spec

def _reset(self):
    self.state = self._make_observation()

    obs = self.state
    legal_moves = self.num_moves()
        
        
    observations_and_legal_moves = {
        'observation': obs,
        'legal_moves':np.logical_not(legal_moves),
    }
    return ts.restart(observations_and_legal_moves)

def _step(self, action):
    if self._current_time_step.is_last():
        return self.reset()
        
    # Apply the action to the state.
    self.apply_move(action)

    obs = self._make_observation()
    legal_moves = self.num_moves()
        
    done = self.state.is_terminal()
    reward = self.invalid_action
    self.invalid_action = 0

    observations_and_legal_moves = {
        'observation': obs,
        'legal_moves': np.logical_not(legal_moves),
    }

    if done:
        return ts.termination(observations_and_legal_moves, reward)
    else:
        return ts.transition(observations_and_legal_moves, reward, self.gamma)
    
def apply_move(self, action):
    if action == 0:
        pass 
    elif action == 1:
        if self.one == 1:
            self.invalid_action = -1
        else:
            self.one = 1
            self.invalid_action = 1
        elif action == 2:
            if self.two == 1:
                self.invalid_action = -1
            else:
                self.two = 1
                self.invalid_action = 1
        elif action == 3:
            if self.one == 1:
                self.one = 0
                self.invalid_action = 1
            else:
                self.invalid_action = -1
        elif action == 4:
            if self.two == 1:
                self.two = 0
                self.invalid_action = 1
            else:
                self.invalid_action = -1

def num_moves(self):
    action_mask = np.ones(5, dtype=np.uint8)
    # IF one and two are present in the observation
    # make available three and four
    if self.one == 1 and self.two == 1:
        action_mask[0] = 0 # wait
        action_mask[3] = 0 # make three available
        action_mask[4] = 0 # make four available
        
        # IF one is present in the observation make three available
    elif self.one == 1:
        action_mask[0] = 0 # wait
        action_mask[2] = 0 # two is available
        action_mask[3] = 0 # three is available
        
    # IF two is present in the observation make four available
    elif self.one == 1:
        action_mask[0] = 0 # wait
        action_mask[1] = 0 # one is available
        action_mask[4] = 0 # make four available
    
    # IF isnt one or two present in the observation
    else:
        action_mask[0] = 0 # wait
        action_mask[1] = 0 # one is available
        action_mask[2] = 0 # two is available
    
    return action_mask

def _make_observation(self):
    one  = self.one
    two  = self.two
    obs = np.array([one, two], dtype=np.float32)
    
    return obs
    
from tf_agents.agents.reinforce import reinforce_agent
from tf_agents.agents.dqn import dqn_agent
from tf_agents.agents.categorical_dqn import categorical_dqn_agent
from tf_agents.networks import categorical_q_network
from tf_agents.networks import q_network
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.utils import common
import tensorflow as tf
from tf_agents.environments import tf_py_environment
import reverb
from tf_agents.networks.mask_splitter_network import MaskSplitterNetwork
from tf_agents.networks.actor_distribution_network import ActorDistributionNetwork 
from tf_agents.specs import tensor_spec
from tf_agents.replay_buffers import reverb_replay_buffer
from tf_agents.replay_buffers import reverb_utils

optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025)
train_step_counter = tf.Variable(0)
fc_layer_params = (256, 256)

def filter_fun(observation):
    return observation['observation'], observation['legal_moves']

train_py_env = CustomMaskedEnvironment()
train_env = tf_py_environment.TFPyEnvironment(train_py_env)

eval_py_env = CustomMaskedEnvironment()
eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)

num_eval_episodes = 10

masked_actor_network = MaskSplitterNetwork(
    splitter_fn=filter_fun,
    wrapped_network= ActorDistributionNetwork(
       train_env.observation_spec(['observation'],
   train_env.action_spec(),
   fc_layer_params=fc_layer_params
   ),
   passthrough_mask=True
)

agent = reinforce_agent.ReinforceAgent(
    train_env.time_step_spec(),
    train_env.action_spec(),
    actor_network=masked_actor_network,
    optimizer=optimizer,
    normalize_returns=True,
    train_step_counter=train_step_counter,
)
agent.initialize()

eval_policy = agent.policy
collect_policy = agent.collect_policy

def compute_avg_return(environment, policy, num_episodes=10):
    total_return = 0.0
    for _ in range(num_episodes):
        time_step = environment.reset()
        episode_return = 0.0

    while not time_step.is_last():
        action_step = policy.action(time_step)
        time_step = environment.step(action_step.action)
        episode_return += time_step.reward
    total_return += episode_return

    avg_return = total_return / num_episodes
    return avg_return.numpy()[0]

replay_buffer_capacity = 90000

table_name = 'uniform_table'
replay_buffer_signature = tensor_spec.from_spec(
      agent.collect_data_spec)
replay_buffer_signature = tensor_spec.add_outer_dim(
      replay_buffer_signature)
table = reverb.Table(
    table_name,
    max_size=replay_buffer_capacity,
    sampler=reverb.selectors.Uniform(),
    remover=reverb.selectors.Fifo(),
    rate_limiter=reverb.rate_limiters.MinSize(1),
    signature=replay_buffer_signature)

reverb_server = reverb.Server([table])

replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(
    agent.collect_data_spec,
    table_name=table_name,
    sequence_length=None,
    local_server=reverb_server)

rb_observer = reverb_utils.ReverbAddEpisodeObserver(
    replay_buffer.py_client,
    table_name,
    replay_buffer_capacity
)

def collect_episode(environment, policy, num_episodes):
    driver = py_driver.PyDriver(
    environment,
    py_tf_eager_policy.PyTFEagerPolicy(
        policy, use_tf_function=True),
    [rb_observer],
    max_episodes=num_episodes)
    initial_time_step = environment.reset()
    driver.run(initial_time_step)


# (Optional) Optimize by wrapping some of the code in a graph using TF function.
agent.train = common.function(agent.train)

# Reset the train step
agent.train_step_counter.assign(0)

# Evaluate the agent's policy once before training.
avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
returns = [avg_return]

for _ in range(num_iterations):

  # Collect a few episodes using collect_policy and save to the replay buffer.
  collect_episode(
      train_py_env, agent.collect_policy, collect_episodes_per_iteration)

  # Use data from the buffer and update the agent's network.
  iterator = iter(replay_buffer.as_dataset(sample_batch_size=1))
  trajectories, _ = next(iterator)
  train_loss = agent.train(experience=trajectories)  

  replay_buffer.clear()

  step = tf_agent.train_step_counter.numpy()

  if step % log_interval == 0:
    print('step = {0}: loss = {1}'.format(step, train_loss.loss))

  if step % eval_interval == 0:
    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
    print('step = {0}: Average Return = {1}'.format(step, avg_return))
    returns.append(avg_return)

everything seems to works fine but i get the following error messages:

logits = tf.compat.v2.where(
tensorflow.python.framework.errors_impl.InvalidArgumentError:
Exception encountered when calling layer
""CategoricalProjectionNetwork"" (type CategoricalProjectionNetwork).


condition [1,5], then [1,2], and else [] must be broadcastable
[Op:SelectV2]
Call arguments received by layer ""CategoricalProjectionNetwork"" (type
CategoricalProjectionNetwork):   • inputs=tf.Tensor(shape=(1, 256),
dtype=float32)   • outer_rank=1   • training=False   •
mask=tf.Tensor(shape=(1, 5), dtype=bool)
[reverb/cc/platform/default/server.cc:84] Shutting down replay server

but acctually without a graphic card maybe reverb is not in my ""usable options"" just a workaround, i would migrate my code to pytorch, rllib, whatever that gives me a usable solution
","['reinforcement-learning', 'tensorflow', 'keras', 'pytorch']",
"IndexError: tensors used as indices must be long, byte or bool tensors","
I want to implement a dueling double DQN algorithm for selecting multiple discrete actions. Since the existing codes available on Github are for choosing a single action, I should modify them.
But when I changed the  choose_action function of D3QNAgent to get multiple actions, I got the following error:
IndexError: tensors used as indices must be long, byte or bool tensors

The complement explanation of the error is
Traceback (most recent call last):
  File ""C:/Users/Desktop/D3QN.py"", line 339, in <module>
    agent.learn()
  File ""C:/Users/Desktop/D3QN.py"", line 157, in learn
    q_pred = T.add(Vs, (As - As.mean(dim=1, keepdim=True)))[indices, actions]  
IndexError: tensors used as indices must be long, byte or bool tensors


Here is the whole code:
import gym
import numpy as np
import copy, json, argparse
import torch as T
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import os
from random import uniform

class ReplayBuffer(object):

    def __init__(self, mem_size, input_shape):
        self.memorySize=mem_size
        self.counter=0

        self.state_transition=np.zeros((self.memorySize, *input_shape), dtype=np.float32)
        self.newstate_transition = np.zeros((self.memorySize, *input_shape), dtype=np.float32)
        self.action_transition = np.zeros((self.memorySize, *input_shape), dtype=np.int32)
        self.reward_transition=np.zeros(self.memorySize, dtype=np.float32)
        self.terminal_transition=np.zeros(self.memorySize, dtype=np.uint8)

    def store_transition(self,state, new_statte, action, reward, done):
        index=self.counter%self.memorySize

        self.state_transition[index]=state
        self.newstate_transition[index] = new_statte
        self.action_transition[index] = action
        self.reward_transition[index] = reward
        self.terminal_transition[index] = done

        self.counter+=1

    def sample_memory(self, batch_size):
        max_mem=min(self.counter, self.memorySize)
        batch=np.random.choice(max_mem, batch_size, replace=False)

        state=self.state_transition[batch]
        new_state = self.newstate_transition[batch]
        action = self.action_transition[batch]
        reward = self.reward_transition[batch]
        done=self.terminal_transition[batch]

        return state, new_state, action, reward, done

class D3QN(nn.Module):

    def __init__(self, lr,   n_action, input_shape,name,  check_dir):
        super(D3QN, self).__init__()
        self.check_dir=check_dir
        self.checkpt_file=os.path.join(self.check_dir, name)

        self.L1=nn.Linear(*input_shape, 512)
        self.V=nn.Linear(512,1)
        self.A=nn.Linear(512, n_action)

        self.optimizer=optim.Adam(self.parameters(), lr=lr)
        self.loss=nn.MSELoss()

        self.device=T.device('cuda:0' if T.cuda.is_available() else 'cpu')
        self.to(self.device)

    def forward(self, state):
        flat1=F.relu(self.L1(state))
        V=self.V(flat1)
        A=self.A(flat1)

        return V, A

    def save_checkpoint(self):
        print('... saving checkpoints ...')
        T.save(self.state_dict(), self.checkpt_file)

    def load_checkpoint(self):
        print('... loading checkpoints ...')
        self.load_state_dict(T.load(self.checkpt_file))

class D3QNAgent(object):

    def __init__(self, nUsers, nChannel, gamma, epsilon,lr, n_action, input_dims, mem_size, batch_size, eps_min=0.01, eps_decay=5e-7, replace=1000,  check_dir='tmp\D3QN'):
        self.nUsers=nUsers
        self.nChannel=nChannel
        self.gamma=gamma
        self.epsilon=epsilon
        self.lr=lr
        self.n_action=n_action
        self.input_dims=input_dims
        self.mem_size=mem_size
        self.batch_size=batch_size
        self.eps_min=eps_min
        self.eps_decay=eps_decay
        self.replace_target_cnt=replace
        self.check_dir=check_dir
        self.learn_step_counter=0
        self.action_space=[i for i in range(self.n_action)]

        self.memery = ReplayBuffer(self.mem_size, self.input_dims)
        self.q_eval = D3QN(self.lr, self.n_action, input_shape=self.input_dims, name='D3QN q_eval',
                           check_dir=self.check_dir)
        self.q_next = D3QN(self.lr, self.n_action, input_shape=self.input_dims, name='D3QN q_next',
                           check_dir=self.check_dir)

    def choose_action(self,observation):
        actions=[]
        for i in range(self.nUsers):
            if np.random.random()> self.epsilon:
                state=T.tensor([observation], dtype=T.float).to(self.q_eval.device)
                _,advantage=self.q_eval.forward(state)
                action=T.argmax(advantage).item()
                actions.append(action)
            else:
                action=np.random.choice(self.action_space)
                actions.append(action)

        return actions

    def store_transition(self, state, new_state, action, reward, done):
        self.memery.store_transition(state, new_state, action, reward, done)

    def replace_target_network(self):
        if self.learn_step_counter % self.replace_target_cnt==0:
            self.q_next.load_state_dict(self.q_eval.state_dict())

    def epsilon_decreament(self):
        if self.epsilon>self.eps_min:
            self.epsilon=self.epsilon-self.eps_decay
        else:
            self.epsilon=self.eps_min

    def save_models(self):
        self.q_eval.save_checkpoint()
        self.q_next.save_checkpoint()

    def load_models(self):
        self.q_eval.load_checkpoint()
        self.q_next.load_checkpoint()

    def learn(self):
        if self.memery.counter < self.batch_size:
            return
        self.q_eval.optimizer.zero_grad()
        self.replace_target_network()

        state, new_state, action, reward, done = self.memery.sample_memory(self.batch_size)
        states = T.tensor(state).to(self.q_eval.device)
        new_states = T.tensor(new_state).to(self.q_eval.device)
        actions = T.tensor(action,dtype=T.int32).to(self.q_eval.device)
        rewards = T.tensor(reward).to(self.q_eval.device)
        dones = T.tensor(done).to(self.q_eval.device)

        indices = np.arange(self.batch_size)  ##### Critical and necessary

        Vs, As = self.q_eval.forward(states)
        Vs_new, As_new = self.q_next.forward(new_states)
        Vs_eval, As_eval = self.q_eval.forward(new_states)

        q_pred = T.add(Vs, (As - As.mean(dim=1, keepdim=True)))[indices, actions]  # for sampled actions
        q_next = T.add(Vs_new, (As_new - As_new.mean(dim=1, keepdim=True)))  # for all actions
        q_eval = T.add(Vs_eval, (As_eval - As_eval.mean(dim=1, keepdim=True)))

        max_actions = T.argmax(q_eval, dim=1)
        q_next[dones]=0.0
        q_target=rewards+self.gamma*q_next[indices,max_actions]

        loss=self.q_eval.loss(q_target,q_pred).to(self.q_eval.device)
        loss.backward()
        self.q_eval.optimizer.step()
        self.learn_step_counter+=1

        self.epsilon_decreament()

class Environ:

    def __init__(self, nUsers,nChannel, Noise, Pmax, Rmin, negative_cost, circuitPower, BW):
        self.nUsers = nUsers
        self.nChannel=nChannel
        self.Noise=Noise
        self.Pmax=Pmax
        self.Rmin=Rmin
        self.negative_cost=negative_cost
        self.circuitPower=circuitPower
        self.BW=BW

        self.state_dim= nUsers
        self.action_dim=nChannel
        self.bs = complex((500 / 2), (500 / 2))
        self.S=(np.zeros(self.nUsers)).reshape(-1)

    def Location(self):
        rx = uniform(0, 500)
        ry = uniform(0, 500)
        Loc = complex(rx, ry)
        return Loc

    def PathGain(self,Loc):
        d = abs(Loc - self.bs)
        d = d ** (-3)
        u = np.random.rand(1, 1)
        sigma = 1
        x = sigma * np.sqrt(-2 * np.log(u))
        h = d * x
        return h

    def reset(self):  # Reset the states
        s=np.zeros(self.nUsers)
        return s.reshape(-1)

    def RecievePower(self,UsersLoc):
        H=self.PathGain(UsersLoc)
        UsersRecievePower = H*self.Pmax
        return UsersRecievePower

    def TotalRate(self, actionRB):
        interference = np.zeros(self.nUsers, dtype=float)+self.Noise
        RecievePower = np.zeros(self.nUsers, dtype=float)
        SINR = np.zeros(self.nUsers, dtype=float)
        Rate = np.zeros(self.nUsers, dtype=float)
        for i in range(self.nUsers):
            Loc_i = self.Location()
            for j in range(self.nUsers):
                if j!=i and actionRB[i] ==actionRB[j] :
                    Loc_j = self.Location()
                    RecievePower[j] = self.RecievePower(Loc_j)
                    interference [i]= interference [i]+ RecievePower[j]
                else:
                    interference[i]= interference[i]
            RecievePower[i] = self.RecievePower(Loc_i)
            SINR[i] = RecievePower[i] / interference[i]
            Rate[i] =self.BW*( np.log2( SINR[i]))
        return Rate

    def computeQoS(self,actionRB):
        TotalRate=self.TotalRate(actionRB)
        QoS= []
        for i in range(self.nUsers):
            if TotalRate[i] >=self.Rmin:
                QoS.append(1.0)
            else:
                QoS.append(0.0)
        return QoS

    def ComputeState(self,actionRB):
        QoS=self.computeQoS(actionRB)
        S = np.zeros( self.nUsers)
        for i in range(self.nUsers):
            S[i]=QoS[i]
        self.S=S
        return self.S.reshape(-1)

    def Reward(self,actionRB):
        Rate = self.TotalRate( actionRB)
        QoS=self.computeQoS(actionRB)
        Satisfied_Users = sum(QoS)
        TotalRate = 0.0
        TotalPower = self.circuitPower
        for i in range(self.nUsers):
            TotalRate = TotalRate + Rate[i]
            TotalPower = self.Pmax
        if Satisfied_Users == self.nUsers:
            reward = TotalRate / TotalPower
            done=True
        else:
            reward = self.negative_cost
            done=False
        # print('Satisfied_Users= ', Satisfied_Users)
        return reward, done

    def step(self,actionRB):
        next_s = self.ComputeState(actionRB)
        r, d = self.Reward(actionRB)
        done = False
        info = None
        if d==True:
            done=True
        return next_s, r, done, info

def plot_learning_curve(x, scores,  filename, lines=None):
    fig=plt.figure()
    ax=fig.add_subplot(111, label=""1"")
    # ax2=fig.add_subplot(111, label=""2"")



    N = len(scores)
    running_avg = np.empty(N)
    for t in range(N):
        running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])

    ax.plot(x,running_avg , color=""C0"")
    ax.set_xlabel(""Training Steps"", color=""C0"")
    ax.set_ylabel(""Score"", color=""C0"")
    ax.tick_params(axis='x', colors=""C0"")
    ax.tick_params(axis='y', colors=""C0"")

    if lines is not None:
        for line in lines:
            plt.axvline(x=line)

    plt.savefig(filename)

nUsers=10
nChannel=30
Noise=1e-14
Pmax=0.01
Rmin=1e+6
negative_cost=-1
circuitPower=50*10**3
BW=180*10**3
max_nsteps=100

if __name__=='__main__':

    env = Environ(nUsers,nChannel, Noise, Pmax, Rmin, negative_cost, circuitPower, BW)

    episodes = 1
    load_checkpoint = False

    agent = D3QNAgent(nUsers, nChannel, gamma=0.99, epsilon=1.0, lr=5e-4,
                      n_action=nChannel, input_dims=[nUsers], mem_size=1000000, batch_size=64,
                      eps_min=0.01, eps_decay=1e-3, replace=100)

    if load_checkpoint:
        agent.load_models()

    filename = 'Resources.jpg'
    scores, eps_history = [], []

    for i in range(episodes):
        done = False
        observation = env.reset()
        score = 0
        nstep=0
        while  nstep<=max_nsteps:
            nstep+=1
            action = agent.choose_action(observation)
            new_observation, reward, done, infor = env.step(action)
            score += reward
            agent.store_transition(observation, new_observation, action, reward, int(done))
            agent.learn()
            observation = new_observation


        scores.append(score)
        avg_score = np.mean(scores[-100:])

        print('episode: ', i, 'score%.1f' % score, 'average score%.1f' % avg_score, 'epsilon %.2f' % agent.epsilon)

        if i>10 and i%10==0:
            agent.save_models()

        eps_history.append(agent.epsilon)

    x=[i+1 for i in range(episodes)]
    plot_learning_curve(x, scores, filename)



I would be grateful if anyone helps me to solve this error.
",['pytorch'],
saliency vs. sensitivity: proper definition and distinction,"
My understanding (to be critisized potentially)
In general, what I understand from saliency and sensitivity in a classification problem  is:

Sensitivity means how sensitive is the predicted class to changes in input features. E.g. given all normalized input features, if I change $X_1$ or $X_2$ for 1 unit, and the class flips for $X_1$ but not $X_2$, then I say the predicted class is more sensitive to $X_1$ than $X_2$
Saliency means how much a feature has contributed to forming the final class. For example for a ground-truth model (given all normalized) $P(Y=1) = \sigma(0.2 X_1 + 120 X_2)$, $X_2$ must have a higher saliency score. now in this example, imagine $X_2 = 5$. We know that changing one unit of $X_2$ does not change the probability that much, so the class is not that sensitive to changes in $X_2$.

In general, sensitivity wants to know how the predicted class is sensitive to perturbations in input features, whereas saliency is more interested in feature importance. For instance, in our logistic example $P(Y=1) = \sigma(0.2 X_1 + 120 X_2)$, if for a sample $X_2=0$ then $X_1$ has much higher sensitivity score than when $X_2 = 10$, whereas, $X_2$ is still a more important (high saliency) feature, as the absence of it ($X_2 = 0$) has played an important role in the assigned class.
Is there any problem with this understanding?
Question: Why literature is confused about it then?
IF my understanding is correct, then don't the gradient based saliency methods technically measure sensitivity, not saliency? In the equation
$$
M^c(X) = \partial P^c / \partial X
$$
the saliency map $M$ for class $c$ is defined as the derivative of class probability w.r.t. the input feature $X$ [1]. I see that to match the sensitivity definition. The question remains for perturbation-based methods as well, as perturbation is text-book sensitivity measurement.
Even more confusing is that there are methods that introduce $\text{input} \times \text{gradient}$ also as saliency [2]. So I was wondering if there is a piece of work that offers rigorous definitions for these two concepts.
","['deep-learning', 'explainable-ai', 'saliency-map', 'sensitivity']",
Understanding Probabilty in NEURAL MACHINE TRANSLATION,"
I am reading the paper ""Neural Machine Translation by Jointly Learning to Align and Translate"" (PDF), (May 19, 2016), by Bahdanau, Cho and Bengio.
I am having trouble with equation 2, page 3:

""The decoder is often trained to predict the next word $y_{t^{\prime}}$ given the context vector $c$ and all the previously predicted words $\left\{y_{1}, \cdots, y_{t^{\prime}-1}\right\}$. In other words, the decoder defines a probability over the translation $\mathbf{y}$ by decomposing the joint probability into the ordered conditionals:
$$
p(\mathbf{y})=\prod_{t=1}^{T} p\left(y_{t} \mid\left\{y_{1}, \cdots, y_{t-1}\right\}, c\right), \tag{2}
$$
where $\mathbf{y}=\left(y_{1}, \cdots, y_{T_{y}}\right)$. With an RNN, each conditional probability is modeled as
$$
p\left(y_{t} \mid\left\{y_{1}, \cdots, y_{t-1}\right\}, c\right)=g\left(y_{t-1}, s_{t}, c\right), \tag{3}
$$
where $g$ is a nonlinear, potentially multi-layered, function that outputs the probability of $y_{t}$, and $s_{t}$ is the hidden state of the RNN. It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used (Kalchbrenner and Blunsom 2013 )."".

My question here:
My start equation, according to the paper, is the following:
\begin{equation}
\mathbf{y} = [y_1,...,y_T]
\end{equation}

According to Bayesian rule: $P(A\cap{B}) = {P(A/B)}*{P(B)}$ this can be written:

\begin{equation}
P(\mathbf{y}) = P(y_1 \cap{y_2} \cap... \cap{y_T})
\end{equation}
which is equivalent to:
\begin{equation}
P(\mathbf{y}) = {P(y_T/ y_1 \cap{y_2} \cap... \cap{y_{T-1}})}*\fbox{$P(y_1 \cap{y_2} \cap... \cap{y_{T-1}})$}
\end{equation}
Applying again Bayes rule:
\begin{equation}
\fbox{$P(y_1 \cap{y_2} \cap... \cap{y_{T-1}})$} = {P(y_{T-1}/ y_1 \cap{y_2} \cap... \cap{y_{T-2}})}*{P(y_1 \cap{y_2} \cap... \cap{y_{T-2}})}
\end{equation}
which gives:
\begin{equation}
P(\mathbf{y}) = {P(y_T/ y_1 \cap{y_2} \cap... \cap{y_{T-1}})*{P(y_{T-1}/y_1 \cap{y_2} \cap... \cap{y_{T-2}})*P(y_1 \cap{y_2} \cap... \cap{y_{T-2}})}}
\end{equation}
which generalizes to:
\begin{equation}
P(\mathbf{y}) = {P(y_T/ y_1 \cap{y_2} \cap... \cap{y_{T-1}})*{P(y_{T-1}/y_1 \cap{y_2} \cap... \cap{y_{T-2}})*...*P(y_2/y_1)*P(y1)}}
\end{equation}
Giving something that has the same form as equation 2:
\begin{equation}
\fbox{$P(\mathbf{y}) = \prod_{t=1}^{T}{P(y_t/ y_1 \cap{y_2} \cap... \cap{y_{t-1}})}$}
\end{equation}
My problem is that, since the beginning, there was no context vector $c$ that is mathematically conditioning vector $\mathbf{y}$, and therefore it is absent from my final equation.
If anyone could help me solve this dilemma, I would be grateful!!
Thanks in advance.
","['natural-language-processing', 'transformer', 'bayesian-probability', 'encoder-decoder']",
What is the difference between planning and model-based machine learning?,"
I am familiar with planning. Given a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed to generate a state which contains the desired goals. The problem can be formalized as a MDP, the known transition and reward functions allow us to determine the next state and the next reward, according to the current state and the action to execute.
But, I do not really know model-based machine learning. And, when I read articles about this subject, I do not understand the difference with planning.
Thank you.
","['machine-learning', 'planning']",
Proof that the Policy Iteration Converges?,"
Let $\mathcal{X}=:\{x_1, x_2, x_3,...,x_n\}$ be the state space. Let $\mathcal{U}:=\{u_1, u_2, u_3,...,u_m\}$ be the set of actions. Let $A^{u_1}, A^{u_2}, A^{u_3},...,A^{u_m}$ be the state transition matrices corresponding to the actions $u_1, u_2, u_3,...,u_m$ respectively. Let $a^{u_k}_{i,j}$ denotes the $(i,j)^{th}$ entry of matrix $A^{u_k}$, i.e., $a^{u_k}_{i,j}=(A^{u_k})_{i,j}$, where $k\in\{1,2,3,...,m\}$. Let $\mathcal{R}(x_i,u_k)\in\mathbb{R}$ denotes the reward associated with the state $x_i$ and action $u_k$. Define the Bellman iteration
map $\mathcal{B}:\mathbb{R}^n\rightarrow \mathbb{R}^n$ as:
$$(Bv)_i=\max_{u_k\in\mathcal{U}} \{ \mathcal{R}(x_i,u_k)+\sum_{j=1}^{n}a^{u_k}_{i,j}(v)_i\}, $$
for an arbitrary vector $v\in\mathbb{R}^n$, where $(v)_i$ denotes the $i^{th}$ entry of $v$.
Let $\pi_{0}:\mathcal{X}\rightarrow \mathcal{U}$ be a deterministic policy. Let $v_{\pi_{0}}$ is the value vector corresponding to the policy $\pi_{0}$.
Now update the policy as follows:
$$\pi_{1}(x_i)=\arg \max_{u_k\in\mathcal{U}} \{\mathcal{R}(x_i,u_k)+\sum_{j=1}^{n} a^{u_k}_{i,j}(v_{\pi_0})_j\}.$$
Now calculate the value vector $v_{\pi_1}$ corresponding to the policy $\pi_1$. Again update the policy as follows:
$$\pi_{2}(x_i)=\arg \max_{u_k\in\mathcal{U}} \{\mathcal{R}(x_i,u_k)+\sum_{j=1}^{n} a^{u_k}_{i,j}(v_{\pi_1})_j\}.$$
Now calculate the value vector $v_{\pi_2}$ corresponding the policy $\pi_2$ and again update the policy as follows:
$$\pi_{3}(x_i)=\arg \max_{u_k\in\mathcal{U}} \{\mathcal{R}(x_i,u_k)+\sum_{j=1}^{n} a^{u_k}_{i,j}(v_{\pi_2})_j\}.$$
We keep iterating the policy in this manner, i.e.,
$$\pi_{k+1}(x_i)=\arg \max_{u_k\in\mathcal{U}} \{\mathcal{R}(x_i,u_k)+\sum_{j=1}^{n} a^{u_k}_{i,j}(v_{\pi_k})_j\}.$$
Now, how I can prove the following:
(1) $v_{\pi_{k+1}}\geq v_{\pi_{k}}$
(2) $v_{\pi_{k}}$ converges to optimal value vector $v^{*}$ as $k$ tends to $\infty$
(3) $v_{\pi_{k+1}}= Bv_{\pi_{k}}$
","['reinforcement-learning', 'proofs', 'value-functions', 'bellman-equations', 'policy-iteration']",
Why Soft Actor-Critic (SAC) uses a double Q trick?,"
Twin Delayed DDPG (TD3) uses a double Q trick since the policy is deterministic like in DDPG, which is to mitigate the maximum overestimation bias in DDPG. However, in SAC, the policy is stochastic, even so, it still uses the double Q trick. So it confuses me that if it doesn't serve the purpose of mitigating the maximum overestimation bias, what is the real intention of this use?
","['ddpg', 'off-policy-methods', 'soft-actor-critic']",
How feasible is it to partition a DNN model into functional pieces?,"
Just read Auto-Split: A General Framework of Collaborative Edge-Cloud AI by a group of Huawei researchers (https://arxiv.org/pdf/2108.13041.pdf). How feasible is it to break up the models and  serve them to the edges and the cloud?
If it is possible, is this task easy to implement? Does Tensorflow, PyTorch or cv.dnn have guides or APIs to achieve the task? I am speaking from the perspective of a small development team that are not experts at machine learning. I can only imagine splitting the model at a non-functional binary level, not at a functional subgraph levels.
","['deep-learning', 'deep-neural-networks']",
Find evils with AI (social deduction games),"
I’ve been playing a game called Town of Salem lately. The goal of the game is for the town to find and lynch all evils in the town. To lynch someone, they must be voted up to the gallows first. This gave me an idea. If we analyze the voting patterns, we can determine who is evil, and who isn’t. Evils won’t vote up other evils, but will vote up town. Town will vote up evils, but are less likely to vote up town.
I came up with a naive way to determine how evil or good someone is.
Note: EN means times not voting evil, T means times voting town, TN means times not voting town, and E means times voting evil.
The evil score will be calculated as 2EN + T.
The good score will be calculated as 2E + TN.
However, I chose these biases on a whim, and not scientifically. Also, I’d prefer one measure that says whether you should trust someone or not, rather than two measures that might conflict with each other.
Thus, I have arrived at my problem. What’s the best way to create a classification AI that takes in EN, T, TN, and E for a player and outputs whether they are evil or not? Should I use a random forest? Should I use a regular old neural network with 4 input nodes and 2 output nodes? How many hidden layers should I have?
By the way, I’ll be training my model on 30 games worth of data, which represents voting data for around 400 players, so my model will be using supervised learning.
Edit: Here are some notes about voting I glossed over. There are two types of voting: voting to put someone on trial, and voting whether someone is guilty or innocent. Town will vote up players to the stand, then the defendant will give a defense speech. Then town votes whether the person should die or not. My AI only cares about the first type of voting because most evils will vote guilty on other evils to not seem suspicious. Thus, the first form of voting is most reliable for determining who is evil.
","['machine-learning', 'classification', 'game-ai']",
Feature Engineering on transactional dataset clustering,"
I have a data set with transactions details from different business (roughly 1 thousand business entities). Each row is a transaction. The structure of the dataset is as follows:




client_id
Sex
Age
transaction_ammount
business_entity




123
M
88
4829
storeA


123
M
88
1049
storeB


255
F
25
1122
storeH




My goal is to cluster the clients depending on their consuming habits, age and sex.
I am having a hard time on deciding on the best features to feed this dataset into a clustering algorithm (probably K-means as a starter).
Some of the things I am planning to do are:

One hot encoding on: sex
Make each store be a column and each row value be the amount of transactions a certain user did pay to that store (for example, if user1 made two transactions to storeB, there will be a 2 in the user1 row on the storeB column).

One of the main things I am struggling with right now is how to sum the transactions' data per user. I would need to run an aggregated operation on them, but don't know which one would be better. Some of the ones I have in my mind:

Average transaction amount per user
Min transaction amount per user
Max transaction amount per user
The above 3 but for each store per user (which would mean that, if I have 1000 stores, I would have to add 3000 thousand columns). This makes sense since each store have a wide range of product prices and running an operation among all the transactions of a user will be misleading.

What feature engineering technique would you recommend me? Is there any additional data wrangling I should do?
","['datasets', 'data-preprocessing', 'clustering', 'feature-selection', 'feature-engineering']","
The average transaction is a central measure, while the minimum and maximum transactions together give an idea of dispersion. However, these can be very sensitive to individual purchases that might not be representative of the personal behaviour of a customer.
For instance, the year a consumer changes their fridge or washing machine an outlier purchase happens. You may want to filter that out of your analysis -or not!
Robust alternatives to explore include the median for location and the interquartile range for spread.
A log transformation of the transaction_ammount may be useful as well. This would transform your arithmetic mean into a geometric mean, thus reducing the impact of large purchases.
The table does not display a date column. That can also help modeling. You may have customers buying small quantities on random days and weekend customers going to your stores mainly on Saturday, or even once a month.
"
Derivation of the sparsemax loss function,"
While reading this interesting paper about a sparse variant of the reknown softmax activation function, I got stuck at the section about loss derivation. In particular, I'm having issues understanding what they call $L_{softmax}$ as a function associated with the softmax activation. They consider the following regularized problem:
\begin{equation}
\min_{\textbf{W},\textbf{b}} \frac{\lambda}{2} \|W\|^2_F + \frac{1}{N}\sum_{i=1}^NL(\textbf{W}\textbf{x}_i + \textbf{b} \, ; \, \textbf{y}_i) \, .
\end{equation}
As they state:

where $L$ is a loss function, $\textbf{W}$ is a matrix of weights, and
$\textbf{b}$ is a bias vector. The loss function associated with the
softmax is the logistic loss (or negative log-likelihood):
\begin{align} L_{softmax}(\textbf{z};k) &= -\log softmax_k(\textbf{z})\\ &= -z_k + \log \sum_j \exp(z_j) 
\end{align}

My question is: is their $L_{softmax}(\textbf{z};k)$ the cross-entropy loss? And how do we see that?
Suppose we have a binary classification problem, then our target $\textbf{y}_i$ is encoded by a one-hot vector like $\textbf{y}_i = (1,0)$. Our (linear) model $f_{W,b}(\textbf{x}_i) := \textbf{W}\textbf{x}_i + \textbf{b}$ is going to produce a logit vector $\textbf{z} = (z_1, z_2)$
that it is going to be turned into probabilities by a sigmoid function:
\begin{equation}
p_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} \, \, , \, \, p_2 = 1-p_1
\end{equation}
These predicted probabilities go inside the cross-entropy loss as $-\sum_i y_i \log p_i$. Given that what I state above is correct, how to see that $L_{softmax}$ and cross-entropy are the same? Probably the slightly different notation is confusing me as well.
","['classification', 'softmax', 'multiclass-classification', 'multi-label-classification']",
Object detection network structure 101,"
I have been working some time now with transfer learning on different types of networks (resnet, yolo, faster-rcnn, etc.), but most of it has been based on tutorials to train the networks on custom datasets.
I am now at a point where I think I ""understand"" how the different models work, but now I want to dive deeper and understand how the structures are built and trained in theory + the coding of the model networks.
I have tried googling, but finding useful material starting from a basic level is like finding a needle in a haystack.
Can anyone point me in the direction of a course, blog or whatnot that explains what happens between the layers of a model and how to code it?
",['object-detection'],
Predicting time spent to build a metal piece using RNN,"
My data consists in many metal pieces which are put together to make a final metal mould.
To make each of this metal pieces, machinery recieves many operations to follow, like chopping, facing, etc... (I don't know if this translations are good), and for each operation are used different tools, at different speeds, and other variables. This metal pieces don't always have the same amount of operations. My goal was to create a RNN model where I use as an input the operations of a metal piece and expect as an output the time to be built. Each input is supposed to be a list (the whole piece) of lists (the different operations required to make the piece). I have some questions about this:

The lists I want to use as an input contain different amount of sublists, even tho every sublist have the same length.
Are RNN's the best approach to this? And if so, which type of RNN's?

I'm sorry if it wasn't clear, but I don't know very much about RNN's and I struggle to explain this problem to other people.
If you know about any similiar problem or any pdf's that might help me in this problem, please tell me :)
Thank you!
","['neural-networks', 'recurrent-neural-networks', 'long-short-term-memory']",
Minimum sampling for maximising the prediction accuracy,"
Suppose that I'm training a machine learning model to predict people's age by a picture of their faces. Lets say that I have a dataset of people from 1 year olds to 100 year olds. But I want to choose just 9 (arbitrary) ages out of this 100 age dataset and still the model should be able to predict the age of a given person. My question is how should I choose the optimal 9 (arbitrary) ages out of the 100 age dataset, so the trained model would perform better across most of the ages?
The model will perform better if I train the model with the entire population, so the question is, how to approximate the performance of this model but with the minimum possible sample selection (not the number of observations, but with minimum number of ages.)
I should address the following questions

How many no of ages should I select from the entire age spectrum
what are the best ages that I should select to train the model

","['bayesian-optimization', 'thompson-sampling', 'sampling']",
Taxonomy of terms in DL [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 12 months ago.







                        Improve this question
                    



I am trying to teach myself DL and among other difficulties I found it quite challenging to structurize a very basic terminology vocabulary especially when it includes not only theoretical stuff but also commercial names.
These are my questions for now:
1.1. What is Cuda?
1.2 Is it used only for learning, or also for running an already trained model?
1.3 When I train a network with GPU, is the training always done with the help of Cuda, or can it also be done on the same GPU without Cuda?
(I've seen YouTube videos where people install Cuda, but in the online course I am taking these days, the lecturer did not mention it, he just gave a couple of instructions on how to install darknet for OCU and for GPU. Maybe he meant that if I have a GPU I've already installed the Cuda?)
1.4 Are Cuda and the Cuda programming languge the same thing?
1.5 Do I need the Cuda programming language to train neural networks on GPU? If not, when and where is this language used?
2.1 What is Jetson?
2.2 Is it an alternative framework just like darknet or tensorflow?
3.1 Why do we need Microsoft Visual Studio to train Yolo? (I am finishing an online course now and that is what needs to be installed.)
3.2 Are there alternatives to Microsoft Visual Studio for these goals?
4.1 amAbout the  frameworks: As far as I know when it comes to Yolo (for example), it can be trained on different frameworks.
But who supports who - is the framework while being created done in a specific way to support different networks (let's say Yolo and r-cnn) or does the one who creates a neural network do it in such a way that it can work with the desired frameworks (for example, tensorflow and darknet)?
4.2 When running the same network on different frameworks what are the differences while training? Do I need to use different commands and different synthax?
4.3 If I train on GPU and as a framework use darknet, and tomorrow I decide to move to tensorflow, not including the coding and syntax, are there things that should be installed again in a different way?
For instance, maybe darknet needs Microsoft Visual Studio while tensorflow doesn't...
","['convolutional-neural-networks', 'tensorflow', 'yolo', 'darknet']",
Predicting occurence within multiple time horizons,"
i recently found a paper (sorry cannot share) where it's about predicting occurence of an event within several future time horizons, e.g. 6h, 12h, 24h.... In order to address the problem that an positive predcition within a shorter time window implies a presence within longer time window, the author uses a cumulative distribution function layer (CDF). In particular,
We use a cumulative distribution function layer (CDF) across different time windows to encourage monotonicity, since the presence [...] within a shorter time window implies a presence [...] within a longer time window.
What is this cumulative distribution function layer? I cannot find any information? Do you have an idea?
","['neural-networks', 'activation-functions']",
How do I detect movements on 2D map and pathfinding?,"
Given a traditional 2D minimap (as you often see in video games), with a dot indicate the character position is always in the middle, how to effective guide the character to uncover the majority of the fog of war?
I already have a mini game of sort, it generates a 2D map like this:

The yellowish cross at the center is the character position. All the walls and obstacles are clearly outlined. The fog of war get uncovered gradually around a radius of the character position. When the character move (any 2d direction) the minimap instantly gets updated.
Goal 1: to make a program to, when given any shot of the minimap, will output one instruction Move(vec2d) to indicate the character should move to that direction.
Goal 2: The follow up goal would be to make this a continuous effort, the program will be feed with a stream of updated shots as the character obey the output instructions.
Example 1: if the given terrain is a narrow corridor, then the list of instructions should tell the character to continue going in the same direction from one side to the other side.
Example 2: If the given terrain is kinda circular, then it should tell the character to walk in one circle.
Example 3: If the given terrain is more complicated (like in the sample) with obstacles & walls, then it should tell the character how to navigate.
I'm a fullstack developer and this is my first try at learning machine learning or any AI at all.
What is the general directions should I follow to reach my stated goals?
What resources, learning materials should I consume first before I start?
I don't want a complete solution, I want to learn enough to solve this on my own.
",['machine-learning'],
How to use strong labels in image classification?,"
I have a dataset where I have the labels cancer & non-cancer, and I also have localized pixel-level annotation masks of important regions/features in the images.
In a binary classification task, how do I optimize my deep learning model to focus on the annotated regions of the image and ignore or pay less attention to the regions outside the annotations?
Additionally, I have a set of images with only image level labels. Is there a way to use both localized labels (strong labels) and image level labels (weak labels) in the same model?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision']","
This is a great question.  Sorry I only have 1 reputation point, so I can't upvote.  The optimal solution likely has to be empirically determined.  I don't think it can be determined a priori.  It would also depend on how you define as optimal (high sensitivity, high specificity, high AUC, or high accuracy).
One approach would be to develop a segmentation model and any image with ""tumor pixels"" would be classified at the image level as ""tumor"".  I don't know for sure (and no one really knows for sure), but I think this method would yield higher sensitivity (but possibly lower specificity) than a straight image classifier.  Which one is likely to be better depends on a lot of things, including how much training data you have, the quality of annotations, and the quality of images.
To answer your second question, yes there is.  There are probably several ways to integrate the different levels of information.  One simple way is to develop a model with weak labels and a model with strong labels and then do a probability fusion, similar to how it is done for multimodal data (see article).
"
How does a CNN work in detecting absence of features?,"
I'm trying to understand how a CNN operates internally. Let's say I'm doing binary classification with 1 output neuron and a sigmoid to classify dog vs no dog. No dog meaning the image does not contain a dog.
So what features does a CNN learn for no dog? Do the convulutional filters just learn the features of dogs and predict no dog when they aren't detected? Or do they learn some arbitrary features present in the no dog images?
Basically, I'm trying to understand how CNNs work to detect ""absence of features.""
","['deep-learning', 'convolutional-neural-networks', 'computer-vision']",
Are there limitations on network output architecture and action mapping in reinforcement learning?,"
I'm easing my way into a toy reinforcement learning problem where my objects can move and take different actions on a simple grid, but I'm having trouble understanding what constraints might exist in how I build my output layer and map it to the environent. I understand that, since vanilla backpropagation doesn't make sense in this context, we use methods like Policy Updating or Q-Learning to be able to differentiate with respect to the gradient. However, it is unclear to me if these methods impose constraints on the ""form"" of the output layer and its mappings to actions?
For instance, in the examples I read, typically there is one output node for each of the available actions in the environment. However, in a complex environment I could see one wishing for a variety of output forms. Perhaps, in my case, I desire to have one series of nodes determine the action to take and another node (i.e. a sin output for the angle) or two (i.e. determining the x/y) to determine the direction (i.e. ""grab"" action at ""angle/position""). Is this viable, and indeed are any arbitrary mappings from the output layer to the environment valid? Or does current reinforcement learning techniques constrain the output and mapping (such as each node must be a possible action choice)?
","['neural-networks', 'reinforcement-learning', 'backpropagation']",
Choosing proper graphic card for deep learning AND gaming [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 12 months ago.







                        Improve this question
                    



Though the combination between gaming and deep learning might not sound ""serious"" enough still this is the case - on the one hand, I need a great GPU for my son to play games, and it will be great if I can make ML on the same GPU. Thus my question is as follows:
I know that the line of GPUs called RTX, such as 3060,3060Ti up to 3090, and 3090Ti should do the job, but. When I go to online stores such as KSP, I get many ""subtypes"" of RTX 3090:

And I am afraid to purchase something that will only fit for gaming and not fit my needs. I am pretty new to ML and I know that a graphic card should support something called CUDA, are all of those cards in the image above support everything needed? Which one of them may be preferable?
","['machine-learning', 'deep-learning', 'gpu']",
use artificial intelligence in predicting the price of stocks,"
Is it possible to use artificial intelligence for example method like reinforcment learning, LSTM, ... in predicting the price of stocks or currencies like Bitcoin, etc.? And has the work been implemented and had a positive result so far?! If the answer is yes, what is the best method?
","['neural-networks', 'machine-learning']","
I searched “machine learning finance” on Google Scholar and hid a bunch of hits.
A few jump out as being explicitly about predicting prices.
Culkin, Robert, and Sanjiv R. Das. ""Machine learning in finance: the case of deep learning for option pricing."" Journal of Investment Management 15.4 (2017): 92-100.
De Spiegeleer, Jan, et al. ""Machine learning for quantitative finance: Fast derivative pricing, hedging and fitting."" Quantitative Finance 18.10 (2018): 1635-1643.
Specifically for Bitcoin:
McNally, Sean, Jason Roche, and Simon Caton. ""Predicting the price of bitcoin using machine learning."" 2018 26th euromicro international conference on parallel, distributed and network-based processing (PDP). IEEE, 2018.
Chen, Zheshi, Chunhong Li, and Wenjun Sun. ""Bitcoin price prediction using machine learning: An approach to sample dimension engineering."" Journal of Computational and Applied Mathematics 365 (2020): 112395.
Further, it’s easy to find postings about machine learning jobs at financial firms.
Consequently, I say that finance is at least interested in applying modern machine learning methods in the pursuit of accurate predictions of asset prices.
Finally, no post on financial machine learning resources would be complete without mentioning Advances in Financial Machine Learning, written by a guy from the hedge fund AQR Capital Management.
"
"Is there a way to freeze training for weights, but not biases in PyTorch? [closed]","







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 11 months ago.







                        Improve this question
                    



I'm constructing a neural network where the weights of my first hidden layer (connected to the input) are all 1 (identity matrix), but the biases are variable.
Is there a way to ""freeze"" any updates/training to the weights in a specific layer, but continue to allow the biases in that specific layer to be updated?
","['neural-networks', 'training', 'pytorch', 'weights', 'bias']",
Can you make Q&A language model stay on topic?,"
I’m thinking of fine-tuning a pre-trained language model for a Q&A task. More specifically, I’d like to fine-tune the model on a single chapter in a classic college textbook. Afterward, the reader of the chapter should be able to engage in a Q&A session with the model about the content of the chapter.
But how do I make sure that the model stays on topic and doesn’t go out of a tangent? I know it is possible when looking at what https://play.aidungeon.io/ has achieved, but I don’t know if it will require me to build a model from the ground for each chapter. Can anyone tell me if I’m out of my mind or if it’s feasible?
Edit:
I've just learned that AI Dungeon's models has also struggled with a tendency to create graphic and sexual content despite not being prompted by players.
","['natural-language-processing', 'chat-bots']","
Perhaps you could train an auxiliary binary classifier to predict whether or not a response from the chatbot was on or off topic. You could use text from the chapters that you fine tune the Q.A. model with to generate examples of ""on topic"" text and then use text from other sources as ""off topic"".
Then, when the model generates an output, you could pass that through the binary on/off-topic classifier. If it is deemed off topic, you could then respond with some canned phrases, indicating that the chatbot is unable to answer the specific question. I have not tried this myself, but it seems to me like a reasonable place to begin.
"
Why does a quantile regression estimator underestimate the variance when using the quantile huber loss?,"
I have a question to quantile regression which is related to distributional Reinforcement Learning. Let the quantile loss (QL) be defined as
\begin{align*}
        \mathcal{L}^{\tau}_{\text{QR}}(\theta) = \mathbb{E}_{\hat{Z} \sim Z}[ \rho_{\tau}(\hat{Z} - \theta)],
\end{align*}
where $\rho_{\tau}(u) = u(\tau - \delta_{\{ u < 0\}})$ for all $u \in \mathbb{R}$. Furthermore, let the quantile huber loss (QHL) be given by
\begin{align}
    \rho_{\tau}^{\kappa}(u) = | \tau - \mathbf{1}_{\{u < 0\}} | \mathcal{L}_{\kappa}(u)
\end{align}
with
\begin{align*}
    \mathcal{L}_{\kappa}(u) = 
        \begin{cases}
            \frac{1}{2}u^{2} & ,\text{if } |u| \leq \kappa \\
            \kappa (|u|- \frac{1}{2}\kappa) & , \text{otherwise}.
        \end{cases}
\end{align*}
In the paper Distributional Reinforcement Learning with Quantile Regression the authors suggest to use the QHL with $\kappa = 1$ instead of the QL in order to avoid the non-differentiability at zero of the QL. However, if I want to learn a distribution, I find that a feedforward neural network does not manage to learn the distribution perfectly when using the QHL but does so when using the QL, see figure below. The theoretical cdf is just a standard normal distribution.
Question: Is there some theoretical justification why, when using the QHL, the network underestimates the variance?
Thanks for any tips and help in advance.

","['reinforcement-learning', 'objective-functions', 'regression', 'probability-distribution', 'density-estimation']",
Does the AlphaZero algorithm keep the subtree statistics after each move during MCTS?,"
This question is regarding the Monte Carlo Tree Search (MCTS) algorithm presented in the AlphaZero paper (arXiv). As described in the paper, each MCTS used 800 simulations to determine the next action. This process builds a search subtree downwards from the root note. During this process, statistics about the nodes (e.g. values & visit counts) are updated in backward passes upwards through the tree. After all 800 simulations are complete, the most promising child node is selected (i.e. the node with the most visits, normalized by temperature), and then 800 new MCTS simulations are started using the selected child node as the new root node.
Question: Once the next round of 800 MCTS simulations starts, do we discard the statistics from the previous tree and thereby start with a ""fresh"" subtree, or do we keep the statistics gathered from the previous round of simulations?
I have found several tutorials/blog posts/repositories that implement either of these options and are contradictory. Furthermore, the wording in the paper seems ambiguous as they speak of ""restarting"" but it is not clear whether they restart after every round of 800 MCTS simulations or after each game is complete.
","['reinforcement-learning', 'monte-carlo-tree-search', 'alphazero']",
Choose the right algorithm,"
I need to select the most suitable ML algorithm for an academic project. I have a dataset earn from lab which includes the long of an element and 3 factors that we believe impact the long of the mentioned element. I want to predict the length of the element by using those 3 factors.
Probably I should use supervised algos but which one?
","['machine-learning', 'algorithm']","
I find it useful to refer to scikit-learn's Choosing the right estimator cheatsheet, using it of course with a bit of consideration and thought.
In your case we are talking about a supervised regression problem in which you have to predict a numeric value from three other values. The first thing to note is that, having only three input features, you should probably first plot the data and try exploring it with your own eyes. You may find out that one or more of those features have a clear relationship with the target variable, for example a linear correlation.
In that case, you could try fitting a linear model to your data.
One important aspect that you don't mention is the number of data points you have available. Having enough data for both training and testing is essential, although the exact number depends on how complex the relationship between variables is. Good luck.
"
PRNG Function Prediction [duplicate],"







This question already has answers here:
                                
                            




Can a neural network be used to predict the next pseudo random number?

                                (4 answers)
                            

Closed 12 months ago.



Can any machine learning algorithm predict the function of pseudo random number generator given only its inputs and outputs?
Here, you can also assume that you don't know the seed for the PRNG.
","['machine-learning', 'function-approximation']","
Yes, this is possible but only within certain cases and depending on the amount of work one is willing to invest. All PRNGs make use of an underlying (deterministic) generative process that can be learned in theory. This blog post nicely illustrates how it could be done. The model learns to predict the next sequence of random numbers without knowing the seed. This makes intuitive sense since deep nets can approximate arbitrary functions and in practice it is often not even clear what the seed value of the generative process would be (e.g. during image classification).
In general, this question has no binary answer but instead depends on a number of factors like the complexity of the PRNGs, the available training data, the network size, and the available compute.
This questions has also been answered here and here.
"
Does sequence matter in LSTM?,"
In general, as long as the items are in order as per time sequence, does it matter if it's in ascending time sequence or descending time sequence for LSTM within the input vector?
eg:
ABCDEF -> G
BCDEFG -> I
vs
FEDCBA -> G
GFEDCB -> I
","['tensorflow', 'long-short-term-memory']",
What is the initial data input to an ASR system? Waveform image?,"
When speech data enters a trained ASR system, is it initially an IMAGE of a waveform? Is the image what is analyzed and sliced into phonemes? I'm just trying to determine whether actual graphics are used at some point in the process, or not.
",['speech-recognition'],
How to detect peak locations via Neural Networks?,"
As part of my masters thesis, I'm developing generative models for ECGs.
Right now, I have a Denoising Diffusion Implicit model (DDIM), that transforms random noise into a valid ECG (2s long, or 1024 sample points with a 512Hz sample frequency). It works quite well, but by itself, it is only capable of producing random ECGs. It would be of much more scientific value to produce a realistic ECG that has R-peaks at specific, predetermined points.
The solution would be a guided diffusion. This relies on an auxillary model, that takes the output of the diffusion model, and gives a prediction of where the peaks currently are. Then I calculate the gradient of the desired peak locations w.r.t the input data (i.e. the output of the DDIM). I add the gradients to the data, and get a slightly transformed noise. Then I can feed it back into the DDIM, and repeat the process until I get a clean ECG with peaks at the desired positions.
The problem is, I can't figure out a good way to do peak prediction using NNs.
Researching the problem, people usually recommend other algorithms for peak detection. But in my case, I have to do it via a NN, so I can calculate the gradients. Papers on the matter usually do Peak classification, or segmentation, not predicting the exact location.
Right now I have the following setup:

A sample ECG, that consists of 1024 points representing 2 seconds. It has 2 to 4 peaks (which is a physiological range for humans), of varying height, normalized to 1 being the highest R-peak.

My peak identifier network is a convolutional NN, that takes the ECG as input, and has 128 output nodes. Each node represents an 8 point range. So if node 5 has the output value of close to 1, it means the model predicts there is a peak somewhere between point 40 and point 48.


My problem now is actually the loss function. The ECG is annotated, so I know the actual peak location and can encode them into the same 128 8-point segments.
Right now I'm using simple categorical cross entropy between the real location and my networks predictions. However, this loses the aspect of ""distance"". If the peak really is in the 5th segment, and my network predicts a peak in the 4th segment, this should result in a lower loss than if the network predicts a peak in the 80th segment.
And because there is a variable amount of peaks (between 2 and 4), I can't simply calculate the distances between the predicted peaks and the real ones, because I don't know how to handle the case where there is 3 peaks, but my model predicts 5. Which peaks do I then use for distance calculation?
The cross entropy loss almost works. The model gets quite close, but it defaults to a strategy where if the real peaks are say at 24, 58 and 100, it predicts 23, 24 and 58 as the 3 highest output nodes. So regions around a peak often have higher outputs than another peak far away.
I'd really appreciate some input here. I'm not fixed on segmenting the peaks into blocks of lengths 8, I just thought it would be easier for a model to approximate the peak location, since ECGs can be quite noisy. Also, with the current architecture, I'd need 1024 output nodes, and cross entropy between all of them, which would make for quite a long training, since my loss does not reward ""distance"" right now.
","['neural-networks', 'convolutional-neural-networks', 'loss']",
Is it possible to train street fighter agents in gym retro environments?,"
Is it possible to train street fighter 2 champion edition agents to play against CPU in gym retro. I used the stable baseline package and after training the model, it seems like there is no difference in performance. The reward I used is the total score as an indicator. I used the random agent as a baseline and it seems the random agent itself is already able to knock out the cpu character, without any training. This means that my agent is not much different from the random agent, but what I want is the agent being able to knock out a particular character with the highest score possible, not just win against that character. Anyone has such experiences?
",['reinforcement-learning'],
How to design a DQN that needs to self-play with very delayed rewards?,"
I got a single-player card game (comparable to BlackJack) in which rewards are VERY delayed. On average a player can estimate the effect of his move fifteen steps deep. As there are 52 cards to play through, effects can even show later (and are not determined) - hard to approximate which actions had the biggest effect on the state.
How would one approach such a set-up ideally?
Are there any good resources (including source code) one could point to if someone wants to develop an agent that is able to do something like that?
The question is very vague as I don't know what details to include. But overall: I want to train my agent via self-play on this single-player card game. One could hypothetically calculate all possible states down (but the game tree explodes exponentially and there is no way one could do this), therefore I want to use a DQN approach with delayed rewards that at least slightly improves the chances of winning the game.
","['rewards', 'self-play', 'delayed-rewards']",
Same 'area under curve' but different parameters?,"
For 'area under the curve'  (AUC) calculations in machine learning, is it the case that if we are mapping the false positives (x-axis) against the true positives (y-axis), that this curve must be monotonically non-decreasing? I ask because, if we calculate the integral for AUC, then if it's not monotonically non-decreasing, then shouldn't we be able to get an uncountable infinity of different parameter values that upon tweaking can nonetheless produce the exact same integral value? For instance, I'm imagining whether it's possible to have a non-monotonic curves that are a hill (goes back down) versus another curve that is like a winding road (goes down and up), and yet have the same integral value. I'm probably misunderstanding the definition of the axes though since it seems the curve has to go from bottom-left and touch the top-right, and that this might prohibit the kinds of examples I'm thinking of?

","['machine-learning', 'training', 'objective-functions', 'weights', 'performance']",
Approaching time series classification with feature dependend label,"
In one of my projects  it's all about  modelling wether a continuos  condition $Y_1$ become greater or less than  a threshold.  For  this  purpose, I have a huge  Collection of time series data of about 20 Features $x_1...x_{20}$. It is important to know that $x_1$ itself  corresponds to previous  Version of $Y_1$. In terms of Ground truth  labels for a classification, a specific point  in time has  label  0 if $Y_1<5$, otherwise 1. Now two general approaches  come  into my  mind on  how to solve this:

Use  a Machine Learning algorithm to do  time  series classification.  That is, try  to predict  with  all features  whether  $Y_1$ will be greater  or  less  than 5 within  e.g. the next 6 hours. In  doing  so  the class is directly  predcited  by  the  model
Use  all features to model  the  continous  outcome  $Y_1$ with  a regression  like  model. Then simply apply the hard  labeling  rule  on  any point of time  of the  predicted  evolution.

Are both  approaches  valid  or  is  one more common?
Thanks!
","['machine-learning', 'classification', 'time-series', 'regression']",
Tree boosting additive loss,"
In the XGBoost documentation, they specify that the additive training is done given an objective $obj^{(t)}$ defined as
$obj^{(t)} = \sum\limits_{i=1}^n  \ell(y_i, \hat{y}_i^{(t-1)}+f_t(x_i))  + \sum\limits_{k=1}^t \omega(f_k)$.
In the following lines, it is specified that this objective is equal to
$\sum\limits_{i=1}^n  \ell(y_i, \hat{y}_i^{(t-1)}+f_t(x_i))  + \omega(f_t)+constant$
though I don't really understand this step - do they mean that
$\sum\limits_{k=1}^{t-1} \omega(f_k) = constant$?
if so, why? and why is this a greedy method?
","['training', 'objective-functions', 'regression', 'decision-trees', 'gradient-boosting']",
Retrieving data from a table by matching ID values,"
I have two columns [Uid, Con_id]; I need to fetch Uid id's which are matched with some Con_id's [From 606 to 615].
For example, all user_id's in each con_id from 606 to 615. But each Uid should match with those Con_id at least once.

","['python', 'data-preprocessing']",
"Q-learning: In tic-tac-toe, how to choose rewards values?","
| I'm experimenting for first times in neural networks and Q-learning.
In tic-tac-toe game, each move has 5 possible esits

invalid move
valid move, match is still open
draft
you lose
you win

Creating the ""reward matrix"", I am not able how to set these values.
This is my first proposal

invalid move: -100
valid move, match is still open; 1
draft: 0
you lose: -10
you win: 100

I maximized invalid move penalty and maximized win reward
I give a certain penalty for loosing
I prefer an open match to a draft, If I can choose, so bilanced 1 vs 0
Could you suggest some other ways to see this problem?
","['q-learning', 'rewards']",
How can the importance sampling ratio be different than zero when the target policy is deterministic?,"
In the book Reinforcement Learning: An Introduction (2nd edition) Sutton and Barto define at page 104 (p. 126 of the pdf), equation (5.3), the importance sampling ratio, $\rho _{t:T-1}$, as follows:
$$\rho _{t:T-1}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}$$
for a target policy $\pi$ and a behavior policy $b$.
However, on page 103, they state:

The target policy $\pi$ [...] may be deterministic [...].

When $\pi$ is deterministic and greedy it gives $1$ for the greedy action and 0 for all other possible actions.
So, how can the above formula give something else than zero, except for the case where policy $b$ takes a path that $\pi$ would have taken as well? If any selected action of $b$ is different from $\pi$'s choice, then the whole numerator is zero and thus the whole result.
","['reinforcement-learning', 'off-policy-methods', 'sutton-barto', 'importance-sampling']","
Good question. I think this part of the book is not well explained.
Off-policy evaluation of $V$ by itself doesn't make sense, IMO.
I think there are two cases here

is if $\pi$ is deterministic, as we probably want in the case of ""control"", i.e. we will determine $\pi$ to be deterministic and in every state choose the action that most likely to maximize the rewards/returns.
In that case, then evaluating $V$ from a different distribution might not be so useful, as $W$ becomes $0$ with high likelihood. I don't see any sense in it.

if $\pi$ is not deterministic. And it's a good question why would we want to evaluate $V_\pi$ from $V_b$, instead of just evaluating it from $V_\pi$ directly.


So, IMO, off-policy evaluation of $V_\pi$ doesn't make any sense.
However, I think the goal here is actually the control algorithm given in the book (using $q(s,a)$, p. 111 of the book [133 of the pdf]). The idea here is to use some arbitrary behavior/exploratory policy and, while it runs, update (""control"") the policy $\pi$. In there, you use the update rule for $W$, which uses the idea of importance sampling - i.e. how to update the expected value of $\pi$ based on $b$. But there it ACTUALLY makes sense.
So, I suspect the evaluation was given by itself just so the reader can better understand how to do the evaluation, though it really doesn't make sense outside the control algorithm.
"
What is the relationship between state value and action Q function?,"
I am currently working of reformulation of Dec-POMDPs (where I recast a Dec-POMDP to a continuous state MDP for example). I am  trying to prove that the action value (q-function) of the new MDP is also optimal for the original Dec-POMDP. The first idea that came to my mind is to prove the state-value $V^*_{newMDP}(\zeta)$ is equal to that of the original $V^*_{Dec-POMDP}(s)$ at each time step, where $\zeta$ is a new state representation (think of it as an equivalent for blief states in POMDPs). If I prove this , does that mean automatically that $Q_{newMDP}(\zeta,a)$=$Q_{Dec-POMDP}(s,a)$ ?
","['reinforcement-learning', 'markov-decision-process']",
Question in derivation of off-policy gradient,"
I'm studying policy gradient with materials from cs285: Deep reinforcement learning from Berkeley (page26 on http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)
I have a question how to derive off-policy gradient.
In detail, I'm wondering how the first image can be converted into the second image.
Can anyone help me? I don't understand how the product term in the middle disappears and just the single term remains.


",['reinforcement-learning'],
Why does my neural network perform different on the same images during training and testing?,"
I use tensorflow keras to build a neural network that classifies images of covid-19 rapid tests into three classes (Negative, Positive, Empty).
During training the tensorboard logs denote a validation accuracy of around 90%. But when I test the network after being trained with the same images it was trained on, the classification performance is way worse (~60%). I observed the same behavior when I trained the network with different images (see section What I have tried).
During training the images are preprocessed to grayscale and resized before being fed into the model. The batch size is 16.
image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
image = cv2.resize(image, (height, width))

To augment the sparse data that I have (~450 images) I am using the keras.preprocessing.image.ImageDataGenerator and its parameters are: width_shift_range=0.1, brightness_range=[0.9, 1.1], height_shift_range=0.1, zoom_range=0.2, horizontal_flip=True, rotation_range=10, shear_range=0.2, fill_mode=""nearest"", samplewise_center=True, samplewise_std_normalization=True
Edit: The images are split into 90/10 training/validation sets.
(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.1, random_state=24)
I am converting the model to tflite because we need it for mobile platforms. I am using this code snippet:
model = tf.keras.models.load_model(model_path)

converter = tf.lite.TFLiteConverter.from_keras_model(model) # path to the SavedModel directory
# converter.optimizations = [tf.lite.Optimize.DEFAULT] # optimizations
tflite_model = converter.convert()

# Save the model.
with open('rapid_test_strip_cleaned_model.tflite', 'wb') as f:
  f.write(tflite_model)

What I have tried:

crop the images to the strip of the casette, train and test the network again
check in the testing (inference) script if the labels are correct
check if the images are converted to grayscale and resized correctly before being fed into the network during testing
test the model before converting it to tflite, using tensorflow.keras.models

Model:
img_width, img_height = (256, 256)

model = Sequential()
inputShape = (img_width, img_height, 1)
model.add(Conv2D(32, (3, 3), activation=""relu"", input_shape=inputShape))
# to prevent overfitting
model.add(Dropout(0.25))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Conv2D(64, (3, 3), activation=""relu""))
# to prevent overfitting
model.add(Dropout(0.25))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Conv2D(128, (3, 3), activation=""relu""))
# to prevent overfitting
model.add(Dropout(0.25))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
  
model.add(Flatten())
model.add(Dense(512, activation=""relu""))
# to prevent overfitting
model.add(Dropout(0.5))
model.add(Dense(3, activation=""softmax""))

opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)

model.compile(loss=""categorical_crossentropy"", optimizer=opt,
            metrics=[""accuracy""])

This is the Tensorboard graph of the training. The straight line is from another training run.
Testing/Inference script:
interpreter = tf.lite.Interpreter(model_path=model_path)

# Load TFLite model and allocate tensors.
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

height = input_details[0]['shape'][1]
width = input_details[0]['shape'][2]

labels = [""positive"", ""negative"", ""initial""]

# load image into numpy array
image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
image = cv2.resize(image, (height, width))
input_arr = img_to_array(image)

input_arr = np.array([input_arr])
# normalize values
input_arr = input_arr / 255.0

interpreter.set_tensor(input_details[0]['index'], input_arr)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
output_data = interpreter.get_tensor(output_details[0]['index'])

results = np.squeeze(output_data)
top_k = results.argsort()[-5:][::-1]

print(labels[top_k[0]])

Ideas about where the problem may be are very appreciated. I am stuck with this problem for two months now.
Thank you!
","['tensorflow', 'python', 'image-recognition', 'keras']",
Exact definition of WRN-d-k (Wide ResNet),"
I am a little confused about the WRN-d-k notation from Wide Residual Networks. To quote the paper,

In the rest of the paper we use the following notation: WRN-n-k denotes
a residual network that has a total number of convolutional layers n and a widening factor k (for example, network with 40 layers and k = 2 times wider than original would be denoted as WRN-40-2).

I need to implement WRN-28-10 as a baseline since it's one of the best-performing models in RobustBench. According to the definition, I need to take an ""original"" ResNet-28 and widen its layers by a factor of 10. However, there are only 5 variants of ResNet defined in the ResNet paper: ResNet-[18,34,50,101,152].
How exactly are ResNet-28 and WRN-28-10 defined?
","['neural-networks', 'deep-learning', 'terminology', 'papers', 'residual-networks']",
ML model giving rank errors on 3D layers on converting 2D images to 3D models,"
i am currently working on a model to convert 2d images to 3d models through a ml model. For this i have taken into reference a research paper which had this diagrammatical flow of layers & i have tried to put that into work but in vain. Would really like to know where i am going wrong, looking forward for help and suggestions. Thanks!
reference image for model-
my code-
image = Input(shape=(None, None, 3))

# Encoder
l1 = Conv2D(64, (3,3), strides = (2), padding='same', activation='leaky_relu')(image)  
l2 = MaxPooling2D()(l1)
l3 = Conv2D(32, (5,5), strides = (2), padding='same', activation='leaky_relu')(l2)
l4 = MaxPooling2D(padding='same')(l3)
l5 = Conv2D(16, (7,7), strides = (2), padding='same', activation='leaky_relu')(l4)
l6 = MaxPooling2D(padding='same')(l5)
l7 = Conv2D(8, (5, 5), strides = (2), padding = 'same', activation = 'leaky_relu')(l6)
l8 = MaxPooling2D(padding='same')(l7)
l9 = Conv2D(4, (3, 3), strides = (2), padding = 'same', activation = 'leaky_relu')(l8)
l10 = MaxPooling2D(padding='same')(l9)
l11 = Conv2D(2, (4, 4), strides = (2), padding = 'same', activation = 'leaky_relu')(l10)
l12 = MaxPooling2D(padding='same')(l11)
l13 = Conv2D(1, (2, 2), strides = (2), padding = 'same', activation = 'leaky_relu')(l12)



# latent variable z
l14 = Reshape((60,512))(l13)
l15 = Dense((512),activation = 'leaky_relu')(l14)
l16 = Dense((128*4*4*4), activation = 'leaky_relu')(l15)
l17 = Reshape((60,4,4,4,128))(l16)

#Decoder
l18 = UpSampling3D(size = (3,3,3))(l17) #-->throws error->IndexError: list index out of range
l19 = Conv3DTranspose(60, (8, 8, 8), strides = (64), padding='same', activation = 'leaky_relu') (l17)
l20 = UpSampling3D((3,3,3))(l19)
l21 = Conv3DTranspose(60, (16,16,16), strides =(32), padding='same', activation = 'leaky_relu')(l20)
l22 = UpSampling3D((3,3,3))(l21)
l23 = Conv3DTranspose(60, (32, 32, 32), strides = (32), padding='same', activation = 'lealy_relu')(l22)
l24 = UpSampling3D((3,3,3))(l23)
l25 = Conv3DTranspose(60, (64, 64, 64), strides = (24), padding='same', activation = 'leaky_relu')(l24)
l26 = UpSampling3D((3,3,3))(l25)
l27 = Conv3DTranspose(60, (64, 64, 64), strides = (1), padding='same', activation = 'leaky_relu')(l26)

model3D = Model(image, l27)

error-
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_17/3800219535.py in <module>
     34 #Decoder
     35 l18 = UpSampling3D(size = (3,3,3))(l17) #-->throws error->IndexError: list index out of range
---> 36 l19 = Conv3DTranspose(60, (8, 8, 8), strides = (64), padding='same', activation = 'leaky_relu') (l17)
     37 l20 = UpSampling3D((3,3,3))(l19)
     38 l21 = Conv3DTranspose(60, (16,16,16), strides =(32), padding='same', activation = 'leaky_relu')(l20)

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    975     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):
    976       return self._functional_construction_call(inputs, args, kwargs,
--> 977                                                 input_list)
    978 
    979     # Maintains info about the `Layer.call` stack.

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)
   1113       # Check input assumptions set after layer building, e.g. input shape.
   1114       outputs = self._keras_tensor_symbolic_call(
-> 1115           inputs, input_masks, args, kwargs)
   1116 
   1117       if outputs is None:

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)
    846       return tf.nest.map_structure(keras_tensor.KerasTensor, output_signature)
    847     else:
--> 848       return self._infer_output_signature(inputs, args, kwargs, input_masks)
    849 
    850   def _infer_output_signature(self, inputs, args, kwargs, input_masks):

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)
    884           # overridden).
    885           # TODO(kaftan): do we maybe_build here, or have we already done it?
--> 886           self._maybe_build(inputs)
    887           inputs = self._maybe_cast_inputs(inputs)
    888           outputs = call_fn(inputs, *args, **kwargs)

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in _maybe_build(self, inputs)
   2657         # operations.
   2658         with tf_utils.maybe_init_scope(self):
-> 2659           self.build(input_shapes)  # pylint:disable=not-callable
   2660       # We must set also ensure that the layer is marked as built, and the build
   2661       # shape is stored since user defined build functions may not be calling

/opt/conda/lib/python3.7/site-packages/keras/layers/convolutional.py in build(self, input_shape)
   1546     if len(input_shape) != 5:
   1547       raise ValueError('Inputs should have rank 5, received input shape:',
-> 1548                        str(input_shape))
   1549     channel_axis = self._get_channel_axis()
   1550     if input_shape.dims[channel_axis].value is None:

ValueError: ('Inputs should have rank 5, received input shape:', '(None, 60, 4, 4, 4, 128)')```

","['machine-learning', 'convolutional-neural-networks', 'convolution', '3d-convolution', '2d-convolution']",
Is large language model and foundation model the same thing?,"
I read a lot about foundation model and large language model.
However, I dont find a clear definition what exactly is a foundation model. Is large language model and foundation model the same thing?
",['deep-learning'],"
At this point in time, there does not appear to be a really widely-agreed-upon definition of ""Foundation models"". If you want one, the best place to go would be this paper from Stanford that coined the term. Generally at least some of the following ideas apply (according to some people they must all apply, according to others only some need to apply):

Trained using unsupervised or self-supervised learning.
Large (deep neural network) model.
Not intended to be used directly for any particular end-goal.
Intended to serve as a basis (""foundation""). You can think of this as ""warm-starting"", starting from a well-trained initial model, that you can then fine-tune with further training (for example, supervised learning) for any specific task you personally have in mind.
Trained on multimodal data (not just text, not just images, not just audio, etc., but a mix of things). In my personal opinion this point really is not strictly necessary, and I think most people would agree, but I've seen some people saying this.

Large Language Models would typically be trained specifically on language-related data (text). So, I suppose an LLM could sometimes serve as a Foundation model, but it's not necessarily the same thing.
NOTE: I would like to remark that, from many experts outside of Stanford, there is quite a bit of pushback against the particular term ""Foundation model"". There seems to be a bit of PR involved, where it seems like they may like to coin a new term for something that really doesn't need a new term. Just saying it's, for example, a large pre-trained model, may be more clear.
"
How exactly does Keras calculate the validation accuracy?,"
After each epoch, Keras provides the following evaluations (depending on how the model is compiled):

train_accuracy
train_loss
validation_loss
validation_accuracy

Keras evaluates the performance of the model using the validation set at the end of each epoch.  But how does Keras do this?  Assume that we are performing binary classification and using a binary loss function.  Assume that there are 100 images in the validation set.
QUESTION:  Can the accuracy can be found by simply calculating accuracy for each item and then average them (/100)?
CONTEXT:  I want to implement a custom loss function that calculates the loss towards reducing validation loss (increasing validation accuracy). Therefore, I need to understand how the accuracy is calculated for the validation set.
","['machine-learning', 'deep-learning', 'tensorflow', 'keras']","
After each epoch, the model is applied to the features of the validation set (x_val) to predict the probability of the labels.  Then, the probabilities are thresholded to generate the predicted label (y_pred).  Finally, the accuracy is calculated as follows:
$$ Accuracy\ =\ \frac{Concordant\ samples}{Total\ samples} $$
, where concordant samples are those for which the predicted (y_pred) and true labels (y_val) agree.
Assume we have a binary classification task and 5 validation samples.  If y_pred = [0, 1, 1, 0, 1] and y_val = [0, 1, 1, 1, 0], then there are 3 concordant samples, giving an accuracy of 3/5 = 60%.
"
Retraining ResNet-50 for iris flower classification,"
I am trying to retrain ResNet-50 for iris flower classification in tensorflow (TensorFlow version: 2.3.0) using the following code
import tensorflow as tf
import cv2, random
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from random import shuffle 
from IPython.display import SVG
import numpy as np # linear algebra
import pandas as pd 
import shutil
import matplotlib.pyplot as plt
%matplotlib inline 
from IPython.display import Image, display
from sklearn.model_selection import train_test_split
import os
print(os.listdir(""./iris recognition/flowers""))

labels = os.listdir(""./iris recognition/flowers"")
num_classes = len(set(labels))
IMAGE_SIZE= 224


# Create model
model = tf.keras.Sequential()
model.add(tf.keras.applications.ResNet50(include_top=False, weights='imagenet'))
model.add(tf.keras.layers.GlobalAveragePooling2D())
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

# Do not train first layer (ResNet) as it is already pre-trained
model.layers[0].trainable = False

# Compile model
from tensorflow.python.keras import optimizers

sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
train_folder = './iris recognition/flowers'

image_size = 224
data_generator =  tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet50.preprocess_input,
                                    horizontal_flip=True,
                                    width_shift_range=0.2,
                                    height_shift_range=0.2,
                                    validation_split=0.2)# set validation split

train_generator = data_generator.flow_from_directory(
    train_folder,
    target_size=(image_size, image_size),
    batch_size=10,
    class_mode='categorical',
    subset='training'
    )
validation_generator = data_generator.flow_from_directory(
    train_folder,
    target_size=(image_size, image_size),
    batch_size=10,
    class_mode='categorical',
    subset='validation'
    )

NUM_EPOCHS = 70
EARLY_STOP_PATIENCE = 5
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint

cb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = EARLY_STOP_PATIENCE)
cb_checkpointer = ModelCheckpoint(filepath = './working/best.hdf5',
                                  monitor = 'val_loss',
                                  save_best_only = True,
                                  mode = 'auto')
import math

fit_history = model.fit(
    train_generator,
    steps_per_epoch=10,
    validation_data=validation_generator,
#     validation_steps=10,
    epochs=NUM_EPOCHS,
    callbacks=[cb_checkpointer, cb_early_stopper])
model.load_weights(""./working/best.hdf5"")

I tried to change the number of epochs , steps_per_epoch and validation_steps however the model accuracy does not improved as shown in  
After trying several essais while changing batch size from 32, 64 and 128.For each essai I tried with and without using :

steps_per_epoch=10.

sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9,
nesterov = True)


The best accuracy I achieved is 72.29% in which the training was the most stable which was when using batch size=128 as shwon in 
Any suggestions please?!
","['tensorflow', 'residual-networks', 'iris-dataset']","
It looks like you're defining an optimizer in the line sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True) and then not using it, since you compile the model with optimizer='adam'. So changing any of the sgd parameters, or even commenting out the line, changes nothing for your model.
you'll need to change to model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) to use the sgd optimizer object in the model.
"
"What is the relationship between data science, artificial intelligence,machine learning and computer vision?","
I am beginner to this field and i am trying to find big picture and i have tried to explore youtube and google images in this regard. According to my understanding ,machine learning is subset of artificial intelligence . But what about computer vision and data science, are they also subset of artificial intelligence? Which is main set? Artificial intelligence or data science?
If someone can share a venn diagram that may be very helpful in understanding the proper placement/arrangement of above mentioned different fields
","['machine-learning', 'computer-vision', 'terminology', 'data-science']","
This Venn diagram might help to visualize the relation between the different fields:

The image is from the free deep learning book by Ian Goodfellow, Yoshua Bengio and Aaron Courville. As you said machine learning is indeed a subset of Artificial intelligence. Artificial intelligence is since its origins a very broad framework with a strong overlap with computer science, neuroscience, game theory, reinforcement learning, linguistics, etc. Computer vision can in general be considered a part of artificial intelligence. However, computer vision comes in various flavors and forms and some people in the field might not be fond of putting their work under the 'AI' umbrella. Data science conceptually overlaps with many of the methods used within AI but it is more focused on statistics, data analytics, modeling, and data mining.
In industry being a data scientist often means 'generating insights from data' (eg via AB testing) while working as something with AI in the title involves some kind of deep learning (eg for natural language understanding).
"
At what size does a picture become unusable for facial recognition?,"
Lets say i have a portrait photo of which the face it contains covers about 90% of the entire photo. I want to be able to detect the face in this photo using facial recognition but i also want to reduce the file size of this photo. The best way to reduce the file size is to make the size dimensions of the photo smaller.
I want to know at what size a face containing photo becomes unusable for facial recognition. For instance could a 25 x 25 pixels photo still be used for facial recognition or would this be too small?
","['computer-vision', 'image-recognition', 'facial-recognition']",
Proper way to check whether a person is linked to the requested class from text,"
I started doing NLP recently and I find this field fascinating but I stumbled upon my first problem. I'm trying to find out the right approach and way on how to detect (classify) whether a specified person is linked with the provided class for the given text. Short description, the dataset I have now contains three columns, name, text and class. For example, name is ""Jeff Bezos"", text is ""Murder was conducted Tuesday evening. The witness was Jeff Bezos"" and the class is ""murder"". I was thinking for training a custom CNER (Crime NER) with another dataset, to first tackle the crime, then do a NER to get the person and ensemble these two models into one but what next, how to later use this to check the text? The other idea I have is to transform the dataset into a graph dataset and use Graph Neural Network, maybe have Jeff Bezos -> witness, if Jeff Bezos is mentioned as ""... the CEO of Amazon"" later on, I might add another vertice that would go Jeff Bezos -> CEO -> Amazon etc. What do you think, what is the right approach according to you and if there are some already trained models for CNER or classifiers that detects whether an entity is linked to another entity within a text, please do let me know
",['natural-language-processing'],
Use AI in Education App,"
I have an Education app need to improve it even more with the power of AI. Currently i'm able to do those tasks:

Each teacher can review a specific student one/many times.This review performed by submit a form with some points to measures student behavior, his academic status in general & and whether he suffers from learning difficulties and the degree of it.
The system have access to student absent, attendance & delays in comming to school early in the morning Also  whether disciplinary sanctions have been issued against him during the school year.

What i really want is a way that make me improve student behavior & enhance his weaknesses points in his academic status in general since i can not deep dive into each subject he study because i'm not expert in them but i targets general things which most students share so need some help from AI experts how i can improve this App with the help of AI to get the most benefits to help improve students more & teach them how to do that.
Note:
My question maybe it general one not try to fix a specific issue and this question may closed or delete but i really want to make this a reality and take the app into Next level to benefit my students since AI is build on Data & this app has a lot of data but i can not figure out how i can take advantage of this data at least now so i need some real guidance here to achieve that.
",['education'],
Input length of Sentence BERT,"
Can Sentence Bert embed an entire paragraph instead of only a sentence? For example, a description of a movie.
If so, what if the word counts exceeded the input limit? I remember Bert can only take up to 512 tokens as inputs?
","['natural-language-processing', 'bert']",
Machine Learning book for fundamentals - Simon Haykin vs. Christopher M. Bishop,"
Since I started studying Machine Learning, I was torn between two books in this area, and I could never decide which one is the best to follow.
The first book is widely used and known: Pattern Recognition and Machine Learning, by Christopher Bishop. I've never head about him before, probably because he is a Computer Scientist and I am an Engineer, and he doesn't have many books, AFAIK. But I know that this one I like a bible for machine learning fundamentals.
However, I came across another book: Neural Networks and Learning Machine, by Simon Haykin. I am very familiar with this author since I already read many of his books throughout my graduation on other topics, such as communication systems, signals and systems, adaptive filters, etc... Then I decided to read it and I like it a lot!
However, I realized that Bishop is much more referenced than Simon Haykin regarding machine learning. Furthermore, both books differ a lot with regard to terms. For instance, the term ""induced local field"", which is largely adopted in the Simon Haykin's book, is not even mentioned once by Bishop (at least, I didn't find it when I looked it up).
All of these aspects make me fell unconformable and doubtful about my book adoption since I could use expressions and terms that I not so used by other authors.
It may be a taste matter, but I do need suggestions whether the Bishop's book is much better than anything else to get the foundations or it is ok to adopt Simon Haykin.
Thank you in advance.
","['neural-networks', 'machine-learning', 'books']","
I don't know much about the Haykin's book.
The Bishop book seem to be the standard text for a graduate-level course in CS departments in top research universities.
I remember back in the day when I took the ML course, the professor used the Bishop book and was very fond of it.
He said he had read the book three times, once as a undergrad, once as a PhD student, and once when he taught from the book as a professor. And he said after three reads, he finally got every detail in the book.
Basically, he was saying the book was quite dense and written in a way that is not easily penetrable, but once you truly get it, you would appreciate the beauty of exposition style of the book.
"
Object detection on UAV images,"
So, the idea is that I have a custom set of UAV images. I zoomed them and I used LabelImg in order to draw the rectangles. I trained the model and when I run the code for object detection it does detect the object it was trained for. My concern is that I used zoomed UAV images in order to train and unzoomed (original size images) images for object detection. Can this be the problem? What could be wrong? I used this code for train: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html and this code  https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/plot_object_detection_saved_model.html#sphx-glr-auto-examples-plot-object-detection-saved-model-py for object detection.
","['python', 'object-detection']",
How to Decide on the Structure of a Neural Network for Time Series Forecasting?,"
Apologies for the noob question. I am attempting time series forecasting (with a combination of lag and categorical features) using tensorflow and struggling to find an optimal combination of RNN/LSTM cells and some attention, convolution components (Luong attention as well as multihead attention), as there seems just too many parameters to decide and an infinite number of possible architectures.
For what it is worth, I did spend some time to understand the functionalities of attention, LSTM etc. and internal operations of the layers. But that does not help in deciding the architecture. So is it purely an art, where I mix some experience with experimentation to arrive at a model architecture, or is there some guidance, method, or even library for an architecture search? Any help will be gratefully appreciated.
For more specifics

I am trying to predict the next seven days of combined sales of some items using past seven days of sell as lookback features, and some item details as categorical features. The resolutions of the target and lookback features are different. Lookback uses past seven individual daily sales of items, but the goal is to predict next seven days total sale. I did the feature engineering and got the training samples already.
The loss function to use is mean-absolute-percentage-error
I am mostly familiar with tensorflow APIs, subclassing the layers, defining the model functionally etc. But cannot decide on a specific architecture of the model with the components to fit the data.

","['tensorflow', 'long-short-term-memory', 'time-series', 'attention']",
Shuffling vs Non-shuffling train/test set yields drastically different results,"
I am currently working with a very deep NN (200mio. to 350mio. params). My data set is roughly of shape (2mio, 350), i.e. 2mio samples and 350 features. In fact, the features are time series. As input to the NN I just pass the current state (1 time step), however that state is derived with some sort of scaling from past states.
Now I made a couple strange observation during training, I can't explain at all:

When I shuffle the whole data set and then do train/test split, the classification performance is extremely good, ~0.93 accuracy (3 classes) and identically generalizes onto the test set during the whole training
When I split the data into train/test and just shuffle train, the performance is less on train, but still acceptable (~0.75 accuracy), but performance on test falls off to ~0.36 accuracy
When I just split the data without shuffling, the train performance further drops to ~0.5 and test performance to ~0.3 (which is worse than randomly guessing)

What is going on here? I checked if I might have introduced some data leakage or lookahead bias in my data set manipulations, but couldn't find anything.
Anybody has an idea what is going on here?
Cheers
","['neural-networks', 'tensorflow', 'keras', 'transformer']","
I don't have much information about your model and your data. But my best guess is that your dataset doesn't have the same distribution everywhere, and that without shuffling it before doing the split, you create subsets of data that are statistically very different i.e. their intrinsic distribution might differ a lot. When you shuffle before splitting, you mitigate this effect because you distribute your data (which might follow divergent patterns according to timestamps or any other feature) in both sets.
"
Linear Actor Critic for continuing task and 1 continuous action => Any comment?,"
I wish to implement an Actor Critic agent using linear functions for a continuing task with one continuous action. Below the resulting pseudo-code I have reached by my own (the initialization part is removed).
$$
\begin{aligned}
& Observe \hspace{0.1cm} X \\
& Loop \hspace{0.1cm} until \hspace{0.1cm} STOP:\\
&  \hspace{0.5cm}\mu = \theta_{\mu }^{T} * X \\
&  \hspace{0.5cm}\sigma = ln\left (\theta_{\sigma }^{T} * X  \right ) \\
&  \hspace{0.5cm}a \leftarrow N(\mu , \sigma ) \\ 
&  \hspace{0.5cm}Take \hspace{0.1cm}action\hspace{0.1cm} a, \hspace{0.1cm}observe\hspace{0.1cm} X' \\
&  \hspace{0.5cm}\delta \leftarrow R - \overline{R} + w^{T} * X' - w^{T} * X \\
&  \hspace{0.5cm}\overline{R} \leftarrow \overline{R} + \alpha _{\overline{R}} * \delta \\
&  \hspace{0.5cm}w \leftarrow w + \alpha_{w} * \delta * (R-\overline{R}+w^{T}*X'-w^{T}*X)*X \\
&  \hspace{0.5cm}\theta_{\mu} \leftarrow \theta _{\mu}  + \alpha_{\mu}*\delta * \frac{a-\mu}{\sigma^{2}}*X \\
&  \hspace{0.5cm}\theta_{\sigma} \leftarrow \theta _{\sigma}  + \alpha_{\sigma}*\delta * \left (  \frac{\left ( a-\mu \right )^{2} }{\sigma^{2}}- 1 \right )*X\\
&  \hspace{0.5cm}X \leftarrow X'
\end{aligned}
$$
My mathematics level is not high. I would be grateful if somebody could check. My questions: 
 1) Can somebody help me to complete the update of theta_mu and theta_sigma (lines with question marks) ? 
I finally found something I missed in Sutton & Barto book, so I could complete the updates and remove question 1.
2) Does somebody see any mistake or have any remark ? 
Many thanks for the attention.
","['reinforcement-learning', 'actor-critic-methods', 'linear-regression']",
Why is the derivative of activation function all positive?,"
All the activation functions I see have positive derivatives.
Will negative ReLU work as well as its positive counterpart or will it lead to instability?
","['neural-networks', 'backpropagation', 'activation-functions']","
By definition from wikipedia:

Let $\varphi$ be a nonconstant, bounded, and continuous function.

This is also being proven for ReLU and other functions that violets some of those properties... however, if you take 1 - sigmoid(x) as your activation, all the properties are satisfied, and the derivative is strictly negative
This can be more ""popularly"" be seen also in the max-out activation function
"
Getting predictions from tab transformer model,"
I am working on implementing the tab transformer model into my dataset, and have it working so far and giving a fair ROC_AUC.I am using the code as seen below for my dataset. The only variation I have made is splitting the train dataset into train and validation, rather than using the test dataset as validation as they do below.
https://keras.io/examples/structured_data/tabtransformer/ , https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/structured_data/ipynb/tabtransformer.ipynb
I am now wanting to create a confusion matrix and ROC_AUC plot for my train and test dataset, but am struggling to work out how to do so. Within the run experiment function, I attempted to split the train_dataset into X_train and y_train however cannot do that because the previous function returns a cache dataset which is needed.
What I would like to do is run code like this:
y_predicted_train = model.predict(X_train)
y_pred_train= []
for pred in y_predicted_train:
    if pred > 0.5:
        y_pred_train.append(1)
    else:
        y_pred_train.append(0)
y_predicted_test = model.predict(X_test)
y_pred_test= []
for pred in y_predicted_test:
    if pred > 0.5:
        y_pred_test.append(1)
    else:
        y_pred_test.append(0)   

And as soon as I have my y_pred_test and y_pred_train I can compute the metrics i need. But I am unsure how to get to that point.
Any help is greatly appreciated.
","['python', 'keras']",
"What would be the Bellman optimality equation for $q_∗(s, a)$ for an MDP with continuous states and actions?","
I'm currently studying Reinforcement Learning and I'd like to know what would be the Bellman optimality equation for action values $q_∗(s, a)$ for a MDP with continuous states and actions, written out using explicit integration (no expectation notation).
The discrete case is
$$
q_*(s,a)  = \sum_{s'}\sum_r p(s',r|s,a)[r+\gamma \max_{a'}q*(s',a')]
$$
My thoughts for the continuous case are:
\begin{align}
q_*(s,a) &= \int_{s'}\int_{r}f_{s',r|s,a}(s',r|s,a)[r+\gamma\max_{a'}q_*(s',a')]drds'
\end{align}
Is this how it would look like?
","['reinforcement-learning', 'bellman-equations', 'continuous-action-spaces', 'continuous-state-spaces']",
Shuffle data inside learning sample in order independet transformer model,"
Does it make sense to create new samples with shuffled items ""tokens"" inside a learning sample for the order independent (no positional encoding) transformer model to improve model accuracy?
","['neural-networks', 'deep-learning', 'transformer']",
Does the Weights of Discriminator get updated when traning Generators in GANs?,"
When we train the GAN we usually train the discriminator first then the generator, first we stop the generator from updating its weight by removing it from the computation graph, using fake_image.detach()
noise=get_noise(num_images,z_dim,device=device)
fake_images=gen(noise)

disc_fake_pred = disc(fake_images.detach()) 
disc_fake_loss=criterion(disc_fake_pred,torch.zeros_like(disc_fake_pred)) 
disc_real_pred = disc(real)
disc_real_loss=criterion(disc_real_pred,torch.ones_like(disc_real_pred))
    
disc_loss = (disc_real_loss + disc_fake_loss)/2
disc_loss.backward()
disc_opt.step()

But when we want to train the discriminator we don't stop discriminator weights all we do is just leave the generator weights in the computational graph, this means discriminator and generator weights will be updated since we didn't stop it
gen_opt.zero_grad()
z_noise = get_noise(num_images, z_dim, device)
fake_images = gen(z_noise)
disc_fake_pred = disc(fake_images) 
gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) 
gen_loss.backward()
gen_opt.step()

But in
","['deep-learning', 'generative-adversarial-networks', 'pytorch', 'discriminator', 'dc-gan']",
how to calculate the virtual time of a reinforcement learning model,"
I have heard about some RL models like alpha go being trained for days but in reality has gained thousands of years of experience and a model which teaches a 3d figure how to walk and fight being trained for some days but has virtually trained for 10 years. I am wondering how does one calculate this from dqn model using stats output like this
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 3.81e+04 |
|    ep_rew_mean      | -1.55    |
|    exploration_rate | 0.488    |
| time/               |          |
|    episodes         | 97       |
|    fps              | 5825     |
|    time_elapsed     | 665      |
|    total_timesteps  | 3878940  |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.73     |
|    n_updates        | 95723    |
----------------------------------

I believe it might have something to do with the time segment of the stats but i do not know how it should calculated.
Any help would be appreciated
","['reinforcement-learning', 'deep-rl', 'dqn', 'environment', 'time']",
additive or multiplicative model?,"
let's say if I have two scores $x_1^i$ and $x_2^i$ for each data point $i$, and I need to make a final score/loss function out of it.
Should I use a weighted sum $w_1 x_1^i + w_2 x_2^i$, or their multiplication $x_1^i x_2^i$? (of course loss is the minus score)
I think multiplication is addition in a different sense? because $\log x_1 x_2 = \log x_1 + \log x_2$?
","['math', 'feature-selection']",
How do I interpret this loss function?,"
In this AI note from https://deeplearning.ai, the loss function below is used for a regression problem. However, I don't know how to interpret this loss function.

First, does the author take the square of the difference between y-hat (prediction) and y (ground truth), so that positive and negative numbers don't cancel each other out? If so, why do we take the norm or distance, as well? Isn't the norm positive anyway? Or does he take the square so that it's more convenient to calculate the derivative?
Second, what does the other 2 in subscript mean? It's not explained in the note and I was also not able to derive it from the context. All I know is that y ∈ R.
It seems like I find it difficult to read mathematical notations. If you know a resource that explains these notations, please let me know.
","['regression', 'loss']",
Master theorem about polynomial classifiers?,"
Does anyone know if there is a theorem or counterexample establishing whether or not for any given binary classification task in some finite (possibly large) dimensional vector space of attributes, that there exists a polynomial classifier that can form a hyperplane sorting all the positive from negatively labelled data points?
To clarify, I know that if a dataset is linearly separable, then we can find such a linear classifier. But my question is more general and asks if without knowing beforehand whether a dataset is separable at all, can we know ahead of time if there exists a polynomial classifier for any n-dimensional vector space of data points?
","['datasets', 'generalization']","
Unless I'm missing something I think you're simply looking for the Rouche Capelli theorem.
A polynomial classifier is just a set of linear equations made of feature vectors and coefficients (parameters) vectors. So to know if a solution exists you would have to compute the rank of the classifier coefficient matrix and expanded matrix and compare their ranks.
"
"Transformers (NLP) High Accuracy, Low Loss","
What does accuracy and loss mean during the training phrase for seq2seq Transformer Models (I'm using the keras API).

I'm getting high accuracy and low loss here but my seq2seq predictions are similar but not perfectly the same as the expected.
","['natural-language-processing', 'transformer']",
Is it possible for PixelCNN to tell us what it generates?,"
I coded PixelCNN with the help of Keras official website. Also, I read the paper. I can use PixelCNN, similar to a decoder or generator (to generate samples). My question is, ""is it possible to train PixelCNN to tell us what is predicted?"".
For example, consider we train an autoencoder on a dataset of animals. The PixelCNN samples from latent space and can predict the encoded picture. Now, I want to know, is there a way for PixelCNN to tell us what (e.g. which animal) it generated after prediction?
I thought about probability methods like softmax, but I am not sure about them.
","['classification', 'autoencoders', 'variational-autoencoder', 'probability', 'conditional-probability']",
How to identify pulse pattern in data,"
So I am using a pulse sensor which is giving out a certain data. When I place my finger on the sensor and see the data plotted in a graph it shows a varying (non-repetitive as no two pulses are exactly the same) but fairly similar kind of curves coming on the screen as is expected. Now here is the problem, even without placing the fingers some garbage data is being thrown by the sensor which does form crests and troughs which are non-repetitive but also non-similar.
I would like an algorithm that can identify when the data is garbage and when its my finger giving out the data. It needs to work as efficiently as a human eye.
","['datasets', 'pattern-recognition', 'data-science']",
Meaning of Large Dataset for machine learning,"
Some online answers about parameters in machine learning mention that it is dependent on the size of dataset we have (if it is a large dataset or not). Is this size related to the number of samples we have or to the number of features for each sample, or both?
","['machine-learning', 'deep-learning', 'datasets']",
Why not use only expert demonstrations in Imitation Learning approaches?,"
Some IL approaches train the agents by using some specific ratio of expert demonstrations to trajectories generated using the policy being optimized.
In the specific paper I'm reading they say ""we experimented with various IL proportions (10-50% by increments of 10%) and observed that the RL/IL ratio does not seem to affect the performance of the trained policy by much.""
My question is: why not rely only on expert demonstrations instead of introducing the noise of trajectories generated by a sub-optimal policy?.
My assumptions are:

This noise helps explore the state space beyond the specific episodes experienced by the expert system.
You might have expert systems that only work on a limited state space and do not scale to bigger, more complex environments. Therefore, although you can't use expert demonstrations in the bigger envs you still want to leverage their experience by learning their behavior in limited settings and the way to avoid overfitting to a specific, constrained policy is by always having some proportion of episodes generated by your policy being learned.

","['reinforcement-learning', 'machine-learning', 'deep-rl', 'imitation-learning']","
Using suboptimal demonstrations in Imitation Learning
The paper you link above uses Behavior Cloning (supervised learning on the expert state-action pairs), which is known to suffer from the ""DAgger problem"" like many other imitation learning algorithms.
In essence, a policy trained under imitation learning from expert demonstrations will inevitably make some suboptimal actions during the episode, and these suboptimal actions lead to states that are increasingly different from states in the expert demonstrations, which leads to more suboptimal actions.
So there are cases where adding noise or perturbation to your demonstration set can lead to a more robust imitation policy[1].
[1] T-Rex https://arxiv.org/abs/1904.06387
Imitation Learning in PRIMAL
As I understand it, the paper you link to only uses expert demonstrations during imitation learning. The suboptimal demonstrations are RL samples being used for an RL objective.
The authors describe their choice to combine IL and RL as follows:

combining RL and IL has been shown to lead to faster, more stable
training as well as higher-quality solutions in robot manipulation.
These advantages are likely due to the fact that IL can help to
quickly identify high-quality regions of the agents state-action
space, while RL can further improve the policy by freely exploring
these regions

"
CNN: Input Normalization for Time Series Data (Grad-CAM),"
I trained a CNN model on univariate time series data. As I often read that it is advisable to normalize the data, e.g., using z-normalization, I started with training on z-normalized versions of the raw data. However, I also tested it with the raw time series samples. In terms of accuracy, both versions reach roughly the same performance on my test data, but I also tried to get some insights on how these predictions came about using the Grad-CAM method. Interestingly, here the model trained on the raw data produces ""explanations"", i.e., heatmaps, that often look a lot more intuitive.
I didn't really find that much on the topic. Intuitively, I would say that it kind of makes sense to use the raw data in this scenario, because the actual values in the samples could be of relevance to distinguish between classes, not only the pattern that would also remain after normalization. Additionally, normalization is important when the input samples have different scales, which is not really the case for the considered scenario. On the other hand, the actual values should not be important, only the information that positive samples are generally on a higher level in certain regions and similar aspects, but that should also be the case in the normalized versions.
How can the better Grad-CAM results on non-normalized data be explained?
Generally, is there some guideline to follow here? Can one say that it is reasonable to use the raw data when the input samples are always roughly in the same scale, or what are conditions under which normalization can negatively impact the performance of the trained model?
","['convolutional-neural-networks', 'time-series', 'normalisation', 'grad-cam']",
Identify features don't hold required information in a ML problem,"
given a ML model that performs poorly, can you differentiate between those two causes:

bad architecture/not enough data
the features do not hold enough information to solve the problem

I'm interested in identifying num 2., what analysis can I perform to prove that num 2. is occurring when solving some ML problem?
Context is supervised learning (and the model is not necessarily a neural net)
","['machine-learning', 'supervised-learning']","
To test hypothesis 1 the only way is to try different architectures and/or approaches. Basically brutal grid search.
To test hypothesis 2 the way to go is data exploration and feature importance analysis. You can compute different correlation scores (Pearson, Spearman, Chi-square) between each individual feature and your target, plus correlation between each pair of features. Comparing to the target gives you an idea of which features are strong predictors and which are weak ones, comparing features tells you how much redundancy is present within you training data.
Keep in mind that decision tree based models allow you to automatically assess which features are strong predictors, so training a random forest or xgboost is one good way to do both things in one shot: training a model and assess your data quality.
"
Reward design or Inverse reinforcement learning?,"
I'm working on a reinforcement learning project where I only have demonstrations (i.e. set of states and actions). During my research on how handle the reward signal, I noticed that research papers often design their reward functions, based on heuristics and human/expert knowledge. Meanwhile, when I read papers on Inverse RL, they claim that such approach can be used in applications where designing the reward function is not trivial, although this approach is not common. I'm not experienced in the field, but I'm wondering how one can decide whether to go for designing the reward function or using IRL, instead?
","['reinforcement-learning', 'reward-functions', 'reward-design', 'inverse-rl']",
Which neural network/ML framework to use for a partial amount of a fixed-time amount of sensor data?,"
Assume I've run a set of initial experiments. For each experiment, I have a set of input signals $I_e(i, t)$, and output signals $O_e(j, t)$ for $i \approx 10$, $j \approx 10$, and $0\leq t\leq t_{max}=7200$ for half-second timesteps. This gets me a set of experiments $1 \leq e \leq N$. So $O_e = F(I_e)$ for some function $F$.
I have a target signal to match the desired output signals for a given $T(i, t)$ by controlling the $j$ input signals on the fly for a new experiment but only have the data for the run for the start to the present time, e.g. $O_{e+1}(i, t')$ with $ 0 \leq t' < t_{max}$. I'd like to do this iteratively, so hopefully each succeeding experiment would be more accurate. This would also need to be run in real-time.
Which ML approach should I use? It seems like reinforcement learning/q-learning would be the right approach? I'm not sure how that would work with partial time data as a state.
","['machine-learning', 'signal-processing', 'real-time']",
The eﬀect of the training dataset size on the train and test error,"
I was trying to mimic the results from deeplearningbook.org from chapter 5 p.115, Fig.5.4. 
I created 40 datasets for each size from 2 to 10^5(30 different sizes). And solved the regression with Moore-Penrose pseudoinverse. I generated test set of size 10^5. But the chart looks very different.

Starting from values of MSE to the fact that test error is smaller than training error from some point on. Can someone point what is wrong?
import random as rnd
import numpy as np
import matplotlib.pyplot as plt

def polynomial_deg_5(x):
    return x**5-4*x**4-3*x**3-x**2-x+2
import matplotlib.pyplot as plt
import numpy as np

# create 1000 equally spaced points between -10 and 10
#x = np.linspace(-100, 100, 1000)
def create_dataset(size):
    x = np.random.rand(size)
    x = np.sort(x)
    x = 200000*x-100000
    return x

x_test = create_dataset(10*5)
# calculate the y value for each element of the x vector
y_test = polynomial_deg_5(x_test)  

fig, ax = plt.subplots()
ax.plot(x_test, y_test)

def solve_regression(degree, t_set):
    X = t_set[0].reshape(-1,1)
    for i in range(2,degree+1):
        x=t_set[0]**i
        x = x.reshape(-1,1)
        X = np.concatenate((X,x), axis=1)
    
    shape = X[:,0].shape[0]
    bias = np.ones((shape,1))
    X = np.concatenate((X,bias),axis=1)
    y = t_set[1].reshape(-1,1)
    w = np.dot(np.linalg.pinv(X),y)
    return w

def expand_matrix(degree, t_set):
    X = t_set.reshape(-1,1)
    for i in range(2,degree+1):
        x=t_set**i
        x = x.reshape(-1,1)
        X = np.concatenate((X,x), axis=1)
    shape = X[:,0].shape[0]
    bias = np.ones((shape,1))
    X = np.concatenate((X,bias),axis=1)
    return X

import scipy.stats as st
from sklearn.metrics import mean_squared_error as mse

def mean_confidence(data, confidence=0.95):
    a = 1.0 * np.array(data)
    n = len(a)
    m, se = np.mean(a), st.sem(a)
    h = se * st.t.ppf((1 + confidence) / 2., n-1)
    return h
X_test = expand_matrix(2,x_test)
basic_sizes = np.array([2,4,6,7,8,9,10])
powers = np.array([1,10,10**2, 10**3,10**4])
basic_sizes = basic_sizes.reshape(1,-1)
powers = powers.reshape(-1,1)
sizes = np.dot(powers,basic_sizes)
sizes = sizes.reshape(1,-1)
sizes = sizes[0]

train_ints = []
train_means = []
test_means = []
test_ints = []

for size in sizes:
    train_mse = []
    test_mse = []
    for i in range(40):
        x_10_train = create_dataset(size)
        y_10_train = polynomial_deg_5(x_10_train)
        #plt.scatter(x_10_train, y_10_train)
        X_10 = expand_matrix(2,x_10_train)
        #print(X_10)
        w = np.dot(np.linalg.pinv(X_10),y_10_train)
        #print(w)
        y_10_pred = np.dot(X_10,w)
        y_test_pred = np.dot(X_test,w)
        
        #plt.plot(x_10_train, y_10_pred)
        train_mse.append(mse(y_10_pred,y_10_train))
        test_mse.append(mse(y_test_pred,y_test))
    train_ints.append(mean_confidence(train_mse))
    train_means.append(np.mean(train_mse))
    test_ints.append(mean_confidence(test_mse))
    test_means.append(np.mean(test_mse))

fig,ax = plt.subplots()
ax.errorbar(sizes, train_means, yerr=train_ints, label='quadratic train')
ax.errorbar(sizes, test_means, yerr=test_ints, label='quadratic test')
ax.legend()
plt.show()

","['machine-learning', 'books', 'generalization']",
Categorical loss function for variable number of labels,"
I have a model for binary classification. The target variable has the different number of labels (instances) in each sample. For example, a batch of size 2 with 2 and 3 instances and correspondingly with 2 and 3 labels (0 and 1):
y_true = np.array([[0., 1., -1., -1.],
                   [1., 0., 1., -1.]])

The maximal number of labels (instances) in a sample is equal to 4. -1 values are used as a mask. I created a function (in TensorFlow) that masks all -1 values and calculates the loss for each unmasked value and then the average of all losses:
def my_loss_fn(y_true, y_pred):
    mask = tf.cast(tf.math.not_equal(y_true, tf.constant(-1.)), tf.float32)
    y_true, y_pred = tf.expand_dims(y_true, axis=-1), tf.expand_dims(y_pred, axis=-1)
    bce = tf.keras.losses.BinaryCrossentropy(reduction='none')
    return tf.reduce_sum(tf.cast(bce(y_true, y_pred), tf.float32) * mask) / tf.reduce_sum(mask)

Is it a correct way mathematically? Should I add some weights depending on number of labels in a sample? Or calculate loss per sample (per row in my example) and then get the average? Should I mask unused labels at all? It looks like my model doesn't learn to classify correctly and predicts 1 for all labels.
","['deep-learning', 'loss', 'binary-classification', 'binary-crossentropy']",
Last linear layer of the decoder of a transformer,"
I am learning the transformers architecture from these two sources:
https://arxiv.org/pdf/1706.03762.pdf
https://jalammar.github.io/illustrated-transformer/
I just wanted to ask about the final step in the decoder. Let's fix testing time. As I understand, the decoder starts with an input of dimension $(N_{words},d_{emb})$, where $N_{words}$ is the number of words already predicted and $d_{emb}$ is the embedding dimension.
Now if we ""follow"" the following decoder steps, at each step (after e.g. the attention layers) we should have a vector of dimension $(N_{words},d_{model})$ where $d_{model}$ is the model dimension. In other words, up to the final linear layer we have $N_{words}$ vectors which are $d_{model}$-dimensional.
Are all these $N_{words}$ vectors fed into the last linear layer (before the softmax) or, as I suspect, only the last of these vectors is used ? In the latter case the last linear layer would be a matrix of dimension $d_{model}\times N_{vocab}$, where $N_{vocab}$ is the vocabulary dimension.
Is this correct ? Are there any issues in what I wrote ? Unluckily from the online sources I was not able to clarify this point...
PS: I conjectured that the last linear layer is using just the last vector, because than I would understand what happens in training time, one would just use in that case all the output vectors from the decoder, instead of just the last one, to have a parallelized prediction.
","['deep-learning', 'transformer']","
Edit
Based on the comments to the original version of this answer, OP indicated that the use case was translation between two languages.
Answer:
At sampling time, the last linear layer of the decoder is going to output a sequence whose length is incremented by one each time you apply the encoder-decoder transformer to the input sentence.
Let's take a practical example, with $w$ denoting the words in the original sentence and $w^{i}$ those in the target language after iteration $i$ of applying the transformer model.
If you have already sampled a sequence $(w^1_1, w^2_2, ..., w^{l}_l)$, the inputs to the encoder and the decoder the next time you apply the model (to get the next token $w^{l+1}_{l+1}$) are going to be respectively $(w_1, w_2, ..., w_T)$ (the original sentence) and $(w^1_1, w^2_2, ..., w^l_l)$. The output of the decoder (after sampling) will be $(w^{l+1}_1, w^{l+1}_2, ..., w^{l+1}_l, w^{l+1}_{l+1})$, but we're only going to keep $w^{l+1}_{l+1}$.
Then, applying the model again to get $w^{l+2}_{l+2}$, the new inputs to the encoder and the decoder are going to be $(w_1, w_2, ..., w_T)$ (again the original sentence) and $(w^1_1, w^2_2, ..., w^l_l, w^{l+1}_{l+1})$. The output of the decoder will be $(w^{l+2}_1, w^{l+2}_2, ..., w^{l+2}_l, w^{l+2}_{l+1}, w^{l+2}_{l+2})$, and we'll keep only $w^{l+2}_{l+2}$.
This continues until the sentence is fully translated.
At each step, we give the whole original sentence to the encoder so that the model can build its translation for the next word by looking at the whole sentence. And we give the sentence translated so far to the decoder so that the translation of the next word can attend to what has already been translated.
To confirm, you can have a look at what they do in the TensorFlow transformer tutorial.
"
Why does providing an extra prediction output help stabilize training?,"
I am reading the PRIMAL: Pathfinding via Reinforcement and Imitation
Multi-Agent Learning paper where they tackle the multi-agent path finding problem using reinforcement learning.
The problem is approached in a decentralized way where each agent learns a policy to reach their goal while cooperating with other agents. One way they encourage cooperation and avoid selfish agent behavior is to give a blocking penalty (i.e. a sharp negative reward when the agent is blocking other agents).
They say that ""adding a blocking prediction output which is updated by minimizing $L_{blocking}$, the log likelihood of predicting incorrectly"" helps stabilize training. They add that ""The “blocking” output of the network is trained to predict
when an agent is blocking others, to implicitly provide the
agent with an “explanation” of the extra penalty it will incur
in this case.""
My question is: By which mechanism does minimizing the blocking prediction loss help the agent learn more stably? Shouldn't the negative reward be enough to disencourage blocking behavior?
The specific architecture used is the following:

where thee 10x10x4 input tensors are just the local map around the agent and each channel is a binary matrix representing different information, namely: Position of other agents, Other agent's goals, obstacles, and the agent's own location. The Goal Position is just a vector pointing in the direction of the goal.
I assume that this additional ""implicit explanation"" helps the agent learn associations between the negative reward that come when blocking and hence somehow learns to take non-blocking actions faster. But how?
I know the exact mechanism might be somewhat mysterious so I would be satisfied at least with some intuitions about why this works. Extra value is given to hinting at precise mechanisms (e.g. The how the gradients of the blocking prediction loss function might affect the behavior of the agent so that it learns better)
","['reinforcement-learning', 'machine-learning', 'deep-rl', 'training', 'multi-agent-systems']",
What does it mean for an episode to start in a state-action pair?,"
In Sutton and Barto on chapter 5 (p.96), they talk about estimating state-action values with Monte Carlo:

For policy evaluation to work for action
values, we must assure continual exploration. One way to do this is by specifying that
the episodes start in a state–action pair, and that every pair has a nonzero probability of
being selected as the start. This guarantees that all state–action pairs will be visited an
infinite number of times in the limit of an infinite number of episodes. We call this the
assumption of exploring starts.

What does it mean for an episode to start in a state-action pair? Examples are welcome
","['reinforcement-learning', 'sutton-barto']",
When do you know that your neural network is learning something when metrics are garbage?,"
When training a neural network for binary classification on a highly imbalanced set its training loss decreases, however validation loss increase even though accuracy is very high (due to highly imbalanced dataset) and its other metrics (auc, recall, precision) are bad.
I want to figure out whether there is something wrong with the model architecture, therefore building a new model would be the right choice or is the model learning something; however you just need to tweak some hyperparameters.
By plotting activation outputs and weights at each layer we can see if there is a vanishing/exploding gradient or saturation at a certain layer (activation output being on extremes when using saturating activation functions like tanh, sigmoid).
My question is, is that only thing we can interpret from weights and activation output values?
What can weights distribution of layers at each layer tell us?
I'm very confused since from the DataCamp - TensorBoard Tutorial it says: “Visualizing network weights is important, because if the weights are wildly jumping here and there during learning it indicates something is wrong with the weight initialization or the learning rate.”,
however from:  Stack Overflow - Understanding TensorBoard (weight) histograms I’ve understood the answer as: ""changes in weight imply learning"" because the answerer looks at the first 3 layers (where their weights do not change at each iteration) and says that it is not learning, and looks at last layer (weight changes) and says that it is learning.
","['machine-learning', 'deep-learning', 'weights-initialization']",
Learning an identity function with convolutional networks,"
I am trying to train networks to achieve what I expected to be a trivial task: learn the identity mapping. However, this is very hard to achieve, and the optimization is hard.
Moreover, I don't want to learn $f_\theta(x)=x\;\;\forall x$, but only $f_\theta(x_1)=x_1$ for one particular example (I am trying to overfit!).
Any ideas why this optimization is so hard? The learning curves oscillate and most of the points are not visually satisfactory.
Code is here: https://colab.research.google.com/drive/1umys2gxJ8arodQ0PhLf5TFicfPx0mh74?usp=sharing
","['convolutional-neural-networks', 'convergence', 'u-net']","
Learning the identity function is not trivial at all.
The main reason is that the identity function is linear, and a neural network try to approximate it in a non linear fashion. Non linear activations in particular compress and expand values that linearly would have the same distance, so they are not suitable to approximate something like the identity function. I see you used a linear activation but the network still learn in a non linear fashion.
Residual Neural Networks (ResNet) were suggested precisely to help a neural network learning the identity function. The skip connection simply does the job of making the output equal to the input, forcing the network to focus on residuals (output - identity), so for a one layer ResNet learning the identity function become trivial cause the network simply has to learn to push all weights to 0.
But without this trick, approximating the identity is simply very hard, this is why your UNet experiment was not successful, even with a single training image.
"
Can YOLOv3 architecture be clearly separated into feature extractor and classifier parts?,"
I am new to machine learning and am confused about whether its architecture has clearly defined boundaries which demarcate the feature extraction and classification part. Or is it that it classifies the features extracted at different scales throughtout the network ?
","['yolo', 'ai-basics']",
Selecting neural network architecture for battery degradation problem,"
I want to design a neural network capable of detecting the degradation of the capacity of a battery. When the capacity is ok, the battery performs like this (x axis is the time in seconds and y axis is the voltage in volts):

However, if there is a problem in the battery, the capacity starts to decrease, which means the following for the voltage vs time:

So deterioration happens when the period of the signal decreases. I would like to know when this period has decreased a certain percentage so I can do some counter-actions.
I understand that I need an LSTM neural network to predict this situation, however I don't know where to start, and all examples I have read are not for this kind of problem. Any thoughts? Also It would be great If you can provide any example/tutorial of a similar problem.
","['long-short-term-memory', 'prediction']",
Why does REINFORCE perform badly at first in Sutton and Barto Figure 13.1?,"
In Sutton and Barto (PDF, page 265), 2nd edition, Figure 13.1 applies REINFORCE to the ""short corridor with switched actions"" environment from Example 13.1. The figure looks like this:

My question is, why is the initial performance so poor? I believe it should be close to optimal even without any training.
My reasoning is this: If we initialize the algorithm with $\boldsymbol{\theta} = \boldsymbol{0}$, then the initial policy has
$$\pi({\tt right}|s,\boldsymbol{\theta}) = \frac{e^{\theta_1}}{e^{\theta_1} + e^{\theta_2}} = 0.5.$$
And from the figure in Example 13.1 (p. 323) --

-- we know that a policy that goes $\tt{right}$ with probability 0.5 in every state has an expected value that is very close to that of the optimal policy, around $-12$ or so. So shouldn't the policies produced by REINFORCE start close to optimal with essentially no training, and then just improve little to none after that?
My own experiment supports my hunch:

But it's possible I'm doing something wrong, both theoretically and computationally.
","['reinforcement-learning', 'sutton-barto', 'reinforce']",
Choice of LSTM for price prediction,"
I have a dataset with features (f) for different stocks (S) and want to infer for price using an LSTM model. Here is my df:




year
S1_price
S1_f1
S1_f2
S2_price
S2_f1
S2_f2
Sn_price
Sn_f1
Sn_f2




2010
100
0.1
0.12
200
0.2
0.22
300
0.3
0.32


2011
105
0.4
0.42
205
0.5
0.52
305
0.6
0.62


2012
110
0.7
0.72
210
0.8
0.82
310
0.9
0.92


n













and so on... (example values).
I would like to predict the prices of every stock by using the features as inputs looking 1yr back into the past.
Example for Stock 1 (predict 2012):
[[0.1 0.12]
 [0.4 0.42]] 110

However, I want to to that for all stocks, so I am not sure which type of LSTM to use.
Your help would be very much appreciated!
",['long-short-term-memory'],"
I tend to like GRU and its variants for this type of problem. Although, if your dataset isn't too large you can test some toy models(smaller versions) of the different types to know which will achieve the best accuracy.
Typically a good size to start with in that case would be about 20-25% of your full model size.
"
"Control variables and cofounding effects in stochastic programming/,model predictive control/reinforcement learning","
How can we be sure that confounding variables/control variables don’t pickup the effect our decisions w.r.t decision variables had on the actual control variable?
Since the term control variable overlap in regression and control theory I want to make it clear that I use the term “control variable” when I refer to an variable aiming to pick up confounding effects that we don’t want to pick up in our decision variables. Decision variables then being the variables that we can control in our optimization scenario.
Consider the resource allocation problem where we want to allocate an portion of the total resource B to two investment options every day over an finite horizon H.
Consider this reward function
$Y_{t} = intercept + c_{1} * x_{1t} + c_{2} * x_{2t} + c_{3} * trend_{t}$
Where $Y_{t}$ is sales at timestep ${t}$, $x_{1t}$ is budget for investmentoption 1 at timestep t, $x_{2t}$ is budget for investmentoption 2 at timestep t.
$Trend_{t}$ is the trend at timestep t and have been fitted on the datasets historical sales data by using fourier decomposition.
The trend variable is thought of as capturing the underlying market trend that we cannot control for.
Consider the following algorithm:

Fit our reward function to the historical data by performing OLS.
Maximize the reward function over the whole horizon s.t to constraints.
Set the allocation for the current timestep.
See the reward for the current timestep.
Add the allocation and reward to our dataset.
Repeat step 1.

Consider the horizon 30. Meaning that we optimize for one month.
Now if I start to optimize this, the actual trend might change, hopefully the sales will start increasing... now, how can I be sure that this change is reflected in our decision variable coefficients and not our trend variable.
Two interesting scenarios is when we leave c1 as it is, a timeinvariant coefficient and if we don’t, let’s say we fit it such that we aim to capture day in month effects on our decision variables thereby letting the algorithm optimize also w.r.t day of month.
I am thinking if this would impose a problem or if im missing out on something. Seems like it should be a quite normal problem in stochastic programming/reinforcement learning/model predictive control.. basically any sequential decision problem where we have a predefinied model with both decision variables and variables that aim to control confounding effects.
To check if this would impose a problem I assume we could check the variance inflation factor and correlation between the trend variable and our decision variables?
But what is the actual solution to the problem?
Maybe fit the trend component to our historic data and then when we roll out the algorithm, we solely extrapolate the trend we saw before we started the algorithm?
","['reinforcement-learning', 'markov-decision-process', 'linear-regression', 'control-problem', 'control-theory']",
Basic question about gradient for nominal regression,"
Say that we want to binary-classify images using a sigmoid function with the entropy-loss function. This algorithm is quite slow.
The sigmoid function is:

I find that this could be traced to the $L(y,y') = -log(y')$ for $y=1$ and $L(y,y') = log(1-y')$,  for $y=0$.
EDIT But actually that is a great, desired property! The problem is the
second term in the total derivative respect to the Loss ($L$):
$$dL = dL/dy * dy/dz ...$$
because it is $dy/dz$ is very small for large or small values of $z$:

Linear regression wouldn't have this problem as that term is 1 (or non existent). But there could be an ""explosion"" of the gradient because of the $Loss = (y-y')^2$.
Question:
So I'd suggest to use a cutoff value for y' in, say, 10 if your labels are (-1 for not, 1 for yes).
Has this been tried before?  If so what were the results?
Is this a reasonable approach or would it just not work?
","['math', 'regression', 'gradient']",
"Binary Classification Model Comparison - Interpretation of Training, Test and Validation Set Performance","
I am looking for some advice regarding the best choice of binary classification model based on training, validation and test set results. Model 1 (results in 1st image) shows better test set results than Model 2, but Model 2 (results in 2nd image) shows results that seem more intuitive to me with better training set performance than its test set performance. I feel as if the Model 1 test set results might have been a bit of a fluke, whereas Model 2 appears more like a well trained model with more long-term reliability.
Any advice on this is much appreciated.


","['binary-classification', 'confusion-matrix']",
"What is the ""attack success rate"" of an Adversarial Attack?","
For a typical adversarial attack, a sample $x_{0}$ is chosen from a training set belonging to class $C_{i}$ and a transformation $A$ is applied such that the adversarial example $x=A(x_{0})$ would be misclassified as $x \in C_{target}$. The adversarial example $x$ could be found iteratively through ${x}={x}_{0}+\eta \cdot \operatorname{sign}\left(\nabla_{{x}} J\left({x}_{0} ; {w}\right)\right)$ for instance.
In this documentation of TextAttack I read

Out of those 157, 29 attacks failed, leading to a success rate of 128/157 or 81.5%.

My question is: how do we chose the starting samples $x_{0}$ (here the 157) to evaluate the success rate of a type of adversarial attack? Do we chose several target classes? Several initial classes $C_{i}$? How many samples do we chose? From which set (training? validation?)
In the DeepFool paper, they'd use the entire test set- but is it standard practice to claim the success rate of a type of attack this way?
","['adversarial-ml', 'testing', 'adversarial-attacks']",
combine two features in dataset?,"
I have  a data set containing the number of security gaps and the level of that gap for a specific website.
Now suppose I have 2 features in this data set, the first feature is the number of a specific security gap and the second feature is the risk of this gap for a specific website.
How can I combine these two features into one?
What is the best way to apply feature engineering to these features?
Thanks
","['machine-learning', 'datasets', 'data-preprocessing', 'feature-maps']",
"In the Policy Gradient Theorem proof, why is $d^\pi(s) = \sum_{k=0}^{\infty}\gamma^{k}Pr(s_0 \rightarrow s, k, \pi)$ true?","
I was reading the original Policy Gradient Paper. I didn't quiet get the last step of the proof for the policy gradient theorem. The proof given in the paper is below:

I don't understand how the last line follows from the second to last line. That is how is the following true? $d^\pi(s) = \sum_{k=0}^{\infty}\gamma^{k}Pr(s_0 \rightarrow s, k, \pi)$
where $d^\pi$ is the stationary distribution of the MDP with fixed policy $\pi$ and $Pr(s_0 \rightarrow s, k, \pi)$ is probability of transitioning from state $s_0$ to state $s$ in exactly $k$ timesteps.
","['reinforcement-learning', 'markov-decision-process', 'policy-gradients', 'policy-gradient-theorem']",
Darknet as a part of Yolo v3,"
I am pretty new to ML and my question may look strange. Especially the last part of it.
1)As far as I understand Darknet53 is an integral part of Yolo just as Resnet50 is a part of R-CNN Am I right?
2)On the other hand I understand that the R-CNN alternatively can be used based on VGG instead of the Resnet, 16 am I right?
3)Can YOLO also be used based on something else but Darknet53 ?
4)If not what was the use in defining Darknet53 as not integral part of YOLO but as some separate section with its own name? Maybe it has some non-Yolo use?
5)I have almost finished an online course in YOLO-V3\4 and I am quite sure that in this course they do not use nor Pytorch nor Tensorflow nor Keras, but when I read  online tutorials they often do mention at least one of them relating to YOLO. So what do they actually do? And what is their advantage comparing to not using them at all as in a course I am doing now?
Thank you very much.
","['tensorflow', 'yolo', 'residual-networks', 'r-cnn', 'darknet']",
Storing training dataset in a platform like mlflow,"
I am pretty new to Machine learning and would like to know whether there are experiment management platforms that also allow storing and managing training datasets (images, in my case). I am familiar with the ML-Flow, but IMHO it doesn't support such an option, am I right? Anyways if there are any, please tell me which. If there are no platforms like this, how would you suggest managing training datasets in combination with existing platforms? I appreciate any help you can provide.
","['machine-learning', 'training-datasets', 'storage']",
Why does importance sampling work with latent variable models?,"
Caveat: importance sampling doesn't actually work for variational auto-encoders, but the question makes sense regardless
In ""L4 Latent Variable Models (VAE) -- CS294-158-SP20 Deep Unsupervised Learning"", we see the following.
We want to optimize the following:
$$
\sum_i\log\sum_zp_z(z)p_\theta(x^{(i)}|z)
$$
But this is hard, since we cannot enumerate over all possible $z$.
So, we could use random sampling instead:
$$
\sum_i\frac{1}{K}\sum_{k=1}^Kp_\theta(x^{(i)}|z_k^{(i)})
$$
where $z_k^{(i)} \sim p_z(z)$.
However, this doesn't work because for most values of $z_k^{(i)}$, $p_\theta$ (and its gradient) will be near 0. For example, consider a 100-dimensional binary latent variable space. The probability you sample the correct $z$ to produce the given $x^{(i)}$ is $0.5^{100}$.
To counter-act this, we can use importance sampling:
$$
\sum_i\log\frac{1}{K}\sum_{k=1}^K\frac{p_z\left(z_k^{(i)}\right)}{q\left(z_k^{(i)}\right)}p_\theta(x^{(i)}|z_k^{(i)})
$$
Where $q(z) = p_\theta(z|x^{(i)})$ ($q$ is good at predicting what latent variable represents the sample $x^{(i)}$).
Of course there are problems with calculating $q$, but let's ignore that for now.
The thing I don't understand here is, sure $q$ might be good at predicting values of $z$, for which $p_\theta(...)$ is non-zero, but we are scaling all of those values by $p_z(z_k^{(i)})$, which will be very small. So the end result is that the final sum will still be very low for a given $x^{(i)}$. (I think) this means when doing back-propagation through $p_\theta(x^{(i)}|z_k^{(i)})$, the gradient update will be very small, which is the problem we were trying to avoid in the first place.
Can someone explain to me what I'm missing?
","['variational-autoencoder', 'probability-distribution', 'importance-sampling', 'bayesian-optimization']",
Are there chatbots which use AI but not machine learning?,"
I want to see the differences between two chatbots where the first one is based on AI without ML and the second one which is based on ML.
",['machine-learning'],
linear layer bert sentence embedding,"
I have a special situation where I need to embed a sentence with bert-sentence transformer and get a numeric value for it. This is not possible with this model, because we get a vector of very high dimensionality len([v1, v2 , ...]) = 384, and I don't know which valid method exists to reduce that dimensionality to a single value. I have the following code to compute the embedding model:
from sentence_transformers import SentenceTransformer
sbert_model = SentenceTransformer('all-MiniLM-L6-v2') 
sentence_embeddings = sbert_model.encode(sentences)
print('BERT Embedding Vector length', len(sentence_embeddings[0])) # = 384

I need a rigorous way to get a one numeric value embedding for each sentence (1000 sentences in total), so the shape of sentence_embeddings.shape() = (1000, 384). I did the following with linear layers:
import torch.nn as nn
def linear_layer(embeddings: np.array, input_m, output_r):
    inputs = torch.from_numpy(embeddings)
    m_to_r_feat = nn.Linear(input_m, output_r)
    out = m_to_r_feat(inputs).flatten()
    return out.detach().numpy()
1d_embedding = linear_layer(sentence_embeddings, 384, 1)

Is this Ok? And do I need to do it for each sentence, or the way I did it is correct?
","['neural-networks', 'natural-language-processing', 'python']",
Policy and Value function as a function of Discount,"
I'm learning MDP's and I'd like to know if I'm doing these exercises correctly:
Consider the following MDP:

The agent starts at the cell marked by $S$. Whenever it enters a cell with a positive number, the agent receives that award and the episode terminates. Consider $X=3$.

Question 1: When $X=3$, what is the optimal policy in $\pi_*$ state $S$ as  a function of the discount?


Question 2: Draw the graph of the optimal value $v_*(s)$ of the starting state $S$, as a function of $\gamma$ (on the $x$-axis, from $0$ to $1$) with $v_*(s)$ on the y-axis.

Answer to question 1: We first compare when the agent is incentivized to go to state $+3$ rather than $+2$, which will be if the return of going to state $+3$ is greater than the return of going to $+2$. Mathematically:
\begin{align}
\gamma^2(3) > \gamma(2) \\
\gamma > 2/3
\end{align}
Now we compare when the agent will go to $+2$ instead of $+1$:
\begin{align}
\gamma(2)>1 \\
\gamma > 1/2
\end{align}
So we have that for $\gamma\leq \frac{1}{2}$, agent chooses to go to $+1$. For  $\frac{1}{2} < \gamma \leq \frac{2}{3}$, agent will go to $+2$, and for $\gamma\gt \frac{2}{3}$, agent will go to $+3$.
Answer to question 2:
By question 1 we would have a graph something like this:

Would this be correct?
","['reinforcement-learning', 'value-functions', 'policies', 'discount-factor']",
Implementing K fold cross validation,"
Here is my current neural network
def finalModel(optimizer= ""Adam"", n_hidden= 20, n_neurons= 100, learning_rate= 0.00001, input_shape=X_train_NN.shape[1:], activation=""selu"", loss = ""mean_squared_error"", epochs = 3, batch_size = 64):
model = Sequential()
options = {""input_shape"": input_shape}
for layer in range(n_hidden):
    model.add(Dense(n_neurons, activation, **options))
    options = {}
model.add(Dense(1, **options, activation='sigmoid'))
optimizer = keras.optimizers.Adam(learning_rate)
model.compile(loss = loss, optimizer = optimizer, metrics = 'AUC')
return model
model = finalModel()

hist = model.fit(X_train_NN, y_train_NN, validation_data=(X_valid, y_valid), epochs=10, batch_size=16)
hist = model.fit(X_train_NN, y_train_NN, validation_data=(X_valid, y_valid), epochs=3, batch_size=16) 
y_predicted_test = model.predict(X_test_BRCAT_ML) > 0.5
mat = confusion_matrix(y_test, y_predicted_test)
labels = ['No cancer', 'Cancer']
 
acc = hist.history['auc']
val = hist.history['val_auc']
epochs = range(1, len(acc) + 1)

It fits fairly well to my data (for what I am expecting) however AUC drops significantly on the validation set and testing set- from .66 on the train data to .52 on validation and 0.51 on test.
I was hoping to implement K fold cross validation to improve the model but can't find any good examples of it online with a model like mine.
Furthermore does anyone have any suggestions for ways I can prevent overfitting of my model? I know decreasing the neurons and hidden layers will definately help but that lowers the AUC all around.
",['keras'],
Getting higher resolution images,"
For some image processing CNN you need to downscale your image to the input of the network. For example, some ask for images of 256x256, while others 512x512. In this process you can conserve the ratio of the image by adding white paddings.
After applying the CNN, you end up with an image of probably the same input size (256x256, 512x512...) but of course modified. For example, I'm interested in networks that do impainting (removes some object in the scene and replaces by something random).
Now, my question is: Given the initial image in high resolution, and the final image in lower resolution (but with some parts changed), is there any model than can improve the resolution of my lower resolution image? I know there are superresolution models that do that, but given I have this higher resolution image I wonder if this image would help in the scaling up process.
","['convolutional-neural-networks', 'image-processing', 'image-generation', 'image-super-resolution']",
Can I ignore duplicate records of dataset for training?,"
I have a a dataset that most of records and their corresponding labels are the same and only timestamp of each records is different from other record.
If I ignore duplicate records in for training some algorithm like DQN, is this a correct approach?
","['machine-learning', 'training']","
This depends on:

Whether the timestamp is additional information. I.e. is the temporal dimension relevant?
Removing samples will shift the distribution of the data set. I.e. if you have 2 possible states, with 90 copies of 1 and 10 copies of the other, removing all duplicates means the model will not come into contact with the 90/10 ratio in the data, but will see a 1/1 ratio, this can bias your model towards the underrepresented class, reducing performance.

"
Question for the derivation of the probability of a trajectory,"
I'm studying reinforcement learning now and I'm quite a newbie to this field.
I have some questions about how to derive the equation as below.
$p_{\theta}(s_{1},a_{1},\dots,s_T,a_T)=p(s_1)\prod_{t=1}^T(\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t))$
The equation above is the probability of a trajectory.
But in my opinion, the equation should be,
$p_{\theta}(s_{1},a_{1},\dots,s_T,a_T)=p(s_1)\prod_{t=1}^T\pi_\theta(a_t|s_t)\prod_{t=1}^{T-1}p(s_{t+1}|s_t,a_t)$
I don't know why $p(s_{T+1}|s_T,a_T)$ emerges in the original equation because there is no $s_{T+1},a_{T+1}$ in the trajectory. I think I'm missing something.
Can somebody help me?
","['reinforcement-learning', 'probability']",
What's the relationship between Bayesian RL and POMDPs?,"
Bayesian RL seems concerned with having uncertainty over the transition function of the environment, while POMDPs try to capture uncertainty over the state one is currently in. However, both end up reducing to something like a belief MDP: an MDP in which everything is known, but the states are points in belief space.
Could anyone explain what the differences between these are in more detail?
","['reinforcement-learning', 'markov-decision-process', 'pomdp', 'bayesian-reinforcement-learning']",
Why in coordinate regression task Resnet34 performs better than Resnet50?,"
I'm doing a coordinate regression task for an object.
something like this (not exactly this), where the label would be (17,40).
So, when I'm using Resnet50 I'm getting poorer results as compared to Resnet34.
Can someone explain to me why it could be happening?

","['computer-vision', 'residual-networks']",
How to obtain the graph in the video tutorial,"
I am watching this video tutorial https://www.youtube.com/watch?v=gmjzbpSVY1A&t=1002s . At 16:34, the author show the variation of the line when weight change from 0.97

to -1, the below graph is produced

How to get the graph as i try to draw on a piece of paper, I cannot get the same shape, please point out where my mistake.
Below is what I drawn starting from the first neuron

and the output at the last neuron shouldn't it be zero?

","['neural-networks', 'relu']",
Why does a PPO agent perform only the action that costs the least?,"
I am trying to implement an intelligent agent that can perform penetration testing within the nasim (link) environment, a network simulator.
I would like to try to use parametric mode for actions, and therefore I need an agent that can work with OpenAI gym's MultiDiscrete actions since nasim implements OpenAI gym. I have therefore chosen to use PPO's implementation of Stable Baselines3 (link).
Everything seems to work when I run the agent on a small scenario (tiny scenario). Still, when I try to run it on a somewhat larger environment (tiny-small scenario) the agent after a few hundred thousand timesteps decides to run only the action that costs the least. Here is the code of the agent:
import gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import nasim

if __name__ == ""__main__"":

    # Build the environment
    env = gym.make(""nasim:TinySmall-v2"")

    # Train the model
    model = PPO(""MlpPolicy"", env, verbose=1)
    model.learn(total_timesteps=500000)

    # Init the environment
    obs = env.reset()

    # Init variables
    total_reward = 0
    number_of_steps = 0
    done = False
    
    # Do the test
    while done != True:
        action, _states = model.predict(obs)
        number_of_steps = number_of_steps + 1
        obs, rewards, done, info = env.step(action)
        print(""Action: "" + str(action) + "", reward: "" + str(rewards))
        total_reward += rewards
        print(""Total reward: "" + str(total_reward))
        print(done)
        print(""Steps: "" + str(number_of_steps))
        env.render(mode='readable')

I tried looking for the cause of the problem in the algorithm, including watching this explanation video (link)., but I could not identify the cause of the problem.
Step to reproduce the problem:

Install stable-baselines3: pip install stable-baselines3
Install version 0.9.1 of nasim (version 0.10 is too recent to be compatible with the PPO implementation of stable-baselines 3): pip install -Iv nasim==0.9.1
Install the last required component: sudo apt install python3-tk
Save the agent code written above in a .py file and run it.

","['reinforcement-learning', 'proximal-policy-optimization']",
How to use NN to generate a model which produces given distributions?,"
For a non-Markovian random walk, each step can go up or down. And for the $n-th$ step, its step size $s(n)$ may depend on the path of walk, and the probability for going up or down may also depend on the path of walk.
Q: Assume that the distribution of this random walk approach a given distribution, how can we use NN to find such a random walk which can generate such expected distribution ?
","['neural-networks', 'machine-learning']",
What are the main arguments predicting a Soft Takeoff vs Hard Takeoff scenario and viceversa?,"
Here I am referring to the term AI Takeoff as described here on LessWrong, with annexed ""Soft"" and ""Hard"" takeoff definitions.
I find that page to be biased slightly more towards the hard takeoff scenario in terms of citing further resources to read, so was wondering if someone could provide a more balanced overview of the arguments from both camps.
","['agi', 'ai-safety']",
Is my Deep Feature Extractor pertinent?,"
I have multiple DNN that can extract features vector from images.
Those can be used for two main goals:

Use them for transfer learning ang faster trainings
Use them as feature extractors, and train only additional, small ""heads"" (in particular if my dataset is really small)

But how can I choose which trained network to use?
For the moment, I use:

A UMAP projection on a labelled dataset (that the networks have never seen), and a compare visually how well my various classes are ""clustered"".

A ""Raw 10-NN Mean"" score. For each point, I count how many of their 10 nearest neighbours are of the same class. I Divide by 10 to get the score.
A ""Class 10-NN Mean"" score. Same as before, but each class has the same weight, regardless of their proportion in the dataset.

Are there any other interesting metrics I could use to compare my feature extractors?
Many thanks!
","['deep-learning', 'features', 'feature-extraction']",
Image suggestions using feature extractor + ANN index,"
I am trying to build an image suggestion engine using resnet151 as a feature extractor and right now I am testing Annoy and Faiss as the ANN.
I am having some issues with images that have similar shapes, but varying color features.
My original method was to use Euclidian distance for distance measure between the reference image and the vectors in the index.
I later read that using the dot product as distance measure might work better because it combines the magnitude and cosine angle, so I have tested that now, but I feel like it is performing worse than with Euclidian distance. Now the index can't tell obviously different images apart.
Whilst researching I stumbled across this question:
How is a ResNet-50 used for deep feature extraction?
Where the writer of the accepted answer states that in some way Euclidian distance can be used as a similarity measure, but not really.
Can anyone shed some light upon why Euclidian distance would be unreliable as a similarity measure and if possible provide suggestions to what other methods can be used for a image suggestion engine without using thousand of reference images?
","['python', 'image-recognition', 'search']",
Alternatives to Bayesian optimization,"
I am given a dataset $\mathcal{D} = \{\mathbf{x}_i\}_{i=1}^n$ and I need to find the point (in my case a material) $\mathbf{x}^*$ that maximizes a property $y$ (which can be obtained from a black-box function $f(\mathbf{x}$), performing the least amount of labeling (because it is expensive).
I have checked Bayesian optimization but it uses a Gaussian process as a surrogate model which makes the training extremely slow for a high-dimensional feature space and large datasets (high $n$).
Are there any alternatives to Bayesian optimization for such a task?
I have checked Particle Swarm optimization but I think it is not applicable to my case, since the particles must be allowed to access any $\mathbf{x}$ while $\mathcal{D}$ contains only specific points.
I should add that the features of $\mathcal{D}$ are all continuous.
","['machine-learning', 'optimization', 'regression', 'bayesian-optimization', 'black-box']",
,,,
Why isn't the evidence $p(x) = 1$ if it's an observed variable?,"
Every explanation of variational inference starts with the same basic premise: given an observed variable $x$, and a latent variable $z$,
$$ p(z|x)=\frac{p(x,z)}{p(x)} $$
and then proceeds to expand $p(x)$ as an expectation over $z$:
$$ p(x) = \int{p(x,z)dz} $$
and then states that it's too difficult to evaluate.
My very very basic question is why is $p(x)$ not simply equal to 1? It's an observed variable!
","['machine-learning', 'probability-theory', 'bayes-theorem', 'variational-inference']","
You're forgetting that $x$ can assume several values with different probability. Let's say that $x$ represent the roll of a fair dice. Then $p(x)$ will be 1/6 for all six possible values of $x$.
$$ p(\theta|x)=\frac{p(x|\theta)p(\theta)}{p(x)} $$
If you rearrange the formula it becomes clear that the whole point of Bayes theorem is to say that we want matching prior $p(\theta)$ and posterior $p(\theta|x)$ distributions.
$$ \frac{p(\theta|x)}{p(\theta)}=1=\frac{p(x|\theta)}{p(x)} $$
cause if the left side holds, then it means that the likelihood predicted by our parameters $p(x|\theta)$ is equal to the real observed probability $p(x)$.
So if we train a model on some observed dice rolls, we expect a perfect model to learn for each face of the dice the real probability $p(x)$ which is 1/6 and not 1.
"
,,,
