{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Head</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tags Count</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brain segmentation to 3d model</td>\n",
       "      <td>my goal is to take a dataset of brain tumor s...</td>\n",
       "      <td>['computer-vision', 'python']</td>\n",
       "      <td>brain segmentation to 3d model  my goal is to ...</td>\n",
       "      <td>2</td>\n",
       "      <td>brain segmentation 3d model goal dataset brain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>active learning regression with random forest</td>\n",
       "      <td>i have a dataset of about 8k points and i am ...</td>\n",
       "      <td>['machine-learning', 'regression', 'uncertaint...</td>\n",
       "      <td>active learning regression with random forest ...</td>\n",
       "      <td>5</td>\n",
       "      <td>active learn regression random forest dataset ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comparing reinforcement learning models</td>\n",
       "      <td>i am currently completing my thesis on optimi...</td>\n",
       "      <td>['reinforcement-learning', 'policy-gradients',...</td>\n",
       "      <td>comparing reinforcement learning models  i am ...</td>\n",
       "      <td>4</td>\n",
       "      <td>compare reinforcement learning model currently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why good model that performs great on holdout ...</td>\n",
       "      <td>i have this binary regression model that has ...</td>\n",
       "      <td>['deep-learning', 'deep-neural-networks', 'pre...</td>\n",
       "      <td>why good model that performs great on holdout ...</td>\n",
       "      <td>5</td>\n",
       "      <td>good model perform great holdout validation da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what are reservoir computers used for today</td>\n",
       "      <td>reservoir computers were very popular in the ...</td>\n",
       "      <td>['machine-learning', 'recurrent-neural-network...</td>\n",
       "      <td>what are reservoir computers used for today   ...</td>\n",
       "      <td>3</td>\n",
       "      <td>reservoir computer today reservoir computer po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Head  \\\n",
       "0                     brain segmentation to 3d model   \n",
       "1      active learning regression with random forest   \n",
       "2            comparing reinforcement learning models   \n",
       "3  why good model that performs great on holdout ...   \n",
       "4       what are reservoir computers used for today    \n",
       "\n",
       "                                                Body  \\\n",
       "0   my goal is to take a dataset of brain tumor s...   \n",
       "1   i have a dataset of about 8k points and i am ...   \n",
       "2   i am currently completing my thesis on optimi...   \n",
       "3   i have this binary regression model that has ...   \n",
       "4   reservoir computers were very popular in the ...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0                      ['computer-vision', 'python']   \n",
       "1  ['machine-learning', 'regression', 'uncertaint...   \n",
       "2  ['reinforcement-learning', 'policy-gradients',...   \n",
       "3  ['deep-learning', 'deep-neural-networks', 'pre...   \n",
       "4  ['machine-learning', 'recurrent-neural-network...   \n",
       "\n",
       "                                                Text  Tags Count  \\\n",
       "0  brain segmentation to 3d model  my goal is to ...           2   \n",
       "1  active learning regression with random forest ...           5   \n",
       "2  comparing reinforcement learning models  i am ...           4   \n",
       "3  why good model that performs great on holdout ...           5   \n",
       "4  what are reservoir computers used for today   ...           3   \n",
       "\n",
       "                                        Text_Cleaned  \n",
       "0  brain segmentation 3d model goal dataset brain...  \n",
       "1  active learn regression random forest dataset ...  \n",
       "2  compare reinforcement learning model currently...  \n",
       "3  good model perform great holdout validation da...  \n",
       "4  reservoir computer today reservoir computer po...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/cleaned_data_2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tags'] = df['Tags'].apply(lambda x: literal_eval(x))\n",
    "all_tags = [item for sublist in df['Tags'].values for item in sublist]\n",
    "print(len(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_set = set(all_tags)\n",
    "unique_tags = list(my_set)\n",
    "print(len(unique_tags))\n",
    "from collections import Counter\n",
    "counts = Counter(all_tags)\n",
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_words = counts.most_common(20)\n",
    "tags_features = [word[0] for word in frequencies_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(tags):\n",
    "    tags_filtered = []\n",
    "    for i in range(0, len(tags)):\n",
    "        if tags[i] in tags_features:\n",
    "            tags_filtered.append(tags[i])\n",
    "    return tags_filtered\n",
    "\n",
    "df['Tags'] = df['Tags'].apply(lambda x: most_common(x))\n",
    "df['Tags'] = df['Tags'].apply(lambda x: x if len(x)>0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Tags'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are losing 10k rows of data, but it is for the greater good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text_Cleaned']\n",
    "y = df['Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "bin = MultiLabelBinarizer()\n",
    "y_bin = bin.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer = 'word', max_features=3000, ngram_range=(1,3), stop_words='english')\n",
    "X = tfidf.fit_transform(df['Text'])\n",
    "print(X.shape, y_bin.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "svc = LinearSVC()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "for classifier in [lr, svc, sgd, mnb, bnb]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classifier.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    print(classifier.__class__.__name__, f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(df['Text_Cleaned'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(token.word_counts))\n",
    "vocab_size = len(token.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = token.texts_to_sequences(df['Text_Cleaned'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 250\n",
    "X = pad_sequences(encoded_text, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, random_state = 42, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_bin.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "                ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text for word2vec model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sent = [sen.split() for sen in df['Text_Cleaned'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentenceembedding using word2vec\n",
    "\n",
    "model_w2v = Word2Vec(list_of_sent, vector_size=100, window=3, min_count=1, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('install', 0.8860217332839966),\n",
       " ('spyder', 0.8797866106033325),\n",
       " ('cli', 0.8775292038917542),\n",
       " ('python37', 0.8757234811782837),\n",
       " ('rstudio', 0.8720542788505554),\n",
       " ('sudo', 0.8712289333343506),\n",
       " ('installation', 0.8690873980522156),\n",
       " ('rpy2', 0.8649067878723145),\n",
       " ('baselines3', 0.8648353219032288),\n",
       " ('installer', 0.8644696474075317)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar('pip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tag_question(question, model, topn=5):\n",
    "    # Preprocess and tokenize the question\n",
    "    question_tokens = preprocess_and_tokenize(question)\n",
    "    \n",
    "    # Calculate the average Word2Vec embedding for the question\n",
    "    question_vector = sum(model.wv[word] for word in question_tokens if word in model.wv) / len(question_tokens)\n",
    "    \n",
    "    # Find the most similar tags\n",
    "    similar_tags = model.wv.most_similar(positive=[question_vector], topn=topn)\n",
    "    \n",
    "    return similar_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('siamese', 0.9015004634857178), ('ffnn', 0.8987703323364258), ('layered', 0.8956176042556763)]\n"
     ]
    }
   ],
   "source": [
    "question = \"How to train a neural network for image classification?\"\n",
    "tags = tag_question(question, model_w2v,3)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
