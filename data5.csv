Head,Body,Tags,First Answer
How is the state-value function expressed as a product of sums?,"
The state-value function for a given policy $\pi$ is given by
$$\begin{align} 
V^{\pi}(s) &=E_{\pi}\left\{r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\cdots \mid s_{t}=s\right\} \\ 
&=E_{\pi}\left\{r_{t+1}+\gamma V^{\pi}\left(s_{t+1}\right) \mid s_{t}=s\right\} \tag{4.3}\label{4.3} \\ 
&=\sum_{a} \pi(s, a) \sum_{s^{\prime}} \mathcal{P}_{s s^{\prime}}^{a}\left[\mathcal{R}_{s s^{\prime}}^{a}+\gamma V^{\pi}\left(s^{\prime}\right)\right] \tag{4.4}\label{4.4}
\end{align}$$
It is given in section 4.1 of the first edition of Sutton and Barto's book (equations 4.3 and 4.4).
I don't understand how equation \ref{4.4} derives from equation \ref{4.3}. How can I get the product in equation \ref{4.4} from the expectation in equation \ref{4.3}?
","['reinforcement-learning', 'math', 'value-functions', 'books', 'expectation']","A quick review of resolving expectations: If you know that a discrete random variable $X$, drawn from set $\mathcal{X}$ has probability distribution $p(x) = \mathbf{Pr}\{X=x \}$, then$$\mathbb{E}[X] = \sum_{x \in \mathcal{X}} xp(x)$$This equation is the core of what is going on when resolving the expectation in your quoted equation.Resolving the expectation to show how the value function of a state relates to the possible next rewards and future states means summing up all possible rewards and next states. There are two components to the distribution over the single step involved - the policy $\pi(a|s)$, and the state progression $P^a_{ss'}$. As they are both independent probabilities, they need to be multiplied to establish the combined probability of any specific trajectory.So, looking at only a single trajectory starting from state $s$, the trajectory of selecting action $a$ and ending up in state $s'$ has a probability of:$$p_{\pi}(a,s'|s) = \pi(a|s) P^a_{ss'}$$Iterating over all possible trajectories to get the expected value of some function of the end of the trajectory $f(s,a,s')$ looks like this:$$\mathbb{E}[f(S_t, A_t, S_{t+1})|S_t=s] = \sum_a \pi(a|s)\sum_{s'}P_{ss'}^a f(s,a,s')$$It is important to note that the sums are nested here, not separately resolved then multiplied. This is standard notation, but you could add some brackets to show it:$$\mathbb{E}[f(S_t, A_t, S_{t+1})|S_t=s] = \sum_a \pi(a|s)\left(\sum_{s'}\left(P_{ss'}^a f(s,a,s')\right)\right)$$In the equation from the book, $f(s,a,s') = R_{ss'}^a + \gamma v_{\pi}(s')$"
Are generative models actually used in practice for industrial drug design?,"
I just finished reading this paper MoFlow: An Invertible Flow Model for Generating Molecular Graphs.
The paper, which is about generating molecular graphs with certain chemical properties improved the SOTA at the time of writing by a bit and used a new method to discover novel molecules. The premise of this research is that this can be used in drug design.
In the paper, they beat certain benchmarks and even create a new metric to compare themselves against existing methods. However, I kept wondering if such methods were actually used in practice. The same question is valid for any comparable generative models such as GAN's, VAE's or autoregressive generative models.
So, basically, are these models used in production already? If so, do they speed up existing molecule discovery and/or discover new molecules? If not, why not? And are there any remaining bottlenecks to be solved before this can be used?
Any further information would be great!
","['generative-adversarial-networks', 'generative-model', 'geometric-deep-learning', 'mo-flow', 'drug-design']",
When should you not use the bias in a layer?,"
I'm not really that experienced with deep learning, and I've been looking at research code (mostly PyTorch) for deep neural networks, specifically GANs, and, in many cases, I see the authors setting bias=False in some layers without much justification. This isn't usually done in a long stack of layers that have a similar purpose, but mostly in unique layers like the initial linear layer after a conditioning vector, or certain layers in an attention architecture.
I imagined there must be a strategy to this, but most articles online seem to confirm my initial perception that bias is a good thing to have available for training in pretty much every layer.
Is there a specific optimization / theoretical reason to turn off biases in specific layers in a network? How can I choose when to do it when designing my own architecture?
","['optimization', 'deep-neural-networks', 'pytorch']","The most usual case of bias=False is in layers before/after Batch Normalization with no activators in between. The BatchNorm layer will re-center the data anyway, removing the bias and making it a useless trainable parameter. Quoting the original BatchNorm paper:Note that, since we normalize $Wu+b$, the bias $b$ can be ignored since its effect will be canceled by the subsequent mean subtractionSimilar thing happens in transformers' LinearNormalization and (as far as I understand how conditioning works) in the GANs' conditioning layer - the data gets re-centered, effectively cancelling the bias.In my experience, that's the most frequent reason to see bias=False, but one can imagine other reasons to remove the bias. As a rule of thumb, I'd say that you don't include bias if you want to ""transform zeros to zeros"" - things like learned rotations can be an example of such (rather exotic) application."
Speech diarization for a conversation detector: A good idea or not?,"
I am trying to write a program in which an ai can detect whether a conversation is occurring or not. The ai does not need to transcribe words or have any meaning about the conversation, simply if one is occurring. A conversation can then simply be defined as having more than one speaker.
Anyways, while searching for past research on the subject, I came across the field of speech diarization, which is where an AI is trained to distinguish the numbers of speakers in a conversation. This seems perfect to me, however, while implementing I came across a few troubles. First of all, it wasn't good. I used this tutorial: https://medium.com/saarthi-ai/who-spoke-when-build-your-own-speaker-diarization-module-from-scratch-e7d725ee279 to write a simple program for this task, but I found it wasn't good at finding if there was a single or two speakers. Also the times where it was distinguishing speakers were all off.
It occurred to me that perhaps speech diarization may not be the best approach for this problem, so I decided to ask here about if this is the best solution, or if there are better ones out there. If it is the best solution, I would love some insight into why this wasn't working for me. Is the tutorial simply not good enough? I used 45 second - 1 minute long clips of just myself speaking or other people speaking with me and it did not work well at all like I said.
","['natural-language-processing', 'audio-processing', 'speech-recognition', 'spectral-clustering']",
How does t-SNE preserves embedding orders?,"
According to the triplet loss Wikipedia page:

t-SNE (t-distributed Stochastic Neighbor Embedding) preserves embedding orders via probability distributions, whereas triplet loss works directly on embedded distances.

I don't understand how does t-SNE preserves embedding order from the description given by its Wikipedia page.
I am trying to understand this claim in order to translate the page into other languages. I don't have a very quick understanding of maths, don't be afraid to explain it as if I was a teenager.
","['dimensionality-reduction', 'embeddings', 'dimensionality']",
Why might the convolution be inappropriate when the task involves incorporating information from very distant locations in the input?,"
When I am reading about convolutional neural networks, I have encountered the following sentence from the textbook(page 341) that says about the limitation of the usage of the convolution in CNNs.

When a task involves incorporating information from very distant
locations in the input, then the prior imposed by convolution may be
inappropriate

My interpretations are

when the object is very small, then the convolution may not work
well.

when the object is very large, then the components of the object are far away from each other and hence the convolution may not work well.


Which of the interpretation is correct? If both are wrong, then what is the correct interpretation.
If possible, please provide an example to understand it.
","['deep-learning', 'convolutional-neural-networks', 'convolution']",
How does Mask R-CNN automatically output a different number of objects on the image?,"
Recently, I was reading Pytorch's official tutorial about Mask R-CNN.
When I run the code on colab, it turned out that it automatically outputs a different number of channels during prediction. If the image has 2 people on it, it would output a mask with the shape 2xHxW. If the image has 3 people on it, it would output the mask with the shape 3xHxW.
How does Mask R-CNN change the channels? Does it have a for loop inside it?
My guess is that it has region proposals and it outputs masks based on those regions, and then it thresholds them (it removes masks that have low probability prediction). Is this right?
","['object-detection', 'semantic-segmentation', 'mask-rcnn', 'non-max-suppression']",Object detection models usually generate multiple detections per object. Duplicates are removed in a post-processing step called Non-Maximum Suppression (NMS).The Pytorch code that performs this post-processing is called here in the RegionProposalNetwork class. The filtering loop you've mentioned performs the NMS and applies the score_thresh threshold (although it seems to be zero by default).
Is a genetic algorithm efficient for a snake game?,"
I am working on a DIY project in which I want to be able to train a neural network to play Snake.

Is a genetic algorithm an efficient way of training a network for this application?

For a GA, what should the inputs of the network be? (distance to walls and fruit or the squares in the proximity of the snake head as a vector)

What would the difference in efficiency be depending on the algorithm and what limitation does each one have? Are there any other alternatives I should consider?


","['neural-networks', 'q-learning', 'genetic-algorithms']",
Why do we use the softmax instead of no activation function?,"
Why do we use the softmax activation function on the last layer?
Suppose $i$ is the index that has the highest value (in the case when we don't use softmax at all). If we use softmax and take $i$th value, it would be the highest value because $e$ is an increasing function, so that's why I am asking this question. Taking argmax(vec) and argmax(softmax(vec)) would give us the same value.
","['neural-networks', 'activation-functions', 'softmax', 'multiclass-classification']","Short answer: Generally, you don't need to do softmax if you don't need probabilities. And using raw logits leads to more numerically stable code.Long answer: First of all, the inputs of the softmax layer are called logits.During evaluation, if you are only interested in the highest-probability class, then you can do argmax(vec) on the logits. If you want probability distribution over classes, then you'll need to exponentiate and normalize to 1 - that's what softmax does.During training, you'd need to have a loss function to optimize. Your training data contains true classes, so you have your target probability distribution $p_i$, which is 1 at your true class and 0 at all other classes. You train the network to produce a probability distribution $q_i$ as an output. It should be as close to the target distribution $p_i$ as possible. The ""distance"" measure between two probability distribution is called cross-entropy:$$ H = - \sum p_i \log q_i $$
As you can see, you only need logs of the output probabilities - so the logits will suffice to compute the loss. For example, the keras standard CategoricalCrossentropy loss can be configured to compute it from_logits and it mentions that:Using from_logits=True is more numerically stable."
"What are the major differences between multi-armed bandits and the other well-known algorithms (DQN, A3C, PPO, etc)?","
I have studied in the past different algorithms, i.e. DQN, DDQN, REINFORCE, A3C, PPO, TRPO, so on. I am doing an internship this summer where I have to use a multi-armed bandit (MAB). I am a bit confused between MAB and the other above algorithms.
What are the major differences between MAB and REINFORCE, for instance? What are the major differences between MAB and the other well-known algorithms (DQN, A3C, PPO, etc)?
EDIT
The @Kostya's answer is fine, but it would be interested for me to have a deeper answer for that question. I am still a bit confused.
Question: Do we use the Markov Reward formula $$G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$ the same way in a Multi-Armed bandit problem versus a DQN problem?
","['reinforcement-learning', 'comparison', 'deep-rl', 'multi-armed-bandits']","You should start with the general definition of Reinforcement Learning problem. And what Markov Decision Process is.DQN, A3C, PPO and REINFORCE are algorithms for solving reinforcement learning problems. These algorithms have their strengths and weaknesses depending on the details of the underlying problem.Multi-Armed Bandit is not even an algorithm - it is a subclass of reinforcement learning problems, where your environment (usually) doesn't have any state transitions and your actions are just a single choice from (usually) fixed and finite set of choices.Multi-Armed Bandit is used as an introductory problem to reinforcement learning, because it illustrates some basic concepts in the field: exploration-exploitation tradeoff, policy, target an estimate, learning rate and gradient optimization. All these concepts are basic vocabulary in RL. I recommend reading (and, very importantly, doing all the exercises) the Sutton and Barto book chapter two to get familiarized with it.Edit: since the answer got popular, I'll address the comments and the question edit.Being a special simplified subset of Markov Decision Processes, Multi Armed Bandit problems allow deeper theoretical understanding. For example, (as per @NeilSlater comment) the optimal policy would be to always go for the best arm. So it makes sense to introduce ""regret"" $\rho$ - the difference between a potential optimal reward and the actual collected reward by agent following your strategy:$$\rho(T) = \mathbb{E}\left[T\mu^* -\sum_{t=1}^T\mu(a_t)\right]$$One can then study asymptotic behavior of this regret as a function of $T$ and devise strategies with different asymptotic properties. As you can see, the reward here is not discounted ($\gamma=1$) - we usually can study the behavior of it as a function of $T$ without this regularization.Although, there is one famous result that uses discounted rewards - the Gittins index policy (note, though, that they use $\beta$ instead of $\gamma$ to denote the factor)."
Does distribution of data augmentation parameters matter?,"
Idea
Let's say we have simple pictures dataset containing 40x40 images of digits. We have only one image of each digit. We want to use that as training set, but we need more data, so we use data augmentation. We use only simple operations like translating and rotating and generate 1000 more images of each digit.
Question
Natural way to do data augmentation will be randomly generating parameters like translate_x, translate_y and rotate and applying them into our base image.
Does distribution of these parameters matter? From one hand we would like to have net that is able to recognize digit placed in the side of the image and rotated as well as digit placed in center and not rotated at all but maybe we don't need such accuracy with those borderline images? Maybe we know that our prediction data will be close to the centered ones so we want high accuracy in that cases and the more digit is translated and rotated the lower our's net accuracy might be.
What I mean is can we augmented data with parameters with e.g. gaussian distribution to make our net more sensitive for cases closest to ideal and less sensitive to this borderline cases. Advantage of that would be less training data that we don't need and more control on characteristic of our neural net.
Disclaimer
This digits case is just simple example to show You what I mean.
","['machine-learning', 'computational-learning-theory', 'statistical-ai', 'data-augmentation']",
How to prove that a regularisation method simplified a neural network?,"
There are a few ways to regularise a neural network, for example dropout or the L1. Now, both these methods, and possibly most other regularisation methods, tend to remove from, or simplify the neural network. The Dropout deactivates nodes and the L1 shrinks the weights of the model, and so on.
The main argument in favour of regularising a neural network is that by simplifying the model you are forcing it to learn more general functions and thus making the neural network more robust to overfitting or noisy input.
Once you have a model trained with, and without, regularisation, it is possible to compare their performance by calculating the error metrics on their outputs. This will prove whether the regularised model performs better than the standard model or not.
However, considering that the regularised model achieved better performance on its error metrics, how to prove that the weights of the regularised model have less variance (simpler) than the standard neural network?
","['neural-networks', 'proofs', 'regularization', 'dropout', 'l1-regularization']",
"Reward firstly increase, but after more episodes, start decrease, and weights diverges","
I'm making a simple deep Q learning algorithm, with cartpole-v1 env.
Like you can see in chart, after many episodes the reward decrease, some possible reasons?
The exploration vs axplotation algorithm is epsilon-decay, I used a target network (used every mini-batch gradient descent update, in calculating actual Q values, and next Q values, is it right?)
The neural network is made from scratch
the complete code is here:
https://github.com/LorenzoTinfena/deep-q-learning-itt-final-project
# %%
from core.dqn_agent import DQNAgent
from cartpole.cartpole_neural_network import CartPoleNeuralNetwork
from cartpole.cartpole_wrapper import CartPoleWrapper
import gym
import numpy as np
import torch
from tqdm import tqdm
import glob
import os
from IPython.display import Video
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from itertools import cycle
import sys
import shutil
from pathlib import Path
import shutil
import pyvirtualdisplay
_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))
_ = _display.start()



# %% [markdown]
# Initialize deep Q-learning agent, neural network, and parameters
# %%
np.random.seed(20)
agent = DQNAgent(env=CartPoleWrapper(gym.make(""CartPole-v1"")),
                nn=CartPoleNeuralNetwork(), replay_memory_max_size=5000, batch_size=30)

DISCOUNT_FACTOR = 0.995
LEARNING_RATE = 0.0001

n_episodes = []
total_rewards = []
number_steps = []
total_episodes = 0


# %% [markdown]
# Training
# %%
while total_episodes <= 10000:
    total_reward, steps = agent.start_episode_and_evaluate(DISCOUNT_FACTOR, LEARNING_RATE, epsilon=0, min_epsilon=0, render=False, optimize=False)
    print(f'\ntotal_episodes_training: {total_episodes}\tsteps: {steps}\ttotal_reward: {total_reward}', flush = True)
    n_episodes.append(total_episodes)
    total_rewards.append(total_reward)
    number_steps.append(steps)

    for i in tqdm(range(50), 'learning...'):
        agent.start_episode_and_evaluate(DISCOUNT_FACTOR, LEARNING_RATE, epsilon=1, epsilon_decay=0.99, min_epsilon=0.01, render=False, optimize=True)
    total_episodes += i+1



","['machine-learning', 'reinforcement-learning', 'deep-learning', 'q-learning', 'dqn']",
Why are weights not initialized with mean=1?,"
I wonder why weights are initialized with zero-mean. It is one of the reasons, why deep architectures cannot be trained without skip connections. Without the skip connections, the zero initialization becomes problematic, because the identity function cannot be learned in earlier layers (this is a simplified explanation I know). But why can we not initialize weights around one? This would enhance the intrinsic learning of the identity function. Of course, the skip connections also allow a better backpropagation of the gradients, but couldn't this be helpful anyways? Can anyone tell me, why this is not done?
","['neural-networks', 'deep-learning', 'weights', 'weights-initialization', 'residual-networks']",
"Is $\min(h_1(s),\ h_2(s))$ consistent?","
If $h_1(s)$ is a consistent heuristic and $h_2(s)$ is a admissible heuristic, is $\min(h_1(s),\ h_2(s))$ consistent?
","['search', 'proofs', 'admissible-heuristic', 'consistent-heuristic', 'heuristic-functions']",
How do transformers understand data and answer custom questions?,"
I recently heard of GPT-3 and I don't understand how the attention models and transformers encoders and decoders work. I heard that GPT-3 can make a website from a description and write perfectly factual essays. How can it understand our world using algorithms and then recreate human-like content? How can it learn to understand a description and program in HTML?
","['neural-networks', 'transformer', 'human-like', 'gpt-3']","Two years after the original question was made I feel it is time to provide an updated answer:How can it understand our world using algorithms and then recreate
human-like content?GPT-3, GPT-4 (transformers, Chat-GPTs and the likes) do not understand our world. They do provide answers based on any info available on the ""training datasets"" and on any additional ""training dataset"" used to ""fine-tune"" the model. Those answers recreate human-like content even if the models/algorithms do not understand nor have a representation of the world."
Normalization of possibly not fully representative data,"
I am trying to train a classification RNN model on a sequence of table medical data, but I stuck with the normalization problem. I realized that I cannot simply use MinMaxScaler, because of 3 problems:

outliers, but I could fight them or use RobustScaler instead.
I am not sure that some features in my dataset include all possible ranges. Like I have max(feature_A) == 10, but with the data update, it could become 20. And if I'll preprocess data the same way I will get bad prediction results.
Some features do not have a limit at all and will only increase with time, like how many years patients were treated, for example. I could suppose that this value is !>100years, for example, but if my mean value is 10 years, it will squeeze feature values a lot.

My dataset is pretty large, like millions of observations, thus there is a pretty good chance that it is representative, though. But I am concerned with the small-time range, like all those observations are for the 2 years only, thus, some feature values (like how many years patients were treated) could still grow their bounds.
How should I handle this?
My concerns example:
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

#### like, initial state
df1 = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 40, 60, 80, 100]})
"""""" output:
   A    B
0  1   10
1  2   40
2  3   60
3  4   80
4  5  100
""""""

scaler.fit_transform(df1)
"""""" output:
array([[0.        , 0.        ],
       [0.25      , 0.33333333],
       [0.5       , 0.55555556],
       [0.75      , 0.77777778],
       [1.        , 1.        ]])
""""""

#### new data arrived, while preprocessing is the same
df2 = pd.DataFrame({'A': [1, 2, 3, 4, 5, 10, 10], 'B': [10, 40, 60, 80, 100, 120, 140]})
"""""" output:
    A    B
0   1   10
1   2   40
2   3   60
3   4   80
4   5  100
5  10  120
6  10  140
""""""

# now 5 in ""A"" scaled to 0.4 instead of 1, same in ""B""
scaler.fit_transform(df2)
"""""" output:
array([[0.        , 0.        ],
       [0.11111111, 0.23076923],
       [0.22222222, 0.38461538],
       [0.33333333, 0.53846154],
       [0.44444444, 0.69230769],
       [1.        , 0.84615385],
       [1.        , 1.        ]])
""""""

PS: I've duplicated this question in different communities (question in ai got most of views):

https://stats.stackexchange.com/questions/523097/normalization-of-possibly-not-fully-representative-data
https://datascience.stackexchange.com/questions/94095/normalization-of-possibly-not-fully-representative-data

","['data-preprocessing', 'sequence-modeling', 'normalisation']",
What is dynamic data sampling in federated learning?,"
I am trying to learn about Federated Learning (FL), but I have a question.
What is dynamic data sampling in FL?

Cai, Lingshuang, et al. ""Dynamic Sample Selection for Federated Learning with Heterogeneous Data in Fog Computing."" ICC 2020-2020 IEEE International Conference on Communications (ICC). IEEE, 2020.

","['definitions', 'federated-learning']",
Is binary classification using CNN possible if the training data only consists of one class?,"
Is binary classification using CNN possible if the training data only consists of one class?
I am working on landslide risk assessment using Convolutional Neural Networks and I want to train a network that can recognize high-risk areas using multi-spectral imagery. The bands will contain numeric and categorical data that I have found to be related to my field of work.
The problem is that I only have historical data indicating where a landslide has happened before and defining zones as low-risk is not reliable in this field (since we are not yet sure how these variables affect the risk or susceptibility, and I don't want to bias my categorization) and my training data will be made up of only one class.
Can this be done? Is training a network from scratch using only one class of training data possible?
If so, after building this network, can I use it to classify any zone and get any meaningful data from its output for risk assessment (for example, output value ""1"" being ""similar to past landslides"" and ""0"" being ""not similar at all"")?
","['machine-learning', 'convolutional-neural-networks', 'binary-classification', 'training-datasets']",
What is the correct formula for updating the weights in a 1-single hidden layer neural network?,"
I'm creating a neural network with 3 layers and no bias.
On internet I saw that the expression for the derivative of the weights between the hidden layer and the output layer was:
$$\Delta W_{j,k} = (o_k - t_k) \cdot f'\left[\sum_j (W_{j,k} \ \cdot o_j)\right] \cdot o_j,$$
where $t$ is the target output, $o$ is the activated output layer and $f'$ the derivative of the activation function.
But the shape of these weights is $\text{output nodes}\times\text{hidden nodes}$, and $\text{hidden nodes}$ can be bigger than $\text{output nodes}$, so the formula is wrong because of I'm taking $o_k$ and $o$ has length $\text{output nodes}$.

In simple terms, what is the right formula for updating these weights?

Also, what is the right formula for updating the weights between the input layer and the hidden layer?


","['neural-networks', 'machine-learning', 'backpropagation', 'calculus']",
"Why does Adam optimizer work slower than Adagrad, Adadelta, and SGD for Neural Collaborative Filtering (NCF)?","
I've been working on Neural Collaborative Filtering (NCF) recently to build a recommender system using Tensorflow Recommenders. Doing some hyperparameter tuning with different optimizers available in the module tf.keras.optimizers, I found out that Adam and its other variants, such as Adamax and Nadam, work much slower than seemingly less advanced optimizers, like Adagrad, Adadelta, and SGD. With Adam and its variants, training each epoch takes about 30x longer.
It came out as a surprise to me, knowing one of the most cherished properties of Adam optimizer is its convergence speed, especially compared to SGD. What could be the reason for such a significant difference in computation speed?
","['neural-networks', 'deep-learning', 'training', 'hyperparameter-optimization', 'optimizers']",
What is the difference between applying shallow-learning methods repeatedly and deep learning?,"
In the book Deep Learning with Python, François Chollet writes (section 1.2.6, page 18)

In practice, there are fast-diminishing returns to successive applications of shallow-learning methods, because the optimal first representation layer in a three-layer model isn't the optimal first layer in a one-layer or two-layer model. What is transformative
about deep learning is that it allows a model to learn all layers of representation jointly, at the same time, rather than in succession (greedily, as it's called).

By shallow learning, we mean traditional machine learning models that aren't deep learning, such as support vector machines.
I understood the above as below.

Using a model with three-layer shallow-learning methods has the same output (predicted) value as using one-layer shallow learning method. The effect of using multiple layers of shallow learning methods is to 'increase running time or repetition'.

Did I understand properly?
","['machine-learning', 'deep-learning', 'comparison']",Quite surprising to find someone reading this same book. I read this part a week ago and the explanation is quite clear in the book :Hope I made it clearer
Is there a clear distinction between Artificial Intelligence and running a sequential program?,"
Artificial Intelligence (AI) is often defined as a machine that is intelligent, or one that can think rationally.
From a high-level perspective, things like self-driving car or Alpha-Go can easily be classified as an AI system, while things like a washing machine that follows a strict sequential program is not considered as AI.
However, what confused me is that when looking at the definition from a low-level perspective, there does not seem to be a clear distinction between AI and non-AI.
For example, consider an Artificial Neural Network from Deep Learning. Fundamentally, it is just a complex non-linear function. Why is this considered AI while a washing machine is not considered that?
Is it because of the learning involved? But then path-finding will not considered as AI too.
Is it because of the calculations? But then traditional calculators will be considered as AI.
Is there even a clear distinction between AI and a sequential program? Or is it just a vague term that is only valid when viewed from a high-level perspective?
","['terminology', 'definitions']","No. As of now, there is no clear distinction between AI and a sequential program. The biggest reason is that there is no officially agreed definition of AI. Since AI researchers themselves hold different opinions of AI, the field itself is constantly redefined regularly, there is no clear-cut distinction between AI and non-AI."
Representing variable-length sequences,"
I want to train a model over variable-length sequential data (e.g. the temperature at different times of day) where the output depends on what the temperature is at a time T.
Ideally, I want to represent the input using a variable-length compacted format of [temperature, duration]. Alternatively, I can divide a matrix into time slices where each cell contains the current temperature.
I prefer the compacted format as it is more space-efficient and allows me to represent arbitrary-length durations, but I am afraid that a Transformer architecture won't be able to figure out what the temperature is at a time T using the compact format.
Is it safe to compact sequential inputs?
","['transformer', 'sequence-modeling', 'matrices']",
Why don't integrated gradients explain samples correctly?,"
I have a linear tabular dataset made of floats. The dataset follows a simple rule like:
if features A and B are in a certain range then target class is 1, otherwise target class is 0.

Since I want to get some interpretability from my ANN model, I opted for using the integrated gradients method implemented by alibi.
Unfortunately, most of individual samples don't show A and B as the leading features as expected. Even more weird is the fact that, when I average the attributions of all the individual samples, A and B get the highest score. In other words, local explanations fail but, on average, the global explanation is correct.
Can anyone help me out to understand why this happens? Isn't integrated gradients method suitable for tabular datasets?
By the way, my baseline is based on a uniform distribution of random floats ranging from 0 to the maximum of each column.
","['deep-learning', 'math', 'explainable-ai', 'gradient']",
What is the name of algorithms that train by competing each other?,"
In some learning algorithms, we don't directly train models by datasets with labels to predict, but rather we create 2 competing models and let them fight/compete against each other.  As the many millions of epochs pass, the models fight each other, and every time each model improves itself (further optimise its weights) to win.  After many epochs of the models smashing each other, eventually they become really strong super-hero models that can totally blow any human out of the water.  This approach seems to be often used with machine learning models that are tasked to play multiplayer games.  Instead of letting them play with the slow humans, they fight with each other to death for many many epochs to become way stronger than any human can naturally be.
What is the name of such kind of machine learning approach?
","['machine-learning', 'deep-learning']",
Machine Learning in relation to personality and behaviors predictions,"
I am tasked with making a machine learning model that predicts personality traits and behaviours of children based on simple and interactive quizzes.
Currently I am lost and have no idea where to start!
I am looking for guidance and where can start my research and the actual coding part and is NLP a good place to start from.
","['machine-learning', 'reference-request', 'prediction', 'algorithm-request']",
Training seq2seq translation model with one source and multiple target,"
So basically I'm training a sequence to sequence model that translates English sentences to Arabic sentences. I'm using the data provided by Anki @ manythings. I realized that some of the sentences in English (source) have multiple sentences in Arabic (target), for example:
This is one case, where the Arabic harakat are not shown but the idea is that the same word has different translations (yes in arabic the first, fourth and fifth are not the same translations).

A better example is the following one:

I'm not sure how to deal with these cases, should I reduce the data and keep one translation, or should I have for each source key a list of target values. Any advice or ""tips & tricks"" in preparing the data before training translation models?
","['neural-networks', 'natural-language-processing', 'recurrent-neural-networks', 'machine-translation', 'seq2seq']",
What type of ANN architecture to choose?,"
I have $N$ number of teachers  each of which has an input feature vector ($25$ dimensional) consisting of positive numerical values for different quality of aspects (for example: lecturing ability, knowledge capacity, communication skills, etc.). I want to design an ANN to output a single quality index based on these quality features.
What type of ANN architecture is appropriate for this problem?
","['neural-networks', 'deep-learning', 'architecture', 'model-request']","It sounds like you have structured/tabular data. So, a fully-connected feedforward network should do the job."
Origins of the name of convolutional neural networks,"
Convolutional neural networks (CNNs) contain convolutional layers. In modern deep learning libraries such as Tensorflow and PyTorch, convolutional layers are implemented by using the cross-correlation operator instead of the convolution operator. The difference is that in convolution, the kernel is flipped before applying it to the input.
For example, in the book ""Deep Learning"", it is explained as follows.

Many machine learning libraries implement cross-correlation but call
it convolution. --- In the
context of machine learning, the learning algorithm will learn the
appropriate values of the kernel in the appropriate place, so an
algorithm based on convolution with kernel flipping will learn a
kernel that is flipped relative to the kernel learned by an algorithm
without the flipping. It is also rare for convolution to be used alone
in machine learning; instead convolution is used simultaneously with
other functions, and the combination of these functions does not
commute regardless of whether the convolution operation flips its
kernel or not.

This makes perfect sense and convincingly argues why implementing the flipping of the kernel would be unnecessary.
But how come CNNs are not commonly called ""cross-correlational neural networks"" instead of ""convolutional neural networks""? To the best of my knowledge, the first concrete implementations of CNNs predate any of the above-mentioned libraries. Did these early implementations of CNNs indeed use the convolution operator, leading to the name? Or is there another reason?
","['deep-learning', 'convolutional-neural-networks', 'terminology', 'history']",
Comparison between TD(0) and MC ( or GAE )?,"
I'm getting started with DRL and have trouble distinguishing TD(0), MC, and GAE; and which scenarios one's better than others. Here is what I understand so far:

TD(0): increment learning, can learn after each step instead of waiting for the episode to end. Update bases on one reward, then low variance. However, high bias.

MC: Learn after each episode, the calculation of return is correct. However, its drawback is high variance. And you have to make decisions in the whole episode without an update of parameters.

GAE: combine returns in all steps, get a better trade-off between variance and bias. However, still has to wait until the end of the episode for an update.


I have some questions as follows:

Is variance and bias about the return of each episode? What are their effects on the outcomes (convergence speed of training process, performance of the model)?

Is increment learning important? The ability to correct behaviors after each step may improve convergence speed. However, it can lead to unstable learning ( if I understand correctly, this is why the Target model in Double DQL only updates its parameters for each k mini-batches ). Which scenarios should I use TD(0) or GAE?

Concretely, in my case, I run parallelly a batch with 12 environments, each with 1000 steps. If I use GAE, I make 12000 decisions for each update. All losses of the model are summed up and calculate gradients, after that, I clip gradients to 2.0. Is that too expensive to learn the correct direction? Should I consider using TD(0) here?


","['reinforcement-learning', 'training', 'monte-carlo-methods', 'temporal-difference-methods']",
How to properly use Flatten layer?,"
Context
I'm trying to create net that will be able to recognize printed-like digits. Something like MNIST, but only for standard printing font.
Images are of the size 40x40 and I'd like to put them into feedforward net since ConvNet seems too powerful for this task.
Question
How should I use Flatten layer in this task?
Code
My current net:
X, test_X, y, test_y = train_test_split(X, y, test_size=0.25, random_state=42)

self.model = Sequential()
self.model.add(Flatten())
self.model.add(Dense(64, activation='relu', input_shape=X.shape[1:]))
self.model.add(Dense(no_classes, activation='softmax'))
self.model.compile(loss=""categorical_crossentropy"",
                   optimizer=""rmsprop"",
                   metrics=['accuracy'])

self.history = self.model.fit(X, y, batch_size=256, epochs=20, validation_data=(test_X, test_y))
print(self.model.summary())

Example images
  
Current results
 
","['machine-learning', 'training', 'image-recognition', 'architecture']","The Flatten layer is used for collapsing an ND tensor into a 1D tensor. In your case, the inputs appear to be $28\times28$ images, so Flatten will convert that into a tensor with shape $1\times768$. Note that no information is lost. Flatten layers are usually used where you have a convolutional layer with dimensions $N\times M \times C$ (where $N$,$M$ are the feature map sizes and $C$ is the number of channels) and want to fully connect with a Dense layer or another layer that only accepts 1D inputs. Flatten can also be used when the network is meant to output a feature vector from a final convolutional layer for image classification purposes using a different technique."
How can abstract graphs be recognized by neural nets?,"
Recognition of optical patterns (as pixel maps) by neural networks is standard. But optical patterns may be only slightly distorted or noisy, and may not be arbitrarily scrambled – e.g. by permutations of rows and columns of the pixel map – without losing the possibility to recognize them. This in turn is the normal case for abstract graphs in their standard representation as adjacency matrices: only under some permutations of nodes a possible pattern is visible. In general, for almost all random graphs under no permutation a pattern is visible, but for all graphs under almost all permutations a pattern is invisible.
How can this be handled in the context of either unsupervised or supervised learning? Assume you have a huge set of graphs with 100 nodes and 1,000 edges, given as 100$\times$100 adjacency matrices under arbitrary permutations, but with only two isomorphism classes. How could a neural network find this out and learn from the samples?
Is this possibly common knowledge: that it can not? Or are there any tricks?
(One trick might be to draw the graph force-directed and hope that it settles in a recognizable configuration. But this to be detectable would require a much larger pixel map than 100$\times$100. But why not?)
","['neural-networks', 'machine-learning', 'pattern-recognition', 'geometric-deep-learning', 'graph-theory']",
What would happen to an agent trained using Markov Decision Process if the goal node changes?,"
I was reading up a paper that did routing based on an MDP, and I was wondering because, in routing, there is a sender node and a receiver node, so if the receiver node changes (sending a message to someone else), would we have to train the MDP algorithm all over again?
This also got me thinking about what would happen even if one node in the process of transmission changes. Does using an MDP for training the agent mean that the obstacle and goals should never change?
","['reinforcement-learning', 'deep-rl', 'markov-decision-process']","It is possible, at design time for a reinforcement learning problem, to allow for changes within an environment. You can make any element into a variable property of the state, that the agent can realistically be told at the start or sense from the environment.If you do add new variable to model the possibility of change:It allows the agent to learn to solve a more general problem where the chosen property can vary.It increases the size of the state space.It requires training to include variations of the new variable.Usually this also increases the time taken to train.It is not always possible to use a state variable for the task - perhaps a goal state is effectively hidden from the agent and the purpose of training is for it to be discovered. In which case, you will require at least some re-training. It may be faster to start with the existing trained agent if the difference is not large.If you cannot simply extend the state representation, and the environment changes in a small enough way, then it may also be possible to use an agent which continuously explores which will re-train itself over time in repsonse to changes in the environment. The DynaQ+ algorithm is an example of a method which is designed to explore and find changes in the agent's environment to allow for this kind of online retraining when things change."
How to deal with a moving target in the Lunar Lander environment with DDPG?,"
I have noticed that DDPG does rather well at solving environments with a static target.
For example, the default of Lunar Lander, the flags do not change position.  So the DDPG model learns how to get to the center of the screen and land fairly quickly.
As soon as I start moving the landing position around randomly and adding the landing position as an input to the model, the model has an extremely hard time putting this connection together.
A few questions/points about adding this complexity to the environment:

Would more nodes per layer or more layers help in figuring out this connection?  I have tested this but seems the bigger I go, the harder it is to learn anything.
Is it a common RL AI issue that it has a hard time connecting data?
I realize that I could change the environment to always have a static target and instead change the position of the lunar lander ship, which in effect accomplishes the same thing, but want to know if we could solve it with moving target
Is there any good documentation on Actor/Critic analyzing models? I have some results where my critic target is falling out but my critic loss is going down nicely. At the same time my actor target is going up and up and eventually plateau.  It is hard to really understand what is happening and would be great to understand actor loss vs critic loss vs critic target.

Essentially, I added a random int (left side of flags), added 1 to get x position middle of landing and add 1 more to get distance of flags to be 3 out of 11 chunks.
rand_chunk = random.randint(0, CHUNKS-3)
self.x_pos_middle_landing = chunk_x[rand_chunk + 1]
self.helipad_x1 = chunk_x[rand_chunk]
self.helipad_x2 = chunk_x[rand_chunk + 2]
height[rand_chunk] = self.helipad_y
height[rand_chunk + 1] = self.helipad_y
height[rand_chunk + 2] = self.helipad_y

Old State:
state = [
        (pos.x - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2),
        (pos.y - (self.helipad_y+LEG_DOWN/SCALE)) / (VIEWPORT_H/SCALE/2),
        vel.x*(VIEWPORT_W/SCALE/2)/FPS,
        vel.y*(VIEWPORT_H/SCALE/2)/FPS,
        self.lander.angle,
        20.0*self.lander.angularVelocity/FPS,
        1.0 if self.legs[0].ground_contact else 0.0,
        1.0 if self.legs[1].ground_contact else 0.0
        ]

Added to State, same as state[0] but using middle of 3 for landing
(self.x_pos_middle_landing - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2)

And update the obs space from 8 spaces to 9
self.observation_space = spaces.Box(-np.inf, np.inf, shape=(9,), dtype=np.float32)

Rewards need to be updated, Old Rewards:
shaping = \
        - 100*np.sqrt(state[0]*state[0] + state[1]*state[1]) \
        - 100*np.sqrt(state[2]*state[2] + state[3]*state[3]) \
        - 100*abs(state[4]) + 10*state[6] + 10*state[7]  # And ten points for legs contact, the idea is if you

New Rewards:
shaping = \
        - 100*np.sqrt((state[0]-state[8])*(state[0]-state[8]) + state[1]*state[1]) \
        - 100*np.sqrt(state[2]*state[2] + state[3]*state[3]) \
        - 100*abs(state[4]) + 10*state[6] + 10*state[7]  # And ten points for legs contact, the idea is if you

DDPG Model
    import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import gym
from tensorflow.keras.models import load_model
import os
import envs
import time
import scipy.stats as stats
# from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy
from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv, VecCheckNan
from stable_baselines.common import set_global_seeds, make_vec_env
from stable_baselines.ddpg import DDPG
from stable_baselines.ddpg.policies import MlpPolicy
# from stable_baselines.sac.policies import MlpPolicy
from stable_baselines import PPO2, SAC
from stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec

if __name__ == '__main__':
    num_cpu = 1  # Number of processes to use
    env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])
    env = VecCheckNan(env, raise_exception=True, check_inf=True)
    n_actions = env.action_space.shape[-1]

    #### DDPG
    policy_kwargs = dict(act_fun=tf.nn.sigmoid, layers=[512, 512, 512], layer_norm=False)
    param_noise = None
    # action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))
    action_noise = NormalActionNoise(0, 0.1)
    model = DDPG

    # # Train Model
    model.learn(total_timesteps=int(3e5))
    model.save('./models/lunar_lander')

Full Code:
    """"""
Rocket trajectory optimization is a classic topic in Optimal Control.

According to Pontryagin's maximum principle it's optimal to fire engine full throttle or
turn it off. That's the reason this environment is OK to have discreet actions (engine on or off).

The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector.
Reward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points.
If the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or
comes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points.
Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame.
Solved is 200 points.

Landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land
on its first attempt. Please see the source code for details.

To see a heuristic landing, run:

python gym/envs/box2d/lunar_lander.py

To play yourself, run:

python examples/agents/keyboard_agent.py LunarLander-v2

Created by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.
""""""


import sys, math
import numpy as np
import random

import Box2D
from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)

import gym
from gym import spaces
from gym.utils import seeding, EzPickle

FPS = 50
SCALE = 30.0   # affects how fast-paced the game is, forces should be adjusted as well

MAIN_ENGINE_POWER = 13.0
SIDE_ENGINE_POWER = 0.6

INITIAL_RANDOM = 1000.0   # Set 1500 to make game harder

LANDER_POLY =[
    (-14, +17), (-17, 0), (-17 ,-10),
    (+17, -10), (+17, 0), (+14, +17)
    ]
LEG_AWAY = 20
LEG_DOWN = 18
LEG_W, LEG_H = 2, 8
LEG_SPRING_TORQUE = 40

SIDE_ENGINE_HEIGHT = 14.0
SIDE_ENGINE_AWAY = 12.0

VIEWPORT_W = 600
VIEWPORT_H = 400


class ContactDetector(contactListener):
    def __init__(self, env):
        contactListener.__init__(self)
        self.env = env

    def BeginContact(self, contact):
        if self.env.lander == contact.fixtureA.body or self.env.lander == contact.fixtureB.body:
            self.env.game_over = True
        for i in range(2):
            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:
                self.env.legs[i].ground_contact = True

    def EndContact(self, contact):
        for i in range(2):
            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:
                self.env.legs[i].ground_contact = False


class LunarLander(gym.Env, EzPickle):
    metadata = {
        'render.modes': ['human', 'rgb_array'],
        'video.frames_per_second' : FPS
    }

    continuous = False

    def __init__(self):
        EzPickle.__init__(self)
        self.seed()
        self.viewer = None

        self.world = Box2D.b2World()
        self.moon = None
        self.lander = None
        self.particles = []

        self.prev_reward = None

        # useful range is -1 .. +1, but spikes can be higher
        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(9,), dtype=np.float32)

        if self.continuous:
            # Action is two floats [main engine, left-right engines].
            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.
            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off
            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)
        else:
            # Nop, fire left engine, main engine, right engine
            self.action_space = spaces.Discrete(4)

        self.reset()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def _destroy(self):
        if not self.moon: return
        self.world.contactListener = None
        self._clean_particles(True)
        self.world.DestroyBody(self.moon)
        self.moon = None
        self.world.DestroyBody(self.lander)
        self.lander = None
        self.world.DestroyBody(self.legs[0])
        self.world.DestroyBody(self.legs[1])

    def reset(self):
        self._destroy()
        self.world.contactListener_keepref = ContactDetector(self)
        self.world.contactListener = self.world.contactListener_keepref
        self.game_over = False
        self.prev_shaping = None

        W = VIEWPORT_W/SCALE
        H = VIEWPORT_H/SCALE

        # terrain
        CHUNKS = 11
        height = self.np_random.uniform(0, H/2, size=(CHUNKS+1,))
        chunk_x = [W/(CHUNKS-1)*i for i in range(CHUNKS)]
        rand_chunk = random.randint(0, CHUNKS-3)
        self.x_pos_middle_landing = chunk_x[rand_chunk + 1]
        self.helipad_x1 = chunk_x[rand_chunk]
        self.helipad_x2 = chunk_x[rand_chunk + 2]
        self.helipad_y = H/4
        height[rand_chunk] = self.helipad_y
        height[rand_chunk + 1] = self.helipad_y
        height[rand_chunk + 2] = self.helipad_y

        self.moon = self.world.CreateStaticBody(shapes=edgeShape(vertices=[(0, 0), (W, 0)]))
        self.sky_polys = []
        for i in range(CHUNKS-1):
            p1 = (chunk_x[i], height[i])
            p2 = (chunk_x[i+1], height[i+1])
            self.moon.CreateEdgeFixture(
                vertices=[p1,p2],
                density=0,
                friction=0.1)
            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])

        self.moon.color1 = (0.0, 0.0, 0.0)
        self.moon.color2 = (0.0, 0.0, 0.0)

        initial_y = VIEWPORT_H/SCALE
        self.lander = self.world.CreateDynamicBody(
            position=(VIEWPORT_W/SCALE/2, initial_y),
            angle=0.0,
            fixtures = fixtureDef(
                shape=polygonShape(vertices=[(x/SCALE, y/SCALE) for x, y in LANDER_POLY]),
                density=5.0,
                friction=0.1,
                categoryBits=0x0010,
                maskBits=0x001,   # collide only with ground
                restitution=0.0)  # 0.99 bouncy
                )
        self.lander.color1 = (0.5, 0.4, 0.9)
        self.lander.color2 = (0.3, 0.3, 0.5)
        self.lander.ApplyForceToCenter( (
            self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),
            self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM)
            ), True)

        self.legs = []
        for i in [-1, +1]:
            leg = self.world.CreateDynamicBody(
                position=(VIEWPORT_W/SCALE/2 - i*LEG_AWAY/SCALE, initial_y),
                angle=(i * 0.05),
                fixtures=fixtureDef(
                    shape=polygonShape(box=(LEG_W/SCALE, LEG_H/SCALE)),
                    density=1.0,
                    restitution=0.0,
                    categoryBits=0x0020,
                    maskBits=0x001)
                )
            leg.ground_contact = False
            leg.color1 = (0.5, 0.4, 0.9)
            leg.color2 = (0.3, 0.3, 0.5)
            rjd = revoluteJointDef(
                bodyA=self.lander,
                bodyB=leg,
                localAnchorA=(0, 0),
                localAnchorB=(i * LEG_AWAY/SCALE, LEG_DOWN/SCALE),
                enableMotor=True,
                enableLimit=True,
                maxMotorTorque=LEG_SPRING_TORQUE,
                motorSpeed=+0.3 * i  # low enough not to jump back into the sky
                )
            if i == -1:
                rjd.lowerAngle = +0.9 - 0.5  # The most esoteric numbers here, angled legs have freedom to travel within
                rjd.upperAngle = +0.9
            else:
                rjd.lowerAngle = -0.9
                rjd.upperAngle = -0.9 + 0.5
            leg.joint = self.world.CreateJoint(rjd)
            self.legs.append(leg)

        self.drawlist = [self.lander] + self.legs

        return self.step(np.array([0, 0]) if self.continuous else 0)[0]

    def _create_particle(self, mass, x, y, ttl):
        p = self.world.CreateDynamicBody(
            position = (x, y),
            angle=0.0,
            fixtures = fixtureDef(
                shape=circleShape(radius=2/SCALE, pos=(0, 0)),
                density=mass,
                friction=0.1,
                categoryBits=0x0100,
                maskBits=0x001,  # collide only with ground
                restitution=0.3)
                )
        p.ttl = ttl
        self.particles.append(p)
        self._clean_particles(False)
        return p

    def _clean_particles(self, all):
        while self.particles and (all or self.particles[0].ttl < 0):
            self.world.DestroyBody(self.particles.pop(0))

    def step(self, action):
        if self.continuous:
            action = np.clip(action, -1, +1).astype(np.float32)
        else:
            assert self.action_space.contains(action), ""%r (%s) invalid "" % (action, type(action))

        # Engines
        tip  = (math.sin(self.lander.angle), math.cos(self.lander.angle))
        side = (-tip[1], tip[0])
        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]

        m_power = 0.0
        if (self.continuous and action[0] > 0.0) or (not self.continuous and action == 2):
            # Main engine
            if self.continuous:
                m_power = (np.clip(action[0], 0.0,1.0) + 1.0)*0.5   # 0.5..1.0
                assert m_power >= 0.5 and m_power <= 1.0
            else:
                m_power = 1.0
            ox = (tip[0] * (4/SCALE + 2 * dispersion[0]) +
                side[0] * dispersion[1])  # 4 is move a bit downwards, +-2 for randomness
            oy = -tip[1] * (4/SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]
            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)
            p = self._create_particle(3.5,  # 3.5 is here to make particle speed adequate
                                    impulse_pos[0],
                                    impulse_pos[1],
                                    m_power)  # particles are just a decoration
            p.ApplyLinearImpulse((ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),
                                impulse_pos,
                                True)
            self.lander.ApplyLinearImpulse((-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),
                                        impulse_pos,
                                        True)

        s_power = 0.0
        if (self.continuous and np.abs(action[1]) > 0.5) or (not self.continuous and action in [1, 3]):
            # Orientation engines
            if self.continuous:
                direction = np.sign(action[1])
                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)
                assert s_power >= 0.5 and s_power <= 1.0
            else:
                direction = action-2
                s_power = 1.0
            ox = tip[0] * dispersion[0] + side[0] * (3 * dispersion[1] + direction * SIDE_ENGINE_AWAY/SCALE)
            oy = -tip[1] * dispersion[0] - side[1] * (3 * dispersion[1] + direction * SIDE_ENGINE_AWAY/SCALE)
            impulse_pos = (self.lander.position[0] + ox - tip[0] * 17/SCALE,
                        self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT/SCALE)
            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)
            p.ApplyLinearImpulse((ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),
                                impulse_pos
                                , True)
            self.lander.ApplyLinearImpulse((-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),
                                        impulse_pos,
                                        True)

        self.world.Step(1.0/FPS, 6*30, 2*30)

        pos = self.lander.position
        vel = self.lander.linearVelocity
        state = [
            (pos.x - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2),
            (pos.y - (self.helipad_y+LEG_DOWN/SCALE)) / (VIEWPORT_H/SCALE/2),
            vel.x*(VIEWPORT_W/SCALE/2)/FPS,
            vel.y*(VIEWPORT_H/SCALE/2)/FPS,
            self.lander.angle,
            20.0*self.lander.angularVelocity/FPS,
            1.0 if self.legs[0].ground_contact else 0.0,
            1.0 if self.legs[1].ground_contact else 0.0,
            (self.x_pos_middle_landing - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2)
            ]
        assert len(state) == 9

        reward = 0
        shaping = \
            - 100*np.sqrt((state[0]-state[8])*(state[0]-state[8]) + state[1]*state[1]) \
            - 100*np.sqrt(state[2]*state[2] + state[3]*state[3]) \
            - 100*abs(state[4]) + 10*state[6] + 10*state[7]  # And ten points for legs contact, the idea is if you
                                                            # lose contact again after landing, you get negative reward
        if self.prev_shaping is not None:
            reward = shaping - self.prev_shaping
        self.prev_shaping = shaping

        reward -= m_power*0.30  # less fuel spent is better, about -30 for heuristic landing
        reward -= s_power*0.03

        done = False
        if self.game_over or abs(state[0]) >= 1.0:
            done = True
            reward = -100
        if not self.lander.awake:
            done = True
            reward = +100
        return np.array(state, dtype=np.float32), reward, done, {}

    def render(self, mode='human'):
        from gym.envs.classic_control import rendering
        if self.viewer is None:
            self.viewer = rendering.Viewer(VIEWPORT_W, VIEWPORT_H)
            self.viewer.set_bounds(0, VIEWPORT_W/SCALE, 0, VIEWPORT_H/SCALE)

        for obj in self.particles:
            obj.ttl -= 0.15
            obj.color1 = (max(0.2, 0.2+obj.ttl), max(0.2, 0.5*obj.ttl), max(0.2, 0.5*obj.ttl))
            obj.color2 = (max(0.2, 0.2+obj.ttl), max(0.2, 0.5*obj.ttl), max(0.2, 0.5*obj.ttl))

        self._clean_particles(False)

        for p in self.sky_polys:
            self.viewer.draw_polygon(p, color=(0, 0, 0))

        for obj in self.particles + self.drawlist:
            for f in obj.fixtures:
                trans = f.body.transform
                if type(f.shape) is circleShape:
                    t = rendering.Transform(translation=trans*f.shape.pos)
                    self.viewer.draw_circle(f.shape.radius, 20, color=obj.color1).add_attr(t)
                    self.viewer.draw_circle(f.shape.radius, 20, color=obj.color2, filled=False, linewidth=2).add_attr(t)
                else:
                    path = [trans*v for v in f.shape.vertices]
                    self.viewer.draw_polygon(path, color=obj.color1)
                    path.append(path[0])
                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)

        for x in [self.helipad_x1, self.helipad_x2]:
            flagy1 = self.helipad_y
            flagy2 = flagy1 + 50/SCALE
            self.viewer.draw_polyline([(x, flagy1), (x, flagy2)], color=(1, 1, 1))
            self.viewer.draw_polygon([(x, flagy2), (x, flagy2-10/SCALE), (x + 25/SCALE, flagy2 - 5/SCALE)],
                                    color=(0.8, 0.8, 0))

        return self.viewer.render(return_rgb_array=mode == 'rgb_array')

    def close(self):
        if self.viewer is not None:
            self.viewer.close()
            self.viewer = None


class RandomTargetLunarLander(LunarLander):
    continuous = True

def heuristic(env, s):
    """"""
    The heuristic for
    1. Testing
    2. Demonstration rollout.

    Args:
        env: The environment
        s (list): The state. Attributes:
                s[0] is the horizontal coordinate
                s[1] is the vertical coordinate
                s[2] is the horizontal speed
                s[3] is the vertical speed
                s[4] is the angle
                s[5] is the angular speed
                s[6] 1 if first leg has contact, else 0
                s[7] 1 if second leg has contact, else 0
                s[8] is the target coordinate
    returns:
        a: The heuristic to be fed into the step function defined above to determine the next step and reward.
    """"""

    angle_targ = s[0]*0.5 + s[2]*1.0         # angle should point towards center
    if angle_targ > 0.4: angle_targ = 0.4    # more than 0.4 radians (22 degrees) is bad
    if angle_targ < -0.4: angle_targ = -0.4
    hover_targ = 0.55*np.abs(s[0])           # target y should be proportional to horizontal offset

    angle_todo = (angle_targ - s[4]) * 0.5 - (s[5])*1.0
    hover_todo = (hover_targ - s[1])*0.5 - (s[3])*0.5

    if s[6] or s[7]:  # legs have contact
        angle_todo = 0
        hover_todo = -(s[3])*0.5  # override to reduce fall speed, that's all we need after contact

    if env.continuous:
        a = np.array([hover_todo*20 - 1, -angle_todo*20])
        a = np.clip(a, -1, +1)
    else:
        a = 0
        if hover_todo > np.abs(angle_todo) and hover_todo > 0.05: a = 2
        elif angle_todo < -0.05: a = 3
        elif angle_todo > +0.05: a = 1
    return a

def demo_heuristic_lander(env, seed=None, render=False):
    env.seed(seed)
    total_reward = 0
    steps = 0
    s = env.reset()
    while True:
        a = heuristic(env, s)
        s, r, done, info = env.step(a)
        total_reward += r

        if render:
            still_open = env.render()
            if still_open == False: break

        if steps % 20 == 0 or done:
            print(""observations:"", "" "".join([""{:+0.2f}"".format(x) for x in s]))
            print(""step {} total_reward {:+0.2f}"".format(steps, total_reward))
        steps += 1
        if done: break
    return total_reward


if __name__ == '__main__':
    demo_heuristic_lander(LunarLander(), render=True)

Here are the results:
Green is Normal Lunar Lander Continuous
Pink is the Random Target Lunar Lander Continuous



","['reinforcement-learning', 'deep-rl', 'actor-critic-methods', 'ddpg', 'gym']",
What is the difference between multi-head and normal output?,"
Let's say that I have a neural network with 2 heads. The first consists of X neurons. The second consists of Y neurons. I have these 2 heads because I want to predict 2 different variables. And I can see the loss for each head during training.
Now, let's say that I have only one head that consists of X+Y neurons. I can interpret the output because I know that the first X neurons describe some variable and the latter Y neurons describe the second variable.
I want to know if there is any difference between these 2 methods (maybe in performance or something). What are the pros and cons? Are there any advantages of one method over another for some particular tasks?
","['neural-networks', 'deep-learning', 'deep-neural-networks']","It depends on what your outputs are. For example, if both outputs are similar then you can use one output branch. However, what if the two outputs are different? With two output branches you can used two different loss functions. Now your model will optimize the two branches separately.Imagine if you have a model that has to output a class label for the input and a real value describing something in the input. One is a classification task and the other is a regression task. And the two  will require different loss functions, so you would use two branches."
Is creating dataset only by augmentation a bad practice?,"
I wonder if creating data set only by augmentation base images is a bad practice.
I mean the situation when you have to train net to predict really simple patterns, for example printed-like digits. And all digits from specific group looks basically the same, for example all one's look the same and so on. The only difference is rotation/translation etc. in the image.
Is it bad way to create data set by taking digit image and randomly rotate, translate and maybe erode/dilate it?
My intuition tells me that something's wrong with that approach, but I cannot find any reason why it should be wrong.
","['machine-learning', 'data-preprocessing', 'data-augmentation', 'training-datasets']",
Should one rescale (normalize) image before or after data augmentation?,"
During image preprocessing pipeline, should one rescale each pixel value to [0, 1] by dividing 255 first, and then perform data transformation such as color distortion, gaussian blur? or vice versa?
I believe for correctness, it may depend on the particular image transformation algorithm, or if you use some libraries. Is there any general advice? If anyone has experience trying both before or after, please share, particularly if you use external library or framework that may make implicit assumption to the value range of the pixel.
","['convolutional-neural-networks', 'image-processing', 'data-augmentation']",
Which neural network can I use to solve this constrained optimisation problem?,"
Let $\mathcal{S}$ be the training data set, where each input $u^i \in \mathcal{S}$ has $d$ features.
I want to design an ANN so that the cost function below is minimized (the sum of the square of pairwise differences between model outputs)  and the given constraint is satisfied, where $w$ is the ANN model parameter vector.
\begin{align}
\min _{w}& \sum_{\{i, j\} \in \mathcal{S}}\left(f\left(w, u^{i}\right)-f\left(w, u^{j}\right)\right)^{2} \\
&f\left(w, u^{i}\right) \geq q_{\min }, \quad i \in \mathcal{S}
\end{align}
What kind of ANN is suitable for this purpose?
","['neural-networks', 'machine-learning', 'architecture', 'model-request', 'constrained-optimization']",
Exploration for softmax should be binary or continuous softmax?,"
Maybe it's silly to ask but for random exploration in an RL for choosing discrete action, that in the neural network last layer softmax will be used, what random samples should we provide? binary like (0,0,1,0,0,0) or continuous softmax like (0.1, 0.15, 0.45, 0.25, 0,5, 0.1)??
if the answer is continuous, what algorithm do you suggest? like generating random numbers between 0 and 1 and then using softmax? (this algorithm mostly provides close numbers and I think it's not the correct way)
","['neural-networks', 'reinforcement-learning', 'tensorflow', 'python', 'softmax']",
PPO agent for vehicle control does not learn to stop at traffic lights,"
I have built a custom RL environment with gym, which simulates the RL vehicle and potential vehicles in front of the RL vehicle as well as traffic lights (including their state; red, yellow, green). I trained a PPO agent from stable_baselines3 without considering setting of hyperparameters and the agent learned to follow the vehicle in front of it without crashing. However it does not learn to stop at red lights after extensive training.
I tried training it without surrounding vehicles to get more interactions of the RL vehicle with traffic lights and this helped the agent to learn stopping a red light. However when I then continue training of the agent in a new environment with surrounding traffic, the agent again un-learns stopping at red lights.
I am still a novice with RL and do not understand as to why this happens and what I can do here. Should I set hyperparameters? Or try a different model? Or should I exchange the default policy of the PPO model?
","['reinforcement-learning', 'deep-rl']","The way I dealt with it was by giving a (very) strong negative reward when committing the mistake (here going under a red light) and the agent should learn to do not do this mistake anymore.It is often better to change actions, rewards or environment rather than acting on hyperparameters in my opinion, but I may be mistaken."
How to train a model for 1 image class to detect anomaly?,"
I want to train a model with python over the images, and these images are for a metal product.
my aim is to detect the defects, to notice if a product is a failure.
what kind of architecture do you suggest? should I train over the class? or should I use an autoencoder?
","['convolutional-neural-networks', 'autoencoders', 'anomaly-detection']",
"Are optimal policies always deterministic, or can there also be optimal policies that are stochastic?","
Let $M$ be an MDP with two states, $A$ and $B$, where $A$ is the starting state, and you always transit to the final state $B$ using two possible actions. $A_1$ gives you rewards that are normally distributed $\mathcal{N}(0, 1)$ and $A_2$ gives you rewards that are normally distributed $\mathcal{N}(0, 3)$.
How many optimal policies do we have? What is the optimal value of the states? Is any policy preferred over the other? Why? If you prefer one over the other, is there some way to detect it using what we have studied?
In my view, there are infinite policies that give the same expected reward.
$\pi_\alpha$ be a policy that is stochastic, which maps as follows - $\pi_\alpha(s, A_1) = \alpha $ and $ \pi_\alpha (s, A_2) = 1 - \alpha$ for $ \alpha \in [0,1]$. It is clear that, for each $\alpha$, we get infinite policies but have the same expected return.
But, according to some google searches (for example, here it says optimal policies are generally deterministic), I found optimal policies are always deterministic. Hence this implies there are only 2 policies, i.e., either take action $A_1$ or $A_2$ but not probabilistic.
So, my doubt is: what are the optimal policies here? Is it deterministic (only 2 policies) or stochastic (infinite)? Or is it an assumption that optimal policies are deterministic?
","['markov-decision-process', 'policies', 'optimality']","I think the result you are referring to is the one that says that there always exists a deterministic optimal policy for an MDP. This is true. But note that this does not imply that a stochastic optimal policy can not exist at the same time.Suppose you have an MDP with one state and two actions $a_1$ and $a_2$, both yielding the reward 0 in expectation (as in your example). Then consider a policy that takes action $a_1$ with probability $\alpha \in [0,1]$ and $a_2$ with probability $1-\alpha$. Either of the two deterministic policies with $\alpha=0$ or $\alpha=1$ are optimal, but so is any stochastic policy with $\alpha \in (0,1)$. All of these policies yield the expected return of 0.This is all assuming your optimality criterion is the expected cumulative (discounted) reward. If you have a different optimality criterion, such as something that accounts for risk, you might distinguish between rewards that have the same expected value but a different variance - but I think that is beyond the scope of the question."
How can we get a differentiable neural network to count things?,"
Imagine I have images with apples in them. I want to train a neural network which can count the number of apples in each image.
BUT, I don't want to use a detector, then count the number of bounding boxes. I'm interested in knowing if there's a way to bake this sort of logic into a differentiable neural network.
The simplest variation of this problem might be: I have an input vector $x \in \{0, 1\}^N$ and I want to count the number of 1s in it. I can make a single layer neural network, setting all the weights to 1, bias to 0, and linear activation. And my answer pops out. But how would I train the network to do this from scratch? Sure, I could regress to an output in $[0,1]$ and multiply the result by $N$, then the network is differentiable. But did the model really learn how to count? If so, would this behaviour be generalisable to counting multiple types of objects at once? Would it generalise to inputs where there can be any number of said object (like an image can have many apples in it, despite the size of the image)?
I want to know if there's a model which can learn how to count.
Here's another way I'm thinking about it: I can look at an aerial view of pine trees and say ""yeah maybe there are 30 trees"", but then I can do the task of looking at and identifying each tree individually, incrementing a counter, and making sure not to revisit that tree. The latter is what I consider truly ""counting"".
","['neural-networks', 'architecture', 'generalization']","Estimating from an observation is a function, but ""really counting"" is a process. Feed-forward neural networks can learn arbitrary functions from training examples, but they cannot represent (and therefore cannot learn) processes. They can attempt to estimate the results of completing a process as a function, but that is not the same thing as actually performing the process.To learn arbitrary processes from examples requires a model with some concept of state and evolution over time in addition to any functions that are required. Recurrent neural networks (RNNs) are suitable models for those kinds of learning problems, but so are other AI learning constructs.This rabbit hole goes deep, and for any model you could build it is possible to ask:Is the system really performing a counting task similar to how a human might attempt the same task?Has the sytem really learned to count, or has it been constructed by the developer so that some parts of the process are inevitable?Has the system learned to count in general within any sub-component, or are the things that it can count tightly coupled across components?There are other questions you could ask too, depending on your goals for producing such a model.It is worth noting that for small quantities and certain patterns, that human ""counting"" does more closely resemble the estimation process, or perhaps is more akin to an NLP problem. For example, consider how you ""count"" the pips on a die - although you can use a counting process to confirm what you see, typically you do not do so. Instead, some part of your brain is supplying a very accurate answer quickly and unconsciously:More generally, counting in humans likely consists of multiple related strategies for converting observations into symbolic and/or sensory representations of quantity. Some may be instinct, some are learned conscious behaviours and some appear to be in-between, perhaps originally learned but turned into subconscious skill through repetition.Ignoring whether or not you are implementing your AI project for counting items in an image wholly within a neural network for now, I think you need the following components:A state that represents and tracks the start, progress so far and end of the counting process with respect to the input being processed.An accumulator that represents quantity counted so far.A detector that can trigger a counting event when it observes something that needs to be counted.A strategy or planner for processing input so that detection events are separated and only triggered once for each valid event.The last component can vary a lot, and humans will use a range of different strategies for counting depending on the difficulty of the task. For instance, you might use working memory (a limited resource in humans) to track a few key points in an image to help segment an image into smaller sections as you work across it. Or you might make visual marks on an image to track each object that had already been counted. For a human, counting strategies can be mixed and matched, and switched between sometimes even during the same counting task.All of these components could be represented in learnable parts of an AI process, but they do not have to be. When you suggest in your question thatI don't want to use a detector, then count the number of bounding boxes.then you are most likely saying that you don't want to use a fixed hard-coded strategy. You want to somehow create a neural network that can discover at least one strategy for processing the input.The trouble you will face is that giving a large RNN model (e.g. LSTM) a dataset of examples with input images and output correct counts will likely be too much of a challenge. Discovery of a robust object counting system from scratch will be too hard.There are a few things that may help you construct something that ""really"" learns to count. Here are a couple of ideas:Curriculum learning, where you start training the neural network with some hand-holding examples, and slowly ramp up the complexity. This is analogous to how we teach children to count.Designed-in modelling for processing strategy. For instance, you could add a virtual fovea and artificial visual saccades and require that the neural network output a sequence of locations within the image that it picks out to run the detector against. This will be a constraint that allows certain types of human-like counting strategy to work, and could simplify the problem to the point that the network has a chance to learn it.A paper that uses a ""fovea"" model for object detection: Object detection through search with a foveated visual system"
Looking for help on initializing continuous HMM model for word level ASR,"
I have been studying HMM implementation approaches on ASR for the last couple of weeks. This probabilistic model is very new to me. I am currently using a Python package called Pomegranate to implement an ASR model of my own for the Librispeech dataset. I plan to use my own small-size dataset once I feel comfortable assessing the results of this one with Librispeech.
My problem: Librispeech (and my own dataset) has word-level transcription labels of the audio. The audio files have several word utterances each. Upon generating the MFCCs, I am not sure how to initialize the HMM matrices since the MFCCs, to my knowledge, capture phoneme level context at 10ms windows whereas I am trying to use the current word-level labels. There is no match up to which word each MFCC window belongs. Are the unique words in my corpus to be considered as the individual states in the transition probability matrix? I’m missing the point of how the extracted MFCCs are fed to the model for initialization and/or training?
I’ve been stumped on this for several days and I can’t seem to understand a clear cut explanation in the literature I have read. Any advice and help is very very much appreciated.
","['speech-recognition', 'markov-chain', 'hidden-markov-model', 'gaussian-mixture-models']",
Reinforcement learning for rearranging the mobile home screen icon layout: what inputs/states do I need to pass into the algorithm?,"
I have a problem where I need to rearrange a particular user's mobile home screen icon layout. Let's say that the social media app usage of a user is high compared to other app usage. So I need the reinforcement algorithm to process this information and send back the instructions to the android operating system as to how the icons needs to be arranged. To address this problem, I have chosen three algorithms:

Q-learning.
State-Action-Reward-State-Action.
Deep Deterministic Policy Gradient.

I have decided to first consider only Q-learning, so I am trying to understand the states, rewards, and the actions I need to pass in order to make this algorithm work.
The principles I have considered are:
– The environment is the mobile device operating system.
– Moving the apps up in the list depending on their usage can be the action where the app can be moved left, right, up or down.
– The reward can be a periodic reward, check if the user has rearranged an app which was given prominence in the list by the algorithm and receive a negative feedback if the user has rearranged the icon position or if it is in the same position receive a positive reward.
The initial challenge I am facing is to understand what inputs/states I need to pass into the algorithm and is there any reinforcement learning library I can use to mimic such an environment?
Are there any resources or papers I can use to solve this problem I am facing?
","['machine-learning', 'reinforcement-learning', 'q-learning', 'algorithm', 'sarsa']",
Understanding conflict set generation for conflict directed backjumping,"
I was reading Constraint Satisfaction Problem chapter from Artificial Intelligence 3rd ed book by Peter Norvig et al. On page 219, section 6.3 it explains computation of conflict set for conflict directed backjumping as follows:

In our example, $SA$ fails, and its conflict set is (say) $\{WA, NT,Q\}$. We backjump to $Q$,and $Q$ absorbs the conflict set from $SA$ (minus $Q$ itself, of course) into its own direct conflict set, which is $\{NT, NSW\}$; the new conflict set is $\{WA, NT, NSW\}$. That is, there is no solution from $Q$ onward, given the preceding assignment to $\{WA, NT, NSW\}$. Therefore, we backtrack to $NT$, the most recent of these. NT absorbs $\{WA, NT, NSW\}−\{NT\}$ into its own direct conflict set $\{WA\}$,giving $\{WA, NSW\}$ (as stated in the previous paragraph). Now the algorithm backjumps to $NSW$, as we would hope.
To summarize: let $X_j$ be the current variable, and let $conf(X_j)$ be its conflict set. If every possible value for $X_j$ fails, backjump to the most recent variable $X_i$ in $conf(X_j)$, and set
$conf(X_i) ← conf(X_i) ∪ conf(X_j) −\{X_i\}$.

I didn't get the line:

Therefore, we backtrack to $NT$, the most recent of these.

How is $NT$ most recent of $Q$'s conflict set $\{WA,NT,NSW\}$?
PS: Here is the map coloring example the book discusses:

","['constraint-satisfaction-problems', 'norvig-russell']",
Why some neural network models in the 1980s shown as circuit models,"
I am familiar with the currently popular neural network in deep learning, which has weights and is trained by gradient descent.
However, I found many papers that were popular in the 1980s and 1990s.
These papers have titles like ""Neural networks to solve optimization problems"".
For example, Hopfield first use this name, and they used ""Neural network"" to solve linear programming problems [1].
Later, Kennedy et.al used ""Neural network"" to solve nonlinear programming problems [2].
I summarize the difference between the current popular neural network and the ""Neural networks"":

They do not have parameter weights and bias to train or to learn from data.
They used a circuit diagram to present the model.
The model can be simplified as an ODE system and has a Lyapunov function as objective.

Please take a look at these two papers in the 1980s:

[Neurons with graded response have computational properties like those of two state neurons][2] (J.J. Hopfield)

Neural Networks for Non-linear Programming (M.P Kennedy & L.O Chua)


Reference:
[1]: J. J. Hopfield, D. W. Tank, “neural” computation of decisions in optimization problems, Biological265 cybernetics 52 (3) (1985) 141–152.
[2]: M. P. Kennedy, L. O. Chua, Neural networks for nonlinear programming, IEEE Transactions on Circuits and Systems 35 (5) (1988) 554–562.
","['neural-networks', 'papers', 'neurons', 'hopfield-network']",
Why isn't RL considered a continual learning strategy itself?,"
I have read about methods that apply continual learning strategies to reinforcement learning.
Since reinforcement learning also learns step by step (i.e., task by task, in a sense) during the training phase, why isn't it itself considered a continual learning strategy?
Of course, I understand that if an agent catastrophically forgets previously learned tasks, there is a need to prevent this and therefore develop strategies to mitigate catastrophic forgetting, but my question is more about the definition. If continuous learning (or online learning) is about learning one task at a time, and RL somehow does this, why is it not considered a continual learning strategy (regardless of the fact that it may not be as effective)?
To clarify, I haven't read anywhere the claim that RL is not a CL approach, but also none that it would be. Only the fact that CL methods are proposed for RL gives me the impression that RL is not considered an approach. Nor have I seen anyone mention RL for this purpose. I'm just wondering why that is.
","['reinforcement-learning', 'incremental-learning', 'online-learning']",
"What does the ""number of channels"" correspond to in U-Net?","
I'm studying the U-Net CNN architecture. I'm new to CNNs and am confused regarding the ""number of channels"".
Referring to the U-Net diagram, the input image is convolved with a 3x3 mask which generates a 570x570 output. This output image is then convolved again by a 3x3 mask to produce a 568 x 568 signal. However, what do the 64's correspond to?
The U-net says something about a multi-channel feature map. But how does convolving an image by a 3x3 mask results in a ""64"".

","['convolutional-neural-networks', 'u-net', 'semantic-segmentation']","In this example you have a gray scale image of size 572x572 and 1 (gray) channel. The first convolution operation consists of 64 filters of size 3x3 and 1 channel per filter. The channel of the filters always fits the channel size of the previous layer (here: the Input).
In the second convolution step of this explicit architecture, you again use 64 filters of size 3x3. In this case, each of these filters consists of 64 channels according to the previous output (64 feature maps/channels). The output of the second convolution consists of 64 feature maps according to the amount of 64 filters in the second convolution. This video from Andrew Ng visualizes it perfectly."
How to alpha-beta pruning to expectiminimax,"

I have this problem above and I'm trying to think of how to apply alpha-beta pruning to the above. The algorithm states that on you're opponents turn the (expecti turn) you just return the lowest value, but does that mean you apply the probabilities to those values? So for the far left you'd get 2 as the largest value then multiply that by 0.5, but then that set's $\\beta$ in the expecti node to $0.5*2=1$ and when it goes into the branch to the right it's comparing values without the probabilities applied to it when updating $\beta$.
","['alpha-beta-pruning', 'expectiminimax']",
Which methods for weight initialization in Neural Networks are currently common practice?,"
I am currently researching the topic of weight initialization methods for (deep) neural networks and I'm a little stuck. The result of my work should be an overview of methods that are currently in use.
I already collected information about different methods (Xavier/He, LSUV, Pre-Training, etc.), but I was wondering if anyone has a method that comes to mind (maybe recently developed) that I should look at more closely?
","['neural-networks', 'reference-request', 'weights', 'weights-initialization']",
What kind of neural network should I build to classify each instance of a time series sequence?,"
Let's say I have the time-series dataset below-left. I would like to train a model in such a way that, if I feed the model with an input like the test sequence below, it should be able to classify each sample with the correct class label.
    Training Sequence:              Test Sequence:

Time,   Bitrate,   Class           Time,   Bitrate    Predicted Class
 0,       312,       1              0,       234      -----> 0
 0.3,     319,       1              0.2,     261      -----> 0
 0.5,     227,       0              0.4,     277      -----> 0
 0.6,     229,       0              0.7,     301      -----> 1 
 0.7,     219,       0              0.8,     305      -----> 1
 0.8,     341,       1              0.9,     343      -----> 1 
 0.9,     281,       0              1.0,     299      -----> 0 
          ...                                ...

So, what kind of neural network should I build to classify each instance of a time series sequence?
","['classification', 'long-short-term-memory', 'time-series', 'algorithm-request', 'model-request']","A time series, usually, requires regular time intervals, but, from looking at your example, it seems that's not the case. You could try to use a MLP and give it as input the Time and Bitrate pairs and make it output the Class.The activation function is what makes your neural network produce its output, i.e. activate the neurons. The loss function calculates the error of your model's predictions. This error is used by the backpropagation algorithm to adjust the model when training.If your data can only be in one of two classes, for example 0 or 1, you have a binary classification problem. I recommend using the Sigmoid as the activation function. If you look at the Sigmoid's plot, you'll notice that, no matter the input, the output will always be within 0 and 1. You can think of this as the probability of the input being in class 0 or 1. So, for example, any input that produces a probability under 0.5 is class 0. Of course, if you want to be more precise you can simply procure the probabilities without the making it fit the labels. For example, an output of 0.7 represents that there is a 70% probability of the input belonging to class 1. Another term for this is Logistic Regression.The Binary cross-entropy calculates the error between the model's prediction and the real class label. This loss function can be applied to many classes, but the binary version keeps it to 0 and 1.Choosing the right activation functions and loss functions will depend on the problem you are trying to solve. Here are some useful guides  on loss functions and activation functions.Additional to  the above, try to use scaling(normalize) the inputs before feeding to the network, so that the network may generalize, as it looks like bitrate value above 300 is likely to be associated with class 1. It may improve performance."
"What is meant by ""real-valued argument"" in this context of the convolution operation?","
Consider the following statement from Deep Learning book (p. 327, chapter 9: Convolutional Networks)

In its most general form, convolution is an operation on two functions
of a real-valued argument.

Suppose $f$ and $g$ are functions on which I want to apply convolution operation. What is meant by two functions of a ""real-valued argument"" in this context?
Does it mean $f$ and $g$  are real-valued functions? Or does it mean $f$ and $g$  are real functions? or any other?

Real-valued function: Function whose codomain is a subset of real numbers

Real function: Function whose domain and codomain are a subset of real numbers.


","['deep-learning', 'convolutional-neural-networks', 'terminology', 'convolution', 'books']",
Is there any reasonable notion of regret for infinite horizon discounted MDPs?,"
I am thinking about episodic MDPs. Usually, in episodic MDPs, it seems that we have a finite fixed horizon per episode and no discount factor. Then, a very intuitive notion of regret after $T$ episodes is to sum over the difference of optimal expected return and achieved expected return.
I was wondering about notions of regret for infinite horizon discounted MDPs. It is not clear to me what a reasonable notion of regret for this setting would be, and I am also not aware of any standard definition of regret in this setting.
Maybe, as a justification for infinite horizon episodic MDPs, this quote by Littman in his paper: Markov games as a framework for multi-agent reinforcement learning

As in MDP's, the discount factor can be thought of as the probability that the game will be allowed to continue after the current move. It is possible to define a notion of undiscounted rewards [Schwartz, 1993], but not all Markov games have optimal strategies in the undiscounted case [Owen, 1982]. This is because, in many games, it is best to postpone risky actions indefinitely. For current purposes, the discount factor has the desirable effect of goading the players into trying to win sooner rather than later.

","['reinforcement-learning', 'definitions', 'markov-decision-process', 'regret']",
How to design training loop in RPN?,"
I have a short question. I understand the concept of RPN but one small details keeps me from implementing it. How should I design the training loop given that I have to use only a subset of anchor boxes (128 positive and 128 negative). In other words, if the ouput of reg layer is a map of every bounding box, how do I update only the bounding boxes that match my 128 positive/128 negative samples
",['faster-r-cnn'],
"In DQN, would it be cheaper to have $N$ neural networks with a single real-valued output, one for each of the $N$ actions?","
In the classical examples of deep q-learning, I often see neural networks in which the input represents the state of the agent, while the output is a tuple with all the values of $Q(s, a)$ predicted for all the possible $N$ actions.
Would it be cheaper to have $N$ neural networks with a single real-valued output, one for each of the $N$ actions?
With cheaper I mean cheaper in terms of the time complexity of a single training step of the network.
","['reinforcement-learning', 'q-learning', 'deep-rl', 'dqn']","Would it be cheaper to have $N$ neural networks with a single real-valued output, one for each of the $N$ actions?I think the ""No Free Lunch"" theorem applies here, or something like it.Your proposed architecture would be an unusual choice in many cases, but might be more efficient in others. For instance, it could be more efficient in the following scenario:The long term value is highly dependent on the immediate action choice, and in a way that relies on state variables differently, depending on the specific action. That means it would be difficult for a single NN to create shared features in its layers, and you could save processing by treating each action as a different prediction problem.This is only an educated guess.As usual, the only way to find out for sure is to try different approaches and compare them. I don't think there is anything other than experience and a little intuition to guide you."
Recommended Time serie forecasting model for Fibonacci levels classification,"
I have a set of time series data which gives me fibonacci levels and the duration at which the value is at this level.
Data structure to look like:
Date / Duration (minutes) / Level

201201 / 380 / 2
.....

210422 / 400 / 4

I'd like to create a NN model (LSTM maybe) that would forecast the next level, the probability it reaches it and this for several steps ahead (1 step = 400 minutes).
Which time series forecasting model would you recommend ?
Thanks in advance.
","['classification', 'long-short-term-memory', 'time-series']","It seems like you are trying to produce two outputs here. This usually makes the models more complex. What if you only predicted the Fibonacci levels for each time step? Then count how many time steps it stays at that level.As for producing the Fibonacci levels themselves, you can look into categorical time series where the values are categories instead of real numbers.You have a many-to-one problem. So, yes the model takes two inputs, Duration and Level, but only produces one output, the Level.The times septs in a model will always be the same. You have to choose a range that is appropriate and all your predictions will be based on that. It's important to note that finding the best time step size is a trial and error process. For example, you could first try using 10 time steps and if you find the model needs more you retrain it with 20 and compare the results."
Which approach best suits vector encodings?,"
I want to build a model that when given two vectors, outputs the probability of one vector being the encoded form of the other. I have 2 strategies for this: (Dataset available)

I can directly feed them in concatenation to a neural network and take the output as the probability.

I can train a conditional GAN with the conditional vector being the encoded vector and using the original vector as the generated one. In this case, the discriminator works as the network that I train in the first approach.


Which approach is better? Am I thinking in the right direction?
","['neural-networks', 'generative-adversarial-networks', 'models']",
How to scale all positive continuous reward?,"
My RL project has all positive continuous rewards for every step and the goal is to have the maximum cumulative reward (episodic reward). The problem is that the rewards are too close and all between 5 and 6, therefore achieving the optimum episodic reward will be harder.
What scaling methods are recommended? (like min-max scaling or reward ** 3)
How can I emphasize the episodic reward?
","['reinforcement-learning', 'q-learning', 'reward-functions', 'normalisation', 'standardisation']",
PPO in continuous control not working,"
I have PPO agent for discrete action space for LunarLander-v2 env in gym and it works well. However, when i am trying to solve continuous version of the same env - LunarLanderContinuous-v2 it is totally failing. I guess i made some mistakes in converting algorithm to continuous version. So, my steps of changing at algorithm are:

Change network: return mu and  var of shape n_actions. I have 2 different last layers for that, for mu i return Tanh of logits and for var i return Softplus of logits.
Choosing action: sampling action from normal distribution with expectation mu and variance var - torch.distributions.multivariate_normal.MultivariateNormal(torch.squeeze(mu), torch.torch.diag_embed(var))
For log of action probability i am using dist.log_prob(actions)

With this small changes my algorithm totally doesn't work. Is it right steps to convert algorithm with discrete action space to algorithm with continuous action space? I really confused, because my PPO for discrete action space work very well and with only this changes it is failing. Could you please suggest what i am doing wrong here?
","['reinforcement-learning', 'policy-gradients', 'proximal-policy-optimization', 'on-policy-methods', 'continuous-action-spaces']",
How do I increase the size of an (almost) balanced dataset?,"
I am trying to add more data points in my (almost) balanced dataset for training my neural network. I have come across techniques such as SMOTE or Random Over Sampling, but they work best for imbalanced data (as they balance the dataset). How can I do this and is it even worth it?
P.S. I know copying the same data points and appending them at the end doesn't add much value, but can we do it, and can it help to increase the prediction accuracy?
","['deep-learning', 'datasets', 'data-preprocessing', 'data-augmentation']",
"How is the variational lower bound for hard attention derived in Show, Attend and Tell","
How is the jump from line 1 to line 2 done in equation 10 of Show, Attend and Tell?

While we're at it, another thing that might be muddying the waters for me is that I'm not clear on what the sum is over. I know that $s$ is indexed as $s_{t,i}$, but the one-hot trick from line 2 to 3 makes me believe that the sum is over just $i$.
","['papers', 'math', 'attention', 'evidence-lower-bound']","This is Jensen's inequality at work.First of all, note that the first line can be rewritten as an expectation$$\sum_{s} p(s \mid \mathbf{a}) \log p(\mathbf{y} \mid s, \mathbf{a}) = \mathbb{E}_{p(s|a)}[\log p(\mathbf{y} \mid s, \mathbf{a})]$$Then Jensen's inequality gives (Note that a log function is a concave function so gives the opposite inequality to what is normally given when explaining Jensen's inequality with respect to convex functions):$$\mathbb{E}_{p(s|a)}[\log p(\mathbf{y} \mid s, \mathbf{a})] \leq \log\mathbb{E}_{p(s|a)}[ p(\mathbf{y} \mid s, \mathbf{a})] $$and then finally you can rewrite the Expectation as a summation.$$\log\mathbb{E}_{p(s|a)}[ p(\mathbf{y} \mid s, \mathbf{a})] = \log \sum_{s} p(s \mid \mathbf{a}) p(\mathbf{y} \mid s, \mathbf{a})$$"
What are the state-of-the-art learning algorithms for contextual bandits with stochastic rewards,"
I am building a solution for an environment with stochastic rewards in an online setting. I am wondering what the state of the art is in this setting.
Is it $\epsilon$-greedy (with logistic regression), LinUCB (with ridge regression), Thompson Sampling (with some approximator)? Could maybe point me to the relevant papers/articles?
","['reinforcement-learning', 'state-of-the-art', 'algorithm-request', 'contextual-bandits']",
When exactly am I overfitting -- contradicting metrics,"
I am training an object detection machine learning pipeline. Among the many metrics provided out of the box by tensorflow object detection API, I look at total_loss and DetectionBoxes_Precision/mAP@.75IOU:

Here the x-axis is the number of steps i.e. model experience. The orange line is for the training data and the blue line is for the validation data.
From the loss graph I would conclude, that at approx 2k steps overfitting starts, so using the model at approx 2k steps would be the best choice. But looking at the precision graph, training e.g. until 24k steps would be a much better model.
Which one is the best model?
Here, loss and the precision metric where picked just for illustrating the dilemma, there are many more metrics available, leading to multiple conclusions about when overfitting actually starts.
","['machine-learning', 'object-detection', 'overfitting', 'generalization']","From the loss graph I would conclude, that at approx 2k steps overfitting starts, so using the model at approx 2k steps would be the best choice. But looking at the precision graph, training e.g. until 24k steps would be a much better model. Which one is the best model?Different metrics will lead you to different conclusions. To detect overfitting under any metric you do need to plot training and validation/dev of the same metric, so it is harder to spot on your second graph.The reason why loss/cost functions and other metrics can disagree on what the best model is, is due to sensitivity to different types of error. Often loss functions will be sensitive to outliers - large differences in predictions versus ground truth in the training or validation data. So it is common to see loss diverging between validation and train first - probably due to a very few bad predictions caused by difficult edge cases that the model is not coping well with - whilst metrics such as accuracy may continue to improve on validation and test sets.The way to resolve this is to pick the metric that is important to you for business reasons (ideally ahead of the training, so you are not biased in your own decision about what to do), and use that to make judgement calls. This is easier to rely on the larger and cleaner your dataset is.One other thing you can do is to some error analysis - find the worst cases for loss function, and check that they are labelled correctly.If things still look inconclusive to you, and dataset size is small enough that results are noisy, then consider using k-fold cross validation to reduce effects of sample bias. This takes more time, but may improve your confidence in your model selection process."
Creating a NLP driven chatbot [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I would like to create a chat bot for an e-commerce website that sells a wide range of general merchandize items, from t-shirts, jumpers to calculators. Its primary objective is to develop a Q&A option for visitors/potential customers, to improve engagement on the website. As such, the chat bot is required to be fairly conversational.
I am experienced in classification et al, but know only the very basics on NLP. Can you provide suggestions on where to begin, e.g., recommended readings/ sources?
Also note, there is currently has no chat bot system in place, and hence no historical conversation data of any form.
","['natural-language-processing', 'reference-request', 'chat-bots', 'resource-request']",
Is there an AI that can extract proper nouns from free text?,"
I have some free text (think: blog articles, interview transcripts, chat comments), and would like to explore the text data by analysing the proper nouns it contains.
I know of many ways to simply look up the text against a 'list' of proper nouns. The problem with the approach is many false positives and false negatives, as well as inaccuracies where one proper noun (e.g. ""John Allen"") is identified as two proper nouns (""John"" and ""Allen""), as well as other problems, mostly to do with long or unusual proper nouns (e.g. ""the Gulf of Carpentaria"" - a single proper noun containing the word ""of"", and long names like ""Joost van der Westhuizen""). These kinds of longer, non-conformist proper nouns tend to really trip up grep-style proper noun identification models.
Does anyone know if any AI available to the public can more accurately identify proper nouns in free text?
","['text-classification', 'text-detection']",
Seq2Seq model produces repeating words,"
My framework is an encoder-decoder (LSTM-to-LSTM) model, similar to this post. The model basically reads a sentence and generate another sentence. But, the thing is, after a few epochs training, the model cannot produce any meaningful output, instead it keeps generating repeating words.
The framework can translate Franch-Enlgish very effectively, but for my problem it generates the result like this.
Can you explain why it produces such result, Thank you

Here is my printed output:


","['recurrent-neural-networks', 'long-short-term-memory', 'seq2seq', 'encoder-decoder']",
Why is it that the state visitation frequency equals the sum of state visitation frequency from initial time step to the horizon?,"
In the maximum entropy inverse reinforcement learning paper, Ziebart et al. show that the state visitation frequency $\rho(s)$ of a state $s$ can be computed as
$$
\rho_{\pi}(s) = \sum_{t}^{T} P(s_t=s|\pi),
$$
which is the sum of the probability that the state being visited at each time step.
I just don't understand why is it the sum? From my perspective, a frequency should be the less than one, so that it should be the average value
$$
\rho_{\pi}(s) = \frac{1}{T}\sum_{t}^{T} P(s_t=s|\pi).
$$
","['reinforcement-learning', 'inverse-rl']",
"Is there any specific SW framework, libraries or algorithms designed for implementing a practical artificial general intelligence (AGI) system? [closed]","







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Any (AGI)-KERAS like libraries useful to develop a real Artificial General Intelligence (AGI) system? Any deep-learning framework to develop AGI applications? any real Transformer-based (like Chat-GPT) libraries that can be used to achieve a minimum level of Artificial General Intelligence?
Existing frameworks/algorithms used in NN, NLP, ML, etc are not enough in my opinion. In my opinion any framework has to be based on building blocks from: Cognitive Science, Neuroscience, Mathematics, Artificial Intelligence, Computer Science, psycology, sociology, etc.
","['machine-learning', 'natural-language-processing', 'agi', 'singularity', 'neuroscience']","Not to my knowledge. The problem is that this is such an enormous task, it cannot really be tackled at once. So the obvious solution is to reduce the scope. In early AI people were using toy domains, whereas nowadays AI systems work more generally (but still perform better if the domain is restricted).So while (slow) progress is being made putting the individual building blocks together, we're still a long way off building an overarching general system from them."
What is the most computationally efficient genetic algorithm?,"
In researching genetic algorithms, it seems that there are various methods of selection and other operator methods that can significantly change the performance. For example, this picture contains some of the methods that could be used:

Presumably, you can mix & match these operators to optimize whatever problem you're trying to solve.
What most people care about is how many iterations it takes to get to the target. This is understandable, but I've seen things that would be inefficient in real systems such as:

using sort on the current population $\mathcal{O}(n \log n)$ and picking the first n members for the mating pool

appending to a constantly resizing slice to create a mating pool instead of rewriting on the current array


What I am looking for is, how can I arrive at the target using the least amount of computation and memory possible. The number of iterations and the time taken to get there is still a secondary priority.
It's possible that it may be the process of picking the right operators, but what I am also considering is how parallelizable the implementation could be as well.
","['genetic-algorithms', 'evolutionary-algorithms', 'time-complexity', 'crossover-operators', 'selection-operators']","First of all, for a lot of realistic problems, the fitness function evaluation is usually orders of magnitude greater in complexity than the rest of the genetic algorithm. This is not always true, but often is true (e.g. imagine trying to optimise a simulation where you need to execute the simulation completely to obtain the fitness). So optimising the GA itself might only be helpful when the fitness function is lightweight (e.g. it's a mathematical function as used in some competitions).While your diagram shows a lot of operators, it doesn't show variations of the GA itself. Here are two:If you really wanted to maximise efficiency in the GA, a steady state approach would probably work best combined with tournament selection, as tournament selection does not require any sorting. The mutation/crossover operators you list are likely to all be quite efficient, but simpler methods are likely to to be the most efficient (e.g. bit flip mutation+1 point crossover). For a realistic problem, however, problem-specific operators usually work best."
Do we need as much information to know if we can can answer a question as we need to actually answer the question?,"
I am reading The Book of Why: The New Science of Cause and Effect by Judea Pearl, and in page 12 I see the following diagram.

The box on the right side of box 5 ""Can the query be answered?"" is located before box 6 and box 9 which are the processes to actually answer the question. I thought that means that telling if we can answer a question would be easier than actually answering it.
Questions

Do we need less information to tell if we can answer a problem (epistemic uncertainty) than actually answer it?
Do we need to try to answer the problem and then realize that we cannot answer it?
Do we answer the problem and at the same time provide an uncertainty estimation?

","['computational-complexity', 'causation', 'uncertainty-quantification', 'epistemic-uncertainty', 'philosophy-of-math']",
Difference in UCB performance when scaling the rewards,"
I notice the following behavior when running experiments with $\epsilon$-greedy and UCB1. If the reward is kept binary (0 or 1) both algorithm's performances are on par with each other. However, if I make the reward continuous (and bounded [0, 1]) then $\epsilon$-greedy remains good but UCB1 performance plummets. As an experiment, I just scaled the reward of 1 by a factor of 1/10 which negatively influences the performance.
I have plotted the reward values estimated by the algorithms and see that (due to the confidence interval term) UCB1 largely overestimates the rewards.
Is there a practical trick to fix that? My guess is that the scaling coefficient $c$ in front of the upper confidence bound is meant just for this case. Nevertheless, the difference in performance is staggering to me. How do I know when and what scaling coefficient will be appropriate?
===
update 1 The reward distribution is very simple. There are 17 arms, for arm 3 and 4 the learning algorithm gets a reward of 1, the other arms return reward of 0. No stochasticity, the algorithm has 1000 iterations.
If I scale the reward by a factor 1/10, for instance, then UCB1 takes a whole lot of time to start catching up with $\epsilon$-greedy
","['reinforcement-learning', 'rewards', 'multi-armed-bandits', 'upper-confidence-bound']",
"What does the approximate posterior on latent variables, $q_\phi(z|x)$, tend to when optimising VAE's","
The ELBO objective is described as follows
$$ ELBO(\phi,\theta) = E_{q_\phi(z|x)}[log p_\theta (x|z)] - KL[q_\phi (z|x)||p(z)] $$
This form of ELBO includes a regularisation term  in the form of the KL divergence which drives $q_\phi(z|x) \rightarrow p(z)$ when optimising ELBO.
However we also have the overall expression for the loglikelihood which is defined as follows (proof provided here)
$$ p_\theta(x) = ELBO(\phi,\theta) + KL[q_\phi(z|x)||p_\theta(z|x)] $$
Rearranging the above equation as follows
$$  \max\limits_\phi ELBO(\phi,\theta) = \max\limits_\phi p_\theta(x) - KL[q_\phi(z|x)||p_\theta(z|x)]  $$
We can see that maximising ELBO w.r.t $\phi$ in this form causes $q_\phi(z|x) \rightarrow p_\theta(z|x)$
These two ways of describing how VAEs learn conflicts my understanding of what happens to the approximate distribution during training.
Is it simply just trying to match both the prior $p(z)$ and the posterior $p_\theta(z|x)$ or am I missing something
","['variational-autoencoder', 'evidence-lower-bound', 'variational-inference']",
What would happen if we set the evaluation function in the best-first search algorithm as the cost of paths taken to new nodes?,"
I am reading Artificial Intelligence: A Modern Approach.
In Chapter 3, Section 3.3.1, The best-first search algorithm is introduced. We learn that in each iteration, this algorithm chooses which node to expand based on minimizing an evaluation function, $f(n)$, for new nodes. And if the expanded nodes are either not already reached, or they generate a less costly path to a reached state, they will be added to the frontier. So, the kind of queue used in the best-first search is a priority queue, i.e., ordering nodes by the function $f(n)$. If we set the $f(n)$ as the depth of the nodes, the queue type will be changed to FIFO (first-in-first-out), which is used in the breadth-first search algorithm.
Therefore, we can change the nature of algorithms using the $f(n)$ function.
I am wondering what would happen if we set $f(n)$ as the cost of the paths taken from the common parent node of new nodes to each new node $n$. Since new nodes might stem from different previous nodes, we might have to measure the cost of these nodes' path all the way back till we find a common parent of them (which, in the worst case, is the root node, indicating the initial state). In this way, each time a new node is chosen for expansion (using $f(n)$), and each time an expanded node is chosen for joining the frontier (using cost function), the choice is taken by the similar criterion since $f(n)$ and the cost function is now identical.
What would be the nature of such an algorithm? Is measuring the cost of paths to new nodes computationally feasible? Can this be a practical algorithm?
I read later sections and realized that the Dijkstra’s algorithm (uniform-cost search) is so similar to what I had in mind. However, it sets the evaluation function as the cost of the path from the root to the current node. I proposed grouping new nodes by their common parent and compare the cost of nodes within a certain group first. Then, after selecting out best nodes in each group, form a new group based on the selected nodes' new common parent, and do this until we reach the root node, when we will have the last group and comparing costs within that group will find the optimal node for us.
Would the algorithm I have in mind have any advantage over Dijkstra's algorithm?
","['search', 'breadth-first-search', 'evaluation-functions', 'best-first-search']",
How do I take the correct classification predictions of an ml algo (i.e. random forest/neural net) and sort the instances in each category?,"
I am trying to sort the instances within each of 5 classification categories in a dataset that has been put through both a random forest classifier and a neural network with 99% accuracy on each.
Essentially what I am trying to do is stack a sorting algorithm on top of a random forest or a neural net (depending on which will boast a much more efficient and swift process in being integrated with a sorting algorithm post-output) so that the correctly classified instances within each category can be sorted into separate organized and easily comprehensible lists.
I have tried researching this but all I have found are examples of ensemble learning and traditional stacking in order to achieve higher overall accuracy on an algorithm.
I am trying to see how I can take the correct predictions from either of these algorithms and sort them by some arbitrary features that I will engineer, but all I am wondering at the moment is how to integrate a sorting algorithm in the first place into the output of a classification algorithm.
","['neural-networks', 'machine-learning', 'classification', 'ensemble-learning', 'random-forests']",
Embedding from Transformer-based model from paragraph or documnet (like Doc2Vec),"
I have a set of data that contains the different lengths of sequences. On average the sequence length is 600. The dataset is like this:
S1 = ['Walk','Eat','Going school','Eat','Watching movie','Walk'......,'Sleep']
S2 = ['Eat','Eat','Going school','Walk','Walk','Watching movie'.......,'Eat']
.........................................
.........................................
S50 = ['Walk','Going school','Eat','Eat','Watching movie','Sleep',.......,'Walk']

The number of unique actions in the dataset are fixed. That means some sentences may not contain all of the actions.
By using Doc2Vec (Gensim library particularly), I was able to extract embedding for each of the sequences and used that for later task (i.e., clustering or similarity measure)
As transformer is the state-of-the-art method for NLP task. I am thinking if Transformer-based model can be used for similar task. While searching for this technique I came across the ""sentence-Transformer""-
https://github.com/UKPLab/sentence-transformers. But it uses a pretrained BERT model (which is probably for language but my case is not related to language) to encode the sentences. Is there any way I can get embedding from my dataset using Transformer-based model?
","['natural-language-processing', 'transformer', 'bert', 'embeddings']",
What algorithm to use to classify data by spatial relations?,"
Let's assume I have dataset of image-like 2D samples where values can be divided into few discrete levels (for example 1, 2, 3 and 4) like in the image below, where each color maps different value, from 1 to 4. Number of how many times given color occurs on the picture varies from sample to sample though.

I would like to classify these images into different classes but based on the spatial relations of these values between each other (not the values themselves). By spatial relations I mean basically (left, right, up, down), for example:

If blue is above and to the right of the red
Another blue is above and to the left of the same red
Yellow is to the right of one blue (same height)
One green is below red
...

My question is, what algorithm (probably some deep neural network) I should use for that task?
I would appreciate even just some keywords or clues of what might help.
","['deep-learning', 'classification', 'image-recognition', 'deep-neural-networks', 'knowledge-representation']",
Video Analysis: Providing a success score for a of a student carrying out a specific task,"
I have an AI/ML challenge in relation to video analysis and am unsure where to start.
I am investigating an application that will grade students performance of carrying out a task, based on analysis of a video of them carrying out the task.
The problem has not been sufficiently defined yet but to get a high level idea, imagine  video showing close up of a trainee doctor performing stitches to close a wound. the AI model would be trained using many videos of someone performing the stitches correctly and score the trainee on a number of criteria.
Most frameworks will allow detection of objects but taking a video of a person carrying out a task and assessing their success using an AI/ML model feels a step above regular object analysis.
Assumption is we will create the training material, having professionals video themselves carrying out the task successfully, which will also be graded by other professionals to provide a rubric of scores.
I understand this is not something that can be simply answered but an idea of where to start would be very helpful.

are there specific areas of AI i should investigate?
are there frameworks that can actually do this ( i have not found any)?

Appreciate any advice.
Thank you
","['training', 'ai-design', 'video-classification']",
Backpropagation - what does rate of change calculated from the partial derivatives actually relate to?,"
I understand conceptually how backpropagation works according to the chain rule, and I understand that partial derivatives calculate the rate of change of a function containing multiple variables with respect to one of those variables, the rest being fixed.
What I'm struggling with is what the value from these partial derivatives actually relates to. I found this https://activecalculus.org/multi/S-10-2-First-Order-Partial-Derivatives.html which gives some good examples. But with a NN I'm not sure what units the results of the derivatives relate to.
One of the examples on the website used z = f(x,y) z horizontal distance travelled of a projectile, x initial speed in feet per second, and y was the angle. So if taking the partial derivative with respect to x the results tell us how much the distance travelled changes with respect to the change in speed. So it might be that for every one foot per second increase of the initial speed, we get an increase of 8 feet horizontal travel if using a fixed value for y.
But when calculating the derivatives for backpropagation, does this mean that if we get an answer of (random value) 0.08, this means that for every change of 1 to the non-static variable we would get a change of 0.08 to our output? And what units (if any) do these values relate to?
","['neural-networks', 'deep-learning', 'backpropagation']",
Why does Alpha Zero's Neural Network flip the board to be oriented towards the current player?,"
While reading the AlphaZero paper in preparation to code my own RL algorithm to play Chess decently well, I saw that the

""The board is oriented to the perspective of the current player.""

I was wondering why this is the case if there are two agents (black and white). Is it because there is only one central DCNN network used for board and move evaluation (i.e. there aren't two separate networks/policies used for the respective players - black and white) in the algorithm AlphaZero uses to generate moves?
If I were to implement a black move policy and a white move policy for the respective agents in my environment, would reflecting the board to match the perspective of the current player be necessary since theoretically the black agent should learn black's perspective of moves while the white agent should learn white's perspective of moves?
","['neural-networks', 'reinforcement-learning', 'alphazero', 'chess', 'multi-agent-systems']","There is a single neural network that guides self-plays in the Monte Carlo Tree Search algorithm. The neural network gets the current state of the board $s$ as an input and outputs current policy $\pi(a|s)$ and value $v(s)$.The action probabilities are encoded in a (8,8,73) tensor. First two dimensions encode the coordinates of the figure to ""pick"" from the board. The third dimension encode where to move this figure: check out this question for a discussion on how all possible moves are encoded in a 73 dimensional vector.Similarly, the inputs of the network are organized in the (8, 8, 14 * 8 + 7 = 119) tensor. The first two 8 x 8 dimensions, again, encode the positions on the board. Then the positions of the figures one plane per 6 figure types: first 6 planes for player's figures, next 6 planes for opponent's figures and two repetition planes. The 14 planes are repeated 8 times supplying predecessor positions to the network. Finally, there are 7 extra planes encoding as a single uniform value over the board - castling rights (4 planes), total move count (2 planes) and the current player color (1 plane).Note that the positions of player's figures and opponent's figures are encoded in fixed layers of the state tensor. If you don't flip the board to the perspective of the player then the network will have very different training inputs for black and white states. It also will have to figure out which direction the pawns can move depending on the current player color. None of that it is impossible, of course - but that unnecessarily complicates something that is already a very hard problem for the DNN to learn.You can go further and completely split the training for white and black players, as you've described. But that'll essentially double the work you'll have to do train your nets (and, I suspect, there would be some stability troubles typical for adversarial training).To summarize - you are generally right - there is no fundamental need to flip the board. All the above details in state encoding are done to simplify the learning task for the deep neural network."
"About the choice of the activation functions in the Multilayer Perceptron, and on what does this depends?","
I've read in this: F. Rosenblatt, Principles of neurodynamics. perceptrons and the theory of brain mechanisms that in the Multilayer Perceptron the activation functions in the second, third, ..., are all non linear, and they all can be different. And in the first layer, they are all linear.
Why?
On what does this depends?

When it is said ""the neural network learns automatically"", in colloquial words, what does it mean?

AFAIK, one first train the NN, then at some point NN learns. When does the ""automatically"" enters then?
Thanks in advance for your help.
","['neural-networks', 'activation-functions', 'feedforward-neural-networks', 'multilayer-perceptrons']",
Is it a good practice to pad signal before feature extraction?,"
Is padding, before feature extraction with VGGish, a good practice?
Our padding technique is to find the longest signal (which is loaded .wav signal), and then, in every shorter signal, put zeros to the size of the longest one. We need to use it because one size of input data is desirable.
Perhaps there is any other techniques you recommend?
The difference between padding before and after the features extraction by accuracy is quite big - more than 20%. Using padding before extraction gives 97% accuracy.
I'd be glad to read your feedback, and explain me why that happens, and tell me if that kind of padding is correct action or is there a better solution.
","['feature-extraction', 'accuracy', 'signal-processing', 'feature-engineering', 'padding']","Padding is a common practice both in image-processing (typically via CNNs) and in sequence-processing tasks (RNNs, Transformers).For CNNs all the standard convolutional layers - Conv1D, Conv2D and Conv3D,- have the padding argument. The padding values can be valid or same for 2d and 3d convolutions. And extra causal type of padding is possible for 1d convolutions and the documentation refers to this paper: WaveNet: A Generative Model for Raw Audio -  which sounds quite close to what you are interested in.This animations might be useful to get a bit more intuition about the convolutions and strides/padding. The general consensus is that using same padding is advantageous for model performance - your network gets more information about the borders of your inputs (and deeper networks are possible).For sequential models padding is even more important. Training samples are usually  of unequal lengths, so you have to pad them with a special token (usually called [PAD] that gets encoded as 0). Here are some examples of this mentioned in tensorflow docs, huggingface.transformers or BERT tutorial."
What would be the importance sampling ratio for off-policy TD learning control using Q values?,"
The off-policy TD learning control using state value function from page 34 of David Silver's RL lecture is:
$$ V(S_t) \leftarrow V(S_t) + \alpha \left( \frac{ \pi(A_t|S_t)}{\mu (A_t|S_t)} (R_{t+1} + \gamma V(S_{t+1})) - V(S_t) \right). $$
I'd like to change this update rule to action value function Q, something like:
$$ Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha \left( \frac{ \pi(A_t|S_t)}{\mu (A_t|S_t)} (R_{t+1} + \gamma Q(S_{t+1},A_{t+1})) - Q(S_t,A_t) \right). $$
Then what is the corresponding importance sampling ratio?
Since $A_t$ is already determined (because we are calculating $Q(S_t,A_t)$), I think $\pi(A_t|S_t)$ is definitely 1. But what about $\mu (A_t|S_t)$? Is it 1 or not?
","['machine-learning', 'reinforcement-learning', 'temporal-difference-methods', 'importance-sampling']","Since $A_t$ is already determined (because we are calculating $Q(S_t,A_t)$), I think $\pi(A_t|S_t)$ is definitely 1. But what about $\mu (A_t|S_t)$? Is it 1 or not?You could assign values of 1 to each to get the right answer, but the situation is different. You can see that more clearly in the definition of action value, $q(s,a)$:$$q_{\pi}(s,a) = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t=s, A_t=a]$$The condition on the expectation $|S_t=s, A_t=a$ means that the value of $A_t$ is already assumed. There is no need to know or use the associated probability, and no need to adjust between probabilities of different policies when estimating returns for the first action choice.In addition, the last action choice used to bootstrap in Q learning is always the maximising action over current Q values. That is an on-policy choice with respect to the target policy $\pi$, so does not need to be adjusted for.For single-step Q learning, there is no need to use importance sampling to adjust for policy differences between behaviour and target policies. The correct update equation rule is:$$ Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha \left( R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) \right). $$This does change for n-step updates, where you do need to take account of differences in action choice between target and behaviour policies for $A_{t+1}, A_{t+2}$ up to $A_{t+n-1}$. However, you should bear in mind that $\pi(a|s)$ is zero for any non-maximising action - if you use weighted importance sampling then any trajectory with an exploring action before calculating the TD update contributes zero update.So, if you are calculating over longer trajectories, in e.g. Q($\lambda$), it is common to see this simplified to a logical test and some kind of shortcut update. You rarely see explicit use of importance sampling - with probability calculations and ratios - in Q learning."
"On what basis is MATLAB ""inflexible"" to perform ML/AI research on it?","
During a course review, I have provided my opinion on the course overall. I stated that MATLAB is also a great environment to program and do research for ML/AI, but my professor seemed to have taken my comments as a joke and told me ""If you take a look at the statistics, then you'll see that MATLAB is not a feasible environment to innovate and research topics in ML/AI"".
As an undergraduate who is new to machine learning, I would hope to understand more and not debate on which is better (MATLAB vs Python), but rather to know whether there is a bias against MATLAB or there are actually reasons that make MATLAB not a good environment to research and program ML topics.
","['research', 'matlab']","(This question is could be considered off-topic or opinion-based, but I will answer it by providing facts that could hinder the adoption of MATLAB by AI researchers).There are 2 main reasons why MATLAB may not be the ""best"" programming language/environment for research (in AI and other areas too)In my view, these are clear obstacles for people that want to share their research and make sure that people can reproduce their results or build new ideas on top of them. However, in some cases, using proprietary software for research may be the best (or even only) option (see e.g. this for more info). In fact, I have also seen MATLAB being used for research in artificial intelligence (here is an example), but the choice of MATLAB might have been due to the familiarity of the authors with the language/environment."
Are neural networks invertible?,"
I am interested in learning about the inverse of neural networks and I would like to understand about the invertibility of neural networks, as for example described in On the Invertibility of Invertible Neural Networks.
Researchers who are working on this domain, can you help me understand these two questions.

Are all neural network invertible ?
What exactly qualifies a neural network to be invertible ?

",['neural-networks'],"The meaning of invertible here is the standard definition of invertibility for a mathematical function $f \colon X \to Y$. Invertible simply means ""the function has an inverse map $f^{-1} \colon Y \to X$"". Equivalently the function $f$ is bijective, which means the following two conditions hold:$f$ is injective: for any two distinct $x_1, x_2 \in X$, $f(x_1) \ne f(x_2)$.$f$ is surjective: for any $y \in Y$, there exists an $x \in X$ such that $f(x) = y$.If this is unfamiliar, you should be able to find some helpful references on Google using these terms.Most obvious neural network architectures cannot possibly be invertible. Consider for example a classifier which takes an image or some other high-dimensional input, and outputs a classification label. This network could only be invertible if there was only one possible input1 which corresponds to each label, which is not the goal of the network.1 I mean this rather literally: if one pixel is even slightly different, this would be a different input to the network and would have to have a different output."
Unable to 'learn' a rotational angle by parametrising the angle as a neural network layer,"
I'm trying to implement a neural network that can capture the drift in a measured angle as a way of dynamic calibration. i.e, I have a reference system that may change throughout the course of the data gathering and would like to train a network layer which actually converts the drifting reference to the desired reference by updating the angle parameter.
For example: Consider the 2d case. We would have a set of 2d points $X\in \mathbb{R}^2$ and a trainable parameter called $\theta$ in the layer. The output of the layer would then be:
$$X_o = XR$$ where
$$R = \begin{bmatrix}
\cos(\theta) & -\sin(\theta) \\ 
\sin(\theta) & \cos(\theta)
\end{bmatrix}$$
Using Adam optimizer I then try to find the $\theta$ which transforms a given angle to the desired reference.
However, the $\theta$ value seems to fluctuate around the initial value probably because of a diverging gradient(?). How can I overcome this issue?
The code is below.
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


class Rotation2D(tf.keras.layers.Layer):
  def __init__(self):
    super(Rotation2D, self).__init__()

  def build(self, input_shape):
    self.kernel = self.add_weight(""kernel"", initializer=tf.keras.initializers.Constant(90),
                                  shape=[1, 1])

  def call(self, input):
    matrix = ([[tf.cos(self.kernel[0, 0]), -tf.sin(self.kernel[0, 0])],
              [tf.sin(self.kernel[0, 0]), tf.cos(self.kernel[0, 0])]])
    return tf.matmul(input, tf.transpose(matrix))

layer = Rotation2D()

t = np.arange(0, 1000)/200.

y_in = np.array([np.sin(t), np.cos(t)]).T
y_ta = np.array([np.cos(t), np.sin(t)]).T

model = tf.keras.Sequential()
model.add(layer)

model.compile(tf.keras.optimizers.SGD(lr=1.), loss='MSE')
model.fit(y_in, y_ta, epochs=1)
for i in range(100):
  print(layer.get_weights())
  model.fit(y_in, y_ta,verbose=0, batch_size=5)
y_out = (model.predict(y_in))

fig, axes = plt.subplots(2, 1)

for i in range(2):
  ax = axes[i]

  ax.plot(y_in.T[i], label = 'input')
  ax.plot(y_ta.T[i], label = 'target')
  ax.plot(y_out.T[i], label = 'prediction')

plt.legend()

plt.show()```

",['deep-neural-networks'],"There are two basic problems with your code:The functions sin(t) and cos(t) (both in numpy and tensorflow) take radians as inputs. Seeing Constant(90) in your code, and the learning rate of 1. I'm guessing that you assume that it is in degrees - that's incorrect.In your training data y_ta is not a rotation of y_in:It is a reflection about y=x diagonal. No wonder it fails to find an appropriate rotation.I just had to change y_ta to:And train with more sensible learning rate:To get the angle (which I then convert to degrees):"
How to train my model using transfer learning on inception_v3 pre-trained model?,"
I am trying to train my model to classify 10 classes of hand gestures but I don't get why am I getting validation accuracy approx. double than training accuracy.
My dataset is from kaggle:
https://www.kaggle.com/gti-upm/leapgestrecog/version/1
My code for training model:
print(x.shape, y.shape)
# ((10000, 240, 320), (10000,))

# preprocessing
x_data = x/255  
le = LabelEncoder()  
y_data = le.fit_transform(y)  
x_data = x_data.reshape(-1,240,320,1)   
x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.25,shuffle=True)  
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Training

base_model = keras.applications.InceptionV3(input_tensor=Input(shape=(240,320,3)),
                                           include_top=False, 
                                           weights='imagenet')
base_model.trainable = False
    
CLASSES = 10
input_tensor = Input(shape=(240,320,1) )
model = Sequential()
model.add(input_tensor)
model.add(Conv2D(3,(3,3),padding='same'))
model.add(base_model)
model.add(GlobalAveragePooling2D())
model.add(Dropout(0.4))
model.add(Dense(CLASSES, activation='softmax'))
model.compile(loss='categorical_crossentropy', 
              optimizer=optimizers.Adam(lr=1e-5), metrics=['accuracy'])

history = model.fit(
x_train,
y_train,
batch_size=64,
epochs=20,
validation_data=(x_test, y_test)
)

I am getting accuracy like:
Epoch 1/20  
118/118 [==============================] - 117s 620ms/step - loss: 2.4571 - accuracy: 0.1020 - val_loss: 2.2566 - val_accuracy: 0.1640
Epoch 2/20  
118/118 [==============================] - 70s 589ms/step - loss: 2.3253 - accuracy: 0.1324 - val_loss: 2.1569 - val_accuracy: 0.2512


I have tried removing the Dropout layer, changing train_test_split, but nothing works.
EDIT:
On changing the dataset to color images from https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset
, I am still getting higher validation accuracy in initial epochs, is it acceptable or I am doing something wrong?

","['tensorflow', 'transfer-learning', 'pretrained-models', 'inception']",
What's new in LaBSE v2?,"
I can't find what's new in LaBSE v2 (https://tfhub.dev/google/LaBSE/2). What are the main highlights of v2 versus v1? And how did you find out?
","['tensorflow', 'bert']",
Why object detection algorithms are poor in optical character recognition?,"
OCR is still a very hard problem. We don't have universal powerful solutions. We use the CTC loss function
An Intuitive Explanation of Connectionist Temporal Classification | Towards Data Science
Sequence Modeling
With CTC | Distill
which is very popular, but it's still not enough.
The simple solution would be to use object detection algorithms for recognizing every single character and combine them to form words and sentences.
We already have really powerful object detection algorithms like Faster-RCNN, YOLO, SSD. They can detect even very complicated objects that are not fully visible.
But I read that these object detection algorithms are very poor if you use them for recognizing characters. It's very strange since these are very simple objects, just a few lines and circles. And mainly grayscale images. I know that we use object detection algorithms to detect the regions of text on big images. And then we recognize this text. Why can't we just use object detection algorithms (small versions of popular neural networks) for recognizing single characters?
Why we use CTC or other approaches (besides the fact that it would require much more labeling)?
Why not object detection?
","['object-detection', 'object-recognition', 'optical-character-recognition', 'ctc-loss']","Good question! Using Yolo to recognise characters would be a good experiment to try. It may be because of the density of characters on a page -- systems like Yolo are very good at detecting a small number e.g. 2,3 or 10, objects, but don't work so well when the number of objects  is the hundreds as you might have with OCR. A better approach might be to try face detection methods that work well with large crowds."
"What does ""statistical efficiency"" mean in this context?","
Consider the following statement(s) from Deep Learning book (p. 333, chapter 9: Convolutional Networks) by Ian Goodfellow et al.

Convolution is thus dramatically more efficient than dense matrix
multiplication in terms of the memory requirements and statistical
eﬃciency.

Book is saying that statistical efficiency is due to the decrease in the number of parameters due to convolution (using kernel) compared to fully connected feed forward neural networks.
What is meant by statistical efficiency in this context? And how does decrease in the number of parameters increase statistical efficiency?
","['convolutional-neural-networks', 'terminology', 'books']","Statistical efficiency in this context essentially means that a CNN would require fewer training examples than a fully connected network to learn. Intuitively this seems reasonable: more parameters to learn should mean more samples needed. Of course it is always desirable to minimise the number of training samples needed, so that's a definite advantage of CNNs.There is a paper on the efficiency of CNNs which attempts to make that statement more precise. They examine the case of a convolutional network using a linear activation function."
How to handle class imbalance when the actual data are that way,"
My supervised learning training data are obtained from actual data; and in real cases, there's one class that happens less often than other classes, just around 5% of all cases.
To be precise, the first 2 classes are in 95% of training data and the last one is in 5%. Training while keeping the data ratio intact will make accuracy reach 50% at the right first step and reaches 90%+ immediately that doesn't make sense.
Should I exclude some data of classes 1 and 2, to make the numbers of samples of 3 classes equal? But it's not a real-world ratio.
","['classification', 'training', 'data-preprocessing', 'supervised-learning', 'imbalanced-datasets']","You can use stratified cross-validation combined with an imbalanced learning technique applied to the training data. Stratification ensures that when you split your data into train and test, the ratio of frequencies between the classes will stay the same, and therefore the test data will always be ""realistic"".However, when training a model (using only the training data, of course), the imbalance may have a negative impact. Therefore, have a look at some of the imbalanced learning techniques that are out there to remedy this situation. For example, you could try these:etc.You should also take care about the metrics you use to assess predictive performance on the test data. Accuracy could be misleading here, so you may instead find metrics like sensitivity and specificity (calculated for each class individually) more informative."
Using states (features) and actions from a heuristic model to estimate the value function of a reinforcement learning agent [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



new to RL here.
As far as i understood from RL courses, that there is two sides of reinforcement learning. Policy Evaluation, which is the task of knowing the value function for certain policy. and Control, which is maximizing the reward or the value function. what if i have a heuristic agent that performs almost acceptable performance in an environment but i want to find a policy that tends to be the optimal policy, is there a way to cut the first half of the task by teaching the agent ? will be a side by side buffer of the (states, actions) be sufficient ?
","['reinforcement-learning', 'deep-rl', 'dqn', 'control-problem', 'policy-evaluation']","Not sure I fully understand your question, are you asking:If the first, I am not sure exactly what you are asking as most RL algorithm doesn't have an explicit policy evaluation phase and even for ones which does (policy iteration for example) you would be using the wrong policy to evaluate.If you are asking about the second one (warm start your agent) that's very much possible. You would do it differently based on the algorithm:Hope this helps, let me know if anything is unclear."
How to design fitness function for multiple objectives?,"
I am currently building a neural network with genetic algorithms that learns to fly a 2D drone to a target. My goal is that it achieves all tasks as fast as possible, but I want the drone to also fly stable and upright. The way I tried to calculate the fitness was to create a function that has the greatest value when the drone does everything I want right.
fitness += 1/distToTarget + cos(drone_angle)

My current inputs are:
difference_target_X
difference_target_Y
velocity_X
velocity_Y
angular_velocity (degree per second)
drone_angle     | = 0;     |_ = 90     _| = -90

The output (I don't think it is important but)
left_thruster_angle
left_thruster_boost
right_thruster_angle
right_thruster_boost

The NN is programmed in unity and the drone uses a 2D rigid body and the NN adds a force to the thruster at the right angle.
How do I get the drone to set the best weights to fulfill all tasks: fly stable, fly fast, fly to the target?
","['neural-networks', 'genetic-algorithms', 'fitness-functions']","Your fitness function has two objectives that are added together, but they are not necessarily on the same scale. The component cos(drone_angle) must have a value from 0..1. The component 1/distToTarget will have a range that depends on how you measure distToTarget; e.g. if distToTarget has a range 0..1000, then this part of the fitness function will always be small far from the target (e.g. distance of 500) and massive when it gets very close (e.g 0.1 distance from the target). So the contribution of both components may not always be equal. Another potential complication is that the cosine function is nonlinear and make a very rapid transition between 0 and 1, as opposed to a smooth transition.I recommend reworking the fitness function to make the two components more equal, e.g. something likeIn this function that should be minimised to zero, both components contribute linearly to the final fitness, both components have a range of 0..1, and you can tune w1 and w2 to give different importance to different components depending on your preference (e.g. you may prefer w1>w2 if staying upright is the most important)."
Why is loss displayed as a parabola in mean squared error with gradient descent?,"
I'm looking at the loss function: mean squared error with gradient descent in machine learning. I'm building a single-neuron network (perceptron) that outputs a linear number. For example:
Input * Weight + Bias > linear activation > output.
Let's say the output is 40 while I expect the number 20. That means the loss function has to correct the weights+bias from 40 towards 20.
What I don't understand about mean squared error + gradient descent is: why is this number 40 displayed as a point on a parabola?
Does this parabola represent all possible outcomes? Why isn't it just a line? How do I know where on the parabola the point ""40"" is?

","['objective-functions', 'gradient-descent']","Mean Square Error (MSE) is a quadratic function and the further you go away from your optimum the bigger (quadratic) the MSE gets. Take $o_{expected}=20$ and $o_{net}=40$ as example. Your MSE is then 400, because
$MSE = (o_{expected}-o_{net})^2$.Just imagine $y = x^2$ with x being the output of your network.
If you want to shift the parabola with optimum at $20$ the formula you get is $y = (20-x)^2$.
For every new case you train the  net on, you get a different parabola with different parameters."
What is a Hebbian linear classifier?,"
I was reading Deep Learning of Representations for Unsupervised and Transfer Learning,
and they state the following:

They have only a small number of unlabeled examples (4096) and very few labeled examples (1
to 64 per class) available to a Hebbian linear classifier (which discriminates according to
the median between the centroids of two classes compared) applied separately to each class
against the others.

I have searched about what a Hebbian linear classifier is, but I couldn't find more than an explanation about what Hebbian learning is, so can anybody explain what Hebbian linear classifier is?
","['machine-learning', 'deep-learning', 'papers', 'hebbian-learning']",
How to source training data in ML for information security?,"
A company entrusts a Data Scientist with the mission of processing and valuing data for the research or treatment of events related to traces of computer attacks. I was wondering how would he get the train data.
I guess he would need to exploit the logs of the different devices of the clients and use statistical, Machine Learning and visualization techniques in order to bring a better understanding of the attacks in progress and to identify the weak signals of attacks... But how would he get labelled data?
He might get the logs of attacks received before, but that might not have the same signature with the attacks that are going to come later? So it might be difficult to create a reliable product?
","['training', 'datasets', 'resource-request', 'ai-security', 'training-datasets']","You have implicitly assumed that supervised learning is being used, given the assumption that labels are needed. But this might lead to the following potential problems:I think a far easy way to approach these kinds of problem would be unsupervised learning: model normal patterns and behaviours in the logs, and then flag any deviations from normality. It may be an attack or it may be new normal behaviour. In the latter case, the model can be updated. There are various approaches here such as clustering, outlier detection and possibly self-supervised learning that might be useful. Dimensionality techniques might also be useful to visualise clusters of ""normal"" behaviour that can be compared to abnormal patterns."
"What does ""semantic gap"" mean?","
I was reading DT-LET: Deep transfer learning by exploring where to transfer, and it contains the following:

It should be noted direct use of labeled source domain data on a new scene of target domain would result in poor performance due to the semantic gap between the two domains, even they are representing the same objects.

Can someone please explain what the semantic gap is?
","['machine-learning', 'terminology', 'papers', 'transfer-learning']","In terms of transfer learning, semantic gap means different meanings and purposes behind the same syntax between two or more domains. For example, suppose that we have a deep learning application to detect and label a sequence of actions/words $a_1, a_2, \ldots, a_n$ in a video/text as a ""greeting"" in a society A. However, this knowledge in Society A cannot be transferred to another society B that the same sequence of actions in that society means ""criticizing""! Although the example is very abstract, it shows the semantic gap between the two domains. You can see the different meanings behind the same syntax or sequence of actions in two domains: Societies A and B. This phenomenon is called the ""semantic gap""."
Identifying if a model is over or under-fitting via graphs,"
I am working on a Neural Network and have plotted the performance of my model. However the plots seem not to fit the ""trends"" (which help you identify the issue with your model) presented in this illustration.
Here is the performance of my model
The loss metric I used was Binary Cross Entropy (due to my problem being a binary classification task).
Is my model over or under-fitting? and how can you tell?
","['neural-networks', 'machine-learning', 'overfitting', 'performance', 'underfitting']",
Comparing heuristics in A* search and rescue operation,"
I was reading a research paper titled A Comparative Study of A-star Algorithms for Search and rescue in Perfect Maze (2011).
I have some doubts regarding it:
1.

The Evaluation Function of $\mathrm{A}^{*}(2)[5]$ is:
$$
f_{2}(i)=g_{2}(i)+h_{2}(i)+h_{2}(j)
$$
Where, $j$ is the father point of the current point, $h_{2}(j)$ is the Euclidean distance from the father point of the current point to the target point. This term is added to the father point for improving the search speed because it reduces the number of nodes.

In this section (page 2 middle-right) it says that the father point is added to improve search speed as it reduces the number of nodes searched.
Is this because the added father point in some way overestimates the cost function, similar to Greedy Best First Search. Can it be interpreted as something between $A^{*}$ and Greedy BFS? If not what is the reason for the increase in speed?
2.

$\mathrm{A}^{*}(3)$ that employed a heuristic function with angle and distance has not been demonstrated well in this experiment, the reason is: in this experiment, we have added deviations not only on distance but also on an angle, so the $A^{*}(3)$ algorithm has no advantage in this searching.

In this section (page 3 upper-right) it is saying that $\mathrm{A}^{*}(3)$ is not so useful as there are deviations in angle also. What does this statement mean how are deviations in angle added? Request help in understanding $\mathrm{A}^{*}(3)$?
I need to understand why one heuristic is better than another. Is there some way to determine that apart from experimental evidence?
","['comparison', 'papers', 'a-star', 'path-finding', 'heuristic-functions']",
Could the inputs of the mean squared-error loss function be transformed to allow larger learning rates?,"
In the context of a neural network $\hat{y} = f_\theta(\mathbf{x})$ with parameters $\theta$ that is trained to perform regression such that the prediction $\hat{\mathbf{y}} = [\hat{y}_1,\hat{y}_2,...,\hat{y}_N]$ is close the target $\mathbf{y} = [y_1,y_2,...,y_N]$, the mean squared-error (MSE) loss function is:
$$
\mathcal{L}(\mathbf{y},\hat{\mathbf{y}}) = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2
$$
The parameters $\theta$ are then adjusted using the gradient descent update rule:
$$
\theta_{k+1} \leftarrow \theta_{k} - \alpha \cdot \nabla \mathcal{L}(\mathbf{y},\hat{\mathbf{y}}_{\theta_k})
$$
Where $\alpha$ is the learning rate. I am aware that if $\alpha$ is too small, the parameters $\theta$ might never converge, or is too slow to converge, to the optimal set of parameters $\theta^*$, and if $\alpha$ is too large, the iterates of $\theta$ could oscillate and also never converge. My question has to do with the latter scenario, where $\alpha$ is too big, which leads to overshooting and oscillation.
A good way of choosing $\alpha$ is using backtracking line search. However, because the neural network has many parameters, it is not practical to perform line search, and $\alpha$ needs to be chosen using another way.
Is it possible to allow a larger value of the learning rate before overshooting and oscillation by ""elongating"" valleys in the MSE loss function $\mathcal{L}(\mathbf{y},\hat{\mathbf{y}})$? More precisely, by transforming $\mathbf{y}$ and $\hat{\mathbf{y}}$ in some way before computing the loss? For example, in practice, I have found the following modification to the MSE loss function to be very helpful in avoiding overshooting and oscillation, even with large learning rates:
$$
\mathcal{L}(\mathbf{y},\hat{\mathbf{y}}) = \frac{1}{N} \sum_{i=1}^N (\log(y_i + \epsilon) - \log(\hat{y}_i + \epsilon))^2
$$
Where $\epsilon$ is a small value. However, I am not sure why this modification helps.
","['neural-networks', 'objective-functions', 'math', 'gradient-descent', 'loss']",
What are the math theorems regarding the Multilayer Perceptron?,"
I've come across a theorem ""Convergence theorem
Simple Perceptron"" for the first time, here-> https://zaguan.unizar.es/record/69205/files/TAZ-TFG-2018-148.pdf, page 27, (is in Spanish)
Are there others like this one but for the Multilayer Perceptron?
Could someone please point me out to them?
Thank you in advance.
","['neural-networks', 'reference-request', 'math', 'multilayer-perceptrons', 'perceptron']",
"Are there architectures to generate pictures from four labels? (VAEs, GANs)","
I want to try something with image creation via NNs. I have come across Variational Autoencoders and Generative Adversarial Networks as possible solutions but have only found image creatinon with CIFAR-100 and GANs (two class-labels, non continuous)
Im looking for an idea for generating a picture with four continuous labels (Age and Rotation around X-, Y-, Z-axis). Is there anything usable for this?
Im especially looking for an VAE-Model on this task.

","['generative-adversarial-networks', 'variational-autoencoder']",
Does Yann LeCun consider k-means self-supervised learning?,"
I was discussing the topic of self-supervised learning with a colleague. After a while we realized we were using different definitions. That's never helpful.
Both of us were introduced to self-supervised learning by reading or listening to Yann LeCun. He is renaming (part of) unsupervised learning to self-supervised learning. For example in this Facebook post.
Probably the definitions of unsupervised and self-supervised learning overlap. But to me the terms are not interchangeable. For example, a prototypical example of age-old unsupervised learning technique is k-means. To me that is unsupervised but not self-supervised learning.
Is Yann LeCun renaming the entire concept unsupervised learning to self-supervised learning? More specifically, is his opinion that we should call clustering and anomaly detection self-supervised learning? And in the limit, does he call k-means supervised-learning?
References are appreciated.
","['terminology', 'unsupervised-learning', 'self-supervised-learning']",
What's the difference between a 1d tensor and a 2d tensor with 1 dimension?,"
I'm doing a TensorFlow tutorial, where they convert an array of the numbers [1,2,3] to a tensor like this:
const xs = tf.tensor2d([1, 2, 3], [3, 1])

The shape is [3,1] because there is one row with 3 numbers.
My question is, why would they use a 2D tensor, isn't this just exactly the same as:
const xs = tf.tensor1d([1, 2, 3])

","['machine-learning', 'tensorflow', 'math', 'linear-algebra']","The required shape of the tensor $T$ depends on the shape of other tensors that are involved in the same operations of that same tensor $T$ and the required/desired shape of the resulting tensor, in the same way that the number of columns of the matrix $M \in \mathbb{R}^{n \times m}$ needs to match the number of rows of the matrix $M' \in \mathbb{R}^{n' \times m'}$ when you perform the matrix multiplication $M M'$, i.e. in order for $M M'$ to be well-defined, $m = n'$.So, in your case, although the tensors contain the same elements, it might not be possible to use both in the same operations. Without more context/details, I cannot specifically answer why the first tensor was used in the tutorial you're mentioning."
"Is this a supervised or reinforcement learning problem, and which algorithm should I use to solve it?","
I have a time series data with a little unusual cost/reward function (I haven't seen it before)
The model must predict a $Y$ value for any $X(t)$.
The reward is computed as follows. The model will receive a reward equal to $Y_\text{true} * Y_\text{prediction}$. But if the reward is a positive value, the model won't receive a positive reward in next $5$ time steps (it will get negative rewards anytime). It means sometimes it is better for the model to predict 0 and wait for a better reward.
I have two questions:

Is it a supervised learning or reinforcement learning problem?

If it is a supervised learning, which optimization method should I use for it?


","['reinforcement-learning', 'supervised-learning', 'algorithm-request']",
Trying to understand why nonlinearity is important for neural networks by analogy,"
Is the reason why linear activation functions are usually pretty bad at approximating functions the same reason why combinations of hermitian polynomials or combinations of sines and cosines are better at approximating a function than combinations of linear functions?
For example, regardless of the amount of terms in this combination of linear functions, the function will always be some form of $y = mx + b$. However, if we're summing sines, you absolutely cannot express a combination of sines and cosines as something of the form $A \sin{bx}$. For example, a combination of three sinusoids cannot be simplified further than $A \sin{bx} + B \sin{cx} + D \sin{ex}$.
Is this fact essentially why the Fourier series is able to approximate functions (other than obviously the fact that $A \sin{bx}$ is orthogonal to $B \sin{cx}$)? Because if it could be simplified into one sinusoid, it could never approximate an arbitrary function because it's lost its robustness? Because with other terms combined, whereas linear functions summed up gain no further ability to approximate, things like sinusoids actually begin to approximate really well with enough terms and with the right constants.
In that vein, is this the reason why non-linear activiation functions (also called non-linear classifiers?) are generally valued more than linear ones? Because linear activation functions simply are lousy function approximators, while, with enough constants and terms, non-linear activation functions can approximate any function?
","['activation-functions', 'function-approximation']",
Object Detection: Can I modify this script to support larger images (Scaled YOLOv4)?,"
I am looking at training the Scaled YOLOv4 on TensorFlow 2.x, as can be found at this link. I plan to collect the imagery, annotate the objects within the image in VOC format, and then use these images/annotations to train the large-scale model. If you look at the multi-scale training commands, they are as follows:
python train.py --use-pretrain True --model-type p5 --dataset-type voc --dataset dataset/pothole_voc --num-classes 1 --class-names pothole.names --voc-train-set dataset_1,train --voc-val-set dataset_1,val  --epochs 200 --batch-size 4 --multi-scale 320,352,384,416,448,480,512 --augment ssd_random_crop

As we know that Scaled YOLOv4 (and any YOLO algorithm at that) likes image dimensions divisible by 32, I have plans to use larger images of 1024x1024. Is it possible to modify the --multi-scale commands to include larger dimensions such as 1024, and have the algorithm run successfully?
Here is what it would look like when modified:
--multi-scale 320,352,384,416,448,480,512,544,576,608,640,672,704,736,768,800,832,864,896,928,960,992,1024

","['deep-learning', 'object-detection', 'yolo', 'scalability']","Yes, the functionality should is there. But, don't you think you are overdoing the scales. You have at least 18 scales mentioned here. Too much of anything is bad. There is a reason it likes things divisible by 32 because at that increase in size something more meaningful will show up in the image. Spamming sizes like this won't help you at all, it would rather waste your time."
Why can a neural network use more than one activation function?,"
From trying to understand neural networks better, I've come upon a tentative notion that an activation function aims to build a function it's approximating via linear combinations with biases and weights as their constants, like Fourier sums and other orthogonal basis functions.
How, then, can one neural network layer use activation function, like a sigmoid, and another one like the output using softmax? How do we know a linear combination of sigmoids and something else can still build that function no matter what? To me, it's like saying a function is approximated using sine functions with $N$ different $k$ values and then also randomly a few Hermite polynomials are thrown in as well. In this case, Hermite polynomials and the sine function aren't even orthogonal (to be honest I haven't checked but I'd assume they're not).
This question highlights some misconceptions I have about activation functions, perhaps, and I'd like to know where I'm going wrong here.
","['math', 'activation-functions', 'function-approximation', 'universal-approximation-theorems']",
What are the pros and cons of using sigmoid or softmax approach when dealing with 2 classes?,"
I know that when using Sigmoid, you only need 1 output neuron (binary classification) and for Softmax - it's 2 neurons (multiclass classification). But for performance improvement (if there is one), is there any difference which of these 2 approaches works better, or when would you recommend using one over the other. Or maybe there are certain situations when using one of these is better than the other.
Any comments or shared experience will be appreciated.
","['neural-networks', 'machine-learning', 'deep-learning', 'activation-functions']",
"In variational autoencoders, why do people use MSE for the loss?","
In VAEs, we try to maximize the ELBO = $\mathbb{E}_q [\log\ p(x|z)] + D_{KL}(q(z \mid x), p(z))$, but I see that many implement the first term as the MSE of the image and its reconstruction. Here's a paper (section 5) that seems to do that: Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse (2019) by James Lucas et al. Is this mathematically sound?
","['objective-functions', 'autoencoders', 'variational-autoencoder', 'mean-squared-error', 'evidence-lower-bound']","If $p(x|z) \sim \mathcal{N}(f(z), I)$, then\begin{align}
\log\ p(x|z) 
&\sim \log\ \exp(-(x-f(z))^2) \\
&\sim -(x-f(z))^2 \\
&= -(x-\hat{x})^2,
\end{align}where $\hat{x}$, the reconstructed image, is just the distribution mean $f(z)$.It also makes sense to use the distribution mean when using the decoder (vs. just when training), as it is the one with the highest pdf value. So, the decoder produces a distribution from which we take the mean as our result."
Decision boundary figure in Least square GAN paper,"
I currently reading Least Square GAN paper. But, I cannot interpret one of its figures. .
Explanation of the figure goes like this:

Figure 1: Illustration of different behaviors of two loss functions. (a): Decision boundaries of two loss functions. Note that the decision boundary should go across the real data distribution for a successful GANs learning. Otherwise, the learning process is saturated. (b): Decision boundary of the sigmoid cross entropy loss function. It gets very small errors for the fake samples (in magenta) for updateing G as they are on the correct side of the decision boundary. (c): Decision boundary of the least squares loss function. It penalize the fake samples (in magenta), and as a result, it forces the generator to generate samples toward decision boundary.

I already knew the vanilla GAN structure. But I could not understand why decision boundary looks like this. Any help will be appreciated
","['deep-learning', 'generative-adversarial-networks', 'generative-model']",
How does the Alpha Zero's move encoding work?,"
I am a beginner in AI. I'm trying to train a multi-agent RL algorithm to play chess. One issue that I ran into was representing the action space (legal moves/or honestly just moves in general) numerically. I looked up how Alpha Zero represented it, and they used an 8x8x73 array to encode all possible moves. I was wondering how it actually works since I got a bit confused in their explanation:

A move in chess may be described in two parts: selecting the piece to move, and then selecting among the legal moves for that piece. We represent the policy $\pi(a \mid s)$ by a $8 \times 8 \times 73$ stack of planes encoding a probability distribution over 4,672 possible moves. Each of the $8 \times 8$ positions identifies the square from which to ""pick up"" a piece. The first 56 planes encode possible ""queen moves"" for any piece: a number of squares $[1..7]$ in which the piece will be moved, along one of eight relative compass directions {N, NE, E, SE, S, SW, W, NW}. The next 8 planes encode possible knight moves for that piece. The final 9 planes encode possible under-promotions for pawn moves or captures in two possible diagonals, to knight, bishop or rook respectively. Other pawn moves or captures from the seventh rank are promoted to a queen.

How would one numerically represent the move 1. e4 or 1. NF3 (and how would the integer for 1. NF3 differ from 1. f3) for example? How do you tell what integer corresponds to which move? This is what I'm essentially asking.
","['reinforcement-learning', 'alphazero', 'chess', 'multi-agent-systems', 'action-spaces']","Let's do the code, so all the details are down.You'll see that the codes dictionary will have 56 entries in it for each (nSquares,direction) pair.The knight moves we'll encode as the long ""two""-cell edge move first and the short ""one""-cell edge second:Now we should have 64 codes. As I understand, the final 9 moves are when a pawn reaches the final rank and chosen to be underpromoted. It can reach teh final rank either by moving N, or by capturing NE, NW. Underpromotion is possible to three pieces. Writing the code:We get 73 codes as described.The distribution over actions is a (8,8,73) tensor (it is not formally a ""policy"", since policy should also depend on state, but lets cut this corner for this discussion):Let's also do codes for columns for convenience:How would one numerically represent the move 1. e4The first two dimensions choose the figure you are moving. So, that'd be e2 pawn. And we move north 'N' by 2 cells.So, we put 1 into the tensor at the appropriate indices. Note that you have to subtract 1 from the row index, to make it zero-based.How would one numerically represent the move 1. NF3The first two dimensions choose the figure you are moving. So, that'd be g1 knight. And we perform north-west N,W knight jump.Generally, the policy is a probability distribution over all possible moves, so the policy tensor would have several non-zero probability values in it. For example an opening policy that does 1.e4 or 1.Nf3 with 50/50 probability would be:Hope this clears things up."
How can the gradient of the weight be calculated in the viewpoint of matrix calculus?,"
Let $\sigma(x)$ be sigmoid function. Consider the case where $\text{out}=\sigma(\vec{x} \times W + \vec{b})$, and we want to compute $\frac{\partial{\text{out}}}{\partial{w}
}.$
Set the dimension as belows:
$\vec{x}$: $(n, n_{\text{in}})$, $W$: $(n_{\text{in}}, n_{\text{out}})$, $\vec{b}$: $(1, n_{\text{out}})$.
Then $\text{out}$ has the dimension $(n, n_{\text{out}})$. So we need to calculate the matrix by matrix derivative, as I know there is no such way to define that.
I know that finally it is calculated as $\vec{x}^T \times (\text{out}\cdot(1-\text{out}))$.
But I can't still get the exact procedure of calculation, why it should be $\vec{x}^T \times (\text{out}\cdot(1-\text{out}))$, not $(\text{out}\cdot(1-\text{out})) \times \vec{x}^T$,I know it by considering dimension, but not by calculation.
My intuition about this problem is that all calculation can be considerd as vector by vector differentiation since $n$ is a batch size number, we can calculate matrix differentiation by considering each column vector.
I'm not sure about my intuition yet, and I need some exact mathematical calculation procedure for the problem,
","['gradient-descent', 'linear-algebra', 'calculus']",
Does a differential evolution algorithm mutate its population during a generation?,"
I'm implementing a differential evolution algorithm and when it comes to evolving a population, the page I am referencing is vague on how the new population is generated.
https://en.wikipedia.org/wiki/Differential_evolution#Algorithm
The algorithm looks like the population is mutated during evolution.
# x, a, b, c are agents in the population
# subscriptable to find position in specific dimension
# pop is filled with Agent objects
for i, x in enumerate(pop):
  [a, b, c] = random.sample(pop[:i]+pop[i+1:], 3)
  ri = random.randint(0, len(x))
  new_position = []
  for j in range(len(x)):
    if random.uniform(0, 1) < CR or j == ri:
      new_pos.append(a[j] + (F * (b[j] - c[j])))
    else:
      new_pos.append(x[j])
  # Agent() class constructor for agent, takes position as arg
  new_agent = Agent(new_pos)
  if fitness(new_agent) <= fitness(x):
    pop[i] = new_agent # replace x with new_agent

But I wonder if instead it means a new population is made and then populated iteratively:
new_pop = []
for i, x in enumerate(pop):
  [a, b, c] = random.sample(pop[:i]+pop[i+1:], 3)
  ri = random.randint(0, len(x))
  new_position = []
  for j in range(len(x)):
    if random.uniform(0, 1) < CR or j == ri:
      new_pos.append(a[j] + (F * (b[j] - c[j])))
    else:
      new_pos.append(x[j])
  new_agent = Agent(new_pos)
  if fitness(new_agent) <= fitness(x):
    new_pop.append(new_agent)
  else:
    new_pop.append(x)
pop = new_pop

Note new_pop is made, and filled with agents as the for loop continues.
The first allows previously evolved agents to be used again in the same generation; in other words, the population is changed during the evolution. The second doesn't allow updated agents to be re-used, and only at the end is the original population changed.
Which is it?
","['search', 'evolutionary-algorithms', 'meta-heuristics', 'pseudocode']","Quoting the original paper:For each target vector $x_{i,G}$ ,a mutant vector is generated according to
$$ v_{i,G+1} = x_{r_1,G} + F\left(x_{r_2,G} + x_{r_3,G}\right)$$And laterTo decide whether or not it should become a member of generation $G + 1$, the trial vector $v_{i,G+1}$ is compared to the target vector $x_{i,G}$ using the greedy criterion.I'd say it is pretty unambiguous that author's intent was to split evolving agents' populations into ""generations"" indexed by $G$. A new generation $G+1$ is created by applying the DE algorithm to the previous generation $G$. No agents are changed within the generation $G$.So, it looks like the answer to your question is ""no"" - the population of the current generation $G$ is not mutated. Your second approach is the correct one."
Open AI Taxi - Agent fails to learn an effective policy,"
I'm trying to solve the openai gym taxi problem (v3) using deep q learning.  I've already had some success with the q-table approach, but for the life of me cannot manage to train a NN to learn a reasonable action policy.  I'm doing the training using an AWS p3.2xlarge instance.
My approach is fairly straightforward, I set up the environment and agent, then run the training loop.
My code more or less looks like this:
import gym
from taxi_agent import Agent

env = gym.make('Taxi-v3').env
optimizer = Adam(learning_rate=0.001)
agent = Agent(env, optimizer)

batch_size = 32
num_of_episodes = 200
timesteps_per_episode = 120

The agent was cobbled together from various examples online:
import numpy as np
import random
from IPython.display import clear_output
from collections import deque
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense, Embedding, Reshape
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard


class Agent:
    def __init__(self, environment, optimizer):
        
        # Initialize atributes
        self._state_size = environment.observation_space.n
        self._action_size = environment.action_space.n
        self._optimizer = optimizer
        
        self.expirience_replay = deque(maxlen=2000)
        
        # Initialize discount and exploration rate
        self.gamma = 0.6
        self.epsilon = 0.5
        
        # Build networks
        self.q_network = self._build_compile_model()
        self.target_network = self._build_compile_model()
        self.align_target_model()

        #: Set up some callbacks
        self.checkpoint_filepath = 'checkpoints/'
        model_checkpoint_callback = ModelCheckpoint(
            filepath=self.checkpoint_filepath,
            save_weights_only=True,
            save_freq='epoch'
        )
        # tensorboard_callback = TensorBoard('logs', update_freq=1)
        self.model_callbacks = [model_checkpoint_callback]
        
        self.history = []

    def store(self, state, action, reward, next_state, terminated):
        self.expirience_replay.append((state, action, reward, next_state, terminated))
    
    def _build_compile_model(self):
        model = Sequential()
        model.add(Embedding(self._state_size, 10, input_length=1))
        model.add(Reshape((10,)))
        model.add(Dense(48, activation='tanh'))
        model.add(Dense(24, activation='tanh'))
        model.add(Dense(self._action_size, activation='linear'))
        
        model.compile(loss='mse', optimizer=self._optimizer)
        return model

    def restore_weights(self):
        path = self.checkpoint_filepath
        print(f""restoring model weights from {path}"")
        self.q_network.load_weights(path)

    def align_target_model(self):
        self.target_network.set_weights(self.q_network.get_weights())
    
    def act(self, state, environment):
        if np.random.rand() <= self.epsilon:
            return environment.action_space.sample()
        
        q_values = self.q_network.predict(state)
        return np.argmax(q_values[0])

    def retrain(self, batch_size, epochs=1):
        minibatch = random.sample(self.expirience_replay, batch_size)
        
        for state, action, reward, next_state, terminated in minibatch:
            
            target = self.q_network.predict(state)
            
            if terminated:
                target[0][action] = reward
            else:
                t = self.target_network.predict(next_state)
                target[0][action] = reward + self.gamma * np.amax(t)
            
            history = self.q_network.fit(state, target, epochs=1, verbose=0, callbacks=self.model_callbacks)
            self.history.append(history.history)

The training loop uses the agent to act in the environment up to a number of batch_size actions.  Next, it retrains the model based on a random sample of the experience for every subsequent timestep.
I have it set to print out feedback whenever the environment terminates (achieves the objective).  In practice this never happens.
I've reloaded trained models from weights and trained for cumulative 24 hours without much success.  I've also tried silly things like updating the target network after N steps just so it learns something - no luck.
If I try to use my trained model to solve an example env instance, it just wants to move south other than the random actions it set to do 50% of the time.
It would be great it someone could give me some advice towards what to try next.  I can keep playing around with hyperparameters but I don't have the best intuition around where to optimize my efforts.
iterations = 0
state = env.reset()
env.render()
while not terminated:
    state = np.reshape(state, [1, 1])
    action = agent.act(next_state, env)
    next_state, reward, terminated, info = env.step(action) 
    next_state = state
    if iterations % 10: env.render()
    iterations += 1
    if iterations > 1000: break

","['reinforcement-learning', 'open-ai', 'gym']",
Semantic segmentation - background or ignore for non-target classes?,"
I am training a deep learning model for semantic segmentation. I am using the cityscapes dataset for training/evaluation.
In cityscapes, there are 34 classes, and of which, we consider only 19 classes and the rest of the classes are ignored. For training, I have assigned the 19 classes with 0-19 traid_ids.
Now, since the rest of the classes are ignored, I have ignored them when computing the loss using cross enropy with ignore_index=255.
But, the above effect can also be achieved by assigning a background class, i.e 20 as bg class and assign all the ignored classes to it.
Now my question is, which method would be better to achieve a high mIoU in cityscapes? And what would be your intuition in choosing the approach?
","['image-segmentation', 'semantic-segmentation']",
"What part of the Vaswani et al. is the ""transformer""?","
Which part of this is the transformer?

Ok, the caption says the whole thing is the transformer, but that's back in 2017 when the paper was published. My question is about how the community uses the term ""transformer"" now.
I'm not looking for an inline response to these questions. They are all a way of asking the same general thing.

Is this whole thing a transformer?
What parts or what relationships between parts make it a transformer?
Equivalently, what aspects can I change before it becomes something else and not a transformer?
If I only care about self-attention I suppose I don't need the right hand column. If I just keep the self-attention, is it still a transformer?

Context about me is I've just become familiar with transformers and have not read much literature on them since this paper.
","['neural-networks', 'deep-learning', 'terminology', 'transformer']",
"In variational autoencoders, what does p(x|z) mean?","
If $x \sim \mathcal{N}(\mu,\,\sigma^{2})$, then it is a continuous variable, and therefore $P(x) = 0$ for any x. One can only consider things like $P(x<X)$ to get a probability greater than 0.
So what is the meaning of probabilities such as $P(x|z)$ in variational autoencoders? I can't think of $P(x|z)$ as meaning $P(x<X|z)$, if $x$ is an image, since $x<X$ don't really make sense (all images smaller than a given one?)
","['papers', 'variational-autoencoder', 'notation']","Whilst you're right that for any continuous distribution $P(X = x) = 0 \;; \forall x \in \mathcal{X}$ where $\mathcal{X}$ is there support of the distribution, they are not referring to probabilities here, rather they are referring to density functions (though this should really be denoted with a lower case $p$ to avoid confusion such as this).$p(x|z)$ is a conditional distribution, which is also allowed in the continuous case -- you can also 'mix and match', i.e. $x$ could be continuous and $z$ could be discrete, and vice-versa.In the paper, all the authors are meaning when they write $p(x|z)$ is the density of $x$ conditioned on $z$; in VAE's with an image application this is the conditional density of the image $x$ given your latent vector $z$."
"Is the bias also a ""weight"" in a neural network?","
I'm learning about how neural networks are trained. I understand how a neuron works, backpropagation, and all that. In neurons, there is a clear distinction between a ""weight"" and a ""bias"".
$$
Y= \sigma(\text{weight} * \text{input})+ \text{bias}
$$
However, all the sources I've found when you train the network you just adjust the weights. Not the bias.

However, they never mention what the bias should do, which leads me to think that you just merge all weights and biases in a $W$ vector and call it weights, even though there are also biases. Is that correctly understood?
","['neural-networks', 'deep-learning', 'terminology', 'backpropagation', 'weights']","Yes, it is not unusual to omit the bias by adding a neuron which always outputs a constant 1, which will then be multiplied by an appropriate weight to give the same formula as you would get using an explicit bias.One notable text using this convention is Understanding Machine Learning: From Theory to Algorithms by  Shai Shalev-Shwartz and Shai Ben-David. In section 20.1 there is a diagram of a neural network where a neuron outputting a constant value is added to each layer which you might find helpful.To understand why this works, suppose the outputs of the previous layer are $u_1, \dots, u_n, u_{n + 1}$, where $u_{n + 1}$ is always $1$. Then a neuron in the next layer (without a bias) computes$$ \sigma\left(\sum_{i = 1}^{n + 1} w_i u_i \right) = \sigma\left(\sum_{i = 1}^n w_i u_i + w_{n + 1}\right),$$
where $\sigma$ is the activation function. So, the weight $w_{n + 1}$ just serves as the bias because it is multiplied by $u_{n + 1} = 1$."
Any RL approaches for this 2D space optimization problem?,"
I have a list of rectangles, they are in a certain order in 2D at the beginning. The task is to move them to get the boundary (rectangular) of the minimal area. It's OK to push off the dotted border as long as the area is minimal.
The starting state may look similar to this (top view):

Any reinforcement learning approaches to this problem? I'm thinking of some actions called 'rotate 90 degs', 'push east', 'push west', 'push south', 'push north', but these actions are still not clear how to be applied, which to push, how far to push.
The 2D state can be mapped to a grid of zeros (free) and ones (occupied) to utilize conv2D layers. Before feeding to conv2D, all rectangle coords should be translated to make the ($x_{min}$,$y_{min}$) be at the origin.
","['machine-learning', 'reinforcement-learning', 'convolutional-neural-networks', 'ai-design', 'optimization']",
What is the most statistically acceptable method for tuning neural network hyperparameters on very small datasets?,"
Neural networks are usually evaluated by dividing a dataset into three splits:

training,
validation, and
test

The idea is that critical hyperparameters of the network such as the number of epochs and the learning rate can be tuned by testing the network on the validation data while keeping the test data completely unseen until a final evaluation that happens only after the hyperparameters have been tuned.
However, if the amount of data is very small (e.g. 10-20 examples per class), then dividing the dataset into three splits may negatively impact the model due to lack of training data, and two splits is therefore preferable. A two split approach that makes a reasonable amount of data available for training is ten-fold stratified cross validation.
My question is -- is it statistically sound to tune hyperparameters by repeatedly evaluating hyperparameter sets using cross validation? Keep in mind that there is no held-out test data in this case, as the amount of available data is too small.  I'd like some evidence/citations if possible showing that specifically for small datasets, this is the best approach for estimating the best hyperparameters that lead to the best generalizable model. Or if there is another approach that is better, I'd like to learn about that too.
","['neural-networks', 'hyperparameter-optimization', 'hyper-parameters', 'testing', 'statistics']",
Does the policy iteration convergence hold for finite-horizon MDP?,"
Most RL books (Sutton & Barto, Bertsekas, etc.) talk about policy iteration for infinite-horizon MDPs. Does the policy iteration convergence hold for finite-horizon MDP? If yes, how can we derive the algorithm?
","['reinforcement-learning', 'markov-decision-process', 'convergence', 'policy-iteration']",
Why is the backpropagation algorithm used to train the multilayer perceptron?,"
I've read in the book Neural Network Design, by Martin Hagan et al. (chapter 11), that, to train the feed-forward neural network (aka multilayer perceptron), one uses the backpropagation algorithm.
Why this algorithm? Could someone please explain in simple terms and detailed terms?
","['neural-networks', 'training', 'backpropagation', 'feedforward-neural-networks', 'multilayer-perceptrons']",
How to explain that a same DNN model have radically different behaviours with each new initialization and training?,"
I'm trying to predict the continuous values of a variable $y$ using a Fully Connected Neural Network while providing it with data from a $(3300, 13)$ matrix $X$ where $X[i, :]=[0,...,1,...,0,x_{i}]$. So the first $12$ elements of a data vector are all zeros except for one element which is equal to $1$ to denote  the belonging of this data to a category. I'd like to add that my $X$ data is normalized with regard to the $13$-th column and that both $X$ and $y$ are shuffled in the same manner. Please find below my code for my model:
model = Sequential()
model.add(Input(shape=(13,)))
model.add(Dense(6, activation = 'relu'))
model.add(Dense(2, activation = 'relu'))
model.add(Dense(1))

model.compile(loss = 'mean_squared_error',
              optimizer = 'adam',
              metrics = ['RootMeanSquaredError'])

history = model.fit(X, y, validation_split = 0.1, epochs=64)

When trying to plot the learning curve using:
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('rmse')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

I get these curves:

There's already an ""unusual"" element to point here; I've noticed that throughout the training the loss decreases but sometimes in an oscillating manner but we don't notice that on the training learning curve.  For example the last four values of the loss are: $2.5176$, $3.4718$, $3.0704$ and it settles down on $3.8177$. I've also noticed that the losses provided by history.history are different than those shown during training, I suspect ones are computed before the epoch and ones after but I'm not sure.
I've tried to predict on the $275$ first elements of the training data. Most of the predictions took the value $4.2138872e+00$ but there are other predictions who took lesser values. I've computed the maximum of the predictions on the whole training set and it is $4.2138872e+00$.
I've also tried to train on the whole training set without a validation set to see what'll happen. I've made sure to rerun the cells of the model so that it doesn't take the weights it already found. I've noticed the same behaviour for the loss during training, but this time there is no constant predicted value that comes up as a maximum limit for the predictions.
I've already asked this question here and a user suggested to me that I should ask this question separately while providing the whole code. I ran the same code I was running and that was giving me the same predictions for no matter for my input vectors.
I think, as the user @Kostya that answered my previous question pointed out, what's happening here is called ""dying ReLus"". It's the same code that I'm running over and over but gives different predictions and the only random parameters are the weights and the biases. I'm sure the biases are initially initialized to zero but I don't know how the weights are handled. I suppose they're randomly generated by a centered and reduced normal distribution.
I have last came to this question: does the number of neurons, hence the number of weights influence the phenomena of ""dying ReLus"" ? I came to think that because if we had a large number of weights, their values are likely to fill that interval where the majority of the probability mass is concentrated. And since we have a small number of weights, we can get some ""outlier"" weights which lead to dyind ReLus.
","['regression', 'relu', 'weights-initialization', 'dense-layers', 'learning-curve']","I'm sure the biases are initially initialized to zero but I don't know how the weights are handled.Looking at the Dense layer docs: by default Dense layers biases are initialized with zeros (bias_initializer='zeros') and weights are initialized with Glorot uniform (kernel_initializer='glorot_uniform').... ""unusual"" element to point here;  I've noticed that throughout the training the loss decreases but sometimes in an oscillating manner.There's nothing unusual about the oscillations. Quite on the contrary - your curves are suspiciously too smooth.I've also noticed that the losses provided by history.history are differentYes, so this is a little gotcha in the keras implementation. For training loss, keras does a running average over the batches throughout an epoch, while for the validation loss it computes it after the epoch finishes. (link)That also explains why your validation loss starts slightly lower than the training one - it actually lags behind the training loss by $\sim1/2$  of an epoch.The fact that both losses stay almost equal suggests that your model don't really rely on training data for prediction - it got saturated (""dying ReLus"") almost instantly.Couple of suggestions that I can make:I don't see you setting up a learning rate. Try making it smaller (like, much smaller). And see if there's any difference.MSE is a nasty loss - especially at large values. Have you tried standardizing the target values?Does the number of neurons, hence the number of weights influence the phenomena of ""dying ReLus"" ?Yes, the less neurons you have the higher the chance that all of the neurons will die out. You can get some intuition about it on https://playground.tensorflow.org/ - following the link, I've got a relu network that trains reasonably well. Try increasing the learning rate and observe saturation in the neurons. Reduce the number of neurons and see how the whole net gets stuck."
Advantages of CNN vs. LSTM for sequence data like text or log-files,"
When do you tend to use CNN rather than LSTM (or the other way round) in classification or generation tasks of sequential data like text or log-data? What are the reasons for the decision and what does it depend on? Are there any papers or statistics that confirm this?
I'm thinking of data like Linux log entries or short sentence of length of less than 20 words/tokens.
Personally i would almost always use LSTM but I'm curious if CNN wouldn't be better in some cases, if its possible to implement them in a meaningful way. On short sentence there isn't much buffer to use CNN if i'm not mistaken.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'long-short-term-memory', 'text-classification']",
What are the consequences of layer norm vs batch norm?,"
I'll start with my understanding of the literal difference between these two. First, let's say we have an input tensor to a layer, and that tensor has dimensionality $B \times D$, where $B$ is the size of the batch and $D$ is the dimensionality of the input corresponding to a single instance within the batch.

Batch norm does the normalization across the batch dimension $B$
Layer norm does the normalization across $D$

What are the differences in terms of the consequences of this choice?
","['deep-learning', 'comparison', 'batch-normalization', 'layer-normalization']",
Expected behavior of adversarial attacks on deep NN?,"
I am trying adversarial attack (AA) for a simple CNNs. Instead of the clean image, my simple CNN is trained with attacked images as suggested by some papers. As the training goes on, I am not sure if the training is well going or something is wrong.
Here is what I observed:
When the epsilon value is large, the classification performance of the model from the adversarial training is low. I understand if the attacked image is given to the model, then the performance is poor. Although the model is from the adversarial training, because the epsilon is large, the model is poorly perform. However, when an clean image is given, the performance of the model is still low. Performance on the clean images are higher than the performance of the attacked images, but not as high as the baseline model without adversarial training.
So, I wonder if the adversarial training also degrades the performance of the model on the clean images. When I read papers, I only see the results on the adversarial Images, not clean images. If you have any experience, it will be very helpful to check if my training code is working well or not.
When the epsilon is very large, the accuracy of the model on clean image is around 15%. The model without the adversarial training is around 81%.
Some details.
I use PGD attack with 5-iterations and epsilon is one of eps = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.03, 0.05, 0.07]. Step size is eps/3. Only one epsilon is selected and the adversarial training is conducted. So there are 8 different models trained with different epsilons.  I use natural image Dataset.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'adversarial-ml']","This seems to be a known problem, and intuitively seems reasonable. You might be interested in the paper Adversarial Training Can Hurt Generalization.The authors suggest that this might be because training on the perturbed data requires the model to learn more robust features, which means more samples are required to obtain performance comparable to a model that is not adversarially trained.You could try collecting or generating additional samples to see if this leads to an improvement. They also mention that in their experiments on the MNIST dataset, using Xavier initialisation led to a significant benefit, so you could experiment with that too."
How to train an LSTM to classify based on rare historic event?,"
I want an LSTM to output one of two classes (Y, N), per frame, based on all the input so far.
My original inputs are very long (~100000 samples long, far more than a standard LSTM training can handle due to vanishing gradients).

If the last seen instance out of the tokens (A, B) was A, output Y.
If the last seen instance out of the tokens (A, B) was B, output N.
The very long sequence is guaranteed to start with either A or B.

If the sequence was short, this would be quite easy.
For example, the following top lines and bottom lines correspond to inputs and required outputs:
ABCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
YNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN

ACCCCCCCCCBCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCACCCCCCCCCCCCCCCCACCCCCCCC
YYYYYYYYYYNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNYYYYYYYYYYYYYYYYYYYYYYYYYY


Looks easy enough, just push batches comprised of chunks of the long sequence to the LSTM and have a coffee, right?
However, for my case, the available inputs are (A, B, C), of which (A, B) are extremely rare, meaning I can have batches comprised of 100% C's. The LSTM has no chance then, if not fed with some current state, telling it about the last A or B seen.
Unfortunately, this ""state"" is really something learned, and I can't just feed it as input AFAIK.
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
????????????????????????????????????????????????????????????????????


I am looking for a standard practice, or other references on how to train an LSTM or other RNN based model to be able to classify based on rare events far in history.
I hope this is clear, if not please ask and I will edit.

Please note that the data is labeled, and labeling can't be generated automatically for this task. The above is just an example for ease of understanding, the reality is more complicated.
","['deep-learning', 'reference-request', 'recurrent-neural-networks', 'long-short-term-memory', 'time-series']",
Showing first layer RGB weights similarly to AlexNet,"
I would like to show the RGB features learned in the first layer of a convolutional neural network similarly to this visualization of the same layer's features from AlexNet:

My learned weights are in the range [-1.1,1.1]. When I use imshow in python or imagesc in Matlab, the weight values are clipped to [0,1], leaving only positive weights intact, everything else black (obviously).
Negative weight values could be informative, so I don't want to clip them. Rescaling the weights to [0,1] works fine for grayscale features, but not for RGB features as it is unclear how negative values of a channel should be visualized. In the above picture 0 furthermore seems to map to the middle of the range (gray).
How are such RGB features visualized so that they look similarly to above AlexNet visualization?
(Sorry for the beginner's question.)
","['neural-networks', 'convolutional-neural-networks']",
How to handle equality constraints in the mutation operation of evolutionary algorithms?,"
I am new in evolutionary algorithms field. I have a chromosome of 6 variables (real variable), where the sum of these variables is equal to 1.
I am looking for mutation formulas that can generate a new chromosome    respecting the equality constraint: in my case, the sum of new chromosome should always equal to 1.
","['genetic-algorithms', 'evolutionary-algorithms', 'constraint-satisfaction-problems', 'mutation-operators']",
How is a ResNet-50 used for deep feature extraction?,"
I'm trying to implement the vehicle re-identification model described in https://arxiv.org/pdf/2004.06271.pdf.
My question focuses on Section 3.2 of the paper, which uses a ResNet-50 for deep feature extraction in order to generate discriminative features which can be used to compare images of vehicles by Euclidean distance for re-identification. It takes a 256x256x3 image as input.
My understanding of ResNet-50 is that its output is of the shape N, where N is the number of classes which an input image could be, and ground truth labels take the form of a one-hot encoding where the '1' value represents the node in the output layer which is associated with the given class.
I am therefore confused by the usage of ResNet-50 in a re-identification task in which the goal is to generate an array of discriminative features which can be compared by Euclidean distance. There is no discrete set of N classes, as the model should work on any of the infinite number of vehicles in the world.
What is the ground truth label in a ResNet-50 in the context of a re-identification task?
","['deep-learning', 'convolutional-neural-networks', 'feature-extraction']","The authors use so-called embeddings, it's a form to represent the images in some meaningful vector form.The procedure to get embedding as follows. First, keep in mind most of the popular convolutional net architectures starts with convolutional layers and then have few fully connected layers. Then do the following.In the case of resnet50, you will get a 2048-value float vector. The property of a neural network is that semantically close images usually have close representation on the last layers and you could use euclidian distance to measure the similarity of images in some sense (not really, but it's another long discussion)I don't know the paper, but I glimpsed through it. You could see in formula 4 the $x$ is embedding representation. Loss at formula 5 is cross-entropy after applying the last linear layer ($Wx + b$) and then softmax.As a side note, you could skip the first step completely and use pretrained weights as a starting point. I.e. in pytorch you could take pretrained on ImageNet classification weights as follows"
Clonal operator in Immune Clonal Strategy,"
I was reading about Immune Clonal Strategy, specifically about Monoclonal operator from Immunity clonal strategies, and it goes as follows:

Here $a_i $ is a point and $a_i = \{ x_1, x_2, \cdots, x_m \}$.
I do not understand what $I_i$ really is, It seems like just copy $a_i$ for $q_i$ times or something like that, can someone please explain to me what is really happened here?
","['machine-learning', 'papers', 'math', 'evolutionary-algorithms', 'biology']",
Relation between discounted MDP and stochastic shortest path problems in RL,"
I have been reading about discounted MDPs and Stochastic Shortest Path (SSP). I recently came to know (from a friend) that every discounted MDP can be converted to an equivalent SSP but not the other way around. Questions:

Is this claim true? Is the discount factor equal to 1 when the MDP is converted to an SSP?
More generally, what is the relationship between these two problem categories?

","['reinforcement-learning', 'markov-decision-process', 'discount-factor']",
Predict time series from initial non-time dependant parameters,"
I'm trying to create an algorithm (neural network) that is able to predict a time series from a set of different parameters that are not given through time. Let's say I have a plane flying under the following conditions:




Parameters
Value




Angle of attack
8 degrees


Lateral angle
12 degrees


Wind speed
-20 m/s


Plane speed
200 m/s




From this point, I would like to predict the translational velocities in x-y-z axis for the next 2-3 seconds.
In order to train my model, I have a data base with different initial situations (input) and different motions of the plane (desired output) linked to their initial situation. Therefore, I want to train my model to predict these motions mentioned before, based only on the initial situation described.
In other words, the basics of what I'm trying to do could be summed up as the following:
Parameters describing the initial situation -> Model -> Time series of translational velocities.
","['neural-networks', 'recurrent-neural-networks', 'prediction', 'time-series', 'ai-basics']",
Number of parameters in Keras/Tensorflow Dense layers [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I am a bit confused about how the number of parameters are calculated in Dense model for the Kera/Tensorflow.
For example, in the figure below I thought that both the statements were the same, but I found a different number of parameters for both.  In particular, I am talking about model.add(Dense(...)) command.

","['neural-networks', 'tensorflow', 'keras']","Check the documentation for Dense layer:Note: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 1 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).That is what happening in your first case - for input dimensions (4,1) you've got d0=4 and d1=1. So it creates a kernel of shape (1,32) that gets applied along the axis of dimension 4. That's why your output shape is (4,32) and you've got 32 weights + 32 biases = 64 parameters.In second case you've got a ""standard"" 32 * 4 fully-connected weight matrix + 32 biases = 160."
How to properly resume training of deep Q-learning network?,"
I'm currently training a deep q-learning network. Due to resource limitations, I am not able to train the model to the desired performance in one go. So what I'm doing now is training the model for a certain number of episodes and save the resultant model. Later, I will load up the previously saved model and resume training.
However, what I'm noticing is that when training resumes, the average rewards goes back to very low again, compared to what it achieved at the end of the previous training session. What I'm currently doing is to load up the previously saved model into the prediction and target models, and I keep all hyperparameters unchanged.

Is this behaviour expected?
If not, how do I properly resume training of a deep q-learning
network?
Do I start off with the epsilon value at the end of the
previous session, currently I reinitialize that as well?

","['reinforcement-learning', 'deep-learning', 'training', 'q-learning']","Do I start off with the epsilon value at the end of the previous session, currently I reinitialize that as well?You should probably re-start with $\epsilon$ at the value you left off at. Using high values of epsilon may cause the neural network to forget some of what it learned from close-to-optimal policies in favour of learning possibly useless values of states and actions that are not important to a more highly-trained agent.Also, you should either save, or wait to refill experience replay memory before restarting training updates to the neural network. Working from a small memory may also cause the neural network to overfit to specific samples and generalise less well - at least temporarily until the memory fills up again.From your description, I am assuming that you are monitoring the average reward per training episode. This is metric that is easy to collect, but that has a problem in off-policy RL. The problem is that you are seeing the results from your behaviour policy, not your learned policy. When using $\epsilon$-greedy in Q learning, with a high value of epsilon, then the behaviour policy is likely to perform badly.Is this behavior expected?I would expect it if you re-set $\epsilon$ to a high value.If not, how do I properly resume training of a deep q-learning network?I recommend that you look at the following things:Re-start with $\epsilon$ at or close to where you left off. Perhaps allow starting epsilon to be passed in to the script as an argument.Measure performance of the greedy policy at checkpoints - e.g. run 100 test episodes at the end of every 500 training episodes - and use plots of that to decide how well your agent is performing. You can still plot training performance, but it should not be your guide to how well the agent is learning.Save experience replay so far at checkpoints too, and reload it on re-start. Alternatively, allow for experience replay to fill significantly before allowing update steps in the training loop.You could consider these in priority order. Note that just the first one may appear to ""fix"" your problem, but that is because you are not yet properly measuring your agent's performance."
Why my Fully Connected Neural Network outputs the same prediction?,"
I have a relatively small data set comprised of $3300$ data points where each data point is a $13$ dimensional vector where the $12$ first dimensions depict a ""category"" by taking the form of $[0,...,1,...,0]$ where $1$ is in the $i-th$ position for the $i-th$ category and the last dimension is an observation of a continuous variable, so typically one data point would be $[1,...,0,70.05]$.
I'm not trying to have something extremely accurate so I went with a Fully Connected Network with two hidden layers each comprising two neurons, the activation functions are ReLus, one neuron at the output layer because I'm trying to predict one value, and I didn't put any activation function for it. The optimizer is ADAM, the loss is the MSE while the metric is the RMSE.
I get this learning curve below:

Eventhough at the beginning the validation loss is lesser than the training loss (which I don't understand), I think at the end it show no sign of overfitting.
What I don't understand is why my Neural Network predicting the same value as long as the $13-th$ dimension takes values greater than $5$ and that value is $0.9747201$. If the $13-th$ dimension takes for example $4.9$ then the prediction would be $1.0005863$. I thought that it has something to do with the ReLu but even when I switched to Sigmoid, I have this ""saturation"" effect. The value is different but I still get the same value when I pass a certain threshold.
EDIT: I'd also like to add that I get this issue even with normalizing the 13th dimension (substracting the mean and dividing by the standard deviation).
I'd like to add that all the values in my training and validation set are at least greater than $50$ if that may help.
","['neural-networks', 'activation-functions', 'dense-layers']","two hidden layers each comprising two neuronsFrom your description it looks like that you only have 6 parameters for your inner layer (2x2 weight matrix + 2 biases). The whole network should be easy to interpret: you've got two 13-dimensional weight vectors $\vec{w}_1,\vec{w}_2$ that are dot-multiplied with the inputs, plus two biases $b$ and activation $\sigma$:$$ l_1 = \sigma\left(\vec{w}_1\vec{x} + b_1\right)$$
$$ l_2 = \sigma\left(\vec{w}_2\vec{x} + b_2\right)$$Then these two values are multiplied by 2x2 matrix + biases, then activation and linear combination.I'd look at how $l_1$ and $l_2$ are distributed. The fact that the outputs don't change is most likely due to the first layer getting saturated somehow. Look at the 13th dimension of $\vec{w}_i$ - it is likely to be large compared to other dimensions.First thing I'd try - standardizing your input 13th dimension, so it is distributed closer to $[0,1]$ ( or $[-1,1]$ ) range."
With Monte Carlo off-policy learning what do we correct by using importance sampling?,"
I do not understand the link of importance sampling to Monte Carlo off-policy learning.
We estimate a value using sampling on whole episodes, and we take these values to construct the target policy.
So, it is possible that in the target policy, we could have state values (or state action values) coming from different trajectories.
If the above is true, and if the values depend on the subsequent actions (the behavior policy), there is something wrong there, or else, better, something I do not understand.
Linking this question with importance sampling, do we use this ro value to correct this inconsistency?
Any clarification is welcome.
","['reinforcement-learning', 'monte-carlo-methods', 'off-policy-methods', 'importance-sampling']","Recall that the definition of a value function is
$$v_\pi(s) = \mathbb{E}\left[G_t | S_t = s\right]\;.$$
That is, the expected future returns given from state $s$ at time $t$ when we follow our policy $\pi$ -- i.e. our trajectory is generated according to $\pi$.Using Monte Carlo methods we typically will estimate our value function by looking at the empirical mean of rewards we see throughout many training episodes, i.e. we will generate many episodes, keep track of all the rewards we see from state $s$ onwards across all of our episodes (this may be the first visit method or the all visit methods) and use these to approximate the expectation that is our value function.The key here is that to approximate the value function in this way, then the episodes must be generated according to our policy $\pi$. If we choose the actions in an episode according to some other policy $\beta$ then we cannot use these episodes to approximate the expectation directly. As an example, this would be like trying to approximate the mean of a Normal(0, 1) distribution with data drawn from a Normal(10, 1) distribution.To account for the fact that the actions came from a different distribution, we have to reweight the returns according to an importance sampling ratio. To see why we need importance sampling, see this question/answer."
Is is not possible to achieve average reward of more than 20-40 with simple Q-Learning,"
I have implemented the simple Q-Learning based solution for AI-gym's Cartpole-v0.
However, despite changing hyper-parameters, and rechecking my code, I cannot get an average reward (N-running reward) of more than 30. My question is, is it not possible to get successful completion of Cartpole without using sophisticated algorithms such as Deep learning etc.?
I am glad to share my code, but I am sure no one would have time to check it.

PS. I know there are many implementations out there, but I have learned from them but I want to implement my own code for learning purpose and do not just want to copy-paste.
PSS (Edit): I have added the code in the answer to this question for reference.
","['q-learning', 'rewards', 'open-ai', 'gym']",
Why is the logarithm of the standard deviation used in this implementation of proximal policy optimization?,"
I am currently writing my bachelor thesis, which is an implementation of proximal policy optimization. Sometimes, I hit a wall because of the gaps in my mathematical knowledge. However, implementing the algorithm helped me to understand the math behind the algorithm.
Unfortunately, I still have a question.
When the action space is continuous, I am using the normal distribution (same as in the PPO implementation by Spinning up). In the mentioned implementation, the logarithm of the standard deviation is used initially to give the same probability to all of the possible action, then they use the standard deviation when choosing an action. Why do we use the logarithm? Why not directly use simply the standard deviation?
I know that the logarithm is easier when it comes to the computations, but I can not see the benefits of the logarithm in the Spinning up implementation.
class MLPGaussianActor(Actor):

    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):
        super().__init__()
        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)
        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))
        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)

    def _distribution(self, obs):
        mu = self.mu_net(obs)
        std = torch.exp(self.log_std)
        return Normal(mu, std)

    def _log_prob_from_distribution(self, pi, act):
        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution

","['reinforcement-learning', 'implementation', 'proximal-policy-optimization', 'normal-distribution']","As you have mentioned using log is nicer because it makes multiplications to additions etc etc(it helps in numerical stability issues). But I think over here, the reason they are doing it like that is because of enforcing a simple constraint in a much more simpler way. In the __init__ we are noticing that the log_std is being formulated instead of the std itself. Would it be wrong if you formulated the std itself? No. But it would be a bit messy to have some constrained imposed on it. For example, if we modeled std it could have been possible for the std to become 0 or negative. Well that's not a correct values of a std. But over here that is being enforced automatically, this kind of little things makes the learning for the model easier. Even if the log_std becomes negative or zero it does not matter because the exp will take care of it by exponentiating it to a positive number."
Do larger numbers of hidden layers have a bigger effect on a classification model's accuracy?,"
I trained different classification models using Keras with different numbers of hidden layers and the same number of neurons in each layer. What I found was the accuracy of the models decreased as the number of hidden layers increased However, the decrease was more significant in larger numbers of hidden layers. The accuracies refer to the test data and were obtained using k-fold=5. Also, no regularization was used. The following graph shows the accuracies of different models where the number of hidden layers changed while the rest of the parameters stayed the same (each model has 64 neurons in each hidden layer):

My question is why is the drop in accuracy between 8 hidden layers and 16 hidden layers much greater than the drop between 1 hidden layer and 8 hidden layers, even though the difference in the number of hidden layers is the same (8).
","['deep-learning', 'hyperparameter-optimization', 'overfitting', 'hidden-layers', 'accuracy']",
How to add prior information when predicting using deep learning models?,"
Background
I'm building a binary classification model for a pair match problem using CNN, e.g. whether person A1 likes product B1 or not. Model input features are sequence features (text descriptions) of the person and the product. The model accuracy is around 78%. So for a new person, the model can predict the probability whether he likes each product in our dataset.
Problem
The model is good if we know nothing about the person. However, in the real scenario, we already know the new person likes one or two products. We want to predict whether he likes other products. Is there any way to incorporate this prior information to improve the model?
My thought
A simple method would just retrain the model, giving the new person's pair higher sample weight. But we can't do this for each new person.
Any suggestion would be appreciated. Thanks
","['convolutional-neural-networks', 'training', 'knowledge-representation', 'bayesian-deep-learning']",
"What is the meaning of ""Our current objective weights every token equally and lacks a notion of what is most important to predict"" in the GPT-3 paper?","
On page 34 of OpenAI's GPT-3, there is a sentence demonstrating the limitation of objective function:

Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important.

I am not sure if I understand this correctly. In my understanding, the objective function is to maximize the log-likelihood of the token to predict given the current context, i.e., $\max L \sim \sum_{i} \log P(x_{i} | x_{<i})$. Although we aim to predict every token that appears in the training sentence, the tokens have a certain distribution, and therefore we do not actually assign equal weight to every token in loss optimization.
And what should be an example for a model to get the notion of ""what is important and what is not"". What is the importance refer to in here? For example, does it mean that ""the"" is less important compared to a less common noun, or does it mean that ""the current task we are interested in is more important than the scenario we are not interested in ?""
Any idea how to understand the sentence by OpenAI?
","['natural-language-processing', 'papers', 'open-ai', 'gpt-3']",
Why does Batch Normalization work?,"
Adding BatchNorm layers improves training time and makes the whole deep model more stable.  That's an experimental fact that is widely used in machine learning practice.
My question is - why does it work?
The original (2015) paper motivated the introduction of the layers by stating that these layers help fixing ""internal covariate shift"". The rough idea is that large shifts in the distributions of inputs of inner layers makes training less stable, leading to a decrease in the learning rate and slowing down of the training.  Batch normalization mitigates this problem by standardizing the inputs of inner layers.
This explanation was harshly criticized by the next (2018) paper -- quoting the abstract:

... distributional stability of layer inputs has little to do with the success of BatchNorm

They demonstrate that BatchNorm only slightly affects the inner layer inputs distributions.  More than that -- they tried to inject some non-zero mean/variance noise into the distributions. And they still got almost the same performance.
Their conclusion was that the real reason BatchNorm works was that...

Instead BatchNorm makes the optimization landscape significantly smoother.

Which, to my taste, is slightly tautological to saying that it improves stability.
I've found two more papers trying to tackle the question: In this paper the ""key benefit"" is claimed to be the fact that Batch Normalization biases residual blocks towards the identity function. And in this paper that it ""avoids rank collapse"".
So, is there any bottom line? Why does BatchNorm work?
","['neural-networks', 'machine-learning', 'papers', 'hidden-layers', 'batch-normalization']",
Are there any inverse RNN layers?,"
Given the model:
Sequence([
GRU(200, input_shape=(None,100), return_sequences=False)
])

Which maps the space (None, 100) -> (200,)
Is there an InverseGRU such that it maps the space (200,) -> (None, 100)
or, at least, is it possible to simulate this behaviour?
","['tensorflow', 'keras', 'recurrent-neural-networks']",
How to fight with unstability in self play?,"
I'm working on a neural network that plays some board games like reversi or tic-tac-toe (zero-sum games, two players). I'm trying to have one network topology for all the games - I specifically don't want to set any limit for the number of available actions, thus I'm using only a state value network.
I use a convolutional network - some residual blocks inspired by the Alpha Zero, then global pooling and a linear layer. The network outputs one value between 0 and 1 for a given game state - it's value.
The agent, for each possible action, chooses the one that results in a state with the highest value, it uses the epsilon greedy policy.
After each game I record the states and the results and create a replay memory. Then, in order to train the network, I sample from the replay memory and update the network (if the player that made a move that resulted in the current state won the game, the state's target value is 1, otherwise it's 0).
The problem is that after some training, the model plays quite well as one of the players, but loses as the other one (it plays worse than the random agent). At first, I thought it was a bug in the training code, but after further investigation it seems very unlikely. It successfully trains to play vs a random agent as both players, the problem arises when I'm using only self play.
I think I've found some solution to that - initially I train the model against a random player (half of the games as the first player, half as the second one), then when the model has some idea what moves are better or worse, it starts training against itself. I achieved pretty good results with that approach - in tic-tac-toe, after 10k games, I have 98.5% win rate against the random player as the starting player (around 1% draws), 95% as the second one (again around 3% draws) - it finds a nearly optimal strategy. It seems to work also in reversi and breakthrough (80%+ wins against random player after the 10k games as both players). It's not perfect, but it's also not that bad, especially with only 10k games played.
I believe that, when training with self play from the beginning, one of the players gains a significant advantage and repeats the strategy in every game, while the other one struggles with finding a counter. In the end, the states corresponding to the losing player are usually set to 0, thus the model learns that whenever there is the losing player's turn it should return a 0. I'm not sure how to deal with that issue, are there any specific approaches? I also tried to set the epsilon (in eps-greedy) initially to some large value like 0.5 (50% chance for a random move) and gradually decrease it during the training, but it doesn't really help.
","['reinforcement-learning', 'convolutional-neural-networks', 'deep-rl', 'epsilon-greedy-policy', 'self-play']","The AlphaZero paper mentions an ""evaluation"" step that seems to deal with the  the problem similar to yours:... we evaluate each new neural network checkpoint against the current best network $f_{\theta_*}$ before using it for data generation ... Each evaluation consists of 400 games ... If the new player wins by a margin of > 55% (to avoid selecting on noise alone) then it becomes the best player $\alpha_{\theta_*}$ , and is subsequently used for self-play generation, and also becomes the baseline for subsequent comparisonsIn the AlphaStar they've use a whole league of agents that was constantly played against each other."
How does the loss landscape look like or change when a model is overfitting?,"
My understanding is that when a model starts overfitting, it no longer learns useful features and starts remembering the training data set. Given enough epochs and sufficient parameters, a model can over-fit any arbitrary dataset. My question is how does the loss landscape look like for the training dataset vs the test dataset when a model is overfitting? Is there a weird dip around some point in the training dataset but the same dip is not there in the test dataset?
","['machine-learning', 'deep-learning', 'overfitting', 'loss']",
"How to Select Model Parameters for Transformer (Heads, number of layers, etc)","
Is there a general guideline on how the Transformer model parameters should be selected, or the range of these parameters that should be included in a hyperparameter sweep?

Number of heads
Number of encoder & decoder layers
Size of transformer model (d_model in Pytorch)
Size of hidden layers

Are there general guidelines like number of decoder layers should be equal to encoder layers? Thank you
","['natural-language-processing', 'transformer', 'hyperparameter-optimization', 'attention', 'gpt']",
"Viola-Jones algorithm: Haar-like features, how are the features extracted?","
If I have an image like this
1 2 3 4 5 6 7 8
a b c d e f g h
...

And I apply a Haar-like feature with a template
1 1 1 1 
-1 -1 -1 -1

Then in the first position we get X1 = 1+2+3+4+a+b+c+d. If we slide one side to the right, we again get X2 = 2+3+4+5+b+c+d+e.
This way we will get X1 and X2 and X3 and so on. Now, how are these values combined to get the feature? Because when we say a feature we are not just running that template in one place, rather we will run it over multiple places in the image. It gives lots of values like X1,X2 and X3 and so on. Now, how are those combined to get the final feature which will be passed to Adaboost?
","['computer-vision', 'object-detection', 'image-processing']","I would look at table 1 of the original paper. While you're reading the alogorithm, try to really focus on Step 2 when you get to it.In summary, each feature is used to train it's own classifier. So in your example, the calculated features X1, X2, ... Xn you describe coorespond to apply some set of feature transforms f_1, f_2, ... f_n to a single image. This is a bit backwards from what actually happens. What the method really does is train a classifier for each feature. So if you had n features, you would have n classifiers. Then in an adaboost fashion, you upweight the classifier that performed the best. I.e, you are upweighting the classifier based soley on th best performing feature. You then repeat and re-weight all the classifiers until you reach convergence."
Is it possible to use an internal layer's outputs in a loss function?,"
For a network of the form:
Input(10)
Dense(200)
Dense(100+10)
Dense(20)
Output()

Those +10 outputs are what I want to add to the standard 20 outputs, for my loss function.
Is this possible - in theory or even with some pre-existing library?
","['neural-networks', 'tensorflow', 'keras', 'objective-functions', 'generative-adversarial-networks']",
Is a true RNN auto encoder possible with Keras/TF,"
I want to get some encodings for temporal data (with a highly varying number of timesteps).
The dataset is of the format: array<TemporalSample = list, SAMPLE_COUNT> (where array is fixed size and list is variable).
The TemporalSamples are simply lists of size TemporalSample::timesteps

Currently, I use a standard RNN network of the form:
model = keras.Sequential()
model.add(layers.GRU(256, dropout=0.1, input_shape=[None, 1], return_sequences=False))
model.add(layers.Dense(len(output_names), activation=""softmax""))
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

The problem is my inputs have variable lengths, if I want an auto-encoder, my output needs to be variable-length (just like my input), and I need an inverse RNN layer (something like layers.InverseGRU(output_shape=[None, 1])) but from my reading this seems like not something that has been considered/done before.
Is this at all possible?
","['machine-learning', 'tensorflow', 'python', 'keras', 'recurrent-neural-networks']",
"Extracting ""hidden"" costs from financial statements using NLP","
I'm designing a NLP model to extract various kinds of ""hidden"" expenses from 10-K and 10-Q financial statements. I've come up with about 7 different expense categories (restructuring costs, merger and acquisitions, etc.) and for each one I have a list of terms/synonyms that different companies call them. I'm new to NLP would like some advice on the best approach for extracting them.
Values are usually hidden in two different areas of the document:
Type 1: Free-form text (footnotes)
Values are nested in sentences. Here are some examples, with the Expense Type and Monetary value indicated.

Exploratory dry-hole costs were \$12.7 million, \$1.3 million, and \$1.0 million for the years ended December 31, 2012, 2011, and 2010, respectively.


2012 includes the recognition of a $3,340 million impairment charge related to the carrying value of Citi's remaining 35% interest in the Morgan Stanley Smith Barney joint venture


During the year ended December 31, 2017, we decided to discontinue the internal development of AMG 899, resulting in an impairment charge of $400 million for the IPR&D asset

Type 2: Table data
SEC statements also contain ""structured"" data in HTML tables. Some line items, like the first row below, correspond to the expense type I'm looking for:




Item
2020
2019
2018




impairment related to real estate assets(2):
398.2
200
0


research and development
100
200
300


other expenses
20
30
40




Correct value = 398.2

I'm thinking about a two-model approach:

Define a new NER model based off the terms I already know (e.g. ""dry-hole costs"", ""impairment charges""). I would need to manually annotate extracts from historic statements that contain these terms for the training set.

For free-form text, it would match the sentence and pass it on for further processing (see 2).
For table data, I would loop over each row using beautifulsoup and pandas, check the first column for a match (e.g. using spaCy's comparison function), and then grab that year's value from the dataframe and finish.


For free-form matches, I still need to grab the monetary value for the correct year (sometimes multiple values are given for various years, see the first example above).


One potential problem here is that sentences like this would cause problems:

We gained $100 million this year, despite facing restructuring charges.

If the NLP algo is split into the above two-model process, model 1 would pass (because it contains a known term like ""restructuring charges""), and model 2 would extract $100 million, which is incorrect because it doesn't actually correspond to the expense itself.
Is there a better solution here? As I said, I'm new to NLP and data extraction so would really appreciate any advice or resources to learn more about solving these types of key/value problems.
","['natural-language-processing', 'named-entity-recognition', 'spacy']",
How many papers about AI / ML were published in the recent years?,"
I am trying to formulate an argument at work saying the disruption in AI/ML is very high and that it is hard to stay ""state of the art"". I would like to support that hypothesis by numbers.
Question:
How many papers were published in 2018-2020 related to AI (or if that is too generic: ML)?
","['machine-learning', 'papers']",
Advice required for identifying bone fragments in CT-scans using STL Files (3D image segmentation),"
I am working on a project related to automating the procedure of manually segmenting some bones in CT scans and hopefully if everything goes alright in this stage, move on to do something more with them - like bone reconstruction etc.
I have been doing extensive research regarding this - and CNNs were something in my target as a ML method that could be used here. Emphasis is more on using Deep learning for this project.
So, what I have - the data: CT scans of chest/shoulder and for each of the CT scan, I have 4-6 STL files of the individual bone fragments or segments located in the shoulder or near shoulder region. I am a tad uncertain as to how to use those individual STL files.
Target: To label/classify/identify these fragments in the CT scan - automate it.
My MOA (Method of Approach) or what I understand - I believe it is object (bone fragment being the object) detection and feature (of those bone pieces that I need to lock on in the CT-scan) extraction using CNNs. I am looking at Mask R-CNN etc, use a pre-trained CNN for this.
But I am not entirely sure if my understanding is correct. This is my first time with this stuff, but hoping to learn more. CT-scans are in nifti format.
I could provide more info if required, would gladly appreciate any insight or help or advice with what could be the way forward and if I am thinking along the correct lines.
Thank you.
","['convolutional-neural-networks', 'object-detection', 'feature-extraction']",
How to recover the target Q network's weights solely from the snapshots of the primary Q network's weights in DQN?,"
Suppose that I have a DQN agent, which has two neural networks: one is the primary Q network and the other is the target Q network. In every update, the target Q network is updated with a soft update strategy:
$$Q_{target} = (1-\tau) \times Q_{target} + \tau \times Q_{prime}$$
I saved the primary Q network's weights every $n$ episodes (say $n=10$), but, unfortunately, I did not save the target Q network's weights.
Say that my training process is aborted for some reason, and now I would like to continue the training using the latest saved weights. I can load the primary Q network's weights, but what about the target Q network's weights? Should I also use the latest primary Q network's weights for the target Q network's weights, or should I use the primary Q network's weights from several episodes ago, or how should it be?
","['reinforcement-learning', 'training', 'deep-rl', 'dqn']","Let's add a step index to your expression$$Q_{target}^{n} = (1-\tau)Q^{n-1}_{target} + \tau\, Q^{n-1}_{primary}$$We can expand it one step further$$Q_{target}^{n} = (1-\tau)^2Q^{n-2}_{target} + (1-\tau)\tau\, Q^{n-2}_{primary} + \tau\, Q^{n-1}_{primary}$$And further$$Q_{target}^{n} = (1-\tau)^3Q^{n-3}_{target} + (1-\tau)^2\tau\, Q^{n-3}_{primary} + (1-\tau)\tau\, Q^{n-2}_{primary} + \tau\, Q^{n-1}_{primary}$$So, I guess, we can write a general formula for $m$ steps behind like:$$Q_{target}^{n} = (1-\tau)^{n-m}Q^{n-m}_{target} + \tau\,\sum_{i=0}^{m-1} (1-\tau)^i Q^{n-i-1}_{primary} $$For $n-m$ large enough $(1-\tau)^{n-m}$ should be close to 0 and you should be able to approximately reconstruct your $Q_{target}^n$ using only the history of $Q_{primary}$esEdit: I've missed that you only have snapshots with some step between them. This is not ideal, but a possible way out would be to use, say, a linear interpolation between snapshot points."
What is the difference between a performance standard and performance measure?,"
I am reading AI: A Modern Approach. In the 2nd chapter when introducing different agent types, i.e., reflex, utility-based, goal-based, and learning agents, I understood that all types of agents, except learning agents, receive feedback and choose actions using the performance measure.
But they do so in different ways. Model-based reflex agents possess an internal state (like a memory), while goal-based agents predict the outcome of actions and choose the one serving the goal. Lastly, utility-based functions measure the 'happiness' of each state using the utility function, which is again an internalization of the performance measure, hence all have similar nature overall.
The learning agents, however, can be wrapped around the entire structure of previous agents. The entire agent's architecture is now called a performance element, and the learning agent has an additional learning element, which modifies each component of the agent, so as to bring the components into closer agreement with the available feedback information. But the feedback information in learning agents does not from the performance measure embedded in the agent's structure, but from a fixed external performance standard, which is part of the critic element*.
For the purpose of illustration, the structure of a utility-based agent and that of a learning agent are presented in the figure:

What boggles my mind is figuring out the actual difference and interaction between performance standard and performance measure, which is perhaps related to those between learning agents and other ones. Here are my thoughts thus far:

Other agents aim for maximizing the performance measure, causing them to do perfect actions. On the other hand, learning agents have the freedom of doing sub-optimal actions, which allow them to discover better actions on the long run using the performance standard.

Through the performance standard's feedback (which comes from the critic as shown in the figure), the learning agent can also learn a utility function or reflex component.


For providing examples, the book states that giving tip to an automated taxi is considered a performance standard. And also

hard-wired performance standards such as pain and hunger in animals can be understood in this way.

But I am still not sure about the discrepancy and interaction between the performance measure and performance standard. For instance, in the automated taxi, when confronting a road junction, the utility-based agent chooses a path that maximizes its utility function. The learning agent, however, must check different roads and after testing them, it receives feedback from outside so that eventually it would detect the user's preference.
But what if we wrap a learning agent around a utility-based agent in such a condition? Which has more effect, the utility function from inside, or the performance standard from outside (critic)? If they happen to contradict each other, which one would have the prevalent effect?
","['terminology', 'intelligent-agent', 'norvig-russell', 'utility-based-agents', 'learning-agents']",
Does gradient descent in deep learning assume a smooth fitness landscape?,"
I've come across the concept of fitness landscape before and, in my understanding, a smooth fitness landscape is one where the algorithm can converge on the global optimum through incremental movements or iterations across the landscape.
My question is: Does deep learning assume that the fitness landscape on which the gradient descent occurs is a smooth one? If so, is it a valid assumption?
Most of the graphical representations I have seen of gradient descent show a smooth landscape.
This Wikipedia page describes the fitness landscape.
","['gradient-descent', 'fitness-functions']",
The MLP output of a neural network can be written as $\|x\|\|w_l\|\cos(\theta_l)$: why is the norm easier to maximize?,"
The MLP output of a neural network is a dot product between the weights and the input and therefore can be written as $\|x\|\|w_l\|\cos(\theta_l)$ (see this for more details), where $x$ is the input, $w_l$ is the weights of layer $l$ and $\theta_l$ is the angle between them.
I read this in a paper: Angular Visual Hardness. The paper stated that it's much easier to maximize the norms $\|x\|$ and $\|w_l\|$ than the cosine similarity. Why is this the case? Just because $\cos(\theta_l)$ gives less weight because it's bounded between $[-1,1]$? Or is it due to the gradient? So, why is the norm easier to maximize?
","['neural-networks', 'deep-learning', 'papers', 'deep-neural-networks']",
Why do you calculate the mean and standard deviation over the complete dataset before training rather than for every batch?,"
In most implementations of neural networks the features are scaled to make the optimization of the loss function as stable as possible.
Mostly a min-max scaler is used. Alternatively, there is also a standard scaler.
Why do you calculate the mean and standard deviation offline over the complete dataset before training? Couldn't this be calculated per batch or even per file? What is the disadvantage? Why doesn't anyone do this?
","['deep-learning', 'training', 'data-preprocessing', 'normalisation', 'standardisation']",
What would be the state of the art image captioning deep learning model?,"
I saw a couple of architectures, like CNN-LSTM, with and without attention model, use of Glove vector, self-critical models, etc. I am overwhelmed looking at different notebooks and architectures, came here for a guidance.  I am looking to build a personal project on image annotations. Also, if I wanted to use this deep learning model together with TFX pipeline, what would be the best type of architecture I can go with?
","['deep-learning', 'convolutional-neural-networks', 'state-of-the-art', 'text-generation', 'model-request']","Here are a couple of Kaggle Kernels, Notebooks and Tutorials for Image Captioning.Kaggle Kernel | Neural Image Captioning: ğŸŒ„ -> ğŸ’¬Kaggle Kernel | Show Attend and TellKaggle Kernel | Flickr Image Captioning : TPU, TF2 & GloveTensorflow Tutorial | Image captioning with visual attentionShow and Tell: A Neural Image Caption Generator by Vinyals et al."
"What are some strong algorithms for Perfect Information, Deterministic Multiplayer Games?","
I have a series of games with the following properties:

3 or more players, but purely non-cooperative (i.e., no coalition forming);
sequential moves;
perfect information;
deterministic state transitions and rewards; and
game size is large enough to make approximate methods required (e.g. $1000^{120}$ for certain problems).

For example, Chinese Checkers, or, for a more relevant example to my work, a multi-player knapsack problem (where each player, in round-robin fashion, can choose without replacement from a set of items with the goal of maximizing their own knapsack value).
Question: what policy improvement operators or algorithms a) converge to optimality or b) provide reasonably strong results on these games?
What have I researched

In a small enough game (e.g., 3-person Nim with (3,4,5) starting board), full tree search is possible.
In a one-person setting, certain exact Dynamic Programming formulations can reduce complexity. For example, in a one-person setting with a small enough knapsack, any standard array-based approach can solve the problem. I'm unsure if or how these ""cost-to-achieve"" shortest path formulations carry over to multi-player games.
In a one-person setting, policy improvement approaches like rollout algorithms and fortified rollout algorithms have the cost improvement property. I'm unsure if this property carries over to multi-player versions.
Work has been done (for example, this thesis) to demonstrate that Monte Carlo Tree search strategies can generate powerful policies on the types of games I'm interested in. I believe it's been proven that they converge to Nash Equilibrium in two-player perfect information games, but am not aware of any guarantees regarding multiplayer games.
For imperfect information games, Monte Carlo Counterfactual Regret Minimization (e.g. here) is required for any convergence guarantees. Given I am working in a perfect information environment, these seem like overkill.

","['game-theory', 'combinatorial-games', 'policy-iteration']",
"Is there a clustering algorithm that can make n clusters and the n+1 ""others"" cluster?","
As far as I know all clustering algorithms assume that all delivered data points have to find its cluster.
My question is, is there an algorithm that could focus only on n clusters (number stated by user) and try to dismiss the rest of the points that (according to algorithm) do not belong to n clusters, like in the picture shown below? Where we know that there are for example 2 classes that we need to cluster (red and green) and the rest (blue) we do not need in any cluster and therefore algorithm does not try to assign them to any cluster?
For example if we would have 1 000 pictures of animals, of which 200 are dogs, 200 are cats and the rest are all other animals known to men and we want to make 1 cluster for cats, 1 for dogs and maybe another for collectively all others that do not match dogs or cats.

","['unsupervised-learning', 'data-science', 'clustering']",
Is the policy gradient expression in Fundamentals of Deep Learning wrong?,"
I don't understand the policy gradient as explained in Chapter-9 (Deep Reinforcement Learning) of the book Fundamentals of deep learning.
Here is the whole paragraph:
Policy Learning via Policy Gradients

In typical supervised learning, we can use stochastic gradient descent to update our parameters to minimize the loss computed from our network's output and the true label. We are optimizing the expression:
$$
\arg \min _{\theta} \Sigma_{i} \log p\left(y_{i} \mid x_{i} ; \theta\right)
$$
In reinforcement learning, we don't have a true label, only reward signals. However, we can still use SGD to optimize our weights using something called policy gradients. We can use the actions the agent takes, and the returns associated with those actions, to encourage our model weights to take good actions that lead to high reward, and to avoid bad ones that lead to low reward. The expression we optimize for is:
$$
\arg \min _{\theta}-\sum_{i} R_{i} \log p\left(y_{i} \mid x_{i} ; \theta\right)
$$
where $y_{i}$ is the action taken by the agent at time step $t$ and where $R_{i}$ is our discounted future return. A In this way, we scale our loss by the value of our return, so if the model chose an action that led to negative return, this would lead to greater loss. Furthermore, if the model is very confident in that bad decision, it would get penalized even more, since we are taking into account the log probability of the model choosing that action. With our loss function defined, we can apply SGD to minimize our loss and learn a good policy.

The first expression about the loss computed in a network already seems false since the log of a probability is always negative, and taking the $θ$ (weights) for which the expression is minimal doesn't seem right because it would favor very unsure answers.
The same goes with the next expression on policy gradient. A very negative $R_i$ and very unsure $p(y_i)$ would both be big negatives and multiplied together give a big positive value. Since there is a - sign in front of the expression, this would be the best configuration for the argmin. Meaning we are looking for weights in the policy that give highly negative rewards and for highly unsure actions. This just doesn't make sense to me.
Is it just a sign error (or we could just change to argmax)? Or is there more to it?
","['deep-rl', 'objective-functions', 'math', 'policy-gradients', 'policies']",
Why the optimal Bellman operator of a Q-function can be approximated by a single point,"
I am currently studying reinforcement learning, especially DQN.
In DQN, learning proceeds in such a way as to minimize the norm (least-squares, Huber, etc.) of the optimal Bellman equation and the approximate Q-function as follows (roughly):
$$
\min\|B^*Q^*-\hat{Q}\|.
$$
Here $\hat{Q}$ is an estimator of Q function, $Q^*$ is the optimal Q function, and $B^*$ is the optimal Bellman operator.
$$
B^*Q^*(s,a)=\sum_{s'}p_T(s'|s,a)[r(s,a,s')+\gamma \max_{a'}Q^*(s',a')],
$$
where $p_T$ is a transition probability, $r$ is an immediate reward, and $\gamma$ is a discount factor.
As I understand it, in the DQN algorithm, the optimal Bellman equation is approximated by a single point, and the optimal Q function $Q^*$ is further approximated by an estimator different from $\hat{Q}$, say $\tilde{Q}$.
\begin{equation}\label{question}
B^*Q^*(s,a)\approx r(s,a,s')+\gamma\max_{a'}Q^*(s',a')\approx r(s,a,s')+\gamma\max_{a'}\tilde{Q}(s',a'),\tag{*}
\end{equation}
therefore the problem becomes as follows:
$$
\min\|r(s,a,s')+\gamma\max_{a'}\tilde{Q}(s',a')-\hat{Q}(s,a)\|.
$$
What I want to ask：
I would like to know the mathematical or theoretical background of the approximation of \eqref{question}, especially why the first approximation is possible. It looks like a very rough approximation. Can the right-hand side be defined as an ""approximate Bellman equation""?　I have looked at various literature and online resources, but none of them mention exact derivation, so I would be very grateful if you could tell me about reference as well.
","['reinforcement-learning', 'dqn', 'math', 'function-approximation', 'bellman-equations']",
Must all CNNs and RNNs not have a fully connected layer in order to be considered as such?,"
In the paper Wrist-worn blood pressure tracking in healthy free-living individuals using neural networks, the authors talk about a combination of feed-forward and recurrent layers, as if FC layers cannot be part of the RNN.
So, must all Convolutional Neural Networks and Recurrent Neural Networks not have a fully connected layer in order to be considered CNNs and RNNs, respectively? If yes, should we consider CNNs and RNNs with an FC layer ""hybrid models""?
","['convolutional-neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'terminology', 'dense-layers']",
Is it possible to optimize a multi-variable function with a reinforcement learning method?,"
I want to use RL instead of genetic or any other evolutionary algorithm in order to find the best parameter for a function.
Here is the problem:
Given a function $$f(x,y,z, \text{data}),$$
where $x$, $y$ and $z$ are some integers from 1 to 50.
So I can say I have a 3-dimensional array which is a way to save fitness values:
$$\text{parameters} = [[1..50], [1..50], [1..50]]$$
The $$\text{data}$$ is another input which is the $f$ needed to do some calculation on.
Currently, I am optimizing it using a genetic algorithm with $$\text{cost}(\text{fitness}) = f(x,y,z,data)$$ which is a customized cost function.
Any value for $x$, $y$, and $z$ will result in a cost for example:
$$f(1, 5, 8, X) = 15$$
$$\text{parameters}: [1, 5, 8] = 15$$
or
$$ \text{parameters}: [2, 9, 11] = 30$$
In the provided example 2, 9, and 11 is a better set of parameters.
So I run a genetic algorithm and make some children with a sequence of x,y, and z. Then I calculate the cost(fitness) and then select them and so on.
I want to know is there any alternative or method in reinforcement learning which I can use instead of a genetic algorithm? If yes, please provide the name or any helpful link.
Note that F is completely defined by the user and should be changed in other contexts.
","['reinforcement-learning', 'optimization', 'genetic-algorithms', 'algorithm-request', 'fitness-functions']","In order to have anything resembling reinforcement learning you must at the very least have a set of states $S$ and a set of actions $A$.In your formulation I can vaguely identify the set of states $S$ as all possible $(x,y,z)$ triplets. But don't see anything in your description that could be interpreted as a set of actions $A$. You either oversimplified the description of your problem or reinforcement learning is not applicable here by lack of very basic ingredients for it."
What is ergodicity in a Markov Decision Process (MDP)?,"
I have read about the concept of ergodicity on the safe RL paper by Moldovan (section 3.2) and the RL book by Sutton (chapter 10.3, 2nd paragraph).
The first one says that ""a belief over MDPs is ergodic if and only if any state is reachable from any other state via some policy or, equivalently, if and only if"":
$$\forall s, s', \exists \pi_r \text{ such that } E_\beta E_{s, \pi_r}^P [B_{s'}] = 1$$
where:

$B_{s'}$ is an indicator random variable of the event that the system reaches state $s'$ at least once, i.e., $B_{s'} = 1 \{ \exists t < \infty \text{ such that } s_t = s'\}$
$E_\beta E_{s, \pi_r}^P[B_{s'}]$ is the expected value for $B_{s'}$, under the belief over the MDP dynamics $\beta$, policy $\pi$ and transition measure $P$.

The second one says ""$\mu_\pi$ is the steady-state distribution, which is assumed to exist for any $\pi$ and to be independent of $s_0$. This assumption about the MDP is known as ergodicity."". They define $\mu_\pi$ as:
$$\mu_\pi(s) \doteq \lim_{t \to \infty} \Pr\{s_t=s \vert a_{0:t-1} \sim \pi\}$$

i.e., there is a chance of landing on state $s$ by executing actions according to policy $\pi$.

I noticed that the first definition requires that at least one policy should exist for each $(s, s')$ pair for the MDP to be ergodic The second definition, however, requires that all policies eventually visit all the states in an MDP, which seems to be a more strict definition.
Then, I came accross the ergodicity definition for Markov chains:

A state $i$ is said to be ergodic if it is aperiodic and positive recurrent. In other words, a state $i$ is ergodic if it is recurrent, has a period of $1$, and has finite mean recurrence time. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic.

This leads me to believe that the second definition (the stricter one) is the most appropriate one, considering the ergodicity definition in an MDP derives from the definition in a Markov chain. As an MDP is basically a Markov chain with choice (actions), ergodicity should mean that independently of the action taken, all states are visited, i.e., all policies ensure ergodicity.
Am I correct in assuming these are different definitions? Can both still be called ""ergodicity""? If not, which one is the most correct?
","['reinforcement-learning', 'definitions', 'markov-decision-process', 'markov-chain', 'ergodicity']","In short, the relevant class of a MDPs that guarantees the existence of a unique stationary state distribution for every deterministic stationary policy are unichain MDPs (Puterman 1994, Sect. 8.3). However, the unichain assumption does not mean that every policy will eventually visit every state. I believe your confusion arises from the difference between unichain and more constrained ergodic MDPs.Puterman defines that a MDP is (emphasis in the following mine):Before unpacking this further, let's first recap what recurrent and transient states in a Markov chain are. The following definitions can be found in Puterman, Appendix A.2. For a state $s$, associate two random variables $\nu_s$ and $\tau_s$ that represent the number of visits and the time of the first visit (or first return if the chain starts in $s$) to state $s$. A state $s$ isIt is also true that $s$ is recurrent if and only if $\mathbb{E}[\nu_s] = \infty$, i.e., it is visited infinitely often, and $s$ is transient if and only if $\mathbb{E}[\nu_s] < \infty$, i.e., it is visited only finitely often (and thereby never again after some finite time).So let's now return to the two types of MDPs above. Consider an arbitrary deterministic stationary policy $\pi$ which maps any state $s$ to an action $a = \pi(s)$. If the MDP is ergodic, then the stationary distribution $\mu_\pi(s)$ exists and is unique, because the Markov chain over states induced by any policy has a single recurrent class (it does not matter in which state $s_0$ the chain starts, the same stationary distribution is reached). There is a single class of recurrent states, i.e., all states are recurrent, therefore any $s$ is visited infinitely often and $\mu_\pi(s) > 0$.Now, if the MDP is unichain, then once again the stationary distribution $\mu_\pi(s)$ exists and is unique, because the Markov chain over states induced by any policy has a single recurrent class. But, importantly, there may exist a policy $\pi$ for which the set of transient states in the induced Markov chain is not empty. Because any transient state $s$ will only be visited a finite number of times, in the (infinite horizon) stationary distribution $\mu_\pi(s)=0$!So indeed, if the MDP is of the stricter ergodic type, every policy will eventually visit every state. This is not true for unichain MDPs however.A final remark: some authors define a policy as ergodic (e.g., Kearns & Singh, 2002), if the resulting Markov chain over states is ergodic (i.e., has a unique stationary distribution). The unichain MDP is a type of MDP where every policy is ergodic.References:Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.Kearns & Singh. Near-Optimal Reinforcement Learning in Polynomial Time. Machine Learning, 49, 209–232, 2002"
Is it possible to use RGB image with decimal values when feeding training data to CNN?,"
I am working with four grayscale images of float32 data type to perform regression using Keras. Three images are stacked using np.dstack to form a RGB data-set. The last grayscale image is used as label. The grayscale images contains different variations, including [0 , 790.65], [ 150.87 , 260.45], [ -2.74174 , 2.4126 ], [-32.927 , 69.333].
If I convert the images to unit8, the maximum values for the first and second image will be 255 and the decimal values for all images will be lost.  I am having difficulty and struggling to find the solution for using the image in the original data type (float32) when I try to use ImageDataGenerator and flow_from_directory. Can anyone suggest way for that?
","['neural-networks', 'convolutional-neural-networks', 'tensorflow', 'python', 'regression']",
What does 'clock rate' mean in the context of recurrent neural networks (RNNs)?,"
I have often encountered the term 'clock rate' when reading literature on recurrent neural networks (RNNs). For example, see this paper. However, I cannot find any explanations for what this means. What does 'clock rate' mean in this context?
",['recurrent-neural-networks'],"The purpose of a clockwork RNN is to help with long term dependencies. Let's say in this case, we have a sentence that starts with ""John went to..."" and at no point again is John's name mentioned throughout the few paragraphs we are passing to our model.As mentioned in the paper, the most common method to combat this (at the time) was using an LSTM that stored long term data in it's cell state, or as put in the paper:[an LSTM] uses a specialized architecture that allows information to be stored in a linear unit called a constant error carousel (CEC) indefinitelyHowever, this requires a whole heap of extra parameters in order to work (input gate, output gate and forget gate which all require parameters). So proposed was the CW-RNN.The fundamental idea behind a clockwork RNN is to have ""modules"" computed periodically. First to clear things up, a timestep is 1 input into the model. So in the case of our example, if we're inputting character by character, timestep 1 is the character ""J"", timestep 2 is ""h"" and so on; ""o"", ""n"", "" "", ""w""...So what is the clock-rate? Well, it's simply how often each module of the CW-RNN is computed. Let's say the hidden layer of the clockwork RNN is split into 8 modules, which I will reference as $M_1, M_2, M_3 ... M_8$, and the associated clock-rates for each of these modules are the powers of 2, so: $1, 2, 4, 8, 16, 32, 64$ and $128$.Before I continue I want to note a potential discrepancy, if you consider the first input to be timestep 0, then it changes how this executed (all modules would be activated at the first timestep), however if you consider it to be 1 (like I do for this example), only module 1 would be executed (again, in this example).So we're at timestep 1, ie ""J"", so we check the timestep against the clock-rates of each module to determine which ones will be executed in the computation of the hidden state and output for this timestep. To do this, we take the mod of the timestep against the time-rate, so: 1 mod 1, 1 mod 2, 1 mod 4 ... 1 mod 128 and if it equals 0 (basically, is the timestep a multiple of the time-rate) then that module will be executed at this timestep. So in this case $M_1$ will be executed. When we input ""h"" at timestep 2, $M_1$ and $M_2$ will both be executed, and will equally contribute to the hidden state (ie, they will be added together) and the output.By performing calculations this way, each module can all simultaneously be responsible for information over different time periods, for example $M_8$ which is only executed every 128 timesteps will be responsible for very long term dependencies, but $M_1$ will cover short term dependencies.So in basic terms, clock-rate refers to how often (what timestep interval) a given module of a Clockwork RNN is computed"
How to interpret this learning curve of my neural network?,"
How to interpret the following learning curves?
Background: The accuracy starts at 50%, because the network has a binary output (0 or 1). I chose an exponentially decreasing learning rate of the optimizer - I believe that this the reason why the network starts learning after 10 epochs or so.
lr_schedule = keras.optimizers.schedules.ExponentialDecay(**h_p[""optimizer""][""adam""])
optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)
h_p[""Compile""][""optimizer""] = optimizer


","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'training']",
How does a model based agent learn the model?,"
I want to build model-based RL. I am wondering about the process of building the model.
If I already have data, from real experience:

$S_1, a \rightarrow R,S_2$
$S_2, a \rightarrow R,S_3$

Can I use this information, to build model-based RL? Or it is necessary that the agent directly interact with the environment (I mean the same above-mentioned data should be provided by the agent)?
","['reinforcement-learning', 'model-based-methods']","If you already have some transition tuples then you can train a model to predict environment dynamics using these. However, you should be careful that your pre-gathered data is diverse enough to 'cover' enough of the state/action space so that your model remains accurate. For instance, when you start training your agent it will likely start to see more of the state space than it did at the start of training (imagine playing Atari, initially your agent will die quickly but as it gets better episodes will get longer) so you would need to make sure you have data for these states that appear late in episodes, otherwise your model will just be overfitting to the start of the episode and will give a poor performance on these other states, thus slowing down or even prohibiting learning of an optimal policy."
What is the difference between environment states and agent states in terms of Markov property?,"
I'm going through the David Silver RL course on YouTube. He talks about environment internal state $S^e_t$, and agent internal state $S^a_t$.
We know that state $s$ is Markov if
$$\mathbb{P}\{S_t=s|S_{t-1}=s_{t-1},...,S_1=s_1\}=\mathbb{P}\{S_t=s|S_{t-1}=s_{t-1}\}.$$
When we say that Decision Process is Markov Decision Process, does that mean:

All environment states must be Markov states
All agent states must be Markov states
Both (All environment states and all agent states must be Markov states)

and according to this, if we specify corresponding MDP as $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma, T)$, is $\mathcal{S}$ the state space of environment states or agent states?
Why I'm confused by this? He claims that environment states are Markov (I'm also confused why, but I'll make another post for this), and then claims that if the agent can directly see environment internal state $S^e_t$, then observations $O_t=S^e_t$, and agent constructs its state trivially as $S^a_t=O_t=S^e_t$. Now, both environment and agent states are Markov (since they are the same), so this makes sense. If we specify MDP as $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma, T)$, it's clear that state space $\mathcal{S}$ is state space of both agent internal states and environment internal states (again, since they are the same).
Now consider the case when the environment is not fully observable. Now $O_t\ne S^e_t$, and agent must construct it's state $S^a_{t}=f(S^a_{t-1}, H_t)$, where $H_t=(O_0, A_0, R_1, O_1,...,O_{t-1}, A_{t-1}, R_t, O_t)$ is history until time step $t$, and $f$ is some function (such as Recurrent Neural Network for example). In the case of $f$ being a recurrent neural network, we have that both environment internal states are Markov (by this hypothesis), and agent internal states are Markov (approximately), so again, the process is an MDP $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma, T)$, but state space of agent states is different to that of environment states, so I'm confused about what is $\mathcal{S}$ here. Is it environment state-space or agent state space?
Lastly, what if $f(S^a_t, H_t)=O_t$? That is, the agent's internal state is simply the last observation. Considering that environment states are always Markovian (again, don't know why we can claim this), but agent states are not, this is the case of POMDP. Even here I don't know what $\mathcal{S}$ stands for in specification of POMDP. Is it environment state-space or action state space?
","['reinforcement-learning', 'terminology', 'markov-decision-process', 'state-spaces', 'markov-property']","$\mathcal S$ is just a set of all possible states. It doesn't matter if it's agents perceived state or true environment state, they are within the same set of states. Agent cannot perceive itself to be in some ""middle"" state that's not in $\mathcal S$, it might think that's in the state that's not the actual environment state but that state is also in set of all states.To give an example, if the car can be blue or red, then the agent might think that the state is blue car or red car, but it cannot think that car is purple because that's not one of the possible states. It might wrongly think that the car is blue when the actual car is red, but that's ok because blue is one of the possible car states. Of course, it might also correctly think that the car state is red."
Can predictions of a neural network using ReLU activation be non-linear (i.e. follow the pattern) outside of the scope of trained data?,"
Training on a quadratic function
x = np.linspace(-10, 10, num=1000)
np.random.shuffle(x)
y = x**2

Will predict an expected quadratic curve between -10 < x < 10.

Unfortunately my model's predictions become linear outside of the trained dataset.
See -100 < x < 100 below:

Here is how I define my model:
model = keras.Sequential([
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.1))

history = model.fit(
    x, y,
    validation_split=0.2,
    verbose=0, epochs=100)

Here's a link to a google colab for more context.
","['neural-networks', 'keras', 'regression']","It isn't too surprising to see behaviour like this, since you're using $\mathrm{ReLU}$ activation.Here is a simple result which explains the phenomenon for a single-layer neural network. I don't have much time so I haven't checked whether this would extend reasonably to multiple layers; I believe it probably will.Proposition. In a single-layer neural network with $n$ hidden neurons using $\mathrm{ReLU}$ activation, with one input and output node, the output is linear outside of the region $[A, B]$ for some $A < B \in \mathbb{R}$. In other words, if $x > B$, $f(x) = \alpha x + \beta$ for some constants $\alpha$ and $\beta$, and if $x < A$, $f(x) = \gamma x + \delta$ for some constants $\gamma$ and $\delta$.Proof. I can write the neural network as a function $f \colon \mathbb R \to \mathbb R$, defined by
$$f(x) = \sum_{i = 1}^n \left[\sigma_i\max(0, w_i x + b_i)\right] + c.$$
Note that each neuron switches from being $0$ to a linear function, or vice versa, when $w_i x + b_i = 0$. Define $r_i = -\frac{b_i}{w_i}$. Then, I can set $B = \max_i r_i$ and $A = \min_i r_i$. If $x > B$, each neuron will either be $0$ or linear, so $f$ is just a sum of linear functions, i.e. linear with constant gradient. The same applies if $x < A$.Hence, $f$ is a linear function with constant gradient if $x < A$ or $x > B$.
$\square$If the result isn't clear, here's an illustration of the idea:
This is a $3$-neuron network, and I've marked the points I denote $r_i$ by the black arrows. Before the first arrow and after the last arrow, the function is just a line with constant gradient: that's what you're seeing, and what the proposition justifies."
Pytorch - Evaluation loss on the training set higher than loss during training,"
    def forward(self, image, proj, proj_inv):
        return self.predict_2d_joint_locations(image, proj, proj_inv)

    def criterion(self, predicted, gt):
        return self.mse(predicted, gt)

    def training_step(self, batch, batch_idx):
        player_images, j2d, j3d, proj, proj_inv, is_synth = batch
        predicted_2d_joint_locations = self.predict_2d_joint_locations(player_images, proj, proj_inv)
        train_loss = self.criterion(predicted_2d_joint_locations, j2d)
        self.log('train_loss', train_loss)
        return train_loss

    def validation_step(self, batch, batch_idx):
        player_images, j2d, j3d, proj, proj_inv, is_synth = batch
        predicted_2d_joint_locations = self.predict_2d_joint_locations(player_images, proj, proj_inv)
        val_loss = self.criterion(predicted_2d_joint_locations, j2d)
        self.log('val_loss', val_loss)
        return val_loss

I have this simple code for training_step() and forward() in Pytorch. Both the functions essentially do the same.
Owing to a relatively small dataset, my model grossly overfits on the training data (as is evident from there being an orders of magnitude of difference between the training and validation losses). But that's fine for now, I am perfectly aware of that and will add more data soon.
What surprises me is when I try to evaluate (infer). I don't have a separate test set (for now) and only have a training and a validation set. When I evaluate on the validation set, the mean squared error turns out to be in the same range as the validation loss my model is based on as expected. However, when I evaluate on the training set, the mean squared error I get is again in the same range as the validation loss (not the training loss).
    if args.val:
            check_dl = dataset.val_dataloader()
        else:
            check_dl = dataset.train_dataloader()

        for player_images,j2d,j3d,proj,proj_inv,is_synth in check_dl:
            if args.visualize:
                # visualize dataset
                player_images = player_images.cpu().numpy()
                j2d_predicted = model(torch.from_numpy(player_images), proj, proj_inv).cpu().detach().numpy()
                print(((j2d - j2d_predicted) ** 2).mean(), model.training_step((torch.from_numpy(player_images),j2d,j3d,proj,proj_inv,is_synth), 0))

When I print print(((j2d - j2d_predicted) ** 2).mean() for images in the training set after fetching the model from the trained checkpoint, I get numbers in the range of the validation loss. I retried the same by printing the loss using the training_step() function, but I again receive high losses (in the validation loss range).
Note: The inference mean squared errors I receive on the training set are high but they are not as high as when the training actually started. So, the pre-trained model is fetched properly. On a model with completely random weights, I should have received orders of magnitudes of higher errors. So, the model is definitely fetched correctly.
I have been scratching my head over this. Any help would be really appreciated.
",['pytorch'],
Does adding a model complexity penalty to the loss function allow you to skip cross-validation?,"
It's my understanding that selecting for small models, i.e. having a multi-objective function where you're optimizing for both model accuracy and simplicity, automatically takes care of the danger of overfitting the data.
Do I have this right?
It would be very convenient for my use case to be able to skip lengthy cross-validation procedures.
","['machine-learning', 'overfitting', 'regularization', 'cross-validation', 'capacity']","It's my understanding that selecting for small models, i.e. having a multi-objective function where you're optimizing for both model accuracy and simplicity, automatically takes care of the danger of overfitting the data.Sort of. A secondary objective function often works as a form of regularisation, and can work to reduce overfit.However, this regularisation is not a magic bullet. The degree of regularisation that you achieve will vary, it may depend on hyper-parameters in the regularisation technique. In your case the relative weightings of objective functions for the accuracy and simplicity of the model will have a sweet spot that minimises overfit without compromising too much and under-fitting instead.It would be very convenient for my use case to be able to skip lengthy cross-validation procedures.Typically using regularisation requires cross-validation in order to find good values for your regularisation hyperparameters. If the regularisation technique you have chosen is a good fit for your problem, then you may not have to search too much - there may be a broad set of values that work well for you.In turn, a good choice of regularisation may mean that your model's accuracy is less sensitive to other hyperparameters of your model, so searching for an accurate model becomes a little easier.However:Does adding a model complexity penalty to the loss function allow you to skip cross-validation?No. Assuming you want to find the best performing model, you still have to perform cross-validation. At best you may have to perform a little less than without the added objective because it has stabilised your model against other factors that can affect generalisation. However, you might have to perform more cross-validation, at least initially, in order to establish useful values of the new relative weighting hyperparameters you added with the secondary objectives. In addition the new simplicity objective function will likely change the best choices for other hyperparameters, such as number of free parameters, learning rate and length of training time.If you were previously performing cross-validation after every epoch of training and picking the best model after the epoch that gave the best accuracy on the cv set, then that is often considered a different form of regularisation called early stopping. You may find you could relax this and test less often during training, because with regularisation based on complexity objectives, the training will tend towards a more stable end point and be less likely to overfit through additional training epochs. Although in my experience cross-validation during training is usually left on by default, in order to plot learning curves and ensure this stability really holds."
Why do we discount the state distribution?,"
In Reinforcement Learning, it is common to use a discount factor $\gamma$ to give less importance to future rewards when calculating the returns.
I have also seen mention of discounted state distributions. It is mentioned on page 199 of the Sutton and Barto textbook that if there is discounting then (for the state distribution) it should be treated as a form of termination, and it is implied that this can be achieved by adding a factor of $\gamma$ to the state transition dynamics of the MDP, so that now we have
$$\mu(s) = \frac{\eta(s)}{\sum_{s'} \eta(s')}\;;$$
where $\eta(s) = h(s) + \sum_{\bar{s}} \eta(\bar{s})\sum_a \pi(a|\bar{s}) \gamma p(s|\bar{s}, a)$ and $h(s)$ is the probability of the episode beginning in state $s$.
In my opinion, the book kind of skips over this and it is not immediately clear to me why we need to discount our state distribution if we have discounting in the episode.
My intuition would suggest that it is because we usually take an expectation of the returns over the state distribution (and action/transition dynamics), but, if we are discounting the (future) rewards, then we should also discount the future states to give them less importance. In Sergey Levine's lectures he provides a brief aside that I think agrees with my intuition but in a rather unsatisfactory way -- he introduces the idea of a 'death state' that we transition into at each step with probability $1-\gamma$ but he does not really provide a rigorous enough justification for thinking of it this way (unless it is just a useful mental model and not supposed to be rigorous).
I am wondering whether someone can provide a more detailed explanation as to why we discount the state distribution.
","['reinforcement-learning', 'markov-decision-process', 'discount-factor']","Not an exhaustive answer, but perhaps this blog post by Alessio Russo may be helpful. In particular, he states howThere is an equivalence between using a discount factor and reaching a terminal state in a Markov Decision Process. [...] Therefore, we simply need to introduce, artificially, the possibility of terminating the trajectory with a certain probability 1-γ.The topic is not 100% clear to me yet either, but I feel like it makes much more sense now why this ""death state"" is used as a model to replace the discounted state distribution."
Do RNNs/LSTMs really need to be sequential?,"
There are many articles comparing RNNs/LSTMs and the Attention mechanism. One of the disadvantages of RNNs that is often mentioned is that while Attention can be computed in parallel, RNNs are highly sequential. That is, the computation of the next tokens depends on the result of previous tokens, thus, RNNs are losing to Attention in terms of speed.
Even though I fully agree that RNNs are sequential as stated above, I think they are still parallelizable by splitting the mini-batch into sub-batches and each of these sub-batches is processed independently by a dedicated thread. For example, a training batch of size 32 can be split into 4 sub-batches of size 8; 4 threads process 4 sub-batches independently. That way, RNNs/LSTMs are parallelizable and this is not a disadvantage compared to Attention.
Is my thought correct?
","['machine-learning', 'deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'attention']","You are talking about model parallelism. But, that's not the reason RNNs/LSTMs are not in vogue.Imagine your ability to read the first line of a page and going on reading and still making connections to the first line until the end of the page.Can RNNs/LSTMs do that? No.
Can Attention (i.e. Transformers) do it? Yes.The reason is simple Attention is effectively an affinity matrix between each and every input that goes into a network. So, it is able to do that. We have a huge memory overload but hey, we want the performance.In case of RNNs/LSTMs, the cells have to do this heavy-lifting, there is only a set amount of information that can be contained in them. That's why you have to forget gate to control information retained.Nevertheless, your thought is correct but that's not the reason for Attention to be in vogue. But, your thought has negative ramifications when we see how to implement it. Also, nevertheless the computation will be still sequential since you can't process input (n + 1) without input n. Local parallelization is possible but not global."
Is there any research on the application of policy gradients to problems where the selection of an action requires the selection of another one?,"
I am working on a problem and want to explore if it can be solved with PPO (or other policy gradient methods). The problem is that the action space is a bit special, compared to classic RL environments.
At each time $t$, we chose between 4 actions: $a_1\in \{0, 1, 2, 3\}$,
but given $a_1 = 0 \text{ or } 2$, we need to chose three more actions: $a_2, a_3, a_4$ (which all three can be chosen from categorical distributions).
I know I can design this kind of policy myself and re-work the entropy terms, and so on for PPO.
My question is: is there any research into this kind of RL?
I am having a hard time finding someone working with problems in which the actions chosen are dependent on other chosen at the same time. I have looked into Hierarchical RL, but the papers I have found have not worked with this particular kind of problem.
If these action spaces were small ($a_2, a_3$ are chosen from categorical distributions with $\sim$800 different options), one solution would be to roll it out into one big policy where each possible combination of actions is represented by one choice in the policy. But my concern of doing this with a bigger action space is that the choice of $a_1 = 1, 3$ where we don't choose the other separate actions will get lost in the policy.
","['reinforcement-learning', 'reference-request', 'policy-gradients', 'proximal-policy-optimization', 'action-spaces']",
How does high entropy targets relate to less variance of the gradient between training cases?,"
I've been trying to understand the Distilling the Knowledge in a Neural Network paper by Hinton et al. But I cannot fully understand this:

When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases [...]

The information part is very clear, but how does high entropy correlate to less variance between training cases?
","['deep-learning', 'papers', 'variance', 'entropy']",
What Constitutes Messages in Junction Tree Algorithm?,"
I'm currently studying the Junction Tree Algorithm: I'm referring to the process of transforming a Bayesian Network into a Junction Tree in order to apply inference. I understand how you build the Junction Tree, but I'm stuck on the idea of message passing.
What exactly are these messages? Are they numbers, or vectors?
If any of you could direct me to a numerical example that would be very appreciated.
","['bayesian-networks', 'bayesian-inference']","I think an example could make you understand better.
Suppose you want to calculate $P(X|S)$, you now need to put evidence on $S$(so you have to change all tables where $S$ appears making probability zero where $S$ does not appear).
At this point, you can proceed with the collect and distribute method in order to propagate the evidence throughout the graph, in order to maintain the global consistency property. Now you can choose any $cluster\space V$ that contains $X$, marginalize with respect to $S$, and the job is done."
What are the differences in testing between traditional software and artificial intelligence?,"
The testing problem in traditional software has been fully explored over the last decades, but it seems that testing in artificial intelligence/machine learning has not (see this question and this one).
What are the differences between the two?
","['machine-learning', 'testing']",
How to improve a trained model over time (i.e. with more predictions)?,"
I built a model using the tutorial on the TensorFlow site. It was a simple image classification neural network.  I trained it and saved the model and weights together on a .h5 file.
Recently, I have been reading about backpropagation. From what I understand, it's basically a way to tell the neural network whether if it's identified the correct output and that it is applied during training data only.
So, I was wondering if there is a way for the model to 'improve' over time as it makes more and more predictions. Or is that not how it would work with Neural Networks?
","['neural-networks', 'deep-learning', 'backpropagation', 'incremental-learning']","That is exactly a neural network works like.Suppose you have a 1000 examples. How you train a network is: First, you divide these 1000 into maybe 100 batches (10 each). After that's done, you feed a batch to the network get its output and compare it with the ground truth, whatever is the error gets backpropagated. Then, for the next batch and then another. Once all these batches are done, you say an epoch is over. So, the number of epoch is effectively the number of times the network has seen the whole data.This is how a neural network gets better."
"In MCTS, what to do if I do not want to simulate till the end of the game?","
I'm trying to implement MCTS with UCT for a board game and I'm kinda stuck. The state space is quite large (3e15), and I'd like to compute a good move in less than 2 seconds. I already have MCTS implemented in Java from here, and I noticed that it takes a long time to actually reach a terminal node in the simulation phase.
So, would it be possible to simulate games up until a specific depth?
Instead of returning the winner of the game after running until the max depth, I could return an evaluation of the board (the board game is simple enough to write an evaluation function), which then back propagates.
The issue I'm having is in handling the backpropagation. I'm not quite sure what to do here. Any help/resources/guidance is appreciated!
","['monte-carlo-tree-search', 'monte-carlo-methods', 'upper-confidence-bound']","Famous example is AlphaZero. It doesn't do unrolls, but consults the value network for leaf node evaluation. The paper has the details on how the update is performed afterwards:The leaf $s'$ position is expanded and evaluated only once by the network to gene-rate both prior probabilities and evaluation, $(P(s′ , \cdot),V(s ′ )) = f_\theta(s′ )$. Each edge $(s, a)$ traversed in the simulation is updated to increment its visit count $N(s, a)$, and to update its action value to the mean evaluation over these simulations, $Q(s,a) = \frac{1}{N(s,a)}\sum_{s,a\to s'}V(s')$  , where $s, a\to s′$ indicates that a simulation eventually reached s′ after taking move a from position s."
What is the advantage of using MCTS with value based methods over value based methods only?,"
I have been trying to understand why MCTS is very important to the performance of RL agents, and the best description I found was from the paper Bootstrapping from Game Tree Search stating:

Deterministic, two-player games such as chess provide an ideal
test-bed for search bootstrapping. The intricate tactics require a
significant level of search to provide an accurate position
evaluation; learning without search has produced little success in
these domains.

I however don't understand why this is the case, and why value based methods are unable to achieve similar performance.
So my question would be:

What are the main advantages of incorporating search based algorithms with value based methods?

","['reinforcement-learning', 'monte-carlo-tree-search', 'value-based-methods']","Assuming a continuous/uncountable state space, we can only estimate our value function using function approximation, so our estimates will never be true for all states simultaneously (because, loosely speaking, we have far more states than weights). If we can look at the (approximated) value of states we take in, say, 5 actions time, it is better to make a decision based on these estimations, taking into account the true rewards observed after the 5 actions.Further, MCTS also allows more implicit exploration as when choosing the actions to expand the tree we are potentially choosing lots of non-greedy actions that lead to better future returns."
"What is the difference between the forward pass of the Multi-Layer Perceptron, Deep AutoEncoder and Deep Belief Network?","
Multi-Layer Perceptron (MLP), Deep AutoEncoder (DAE), and Deep Belief Network (DBN) are trained differently.
However, do they follow the same process during the inference phase, i.e., do they calculate a weighted sum, then apply a non-linear activation function, for each layer until the last layer, or is there any difference? Moreover, are they only composed of fully connected layers?
","['autoencoders', 'multilayer-perceptrons', 'dense-layers', 'deep-belief-network', 'forward-pass']",
Off-policy Bellman Operators: Writing Operator and Weight Update Function for a 2-State System,"
I am studying for RL on my own and was trying to solve this question I came across.

Write an operator function $T(w, \pi, \mu, l, g)$ that takes weights $w$, a target policy $\pi$, a behaviour policy $\mu$, a trace parameter $l$, and a discount $g$, and outputs an off-policy-corrected lambda-return. For this question, implement the standard importance-weighted per-decision lambda-return. There will only be two actions, with the same policy in each state,
so we can define $\pi$ to be a number which is the target probability of selecting action a in any state (s.t. $1 - \pi$ is the probability of selecting $b$), and similarly for the behaviour $\mu$.

Write an expected weight update, that uses the operator function $T$ and a value function $v$ to compute the expected weight update. The expectation should take into account the probabilities of actions in the future, as well as the steady-state (=long-term) probability of being in a state. The step size of the update should be  $\alpha=0.1$.


Here is how my solution looks like (I am a total beginner in RL and in addition to studying Rich's book, I was trying to solve the basic intro course assignments as well to help understand the topic in detail.
x1 = np.array([1., 1.])
x2 = np.array([2., 1.])

def v(w, x):
    return x.T*w

def T(w, pi, mu, l, g):
    states = [0, 1]
    n_states = len(states)
    #initial_dist = np.array([[1.0, 0.0]])
    transition_matrix = np.array([[pi, 1-pi],
                                  [pi, 1-pi]])
    
    if pi <= mu: # thresholding to select the state
        val = v(w, x1)
    else:
        val = v(w, x2)
        pi = 1 - pi

    l_power = np.power(l, n_states - 1)
    lambda_corrected = l_power * val
    lambda_corrected *= 1 - l

    return lambda_corrected - val

def expected_update(w, pi, mu, l, g, lr):
    delta = T(w, pi, mu, l, g)

    w += lr * delta
    return w

The state diagram looks like this where there are two states $s_0$ and $s_1$. All rewards are $0$ and the state features $x_0 = x(s_0)$ and $x_1 = x(s_2)$ for two states are given as $x_1$ and $x_2$ in the code ([1., 1.], [2., 1.]) and also there are only two actions in each state $a$ and $b$. Action an always transitions to state $s_0$ (i.e. from s1 or from s0 itself) and action b always transitions to state $s_1$ (i.e. from $s_0$ or $s_1$ itself): 
This is how the caller portion of the code looks like.
def caller(w, pi, mu, l, g):
  ws = [w]
  for _ in range(100):
    w = w + expected_update(w, pi, mu, l, g, lr=0.1)
    ws.append(w)
  return np.array(ws)

mu = 0.2 # behaviour
g = 0.99  # discount

lambdas = np.array([0, 0.8, 0.9, 0.95, 1.])
pis = np.array([0., 0.1, 0.2, 0.5, 1.])

I would appreciate any help.

Edit:
I tried implementing the T() following the Bellman backup operator, but I am still not sure if I did this right or not.
return pi * g*v(w, x1) + (1-pi) * g*v(w, x2)

","['reinforcement-learning', 'python', 'implementation', 'bellman-operators']",
Can residual connections be beneficial when we have a small training dataset?,"
I have a classification problem, for which an inadequate amount of training data is available. Also, there is no known practical data augmentation approach for this problem (as no unlabelled data is available either), but I am working on it.
As we know, deep neural networks require a large amount of data for training, especially when a deep architecture with many layers is used. Using these complex architectures with less data can easily lead to over-fitting. Residual connections can shortcut some blocks or layers, which can result in simpler models, while we have the benefit of complex structures.
Can residual connections be beneficial when we have a small training dataset?
","['deep-learning', 'convolutional-neural-networks', 'datasets', 'overfitting', 'residual-networks']","Can residual connections be beneficial when we have a small training dataset?The usual rule of data science investigations applies here: Try it, measure the results, then you will know.It is very hard to tell, a priori, whether a specific architectural or hyperparameter choice will impact the performance of a neural network on a given problem.In this case, you are wondering whether residual networks using skip connections might help when you have a relatively low amount of training data.On the pro side, effects of skip connections that help correct vanishing gradients, and treat each block as learning the difference from an identity function, will still work for your problem. That means you will have some freedom to explore adding layers without worrying about the negative impacts of doing so.On the con side, it is unlikley that you will benefit from very deep networks as there will not be enough examples to learn truly complex functions from.You may find that adding depth, but reducing ""width"", i.e. the number of artificial neurons in each layer, will work.If you have a low amount of training data and a difficult problem to solve, then residual networks are not a magic fix. The best you could hope for is a relatively stable statistical model that works on the simpler differences between your training examples. However, it may be possible that tuning a neural network by searching through different architectures will be a worthwhile exercise.I would also suggest that for one of the good networks that you vary the number of examples used to train the network in multiple training runs, and plot a learning graph showing number of examples versus accuracy (or other metric that you may be interested in). This graph will help you decide whether collecting more training data would be worthwhile because you will have a rough estimate of the gradient for how much new training examples could improve your results."
"Given a sequence of states followed by the agent, is it possible to find the Q-value for a state-action pair not in this sequence?","
Assume you are given a sequence of states followed by the agent, generated by a random policy, $[s_0, s_1, s_2, \dots,  s_n]$. Furthermore, assume the MDP is fully observable and time is discrete.
Is it possible to find the Q-value for a state-action pair $(s_j, a_j)$ which was not encountered along this sequence?
From my understanding of the MDP, yes, it would be possible. However, I'm unsure how to get this Q-value.
","['reinforcement-learning', 'q-learning', 'markov-decision-process', 'value-functions']",
What is the relation between self-taught learning and transfer learning?,"
I am new to transfer learning and I start by reading A Survey on Transfer Learning, and it stated the following:

according to different situations of labeled and unlabeled data in the source domain, we can
further categorize the inductive transfer learning setting into two cases:
case $(a)$ (It is irrelevant to my question).
case $(b): $ No labeled data in the source domain are
available. In this case, the inductive transfer
learning setting is similar to the self-taught
learning setting, which is first proposed by Raina
et al. [22]. In the self-taught learning setting, the
label spaces between the source and target
domains may be different, which implies the
side information of the source domain cannot be
used directly. Thus, it’s similar to the inductive
transfer learning setting where the labeled data
in the source domain are unavailable.

From that, I understand that self-taught learning is inductive transfer learning.
But I opened the paper of self-taught learning that was mentioned (i.e paper by Raina
et al. [22].), and It stated the following in the introduction:

Because
self-taught learning places significantly fewer restrictions on
the type of unlabeled data, in many practical applications
(such as image, audio or text classification) it is much easier
to apply than typical semi-supervised learning or transfer learning
methods.

And here it looks like transfer learning is different from self-taught learning.
So what is the right relation between them?
","['comparison', 'terminology', 'papers', 'unsupervised-learning', 'transfer-learning']",
"Why does $E_q[\log p(\mathbf{w}|\mathbf{z},\beta)]=\sum_{n=1}^{N}\sum_{i=1}^{k}\sum_{j=1}^{V}\phi_{ni}w_n^j\log \beta_{ij}$ hold in LDA?","
I'm having trouble understanding an equality that comes up in the original LDA paper by Blei et al.:
Consider the classical LDA model, i.e. for every document $\textbf{w}=(w_1,\ldots,w_N)$ in a text corpus $\mathcal{D}=\{w_1,\ldots,w_M\}$ assume that the document is created as follows$^{\dagger}$:

Choose $N\sim \text{Poisson}(\xi)$.

Choose $\theta\sim\text{Dir}(\alpha)$.

For each of the $N$ words $w_n$:
(a) Choose a topic $z_n\sim \text{Multinomial}(\theta)$.
(b) Choose a word $w_n$ from $p(w_n|z_n,\beta)$, a multinomial probability conditioned on the topic $z_n$.


In order to do inference for LDA, the authors use a variational approach with the following graphical model where $\gamma$ and $\phi$ are Dirichlet and multinomial parameters, respectively:

Let us write $p$ for the original LDA distribution and $q$ for the variational one. The equality I don't understand is given in the appendix$^{*}$ of the paper and states that:
$$E_q[\log p(\mathbf{w}|\mathbf{z},\beta)]=\sum_{n=1}^{N}\sum_{i=1}^{k}\sum_{j=1}^{V}\phi_{ni}w_n^j\log \beta_{ij}.$$
My work so far:
We can write the RHS as
$$\sum_{n=1}^{N}\sum_{i=1}^{k}\sum_{j=1}^{V}\phi_{ni}w_n^j\log \beta_{ij}=\sum_{n=1}^{N}\sum_{i=1}^{k}\sum_{j=1}^{V}q(z_n=i)\cdot w_n^j\cdot \log p(w_n^j=1|z_n=i)$$
and the LHS via exchangeability and de Finetti's theorem as
$$p(\mathbf{w}|\mathbf{z},\beta)]=\sum_{n=1}^{N}E_q[\log p(w_n|z_n,\beta)].$$
I now want to obtain the double sum from the right-hand side on the LHS as well. This looks like some sort of expected value with respect to $q$, of a discrete random variable that only depends on the values of $z_n$, conditional on $w_n$ (seemingly, as if $w_n$ was fixed) but the R.V. that we do have on the left-hand side is $\log p(w_n|z_n,\beta)$ which depends on both the values of $z_n$ and $w_n$, both not fixed. How do I continue?

$^{\dagger}$ Also assume that both $M$, the number of documents, and $k$, the dimensionality of the Dirichlet, are known and fixed. Furthermore, let $V$ denote the number of possible words (the ""size of the vocabulary"") and write every word as a $\{0,1\}^V$ vector with zeros everywhere except for the index of the word in the vocabulary list. Finally, let $\beta$ be a $k\times V$ matrix with $\beta_{ij}=p(w^j=1|z^i=1)$ and assume that both words and documents are exchangeable.
$^*$ Eq. 15, A.3
","['machine-learning', 'papers', 'topic-model', 'latent-dirichlet-allocation']",
I want to determine how similar a given song is to Queen's songs. Am I headed in the right direction?,"
I've asked this question before (@ Reddit) and people suggested CNNs on a mel spectrogram more than anything else. This is great.
But I'm sort of stuck at: label some music data as ""queen"" and ""not queen"" and have this be the training set. Like, download 300 songs, 70 queen (that's all they have) and 230 not queen, create their mel spectrograms using some python package that can do that.
First of all, is 300 songs even enough?
I only have a basic understanding of what I'm doing. I need some help
","['convolutional-neural-networks', 'audio-processing']",
Issue with graphical interpretation of the universal approximation theorem,"
This article attempts to provide a graphical justification of the universal approximation theorem.
It succeeds in showing that a linear combination of two sigmoids can produce essentially a bounded constant function or step function, and thus can therefore to a reasonable degree of approximation produce any function by essentially splitting up any function into a cluster (linear combination?) of these towers or steps.
However, he produced the steps and towers using specific weight parametrizations.
However, since when are we allowed to specify weights and biases? Isn't this all out of our hands and in the hands of cost function minimization?
I don't understand why he was dealing with setting weights to this, biases to that, when in my experience that is all done by ""the machine"" to minimize the cost function. I doubt the weights to minimize the cost function are arranged in the ways specified in order to form the towers and steps that were formed in this tutorial, so I kind of don't understand what all the hub-ub is all about.
","['feedforward-neural-networks', 'stochastic-gradient-descent', 'universal-approximation-theorems']",
Bellman Expectation Equation leading to results where value iteration would not converge to the optimal policy,"
When applying the bellman expectation equation:
$$v(s)=\mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) \mid S_{t}=s\right]$$
to the MRP below, states further away from the terminal state will have the same value ${v(s)}$ as states closer to the terminal states. Even though it is clear that the expected total reward from states further away is lower. If the discount factor $\gamma$ would be even lower states further away would get a higher value. If we now make this an MDP where the agent can decide to go either direction from all states (with the first state having an action leading to itself), the agent would then choose to go further away from the terminal. Getting less reward over the whole episode. So, this seems to be an example where policy/value iteration would not converge to an optimal policy. I know there is something wrong with the reasoning here. I just cannot seem to figure out what.
What am I missing here?

EDIT: So, the problem actually was that I didn't take into account that the terminal state has to get a value of 0. If you put it at 0 at all times this will converge as expected because all the other states will get lower and lower values while, assuming a greedy policy, the one-to-last state will retain a value of -1. After a bit over 10 iterations (if gamma is close to 1) it will converge because the states further away will get a value less than -1.
","['value-functions', 'bellman-equations', 'value-iteration', 'policy-iteration']",
Can RNNs get inputs and produce outputs similar to the inputs and outputs of FFNNs?,"
RNN and LSTM models have many architectures that can be modified. We can also compose their input and output data. However, in the examples that I found on the web, the inputs and outputs of RNNs/LSTMs are usually sequences.
Let's say we have a 3-column dataset:
data= np.array([[1.022 0.94  1.278]
                [2.096 1.404 2.035]
                [1.622 2.348 1.909]
                [1.678 1.638 1.742]
                [2.279 1.878 2.045]])

where the first two columns contain the inputs (features) and the third one contains the labels.
Usually, when modeling with feedforward neural networks (FFNNs), the input and output look like this:
Input:
x_input = np.vstack((data[:, 0], data[:, 1])).reshape(5, 2)

[[1.022 2.096]
 [1.622 1.678]
 [2.279 0.94 ]
 [1.404 2.348]
 [1.638 1.878]]

Output:
y_output = np.vstack((data[:, 2])).reshape(5, 1)

[[1.278]
 [2.035]
 [1.909]
 [1.742]
 [2.045]]

When modeling with RNN, the input and output are:
Input:
[[1.022 0.94  1.278]
 [2.096 1.404 2.035]
 [1.622 2.348 1.909]

Output (as a sequence):
 [1.678 1.638 1.742]
 [2.279 1.878 2.045]]

I would like to ask: Is it possible to model the input and output as an ANN model when modeling with RNN? Would it be correct?
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'feedforward-neural-networks', 'multilayer-perceptrons']","Yes, it is possible. What you have shown in case of ANN is what happens in a regression model using NNs. What you have shown in case of RNN is what happens when you are doing sequence-to-sequence translation (like French to English).If you want to get single values like in case of ANN, suppose you are doing regression, then, in the end, you will flatten the features aggregated by RNN (in case of Tensorflow, use Flatten layer and in case of PyTorch, you can directly do it). It should be then followed by a dense layer of 3 (in case of Tensorflow) or linear layer of 3 (in case of PyTorch), if I am talking about your example.Since, you have shown values above 1, I presume you are doing some kind of regression. But, it would be good idea to normalize your outputs in case of regression, it makes the optimization easier.If you want to classification then in the last layer, use Dense layer with softmax (in case of Tensorflow) and softmax followed by a linear layer (in case of PyTorch)."
Why does the implementation of REINFORCE algorithm minimize the gradient term but not the loss?,"
I read the book ""Foundation of Deep Reinforcement Learning,  Laura Graesser and Wah Loon Keng"", and when I go through the REINFORCE algorithm, they show the objective function:
$$
J\left(\pi_{\theta}\right)=\mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)]=\mathbb{E}_{\tau \sim \pi_{\theta}}\left[\sum_{t=0}^{T} \gamma^{t} r_{t}\right]
$$
and the gradient of the objective:
$$
\nabla_{\theta} J\left(\pi_{\theta}\right)=\mathbb{E}_{\tau \sim \pi_{\theta}}\left[\sum_{t=0}^{T} R_{t}(\tau) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)\right]
$$
But when they implement it,
class Pi(nn.Module):
    def __init__(self, in_dim, out_dim):
        super(Pi, self).__init__()
        layers = [
                nn.Linear(in_dim, 64),
                nn.ReLU(),
                nn.Linear(64, out_dim)
        ]
        self.model = nn.Sequential(*layers)
        self.onpolicy_reset()
        self.train()

    def onpolicy_reset(self):
        self.log_probs = []
        self.rewards = []

    def forward(self, x):
        pdparam = self.model(x)
        return pdparam

    def act(self, state):
        x = torch.from_numpy(state.astype(np.float32))
        pdparam = self.forward(x) # (1, num_action), each number represent the raw logits for that specific action
        # model contain the paremeters theta of the policy, pd is the probability 
        # distribution parameterized by model's theta 
        pd = Categorical(logits = pdparam)
        action = pd.sample()
        log_prob = pd.log_prob(action)
        self.log_probs.append(log_prob)
        return action.item()

def train(pi, optimizer):
    T = len(pi.rewards)
    rets = np.empty(T, dtype = np.float32)
    future_ret = 0.0
    for t in reversed(range(T)):
        future_ret = pi.rewards[t] + gamma*future_ret
        rets[t] = future_ret

    rets = torch.tensor(rets)
    log_probs = torch.stack(pi.log_probs)
    loss = -log_probs*rets
    loss = torch.sum(loss)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss

def main():
    env = gym.make('CartPole-v0')
    # in_dim is the state dimension
    in_dim = env.observation_space.shape[0]
    # out_dim is the action dimension
    out_dim = env.action_space.n
    pi = Pi(in_dim, out_dim)
    optimizer = optim.Adam(pi.parameters(), lr = 0.005)
    for epi in range(300):
        state = env.reset()
        for t in range(200): # max timstep of cartpole is 200
            action = pi.act(state)
            state, reward, done, _ = env.step(action)
            pi.rewards.append(reward)
        # env.render(mode='rgb_array')
            if done:
                break
        loss = train(pi, optimizer)
        total_reward = sum(pi.rewards)
        solved = total_reward > 195.0
        pi.onpolicy_reset()
        print(f'Episode {epi}, loss: {loss}, total reward: {total_reward}, solve: {solved}')
    return pi

In train(), they minimize the gradient term, and I can not understand why is that.
Can someone shed light on that?
I am new to this so please forget me if this question is stupid.
","['reinforcement-learning', 'objective-functions', 'policy-gradients', 'reinforce']","They are not maximizing the gradient, the gradient is of the form
\begin{equation}
\nabla_{\theta} J \approx \sum_{t=0}^T G_t \nabla_{\theta} \log(\pi_{\theta}(a_t|s_t))
\end{equation}
that means that when implementing it in software you can form your objective as
\begin{equation}
J = \sum_{t=0}^T G_t \log(\pi_{\theta}(a_t|s_t))
\end{equation}
and then taking the gradient of that objective is equal to the policy gradient."
Understanding advantage estimator in proximal policy optimization,"
I was reading Proximal Policy Optimization paper. It states following:

The advantage estimator used is:
$\hat{A}_t=-V(s_t)+r_t+\gamma r_{t+1}+...+\gamma^{T-t+1}r_{T-1}+\color{blue}{\gamma^{T-t}}V(s_T) \quad\quad\quad\quad\quad\quad\quad(10)$
where $t$ specifies the time index in $[0, T]$, within a given length-$T$ trajectory segment. Generalizing
this choice, we can use a truncated version of generalized advantage estimation, which reduces to
Equation (10) when $λ = 1$:
$\hat{A}_t=\delta_t+(\gamma\lambda)\delta_{t+1}+...+(\gamma\lambda)^{T-t+1}\delta_{T-1}\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(11)$
where, $\delta_t=r_t+\gamma V(s_{t+1})-V(s_t)\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(12)$

How equation (11) reduces to equation (10). Putting $\lambda=1$ in equation (11), we get:
$\hat{A}_t=\delta_t+\gamma\delta_{t+1}+...+\gamma^{T-t+1}\delta_{T-1}$
Putting equation (12) in equation (11), we get:
$\hat{A}_t$
$=r_t+\gamma V(s_{t+1})-V(s_t) $
$+\gamma[r_{t+1}+\gamma V(s_{t+2})-V(s_{t+1})]+...$
$+\gamma^{T-t+1}[r_{T-1}+\gamma V(s_{T})-V(s_{T-1})]$
$=-V(s_t)+r_t\color{red}{+\gamma V(s_{t+1})} $
$+\gamma r_{t+1}+\gamma^2 V(s_{t+2})\color{red}{-\gamma V(s_{t+1})}+...$
$+\gamma^{T-t+1}r_{T-1}+\color{blue}{\gamma^{T-t+2}} V(s_{T})-V(s_{T-1})$
I understand the terms cancels out. I am not getting the difference in blue colored power of $\gamma$ in last terms. I must have made some stupid mistake.
","['reinforcement-learning', 'policy-gradients', 'actor-critic-methods', 'proximal-policy-optimization']",
How does the output distribution of a GAN change if the parameters are slightly purturbed?,"
Suppose $G_{\phi}:\mathcal{Z}\rightarrow \mathcal{X}$ is a generator (neural network, non-invertible) that can sample from some distribution $\pi$ on $\mathcal{X}$. That is, $G_{\phi}(z)\sim \pi$ when $z\sim \mathcal{N}(0,I)$. Let $\phi+\delta_{\phi}$ represent a (small) perturbation of the parameters of $G_{\phi}$ and let $G_{\phi+\delta_{\phi}}(z)\sim \pi'$ when $z\sim \mathcal{N}(0,I)$.
Are there any results that quantify or bound $\mathcal{D}(\pi,\pi')$ in terms of $\delta_{\phi}$, where $\mathcal{D}$ is a distance measure for distributions (let's say KL-divergence, or the Wasserstein-1 distance)?
Basically, I want to know what kind of geometry is induced on the space of distributions by the Euclidian geometry on the parameter space of a generative adversarial network.
To explain further, let's consider a parametric family of distributions $p_{\phi}$, where $\phi\in\Phi$ (some parameter space). It is a fairly well-known result in statistics that $\text{KL}(p_{\phi}||p_{\phi+\delta_{\phi}})\approx \frac{1}{2}\delta_{\phi}^\top F_{\phi} \delta_{\phi}$, where $F_{\phi}$ is the Fisher information matrix. When the family $p_{\phi}$ is generated by a GAN with parameter $\phi$ (in which case we don't know $p_{\phi}$ in closed-form), can we have an analogous result?
",['generative-adversarial-networks'],
Is it normal getting noise values in the error history along training iteration?,"
I'm giving my first steps in really learning machine learning.
As an exercise in my online course, it was asked for me to code the Cost function of some neural network that should resolve the handwritten problem with digits between 1 to 10.
As most of you know, the cost function of NN is given by:
$$
J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[-y_{k}^{(i)} \log \left(\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)-\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)\right]
$$
so I tried to code it considering the following information:

where $\mathrm{h_{\theta}}(\mathrm{x^{i}})$ is computed as shown in Figure 2 and $K=10$ is the total number of possible labels. Note that $h_{\theta}\left(x^{(i)}\right)_{k}= a_{k}^{(3)}$ is the activation (output value) of the $k$ -th output unit Also, recall that whereas the original labels (In the variable $y$) were $1,2, \ldots, 10$, for the purpose of training a neural network, we need to recode the labels as vectors containing only values 0 or 1, so that
$$
y=\left[\begin{array}{c}
1 \\
0 \\
0 \\
\vdots \\
0
\end{array}\right],\left[\begin{array}{l}
0 \\
1 \\
0 \\
\vdots \\
0
\end{array}\right], \ldots . \text { or }\left[\begin{array}{c}
0 \\
0 \\
0 \\
\vdots \\
1
\end{array}\right]
$$
For example, if $x^{i}$ is an image of the digit $5,$ then the corresponding $y^{i n}$ (that you should use with the cost function) should be a $10-$ dimensional vector with $y_{5}=1,$ and the other elements equal to $0$. You should implement the feedforward computation that computes $\mathrm{h_{\theta}}(\mathrm{x^{i}})$ for every example $i$ and sum the cost overall examples. Your code should also work for a dataset of any size, with any number of labels (you can assume that there are always at least $K \geq 3$ labels)

plotting the graph, I got:

here's my error cost function code:
function [J grad] = nnCostFunction(nn_params, ...
                                   input_layer_size, ...
                                   hidden_layer_size, ...
                                   num_labels, ...
                                   X, y, lambda)

% Setup some useful variables
 m = size(X, 1)
% bias  = ones(m,1)';
Theta1 = [ ones(401,1)  reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
                 hidden_layer_size, (input_layer_size + 1))']';

Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
                 num_labels, (hidden_layer_size + 1));

J =0;
error_history = zeros(1,m);
y_coded = zeros(1,num_labels);
for i = 1:m
    y_coded(y) = 1;
    X_in = [1 X(i,:)]';
    hypotesis_array = sigmoid(Theta2*sigmoid(Theta1*X_in));
    
    for k =1:num_labels
        J = J  -(y_coded(k)*log10(hypotesis_array(k)) -  (1- y_coded(k))*log10(hypotesis_array(k)));
    end
    J =J/m;
    error_history(i) = J;
    
    
end
plot(1:5000, error_history);
ylabel(""Error_Value"");
xlabel(""training iteration"");

I did that considering that the weights were previously given.
Is it normal getting this noise error history value or I did something wrong with my code?
","['neural-networks', 'objective-functions']",
Are there RL algorithms that also try to predict the next state?,"
So far I've developed simple RL algorithms, like Deep Q-Learning and Double Deep Q-Learning. Also, I read a bit about A3C and policy gradient but superficially.
If I remember correctly, all these algorithms focus on the value of the action and try to get the maximum one. Is there an RL algorithm that also tries to predict what the next state will be, given a possible action that the agent would take?
Then, in parallel to the constant training for getting the best reward, there will also be constant training to predict the next state as accurately as possible? And then have that prediction of the next state always be passed as an input into the NN that decides on the action to take. Seems like a useful piece of information.
","['reinforcement-learning', 'deep-rl', 'model-based-methods', 'algorithm-request']","Yes, there are algorithms that try to predict the next state. Usually this will be a model based algorithm -- this is where the agent tries to make use of a model of the environment to help it learn. I'm not sure on the best resource to learn about this but my go-to recommendation is always the Sutton and Barto book.This paper introduces PlanGAN; the idea of this model is to use a GAN to generate a trajectory. This will include not only predicting the next state but all future states in a trajectory.This paper introduces a novelty function to incentivise the agent to visit unexplored states. The idea is that for unexplored states, a model that predicts the next state from the state-action tuple will have high error (measured by Euclidean distance from true next state) and they add this error to the original reward to make a modified reward.This paper introduces Dreamer. This is where all learning is done in a latent space and so the transition dynamics of this latent space must be learned, another example of needing to learn the next state.These are just some examples of papers that try to predict the next state, there are many more out there that I would recommend you look for."
Optimizer that prevents parameters from oscillating,"
When we perform gradient descent, especially in an online setting where the training data is presented in a non-random order, a particular 1-dimensional parameter (such as an edge weight) may first travel in one direction, then turn around and travel the other way for a while, then turn around and travel back, and so forth.  This is wasteful, and the problem is that the learning rate for that parameter was too high, making it overshoot the optimal point.  We don't want parameters to oscillate as they are trained; instead, ideally, they should settle directly to their final values, like a critically damped spring.
Is there an optimizer that sets learning rates based on this concept?  Rprop seems related, in that it reduces the learning rate whenever the gradient changes direction.  The problem with Rprop is that it only detects oscillations of period 2.  What if the oscillation is longer, e.g. the parameter is moving in a sine wave with a period of dozens or hundreds of time steps?  Looking for an optimizer that can suppress oscillations of any period length.
Let's be specific.  Say that $w$ is a parameter, receiving a sequence of gradient updates $g_0, g_1, g_2, ... $ .  I am looking for an optimizer that would pass the following tests:

If $g_t = sin(t) - w$, then $w$ should settle to the value 0.
If $g_t = sin(t) + 100 - 100 cos(0.00001t) - w$, then $w$ should settle to the value 100.
If $g_t = sin(t) - w$ for $0 < t < 1000000$, and $g_t = sin(t) + 100 - w$ for $1000000 \leq t$, then $w$ should at first settle to the value 0, and then not too long after time step $1000000$ it should settle to the value 100.
If $g_t = sin(t) - w$ for $floor(t / 1000000)$ even, and $g_t = sin(t) + 100 - w$ for $floor(t / 1000000)$ odd, then $w$ should at first settle to the value 0, then not too long after time step $1000000$ it should settle to the value 100, and then not too long after step $2000000$ it should settle back to 0, but eventually after enough iterations it should settle to the value 50 and stop changing forever after.

","['optimization', 'gradient-descent']",
"In ensemble learning, does accuracy increase depending on the number of models you want to combine?","
I want to predict using the same model as multivariate time series data in a time series prediction problem.
Example:
pa = model predict result(a)
pb = model predict result(b)
pc = model predict result(c)
...
model ensemble([pa, pb, pc,...]) -> predict(y)

Can I expect a better performance of our model by using a model ensemble with more kinds of time series data here?
","['machine-learning', 'ensemble-learning']","Yes, you can. Let's say you have 5 classes named a,b,c,d,e. You fit your data into a SVM Classifier and a Random Forest Classifier. Assume that, SVM classified ""a"" and ""b"" class well and RFC classified ""c"",""d"",""e"" well. So, ensembling these two models is going to increase accuracy  dramatically. Ensemble learning is really good when a model generating many ""False-Positive"" and ""False-Negative"". You can also use weighted ensemble learning methods. You can set greater weights for reliable models & lower weights for untrustable models."
"In a convolutional neural network, how is the error delta propagated between convolutional layers?","
I'm coding some stuff for CNNs, just relying on numpy (and scipy just for the convolution operation for pure performance reasons).
I've coded a small network consisting of a convolutional layer with several feature maps, the max pooling layer and a dense layer for the output, so far so good, extrapolating the backpropagation from fully connected neural networks was quite intuitive.
But now I'm stuck when several convolutional layers are chained. Imagine the following architecture:

Output neurons: 10
Input matrix (I): 28x28
First convolutional layer (CN1): 3x5x5, stride 1 (output shape is 3x24x24)
First pooling layer (MP1): 2x2 (output shape is 3x12x12)
Second convolutional layer (CN2): 3x5x5, stride 1(output shape is 3x8x8)
Second pooling layer (MP2): 2x2 (output shape is 3x4x4)
Dense layer (D): 10x48 (fully connected to flattened MP2)

Propagating the error back:

Error delta in output layer: 10x1 (cost delta)
Error delta in MP2: 3x4x4 (48x1 unflattened, calculating the error delta for the dense layer as usual)
Error delta in CN2: 3x8x8 (error delta of MP2 but just upsampled)

How do I keep from here? I don't know how to keep propagating the error to the previous layer, if the error delta in the current one is 3x8x8, and the kernel 3x5x5, performing the convolution between the error delta and the filter for calculating the delta for the previous layer, that gives a 3x4x4 delta.
","['neural-networks', 'convolutional-neural-networks', 'backpropagation']",
"How can ""any process you can imagine"" be thought of as function computation?","
I stumbled upon this passage when reading this guide.

Universality theorems are a commonplace in computer science, so much
so that we sometimes forget how astonishing they are. But it's worth
reminding ourselves: the ability to compute an arbitrary function is
truly remarkable. Almost any process you can imagine can be thought of
as function computation.* Consider the problem of naming a piece of
music based on a short sample of the piece. That can be thought of as
computing a function. Or consider the problem of translating a Chinese
text into English. Again, that can be thought of as computing a
function.  Or consider the problem of taking an mp4 movie file and
generating a description of the plot of the movie, and a discussion of
the quality of the acting. Again, that can be thought of as a kind of
function computation.* Universality means that, in principle, neural
networks can do all these things and many more.

How is this true? How can any process be thought of as function computation? How would one compute function in order to translate Chinese text to English?
","['neural-networks', 'machine-learning', 'deep-learning', 'function-approximation', 'universal-approximation-theorems']",
What are some use cases of discrete optimization in Deep Learning?,"
When we talk of optimization, it usually boils down to gradient descent and its variants in the context of deep learning. However, I wonder if there are some works that use discrete optimization in one way or another in deep learning.
In brief, what are some applications of discrete optimization to deep learning?
","['neural-networks', 'deep-learning', 'optimization']",
Do Gradient Descent and Natural Gradient solve the same problem?,"
I am troubled by natural gradient methods.
If we have a function f(x) we wish to minimize, gradient descent minimizes f(x) of course, but what does the natural gradient do?
I found on https://towardsdatascience.com/natural-gradient-ce454b3dcdfa:

Instead of fixing the euclidean distance each parameter moves(distance in the parameter space), we can fix the distance in the distribution space of the target output.

Where did the distributions come from? If we wish to minimize f(x), the target output is just a minimizer x* right, and not a distribution, or am I missing something?
","['deep-learning', 'gradient-descent', 'geometric-deep-learning', 'information-theory']",
Is the target assumed to be a noisy version of the output of the model in machine learning?,"
I wonder if the following equation (you can find it in almost every ML book) refers to a general assumption that we make when using machine learning:
$$y = f(x)+\epsilon,$$
where $y$ is our output, $f$ is e.g. a neural network and $\epsilon$ is an independent noise term.
Does this mean that we assume the $y$'s contained in our training data set come from a noised version of our network output?
","['machine-learning', 'deep-learning', 'statistical-ai']",
How to transfer declarative knowledge into neural networks,"
Humans learn facts about the world like ""most A are B"" by own experience and by being told so (by other people or texts). The systems and mechanisms of storage and usage of such facts (by an ""experience system"" and a ""declarative system"") are presumably quite different and may have to do with ""episodic memory"" and ""semantic memory"". Nevertheless at least in the human brain the common currency are synaptic weights, and it would be quite interesting to know how these two systems cooperate.
I assume that machine learning is mainly concerned with ""learning by own experience"" (= training data + annotations), be it supervised or unsupervised learning. I wonder which approaches there are that allow a neural network to ""learn by being told"". One brute force approach might be to translate a declarative statement like ""most A are B"" into a set of synthetic training data, but that's definitely not how it works for humans.
","['neural-networks', 'machine-learning', 'deep-learning', 'declarative-programming']",
How can I train a model to recognize object with zoomed-in image?,"
Humans are good at guessing animals with zoomed-in images from patterns of fur/skin.
(For example, if we saw a black-white pattern fur, it must be a zebra)
I have some experience guessing a car model from an interior/exterior photo without a brand logo.
(based on the dashboard/gear level/air vent or something like that)
I think it would be helpful for my coworkers to have such a model.
(I'm working at a car forum, and I have some limited experience working with TensorFlow).
Is this possible?
Where should I start with?
","['image-recognition', 'object-recognition']",
How do I implement the cross-entropy-method for a RL environment with a continuous action space?,"
I found many tutorials and posts on how to solve RL environments with discrete action spaces using the cross entropy method (e.g., in this blog post for the OpenAI Gym frozen lake environment).
However now I have built my first custom environment, which simulates a car driving on a road with leading and following vehicles. I want to control the acceleration of my vehicle without crashing into anyone. The state consists of the velocity and distance to the leading and following vehicles. The observation and action spaces are continuous and not discrete, which is why I cannot implement my training loop like in the examples that use the cross entropy method. That is, because the method relies on modifying each tuple for training <s, a, r> (state, action, reward) so that the probability distribution in a is equal to 1 in one dimension and equal to 0 in all others (meaning, it it very confident in its action, i.e., [0, 1, 0, 0]).
How do I implement the cross entropy method for a continuous action space (in Python and Pytorch) or is that even possible? The answer to this question probably describes what I want to do in a very mathematical form, but I do not understand it.
","['neural-networks', 'reinforcement-learning', 'cross-entropy']",
"How to simplify policy gradient theorem to $E_{\pi}[G_t \frac{\nabla_{\theta}\pi(a|S_t,\theta)}{\pi(a|S_t,\theta)}]$?","
In ""Introduction to Reinforcement Learning"" (Richard Sutton) section 13.3(Reinforce algorithm) they have the following equation:
\begin{align}
 \nabla_{\theta}J &\propto \sum_s \mu(s) \sum_a q_{\pi}(s,a)\nabla_{\theta}\pi(a|s,\theta) \\
                  &= E_{\pi}[\sum_a q_{\pi}(S_t,a) \nabla_{\theta}\pi(a|S_t,\theta)] \tag{1}\label{1}
\end{align}
But in my opinion equation 1 should be expectation over state distribution: $$E_{\mu}[\sum_a q_{\pi}(S_t,a) \nabla_{\theta}\pi(a|S_t,\theta)]$$
If I am right here then the rest of the lines follows like this:
\begin{align}
\nabla_{\theta}J &= E_{\mu}[\sum_a q_{\pi}(S_t,a) \nabla_{\theta}\pi(a|S_t,\theta)] \\
                  &= E_{\mu}[\sum_a \pi(a|S_t,\theta) q_{\pi}(S_t,a) \frac{\nabla_{\theta}\pi(a|S_t,\theta)}{\pi(a|S_t,\theta)}] \\
                   &= E_{\mu}[E_{\pi}[q_{\pi}(S_t,A_t)\frac{\nabla_{\theta}\pi(A_t|S_t,\theta)}{\pi(A_t|s,\theta)}]]\\
                   &= E_{\mu}[E_{\pi}[G_t\frac{\nabla_{\theta}\pi(a|S_t,\theta)}{\pi(a|S_t,\theta)}]]
\end{align}
Now the final update rule using stochastic gradient descent will be:
$$\triangle \theta = \alpha E_{\pi}[G_t\frac{\nabla_{\theta}\pi(A_t|S_t,\theta)}{\pi(A_t|S_t,\theta)}] \tag{2}$$
I think I am doing something wrong here because this equation 2 does not match with the book also with other materials. Can anyone please show me where I am doing wrong?
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'reinforce', 'sutton-barto']",
MicroPython MicroMLP: How do I reward the program based on state?,"
I have been trying to use MicroMLP to teach a small neural network to converge to correct results.  Ultimately, I want to have three outputs, one which is high priority (X must be as close to XTarget as possible; Y and Z must be within bounds, and should approach YTarget and ZTarget as best as possible).  Right now, I'm just trying to get convergence of one variable to understand this library.
The below code works for xor, but I don't really understand why it works, or how to extend this to reward behavior:
from microMLP import MicroMLP
import utime
import random
import machine
import gc

DEPTH=3
mlp = MicroMLP.Create( neuronsByLayers           = [DEPTH, DEPTH, 1],
                       activationFuncName        = MicroMLP.ACTFUNC_GAUSSIAN,
                       layersAutoConnectFunction = MicroMLP.LayersFullConnect )

nnFalse  = MicroMLP.NNValue.FromBool(False)
nnTrue   = MicroMLP.NNValue.FromBool(True)

led = machine.Pin(25, machine.Pin.OUT)
tl = 0
xor = []
c=0
for i in range(5000):
    if not i % 100:
        led.toggle()
        gc.collect()
        print("" Iteration: %s \t Correct: %s of 10"" % (i,c))
        c = 0
    xor.clear()
    xorOut = nnFalse
    for j in range(DEPTH):
        if random.random() > 0.5:
            xor.append(nnTrue)
            xorOut = nnFalse if xorOut == nnTrue else nnTrue
        else:
            xor.append(nnFalse)
    p = mlp.Predict(xor)
    mlp.QLearningLearnForChosenAction(None, xorOut, xor, 0)
    if p[0].AsBool == xorOut.AsBool:
        c += 1

led.off()

print( ""LEARNED :"" )

c = 0
tries = 0
for i in range(100):
    led.toggle()
    gc.collect()
    xor.clear()
    xorOut = nnFalse
    for j in range(DEPTH):
        if random.random() > 0.5:
            xor.append(nnTrue)
            xorOut = nnFalse if xorOut == nnTrue else nnTrue
        else:
            xor.append(nnFalse)
    tries += 1
    p = mlp.Predict(xor)
    c += 1 if mlp.Predict(xor)[0].AsBool == xorOut.AsBool else 0
 
print( ""  %s of %s"" % (c, tries) )

del mlp
print(gc.mem_alloc())
gc.collect()
print(gc.mem_alloc())

I'm trying to achieve two goals, first for me to understand, second for the machine to do useful work.
Goal #1:  learn to adjust a value properly.
Inputs:

Target (0,1)
Value (0,1)

Outputs:

An adjustment toward Value (-1,1)

Possibly this has to be (0,1) so I've considered using the adjustment as adjustment - 0.5 to put it into the (-0.5,0.5) range



I want to reward the thing based on the degree to which value comes closer to the target.  (As a special case, if it's impossible to adjust that far given the output, I want to maximize its reward for making the maximum adjustment.)  I don't want to know the value adjustment should target; I only want to know that whatever value it gave produced a state I like better, and what that value was.  If I can know the correct output, I don't need deep learning.
Goal #2, the later one I expect to be able to do myself if I can get one variable working, is to have several inputs and three outputs.  These inputs relate to the current targets and the deviation from those targets.  One of these is of the highest priority to track toward a target value; the other two should track toward a target value, but are allowed to deviate by some amount with no harm done.  If I can just figure out how to use the neural network, I should be able to assemble that.
Does this sound reasonable?  Is this the correct tool, or is Q-Learning wrong for this?
Feel free to suggest a better package for regular Python as well, although MicroMLP is the only usable one of which I'm aware for the platform I'm targeting.  I'll likely want a much more powerful one that I can use with extra available hardware if present.
If I get something I can work with, I'll write documentation and submit a PR to the MicroMLP repo so nobody has to ask this again.
","['neural-networks', 'python', 'q-learning']",
What is the difference between a distribution model and a sampling model in Reinforcement Learning?,"
The book from Sutton and Barto, Reinforcement Learning: An Introduction, define a model in Reinforcement Learning as

something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave.

In this answer, the answerer makes a distinction:

There are broadly two types of model:

A distribution model which provides probabilities of all events. The most general function for this might be $p(r,s'|s,a)$ which is the probability of receiving reward $r$ and transitioning to state $s'$ given starting in state $s$ and taking action $a$.

A sampling model which generates reward $r$ and next state $s'$ when given a current state $s$ and action $a$. The samples might be from a simulation, or just taken from history of what the learning algorithm has experienced so far.



The main difference is that in sampling models I only have a black box, which, given a certain input $(s,a)$, generates an output, but I don't know anything about the probability distributions of the MDP. However, having a sampling model, I can reconstruct (approximately) the probability distributions by running thousands of experiments (e.g. Monte Carlo Tree Search).
On the other hand, if I have a distribution model, I can always sample from it.
I was wondering if

what I wrote is correct;

this distinction has been remarked in literature and where I can find a more in-depth discussion on the topic;

someone has ever separated model-based algorithms which use a distribution model and model-based algorithms which use only a sampling model.


","['reinforcement-learning', 'comparison', 'reference-request', 'model-based-methods', 'transition-model']",
Accuracy Not Going Above 30%,"
I am trying to make a big classification model using the coco2017 dataset. Here is my code:
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import IPython.display as display
from PIL import Image, ImageSequence
import os
import pathlib
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2
import datetime

gpus = tf.config.list_physical_devices('GPU')
if gpus:
  try:
    # Currently, memory growth needs to be the same across GPUs
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")
  except RuntimeError as e:
    # Memory growth must be set before GPUs have been initialized
    print(e)

epochs = 100
steps_per_epoch = 10
batch_size = 70
IMG_HEIGHT = 200
IMG_WIDTH = 200

train_dir = ""Train""
test_dir = ""Val""

train_image_generator = ImageDataGenerator(rescale=1. / 255)

test_image_generator = ImageDataGenerator(rescale=1. / 255)

train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                           directory=train_dir,
                                                           shuffle=True,
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           class_mode='sparse')

test_data_gen = test_image_generator.flow_from_directory(batch_size=batch_size,
                                                         directory=test_dir,
                                                         shuffle=True,
                                                         target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                         class_mode='sparse')

model = Sequential([
    Conv2D(265, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Flatten(),
    keras.layers.Dense(256, activation=""relu""),
    keras.layers.Dense(128, activation=""relu""),
    keras.layers.Dense(80, activation=""softmax"")
])

optimizer = tf.keras.optimizers.Adam(0.001)
optimizer.learning_rate.assign(0.0001)

model.compile(optimizer='adam',
              loss=""sparse_categorical_crossentropy"",
              metrics=['accuracy'])

model.summary()
tf.keras.utils.plot_model(model, to_file=""model.png"", show_shapes=True, show_layer_names=True, rankdir='TB')
checkpoint_path = ""training/cp.ckpt""
checkpoint_dir = os.path.dirname(checkpoint_path)

cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

os.system(""rm -r logs"")

log_dir = ""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
history = model.fit(train_data_gen,steps_per_epoch=steps_per_epoch,epochs=epochs,validation_data=test_data_gen,validation_steps=10,callbacks=[cp_callback, tensorboard_callback])
model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
model.save('model.h5', include_optimizer=True)

test_loss, test_acc = model.evaluate(test_data_gen)
print(""Tested Acc: "", test_acc)
print(""Tested Acc: "", test_acc*100, ""%"")

I have tried different optimizers like SGD, RMSProp, and ADAM. I also tried changing the configuration of the hidden layers. I also tried to change the metrics from accuracy to sparse_categorical_accuracy with no improvement. I cannot go beyond 30% accuracy. My guess is that the MaxPooling is doing something because I just added it but don't know what it means. Can somebody explain what the MaxPooling Layer does and what is stopping my neural network from gaining accuracy?
","['convolutional-neural-networks', 'classification', 'image-recognition', 'accuracy', 'coco-dataset']","You have two questions in one.I would say no, the maxpool is a standard operation for convolution networks, it down-samples the intermediate representation to reduce the necessary computations, improve the regularization, and adds translation invariance to some degree. Originally averaging was used to downsample over few neighbor pixels, for example, 2x2 were averaged to one pixel. Then it was discovered max-pool often performs better in practice, where you took the max value out of these 2x2 pixels. The way you applied is ok in general.I see two issues here - first one is COCO dataset is not a classification dataset. It's an object detection dataset and there are many objects on the same image. I.e. there is an image with a person on a bicycle and a car behind him. Which class the model should assign - a person, a bicycle, or a car? The model can't know. To check if it's the issue try top-5 accuracy - it tells if the correct answer would be among top-5 guesses of the network. I would also recommend to watch the images and try to manually guess the class for few dozens of them, that would help to build the intuitionThe second thing is that your model is not that deep and 30% accuracy is not bad, i.e. the random guess would be around 1% and your model doing x30 times better. You could try models like resnet - it's still quite fast, but should be doing noticeably better."
Are hill climbing variations always optimal and complete?,"
Are hill climbing variations (like steepest ascent hill climbing, stochastic hill climbing, random restart hill climbing, local beam search) always optimal and complete?
","['search', 'proofs', 'hill-climbing', 'optimality', 'completeness']","No, they are prone to get stuck in local maxima, unless the whole search space is investigated.A simple algorithm will only ever move upwards; if you imagine you're in a mountain range, this will not get you very far, as you will need to go down before going up higher. You can see that going down a bit will have a net benefit, but the search algorithm will not be able to see that.Random restart (and similar variations) allow you to do that, up to a point. Imagine you have ten people that you parachute over your mountain range, but they can only go upwards. Now you've got a better chance of finding a higher peak, but there's still no guarantee that any of them will reach the highest one."
How can Image Caption work?,"
I have two models and a file contains captions for images. The output of model 1 is .pkl files that contain the features of the images. Model 2 is the language model that will be trained with the captions. How can I link between two models to predict a caption for any image? The output of model 1 should be the input of model 2. But the features only are not enough so the input of model 2 will be .pkl files + caption file. Right?
If someone can help me in getting the link between the two models, I will appreciate it.
","['neural-networks', 'convolutional-neural-networks', 'long-short-term-memory', 'object-detection', 'feature-extraction']",The Standard Image Captioning Pipeline is to train the model in a single batch(or mini-batch) i.e. get the features from the CNN Image encoder and then feed that into an RNN decoder (features + Real Captions) to produce output captions for the Image.The training loop in PyTorch would look something like this:I'd suggest you go through the paper Show and Tell: A Neural Image Caption Generator.I also made this Kaggle Kernel implementing the paper from scratch. Should help clear up any other doubts.
Extracting values from text based on keywords,"
I am trying to read a PDF file and put it in Python string and trying to fetch information based on keywords. The text here is completely irregular.
Example of text

Blockquote
Ram has taken an insurance of his premises with total sum insured of INR 256,200,000,000. XYZ company provides an insured with limit of liability of INR 100,250,000 and 90 days indemnity period. Insured with deductible of INR 200,000.

Here I want to find 3 things from this text

limit of liability amount
Deductible amount
Sum insured amount

For example
Limit of liability  = 100,250,000
","['natural-language-processing', 'recurrent-neural-networks', 'natural-language-understanding']",
Does anybody know what would happen if I changed input shape of pytorch models?,"
In this https://pytorch.org/vision/stable/models.html tutorial it clearly states:

All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].

Does that mean that for example if I want my model to have input size 128x128 it is or if I calculate mean and std which is unique to my dataset that it is gonna perform worse or won't work at all? I know that with tensorflow if you are loading pretrained models there is a specific argument input_shape which you can set according to your needs just like here:
tf.keras.applications.ResNet101(
include_top=True, weights='imagenet', input_tensor=None,
input_shape=None, pooling=None, classes=1000, **kwargs)

I know that I can pass any shape to those (pytorch) pretrained models and it works. What I wanna understand is can I change input shape of those models so that I don't decrease my models training performance?
","['deep-learning', 'pytorch']","Each machine learning model should be trained by constant input image shape, the bigger shape the more information that the model can extract but it also needs a heavier model.A model's parameters will adapt with the datasets it learns on, which means it will perform well with the input shape that it learned. Therefore, to answer your question ""What I wanna understand is can I change input shape of those models so that I don't decrease my models training performance?"", the answer is no, it will decrease the performance.""I know that I can pass any shape to those (pytorch) pretrained models and it works.""$\Rightarrow$ this happened because Pytorch team replace all Pooling layer with Adaptive Average Pooling 2d so you can pass any shape of the image into the model without any bugs."
SAGAN - is there a mistake in the original paper?,"
in the original paper the following scheme of the self-attention appears:
https://arxiv.org/pdf/1805.08318.pdf

In a later overview:
https://arxiv.org/pdf/1906.01529.pdf
this scheme appears:

referring the original paper.
My understanding more correlates with the second paper scheme, as:

Where there is two dot-product operations and three hidden parametric matrices:
$$W_k, W_v, W_q$$
which corresponds to $W_f, W_g, W_h$ without $W_v$ as it in the original paper explanation, which is as following:

Is this a mistake in the original paper ?
","['deep-neural-networks', 'generative-adversarial-networks', 'image-processing', 'attention', 'attn-gan']",
Is the final model scaling done on the full training set?,"
We have our training set and our test set. When we scale our data we ""fit"" the scaler transform to the training set and then we scale both the training set and test set using this scaler object. Using splitting and cross-validation techniques, one can use the training set as training and validation. Finally, reporting on the test set.
Now, if I want to use a model in a real-life environment, it's common to use the entire dataset (training and test) to train our already optimized model to obtain a final ready for the production model.
My question is regarding scaling. Should we fit the scaler to the entire set and then scale? Or can we simply append the scaled training set and scaled test set (both have been scaled using the training set's scaling parameters)?
I am making use of sklearn.preprocessing.PowerTransformer. Using ""Yeo-Johnson's"" power transform and also standardizing the data.
","['neural-networks', 'deep-learning']","The short answer is yes.When you merge the test set into the train set, you try to squeeze available data till the last drop. The cons and pros of this approach have been considered in other questions in the network 1, 2. But if you decided to go for it, there is no point to not use the whole dataset for the scaling transformation, as the trend ""more data leads (generally) to better models"" is valid to the scaling to the same extent it's valid for the model itself."
Can an existing transformer model be modified to estimate the next most probable number in a sequence of numbers?,"
Models based on the transformer architectures (GPT, BERT, etc.) work awesome for NLP tasks including taking an input generated from words and producing probability estimates of the next word as the output.
Can an existing transformer model, such as GPT-2, be modified to perform the same task on a sequence of numbers and estimate the next most probable number? If so, what modifications do we need to perform (do we still train a tokenizer to tokenize integers/floats into token IDs?)?
","['transformer', 'attention', 'bert', 'gpt', 'forecasting']",
Why does GPT-2 Exclude the Transformer Encoder?,"
After looking into transformers, BERT, and GPT-2, from what I understand, GPT-2 essentially uses only the decoder part of the original transformer architecture and uses masked self-attention that can only look at prior tokens.
Why does GPT-2 not require the encoder part of the original transformer architecture?
GPT-2 architecture with only decoder layers

","['natural-language-processing', 'transformer', 'attention', 'bert', 'gpt']",
How to get more accuracy of the logistic regression model?,"
I am working on a Baby Crying Detection model using logistic regression.
Out of $581$ audios, $222$ are of a baby crying. Each audio is of $5$ seconds.
what I have done is convert each audio into numbers. and those numbers go into a .csv file. so first I took $100$ samples from each audio, then $1000$ samples, and then all $110250$ samples into a .csv file, and at the end of each of them was a number 1 (crying) or 0 (not crying). Then I trained the model using logistic regression from that .csv file.
The Problem I m facing is that with $100$ samples the 64% accuracy on each audio,  while with 1000 samples and 110250 samples(Full dataset) it reaches to 66% accuracy only. How can I improve the accuracy of my model to upto 80% using logistic regression.
I can only use simple logistic regression because I have to deploy the model on Arduino.
","['regression', 'audio-processing', 'binary-classification', 'logistic-regression']",
"For image preprocessing, is it better to use normalization or standartization?","
For a neural network model that classifies images, is it better to use normalization (dividing by 255.0) or using standardization (subtract mean and divide by STD)?
When I started learning convolutional neural networks, I always used normalization because it's simple and effective, but then I started to learn PyTorch and in one of the tutorials https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html they preprocess images like this:
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)

The transform object is created, which has the NORMALIZE parameter, which in itself has the mean and STD values for each channel.
At first, I didn't understand how this works, but then learned how standardization works from Andrew Ng's video, but I didn't find the answers to why is it better to use standardization over normalization or vice-versa. I understand that normalization scales inputs from [0, 1], and standardization first subtracts mean, so that dataset would be centered around 0, and divides everything by STD, so that it would normalize the variance.
Though I know how each of these techniques work (I think I know), I still don't understand why would anybody use one over the other to preprocess images.
Could anybody explain where and why would you use normalization or standardization (if possible could you give an example)? And as a side question: is it better to use the combined version where first you normalize the image and then standardize it?
","['machine-learning', 'convolutional-neural-networks', 'image-processing', 'normalisation', 'standardisation']",
"Do these FOL formula both represent ""You can fool some of the people all of the time""?","

You can fool some of the people all of the time.

This can be represented in FOL as follows
$$\exists x \; \forall t  \; (\text{person}(x) \land \text{time}(t)) \Rightarrow \text{can-fool}(x,t) \tag{1}\label{1}$$
Is $\exists x \; \forall t \; \text{can-fool}(\text{person}(x), \text{time}(t))$ equivalent to (\ref{1}) ?
","['logic', 'symbolic-ai']","(1) can be paraphrased as ""There exists an x, and for any t if x is a person and t is a time, then x can be fooled at time t"" (I would use fool-able instead of can-fool, as it is closer to the intended meaning).(2) would be ""There exists an x, and for any t, you can fool x is a person and t is a time.""They are not equivalent: person(x)  and time(t) are boolean predicates, which return a truth value: they are true if x is a person, and t is a time, respectively. So in (1) they act as a constraint on the values that x and t can take. If x was a saucepan, then person(x) would be false, and thus you wouldn't be able to claim that you can fool a saucepan.So can-fool takes two arguments: one for which person is true, and one for which time is true. But in (2), the arguments are actually the boolean truth values: if x was ""Falstaff"" and t was ""yesterday"", then in (1) the premise would be true, as person(""Falstaff"") and time(""yesterday"") are true, and so you conclude can-fool(""Falstaff"", ""yesterday"").In (2) that becomes can-fool(person(""Falstaff""), time(""yesterday"")), which evaluates to can-fool(true, true), and that won't work."
Predicting the probability of a periodically happening event occurring at a given time,"
I have encountered this problem on how to predict the probability of a periodically happening event occurring at a given time.
For example, we have an event called being_an_undergrad. There are many data points: bob is an undergrad from (1999 - 2003), Bill is an undergrad from (1900 - 1903), Alice is an undergrad from (1900 - 1905), and there are many other data points such as (2010 - 2015), (2011 - 2013) ....
There are many events(data points) of being_an_undergrad. The lasting interval varies, it might be 1 year, 2 years, 3 years, .... or even 10 years. But the majority is around 4 years.
However, I am wondering given all the data points above. If I now know that Jason starts college in 2021, and how can I calculate/predict the probability that he will still be an undergrad in 2022? and 2023? and 2024 .... 2028, etc.
My current dataset consists of 10000 tuples representing events of different relations. The relations are all continuous relations similar to the example above. There are about 10 continuous relations in total in this dataset, such as isMarriedTo, beingUndergrad, livesIn, etc. For each relation, there are about 1000 data points(1000 durations) about this relation, for example,
<Leo, isUndergrad, Harvard, 2010 - 2011>, <Leo, isUndergrad, Stanford, 2013 - 2016>.....

<Jason, livesIn, US, 1990 - 2021>, <Richard, livesIn, UK, 1899- 1995> ...

My problem now is that I want to get a confidence level(probability) when I want to predict one event happening at a specific time point. For example, I want to predict the probability that event <Jason, livesIn, US, 2068> happens, given:
1.the above datasets which includes info about the relation: livesIn
2.the starting time when Mike lives in US, say he started to live in US since 2030.
I have used normal distribution to simulate, but I am wondering if there are any other better AI / ML / Stats approaches. Thanks a lot!
","['machine-learning', 'regression', 'time-series', 'probability', 'probability-distribution']",
Is there any comprehensive book that reviews topics in the area of brain-inspired computing?,"
I am looking to write my master's thesis next year about brain-inspired computing. Hence, I am looking to get a good overview of this domain.
Do you know of any comprehensive book that reviews topics in the area of brain-inspired computing (such as spiking neural networks)?
In spirit and scope, it should be similar to Ian Goodfellow's book deep learning.
","['reference-request', 'neuromorphic-engineering', 'books', 'neuroscience', 'human-inspired']","The most popular theoretical framework in use currently, in the neuromorphic (brain-inspired) computing community is the Neural Engineering Framework (NEF). Neural Engineering by Chris Eliasmith and Charles Anderson explains the framework comprehensively.As a follow up to that, How to Build a Brain by Chris Eliasmith describes the more recent and more high-level description of how to get spiking neural networks to actually perform multiple functions: the Semantic Pointer Architecture (SPA).If you're looking for hardware descriptions too, the research publications on the chips Neurogrid, Brainscales, Braindrop, SpiNNaker, Loihi, TrueNorth etc. provide some good high-level descriptions of how to actually build the aforementioned chips."
How is $v_*(s) = \max_{\pi} v_\pi(s)$ also applicable in the case of stochastic policies?,"
I am reading Sutton & Bartos's Book ""Introduction to reinforcement learning"". In this book, the defined the optimal value function as:
$$v_*(s) = \max_{\pi} v_\pi(s),$$ for all $s \in \mathcal{S}$.
Do we take the max over all deterministic policies, or do we also look at stochastic policies (is there an example where a stochastic policy always performs better than a deterministic one?)
My intuition is that the value function of a stochastic policy is more or less a linear combination of the deterministic policies it tries to model, however, there are some self-references, so it is not mathematically true).
If we do look over all stochastic policies, shouldn't we take the supremum? Or do we know, that the supremum is achieved, and therefore it is truly a maximum?
","['markov-decision-process', 'value-functions', 'stochastic-policy', 'optimal-policy', 'optimality']","The value function is defined as $v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$ where $G_t$ are the (discounted) returns from time step $t$. The expectation is taken with respect to the policy $\pi$ and the transition dynamics of the MDP.Now, as you pointed out the optimal value function is defined as $v_*(s) = \max_\pi v_\pi(s)\; ; \;\forall s \in \mathcal{S}$. All we are doing here is choosing a policy $\pi$ that maximises the value function; this can be a deterministic or a stochastic policy, though intuitively it is likely to be deterministic unless for some states that are two (or more) actions with the same expected value, in which case you can take any of said actions with equal probability, thus making the policy stochastic.For a finite MDP (which is what I assumed above too), we know that an optimal value function exists (this is mentioned in the book) so taking the maximum is fine here."
Reinforcement Learning for an environment that is non-markovian [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I will start working on a project where we want to optimize the production of a chemical unit through reinforcement learning approach. From the SME's, we already obtained a simulator code that can take some input and render us the output. A part of our output is our objective function that we want to maximize by tuning the input variables. From a reinforcement learning angle, the inputs will be the agent actions, while the state and reward can be obtained from the output. We are currently in the process of building a RL environment, the major part of which is the simulator code described above.
We were talking to a RL expert and she mentioned that one of the thing that we have here conceptually wrong is that our environment will not have the Markov property in the sense that it is really a 'one-step process' with the process not continuing from the previous state and there is no sort of continuity in state transitions. She is correct there. This made me think, how can we get around this then. Can we perhaps append some part of the current state to the next state etc. More importantly, I have seen RL applied to optimal control in other examples as well which are non-markovian ex. scheduling, tsp problems, process optimization etc. What is the explanation in such cases? Does one simply assumes process to be markovian with unknown transition function?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'markov-decision-process', 'markov-property']","RL is currently being applied to environments which are definitely not markovian, maybe they are weakly markovian with decreasing dependency.You need to provide details of your problem, if it is 1 step then any optimization system can be used."
Why does my neural network to solve the XOR problem always output 0.5?,"
I'm trying to create a neural network to simulate an XOR gate.
Here's my dataset:
╔════════╦════════╗
║ x1, x2 ║ y1, y2 ║
╠════════╬════════╣
║  0, 0  ║  0, 1  ║
║  0, 1  ║  1, 0  ║
║  1, 0  ║  1, 0  ║
║  1, 1  ║  0, 1  ║
╚════════╩════════╝

And my neural network:

I use logistic loss to get the error between target $y_{k}$ and output $\hat{y}_{k}$:
$$ E(y_{k}, \hat{y}_{k}) = - y_{k} \cdot log(\hat{y}_{k}) + (1 - y_{k}) \cdot log(1 - \hat{y}_{k}) $$
And then use the chain rule to update the weights. For example weight $w_{3}$'s contribution to the error is:
$$ \sum_{k=1}^{2} \frac{\partial E(y_{k}, \hat{y}_{k})}{\partial w_{3}} = \sum_{k=1}^{2} \left(\frac{\partial E(y_{k}, \hat{y}_{k})}{\partial \hat{y}_{k}} \cdot \frac{\partial s_{k}}{\partial c_{1}}\right) \cdot \frac{\partial c_{1}}{\partial w_{3}} $$
Which in developed form is:
$$ \sum_{k=1}^{2} \frac{\partial E(y_{k}, \hat{y}_{k})}{\partial w_{3}} = \left( \left( - \frac{y_{1}}{\hat{y}_{1}} + \frac{1 - y_{1}}{1 - \hat{y}_{1}} \right) \cdot \hat{y}_{1} \cdot (1 - \hat{y}_{1}) + \left( - \frac{y_{2}}{\hat{y}_{2}} + \frac{1 - y_{2}}{1 - \hat{y}_{2}} \right) \cdot (- \hat{y}_{2}) \cdot\frac{c_{1}}{c_{1} + c_{2}} \right) \cdot s_{0} $$
My issue is that, after a couple epochs of training on the entire dataset, the network always outputs:
$$ \hat{y}_{1} = \hat{y}_{2} = 0.5 $$
What am I doing wrong?
","['neural-networks', 'implementation', 'softmax', 'xor-problem', 'categorical-crossentropy']","One neuron on its own can only solve linearly separable problems. You need a combination of neurons to solve non-linearly separable problems.For the XOR case, you need at least 2 neurons at the first layer, and 1 neuron at the output layer to properly classify it.Keep in mind that, sometimes, the 3-neurons network might get stuck in a local minimum as well. You will need some luck in the random initialization of weights. Using the right seed during the random initialization of weights can help converge, and some other seed will only result in a stuck network."
Why (not) using pre-processing before using Transformer models?,"
Regarding the use of pre-processing techniques before using Transformers models, I read this post that apparently says that these measures are not so necessary nor interfere so much in the final result.
The arguments raised seemed to me quite convincing, but someone would know how to explain better, perhaps with a bibliographic reference, why is it not so necessary to use these techniques?
","['natural-language-processing', 'pytorch', 'transformer', 'data-preprocessing']",
Why does PCA of the vertices of a hexagon result in principal components of equal length?,"
I do PCA on the data points placed in the corners of a hexagon, and get the following principal components:

The PCA variance is $0.6$ and is the same for each component. Why is that? Shouldn't it be greater in the horizontal direction than in the vertical direction? The data is between $-1$ and $1$ in the $x$-direction but only between $-\sqrt{3}/2$ and $\sqrt{3}/2$ in the $y$-direction. Why PCA results in the equal length components?
The length of each vector in the picture is the twice the square root of the variance.
UPDATE: added more points, the variances changed to $0.477$ but still they are equal.

UPDATE 2: Added even more points, the variances changed to $0.44$ but still they are equal.

","['principal-component-analysis', 'dimensionality-reduction']","Assuming that the $6$ vertices of the hexagon are on the unit circle,Since ${\bf A} {\bf A}^\top - 3 \, {\bf I}_2 = {\bf O}_2$, any two orthogonal directions could be the principal components."
Is it possible that the model is overfitting when the training and validation accuracy increase?,"
I am aware of similar questions that have been asked, and I have gone through many. I want to bring my case to SE to understand better what my results are.
I am working with a large dataset (around 75million records), but, for the purpose of testing techniques, I am actually using 2M records. I am working towards malicious traffic identification using NetFlow data. After employing some undersampling to have a balanced dataset according to my target variable (benign or attack) I have 1,240,950 of records in the training set and 310,238 in the validation set. Therefore I believe there is a good amount of data to train a Deep neural network properly.
After using Yeo-Yohnsons transform and standardizing the data, I train the network with a very basic model:
def basem():    
    
    model = Sequential()
    
    model.add(Dense(25, input_dim=38))
    model.add(Activation(""relu""))
    
    model.add(Dense(50))
    model.add(Activation(""relu""))
    
    model.add(Dense(50))
    model.add(Activation(""relu""))
    
    model.add(Dense(25))
    model.add(Activation(""relu""))
    
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=""adam"", metrics=['accuracy'])
    return model


model_base = basem()
model_base._name = 'base'

history_base = model_base.fit(X_train, y_train, batch_size=2048, 
                    epochs=15, validation_data=(X_val,y_val), shuffle=True)

This gives me the following plot 
It maybe because I am a newbie, but this plot looks too perfect. It is weird to see validation and training accuracy growing together, although I believe this is what we want right? But now I have the feeling it is overfitting. Therefore I use the model and a 5-fold cross validation to understand how well it generalizes. Results, mean accuracy and mean std(%), are:
test acc: 0.9816503485233088
test_prec: 0.9840033637114158
test_f1: 0.9816046990113001
test_recall: 0.9792384866432975
test_roc_auc: 0.9980004347946355

Dev acc: 0.052931962886091546
Dev prec: 0.2854656099314699
Dev f1: 0.057228805478181974
Dev recall: 0.3597811552056071
Dev roc auc: 0.0036456892671197097

If I understand correctly, accuracy is high which is generally good and the standard deviation is very low for each metric, the highest being 0.359% for recall. Does this mean my model generalizes well?
Edit
Adding dropout (0.3) to each layer yields the following:

Now, my validation accuracy is higher than my training. I can't make sense of any of this.
","['deep-learning', 'overfitting', 'accuracy', 'generalization', 'dropout']",
Can RNNs be used to classify these time series into two classes?,"
My task is to classify into two classes the time series like these shown in the figure.

The figure shows one class on the left sub-figure and second one on the right. The series are shown in pairs for more clarity, but each series on the left (and right) belongs to one of the respective classes. The scale of the right panel is reduced to show all series, but these series are of the same amplitude as the left ones.
Is it possible to apply RNN (or other methods) to classify the series of this kind into two classes?
I have never used neural networks, but I am just looking for an adequate method for this problem.
","['recurrent-neural-networks', 'time-series', 'binary-classification', 'algorithm-request', 'model-request']",
Huge dimensionality of input and output — any recommendations?,"
At work there is an idea of solving a problem with machine learning. I was assigned the task to have a look at this, since I'm quite good at both mathematics and programming. But I'm new to machine learning.
In the problem a box would be discretized into smaller boxes (e.g. $100 \times 100 \times 100$ or even more), which I will call 'cells'. Input data would then be a boolean for each cell, and output data would be a float for each cell. Thus both input and output have dimensions of order $10^6$ to $10^9$.
Do you have any recommendations about how to do this? I guess that it should be done with a ConvNet since the output depends on relations between close cells.
I have concerns about the huge dimensions, especially as our training data is not at all that large, but at most contains a few thousands of samples.

Motivation
It can be a bit sensitive to reveal information from a company, but since this is a common problem in computational fluid dynamics (CFD) and we already have a good solution, it might not be that sensitive.
The big boxes are virtual wind tunnels, the small boxes ('cells' or voxels) are a discretization of the tunnel. The input tells where a model is located and the output would give information about where the cells of a volume mesh need to be smaller.
","['machine-learning', 'convolutional-neural-networks', 'reference-request', 'dimensionality']",
"In Probabilistic Graphical Model (written by Daphne Koller), what's the meaning of ""parameter"" in representation of the distribution?","
I just started to read the PGM book written by Daphne Koller.
In the chapter of Bayesian Network Representation(Chapter 3), there are some descriptions about the standard parameterization of the joint distribution corresponding to n-trial coin tosses.

The book also says,

Here I'm very confused about the meaning of $ 2^n parameters $. In terms of random variable or probability distribution, parameter means characteristic of the distribution. But parameter in this paragraph sounds like $O(2^n)$ space complexity. Because it also describes that we can reduce the space of all joint distribution to $n$-dimension by using expression $ \prod_{i} \theta_{x_{i}} $.

So, what's the meaning of parameter in this context? Does it mean space complexity for computation of the joint distribution?
","['terminology', 'bayesian-networks', 'books', 'probabilistic-graphical-models']",
Should I use U-net to label keys in a keyboard image?,"
This is a 600*800 image.

Which algorithm/model should I use to get an image like the one below, in which each key is detected and labeled by a rectangle?
I guess this is some kind of a segmentation problem where U-net is the most popular algorithm, though I don't know how to apply it to this particular problem.

","['computer-vision', 'object-detection', 'algorithm-request', 'model-request', 'template-matching']","If you just need to draw a rectangle around each key, this is an object detection or template matching problem, so you can use any of the available models for object detection (e.g. YOLO) or any technique for multi-template template matching (e.g. you can use sequential RANSAC or t-linkage). In the first case, you will need a labeled dataset, while, in the second case, you will need the original image and the templates (in your case, a template would be an image of a key).So, no, this is not a segmentation problem (which would be the task of classifying each pixel in the objects of interest, and not just locating the objects)."
Is there a way of path reconstruction using only the history of belief states?,"
Given a history of belief states, is there a common method that backtracks the most likely path of ending up in the current belief state?
I have a Markov model which calculates belief states after every step. The belief state is a representation of the most likely states one could be in. A belief state may look like this:
$$b=[1,0,0,0,0],$$ where I am in the state $s_0$ with 100% certainty.
I can store the belief state history like $b_0, b_1, b_2,\dots, b_n$.
Is there a common way to represent and estimate the most likely states one has been in?
A naive approach could be to just look for the state with the highest value per belief state and take that as the node along the reverse path. But I am not confident enough, if that is a common and a good practice, as it is not considering the fuzziness, which comes with a belief state. But then again, if I would take all states that are bigger than 0, I might not know which state leads to which state and if that transition is even possible.
","['markov-decision-process', 'pomdp']",
Are there relatively new research papers that describe how to make back-propagation more efficient?,"
I read Yann LeCun's paper Efficient BackProp, which was published in 2000. I looked for similar but more recent papers on Arxiv, but I have not yet found any.
Are there relatively new research papers that describe how to make back-propagation more efficient?
So, I am looking for papers similar to Efficient Backprop by LeCun but newer. The papers could describe why ReLU now ""dominates"" tanh or even sigmoid (but tanh was Yann's favorite, as explained in the paper). ReLU is just one thing I am interested in, but the paper could also analyze e.g. the inputs from a statistical standpoint.
","['reference-request', 'backpropagation']",
How should the 1-point crossover and mutation be defined for the problem of finding the largest circle that does not enclose any point?,"

For a random scattering of points, in a bounded area, the goal is to find the largest circle that can be drawn inside those same bounds that does not enclose any points. Solving this problem with a genetic algorithm requires deciding how to encode as a genome, information sufficient to represent any solution. In this case, the only information we need is the center point of the circle, so our genome of point $p_i$ will look like $(x_i, y_i)$, representing the Cartesian coordinates.

In this case, what does each of the genetic operators mean for this simplistic genome? Geometrically speaking, what would a 1-point crossover look like in this case? What about mutation?
This is my answer, but I am not sure.
Consider two individuals with 2 variables each (2 dimensions), $p_1=(x_1, y_1)$ and $p_2=(x_2, y_2)$. For each variable, the parent who contributes its variable to the offspring is chosen randomly with equal probability. Geometrically speaking, a 1-point crossover would represent a quadrilateral in the Cartesian space, where one of the diagonals is formed by the parents and the other one by the offsprings $c_1=(x_1, y_2)$ and $c_2=(x_2, y_1)$.
On the other hand, a mutation operator is an r-geometric mutation under the metric $d$ if all its offsprings are in the $d$-ball of radius $r$ centered in the parent.
The radius (fitness function) would be the distance between the center (genome) and the closest point (star) from the random points in the bounded area.
","['genetic-algorithms', 'crossover-operators', 'homework', 'mutation-operators', 'chromosomes']",
How does the K-dimensional WL test work?,"
I am reading a paper on the K-WL GCN. I did not complete the paper yet, but I just skimmed over it. There I am trying to understand the K-WL test (page 3 Weisfeiler-Leman Algorithm). I think my understanding is quite ambiguous, so I looking for an example problem that is solved using K-WL test. But I can't find any of them on the web.
Does anyone have any solved example problem on K-WL or can anyone explain to me how the K-WL test works?
Note: If anyone also explains how K-WL GCN uses the K-WL test, I will be thankful.
","['papers', 'geometric-deep-learning', 'graph-neural-networks']",
Are there heuristics that play Klondike Solitaire well?,"
Are there heuristics that play Klondike Solitaire well?
I know there are some good exhaustive search solvers for Klondike Solitaire. The best one that I know of is Solvitaire (2019) which uses DFS, (see paper, code).
I searched the web for heuristics that plays as a human would play, with no backwards moves, however, I found only one.  In that paper, they report on a win-rate of 13.05%.  In comparison, human experts reach 36.6% win-rate in thoughtful solitaire which is Klondike Solitaire where the location of all the cards is known. Source: Solitaire: Man Versus Machine (2005).
Are there any other published heuristics for Klondike Solitaire?
When determining if a heuristic is interesting, I would consider its win-rate and the similarity to how humans are playing.
","['reference-request', 'heuristics', 'games-of-chance', 'card-games']",
What are possible ways to combat overfitting or improve the test accuracy in my case?,"
I have asked a question here, and one of the comments suggested that this is a case of severe overfitting. I made a neural network, which uses residual boosting (which is done via a KNN), and I am still just able to get < 50% accuracy on the test set.
What should I do?
I tried everything from reducing the number of epochs to replacing some layers with dropout.
Here is the source code.
","['neural-networks', 'tensorflow', 'overfitting', 'accuracy', 'test-datasets']","There are a few issues you need to address first.I may have made some points for things you have already done as there was a lot of code for me to look over in your source. But the biggest issue I noticed was that your data wasn't normalised at all and that usually creates many issues, so definitely try that first if you haven't already."
How to construct Transformers to predict multidimensional time series?,"
There is plenty of information describing Transformers in a lot of detail how to use them for NLP tasks. Transformers can be applied for time series forecasting.  See for example ""Adversarial Sparse Transformer for Time Series Forecasting"" by Wu et al.
For understanding it is best to replicate everything according to already existing examples. There is a very nice example for LSTM with flights dataset https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/.
I guess I would like to know how to implement transformers for at first univariate (flight dataset) and later for multivariate time series data. What should be removed from the Transformer architecture to form a model that would predict time series?
","['deep-learning', 'transformer']","There is an implementation of the paper (""Adversarial Sparse Transformer for Time Series Forecasting""), in Python using Pytorch, here. Although it has the training and evaluation functionality implemented, it appears to be lacking a function for running a prediction.  Maybe you can fork it and extend it.UPDATEThere is also a paper, ""Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"", by Zhou et al., which does forecasts on univariate and multivariate data.  Their code is here."
Could I just choose the other (non-predicted) class when the accuracy is low?,"
I have a binary classification problem.
My neural network is getting between 10% and 45% accuracy on the validation set and 80% on the training set. Now, if I have a 10% accuracy and I just take the opposite of the predicted class, I will get 90% accuracy.
I am going to add a KNN module that shuts down that process if the inputted data is, or is very similar to the data present in the data set.
Would this be a valid approach for my project (which is going to go on my resume)?
","['neural-networks', 'deep-learning', 'overfitting', 'accuracy', 'binary-classification']","The short answer is no, you shouldn't do that.There is a ""distribution shift"" thing when you have different x-y relation on the validation set then on the train set. The distribution shift would deteriorate your model performance and you should try to avoid that. The reason it's bad - ok, you find the way to fix the model for validation data, but what about novel test data? Will it be like a train? Will it be like validation? You don't know and your model is worthless in fact.What you can do"
"I have 5000 html files (structured text), how can I generate a new one that ""resembles"" those?","
I don't know anything about ML or NLP, but I was asked by someone to create brand new statutes (written laws) that resemble the ones currently in effect in my country. I have already gathered the laws, and have 5000 html files now, one per law.
The average size of each html file is 49 kB. The entire corpus is 300 MB.
I have two alternative goals (doing both would be perfect of course):

Generate a new, complete HTML file, that would imitate the 5000 existing ones (it would typically have 1 big heading at the top, sub-headings, articles with their own title and number, etc.)

Generate sentences that sound as if they could be found in a typical law (the laws are written in French)


Is any of those goals feasible, with such a small corpus (~300 MB total)?
Should I try and fine-tune an existing model (but in that case, wouldn't the small size of my corpus be a problem? Wouldn't it be ""drowned out"" in the rest of the training data?), or should I create one from scratch?
I've tried following guides on huggingface, but between the obsolete files, the undocumented flags and my general lack of knowledge of the subject, I'm completely lost.
Thanks in advance.
BTW, if you want to take a peek at the data, there it is: https://github.com/Biganon/rs/
","['natural-language-processing', 'natural-language-generation']",
Why is it a problem if the outputs of an activation function are not zero-centered?,"
In this lecture, the professor says that one problem with the sigmoid function is that its outputs aren't zero-centered. Are the explanation provided by the professor regarding why this is bad is that the gradient of our loss w.r.t. the weights $\frac{\partial L}{\partial w}$ which is equal to $\frac{\partial L}{\partial \sigma}\frac{\partial \sigma}{\partial w}$ will always be either negative or positive and we'll have a problem updating our weights as she shows in this slide, we won't be able to move in the direction of the vector $(1,-1)$. I don't understand why since she only talks about one component of our gradient and not the whole vector. if the components of the gradient of our loss will have different signs which will allow us to adjust to different directions I'm I wrong? But the thing that I don't understand is how this property generalizes to non zero-centered functions and non-zero centered data?
","['backpropagation', 'activation-functions', 'sigmoid', 'gradient']","Yes, if the activation function of the network is not zero centered, $y = f(x^{T}w)$ is always positive or always negative. Thus, the output of a layer is always being moved to either the positive values or the negative values. As a result, the weight vector needs more updates to be trained properly, and the number of epochs needed for the network to get trained also increases. This is why the zero centered property is important, though it is NOT necessary.Zero-centered activation functions ensure that the mean activation value is around zero. This property is important in deep learning because it has been empirically shown that models operating on normalized data––whether it be inputs or latent activations––enjoy faster convergence.Unfortunately, zero-centered activation functions like tanh saturate at their asymptotes –– the gradients within this region get vanishingly smaller over time, leading to a weak training signal.ReLU avoids this problem but it is not zero-centered. Therefore all-positive or all-negative activation functions whether sigmoid or ReLU can be difficult for gradient-based optimization. So, To solve this problem deep learning practitioners have invented a myriad of Normalization layers (batch norm, layer norm, weight norm, etc.). we can normalize the data in advance to be zero-centered as in batch/layer normalization.Reference:A Survey on Activation Functions and their relation with Xavier and He Normal Initialization"
Bayesian Perceptron: How is it compatible to Bayes Theorem?,"
I found a very interesting paper on the internet that tries to apply Bayesian inference with a gradient-free online-learning approach: [Bayesian Perceptron: Bayesian Perceptron: Towards fully Bayesian Neural Networks.
I would love to understand this work, but, unfortunately, I am reaching my limits with my Bayesian knowledge. Let us assume that we have the weights $\mathcal{w}$ of our model and observed the data $\mathcal{D}$. Using the Bayes rule, we obtain the posterior according to $$p(\mathcal{w}|D)=\frac{p(D|\mathcal{w})p(\mathcal{w})}{p(D)}$$.
In words: we update our prior belief over our weights by multiplying the prior with the likelihood and divide everything by the evidence. In order to calculate the true posterior, we would need to calculate the evidence by marginalizing over (intergrating out) our unknown parameters. This gives the integral $$p(D) = \int p(D|\mathbf{w})p(\mathbf{w})dw$$.
So far so good. Now I refer to the paper mentioned above. Here, the approach is presented exemplarily on a neuron whose weighted sum is called $a$, which is then given to the activation function $f(.)$. Moreover it is assumed that $\mathbf{w}\sim N (\mu_w, \mathbf{C}_w)$. Because of the linearity, it can be exploited that also $\mathbf{a}\sim N (\mu_a, \mathbf{C}_a)$.
What I am confused about now is formula (14), which seems to show the compute the true posterior:
$$p(w) = \int p(a, w|D_i)da = \int p(w|a, D_i)p(a|D_i)da$$
How is this formula of the posterior compatible with the Bayes Theorem? Where is the evidence, likelihood and prior?
","['machine-learning', 'papers', 'bayesian-neural-networks', 'bayes-theorem']","Thanks for asking the question. I'm the author of the paper.The key point is that the weights $w$ cannot be updated directly with the new data as $w$ is not directly related with the output $y$ (see equation (1)). First, it is necessary to update $a$ first, which actually is directly related with $y$ via the activation function, i.e., $y = f(a)$ with $f(.)$ being the activation function (see equation (2)). Hence, you will find the Bayes rule in equation (16) with all the quantities you are missing (likelihood, prior, etc.). This equation is used to calculate the posterior $p(a|D_i)$. This posterior is then used to update the weights by plugging $p(a|D_i)$ into equation (14). The solution of equation (14), i.e., the posterior mean and covariance matrix of $w$, is provided by means of equation (22). Algorithm 2 provides a summary of all necessary calculations.I hope this answers your question. BTW: we have extended this algorithm to a full neural network. The scientific paper describing the whole procedure is currently under review."
How widely accepted is the definition of intelligence by Marcus Hutter & Shane Legg?,"
I came across several papers by M. Hutter & S. Legg.
Especially this one:
Universal Intelligence: A Definition of Machine Intelligence, Shane Legg, Marcus Hutter
Given that it was published back in 2007, how much recognition or agreement has it received?
Has any other work better formalizing the idea of intelligence been done since?
What is considered current standard on the topic in the field?
","['agi', 'definitions', 'intelligent-agent', 'intelligence']",
How would the performance of federated learning compare to the performance of centralized machine learning when the data is i.i.d.?,"
How would the performance of federated learning (FL) compare to the performance of centralized machine learning (ML), when the data is independent and identically distributed (i.i.d.)?
Moreover, what is the difference in the performance of FL when the data is i.i.d. as compared to non-i.i.d?
","['machine-learning', 'reference-request', 'performance', 'federated-learning', 'iid']","There are some works that do this comparison. Briefly, it's been observed that the performance of models trained via FL drops as data distributions between participating agents differ. When data is IID-like though, performance is comparable to centralized training. Some works that I'm aware of are as follows:There are probably many more around. It's an active area of research."
"Why do we need to have two heads in D3QN to obtain value and advantage separately, if V is the average of Q values?","
I have two questions on the Dueling DQN paper. First, I have an issue on understanding the identifiability that Dueling DQN paper mentions:

Here is my question: If we have given Q-values $Q(s, a; \theta)$ for all actions, I assume we can get value for state $s$ by:
$$V(s) = \frac {1} {|Q|} \sum_{a \in \mathcal{Q}} Q(s, a; \theta)$$
and the advantage by:
$$A(s,a) = Q(s, a; \theta) - V(s), ~~~ \forall ~a ~in ~\mathcal{A}(s)$$
in which $\mathcal{A}(s)$ is the action space for state $s$. If this is correct, why do we need to have two heads in the network to obtain value and advantage separately?
and then obtain Q-value using
$$Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + \left( A(s, a; \theta, \alpha) - \max_{a' \in | \mathcal{A} |} A(s, a'; \theta, \alpha) \right). \tag{8}$$
or
$$Q(s, a; \theta, \alpha, \beta) = V (s; \theta, \beta) + \left( A(s, a; \theta, \alpha) − \frac {1} {|A|} \sum_{a' \in \mathcal{A}} A(s, a'; \theta, \alpha) \right). \tag{9}$$
Am I missing something?
My second question is why Dueling DQN does not use the target network as it is used in the DQN paper?
","['reinforcement-learning', 'dqn', 'value-based-methods', 'd3qn']","Regarding your first question, $$V^{\pi}(s) = \sum_{a \in A}\pi(a|s)Q^{\pi}(s,a)$$ so recovering the value function from Q really depends on what policy $\pi$ you are using. Hence, you can't really recover the value function $V(s)$ from the $Q(s,a)$ values without knowing your policy distribution for state $s$.However, you can recover $Q^{\pi}(s,a)$ values if we know $V^{\pi}(s)$ and $A^{\pi}(s,a)$. This is because
$$A^{\pi}(s,a) = V^{\pi}(s,a) - Q^{\pi}(s,a)$$by definition of advantage. And this is why you need 2 heads to recover the $Q$ values from the Value and Advantage functions. In the original paper, the author's do not use this direct equation to recover $Q^{\pi}(s,a)$ values due to ""identifability"" issue and the fact that both $V^{\pi}(s)$ and $Q^{\pi}(s,a)$ are only estimates.Regarding your second question, I believe the author's appllied the Duelling architecture on Double Deep Q Networks, which is an improvement over the single DQN used by Minh et al in learning atari. I do think that you can still use a a target network as in the single DQN case if you wanted to."
Bayesian Perceptron: Why to marginalize over neuron's output instead of it's weights?,"
I found a very interesting paper on the internet that tries to apply Bayesian inference with a gradient-free online-learning approach: Bayesian Perceptron: Towards fully Bayesian Neural Networks.
I would love to understand this work, but unfortunately I am reaching my limits with my Bayesian knowledge. Let us assume that we have the weights $\mathcal{w}$ of our model and observed the data $\mathcal{D}$. Using the Bayes rule, we obtain the posterior according to $$p(\mathcal{w}|D)=\frac{p(D|\mathcal{w})p(\mathcal{w})}{p(D)}$$.
In words: we update our prior belief over our weights by multiplying the prior with the likelihood and divide everything by the evidence. In order to calculate the true posterior, we would need to calculate the evidence by marginalizing over (intergrating out) our unknown parameters. This gives the integral $$p(D) = \int p(D|\mathbf{w})p(\mathbf{w})dw$$.
So far so good. Now I refer to the paper mentioned above. Here, the approach is presented exemplarily on a neuron whose weighted sum is called $a$, which is then given to the activation function $f(.)$. Moreover it is assumed that $\mathbf{w}\sim N (\mu_w, \mathbf{C}_w)$. Because of the linearity it can be exploited that also $\mathbf{a}\sim N (\mu_a, \mathbf{C}_a)$.
What I am confused about now is formula (14), which seems to show the compute the true posterior:
$$p(w) = \int p(a, w|D_i)da = \int p(w|a, D_i)p(a|D_i)da$$
Why is $a$ integrated out here and not $w$? We want a distribution over $w$, don't we? But without marginalization over $w$ there is still uncertainty inside $w$
Glad about any help and food for thought;)
","['machine-learning', 'papers', 'bayesian-neural-networks', 'bayes-theorem', 'probability-theory']",
Is vectorizing backpropagation feasible?,"
Does it make sense to have the backpropagation of a neural network layer happen all at once if the learning rate is lowered? This would mean the new weights of that layer would be independent of each other, but, it would be extremely fast. Is this method feasible in any way for a neural network, or would it create a cost threshold which the network can't reach because of it's independent inaccuracy?
","['neural-networks', 'deep-learning', 'backpropagation']",
Is it likely that a sentient AI experience synesthesia?,"
The reason I ask this question is because we humans tend to compartmentalize our sensory inputs, except in some individuals that experience synesthesia.  If an Artificial Intelligence Entity (AIE) can correlate all sensory input (as a bunch of tensors), wouldn't that be the ultimate form of synesthesia?
An AIE geometry / color synesthesia might lead to an explanation of how Joan Miro colorized his doodles.
","['computer-vision', 'audio-processing', 'sentience']",
Do text compression tests qualify winRar or 7zip as intelligent?,"
I read this paper Text Compression as a Test for Artificial Intelligence, Mahoney, 1999.
So far I understood the following:
Text Compression tests can be used as an alternative to Turing Tests for intelligence.
The Bits per character score obtained from compression of a standard benchmark corpus, can be used as a quantitative measure for intelligence
My questions:

Is my understanding of the topic correct?
Does this mean that applications like 7zip/WinRar are intelligent?
How are the ways a human compresses information (as in form of summary) and ways a computer compresses (using Huffman coding or something) are compatible? How can we compare that?

","['intelligent-agent', 'intelligence-testing', 'data-compression']",
What is the intuition behind variational inference for Bayesian neural networks?,"
I'm trying to understand the concept of Variational Inference for BNNs. My source is this work. The aim is to minimize the divergence between the approx. distribution and the true posterior
$$\text{KL}(q_{\theta}(w)||p(w|D) = \int q_{\theta}(w) \ log \frac{q_{\theta}(w)}{p(w \mid D)} \ dw$$
This can be expanded out as $$- F[q_{\theta}] + \log \ p(D)$$ where $$F[q_{\theta}] = -\text{KL}(q_{\theta}(w) || p(w)) + E[\log p(D \mid w)]$$
Because $log \ p(D)$ does not contain any variational parameters, the derivative will be zero. I really would like to summarize the concept of VI in words.
How can one explain the last formula in words intuitive and with it on the fact that one approximates a function without really knowing it / able to compute it?
My attempt would be: Minimizing the KL between the approximate distribution and the true posterior boils down in minimizing the KL between the approximate distribution and the prior (?) and maximizing the log-likelihood that the parameters of the approximate distribution resulted in the data. Is this somehow correct?
","['neural-networks', 'machine-learning', 'bayesian-deep-learning', 'bayesian-neural-networks', 'variational-inference']","Your description of what is going on is more or less correct, although I am not completely sure that you have really understood it, given your last question.So, let me enumerate the steps.The computation of the posterior is often intractable (given that the evidence, i.e. the denominator of the right-hand side of the Bayes' rule, might be numerically expensive to approximate/compute or there's no closed-form solution)To address this intractability, you cast the Bayesian inference problem (i.e. the application of Bayes' rule) as an optimization problemYou assume that you can approximate the posterior with another simpler distribution (e.g. a Gaussian), known as the variational distributionYou formulate this optimization problem as the minimization of some notion of distance (e.g. the KL divergence) between the posterior and the VDHowever, the KL divergence between the posterior and the VD turns out to be intractable too, given that, if you expand it, you will find out that there's still an evidence termTherefore, you use a tractable surrogate (i.e. equivalent, up to some constant) objective function, which is known as the evidence lower bound (ELBO) (which is sometimes known as the variational free energy), which is the sum of 2 termsTo address your last doubt/question, the ELBO does not contain the posterior (i.e. what you really want to find), but only the variational distribution (you choose this!), the prior (which you also define/choose), and the likelihood (which, in practice, corresponds to the typical usage of the cross-entropy; so the only thing that you need more, with respect to the traditional neural networks, is the computation of the KL divergence): in other words, you originally formulate the problem as the minimization of the KL divergence between the posterior and the VD, but this is just a formulation."
Does a trajectory in reinforcement learning contain the last action?,"
From what I learn from CS285 and OpenAI's spinning up, a trajectory in RL is a sequence of state-action pairs:
$$\tau = \{s_0, a_0, ..., s_t, a_t\}$$
And the resulting trajectory probability is:
$$ P(\tau \mid \pi)=\rho_{0}\left(s_{0}\right) \prod_{t=0}^{T-1} P\left(s_{t+1} \mid s_{t}, a_{t}\right) \pi\left(a_{t} \mid s_{t}\right) $$

From CS285: http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf

From spinning up: https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#trajectories


However, from my derivation, the above trajectory probability actually corresponds to the following sequence where the last action $a_t$ is absent:
$$ \tau = \{s_0, a_0, ..., s_t\} $$
Can someone please help me clarify this confusion?
","['reinforcement-learning', 'terminology', 'notation']",
How to have zero value or a value between 200 and 400 in the output of a deep learning model?,"
I want to implement a DDPG method and obviously, the action space will be continuous. I have three outputs. The first output should be zero or a value between 200 and 400, and the other outputs have similar conditions. I don't know how can I implement this condition in the layers and activation functions. Should I use a binary activation before the scaled sigmoid function? How can I scale the activation function for this example?

(a1 = 0) or (200 < a1 < 400)
(a2 = 0) or (100 < a2 < 500)
(a3 = 0) or (200 < a3 < 1000)

","['reinforcement-learning', 'deep-learning', 'policy-gradients', 'ddpg', 'continuous-action-spaces']",
Why is the sigmoid function interpreted as a saturating firing rate of a neuron?,"
I've seen several people say that sigmoids are like a saturating firing rate of a neuron but I don't see how or why they interpret it as such. I especially don't see the relationship between a ""rate"" (so a number of something over time, I guess here it's the number that a neuron activates in a unit of time) and the sigmoid graph. For me it resembles more to the voltage output of an operational amplifier in some cases.
","['artificial-neuron', 'neurons', 'sigmoid']",
How to train a sequence labeling model with annotations from three annotators?,"
I have a dataset of movie reviews annotated by 3 persons. The following example contains one sentence with corresponding annotations from 3 different persons.
sentence = ['I', 'like', 'action', 'movies','!']
annotator_1 = ['O','O', 'B_A', 'I_A', 'O'] 
annotator_2 = ['O','O', 'B_A', 'I_A', 'O'] 
annotator_3 = ['O','O', 'B_A', 'O', 'O']

The labels follow the BIO format. That is, B_A means the beginning of aspect-term (action) and I_A indicates inside of aspect-term (movie).
Unfortunately, the annotators do not agree always together. While the first two persons assigned the right labels for aspect-term (action movie), the last one mislabeled the token (movies).
I am using Bi-LSTM-CRF sequence tagger to train the model. However, I am not sure if am using the training data correctly.
Is it correct to feed the model the same sentence with annotations from 3 persons? Then test it in the same way, i.e., the same sentence with different annotations?
Another question.
I merged the annotations in one final list of labels as follows:
final_annotation = ['O','O', 'B_A', 'I_A', 'O']

In this case, the final label is chosen based on the majority of labels among three annotators.
Is it right to feed the model the same sentence with corresponding annotations from all users during the testing phase?
","['neural-networks', 'natural-language-processing', 'python', 'sentiment-analysis', 'named-entity-recognition']","Both ways are valid. It depends on what you want from the model and expect from the data. Generally though I would use 1 assumption and stick with it (unless there was a specific reason not to), so I would use all lines for test if training done that way, and same for majority.Also note if you ever get more than 3 people, you can choose to do a variance based approach (use data if only x% agree, throw away otherwise (or you could even weigh controversial labels lower))"
What are some most promising ways to approximate common sense and background knowledge?,"
I learned from this blog post Self-Supervised Learning: The Dark Matter of Intelligence that

We believe that self-supervised learning (SSL) is one of the most promising ways to build such background knowledge and approximate a form of common sense in AI systems.

What are other most promising ways which would be competitive to self-supervised learning?
I only know the knowledge base, but I don't think it would be that promising due to the curation problem of the large scale Automated Knowledge Base Construction.
","['neural-networks', 'pretrained-models', 'self-supervised-learning', 'commonsense-knowledge']",
How is the discounted maximum entropy objective obtained for soft-q-learning and SAC,"
In the soft q-learning paper, they provide an expression for the maximum entropy objective that takes discounting into account.
My main question is: can someone explain how they incorporated discounting into the objective?
I've also got a few other questions related to the form of the discounted objective as well.
The first one being is: they first define the objective in way of obtaining $\pi_{\text{MaxEnt}}^*$.
In this first expression,
$$
\pi_{\mathrm{MaxEnt}}^{*}=\arg \max _{\pi} \sum_{t} \mathbb{E}_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \rho_{\pi}}\left[\sum_{l=t}^{\infty} \gamma^{l-t} \mathbb{E}_{\left(\mathbf{s}_{l}, \mathbf{a}_{l}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_{t}\right)\right) \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right]\right],
$$
I don't really understand the purpose of the inner expectation. If it's an expectation over $(s_l,a_l)$, the terms within the expectation are constants, so they can be taken out of the expectation and even the inner sum too. So, I think the subscript might be wrong, but was hoping someone could confirm this.
My second issue is: they rewrite the maximum entropy objective using $Q_{soft}$ in (16)
$$J(\pi) \triangleq \sum_{t} \mathbb{E}_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \rho_{\pi}}\left[Q_{\mathrm{soft}}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_{t}\right)\right)\right]$$
I'm not sure how they do this. If someone could provide a proof of this connection, that would be much appreciated.
","['reinforcement-learning', 'papers', 'proofs', 'soft-actor-critic']",
"In gradient descent's update rule, why do we use $\sigma(z^{l-1})\frac{\delta C_0}{ \delta w^{l}}$ instead of $\frac{\delta C_0}{\delta w^{l}}$?","
I am trying to code a two layered neural network simple NN as I have described here https://itisexplained.com/html/NN/ml/5_codingneuralnetwork/
I am getting stuck on the last step of updating the weights after calculating the gradients for the outer and inner layers via back-propagation
#---------------------------------------------------------------

# Two layered NW. Using from (1) and the equations we derived as explanations
# (1) http://iamtrask.github.io/2015/07/12/basic-python-network/
#---------------------------------------------------------------

import numpy as np
# seed random numbers to make calculation deterministic 
np.random.seed(1)

# pretty print numpy array
np.set_printoptions(formatter={'float': '{: 0.3f}'.format})

# let us code our sigmoid funciton
def sigmoid(x):
    return 1/(1+np.exp(-x))

# let us add a method that takes the derivative of x as well
def derv_sigmoid(x):
   return x*(1-x)

# set learning rate as 1 for this toy example
learningRate =  1

# input x, also used as the training set here
x = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1]  ])

# desired output for each of the training set above
y = np.array([[0,1,1,0]]).T

# Explanaiton - as long as input has two ones, but not three, ouput is One
""""""
Input [0,0,1]  Output = 0
Input [0,1,1]  Output = 1
Input [1,0,1]  Output = 1
Input [1,1,1]  Output = 0
""""""

input_rows = 4
# Randomly initalised weights
weight1 =  np.random.random((3,input_rows))
weight2 =  np.random.random((input_rows,1))

print(""Shape weight1"",np.shape(weight1)) #debug
print(""Shape weight2"",np.shape(weight2)) #debug

# Activation to layer 0 is taken as input x
a0 = x

iterations = 1000
for iter in range(0,iterations):

  # Forward pass - Straight Forward
  z1= x @ weight1
  a1 = sigmoid(z1) 
  z2= a1 @ weight2
  a2 = sigmoid(z2) 

  # Backward Pass - Backpropagation 
  delta2  = (y-a2)
  #---------------------------------------------------------------
  # Calcluating change of Cost/Loss wrto weight of 2nd/last layer
  # Eq (A) ---> dC_dw2 = delta2*derv_sigmoid(z2)
  #---------------------------------------------------------------

  dC_dw2  = delta2 * derv_sigmoid(a2)

  if iter == 0:
    print(""Shape dC_dw2"",np.shape(dC_dw2)) #debug
  
  #---------------------------------------------------------------
  # Calcluating change of Cost/Loss wrto weight of 2nd/last layer
  # Eq (B)---> dC_dw1 = derv_sigmoid(a1)*delta2*derv_sigmoid(a2)*weight2
  # note  delta2*derv_sigmoid(a2) == dC_dw2 
  # dC_dw1 = derv_sigmoid(a1)*dC_dw2*weight2
  #---------------------------------------------------------------
  
  dC_dw1 =  (np.multiply(dC_dw2,weight2.T)) * derv_sigmoid(a1)
  if iter == 0:
    print(""Shape dC_dw1"",np.shape(dC_dw1)) #debug
  

  #---------------------------------------------------------------
  #Gradinent descent
  #---------------------------------------------------------------
 
  #weight2 = weight2 - learningRate*dC_dw2 --> these are what the textbook tells
  #weight1 = weight1 - learningRate*dC_dw1 

  weight2 = weight2 + learningRate*np.dot(a1.T,dC_dw2) # this is what works
  weight1 = weight1 + learningRate*np.dot(a0.T,dC_dw1) 
  

print(""New ouput\n"",a2)


Why is
  weight2 = weight2 + learningRate*np.dot(a1.T,dC_dw2)
  weight1 = weight1 + learningRate*np.dot(a0.T,dC_dw1) 

done instead of
  #weight2 = weight2 - learningRate*dC_dw2
  #weight1 = weight1 - learningRate*dC_dw1 

I am not getting the source of the equation of updating the weights by multiplying with the activation of the previous layer
As per gradient descent, the weight update should be
$$
  W^{l}_{new} = W^{l}_{old} - \gamma * \frac{\delta C_0}{\delta w^{l}}
$$
However, what works in practice is
$$
 W^{l}_{new} = W^{l}_{old} - \gamma * \sigma(z^{l-1})\frac{\delta C_0}{ \delta w^{l}},
$$
where $\gamma$ is the learning rate.
","['deep-learning', 'backpropagation', 'gradient-descent']",
Where is the difference between a neural network mapping a problem space and learning a behaviour?,"
I've been looking at neural networks for control applications. Let's say I used an RL algorithm to train a controller for the cart pole balancing problem.
Assuming the neural network is simple and very small, I can pretty much deduce what exactly the network is doing. For instance, if the network takes inputs for pole angle and cart position and outputs a motor force, the neural network is approximating a function that will move the cart left if the pole is falling left etc. and I can forward propagate through the network manually, again assuming that it is simple. In this case however, I could say that the neural network isn't truly learning a behavior, and instead is just mapping the problem space.
However, what if I trained another, larger network for a similar problem, where there are environmental uncertainties that randomly occur (ie. oil patch on the ground so the cart dynamics change, or the ground is made of ice, or there are stochastic disturbances that simulate someone bumping the cart). If the training is successful, the resulting neural network would be learning the behaviour of balancing the cart for a variety of situations (robustness), instead of just pushing it left or right depending on the pole angle.
The cart pole problem may not be the best example for this since it's a relatively simple control problem, but for more complex behaviors (ie. autonomous driving), where does this inflection point between learning and mapping exist?
Is this even a valid question, or am I just completely mistaken and everything is technically just a function approximation and there is never any ""true"" robust learning happening?
",['philosophy'],
Can Facebook's LASER be used like BERT?,"
Can Facebook's LASER be fine-tuned like BERT for Question Answering tasks or Sentiment Analysis?
From my understanding, they created an embedding that allows for similar words in different languages to be close to each other. I just don't understand if this can be used for fine-tuning tasks like BERT can.
","['deep-learning', 'natural-language-processing', 'sentiment-analysis', 'question-answering', 'fine-tuning']",
How should I incorporate numerical and categorical data as part of the inputs to the U-net for semantic segmentation?,"
I am using a U-Net to segment cancer cells in images of patients' arms. I would like to add patient data to it in order to see if it is possible to enhance the segmentation (patient data comes in the form of a table containing features such as gender, age, etc.). So far, my researches have led me nowhere. What can I do in order to achieve this?
","['deep-learning', 'data-preprocessing', 'u-net', 'semantic-segmentation', 'multi-task-learning']",What you want to do is called multi-task learning. Here's what you do:This is in regards to TensorFlow. The same can be done in PyTorch easily.
Why the number of training points to densely cover the space grows exponentially with the dimension?,"
In this lecture (minute 42), the professor says that the number of training examples we need to densely cover the space of training vectors grows exponentially with the dimension of the space. So we need $4^2=16$ training data points if we're working on $2D$ space. I'd like to ask why this is true and how is it proved/achieved? The professor was talking before about K-Nearest Neighbors and he was using $L^{1}$ and $L^{2}$ metrics. I don't think these metrics induce a topology that makes a discrete set of points dense in the ambient space.
","['classification', 'k-nearest-neighbors', 'curse-of-dimensionality']","First, let's try to build some intuition for what we mean when we say that we want to ""densely cover"" a $d$-dimensional space $\mathbb{R}^d$ of real numbers. For simplicity, let's assume that all values in all dimensions are restricted to lie in $[0, 1]$. Even with just a single dimension $d=1$, there are actually already infinitely many different possible values even in such a restricted $[0, 1]$ range.But generally we don't actually care about literally covering every single possible value. Generally, we expect that points in this $d$-dimensional space that are ""close"" to each other also ""behave"" similarly, that there's some level of ""continuity"". Hence, to get ""sufficient"" or ""good"" or ""dense"" coverage of the space, you can somewhat informally assume that every data point you have occupies some space around it. This is the intuition behind Lutz Lehmann's comment under your question: you can think of every point as being a $d$-dimensional cube occupying some volume of your $d$-dimensional space.Now, if you have a $d$-dimensional space of size $[0, 1]$ along every dimension, and you have little cubes that occupy a part of that space (for instance, cubes of size $0.1$ in every dimension), you will indeed find that the number of cubes you need to fill up your space scales exponentially in $d$. The basic idea is: if some number of cubes $K$ is sufficient to fill up the $d$-dimensional space, and if you then increase the dimensionality to $d+1$, you'll need $dK$ cubes to fill the new space. When you add a new dimension, the complete previous space becomes essentially just one ""slice"" of the new space.For dimensions $d = 1, 2, 3$, this is fairly easy to visualise. If you have $d=1$, your space is really just a line, or a line segment if you constrain the values to lie in $[0, 1]$. If you have a $[0, 1]$ line segment, and you have little cubes of length $0.1$, you'll need just ten of them to fill up the line.Now imagine that you add the second dimension. Suddenly your line becomes an entire plane, or a $10\times10$ square grid. The $10$ cubes are now only sufficient to fill up a single row, and you'll have to repeat this $10$ times over to fill up the entire $2$D space; you need $10^2 = 100$ cubes.Now imagine that you add the third dimension. What used to be a plane gets ""pulled out"" into an entire three-dimensional cube -- a large cube, which will require many little cubes to fill! The plane that we had previously is again just a flat slice in this larger $3$D space, and the entire strategy for filling up a plane will have to be repeated $10$ times over to fill up $10$ such slices of the $3$D space; this now requires $10^3 = 1000$ cubes.Past $3$ dimensions, the story continues in exactly the same way, but is a bit harder for us humans to visualise."
Why does this paper say that the Nash-equilibrium of GAN is given by a discriminator which is 0 everywhere on the data distribution?,"
I am facing difficulty in understanding the bolded portion of the following statement from this paper

GANs are defined by a min-max two-player game between a discriminative network $D_\Psi(x)$ and generative network $G_\theta(z)$. While the discriminator tries to distinguish between real data point and data points produced by the generator, the generator tries to fool the discriminator. It can be shown that if both the generator and discriminator are powerful enough to approximate any real-valued function, the unique Nash-equilibrium of this two-player game is given by a generator that produces the true data distribution and a discriminator which is 0 everywhere on the data distribution.

My understanding is that discriminator gives $\dfrac{1}{2}$ for any further inputs after training. But, what is the $0$ mentioned?
","['papers', 'generative-adversarial-networks', 'discriminator']",
How to deal with losses on different scales in multi-task learning?,"
Say I'm training a model for multiple tasks by trying to minimize sum of losses $L_1 + L_2$ via gradient descent.
If these losses are on a different scale, the one whose range is greater will dominate the optimization. I'm currently trying to fix this problem by introducing a hyperparameter $\lambda$, and trying to bring these losses to the same scale by tuning it, i.e., I try to minimize $L_1 +\lambda \cdot L_2$ where $\lambda > 0 $.
However, I'm not sure if this is a good approach. In short, what are some strategies to deal with losses having different scales when doing multi-task learning? I'm particularly interested in deep learning scenarios.
","['deep-learning', 'objective-functions', 'gradient-descent', 'multi-task-learning']",
Is my approach to building an RNN to predict the probability that the word is in English appropriate?,"
Goal
To build an RNN which would receive a word as an input, and output the probability that the word is in English (or at least would be English sounding).
Example
input:  hello 
output: 100%

input:  nmnmn 
output: 0%

Approach
Here is my approach.
RNN
I have built an RNN with the following specifications: (the subscript $i$ means a specific time step)
The vectors (neurons):
$$
x_i \in \mathbb{R}^n \\
s_i \in \mathbb{R}^m \\
h_i \in \mathbb{R}^m \\
b_i \in \mathbb{R}^n \\
y_i \in \mathbb{R}^n \\
$$
The matrices (weights):
$$
U \in \mathbb{R}^{m \times n} \\
W \in \mathbb{R}^{m \times m} \\
V \in \mathbb{R}^{n \times m} \\
$$
This is how each time step is being fed forward:
$$
y_i = softmax(b_i) \\
b_i = V h_i \\
h_i = f(s_i) \\
s_i = U x_i + W h_{i-1} \\
$$
Note that the $ + W h_{i-1}$ will not be used on the first layer.
Losses
Then, for the loss of each layer, I used cross entropy ($t_i$ is the target, or expected output at time $i$):
$$
L_i = -\sum_{j=1}^{n} t_{i,j} \ln(y_{i,j})
$$
Then, the total loss of the network:
$$
L = \sum L_i
$$
RNN diagram
Here is a picture of the network that I drew:

Data pre-processing
Here is how data is fed into the network:
Each word is split into characters, and every character is split into a one-hot vector. Two special tokens START and END are being appended to the word from the beginning and the end. Then the input at each time step will be every sequential character without END, and the output at each time step will be the following character to the input.
Example
Here is an example:

Start with a word: ""cat""
Split it into characters and append the special tags: START  c  a  t  END
Transform into one-hot vectors: $v_1, v_2, v_3, v_4, v_5$
Then the input is $v_1, v_2, v_3, v_4$ and the output $v_2, v_3, v_4, v_5$

Dataset
For the dataset, I used a list of English words.
Since I am working with English characters, the size of the input and output is $n=26+2=28$ (the $+2$ is for the extra START and END tags).
Hyper-parameters
Here are some more specifications:

Hidden size: $m=100$
Learning rate:  $0.001$
Number of training cycles:  $15000$ (each cycle is a loss calculation and backpropagation of a random word)
Activation function: $f(x) = \tanh(x)$

Problem/question
However, when I run my model, I get that the probability of some word being valid is about 0.9 regardless of the input.
For the probability of a word begin valid, I used the value at the last layer of the RNN at the position of END tag after feeding forward the word.
I wrote a gradient checking algorithm and the gradients seem to check up.
Is there conceptually something wrong with my neural network?
I played a bit with $m$, the learning rate, and the number of cycles, but nothing really improved the performance.
","['neural-networks', 'natural-language-processing', 'recurrent-neural-networks', 'data-preprocessing']","while using a neural network for this type of problem is not the ideal use-case, it is a good exercise.In terms of conceptual issues, the most concerning that I see is the loss: $\sum_{i=1}^N L_i$.First issue, is that it validates loss at each time step equivalently. This is probably not ideal because in the example (cat), we dont expect it to know its English from just c or ca, but at cat. The quickest fix and probably the best, is to just use $L = L_N$. Though an argument would be that the model should become more and more aware as it gets more letters, and this loss function doesnt achieve that, so another solution would be to add another fixed parameter that you can play with: $L = \sum_i r^{-i}L_i$ where $0 \lt r \lt 1$. Note that $r^{-i}$ can be replaced with any function that increases in size.Another issue with the loss is it doesnt normalize, meaning on average, larger words will hold more weight to the model than smaller ones, while this may be intended it should be noted and considered (also note if you do end up going with $L=L_N$ this will no longer be a concern.Hope this helps"
Does the Bayesian MAP give a probability distribution over unseen t*?,"
I'm working my way through the Bayesian world. So far, I've understood that the MLE or the MPA are point estimates, therefore using such models just output one specific value and not a distribution.
Moreover, vanilla neuronal networks do in fact s.th. like MLE, because minimizing the squared-loss or the cross-entropy is similar to finding parameters that maximize the likelihood. Moreover, using neural networks with regularisation is comparable to the MAP estimates, as the prior works like the penalty term in error functions.
However, I've found this work. It shows that the weights $W_{PLS}$ gained from a penalized least-squared are the same as the weights $W_{MAP}$ gained through maximum a posteriori:

However, the paper says:

The first two approaches result in similar predictions, although the MAP Bayesian model does give a probability distribution for $t_*$ The mean of this distribution is the same as that of the classical predictor $y(x_*; W_{PLS})$, since $W_{PLS} = W_{MAP}$

What I don't get here is how can the MAP Bayesian give a proability distribution over $t_*$, when it is only a point estimate?
Consider a neuronal network - a point estimate would mean some fixed weights, so how can there be a output probability distribution? I thought that this is only achieved in the true Bayesian, where we integrate out the unknown weights, therefore building something like the weight averaged of all outcomes, using all possible weights.
Can you help me?
","['machine-learning', 'maximum-likelihood', 'bayesian-inference']",
Is there a relationship between Computer Algebra and NLP?,"
My intuition is that there is some overlap between understanding language and symbolic mathematics (e.g. algebra). The rules of algebra are somewhat like grammar, and the step-by-step arguments get you something like a narrative. If one buys this premise, it might be worth training an AI to do algebra (solve for x, derive this equation, etc).
Moreover, when variables represent ""real"" numbers (as seen in physics, for example) algebraic equations describe the real world in an abstracted, ""linear,"" way somewhat similar to natural language.
Finally, there are exercises in algebra, like simplifying, deriving useful equations, etcetera which edge into the realm of the subjective, yet it is still much more structured and consistent than language. It seems like this could be a stepping stone towards the ambiguities of natural language.
Can anyone speak to whether this has either (1) been explored or (2) is a totally bogus idea?
","['natural-language-processing', 'comparison', 'symbolic-computing']","There isn't, really. Natural language is way more complex and irregular than algebra, which is far more formalised and unambiguous.So far, in NLP, most success/progress has been made in little toy domains, which exclude most of the complexities of real life, including many ambiguities.When you say the rules of algebra are somewhat like grammar, then that is because it is essentially a formal language, for which we can specify a grammar. There is currently no complete grammar for any human language (and I doubt there ever will be), let alone a formal one that can be processed by computer.This was one of the reasons why the first AI boom, where a lot of over-hyped promises where made about being able to translate Russian into English automatically, failed abysmally: natural languages are more than just formal grammars of lexical items.Stochastic approaches have gone some way towards pragmatic solutions, but when it comes to understanding language they are basically a fudge. And don't get me started on deep learning approaches to NLP.So the only relationship is that we use the term 'grammar' for the descriptive formalisms in both cases; a formal grammar of algebra would be very different from a grammar for a human language.This doesn't mean, however, that approaches developed in the field of NLP cannot be applied to algebra: even those which failed in NLP because they were overly limiting. To find out more about this, look for Chomsky Hierarchy -- that describes the different expressive powers of formal languages.But I would argue that human language is outside of that, because it is not a formal language."
Convert LSTM univariate Autoencoder to multivariate Autoencoder,"
I have the following code snippet which takes in a single column of value i.e. 1 feature. How do I modify the LSTM model such that it accepts 3 features?
import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Input, Dropout
from keras.layers import Dense
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from keras.models import Model
import seaborn as sns    

dataframe = pd.read_csv('GE.csv')
dataframe.head()

df = dataframe[['Date', 'EnergyInWatts']]
df['Date'] = pd.to_datetime(df['Date'])
sns.lineplot(x=df['Date'], y=df['EnergyInWatts'])

#train, test = df.loc[df['Date'] <= '2003-12-31'], df.loc[df['Date'] > '2003-12-31']
train = df.loc[df['Date'] <= '2003-12-31']
test = df.loc[df['Date'] > '2003-12-31']

scaler = StandardScaler()

scaler = scaler.fit(train[['EnergyInWatts']])

train['EnergyInWatts'] = scaler.transform(train[['EnergyInWatts']])
test['EnergyInWatts'] = scaler.transform(test[['EnergyInWatts']])

seq_size = 30 


def to_sequences(x, y, seq_size=1):
    x_values = []
    y_values = []

    for i in range(len(x)-seq_size):
        #print(i)
        x_values.append(x.iloc[i:(i+seq_size)].values)
        y_values.append(y.iloc[i+seq_size])
        
    return np.array(x_values), np.array(y_values)

trainX, trainY = to_sequences(train[['EnergyInWatts']], train['EnergyInWatts'], seq_size)
testX, testY = to_sequences(test[['EnergyInWatts']], test['EnergyInWatts'], seq_size)


model = Sequential()
model.add(LSTM(128, input_shape=(trainX.shape[1], trainX.shape[2])))
model.add(Dropout(rate=0.2))
model.add(RepeatVector(trainX.shape[1]))
model.add(LSTM(128, return_sequences=True))
model.add(Dropout(rate=0.2))
model.add(TimeDistributed(Dense(trainX.shape[2])))
model.compile(optimizer='adam', loss='mae')
model.summary()

history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_split=0.1, verbose=1)
```

","['machine-learning', 'long-short-term-memory', 'autoencoders']","If each of your three features is a scalar then my first attempt would be to combine them into a vector for each step in the sequence. So instead of LSTM(128, input_shape=(30,1)) for a length-30 univariate sequence you would say LSTM(128, input_shape=(30,3)) for a multivariate (3) sequence. Similarly your output would become TimeDistributed(Dense(3, activation='linear')).If your input features are each a vector then you should consider switching from the sequential API to the functional API. You would need three separate Input objects followed by a Concatenate layer before the LSTM layer. You would have three Dense layers to be your corresponding outputs. I tried adapting your code:"
What amount of ressources is involved in building an image recognition system? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I would like to have an order of magnitude of ressources required to build an image recognition system.
Let say you want to build a startup company which main product will have to distinguish 20 different kinds of objects (bottle, dogs, car, flowers...). Images are already tagged.

How many images are needed as a learning set ? 1k, 10k, 100k, 1
million ?
What kind of hardware and how long will the learning process take ?
How many developers, how much time ​?
Does it changes a lot if the number of target output is reduced to two kinds, or increased to one thousands ?

​A link to a real life paper would be perfect. Thank you
","['image-processing', 'hardware-evaluation']","One answer is infinite amount of time because it can always be better.Another answer is:To conclude, please be aware that your question is super open ended, and my answer is bad (but good enough for now maybe), but a good answer doesn't really exist. It's always going to be context dependent. For instance, you never said whether you need 90% or 99% accuracy."
How to split data for meta-learning?,"
I've been trying to understand the meta-learning paradigm, more precisely, the optimization-based models, such as MAML, but I have a hard time understanding how I should correctly split my data to train such models.
For example, let's consider we have 4 traffic datasets, and we would like to use 3 of them as source datasets (to train the model) and the remaining one as target (to fine-tune on it and then test the model performance). As far as I understood, for each source dataset, I need to split it into train and validation. During training, I would randomly select 2 batches of samples from the training datasets, use one batch to compute the task-specific parameters and the other one to compute the loss. Then repeat the same process with the validation dataset, such that I can select the best candidate model. After the training is done, I need to fine-tune the model on the target dataset. I assume the process is the same, but I need to use the target dataset instead.
During testing (after the model is fully learned and fine-tuned), how exactly do I test it? It is the same procedure as if I was training a supervised model? I would like to know if the setup I described is correct and it fits the meta-learning paradigm.
","['meta-learning', 'training-datasets', 'model-agnostic-meta-learning']",
How can I model any structure for a neural network?,"
Hello I am currently doing research on the effect of altering a neural network's structure. Particularly I am investigating what affect would putting a random DAG (directed acyclic graph) in the hidden layer of a network instead of a usual fully connected bipartite graph.
For instance my neural network would look something like this:

Basically I want the ability to create any structure in my hidden layer as long as it remains a DAG [add any edge between any node regardless of layers]. I have tried creating my own library to do so but it proved to be much more tedious than anticipated therefore I am looking for ways to do this on existing libraries such as Keras, pytorch, or tensorflow.
","['neural-networks', 'tensorflow', 'keras', 'pytorch', 'implementation']",
What is this algorithm? Is it a variant of Monte-Carlo Tree Search?,"
I'm using a Neural Network as an agent in a simple car racing game. My goal is to train the network to imitate a brute-force tree search to an arbitrary depth.
My algorithm goes something like the following:

The agent starts with depth 0
Generate a bunch of test states
For each test state s:

For each action a:

s' = env.step(s, a)
for x in range(agent.depth): s' = env.step(s', agent.predict(s'))
calculate reward at state s'


Set the label for test state s as whichever action a produced the highest reward


Train the agent using the test states and labels, and increment agent.depth
Loop until desired depth

The idea is that an agent trained in this way to depth N should produce output close to a brute-force tree search to depth N...so by using it to play out N moves, it should be able to find me the best final state at that depth. In practice, I've found that it performs somewhere between N and N-1 (but of course it never reaches 100% accuracy).
My question is: what is the name of this algorithm? When I search for tree search with playout, everything talks about MCTS. But since there's no randomness here (the first step is to try ALL actions), what would this be called instead?
","['neural-networks', 'monte-carlo-tree-search', 'tree-search']","In short it looks like you have constructed a valid reinforcement learning method, but it does not have much in common with Monte Carlo Tree Search. It may have some weaknesses compared to more established methods, that means it will work better in some environments rather than others.Your approach may be novel, in that you have combined ideas into a method which has not been used in this exact form before. However, it is following the principles of general policy improvement (GPI):Estimate action values of a current policy.Create a new policy by maximising action choices with respect to latest estimates.Set current policy to new policy and repeat.Your method covers exploration with a deterministic policy by sweeping through a list of state and action pairs. This resembles policy iteration, or perhaps exploring starts in Monte Carlo control. It evaluates each state/action pair by following the current policy up to a time step horizon. This has some weakness, but it may work well enough in some cases.The depth iteration is more complex to analyse. It is not doing what you suggest - i.e. it does not make the whole algorithm equivalent somehow to a tree search. Technically the value estimates with a short horizon will be poor, biased estimates of true value in the general case, but on the plus side they may still help differentiate between good and bad action choices in some situations. It may even help as a form of curriculum learning by gathering and training on data about immediately obvious decisions first (again I expect that would be situational, not something you could rely on).Overall, although you seem to have found a nice solution for your racing game, I think your method will work unreliably in a general case. Environments with stochastic state transitions and rewards would be a major problem, and this is not something you could fix without major changes to the algorithm.I could suggest that you to try one or more standard published methods, such as DQN, A3C etc, and compare them with your approach on the same environments in different ways. E.g. how much time and computation it takes to train to an acceptable level, or how close to optimal each method can get to.The main comparison with Monte Carlo Tree Search is that you evaluate each state, action pair with a kind of rollout. There is lots more going on in MCTS that you are not doing though, and the iteration with longer rollouts each time is not like anything in MCTS and does not compensate for missing parts of MCTS in your algorithm."
What's the likelihood in Bayesian Neural Networks?,"
I'm trying to understand the concept behind BNN.
Their are based on the Bayes Theorem:
$$p(w \mid \text{data}) = \frac{p(\text{data} \mid w)*p(w)}{p(\text{data})}$$
which boils down to
$$\text{posterior} = \frac{\text{likelihood} * \text{prior}}{\text{evidence}}.$$
I understand that if we assume a Gaussian distribution for the model, the likelihood function comprises a product (or sum if we use log) of each data point inserted into the Gaussian pdf. The parameters which we can change are $\mu$ and $\sigma^2$. We want to increase the likelihood because higher values of the Gaussian pdf means higher probability.
How do things work when using a neural network? I assume that the likelihood function looks s.th. like inserting each data point inside the calculation (weighted sums) of the neural network. But does the neural network need a softmax layer at the end so that we can interpret the outputs as probabilities/likelihoods? Or do we measure likelihood by applying some error measurement like cross-entropy or squared-loss?
","['bayesian-deep-learning', 'bayesian-neural-networks', 'bayes-theorem']",
How to avoid being stuck local optima in q-learning and q-network,"
When using the Bellman equation to update q-table or train q-network to fit greedy max values, the q-values very often get to the local optima and get stuck although randomization rate ($\epsilon$) has already been applied since the start.
The sum of q-values of all very first steps (of different actions at the original location of the agent) increases gradually until a local optimum is reached. It gets stuck and this sum of q-values starts decreasing slowly a bit by a bit.
How to avoid being stuck in a local optimum? and how to know if the local optimum is already the global optimum? I may think of this but it's chaotic: Switch on randomization again for a while, worse values may come at first but maybe better in the future.
","['machine-learning', 'reinforcement-learning', 'q-learning', 'optimization', 'bellman-equations']","I found out the problem of why the optimization process got stuck and never moved closer to the global optimum. It's because of the rate between 'explore' or 'exploit'.Basically, in RL, the agent explores by doing a random action and to find new solutions, exploits the existing so-called known max future rewards to do the max action.Initially, I put the agent to explore when $random() < 1/(replay\_index+1)$, exploration rate reduces too quick (<10% after 10 iterations), and when the number of replays (number of times to play again from start) is not enough, the explore rate at the end of the loop is almost zero, and nothing new learned.The solution opted is allowing 'explore' and 'exploit' to have the same rate (or lower exploration a bit is also ok), pseudo-code:Explore rate can be reduced correctly this way:With the half-explore/half-exploit scheme above, the agent will learn to infinity, so, it is kinda sure that the global optimum would be reached. When knowing from practice the number of iterations that should be used, 'exploit' may be utilized more for faster convergence.Note that the 'explore' and 'exploit' rates are put equal above, the but q-table or q-network is still better and better due to having another 'exploit'-kind when updating q-table or fitting q-network with Bellman equation, there's another 'exploit' here, the 'max' in Bellman equation:Pseudo-code:"
How to forecast multiple target attributes in Python?,"
I need to forecast two non-correlated time-series (non-stationary). A sample is presented below:
414049364,21773560
414049656,21773926
414049938,21774287
414050204,21774638
414050453,21774975
414050682,21775296
414050895,21775597
414051093,21775874
414051278,21776125
414051453,21776344
414051620,21776530
414051780,21776678
414051935,21776785
414052089,21776849
414052242,21776865

The above is the input (two attributes) and the output (prediction) is composed of two targets (the same as input) for instance,
414052252,21776765

However, current regression techniques only consider a single attribute (class) forecasting but two or more. I've checked the following site https://machinelearningmastery.com/multi-output-regression-models-with-python/ for multi-target regression or predictive clustering trees. Unfortunately, I don't know how to adapt my data to those techniques. Ideally, I would like to predict multiple steps.
Any idea?
","['python', 'regression', 'time-series']","In general, multi output models is not that different.
I.e.In your particular case there are two much bigger factors.Edit: here is simple exampleNext you could normalize the values, use few previous steps as a features, use more complex algorithm then linear regression, and make train/eval split for better model evaluation."
Why is the behaviour policy denoted by $\beta$ and the exploration policy by $ \mu'$ in the DDPG paper?,"
I am learning about the deep deterministic policy gradient (DDPG) (Lillicrap et al, 2016) and got confused about the notation of the behavior policy.
Lillicrap et al. denote the policy gradient by
$$\nabla _{\theta^\mu} J \approx \mathbb{E}_{s_t \sim \rho^\beta} \left[ \nabla _{\theta^\mu} Q(s,a|\theta^Q) | s=s_t, a=\mu(s_t ; \theta ^\mu)  \right],$$
where $\beta$ denotes the behavior policy (equation 5 in the original paper).
However, when they talk about exploration, they denote the exploration policy by $ \mu'$. This notation seems confusing to me since the target actor network is also denoted by $\mu'(s|\theta^{\mu'})$.
As far as I understand, the exploration policy is not directly linked to the target critic network but rather corresponds to the previously mentioned behavior policy $\beta$. Is this correct or am I understanding it wrong?
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'ddpg', 'notation']","You are right, it is sloppy notation by the authors. However, the target network is not necessarily linked to the behaviour policy $\beta$ either.Essentially when they take the expectation with respect to $\rho^\beta$ they are taking expectation with respect to a state distribution induced by some policy $\beta$ that is not necessarily the same as our current policy -- this is what makes DDPG an off-policy algorithm.The target actor network $\mu'$ is used in the loss function for the critic; the target for the critic (ignoring parameters for brevity) $y = r + \gamma Q(s', \mu'(s'))$ where $s'$ is the state we transitioned to from $s$ when we took an action $a$.Now, $a$ was sampled as part of the tuple $(s, a, r, s')$ from our replay buffer, meaning that $a$ will have been chosen according to some past version of our policy, plus some exploration noise $\mathcal{N}$. The way this links to the behaviour policy $\beta$ is that because we have sampled it from some old version of our policy, i.e. not our current policy, it is instead coming from this behaviour policy $\beta$.The target network $\mu'$ is simply a copy of the current actor network where the weights are updated using the polyak averaging technique and is not really related to the behaviour policy $\beta$, at least not in any useful way for you to think about it."
Is there a bias-variance equivalent in unsupervised learning?,"
In supervised learning, bias, variance are pretty easy to calculate with labeled data. I was wondering if there's something equivalent in unsupervised learning, or like a way to estimate such things?
If not, how do we calculate loss functions in unsupervised learning?
","['variance', 'unsupervised-learning']",
Is it appropriate to use a softmax activation with a categorical crossentropy loss?,"
I have a binary classification problem where I have 2 classes. A sample is either class 1 or class 2 - For simplicity, lets say they are exclusive from one another so it is definitely one or the other.
For this reason, in my neural network, I have specified a softmax activation in the last layer with 2 outputs and a categorical crossentropy for the loss. Using tensorflow:
model=tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units=64, input_shape=(100,), activation='relu'))
model.add(tf.keras.layers.Dropout(0.4))
model.add(tf.keras.layers.Dense(units=32, activation='relu'))
model.add(tf.keras.layers.Dropout(0.4))
model.add(tf.keras.layers.Dense(units=2, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

Here are my questions.

If the sigmoid is equivalent to the softmax, firstly is it valid to specify 2 units with a softmax and categorical_crossentropy?

Is it the same as using binary_crossentropy (in this particular use case) with 2 classes and a sigmoid activation, and if so why?


I know that for non-exclusive multi-label problems with more than 2 classes, a binary_crossentropy with a sigmoid activation is used, why is the non-exclusivity about the multi-label case uniquely different from a binary classification with 2 classes only, with 1 (class 0 or class 1) output and a sigmoid with binary_crossentropy loss.
","['binary-classification', 'sigmoid', 'softmax', 'categorical-crossentropy', 'binary-crossentropy']","Let's first recap the definition of the binary cross-entropy (BCE) and the categorical cross-entropy (CCE).Here's the BCE (equation 4.90 from this book)$$-\sum_{n=1}^{N}\left( t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right) \label{1}\tag{1},$$whereHere's the CCE (equation 4.108)$$
-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{n k}\label{2}\tag{2},
$$
whereLet $K=2$. Then equation \ref{2} becomes$$
-\sum_{n=1}^{N} \sum_{k=1}^{2} t_{n k} \ln y_{n k} =
-\sum_{n=1}^{N} \left( t_{n 1} \ln y_{n 1} + t_{n 2} \ln y_{n 2} \right)
\label{3}\tag{3} 
$$So, if $[y_{n 1},  y_{n 2}]$ is a probability vector (which is the case if you use the softmax as the activation function of the last layer), then, in theory, the BCE and CCE are equivalent in the case of binary classification. In practice, if you are using TensorFlow, to choose the most suitable loss function for your problem, you could take a look at this answer."
Why doesn't U-Net work with images different from the dataset?,"
I have implemented a U-Net, similar to this implementation, but for a different dataset, this one, to segment roads.
It works fine using the test folder images, but, for example, when I pick a print from bing maps and try to infer with the trained model, this is returned:

Why this is happening?
I already tried to change the thresholding values, normalization, etc.
Tensorboard
","['datasets', 'image-segmentation', 'u-net', 'training-datasets', 'test-datasets']",
Train agent to surround a burning fire,"
I have built a wildfire 'simulation' in unity. And I want to train an RL agent to 'control' this fire. However, I think my task is quite complicated, and I can't work out to get the agent to do what I want.
A fire spreads in a tree-like format, where each node represents a point burning in the fire. When a node has burned for enough time, it spreads in all possible cardinal directions (as long as it does not spread to where it came from). The fire has a list of 'perimeter nodes' which represent the burning perimeter of the fire. These are the leaf nodes in the tree. The rate of spread is calculated using a mathematical model (Rothermel model) that takes into account wind speed, slope, and parameters relating to the type of fuel burning.
I want to train the agent to place 'control lines' in the map, which completely stops the fire from burning. The agent will ideally work out where the fire is heading and place these control lines ahead of the fire such that it runs into these lines.
Please could you guide me (or refer me to any reading that would be useful) on how I can decide the rules by which I give the model rewards?
Currently, I give positive rewards for the following:

the number of fire nodes contained by a control line increases.

And I give negative rewards for:

the number of fire nodes contained by a control line does not increase.
the agent places a control line (these resources are valuable and can only be used sparingly).

I end the session with a win when all nodes are contained, and with a loss if the agent places a control line out of the bounds of the world.
I am currently giving the agent the following information as observations:

the direction that the wind is heading, as a bearing.
the wind speed
the vector position that the fire is started at
the current percentage of nodes that are contained
the total number of perimeter nodes

I am new to RL, so I don't really know what is the best way to choose these parameters to train on. Please could you guide me to how I can better solve this problem?
",['reinforcement-learning'],
Can we use Multiple data as Input in a NN for a single Output?,"
So I am new to NN and I'm trying to go deep and apply it to my subject. I would like to ask: the input of the NN can be 2 or more values for example-> the measurement of a value, distance, and time? An example of input data would be [ [1,2,3, ....],[11,22,33, .....],[5] ] whose output is a value 1 for example or cat or an generated model.
","['neural-networks', 'tensorflow']","Yes, it can be those multiple inputs, you would have to design the network like that (to have 3 input layers and then pool them afterwards).But, an easier solution would be to flatten this array and put it through 1D CNN and finally put it through a softmaxx."
Multi-target regression using scikit-learn without ytrain,"
I would like to use the multi-target regression with scikit-learn. However, the examples I've seen use Xtrain and ytrain?
What is ytrain in regression?
I know y it is used for classes in classification. My data is composed of two columns of data, and I want to predict both values independently (but using a single MTR). So, I have clear X is a training set of n number of samples from those two values, however, I don't know how to create y.
My data is composed of two attributes not correlated, I guess this is Xtrain. What should I use as ytrain?
I'm based on this source https://machinelearningmastery.com/multi-output-regression-models-with-python/.
Any clue?
","['python', 'regression', 'multi-label-classification']",
How is the AI in 3d games implemented?,"
A few days ago, I started looking a bit more into AI and learning about the way it works, and it is very interesting, but I can't find a clear answer on how the artificial intelligence is implemented in 3d shooter games, like COD or practically any 3d game.
I just don't understand how they teach the enemies such different things based on the game to fit its narratives. For example, is the enemy ""AI"" in 3d games just a bunch of if-else statements, or do they actually teach the enemies to think strategically? In big AAA games, you can clearly see that enemies hide from you in shootings and peek to shoot not just rush and get killed.
So, how is the AI in 3d games implemented? How do they code it? You don't need to explain in detail, but just give me the idea. Do they use algorithms?
","['reinforcement-learning', 'game-ai', 'implementation', 'gaming', 'algorithm-request']","Nowadays, if you search for AI online, you will find a lot of material about machine learning, natural language processing, intelligent agents and neural networks. These are not the whole of AI by any means, expecially in a historical context, but they have recently been very successful, there is lots of published material about them.Games, especially action games, tend not to use these new popular technologies because they have other priorities. However, under the broadest definitions, there is good overlap between AI in general and game AI, or more specifically enemy AI within a game:Computer-controlled opponents within a game are effectively autonomous agents operating within an environment.The game's purpose is to entertain the player, and this usually translates into requiring challenging and believable behaviour for actions taken by opponents (for some value of ""believable"").A 3D map with a physics engine, other moving agents and possible hazards is a complex environment that needs some effort from a developer in order to define working behaviours.Many games need to solve one or two classic AI problems such as pathfinding.All the above means that the term ""Game AI"" is a good choice for describing how enemy agents are designed and implemented. However, there are large differences between goals of AI applied today in a 3D shooter and the goals of AI used in industry to control decision making.Action games have a low CPU budget available per enemy for decision-making. Control systems in industry can have dedicated machinery, sometimes multiple machines, just to run the AI.Action game priority is the game experience of the player, and this translates into strict targeting of a definition of ""believable"" for enemy behaviour that will entertain. In contrast, AI used in industry has accuracy as a high priority.Developing enemy AI in games is a specialist skill, and only partly related to the purpose of this site. For detailed discussion you may want to look into Game Development Stack Exchange which has many questions and answers on topics like Enemy AIVery roughly, the key traits of enemy AI in a game like Call of Duty could look like this (I have no inside knowledge of how COD does this, so may be inaccurate in places):Enemies will have one or more modes of behaviour defined. This might be as simple as ""Idle"" and ""Attack Player"", or there could be many. For COD I would suspect there are many, and some may be in a hierarchy - e.g. there may be several sub-types of ""Attack"" behaviour depending on the enemy design.Within a mode of behaviour, there may be a few different components defined, only some of which use AI routines. For instance, there will be specific animations related to walking or standing which are not AI, but there will also be some pathfinding AI if the agent is attempting to move around.Decisions to switch between different modes of behaviour are often scripted with simple triggers, such as detecting player visibility using ray-casting between the location of player and enemy. When game AI fails to produce realistic results, it is often these high level triggers being brittle and not covering edge cases that causes it. Depending on complexity of the game and number of behaviours, there may be an algorithm managing the transitions between them, such as a finite state engine or behaviour trees.Enemies will be presented with highly simplified observations of the game world from their perspective, in order to speed AI decisions.AI systems will have CPU budget restricted. A 3D game spends significant resources rendering scenes, and will often try to render quickly, e.g. 100 times per second. There are multiple ways that AI budget can be allocated, but it is relatively common for AI calculations to be spread over muliple frames, and for search structures for tasks like path-finding to persist over time.There will be a middle ground between scripted behaviour and AI-driven behaviour where analysis is done as part of game design. For instance, pathfinding routes may be pre-calculated to some degree. A system of way points is one example of this - it might be set by the game designer, it may be calculated by an AI component of the game asset-building pipeline, or it may be dynamically calculated and cached during a game session so that multiple enemy units can share it.When complex AI is not needed to achieve a goal, when a simple caclulation or ""puppet-like"" behaviour would do just fine, then this could be chosen instead. For instance, an enemy ""aiming"" at a player can be a simple vector calculation, perhaps with a fudge factor of a miss chance depending on the range to make the enemy seem fallible and not as much like a machine.I do not know how the behaviour of enemies using cover is implemented in COD. A simple variant would be to have each enemy hard-coded to use a specific piece of cover that usually works well against the player due to map design. However, it is definitley possible to have a search algorithm running (perhaps over multiple frames) that assesses nearby locations that the enemy could reach against the player's current position, and then pick those as variables to plug into the ""take cover and fire on player"" scripted behaviour. That assessment would use the same kind of visibilty detection between enemy and player that is used to trigger changes between ""Idle"" and ""Attack"" behaviours for the enemy.As an aside, one related thing I find interesting is in using modern AI techniques to blend animations and make interactions between actors and the environment look more realistic. Although it is a lower-level feature than the question about enemy behaviour you are asking, it is an interesting cross-over between robotics and game playing that we will likely see in next-generation games, and probably applied first to the more detailed player model animations. Here is another example applied to a humanoid agent switching smoothly between different tasks, which any game player will recognise as something which game engines cannot do well at the moment - there are usually many jarring transitions caused by events in the game."
Are there regularisation methods related only to architecture of the CNNs?,"
Are there any methods of regularisation of deep neural networks, particularly CNNs (or generally ANN but that will also work on CNNs) that are related only to the network's architecture and not the training itself?
I mean maybe something like how deep they are, amount of conv/pooling/fully connected layers, size of filters, size of steps of filters, etc. any pointers that would help with regularisation.
EDIT: To explain deeper what I mean I might add that I am exploring an experimental idea for the training of the CNNs that is not in any way related to typical gradient descent with backpropagation. That is why typical methods related to training will not work. I can see already that the models train satisfactorily on the training set but don't perform that well on a test set and since I didn't figure out any regularization methods for this type of training I thought maybe there are some related to architecture, that the training process will have to abide.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'architecture', 'regularization']",
What to do with a GAN that trained well but got worse over time?,"
I am training a WGAN-GP network based on the following paper, though I am using a different dataset. Now, for the first ~ 60-70 epochs, my network trained really well, which I could see in the loss going down, but I also made sure to regularly check the quality of the images.
Unfortunately, what I am seeing now (for the last $20$ epochs) is that the generator is getting worse and worse, the images don't look that good anymore. I save checkpoints every epochs, so in principle, I could stop training and get myself a state of the network from where it was still performing quite okay.
However, my question would be: How can I improve the training of the GAN? Would you decrease the learning rate?
I use a batch size of 124 and a learning rate of 1e-3. Maybe I could/should continue training (with a checkpoint that was still quite okay) with a learning rate of 5e-4?
Any other hints would be appreciated!
","['papers', 'generative-adversarial-networks', 'hyperparameter-optimization', 'implementation', 'wasserstein-gan']",
What is the input to the left most LSTM cell c(t-1) and h(t-1)?,"
Given an LSTM model with 3 cells shown below, what would be the input to the left most cell c(t-1) and h(t-1)?
","['tensorflow', 'keras', 'long-short-term-memory']",
What is the name of this algorithm that estimates the gradient with an average by sampling from a distribution?,"
Consider maximizing the function $R(w)$ with parameter $w$ using gradient ascent. However, we don't know the gradient $\nabla_wR(w)$ formula. Now suppose $w$ is sampled from a probability distribution $\pi(w,\theta)$ parameterized by $\theta$. Then we can define
$$J(\theta)=E[R(w)]=\int R(w)\pi(w,\theta)dw.$$
And we have
$$\nabla_\theta J(\theta)=E[R(w)\nabla_\theta \log \pi(w,\theta)]$$.
Then, if we sample $w_1,\ldots,w_N$, we can estimate the gradient as $$\nabla_\theta J(\theta)\approx \frac{1}{N}\sum_{i=1}^N R(w_i) \nabla_\theta \log \pi(w_i, \theta).$$
It looks like REINFORCE algorithm in Deep Reinforcement Learning. Does this algorithm have a name? Is the above derivation correct?
I wonder if it is useful in optimizing $R(w)$ function.
","['reinforcement-learning', 'deep-rl', 'terminology', 'gradient-descent', 'reinforce']",
What are the purposes of pooling in CNNs?,"
There are at least three questions on this site related to this

What is the effect of using pooling layers in CNNs?
Is pooling a kind of dropout?
What are the benefits of using max-pooling in convolutional neural networks?

I got the following useful information regarding the purpose of pooling. As per my understanding, the purposes of pooling, based on priority, in general, are as follows:

To decrease the size of the feature maps
To make the model stronger in feature extraction

Are there any other purposes of pooling in CNN other than them?
","['convolutional-neural-networks', 'pooling']",
"Proving existence or non existence of reward function to make given policy ""uniquely"" optimal when reward function is dependent only on S or both S,A","
I was going through paper titled ""Algorithms for Inverse Reinforcement Learning"" by Andrew Ng and Russell.
It states following basics:


MDP $M$ is a tuple $(S,A,\{P_{sa}\},\gamma,R)$, where

$S$ is a finite seto of $N$ states
$A=\{a_1,...,a_k\}$ is a set of $k$ actions
$\{P_{sa}(.)\}$ are the transition probabilities upon taking action $a$ in state $s$.
$R:S\rightarrow \mathbb{R}$ is a reinforcement function (I guess its what it is also called as reward function) For simplicity in exposition, we have written rewards as $R(s)$ rather than $R(s,a)$; the extension is trivial.


A policy is defined as any map $\pi : S \rightarrow A$

Bellman Equation for Value function $V^\pi(s)=R(s)+\gamma \sum_{s'}P_{s\pi(s)}(s')V^\pi(s')\quad\quad...(1)$

Bellman Equation for Q function $Q^\pi(s,a)=R(s)+\gamma \sum_{s'}P_{sa}(s')V^\pi(s')\quad\quad...(2)$

Bellman Optimality:  The policy $\pi$ is optimal iff, for all $s\in S$, $\pi(s)\in \text{argmax}_{a\in A}Q^\pi(s,a)\quad\quad...(3)$

All these can be represented as vectors indexed by state, for which we
adopt boldface notation $\pmb{P,R,V}$.

Inverse Reinforcement Learning is: given MDP $M=(S,A,P_{sa},\gamma,\pi)$, finding $R$ such that $\pi$ is an optimal policy for $M$

By renaming actions if necessary, we will assume
without loss of generality that $\pi(s) = a_1$.



Paper then states following theorem, its proof and a related remark:

Theorem: Let a finite state space $S$, a set of actions $A=\{a_1,..., a_k\}$, transition probability matrices ${\pmb{P_a}}$, and a discount factor $\gamma \in (0, 1)$ be given. Then the policy $\pi$ given by $\pi(s) \equiv a_1$ is optimal iff, for all $a = a_2, ... , a_k$, the reward $\pmb{R}$ satisfies  $$(\pmb{P}_{a_1}-\pmb{P}_a)(\pmb{I}-\gamma\pmb{P}_{a_1})^{-1}\pmb{R}\succcurlyeq 0 \quad\quad ...(4)$$
Proof:
Equation (1) can be rewritten as
$\pmb{V}^\pi=\pmb{R}+\gamma\pmb{P}_{a_1}\pmb{V}^\pi$
$\therefore\pmb{V}^\pi=(\pmb{I}-\gamma\pmb{P}_{a_1})^{-1}\pmb{R}\quad\quad ...(5)$
Putting equation $(2)$ into $(3)$, we see that $\pi$ is optimal iff
$\pi(s)\in \text{arg}\max_{a\in A}\sum_{s'}P_{sa}(s')V^\pi(s')  \quad...\forall s\in S$
$\iff \sum_{s'}P_{sa_1}(s')V^\pi(s')\geq\sum_{s'}P_{sa}(s')V^\pi(s')\quad\quad\quad\forall s\in S,a\in A$
$\iff \pmb{P}_{a_1}\pmb{V}^\pi\succcurlyeq\pmb{P}_{a}\pmb{V}^\pi\quad\quad\quad\forall a\in A\text{\\} a_1 \quad\quad ...(6)$
$\iff\pmb{P}_{a_1} (\pmb{I}-\gamma\pmb{P}_{a_1})^{-1}\pmb{R}\succcurlyeq\pmb{P}_{a} (\pmb{I}-\gamma\pmb{P}_{a_1})^{-1}\pmb{R} \quad\quad \text{...from (5)}$
Hence proved.
Remark: Using a very similar argument, it is easy to show (essentially by replacing all inequalities in the proof above with strict inequalities) that the condition $(\pmb{P}_{a_1}-\pmb{P}_a)(\pmb{I}-\gamma\pmb{P}_{a_1})^{-1}\pmb{R}\succ 0 $ is necessary and sufficient for $\pi\equiv a_1$ to be the unique optimal policy.

I dont know if above text from paper is relevant for what I want to prove, still I stated above text as a background.
I want to prove following:

If we take $R : S → \mathbb{R}$—there need not exist $R$ such that $π^*$ is the unique optimal policy for $(S, A, T, R, γ)$
If we take $R : S × A → \mathbb{R}$. Show that there must exist $R$ such that $π^*$ is the unique optimal policy for $(S, A, T, R, γ)$.

I guess point 1 follows directly from above theorem as it says ""$\pi(s)$ is optimal iff ..."" and not ""unique optimal iff"". Also, I feel it also follows from operator $\succcurlyeq$ in equation $(6)$. In addition, I feel its quite intuitive: if we have same reward for any given state for every action, then different policies choosing different actions will yield same reward from that state hence resulting in same value function.
I dont feel point 2 is correct. I guess, this directly follows from the remark above which requires additional condition to hold for $\pi$ to be ""uinque optimal"" and this condition wont hold if we simply define $R : S × A → \mathbb{R}$ instead of $R : S → \mathbb{R}$. Additionally, I feel, this condition will hold iff we had $=$ in equation $(3)$ instead of $\in$ (as this will replace all $\succcurlyeq$ with $\succ$ in the proof). Also this also follow directly from point 1 itself. That is we can still have same reward for all actions from given state despite defining reward as $R : S × A → \mathbb{R}$ instead of $R : S → \mathbb{R}$, which is the case with point 1.
Am I correct with the analysis in last two paragraphs?
Update
After some more thinking, I felt I was doing it all wrong. Also I feel the text from the paper which I specified is of not much help in proving these two points. So let me restate new intuition for proofs for the two points:

For $R: S\rightarrow \mathbb{R}$, if some state $S_1$ and next state $S_2$ has two actions between them, $S_1-a_1\rightarrow S_2$ and $S_1-a_2\rightarrow S_2$, and if optimal policy $π_1^*$ chooses $a_1$, then $π_2^*$ choosing $a_2$ will also be optimal, thus making NONE ""uniquely"" optimal since both $a_1$ and $a_2$ will yield same reward as reward is associated with $S_1$ instead of with $(S_1,a_x)$.

For $R: (S,A)\rightarrow\mathbb{R}$, we can assign large reward say $+∞$ to all actions specified in given $π^*$ and $-∞$ to all other actions. This reward assignment will make $π^*$ a unique optimal policy.


Are above logics correct and enough to prove given points?
","['reinforcement-learning', 'deep-rl', 'inverse-rl']",
It is mathematically correct to use a Policy Gradient method for 1-step trajectories?,"
I have come across a Google paper that uses the REINFORCE algorithm (a Policy Gradient Method) for a case where the trajectory of the episodes it proposes would be only one step.
When trying to replicate the experiments they propose I found that there are some problems with the stability of the method (maybe that's why it is not accepted by peer review).
Researching on my own I have found something I suspected, and that is that the problem they present could be solved as a multiarmed bandit problem in THIS link. But because of this event, I have the doubt if using methods based on trajectories (such as Policy Gradient Methods) has some mathematical problem in situations where the trajectory is a single step.
PS: I think the problem of this paper may be also that they average only after one execution of a trajectory and not over k trajectories as it is necessary for a Policy Gradient Method, so I would also like to know the opinion of more people about this issue.
","['reinforcement-learning', 'policy-gradients', 'reinforce']","The fundamental idea behind policy gradient is just to maximise the return averaged across all probably trajectories, i.e$$\begin{align} J(\theta) &= E[\sum\limits_{t=1}^{\tau}r(s_t,a_t)]\\
&=E_{\tau\sim p(\tau)}[R(\tau)]
\end{align}$$Where $\tau$ represents the probability of selecting a particular trajectory, if the trajectories all have fixed length then $\tau$ only has non-zero probability for trajectories of the specified length which in this case is 1.The REINFORCE algorithm takes this expression and with some simplifications (causality to improve variance) and manipulation (log trick) obtains the gradient, pretty much as simple as that.Intuition of algorithm in paperIn the algorithm they denote the $\pi_\theta$ (which is typically reserved for describing the policy) as the probability of the selection vector. By considering instead that the function $h_{\theta}$ as the policy instead I think it can be seen that they are actually averaging over multiple trajectories.So we instead think of the data points as state-action pairs that we pass into $h_\theta$ to get the probability of selecting said action for a given state. These probabilities then dictates whether we choose the action or not. An alternative way to interpret this as if we imagine the action space for each state as binary then we can think of the ""other action"" as not impacting the predictor.The gradient used for updating the parameters associated with the policy uses the log probability of $\pi_\theta$ which if we expand it, expressing it using $h_\theta$, (as done on page 4)  we can see it's the sum of the log probabilities of selecting (or not selecting) each data point.By considering the return for each data point constant (each data point has the same loss incurred by predictor model on the validation set) and absorbing the average over batch size into the step size $\beta$ it could be interpreted as an averageStability issuesRL is plagued with stability issues, be it selection of hyper parameters, random seeding etc. It's hard to often pinpoint why results aren't exact but as long as you get something in a similar ball park i'd say thats pretty good going"
What is the effect of K in K-NN on the VC dimension?,"
What is the effect of K in K-NN on the VC dimension? When K increases, is the VC dimension decreased or increased, or we can't say anything about this? Is there a reference book that discusses this?
","['machine-learning', 'reference-request', 'computational-learning-theory', 'vc-dimension', 'k-nearest-neighbors']",
Finding Specific Patterns in Data,"
I'm trying to research modeling that can help me find very specific patterns in data.  I've done a fair amount of work about generalized predictions with machine learning, but I'm very confused about how to approach something that gets into very specific predictions.
As an example, for an IT infrastructure that has 1000 Cisco Router across the world.  I'm trying to find patterns in this data when outages occur.  Outages typically are either power related on transport circuit related.  I have historical data for over a year.  I'm trying build a model to help predict the outage type.  But, I want it to have very specific knowledge of previous outages.  Maybe there is a pattern when Routers 17,245 and 813 fail it is always a power problem at each of the sites.
I will have input data about geospacial, diagnostics, and other IT type information.
I know a lot of modeling can generalize this type of scenario, but I'm trying to see if there are options to remember more specific patterns within a large dataset.
",['neural-networks'],
Q-learning in gridworld with random board,"
I'm trying to use Q-learning in order to solve Wumpus world environment.
Wumpus world is a toy problem on 4x4 gridworld. The agent starts in entry position of the cave, looks for gold (agent can sense that he is on the gold field), then he has to pick it up and leave cave in the entry position. Some fields are safe, other contain pit or wumpus (monster). If agent move to the pit or wumpus field he dies. The fields next to wumpus or pit (not diagonally!) have properties that agent can sense - stench (wumpus), breeze (pit). Agent achieve positive reward if he leaves cave with gold and negative if he dies. Action space: turn right/left, move forward, shoot arrow (if shot in good direction it can kill wumpus, only 1 available), pick up gold, leave cave. There: https://www.javatpoint.com/the-wumpus-world-in-artificial-intelligence you can find more detailed description of environment.
It is easy to solve this problem if the gridworld is constant. I have a huge problem to even start thinking about it if gridworld is random in every learning episode (random fields and random size) and also random in testing. How should I define it (especially state space)? How should I create q table? I'm using Python.
Thank you in advance for any help.
","['reinforcement-learning', 'q-learning', 'state-spaces']","From your linked description of the game, we can see it has a key property when used normally in AI teaching:This makes sense, the problem of avoiding hazards would be trivial if the full map was revealed to the agent. The problem is also not about learning a specific map.You could use the agent's perception to construct a simple state table based on current observations:This would be relatively easy to build into a Q table, and might have some success. However, it would likely perform a lot worse than the propositional logic and planning suggested suggested at https://www.javatpoint.com/the-wumpus-world-in-artificial-intelligence for two reasons:Agent will not take account of knowledge specific to the current map that it has gathered. This is critical, because the stench and breeze sensors only tell you that at least one of the adjacent rooms has a hazard. In theory you know that this will not be any of the rooms that the agent has already visited, but a simple state representation based on current observations will not capture this.Agent will not plan using the deterministic rules of the game that you know and could code for.There are a few different approaches you could take to improve on these issues. Sticking with Q learning and trying to solve this by improving the state representation, you could look at the suggested knowledge-building structure from the java T point site, and replicate something like it as input features:When multiplied by 16 to cover each room, you will end up with ~84 binary flags. Although many combinations will not be possible, this is still going to be far too large a state space to use with a Q table. You would probably use a simple neural network and DQN agent to solve this problem using Q learning."
Can you correlate decision boundary of final layer of a neural network to predictive distribution?,"
I was reading in a On the Decision Boundary of Deep Neural Networks
 that the final layer of a MLP can be equated to an SVM and can generate decision boundaries similar to methods with SVM. I was wondering if using this boundary detection method or another can you quantify how much probability a model assigns to each bin where a bin is a class. So for example, if a project of an input before the final layer has a SVM margin of let's say 4 from the boundaries and classification 1 and 2, can we determine how much probability it'll give class 1 or 2 after the final layer?
","['supervised-learning', 'support-vector-machine', 'decision-theory']",
Resources for Computer Vision Algorithms and Applications,"
Are there any videos or other books/notes/slides that anyone has come across that follow Computer Vision Algorithms and Applications by Richard Szeliski? We are using this book in class but the professor did a bad job explaining and I have some trouble getting through the book. Thanks a lot!
","['machine-learning', 'computer-vision', 'reference-request']",
Preparing data set for the YOLO algorithm,"
Hi I am working on a project which requires the You Only Look Once algorithm in order to classify and localise objects within images. I have to prepare my dataset (which has 2 classes, and predicts 6 objects per grid cell, and the 448 * 448 image is split into a 7*7 grid). What would be a viable approach to do that? I found  this code, found in this  article. However I do not understand why he has done what he has done, e.g why is he specifically checking the 24th element of the “box”, and so what element of the box would I have to check? Is there any tutorial running through that? Would it be possible for someone to explain or even adapt his approach to fit my dataset?
FYI: I am coding the YOLO algorithm from scratch
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'keras', 'yolo']","Ok, let go step by step.What you are working on is YOLOv1, in this version of the YOLO algorithm, the maximum bounding boxes that the model can return is 7x7 = 49 boxes as 49 cells since the output shape is 7x7x30.For each box, the depth of output is 30 because the number of labels of PASCAL VOCS 2012 is 20 (the author of YOLOv1 trained on this dataset) so from index 0 to index 19 will represent the label of that bounding box. From 20 to 23 are the position and size of that box.The 24th represents 2 things, first is the confidence of that box, since this is ground truth so the confidence should be 1. Second, you know that YOLOv1 can only return 49 boxes at maximum (actually, you can edit the number of boxes by yourself) so the ground truth should only handle one per cell, hence the 24th is the binary value to make sure there are no duplicates bounding boxes in one cell. (25 to 29 is because the author predict 2 bounding boxes per cell)In your case, the output should be 7x7x(2 + 5 x 2) = 7x7x12 with 2 classes and 2 boxes per cell."
How do I get started with multi-agent reinforcement learning?,"
Is there any tutorial that walks through a multi-agent reinforcement learning implementation (in Python) using libraries such as OpenAI's Gym (for the environment), TF-agents, and stable-baselines-3?
I searched a lot, but I was not able to find any tutorial, mostly because Gym environments and most RL libraries are not for multi-agent RL.
","['reinforcement-learning', 'reference-request', 'deep-rl', 'gym', 'multi-agent-rl']",
"""Porpoising"" in latter stages of validation loss and MSE charts in Keras","
Performing a prediction of a continuous y target using Keras, the simple structure of the code revolves around;
model = Sequential()  
model.add(Dense(200, input_dim=15, activation= ""relu""))  
model.add(Dense(750, activation= ""relu""))  
model.add(Dense(500, activation= ""relu""))  
model.add(Dense(750, activation= ""relu""))  
model.add(Dense(500, activation= ""relu""))  
model.add(Dense(200, activation= ""relu""))  
model.add(Dense(100, activation= ""relu""))  
model.add(Dense(50, activation= ""relu""))  
model.add(Dense(1)) 

model.compile(loss= 'mse' , optimizer='adam', metrics=['mse','mae'])  
history=model.fit(X_train, y_train, batch_size=50,  epochs=150,  
                  verbose=1, validation_split=0.2)

This has resulted in the following metric chart;


What might be causing these, and how to eliminate (or greatly reduce) them?
UPDATE: Just reduced the learning rate to 0.0001 per Neil Slater's suggestion, and the loss curve may have had the spikes reduced, though the scale of the graph has changed. The training loss has increased from 0.00007 to 0.00037, the validation loss from 0.0014 to 0.002, and the prediction error increased from 0.037 to 0.046.

I then changed epsilon from it's value of 1e-07 to 0.1 and increased the epochs from 150 to 500. The validation loss increased to 0.0082 and the prediction error increased to 0.093, with the corresponding model loss shown below.

While not an overall improvement at either step, this did remove the spikes as I requested, hence Neil's advice gives me additional considerations to explore and measure within the Adam optimizer (along with other optimizers), so I consider this to have been an important learning experience. One such exploration uncovered this more detailed explanation of optimizers than I had been exposed to before, as well as this 3D visualization of loss topologies and the effect differing optimizers and parameters have on finding the optimal minima (keep a sharp eye on the options being chosen in the upper right corner).
","['python', 'keras', 'mean-squared-error']","What might be causing these, and how to eliminate (or greatly reduce) them?It is difficult to be sure just from the graph, but I note you are using the Adam optimiser. It shares with a few other optimisers (most notably RMSProp) that it divides current gradients by a rolling mean of recent gradients to set step sizes. This can cause some minor instability when gradients get close to zero for a long while before growing again. This might happen at a saddle point where only some fraction of the network parameters are critical to results for a few iterations before hitting some other direction of slope where changing the ""quiet"" parameters becomes important again. This is more likely to occur as loss values become small, and close to perfect convergence.There are a couple of hyperparameters in Adam that may reduce the effect:Reduce learning rate. The default learning rate in Adam is often set to 0.001, but you will find a lot of researchers will reduce that to magnitudes around 0.0001 or 0.00001Increase epsilon. This is effectively the minimum rolling average gradient. The default is 1e-7 but it sometimes needs to be increased significantly, it depends on the loss surface. The official documentation suggests that it may even be useful to increase it up to 1.0"
Why do we sample vectors from a standard normal distribution for the generator?,"
I am new to GANs. I noticed that everybody generates a random vector (usually 100 dimensional) from a standard normal distribution $N(0, 1)$. My question is: why? Why don't they sample these vectors from a uniform distribution $U(0, 1)$? Does the standard normal distribution has some properties that other probability distributions don't have?
","['generative-adversarial-networks', 'probability-distribution', 'normal-distribution', 'uniform-distribution']","It has become our human bias that data will arrive from a normal distribution. It is also the most prevalent distribution in nature occurring in many places. Hence, we sample from a normal distribution. Also, central limit theorem works around means lying around normal distribution.It is not taboo to use others if they are helpful to your network. But, a data from one distribution can transformed into other and if that is something that is required, the network will to do it since it can approximate anything (although a weak assumption, but, hey, its working right?)."
Should I need to use BERT embeddings while tokenizing using BERT tokenizer?,"
I am new to BERT and NLP and I am a little confused with tokenization and word embedding.
My doubt is if I use the BertTokenizer for tokenizing a sentence then do I have to compulsorily use BertEmbedding for generating its corresponding word vectors of the tokens or I can train my own word2vec model to generate my word embedding while using BertTokenizer?
Pardon me if this question doesn't make any sense.
","['natural-language-processing', 'word-embedding', 'bert', 'word2vec']",
Validation Accuracy remains constant while training VGG?,"
I posted this question on stackoverflow and got downvoted for unmentioned reason, so I'll repost it here, hoping to get some insights
This is the plot

This is the code:
with strategy.scope():

  model2 = tf.keras.applications.VGG16(
    include_top=True,
    weights=None,
    input_tensor=None,
    input_shape=(32, 32, 3),
    pooling=None,
    classes=10,
    classifier_activation=""relu"",
  )

  model2.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])
  
  history = model2.fit(
            train_images, train_labels,epochs=10, 
            validation_data=(test_images, test_labels)
            )

I'm trying to train VGG16 from scratch, hence not importing their weights I also tried a model which I created myself, with same hyperparameters, and that worked fine
Any help is highly appreciated
Heres the full code
","['neural-networks', 'deep-learning', 'tensorflow']","Ok, I solved this problem
The simple thing was that learning rate was too big
I changed the code to thisinstead ofand it seems to work well"
Is this LSTM layer learning anything?,"
I've trained a CNN-LSTM model but the results weren't satisfactory, so I took a look at my weight distributions and this is what I got:

I don't understand. Is this layer learning anything? Or no?
Update: I've also tried LeakyReLU activation and also removed l2 regularization and this is what I got:

So I guess my layer isn't learning or does take more epochs to train LSTM layers? The gradients are not vanishing because the CNN layer before this is changing.
","['tensorflow', 'long-short-term-memory', 'weights']",
Can I use the SIFT feature detector on data other than images?,"
I know how to use SIFT algorithm for images but I never use it for other kinds of data. I have tabular data (x, y, z, time) where x,y,z is the joint position along x, y, z coordinates. Now, can I apply the SIFT algorithm to this data to find features that will act as input to traditional machine learning algorithms, like SVM, DT, etc.?
","['machine-learning', 'computer-vision', 'feature-extraction', 'sift']",
Why is my siamese network learning very well in e.g. 1 out of every 5 runs?,"
Why is my siamese network learning very well in e.g. 1 out of every 5 runs? The rest of the time it's not learning and maintains an accuracy of 0.5.
Any explanations? Is the contrastive loss taken in the embedded space to loose of a constraint?
The task is greyscale signature matching.
Additionally, trying the model on facial matching gives a constant 0.5 accuracy, no learning at all - the images are RGB, and maybe it's a higher-order task in general.
Anyways, would appreciate any and all enlightenment in this matter.
P.S. I'm thinking to try a variational autoencoder for the face dataset, where I then use the trained encoder as the siamese network ""head"".
I would appreciate any guidance or thoughts on this approach as well.
","['deep-learning', 'facial-recognition', 'accuracy', 'siamese-neural-network']",
How can I compress the states of a reinforcement learning agent?,"
I'm working on a problem that involves an RL agent with very large states. These states consist of several pieces of information about the agent. The states are not images, so techniques like convolutional neural networks will not work here.
Is there some general solutions to reduce/compress the size of the states for reinforcement learning algorithms?
","['reinforcement-learning', 'deep-rl', 'state-spaces', 'data-compression']",
Compute state space from variables in Q-learning (RL),"
I'm trying to use Q-learning, but I'm stuck because I don't know how to compute the state.
Let's say, in my problem, there are the following variables, which I'm using to compute state:
x in range 0-3
y in range 0-3
d in range 0-3
g in range 0-1
a in range 0-1
s in range 0-4
br in range 0-4
bu in range 0-4
gl in range 0-1
So, the state space is equal to $64000$ ($4 * 4 * 4 * 2 * 2 * 5 * 5 * 5 * 2$). I'd like to create a number, from the above variables, which is contained in the range $[0, 63999]$.
My previous idea was to create a binary number from the binary representation of state variables (just write them next to each other and convert into an int). It seems to fail if a variable is not a power of two (bonus question: why doesn't it work?).
","['reinforcement-learning', 'q-learning', 'state-spaces']","It seems to fail if a variable is not a power of two (bonus question: why doesn't it work?).This does not work because you are wasting some space, some values are not used in your representation.For example with your 0-4 variables, you need 3 bits, but you only use 000, 001, 010, 011 and 100 values. The values 101, 110 and 111 are still part of the representation you are using. It doesn't matter that you don't need to use them, you have created a representation where they exist. Every time you encode 5 values as 3 bits, you are being 62.5% efficient, and the efficiencies multiply to get the overall efficiency of your representation (proportion of states you need to represent compared to the size of the representation).Sometimes this is acceptable. Your example state representation using bitwise coding per variable would still fit easily into a 32 bit integer for storage, and you also have the convenience of easily extracting the individual values quickly using bit masks. It's a bit less convenient if you were hoping to build a Q table as an array with the state as a simple offset.In your example case it is not super-wasteful, your array will be around 4 times larger than optimal due to wastage in 3 separate places. For convenience of coding simplicity you might accept this 8MB space per action over 2MB space per action (assuming 32 bit floating point values for action values). 6MB of dead space is not much to worry about - the chances are that the programming language you have loaded to run it wastes far more space on features that you are not using.If you have a few more variables and a few more wasted bit patterns, then the waste could be more noticeable and important. That might also be true if you are using a space-efficient compiled language on a system where memory resources are low.Let's say in my problem there are following variables which i'm using to compute state . . . So the state space is equal to 64000 (4 * 4 * 4 * 2 * 2 * 5 * 5 * 5 * 2)You are very close to a working answer for the most efficient representation here. You can use products of each variable's size to separate terms, to create multiplication factors for each variable. Imagine building up a cuboid out of the first three terms, and needing to address each cubic building block sequentially with an index position $i$. You would do this:$$i = x + 4y + (4 \times 4)d = x + 4y + 16d$$If you had only thie first 5 variables you would do this:$$i = x + 4y + (4 \times 4)d + (4 \times 4 \times 4)g + (4 \times 4 \times 4 \times 2)a = x + 4y + 16d + 64g + 128a$$To cover your whole set of variables, keep extending the same pattern. Each variable added to the end of the list is multiplied by the product of the space required by all previous variables. The last variable in your example, gl, would be multiplied by $32000$.Reversing this encoding is more involved, especially if you only want to access a single variable. You need to start with the value of $i$ then repeatedly use an integer divmod operation to find the next unknown variable at the end of the list - using the same multiplier for that variable as when constructing the code - and a remainder for calculating the next variable along.Luckily in reinforcement learning you probably don't need to do any reversing of state id to individual variables - you can maintain a structure of all current variables for the state when running the environment, and only need this compressed version to create an offset into the Q table for lookups for the policy or updates to stored values."
Are training sequences for LMs sampled in an IID fashion?,"
If I understand correctly, when training language models, we take a document and then chunk the document into a sequences of k tokens. So if the document is of length 30 and k=10, then we'll have 20 chunks of 10 tokens each (token 1-11, 2-12, and so on).
However these training sequences are not iid, right? If so, are there any papers that try and deal with this?
","['natural-language-processing', 'language-model', 'iid']",
In VQ-VAE code what does this line of code signify?,"
The VQ-VAE implimentation:https://colab.research.google.com/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb
quantized = inputs + (quantized - inputs).detach()

Why are we subtracting and adding input to quantized result?
",['variational-autoencoder'],
"How would I design a finite budget, cascaded multi agent deep reinforcement learning model?","
In most of the multi-agent reinforcement learning models I've found, it seems to generate the observations for each of the agents simultaneously and then uses a centralized critic to assess all of the agent's actions together.
However, what if two agents have a finite amount of resources to allocate, and the more one agent spends the less the other agent can spend. So really, the state space of the second agent is conditional on the action of the first agent.
Are there any papers or resources that describe an architecture like this?
","['reinforcement-learning', 'multi-agent-systems']",
"How to define a ""don't care"" class in time series classification in Pytorch?","
This is a theoretical question.
Setup
I have a time series classification task in which I should output a classification of 3 classes for every time stamp t.
All data is labeled per frame.
The problem:
In the data set are more than 3 classes [which are also imbalanced].
My net should see all samples sequentially, because it uses that for historical information.
Thus, I can't just eliminate all irrelevant class samples at preprocessing time.
In case of a prediction on a frame which is labeled differently than those 3 classes, I don't care about the result.

My thoughts:

The net will predict for 3 classes
The net will only learn (pass backward gradient) for valid classes, and just not calculate loss for other classes.

Questions

Is this the way to go for ""don't care"" classes in classification?
How to calculate loss only for relevant classes in Pytorch?
Should I apply some normalization per batch, or change batch norm layers if dropping variable samples per batch?

I am using nn.CrossEntropyLoss() as my criterion, which has only mean or sum as reductions.
I need to mask the batch so that the reduction will only apply for samples whose label is valid.
I could use reduction='none' and do that manually, or I could do that before the loss and keep using reduction='mean'.
Is there some method to do this using built in Pytorth tools?
Maybe this can be done in the data-fetching phase somehow?

I am looking some standard, vanilla, thumb rule implementation to tackle this. The least fancy the better.

I am aware this is more than a single question.
They are still not separable, as the solution will be unified most likely.
","['deep-learning', 'classification', 'long-short-term-memory', 'time-series', 'normalisation']",
Why is no activation function needed for the output layer of a neural network for regression?,"
I'm a bit confused about the activation function in the output layer of a neural network trained for regression. In most tutorials, the output layer uses ""sigmoid"" to bring the results back to a nice number between 0 and 1.
But in this beginner example on the TensorFlow website, the output layer has no activation function at all? Is this allowed? Wouldn't the result be a crazy number that's all over the place? Or maybe TensorFlow has a hidden default activation?
This code is from the example where you predict miles per gallon based on horsepower of a car.
// input layer
model.add(tf.layers.dense({inputShape: [1], units: 1}));

// hidden layer
model.add(tf.layers.dense({units: 50, activation: 'sigmoid'}));

// output layer - no activation needed ???
model.add(tf.layers.dense({units: 1}));

","['neural-networks', 'tensorflow', 'activation-functions', 'regression']","In regression, the goal is to approximate a function $f: \mathcal{I} \rightarrow \mathbb{R}$, so $f(x) \in \mathbb{R}$. In other words, in regression, you want to learn a function whose outputs can be any number, so not necessarily just a number in the range $[0, 1]$.You use the sigmoid as the activation function of the output layer of a neural network, for example, when you want to interpret it as a probability. This is typically done when you are using the binary cross-entropy loss function, i.e. you are solving a binary classification problem (i.e. the output can either be one of two classes/labels).By default, tf.keras.layers.Dense does not use any activation function, which means that the output of your neural network is indeed just a linear combination of the inputs from the previous layer. This should be fine for a regression problem."
It is possible to use deep learning to give approximate solutions to NP-hard graph theory problems?,"
It is possible to use deep learning to give approximate solutions to NP-hard graph theory problems?
If we take, for example, the travelling salesman problem (or the dominating set problem). Let's say I have a bunch of smaller examples, where I compute the optimal values by checking all possibilities, can this be then used for bigger problems?
In particular, let's say I take a large graph and just optimize subgraphs of this large graph. This is perhaps a more general question: My experience with deep learning (TensorFlow/Keras) is to predict values. How can I get graph isomorphism and/or a list of local moves on the graph, to obtain a better solution? Can ML/DL give you a list of moves or local changes to get closed to an optimal value, or does it just return the predicted optimal value?
","['neural-networks', 'deep-learning', 'tensorflow', 'keras', 'graph-theory']",
One hot encoding vs dummy variables best practices for explainable AI (XAI),"
When creating artificial columns for your categorical variables there are two mainstream methods you could use:
Disclaimer: For this example, I use the following definitions of dummy variables and one-hot-encoding. I'm aware both methods can be used to either return n or n-1 columns.
Dummy variables: each category is converted to it's own column and the value 0 or 1 indicates if that category is present for each record
one-hot-encoding: similar to dummy variables, but one column is dropped, as its value can be derived from the other columns. This is to prevent multicollinearity and the dummy variable trap.
As an arbitrary example, let's take people's favorite color: pink, blue and green. For a person who's favorite color is pink, the dummy and one-hot-encoded data would look as follows:
dummy variables




person_id
favorite_color_pink
favorite_color_blue
favorite_color_green




xyz
1
0
0




one-hot-encoded variables




person_id
favorite_color_blue
favorite_color_green




xyz
0
0




From a statistics point of view, I would use the one-hot encoded columns to build my model. In addition, I can infer the favorite color is pink, because I encoded the variables.
However, when I'm applying XAI to explain the prediction to someone else and they see the favorite color wasn't blue or green. I'm not so sure they will infer the favorite color was pink unless it's explicitly stated. So using dummy variables might serve explainability better, but brings other risks..
Are there any best practices on this?
","['reference-request', 'explainable-ai']","Pasting an answer here from a colleague, credits go to Amelie Groud""we encountered the same issue during our PoC, not only with encoding but also with normalization (seeing ""age=0.18"" wasn't super meaningful).If you are using model-agnostic techniques (such as Lime, PDP or ANCHOR), you should be able to apply XAI on your original data, before applying the encoding.
With these techniques, usually we need 2 main elements: 1. an input dataset and 2. a ""predict()"" function (calling your trained model).
From there you have 2 possibilities:If you choose the 2nd option, then you are free to choose any encoding strategy you see fit. Plus, most XAI techniques have some embedded capacity to deal with categorical data so instead of seeing ""pink=1, blue=0, green=0"", you will see ""color=pink"".Here is an example with LIME but it works similarly for other XAI techniques: https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html
with the predict function defined as predict_fn = lambda x: rf.predict_proba(encoder.transform(x))""Please feel free to add more answers/views if you have a different way of dealing with the question at handRegards,
Koen"
Is feature engineer an important step for a deep learning approach?,"
I'd like to ask you if feature engineering is an important step for a deep learning approach.
By feature engineering I mean some advanced preprocessing steps, such as looking at histogram distributions and try to make it look like a normal distribution or, in the case of time series, make it stationary first (not filling missing values or normalizing the data).
I feel like with enough regularization, the deep learning models don't need feature engineering compared to some machine learning models (SVMs, random forests, etc.), but I'm not sure.
","['machine-learning', 'deep-learning', 'data-preprocessing', 'feature-engineering']","No, feature engineering is not an important step for deep learning (EDIT: compared to other techniques) provided that you have enough data. If your dataset is big enough (which varies from task to task), you can perform what is called an end-to-end learning.To further clarify, according to this article, deep neural nets trained with backpropagation algorithm are basically doing an automated feature engineering.I feel like with enough regularization, the deep learning models don't need feature engineering compared to some machine learning models (SVMs, random forests, etc.)That is basically correct. Beware, you need a large dataset. When a large dataset is not available, you will do some manual work (feature engineering).Nevertheless, it is always a good idea to look at your data first!EDITI would also like to quote Rich Sutton here:We want AI agents that can discover like we can, not which contain
what we have discovered. Building in our discoveries only makes it
harder to see how the discovering process can be done.Perhaps this statement is more true with Deep Learning than with previous techniques, but we are not quite there yet. And as user nbro rightfully pointed out in the comments below, you may still need to normalise your data, pre-process it, remove outliers, etc. Thus in practice, you may still need to transform your data to a certain degree, depending on many factors."
What is the difference between a language model and a word embedding?,"
I am self-studying applications of deep learning on the NLP and machine translation.
I am confused about the concepts of ""Language Model"", ""Word Embedding"", ""BLEU Score"".
It appears to me that a language model is a way to predict the next word given its previous word. Word2vec is the similarity between two tokens. BLEU score is a way to measure the effectiveness of the language model.
Is my understanding correct? If not, can someone please point me to the right articles, paper, or any other online resources?
","['natural-language-processing', 'comparison', 'word-embedding', 'language-model', 'bleu']","Simplified: Word Embeddings does not consider context, Language Models does.For e.g Word2Vec, GloVe, or fastText, there exists one fixed vector per word.Think of the following two sentences:The fish ate the cat.andThe cat ate the fish.If you averaged their word embeddings, they would have the same vector, but, in reality, their meaning (semantic) is very different.Then the concept of contextualized word embeddings arose with language models that do consider the context, and give different embeddings depending on the context.Both word embeddings (e.g Word2Vec) and language models (e.g BERT) are ways of representing text, where language models capture more information and are considered state-of-the-art for representing natural language in a vectorized format.BLEU score is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Which is not directly related to the difference between traditional word embeddings and contextualized word embeddings (aka language models)."
What model structure I should use to train on low res and blurry images?,"
I am looking for advice or suggestion.
I have photos like these: photo_1 and photo_2 and many more similar to that. The average shape of these photos is about 160 x 100. What we are doing is we are trying to find wheather or not person in a photo is wearing safety vest and helmet (if person is wearing both it is 1, if something is missing or both are missing it is 0). Training data consists of about 5k almost equally distributed image sets. I have tried to use augmentation techniques (flipping, adding noise, brighness correction) but results didn't improove.
I tried to train on many pretrained popular models: resnet101, mobilenet_v2, efficientneyb3, efficientneyb0, DenseNet121, InceptionResNetV2, InceptionV3, ResNet152V2, ResNet50V2, but results are not eyepleasing. I have tried different input sizes ranging from 224x224 to 112x112 but result didn't improve as much as I would have liked it to be. And the weird thing is that the image shape does not correlate to wheather or not there are more wrong predictions using bigger or smaller images.
As a side not I would lik to ask couple questions:

Should I use my own written small net?
Are the models that I use too big for this problem?

Any advice will be appreciated.
","['deep-learning', 'computer-vision', 'classification']",
"Should one use an ""other"" category in image classification?","
In image classification, there are sometimes images that do not fit in any category.
For example, if I build a CNN in Keras to classify Dogs and Cats, does it help (in terms of training time and performance) to create an ""other"" (or ""unclassified"") category in which images of houses, people, birds, etc., are classified? Is there any research paper that discusses this?
A similar question was asked before here, but, unfortunately, it has no answer.
","['convolutional-neural-networks', 'computer-vision', 'keras', 'image-recognition', 'reference-request']","It is not advisable because if you use an ""other"" class, you are just increasing problems for your network. Since ""other"" means not dog and not cat, then, what common feature does it have? Most of the time the ""other"" images won't have many features in common. If they do, then go ahead and make an ""other"" class.There is a better way: if the probabilities for both cat and dog are less than a threshold (you need to decide that, take, 0.5), then, you can say it is an ""other"" object."
Why do popular object detecting models output heatmaps instead of coordinators of object directly?,"
I think heatmap outputs of architectures like CenterNet, OpenPose, etc. can be changed to coordinator outputs, and loss functions like focal loss can be modified so they can deal with coordinators instead of heatmaps. Is there any particular reasons that researchers use heatmaps instead of coordinators?
","['image-recognition', 'object-detection']",
Can a large discrete action space be represented using Gaussian distributions?,"
I have a large 1D action space, e.g. dim(A)=2000-10000. Can I use continuous action space where I could learn the mean and std of the Gaussian distributions that I would use to sample action from and round the value to the nearest integer? If yes, can I extend this idea to multi-dimensional large action space?
","['reinforcement-learning', 'deep-rl', 'continuous-action-spaces', 'discrete-action-spaces']","The answer is ""it depends"". Once you have arranged the actions into order, a key trait is whether the action value function has a simple enough shape that sampling from a Gaussian policy function would give consistent expected returns, enough that learning can occur. If the underlying ""true"" value function has a lot of high frequency noise then learning would be slow. In the worst case, if the action value $Q(s,a_{n})$ and $Q(s,a_{n+1})$ is not correlated for any $n$, then it will not be possible to learn with the approximation at all.You may have some sense of how similar actions $a_{n}$ and $a_{n+1}$ are. If the actions represent different ordinal choices, such as selecting an integer number of items to perform some task with such as buy/sell or transport, then in many environments there will often be a strong correlation between outcome of choosing e.g. $a_{900}$ and $a_{901}$. If this holds in general, then that is a good indicator that you can treat $n$ as being continuous, and use learned parameters of a simple distribution function to find optimal policies (and in addition this could be far more efficient than using a discrete representation).It might not matter if for a small fraction of cases the difference in outcomes between $a_{n}$ and $a_{n+1}$ is large, provided that successive approximations to the optimal policy can improve towards optimal through adjusting mean $\mu(s)$ and standard deviation $\sigma(s)$.There may still be difficult cases that cannot be learned by the approximation - for instance if a specific action $a_n$ is optimal for a given state $s$, but $a_{n-1}$ and $a_{n+1}$ are a lot worse, then the training process of a typical policy gradient approach may never settle upon $\mu(s) = n$ and $\sigma(s) \approx 0$, because any intermediate values between the starting policy and the optimal one will perform badly.For expanding into more dimensions, the same ideas apply to each dimension separately. You may want to use different distributions, or even have one dimension that uses a continuous model with a few parameters whilst another remains discrete with a free parameter for each choice."
Is there any known technique to determine a graph from a 1D signal pattern?,"
I'd like to evaluate the possibility of using a Machine/Deep Learning technique as a sort of pattern recognition and parameters estimation.
The problem I want to address can be stated as follows: Let's consider that I have a set of interacting ""particles"" that can be represented as a graph in which the vertices represent the particles and the edges the magnitude of the interaction amongst them. For instance, in the diagram below I'm showing a particle graph formed by 4 interacting particles.

So each particle/vertex has a value (e.g. $A=3.1$, $B = 4.2$, etc.) and each edge contains the magnitude of the interaction between two connected nodes/àrticles (e.g. $AB = 5.3$, $AC = 1.1$, $DB = 0$, etc).
With all this information, there exists a quantum mechanics algorithm that, after some complex calculations, results in a 1D signal (the pattern; essentially a vector of X-Y values). The overall process is illustrated in the figure below:

The appearance of the obtained signal will therefore depend upon the values of the graph. The goal is, in this case, the inverse problem: given one of these 1D signals (that is, a characteristic pattern), is it possible to determine the graph with its corresponding values?
I could create a training set formed by a very large number of simulated graphs with corresponding 1D patterns.
Since my experience with ML has so far focused only on simple classification problems, it is not clear to me which ML method would be more convenient or whether or not this problem can actually be addressed by an ML technique. Any general recommendation or advice would be highly appreciated.
","['machine-learning', 'pattern-recognition', 'geometric-deep-learning', 'algorithm-request']",
When is it necessary to manually extract features to feed into the neural network rather than providing raw data?,"
Usually, Neural Networks uses raw data. You do not need to extract features manually. NN's can find & extract good features which is a pattern of an image, signal or any kind of data. When we check layer outputs in a NN, we can see and visualize how NNs extract features.
Do neural networks extract features by themselves every time? When is it necessary to manually extract or engineer features to feed into the neural network rather than providing raw data?
For example, I had a time series sensor data. When I use LSTM & GRU on a raw dataset, I had bad test accuracy but when I extract some features manually I had really good test set accuracy results. I extract Fast Fourier Transform, Cross-correlation features which helped a lot to increase accuracy. ""Extraction of features manually"" helped to solve my problem.
","['neural-networks', 'deep-learning', 'feature-extraction', 'features', 'feature-engineering']","Yes, neural networks learn features themselves freeing you from the need to manually engineer them. I will illustrate it here with a toy problem.Let's assume that we want to learn the areas of parallelograms built on pairs of vectors:The input data are six coordinates: $(x_1, y_1, x_2, y_2, x_3, y_3)$.The targets (areas) are $y = |ad-bc|$, where $a=x_3-x_1$, $b=y_3-y_1$, $c=x_2-x_1$, $d=y_2-y_1$.To learn the areas from coordinates, I will use my favorite machine learning library super_magic_learnIt will initialize a network with random activation functions in neurons, and random connections between them having random weights. It also randomly assigns some neurons as inputs, while other outputs or internal ones.Then I train my networkDuring training, the activation functions inside neurons change, the connections between neurons form, disappear, and form again, and their weights are adjusted. Some neurons organize in layers, the number of neurons in each layer changes, and finally the trained network is as follows:It solves the task with 100% accuracy for both the training and test data, and it solves it using only raw data: coordinates. No need to engineer features.However, you probably don't have access to the library super_magic_learn. Let's see what can we do with a slightly more inferior tensorflowNow calculate the performance on the test set$R^2$:Not good. What will happen, if I engineer some features?Let's train the same model but instead of feeding it with raw data, the inputs will be the following manually engineered features: $a=x_3-x_1$, $b=y_3-y_1$, $c=x_2-x_1$, $d=y_2-y_1$ (don't forget to change input_dim=4 in the first layer).$R^2$:Now it is much better. Less than 100% though.Why neural network in tensorflow performs poorly on raw data and needs feature engineering while the super_magic_learn works perfectly on raw data and does not need any feature engineering?The reason is that tensorflow or any other library that I know, is much more restricted than my beloved super_magic_learn. The restrictions are as follows (note a very small problem though: super_magic_learn does not exist but I wish it were):The textbooks are right: ideally, a neural network should learn just from the raw data. But this is true only about my idealized library and not so much about existing real-world implementations.To make a network really learn features for any task, it should be freed from these restrictions.If you put so many restrictions on the architecture, activation functions, and other parameters, so that they cannot be learned from the data during training, then you have to engineer them yourself and adjust them manually for your task. If you engineer them correctly then your network will learn happily from the raw data. But it might perform poorly on other tasks.Such is the case with convolutional neural networks. They were designed taking into account transnational equivariance of features in images that's why they can learn features from raw image data. However, they don't necessarily perform well in other domains."
"Why does Q-value become negative during training of DQN, while the agent learns to play?","
I have implemented a simple version of the DQN algorithm for CartPole-v0. The algorithm works fine, in the sense that achieves the highest possible scores. The below diagram shows the cumulative reward versus training episode.

The scary part is when I tried to plot the q values during training. For this purpose, 1000 random states were generated and stored. After each training episode, I fed these states to the Q-network and computed the Q-value for all actions. Then for each state, the max Q-value was computed. Finally, these max Q-values were averaged to yield a single number. The following plot shows this quantity during training.

As you can see, there is a slight increase in average-max-q in the beginning and it is followed by a sharp decrease. My question is, how can this value be negative? Since I know all rewards received by the agent are positive, and expected cumulative reward of a state can not be negative.
Edit
I added the code for clarity:
from matplotlib import pyplot as plt

import torch as th
import torch.nn as nn
import torch.nn.functional as F

import gym
import random
import numpy as np
from collections import deque

from test_avg_q import TestEnv


class ReplayBuffer():
    def __init__(self, maxlen):
        self.buffer = deque(maxlen=maxlen)

    def add(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size):
        sample_size = min(len(self.buffer), batch_size)
        samples = random.choices(self.buffer, k=sample_size)
        return map(list, zip(*samples))


class QNetwork():
    def __init__(self, state_dim, action_size):
        self.action_size = action_size
        self.q_net = nn.Sequential(nn.Linear(state_dim, 100),
                                   nn.ReLU(),
                                   nn.Linear(100, action_size))
        self.optimizer = th.optim.Adam(self.q_net.parameters(), lr=0.001)

    def update_model(self, state, action, q_target):
        action = th.Tensor(action).to(th.int64)
        action_one_hot = F.one_hot(action, num_classes=self.action_size)
        q = self.q_net(th.Tensor(state))
        q_a = th.sum(q * action_one_hot, dim=1)
        loss = nn.MSELoss()(q_a, q_target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


class DQNAgent():
    def __init__(self, env):
        self.state_dim = env.observation_space.shape[0]
        self.action_size = env.action_space.n
        self.q_network = QNetwork(self.state_dim, self.action_size)
        self.replay_buffer = ReplayBuffer(maxlen=1_000_000)
        self.gamma = 0.97
        self.eps = 1.0

    def get_action(self, state):
        with th.no_grad():
            q_state = self.q_network.q_net(th.Tensor(state).unsqueeze(0))
            action_greedy = th.argmax(q_state).item()
        action_random = np.random.randint(self.action_size)
        action = action_random if random.random() < self.eps else action_greedy
        return action

    def train(self, state, action, next_state, reward, done):
        self.replay_buffer.add((state, action, next_state, reward, done))
        states, actions, next_states, rewards, dones = self.replay_buffer.sample(50)
        with th.no_grad():
            q_next_states = self.q_network.q_net(th.Tensor(next_states))
            q_next_states[dones] = th.zeros(self.action_size)
            q_targets = th.Tensor(rewards) + self.gamma * th.max(q_next_states, dim=1)[0]
        self.q_network.update_model(states, actions, q_targets)

        if done: self.eps = max(0.1, 0.99 * self.eps)


env_name = ""CartPole-v0""
env = gym.make(env_name)

agent = DQNAgent(env)
num_episodes = 400
testEnv = TestEnv()
avg_qs = []
rewards = []
render = False

for ep in range(num_episodes):
    state = env.reset()
    total_reward = 0
    done = False
    while not done:
        action = agent.get_action(state)
        next_state, reward, done, info = env.step(action)
        agent.train(state, action, next_state, reward, done)
        total_reward += reward
        state = next_state
        if render and ep > 80:
            env.render()

    avg_qs.append(testEnv.run(agent.q_network.q_net))
    rewards.append(total_reward)
    print(""Episode: {}, total_reward: {:.2f}"".format(ep, total_reward))

plt.plot(avg_qs)
plt.show()
plt.figure()
plt.plot(rewards)
plt.show()


here is the code for class TestEnv:
import torch as th
from torch.utils.data import Dataset, DataLoader


class TestEnv:
    def __init__(self):
        self.dataloader = DataLoader(TestDataset(), batch_size=100, num_workers=10)

    def run(self, model):
        out_list = []
        model.eval()
        with th.no_grad():
            for batch in self.dataloader:
                out = model(batch)
                out_list.append(out)
            qs = th.cat(out_list, dim=0)
            maxq = th.max(qs, dim=1)[0]
            result = th.mean(maxq)
        return result.item()



class TestDataset(Dataset):
    def __init__(self):
        self.db = th.load('test_states.pt')

    def __len__(self):
        return len(self.db)

    def __getitem__(self, idx):
        return self.db[idx]


","['deep-rl', 'dqn', 'gym']",
Is there a full and precise formulation of Theorem 1 in the Integrated Gradients paper?,"
Theorem 1 (page 5)  in the paper about Integrated Gradients states that

Integrated gradients is the unique path method that is symmetry-preserving.

What I miss is

A precise formulation of the theorem: in particular, the exact properties that must be satisfied by the function $f$ used in the proof (continuity, differentiability, etc.). Also, should the paths be assumed to be monotonic?

A consistent definition of function $f$ in the proof - note that $f$ is defined inconsistently, e.g. in the region where $x_i<a$ and $x_j>b$, where it is not clear whether its value should be $0$ or $(b-a)^2$.


Point 2 is easy to fix with an appropriate redefinition (e.g. replacing ""if $\text{max}(x_i,x_j)\geq 0$"" with ""else if $\text{max}(x_i,x_j)\geq 0$""). What it is not clear if whether there is a redefinition that:

preserves the properties that have been assumed in the rest of the paper, in particular in Proposition 1 (proving completeness), where the function is assumed to be continuous everywhere, and the set of discontinuous points of each of its partial derivatives along each input dimension has measure zero, and

the function is a constant for $t \notin [t_1,t_2]$.


Does anybody have a precise formulation and full proof of the theorem?
","['deep-learning', 'image-recognition', 'papers', 'math']",
How to find good features for a linear function approximation in RL with large discrete state set?,"
I've recently read much about feature engineering in continuous (uncountable) feature spaces. Now I am interested what methods exist in the setting of large discrete state spaces. For example consider a board game with grid as a basic layout. Each position on a grid can contain exactly one of multiple elements and the agent makes decisions according to the current board position. If the grid is large enough, say 30x30, and there are only two different elements we could model the states as a linear model with $2*30*30 = 1800$ variables (using dummy variables) and this model can't even distinguish relationships between positions. For this we would need to use $\binom{90}{2}$ or even $\binom{90}{k}$, $k = 2, 3, 4$ more features.
How would one approach this problem? Are the methods for feature selection for linear approximations, which even automatically find/learn non-linear combinations? What was the approach to solving these problems when NN where not around?
","['reinforcement-learning', 'function-approximation', 'feature-engineering', 'heuristic-functions', 'discrete-state-spaces']",
Did the Hutter Prize help research in artificial intelligence in any way?,"
Wikipedia states:


The Hutter Prize is a cash prize funded by Marcus Hutter which rewards data compression improvements on a specific 1 GB English text file.
The goal of the Hutter Prize is to encourage research in artificial intelligence (AI). The organizers believe that text compression and AI are equivalent problems.


Did the Hutter Prize help research in artificial intelligence in any way?
","['research', 'history', 'data-compression']",
What is the difference between feature extraction with or without data augmentation?,"
Here's an extract from Chollet's book ""Deep Learning with Python"" about using pre-trained CNN to predict class from a photo set (p. 146):

At this point, there are two ways you could proceed:

Running the convolutional base over your dataset, recording its output to a Numpy array on disk, and then using this data as input to
a standalone, densely connected classifier similar to those you saw in
part 1 of this book. This solution is fast and cheap to run, because
it only requires running the convolutional base once for every input
image, and the convolutional base is by far the most expensive part of
the pipeline. But for the same reason, this technique won’t allow you
to use data augmentation.

Extending the model you have (conv_base) by adding Dense layers on top, and running the whole thing end to end on the input data. This
will allow you to use data augmentation, because every input image
goes through the convolutional base every time it’s seen by the model.
But for the same reason, this technique is far more expensive than the
first.



The first method is called (1) and the second is (2).
If I use data augmentation to expand my data set, then could (1) be as good ad (2)? If no, why?
","['deep-learning', 'feature-extraction', 'data-augmentation']","There are two ways that you could perform data augmentation:Up front, by expanding the input dataset into a larger one, performing a range of changes to each input then storing the result. This appears to be what you are suggesting.Just in time, by sampling from possible augmentations on each epoch, or even per sample when building a mini-batch. This appears to be what Chollet is suggesting.Chollet's approach allows for augmentation to include finer degrees of augmentation that are different each time an input is considered, e.g. rotations of any angle, selecting a slightly different area from a larger image each time. For your approach you could consider the same set of augmentations but they would have to be ""frozen in"" at the time of building a dataset, and you would not be able to consider all possible variations for each image because it would make the dataset too large.Both approaches are valid, and both would be called data augmentation. Chollet's approach obtains better re-use of image samples in the long term, and may result in better generalisation in the final trained network. Your approach may allow for more efficient use of CPU time to obtain a result that passes a threshold in accuracy. In some cases the difference between approaches may be minor compared to that caused by other changes in hyperparameters."
Which approach should I use to classify points above and below a sine function $y(x) = A + B \sin(Cx)$?,"
In a linear regression problem, a line can divide a data set into two categories. So, basically, points above the line belong to category 1, and points below the line belong to category -1.
However, my professor has asked me to write a C++ program in which the program will classify whether the data points lie above or below a sine function.
Let me explain a bit more. So, first, we will generate a data set $$D = \{(x_i, y_i) \} \label{0}\tag{0} $$ with random $x$ and $y$ coordinates, for example, according to this equation
$$y(x) = A + B \sin(Cx)\label{1}\tag{1},$$
where $A$, $B$, and $C$ are known.
The data points above the sine function will have a label 1 on them, and the points below the function will have -1.
Now, this data set $D$ in \ref{0} has to be fed to a C++ program. This C++ program has to somehow learn the curve separating the two data point categories. After training, the program will then classify some new query data points.
The key difficulty is that the program does not know in advance that the points were scattered around a sine curve. It does not know the values of $A$, $B$, or $C$ in equation \ref{1}. It also does not know that the curve is a sine curve.
Now, this is where I am stuck. I do not know if I need to use a neural network to solve this problem. If a neural network is to be used, then I presume that backpropagation will have to be used in some way. I can generate the data set and I can feed the data into the program.
Which approach (algorithm and model) should I use to solve this problem?
I have studied linear classification with the perceptron learning algorithm, but this sine-classifier stuff is a huge step-up for me. Another important thing is that I am not allowed to use any ready-made C++ libraries for Machine Learning. If a neural network solution is needed, then I will have to design the neural network from scratch. Note that I don't need any C++ code, but I am just looking for some guidance on how to approach this problem.
","['neural-networks', 'machine-learning', 'binary-classification', 'c++', 'model-request']",
How can I select features for a symbolic regression problem to be solved with genetic programming?,"
I want to solve a symbolic regression problem with genetic programming. My dataset is similar to this one, but I have 30 features, and I want to use only the most sensitive features. I found this library interesting for Symbolic Regression, but could not find the right approach for feature selection.
","['genetic-algorithms', 'feature-selection', 'genetic-programming', 'algorithm-request']",
"Where does the hierarchical reinforcement learning framework name ""MAXQ"" come from?","
I've been researching different frameworks for hierarchical RL (mainly options, HAMs, and MAXQ) and noticed that both options and HAMs have names that relate to how they function. I can't seem to find anything stating how MAXQ got its name and I was wondering if anyone knew what the name was referencing.
","['reinforcement-learning', 'terminology', 'hierarchical-rl', 'maxq']",
"Is there a way, while training (with contrastive learning) the embedding network, to find the test accuracy?","
I aim to do action recognition in videos on a private dataset.
To compare with the existing state-of-the-art implementations, other guys published their code on Github, like the one here (for the paper Self-supervised Video Representation Learning Using Inter-intra Contrastive Framework). Here, the author first trains the embedding network (3D ResNet without final classification layer) with contrastive learning. Finally, he adds a final layer and finetunes the weights, training the whole network again for some epochs.
Now, here is my doubt: Is there a way, while training just the embedding network, to find the test accuracy?
One way to tell if the final accuracy after finetuning would be good is to see if the training loss is decreasing or not. If the training loss decreases, that certainly builds up the hope that the test accuracy would be improving during the training, but in no way gives an idea about how much the test accuracy would be.
Another way is to plot the t-SNE, see if, on the test data, the data points from the same class are close together, thus forming a cluster. Then it could be said that the test accuracy would also be good. But it's not quantifiable, and hence it would be hard to compare the t-SNE plots obtained from two different models.
I was also suggested to add a final layer to my embedding network and just test it on the test data, without training or fine-tuning again. The reason for that is that the embedding network should have learned the weights reasonably by now; even if I finetune the model, the test dataset's test accuracy won't vary a lot. I need some advice here. Is that suggestion good? Are there any potential pitfalls with this suggestion?
Or do you have any other possible suggestions I could try?
","['deep-learning', 'accuracy', 'action-recognition', 'testing', 'test-datasets']",
How to train a neural network with few weights and biases held constant?,"
I am a beginner in neural networks. I am building a neural network with 3 layers. The input $X$ has 7 features and the output $Y$ is a real number. In the hidden layer, there are two nodes. The bottom node contains weights and biases which should be hard set.

Now, I want to train this neural network with the training data $X$ and $Y$, such that the red weights are held constant while all other weights are learnable.
Is there a way of doing this during the training of the neural network? I'm using TensorFlow and Keras, so, if you could provide also the code necessary to do this, that would be very useful.
","['neural-networks', 'tensorflow', 'keras', 'weights']",
"Why would my neural network have either an accuracy of 90% or 10% on the validation data, given a random initialization?","
I'm making a custom neural network framework (in C++, if that is of any help). When I train the model on MNIST, depending on how happy the network is feeling, it'll give me either 90%+ accuracy, or get stuck at 10-9% (on validation set).
I shuffle all my data before feeding it to the neural net.
Is there a better randomizer I should be using, or maybe I am not initializing my weights properly (Using srand to generate values between +/-0.1). Did I somehow hit a saddle point?
My network consists of 784 size input layer, 256, 64, 32, 16 neuron hidden layers, all with RELU, and 10 output with SMAX
Where should I start investigating based on this kind of behavior, when I can't even replicate what is going on?
","['neural-networks', 'accuracy', 'weights-initialization', 'mnist', 'validation']",
"Given the word embeddings, how do I create the sentence composed of the corresponding words?","
I have done some reading. I want to implement an LSTM with pre-trained word embeddings (I also have plans to create my word embeddings, but let's cross that bridge when we come to it).
In any given sentence, you don't usually need to have all the words as most of them do not contribute to the sentiment, such as the stop words and noise. So, let's say there is a sentence. I remove the stop words and anything else that I deem unnecessary for the project. Then I run the remaining words through the word embedding algorithm to get the word vectors.
Then what? How does it represent the sequence or the sentence 'cause it's just vector for a word.
For example, take the sentence:

The burger does not taste good.

I could remove certain words and still retain the same sentiment like so:

Burger not good.

Let's assume some arbitrary vectors for those three words:

Burger: $[0.45, -0.78, .., 1.2]$

not: $[9.6, 4.0, .., 5.6]$

good: $[3.5, 0.51, 0.8]$


So, those vectors represent the individual words. How do I make a sentence out of them? Just concatenate them?
","['natural-language-processing', 'long-short-term-memory', 'word-embedding', 'sentiment-analysis', 'word2vec']",
Intuition behind $1-\gamma$ and $\frac{1}{1-\gamma}$ for calculating discounted future state distribution and discounted reward,"
In the appendix of the Constrained Policy Optimization (CPO) paper (Arxiv), the authors denote the discounted future state distribution $d^\pi$ as:
$$d^\pi(s) = (1-\gamma) \sum_{t=0}^\infty{\gamma^t P(s_t = s \vert \pi)}\tag1$$
and the discounted total reward $J(\pi)$ as:
$$J(\pi) = \frac{1}{1-\gamma} E_{\substack{s\sim d^\pi \\ a \sim \pi \\ s' \sim P}}[R(s,a,s')]\tag2$$
I have two questions regarding these equations.
Question 1
Intuitively, I understand that $d^\pi(s)$ returns the discounted probability of landing on state $s$ when executing policy $\pi$.
I understand that the summation part of $(1)$ results in values that are greater than $1$, and are, therefore, not fit for a probability distribution. But I do not understand why the value that results from this is multiplied by $(1-\gamma)$.
I have read in this question that ""$(1−\gamma)$ normalizes all weights introduced by γ so that they are summed to $1$"". I have confirmed that this is true, but I don't understand why.
I tested this with a simple example:
Suppose there is are only two states $s_A$ and $s_B$ and the probabilty of landing on $s_A$ is $0.4$ and on $s_B$ is $0.6$, independently of the previous state or action taken (therefore, independently of the policy $\pi$). Also suppose we set the maximum number of time steps $t_{max} = 1000$ (to make the equation easy to compute) and $\gamma = 0.9$.
Then:
$$d^\pi(s_A) = (1-0.9) \sum_{t=0}^{1000} 0.9^t \cdot 0.4 \approx (1-0.9) \cdot 4$$
and
$$d^\pi(s_B) \approx (1-0.9) \cdot 6$$
So indeed if we sum them and multiply by $(1-\gamma)$ we get:
$$(1-0.9)\cdot(4+6) = 1$$
Q: My question is why does multiplying by $(1-\gamma)$ normalize to $1$? And what does $(1-\gamma)$ represent in this context?
Question 2
Similarly, I can't understand the use of $\frac{1}{1-\gamma}$ in $(2)$.
Q: How does multiplying the expected value of the reward function by $\frac{1}{1-\gamma}$ result in the discounted reward, instead of multiplying by $\gamma$? What does $\frac{1}{1-\gamma}$ represent?
","['reinforcement-learning', 'reward-functions', 'constrained-optimization']","Question 1The taylor expansion of $\frac{1}{1-\gamma}$ at $\gamma= 0$ is as follows$$\frac{1}{1-\gamma} = 1 + \gamma + \gamma^2 + \dots$$When you multiply by $1-\gamma$ you get$$ 1 = (1-\gamma)(1 + \gamma + \gamma^2 + \dots)$$Which can be equivalently written as$$1 = (1-\gamma)\sum_\limits{i=0}^{\infty}\gamma^i$$Hence we can see that by multiplying the coefficients by $(1-\gamma)$ we get a weighted sum of transition probabilitiesQuestion 2Multiplying by $(1-\lambda)$ is to cancel out the normalisation term which was included in the definition of the discounted distribution of the states. Partially expanding out your expressions for the discounted reward you get this$$ J(\pi) = \frac{1}{1-\gamma}\sum_\limits{s}E_
{a\sim\pi,s'\sim P}[R(s,a,s')]\cdot\left((1-\gamma)\sum_\limits{t=0}^{\infty}\gamma^tP(s_t=s|\pi)\right)$$The discounted return form is usually denoted$$J(\pi) = E[\sum\gamma^iR_i(s,a,s')]$$Where you notice the discounting terms aren't normalised which motivates cancelling it out"
Can the law of iterated expectation be used on the inner expectation of the DQN cost function described in the DQN paper,"
Is the expression for the DQN cost function, Equation (2) of the DQN paper
$$\begin{align}L_1 &= E_{\mu,\pi}\left[\left(y_i - q(s,a;\theta)\right)^2\right]\\
&=E_{\mu,\pi}\left[\left(E_{\mathcal{E}}[r + \gamma \max\limits_{a'}q(s',a';\theta^-)] - q(s,a;\theta)\right)^2\right] \end{align}$$
equivalent to this? (Substituting the expression for $y_i$ defined in the paragraph directly after, $\mathcal{E}$ represents the transition distribution governed by the environment, $\pi$ represents the behaviour policy and $\mu$ represents the stationary distribution of states)
$$L_2 = E_{\mu,\pi,\mathcal{E}}\left[\left(r + \gamma \max\limits_{a'}q(s',a';\theta^-) - q(s,a;\theta)\right)^2\right]$$
Can the law of iterated expectation be used to derive the second expression from the first, if not, is there another way to go about showing their equivalence IF they are equivalent.
It seems as though $L_2$ is used for sampling but I'm not sure how it's possible to get here from the original cost function $L_1$. If it is possible to use $L_2$ to sample I assume that means the two expressions must be equivalent. The second expression is used for sampling in the DQN paper here.
I do realise that the gradient for each function is the same and thus so is the $n^{th}$ derivative for some $n\geq1$ and since the curvature and optimas align I guess that also means they are the same function (minus some constant difference)?
$$\nabla_{\theta} L = E_{\mu,\pi,\mathcal{E}}\left[\left(r + \gamma \max\limits_{a'}q(s',a';\theta^-) - q(s,a;\theta)\right)\nabla q(s,a;\theta)\right]$$

Related Question
A related problem that concerns equivalence of sampling from $L_1$ and $L_2$. Is it possible to sample from a nested expectation that is squared as follows?
$$E[E[X|Y]^2] \approx \frac{1}{n}\sum X^2$$
Where $X$ is generated according to the marginalised distribution $P(X)$. I don't think it is true  since $E[X]^2 \neq E[X^2]$ which should mean sampling from $L_1$ and $L_2$ are not equivalent.
","['deep-learning', 'dqn', 'proofs']",
CAPTCHA based on text comprehension and random tokens,"
I developed a novel type of CAPTCHA based on text comprehension and random tokens. Given a task Pick the first pair of adjacent letters and a random token 8NBA596V, the user has to provide the solution NB. It offers basic protection and an attacker can solve individual tasks with specific effort. I am curious, whether contemporary AI can solve it generically?
You can access more example tasks here:
https://www.topincs.com/manual/captcha
There is a task database and at every attempt a new task is presented with a new random token. They always have a solution of varying length and pure guessing thus has limited chances of success. It is easy to attack an individual task by writing a small piece of code, thus a large task database is essential. What intrigues me is the question whether natural language processing or machine learning at its current state can attack the CAPTCHA generically by building a model of the meaning of the task
– essentially a predicate in a tiny universe of discourse – and then applying it to the random token.
","['natural-language-processing', 'captcha']",
Which meta-learning approach selection methodology should I use for similarity learning of an image?,"
Meta-learning has 3 broad approaches: model, metric and optimization-based approach. Each of them has its own sub-approach, like matching network, meta-agonistic and Siamese-based network, and so on.
How do I decide which approach to select for a task? For my case, I have a noisy image, and they need to be compared with 10 different new images every time. Do I have to start with the trial and error method, or there is some methodology behind this approach selection?
","['convolutional-neural-networks', 'meta-learning', 'model-agnostic-meta-learning', 'template-matching']",
Face recognition from single image provided,"
I am working on a computer vision project, based on face detection to record the time spent by a person in an office.
It consists of detecting the face by camera number 1 (input), temporarily storing the detected face, calculating the time spent until this same person leaves and his face is detected by camera number 2. (We don't have a customer database).
Is there a better approach to follow? I would also appreciate articles to read on the topic.
","['neural-networks', 'computer-vision', 'face-recognition']","Matching 2 image of the same person can be done by help of ""Siamese Neural network"". Here they compare feature of 2 images and if 2 features distance are very close then it's a match. Good thing about this is you do not need person face to match in database. You can use pre-trained network like deepface and use it to compare. However, I guess you will have more trouble connecting real time camera input. As you have to store camera 1 input so that it can be used later for comparison with camera 2 images."
Why are the landmark retrieval and facial recognition literature so divergent?,"
Context and detail
I've been working on a particular image retrieval problem and I've found two popular threads in the literature:
Image retrieval (usually benchmarked with landmark retrieval datasets)

[x] Neural codes for Image Retrieval
[x] Particular object retrieval with integral max-pooling of CNN activations
[ ] Deep Image Retrieval: Learning global representations for image search
[ ] End-to-end Learning of Deep Visual Representations for Image Retrieval
[ ] Large-Scale Image Retrieval with Attentive Deep Local Features
[ ] Fine-tuning CNN Image Retrieval with No Human Annotation

Face recognition/verification:

[x] Facenet
[x] Deep Face Recognition
[x] SphereFace
[ ] Arcface

I'm still making my way through these lists and more (I've checked the ones I've looked at already) but I'm starting to get a sense that there's not much overlap in the techniques used, or the collective trains of thought in the research community. Here are the main points of divergence where I think both communities should be borrowing from each other.

Facial recognition seems to focus on getting embeddings to be as discriminative as possible by playing around with loss functions and training methods, whereas image retrieval seems to care more about ways of extracting feature descriptors from CNN pretrained backbones (types of pooling operations, which feature maps to look at, etc..).
Image retrieval has a considerable amount of work on what needs to happen after an embedding is obtained. Eg: dimensionality reduction, whitening + l2 norm, databise-side augmentation, query expansion, reranking etc
Facial recognition cares about keeping a minimum margin between non-matching faces in order to avoid mismatches, but I would think that should be imposed in image retrieval tasks as well (this is kind of a sub-point to my first point)

So to sum up: Why is it that facial recognition focuses on generating discriminative embeddings, while landmark retrieval focusses on generating rich ""descriptors""? Why does landmark retrieval use this cool bag of tricks for database search while facial recognition just mentions kNN? Shouldn't all these considerations boost performance in either domain?
","['convolutional-neural-networks', 'facial-recognition', 'content-based-image-retrieval']","Landmark retrieval has photographs of landmarks that you need to find out. Consider the degrees of freedom for this, the landmarks can many different colours (more than humans' faces) and also the colour range is all over the place (a landmark may be blue or white or red). The shapes of the various landmarks will also vary.Now, consider face recognition problem. All humans faces look alike morphologically. If you look at the colour, it is not as varied as landmark recognition.Because of the inherent data in both the problems, the research focuses on the diverging lines of thought. Rich descriptors are good for landmarks because the data itself is very rich and mired with variation. On the other hand, discriminative features are more desirable for face recognition because faces are more similar and less rich in variation, so differentiating between is hard.It is the requirement of the problem that steers research in diverging directions."
"How to define machine learning to cover clustering, classification, and regression?","
How to define machine learning to cover clustering, classification, and regression? What unites these problems?
","['machine-learning', 'classification', 'definitions', 'regression', 'clustering']",
How to deal with a variable number of channels of the inputs?,"
I have a problem in which my input data may have a varying number of channels. Let me explain with an example.

Imagine we have a classification problem in which we wish to identify
if certain species are present in wildlife photographs. This can be
done via a neural network including maybe some convolutions. For the
first layer of the network we could set up a convolutional layer with
3 input channels (one for R, G and B respectively) and this would
probably work well enough.
Now imagine that someone comes along with some new data for us and
this time they have not only taken regular RGB images but they have
used an IR-camera as well. Great, but how do we treat this data, we
have one more channel?! One could of course simply add an extra channel
and re-train the network but that would mean that our old data (without
IR-info) is useless and what if someone comes along with a
UV-camera.....

My situation is similar but I will most definitely be dealing with varying numbers of channels and the range can be quite wide (from 5 channels all the way up to maybe 50). Is there a good way of dealing with a situation like this?
","['convolutional-neural-networks', 'data-preprocessing']",
"Why different images of the same person, under some restrictions, are in a 50 dimension manifold?","
In this lecture (starting from 1:31:00) the professor says that the set of all images of a person lives in a low dimensional surface (compared the the set of all possible images). And he says that the dimension of that surface is 50 and that they get this number by adding the three translations of the body, the three rotations of the head and the independent movements of the face's muscles. He also adds that it may be more than 50 but less than 100. How do we get the number 50 ?
The professor previously said (in the same lecture, 1:29:00) that the set of all the images that we could describe as natural and that we could interpret are in a manifold. I try to understand how the number 50 came up like the following: let's take an image of a person, since it's ""natural"" then it belongs to that manifold. Hence there is an open set to which this image belongs to and there is a homeomorphic map from this open set to an euclidean space. Let's suppose (I don't know why but it's the only possible thing I could come up with to understand) that all the images of that same person, regardless of his position and expressions..., are in that open space then through the homeomorphic mapping we have the ""same points"" in an euclidean space, do we get the base of it by decomposing all the possible movements of the person?
I hope someone can clarify things for me, it seems this doesn't only work with images but all types of non-structured types of data.
","['datasets', 'representation-learning', 'disentangled-representation']","The number 50 is essentially just a guess based on results when compressing and/or generating data of a certain type. The variables such as ""the three translations of the body, the three rotations of the head and the independent movements of the face's muscles"" are examples only. There is no known formal map with well-defined parameters that defines a well understood manifold of ""clear images of this person"" in natural images. The lecturer has not constructed such a map as far as I can tell, but has done some related experiments.Experimentally, it is possible to establish parameter vectors that work, with models like Variational Autoencoders and Generative Adversarial Networks. Depending on the size of the target image, and amount of variation in subject matter that you want to allow for (pose, lighting, clothing, hair style, makeup, camera properties etc), you will end up with different sizes of embedding vectors that appear to capture the important variations. When dealing with multiple people, it is common to see vector sizes of 64, 128, 256.The lecture suggests compressing images with a clear background, consistent lighting, same person with only changes being in pose. Around 50 dimensions for this relatively simple image space seems reasonable, given facial recognition engines that work well in a more complex domain using 128 dimensional embeddings.I expect that the lecturer has seen experimental evidence that a vector of 50 dimensions performs well at representing all variations in these images, plus smaller vectors perform measurably worse and larger vectors do not perform better. This experiment is possible by constructing something like a VAE with a specific size of embedding vector, training it, then measuring loss when reconstructing a set of test images."
What is the size of 6-players no limit Texas holdem Poker?,"
What is the number of game states/information sets in 6-players, no limit, Texas Holdem?
A year ago, Pluribus reached a super-human level in 6-players no limit Holdem Poker.  I am interested in the size of poker because it is a simple heuristic method to compare the complexity of different games.

In the paper Measuring the Size of Large No-Limit Poker Games (2013), they write

The size of a game is a simple heuristic that can be used to describe
its complexity and compare it to other games, and a game’s size can be
measured in several ways. The most commonly used measurement is to
count the number of game states in a game.
...
In imperfect information games, an alternate measure is to count the number of decision points, which are more formally called information sets.

Here's the definition of game states and information sets.

Game states are the number of possible sequences of actions by the players or by chance, as viewed by a third party that observes all of the players' actions. In the poker setting, this would include all of the ways that the players private and public cards can be dealt and all of the possible betting sequences.
Information sets: When a player cannot observe some of the actions or chance events in a game, such as in poker when the opponent’s private cards are unknown, many game states will appear identical to the player. Each such set of indistinguishable game states forms one information set, and an agent's strategy or policy for a game must necessarily depend on its information set and not on the game state: it cannot choose to base its actions on information it does not know.


Here are the number of game states of certain variants of Poker.

2-players no limit Holdem - un-abstracted game has approximately $10^{75}$ game states.  Measuring the Size of Large No-Limit Poker Games (2013)
2-players limit Holdem - approximately  $10^{18}$ game states. Abstracted game has $10^7$ game states.  Approximating Game-Theoretic Optimal Strategies for Full-scale Poker (2003). 
6-players no limit Holdem - ?  -  in the publication of Pluribus that won professional poker players they do not state the size of the game. Superhuman AI for multiplayer poker (2019)

","['game-ai', 'game-theory', 'games-of-chance', 'poker', 'state-space-complexity']",
What are the main differences between YOLOv3 and RetinaNet object detection algorithms?,"
I am looking at a certain project that compares performance on a certain dataset for an object detection problem using YOLOv3 and RetinaNet (or the ""SSD_ResNet50_FPN"" from TF Model Zoo). Both YOLOv3 and RetinaNet seem to have similar features like detection at scales, skip connections, etc.
So, what is the exact main difference between YOLOv3 and SSD_ResNet50_FPN?
","['computer-vision', 'tensorflow', 'object-detection', 'yolo', 'single-shot-multibox-detector']",
What's up with Neural Stochastic Differential Equations from a practical standpoint?,"
I've spent a few days reading some of the new papers about Neural SDEs. For example, here is one from Tzen and Raginsky and here is one that came out simultaneously by Peluchetti and Favaro. There are others which I plan to read next. The basic idea, which are attained via different routes in each paper, is that if we consider the input data arriving at time $t=0$ and the output data arriving at time $t=1$, and with certain assumptions on the distribution of network weights and activations, the evolution of the data from one layer to the next inside the network is akin to a stochastic process. The more layers you have, the smaller the $\Delta t$ is between the layers. In the limit as the number of layers goes to infinity, the network approaches a true stochastic differential equation.
I am still working on the math, which is my main objective. However, what I find missing from these papers is: Why is this important? The question is not, why is this interesting?. It is certainly interesting from a purely mathematical perspective. But what is the importance here? What is the impact of this technology?
I was at first excited about this because I thought it proposed a way to apply a neural network to learn the the parameters of an SDE by fitting it to real time-series data where we don't know the form of the underlying data generation process. However I noticed in the experiment of Peluchetti and Favaro is simply the MNIST data set, while the data experiment from Tzen and Raginsky is in fact a simulated SDE. The later fit more with my intuition.
So, again, my question is, what is the general importance of Neural SDEs? And a secondary question is: am I correct in thinking this technology proposes a new way to fit a model to data which we suppose is generated by a stochastic process?
","['papers', 'deep-neural-networks', 'applications']",
How do I calculate the probabilities of the BERT model prediction logits?,"
I might be getting this completely wrong, but please let me first try to explain what I need, and then what's wrong.
I have a classification task. The training data has 50 different labels. The customer wants to differentiate the low probability predictions, meaning that, I have to classify some test data as ""Unclassified / Other"" depending on the probability (certainty?) of the model.
When I test my code, the prediction result is a numpy array. One example is:
[[-1.7862008  -0.7037363   0.09885322  1.5318055   2.1137428  -0.2216074
   0.18905772 -0.32575375  1.0748093  -0.06001111  0.01083148  0.47495762
   0.27160102  0.13852511 -0.68440574  0.6773654  -2.2712054  -0.2864312
  -0.8428862  -2.1132915  -1.0157436  -1.0340284  -0.35126117 -1.0333195
   9.149789   -0.21288703  0.11455813 -0.32903734  0.10503325 -0.3004114
  -1.3854568  -0.01692022 -0.4388664  -0.42163098 -0.09182278 -0.28269592
  -0.33082992 -1.147654   -0.6703184   0.33038092 -0.50087476  1.1643585
   0.96983343  1.3400391   1.0692116  -0.7623776  -0.6083422  -0.91371405
   0.10002492]]

I'm then using numpy.argmax() to identify the correct label.
My question is, is it possible to define a threshold (say, 0.6), and then compare the probability of the argmax() element so that I can classify the prediction as ""other"" if the probability is less than the threshold value?

Edit 1:
We are using 2 different models. One is Keras, and the other is BertTransformer. We have no problem in Keras since it gives the probabilities so I'm skipping Keras model.
The Bert model is pretrained. Here is how it is generated:
def model(self, data):
        number_of_categories = len(data['encoded_categories'].unique())
        model = BertForSequenceClassification.from_pretrained(
            ""dbmdz/bert-base-turkish-128k-uncased"",
            num_labels=number_of_categories,
            output_attentions=False,
            output_hidden_states=False,
        )

        # model.cuda()

        return model

The output given above is the result of model.predict() method. We compare both models, Bert is slightly ahead, therefore we know that the prediction works just fine. However, we are not sure what those numbers signify or represent.
Here is the Bert documentation.
","['classification', 'python', 'bert', 'probability', 'multiclass-classification']","Your call to model.predict() is returning the logits for softmax. This is useful for training purposes.To get probabilties, you need to apply softmax on the logits.Now you can apply your threshold same as for the Keras model."
Does it make sense to combine classifiers trained on the same dataset?,"
I am working on a classification problem.
I have a dataset $S$ and I am training several prediction algorithms using S: Naive Bayes, SVM, classification trees.
Intuitively, I was planning to combine my models, and, for each data point in the test sample $S'$, take the majority vote as my prediction.
Does that make sense? I feel this is a very simplistic way to combine different models.
","['machine-learning', 'classification', 'ensemble-learning']",
Does Linear Discriminant Analysis make dimensionality reduction before classification?,"
I'm trying to understand what LDA exactly does when used as a classifier. I've understood how the dimensionality reduction works and I've understood that the classification task is carried out with the application of Bayes' theorem, but I still can't figure out if LDA executes both operations when used as a classification algorithm.
Is it correct to say that LDA, as a classifier, executes by itself dimensionality reduction and then applies Bayes' theorem for classification?
If that makes any difference, I've used LDA in Python from the sklearn library.
","['machine-learning', 'classification', 'dimensionality-reduction']",
Handling imbalanced data with multiple targets,"
I have the model which has 3 outputs (it is a regression task, I have the angle of the steering wheel, brake and acceleration). I can divide my values to some smaller bins and in this way I can change this into classification problem. I can balance data to have the same number of data points in each bin.
But now I wonder how to balance this data correctly.
I found some good resources and libraries
imbalanced-learn | Python official documentation
multi-imbalance | Python official documentation
Multi-imbalance | Poznan University of Technology
But to my understanding, these algorithms can deal with imbalanced data (in normal and multi class classification) only if you have one output.
But I have 3 outputs. And these outputs can be correlated somehow.
How to balance them correctly?
I thought about 2 ideas:

Creating tuples consist of 3 elements and balancing in such a way that you have the same number of different tuples
But you can have this situation:
(A, X, 1), (A, Y, 2), (A, Y, 3), (B, Z, 3)
These tuples are different, but you can see that we have a lot of tuples with the value A at first position. So the data is still quite imbalanced.

Balancing data iteratively considering only one column at a time. You balance first column, then you balance second column etc.


Are these ideas good or not? Maybe there are some other options for balancing data if you have multiple targets?
","['deep-learning', 'classification', 'regression', 'imbalanced-datasets']",
What are the benefits of Cross Stage Partial Connections over Residual Connections?,"
Cross Stage Partial Connections (CSPC) try to solve the next problems:

Reduce the computations of the model in order to make it more suitable for edge devices.
Reduce memory usage.
Better backpropagate the gradient.

I cannot really understand how the first two points are actually achieved with this type of connection. Furthermore, in CSPC, the skip connection is just a slice of the feature map, and in Residual Connections, the skip connection is all the feature map. Aren't CSPC and Residual Connections (with concatenation) actually ""almost"" the same thing? Then, what advantages do you get for connecting with deeper layers only a slice of the previous feature map (CSPC) vs the whole feature map (Residual Connection)?
","['deep-learning', 'residual-networks']",
How to normalize images before training?,"
I have seen people normalize images by just dividing 255. But why? Why not use mean normalization or Z-score Normalization? I also came across this StackOverflow topic while searching but the answers there were not enough enlightening for me.
","['convolutional-neural-networks', 'data-preprocessing', 'normalisation', 'standardisation']",
"If $h_1(n)$ is admissible, why does A* tree search with $h_2(n) = 3h_1(n)$ return a path that is at most thrice as long as the optimal path?","
Consider a heuristic function $h_2(n) = 3h_1(n)$. Where $h_1(n)$ is admissible.
Why are the following statements true?

$A^*$ tree search with $h_2(n)$ will return a path that is at most thrice as long as the optimal path.
$h_2(n)  + 1$ is guaranteed to be inadmissible for any $h_1(n)$

","['proofs', 'a-star', 'admissible-heuristic', 'tree-search', 'heuristic-functions']",
What are the different types of goals for an AI system called?,"
I remember reading about two different types of goals for an intelligence. The gist was that the first type of goal is one that ""just is"" - it's an end goal for the system. There doesn't need to be any justification for wanting to achieve that goal, since wanting to do that is a fundamental purpose for that system. The second type of goal is a stepping stone, for lack of better words. Those aren't end goals in and of themselves, but they would help the system achieve its primary goals better.
I've forgotten the names for these types of goals and Googling didn't help me much. Is there a standard definition for these different types of goals?
","['terminology', 'goal-based-agents']",
What is the difference between zero-padding and character-padding in Recurrent Neural Networks?,"
For RNN's to work efficiently, we vectorize the operations, which results in an input matrix of shape
(m, max_seq_len) 

where m is the number of examples, e.g. sentences, and max_seq_len is the maximum length that a sentence can have. Some examples have smaller lengths than this max_seq_len. A solution is to pad these sentences.
One method to pad the sentences is called ""zero-padding"". This means that each sequence is padded with zeros. For example, given a vocabulary where each word is related to some index number, we can represent a sentence with length 4,
I am very confused 

by
[23, 455, 234, 90] 

Padding it to achieve a max_seq_len=7, we obtain a sentence represented by:
[23, 455, 234, 90, 0, 0, 0] 

The index 0 is not part of the vocabulary.
Another method to pad is to add a padding character, e.g. <<pad>>, in our sentence:
I am very confused <<pad>>> <<pad>> <<pad>>

to achieve the max_seq_len=7. We also add <<pad>> in our vocabulary. Let's say its index is 1000. Then the sentence is represented by
[23, 455, 234, 90, 1000, 1000, 1000]

I have seen both methods used, but why is one used over the other? Are there any advantages or disadvantages comparing zero-padding with character-padding?
","['natural-language-processing', 'recurrent-neural-networks', 'seq2seq', 'padding']",
Why identity mapping is so hard for deeper neural network as suggested by Resnet paper?,"
In resnet paper they said that a deeper network should not produce more error than its shallow counterpart since it can learn the identity map for the extra added layer. But empirical result shown that deep neural networks have a hard time finding the identity map. But the solver can easily push all the weights towards zero and get an identity map in case of residual function($\mathcal{H}(x) = \mathcal{F}(x)+x$). My question is why it is harder for the solver to learn identity maps in the case of deep nets?
Generally, people say that neural nets are good at pushing the weights towards zero. So it is easy for the solver to find identity maps for residual function. But for ordinary function ($\mathcal{H}(x) = \mathcal{F}(x)$) it have to learn the identity like any other function. But I do not understand the reason behind this logic. Why neural nets are good to learn zero weights ?
","['deep-learning', 'convolutional-neural-networks']",
What's the best way to take a list of lists as DQN input?,"
I have my own environment for the DQN algorithm. In my environment, the state space is represented by a list of lists, where each sublist can be of different lengths. In my case, the length of the global list is 300 and the length of each of the sublists varies from 0 to 10. What is the best way to use such state representation as a DQN input if I want to use the PyTorch platform?
#exapmle state with only 4 sublists and each sublist length can be highest 5
state = [[1,2,3,4], [1,20,20], [10], [20,4,5,6,7]]

I am thinking of using the raw data with zero(s) at the end of every sublist to make them all of the equal lengths.
state = [[1,2,3,4,0], [1,20,20,0,0], [10,0,0,0,0], [20,4,5,6,7]]

Then I can convert them to torch.tensor (and maybe flattened) and take that as input in DQN. However, I am wondering - is there a better approach?
","['reinforcement-learning', 'dqn', 'pytorch', 'state-spaces']",
Setting up a deep learning architecture for multi-dimensional data,"
The input data is thousands, millions of 4x1000 matrices. Each row consists of 3 small natural numbers (1000 combinations) and a corresponding real number between 0 and 1.
The output is a 1x1000 vector for each of the matrices. Each output vector value [1 or 0] is not defined by the corresponding 4-argument entry, but the whole 4x1000 matrix. Each input matrix defines a few valid output 1x1000 vectors that cannot be computed analytically from the 4x1000 matrices.
What would be the options for setting up a deep learning architecture to try to tackle this challenge?
","['deep-learning', 'convolutional-neural-networks', 'model-request']",
How do we know that the neurons of an artificial neural network start by learning small features?,"
I'd like to ask you how do we know that neural networks start by learning small, basic features or ""parts"" of the data and then use them to build up more complex features as we go through the layers. I've heard this a lot and seen it on videos like this one of 3Blue1Brown on neural networks for digit recognition. It says that in the first layer the neurons learn and detect small edges and then the neurons of the second layer get to know more complex patterns like circles... But I can't figure out based on pure maths how it's possible.
","['neural-networks', 'convolutional-neural-networks', 'deep-neural-networks', 'features', 'representation-learning']","We do it experimentally; you're able to look at what each layer is learning by tweaking various values throughout the network and doing gradient ascent. For more detail, watch this lecture: https://www.youtube.com/watch?v=6wcs6szJWMY&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=12 it provides many methods used for understanding exactly what your model is doing at a certain layer, and what features it has learnt."
What are examples of good free books that cover the back-propagation algorithm?,"
What are examples of good free books that cover the back-propagation used to train multilayer perceptrons? I've just started to learn about artificial neural networks, so I'm looking for books that cover the theoretical basics of back-propagation.
","['neural-networks', 'reference-request', 'backpropagation', 'feedforward-neural-networks', 'multilayer-perceptrons']",
"If I want to predict two unrelated values given the same sequence of data points, should I have a model with two outputs or two models?","
I want to predict two separate y-values (not really logically connected) based on an input sequence of data (values x). Using LSTM cells.
Should I train two models separately or should I just increase the dimension of the last layer to 2 instead of 1 and feed the fitting algorithm y-values as 2D pairs? In other words, will there be a lot of interference or can I expect on average similar results with one or two models?
","['neural-networks', 'tensorflow', 'recurrent-neural-networks', 'long-short-term-memory']",
Which algorithm can be used for extracting text patterns in tabular data?,"
I am working with tabular data that is similar to the below:




Name
Phone Number
ISO3 Country
Amount
Email
...
...
Outcome
Possible Reason




Leona Sunfurry
(555)-555-5555
United States
58.96
leo_sun@gmail.com
...
...
0
Not ISO3 country


Diana Moonglory
(333)-555-5555
USA
8.32
di.moon@gmail.com
...
...
1



Fiora Quik
(111)-555-5555
FRA
0.35
null
...
...
1



Darius Guy
12345678901234
CAN
555.01
null
...
...
0
Too many digits in phone


LULU
(333)-555-5555
CAN
0.00
null
...
...
0
Odd name format


Eve K.
(111)-555-5555
FRA
69.25
e.k@gmail.com
...
...
1



Lucian Light
(999)-555-5555
ENG
65.00
null
...
...
1



Lux D.
(333)-555-5555
USA
11.64
test@test.com
...
...
1



Jarvin Crown
(333)-555-5555
USA
1357.13
j4@gmail.com
...
...
0
Unknown reason




The table contains information about users. Some of the fields are user-generated while others are generated by the program (like device location, amount, etc.). When this data is collected, it is sent to third parties (we will say a bank). Sometimes the bank rejects the data and it is not good for our users. The rejection could have happened because the user did not input the data correctly or the banks did not like how a field is formatted despite the data being correct and acceptable to other banks.
So we want to find the fields that are causing the most errors and how to fix the issue.
Does it make sense to do pattern recognition on the values to find the reason why the row was rejected? It would need to be an alpha-numeric type of algorithm, it seems.
We know the outcomes from the bank which is labeled as Outcome.  Although we have labeled data, it still feels like we need an unsupervised learning algorithm because we do not have labels on why the rows of data were rejected.
Does anyone know what type of algorithm would be best? Any feedback would be appreciated!
","['unsupervised-learning', 'pattern-recognition', 'text-classification', 'algorithm-request']","You should first segregate the rejected samples. You can use then use string matching or something more complex (like creating embeddings and then, taking L2 distance between them) between the different field names you have and the comment for rejection. Whichever field gets the highest score, you increase the rejection count for that field. In the end, you will have a tally of who is your biggest enemy.You can create some rules which prevent injection of wrong data (like your password should be 7 characters long or something along these lines) or post-process your entries to match a uniform format."
Are there any known disadvantages of implementing vanilla Q-learning on a discretized-state space environment?,"
For an RL problem on a continuous state space, the states could be discretized into buckets and these buckets used in implementing the Q-table. I see that is what is done here. However, according to van Hasselt from his book, this discretization changes the problem into a partially observable MDP (POMDP), and this is understandable. And I know POMDPs require special treatment from the vanilla Q-learning we are used to (observation space, belief states, etc).
But my question is: is there a specific technical reason why a discretized-state problem (which is now POMDP) should be solved using POMDP algorithms, instead of plainly constructing a vanilla Q-table using the discretized states (i.e. the buckets from discretization)? In other words, is there a disadvantage in not using POMDP algorithms to tackle the discretized-state problem?
","['reinforcement-learning', 'q-learning', 'markov-decision-process', 'pomdp']",
Why are CNN binary classifier output probability distributions often skewed?,"
I've been working on a lot of simple resnet18 binary classifiers lately and I've started to notice that the probability distributions are often skewed one way or the other. This figure shows one such example. The red and blue color code the negative and positive ground truths respectively. And the bottom axis is the output prediction of the binary classifier (sigmoid activated output neuron). Notice how the red is more bunched towards 0, but the blue has quite some spread.

At first I began to reason this to myself with arguments like ""well the positive clues in the image have a small footprint, so they are hard to find, therefore the model should be unsure about positives more of the time.""
Later I found oppositely skewed distributions and tried to say ""well the positive clues in the image have a small footprint, so it might be easy to confuse some other things for the positive clues, therefore the model should be unsure about negatives more of the time""
You can see where I'm going with this. It took me training up quite a few models like this in a short amount of time to realise I was kidding myself. Even the exact same architecture and similar dataset may produce a different skew over different training runs. And if you think about it, negative probability is just the complement of positive probability, so any argument you make in favor of one over the other can be easily reversed.
So what's influencing this skew? Why is there a skew at all? If there's a skew, is it because of something ""real"", or is it just random?
These all may seem like philosophical questions, but they have great practical significance. Because that skew basically tells me where I should put my decision threshold in production level inference!
","['convolutional-neural-networks', 'probability-distribution', 'explainable-ai', 'binary-classification']",
What is a good convergence criterion for Q-learning in a stochastic environment?,"
I have a stochastic environment and I'm implementing a Q-table for the learning that happens on the environment. The code is shown below. In short, there are ten states (0, 1, 2,...,9), and three actions: 0, 1, and 2. The action 0 does nothing, action 1 subtracts 1 with a probability of 0.7, and action 2 adds 1 with a probability of 0.7. We get a reward of 1 when we are in state 5, and 0 otherwise.
import numpy as np
import matplotlib.pyplot as plt

def reward(s_dash):
    if s_dash == 5:
        return 1
    else: 
        return 0
states = range(10)
Q = np.zeros((len(states),3))
Q_previous = np.zeros((len(states),3))
episodes = 2000
trials = 100
alpha = 0.1
decay = 0.995
gamma = 0.9
ls_av = []
ls = []
for episode in range(episodes):
    print(episode)
    s = np.random.choice(states)
    eps = 1
    for i in range(trials):
        eps *= decay
        p = np.random.random()
        if p < eps:
            a = np.random.randint(0,3)
        else:
            a = np.argmax(Q[s, :])

        if a == 0:
            s_dash = s
        elif a == 1:
            if p >= 0.7:
                s_dash = max(s-1, 0)
            else:
                s_dash = s
        else:
            if p >= 0.7:
                s_dash = min(s+1, 9)
            else:
                s_dash = s
        r = reward(s_dash)
        Q[s][a] = (1-alpha)*Q[s][a] + alpha*(r + gamma*np.max(Q[s_dash]))
        s = s_dash
    ls.append(np.max(abs(Q - Q_previous)))
    Q_previous = np.copy(Q)
print(Q)
for i in range(10):
    print(i, np.argmax(Q[i, :]))
plt.plot(ls)
plt.show()


When I plot the absolute value of the maximum change in the Q-table at the end of each episode, I get the following, which indicates that the Q-table is constantly being updated.

However, I see that when I print out the action with the max Q-value for each state, it shows what I expect to be the optimal policy. For each state, the best action is given as shown below:
(0, 2)
(1, 2)
(2, 2)
(3, 2)
(4, 2)
(5, 0)
(6, 1)
(7, 1)
(8, 1)
(9, 1)


My question is: why do I not have convergence in the Q-table? If I had a stochastic environment for which I didn't know before-hand what the optimal policy is, how will I be able to judge if I need to stop training when the Q-table isn't converging?
","['reinforcement-learning', 'training', 'q-learning', 'markov-decision-process']","To obtain guarantees of convergence for Q table values, you need to decay the learning rate, $\alpha$, at a suitable rate. Too fast and convergence will be to inaccurate values. Too slow and convergence never happens.For sticking with theoretical guarantees, the learning rate decay process should generally follow the rule that $\sum_t \alpha_t = \infty$ but $\sum_t \alpha_t^2 \ne \infty$ - an example of a learning rate schedule that does this is $\alpha_t = \frac{1}{t}$, although in practice that specific choice could lead to very slow convergence.Choosing a good starting $\alpha$ and a good decay schedule will depend on the problem, and you may want to base it on experience with similar problems. However, it is not that common to need to gurantee convergence of action values in value-based reinforcement learning. In control problems you often care more about have finding an optimal policy than about perfectly accurate action values. Further to that, many interesting control problems are too complex to solve perfectly in tabular form, so you expect some approximation. It seems relatively common just to pick a learning rate for the problem and stick with it.If you make your learning rate lower, the Q table will converge to more stable Q values, but possibly at the expense of taking longer to converge on the optimal policy."
How to use mixed data for image segmentation?,"
I have a task for which I have to do image segmentation (cancer detection on MRIs). If possible, I would also like to include clinical data (i.e. numeric/categorical data which comes in the form of a table with features such as age, gender, ...).
I know that for classification purposes, it's possible to create a model that uses both numeric data as well as image data (as mentioned in the paper by Huang et al. : ""Multimodal fusion with deep neural networks for leveraging CT imaging and electronic health record: a case‑study in pulmonary embolism detection""
The problem I have is that, for image segmentation tasks, it doesn't really make sense to me as to how to use both types of data.
In the above-mentioned paper, they create one model with only the image data and another with only the numeric data, and then they fuse them (there are multiple strategies for fusing them together). For classification tasks, it makes sense. However, for my task, it does not make sense to have a model which only uses the clinical data for image segmentation and that's where I get confused.
How do you think I should proceed with my task? Is it even possible to mix both types of data for image segmentation tasks?
","['deep-learning', 'image-segmentation', 'u-net']","You can try doing image segmentation the traditional way, just using the image data. If you want to use the non-image data, then, you can introduce classification as another task for your network. It will provide some regularization to your model. But, this is one way you can still use non-image data whilst still working with image outputs."
What are the state-of-the-art Person-Detektion / Human-Segmentation?,"
I would like to use a deep learning approach to detect people in videos. I have found some freely accessible implementations like Human Segementation with Pytorch or BodyPix / DeepLab / Pixellib with Tensorflow. They all work well, but with many it happens that, for example, half hand is not detected or if a person is sitting in the picture only the legs and the head are detected. Are there other approaches to detect people who are freely accessible or is that state-of-the-art?
I had imagined such problems have been solved, but I don't know so much about it. Thanks for your answers.
","['convolutional-neural-networks', 'object-detection', 'image-segmentation', 'semantic-segmentation']",
Determining minimal state representation for maze game,"
I came across this question set. It asks following question:

Letâ€™s revisit our bug friends from assignment 2. To recap, you control one or more insects in a rectangular maze-like environment with dimensions M Ã— N , as shown in the figures below. At each time step, an insect can move North, East, South, or West (but not diagonally) into an adjacent square if that square is currently free, or the insect may stay in its current location. Squares may be blocked by walls (as denoted by the black squares), but the map is known.
For the following questions, you should answer for a general instance of the problem, not simply for the example maps shown.
(a) You now control a single flea as shown in the maze above, which must reach a designated target location X. However, in addition to moving along the maze as usual, your flea can jump on top of the walls. When on a wall, the flea can walk along the top of the wall as it would when in the maze. It can also jump off of the wall, back into the maze. Jumping onto the wall has a cost of 2, while all other actions (including jumping back into the maze) have a cost of 1. Note that the flea can only jump onto walls that are in adjacent squares (either north, south, west, or east of the flea).

i. Give a minimal state representation for the above search problem.
Sol. The location of the flea as an (x, y) coordinate.
ii. Give the size of the state space for this search problem.
Sol. M âˆ— N
(b) You now control a pair of long lost bug friends. You know the maze, but you do not have any information about which square each bug starts in. You want to help the bugs reunite. You must pose a search problem whose solution is an all-purpose sequence of actions such that, after executing those actions, both bugs will be on the same square, regardless of their initial positions. Any square will do, as the bugs have no goal in mind other than to see each other once again. Both bugs execute the actions mindlessly and do not know whether their moves succeed; if they use an action which would move them in a blocked direction, they will stay where they are. Unlike the flea in the previous question, bugs cannot jump onto walls. Both bugs can move in each time step. Every time step that passes has a cost of one.

i. Give a minimal state representation for the above search problem.
Sol. A list of boolean variables, one for each position in the maze, indicating whether the position could contain a bug. You donâ€™t keep track of each bug separately because you donâ€™t know where each one
starts; therefore, you need the same set of actions for each bug to ensure that they meet.
ii. Give the size of the state space for this search problem.
Sol. $2^{MN}$

I don't get why the (a).i. uses $(x,y)$ coordinates whereas (b).i. uses boolean list. I guess they can be used interchangeablly right? And correspondingly the answers to ii will change.
Update
I now understand following:

For single flea maze, the representation $(x,y)$ will have $M\times N$ state space, whereas boolean list will have $2^{M\times N}$ state space. For two bug maze, the representation $(x_1,y_1,x_2,y_2)$ will have $(M\times N)^2$ state space, whereas boolean list will have $2^{M\times N}$ state space. I am able to understand, we prefer $(x,y)$ representation for single flea maze since $M\times N < 2^{M\times N}$. But for two bug maze, I am not able to understand why we prefer boolean list representation (and not $(x_1,y_1,x_2,y_2)$ representation), since $(M\times N)^2<2^{M\times N}$.

","['game-ai', 'search', 'heuristics']",
"In classification, how does the number of classes affect the model size and amount of data needed to train?","
When solving a classification problem with neural nets, be it text or images, how does the number of classes affect the model size and amount of data needed to train?
Are there any soft or hard limitations where the number of outputs starts to stall learning?
Do you know about any analysis of how the number of classes scales the model?
Does the optimal size increase proportionally with the number of outputs? Does it increase at all? If it does increase, is the relationship linear or exponential?
","['neural-networks', 'classification', 'computational-learning-theory', 'architecture', 'capacity']",
How to implement the deconv which is used in “Visualizing and Understanding Convolutional Networks”,"
I'm trying to understand the deconv referenced in the paper Visualizing and Understanding Convolutional Networks
The paper states (section 2, p. 3):

the deconvnet uses transposed versions of the same filters, but applied to the rectified maps

Is it possible to implement this step in a short code example? Given an unpooled, rectified map; how would the transposed filter be applied against it?
I did try looking at the referenced paper Adaptive Deconvolutional Networks for Mid and High Level Feature Learning. However, I'm not wrapping my head around its explanations too well; and it references a third paper with regard to its work on ""layers of convolutional sparse coding"" (deconvolution [M. Zeiler, D. Krishnan, G. Taylor, and R. Fergus. Deconvolutional networks. In CVPR, 2010]), but this 2010 paper appears to require access to download.
","['papers', 'transpose-convolution']",
Does stochasticity of an environment necessarily mean non-stationarity in MDPs?,"
Is a stochastic environment necessarily also non-stationary? To elaborate, consider a two-state environment ($s_1$ and $s_2$), with two actions $a_1$ and $a_2$. In $s_1$, taking action $a_1$ has a certain probability $p_1$ of transitioning you into $s_2$, and a probability $1-p_1$ of keeping you in $s_1$. There is also a similar probability for taking $a_2$ in $s_1$, and taking either action in $s_2$. Let's also say that there is a reward $r$ given only when a transition occurs from either state, and 0 otherwise. This is a stochastic environment. But isn't this non-stationary in one sense and stationary in another? I think it is stationary because the expected return from taking a particular action in a particular state converges to a constant value. But it is non-stationary in the sense that the reward obtained from taking a certain action in a given state may change at a given time. Which is really the case?
","['reinforcement-learning', 'markov-decision-process']","Is a stochastic environment necessarily also non-stationary?No.A stochastic environment (i.e. an MDP with a transition model $p(s', r \mid s, a)$) can be stationary (i.e. $p$ does not change over time) or non-stationary ($p$ changes over time). Similarly, a deterministic environment, i.e. the probabilities are $1$ or $0$, can also be either stationary or not. To emphasize that an MDP may be non-stationary, you could write $p$ as a function of time, i.e. $p_t$ (you also do the same thing for the reward function if it's separate from the transition function).The same idea applies to a stochastic/deterministic policy, which can either be stationary or not.A non-stationary environment may lead to a non-stationary policy (or may require you to relearn a model of the environment, if you need to learn a model of the environment) [1]. However, note that a stochastic environment (i.e. an MDP) does not necessarily imply a stochastic policy (actually, under some conditions, stationary and stochastic MDPs are known to have a deterministic optimal policy [1]).In general, if something (e.g. environment, policy, value function or reward function) is non-stationary, it means that it changes over time. This can either be a function or a probability distribution. So, a probability distribution (the stochastic part of an MDP) can change or not over time. If it changes over time, then it makes the MDP non-stationary.But it is non-stationary in the sense that the reward obtained from taking a certain action in a given state may change at a given timeInformally, you could say that the empirical reward obtained is non-stationary because it changes over time, due to the stochasticity of the reward function, behaviour policy, etc., but the dynamics (transition function and reward function) would still be fixed, so the environment would still be stationary. So, there's a difference between the environment and the experience that you collected so far (with some behaviour policy)."
Why did the developement of neural networks stop between 50s and 80s?,"
In a video lecture on the development of neural networks and the history of deep learning (you can start from minute 13), the lecturer (Yann LeCunn) said that the development of neural networks stopped until the 80s because people were using the wrong neurons (which were binary so discontinuous) and that is due to the slowness of multiplying floating point numbers which made the use of backpropagation really difficult.
He said, I quote, ""If you have continuous neurons, you need to multiply the activation of a neuron by a weight to get a contribution to the weighted sum.""
But the statement stays true even with binary (or any discontinuous activation function) neurons. Am I wrong? (at least, as long as you're in the hidden layer, the output of your neuron will be multiplied by a weight I guess). The same professor said that the perceptron, ADALINE relied on weighted sums so they were computing multiplications anyways.
I don't know what I miss here and I hope someone will enlighten me.
","['neural-networks', 'backpropagation', 'history', 'weights', 'perceptron']","I will first address your main question ""Why did the development of neural networks stop between 50s and 80s?""
In 40-50s there was a lot of progress (McCulloch and Pitts); the perceptron was invented (Rosenblatt). That gave rise to an AI hype giving many promises (exactly like today)!However, Minsky and Papert have proved in 1969 that a single-layer architecture is not enough to build a universal approximating machine (see e.g. Minsky, M. & Papert, S. Perceptrons: An Introduction to Computational Geometry, vol. 165 (1969)). That led to the first disappointment in ""AI"". Which lasted until several major breakthroughs in 1980s: the proof of universal approximating capabilities of multi-layer perceptron by Cybenko, the popularisation of backpropagation algorithm (Hinton and colleagues), etc.I agree with LeCun that using continuous activation functions have enabled the backpropagation algorithm at the time. It is only recently that we have learned to backpropagate in networks with binary activation functions (2016!)."
Does feature scaling have any benefits if all features are on the same scale?,"
By scaling features, we can prevent one feature from dominating the decisions of a model. For example, say heights (cm), and age (years) are two features in my data. Since range of heights is larger than of years, a trained model could weight importance of heights much more than years. This could result in a poor model in return.
However, say that all of my features are binary, they take a value of either 0 or 1. In such a case, does feature scaling still have any benefits?
","['data-preprocessing', 'features', 'feature-engineering']","If all you features are binary, then, you don't need to apply normalization on them. Since their values are on the same scale already."
How can I estimate the minimum number of training samples needed to get interesting results with WGAN?,"
Let's say we have a WGAN where the generator and critic have 8 layers and 5 million parameters each. I know that the greater the number of training samples the better, but is there a way to know the minimum number of training examples needed? Does it depend on the size of the network or the distribution of the training set? How can I estimate it?
","['computational-learning-theory', 'wasserstein-gan', 'sample-efficiency']",
Can I do topic classification of Arabic text (software requirements) without a training dataset?,"
I am trying to make a text classification for Arabic data. The problem is that there is no labeled Arabic dataset for this data. My question is then: is possible to do a classification without a training dataset? If yes, what methods can I use?
","['classification', 'algorithm-request']",
How to restrain a model's outputs to a certain range without affecting its representative capacity?,"
CONTEXT
I am trying to build a regression model that finds the optimal parameters for a given input. The data I am using are point clouds, with N points and 3 coordinates (x,y,z) each. Each point cloud is divided into neighborhoods of constant size and, during inference, a batch of these neighborhoods are fed into the model which outputs a set of parameters. The parameters represent a family of surfaces and the goal is to find parameters such that the surface fits the neighborhood of points as tightly as possible (in the least squares sense).
THE ISSUE
The problem is that each type of parameter must fall into a specific range, otherwise it has no meaning. For example the first two parameters must lie inside [0.1, 1.9], the next three must be strictly positive etc.. I have tried restraining the outputs by adding a scaled sigmoid activation or simply clamping the output to the range that I want. However, it seems that such hacks result in saturation, the model outputs negative values and all the outputs become 0 from clamping.
I can't imagine I'm the first one to encounter such a problem, but I haven't been able to find out a way to solve it. Is there a defacto way of dealing with this situation?
P.S. I am not including details of the model architecture to keep this question general interest, but I will include them upon request, if it helps.
","['deep-learning', 'regression', 'geometric-deep-learning']",
Find the expected reward in an expectimax-based dice rolling game?,"
I have this question that I'm kinda stuck on.
It's a game scenario in which we set up an expectimax tree. In the game, you have 3 dice with sides 1-4 that you roll at the beginning. Then, depending on the roll, the player can choose one of the dice to reroll or not reroll anything. Points are assigned like so:

10 points if there's 2 of a kind
15 if there's 3 of a kind
7 if there's a series like 1-2-3 or 2-3-4
Otherwise, or if the sum is higher than the rewards from above, the score = sum of the rolls

For additional context, this is an example expectimax tree I came up with, for the case that the player rolled a 1,2,4 and is considering rerolling or not:

Now lets introduce a new agent -- a robot that's supposed to help the human player. we assume

the human player choses any action with uniform probability regardless of the initial roll
there's a robot that, given a configuration of dice and the human's desired action, actually implements the action with probability 1-p and overrides it with a ""no reroll"" order with probability p>0. It has no effect if the human's decision is already to not reroll.

For that scenario, I came up with this expectimax tree:

Now for the part I'm actually stuck on -- lets define A, B, C, and D as the expected reward of performing actions ""reroll die 1"", ""reroll die 2"", ""reroll die 3"", and ""no reroll."" How do we find $R_H$, the expected reward for the human acting without the robot's help, and $R_{AH}$ the expected reward for if the robot helps? We can't use p in the expression, we only have access to A,B,C,D and we're supposed to write it in the form $X + Y_p$
*EDIT: I asked again and the question was worded weirdly. They said we should definitely use p. What was meant by not using p is X and Y themselves can't contain p. But Y will be multiplied by p in the final simplified form.
For $R_{H}$ I think the answer should be $\frac{(A + B + C + D}{4}$ because of uniform distribution over A-D.
I'm supposing that $R_{AH}$ would be $\frac{(A + B + C)(1-p) + D + Dp}{4}$? Because the robot doesn't override with probability $1-p$, but he can only override A-C and does with probability $p$.
I think something feels slightly wrong about my answer but I'm not sure what.
","['game-ai', 'game-theory', 'decision-trees', 'tree-search']","If I understand,Thus the expected reward for if the robot helps is$$R_{AH}= p\cdot D+(1-p)\frac{A+B+C+D}{4} = \frac{(A+B+C)(1-p)+D+3pD}{4}$$"
How are certain machine learning models able to produce variable-length outputs given variable-length inputs?,"
Most machine learning models, such as multilayer perceptrons, require a fixed-length input and output, but generative (pre-trained) transformers can produce sentences or full articles of variable length. How is this possible?
","['machine-learning', 'natural-language-processing', 'recurrent-neural-networks', 'transformer', 'natural-language-generation']","In short, repetition with feedback.You are correct that machine learning (ML) models such as neural networks work with fixed dimensions for input and output. There are a few different ways to work around this when desired input and output are more variable. The most common approaches are:Padding: Give the ML model capacity to cope with the largest expected dimensions then pad inputs and filter outputs as necessary to match logical requirements. For example, this might be used for an image classifier where the input image varies in size and shape.Recurrent models: Add an internal state to the ML model and use it to pass data along with each input, in order to work with sequences of identical, related inputs or outputs. This is a preferred architecture for natural language processing (NLP) tasks, where LSTMs, GRUs, and transformer networks are common choices.A recurrent model relies on the fact that each input and output is the same kind of thing, at a different point in the sequence. The internal state of the model is used to combine information between points in the sequence so that for example a word in position three in the input has an impact on the choice of the word at position seven of the output.Generative recurrent models often use their own output (or a sample based on the probabilities expressed in the output) as the next step's input.It is well worth reading this blog for an introduction and some examples: The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy"
How is the transformers' output matrix size arrived at?,"
In this tensorflow article, the comments in the code say that MHA should output with one of the dimensions being the sequence length of the query/key. However, that means that the second MHA in the decoder layer should output something with one of the dimensions being the input sequence length, but clearly it should actually be the output sequence length! From all that I have read on transformers, it seems that the output of the left side of the SHA should be a matrix with dimensions q_seq_length x q_seq_length, and the output of the right side of the SHA should be v_seq_length x d_model. These matrices can't even multiply when using the second MHA in the decoder to incorporate the encoder output! Please help. I would appreciate a clear-cut explanation. Thanks
","['transformer', 'attention', 'encoder-decoder']","Aha, I understand now! In the paper, the diagram for SHA has its inputs in the order Q, K, V, while the diagram for MHS has its inputs in the order V, K, Q! In the grand diagram for the entire net, I thought the arrows for the inputs were in the order for SHA, but they are actually in the order for MHA. It is a bit confusing that when changing to the MHA diagram, they decided to swap the order of inputs in the paper, but it all makes sense now. output_seq x d_q multiplies d_q x input_seq to get output_seq x input_seq, which finally multiplies with input_seq x d_v to get output_seq x d_v, which then concatenates to output_seq x hd_v, so after the final linear layer it becomes output_seq x d_model, the standard in the decoder layer. Hopefully, anyone else who mixed up orders from the diagram will understand now."
Why Autoencoder Weights Are Not Always Tied,"
To me, tying weights in an autoencoder makes sense if we think of the auto encoder as doing PCA. Why in any situation would it make sense to not tie the weights? If we don't tie the weights, would it not try to learn something that is PCA anyway or rather something that might not be as optimal as PCA?
Also, if weights are not tied, it doesn't make sense to me that the auto-encoder is invertible i.e. if the decoder is looking for an inverse operation because it's a mapping between spaces of different dimension which should not invertible.
So, if the weights are not tied then why do we expect the decoder to learn anything meaningful i.e neither PCA nor an inverse operation?
","['neural-networks', 'autoencoders', 'principal-component-analysis']",
AI approach for layout mapping,"
I am researching different AI approaches and was curious what approach would be useful in my scenario.
Assume you are tiling a room. The tiles, and the room itself, can be any shape. In this room you could encounter N number of obstacles, such as a wall, or built-in. The goal is to layout the tiles, taking into account cutting into the obstacles mentioned above, along with the shape and dimensions of the destination room. This would have to account the shape, and measurements of said tile being placed onto the room.
Which AI approach would prove useful in this scenario?
","['ai-design', 'ai-basics']",
Bias gradient of layer before batch normalization always zero,"
From the original paper and this post we have that batch normalization backpropagation can be formulated as

I'm interested in the derivative of the previous layer outputs $x_i=\sigma(w X_i+b)$ with respect to $b$, where $\{X_i\in\mathbb{R}, i=1,\dots,m\}$ is a network input batch and $\sigma$ is some activation function with weight $w$ and bias $b$.
I'm using Adam optimizer so I average the gradients over the batch to get the gradient $\theta=\frac{1}{m}\sum_{i=1}^m\frac{\partial l}{\partial x_i}\frac{\partial x_i}{\partial b}$.
Further, $\frac{\partial x_i}{\partial b}=\frac{\partial}{\partial b}\sigma(wX_i+b)=\sigma'(wX_i+b)$.
I am using ReLu activation function and all my inputs are positive, i.e. $X_i>0 \ \forall i$, as well as $w>0$ and $b=0$. That is, I get
$\frac{\partial x_i}{\partial b} = 1\ \forall i$. That means that my gradient $\theta=\frac{1}{m}\sum_{i=1}^m\frac{\partial l}{\partial x_i}$ is just the average over all derivatives of the loss function with respect to $x_i$. But summing all the $\frac{\partial l}{\partial x_i}$ up is zero, which can be seen from the derivations of $\frac{\partial l}{\partial x_i}$ and $\frac{\partial l}{\partial \mu_B}$.
This means that my bias change would always be zero, which makes no sense to me. Also i created a neural network in Keras just for validation and all my gradients match except the bias gradients, which are not always zero in Keras.
Does one of you know where my mistake is in the derivation?
Thanks for the help.
","['neural-networks', 'backpropagation', 'gradient-descent', 'feedforward-neural-networks', 'batch-normalization']",
Using numerical/categorical data and image data to detect objects,"
Let's say that I want to create a program capable of detecting lamps on some pictures. Those pictures can be, for instance, of a room, a street, etc.
I would like to know if the following is possible:

Create a program that is trained using both pictures of lamps (so a dataset of images) and numerical/categorical data of lamps (so that would be a .csv file which contains features ranging from type, height, etc.)
The idea is to combine both types of data in order to detect on another unseen picture if it contains a lamp.

I'm not entirely sure if the resulting algorithm will be performant, but I would like to try.
If what I described above is possible, could you please point me in the right direction as to which literature to consult ? My research have lead me nowhere.
",['deep-learning'],
Which RL algorithm would be suitable for this multi-dimensional and continuous action space?,"
Is there an RL approach/algorithm that would be suited for the following kind of problem?

There is a continuous action space with an action value $A_{a,t}$ for each action dimension $a$.
The objective function is a non-linear function of the satisfaction factors $S_{s}$ for each satisfaction dimension $s$ and some other random & independent factors. This objective function can be known to the agent.
Each satisfaction factor depends on an independent variable $\Delta^S_{s,t}$ and the effect $\delta^S_{a,s,t}$ of each action: $S_{s,t}=\Delta^S_{s,t} +\sum_a A_{a,t} * \delta^S_{a,s,t}$.
Each action can further have an effect $\delta^R_{a,r,t}$ on the inventory factors $I_{r,t}$ for each resource dimension $r$, with inventories being kept between time-steps and a factor $\Delta^R_{r,t}$ that is added or removed from the inventory at each step independent of the actions: $I_{r,t+1}=I_{r,t}+\Delta^R_{r,t} + \sum_a A_{a,t} * \delta^R_{a,r,t}$
The agent is constrained by each of these resources (i.e. the inventory has to remain positive).
The agent should be able to deal both with $\delta$ and $\Delta$ factors that are visible (states) and invisible (have to be learned).
A trained agent should be able to know how to adapt to changes of the $\delta$ and $\Delta$ factors, as well as the introduction or removal of activity dimensions.

EDIT: I have adapted the problem description after some feedback.
","['reinforcement-learning', 'multi-armed-bandits', 'continuous-action-spaces', 'algorithm-request']",
$\epsilon$-greedy policy in environments where actions are performed in a long term. Does it has influence?,"
I'm working in an environment where once an action $a \in A$ is performed, it must hold this action selection for a while. To clarify this, assumes a horizon length $h$ and the set of actions: $\{a_{1}, a_{2}, a_{3}\} \in A$. Assumes now that the length $h$ to converge for an optimal solution must be split by 3 where each action is applied distinctly because if one of these actions is selected twice during an episode, the reward penalizes it severely in such way that there's no possible policy that can get a better return than selects each action just once. Thus, the length of each of ""sub-horizon"" $h_{i}$ is $h_{i} > 0$ and $h_{i} < h - 2$.
Now seeing the DQN setting in the naive approach for exploration using $\epsilon$-greedy policy that selects at each time-step an action following:
n = random value between 0 and 1
if n < epsilon then:
   a = random action in A
else:
   a = maxQ(S, a)

For my particular problem (and probably many with similar settings), this way of selects an action seems oppositely and hard to converge for the optimal solution, shifting the distribution of samples for far from that. Does it make sense?
Actually, I believe that sticky actions, could be way beneficial for many similar settings, beyond the scope of his original approach.
","['deep-rl', 'dqn', 'epsilon-greedy-policy']",
Adding corpus to BERT for QA,"
I was wondering about SciBERT's QA abilities using SQuAD. I have a scarce textual dataset consisting of less than 100 files where doctors are discussing cancer in dialogues. I want to add it to SciBERT to see if the QA abilities will improve in the cancer disease domain.
After concatenating them into one large file which will be our vocab, I then clean the file (all char to lower, white space splitting, char filtering, punctuation, stopword filtering, short tokens and etc) which leaves me with a list of 3000 unique tokens
If I wanted to add these tokens, do I just do scibert_tokenizer.add_tokens(myList) where myList is the 3k tokens?
I can confirm that more tokens are added doing print(len(scibert_tokenizer)) and I can see that embeddings do change such as corona and ##virus changes to coronavirus and ##virus.
Does the model need to be trained from scratch again?
","['natural-language-processing', 'bert', 'fine-tuning']",
What is the purpose of storing the action $a$ within an experience tuple?,"
From what I understand, experience replay works by storing tuples of $(s, a, r, s')$  to be sampled for training. I understand why we store $s$, $r$ and $s'$. However, I do not understand the need for storing the action $a$.
As I recall, the reward $r$ and the next state $s'$ are both used to calculate the target values. We can then compare these target values to the output we get when we do a forward-pass using state $s$ It seems to me that the stored action $a$ is not required for this process to work; or am I missing something? Why would we store the action $a$ if it isn't used in the training process itself?
Please, forgive me if this question has been answered before. I looked, but was unable to find anything other than generic explanations as to what experience replay is and why we do it.
","['reinforcement-learning', 'experience-replay']","We need to store the action $a$ as it tells us the action that we took in the state that we are backing up.Suppose we are in state $s$ and we take action $a$, then we will receive a reward $r$ and next state $s'$. The goal of RL, and in particular DQN (I mention DQN as it is the first algorithm that comes to mind when I think of a replay buffer but it is of course not the only algorithm to make use of one), is that we are trying to learn optimal state-action value functions $Q(s, a)$. We thus want our value function to be able to predict $y = r + \gamma \max_{a'}Q(s', a')$, i.e. given $s$ and $a$ we want to be able to predict $y$. As you can see, we need to know which action we took in state $s$ so that we can train our value function to approximate $y$, and the value function clearly depends on $a$, hence we need to also store $a$ in our experience tuple."
"When using PCA for dimensionality reduction of the feature vectors to speed up learning, how do I know that I'm not letting the model overfit?","
I'm following Andrew Ng's course for Machine Learning and I just don't quite understand the following.
Using PCA to speed up learning

Using PCA to reduce the number of features, thus lowering the chances for overfitting

Looking at these two separately, they make perfect sense. But practically speaking, how am I going to know that, when my intention is to speed up learning, I'm not letting the model over-fit?
Do I've to find a middle-ground between these two scenarios when applying PCA? If so how exactly can I do that?
","['machine-learning', 'training', 'overfitting', 'principal-component-analysis', 'dimensionality-reduction']",
Is there an ideal range of learning rate which always gives a good result almost in all problems?,"
I once read somewhere that there is a range of learning rate within which learning is optimal in almost all the cases, but I can't find any literature about it. All I could get is the following graph from the paper: The need for small learning rates on large problems

In the context of neural networks trained with gradient descent, is there a range of the learning rate, which should be used to reduce the training time and get a good performance in almost all problems?
","['neural-networks', 'deep-learning', 'reference-request', 'gradient-descent', 'learning-rate']",
What are the metrics to be used for unsupervised monocular depth estimation in computer vision?,"
I am currently replicating the results of this paper. In this paper they have not mentioned how they are evaluating the results as no ground truth is available for comparison. Same goes for other papers of this topic (unsupervised depth estimation). So I am very much confused about how to evaluate the model for overfitting, underfitting for this.
","['deep-learning', 'convolutional-neural-networks', 'computer-vision']",
How would I compute the optimal state-action value for a certain state and action?,"
I am currently trying to learn reinforcement learning and I started with the basic gridworld application. I tried Q-learning with the following parameters:

Learning rate = 0.1
Discount factor = 0.95
Exploration rate = 0.1
Default reward = 0
The final reward (for reaching the trophy) = 1

After 500 episodes I got the following results:

How would I compute the optimal state-action value, for example, for state 2, where the agent is standing, and action south?
My intuition was to use the following update rule of the $q$ function:
$$Q[s, a] = Q[s, a] + \alpha (r + \gamma  \max_{a'}Q[s', a'] — Q[s, a])$$
But I am not sure of it. The math doesn't add up for me (when using the update rule).
I am also wondering either I should use the backup diagram to find the optimal state-action q value by propagating the reward (gained from reaching the trophy) to the state in question.
For reference, this is where I learned about the backup diagram.
","['reinforcement-learning', 'q-learning', 'value-functions', 'bellman-equations']","It seems that you are getting confused between the definition of a Q-value and the update rule used to obtain these Q-values.Remember that to simply obtain an optimal Q-value for a given state-action pair we can evaluate$$Q(s, a) = r + \gamma \max_{a'} Q(s', a)\;;$$where $s'$ is the state we transitioned into (note that this only holds when obtaining the optimal Q-value, if we were using a stochastic policy then we would have to introduce expectations).Now, this assumes that we have been given/obtained the optimal Q-values. To obtain them, we have to use the update rule (or any other learning algorithm) that you mentioned in your question."
Is there a document with a list of conjectures or research problems regarding reinforcement learning (like the Millennium Prize Problems)?,"
Is there a document with a list of conjectures or research problems regarding reinforcement learning like the Millennium Prize Problems?
","['reinforcement-learning', 'reference-request', 'deep-rl', 'research']","It's not really an exhaustive list, but Hutter maintains a small list of problems (click on the bullet point ""Universal AI Book"" here) related to AIXI (a reinforcement learning agent), some of which have already been solved. The money awards are in the range of 50-500 euros, so they are not as financially important as the Millennium Prize Problems."
How to pathfind with volatile probabilities (Slay the spire),"
I'm attempting to write an AI for the game Slay the Spire.  One of the tasks it will need to do is navigate the map.  The map is a directed acyclic graph with the same start and end node.
Each node (including the end node) will have 2 values associated with it: Expected value and death probability.  The goal of the AI should be to maximize the expected value without dying.  So far, none of this seems tough.
The twist here is that the death probability changes over time.  Some nodes (elites) have high volatility:  The death probability may go up or down drastically as we move through the map.  The pathfinding algorithm would need to consider adjacent alternatives.  A path that allows me to switch to a low-volatile node if things are getting tough is important.
As an example, the following map has two major routes.  Both routes have an elite (the creature with horns), but the one on the right is a forced elite, while the one on the left can be skipped if death probability is too high.  The ability for me to be flexible mid-route is an attractive feature, and one I'd like to take into account when pathfinding somehow.

How can my path-finding algorithm take into account adjacent paths/path flexibility?  Is this even a job for pathfinding at all?
","['path-planning', 'path-finding']",
Theoretical limits on correlation between classification algorithm performances,"
Are there any known theoretical bounds, or at least heuristic approaches, regarding the relation or correlation between the performances of any two different classification algorithms?
For example, would there exist binary classification datasets for which, say, $k$-nearest-neighbour classifiers would perform with say >90% accuracy, whereas say decision tree classifiers would do no better than 50-60%? (Accuracy here is measured by say $k$-fold cross-validation.)
It seems to me, at first glance, that a dataset which is able to achieve a very high accuracy on some classification algorithm would necessarily have some structure that would make it highly improbable that some other general classification algorithm would be able to perform very poorly. Yet it's also not impossible that there might be some 'exotic' type of dataset that does exhibit such a phenomenon.
","['machine-learning', 'classification', 'datasets']",
Why does Q-learning converge under 100% exploration rate?,"
I am working on this assignment where I made the agent learn state-action values (Q-values) with Q-learning and 100% exploration rate. The environment is the classic gridworld as shown in the following picture.

Here are the values of my parameters.

Learning rate = 0.1
Discount factor = 0.95
Default reward = 0

Reaching the trophy is the final reward, no negative reward is given for bumping into walls or for taking a step.
After 500 episodes, the arrows have converged. As shown in the figure, some states have longer arrows than others (i.e., larger Q-values). Why is this so? I don't understand how the agent learns and finds the optimal actions and states when the exploration rate is 100% (each action: N-S-E-W has 25% chance to be selected)
","['reinforcement-learning', 'q-learning', 'convergence', 'epsilon-greedy-policy', 'exploration-strategies']","Q-learning is guaranteed to converge (in the tabular case) under some mild conditions, one of which is that in the limit we visit each state-action tuple infinitely many times. If your random random policy (i.e. 100% exploration) is guaranteeing this and the other conditions are met (which they probably are) then Q-learning will converge.The reason that different state-action pairs have longer arrows, i.e. higher Q-values, is simply because the value of being in that state-action pair is higher. An example would be the arrow pointing down right above the trophy -- obviously this has the highest Q-value as the return is 1. For all other states it will be $\gamma^k$ for some $k$ -- to see this remember that a Q-value is defined as$$Q(s, a) = \mathbb{E}_\pi \left[\sum_{j=0}^\infty \gamma^j R_{t+j+1} |S_t = s, A_t = a \right]\;;$$
so for any state-action pair that is not the block above the trophy with the down arrow $\sum_{j=0}^\infty \gamma^j R_{t+j+1}$ will be a sum of $0$'s plus $\gamma^T$ where $T$ is the time that you finally reach the trophy (assuming you give a reward of 1 for reaching the trophy)."
Is there any situation in which breadth-first search is preferable over A*?,"
Is there any situation in which breadth-first search is preferable over A*?
","['comparison', 'search', 'a-star', 'breadth-first-search']",
Why does this formula $\sigma^2 + \frac{1}{T}\sum_{t=1}^Tf^{\hat{W_t}}(x)^Tf^{\hat{W_t}}(x_t)-E(y)^TE(y)$ approximate the variance?,"
How does:
$$\text{Var}(y) \approx \sigma^2 + \frac{1}{T}\sum_{t=1}^Tf^{\hat{W_t}}(x)^Tf^{\hat{W_t}}(x_t)-E(y)^TE(y)$$
approximate variance?
I'm currently reading What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision, and the authors wrote the above formula for the approximate estimation for the variance. I'm confused how the above is an approximation for $\frac{\sum(y-\bar{y})^2}{N-1}$. So, in the above equation, they're using a Bayesian Neural Network to quantify uncertainty. $\sigma$ is the predictive variance (kind of confused how they get this). $x$ is the input and $y$ is the label for the classification. $f^{\hat{W_t}}(\cdot)$ output a mean to a Gaussian distribution, with $\sigma$ being the SD for that distribution and $T$ is a predefined number of samples because the gradient is evaluated using Monte Carlo sampling.
","['bayesian-deep-learning', 'bayesian-neural-networks', 'uncertainty-quantification', 'variance']",
Can the hidden state of an RNN be a matrix?,"
If I'm dealing with a sequence of images as the input (frame by frame), and I want to output a matrix at each timestamp, can the hidden state be a matrix?
",['recurrent-neural-networks'],
Why is Openai's PPO2 implementation differentiable?,"
I'm trying to understand the concept behind the implementation of the OpenAI PPO2 algorithm. The loss function that is minimized is as follows: loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef.
First question: The computation of pg_loss requires to use operations like tf.reduce_mean and tf.maximum. Are these two functions differentiable? Apparently, they are, otherwise, it would not work. Can someone explain why so I can understand the implementation?
Second question: During training, an action is sampled by using the Gumbel Distribution: Noise from such a distribution is added to the logits and then tf.argmax is applied. This index is then used to calculate the negative log-likelihood. However, the tf.argmax should also not be differentiable, so how can this work?
","['reinforcement-learning', 'backpropagation', 'proximal-policy-optimization']",
How should I simulate this Markov Decision Process?,"
I am working on solving a problem on nodes in a graph communicating with each other. They try to estimate a central state using Kalman consensus filter, with the connections described by the graph's adjacency matrix. Considering time to be discrete, the adjacency matrix changes at each time instant with a transition probability matrix (unknown) to some other matrix (in some finite set of matrices). I want to simulate this in python to solve an MDP with some cost/reward function. (An external agent is concerned with this cost/reward and takes actions accordingly)
Since the state space can be large, my advisor suggested using deep-RL techniques. However I have only studied (formally) basic RL (Q learning, stochastic approximation, etc. with finite states and finite actions at every instant). I tried looking at RL libraries but I can't figure out which one to pick. And even before that I am very confused by how to simulate KCF between nodes in python (from scratch?). How should I proceed?
","['reinforcement-learning', 'python', 'deep-rl', 'open-ai']",
Should I use batch gradient descent when I have a small sample size?,"
I have a dataset with an input size of 155x155, with the output being 155 x 1 with a 3-4 layer neural net being used for regression. With such a small sample size, should I use full batch gradient descent (so all 155 samples) or use mini batch/stochastic gradient descent. I have read that using smaller mini batch sizes allows better generalisation, but as the batch size is very very small computationally it shouldn't be a burden to use BGD.
","['neural-networks', 'gradient-descent']",
Is there any research work on known malware detection systems based on AI?,"
I'm working on writing an article about the possibilities of modern AI-based algorithms to produce invisible self-learning malware, that can distribute itself throughout the internet and create flexible botnets.
So far, I can not find any additional information about that, except this one.
Is there any research work on known malware detection systems based on AI?
","['neural-networks', 'machine-learning', 'reference-request', 'ai-security']",
Is data leakage relevant when scaling across samples?,"
I have a question about data leakage when pre-processing data for a neural network and whether data leakage actually applies in my instance.
I have variance stabilising transformed genomic data. Because it is genomic data we know apriori that lower numbers translate to lower levels of a gene being made and vice versa. Before input into the neural network, the data are squashed to between 0 and 1 using sklearn:
preprocessing.minmax_scale(data, feature_range=(0,1), axis=1)

The min_max scaling needs to be done across sample (axis=1) as opposed to features because of this apriori assumption of gene levels - low genes need to remain low and vice-versa...
Because of this, my question is: do training samples still need to be scaled separately from test samples as it doesn't seem there is a risk of data leakage here? Is this the correct assumption to make?
","['machine-learning', 'data-preprocessing', 'normalisation']",
GAN Generator Output w/ Periodic Noise,"
I am training a Semi-Supervised GAN, using multivariate time-series with window of shape (180*80) with the generator and discriminator architecture below. My data is scaled using Robust Scaler, so I kept linear activation for the generator output.
During the training I get noise in the generated signals and I can't understand the reason why whereas the original data is smooth. What can be the reason for this noise?

def make_generator_model(noise):
    w_init = tf.random_normal_initializer(stddev=0.02)
    gamma_init = tf.random_normal_initializer(1., 0.02)
    
    def residual_layer(layer_input):
        
        res_block = Conv1D(128, 3, strides=1, padding='same')(layer_input)
        res_block = BatchNormalization(gamma_initializer=gamma_init)(res_block)
        res_block = LeakyReLU()(res_block)
        res_block = Conv1D(128, 3, strides=1, padding='same')(res_block)
        res_block = BatchNormalization(gamma_initializer=gamma_init)(res_block)
        res_block = LeakyReLU()(res_block)
        res_add = Add()([res_block, layer_input])
        
        return res_add
    
    in_noise = Input(shape=(100,))
    

    gen = Dense(180*65, kernel_initializer=w_init, use_bias=None)(in_noise)
    gen = BatchNormalization(gamma_initializer=gamma_init)(gen)
    gen = LeakyReLU()(gen)

    gen = Reshape((180, 65))(gen)
    #assert model.output_shape == (None, 45, 256) # Note: None is the batch size

    gen = Conv1D(64, 7, strides=1, padding='same', kernel_initializer=w_init, use_bias=None)(gen)
    #assert model.output_shape == (None, 45, 128)
    gen = BatchNormalization(gamma_initializer=gamma_init)(gen)
    gen = LeakyReLU()(gen)
        
    gen = Conv1D(64, 4, strides=2, padding='same', kernel_initializer=w_init, use_bias=None)(gen)
    #assert model.output_shape == (None, 45, 128)
    gen = BatchNormalization(gamma_initializer=gamma_init)(gen)
    gen = LeakyReLU()(gen)
    
    gen = Conv1D(128, 4, strides=2, padding='same', kernel_initializer=w_init, use_bias=None)(gen)
    #assert model.output_shape == (None, 45, 128)
    gen = BatchNormalization(gamma_initializer=gamma_init)(gen)
    gen = LeakyReLU()(gen)
    
    for i in range(6):
        gen = residual_layer(gen)

    gen = Conv1DTranspose(128, 4, strides=2, padding='same', kernel_initializer=w_init, use_bias=None)(gen)
    #assert model.output_shape == (None, 90, 64)
    gen = BatchNormalization(gamma_initializer=gamma_init)(gen)
    gen = LeakyReLU()(gen)
    
    gen = Conv1DTranspose(128, 4, strides=2, padding='same', kernel_initializer=w_init, use_bias=None)(gen)
    #assert model.output_shape == (None, 90, 64)
    gen = BatchNormalization(gamma_initializer=gamma_init)(gen)
    gen = LeakyReLU()(gen)
    

    out_layer = Conv1D(65, 7, strides=1, padding='same', kernel_initializer=w_init, use_bias=None)(gen)
    #assert model.output_shape == (None, 180, 65)
    
    model = Model(in_noise, out_layer)

    return model

def make_discriminator_model(n_classes=8):
    w_init = tf.random_normal_initializer(stddev=0.02)
    gamma_init = tf.random_normal_initializer(1., 0.02)   
    
    in_window = Input(shape=(180, 65))

    disc = Conv1D(64, 4, strides=1, padding='same', kernel_initializer=w_init)(in_window)
    disc = LeakyReLU()(disc)
    disc = Dropout(0.3)(disc)

    disc = Conv1D(64*2, 4, strides=1, padding='same', kernel_initializer=w_init)(disc)
    disc = LeakyReLU()(disc)
    disc = Dropout(0.3)(disc)
    
    disc = Conv1D(64*4, 4, strides=1, padding='same', kernel_initializer=w_init)(disc)
    disc = LeakyReLU()(disc)
    disc = Dropout(0.3)(disc)
    
    disc = Conv1D(64*8, 4, strides=1, padding='same', kernel_initializer=w_init)(disc)
    disc = LeakyReLU()(disc)
    disc = Dropout(0.3)(disc)
    
    disc = Conv1D(64*16, 4, strides=1, padding='same', kernel_initializer=w_init)(disc)
    disc = LeakyReLU()(disc)
    disc = Dropout(0.3)(disc)
    
    disc = Flatten()(disc)
    
    disc = Dense(128)(disc)
    disc = Dense(128)(disc)
    
    out_layer = Dense(1)(disc)
    
    c_out_layer = Dense(8, activation='softmax')(disc)
    
    model = Model(in_window, out_layer)
    c_model = Model(in_window, c_out_layer)

    return model, c_model

","['neural-networks', 'convolutional-neural-networks', 'generative-adversarial-networks', 'semi-supervised-learning']",
PPO2: Intuition behind Gumbel Softmax and Exploration?,"
I'm trying to understand the logic behind the magic of using the gumbel distribution for action sampling inside the PPO2 algorithm.
This code snippet implements the action sampling, taken from here:
def sample(self):
    u = tf.random_uniform(tf.shape(self.logits), dtype=self.logits.dtype)
    return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1) 

I've understood that is a mathematical trick to be able to backprop over the action sampling in case of categorical variables.

But why can't I just put a softmax layer on top of the logits and sample according to the given probabilities? Why do we need u?

Tere is still the argmax which is not differential. How can backprob work?

Does u allows exploration? Imagine that at the beginning of the learning process, Pi holds small similar values (nothing is learned so far). In this case the action sampling does not always choose the maximum value in Pi because of logits-tf.log(-tf.log(u)).
In the further course of the training, larger values arise in Pi, so that the maximum value is also taken more often in the action sampling? But doesn't this mean that the whole process of action sampling is extremely dependent on the value range of the current policy?


","['reinforcement-learning', 'probability', 'proximal-policy-optimization']",
setting up last layer in tensoflow for class type of label [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.







                        Improve this question
                    



I am creating a NN in tensorflow keras. the inputs are all float and the output is a class.
The output currently encoded as a float, but only has 4 values (0,1,2,3).
My model is similar to this:
model = tf.keras.Sequential([
    normalize,
    layers.Dense(128, activation='relu'),
    layers.Dense(254, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(254, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(4)
])

model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'],
                           optimizer = tf.optimizers.Adam())

history=model.fit(data_set_features, data_set_labels,validation_split=0.33, epochs=100)

is the model last layer correct?
what type of activation should I use and loss function?
","['tensorflow', 'keras', 'objective-functions', 'activation-functions']","Since you're using categorical cross-entropy loss, the last layer (output layer) should come with softmax activation instead of identity (as being blank in layers.Dense(4)).And SparseCategoricalCrossentropy is different from CategoricalCrossentropy, for categorical cross-entropy, the shape of output and label should be the same, for example:When using categorical cross-entropy (example labels in one-hot):When using sparse categorical cross-entry (labels are class indices):"
Is there a methodology for splitting up annotated orthophotos into smaller photos that retain the original bounding boxes?,"
I'm trying to train an object detection algorithm (i.e. YOLOv4 Scaled, Faster R-CNN) on data taken from large orthophotos. Let's say I have one class, and I label the entire orthophoto with bounding boxes. After labeling, is there a way to slice up the entire image into individual photos of specified pixel sizes (i.e. 416x416 pixels) while keeping the bounding boxes? I can easily slice the photo into the specified dimensions, but the problem I am having is keeping the bounding boxes in these new images.
That way, I would not be exhausting my GPU's memory requirements.
","['deep-learning', 'computer-vision', 'object-detection', 'data-labelling']","You can reduce your photo size and scale the corresponding boxes to the new dimensions (416x416).Or if you want to go with your technique, you can slice the image and then, check if the bounding box lies in the slice, then, reorient it according to the slice you took.Take a look at albumentations library for this."
Is it a good idea to use different width and height of the kernel in a CNN?,"
I always see that the width and height of the kernel are the same. But is it a good idea to use different numbers?
Recently I tried to use GoogLeNet (which expects images to be 224x224) on my images (500x150) and I got an error:

Negative dimension size caused by subtracting 7 from 5 for 'average_pooling2d_5/AvgPool'...

I know that this error is because the height of my image is too small. If I use the height of about 200, then everything is ok. So, maybe, in this situation, I could just use a smaller height and bigger width in the kernel. For example (5, 3).
Is it a good idea in this case? Or in general? How can it affect the accuracy of the network and the ability to extract different features?
","['convolutional-neural-networks', 'image-processing', 'convolution', 'hyper-parameters', 'filters']","It depends on your application. In case of text recognition, non-uniform kernels are used since the information about text is less on the horizontal axis and more on the vertical axis.If in your case it is applicable then, it will be good idea. But, if it is not you are better off using a smaller uniform kernel (2x2, maybe). You can also zero-pad your image to make it uniform before putting it through convolutions. Also, check if you are doing 'valid' or 'same' padding in your convolutions since 'valid' convolutions chip away at your image dimensions."
Can CNNs be made robust to tricks where small changes cause misclassification?,"
I while ago I read that you can make subtle changes to an image that will ensure a good CNN will horribly misclassify the image. I believe the changes must exploit details of the CNN that will be used for classification. So we can trick a good CNN into classifying an image as a picture of a bicycle when any human would say it's an image of a dog. What do we call that technique, and is there an effort to make image classifiers robust against this trick?
","['convolutional-neural-networks', 'reference-request', 'terminology', 'adversarial-ml']","These are known as adversarial attacks, and the specific examples that are misclassified are known as adversarial examples.There is a reasonably large body of work on finding adversarial examples, and on making CNNs more robust (i.e. less prone to these attacks). An example is the DeepFool algorithm, which can be used to find perturbations of data which would cause the label to change.There are several techniques in the literature which are used to fight against adversarial attacks. Here is a selection:Augmenting the training data with various random perturbations. This is intended to make the model more robust to the typical adversarial attack where random noise is added to an image. An example of this approach is discussed in [1].Constructing some sort of model to ""denoise"" input before feeding into the CNN. An example of this is Defense-GAN [2], which uses a generative adversarial model which models the ""true"" image distribution and finds an approximation of the input closer to the real distribution.[1] Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy. Explaining and Harnessing Adversarial Examples. ICLR (2015). URL.[2] Pouya Samangouei, Maya Kabkab, Rama Chellappa. Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models. ICLR (2018). URL."
"Why can we take the action $a$ from the next state $s'$ in the max part of the Q-learning update rule, if that action doesn't lead to any reward?","
I'm using OpenAI's cartpole environment. First of all, is this environment not Markov?
Knowing that, my main question concerns Q-learning and off-policy methods:
For me, there is something weird in updating a Q value based on the max Q for a state and a reward value that was not from the action taken? How does this make learning better and makes you learn the optimal policy?
","['q-learning', 'markov-decision-process', 'off-policy-methods', 'on-policy-methods', 'markov-property']","I'm using OpenAI's cartpole environment. First of all, is this environment not Markov?The OpenAI Gym CartPole environment is Markov. Whether or not you know the transition probabilities does not affect whether the state has the Markov property. All that matters is that knowing the current state is enough to be determine the next state and reward in principle. You do not need to explicitly know the state transition model, or the reward function.An example of a non-Markov state for CartPole would be if one of the features was missing - e.g. the current position of the cart, or the angular velocity of the pole. It is still possible to have agents attempt to solve CartPole with such missing data, but the state would no longer be Markov, and it would be a harder challenge.For me, there is something weird in updating a Q value based on the max Q for a state and a reward value that was not from the action taken? How does this make learning better and makes you learn the optimal policy?To recap, you are referring to this update equation, or perhaps some variation of it:$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R_{t+1} + \gamma\text{max}_{a'}Q(S_{t+1},a') - Q(S_t, A_t))$$Another correction here, the reward value is related to the action taken and the experience that happened. This is important - the values of $R_{t+1}$ and $S_{t+1}$ are from the experience that was observed after taking action $A_t$, they do not come from anywhere else. In fact this is the only data from experience that is inserted into the update, everything else is based on action value estimates in a somewhat self-referential way (a.k.a. ""bootstrapping"").How does this converge to an optimal policy? It follows a process known as generalised policy iteration (GPI), which works as follows:The target policy is to always take the best action according to your current estimatesIn off policy methods only (i.e. does not apply to all GPI), there is a conversion needed from experience in a behaviour policy to value estimates for the target policy. In single-step Q learning, this is where the $\text{max}_{a'}Q(S_{t+1},a')$ comes from. It is calculating the future value from following the target policy*. No interim values are needed, because you are estimating a new value for $Q(S_t,A_t)$, there are no other time steps where you need to care about difference between behaviour and target policy (there is just ""now"" and ""all the future, which is bootstrapped"")Once you update the action value estimates in Q table, that automatically updates the target policy because it always takes the best estimated action.Each update improves your value estimates, due to injecting some real experienced data in the form of $R_{t+1}$ and $S_{t+1}$.Each time the estimates improve, the target policy can also improve and become closer to the optimal policy.When the target policy changes, this makes the estimates less accurate, because they were made for the previous policy. However, the next time the same states and actions are visited they will be updated based on the new improved policy.With a stochastic environment, it is possible for estimates to get worse due good or bad luck for the agent when it tries different actions. However, over time, the law of large numbers will win, and the value estimates will converge to close to their true values for the current policy, which will make policy improvement inevitable if it is possible (caveat, it is only close to inevitable for the tabular Q-learning method which can be proven to converge eventually on an optimal policy).There is proof that always taking the maximising action is a strict improvement to a policy when estimates are accurate. It is called the Policy Improvement Theorem.* To be clear, there is nothing particularly clever about taking the max here. It is literally what the target policy has been chosen to do, so the term $\text{max}_{a'}Q(S_{t+1},a')$ is just the same as $V_{\pi}(S_{t+1})$ - the value of being in state $S_{t+1}$ for the current target policy $\pi$."
How to improve prediction performance of periodic data?,"
I have a 1 column dataset of $50 000$ points where 95% of the values equal $-50$. The data looks like  the following: $$\begin{matrix}
\text{time} & \text{value}\\
1&-50 \\
2&-50 \\
3&-50 \\
4& -50 \\
5&3 \\
6&-50\\
7&-50\\
8&5 
\end{matrix}$$
As an addition, I know the exact time instance in which I will get value $\neq -50$ (as in the example above these are instances $5$ and $8$). The data is somewhat periodic, so the values which are different from $-50$ are chosen from a finite set $\mathcal{S}$.
To predict the values I use a 3 layer LSTM network with l2 regularizer where along with the values I input another column that looks like that:
$$\begin{matrix}
\text{time} & \text{value} & \text{expect a change}\\
1&-50 & 0 \\
2&-50 & 0\\
3&-50 & 0\\
4& -50 & 1\\
5&3 & 0\\
6&-50& 0\\
7&-50& 1\\
8&5 & 0 
\end{matrix}$$
which identifies the change one time instant in advance, so the LSTM will know to expect a change. However, the prediction performance is quite poor, it always predicts the change in the value but is far from real one and usually takes values out of the set $\mathcal{S}$. Any idea of how this could be improved?
","['long-short-term-memory', 'prediction']",
Are the Word2Vec encoded embeddings available online? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.







                        Improve this question
                    



I am trying to do an NLP project and was wondering if there is anywhere online where the Word2Vec embeddings are stored (the actual n-dimmensional vectors).
I want to search up a word and see what its encoding is. I have tried looking but couldn't find anything.
Thank you
","['deep-learning', 'natural-language-processing', 'word-embedding', 'word2vec', 'embeddings']","The gensim library for Python provides both a convenient way of manipulating Word2Vec embeddings, but also downloadable data sets and pretrained models. That last part is what you're looking for. Wikipedia2Vec also has multilingual embeddings. Then this guy (whom I've never heard of) has an assortement of languages I haven't seen other places. There's another 44 models hosted by the European NLPL. You can also search Kaggle, but a lot of this looks like training data, models I've already linked, or just cruft."
Will there be some promising techniques that can make AI greener and affordable in the future?,"
The recent advances in machine learning were mostly achieved by the hardware, and the hardware is said to continue driving the development of AI, but I was still shocked by this thread which reads that the projected future cost for the largest model would be 1B dollars in 2025. And I learned that universities are suffering from an academic AI brain drain partly due to the scarce hardware resources.

Some people proposed the so-called Green AI that encourages sustainable AI development but provides few constructive methods to prevent the trend.
I wonder if the redder and redder AI would be in fact truly inevitable. It seems to me that all companies should build an expensive compute infrastructure to be competitive, but I think the investment would be very risky since most companies cannot get a higher return.
But on the other hand, we human beings have evolved tens of millions of years or billions of years(life) on earth with hundreds of billions of brains that have ever lived on earth as a whole ""human brain"". The biological wetware seems much much redder than the nowadays hardware and has consumed much much more energy than all the supercomputers. To make machines as intelligent as we humans shouldn't we pay as high a price? It reminds me of the NFL theorem but it should be imprecise in this scenario.
So, will there be some promising techniques on the algorithm side that can make AI greener, affordable and sustainable in the future? If not, could anyone please explain why AI should be unavoidably red and inevitably redder?
","['machine-learning', 'deep-learning', 'ethics', 'green-ai']",
Converting age and sex variables to a 64-unit dense layer,"
I am studying a preprint for my own learning (https://www.medrxiv.org/content/medrxiv/early/2020/04/27/2020.04.23.20067967.full.pdf) and I am befuddled by the following detail of the neural network architecture:

This is in accord with the paper's description of the architecture (p. 5):

Age and sex were input into a 64-unit hidden layer and was concatenated with the other branches.

How can the two scalars of age and sex be implemented as a 64-unit dense layer?
","['convolutional-neural-networks', 'papers', 'embeddings']","Convert them into numbers (using one-hot vectors or direct numerical representations) and then concatenate them. Then, you can pass them through the Dense layer."
Why is tf.abs non-differentiable in Tensorflow?,"
I understand why tf.abs is non-differentiable in principle (discontinuity at 0) but the same applies to tf.nn.relu yet, in case of this function gradient is simply set to 0 at 0. Why the same logic is not applied to tf.abs? Whenever I tried to use it in my custom loss implementation TF was throwing errors about missing gradients.
","['tensorflow', 'backpropagation', 'relu', 'gradient']","By convention, the $\mathrm{ReLU}$ activation is treated as if it is differentiable at zero (e.g. in [1]). Therefore it makes sense for TensorFlow to adopt this convention for tf.nn.relu. As you've found, of course, it's not true in general that we treat the gradient of the absolute value function as zero in the same situation; it makes sense for it to be an explicit choice to use this trick, because it might not be what the code author intends in general.In a way this is compatible with the Python philosophy that explicit is better than implicit. If you mean to use $\mathrm{ReLU}$, it's probably best to use tf.nn.relu if it is suitable for your use case.[1] Vinod Nair and Geoffrey Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. ICML'10 (2010). URL."
"Is it possible to make a neural network to solve this ""reaction time test""?","
I'm thinking about writing an essay on the comparison between the human nervous system (reaction time) and a neural network that does the same reaction time test. I am very new in this area, so I was wondering if I can build a neural network that can perform a test like this: https://humanbenchmark.com/tests/reactiontime
I just wanted to know how I should approach this problem, and what would be the best way to compare it to the human nervous system.
I have thought about maybe using an image classification neural network, and have it looks for different colors and such, but not too sure about its technical aspects as of yet. Any help is appreciated.
","['neural-networks', 'classification', 'model-request']","Basically, what you want to do is an anomaly detection at the fastest speed possible. You need to sample the image at two timesteps and if there is a difference you click. The smaller the difference between your timesteps the better it is.But, you would not need a neural network for it. Since, it is a single colour that will change. A color is represented by a 3-tuple (RGB) so you just need to compare them and if they are different then you click. A single if condition will suffice for it."
Classifying generated samples with Wasserstein-GAN as real or fake,"
I'm quite new to GANs and I am trying to use a Wasserstein GAN as an augmentation technique. I found this article
https://www.sciencedirect.com/science/article/pii/S2095809918301127,
and would like to replicate their method of evaluating the GAN. The method is shown in the figure.
In the article they write that they extract the generated samples that fooled the discriminator and use these to train a classifier. They also say that they use a Wasserstein GAN. Does anyone know how it is possible to extract samples that fooled the discriminator, since for a Wasserstein GAN the critic (discriminator) only puts a rating and not a label on the generated data?

","['machine-learning', 'generative-adversarial-networks', 'wasserstein-gan']",
What is Federated Learning?,"
How would you explain Federated Learning in simple layman terms for a non-STEM person?
What are the main ideas behind Federated Learning?
","['terminology', 'definitions', 'academia', 'federated-learning']","The analogy is to a federal system of government. In a federation, smaller pieces follow the direction of a higher piece. In federated machine learning, you give your data for processing to the higher machine. The federation in this analogy is a collection of smaller computers. The central computer breaks up your data and gives portions of it to each smaller computer. When those computers are done they return the results and the central computer reassembles them into a single model.The main idea is distributed processing. Some benefits are:"
Sentiment analysis does not handle neturals [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.







                        Improve this question
                    



I'm writing some financial tools,  I've found highly performant models for question and answering but when it comes to sentiment analysis I haven't found anything that good.  I'm trying to use huggingface:
from transformers import pipeline
classifier = pipeline('sentiment-analysis')
print(classifier(""i'm good""))
print(classifier(""i'm bad"")) 
print(classifier(""i'm neutral""))
print(classifier(""i'm okay"")) 
print(classifier(""i'm indifferent"")) 

Which returns results

[{'label': 'POSITIVE', 'score': 0.999841034412384}]


[{'label': 'NEGATIVE', 'score': 0.9997877478599548}]


[{'label': 'NEGATIVE', 'score': 0.999396026134491}]


[{'label': 'POSITIVE', 'score': 0.9998164772987366}]


[{'label': 'NEGATIVE', 'score': 0.9997762441635132}]

The scores for all of the neutral words come up very high in a positive or negative direction,  I would of figured the model would put the score lower.
I've looked at some of the more fine-tuned models yet they seem to perform the same.
I would assume there would be some pretrained models which could handle these use cases.  If not, How can I find neutral sentiments?
","['python', 'transformer', 'bert', 'sentiment-analysis']","Yes, there is. You can try Spacy. Here you go."
Clarifying representation of Neural Nerwork input for Chess Alpha Zero,"
In the Alpha Zero paper (https://arxiv.org/pdf/1712.01815.pdf) page 13, the input for the NN is described. In the beggining of the page, the authors state that:
""The input to the Neural Network is an N x X x (MT + L) image stack [...]""
From this, I understand that (for one training example) each input feature is an 8x8 plane. (Technically speaking, every value of every 8x8 plane is a feature, but for the purpose of the question let's suppose that a plane is an input feature).
In the description of the table on top of the image, the following statement is made:
""[...] Counts are represented by a single real-valued input; other input features are represented by a one-hot encoding using thespecified number of binary input planes. [...]""
I understand how they convert the P1 and P2 pieces to one-hot encodings. My questions are:

When they say single real-valued input, since every input feature should be an 8x8 plane, do they mean that they create an 8x8 plane where every entry has the same single-real value? For example, for the 'Total move count' plane, if 10 moves had been played in the game so far, it would look like the one below?

  move_count_plane = [[10, 10, 10, 10, 10, 10, 10, 10],
                      [10, 10, 10, 10, 10, 10, 10, 10],
                      [10, 10, 10, 10, 10, 10, 10, 10],
                      [10, 10, 10, 10, 10, 10, 10, 10],
                      [10, 10, 10, 10, 10, 10, 10, 10],
                      [10, 10, 10, 10, 10, 10, 10, 10],
                      [10, 10, 10, 10, 10, 10, 10, 10],
                      [10, 10, 10, 10, 10, 10, 10, 10]]


For the 'Repetitions' plane, is it the same case as above? They mean a plane where every value is the number of times a specific board setup has been reached? For example, if a specific position has been reached 2 times, then the repetitions plane for that position would be

  # for a specific timestep in the T=8 step history
  repetitions_plane = [[2, 2, 2, 2, 2, 2, 2, 2],
                       [2, 2, 2, 2, 2, 2, 2, 2],
                       [2, 2, 2, 2, 2, 2, 2, 2],
                       [2, 2, 2, 2, 2, 2, 2, 2],
                       [2, 2, 2, 2, 2, 2, 2, 2],
                       [2, 2, 2, 2, 2, 2, 2, 2],
                       [2, 2, 2, 2, 2, 2, 2, 2],
                       [2, 2, 2, 2, 2, 2, 2, 2]]

? Also, why do they keep 2 repetitions planes? Is it one for every player? (8 repetition planes for the past T=8 moves for P1, and more 8 repetition planes for the past T=8 moves for P2?)
Thanks in advance.
","['reinforcement-learning', 'deep-rl', 'alphazero', 'chess']","For anyone wondering, I believe to have found the answer:Yes, it will be an 8x8 plane where all the entries are the same, the number of moves (or mpves with no progress).There are two repetitions planes (for each position from the most recent T=8 positions):a) The first repetition plane will be a plane where all the entries are 1's if the position is being repeated for the first time. Else 0's.b) The second repetition plane will be a plane where all the entries are 1's if the current position is being repeated for the second time. Else 0's."
Late Onset Augmentation,"
If I train a U-Net model for image segmentation (e.g. medical images) and start training until it converges and then add augmentation - can i expect similar results as if i train with augmentation from the beginning ?

","['u-net', 'generalization', 'data-augmentation']",
NLP: Are hashtags tokenised?,"
I am exploring a potential NLP project. I was wondering what generally is done with the hashtags words (e.g. #hello). Are those words ignored? is the # removed and the word tokenised? Is it tokenised with the #?
","['natural-language-processing', 'recurrent-neural-networks', 'word-embedding']",Word including Hash tag need to be tokanised. It has a special meaning and given a context. The word by itself has a different meaning.
How can we find the value function by solving a system of linear equations without knowing the policy?,"
An MDP is a Markov Reward Process with decisions, it’s an environment in which all states are Markov. This is what we want to solve. An MDP is a tuple $(S, A, P, R, \gamma)$, where $S$ is our state space, $A$ is a finite set of actions, $P$ is the state transition probability function,
$$P_{ss'}^a = \mathbb{P}[S_{t+1} = s' | S_t = s, \hspace{0.1cm}A_t = a] \label{1}\tag{1}$$
and
$$R_s^a = \mathbb{E}[R_{t+1}| S_t =s, A_t = a]$$
and a discount factor $\gamma$.
This can be seen as a linear equation in $|S|$ unknowns, which is given by,
$$V = R + \gamma PV  \hspace{1mm}  \label{2}\tag{2}$$
$V$ is value of a state vector, $R$ is immediate reward vector, $P$ is transition probability matrix, where each element at $(i,j)$ in $P$ is given by,  $ P[i][j] = P(i \mid j)$ i.e., probability that I am in state $j$ going to state $i$.
As $P$ is given, we treat, equation $\ref{2}$ as a linear equation in $V$.
But $P[i][j] = \sum_a (\pi(a \mid j) \times \mathrm{p}(i \mid j, a) )$. But, $ \pi (a \mid s)$ (i.e., probability that I will take action a in state s) is NOT given.
So, how can we frame this problem as the solution to a system of linear equations in \ref{2}, if we only know $ P^a_{ss'}$ and we do not know $ \pi(a \mid s)$, which is needed to calculate $P[i][j]$?
","['markov-decision-process', 'policies', 'bellman-equations', 'transition-model']","Your equations all look correct to me.It is not possible to solve the linear equation for state values in the vector $V$ without knowing the policy.There are ways of working with MDPs, through sampling of actions, state transitions and rewards, where it is possible to estimate value functions without knowing either $\pi(a|s)$ or $P^{a}_{ss'}$. For instance, Monte Carlo policy evaluation or single-step TD learning can both do this. It is also common to work with $\pi(a|s)$ known but $P^{a}_{ss'}$ and $R^{a}_{s}$ unknown in model-free control algorithms such as Q learning.However, in your case, you are correct, in order to resolve the simultaneous equations you have presented, you do need to know $\pi(a|s)$This is not as limiting as you might think. You can construct a control method using simultaneous equations, by starting with the policy set to some arbitrary policy. Either a randomly-chosen deterministic policy or the equiprobable policy are reasonable first guesses. Then, after each solution to linear equations, you improve the policy so that each action choice maximises the expected return. This is essentially the policy iteration algorithm but replacing the policy evaluation step with the linear equations method for calculating the values."
Is it practical to train AlphaZero or MuZero (for indie games) on a personal computer?,"
Is it practical/affordable to train an AlphaZero/MuZero engine using a residential gaming PC, or would it take thousands of years of training for the AI to learn enough to challenge humans?
I'm having trouble wrapping my head around how much computing power '4 hours of Google DeepMind training' equates to my residential computer running 24/7 trying to build a trained AI.
Basically, are AlphaZero or MuZero practical for indie board games that want a state of the art AI, or is it too expensive to train?
","['training', 'game-ai', 'alphazero', 'muzero']","The vast majority of neural networks are now trained on graphics processing units (GPUs) or specialised accelerator hardware such as tensor processing units (TPUs).In Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm, Silver et al. say that the training process involved 5,000 first-generation TPUs generating self-play games and 64 second-generation TPUs for training. This is certainly far beyond what any practical gaming computer is likely to achieve, as you'll likely only have one GPU, and that might not even rival a single TPU. Training on the CPU will be substantially slower again than either a GPU or TPU. Training would be orders of magnitude slower; you might find these benchmarks by Wang et al. of interest."
Is it okay to calculate the validation loss over batches instead of the whole validation set for speed purposes?,"
I have about 2000 items in my validation set, would it be reasonable to calculate the loss/error after each epoch on just a subset instead of the whole set, if calculating the whole dataset is very slow?
Would taking random mini-batches to calculate loss be a good idea as your network wouldn't have a constant set? Should I just shrink the size of my validation set?
","['machine-learning', 'loss', 'batch-learning', 'validation-loss']",
Is there any difference between ConvNet and CNN?,"
ConvNet stands for Convolutional Networks and CNN stands for Convolutional Neural Networks.
Is there any difference between both?
If yes, then what is it?
If no, is there any reason behind using ConvNet at some places and CNN at some other places in literature?
","['convolutional-neural-networks', 'terminology']",
Why does the VAE using a KL-divergence with a non-standard mean does not produce good images?,"
I know I can make a VAE do generation with a mean of 0 and std-dev of 1.
I tested it with the following loss function:
def loss(self, data, reconst, mu, sig):
    rl = self.reconLoss(reconst, data)
    #dl = self.divergenceLoss(mu, sig)
    std = torch.exp(0.5 * sig)
    compMeans = torch.full(std.size(), 0.0)
    compStd = torch.full(std.size(), 1.0)
    dl = kld(mu, std, compMeans, compStd)
    totalLoss = self.rw * rl + self.dw * dl
    return (totalLoss, rl, dl)

def kld(mu1, std1, mu2, std2):
    p = torch.distributions.Normal(mu1, std1)
    q = torch.distributions.Normal(mu2, std2)
    return torch.distributions.kl_divergence(p, q).mean()

In this case, mu and sig are from the latent vector, and reconLoss is MSE. This works well, and I am able to generate MNIST digits by feeding in noise from a standard normal distribution.
However, I'd now like to concentrate the distribution at a normal distribution with std-dev of 1 and mean of 10. I tried changing it like this:
compMeans = torch.full(std.size(), 10.0)

I did the same change in reparameterization and generation functions. But what worked for the standard normal distribution is not working for the mean = 10 normal one. Reconstruction still works fine but generation does not, only producing strange shapes. Oddly, the divergence loss is actually going down too, and reaching a similar level to what it reached with standard normal.
Does anyone know why this isn't working? Is there something about KL that does not work with non-standard distributions?
Other things I've tried:

Generating from 0,1 after training on 10,1: failed
Generating on -10,1 after training on 10,1: failed
Custom version of KL divergence: worked on 0,1. failed on 10,1
Using sigma directly instead of std = torch.exp(0.5 * sig): failed

Edit 1:
Below are my loss plots with 0,1 distribution.
Reconstruction:

Divergence:

Generation samples:

Reconstruction samples (left is input, right is output):

And here are the plots for 10,1 normal distribution.
Reconstruction:

Divergence:

Generation sample:

Note: when I ran it this time, it actually seemed to learn the generation a bit, though it's still printing mostly 8's or things that are nearly an 8 by structure. This is not the case for the standard normal distribution. The only difference from last run is the random seed.
Reconstruction sample:

Sampled latent:
tensor([[ 9.6411,  9.9796,  9.9829, 10.0024,  9.6115,  9.9056,  9.9095, 10.0684,
         10.0435,  9.9308],
        [ 9.8364, 10.0890,  9.8836, 10.0544,  9.4017, 10.0457, 10.0134,  9.9539,
         10.0986, 10.0434],
        [ 9.9301,  9.9534, 10.0042, 10.1110,  9.8654,  9.4630, 10.0256,  9.9237,
          9.8614,  9.7408],
        [ 9.3332, 10.1289, 10.0212,  9.7660,  9.7731,  9.9771,  9.8550, 10.0152,
          9.9879, 10.1816],
        [10.0605,  9.8872, 10.0057,  9.6858,  9.9998,  9.4429,  9.8378, 10.0389,
          9.9264,  9.8789],
        [10.0931,  9.9347, 10.0870,  9.9941, 10.0001, 10.1102,  9.8260, 10.1521,
          9.9961, 10.0989],
        [ 9.5413,  9.8965,  9.2484,  9.7604,  9.9095,  9.8409,  9.3402,  9.8552,
          9.7309,  9.7300],
        [10.0113,  9.5318,  9.9867,  9.6139,  9.9422, 10.1269,  9.9375,  9.9242,
          9.9532,  9.9053],
        [ 9.8866, 10.1696,  9.9437, 10.0858,  9.5781, 10.1011,  9.8957,  9.9684,
          9.9904,  9.9017],
        [ 9.6977, 10.0545, 10.0383,  9.9647,  9.9738,  9.9795,  9.9165, 10.0705,
          9.9072,  9.9659],
        [ 9.6819, 10.0224, 10.0547,  9.9457,  9.9592,  9.9380,  9.8731, 10.0825,
          9.8949, 10.0187],
        [ 9.6339,  9.9985,  9.7757,  9.4039,  9.7309,  9.8588,  9.7938,  9.8712,
          9.9763, 10.0186],
        [ 9.7688, 10.0575, 10.0515, 10.0153,  9.9782, 10.0115,  9.9269, 10.1228,
          9.9738, 10.0615],
        [ 9.8575,  9.8241,  9.9603, 10.0220,  9.9342,  9.9557, 10.1162, 10.0428,
         10.1363, 10.3070],
        [ 9.6856,  9.7924,  9.9174,  9.5064,  9.8072,  9.7176,  9.7449,  9.7004,
          9.8268,  9.9878],
        [ 9.8630, 10.0470, 10.0227,  9.7871, 10.0410,  9.9470, 10.0638, 10.1259,
         10.1669, 10.1097]])

Note, this does seem to be in the right distribution.
Just in case, here's my reparameterization method too. Currently with 10,1 distribution:
def reparamaterize(self, mu, sig):
        std = torch.exp(0.5 * sig)
        epsMeans = torch.full(std.size(), 10.0)
        epsStd = torch.full(std.size(), 1.0)
        eps = torch.normal(epsMeans, epsStd)
        return eps * std + mu

","['pytorch', 'variational-autoencoder', 'kl-divergence', 'mnist']",
Can I train a DQN on the same dataset for multiple epochs?,"
I am trying to learn about reinforcement learning and chose the stock market to experiment with. I have minute by minute historical data on a particular stock for the past 20 years. I am using a generator to feed the data into my DQN. I've been running some automated tuning on the hyperparameters and seem to have found some good values.
Now I am wondering if I should be training on the dataset more than once or whether that would cause the network to simply memorize past experiences and cause overfitting. Is there a standard practice when it comes to training on historical data in regards to the number of epochs?
Edit: I'm not nessesarily looking for an answer to how many epochs I should be using, rather I'd like to know if running over the same data more than once is okay with DQNs
","['machine-learning', 'reinforcement-learning', 'dqn', 'actor-critic-methods']",
Error in MobileNet V1 Architecture?,"
From the architecture table of the first MobileNet paper, a depthwise convolution with stride 2 and an input of 7x7x1024 is followed by a pointwise convolution with the same input dimensions, 7x7x1024.
Shouldn't the pointwise layer's input be 4x4x1024 if the depthwise conv. layer was stride 2? (Assumming padding of 1)
Is this an error on the author's side? Or are there something that I've missed between these layers?
I've checked implementations of MobileNet V1 and it seems that everyone just treated this depthwise layer's stride as 1.

","['convolutional-neural-networks', 'convolutional-layers']",
"How can I apply naive Bayes classifier for three classes (Positive, Negative and Neutral) in text data?","
I found a naive Bayes classifier for positive sentiment or a negative sentiment Citius: A Naive-Bayes Strategy for Sentiment Analysis on English Tweets. But with most available datasets online, sentiments are classified into 3 types: positive, negative, and neutral.
How does the naive Bayes formula change for such cases? Or does it remain the same, and we only consider the positive and negative to calculate the log likelihoods-likelihoods?
","['sentiment-analysis', 'naive-bayes']",
"What is the consensus on the ""correct"" temperature settings for the AlphaZero algorithm?","
In the AlphaZero learning algorithm, during self-play to generate training games, the move played is chosen with probability proportional to the MCTS visits raised to the $\tau$-th power, where $\tau$ is the so called temperature. Higher temperatures correspond to more exploration. It seems that in deepmind's original paper (on AlphaGo Zero if I'm not mistaken) it is mentioned that temperature is decayed to zero after move 30 in Go/Baduk, then this is contradicted in the AlphaZero with it saying that temperature is not decayed at all, and finally in AlphaZero's pseudocode I believe it is implied that the temperature is decayed after some number of moves. Specifically I believe that lczero concluded that they decayed after 15 moves for chess. It's not clear to me after searching what the current training regime for lczero is with regards to temperature. Also, I believe that ELF openGo efforts used $\tau=1$ for the entire game.
Question: Is there a consensus on what $\tau$ should be? Does it matter if the training is in early phases or not (i.e. if the AI is not advanced yet is it beneficial to explore seemingly ""worse"" moves?) How dependent on the game is this optimal $\tau$? If I have a game which lasts 50 moves average, and I want to decay $\tau$, is there a best practice?
",['alphazero'],
Is it possible to transform audio with neural networks to make it sound like 3d sound,"
so the idea is to feed neural network data like
input: mono audio(extracted from existing 3d audio) output: 3d audio
after training it should convert mono audio to 3d sound
do you think it is possible? does it already implemented?(I didn't found)
P.S
it should sound like https://www.youtube.com/watch?v=kVH_y0rOyGM
not like usual 3d youtube.com/watch?v=QFaSIti5_d0
","['neural-networks', 'tensorflow', 'python', 'keras']",
How is this Pytorch expression equivalent to the KL divergence?,"
I found the following PyTorch code (from this link)
-0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp())

where mu is the mean parameter that comes out of the model and sigma is the sigma parameter out of the encoder. This expression is apparently equivalent to the KL divergence. But I don't see how this calculates the KL divergence for the latent.
","['pytorch', 'proofs', 'implementation', 'variational-autoencoder', 'kl-divergence']","The code is correct. Since OP asked for a proof, one follows.The usage in the code is straightforward if you observe that the authors are using the symbols unconventionally: sigma is the natural logarithm of the variance, where usually a normal distribution is characterized in terms of a mean $\mu$ and variance. Some of the functions in OP's link even have arguments named log_var.$^*$If you're not sure how to derive the standard expression for KL Divergence in this case, you can start from the definition of KL divergence and crank through the arithmetic. In this case, $p$ is the normal distribution given by the encoder and $q$ is the standard normal distribution.
$$\begin{align}
D_\text{KL}(P \| Q) &= \int_{-\infty}^{\infty} p(x) \log\left(\frac{p(x)}{q(x)}\right) dx \\
&= \int_{-\infty}^{\infty} p(x) \log(p(x)) dx - \int_{-\infty}^{\infty} p(x) \log(q(x)) dx
\end{align}$$
The first integral is recognizable as almost definition of entropy of a Gaussian (up to a change of sign).
$$
\int_{-\infty}^{\infty} p(x) \log(p(x)) dx = -\frac{1}{2}\left(1 + \log(2\pi\sigma_1^2) \right)
$$
The second one is more involved.
$$
\begin{align}
-\int_{-\infty}^{\infty} p(x) \log(q(x)) dx 
&= \frac{1}{2}\log(2\pi\sigma_2^2) - \int p(x) \left(-\frac{\left(x - \mu_2\right)^2}{2 \sigma_2^2}\right)dx \\
&= \frac{1}{2}\log(2\pi\sigma_2^2) + \frac{\mathbb{E}_{x\sim p}[x^2] - 2 \mathbb{E}_{x\sim p}[x]\mu_2 +\mu_2^2} {2\sigma_2^2} \\
&= \frac{1}{2}\log(2\pi\sigma_2^2) + \frac{\sigma_1^2 + \mu_1^2-2\mu_1\mu_2+\mu_2^2}{2\sigma_2^2} \\
&= \frac{1}{2}\log(2\pi\sigma_2^2) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2}
\end{align}
$$
The key is recognizing this gives us a sum of several integrals, and each can apply the law of the unconscious statistician. Then we use the fact that $\text{Var}(x)=\mathbb{E}[x^2]-\mathbb{E}[x]^2$. The rest is just rearranging.Putting it all together:
$$
\begin{align}
D_\text{KL}(P \| Q) &= -\frac{1}{2}\left(1 + \log(2\pi\sigma_1^2) \right) + \frac{1}{2}\log(2\pi\sigma_2^2) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} \\
&= \log (\sigma_2) - \log(\sigma_1) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}
\end{align}
$$In this special case, we know that $q$ is a standard normal, so
$$
\begin{align}
D_\text{KL}(P \| Q) &= -\log \sigma_1 + \frac{1}{2}\left(\sigma_1^2 + 
\mu_1^2 - 1 \right) \\
&= - \frac{1}{2}\left(1 + 2\log \sigma_1- \mu_1^2 -\sigma_1^2   \right)
\end{align}
$$
In the case that we have a $k$-variate normal with diagonal covariance for $p$, and a multivariate normal with covariance $I$, this is the sum of $k$ univariate normal distributions because in this case the distributions are independent.The code is a correct implementation of this expression because $\log(\sigma_1^2) = 2 \log(\sigma_1)$ and in the code, sigma is the logarithm of the variance.$^*$The reason that it's convenient to work on the scale of the log-variance is that the log-variance can be any real number, but the variance is constrained to be non-negative by definition. It's easier to perform optimization on the unconstrained scale than it is to work on the constrained scale in $\eta^2$. Also, we want to avoid ""round-tripping,"" where we compute $\exp(y)$ in one step and then $\log(\exp(y))$ in a later step, because this incurs a loss of precision. In any case, autograd takes care of all of the messy details with adjustments to gradients resulting from moving from one scale to another."
BlackOut - ICLR 2016: need help understanding the cost function derivative,"
In the ICLR 2016 paper BlackOut: Speeding up Recurrent Neural Network Language Models with very Large Vocabularies, on page 3, for eq. 4:
$$ J_{ml}^s(\theta) = log \ p_{\theta}(w_i | s) $$
They have shown the gradient computation in the subsequent eq. 5:
$$ \frac{\partial J_{ml}^s(\theta)}{\partial \theta} =  \frac{\partial}{\partial \theta}<\theta_i \cdot s> - \sum_{j=1}^V p_{\theta}(w_j|s)\frac{\partial}{\partial \theta} <\theta_j \cdot s>$$

I am not able to understand how they have obtained this - I have tried to work it out as follows:
from eq. 3 we have
$$ p_{\theta}(w_i|s) = \frac{exp(<\theta_i \cdot s>)}{\sum_{j=1}^V exp(<\theta_j \cdot s>)} $$
re-writing eq. 4, we have:
$$\begin{eqnarray} 
J_{ml}^s(\theta) &=& log \ \frac{exp(<\theta_i \cdot s>)}{\sum_{j=1}^V exp(<\theta_j \cdot s>)}      \nonumber \\
  &=& log \ exp(<\theta_i \cdot s>) - log \ \sum_{j=1}^V exp(<\theta_j \cdot s>) \nonumber  \nonumber
\end{eqnarray}$$
Now, taking derivatives w.r.t. $ \theta $:
$$\begin{eqnarray} 
\frac{\partial}{\partial \theta} J_{ml}^s(\theta) &=& \frac{\partial}{\partial \theta} log \ exp(<\theta_i \cdot s>) - \frac{\partial}{\partial \theta} log \ \sum_{j=1}^V exp(<\theta_j \cdot s>) \nonumber  \nonumber
\end{eqnarray}$$

So, that's it; the second term (after the negative sign), how did that change to the term they have given in eq. 5? Or did I commit a blunder?

Update
I did commit a blunder and I have edited it out, but, the question remains!
correct property:
$$log \ (\prod_{i=1}^K x_i) = \sum_{i=1}^K log \ (x_i)$$
","['papers', 'objective-functions', 'calculus', 'derivative']",
"Is it possible to predict $x^2$, $\log(x)$, or variable function of $x$ using RNN?","
There were some posts that using RNN can predict the next point of the sine wave function with data history.
However, I wondered if it also works on all the functions of $x$, such as $x^2$, $x^3$, $\log(x)$, $\frac{1}{(x+1)}$ functions.
","['neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'function-approximation', 'universal-approximation-theorems']","Yes, RNN can work on the functions you have mentioned. In fact, neural networks can approximate anything (Universal Approximation Theorem). This question also reminds me of Neural Turing Machine.But, it would be a complete waste to use RNNs or NNs for such a task."
Is (log-)standard deviation learned in TRPO and PPO or fixed instead?,"
After having read Williams (1992), where it was suggested that actually both the mean and standard deviation can be learned while training a REINFORCE algorithm on generating continuous output values, I assumed that this would be common practice nowadays in the domain of Deep Reinforcement Learning (DRL).
In the supplementary material associated with the paper introducing Trust Region Policy Optimization (TRPO), however, it is stated that:

A neural network with several fully-connected
(dense) layers maps from the input features to the mean of a Gaussian distribution. A separate set of parameters specifies the log standard deviation of each element. More concretely, the parameters include a set of weights and biases for the neural network computing the mean, $\{W_i , b_i\}_{i=1}^L$ , and a vector $r$ (log standard deviation) with the same dimension as $a$. Then, the policy is defined by the normal distribution $\mathcal{N}(\text{mean}=\text{NeuralNet}(s; \{W_i , b_i\}_{i=1}^L), \text{stdev}=\text{exp}(r))$.

where $s$ refers to a state and $a$ to a predicted action (respectively a vector of actions if multiple outputs are generated concurrently).
To me this suggests that the standard deviation stdev (being a function of $r$) is actually not learned when training a TRPO agent, but that it is solely determined by some possibly constant vector $r$.
Since I found the idea of adjusting both the mean and standard deviation together when training a REINFORCE agent quite reasonable, I got wondering whether it is actually true that TRPO agents do not treat the standard deviation for sampling output values as a trainable parameter, but just as a function of the state-independent vector $r$.
(Pretty much the same shall then apply to Proximal Policy Optimization (PPO) agents as well, since they are reported to follow TRPO's model architecture in the continuous output case.)
In search for an answer, I browsed OpenAI's baselines repository containing reference implementations of both TRPO and PPO.
In my understanding of their code, the code seems to confirm my assumption that standard deviation is a non-trainable parameter and that it is, instead of being trainable, taken to be a constant.
Now, I was wondering whether my understanding of the procedure how TRPO (and PPO) computes standard deviation(s) is correct or whether I misunderstood or overlooked something important here.
","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization', 'trust-region-policy-optimization']",
Would it be possible to determine the dataset a neural network was trained on?,"
Let's say we have a neural network that was trained with a dataset $D$ to solve some task. Would it be possible to ""reverse-engineer"" this neural network and get a vague idea of the dataset $D$ it was trained on?
","['neural-networks', 'machine-learning', 'datasets', 'generative-model']","You can already do this with some neural networks, such as GANs and VAEs, which are generative models that learn a probability distribution over the inputs, so they learn how to produce e.g. images that are similar to the images they were trained with.Now, if you're interested in whether there is a black-box method, i.e. a method that, for every possible neural network, would tell you the dataset a neural network was trained with, that seems to be a harder task and definitely an ill-posed problem, but I suspect that people working on adversarial machine learning have already attempted or will attempt to do something similar."
How to keep track of the subject/entity in a sentence?,"
I'm working on Sentiment Analysis, using HuggingFace to perform sentiment analysis on articles
 classifier = pipeline('sentiment-analysis', model=""nlptown/bert-base-multilingual-uncased-sentiment"")
 classifier(['We are very happy to show you the ğŸ¤— Transformers library.',  ""We hope you don't hate it.""])

This returns

label: POSITIVE, with score: 0.9998


label: NEGATIVE, with score: 0.5309

Now I'm trying to understand how to keep track of a subject when performing the sentiment analysis.
Suppose I'm given a sentence like this.

StackExchange is a great website. It helps users answer questions.  Hopefully, someone will help answer this question.

I would like to keep track of the subject when performing sentiment analysis. In the example above, in the 2nd sentence 'it' refers to 'StackExchange'. I would like to be able to do track a subject between sentences.
Now, I could try to manually try to parse this by finding the verb and trying to figure find the phrase that comes before it. However, it doesn't sound like a very safe or accurate way to find the subject.
Alternatively, I could train similar to a Named Entity Recognition. However, finding a dataset for this is very hard, and training it would be very time-consuming.
How can I keep track of an entity within an article?
","['natural-language-processing', 'bert', 'sentiment-analysis']","What you're describing is known as coreference resolution. More specifically, this example is anaphora resolution. The short answer is that this is an open research question and there is no well-established solution.You mentioned Hugging Face in your question. The neuralcoref module in spaCy is itself from Hugging Face (note the reflexive anaphor used for emphasis in this sentence). If you're not a spaCy kind of person, then there's also Stanford's CoreNLP in Java that has coreference resolution. A Python wrapper is also available.I also wanted to address a couple other topics you mentioned. You are right in that they're all somewhat connected. But you need to scope down your goal/research question because what you're aiming to achieve is too difficult for a first task. Named entity recognition, constituency parsing, and sentiment analysis. Pick just one to focus on."
How exactly is Monte Carlo counterfactual regret minimization with external sampling implemented?,"
I have read many papers, such as this or this, explaining how external sampling works, but I still don't understand how the algorithm works.
I understand you divide $Q$, which is the set of all terminal histories into subsets $Q_1,..., Q_n$.
What is the probability of reaching some subset $Q_i$? Is it just the product of chance probability, the opponent's probability, and my probability?
As I understand it, the sampling only occurs in the opponent's information sets. How does that work? If there are two players, player 1 strategy is based on what strategy I use.
What happens after you have determined a subset $Q_i$ you want to sample? How many times do you iterate over the subset $Q_i$?
I have searched around and I cannot find any Python code that uses external sampling, but plenty of papers that give formulas, but do not explain the algorithm in detail. So, a Python example of MC-CFR external sampling would probably make it a lot easier for me to understand the algorithm.
","['python', 'papers', 'monte-carlo-methods', 'game-theory', 'games-of-chance']",
"Best Machine Learning Model for ""Predicted"" Image Generation","
I am currently working on undergraduate research to determine hotspots for hand-surface contact. Ideally, I would like to give the model a depth image as input:

Example of synthetic depth image
and return an image mask indicating where the surface was touched:

Example of synthetic contact mask
I have worked with Machine Learning before but am struggling to determine what model I should use. My understanding is that CNNs are typically intended for classification tasks. And while GANs are used to generate new images, they can produce these images independently of an input. Assuming I have a large dataset of depth images and the respective black and white contact mask, what model can be used to efficiently predict a contact mask given an unseen depth image?
","['convolutional-neural-networks', 'generative-adversarial-networks', 'image-processing', 'image-generation']",
Confusion about computing policy gradient with automatic differentiation ( material from Berkeley CS285),"
I am taking Berkeley’s CS285 via self-study. On this particular lecture regarding Policy Gradient, I am very confused about the inconsistency between the concept explanation and the demonstration of code snippet. I am new to RL and hope someone could clarify this for me.
Context
1.The lecture defines policy gradient as follow:

log(pi_theta(a | s)) denotes the log probability of action given state under policy parameterized by theta
gradient log(pi_theta(a |s)) denotes the gradient of parameter theta with respect to the predicted log probability of action
2.The lecture defines a pseudo-loss.By auto differentiate the pseudo-loss, we recovery the policy gradient.

Here Q_hat is short hand of sum of r(s_i_t, a_i_t) in the Policy gradient equation under 1)

The lecture then proceeds to gives a pseudo-code implementation of 2)


My confusion



From 1) above,
gradient log(pi_theta(a |s)) denotes the gradient of parameter theta with respect to the predicted log probability of action , not a loss value calculated from a label action and predicted action.
Why does the below in 2) implies that gradient log(pi_theta(a |s)) just morph into output of loss function instead of just predicted action probability as defined in 1) ?

2.
In this pseudo-code implementation,

Particularly, this line below.
negative_likelihoods = tf.nn.softmax_cross_entrophy_with_logis(labels=actions, logits=logits)


Where does the actions even coming from ? If it comes from collected trajectory, aren’t the actions result of logits = policy.predictions(states) to begin with ?
Then won’t tf.nn.softmax_cross_entrophy_with_logis(labels=actions, logits=logits) always return 0 ?

Based on the definition of policy gradient in 1), shouldn’t the implementation of pseudo-loss be like below ?


# Given:
# actions - (N*T) x Da tensor of actions
# states - (N*T) x Ds tensor of states
# q_values – (N*T) x 1 tensor of estimated state-action values
# Build the graph:
logits = policy.predictions(states) # This should return (N*T) x Da tensor of action logits 
weighted_predicted_probability = tf.multiply(torch.softmax(logits), q_values)
loss = tf.reduce_mean(weighted_predicted_probability )
gradients = loss.gradients(loss, variables)


","['reinforcement-learning', 'deep-rl', 'policy-gradients']",
Is there a graph neural network algorithm that can deal with a different number of input and output nodes?,"
I am new to graph neural networks and their applications. I have an input graph $G = \{V, E\}$ and an output graph $G' = \{V', E'\}$ where the number of nodes $V$ and $V'$ are different. I am trying to learn the function where $f(G) = G'$ and $V > V'$, thus, the function is mapping many-to-one ($n$ number of nodes map to one). The Graph Convolution Network (GCN) seems to have the same number of nodes in input and output with the function being learnt. Could I utilize the GCN for my task?
","['neural-networks', 'reference-request', 'graph-neural-networks', 'model-request', 'algorithm-request']","I suggest you look into link prediction. I have had good luck with the StellarGraph library. They have several algorithms implemented, including GCN.Link prediction is a binary classification problem. Given two nodes, $v_i$ and $v_j$, does there exist a link between them? Using a library like StellarGraph will also produce node embeddings while performing link prediction.For you scenario I'm picturing a three step process:In the link prediction tasks that I've outlined, you can use GCN with StellarGraph. So there should be no problems in terms of the number of nodes."
"Is there a machine learning model that can be trained with labels that only say how ""right"" or ""wrong"" it was?","
I'm trying to find the name for a model that is used to output a decision (maybe something like right, left, or do nothing = -1, 0,1) but that can be trained with labels that contain how ""correct"" or ""incorrect"" it was. I've tried to google around and ask some friends in my machine learning class, but no one seems to have an answer.
The classic example I seem to always see is the models used in the snake game. We don't know what the right decision was per se, but we can say that if it ran into the wall, that was really wrong. Or if it got an apple and gained 50 points, then it was correct and if it got 2 apples and gained 100 points then it was even more correct, etc.
I'm looking for a network where the exact labels don't exist, but where we can penalize or reward its decisions.
I'm assuming this requires some kind of modified cost function, but I would imagine this type of network already exists. I'm hoping someone can provide me with the name for this type of network and whether or not there is a Keras frontend for something like this.
","['machine-learning', 'reinforcement-learning', 'tensorflow', 'deep-rl', 'models']","What you are looking for is called ""reinforcement learning"".A reinforcement learning algorithm will try to maximize a reward function. This reward represents how ""good"" or ""bad"" an action is in the actual context. For example, in the snake game, your reward will be positive for eating an apple and negative when the snake hits a wall.The interesting thing is that, with reinforcement learning, you can learn without having a reward at each step. In the case of the snake game, your agent can learn that going in the direction of the apple is better than going in the direction of the wall, even if none of this action will directly give a reward (positive or negative).If you want to use a neural network as your post seem to imply then you should look at deep Q-learning, a reinforcement learning algorithm, which use a neural network to learn to predict the expected reward of a couple (state, action)."
Does the weight vector form imply feature space curvature?,"
I came across this sentence when exploring a simple nearest neighbor classifier method using Euclidean distance (link):

The slightly odd thing about using the Euclidean distance to compare features is that each dimension (or feature) is compared using the same scale.

This got me thinking - if flat feature space implies that each feature contributes equally to the distance (score function), then curved feature space changes the scale between features, so that the features then contribute different amounts to the score function. For example, imagine we have a 2D feature space - a flat piece of paper - with two points, $X_1$ and $X_2$ on it, between which we wish to calculate the distance. If we then bend this into U-shape along, say, y-axis (so, no curvature introduced in y-dimension), the distances along the x-axis would be larger in the bent case than in the flat case:

In other words, feature x would contribute more to the score function than feature y. This sounds awfully like weighing the feature inputs with weight vectors. Does this imply, that weight vectors (and matrices) have a direct effect on curvature of feature space? Does an identity weight matrix (or a vector of all 1s) imply our feature space is flat (and curved otherwise)? Lastly, could it then be said that whenever we are training an ML model, we are in fact learning the approximate curvature of the feature space we wish to model?
","['weights', 'features']",
CNN leaf segmentation throught classification of edges how to improve,"
I am trying to design a CNN that can do pixel wise segmentation of edges leaves in dense foliage agriculture images. Such as these:

On the basis of this article https://arxiv.org/pdf/1904.03124.pdf, two classes are defined, such as the external contours and the internal contours of the leaves boundaries. In addition, a multi-scale approach is used trough a unet like architecture and a auxiliary loss was used to learn the edge detection at different scales (which corresponds to the main idea of this article https://arxiv.org/pdf/1804.01646.pdf). I learn the network using a mIoU loss whose weight varies depending on the class and the scale. Finally, my last activation layer is a clipped ReLU. The results are starting to be good :

However, the network is not able to reconnect some internal edges. The following image shows a broken inner edge that does not continue to the next part (in blue) :

So I'm looking for some paper, git, codes, whatever that can improve the reconstruction of missing edges (inside or outside the CNN).
","['convolutional-neural-networks', 'computer-vision', 'tensorflow']",
What approach to use for selecting one of the category according to short category text?,"
I need some tool to classify articles based on short category text which consists of two or three words separated by '-'. The RSS/XML tag content is for example:

Foreign - News


Football - Foreign

I created my own categories and now I need to classify categories from parsed RSS of this news source, so it fits news categories defined by me.
I would for example need all articles containing category ""football"" to be identified as a category Sport but sometimes those categories XML tags contains exact match like Foreign - News should belong in the DB to category defined by me as Foreign.
I can of course also use longer description text if that would be needed but I think for this simple problem that would not be even necessary.
Since I used only trained decision trees frameworks so far for another project, I would like to hear advice about approach, AI technique or particular framework I can use to solve this problem. I don't want to get into a dead-end street by my own poor in this field not experienced decision.
","['machine-learning', 'classification', 'python', 'algorithm', 'text-classification']","For this application, you can frame it as text classification. Look at SpaCy. You just need to create embeddings for your text and put a Softmax in the end. You can get those embeddings from BERT or anything else out there. You can in fact just use GLOVE vectors and others like it, concatenate them and then train a classifier."
Is the framework provided by this paper for checking the constraints of AI systems really new?,"
The authors of this paper present a framework for checking the constraints of AI systems using formal argumentative logic between 2 agents: an interrogating agent and a suspect agent. The interrogating agent is attempting to find contradictions in the responses of the suspect agent by querying about information and the suspect agent must provide all the relevant information.
Is this framework really new? I am pretty certain that I saw a very similar framework in the context of program verification some years ago.
","['neural-networks', 'papers', 'research']",
What is the definition of the hinge loss function?,"
I came across the hinge loss function for training a neural network model, but I did not know the analytical form for the same.
I can write the mean squared error loss function (which is more often used for regression) as
$$\sum\limits_{i=1}^{N}(y_i - \hat{y_i})^2$$
where $y_i$ is the desired output in the dataset, $\hat{y_i}$ is the actual output by the model, and $N$ is the total number of instances in our dataset.
Similarly, what is the (basic) expression for hinge loss function?
","['neural-networks', 'definitions', 'support-vector-machine', 'binary-classification', 'hinge-loss']",
What is the paper that states that humans incorrectly trust the incorrect explanations of the AI?,"
I was reading a paper on the subject of explainable AI and interpretability, in particular the tendency of people (even experts) to excessively trusting explanations given by AI. In the intro the author describes riding in a self-driving car with a screen on the passenger side that depicts the car's vision and classification of the objects on the road, ostensibly to improve the level of trust in the car's decision-making. Later the author quotes a study in which experts in a field give good ratings to an AI's explanations for its decision-making, even when the explanations given are intentionally incorrect.
I cannot for the life of me remember which paper this is or what it covers in its main sections, and after searching through dozens of my saved papers as well as online search engines I cannot recover it. The paper also mentions a specific term for trusting machines/AI, which I also can't remember and would definitely help me find the paper if I could.
If anyone is familiar with this paper or the study it quotes, I would really appreciate a link.
","['reference-request', 'papers', 'explainable-ai']",
What is the relation between the context in contextual bandits and the state in reinforcement learning?,"
Conceptually, in general, how is the context being handled in contextual bandits (CB), compared to states in reinforcement learning (RL)?
Specifically, in RL, we can use a function approximator (e.g. a neural network) to generalize to other states. Would that also be possible or desirable in the CB setting?
In general, what is the relation between the context in CB and the state in RL?
","['reinforcement-learning', 'terminology', 'function-approximation', 'state-spaces', 'contextual-bandits']","The notion of a state in reinforcement learning is (more or less) the same as the notion of a context in contextual bandits. The main difference is that, in reinforcement learning, an action $a_t$ in state $s_t$ not only affects the reward $r_r$ that the agent will get but it will also affect the next state $s_{t+1}$ the agent will end up in, while, in contextual bandits (aka associative search problems), an action $a_t$ in the state $s_t$ only affects the reward $r_r$ that you will get, but it doesn't affect the next state the agent will end up in. The typical problem that can be formulated as a contextual bandit problem is a recommender system.In CBs, like in RL, the agent also needs to learn a policy, i.e. a function from states to actions, but actions that you take in a certain state are independent of the actions you take in other states.So, as Sutton and Barto put it (2nd edition, section 2.9, page 41), contextual bandits are an intermediate problem between (context-free) bandits (where there is only one state or, equivalently, no state at all) and the full reinforcement learning problem.Another important characteristic of many RL algorithms, such as Q-learning, is that they assume that the state is Markov, i.e. it contains all necessary info to take the optimal action, but, of course, RL is not just applicable to fully observable MDPs. In fact, even Q-learning has been applied to POMDPs, with some approximations and tricks.Regarding the use of neural networks to approximate $q(s, a)$ or a policy in CBs, in principle, this is possible. However, given that the optimal action in a state $s$ is independent of the optimal action in another state $s'$, this is probably not useful, but I cannot guarantee you that this has not been successfully done, because I've not yet read the relevant literature (maybe someone else will provide another answer to address this aspect)."
Incorrect node expansion in game board with A* search,"
I have the following game board below, and we're using A* search to find the optimal path from the agent to the key. There are 8 directions. Up, down, left, right have a cost of 1, and diagonal directions have cost 3. We will be using a priority queue with function $f(v) = g(v) + h(v)$ where $g(v)$ is the backwards cost from the goal through the given edges and up to the vertex v while $h(v)$ is the optimal least cost distance from v to the goal node.

So I calculated the f(s) for the different states, assuming no prior edges specified:

And then I started the search and these are the steps I took: expand C: (CD,3), (CE,3), (CF,3), (CA,5), (CB,5)
expand CD: (CDF,3),(CE,3), (CF,3), (CA,5), (CB,5), (CDB,5)
expand CDF: (CDFH,3), (CE,3), (CF,3), (CA,5), (CB,5), (CDB,5), (CDFG,6)
expand CDFH: (CE,3), (CF,3), (CA,5), (CB,5), (CDB,5), (CDFG,6)
So I only expanded, C,D,F,H. I got the correct answer for the optimal path, but not the correct answer for nodes expanded, which is supposed to be C, D, E, F, G, H. What am I doing wrong?
","['game-ai', 'search', 'heuristics', 'graphs', 'a-star']",
Understanding policies in helicopter control in the paper by Andrew Ng et al,"
I was going through this paper on helicopter flight control using reinforcement learning by Andrew Ng et al.
It defines two policy classes to learn two policies, one for hovering the helicopter and another for maneuvering (aka trajectory following). The goal for hovering policy is defined as follows:

We want a controller that, given the current helicopter state and a desired hovering position and orientation $\{x^*, y^*, z^*, \omega^*\}$ computes controls $a\in [-1,1]^4$ to make it hover stably.

The goal for maneuvering policy is given in term of hovering policy as follows:

Given a controller for keeping a system’s state at a point $(x^*,y^*,z^*,\omega^*)$, one standard way to make the system move through a particular trajectory is to slowly vary $(x^*,y^*,z^*,\omega^*)$ along a sequence of set points on that trajectory points.

The neural network for these policy classes is shown as follows:

In this, $(\dot{x},\dot{y},\dot{z})$ are velocity estimates, $(\phi,\theta)$ is helicopter roll and pitch estimates and $\dot{w}$ is angular velocity component estimate (more on this at the bottom of page 1 of the linked paper).
Each edge with an arrow in the picture denotes a tunable parameter. The solid lines show the hovering policy class. The dashed lines show the extra weights added for trajectory
following (maneuvering). With this observation, I had following doubts:
Q1. Does addition of dashed lines to hovering policy to get maneuvering policy makes manuvering policy superset of hovering policy?
Q2. Rephrasing Q1: can we use maneuvering policy for hovering task (say by setting weights corresponding to dashed lines to zero)?
Q3. If maneuvering policy is indeed a superset of hovering policy, why the authors dont use just maneuvering policy for both tasks or maneuvering policy for hovering task also? Is it because it involves computation involving helicopter's additional sub dynamics represented by dashed line and this additional computation is not required for hovering task?
Or am I completely getting wrong with all these questions?
","['neural-networks', 'reinforcement-learning', 'deep-learning', 'deep-rl', 'papers']",
How would one disambiguate between two meanings of the same word in a sentence?,"

The boy lifted the bat and hit the ball.

In the above sentence, the noun ""bat"" means the wooden stick. It does not mean bat, the flying mammal, which is also a noun. Using NLP libraries to find the noun version of the definition would still be ambiguous.
How would one go about writing an algorithm that gets the exact definition, given a word, and the sentence it is used in?
I was thinking you could use word2vec, then use autoextend https://arxiv.org/pdf/1507.01127.pdf to differentiate between 2 different lexemes e.g. bat (animal) and bat (wooden stick).
Then the closest cosine distance between the dictionary definition and any of the words of the sentence might indicate the correct definition.
Does this sound correct?
","['natural-language-processing', 'natural-language-understanding', 'word2vec', 'semantic-networks']",
How UCT in MCTS selection phase avoids starvation?,"
The first step of MCTS is to keep choosing nodes based on Upper Confidence Bound applied to trees (UCT) until it reaches a leaf node where UCT is defined as
$$\frac{w_i}{n_i}+c\sqrt{\frac{ln(t)}{n_i}},$$
where

$w_i$= number of wins after i-th move
$n_i$ = number of simulations after the i-th move
$c$ = exploration parameter (theoretically equal to $\sqrt{2}$)
$t$ = total number of simulations for the parent node

I don't really understand how this equation avoids sibling nodes being starved, aka not explored. Because, let's say you have 3 nodes, and 1 we'll call it node A is chosen randomly to be explored, and just so happens to simulate a win. So, node A's UCT$=1+\sqrt(2)\sqrt{\frac{ln(1)}{1}}$, while the other 2 nodes UCT = 0, because they are unexplored and the game just started, so by UCT the other 2 nodes will never be explored no? Because after this it'll go into the expansion phase and expansion only happens it reaches a leaf node in the graph. So because node A is the only one with a UCT $> 0$ it'll choose a child of node A and it will keep going down that node cause all the siblings of node A have a UCT of 0 so they never get explored.
","['monte-carlo-tree-search', 'upper-confidence-bound']","First explore the nodes A,B,C once.For reference see this paper by David Silver and Sylvain Gelly, Combining Online and Offline Knowledge in UCTIf any action from the current state $s$ is not represented in the tree, $\exists a \in \mathcal{A}(s),(s, a) \notin \mathcal{T},$ then the uniform random policy $\pi_{\text {random }}$ is used to select an action from all unrepresented actions, $\tilde{\mathcal{A}}(s)=\{a \mid(s, a) \notin \mathcal{T}\}$."
Extracting information from RNA sequence,"
I am relatively new to machine learning, and I am trying to use a deep neural network to extract some information from sequences of RNA.
A quick overview of RNA: there is both sequence and structure. I am currently expressing the sequence with one-hot encoding (so a sequence of length $60$ would be expressed as a $60 \times 4$ matrix, with one row for each letter of the sequence, and one column for each possible value of that letter). I am also feeding the 2D structure of the RNA into the network, which is expressed as $60 \times 60$ matrix for a sequence of length $60$.
I am trying to use these inputs to predict a single continuous value for a given sequence.
Currently, I am using pretty much the exact setup from this tutorial. I chose this architecture because it allows me to separate the inputs (sequence and structure) and have individual layers for them before merging them into a single model. I think this makes more sense than trying to glue the two separate pieces of data together into a single input.
However, the model doesn't seem to be learning anything - validation loss decreases very slightly then plateaus.
If anyone has suggestions, especially someone who has worked with RNA, DNA, or proteins before, I would really, really appreciate it. I am new to this, and I am not sure how to improve my model from here.
def create_mlp(height,width,filters=(16, 16, 32, 32, 64), regress=False):
    # initialize the input shape and channel dimension, assuming
  # TensorFlow/channels-last ordering
  inputShape = (height, width)
  chanDim = -1
  # define the model input
  inputs = Input(shape=inputShape)
  # loop over the number of filters
  for (i, f) in enumerate(filters):
    # if this is the first CONV layer then set the input
    # appropriately
    if i == 0:
      x = inputs
    # CONV => RELU => BN => POOL
    x = Conv1D(f, 3, padding=""same"")(x)
    x = Activation(""relu"")(x)
    x = BatchNormalization(axis=chanDim)(x)
    x = MaxPooling1D(pool_size=2)(x)
  # flatten the volume, then FC => RELU => BN => DROPOUT
  print(x.shape)
  x = Flatten()(x)
  x = Dense(16)(x)
  x = Activation(""relu"")(x)
  x = BatchNormalization(axis=chanDim)(x)
  x = Dropout(0.5)(x)
  # apply another FC layer, this one to match the number of nodes
  # coming out of the MLP
  x = Dense(4)(x)
  x = Activation(""relu"")(x)
  # check to see if the regression node should be added
  if regress:
    x = Dense(1, activation=""linear"")(x)
  # construct the CNN
  model = Model(inputs, x)
  # return the CNN
  return model

def create_cnn(width, height, depth, filters=(16, 16, 32, 32, 64), regress=False):
  # initialize the input shape and channel dimension, assuming
  # TensorFlow/channels-last ordering
  inputShape = (height, width, depth)
  chanDim = -1
  # define the model input
  inputs = Input(shape=inputShape)
  # loop over the number of filters
  for (i, f) in enumerate(filters):
    # if this is the first CONV layer then set the input
    # appropriately
    if i == 0:
      x = inputs
    # CONV => RELU => BN => POOL
    x = Conv2D(f, (3, 3), padding=""same"")(x)
    x = Activation(""relu"")(x)
    x = BatchNormalization(axis=chanDim)(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
  # flatten the volume, then FC => RELU => BN => DROPOUT
  print(x.shape)
  x = Flatten()(x)
  x = Dense(16)(x)
  x = Activation(""relu"")(x)
  x = BatchNormalization(axis=chanDim)(x)
  x = Dropout(0.5)(x)
  # apply another FC layer, this one to match the number of nodes
  # coming out of the MLP
  x = Dense(4)(x)
  x = Activation(""relu"")(x)
  # check to see if the regression node should be added
  if regress:
    x = Dense(1, activation=""linear"")(x)
  # construct the CNN
  model = Model(inputs, x)
  # return the CNN
  return model

mlp = create_mlp(l, 4, regress=False)
  cnn = create_cnn(l, l, 1, regress=False)
  # create the input to our final set of layers as the *output* of both
  # the MLP and CNN
  #cnn.output.reshape()
  combinedInput = concatenate([mlp.output, cnn.output])

","['neural-networks', 'tensorflow', 'prediction', 'biology']",
What is the reverse of passing a Turing test by a human pretending to be a robot that can't be identified?,"
A Turing Test is a method of inquiry for determining whether or not a computer is capable of thinking like a human being. In an ideal Turing test, it would be clear to differentiate between a real human being and a robot or AI with human characteristics.
However, it is also possible in a Turing test that a human tries to mimic the behaviour of a computer so that the person applying the test cannot distinguish between a human being and a robot/AI.
Is this a concept that is explored much in computer science? As in research into the variations of Turing tests that can be used to identify whether a human is trying to mimic or impersonate as a robot or AI.
","['research', 'human-like', 'turing-test', 'robots']",
classification of unseen classes of image in open set classification,"
I have a scanned image, and they need to be classified in one of the pre-defined image classes, so that it can be sorted. However, the problem is the open nature of the classes. At testing time, new classes of scanned images can be added and the model should not only classify them as unseen (open set image recognition), but it should be able to tell in which new class it should belong (not able to figure out the implementation for this.)
So, I am thinking that the below option can work for the classification of unseen classes

Zero-shot learning: Once the image is classified as unseen, we can then apply zero-shot learning to find its respective class for sorting.

Template matching: Match the test image of unseen classes with all available class images, and, once we have a match, we can do sorting of images.

Meta learning-based approach: I am not sure how to implement this, suggestions are much appreciated.


Note: I already tried the classical computer vision approach, but it's not working out. So, more open for neural net-based approach.
Is my approach to solving the problem correct? If possible, suggest some alternative to find the corresponding match/classification of the unseen class image. As I could think of these 2 alternative solutions only.
","['neural-networks', 'classification', 'representation-learning', 'active-learning', 'zero-shot-learning']",
Which metric should I use to assess the quality of the clusters?,"
I have a model that outputs a latent N-dimensional embedding for all data points, trained in a way that clusters data-points from the same class together, while being separated from other clusters belonging to other different classes.
The N-dimensional embedding is projected down to 2D using UMAP. At each epoch, I wish to test the clustering capability of the model on these 2D projections for use as validation accuracy. I have the labels for each class.
How should I proceed?

","['unsupervised-learning', 'clustering', 'metric', 'umap']",
How do you calculate KL divergence on a three-dimensional space for a Variational Autoencoder?,"
I'm trying to implement a variational auto-encoder (as seen in Section 3.1 here: https://arxiv.org/pdf/2004.06271.pdf).
It differs from a traditional VAE because it encodes its input images to three-dimensional latent feature maps. In other words, the latent feature maps have a width, height and channel dimension rather than just a channel dimension like a traditional VAE.
When calculating the Kullback-Liebler divergence as part of the loss function, I need the mean and covariance that is the output of the encoder. However, if the latent feature maps are three-dimensional, this means that the output of the encoder is three-dimensional, and therefore each latent feature is a 2D matrix.
How can I derive a mean and covariance from a 2D matrix to calculate the KL divergence?
","['convolutional-neural-networks', 'computer-vision', 'math', 'variational-autoencoder', 'kl-divergence']","Your three dimensional latent representation consists of two images of mean pixels and covariance pixels as shown in Fig. 3. Which represents a Gaussian distribution with the mean and covariance for each pixel in the latent representation. Each pixel value is a random variable.Now, have a close look at KL-loss Eq. 3 and it's corresponding description in the paper:$$\mathcal{L}_{KL} = \frac{1}{2 \times (\frac{W}{16} \times \frac{H}{16}) } \sum^M_{m = 1}[\mu^2_m + \sigma^2_m - \log(\sigma^2_m) - 1]$$Finally, $M$ is  the  dimensionality  of  the  latent  features $\theta \in \mathbb{R}^M$ with  mean $\mu  = [\mu_1,...,\mu_M]$ and covariance matrix $\Sigma = \text{diag}(\sigma_1^2,...,\sigma_M^2)$, [...].The covariance matrix is diagonal, thus all pixel values are independent of each other. That is the reason why we have this nice analytical form for the KL-divergence given by Eq. 3. Therefore you can treat your 2D random matrix simply as a random vector of size $M = \frac{W}{16} \times \frac{H}{16}$ ($\times 3$ if you like to include color dimension). The third dimension (RGB channel) can be considered independent as well, therefore it can be also flattened to a vector and appended. Indeed this is what is done in the paper indicated by the second half of the sentence from above:that are reparameterized by via sampling from a standard multivariate Gaussian $\epsilon \sim \mathcal{N}(0,I_M)$, i.e. $\theta = \mu + \Sigma^{\frac{1}{2}}\epsilon$."
Using one-class classification first to find anomalies then apply multi-class classification,"
I'm new to machine learning and trying to apply it for fault detection, an idea came to mind which is using only anomaly detection after which if the results after a while come up as positive, a multi-class classification algorithm (using 7 different classes) is used to classify the fault type. would that be efficient and saves on resources power?
","['classification', 'multiclass-classification']",
Can the quality of randomness in neural network initialization affect model fitting?,"
This is a topic I have been arguing about for some time now with my colleagues, maybe you could also voice your opinion about it.
Artificial neural networks use random weight initialization within a certain value range. These random parameters are derived from a pseudorandom number generator (Gaussian etc.) and they have been sufficient so far.
With a proper sample simple, pseudorandom numbers can be statistically tested that they are in fact not true random numbers. With a huge neural network like GPT-3 with roughly 175 billion trainable parameters, I guess that if you would use the same statistical testing on the initial weights of GPT-3 you would also get a clear result that these parameters are pseudorandom.
With a model of this size, could in theory at least the repeatable structures of initial weights caused by their pseudorandomness affect the model fitting procedure in a way that the completed model would be affected (generalization or performance-wise)? In other words, could the quality of randomness affect the fitting of huge neural networks?
","['neural-networks', 'weights-initialization', 'randomness']",
What are the differences between an agent and a model?,"
In the context of Artificial Intelligence, sometimes people use the word ""agent"" and sometimes use the word ""model"" to refer to the output of the whole ""AI-process"". For examples: ""RL agents"" and ""deep learning models"".
Are the two words interchangeable? If not, in what case should I use ""agents"" instead of ""models"" and vice versa?
","['terminology', 'definitions', 'models', 'intelligent-agent']","The other answer defines an agent as a policy (as it's defined in reinforcement learning). However, although this definition is fine for most current purposes, given that currently agents are mainly used to solve video games, in the real world, an intelligent agent will also need to have a body, which Russell and Norvig call an architecture (section 2.4 of the 3rd edition of Artificial Intelligence: A Modern Approach, page 46), which should not be confused with an architecture of a model or neural network, but it's the computing device that contains the physical sensors and actuators for the agent to sense and act on the environment, respectively. So, to be more general, the agent is defined as followsagent = body + policy (brain)where the policy is what Russell and Norvig call the agent program, which is an implementation of the agent function.Alternatively, it can be defined as followsAn agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.This is just another definition given by Russell and Norvig, which I also report in this answer, where I describe different types of agents. Note that these definitions are equivalent. However, in the first one, we just emphasize that we need some means to ""think"" (brain) and some means to ""behave"" (body).These definitions are quite general, so I think people should use them, although, as I said above, sometimes people refer to an agent as just the policy.In this answer, I describe what a model is or what I like to think a model is, and how it is different from a function.In AI, a model can refer to different but somehow related concepts.For example, in reinforcement learning, a model typically refers to $p(s', r \mid s, a)$, i.e. the joint probability distribution over the next state $s'$ and reward $r$, given the current state $s$ and action $a$ taken in $s$.In deep learning, a model typically refers to a neural network, which can be used to compute (or model) different functions. For example, a neural network can be used to compute/represent/model a policy, so, in this case, there would be no actual difference between a model and an agent (if defined as a policy, without a body). However, conceptually, at a higher-level, these would still be different (in the same way that biological neural networks are different from the brain).More generally, in machine learning, a model typically refers to a system that can be changed to compute some function. Examples of models are decision trees, neural networks, linear regression models, etc. So, as I also state in the other answer, I like to think of a model as a set of functions, so, in this sense, a model would be a hypothesis class in computational learning theory. This definition is roughly consistent with $p(s', r \mid s, a)$, which can also be thought of as a (possibly infinite) set of functions, but note that a probability distribution is not exactly a set of functions.In the context of knowledge bases, a model is an assignment to the variables, which represents a ""possible world"". See section 7.3, page 240, of the cited book.There are possible other uses of the word model (both in the context of AI, e.g. in the context of planning, there's often the idea of a conceptual model, which is similar to an MDP in RL, and in other areas), but the definitions given above should be more or less widely applicable in their contexts.Given that there are different possible definitions of a model depending on the context, it's not easy to briefly state what the difference between the two is.So, here's the difference in the context of RL (and you can now find out the differences in other contexts by using the different definitions): an agent can have a model of the world, which allows it to predict e.g. the reward it will receive given its current state and some action that it decides to take. The model can allow the agent to plan. In this same context, a model could also refer to the specific system (e.g. a neural network) used to compute/represent the policy of the agent, but note that people usually refer to $p(s', r \mid s, a)$ when they use the word model in RL. See this post for more details."
What kind of deep learning model does latest version of AlphaFold use for protein folding problem?,"
I understand there are multiple versions used in AlphaFold. What kind of deep learning model does the more advanced version use? CNN, RNN, or something else?
(Additionally, is there an open-source reference model for the protein folding problem?)
","['deep-learning', 'alpha-fold']",
What does the outputlayer of BERT for masked language modelling look like?,"
In the tutorial BERT – State of the Art Language Model for NLP the masked language modeling pre-training steps are described as follows:

In technical terms, the prediction of the output words requires:

Adding a classification layer on top of the encoder output.

2.Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.
3.Calculating the probability of each word in the vocabulary with softmax.

In the Figure below this process is visualized and also from the tutorial.
I am confused about what exactly is done. Does it mean that each output vector O is fed into a fully connected layer with embedding_size neurons and then multiplied by the embedding matrix from the input layer?
Update:
In the tutorial The Illustrated GPT-2 (Visualizing Transformer Language Models) I found an explanation for GPT-2 which seems to be similar to my question.
In the tutorial is said that each output vector is multiplied by the input embedding matrix to get the final output.
Does the same mechanic apply to BERT?

","['natural-language-processing', 'transformer', 'attention', 'word-embedding', 'bert']",
How to deal with evolutionary/genetic fitness function that can have both negative and positive values?,"
I am optimising function that can have both positive and negative values in pretty much unknown ranges, might be -100, 30, 0.001, or 4000, or -0.4 and I wonder how I can transform these results so I can use it as a fitness function in evolutionary algorithms and it can be optimised in a way that, for example, it can go from negative to positive along the optimisation process (first generation best chromosome can have -4.3 and best at 1000 generation would have 5.9). Although the main goal would always be to maximise the function.
Adding a constant value like 100 and then treating it simply as positive is not possible because like I said, the function might optimise different ranges of results in different runs for example (-10000 to +400 and in another run from -0.002 to -0.5).
Is there a way to solve this?
","['genetic-algorithms', 'optimization', 'evolutionary-algorithms', 'fitness-functions']","If I understand correctly your problem, you always want to maximize some function $h(x)$, which is defined as follows $h: \Gamma \rightarrow \mathbb{R}$, where $\Gamma$ is the space of genotypes. However, at every generation $g$, you don't know exactly $h(i)$ of each individual $i$, i.e., maybe in one generation $g$, all $h(i)$, for $i=1, \dots, N$ (where $N$ is the size of the population), are negative, but in the next generation $g+1$, due to the mutations and crossovers, that may not be the case anymore, so you don't know how to shift $h(i), \forall i$, so that they are all positive and you can compute the probability of being selected, assuming you are using the fitness proportionate selection.If my intepretation is correct, then, at every generation $g$, you just need to find $w = \operatorname{min}_i h(i)$, then shift all $h(i)$ by adding $|w| + \epsilon$ (where $\epsilon$ can be zero or a very small number) to all $h(i)$, so you would compute the fitness as follows $f(i) = h(i) + |w| + \epsilon$, for all $i$. Here are all the steps to compute the probability of individual $i$ being selected $p(i)$This technique is known as windowing, which I have used in the past to solve the exact same problem that you seem to be trying to solve (check it here)."
What trait of a planning problem makes reinforcement learning a well suited solution?,"
Planning problems have been the first problems studied at the dawn of AI (Shakey the robot). Graph search (e.g. A*) and planning (e.g. GraphPlan) algorithms can be very efficient at generating a plan. As for problem formulation, for planning problems PDDL is preferred. Although planning problems in most cases only have discrete states and actions, the PDDL+ extention covers continuos dimensions of the planning problem also.
If a planning problem has non-deterministic state transitions, classical planning algorithms are not a well suited solution method, (some) reinforcement learning is considered to be a well suited solution in this case. From this point of view, if a planning problem has a state transition probability less than 1, classical planning methods (A*, GraphPlan, FF planner, etc.) are not the right tool to solve these problems.
Looking at reinforcement learning examples, in some cases environments are used to showcase reinforcement learning algorithms which could be very well solved by search/planning algorithms. They are fully deterministic, fully observable and sometimes they even have discrete action and state spaces.
Given an arbitrary fully deterministic planning problem, what is/are the chartacteristic(s) which make reinforcement learning, and not ""classical planning"" methods better suited to solve the problem?
","['reinforcement-learning', 'markov-decision-process', 'planning', 'dynamic-programming']",
Dealing with bias in multi-channel auto encoders,"
The problem
I have a multi-channel 1D signal I want to auto-encode.
I am unable to resonstruct the input when the number of channels increases.

Code
I am using a convolutional encoder, and a convolutional decoder:
latent_dim: 512, frames_per_sample: 128
    self._encoder = nn.Sequential(
        nn.Conv1d(in_channels=self._n_in_features, out_channels=50, kernel_size=15, stride=1, padding=7),
        nn.LeakyReLU(inplace=True),
        nn.Conv1d(in_channels=50, out_channels=50, kernel_size=7, stride=1, padding=3),
        nn.LeakyReLU(inplace=True),
        nn.Conv1d(in_channels=50, out_channels=50, kernel_size=3, stride=1, padding=1),
        nn.LeakyReLU(inplace=True),
        # nn.Flatten(start_dim=1, end_dim=-1)
        nn.Conv1d(in_channels=50, out_channels=1, kernel_size=1, stride=1, padding=0),
        nn.Flatten(start_dim=1, end_dim=-1),
        nn.Linear(frames_per_sample, self._config.case.latent_dim)
    )

and
    start_channels = 256
    start_dim = frames_per_sample // (2 ** 4)

    start_volume = start_dim * start_channels
    self._decoder = nn.Sequential(
            nn.Linear(self._config.case.latent_dim, start_volume),
            nn.LeakyReLU(inplace=True),
            # b, latent
            nn.Unflatten(dim=1, unflattened_size=(start_channels, start_dim)),
            # b, start_channels, start_dim
            nn.Upsample(scale_factor=2, mode='linear', align_corners=False),
            # b, start_channels, start_dim*2
            nn.Conv1d(in_channels=start_channels, out_channels=128, kernel_size=3, stride=1, padding=1),
            # b, 128, start_dim*2
            nn.LeakyReLU(inplace=True),
            # b, 128, start_dim*2
            nn.Upsample(scale_factor=2, mode='linear', align_corners=False),
            # b, 128, start_dim*4
            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=7, stride=1, padding=3),
            # b, 64, start_dim*4
            nn.LeakyReLU(inplace=True),
            # b,64, start_dim*4
            nn.Upsample(scale_factor=2, mode='linear', align_corners=False),
            # b, 64, start_dim*8
            nn.Conv1d(in_channels=64, out_channels=32, kernel_size=11, stride=1, padding=5),
            # b, 32, start_dim*8
            nn.LeakyReLU(inplace=True),
            # b, 32, start_dim*8
            nn.Upsample(scale_factor=2, mode='linear', align_corners=False),
            # b, 32, start_dim*16
            nn.Conv1d(in_channels=32, out_channels=16, kernel_size=21, stride=1, padding=10),
            # b, 16, start_dim*16
            nn.LeakyReLU(inplace=True),
            # b, 16, start_dim*16
            nn.Conv1d(in_channels=16, out_channels=self._n_features, kernel_size=3, stride=1, padding=1),
    )

I am not putting the entire code/data here because this is a theoretical question, and I don't expect anyone to go and run this.

Results
The result (orange) has artifacts on the edges, relative to the input data (blue):
This is easy to see on training examples:

Worse - for unseen examples (validation), reconstruction misses on the bias


Observation
The above only starts to happen when adding more channels, which have different biases.
I am normalizing the entire dataset to sit between -1 and 1, but still each channel has its own typical boundary.
Here is a (nice) result, for a single channel:


What I think
My guess - Multiple channels force filters to have a single bias, which doesn't fit all of them.
The edges problems are due to bias + zero padding, and the validation data is due to bias that doesn't agree with all channels.

Questions:

Does my analysis make sense?
What is a possible way to solve this?


My thoughts:

A distinct bias per channel on at least the last layer. How to do this in Pytorch?
Normalizing per-sample (and not per channel) just before passing it to the model, then de normalizing the reconstructed sample.

I don't know how to correctly implement either of those, nor if they make sense, or how to check.
Also posted here, but I think this also belongs on ai.stackexchange
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'autoencoders', 'overfitting']",
Data Augmentation of store images using handwritten labels,"
I am new to AI and NN. I've started learning using Geron's book on Tensorflow.
My first project (""Smart Shelf"") is to determine which items in a store have been purchased and need refilled. The store camera periodically takes pictures of the tops of items on store shelves. To start, we have only 5 distinct products.
We have created ~250 handwritten images of product-labels that cover these 5 distinct products. So far, the training results are way below our expectation.
I am thinking to augment the training data and see whether it would make any difference.
I have thought about the following strategies:

Train the model again using grayscale images. https://stackoverflow.com/questions/45320545/impact-of-converting-image-to-grayscale/45321001
Invert images, translate them horizontally or vertically https://www.tensorflow.org/tutorials/images/data_augmentation, https://nanonets.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/

Which of the above will yield better results and why? I am curious. Thanks for any help. I feel that I know various data augmentation techniques, but not sure how and why to apply them.

It seems this is a popular question, as learned from Choosing Data Augmentation smartly for different application etc.
","['neural-networks', 'training', 'data-preprocessing', 'data-augmentation', 'data-labelling']",
What machine learning model should I use for a random dice-based game?,"
Consider a game like Pig (https://en.wikipedia.org/wiki/Pig_(dice_game)), but with a few additions: namely functions of both player's score and turn number that have unique impacts on scoring.
What machine learning model should I use to try and get the optimal number of dice roles per turn (say number of dice roles are bounded between 1 and 10)?
I was reading this tutorial: https://towardsdatascience.com/playing-cards-with-reinforcement-learning-1-3-c2dbabcf1df0, and they suggested reinforcement learning with a Q value function. I don't know how this work though, because turn number isn't bounded, but also needs to be a parameter to the Q value function. Multiplying the range of all parameters suggest this Q value function needs 2,000,000 states. Is this too many? - I have no idea how to judge this.
Is there a better model I should use to try and solve this problem which at its core takes the parameters of (my_score, opponent_score, turn_number) and should return a number 0 - 10 representing how many dice to roll.
","['machine-learning', 'random-variable']",
Does Monte Carlo Tree Search not work on games without the same initial state?,"
I'm curious how you would apply Monte Carlo Tree Search to a game that has a random initial state. You generate a tree where the root node is the initial state, then you expand if the options from that state are not explored yet.
I'm also wondering how this works in 2 player games. After your opponent moves, does each state in the tree have a key to look up in a dictionary? Otherwise, the algorithm won't know what to do when there's a jump in a state between choosing your action on your turn and when your opponent moves, unless you also store your opponent's move in the tree.
",['monte-carlo-tree-search'],
Is there a convention on the order of multiplication of the weights with the inputs in neural nets?,"
Is there a convention on how the input data and the weights are multiplied? The input data can be anything, including the result from the previous layers.
There are two options:
Option 1:
$$\begin{bmatrix}i_1 & i_2\end{bmatrix} \times \begin{bmatrix} w_1 & w_2 & w_3\\w_4 & w_5 & w_6\end{bmatrix} = \begin{bmatrix}i_1*w_1 + i_2*w_4 & i_1*w_2+i_2*w_5 &i_1*w_3+i_2*w_6\end{bmatrix}$$
Option 2:
$$\begin{bmatrix} w_1 & w_4\\ w_2 & w_5\\ w_3 & w_6\end{bmatrix} \times \begin{bmatrix}i_1 \\ i_2\end{bmatrix}  = \begin{bmatrix}i_1*w_1 + i_2*w_4 & i_1*w_2+i_2*w_5 &i_1*w_3+i_2*w_6\end{bmatrix}$$
","['neural-networks', 'implementation', 'weights']",
Multi class text classification when having only one sample for classes,"
I have a dataset of texts, each text was identified with an ID number. I would like to do a prediction by finding the best match ID number for upcoming new texts. To use multi text classification, I am not sure if this is the right approach since there is only one text for most of ID numbers. In this case, I wouldn't have any test set. Can up-sampling help? Or is there any other approach than classification for such a problem?
The data set looks like this:
id1 'text1', id2 'text2', id3 'text3', id3 'text4', id3 'text5', id4 'text6', . . id200 'text170'
I would appreciate any guidance to find the best approach for this problem.
","['prediction', 'text-classification', 'imbalanced-datasets', 'multiclass-classification']",
Beating iterative alpha beta search in Isolation Game,"
I'm having trouble beating an AI in Isolation game: https://en.wikipedia.org/wiki/Isolation_(board_game) with 3 queens on a 7x7 board. I tried applying alpha beta iteration with a scoring function on the state. I tried 3 scoring functions:

(0.75/# moves taken) * number of legal moves - (# moves taken/0.75) * number of opponent legal moves
number of legal moves - 3 * number of opponent legal moves
3 * number of legal moves - number of opponent legal moves

The first is an annealed aggressive strategy so the agent gets more aggressive as the game goes longer. The 2nd is a pure aggression strat and the last is a pure defensive strat. None of them consistently beat standard alpha beta iteration with the state scoring function: number of legal moves -  number of opponent legal moves. They all broke roughly even.
Any suggestions of scoring state functions or search algorithms are appreciated. The search has a limit of 6000 seconds per turn though.
","['alpha-beta-pruning', 'iddfs']",
Does the order of data augmentation and normalization matter?,"
What is the preferred order of data augmentation and normalization? Is it the former followed by the latter?
","['deep-learning', 'convolutional-neural-networks', 'keras', 'image-processing', 'data-science']",
Embedding Isolation game states into key values for RL,"
I'm trying to think of how I can embed a game's state into a unique key value. The game I'm specifically working with is Isolation: https://en.wikipedia.org/wiki/Isolation_(board_game). The game state has the coordinates of player 1's pawn, coordinates of player 2's pawn, coordinates of free spaces and coordinates of already used spaces. Is there a way to embed this into a unique key value? My plan is to generate a dict and use that for value iteration with RL to learn the optimal value function for every state.
","['reinforcement-learning', 'game-ai']",
How should I model the state and action spaces for a problem where the goal is to draw a line between two points?,"
I have a problem where the goal is for the agent to draw a single line between two points on a  $500 \times 500$ white image.
I have built my DQN. For now, the output layer's size of the network is $[1, 500 * 500]$. This way, when a Q value is given within a single step, it can be mapped to a single coordinate within the space of the image.
So, with that, I'm able to get a starting point for the line. However, what that doesn't give me is the location of the second point for completion of the line.
One thing I have tried is drawing a line for every two steps. However, this means that the state of the environment does not change for each step.
Should the goal be to change the environment/state for each step or does this not matter? Maybe I have not found the ideal way of modelling the state and action spaces for this problem.
","['reinforcement-learning', 'dqn', 'state-spaces', 'action-spaces', 'double-dqn']",
Is it possible to use k-nearest neighbour for classification with more than two attributes?,"
If I were to have a dataset of 9 attributes of different types that describe current weather, such as temperature, humidity, etc., and want to classify the current weather by use of a k-NN algorithm, is this possible?
From what I understand, k-NN has two different attributes that are plotted, and, wherever a point is drawn, its nearest neighbors will classify it.
Could I do the same thing but each data point is placed based on its 9 attributes?
","['machine-learning', 'classification', 'k-nearest-neighbors']",The number of features is not important to use K-NN algotihm. You have to decide distance measure to detect neighbors. I share with you some links that you can check to see which kinds of distance measures that you can use. Just decide the meause and use your feature vectors in the measure.https://www.kdnuggets.com/2020/11/most-popular-distance-metrics-knn.htmlhttps://medium.com/@luigi.fiori.lf0303/distance-metrics-and-k-nearest-neighbor-knn-1b840969c0f4
Can an ML model sort a random sequence of numbers from 1 to $ 2^{2^{512}} $ in our universe in infinite time?,"
I am pondering on the question in the title. As a human being, somehow I can sort a random sequence of numbers from 1 to $ 2^{2^{512}} $  in our universe in infinite time (But I am not sure.). Can an ML model do that in our universe if it is provided with infinite time? There is no restriction on how the learning algorithm is supposed to learn how to sort. (Be careful, even $ 2^{512} $ is bigger than the number of atoms in the universe. Therefore you will have limited memory.)
","['machine-learning', 'computational-learning-theory', 'learning-algorithms']",
What kind of word embedding is used in the original transformer?,"
I am currently trying to understand transformers.
To start, I read Attention Is All You Need and also this tutorial.
What makes me wonder is the word embedding used in the model. Is word2vec or GloVe being used? Are the word embeddings trained from scratch?
In the tutorial linked above, the transformer is implemented from scratch and nn.Embedding from pytorch is used for the embeddings. I looked up this function and didn't understand it well, but I tend to think that the embeddings are trained from scratch, right?
","['natural-language-processing', 'transformer', 'attention', 'word-embedding']","I have found a good answer in this blog post The Transformer: Attention Is All You Need:we learn a “word embedding” which is a smaller real-valued vector representation of the word that carries some information about the word. We can do this using nn.Embedding in Pytorch, or, more generally speaking, by multiplying our one-hot vector with a learned weight matrix W.There are two options for dealing with the Pytorch nn.Embedding weight matrix. One option is to initialize it with pre-trained embeddings and keep it fixed, in which case it’s really just a lookup table. Another option is to initialize it randomly, or with pre-trained embeddings, but keep it trainable. In that case the word representations will get refined and modified throughout training because the weight matrix will get refined and modified throughout training.The Transformer uses a random initialization of the weight matrix and refines these weights during training – i.e. it learns its own word embeddings."
Number Series Continuation? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I am new to AI.
I have a series of numbers ranging from x to y and I have a lot of data to train with
What I am trying to do is, let's say from 0 to 1, I train it with data calculated over time and predict what may happen next, training it with my data and then feeding it the last few days and continue the pattern.
I have been thinking about using char-rnn, but from what i understand the data exported is arbitrary and not a continuation of a series. I oftentimes see videos on youtube ""AI continues this song"" so I'm wondering which I can use and where I can get started to do this myself.
Thank you and have a nice day ☺
","['machine-learning', 'training', 'pattern-recognition']","I don't actually understand your question, but if your data is completely arbitrary, there is nothing to predict, it have no patterns to recognize or something like that.But if you say that you are working with time-series data and it have some patterns, then you could start by trying to implement just the forward propagation of a simple RNN. I am really new as well at AI and the first RNN I code was a Elman RNN, and they were very simple equations to implement. I recommend you trying that, and then implement the backpropagation to that RNN.Something that really helped me starting was searching simple github scrips for RNNs (around 100 lines of code) so that you can see their architecture.About the example you give, the one about songs, they are actually really predictable, because you have the rime, rithm and tempo, and also they use to repeat the same secuence of notes every stanza.Hope it helps :)"
What error should I use for RNN?,"
I'm relatively new to machine learning, and I don't know what error I should use for an RNN.
I want to use a simple Elman RNN to predict the cases of Covid-19 there will be in a hospital for the next 15 days. I modeled this as a regression problem, treating the input like a bunch of dots in a graph to predict the tendency that the data is going to take (only show if there will be more cases or less).
With that bunch of dots I in fact refer to this:

Then I would treat this problem as a regression.
I actually don't have anything programmed yet. Firstly I want to write it all on a paper and then get down to work.  I am also considering focusing the problem to predict the actual plot of the time-series input, but right now I want to try the regression.
I've come to the conclusion that I can use these four different errors:

MSE
RMSE
Entropy
Cross-entropy

What are the different characteristics of these errors? Which to use? Where and when to use them?
","['recurrent-neural-networks', 'objective-functions', 'cross-entropy', 'mean-squared-error', 'root-mean-square']","To provide a good answer would fill several pages.  To keep it very simple try many different loss functions on your model.  Your goal is to have the highest performance based on some desired prediction metric (e.g., RMSE, MAE, MAPE, etc.).  You almost always have plenty of time to try many loss functions so you don't need to have a full understanding, and few people do, to start your project.I recommend you read the following to learn more:"
Visualizing the Loss Landscape of Neural Nets: Meaning of the word 'filter'?,"
I found myself scratching my head when I read the following phrase in the paper Visualizing the Loss Landscape of Neural Nets:

To remove this scaling effect, we plot loss functions using filter-wise normalized directions. To obtain such directions for a network with parameters $\theta$, we begin by producing a random Gaussian direction vector $d$ with dimensions compatible with $\theta$. Then, we normalize each filter in $d$ to have the same norm of the corresponding filter in $\theta$. In other words, we make the replacement $d_{i,j} \leftarrow d_{i,j} \| d_{i,j}\|  \| \theta_{i,j}\| $

I'm completely unclear what the authors are referring to when they refer to the filters of the vector $d$ in weight space. As far as I can tell, the vector $d$ is a standard vector in weight space ($W$) with a number of components equal to the number of changeable weights in the network. In my opinion, it could be said that each layer in the network can be visualized as a vector in weight space ($\theta_{i}$) with:
$$\theta = \sum_{i}\theta_{i}$$
and then maybe these vectors $\theta_{i}$ are called filters? But how this would have anything to do with the random vector $d$, generated in this space, remains a complete mystery to me.
","['neural-networks', 'terminology', 'papers', 'filters']",
Would either $L_1$ or $L_2$ regularisation lower the MSE on the training and test data?,"
Consider linear regression. The mean squared error (MSE) is 120.5 for the training dataset. We've reached the minimum for the training data.
Is it possible that by applying Lasso (L1 regularization) we would get a lower MSE for the training data? Would it get lower for the test data? Would this also hold for ridge regression (L2 regularization)?
","['linear-regression', 'mean-squared-error', 'l2-regularization', 'l1-regularization']","The answer is largely the same whether we consider $\ell_1$ or $\ell_2$ regularisation, so I will just speak generally about regularisation.Given some training data $\{(x_i, y_i)\}_{i = 1}^n$, a linear regression line $Y = aX + b$ fit using the least squares method looks for coefficients that minimise the sum of squares, i.e. they are the minimisers given by$$ \mathrm{arg\,min}_{a, b} \sum_{i = 1}^n \left(y_i - (ax_i + b)\right)^2.$$This gives the same coefficients as minimising the mean square error$$ \mathrm{MSE}\left((x_1, y_1), \dots, (x_n, y_n)\right) = \frac{1}{n} \sum_{i = 1}^n \left(y_i - (ax_i + b)\right)^2.$$So, by definition, the coefficients $(a, b)$ minimise the MSE on the training data. Any regularisation will only increase the MSE on the training data.The main point of regularisation is to prevent overfitting on the data and improve the generalisation performance (i.e. on the test set).With an appropriate parameter for regularisation, you may obtain a smaller MSE on the test set. This depends on your dataset and the parameters you choose: strong regularisation may lead to underfitting, whereas weak regularisation might not make much difference to the coefficients that you fit."
Is there a reason why no one combines word embeddings with the median?,"
Could you combine word embeddings with the median per dimension to get a document embedding? In my case I have a huge amount of words to build one document, which in turn should describe a topic. I feel like using the median is the right thing to do, as I get the most common parameter value per dimension. However, I cannot find anyone trying it before. This is why I'm wondering, is there something speaking against it?
","['natural-language-processing', 'word-embedding']",
Is it possible to do face recognition with just the eyes?,"
Assuming the input photo is focused on a person's face, if the person is wearing a surgical mask, most face recognition software fail to identify the subject's face.
Most facial landmark models are trained to identify at least the eyes and the tip of the nose (for example, dlib's 5 point landmark).
Is it possible to construct a model that is trained to identify a face based on only the eyes?
Edit: Sorry for my broken english, but by ""eyes"" I mean the periocular area. I am terribly sorry because english isn't my first language.
","['machine-learning', 'deep-learning', 'facial-recognition']",
Why do I get the best policy before Q values converge using DQN?,"
I have implemented DQN algorithm and wonder why during testing, the best performance is achieved by a policy from about 300 episode, when mean Q values converge at about 800 episode?

Mean Q-values are calculated on a fixed set of states by taking mean of max Q-values for each state.
By convergence I mean that the plot of mean Q-values converge to some level (those values does not increase to infinity).

It can be seen in here (page 7) that mean Q-values converge and average rewards plot is quite noisy. I get similar results and in tests, the best policy is where the peaks are during training (average reward plot). I don't understand why don't I get better average scores over time (and better policies) when Q-values converge.
","['dqn', 'deep-rl', 'convergence', 'value-functions', 'policies']",
How to scrape product data on supplier websites?,"
I'm currently trying to build a semantic scraper that can extract product information from different company websites of suppliers in the packaging industry (with as little manual customization per supplier/website as possible).
The current approach that I'm thinking of is the following:

Get all the text data via scrapy (so basically a HTML-tag search). This data would hopefully be already semi-structured with for example: name, description, product image, etc.
Fine-tune a pre-trained NLP model (such as BERT) on a domain specific dataset for packaging to extract more information about the product. For example: weight and size of the product

What do you think about the approach? What would you do differently?
One challenge I already encountered is the following:

Not all of the websites of the suppliers are as structured as for example e-commerce sites are → So small customisations of the XPath for all websites is needed. How can you scale this?

Also does anyone know an open-source project as a good starting point for this?
","['natural-language-processing', 'python', 'pretrained-models', 'fine-tuning']",
What should the input and output of the Q-network be in the case of an ordinal action space?,"
I recently started looking into implementations of the DQN algorithm (e.g. TensorFlow) in some more detail. All the implementations that I found use a network that gives an output for each possible action (e.g. if you have three possible actions you will have three output units in your network). This makes a lot of sense from a computational standpoint and seems to work fine if you are dealing with categorical action spaces (e.g ""left"" or ""right"").
However, I am currently working with an action space that I discretized and the actions have an ordinal meaning (e.g. you can drive left or right in 5-degree increments). I assume that the action-value function has some monotonicity in the action component (think driving 45 degrees to the left instead of 40 will have a similar value).
Am I losing information on the similarity of actions, if I use a network that has an output unit for each possible action?
Are there implementations of the DQN available in which actions are used as network inputs?
","['reinforcement-learning', 'dqn', 'deep-rl', 'action-spaces', 'discretization']","Yes it is possible to use the action as input to neural network in DQN. For discrete actions represented as one-hot encoded features, the difference is minor:If all actions are in the output, your neural network function is $f(s): \mathcal{S} \rightarrow \mathbb{R}^{|\mathcal{A}|} = [\hat{q}(s,a_1), \hat{q}(s,a_2), \hat{q}(s,a_3) ...]$, and you take the maximum value from the output vector as the greedy action.If the action is provided as an input argument, your neural network function is $f(s,a): \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R} = \hat{q}(s,a)$, and to find the maximum value you construct and run a mini-batch over all possible values of $a$ for the given state.Also see the answer to this question: Why does Deep Q Network outputs multiple Q values?In your case, you would like to take advantage of similar values of $a$ because you expect that to work well with the approximation. As you correctly suggested, this will only work with the second approach using action as an input. So, use the steering angle, normalised into a suitable range for input to a neural network, as an input. Every time that you need to find $\text{max}_a Q(s,a)$ for the Q-learning algorithm, you must construct a mini-batch of the current state concatenated with each of the discrete steering angles that you want to consider as actions in the DQN, and run the current (or target) neural network forward.If you want to go further and use a continuous action space, you will need to change which reinforcement learning method you are using. The various policy gradient and actor-critic approaches, such as REINFORCE, A3C, DDPG etc can cope with continuous actions, because they drop the need to find $\text{max}_a Q(s,a)$, which becomes impractical for very large action spaces."
"Why do feedforward neural networks require the inputs to be of a fixed size, while RNNs can process variable-size inputs?","
Why does a vanilla feedforward neural network only accept a fixed input size, while RNNs are capable of taking a series of inputs with no predetermined limit on the size? Can anyone elaborate on this with an example?
","['neural-networks', 'recurrent-neural-networks', 'feedforward-neural-networks', 'multilayer-perceptrons']","You are talking about two different types of 'size'. The size of the input for a FFNN and a RNN must always remain fixed for the same network architecture, i.e. they take in a vector $x \in \mathbb{R}^d$ and could not take as input for instance a vector $y \in \mathbb{R}^b$ where $b \neq d$. The size you refer to in the context of the RNN is the length of the input sequence.What you are getting confused with is that RNN's can make predictions relating to sequences, that is imagine now rather than one $x$ we have a sequence of related, i.e. not i.i.d. data such as time series data, data $\{x_i\}_{i=1}^n$. Assuming we are given some initial $h_0$ (a hidden state) then an RNN will take as input $x_1$ and $h_0$ and output a prediction $y_1$ and a new hidden state $h_1$. In general an RNN will take as input $x_n$ and $h_{n-1}$ and output $y_n$ and $h_n$ where the hidden state is passed as input to the RNN at the next time step. However, the dimensionality of all the $x$'s and $h$'s will all be the same, i.e. $x_i \in \mathbb{R}^d$ and $h_i \in \mathbb{R}^c$ for all $i$, where $d$ does not necessarily have to be equal to $c$ (and in my experience rarely is).Note that RNN's can also perform sequence to sequence prediction (such as language translation) where the predicted sequence can be a different length to the input sequence, as is the case when doing translation (the input sentence is not necessarily the same length in the translated language). They do this by having an encoder and a decoder which are two separate RNNs. The encoder is fed the input sequence and we maintain all the hidden states outputted by the encoder $\{e_i\}_{i=1}^n$. The decoder is then given a token that represents the start of a sequence and the last hidden state of the encoder $e_n$ as input to which it will make a prediction on what the word should be (I believe but am not 100% certain that the decoder outputs a probability distribution over the dictionary of words it can predict from) and then the word chosen is passed as input to the decoder at the next step, along with the hidden state from the decoder at the previous step and another word is predicted. This continues until the system predicts an End of Sequence token (or until it is forced to by some time limit). More details can be found in this paper but I believe in NLP it is much more common to use attention models now rather than the methods introduced in this paper.Now, with this data $\{x_i\}_{i=1}^n$ you could in theory pass each one individually to a FFNN but a) it would not capture the sequential nature of the data as the assumption is that each data point is independent of the others and you can see this from the architecture of a FFNN -- they are Directed Acyclic Graphs, the Acyclic-ness is what causes the issue as there is no recursion which stops any sequential information from being passed from one time step to another and b) training using SGD would likely cause issues as we have violated the i.i.d. assumption needed for SGD to converge to local optima. More info on the i.i.d. requirement can be found here."
Recent methods for Decision Support System (DSS),"
In Decision Support System (DSS), we rank items based on predetermined weighted criteria. For example, we want to rank prospective programmers based on their working experience, required salary, set of skills, age, etc. We rank using weights for each criterion that we have previously defined. The simplest method is using Simple Additive Weighting (SAW).
As far as I know, DSS is included in knowledge-based AI (it's a mandatory subject in AI specialization in most universities in my country).
My question:

With the development of AI/ML/DL today, is there another modern approach that can be used to solve similar problems?

At first, I thought it's similar with Content-Based Recommender System, but it looks different as we don't have ""user"" in DSS.
","['recommender-system', 'knowledge-base', 'decision-support-system']",
Can ML/DL solve my classification problem?,"
I'm new to AI but would still like to try and get a project off the ground.
I've read a lot about ML/DL the past few days but I just can't figure out if my problem can be solved with ML/DL. What I'm trying to do looks like a classification job to me but maybe isn't.
I have 100s of images of compacted soil samples, on these images there may be multiple layers visible.
I will include a picture below, this sample had a sticker on it, normally they don't. On the image there are 3 layers, separated above and under the sticker.
With every image there is data (xml file) available on the size of the layer(s) and the type of soil in that layer, which costs a lot of time to produce, so I want to automate this classification in the future.
The data files contains info like:
layer0:
    type 004
    2cm
    12cm
layer1:
    type 003
    12cm
    25cm

If there would be just one layer, the AI could learn what these layers look like and sort them in the right soil class.
But I don't know if my problem can be solved with AI as there could be 1, 2, 3 or 4 different layers (classes) on one image and I haven't seen any examples on classification where there can be multiple classes in one image.
As AI is quite a steep learning curve I would like to know if my problem is suited for ML/DL before I spend more of my nights reading for something that might not work.
I've read numerous websites and a few short books but can't find an answer to my questions.
Can ML/DL solve my multi-class single-image classification problem and which strategy should I read into?

","['machine-learning', 'deep-learning', 'classification']","A simple sanity-check on whether an image classifier can perform a task in theory is:Can a human expert, using the same image plus a list of catgeories that they are familiar with, perform the same task?It is important you only consider the contents of the image (or in general the data you are prepared to supply to the classifier) and the expert's general knowledge. The expert is not allowed to collect more data for instance, or interact with the sample other than maybe take a few measurements on the pixels.This sanity check doesn't tell you how hard the problem is. It also does rule out problems that are very hard or impossible for humans but actually quite easy for computers. However, it is a good start because nowadays single-purpose computer vision classifier tasks often rate similarly to or better than humans performing the same task. You are effectively checking ""is the data I need for the inference actually in the image?""Multi-class classifiers are possible in several ways. One way would be to have separate heads to the neural network to classify each layer, and maybe a layer present binary flag to allow for varying numbers of layers. This is similar to an architecture called YOLO which classifies 0 or 1 objects and their locations over multiple grid squares within an image. Your architecture would need to be different to YOLO, but you could use a lot of the ideas from it, such as having an output with multiple multi-class classifiers, one for each soil layer.I have 100s of images of compacted soil samplesOne issue you will face is that you would need a very large number of labelled images in order to train a neural network with cutting-edge performance for this task from scratch. So you will want to look into transfer learning, which involves taking an existing image classifier trained on e.g. ImageNet and adapting it to your problem before training with your smaller dataset.The small amount of sample image data you have will be a major limiting factor in your case. Sadly no-one can tell you before you attempt the project whether you have enough for a deep learning approach. That is probably where your largest risk of failure is."
UCB-like algorithms: how do you compute the exploration bonus?,"
My question concerns Stochastic Combinatorial Multiarmed Bandits. More specifically, the algorithm called CombUCB1 presented in this paper. It is a UCB-like algorithm.
Essentially, in each round of the sequential game the learner chooses a super-arm to play. A super-arm is a $d$-dimensional vector $a \in \mathcal{A}$ where $\mathcal{A} \subset \{0,1\}^d$. In each super-arm $a$, when the $i$-element equals to $1$ ( $i \in \{0, \dots, d\}, a(i)=1$ ), that means that the basic action $i$ is active. Basically, in each round the learner plays the basic actions that are active in the chosen super-arm. The rewards of the basic actions are stochastics and a super-arm receives as a reward the sum of the rewards of the basic active actions.
The algorithm mentioned above presents a UCB-like algorithm, where with each basic action is associated a UCB-index and in each round the learner plays the super-arm that maximises that index.
My question concerns the confidence interval around the mean of the rewards of the basic actions, presented in equation $2$ of the mentioned paper. Here, the exploration bonus is
$c_{t,s} = \sqrt{\frac{1.5 \log t}{s}}$
I don't understand where that $1.5$ is coming from.
I've always known that one needs to use Chernoff-Hoeffding inequality to derive the exploration bonus in a UCB-algorithm. Am I wrong and it needs to be computed in other ways? I've always seen the same coefficient but with $2$ instead of $1.5$ (reference).
Could someone explain me where does $1.5$ come from, please?
I know there is a similar question here, but I cannot really understand how that works here.
Thank you in advance in case you have time to read and answer my questions.
","['multi-armed-bandits', 'online-learning', 'upper-confidence-bound']",
"What is the purpose of ""alignment"" in the self-attention mechanism of transformers?","
I've been reading about transformers & have been having some difficulty understanding the concept of alignment.
Based on this article

Alignment means matching segments of original text with their corresponding segments of the translation.

Does this mean that, with transformers, we're adding the fully translated sentences as inputs too? What's the purpose of alignment? How exactly do these models figure out how to match the different segments together? I'm pretty sure there's some underlying assumption/knowledge that I'm not fully getting -- but I'm not entirely sure what.
","['neural-networks', 'transformer', 'attention', 'machine-translation']",
How to make an ensemble model of two LSTM models with different window sizes i.e. different data shapes,"
Below is the Python code for making an ensemble model. All the inputs are the same for all three models. But what if the models have different input shapes due to different window size, such as LSTM models. So the input shapes for Model A would be (window_size_A, features) and for Model B would be (window_size_B, features). The window sizes are different but the number of features are the same. As such, due to the different window size, the training data of the same dataset is split differently for each model such that the X_train.shape for model A: (train_data_A, window_size_A, output) And for Model B: (train_data_B, window_size_B, output). Note the training data is from the same dataset but the length is different due to the different window size. How would you make an ensemble of these models?
def get_model():
    inputs = keras.Input(shape=(128,))
    outputs = layers.Dense(1)(inputs)
    return keras.Model(inputs, outputs)


model1 = get_model()
model2 = get_model()
model3 = get_model()

inputs = keras.Input(shape=(128,))
y1 = model1(inputs)
y2 = model2(inputs)
y3 = model3(inputs)
outputs = layers.average([y1, y2, y3])
ensemble_model = keras.Model(inputs=inputs, outputs=outputs)

","['python', 'keras', 'long-short-term-memory', 'ensemble-learning']",
How to improve the reward signal when the rewards are sparse?,"
In cases where the reward is delayed, this can negatively impact a models ability to do proper credit assignment. In the case of a sparse reward, are there ways in which this can be negated?
In a chess example, there are certain moves that you can take that correlate strongly with winning the game (taking the opponent's queen) but typically agents only receive a reward at the end of the game, so as to not introduce bias. The downside is that training in this sparse reward environment requires lots of data and training episodes to converge to something good.
Are there existing ways to improve the agent's performance without introducing too much bias to the policy?
","['reinforcement-learning', 'reward-functions', 'sparse-rewards', 'delayed-rewards', 'potential-reward-shaping']","Andrew Y. Ng (yes, that famous guy!) et al. proved, in the seminal paper Policy invariance under reward transformations: Theory and application to reward shaping (ICML, 1999), which was then part of his PhD thesis, that potential-based reward shaping (PBRS) is the way to shape the natural/correct sparse reward function (RF) without changing the optimal policy, i.e. if you apply PBRS to your sparse RF, the optimal policy associated with the shaped, denser RF is equal to the optimal policy associated with the original unshaped and sparse RF. This means that PBRS creates an equivalence class of RFs associated with the same optimal policy, i.e. there are multiple RFs associated with the same optimal policy. So, PBRS is the first technique that you can use to deal with sparse reward functions.To give you more details, let $R\left(s, a, s^{\prime}\right)$ be your original (possibly sparse) RF for your MDP $M$, then$$R\left(s, a, s^{\prime}\right)+F\left(s, a, s^{\prime}\right)\tag{1}\label{1}$$is the shaped, denser RF for a new MDP $M'$.In PBRS, we then define $F$ (the shaping function) as follows$$
F\left(s, a, s^{\prime}\right)=\gamma \Phi\left(s^{\prime}\right)-\Phi(s),
\tag{2}\label{2}
$$
where $\Phi: S \mapsto \mathbb{R}$ is a real-valued function that indicates the desirability of being in a specific state. So, if we define $F$ as defined in \ref{2}, we call it a potential-based shaping function.The intuition of $F$ in \ref{2} is that it avoids the agent to ""go in circles"" to get more and more reward. To be more precise, let's say that there's a state $s^* = s_t$ that is desirable but not the goal state. If you shaped the reward function by adding a positive reward (e.g. 5) to the agent whenever it got to that state $s^*$, it could just go back and forth to that state in order to get the reward without ever reaching the goal state (i.e. reward hacking). That's, of course, not desirable. So, if your current state is $s^*$ and your previous state was $s_{t-1}$, and whenever you get to $s_{t-1}$ you just shaped the reward function by adding a zero reward, to avoid going back to $s_{t-1}$ (i.e. to avoid that the next state $s_{t+1} = s_{t-1}$, and then go back to $s^*$ again, $s_{t+2} = s^*$, i.e. going in circles), if you use \ref{2}, you will add to your original sparse reward function the following$$
F\left(s, a, s^{\prime}\right)=\gamma 0 - 5 = -5,
\tag{3}\label{3}
$$In other words, going back to $s_{t+1}$ (to later try to go back to $s^*$) is punished because this could again lead you to go back to $s^*$. So, if you define $F$ as in equation \ref{2}, you avoid reward hacking, which can arise if you shape your RF in an ad-hoc manner (i.e. ""as it seems good to you"").One of the first papers that reported this ""going in circles"" behaviour was Learning to Drive a Bicycle using Reinforcement Learning and ShapingWe agree with Mataric [Mataric, 1994] that these heterogeneous reinforcement functions have to be designed with great care. In our first experiments we rewarded the agent for driving towards the goal but did not punish it for driving away from it. Consequently the agent drove in circles with a radius of 20–50 meters around the starting point. Such behavior was actually rewarded by the reinforcement functionYou probably also want to watch this video.Another approach to solving the sparse rewards problem is to learn a reward function from an expert/optimal policy or from demonstrations (inverse reinforcement learning) or to completely avoid using the reward function and simply learn the policy directly from demonstrations (imitation learning).Note that I'm not saying that any of these solutions fully solves all your problems. They have advantages (e.g. if done correctly, PBRS can speed the learning process) but also disadvantages (e.g., in the case of IRL, if your expert demonstrations are scarce, then your learned RF may also not be good), which I will not discuss further here (also because I don't currently have much practical experience with none of these techniques)."
How to treat (label and process) edge case inputs in machine learning?,"
In every computer vision project, I struggle with labeling guidelines for border cases. Benchmark datasets don't have this problem, because they are 'cleaned', but in real life unsure cases often constitute the majority of data.
Is 15% of a cat's tail a cat? Is a very blurred image of a cat still a cat? Are 4 legs of a horse, but the rest of its body of the frame still a horse?
Would it be easier or harder to learn a regression problem instead of classification? I.e by taking 5 subclasses of class confidence (0.2,0.4,0.6,0.8,1.) and using them as soft targets?
Or is it better to just drop every unsure case from training or/and testing set?
I experimented a lot with different options, but weren't able to get any definitive conclusion. This problem is so common that I wonder if it has already been solved for good by someone?
","['computer-vision', 'classification', 'datasets', 'object-detection', 'yolo']",
How to go about classifying 1000 classes?,"
I am trying to find research paper with theory(preferably implementation) that is about classifying 1000 (or more) classes. I have heard of an implementation, that initially clustering needs to be done then classification with something like softmax. Does anyone know of any research paper that implements 1000+ class classification.
","['machine-learning', 'classification', 'text-classification']",
"Is a learned policy, for a deterministic problem, trained in a supervised process, a stochastic policy?","
If I trained a neural network with 4 outputs (one for each action: move down, up, left, and right) to move an agent through a grid (deterministic problem). The output of the neural network is a probability distribution over the 4 actions, due to the softmax activation function.
Is the policy (based on the neural network) a stochastic policy, even if the action space is discrete?
","['neural-networks', 'policies', 'deterministic-policy', 'stochastic-policy', 'softmax-policy']",
Improving Mask RCNN by arbitrary scaling head input,"
Currently, I am looking at how Mask R-CNN works. It has a backbone, RPN, heads, etc. The backbone is used for creating the feature maps, which are then passed to the RPN to create proposals. Those proposals would then be aligned with feature maps and rescaled to some $n \times n$ pixels before entering box head or mask head or keypoint head.
Since conv2D is not scale-invariant, I think this scaling to $n \times n$  would introduce scale-invariant characteristics.
For an object that is occluded or truncated, I think scaling to $n \times n$ is not really appropriate.
Is it possible if I predict the visibility of the object inside the box head (outputting not only xyxy [bounding box output], but also xyxy+x_size y_size [bounding box output  + width height scale of object]). This x_size and y_size value would then be used to rescale $n \times n$  input.
So, if only half of the object is seen (occluded or truncated), inputs inside the keypoint head or mask head would be 0.5x by 0.5x.
Is this a good approach to counter occlusion and truncation?
","['convolutional-neural-networks', 'object-detection', 'mask-rcnn']",
Why is the update in-place faster than the out-of-place one in dynamic programming?,"
In Barto and Sutton's book, it's written that we have two types of updates in dynamic programming

Update out-of-place
Update in-place

The update in-place is the faster one. Why is that the case?
This is the pseudocode that I used to test it.
if in_place:
    state_values = new_state_values
else:
    state_values = new_state_values.copy()
old_state_values = state_values.copy()

for i in range(WORLD_SIZE):
    for j in range(WORLD_SIZE):
        value = 0
        for action in ACTIONS:
            (next_i, next_j), reward = step([i, j], action)
            value += ACTION_PROB * (reward + discount * state_values[next_i, next_j])
        new_state_values[i, j] = value

max_delta_value = abs(old_state_values - new_state_values).max()
if max_delta_value < 1e-4:
    break

Why is the in-place version faster, and what is the difference? What I think is that it is only better for storage usage, I don't understand the speed increase part.
","['reinforcement-learning', 'sutton-barto', 'policy-evaluation', 'dynamic-programming']","When you make updates in-place, then some of the entries in state_values[next_i, next_j] that you are referencing herewill already be updated earlier in the same loop. Which means you get to use the latest and likely more accurate values earlier.The strength of this effect varies depending on the order that states and actions are visited. If you can manage to visit them in reverse order that they would appear in natural trajectories, then the speed improvement will be very noticeable."
What is the target output for updating a Deep Q Network,"
I'm trying to implement Deep Q-Learning for a pet problem having a continuous state space and discretized action space.
The algorithm for table-based Q-Learning updates a single entry of the Q table - i.e. a single $Q(s, a)$. However, a neural network outputs an entire row of the table - i.e. the Q-values for every possible action for a given state. So, what should the target output vector be for the network?
I've been trying to get it to work with something like the following:
q_values = model(state)
action = argmax(q_values)
next_state = env.step(state, action)
next_q_values = model(next_state)
max_next_q = max(next_q_values)

target_q_values = q_values
target_q_values[action] = reward(next_state) + gamma * max_next_q

The result is that my model tends to converge on some set of fixed values for every possible action - in other words, I get the same Q-values no matter what the input state is. (My guess is that this is because, since only 1 Q-value is updated, the training is teaching my model that most of its output is already fine.)
What should I be using for the target output vector for training? Should I calculate the target Q value for every action, instead of just one?
","['neural-networks', 'reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']","As you say, the output of a $Q$ network is typically a value for all actions of the given state. Let us call this output $\mathbf{x} \in \mathbb{R}^{|\mathcal{A}|}$. To train your network using the squared bellman error you need first calculate the scalar target $y = r(s, a) + \max_a Q(s', a)$. Then, to train the network we take a vector $\mathbf{x'} = \mathbf{x}$ and change the $a$th element of it to be equal to $y$, where $a$ is the action you took in state $s$; call this modified vector $\mathbf{x'}_a$. We calculate the loss $\mathcal{L}(\mathbf{x}, \mathbf{x'}_a)$ and back propagate through this to update the parameters of our network.Note that when we use $Q$ to calculate $y$ we typically use some form of target network; this can be a copy of $Q$ where the parameters are only updated every $i$th update or a network whose weights are updated using a polyak average with the main networks weights after every update.Judging by your code it looks as though your action selection is what might be causing you some problems. As far as I can tell you're always acting greedily with respect to your $Q$-function. You should be looking to act $\epsilon$-greedily, i.e. with probability $\epsilon$ take a random action and act greedily otherwise. Typically you start with $\epsilon=1$ and decay it each time a random action is taken down to some small value such as 0.05."
NN to find arbitrary transformation,"
Problem description
I'm creating a clock with 4 seven-segment LED displays. In an effort to get more familiar with tensorflow, I figured I should try to drive this clock with use of a Neural Network.
The input of the network is the Unix time.
Initially I wanted to make the network output a UINT32 value, which I can then bit-shift into the shift-registers I use for driving the LEDS. Because this proved to be unsuccessful, I removed the last step, and instead went with 32 booleans for every led-segment as output.
Current status
This last action was unfruitful as well, the best I get my network is a loss of about 0.48, indicating to me that it's best effort is to guess what the output could be.
Input
print(x_validate[0])
print(y_validate[0])
3116360099
[0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0]

The input is the unix time code. I tried normalizing this by dividing by 2^32, but this didn't make any significant difference.
Validation is an array with either 0 or 1 based on whether the LED needs to be on or off. The first 8 bits represent the first 7-segment display, etc. (The 8th bit is never used here because that's connected to the dot on the display.)
Data generation
# Number of sample datapoints
SAMPLES = 2000000

# Generate a uniformly distributed set of random numbers in the range from
# 0 to uin32 max
x_values = np.random.uniform(
    low=0, high=2**32, size=SAMPLES).astype(np.uint32)

# Shuffle the values to guarantee they're not in order
np.random.shuffle(x_values)

print(x_values)

# Time helper function 
def to_utc(x):
  return datetime.utcfromtimestamp(x).replace(tzinfo=timezone.utc).astimezone(pytz.timezone(""Europe/Amsterdam"")).strftime('%H%M')
to_utc_batch = np.vectorize(to_utc)

y_values = to_utc_batch(x_values)
print(y_values)

# translate to bitstream
def lookup(number):
  switch = {
      0: [1,1,1,1,1,1,0,0],
      1: [0,1,1,0,0,0,0,0],
      2: [1,1,0,1,1,0,1,0],
      3: [1,1,1,1,0,0,1,0],
      4: [0,1,1,0,0,1,1,0],
      5: [1,0,1,1,0,1,1,0],
      6: [1,0,1,1,1,1,1,0],
      7: [1,1,1,0,0,0,0,0],
      8: [1,1,1,1,1,1,1,0],
      9: [1,1,1,1,0,1,1,0]
  }
  return switch.get(number)

print(y_values)

def compile_output(value):
  f = []
  for i, c in reversed(list(enumerate(value))):
    f = f + lookup(int(c))
  return f

output_values = []

for y in y_values:
  output_values.append(compile_output(y))

y_values = output_values

After, data is distributed in 3 sets:
Training: 60%
Validation: 20%
Testing: 20%

Model
model = tf.keras.Sequential()
model.add(keras.layers.Dense(32, activation='relu', input_shape=(1,)))
model.add(keras.layers.Dense(64, activation='relu'))
model.add(keras.layers.Dense(64, activation='relu'))

# Final layer
model.add(keras.layers.Dense(32, activation='sigmoid'))

model.compile( optimizer='adam', loss='binary_crossentropy', metrics=[""accuracy""])

I went with binary_crossentropy and sigmoid as I figured the case is essentially a multi-label setup.
I have tried the following already, but did not succeed:

Add more layers
Make dense layers wider (to max of 512)
Add a Dropout layer -> so to have it try out more things to find the relation
Use Softmax activation
Normalize input data by dividing Unix time by 2^32
enlarge sample data to 20.000.000
do anything between 10 and 50 epochs ( I usually quit after 15 epochs, when absolutely no change was observed between the last 5 epochs)

Question

Why is this model not successful in finding a relation between the data?
How can I improve this model or the data so it will be able to succeed?

Bonus
When successful, ideally the output of the model would be a UINT32 number instead of this array of booleans. Any tips on how to get there would be appreciated as well.
Edit:
Really sorry, left out this particular line:
# Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=64,
                    validation_data=(x_validate, y_validate))

","['machine-learning', 'tensorflow']",
Can the rewards be matrices when using DQN?,"
I have a basic question. I'm working towards developing a reward function for my DQN. I'd like to train an RL agent to edit pixels on an image. I understand that convolutions are ideal for working with images, but I'd like to observe the agent doing it in real-time. Just a fun side project.
Anyway, to encourage an RL agent to craft a specific image I'm crafting a reward function that returns a $N \times N$ dimensional matrix. Which represents the distance between the state of the target image (RGB values for each pixel location) and the image the agent crafted.
Generally speaking, is it better for rewards to be a scalar, or is using matrices okay?
","['reinforcement-learning', 'dqn', 'reward-functions', 'reward-design', 'multi-objective-rl']","Generally speaking, is it better for rewards to be a scalar, or is using matrices okay?Rewards need to be scalar, real values to match to standard theory of Markov decision processes (MDPs) and reinforcement learning (RL) methods.Although it is possible to accumulate matrices in various ways, by e.g. simple matrix addition, and come up with an analog for expected return which would be a weighted sum of matrices, you then get stuck. There is no fixed way to rank matrices and decide whether one is a better result than another. This is a requirement for any learning process that aims to improve at a task - it needs feedback that changes it makes are better or worse related to some reference. As a result, most objective functions and metrics in optimisation use real-valued scalars, which can always be placed into order to decide a highest or lowest value.This does not prevent you using a matrix representation for your project, if it is a natural fit. To turn it into a usable reward, you will need to convert that matrix into a real-valued metric. Perhaps the L2 norm or other standard measure that summarises the matrix will be good for your task.It is possible to process multiple scalar rewards at once with single learner, using multi-objective reinforcement learning. Applied to your problem, this would give you access to a matrix of policies, each of which maximised the reward value of one cell within the matrix. It also allows for switching between objectives in a hierarchical manner using a ""policy of policies"" if some preference for what to achieve changes. It is not 100% clear, but I do not think this is what you want to do."
"Why are the weights of the previous layers updated only considering the old values of the weights of the later layer, not the updated values?","
Why are the weights of a neural net updated only considering the old values of the later layer, not the already updated values?
I use this example to explain my problem. When applying the backpropagation chain rule, the weights  of the previous layer ($w_1, w_2, w_3, w_4$) are updated making use of the chain rule:
$$\frac{\partial E_{total}}{\partial w_1} = \frac{\partial E_{total}}{\partial out_{h1}} * \frac{\partial out_{h1}}{\partial net_{h1}}*\frac{\partial net_{h1}}{\partial w_1}$$
He then says:
$$\frac{\partial net_{o1}}{\partial out_{h1}}=w_5$$
Although he has already calculated the updated value for $w_5$, he uses the old value of $w_5$ to update $w_1$? Because the updated value of $w_1$ will have an impact on the outcome together with the updated value of $w_5$?
","['deep-learning', 'backpropagation']",
"Why does my model not improve when training with mini-batch gradient descent, while it does with Adam?","
I am currently experimenting with the U-Net. I am doing semantic segmentation on the 2018 Data Science Bowl dataset from Kaggle without any data augmentation.
In my experiments, I am trying different hyper-parameters, like using Adam, mini-batch GD (MBGD), and batch normalization. Interestingly, all models with BN and/or Adam improve, while models without BN and with MBGD do not.
How could this be explained? If it is due to the internal covariate shift, the Adam models without BN should not improve either, right?
In the image below is the binary CE (BCE) train loss of my three models where the basic U-Net is blue, the basic U-Net with BN after every convolution is green, and the basic U-Net with Adam instead of MBGD is orange. The learning rate used in all models is 0.0001. I have also used other learning rates with worse results.

","['u-net', 'batch-normalization', 'adam', 'mini-batch-gradient-descent', 'semantic-segmentation']","Well, some time ago I also faced the same issue in the semantic segmentation task. Batch normalization is expected to improve convergence, because the normalization of activations prevents the explosion of the gradients magnitude and leads to more steady convergence.Adam is an adaptive optimizer with momentum and division by the weighted sum of gradients on previous iterations squared. https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c.The loss surfaces of the neural networks is a difficult and poorly understood topic in the present. I suppose, that the poor convergence of SGD is caused by the roughness of loss surface, where the gradient makes big leaps, and jumps over the mimima. The adaptive learing strategy of the Adam, on the other hand, allows to dive into the valleys."
What are the popular approaches to estimating the Q-function?,"
I need the q-value for my RL training, there are some approaches:

Brute-force the action sequence (this won't work for long sequence)
Use a classic algorithm to optimise and estimate (this ain't much AI)
Create Monte Carlo samples and train an approximator network for calculating q-value

I find the Monte Carlo method above rather widely applicable to different problems, and the more computing power, the more precise it is. Any other methods for calculating q-value?
","['reinforcement-learning', 'q-learning', 'monte-carlo-methods', 'value-functions']","There's are some solutions to calculating q-values; find the exact values:Estimate the q-values:Gradually optimise the q-values:(1) Do optimisation based on Bellman equation (Q-learning):
$$ q_t(s_t,a_t) = q_t(s_t,a_t) + \alpha(r + \gamma\times\max(q_{t+1}(s_{t+1},a_{t+1})) - q_t(s_t,a_t))$$Bellman equation is true when the temporal difference (the part multiplied with $\alpha$) reaches zero, which means the max of time t+1 reaches exact value.(2) Do optimisation based on Bellman equation (Q-network), fit the neural network to expected value:
$$r + \gamma\times\max(q_{t+1}(s_{t+1},a_{t+1}))$$"
Single-value loss/training in a CNN with a tensor output,"
I am playing around with an idea of using using Q-learning with a DQN (Deep Q-Network), to determine the optimal position of a number of 'units' on a grid of allowed locations, according to some reward-metric that is only calculated after placing all of the units. I am following much of Deep Convolutional Q-Learning with Python and TensorFlow 2.0, but I diverge with the grid output.
To do this, I use a CNN that takes in a one-hot encoded grid of the units (0=no units here, 1=one unit here), and outputs a grid with the same shape as the input with expected rewards of placing a unit at this location.
I can use the following Tensorflow/Keras code to get the expected rewards, where units can be placed in 3 dimensions and channels determining the different unit-styles:
from tensorflow.keras import layers, models

model = models.Sequential(name=""DQN"")
model.add(
    layers.Conv3D(
        filters=10,
        kernel_size=3,
        activation=""relu"",
        padding=""same"",
        input_shape=input_shape,
        bias_initializer=""random_normal""
    )
)
model.add(layers.Conv3D(filters=10, kernel_size=3, activation=""relu"", padding=""same""))
model.add(layers.Conv3D(filters=input_shape[-1], kernel_size=3, activation=""relu"", padding=""same""))

model.compile(optimizer=""adam"", loss=tf.keras.losses.Huber(), metrics=[""accuracy""])

Currently I am using a very simple training scheme, where Q-values are first generated from the current state. At the position where the agent earlier placed a unit, the calculated true reward is given and trained against.
If the following state was a terminated state, the calculated reward is used directly, while the discounted reward is used in non-terminated states.
for state, action, next_state, reward, terminated in minibatch:
    # state: one-hot grid
    # action: index in the state where a unit is placed by the agent
    # terminated: True/False whether the 'next_state' is the terminated state

    q_target = model.predict(state)

    if terminated:
        q_target[0][action] = reward
    else:
        following_target = model.predict(next_state)
        q_target[0][action] = reward + gamma * np.amax(following_target)

    model.fit(state, q_target, epochs=1, verbose=0)

This means, that only a single value in the entire training tensor is the true reward - all other are approximated by the CNN.
However, all of the expected rewards are used in training, instead of this singular value. So I was considering whether it would be possible to train the CNN towards this single value only, and whether it would make any sense at all?
I thought of creating a custom loss function that would calculate the loss function for this single action, so training is done against this. However, I can't really figure out how I would go about doing this. I've looked at something like Custom training with tf.distribute.Strategy, but I wasn't successful at it..
","['training', 'tensorflow', 'python', 'keras', 'objective-functions']",
What model to use to get a robust model to predict next 3 days of sales even for products that have just sold once ever?,"

PROJECT: I am working on an e-commerce site where digital products can run out so there is need to reorder them 72h before they run out (reordering them sooner is not a problem but having notification a bit later so if the product would sell better that would be a problem because we cannot reorder products in time).
GOAL: is to know if products run out at least 72h earlier.
DATA COLUMNS: sales datetime, product id, current number of products, price of product, what currency it was purchased in, other data like profit currency was used for the purchase…
SIZE: Before grouping I have a few millions of rows after grouping hundreds of thousands so it is a lot of data point but DASK can handle them.
GRUPPING COLUMNS: I have grouped the data by PURCAHSEDATE & ID so each day has the product that were sold with all its feature. Features have been aggregated mostly buy summing (profit, expenses) and mean (percentage features like margin%)
HOW FAR I HAVE GONE WITH THE PROECJT: I have looked up a couple of Kaggle projects online that were focused on use https://www.kaggle.com/tejasrinivas/simple-xgb-starter
PROBLEMS: A.) Some product has been sold in the past but they are selling out in 1-2 days so it is hard to put trendline on it. B.) Some item just has 1-2 days of data because it just started to sell a few days ago. C.) I also have data of products that have been sold a lot for a mid or long run (hundreds of days thousands of times). So I could do time series modelling on the whole of the sales but for each individual item I don't always have data on it
CURRENT RESULTS: I have used XGBOSOT Regression like It predicts well number of products sales after the days is over with all the features, but that is not the goal  - https://www.kaggle.com/tejasrinivas/simple-xgb-starter
PROJECT RECOMMEND:I am trying to use the following pick ideas from the following competitions: https://www.kaggle.com/c/demand-forecasting-kernels-only/notebooks?competitionId=9999&sortBy=voteCount  ,  https://www.kaggle.com/c/competitive-data-science-predict-future-sales/code
GOAL: simple and easy solution, not LSTM or something complicated but something quick and easy (like xgboost regression so if I have more data I can use rapids.ai to GPU teach it) to implement because as I said it is not a problem if it is missing on the time frame on the positive side and the item gets reordered 96h early and not 72h early. I am guessing that somehow, I should shift the dates but as I said in many case items have not enough dates to shift their sales date.

","['python', 'time-series', 'regression']","If you have sold only once or very few items you will need some prior input (domain knowledge). One term for search is intermittent time series. Here is a stored search.When you have many time series, related, and interest in both totals and single series, that is called hierarchical forecasting.  One expert is here (the author of that blog was the founder of sister site Cross Validated).With time series forecasting it is often difficult to beat simple methods,  see https://stats.stackexchange.com/questions/135061/best-method-for-short-time-series/135146#135146"
"In Q-learning, wouldn't it be better to simply iterate through all possible states?","
In Q-learning, all resources I've found seem to say that the algorithm to update the Q-table should start at some initial state, and pick actions (which are sometimes random) to explore the state space.
However, wouldn't it be better/faster/more thorough to simply iterate through all possible states? This would ensure that the entire table is updated, instead of just the states we happen to visit. Something like this (for each epoch):
for state in range(NUM_STATES):
  for action in range(NUM_ACTIONS):
    next_state, reward = env.step(state, action)
    update_q_table(state, action, next_state, reward)

Is this a viable option? The only drawback I can think of is that it wouldn't be efficient for huge state spaces.
","['reinforcement-learning', 'q-learning', 'exploration-exploitation-tradeoff', 'exploration-strategies']","If your algorithm is executed multiple (or enough) times using an outer loop, it would converge to similar results as Q-learning would with $\gamma = 0$ (as you don't look what is the expected future reward).In this case, the difference is that you would pass as much time to explore each possible couple of (state, action) while Q-learning would pass more time on the pair which seem more promising and, as you've said this wouldn't be efficient for a problem with a huge number of pair (state, action).If the algorithm is executed only once, then, even for a problem with a few pairs (state, action), you need to assume that an action effected on a state will always bear the same result for your method to work.In most cases, it isn't true either because there is some sort of randomness in the reward system or in the action (your agent can fail to make an action) or because the state of your agent is limited to its knowledge and so doesn't represent perfectly the world (and so the consequence of its action can vary just like if the reward had some randomness).Finally, your algorithm doesn't look at the expected future reward, so it would be equivalent to having $\gamma = 0$. This could be fixed by adding a new loop updating the table after your current loops if you execute your algorithm only one time or by adding the expected future reward directly to your Q-table if there is an outer loop.So, in conclusion, without the outer loop, your idea would work for a system with few pairs of (state, action), where your agent has a perfect and complete knowledge of its world, the reward doesn't vary, and where an agent can't fail to accomplish an action.While these kinds of systems indeed exist, I don't think that it's an environment where one should use Q-learning (or another form of reinforcement learning), except if it's for educational purposes.With an outer loop, your idea would work if you are willing to pass more time training to have a more precise Q-table on the least promising pair of (state, action)."
Is there any research on the detection of the user's emotion and stress based on the mouse movement and keyboard?,"
I have to create a model that can detect the user's emotion and stress level based on their mouse movement and keyboard typing activity. I didn't found any research work based on this. Is there any research on this?
","['reference-request', 'emotional-intelligence', 'affective-computing', 'emotion-recognition']",
Why does sigmoid saturation prevent signal flow through the neuron?,"
As per these slides on page 35:

Sigmoids saturate and kill gradients.
when the neuron's activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero.
the gradient and almost no signal will flow through the neuron to
its weights and recursively to its data.

So, if the gradient is close to zero, then the error correction would be very minimal. But why would that cause that no signal flow through the neuron?
$$w(n+1) = w(n) - \text{gradient}$$
That would only cause the weights not to change.
","['neural-networks', 'backpropagation', 'weights', 'sigmoid', 'vanishing-gradient-problem']",
Understanding loss function gradient in asynchronous advantage actor-critic (A3C) algorithm,"
This is a question I posted here. I am asking it on this StackExchange branch as well, so that more people who could potentially answer get to see the question.
In the A3C algorithm from the original paper:

the gradient with respect to log policy involves the term
$$\log \pi(a_i|s_i;\theta')$$
where $s_i$ is the state of the environment at time step $i$, and $a_i$ is the action produced by the policy. If I understand correctly, the output of the policy is a softmax function, so that if there are $n$ different actions, then we get the $n$-dimensional vector output
$$\pi(s_i;\theta')=\left(\frac{e^{o_1(s_i)}}{\sum_{l=1}^n e^{o_l(s_i)}},\frac{e^{o_2(s_i)}}{\sum_{l=1}^n e^{o_l(s_i)}},...,\frac{e^{o_n(s_i)}}{\sum_{l=1}^n e^{o_l(s_i)}}\right),$$
where the $o_j(s)$ are softmax layer activations obtained from forward propagation of state $s_i$ through the neural network.
Do I understand correctly that in the A3C algorithm above the term $\log \pi(a_i|s_i;\theta')$ refers to
$$\log \pi(a_i|s_i;\theta') = \log\left(\frac{e^{o_j(s_i)}}{\sum_{l=1}^n e^{o_l(s_i)}}\right)$$
with index $j$ referring to the position of the largest element in vector $\pi(s_i;\theta')$ above? Or maybe all action options should be contributing according to their probabilistic weights, like so
$$\log \pi(a_i|s_i;\theta') = \sum_{j=1}^n\log\left(\frac{e^{o_j(s_i)}}{\sum_{l=1}^n e^{o_l(s_i)}}\right)~~~?$$
Or perhaps neither of these expressions is correct? In that case, what is the correct explicit formula for the expression $\log \pi(a_i|s_i;\theta')$ in terms of softmax layer activations $o_j$?
","['reinforcement-learning', 'policy-gradients', 'softmax', 'a3c']",
How are Ground truth provided to each Pyramid map in RetinaNet or YOLOv3 Paper? How is the mapping of Feature Pyramids done to Ground Truth,"
SO the YOLO V3 and RetinaNet both uses the Feature pyramids which look something like this:
(except b and e which have one output)
I'm just confuse how the predictions and training is done?
Do we have to give EACH feature map a different Y label? IF yes, how is that possible? We need to have N different ground truth in my opinion. (Also ther'll be 3 different losses I think?)
If not, then how are these done at once?
There is a lot of confusion on these networks because I am not able to get my head around How are y-labels provided, trained and predicted in YOLOv3 and RetinaNet . Everything will make sense about loss, multioutputs and all if I know this one thing.
","['deep-learning', 'computer-vision', 'object-detection', 'object-recognition', 'yolo']",
Are there any active areas of research in machine learning that do not involve neural networks at all?,"
So far, I have not been able to find many papers that do not involve neural networks, so I was hoping I can gain some insight here. Any references would be greatly appreciated.
","['neural-networks', 'machine-learning', 'reference-request', 'research']","If you look into the top conferences on machine learning and neural networks, such as NeurIPS, ICLR, and ICML, you will find many papers related to neural networks and deep learning, given that these are still very hot/promising topics. However, occasionally, you will find accepted papers that do not involve neural networks. Here's a small list of them that I've found after a quick search.So, yes, research in machine learning is not exclusively devoted to neural networks and deep learning. You can probably find more papers here or in the proceedings of similar conferences or journals."
Why does the relativistic discriminator increase the probability that generated data are real and decrease the probability that real data are real?,"
I was reading the ESRGAN whitepaper, where I came across this line:

Relativistic discriminator [2] is developed not only to increase the probability that generated data are real but also to simultaneously decrease the probability that real data are real.

Can somebody explain this?
","['deep-learning', 'papers', 'generative-adversarial-networks', 'discriminator']",
"If one of the inputs to a neural network (that represents a policy) is noisy and degrades the performance, would this architecture solve the issue?","
I'm using genetic algorithms to train deep reinforcement learning (DRL) agents, similarly to what was done in this paper. DRL policies are therefore represented by deep neural networks, which map states into deterministic actions. My state space consists of three state variables $v_1, v_2$ and $v_3$. Variable $v_1$ is extremely noisy and seems to be degrading the performance (i.e. the return or discounted cumulative reward) of my RL agent but, for certain reasons, I have to include it. The return is precisely my fitness function.
Currently, my DNN looks like this:

There is only 1 output since the action space is 1-dimensional (one degree of freedom, which is used to control a system).
The DNN tends to overfit more quickly when $v_1$ is present. I'm considering creating a custom NN that looks like this:

By doing this I would reduce the complexity of the influence of the variable $v_1$ on the output, since the number of layers between $v_1$ and the output node would be reduced.
I have reasons to believe that the optimal control depends linearly or (something close to linearity) on $v_1$.
Does this make any practical sense and are there are reasons why one should avoid doing this?
","['neural-networks', 'deep-rl', 'genetic-algorithms', 'regularization']",
Is the GAN architecture better suited for medical image denoising than the CNN?,"
I'm considering using GANs for medical image denoising, based on previous literature, like this and this. My input to the GAN would be a high-noise image and my ideal output would be a low-noise, high-quality image.
Is the GAN architecture better suited for applications where the inputs are just random noise? Is the discriminator necessary in this case or is it better to just use a Deep CNN/Autoencoder? How do I justify using a GAN for my application?
","['convolutional-neural-networks', 'generative-adversarial-networks', 'image-processing', 'denoising-autoencoder', 'image-denoising']",
Can stochastic gradient descent be properly used in any sample based learning algorithm in Reinforcement Learning?,"
Assuming we use an MSE cost function of the form
$$ \sum_s\mu(s)(V_{\pi}(S_t)-\hat{V}(S_t,\theta_t))^2 =  E_{\mu(s)}[(V_{\pi}(S_t)-\hat{V}(S_t,\theta_t))^2])$$
The Stochastic Gradient Descent is used to approximate the true update algorithm, which looks like this
$$\theta_{t+1} = \theta_{t} - \frac{\eta_t}{2}\nabla_{\theta}(E_{\mu(s)}[(V_{\pi}(S_t)-\hat{V}(S_t,\theta_t))^2])$$
to this
$$\theta_{t+1} = \theta_{t} - \frac{\eta_t}{2}\nabla_{\theta}(U_t-\hat{V}(S_t,\theta_t))^2$$
where, for simplicity, $U_t$ represents an unbiased estimate of the true value function $V_{\pi}(s_t)$. This expression is the source of many learning algorithms used in reinforcement learning.
One of the conditions for SGD requires that samples used for updating the parameters must be I.I.D according to the distribution $\mu(s)$. However, in both on-policy and off-policy learning methods, updates at each time-step are based on trajectories generated. Since, along a trajectory, the state $s_{t+1}$ depends on the state at ${s_t}$, this means that the sample used to update $\theta_t$ and $\theta_{t+1}$ are not independent. Many, if not all, sample-based learning algorithms used in RL rely on using SGD, such as the Gradient Monte Carlo Algorithm but I've not really seen anywhere that mentions these algorithms have the ""issue"" that I mention so I feel like I'm missing something. More formally,

My Question: Does the fact that parameter updates are not I.I.D mean we can't really use stochastic gradient descent AT ALL in learning algorithms, and, if so, why then do these algorithms ""work""?

As far as I know, this question applies equally to all forms of parameterised function approximation that are used with learning algorithms (tabular functions*, linear functions and non-linear functions). But, if anyone knows a special reason as to whether these cases should be treated separately could they make clear why
*I understand that when learning algorithms with tabular functions, there exists theory beyond SGD that ensures convergence, however, I'm not entirely sure what this theory is and whether if this makes them exempt, so if anyone knows whether or not it does make them exempt could they also make this clear!

Edit:
It has been highlighted in the comments that replay buffers have been used to resolve the issue of correlated sampling in cases such as DQN and variants of it. This implies correlated sampling is an issue in these cases. Aside from this, I've not heard of replay buffers being used elsewhere (correct me if I'm wrong), so why are replay buffers needed with this off-policy NN approach but not in other learning algorithms given that they all suffer from the issue of correlated sampling.
","['reinforcement-learning', 'deep-rl', 'function-approximation', 'experience-replay', 'stochastic-gradient-descent']",
What is the first short film completely made by AI?,"
I have created (not me exactly) a short film entirely made by AI. There many short films (like Sunspring) 'written' by AI but were acted out by humans. In my short film, the story is by the AI, the music is by the AI, the title art is by the AI, the visuals and acting is by the AI (yes the AI can act) and the dialogue is by the AI. So, everything is AI. What I wanted to know is if this is the first one like this. I can't seem to find others online.
","['reference-request', 'story-generation']",
"Can I use a CNN for template matching, so that there is robustness, as the background of the target image is not that good?","
I have to extract part of a source image, then I have to check if it is similar or almost similar to any of the 10 target images, so that I can do further processing on that one specific target image, which is similar to the source image. It's like template matching, but they have to loop over 10 different images to find whether a matching template is found in any of those images or not.
I wanted to use a CNN-based solution, as a classical distance-based solution is giving poor results.
Can I use a CNN for template matching, so that there is robustness, as the background of the target image is not that good, and it causes a problem? If some resource can be pointed that would be great too.
","['convolutional-neural-networks', 'computer-vision', 'reference-request', 'image-processing', 'template-matching']",
How to design a fitness function for a problem where there are 2 objectives?,"
I am told to express a fitness function for a question I have been presented. I am unsure how I would express the function. In words, what I have written down makes sense but turning this into a mathematical formula is proving a bit difficult. My understanding is:
The fitness function for this scenario will want to ensure that the best offer for building the computers is chosen whilst the price of the final optimal offer is low.
The fitness function in this case would want to consider a few factors. The first factor is that the quantity of the computer parts was enough that each offers that were returned had a sufficient quantity of parts. Ideally, it would be best if we did not have any duplicates of parts in the offers. The cost is low too, but all parts have been found amongst the different offers that we have.
The fitness function will need to ensure all of this is factored in.
The scenario and question are below:

For the production of a number of laptops, a computer company needs a quantity of each component such as screens  (S),  hard drives  (HD),  optical drives  (OD),  RAM,  video cards  (VC),  CPU,  Ports,  etc.  The company received a number of priced offers. Note that offers do not contain all components. As examples:

Offer 1: 1000 RAMs, 800 HDs, 2000 ODs â€“ Â£75K
Offer 2: 1850 S, 1570 OD - Â£40K
Offer 3: 3000 HD, 2000 RAM â€“ Â£70K
Offer 4: 1500 RAM, 2000 VC, 1700 S â€“ Â£55Ketc.

The company would be interested to accept cheaper offers in the first place. Answer the following: Give the expression of the fitness function.

Any help would be greatly appreciated ğŸ˜Š.
","['genetic-algorithms', 'homework', 'fitness-functions', 'fitness-design']","If we assume that each laptop requires 1 component of each type, so 1 screen, 1 hard drive, 1 RAM, etc. (i.e. the company has no preference for the type of component), then the company, to maximize the number of laptops it can build (which is supposedly the ultimate goal of the company), it shouldmaximize the number of instances of the least available component in the offer, andminimize the cost of the offerSo, one possible fitness function would then need to take these 2 objectives into account, so this would be a multi-objective optimization problem.Recall that a fitness function evaluates individuals in the population. If we assume that the individuals are the offers and the goal would be to find the best offer in the space of offers, then we can devise some fitness function $f$ of the form$$
f(o) = (1 - \alpha)\frac{1}{1 + p(o)} + \alpha \min(o), \label{1}\tag{1}
$$
whereThe fitness function $f$ in equation \ref{1} will be maximal when $p(o) = 0$ and $\min(o) = N$, where $N$ is some maximum threshold of possible number of items of the same type that an offer can have.I have just come up with this fitness function. I don't know whether it will work in practice or not, but this is the idea of what you have to do.  You can design other similar fitness functions (for example, how would you design a fitness function that takes into account that the number of components of each type should be more or less the same?). You may want to try to implement this e.g. with DEAP and see how it behaves. You probably also want to make sure that $o$ are arrays of integers (and not floating-point numbers). You probably also want to read this answer."
Object detection approaches without anchors and NMS,"
The Context
From all of the problems I have worked with in computer vision, the most challenging one is the object detection. This is not because the problem itself is complex to understand or bad formulated. But because we need to inject some strong priors about how we understand the world. Those priors are the anchors (which are priors about object shapes, aspect ratio...).
This prior information, although very simple to understand, it is very hard to inject on the training logic. Hence making the computation of the ground truth very messy and prone to errors. It is even  harder when different object detection backbones propose different ground truth computation methods.
The Question
From mid-2019 till now there is a growing trend on research about one-stage object detectors that do not rely on anchors: hence dropping the costly NMS postprocessing and in some cases even the IoU computation. I would like to do a proof of concept with some of them so here is my question:
What are some good object detectors that do not use anchors? Or said in other words, what are the go-to object detectors for this new research trend?
","['computer-vision', 'object-detection', 'model-request']",
Reasoning behind performance improvement with hopfield networks,"
In the paper Hopfield networks is all you need, the authors mention that their modern Hopfield network layers are a good replacement for pooling, GRU, LSTM, and attention layers, and tend to outperform them in various tasks.
I understand that they show that the layers can store an exponential amount of vectors, but that should still be worse than attention layers that can focus parts of an arbitrary length input sequence.
Also, in their paper, they briefly allude to Neural Turing Machine and related memory augmentation architectures, but do not comment on the comparison between them.
Has someone studied how these layers help improve the performance over pooling and attention layers, and is there any comparison between replacing layers with Hopfield layers vs augmenting networks with external memory like Neural Turing Machines?
Edit 29 Jan 2020
I believe my intuition that attention mechanism should outperform hopfield layers was wrong, as I was comparing the hopfield layer that uses an input vector for query $R (\approx Q)$ and stored patterns $Y$ for both Key $K$ and Values $V$. In this case my assumption was that hopfield layer would be limited by its storage capacity while attention mechanism does not have such constraints.
However the authors do mention that the input $Y$ may be modified to ingest two extra input vectors for Key and Value. I believe in this case it would perform hopfield network mapping instead of attention and I do not know how the 2 compare.

","['neural-networks', 'attention', 'neural-turing-machine', 'hopfield-network']",
"Is my proof of equation 0.6 in the book ""Reinforcement Learning: Theory and Algorithms"" correct?","
In Sham Kakade's Reinforcement Learning: Theory and Algorithms, this equation (page 17) is used preceding the proof of performance difference lemma.

I am attempting to prove equation 0.6. Here is my current attempt:
\begin{align*}
        \mathbb{E}_{\tau \sim \rho^\pi}\left[\sum\limits_{t=0}^\infty \gamma^t f(s_t,a_t)\right] &= \sum\limits_{t=0}^\infty \gamma^t \mathbb{E}_{\tau \sim \rho^\pi} [f(s_t,a_t)]\\
        &= \sum\limits_{t=0}^\infty \gamma^t \mathbb{E}_{s_t, a_t} [f(s_t,a_t)]\\
        &= \sum\limits_{t=0}^\infty \gamma^t \sum\limits_{s, a} \mathbb{P}(s_t = s, a_t = a) f(s,a)\\
        &= \sum\limits_{t=0}^\infty \gamma^t \sum\limits_{s} \mathbb{P}(s_t = s) \sum\limits_{a}\pi(a_t = a|s_t = s) f(s,a)\\
        &= \frac{1 - \gamma}{1 - \gamma}\sum\limits_{t=0}^\infty \gamma^t \sum\limits_{s} \mathbb{P}(s_t = s) \mathbb{E}_{a \sim \pi(s)} [f(s,a)]\\
        &= \frac{1}{1 - \gamma} \sum\limits_{s} (1-\gamma) \sum\limits_{t=0}^\infty \gamma^t \mathbb{P}(s_t = s) \mathbb{E}_{a \sim \pi(s)} [f(s,a)]\\
        &=\frac{1}{(1-\gamma)} \mathbb{E}_{s \sim d^\pi}\left[\mathbb{E}_{a \sim \pi(s)}\left[f(s,a)\right]\right] \\
\end{align*}
Is the swapping of expectation and summation in this way allowed (given that the series converges)?
Note that this is not the proof of the performance difference lemma, but just an attempt to show equation 0.6, which is used but not proved in the book.
","['reinforcement-learning', 'proofs']","The expectation of a sum is equal to the sum of the expectation this just follows from the linearity property of expectations$$ \begin{aligned} E[\sum_{t} f(s_t,a_t)] &= \sum_{\tau} p(\tau)\left(\sum_t f(s_t,a_t)\right)
 \\ &= \sum_\tau\sum_{t}p(\tau)f(s_t,a_t) 
 \\ &= \sum_t\sum_\tau p(\tau)f(s_{t,\tau},a_{t,\tau}) 
 \\ &= \sum_tE[f(s_t,a_t)]
\end{aligned} $$note that in the penultimate line I swapped the sums around which can be understood as looking at all trajectories for a single time-step instead of looking at time-steps within  a single trajectory (the second expression). I append subscript for variables in the penultimate line for clarity since they also depend on which trajectory they were drawn from.As mentioned by nboro in the comments, the linearity of property only holds for infinite sums if$$\sum_{i=0}^\infty E[|X|]< \infty  \quad or \quad E\left[\sum_{i=0}^{\infty}|X|\right]. <\infty$$More details can be found here. Using this theory alongside knowledge that since we know that value functions in RL are always bounded (in continuing tasks discounting factors are introduced) then we can say there exists an $M$ such that$$ |f(s_{t,\tau},a_{t,\tau})| \leq M \qquad \forall t,\tau $$Then$$ \begin{aligned}\sum_t\sum_\tau \lambda^t p(\tau)|f(s_{t,\tau},a_{t,\tau})| &\leq \sum_t\sum_\tau \lambda^t p(\tau)M 
\\ &=M\sum_t\lambda^t\sum p(\tau)
\\ &=M\sum\lambda^t
\\ &< \infty
\end{aligned}$$As for the rest of the proof is there anything else you need clarity on? it looks good to me. From that step to the next step where you change distribution from $p(\tau)$ to $p(s,a)$ is just because of marginalisation but I think you got that."
What is the meaning of the terms in this evaluation function for chess?,"
I'm trying to improve my evaluation and I saw this here
materialScore = kingWt  * (wK-bK)
              + queenWt * (wQ-bQ)
              + rookWt  * (wR-bR)
              + knightWt* (wN-bN)
              + bishopWt* (wB-bB)
              + pawnWt  * (wP-bP)

How do I get the value, let's say wK? Do I get the position of the king and score it relative to the board? For example, wK is more safe than the bK, so let's say wK - bK = 1 - 0.5. So, the result will be 90 * (0.5). Is this really how it works?
","['game-ai', 'minimax', 'chess', 'evaluation-functions', 'negamax']","Is this really how it works?Yes and no. An evaluation function based on pure material advantage is a perfectly legal function. It's certainly better than nothing. However, it's too simple in practice.The state-of-the-art methods involve neural network. Google ""Stockfish NNUE"" for details."
What are the biggest barriers to get RL in production?,"
I am studying the state of the art of Reinforcement Learning, and my point is that we see so many applications in the real world using Supervised and Unsupervised learning algorithms in production, but I don't see the same thing with Reinforcement Learning algorithms.
What are the biggest barriers to get RL in production?
","['reinforcement-learning', 'deep-rl', 'applications']","There is a relatively recent paper that tackles this issue: Challenges of real-world reinforcement learning (2019) by Gabriel Dulac-Arnold et al., which presents all the challenges that need to be addressed to productionize RL to real world problems, the current approaches/solutions to solve the challenges, and metrics to evaluate them. I will only list them (based on the notes I had taken a few weeks ago). You should read the paper for more details. In any case, for people that are familiar with RL, they will be quite obvious.There's also a more recent and related paper An empirical investigation of the challenges of real-world reinforcement learning (2020) by Gabriel Dulac-Arnold et al, and here you have the associated code with the experiments.However, note that RL (in particular, bandits) is already being used to solve at least one real-world problem [1, 2]. See also this answer."
"Is there a natural way to define the terminal state from the MDP transition probabilities $p(s',r|s,a)$?","
I'm learning the basics of RL and I'm struggling to understand the notion of terminal state in MDPs.
To ask my question straightforwardly: is there a natural way to define the terminal state from the MDP transition probabilities $p(s',r|s,a)$? If I need to be more restrictive, assume a game setting, for example, chess.
My first hypothesis would be to define the terminal state as the state $s_T$ such that $p(s',r|s_T,a) = p(s',r|s_T)$, a state from which the transition is independent of the agent's actions. But that does not seem quite right. First, there is no particular reason why this state should be unique. Second, from this definition, it could also just be an intermittent state of ""lag"".
","['reinforcement-learning', 'markov-decision-process', 'state-spaces']",
Can I add expert data to the replay buffer used by the DDPG algorithm in order to make it converge faster?,"
I am working on a restricted reinforcement learning environment, i.e. the environment breaks very often (i.e.: the communication between the simulator and reinforcement learning agent breaks after some time). So, it is getting difficult for me to continue training in this environment.
The continuous state-space is $\mathcal{S}  \subseteq \mathbb{R}^{10}$ and the continuous action-space $\mathcal{A} \subseteq \mathbb{R}^{2}$.
What I want to know is whether I can add expert data to the replay buffer, given that DDPG is an off-policy algorithm?
Or I should go with the behavior cloning technique to train the actor-network only, so that it converges rapidly?
I just want to get the work done first and then I can think of exploring the environment.
","['reinforcement-learning', 'ddpg', 'off-policy-methods', 'experience-replay', 'behavioral-cloning']","What I want to know is whether I can add expert data to the replay buffer, given that DDPG is an off-policy algorithm?You certainly can, that is indeed one of the advantages of off-policy learning algorithms; they're still ""correct"", regardless of which policy generated the data that you're learning from (and a human expert providing the experience to learn from can also be viewed as such a ""policy"").There are potential issues to be aware of though. For example, if you just put some expert-generated data in there and don't allow your agent to explore by itself, the experiences that you can learn from may be quite limited in the parts of the state-action space that they explore. So if your expert does not sufficiently explore the entire space, you cannot expect the agent to learn how to act if for whatever reason it ever ends up in some unexplored space. This is no different from what would happen if you trained with an agent that had too little exploration (like a greedy agent).Or I should go with the behavior cloning technique to train the actor network only, so that it converges rapidly?I cannot confidently say which approach would work better, so I cannot really answer this... I imagine the answer may also be different for specific different problem domains. But the basic principle of learning from expert data with an off-policy algorithm is not inherently wrong."
Gradual decrease in performance of a DDPG agent,"
I'm trying to solve the OpenAI's CarRacing-v0 environment with the DDPG algorithm. I've observed that after a period of learning, the agent's performance starts to deteriorate slowly. For some hyperparameter configurations this is followed by a rebound and again a slump. Here's what a typical reward plot looks like:
Since I'm new to reinforcement learning (this is my first shot at it), I don't know if this a common phenomenon. I know of catastrophic forgetting, but I believe that's not the case here, since this is more akin to a ""languishing dementia"". As far as I understand, ""catastrophic forgetting"" is an abrupt event, which contrasts with a gradual change I've been seeing in my attempts.
Is this some kind of general phenomenon with coverage in the existing literature or is this rather a quirk of my specific setup (algorithm + hyperparameters) for which the solution would be ""change the setup""?
For reference, the implementation I'm using: https://github.com/hirekk/pytorch-rl
","['reinforcement-learning', 'reference-request', 'deep-rl', 'ddpg', 'catastrophic-forgetting']",
Why are most commonly used activation functions continuous?,"
I have come to notice that the most commonly used activation functions are continuous. Is there any specific reason behind this? Results such as this paper have worked on training networks with discontinuous activations yet this does not seem to have taken off. Does anybody have insight into why this happens, or better yet an article talking about this?
","['neural-networks', 'reference-request', 'optimization', 'gradient-descent', 'activation-functions']",
"How can I go from $R(s)$ to $R(s,a)$ in this specific MDP?","
I'm trying to implement a research paper, as explained in this other post, here the author of the paper assumed R as a function of both states and actions, while the code (and the MDP) I'm using to test this algorithm assumes R as a function of only states.
My question is:
Given $\mathcal{X}$ as the set of states of an MDP and $\mathcal{A}$ as the set of actions of an MDP. Supposing I have four states ($1$,$2$,$3$,$4$), two actions $a$ and $b$ and a reward function $R: \mathcal{X}\to\mathbb{R}$ s.t.
$R(1) = 0$
$R(2) = 0$
$R(3) = 0$
$R(4) = 1$
If I need to change the current reward function to a new reward function $R:\mathcal{X}\ \times \mathcal{A} \to\mathbb{R}$ is it ok to compute it as $\forall a,R(s,a) = R(s)$?
$R(1,a) = 0$
$R(1,b) = 0$
$R(2,a) = 0$
$R(2,b) = 0$
$R(3,a) = 0$
$R(3,b) = 0$
$R(4,a) = 1$
$R(4,b) = 1$
More generally, what's the correct way of generalising a reward function
$R: \mathcal{X}\to\mathbb{R}$ to a reward function $R:\mathcal{X}\ \times \mathcal{A} \to\mathbb{R}$?
","['reinforcement-learning', 'markov-decision-process', 'reward-functions']","As explained here, I can write $R(s,a) = R(s)\  \forall a$ since the reward of my specific MDP is dependent exclusively to the state $s$."
Using LSTM model to train spatial inputs,"
I have an $x$-$y$ plane, inside that plane I have 9 paths $(p_1, p_2, \dots, p_3)$. Each path is classified into one of the three classes $(c_1, c_2, c_3)$. Each path has 100 coordinates points i.e $((x_1, y_1),(x_2, y_2), \dots, (x_n, y_n))$. Totally I have 1800 input coordinate points. Now I am interested in training the LSTM model in such a way that if I feed some test path $p_{10}$, the model should be supposed to predict which class it belongs to. This is my problem definition. Regarding this, I have some questions

First of all, is it necessary to use LSTM models to obtain a solution?
Are there any other simple models to attain a solution to this problem?
I did some literature surveys for this kind of problem using LSTM, they are having time has one of the parameters along with $(x_1, y_1, t_1)$.

The paper I have read is ""A Single-Shot Approach Using an LSTM for Moving Object Path Prediction"".
I am a beginner to sequence model neural networks. A link or examples to similar works is very much beneficial.
",['long-short-term-memory'],
Accuracy goes straight for about 200 epochs then start increasing,"
Can anyone explain the following observation?
Why did the accuracies keep to be a straight line with a very smooth decrease of loss?
Is this because of the learning rate or other reasons?
Some info:
The input is in the dimension of (319,50,40) as (batch, step, features)
The dataset consists of 319 samples. It was split using train_test_split() to yield 0.2 test size.
I used a self-attention LSTM model with 4 Dense layers and 1 self-attention LSTM layer. The codes are long, I will post them if it is needed.
The hyperparameters:
batch_size= 100
epochs =1400
learning_rate = 0.00014
optimizer = 'RMS'
num_classes = y_train.shape[1]
dropout=0.37

In addition,
If I don't set random seed and keep shuffle=True, Sometimes, I get a horizontal line till the endo of training without any increasing.

","['machine-learning', 'deep-learning']",
House price inflation modelling,"
I have a data set of house prices and their corresponding features (rooms, meter squared, etc). An additional feature is the sold date of the house. The aim is to create a model that can estimate the price of a house as if it was sold today. For example a house with a specific set of features (5 rooms, 100 meters squared) and today's date (28-1-2020), what would it sell for? Time is an important component, because prices increase (inflate over time). I am struggling to find a way to incorporate the sold date as a feature in the gradient boosting model.
I think there are a number of approaches:

Convert the data into an integer, and include it directly in the model as a feature.
Create a separate model for modelling the house price development over time. Let's think of this as some kind of an AR(1) model. I could then adjust all observations for inflation, so that we would get an inflation adjusted price for today. These inflation adjusted prices would be trained on the feature set.

What are your thoughts on these two options? Are there any alternative methods?
","['machine-learning', 'gradient-boosting']","The sold date is a feature like any other. You can do this as follow. I am assuming the features are in a pandas data frame  called df where the column date is called date. Easiest way is to use the pandas to_datetime function. Documentation is here.This function will modify the df data frame. It will create 3 new columns labeled
date year, date month and date day and it will remove the date column from the data frame.
Now these new columns along with the other features can be used to train your model."
Are there any approaches to AGI that will definitely not work?,"
Is there empirical evidence that some approaches to achieving AGI will definitely not work? For the purposes of the question the system should at least be able to learn and solve novel problems.
Some possible approaches:

A Prolog program
A program in a traditional procedural language such as C++ that doesn't directly modify its own code
A program that evolves genetically in response to selection pressures in some constructed artificial environment
An artificial neural net
A program that stores its internal knowledge only in the form of a natural human language such as English, French, etc (which might give it desirable properties for introspection)
A program that stores its internal knowledge only in the form of a symbolic language which can be processed unambiguously by logical rules

",['agi'],"Very interesting question. Assuming that the programming languages used are powerful enough (say Turing Complete), all of the above actually should lead to an AGI. The difference is in how efficiently they can do it, both in term of number of computations required and in the length of the program.So the question could be rephrased as: which approach cannot lead to an AGI using less than X resources and being shorter than Y characters?
The second question is basically asking the Kolmogorov complexity of the AGI in that language, which is uncomputable. Since we cannot find the shortest program, I don't think we can make conclusions about the maximum program efficiency either.
In summary I don't see a way to rule out any of those approaches (but I would be very happy to be proven wrong)."
How to predict multiple set of coordinates (of bounding boxes) for signboards text localization through neural network?,"
I am creating a signboard translation application from scratch. I have images of signboards where there are multiple texts and I have the corresponding set of coordinates of bounding boxes for multiple texts. I want to create a regression model which will try to predict the coordinates if there is some text in the image. I am really stuck at a place. In some cases, I have multiple words in the image, so each word will have its own set of coordinates. So, how can I make a model such that if there is a single word then it will output single set of coordinates, but if there are 5 words then it should give me 5 set of coordinates? The number of output may vary with each image. What kind of neural net should I use? I don't want to use sliding window approach. Please help me out.
","['machine-learning', 'deep-learning', 'computer-vision']",
Estimating dimensions to reduce input image size to in CNNs,"
Considering input images to a CNN that have a large dimension (e.g. 256X256), what are some possible methods to estimate the exact dimensions (e.g. 16X16 or 32X32) to which it can be condensed in the final pooling layer within the CNN network such that the important features are retained? I have found references to using linear dimensionality estimates (such as PCA) and the Riemannian Metric for non-linear estimation, but am not confident of how accurate the predicted dimensions may be.
One paper that explores this issue in Deep Neural Networks in a better way can be found here. Answers specifically pertaining to processing of SAR images would be more helpful.
","['convolutional-neural-networks', 'deep-neural-networks', 'dimensionality-reduction', 'principal-component-analysis']",
What are some of the main high level approaches to applying ML on kinematic sensor data?,"
I've just started a project which will involve having to detect certain events in a stream of kinematic sensor data. By searching through the literature, I've found a lot of highly specific papers, but no general reviews.
If I search up on computer vision, I'm likely to get 100s of articles giving overviews of different types of architectures for various vision tasks. They would look something like this:

We mainly use CNNs which work like this ...
For object detection we use one or two stage detectors which look like this...
For video classification we can use 3D CNNs or RNNs...
.... etc

So I'm looking for something similar with regard to kinematic motion sensors. As was pointed out to me on the signal processing SE, ""kinematic"" could mean a lot of things. So specifically, I'm referring to 1d time series data for:

acceleration/velocity/position
angular velocity / absolute orientation

","['machine-learning', 'reference-request', 'signal-processing']",
"What is the dimensionality of these derivatives in the paper ""Active Learning for Reward Estimation in Inverse Reinforcement Learning""?","
I'm trying to implement in code part of the following paper: Active Learning for Reward Estimation in Inverse Reinforcement Learning.
I'm specifically referring to section 2.3 of the paper.
Let's define $\mathcal{X}$ as the set of states, and $\mathcal{A}$ as the set of actions. We then sample a set of observations $\mathcal{D}$ from an agent which follows an optimal policy.
$$
\mathcal{D}=\left\{\left(x_{1}, a_{1}\right),\left(x_{2}, a_{2}\right), \ldots,\left(x_{n}, a_{n}\right)\right\}
$$
Our goal is to find the reward vector $\mathbf{r}$ s.t. the total likelihood $\Lambda_{r}(\mathcal{D})$ is maximised (every time we compute a new $\mathbf{r}$, the likelihood is updated by computing the action-value function $Q_{r}^{*}$ and taking the softmax).
$$
L_{r}(x, a)=\mathbb{P}[(x, a) \mid r]=\frac{e^{\eta Q_{r}^{*}(x, a)}}{\sum_{b \in A} e^{\eta Q_{r}^{*}(x, b)}}
$$
$$
\Lambda_{r}(\mathcal{D})=\sum_{\left(x_{i}, a_{i}\right) \in \mathcal{D}} \log \left(L_{r}\left(x_{i}, a_{i}\right)\right)
$$
Then, the paper suggests how to compute the derivatives w.r.t. $\mathbf{r}$ by defining the following quantities:
$$
\left[\nabla_{r} \Lambda_{r}(\mathcal{D})\right]_{x a}=\sum_{\left(x_{i}, a_{i}\right) \in \mathcal{D}} \frac{1}{L_{r}\left(x_{i}, a_{i}\right)} \frac{\partial L_{r}\left(x_{i}, a_{i}\right)}{\partial r_{x a}}
$$
$$
\nabla_{r} L_{r}(x, a)=\frac{d L_{r}}{d Q^{*}}(x, a) \frac{d Q^{*}}{d r}(x, a)
$$
Then, considering $\mathbf{T}=\mathbf{I}-\gamma \mathbf{P}_{\pi^{*}}$
$$
\frac{\partial Q^{*}}{\partial r_{z u}}(x, a)=\delta_{z u}(x, a)+\gamma \sum_{y \in \mathcal{X}} \mathrm{P}_{a}(x, y) \mathbf{T}^{-1}(y, z) \pi^{*}(z, u)
$$
$$
\frac{d L_{r}}{d Q_{y b}^{*}}(x, a)=\eta L_{r}(x, a)\left(\delta_{y b}(x, a)-L_{r}(y, b) \delta_{y}(x)\right)
$$
with $x, y \in \mathcal{X}$ and $a, b \in \mathcal{A} .$
In the above expression, $\delta_{u}(v)$ denotes the Kronecker delta function.
Finally, the update is trivially computed by
$$
\mathbf{r}_{t+1}=\mathbf{r}_{t}+\alpha_{t} \nabla_{r} \Lambda_{r_{t}}(\mathcal{D})
$$
Here I suppose that the paper's author is considering $\mathbf{r}$ as a matrix of dimension number of states $\times$ number of actions (i.e. each element of this matrix represents $R(s,a)$)
My question is: what is the dimensionality of $\frac{d L_{r}}{d Q^{*}}(x, a)$ and $\frac{d Q^{*}}{d r}(x, a)$? (is that a point-wise product, a matrix-matrix product, a vector-matrix product?)
The more reasonable solution, dimensionally speaking, for me would be something like:
$$
\nabla_{r} L_{r}(x, a)=\\
\frac{d L_{r}}{d Q^{*}}(x, a) \frac{d Q^{*}}{d r}(x, a) = \\
\left(\sum_{s'\in\mathcal{X}}\sum_{a'\in\mathcal{A}}\frac{d L_{r}}{d Q^{*}_{s'a'}}(x, a)\right)
\begin{bmatrix}
\frac{d Q^{\star}}{d r_{s_1a_1}}(x, a) & \dots &\frac{d Q^{\star}}{d r_{s_1a_m}}(x, a) \\
\vdots& \ddots & \vdots \\
\frac{d Q^{\star}}{d r_{s_na_1}}(x, a) & \dots & \frac{d Q^{\star}}{d r_{s_na_m}}(x, a)
\end{bmatrix}
$$
(where $n = |\mathcal{X}|$ and $m = |\mathcal{A}|$)
","['reinforcement-learning', 'papers', 'rewards', 'inverse-rl', 'derivative']",
Reinforcement Learning algorithm with rewards dependent both on previous action and current action,"
Problem description:
Suppose we have an environment, where a reward at time step $t$ is dependent not only on the current action, but also on previous action in the following way:

if current action == previous action, you get reward = $R(a,s)$
if current action != previous action, you get reward = $R(a,s) - \text{penalty}$

In this environment, switching actions bears a significant cost. We would like the RL algorithm to learn optimal actions under the constraint that switching action is costly, i.e. we would like to stay in selected action as long as possible.
The penalty is significantly higher than an immediate reward, so if we do not take it into account, the model evaluation will have a negative total reward with almost 100% probability, since the agent will be constantly switching and extracting rewards from environment that are smaller than the cost of switching actions.
Action space is small (2 actions: left, right). I'm trying to beat this game with PPO (Proximal Policy Optimization)
Questions

How one might address this constraint: i.e. explicitly make the agent learn that switching is costly and it's worth sitting in one action even if immediate rewards are negative?

How can you make the RL algorithm learn that it's not the reward term $R(a_t|s_t)$ that is negative, and thus decreasing $Q(a_t|s_t)$ and $V(s_t)$, but it's the penalty term (taking the action that is different from the previous action at step $t-1$) that is pushing total reward down?


","['reinforcement-learning', 'proximal-policy-optimization', 'reward-design', 'state-spaces', 'markov-property']","The answer to both your concerns is:It is all you need to do. It gives the agent the data it needs to learn the association of negative reward from not matching the previous action.By making this data part of the state, you re-establish the Markov property in the MDP model of the environment, which you had otherwise lost by making the reward dependent on a variable that was both systematically changing and hidden from the agent.The state in a MDP is often not just the current observations that the environment provides, but can include any relevant knowledge that the agent has. At the extreme that can include a complete history of all observations and actions taken to date. It is common practice to derive the state as a summary of recent history of observations and actions taken so far. In your case, all you need do is concatenate the previous action to the observation, because you know about the constraint and how it affects optimisation."
Why is my GAN more unstable with bigger networks?,"
I am working with generative adversarial networks (GANs) and one of my aims at the moment is to reproduce samples in two dimensions that are distributed according to a circle (see animation). When using a GAN with small networks (3 layers with 50 neurons each), the results are more stable than with bigger layers (3 layers with 500 neurons each). All other hyperparameters are the same (see details of my implementation below).
I am wondering if anyone has an explanation for why this is the case. I could obviously try to tune the other hyperparameters to get good performance but would be interested in knowing if someone has heuristics about what is needed to change whenever I change the size of the networks.


Network/Training parameters
I use PyTorch with the following settings for the GAN:
Networks:

Generator/Discriminator Architecture (all dense layers): 100-50-50-50-2 (small); 100-500-500-500-2 (big)
Dropout: p=0.4 for generator (except last layer), p=0 for discriminator
Activation functions: LeakyReLU (slope 0.1)

Training:

Optimizer: Adam
Learning Rate: 1e-5 (for both networks)
Beta1, Beta2: 0.9, 0.999
Batch size: 50

","['neural-networks', 'machine-learning', 'deep-learning', 'generative-adversarial-networks', 'heuristics']",
How to incorporate a symmetry constraint in the loss function to train a CNN?,"
I have a task of extremely sparse binary segmentation, i.e. the segmentation mask contains either 0 or 1, and there are ~95% zeros and only ~5% ones. I use the focal loss to address the sparseness (which is equivalent in my case to imbalances). I have another piece of information that I want to incorporate in the loss term.
The desired output is always symmetric over the diagonal. I was searching for a way to use this information in the loss, but I couldn't find a solution. How would I do this?
For some example of the symmetry in the segmentation maps, I added an arrow to show the axis of symmetry:


","['convolutional-neural-networks', 'objective-functions', 'pytorch', 'image-segmentation']","If you know it is symmetric, then you could do a couple things.Don't bother learning both halves of the image. Just put a zero mask over the upper or lower half of the output matrix and just have the network regress the other half. Just don't make the network do more work than it needs to do.In your case, it looks like you could create two loss functions added together.$focal(x, y) + focal(x^T, y)$This will help the network learn both halves equally.This might be silly but adding a Huber loss between $x$ and $x^T$ might help promote symmetry, but I'm not as much if a fan of it. Personally I'm more partial to (1).You could take the loss function of (2) at train time but at test time just use whatever half had better metrics and copy that to the bottom half.Add a post processing custom layer that takes the average of the two halves so that you guarantee symmetry.$x' = \frac{1}{2}(x + x^T)$Then do your normal focal loss. Personally I like this the best since it always guarantees a symmetric output and is a pretty easy custom layer."
How does the embeddings work in vision transformer from paper?,"
I get the part from the paper where the image is split into P say 16x16 (smaller images) patches and then you have to Flatten the 3-D (16,16,3) patch to pass it into a Linear layer to get what they call ""Liner Projection"". After passing from the Linear layer, the patches will be vectors but with some ""meaning"" to them.
Can someone please explain how the two types of embeddings are working?
I visited this implementation on github, looked at the code too and looked like a maze to me.
If someone could just explain how these embeddings are working in laymen's terms, I'll look at the code again and understand.
","['neural-networks', 'computer-vision', 'transformer', 'embeddings', 'vision-transformer']",
Factors that causing totally different outcomes from an exactly same model and datasets,"
Here is a model that trains time series data in (batch, step, features) way.
I have kept the random state for train test split function the same. Every parameter below the same, running the model training yields different outcomes every time and the outcomes are drastically different.
What may be the factors that led to this? Regularization?



X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=666)
def attention_model(X_train, y_train, X_test, y_test,num_classes,dropout=0.2, batch_size=68, learning_rate=0.0001,epochs=20,optimizer='Adam'):
    
    Dense_unit = 12
    LSTM_unit = 12
    
    attention_param = LSTM_unit*2
    attention_init_value = 1.0/attention_param
    
    
    u_train = np.full((X_train.shape[0], attention_param),
                      attention_init_value, dtype=np.float32)
    u_test = np.full((X_test.shape[0],attention_param),
                     attention_init_value, dtype=np.float32)
    
    
    with keras.backend.name_scope('BLSTMLayer'):
        # Bi-directional Long Short-Term Memory for learning the temporal aggregation
        input_feature = Input(shape=(X_train.shape[1],X_train.shape[2]))
        x = Masking(mask_value=0)(input_feature)
        x = Dense(Dense_unit,kernel_regularizer=l2(0.005), activation='relu')(x)
        x = Dropout(dropout)(x)
        x = Dense(Dense_unit,kernel_regularizer=l2(0.005),activation='relu')(x)
        x = Dropout(dropout)(x)
        x = Dense(Dense_unit,kernel_regularizer=l2(0.005),activation='relu')(x)
        x = Dropout(dropout)(x)
        x = Dense(Dense_unit,kernel_regularizer=l2(0.005), activation='relu')(x)
        x = Dropout(dropout)(x)


        y = Bidirectional(LSTM(LSTM_unit,activity_regularizer=l2(0.000029),kernel_regularizer=l2(0.027),recurrent_regularizer=l2(0.025),return_sequences=True, dropout=dropout))(x)
#         y = Bidirectional(LSTM(LSTM_unit, kernel_regularizer=l2(0.01),recurrent_regularizer=l2(0.01), return_sequences=True, dropout=dropout))(y)

    with keras.backend.name_scope('AttentionLayer'):
        # Logistic regression for learning the attention parameters with a standalone feature as input
        input_attention = Input(shape=(LSTM_unit * 2,))
        u = Dense(LSTM_unit * 2, activation='softmax')(input_attention)

        # To compute the final weights for the frames which sum to unity
        alpha = dot([u, y], axes=-1)  # inner prod.
        alpha = Activation('softmax')(alpha)

    with keras.backend.name_scope('WeightedPooling'):
        # Weighted pooling to get the utterance-level representation
        z = dot([alpha, y], axes=1)

    # Get posterior probability for each emotional class
    output = Dense(num_classes, activation='softmax')(z)

    model = Model(inputs=[input_attention, input_feature], outputs=output)

    optimizer = opt_select(optimizer,learning_rate)
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizer)


    hist = model.fit([u_train, X_train], 
                     y_train, 
                     batch_size=batch_size, 
                     epochs=epochs, 
                     verbose=2, 
                     validation_data=([u_test, X_test], y_test))
    

#kernel_regularizer=l2(0.002),recurrent_regularizer=l2(0.002),
    return hist


batch_size= 150
#217
epochs = 1000
learning_rate = 0.00081
optimizer = 'RMS'
num_classes = y_train.shape[1]
dropout=0.22

tf.keras.backend.clear_session()

history = attention_model(X_train, y_train, X_test, y_test, num_classes,dropout = dropout,batch_size=batch_size, learning_rate=learning_rate,epochs=epochs,optimizer=optimizer
)

","['machine-learning', 'deep-learning', 'recurrent-neural-networks', 'attention']","By default, Keras sets shuffle argument True, so you should set the numpy seed before importing Keras.CPUGPU"
Why don't those developing AI Deepfake detectors use two detectors at once so as to catch deepfakes in one or the other?,"
Why don't those developing AI Deepfake detectors use two differently trained detectors at once that way if the Deepfake was trained to fool one of the detectors the other would catch it and vice-versa?
To be clear this is really a question of can deepfakes be made to fool multiple high-accuracy detectors at the same time. And if so then how many can they fool before they become human detectable from noticeable noise?
I've heard of papers where they injected a certain noise into their deepfake videos which allows them to fool a given detector (https://arxiv.org/abs/2009.09213, https://delaat.net/rp/2019-2020/p74/report.pdf), so I thought well if they simply used two high-accuracy detectors then any pattern of noise used to fool one detector would interfere with the pattern of noise used to fool the other detector.
","['machine-learning', 'generative-adversarial-networks', 'deepfakes', 'video-classification']","Because it is possible to fool many different models at once.
See table 2 in this paper, for an example using adversarial perturbations: https://arxiv.org/pdf/1610.08401.pdfThat being said, there is no reason to think that using two detectors at once will not increase chance to detect deepfakes. It will just not resolve the problem completely."
How should I implement the state transition when it is a Gaussian distribution?,"
I am reading this paper Anxiety, Avoidance and Sequential Evaluation and is confused about the implementation of a specific lab study. Namely, the authors model what is called the Balloon task using a simple MDP for which the description is below:

My confusion is the following sentence:

...The probability of this bad transition was modeled using normal density function, with parameters $N(16, 0.5)$

But the fact that this is a continuous, normal distribution makes me stumped. In MDP's, usually there is a nice, discrete transition matrix and so there is no ambiguity as to how to implement it. For instance, if they said the transition to a bad state is modeled by a Bernoulli random variable with parameter $p,$ then it is clear how to implement it. I would do something like:
def step(curr_state, curr_action):
   if uniform random variable(0,1) < p:
      next_state = bad state

But they are using a normal random variable for this ""bad"" transition, so how do I implement this?
","['reinforcement-learning', 'markov-decision-process', 'implementation', 'temporal-difference-methods', 'transition-model']","I figured this out by going to the author's publicly available github code. It turned out  the authors were just generating the transition probability $p$ from $\mathcal{N}(\mu,\sigma^2)$ at the beginning of each episode for some reason. Answering it myself for the sake of not leaving this question unanswered."
Underfitting a single batch: Can't cause autoencoder to overfit multi-sample batches of 1d data. How to debug?,"
TL;DR
I am unable to overfit batches with multiple samples using autoencoder.
Fully connected decoder seems to handle more samples per batch than conv decoder, but then also fails when number of samples increases.
Why is this happening, and how to debug this?

In depth
I am trying to use an auto encoder on 1d data points of size (n, 1, 1024), where n is the number of samples in the batch.
I am trying to overfit to that single batch.
Using a convolutional decoder, I am only able to fit a single sample (n=1), and when n>1 I am unable to drop the loss (MSE) below 0.2.
In blue: expected output (=input), in orange: reconstruction.
Single sample, single batch:

Multiple samples, single batch, loss won't go down:

Using more than one sample, we can see the net learns the general shape of the input (=output) signal, but greatly misses by an over-all constant.

Using a fully connected decoder does manage to reconstruct batches of multiple samples:


Relevant code:
class Conv1DBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        self._in_channels = in_channels
        self._out_channels = out_channels
        self._kernel_size = kernel_size

        self._block = nn.Sequential(
                nn.Conv1d(
                        in_channels=self._in_channels,
                        out_channels=self._out_channels,
                        kernel_size=self._kernel_size,
                        stride=1,
                        padding=(self._kernel_size - 1) // 2,
                ),
                # nn.BatchNorm1d(num_features=out_channels),
                nn.ReLU(True),
                nn.MaxPool1d(kernel_size=2, stride=2),
        )

    def forward(self, x):
        for layer in self._block:
            x = layer(x)
        return x


class Upsample1DBlock(nn.Module):
    def __init__(self, in_channels, out_channels, factor):
        super().__init__()
        self._in_channels = in_channels
        self._out_channels = out_channels
        self._factor = factor

        self._block = nn.Sequential(
                nn.Conv1d(
                        in_channels=self._in_channels,
                        out_channels=self._out_channels,
                        kernel_size=3,
                        stride=1,
                        padding=1
                ),  # 'same'
                nn.ReLU(True),
                nn.Upsample(scale_factor=self._factor, mode='linear', align_corners=True),
        )

    def forward(self, x):
        x_tag = x
        for layer in self._block:
            x_tag = layer(x_tag)
        # interpolated = F.interpolate(x, scale_factor=0.5, mode='linear') # resnet idea
        return x_tag

encoder:
self._encoder = nn.Sequential(
            # n, 1024
            nn.Unflatten(dim=1, unflattened_size=(1, 1024)),
            # n, 1, 1024
            Conv1DBlock(in_channels=1, out_channels=8, kernel_size=15),
            # n, 8, 512
            Conv1DBlock(in_channels=8, out_channels=16, kernel_size=11),
            # n, 16, 256
            Conv1DBlock(in_channels=16, out_channels=32, kernel_size=7),
            # n, 32, 128
            Conv1DBlock(in_channels=32, out_channels=64, kernel_size=5),
            # n, 64, 64
            Conv1DBlock(in_channels=64, out_channels=128, kernel_size=3),
            # n, 128, 32
            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=32, stride=1, padding=0),  # FC
            # n, 128, 1
            nn.Flatten(start_dim=1, end_dim=-1),
            # n, 128
        )

conv decoder:
self._decoder = nn.Sequential(
    nn.Unflatten(dim=1, unflattened_size=(128, 1)),  # 1
    Upsample1DBlock(in_channels=128, out_channels=64, factor=4),  # 4
    Upsample1DBlock(in_channels=64, out_channels=32, factor=4),  # 16
    Upsample1DBlock(in_channels=32, out_channels=16, factor=4),  # 64
    Upsample1DBlock(in_channels=16, out_channels=8, factor=4),  # 256
    Upsample1DBlock(in_channels=8, out_channels=1, factor=4),  # 1024
    nn.ReLU(True),
    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1),
    nn.ReLU(True),
    nn.Flatten(start_dim=1, end_dim=-1),
    nn.Linear(1024, 1024)
)

FC decoder:
self._decoder = nn.Sequential(
    nn.Linear(128, 256),
    nn.ReLU(True),
    nn.Linear(256, 512),
    nn.ReLU(True),
    nn.Linear(512, 1024),
    nn.ReLU(True),
    nn.Flatten(start_dim=1, end_dim=-1),
    nn.Linear(1024, 1024)
)


Another observation is that when the batch size increases more, to say, 16, the FC decoder also starts to fail.
In the image, 4 samples of a 16 sample batch I am trying to overfit


What could be wrong with the conv decoder?
How to debug this or make the conv decoder work?
Please notice that the same infrastructure with only the encoder and decoder different do manage to overfit and generalize over MNIST.
(This is also posted here, but I think this is still ok to do. If not, please tell me and I will delete one).
","['deep-learning', 'convolutional-neural-networks', 'autoencoders', 'overfitting', 'underfitting']",
Aren't scores in the Wasserstein GAN probabilities?,"
I am quite new to GAN and I am reading about WGAN vs DCGAN.
Relating to the Wasserstein GAN (WGAN), I read here

Instead of using a discriminator to classify or predict the probability of generated images as being real or fake, the WGAN changes or replaces the discriminator model with a critic that scores the realness or fakeness of a given image.

In practice, I don't understand what the difference is between a score of the realness or fakeness of a given image and a probability that the generated images are real or fake.
Aren't scores probabilities?
","['deep-learning', 'terminology', 'generative-adversarial-networks', 'probability', 'wasserstein-gan']","Figure 3 in the original WGAN paper is actually quite helpful to understand the difference between the score in WGAN and the probability in GAN (see screenshot below). The blue distribution are real samples, and the green one are fake samples. The Vanilla GAN trained in this example identifies the real samples as '100% real' (red curve) and the fake samples as '100% fake'. This leads to the problem of vanishing gradients and the well-known mode collapse of original GANs.The Wasserstein GAN, on the other hand, gives each sample a score. The benefit of the score is that we can now identify samples that are more likely real than others, or more likely fake. For example, the further to the left a distribution is located, the more negative the WGAN score will be. We have therefore a continuum that doesn't end in 0 and 1, but can compare between samples that are 'good' and those which are 'better'. A normal GAN would identify both as 'good', making further improvement difficult."
"What is a ""learned policy"" in Q-learning?","
I am completing an assignment at the moment. One of the assignment questions asks how you identified the learned policy and how you obtained it. The question is a reinforcement learning question, and the task is to apply the Q-learning algorithm to fill out a Q-table (which I've done) but confused on what it may mean by the learned policy.
So, what is a ""learned policy"" in Q-learning?
","['reinforcement-learning', 'q-learning', 'terminology', 'value-functions', 'policies']","A Q table allows you to look up any state/action pair in it and find the associated action value. It is not itself a policy. However, in order to calculate the action values, you will have assumed something about the policy.The most common policy scenarios with Q learning are that it will converge on (learn) the values associated with a given target policy, or that it has been used iteratively to learn the values of the greedy policy with respect to its own previous values. The latter choice - using Q learning to find an optimal policy, using generalised policy iteration - is by far the most common use of it.A policy is not a list of values, it is a map from state to actions. The question wants you to show the policy that you have learned the Q values for.The policy in your case is therefore likely to be to pick the action that has the highest action value in each state. You may be able to decribe your answer in text (""always turn left unless next to the exit"") or as a graphic (draw arrows on a grid world to show the preferred direction). Or you could write out a table of states showing the chosen action in each one.The maths notation for how you derive the policy from a Q table can be written:$$\pi(s) = \text{argmax}_a Q(s,a)$$Or a bit more formally:$$\pi: \mathcal{S} \rightarrow \mathcal{A} = \text{argmax}_{a \in \mathcal{A}(s)} Q(s,a)\qquad \forall s \in \mathcal{S}$$"
Can object detection approaches be used to solve text/detection problems?,"
I have been working on text detection and recognition for almost two months and new on this field. So far, I have fine-tuned, tested, and trained several text detection/recognition methods, such as CRAFT, TextFuseNet, CharNet for detection, and clova.ai model for recognition. Now I come up with this question:

Can object detection approaches(yolov5,efficientDet) be used to solve text/detection problems?

","['machine-learning', 'object-detection', 'object-recognition', 'text-detection']",
What is the difference between out of distribution detection and anomaly detection?,"
I'm currently reading the paper Likelihood Ratios for Out-of-Distribution Detection, and it seems that their problem is very similar to the problem of anomaly detection. More precisely, given a neural network trained on a dataset consisting of classes $A,B,$ and $C$, then they can detect if an input to the neural network is anomalous if it is different than these three classes. What is the difference between what they are doing and regular anomaly detection?
","['neural-networks', 'comparison', 'papers', 'anomaly-detection']","You observation is correct although the terminology needs a little explaining.The term 'out-of-distribution' (OOD) data refers to data that was collected at a different time, and possibly under different conditions or in a different environment, then the data collected to create the model.  They may say that this data is from a 'different distribution'.Data that is in-distribution can be called novelty data.  Novelty detection is when you have new data (i.e. OOD) and you want to know whether or not it is in-distribution.  You want to know if it looks like the data you trained on.
Anomaly detection is when you test your data to see if it is different than what you trained the model.  Out-of-distribution detection is essentially running your model on OOD data.  So one takes OOD data and does novelty detection or anomaly detection (aka outlier detection).Below is a figure from What is anomaly detection?In time series modeling, the term 'out-of-distribution' data is analogous to 'out-of-sample' data and 'in-distribution' data is analogous with 'in-sample' data."
"Relationship between Rewards and Q Value (Graph between Q(s, a) vs episodes)","
I'm employing the Actor-Critic algorithm. The critic network approximates the action-value function, i.e. $Q(s, a)$, which determines how good a particular state is, when provided with an action.
$Q(s, a)$ is approximated using the backpropagation of the temporal difference error (TD error). We can understand that $Q(s, a)$ has been approximated properly when TD error is minimized, i.e. when it is saturated at lower values.
My question is, when exactly can you say that $Q(s, a)$ is approximated properly, if you don't have TD error, i.e. if you have to plot the graph between $Q(s, a)$ vs episodes, then what would be the optimal behaviour?
Will it be increasing exponential with saturation value around reward values, or increasing exponential with saturation (around any value)?
Follow up: What can be the possible mistake, if the output of Q-value function is around 5x the rewards, and not saturating?
","['reinforcement-learning', 'q-learning', 'actor-critic-methods']","TLDR;The output of Q-value function will eventually saturate. Can't say when, but it will surely do.if you don't have TD errorIt meant, if I don't have logs of the error.$Q(s, a)$ vs Episodes GraphTo understand, how $Q(s, a)$ behaves as the episodes increase.I was under a wrong impression that, the $Q(s, a)$ will saturate around the values given by the reward function.As evident from Loss vs Training Episodes Curve, we can see that loss (TD Error) is almost saturated. However, $Q(s, a)$ vs training episodes curve is not saturated yet.
The only explanation for the above two graphs could be given as follows:
The target estimate ($r + \gamma Q(s', a')$) and $Q(s, a)$ are almost similar due to which, the error is quite low. But, $Q(s, a)$ is still nowhere near optimal value $Q^{*}(s, a)$.Hence, I gave it another shot and made it run for twice training episodes i.e. 20000, and below are the results.A nearly saturated loss
and a nearly saturating $Q(s, a)$.
.Note that, the value of $Q(s, a)$ saturating is around 250 - 300 (will run it for more iterations) and it is nowhere around the reward values $ \in [-100, 35]$.Hence, the $Q(s, a)$ vs episodes will saturate."
How should we regularize an LSTM model?,"
There are five parameters from an LSTM layer for regularization if I am correct.
To deal with overfitting, I would start with

reducing the layers
reducing the hidden units
Applying dropout or regularizers.

There are kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, dropout and recurrent_dropout.
They have their definitions on the Keras's website, but can anyone share more experiences on how to reduce overfitting?
And how are these five parameters used? For example, which parameters are most frequently used and what kind of value should be input? ?
","['recurrent-neural-networks', 'long-short-term-memory', 'overfitting', 'regularization', 'dropout']",
"Is using a LSTM, CNN or any other neural network model on top of a Transformer(using hidden states) overkill?","
I have recently come across transformers, I am new to Deep Learning. I have seen a paper using   CNN and BiLSTM on top of a transformer, the paper uses a transformer(XLM-R) for sentiment analysis in code-mixed domain. But many of the blogs only use a normal feed formal network on top of the transformer.
I am trying to use transformers for sentiment analysis, short text classification.
Is it overkill to use models like CNN and BiLSTM on top of the transformer considering the size of the data it is trained on and its complexity?
","['deep-learning', 'natural-language-processing', 'transformer', 'text-classification', 'sentiment-analysis']",
"Is the working of RNNs, LSTM and GRU sequential or parallel?","
You take any blog or any example and all they tell you about is the given picture below.

It has 4 different matrices and 3 of whose weights are shared. So, I'm wondering how is this achieved in practice?
Please correct me:
I think the first word ""hello"" goes in as a one-hot encoded form and changes the Hidden matrix. And then after it, ""world"" goes and gets multiplied and then changes the matrix again and so on. What people make it look like is that all of the words going are in Parallel. It can't be the case because the Hidden matrix is dependent on the previous word and without changing the matric, you can not pass the current word. Please correct if my idea is wrong but I think the execution is in sequential order.
","['neural-networks', 'deep-learning', 'natural-language-processing', 'recurrent-neural-networks', 'long-short-term-memory']","Yes, you are correct and it was one of original motivations, which inspired the invention of the Attention mechanism in seq2seq problems https://arxiv.org/pdf/1706.03762.pdf.There is a quote from this paper:Recurrent models typically factor computation along the symbol
positions of the input and output sequences. Aligning the positions to
steps in computation time, they generate a sequence of hidden states $h_t$,
as a function of the previous hidden state $h_{t−1}$ and the input for
position $t$. This inherently sequential nature precludes parallelization
within training examples, which becomes critical at longer sequence
lengths, as memory constraints limit batching across examples.On the other hand, Transformer architectures have a loot loom for parallelization, because they take the whole sequence at once, and multiple heads can be executed in parallel."
Reinforcement learning and Graph Neural Networks: Entropy drops to zero,"
I am currently working on an experiment to link reinforcement learning with graph neural networks.
This is my architecture:
Feature Extraction with GCN:

there is a fully meshed topology with 23 nodes. Therefore there are 23*22=506 edges.
the original feature vector comprises 43 features that range from about -1 to 1.
First, a neuronal network f takes calculates a vector per edge, given the source node and target node features.
After we have calculated 506 edge vectors, function u aggregates the results from f per node (aggregation over 22 edges)
A function g takes the original target feature vector and concatenates the aggregated results from u. Finally, the output dimension of g determines the new feature vector size for each node.
At last, the function agg decides which information is returned from the feature extraction, e.g. just flatten the 23xg_output_dim feature vectors or building the average

After that:

The output of the feature extractor is passed to the OpenAi Baseline PPO2 Implementation. The frameworks adds a flatten layer to the output and maps it to 19 action values and 1 value-function value.

I have made some observations in the experiments and do not manage to explain them. Hyperparameter are: an output dimension for f and g of 512. U=sum, aggr=flatten. A tanh activation is applied on the outputs of f  and g. For PPO2: lr=0.000343, stepsize=512.
This gives me  the following weight matrices:
<tf.Variable 'ppo2_model/pi/f_w_0:0' shape=(86, 512) dtype=float32_ref>    
<tf.Variable 'ppo2_model/pi/g_w:0' shape=(555, 512) dtype=float32_ref>   
<tf.Variable 'ppo2_model/pi/w:0' shape=(11776, 19) dtype=float32_ref>    
<tf.Variable 'ppo2_model/vf/w:0' shape=(11776, 1) dtype=float32_ref>

The following problem occurs. Normally you wait for the entropy in the PPO2 to decrease during the training, because the algorithm learns which actions lead to more reward.
With the described hyperparameters, the entropy drops abruptly to 0 within 100 update steps and stays zero even after >15.000 updates (=150M steps in the game). This means that the same action is always selected.
What I found out: the problem is that by making the sum over 22 edges, very large values are created (maximum 22*1 and 22*-1). The values are then given to the function g and thus ends up in the saturation region of the tanh. As a result, the new features of the 23 nodes contain many 1's and -1's. Because we flatten, the weighted sum of 11776 input neurons flows into each of the 19 action neurons, resulting in very large values in the policy. An action is then calculated from the policy with the following formula:
u = tf.random_uniform(tf.shape(logits), dtype=logits.dtype)
action = tf.argmax(logits - tf.log(-tf.log(u)), axis=-1), 

Most of the time tf.log(-tf.log(u) gives sommething between 2 and -2 (in my opinion). This means that as soon as a very large value appears in the policy, the corresponding action is always selected and not the second or third most probable one, which might lead to more exploration.
What I don't understand 1): As soon as negative reward occurs, shouldn't the likelihood decrease again, so that in the end I choose other actions again?
I did some experiments with relu and elu activations:
These are the value histogram of the output of g after the using relu, tanh and elu:

These are the value histograms of the policy, when using relu, tanh and elu:

Histogram over resulting actions:
What I don't understand 2:Using Relu you see that in the first steps in the policy were large values, but then the model learns to reduce the range, which is why in this example also the entropy does not drop. Why does this not work when using tanh or elu?

We have found out 3 things with which the problem does not occur or is delayed. These are my assumptions. Are the correct in your opinion?:

using smaller output dimension of f and g, like 6 or using aggr=mean -> For each of the 19 action neurons, less input neurons are averaged -> smaller values in the policy --> more exploration
Using u=mean and not sum, averages the outputs of f, therefore the aggregated values are not only 1 and -1
Smaller learning rate -> Making the weights too big, increases the chance of the 19 action values to be  big. If there is no negativ reward, there is no need for the algorithm to make the weights smaller.

I know this is a lot of information, so I would be grateful for any small tip.!
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'graph-neural-networks']",
Do AlphaZero/MuZero learn faster in terms of number of games played than humans?,"
I don't know much about AI and am just curious.
From what I read, AlphaZero/MuZero outperform any human chess player after a few hours of training. I have no idea how many chess games a very talented human chess player on average has played before he/she reaches the grandmaster level, but I would imagine it is a number that can roughly be estimated. Of course, playing entire games is not the only training for human chess players.
Nonetheless, how does this compare to AI? How many games do AI engines play before reaching the grandmaster level? Do (gifted) humans or AI learn chess faster?
","['comparison', 'training', 'alphazero', 'chess', 'muzero']","According to Table S3 of the AlphaZero paper (p. 15)AlphaZero was trained for 9 hours and, during these 9 hours, it played 44 million games of chess.According to this Wikipedia article, the longest human lifespan is that of Jeanne Calment, who lived to age 122 years and 164 days.Let's assume that humans cannot live more than 123 years (which is a reasonable assumption, although this record could eventually be broken). Let's also assume that a chess game lasts at least 10 minutes, which means that you can play at most 6 games in 1 hour, which means that you can play at most $6*24 = 144$ games in one day (assuming that you never sleep, which is, of course, impractical, but I'm just trying to show you an upper bound). Let's say that a year has 365 days. So, here is roughly the maximum number of games that a human could play
$$
6*24*365*123 = 6464880 \tag{1}\label{1},
$$
which is smaller than 44 million games by a factor of more than 6, i.e. any human could at most play 1/6 of the games that AlphaZero played, and \ref{1} is a very loose upper bound that doesn't take into account that humans need to sleep, eat, and do many other things.So, humans learn to play chess a lot slower than AlphaZero. This is not surprising at all, given that computers can perform calculations a lot faster than us (that's why they are called computers), and this has been the case for many years. We (humans) just made computers make the right calculations for them to approximately play chess better than us. That's it."
Can cryptocurrency charts be estimated using neural networks?,"
If I were to make a neural network that predicts the value of e.g. Bitcoin tomorrow based on the chart of the last month, would that work? Of course, 100% accuracy cannot be reached, but a success rate over 50% on determining if I should buy or sell Bitcoin could be very profitable. Have there been any attempts to create such neural networks so far?
","['neural-networks', 'reference-request', 'prediction']",
"What should the initial UCT value be with MCTS, when leaf's simulation count is zero? Infinity?","
I am implenting a Monte Carlo Tree Search algorithm, where the selection process is done through Upper Confidence Bound formula:
def uct(state):
        log_n = math.log(state.parent.sim_count) 
        explore_term = self.exploration_weight * math.sqrt(log_n / state.sim_count)
        exploit_term = (state.win_count / state.sim_count)

        return exploit_term + explore_term

I have trouble however choosing the initial value for UCT, when the sim_count of the node is 0. I tried with +inf (which would be appropriate as approaching lim -> 0 from the positive side would give infinity), but that just means the algorithm will be always choosing an unexplored child.
What would you suggest as a initial value for the uct?
Thank you in advance!
","['monte-carlo-tree-search', 'upper-confidence-bound', 'tree-search']","Assigning a value of $\infty$ to unvisited nodes is indeed the ""default"" or most basic choice, and it indeed ensures that the search never visits a node for a second time if it also still has siblings that have not had any visits. But many other kinds of values have been tried in the literature too.Gelly and Wang, in ""Exploration exploitation in Go: UCT for Monte-Carlo Go"" referred to the parameter as ""First-play Urgency"" (FPU), and indeed really treated it as a hyperparameter; they simply tried various different values and some were found to work better than others.Other more specific values that you may want to consider (but in most cases we can't really say much about which one will be better than which other ones, without empirical evaluations) are:"
How to deal with dynamically changing input tensor in neural networks without padding?,"
I have a dataset about the monitored health/growth of a community of people. The dataset has tensor shaped (batch_size, features, person, window), where:

person==10 means there are 10 people in the community
features==9 means that there are 9 features being monitored, for example, blood pressure, sugar level, ..etc
window==15 means the recorded value of each feature every day for 15 days (time dimension)

Moreover, people can join/leave the community, so the person dimension would increase/decrease over time. For simplicity, the window dimension is fixed at 15, then a new person that joined has to be in the community for a minimum of 15 days to be included in the dataset as 1 data point/sample. Also, say the number of features is fixed at 9. Hence, for this problem, only the number of people at an instance may change over each input interaction.
For example, assume batch_size==1 then the input dimension into the neural network would be something like:
Iter 1: (1, 9, 7, 15)
Iter 2: (1, 9, 7, 15)
Iter 3: (1, 9, 7, 15)
Iter 4: (1, 9, 8, 15) # 1 person joins the community
Iter 5: (1, 9, 8, 15)
Iter 6: (1, 9, 7, 15) # 1 person left the community
Iter 7: (1, 9, 6, 15) # 1 person left the community
Iter 8: (1, 9, 6, 15)
Iter 9: (1, 9, 10, 15) # 4 person joins the community 

Is there a way to deal with this dynamically changing input tensor in neural networks without padding? As we won't know in advance how many people will join/leave the community (related to continual learning?) hence won't know the maximum pad.
Also, how to deal when batch_size is not 1?
","['deep-learning', 'tensorflow', 'deep-neural-networks', 'pytorch', 'graph-neural-networks']",
How to recognize sequence of digits in an image,"
I am learning to program neural networks and others, and I would like to know how I can get the numbers that are in an image, for example, if I pass an image that has 123 written, get with my model that there are 123 written, I have tried to use PyTesseract is not very precise, and I would like to do it with a neural network, my current code is quite simple, it recognizes the digits of the mnist dataset such that:
import tensorflow as tf
from tensorflow.keras import Sequential, optimizers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
import matplotlib.pyplot as plt

mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

print('train_images.shape:', train_images.shape)
print('test_images.shape:', test_images.shape)
plt.imshow(train_images[0])

train_images = train_images.reshape((60000, 28, 28, 1))
test_images = test_images.reshape((10000, 28, 28, 1))

train_images = train_images.astype('float32') / 255
test_images = test_images.astype('float32') / 255

train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
 

model = Sequential()

model.add(Conv2D(32, (5, 5), activation = 'relu', input_shape = (28, 28, 1)))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (5, 5), activation = 'relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Flatten())

model.add(Dense(10, activation = 'softmax'))

model.summary()

model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])

model.fit(train_images, train_labels, batch_size = 100, epochs = 5, verbose = 1)

test_loss, test_accuracy = model.evaluate(test_images, test_labels)

print('Test accuracy:', test_accuracy)

but I would need to know how I can pass an image with a sequence of digits to it, and that it recognizes the digits in question, does anyone know how I could do it?
","['neural-networks', 'computer-vision', 'tensorflow', 'python', 'image-recognition']",
"Alpha Zero does not converge for Connect 6, a game with huge branching factor - why?","
I have a problem with applying alpha zero self-play to a game (Connect 6) with a huge branching factor (30,000 on average).
I have implemented the MCTS as described but I found that during the MCTS simulations for the first move, because P(s, a) is so much smaller than Q(s, a), the MCTS search tree is extremely narrow (in fact, it only had 1 branch per level, and later I have added dirichlet noise and it changed the tree to have 2 child branches instead of 1, per level).
With some debugging I figured that after the first simulation, the last visited child would have a Q value which is on average 0.5, and all the rest of the children would have 0 for their Q values (because they haven't been explored) while their U are way smaller, about 1/60000 on average. So comparing all the Q + U on the 2nd and subsequent simulations would result in selecting the same node over and over again, building this narrow tree.
To make matter worse, because the first simulation built a very narrow tree, and we don't clear the statistics on the subsequent simulations for the next move, the end result is the simulations for the first move dictated the whole self-play for the next X moves or so (where X is number of simulations per move) - this is because the N values on this path is accumulated from the previous simulations from the prior moves. Imagine I run 800 simulations per move but the N value on the only child inherited from previous simulations is > 800 when I started the simulation for the 3rd move in the game.
This is a basically related to question 2 raised in this thread:
AlphaGo Zero: does $Q(s_t, a)$ dominate $U(s_t, a)$ in difficult game states?
I don't think the answer addressed the problem I am having here. Surely we are not comparing Q and U but when Q dominates U then we ended up building a narrow and deep tree and the accumulated N values are set on this single path, preventing the self-play to explore meaningful moves.
At the end of the game these N values on the root nodes of moves played ended up training the policy network, reinforcing these even more on the next episode of training.
What am I doing wrong here? Or is AlphaZero algorithm not well suited for games with branching factor in this order?
Note: as an experiment I have tried to use a variable C PUCT value, which is proportional to the number of legal moves on each game state. However this doesn't seem to help either.
","['machine-learning', 'deep-learning', 'monte-carlo-tree-search', 'alphazero']","There are a few things you can do to address this(1) Setting Q correctly on unexplored nodesWith some debugging I figured that after the first simulation, the last visited child would have a Q value which is on average 0.5, and all the rest of the children would have 0 for their Q valuesThis already points out a bug, as the rest of the children should start with the ""average Q-value"". Remember that Q is just our guess for the EV of an action. Your comment says that you set it to ""0.5"", but this still isn't quite right. You'll want to modify this by initializing it to the ""average value"" of a ""random move"" in that position.For Chess and Go, this is quite close to 0 from most positions, since most valid moves are terrible. For your game, it's possible that this value is higher, if most moves are ""reasonable"". But, it should still be set to the ""Average value of a random move in that particular position"", not simply 0.5.When your network is very weak, its choice will have a value of approximately ""the average value of a random move"", so by setting the initial Q correctly, it will cause the difference in Q between two different moves to be very small, letting the ""U"" term dominate initial exploration. To be precise, the difference in Q will tell you How much better you current path is, over just picking some other random move, which is exactly what you want the difference in Q to be.See Lc0's ""FPU / First Play Urgency"" configuration, which sets unvisited node's initial value to be 0.44 less than the parents' evaluation.(2) Increase $C_{puct}$ with respect to the number of paths down that edge.In the face of extreme depth down a particular path, $C_{puct}$ should begin to increase to at least ""double-check"" the moves we've been ignoring thusfar. AlphaZero already does this:With their numbers, $C_{puct}$ starts at 2.5, but rises to 2.8 when Depth = 20k, and 2.97 when Depth = 40k. ($2.5 + \log(2)$, and $2.5 + \log(3)$, respectively). You'll have to adjust this manually, often in relation to your simulations-per-move.(3) Increase Dirichlet NoiseI'm not sure what $\alpha$ you used, but based on AlphaZero, $\alpha = 10/60000 =$ 1.66e-4 is what you should be aiming for, if you want to match AlphaZero's configuration of 10/AverageValidMoves."
What do the variables in the cross-correlation formula mean?,"
I understand what cross-correlation does given a kernel and an input image, but the formula confuses me a little. Given here in Goodfellow's Deep Learning (page 329), I can't quite understand what $m$ and $n$ are. Are they the dimensions of the kernel along the height and width dimensions?
$$S(i,j) =(K*I)(i,j) = \sum_m \sum_n I(i+m, j+n)K(m,n)$$
So, for the input image $I$ and kernel $K$, we take the sum product of $I*K$, but what do the $m$ and $n$ represent? How is the input image $I$ indexed?
","['deep-learning', 'convolutional-neural-networks', 'convolution', 'notation', 'cross-correlation']","It takes a little bit of time to fully understand the 2D convolution/cross-correlation and to relate it to the usual diagrams of the convolution operation, so, before addressing your questions, let me first try to break the definition of the 2D cross-correlation down, from the left to right.$$S(i,j) =(K*I)(i,j) = \sum_m \sum_n I(i+m, j+n)K(m,n) \label{1}\tag{1}$$$S$ is the function that is the cross-correlation of the functions $K$ and $I$, so $S(i, j)$ is the cross-correlation of $K$ and $I$ at the pixels $i$ and $j$The symbol $*$ in $K*I$ is the cross-correlation/convolution symbol, but sometimes the cross-correlation/convolution is also denoted as $\circledast$$K*I$ is the function that results from the cross-correlation of $K$ and $I$, i.e. $S$, so $(K*I)(i,j)$ is the value of the function $S$ (the cross-correlation of $K$ and $I$) at the inputs (or pixels) $i$ and $j$. In other words, $(K*I)(i,j)$ is just another way of writing $S(i,j)$ that emphasizes that we took the cross-correlation of $K$ and $I$, but they are exactly the same thing.The double summation $\sum_m \sum_n$ is because we are computing the 2D cross-correlation, i.e. over the $x$ and $y$ dimensions of the image and kernel. This is just the definition of the 2D cross-correlation.The $m$ and $n$ are the indices of the summations, one across the $x$-axis and the other across the $y$-axis. Let $m = x$ and $n = y$, so we can rewrite equation \ref{1} as follows.
$$S(i, j) =(K*I)(i, j) = \sum_x \sum_y I(i + x, j + y)K(x, y) \label{2}\tag{2}$$
Now, it should be clearer that we are indexing across the $x$ and $y$ dimensions.Now, let me further restrict the range of the summations. Let's say from $x=-1$ to $x=1$ and from $y=-1$ to $y=1$, then we can rewrite equation \ref{2} as follows
$$S(i, j) =(K*I)(i, j) = \sum_{x=-1}^1 \sum_{y=-1}^1 I(i + x, j + y)K(x, y) \label{3}\tag{3}$$.Why do I want to do this? Let me explain why. Consider now the following kernel $K$ (which happens to be a Gaussian kernel)
$$
K = \begin{bmatrix}\ \ \color{blue}{\frac {1}{16}} &\ \ \frac {1}{8} &\ \ \frac {1}{16} \\\ \ \frac {1}{8} &\ \ \frac {1}{4} &\ \ \color{red}{ \frac {1}{8}} \\\ \ \frac {1}{16} &\ \ \frac {1}{8} &\ \ \frac {1}{16}\end{bmatrix}
$$
Note that this is the output of the function $K$ or, more precisely, its support. Let's assume that $\frac {1}{4}$ is at the index/pixel $(0, 0)$. Then, for example, the top-left $ \color{blue}{\frac {1}{16}} $ is at index/pixel $(-1, -1)$ and the middle-right $\color{red}{ \frac {1}{8}}$ at pixel $(1, 0)$Now, consider any image $I$ represented as a 2D matrix (i.e. its support), with, for example, dimensions $U \times V$. For concreteness, let $U = 5$ and $V=5$. Moreover, the middle pixel of the image is at index $(0, 0)$, as for the kernel. Let's say the image is the following
$$
I = 
\begin{bmatrix}
0 & 1 & 0 & 1 & 0 \\
1 & 1 & 0 & 1 & 1 \\
0 & 0 & \color{green}{0} & 1 & 1 \\
1 & 1 & 0 & 0 & 0 \\
0 & 1 & 0 & 1 & 1 \\
\end{bmatrix}
$$
So, $\color{green}{0}$ is at index/pixel $(0,0)$.Now, let's say that $i = 0$ and $j=0$ in equation \ref{3}. This means that we compute the cross-correlation between $I$ and $K$ at the index $(0, 0)$, where, in the case of the image $I$, the value is $\color{green}{0}$.With these settings, it should be clearer now that equation \ref{3} means that the cross-correlation between $K$ and $I$ at index/pixel $i=0$ and $j=0$ is a 2D dot (or scalar) product. If it is not clear, then let be take the $3\times 3$ submatrix of $I$ centered at $(i, j) = (0, 0)$ (below, $(0, 0)^{3 \times 3}$ is just the notation that I came up with to indicate that).
$$
I_{(0, 0)^{3 \times 3}} = 
\begin{bmatrix}
1 & 0 & 1 \\
0 & \color{green}{0} & 1  \\
1 & 0 & 0 
\end{bmatrix}
$$
Then the cross-correlation in equation \ref{3} is just
$$
S(i, j) = S(0, 0) = (K * I)(0, 0) = 
\sum
\begin{bmatrix}
1 \frac {1}{16} & 0 \frac {1}{8} & 1 \frac {1}{16} \\
0 \frac {1}{8} & \color{green}{0} \frac {1}{4}  & 1 \frac {1}{8}  \\
1 \frac {1}{16} & 0 \frac {1}{8} & 0 \frac {1}{16} 
\end{bmatrix},
$$
where $\sum$ is a sum across all elements, i.e.
\begin{align}
S(0, 0) 
&= 1 \frac {1}{16} + 0 \frac {1}{8} + 1 \frac {1}{16} + 
 0 \frac {1}{8} + \color{green}{0} \frac {1}{4} + 1 \frac {1}{8} + 
1 \frac {1}{16} + 0 \frac {1}{8} + 0 \frac {1}{16}  \\
& = \frac {1}{16} + \frac {1}{16} + \frac {1}{8} + \frac {1}{16} \\
& = \frac {3}{16} + \frac {1}{8} \\
&= \frac {5}{16} 
\end{align}Now, to answer your questions/concerns more directly.Are they the dimensions of the kernel along the height and width dimensions?No. They are the indices that determine the neighbourhood around $i$ and $j$ where you want to compute the cross-correlation.So, for the input image $I$ and kernel $K$, we take the sum product of $I*K$In this case, the symbol $*$ does not denote the product, but the cross-correlation. Maybe by ""sum product"" you meant the cross-correlation (or 2D dot product), but I'm not sure.How is the input image $I$ indexed?The input image is indexed with $m$ and $n$, but you start at $i$ and $j$, that's why you use the actual indices $i+m$ and $j+n$."
How to build a test set for a model in industry?,"
Most of the tutorials only teach us to split the whole dataset into three parts: training set, develop set, and test set. But in the industry, we are kind of doing test-driven development, and what comes most important is the building of our test set. We are not given a large corpus first, but we are discussing how to build the test set first.
The most resource-saving method is just sampling(simple random sampling) cases from the log and then having them labeled, which represents the population. Perhaps we are concerning that some groups should be more important than others, then we do stratified sampling.
Are there any better sampling methods?
What to do when we are releasing a new feature and we cannot find any cases of that feature from the user log?
","['testing', 'test-datasets']",
Are Genetic Algorithms suitable for problems like the Knuth problem?,"
We all know that Genetic Algorithms can give an optimal or near-optimal solution. So, in some problems like NP-hard ones, with a trade-off between time and optimal solution the near-optimal solution is good enough.
Since there is no guarantee to find the optimal solution, is GA considered to be a good choice for solving the Knuth problem?
According to Artificial intelligence: A modern approach (third edition), section 3.2 (p. 73):

Knuth conjectured that, starting with the number 4, a sequence of
factorial, square root, and floor operations will reach any desired
positive integer.

For example, 5 can be reached from 4:

floor(sqrt(sqrt(sqrt(sqrt(sqrt((4!)!))))))

So, if we have a number (5) and we want to know the sequence of the operations of the 3 mentioned ones to reach the given number, each gene of the chromosome will be a number that represents a certain operation with an additional number for (no operation) and the fitness function will be the absolute difference between the given number and the number we get from applying the operations in a certain order for each the chromosome (to min). Let's consider that the number of the iterations (generations) is done with no optimal solution and the nearest number we have is 4 ( with fitness 1), the problem is that we can get 4 from applying no operation on 4 while for 5 we need many operations, so the near-optimal solution is not even near to the solution.
So, is GA is not suitable for this kind of problems? Or the suggested chromosome representation and fitness function are not good enough?
","['genetic-algorithms', 'evolutionary-algorithms', 'genetic-programming', 'norvig-russell']","Before trying to answer your question more directly, let me clarify something.People often use the term genetic algorithms (GAs), but, in many cases, what they really mean is evolutionary algorithms (EAs), which is a collection of population-based (i.e. multiple solutions are maintained at the same time) optimization algorithms and approaches that are inspired by Darwinism and survival of the fittest. GAs is one of these approaches, where the chromosomes are binary and you have both the mutation and cross-over operation. There are other approaches, such as evolution strategies or genetic programming.As you also noticed, EAs are meta-heuristics, and, although there is some research on their convergence properties [1], in practice, they may not converge. However, when any other potential approach has failed, EAs can be definitely useful.In your case, the problem is really to find a closed-form (or analytical) expression of a function, which is composed of other smaller functions. This really is what genetic programming (in particular, tree-based GP) was created for. In fact, the Knuth problem is a particular instance of the symbolic regression problem, which is a typical problem that GP is applied to. So, GP is probably the first approach you should try.Meanwhile, I have implemented a simple program in DEAP that tries to solve the Knuth problem. Check it here. The fitness of the best solution that it has found so far (with some seed) is 4 and the solution is floor(sqrt(float(sqrt(4)))) (here float just converts the input to a floating-point number, to ensure type safety).  I used the difference as the fitness function and ran the GP algorithm for 100 generations with 100 individuals for each generation (which is not a lot!). I didn't tweak much the hyper-parameters, so, maybe, with the right seed and hyper-parameters, you can find the right solution.To address your concerns, in principle, you could use that encoding, but, as you note, the GA could indeed return $4$ as the best solution (which isn't actually that far away from $5$), which you could avoid my killing, at every generation, any individuals that have just that value.I didn't spend too much time on my implementation and thinking about this problem, but, as I said above, even with genetic programming and using only Knuth's operations, it could get stuck in local optima. You could try to augment my (or your) implementation with other operations, such as the multiplication and addition, and see if something improves."
"For binary classification learning problems, how should I label instances where I'm only 60% sure?","
I've come across a few binary classification problems lately where the labelling was challenging even for an expert. I'm wondering what I should do with this. Here are some of my suggestions to get the ball rolling:

Make a third category called ""unsure"" then make it a three-class classification problem instead.
Make a third category called ""unsure"" and just remove these from your training set.
Make a third category called ""unsure"" and during training model this as a 0.5 such that the binary cross entropy loss looks like $-0.5\log(\hat{y})-0.5\log(1-\hat{y})$
Allow the labeller to pick a percentage on a sliding scale (or maybe multiple choice: (0%, 25%, 50%, 75%, 100%), and take that into account when calculating cross entropy (as in my point above).

I recently saw a paper which goes for option 2, although that's not enough to convince me. Here's the relevant quote:

In case of a high-level risk, collision is imminent and
the driver must react in less than 0.5 s (TTC < 0.5s). For
low-level risk, the TTC is more than 2.0 s (TTC > 2.0s).
Videos that show intermediate-level risk (0.5 s ≤ TTC
≤ 2.0 s), which is a mixture of high- and low-level risks,
were not included in the NIDB because when training
a convnet, it must be possible to make a clear visual
distinction of risk.

","['neural-networks', 'classification', 'data-labelling']",
Is it possible to ensure the convergence when training a RNN weight on its SVD decomposition?,"
I'm reading the following paper in which the author seems to do 2 things interesting:

The hidden-to-hidden weight matrix of the RNN is SVD decomposed and train separately.
Each orthogonal part of the decomposition is optimized multiplicatively according to Cayley Transformation to maintain its orthogonal properties.

Now, I'm not so strong with the math behind the technique, but I could be hand-waving and say that albeit being multiplicative, it is just another method of gradient descent, and each orthogonal part is still minimizing the Loss function. So far so good.
But what they are doing is actually split the original optimization problem into multiple sub-optimization (2 for orthogonal matrices and n for the number of singular values), and then multiplied the result together. How can we be sure about the convergence and the optimality of such method? Or is this the case where we can say nothing and let the experiment speak for themselves?
","['neural-networks', 'recurrent-neural-networks', 'papers', 'gradient-descent', 'optimization']",
CSP heuristic to simultaneously reduce conflicts and find near optimal assignment,"
I am trying to design a good heuristic to solve a constraint satisfaction problem (CSP). I think that a possible heuristic to use is
$$h_1(\text{state}) = \text{number of conflicts in state}$$
However, of the possible solutions to the CSP, some have a lower cost (are better). So I can't just use $h_1$ as my heuristic.
Since the state space is pretty huge, I want to use the local search with a heuristic $h$, that guides my variable assignments towards a low-cost solution while reducing the conflicts. The way I am thinking about going about this is: of the variable assignments which do not cause conflicts (are valid), apply $h$ to them, pick the variable assignment which has the lowest/best value $h$ value. So $h$ would not handle conflicts, I would make sure any assignments considered by $h$ are guaranteed to be valid.
Ideally, though, I want $h$ to both drive down the conflicts to 0 and simultaneously guide the assignments to the lowest cost solution. Is this generally possible?
","['heuristics', 'constraint-satisfaction-problems', 'local-search']",
Is non-negative matrix factorization for machine learning obsolete?,"
I am taking a course about using matrix factorization for machine learning.
The first thing that came into my mind is by using the matrix factorization we are always limited to linear relationships between the data, which is very limiting to predict complex patterns.
In comparison with neural networks, where we can use a non-linear activation function. It seems to me that all the tasks that matrix factorization can achieve will score better using a simple multilayer neural network.
So, can I conclude that NMF and matrix factorization for machine learning, in general, are not that practical, or there are cases where it's better to use NMF?
","['neural-networks', 'machine-learning', 'reference-request', 'applications']",
"What is the difference between Q-learning, Deep Q-learning and Deep Q-network?","
Q-learning uses a table to store all state-action pairs. Q-learning is a model-free RL algorithm, so how could there be the one called Deep Q-learning, as deep means using DNN; or maybe the state-action table (Q-table) is still there but the DNN is only for input reception (e.g. turning images into vectors)?
Deep Q-network seems to be only the DNN part of the Deep Q-learning program, and Q-network seems the short for Deep Q-network.
Q-learning, Deep Q-learning, and Deep Q-network, what are the differences? May be there a comparison table between these 3 terms?
","['reinforcement-learning', 'comparison', 'q-learning', 'dqn', 'deep-rl']","In Q-learning (and in general value based reinforcement learning) we are typically interested in learning a Q-function, $Q(s, a)$. This is defined as
$$Q(s, a) = \mathbb{E}_\pi\left[ G_t | S_t = s, A_t = a \right]\;.$$For tabular Q-learning, where you have a finite state and action space you can maintain a table lookup that maintains your current estimate of the Q-value. Note that in practice even the spaces being finite might not be enough to not use DQN, if e.g. your state space contains a large number, say $10^{10000}$, of states, then it might not be manageable to maintain a separate Q-function for each state-action pairWhen you have an infinite state space (and/or action space) then it becomes impossible to use a table, and so you need to use function approximation to generalise across states. This is typically done using a deep neural network due to their expressive power. As a technical aside, the Q-networks don't usually take state and action as input, but take in a representation of the state (e.g. a $d$-dimensional vector, or an image) and output a real valued vector of size $|\mathcal{A}|$, where $\mathcal{A}$ is the action space.Now, it seems in your question that you're confused as to why you use a model (the neural network) when Q-learning is, as you rightly say, model-free. The answer here is that when we talk about Reinforcement Learnings being model-free we are not talking about how their value-functions or policy are parameterised, we are actually talking about whether the algorithms use a model of the transition dynamics to help with their learning. That is, a model free algorithm doesn't use any knowledge about $p(s' | s, a)$ whereas model-based methods look to use this transition function - either because it is known exactly such as in Atari environments, or it must need to be approximated - to perform planning with the dynamics."
How to implement very simple move-ordering for alpha-beta pruning,"
I've done implementing alpha-beta, and transpositional table on my search tree algorithm so I decided to implement move-ordering next. But once I implemented it, it's way more longer to respond than before?
Here's my code so far:
function sortMoves(chess)
{
    const listA = [], listB = chess.moves({ verbose: true });
    const scores = [];

    // calc best moves
    const moves = [...listB];
    for (const move of moves)
    {
        const state = chess.move(move, {
            promotion: 'q'
        });
        scores.push(evaluate(chess.board()));
        chess.undo();
    }
    
    // sort move
    for (var i = 0; i < Math.min(5, moves.length); i++)
    {
        let maxEval = -Infinity;
        let maxIndex = 0;
        
        for (var j = 0; j < scores.length; j++)
        {
            if (scores[j] > maxEval)
            {
                maxEval = scores[j];
                maxIndex = j;
            }
        }
        
        scores[maxIndex] = -Infinity;
        listA.push(moves[maxIndex]);
        listB.splice(maxIndex, 1);
    }
    
    const newList = listA.concat(listB);
    return newList;
}

I am expecting for this to respond quicker than before but it turns out it's not. So my question is am I actually sorting the moves correctly? or should I write some code for the alpha-beta pruning related to the sorted moves?
Here's my negamax function:
function negamax(chess, depth, depthOrig, alpha, beta, color)
{
    // transposition table look up
    const alphaOrig = alpha;
    const hashKey = zobrist.hash(chess);
    const lookup = transposition.get(hashKey);
    if (lookup)
    {
        if (lookup.depth >= depth)
        {
            if (lookup.flag === EXACT)
                return lookup.score;
            else if (lookup.flag === LOWERBOUND)
                alpha = Math.max(alpha, lookup.score);
            else if (lookup.flag === UPPERBOUND)
                beta = Math.min(beta, lookup.score);
            
            if (alpha >= beta)
                return lookup.score;
        }
    }


    if (depth === 0 || chess.game_over())
    {
        // if current turn is checkmated,
        // remove the king on the board
        // so the AI knows if the move
        // will lead to checkmate or not, if
        // it's remove on the board, 
        // the checkmated team will
        // reduce the king's value leading
        // the AI to move in checkmate
        const kingPos = getPiecePos(chess, 'k', chess.turn());
        if (chess.in_checkmate())
            chess.remove(kingPos);
        
        const evaluation = evaluate(chess.board());
        chess.put({ type: chess.KING, color: chess.turn() }, kingPos);

        return color * evaluation;
    }

    
    /* let moves = chess.moves();
    if (lookup)
    {
        console.log(moves, depth)
        const bestMove = lookup.move;
        const moveIndex = moves.indexOf(bestMove);
        const arr = moves.splice(moveIndex, 1);
        moves = arr.concat(moves);
        console.log(moves, depth)
    } */
    const moves = sortMoves(chess);
    /* const moves = chess.moves({ verbose: true }); */


    let count = 0;
    let score = -Infinity;
    let bestMove = null;
    if (lookup)
        bestMove = lookup.move;

    
    for (const move of moves)
    {
        const state = chess.move(move, {
            promotion: 'q'
        });
        searchedMoves++;
        if (count === 0)
            score = -negamax(chess, depth-1, depthOrig, -beta, -alpha, -color);
        else
        {
            score = -negamax(chess, depth-1, depthOrig, -alpha-1, -alpha, -color);
            if (alpha < score < beta)
                score = -negamax(chess, depth-1, depthOrig, -beta, -score, -color);
        }
        chess.undo();

        if (score > alpha)
        {
            alpha = score;
            bestMove = move;
        }

        // do I add something on this part?
        count++;
        if (alpha >= beta)
            break;
    }

    
    // transposition table store
    const key = zobrist.hash(chess);
    keyArr.push(key);
    const entry = new Transposition();
    entry.score = score;
    entry.depth = depth;
    entry.move = bestMove;
    if (score <= alphaOrig)
        entry.flag = UPPERBOUND;
    else if (score >= beta)
        entry.flag = LOWERBOUND;
    else
        entry.flag = EXACT;
    transposition.set(key, entry);


    return alpha;
}
```

","['chess', 'alpha-beta-pruning']",
What does Dice Loss should receive in case of binary segmentation,"
I implemented Dice loss class in pytorch:
import torch
import torch.nn as nn


class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        smooth = 1.

        input_flat = inputs.contiguous().view(-1)
        target_flat = targets.contiguous().view(-1)

        intersection = (input_flat * target_flat).sum()
        A_sum = torch.sum(input_flat * input_flat)
        B_sum = torch.sum(target_flat * target_flat)

        dsc = (2. * intersection + smooth) / (A_sum + B_sum + smooth)
        return 1 - dsc 

Now I tested it in 2 scenarios:

where inputs is the prediction from the network without applying activation (in my case sigmoid), only convolution with a kernel of size 1.
where inputs are the result of the network including activation of the sigmoid.

Now I get comparable results between the 2 ways, but I was wondering what is the ""Right"" way out of the 2.
","['deep-learning', 'objective-functions', 'activation-functions', 'image-segmentation', 'dice-loss']",
Is there any neural network model that can perform multiple NLP steps at once?,"
I realize most NLP algorithms have multiple steps. (e.g. OCR/speech rec > syntax > semantics > response logic > semantic output > natural language output)
Is there any NN model that can perform multiple steps in NLP at once? For example, a single network which accepts audio input and returns a semantic analysis of the given speech, or a single network which accepts text input and returns natural language output?
","['neural-networks', 'natural-language-processing', 'natural-language-understanding', 'expert-systems', 'natural-language-generation']",
What is the best approach for sentiment analysis when the text is very brief?,"
I'm working on a project to do sentiment analysis but my data is not long and properly formatted text. It's more likely to be very short sentences, e.g. tweets (in full tweet lingo), quick reviews of maybe 2-5 short sentences, etc.
If my text is of that nature, what approach would you recommend? E.g. CNNs (spaCy has a ready-made text classifier), LSTM (e.g. something like Keras), etc.
What are the pros/cons of your suggested approach (i.e. why is it better suited for classifying short paragraphs/sentences)?
I'm starting out in the area so any links/papers/etc. will be most welcome!
","['natural-language-processing', 'reference-request', 'sentiment-analysis']",
Is it possible and if so does it make sense to have dense layers in between LSTM layers?,"
I am new to LSTMs and I was wondering if it is possible to have LSTM layer then dense then LSTM again and does it make sense?
",['long-short-term-memory'],
Does DQN generalise to unseen states in the case of discrete state-spaces?,"
In my understanding, DQN is useful because it utilises a neural network as a q-value function approximator, which, after the training, can generalise to unseen states.
I understand how that would work when the input is a vector of continuous values, however, I don't understand why DQN would be used with discrete state-spaces. If the input to the neural network is just an integer with no clear structure, how is this supposed to generalise?
If, instead of feeding to the network just an integer, we fed a vector of integers, in which each element represents a characteristic of the state (separating things like speed, position, etc.) instead of collapsing everything in a single integer, would that generalise better?
","['reinforcement-learning', 'dqn', 'deep-rl', 'generalization', 'discrete-state-spaces']","An environment is said to have a discrete state-space, when the number of all possible states of the environment is finite. For example, $3\times3$ Tic-tac-toe game has a discrete state-space, since there are 9 cells on the board and only so many different ways to arrange Os and Xs.A state-space can be discrete regardless of whether integers or non-integers are used to describe it. For example, consider an environment where a state is represented with a single number. If the set of all possible states is $ \{0, 0.3, 0.5, 1\}$, your state-space is discrete, because there are only $4$ states. However, if all possible states is the set of real numbers from $0$ to $1$, than it's not discrete anymore - because there are infinitely many of them. State-space can still be discrete, even if possible states are represented with multiple numbers. For example, our environment could be $10\times10\times10$ cube, where the agent is only allowed to stand on integer coordinates. In this scenario, there are $1000$ different places where the agent can be and hence the state-space is discrete.The Deep Q-Network can be designed to accept any type of input; just like a regular ANN, it's not restricted to only one integer. An input to DQN is the state of the environment, regardless of how it's represented. For the previous example, you can setup the DQN to have an input layer with $3$ neurons, each one accepting an integer that describes agent's position along the $x, y, z$ axes.One downfall of Q-learning is that when the environment has a very large number of states and actions, representing each state-action pair becomes impractical in terms of memory. Think of chess, where there are so many different possible positions, and multiple available moves in each of them. Moreover, for an agent to learn properly, every state-action pair value must be visited (agent needs to determine Q-values), which can impractical in terms of training time.DQN algorithm takes care of these problems: it only needs to store the neural network (also few other things if you use a variation of DQN), and it doesn't need to visit every state-action pair to learn. They way it learns is by adjusting the weights and biases in the network in order to approximate the optimal policy. Given the algorithm is implemented correctly, the agent should be able to pick up some useful patterns (or solve the environment).I used this paper as a reference for one of my projects. It implements the DQN algorithm to learn to play Sungka (a game similar to Mancala), which has finite number of possible states and actions."
Where does this variation of the importance sampling weight come from?,"
I have seeing a variation in importance sampling (IS) in Prioritized Experience Replay (PER) in some implementations regarding the original paper approach stated as (in section 3.4):
$$
w_{i}=\left(\frac{1}{N} \cdot \frac{1}{P(i)}\right)^{\beta}
$$
For something like this:
$$
w_{i}=\left(\frac{\min (P(i))}{P(i)}\right)^{\beta}
$$
Does anyone know where it comes from? A reference that explains the reason for that new formula and improvements obtained?
My intuition guides me to some conclusions, not necessarily correct, using this new formula:

In the beginning, supposing that the PER stills have empty positions, $\min(P(i)) \sim 0$, not giving too much weight for samples. But it grows substantially once the capacity is achieved as well as when the error becomes low (plus the incrementing Beta)

A code on github that applies this: link
","['reinforcement-learning', 'dqn', 'deep-rl', 'experience-replay', 'importance-sampling']",
How to train a policy model incrementally to solve a problem similar to the vehicle routing problem?,"
I have a problem similar to the vehicle routing problem (VRP) that I want to solve with reinforcement learning. In this problem, the agent starts from the point $(x_0, y_0)$, then it needs to travel through $N$ other points, $(x_1, y_1), \dots, (x_n, y_n)$. The goal is to minimize the distance traveled.
Right now, I am modeling a state as a point $(x, y)$. There are 8 possible actions: go east, go north-east, go north, go north-west, go-west, go south-west, go south, go south-east. Each action goes by a pace of 100 metres.
After reaching near a destination point, that destination point is removed from the list of destination points.
The reward is the reciprocal of total distance until all destination points reached (there's a short optimisation to arrange the remaining points for a better reward).
I'm using a DNN model to keep the policy of a reinforcement learning agent, so this DNN maps a certain state to suitable action.
However, after every action of the agent with a good reward, the training data are added with 1 more sample, it's kinda incremental learning.
Should the policy model be trained again and again with every new sample added in? This does take too much time.
Any better RL approach to the problem above?
","['reinforcement-learning', 'training', 'deep-rl', 'incremental-learning', 'travelling-salesman-problem']","I found out a concept called 'Experience Replay', which trains a single step every time a new data sample is added instead of training to max epochs.That is, instead of this training loop:Do training this way (for a single-episode ML problem, no incremental):Do training this way (for a multi-episode ML problem, incremental data):For multiple-episode problems especially those problems with unlimited episodes, the training loop needs to forget (ie. exclude from training) old episodes which are very distant in the past, or select a random number of old episodes (consider them a batch, random batch) to be in every round of experience replay. Anyway, without eliminating some old data, the amount of training data are too much since it's unlimited number of episodes."
What dataset might Elon Musk's Dall-E have used?,"
Dall-E, it can generate many imaginative images from the description, even some peculiar images, how did they actually create this kind of dataset to train this AI , because there is not much of that kind of data which include weird images and descriptive text, how did they create this massive dataset. Does anyone have any idea?
If you have no idea what I am talking about, please refer to this link: https://openai.com/blog/dall-e/.
","['natural-language-processing', 'datasets', 'generative-adversarial-networks', 'image-generation', 'natural-language-understanding']",
How to make input variable as trainable parameter in a neural network?,"
I am working on an optimization problem. First, I have done forward training to work the network as a surrogate model, then I freeze the output and I want to find an optimal value of input for a given output.
","['neural-networks', 'training', 'optimization', 'inverse-rl']",
How to make SAC (Soft-Actor-Critic) learn a policy?,"
I cannot make SAC learn a task in a certain environment. The point is that it actually sometimes finds a very good policy, but it never learns the policy in the end. I am using the SAC implementation from stable-baselines3, which is correct as far as I have seen.
I have an environment driven by complex dynamics. I have to control a variable to be in a certain range. Every time the variable goes out of minimum or maximum range the environment is done. The action is continuous (between 0 and 30). My goal is to keep the variable (1D) in the range for as long as possible (millions of steps per episode would be ideal). There are certain characteristics of the environment that may make it particular:

The action can only drive the variable to lower values. The variable can go up as a result of the environment dynamics (not controlled) and as a consequence of certain events (not controlled) that occur at random intervals.
The observation is a noisy sample of the variable. The observation is just a real number.
The effect of actions in the variable is usually delayed. That is, applying an action does not immediately lower the value of the variable.

I have tried SAC with many different hyperparameters. It sometimes find very good policies, policies that last for thousands and even millions of steps in evaluation or rollout. But it never learns such policies. Even saving the policy in those cases, they are not able to produce a lone episode later. In the attached image, it can be seen that during the training (in some evaluations) the policy is able to run for thousand of steps. But then it never learns that. I only show 500K here steps but I have run test for 1.5 million training timesteps.

So, my question is (I have several ones actually):

Is SAC not suitable for this problem? I have also run TD3 and PPO but without better results and SAC is the only one actually able to find those  policies that make very long episodes. Any other algorithm?
I have tried several reward functions, and, in the end, a simple one that gives 1 for every step and 0 when done is the one that seems to give better results. In the image, the reward is one for every step and -100 when done.
Since the values of the variable are time correlated due to the dynamics, I have also tried with RNN actors (with TF Agents), but results do not improve.
I cannot see any relationship between the actor loss and critic loss and the results (maybe that is my problem). The loss seem to be larger when the episodes are longer (which is what I want).

Any advice is highly appreciated. Thanks
","['objective-functions', 'continuous-action-spaces', 'soft-actor-critic']",
Can someone explain to me this implementation of Tile Coding using Hash Tables?,"
The code below is adapted from this implementation.
from math import floor

basehash = hash

class IHT:
    ""Structure to handle collisions""
    def __init__(self, sizeval):
        self.size = sizeval
        self.overfullCount = 0
        self.dictionary = {}

    def count (self):
        return len(self.dictionary)

    def getindex (self, obj, readonly=False):
        d = self.dictionary
        if obj in d: return d[obj]
        elif readonly: return None
        size = self.size
        count = self.count()
        if count >= size:
            if self.overfullCount==0: print('IHT full, starting to allow collisions')
            self.overfullCount += 1
            return basehash(obj) % self.size
        else:
            d[obj] = count
            return count

def hashcoords(coordinates, m, readonly=False):
    if type(m)==IHT: return m.getindex(tuple(coordinates), readonly)
    if type(m)==int: return basehash(tuple(coordinates)) % m
    if m==None: return coordinates


def tiles(ihtORsize, numtilings, floats, ints=[], readonly=False):
    """"""returns num-tilings tile indices corresponding to the floats and ints""""""
    qfloats = [floor(f*numtilings) for f in floats]
    Tiles = []
    for tiling in range(numtilings):
        tilingX2 = tiling*2
        coords = [tiling]
        b = tiling
        for q in qfloats:
            coords.append( (q + b) // numtilings )
            b += tilingX2
        coords.extend(ints)
        Tiles.append(hashcoords(coords, ihtORsize, readonly))
    return Tiles

if __name__ == '__main__':
    tc=IHT(4096)
    tiles = tiles(tc, 8, [0, 0.5], ints=[], readonly=False)
    print(tiles)

I'm trying to figure out how the function tiles() works. It implements tile coding, which is explained in ""Reinforcement Learning: An Introduction"" (2020) Sutton and Barto on page 217.
So far I've figured out:

qfloats rescales the floating numbers to be the largest integer less than or equal to the original floating number, these are then re-scaled by the number of tilings.

Then, for each tiling, a list is created, the first element of the list is the tiling number, followed by the coordinates of the floats for that tiling, i.e. each of the re-scaled numbers is offset by the tiling number, b and integer divided by numtilings.

Finally, hashcoords first checks the dictionary d to see if this list has appeared before, if it has it returns the number relating to that list. It not it either creates a new entry with that list at the key and the count as the value or if the count is more than or equal to the size it adds one to overfullCount and returns basehash(obj) % self.size.


I'm struggling to understand two parts:

What is the tilingX2 part doing?

Why is tilingX2 added to b after the first coordinate has been calculated? It seems to me that each coordinate should be treated separately

And why by a factor of 2?

What is the expression basehash(obj) % self.size doing? I'm quite new to the concept of hashing. I know that, generally, they create a unique number for a given input (up to a limit), but I'm really struggling to understand what is going on in the line above.


","['reinforcement-learning', 'python', 'implementation', 'function-approximation', 'tile-coding']",
How to implement REINFORCE with eligibility traces?,"
The pseudocode below is taken from Barto and Sutton's ""Reinforcement Learning: an introduction"". It shows an actor-critic implementation with eligibility traces. My question is: if I set $\lambda^{\theta}=1$ and replace $\delta$ with the immediate reward $R_t$, do I get a backwards implementation of REINFORCE?

","['reinforcement-learning', 'actor-critic-methods', 'reinforce', 'eligibility-traces']",
Understanding Batch Normalization for CNNs,"
I am trying to understand how batch normalization (BN) works in CNNs. Suppose I have a feature map tensor $T$ of shape $(N, C, H, W)$
where $N$ is the mini-batch size,
$C$ is the number of channels, and
$H,W$ is the spatial dimension of the tensor.
Then it seems there could a few ways of going about this:
Method 1: $T_{n,c,x,y} :=  \gamma*\frac {T_{c,x,y} - \mu_{x,y}} {\sqrt{\sigma^2_{x,y} + \epsilon}} + \beta$ where $\mu_{x,y} = \frac{1}{NC}\sum_{n, c} T_{n,c,x,y}$ is the mean for all channels $c$ for each batch element $n$ at spatial location $x,y$ over the minibatch, and
$\sigma^2_{x,y} = \frac{1}{NC} \sum_{n, c} (T_{n, c,x,y}-\mu_{c})^2$ is the variance of the minibatch for all channels $c$ at spatial location $x,y$.
Method 2: $T_{n,c,x,y} :=  \gamma*\frac {T_{c,x,y} - \mu_{c,x,y}} {\sqrt{\sigma^2_{c,x,y} + \epsilon}} + \beta$ where $\mu_{c,x,y} = \frac{1}{N}\sum_{n} T_{n,c,x,y}$ is the mean for a specific channels $c$ for each batch element $n$ at spatial location $x,y$ over the minibatch, and
$\sigma^2_{c,x,y} = \frac{1}{N} \sum_{n} (T_{n, c,x,y}-\mu_{c})^2$ is the variance of the minibatch for a channel $c$ at spatial location $x,y$.
Method 3: For each channel $c$ we compute the mean/variance over the entire spatial values for $x,y$ and apply the formula as
$T_{n, c,x,y} := \gamma*\frac {T_{n, c,x,y} - \mu_{c}} {\sqrt{\sigma^2_{c} + \epsilon}} + \beta$, where now $\mu_c = \frac{1}{NHW} \sum_{n,x,y} T_{n,c,x,y}$ and $\sigma^2{_c} = \frac{1}{NHW} \sum_{n,x,y} (T_{n,c,x,y}-\mu_c)^2 $
In practice which of these methods is used (if any) are correct for?
The original paper on batch normalization , https://arxiv.org/pdf/1502.03167.pdf , states on page 5 section 3.2, last paragraph, left side of the page:

For convolutional layers, we additionally want the normalization to
obey the convolutional property – so that different elements of the
same feature map, at different locations, are normalized in the same
way. To achieve this, we jointly normalize all the activations in a
minibatch, over all locations. In Alg. 1, we let $\mathcal{B}$ be the set of all
values in a feature map across both the elements of a mini-batch and
spatial locations – so for a mini-batch of size $m$ and feature maps of
size $p \times q$, we use the effective mini-batch of size $m^\prime = \vert \mathcal{B} \vert = m \cdot pq$. We learn a pair of parameters $\gamma^{(k)}$ and $\beta^{(k)}$ per feature map,
rather than per activation. Alg. 2 is modified similarly, so that
during inference the BN transform applies the same linear
transformation to each activation in a given feature map.

I'm not sure what the authors mean by ""per feature map"", does this mean per channel?
","['convolutional-neural-networks', 'batch-normalization']",
"In UCB, is the actual upper bound an upper bound of an one-sided or two-sided confidence interval?","
I'm a bit confused about the visualization of the upper bound (following the notation of (c.f. Sutton & Barto (2018))
$$Q_t(a)+C\sqrt{\frac{\mathrm{ln}(t)}{N_t(a)}}$$
In many blog posts about the UCB(1)-algorithm, such as visualized in the following image (c.f. Link ):

Isn't the upper (confidence) bound simply the upper bound of a one-sided confidence interval instead of a two-sided confidence interval as shown in the image above?
A lower bound of the interval is completely useless in this case, or am I wrong?
","['machine-learning', 'reinforcement-learning', 'multi-armed-bandits', 'upper-confidence-bound']",
When do two identical neural networks have uncorrelated errors?,"
In Chapter 9, section 9.1.6, Raul Rojas describes how committees of networks can reduce the prediction error by training N identical neural networks and averaging the results.
If $f_i$ are the functions approximated by the $N$ neural nets, then:
$$
Q=\left|\frac{1}{N}(1,1, \ldots, 1) \mathbf{E}\right|^{2}=\frac{1}{N^{2}}(1,1, \ldots, 1) \mathbf{E} \mathbf{E}^{\mathrm{T}}(1,1, \ldots, 1)^{\mathrm{T}}\tag{9.4}\label{9.4}
$$
is the quadratic error of the average of the networks, where
$$
\mathbf{E}=\left(\begin{array}{cccc}
e_{1}^{1} & e_{2}^{1} & \cdots & e_{m}^{1} \\
\vdots & \vdots & \ddots & \vdots \\
e_{1}^{N} & e_{2}^{N} & \cdots & e_{m}^{N}
\end{array}\right),
$$
and $\mathbf{E}$'s rows are the errors of the approximations of the $N$ functions, i.e. $\mathbf{e}^{i} = f_i(\mathbf{x}^{i}) - t_i$, for each of the input-output pairs $\left(\mathbf{x}^{1}, t_{1}\right), \ldots,\left(\mathbf{x}^{m}, t_{m}\right)$ used in training.
Is there a way to assure that the errors for a neural network are uncorrelated to the errors of the others?
Raul Rojas says that the uncorrelation of residual errors is true for a not too large $N$ (i.e. $N < 4$). Why is that?
","['neural-networks', 'proofs', 'statistics', 'ensemble-learning']",
Bounding Box Regression - An Adventure in Failure [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 12 months ago.







                        Improve this question
                    



I've solved many problems with neural networks, but rarely work with images.  I have about 18 hours into creating a bounding box regression network and it continues to utterly fail.  With some loss functions it will claim 80% accuracy during training and validation (with a truly massive loss on both) but testing the predictions reveals a bounding box that only moves one or two pixels in any given direction and seems to totally ignore the data.  I've now implemented a form of IoU loss, but find that IoU is pinned at zero... which is obviously true based on the outputs after training. :). I'd like someone to look this over and give me some advice on how to proceed next.
What I Have
I am generating 40000 examples of 200x100x3 images with a single letter randomly placed in each.  Simultaneously I am generating the ground truth bounding boxes for each training sample.  I have thoroughly validated that this all works and the data is correct.
What I Do To It
I am then transforming the 200x100x3 images down to greyscale to produce a 200x100x1 image.  The images are then normalized and the bounding boxes are scaled to fall between 0 and 1.  In simplified form, this happens:
x_train_normalized = (x_data - 127.5) / 127.5
y_train_scaled = boxes[:TRAIN]/[WIDTH,HEIGHT,WIDTH,HEIGHT]

I've been through this data carefully, even reconstituting images and bounding boxes from it.  This is definitely working.
Training
To train, after trying mse and many others, all of which fail equally badly, I have implemented a simple custom IOU loss function.  It actually returns -ln(IoU).  I made this change based on a paper since the loss was (oddly?) pinned at zero over multiple epochs.
(Loss function:)
import tensorflow.keras.backend as kb
def iou_loss(y_actual,y_pred):
    b1 = y_actual
    b2 = y_pred
#    tf.print(b1)
#    tf.print(b2)
    zero = tf.convert_to_tensor(0.0, b1.dtype)
    b1_ymin, b1_xmin, b1_ymax, b1_xmax = tf.unstack(b1, 4, axis=-1)
    b2_ymin, b2_xmin, b2_ymax, b2_xmax = tf.unstack(b2, 4, axis=-1)
    b1_width = tf.maximum(zero, b1_xmax - b1_xmin)
    b1_height = tf.maximum(zero, b1_ymax - b1_ymin)
    b2_width = tf.maximum(zero, b2_xmax - b2_xmin)
    b2_height = tf.maximum(zero, b2_ymax - b2_ymin)
    b1_area = b1_width * b1_height
    b2_area = b2_width * b2_height

    intersect_ymin = tf.maximum(b1_ymin, b2_ymin)
    intersect_xmin = tf.maximum(b1_xmin, b2_xmin)
    intersect_ymax = tf.minimum(b1_ymax, b2_ymax)
    intersect_xmax = tf.minimum(b1_xmax, b2_xmax)
    intersect_width = tf.maximum(zero, intersect_xmax - intersect_xmin)
    intersect_height = tf.maximum(zero, intersect_ymax - intersect_ymin)
    intersect_area = intersect_width * intersect_height

    union_area = b1_area + b2_area - intersect_area
    iou = -1 * tf.math.log(tf.math.divide_no_nan(intersect_area, union_area))
    return iou

The Network
This has been through many, many iterations.  As I said, I've solved many other problems with NNs... This is the first one to get me completely stuck.  At this point, the network is dramatically stripped down but continues to fail to train at all:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers

tf.keras.backend.set_floatx('float32') # Use Float32s for everything

input_shape = x_train_normalized.shape[-3:]
model = keras.Sequential()
model.add(layers.Conv2D(4, 16, activation = tf.keras.layers.LeakyReLU(alpha=0.2), input_shape=input_shape))
model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
model.add(layers.Dropout(0.2))
model.add(layers.Flatten())
model.add(layers.Dense(200, activation = tf.keras.layers.LeakyReLU(alpha=0.2)))
model.add(layers.Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.2)))
model.add(layers.Dense(4, activation=""sigmoid""))

model.compile(loss = iou_loss, optimizer = ""adadelta"", metrics=['accuracy'])
history = model.fit(x_train_normalized, y_train_scaled, epochs=8, batch_size=100, validation_split=0.4)

All pointers are welcome!  In the meantime I'm implementing a center point loss function to see if that helps at all.
","['tensorflow', 'object-detection', 'weights-initialization', 'bounding-box']","In the end, this problem turned out to be largely a matter of the gradient descent falling into local minima.For those reading for posterity, one of the issues in ML that is difficult to work around is that we cannot intuitively choose reasonable initial values for the weights, biases, and kernels (in the CNN).  As a result, we typically allow them to initialize randomly.  This can present some challenges.One of the biggest challenges is that when you start from a random starting point, it's difficult to tell someone how to completely replicate your experiments.  This isn't terribly important in the end since you can provide them with the saved parameters from your trained model.  However, this can also lead to networks that appear to be ""bad"" that are in fact perfectly fine.In this case, I had spent much of the time initializing the CNN with a uniform initializer (not present in the code above).  I will sometimes use a random seed or some other function to generate initial values so that I can better improve networks through genetic search tools.It seems that the uniform initializers combined with the various network iterations and this particular data lead to absolutely abysmal training performance and non-convergence.When I ran the network as above with random initializations and one or two tweaks, it converged well.  Some training iterations will pin one of the sides of the bounding box at the edge, some will never converge, but I've managed to successfully train several that are in the 96-98% accuracy range for the bounding boxes in my test set of 20000, so all is well!"
Are there any new weight initialization techniques for DNN published after 2015?,"
Considering weights initialization in my personal projects, I always used some standard techniques such as:

Glorot (also known as Xavier) initialization (2010).
Mertens initialization (2010).
He initialization (2015).

As it is a very active research field, are there some innovations in recent years that have increased the performance of DNNs?
I am thinking specifically of architectures such as DNNs and CNNs with activation functions, such as ReLU,  ELU, PReLU, Leaky ReLU, SELU, Swish, and Mish.
","['deep-learning', 'reference-request', 'deep-neural-networks', 'weights', 'weights-initialization']",
How does replacing states with latent representations help RL agents?,"
I have seen many papers using autoencoders to replace images (states) with latent representations. Some of those methods have shown higher rewards using such techniques. However, I do not understand how this helps the RL agent learn better. Perhaps viewing latent representations allows the agent to generalize to novel states more quickly?
Here are 2 papers I have read -

Deep Spatial Autoencoders for Visuomotor Learning
DAQN: Deep Auto-encoder and Q-Network

","['reinforcement-learning', 'papers', 'autoencoders', 'generalization', 'state-spaces']","In short, it is much easier for the agent to learn from a smaller dimensional state space. This is because the agent must also do representation learning; i.e. it must also infer what the state is telling it as part of the learning process. If you think of the architecture used in DQN to solve Atari, they had a CNN that outputted a vector which was then passed through some dense layers. Here the representation learning was done by the CNN and was trained using an end-to-end approach i.e. all updates to the network weights were done through the reinforcement learning objective; that is there is no supervised or unsupervised learning that takes place.This can be particularly difficult when you combine images with sparse rewards as there is not a lot of feedback so the representation learning can take a long time. This paper gives a good description of the problem of decoupling representation learning from reinforcement learning with a nice solution.The other main 'problem setting' I have seen images replaced with a latent state is when the authors are looking at planning. The problem with doing any kind of planning is that a model of the transition dynamics, $p(s' | s, a)$, is needed. For high dimensional state spaces such as images, this can be very difficult to predict and even relatively small errors will quickly compound so if you use the model to predict multiple time steps into the future the planner is useless because of these compounding errors. I think there is a discussion on this in this paper (certainly there will be references therein that point you in the right direction)."
What is the state-space complexity of Spades?,"
AI reached super-human level in many complex games, including imperfect information games such as six-player no-limit Texas hold’em poker.  However, it still did not reached that level in Trick-taking card games such as Spades, Bridge, Skat and Whist.  In a related question, I am asking Why Trick-Taking games are a challenge for AI.
An important factor that makes those games a challenge for AI is their size, to be precise lets talk about the State-space complexity which is define as the number of legal game positions reachable from the initial position of the game  [Victor Allis (section 6.2.4)].
What is the size of Spades?
","['game-ai', 'game-theory', 'games-of-chance', 'state-space-complexity']",
What are acting as weights in a convolution neural network?,"
Looking at some old notes I took on CNN's and I wrote down that the weights in a CNN are acting like filters in a CNN but to be honest I don't really know what the weights are acting as in a CNN and was wondering if someone could explain that clearly to me.
","['convolutional-neural-networks', 'weights']",
Predicting continous value with CNN (prediction of fruit maturity),"
I want to train some IA algorithm to be able to evaluate the maturity of a fruit (say, measured in numbers of days before rotten) based on an image of the fruit.
My first instinct is to go with convolutional neural network (CNN), since those have proven very efficient for recognizing images. However, I am not sure what the output layer should look like in this case.
I could separate the data into a bunch of classes (1 day left, 2 days left, 3 days left, etc.) and use one output node for each of these classes, as in an usual classification task, but in doing so I completely lose the continuous nature of the output, which makes me think it might not be the optimal way to proceed.
Another option would be to just have a unique output node, whose activation would correspond to the continuous value to predict, here the number of days left (normalized appropriately to lie between 0 and 1). This would have the advantage of taking the continuity into account, but I have been told that neural networks aren't made to predict values in that way, they really are best suited for classification into discrete classes.
What do you think would be the best way to proceed? Is there another way to nudge a neural network so that its output is continuous? Or maybe CNN just aren't suited for this task? If you have any suggestions of other algorithms that would be efficient for this kind of task, I would be happy to know them.
","['convolutional-neural-networks', 'image-recognition', 'continuous-tasks']",
How are the parameters $\alpha_i$ of hard attention trained?,"
I have a question about Show, Attend and Tell: Neural Image CaptionGeneration with Visual Attention paper by Xu. The basic mechanism of stochastic hard attention is that each pixel of the input image has a corresponding parameter $\alpha_i$, which describes the probability that this pixel will be chosen for further processing.
But I don't see an explanation of how to train or define this parameter in the paper. Can someone explain how to train this $\alpha_i$ for each pixel?
","['computer-vision', 'training', 'papers', 'attention']",
Is better to reward short- or long-term progress in Q-learning?,"
I have been training some kind of agent to reach a target using a Q-learning based approach, and I have tried two different types of rewards:

Long-term reward: $\mathrm{reward} = - \mathrm{distance}(\mathrm{agent,target})(t+1)$

Short-term reward: $\mathrm{reward} = \mathrm{distance}(\mathrm{agent,target})(t) - \mathrm{distance}(\mathrm{agent,target})(t+1)$


In the first case, I am rewarding the current progress. In the second case, I am rewarding direct progression, but this may lead to less progression in the future. My question is, what kind of reward does Q-learning need?
I understand that the $\gamma$ factor should incorporate long term rewards, so it makes more sense to reward direct progression. However, using long-term rewards gave better results for my scenario...
","['reinforcement-learning', 'q-learning', 'dqn', 'reward-design', 'reward-functions']",
"In this implementation of pix2pix, why are the weights for the discriminator and generator losses set to 1 and 100 respectively?","
I am working on a pix2pix GAN model that was inspired by the code in this Github repository. The original code is working and I have already customized most of the code for my needs. However, there is one part I am unable to understand.
The pix2pix GAN is a conditional GAN network that takes an image as a condition and outputs a modified image - such as blurry to clear, facades to buildings, filling up cut out part of an image, etc. The combined model thus takes as input a conditional image, the discriminator compares it with the dummy matrix named valid or fake, containing 0s or 1s according to validity (0 for generated samples, 1 for real samples). The generator loss is according to similarity with real sample + discriminator. The following code corresponds to what I told:
self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])
self.combined.compile(loss=['mse', 'mae'],
                      loss_weights=[1, 100],
                      optimizer=optimizer)

The losses are thus set as MSE for discriminator output and MAE for generator. That seems to be OK, but I can not understand why the implementation uses 1 and 100 for the weights of the discriminator and generator losses, respectively, which seems to imply that the discriminator loss is 100 times lower than the loss of the generator. I couldn't find the reason in the original article. Are my understandings of the GAN incorrect?
Disclaimer: I have posted this question on Stats SE, but have no luck with answers. Maybe it is more suitable for AI.
","['objective-functions', 'generative-adversarial-networks']","After further research, I have found the answer. nbro was of course right, the weighting is not implementation dependant, it was already introduced in the paper (arXiv). However, there is minimal information about that, only it is mentioned within the optimization function:$\arg \min_G \max_D \mathcal{L}_{cGAN}(G,D) + \lambda \mathcal{L}_{L1}(G)$In fact, the parameter lambda stands for weight of $L_1$ loss. There should be a second parameter $\epsilon$ for $\mathcal{L}_{cGAN}(G,D)$ because in real implementation you can actually change that as well. In theory, however, only modulation of $L_1$ loss is needed. The paper does not state why the $\lambda$ parameter is needed, only mentions that setting it to 0 will lead to pure cGAN implementation. David Brownlee in his blog states:The adversarial loss influences whether the generator model can output
images that are plausible in the target domain, whereas the L1 loss
regularizes the generator model to output images that are a plausible
translation of the source image. As such, the combination of the L1
loss to the adversarial loss is controlled by a new hyperparameter
lambda, which is set to 10, e.g. giving 10 times the importance of the
L1 loss than the adversarial loss to the generator during training.The loss weights thus are hyperparameters that tell the network how much plausible translation of the source image do we need."
"When calculating the cost in deep Q-learning, do we use both the input and target states?","
I just finished Andrew Ngs's deep learning specialization, but RL was not covered, so I don't know the basics of RL. So, I have been having trouble understanding the cost function in deep Q-learning. Like other cost functions in machine learning, you usually have $\hat{y}$ (the network prediction) and $y$ (the target, or what the network is being optimized for.)
I've read through a few online articles on deep Q-learning. So far, there has been no mention of setting up a target state ($y$) for the agent to produce. There has been mention of calculating a temporal-difference, however, which is where I am confused.
When calculating the cost function, are you taking the input state ($\hat{y}$) and a target state ($y$) into consideration to determine the temporal-difference?
Otherwise, I'm not sure how the cost function could determine a reward based on the input alone (state of the environment the agent is in.).
","['reinforcement-learning', 'q-learning', 'dqn', 'objective-functions', 'temporal-difference-methods']","I will first explain briefly to you the difference between supervised learning and reinforcement learning to make sure that you don't have any misunderstandings. In supervised learning you are provided with some data $\{(\textbf{x}_i, y_i)\}_{i=1}^n$ where $\textbf{x}_i$ are the features for data point $i$ and $y_i$ is its true label. Now, the aim of supervised learning is to learn a function $f$ that can accurately predict the label of a data point given its features. In deep learning this function is a neural network, $f_\theta(\cdot)$. To optimise the parameters we obtain the models prediction for the label of $y$, denoted typically by $\hat{y} = f_\theta(x)$ and we look to optimise the parameters of the function $\theta$ by minimising the loss $\mathcal{L}(\hat{y}, y)$ (note that here the loss is a function $\mathcal{L}: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$.In reinforcement learning things are quite different. We are not provided with any data. Instead we have a Markov Decision Process (MDP) and an environment that we can interact with. The state space is defined by the MDP and we can get samples, mainly tuples of the form $(s, a, r, s')$, that we can use to teach our agent how to find an optimal policy - typically an optimal policy is one that takes an action given the current state that will maximise the sum of the future rewards of the episode. So in reinforcement you can probably see that things are quite different to supervised learning, mainly in the way that we 'obtain' our data.Of course these explanations are gross oversimplifications of the learning process in both paradigms and should only be used as a brief example to try to emphasise the different between the two learning paradigms.Now, there are two ways you can parameterise your Q-function using a neural network:The second way is usually reserved for instances where you have a huge action space but only ever would consider a few feasible actions at each state - this saves computational complexity.Now, to answer your question:In case 1) you are assumed to have access to a transition tuple $(s, a, r, s')$. The temporal difference, which we will use as the target, is $\hat{y} = r + \max_a Q(s', a)$. Now, as our output of $Q(s', \cdot)$ is an $\mathbb{R}^{|\mathcal{A}|}$ what we do is make a forward pass of the network for the current $(s, a)$ tuple, i.e. we get $x = Q(s, a)$, and then change the element that corresponds to the action which satisfies $\arg\max_a Q(s', a)$ of $x$ to $\hat{y}$, so we have now got our augmented input $\tilde{x}$ which serves as our target.To make that step a bit clearer, suppose we have a 2-dimensional action space and the $\arg\max_a Q(s', a)$ is the action in the first dimension, then we would change the first dimension of $x$ to be $\hat{y}$.We then train the network using the Mean Squared Error Loss between $x$ and $\tilde{x}$ - note that no gradient information is retained when we do the forward pass to get $\tilde{x}$; in fact we usually don't use a current version of the $Q$ network, we use an 'old' version of the network called the target network (I imagine there's probably a question about this network on the site already so I won't explain it in detail).In case 2) the idea is much more simple as the outputs of the network are scalars so you can just train your network using the MSE between the scalar values of $Q(s, a)$ and $\hat{y}$ as defined above, the caveat here is that to calculate $\arg\max_a Q(s', a)$ you have to make $|\mathcal{A}|$ forward passes of the network for all possible $(s', a)$, for fixed $s'$, tuples which is why method 1) is usually preferred.Now, whilst we may use techniques from supervised learning to optimise the Q-function, you can hopefully see the differences between supervised and reinforcement learning.In short, the temporal difference is the target that you train your network towards being able to predict, and the state is (typically) the input into your neural network. For a more in depth discussion of RL being framed as supervised learning please see this answer."
Keras 1D CNN always predicts the same result even if accuracy is high on training set,"
The validation accuracy of my 1D CNN is stuck on 0.5 and that's because I'm always getting the same prediction out of a balanced data set. At the same time my training accuracy keeps increasing and the loss decreasing as intended.
Strangely, if I do model.evaluate() on my training set (that has close to 1 accuracy in the last epoch), the accuracy will also be 0.5. How can the accuracy here differ so much from the training accuracy of the last epoch? I've also tried with a batch size of 1 for both training and evaluating and the problem persists.
Well, I've been searching for different solutions for quite some time but still no luck. Possible problems I've already looked into:

My data set is properly balanced and shuffled;
My labels are correct;
Tried adding fully connected layers;
Tried adding/removing dropout from the fully connected layers;
Tried the same architecture, but with the last layer with 1 neuron and sigmoid activation;
Tried changing the learning rates (went down to 0.0001 but still the same problem).


Here's my code:
import pathlib
import numpy as np
import ipynb.fs.defs.preprocessDataset as preprocessDataset
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras import Input
from tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten, Dropout, Dense
from tensorflow.keras.optimizers import SGD

main_folder = pathlib.Path.cwd().parent
datasetsFolder=f'{main_folder}\\datasets'
trainDataset = preprocessDataset.loadDataset('DatasetTime_Sg12p5_Ov75_Train',datasetsFolder)
testDataset = preprocessDataset.loadDataset('DatasetTime_Sg12p5_Ov75_Test',datasetsFolder)

X_train,Y_train,Names_train=trainDataset[0],trainDataset[1],trainDataset[2]
X_test,Y_test,Names_test=testDataset[0],testDataset[1],testDataset[2]

model = Sequential()

model.add(Input(shape=X_train.shape[1:]))

model.add(Conv1D(16, 61, strides=1, padding=""same""))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling1D(2, strides=2, padding=""valid""))

model.add(Conv1D(32, 3, strides=1, padding=""same""))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling1D(2, strides=2, padding=""valid""))

model.add(Conv1D(64, 3, strides=1, padding=""same""))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling1D(2, strides=2, padding=""valid""))

model.add(Conv1D(64, 3, strides=1, padding=""same""))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling1D(2, strides=2, padding=""valid""))

model.add(Conv1D(64, 3, strides=1, padding=""same""))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Flatten())
model.add(Dropout(0.5))

model.add(Dense(200))
model.add(Activation('relu'))

model.add(Dense(2))
model.add(Activation('softmax'))

opt = SGD(learning_rate=0.01)

model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])

model.summary()

model.fit(X_train,Y_train,epochs=10,shuffle=False,validation_data=(X_test, Y_test))

model.evaluate(X_train,Y_train)


Here's model.fit():
model.fit(X_train,Y_train,epochs=10,shuffle=False,validation_data=(X_test, Y_test))

Epoch 1/10
914/914 [==============================] - 277s 300ms/step - loss: 0.6405 - accuracy: 0.6543 - val_loss: 7.9835 - val_accuracy: 0.5000
Epoch 2/10
914/914 [==============================] - 270s 295ms/step - loss: 0.3997 - accuracy: 0.8204 - val_loss: 19.8981 - val_accuracy: 0.5000
Epoch 3/10
914/914 [==============================] - 273s 298ms/step - loss: 0.2976 - accuracy: 0.8730 - val_loss: 1.9558 - val_accuracy: 0.5002
Epoch 4/10
914/914 [==============================] - 278s 304ms/step - loss: 0.2897 - accuracy: 0.8776 - val_loss: 20.2678 - val_accuracy: 0.5000
Epoch 5/10
914/914 [==============================] - 277s 303ms/step - loss: 0.2459 - accuracy: 0.8991 - val_loss: 5.4945 - val_accuracy: 0.5000
Epoch 6/10
914/914 [==============================] - 268s 294ms/step - loss: 0.2008 - accuracy: 0.9181 - val_loss: 32.4579 - val_accuracy: 0.5000
Epoch 7/10
914/914 [==============================] - 271s 297ms/step - loss: 0.1695 - accuracy: 0.9317 - val_loss: 14.9538 - val_accuracy: 0.5000
Epoch 8/10
914/914 [==============================] - 276s 302ms/step - loss: 0.1423 - accuracy: 0.9452 - val_loss: 1.4420 - val_accuracy: 0.4988
Epoch 9/10
914/914 [==============================] - 266s 291ms/step - loss: 0.1261 - accuracy: 0.9497 - val_loss: 4.3830 - val_accuracy: 0.5005
Epoch 10/10
914/914 [==============================] - 272s 297ms/step - loss: 0.1142 - accuracy: 0.9548 - val_loss: 1.6054 - val_accuracy: 0.5009

Here's model.evaluate():
model.evaluate(X_train,Y_train)

914/914 [==============================] - 35s 37ms/step - loss: 1.7588 - accuracy: 0.5009

Here's model.summary():
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d (Conv1D)              (None, 4096, 16)          992       
_________________________________________________________________
batch_normalization (BatchNo (None, 4096, 16)          64        
_________________________________________________________________
activation (Activation)      (None, 4096, 16)          0         
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 2048, 16)          0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 2048, 32)          1568      
_________________________________________________________________
batch_normalization_1 (Batch (None, 2048, 32)          128       
_________________________________________________________________
activation_1 (Activation)    (None, 2048, 32)          0         
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 1024, 32)          0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 1024, 64)          6208      
_________________________________________________________________
batch_normalization_2 (Batch (None, 1024, 64)          256       
_________________________________________________________________
activation_2 (Activation)    (None, 1024, 64)          0         
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 512, 64)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 512, 64)           12352     
_________________________________________________________________
batch_normalization_3 (Batch (None, 512, 64)           256       
_________________________________________________________________
activation_3 (Activation)    (None, 512, 64)           0         
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 256, 64)           0         
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 256, 64)           12352     
_________________________________________________________________
batch_normalization_4 (Batch (None, 256, 64)           256       
_________________________________________________________________
activation_4 (Activation)    (None, 256, 64)           0         
_________________________________________________________________
flatten (Flatten)            (None, 16384)             0         
_________________________________________________________________
dropout (Dropout)            (None, 16384)             0         
_________________________________________________________________
dense (Dense)                (None, 200)               3277000   
_________________________________________________________________
activation_5 (Activation)    (None, 200)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 402       
_________________________________________________________________
activation_6 (Activation)    (None, 2)                 0         
=================================================================
Total params: 3,311,834
Trainable params: 3,311,354
Non-trainable params: 480
_________________________________________________________________

","['deep-learning', 'convolutional-neural-networks', 'python', 'keras', '1d-convolution']",The solution for my problem was implementing Batch Renormalization: BatchNormalization(renorm=True). In addition normalizing the inputs helped a lot improving the overall performance of the neural network.
Current research on Gödel machines,"
Is there any current research on Gödel machines? It seems that the last article by Jürgen Schmidhuber on this topic was published in 2012: http://people.idsia.ch/~juergen/goedelmachine.html
","['reference-request', 'agi', 'problem-solving', 'godel-machine']",
How to prove the formula of eligibility traces operator in reinforcement learning?,"

I don't understand how the formula in the red circle is derived. The screenshot is taken from this paper
","['reinforcement-learning', 'proofs', 'bellman-equations', 'eligibility-traces']","I will refer to $\mathcal T^{\pi} $as $\mathcal T$ and $P^{\pi}$ as $P$ for notational simplicity
\begin{align}
(\mathcal{T})^{n+1} Q &= \mathcal{T}(\mathcal{T}(...(\mathcal{T}(Q))))\\
&= r + \gamma P(r + \gamma P(...(r + \gamma P Q)))\\
&= r + r\sum_{i=1}^{n} \gamma^i P^i + \gamma^{n+1} P^{n+1} Q
\end{align}\begin{align}
\mathcal{T}_{\lambda}Q &= (1-\lambda) \sum_{n=0}^{\infty} \lambda^n (\mathcal{T})^{n+1} Q\\
&=(1-\lambda)\{\lambda^0 (\mathcal T)^1Q + \lambda^1 (\mathcal T)^2Q + \lambda^2 (\mathcal T)^3Q + \ldots   \}
\end{align}when you plug in expression for $(\mathcal T)^i Q$ inside this sum and rearrange you get 3 sums
\begin{equation}
\mathcal{T}_{\lambda}Q = (1-\lambda) \sum_{n=0}^{\infty} \lambda^n r + (1-\lambda)\sum_{n=1}^{\infty} \lambda^n \gamma^n P^n r + (1-\lambda)\sum_{n=0}^{\infty} \lambda^n \gamma^{n+1} P^{n+1} Q
\end{equation}"
Why is neural networks being a deterministic mapping not always considered a good thing?,"
Why is neural networks being a deterministic mapping not always considered a good thing?
So I'm excluding models like VAEs since those aren't entirely deterministic. I keep thinking about this and my conclusion is that often times neural networks are used to model things in reality, which often time do have some stochasticity and since neural networks are deterministic if they are not trained on enough examples of the possible variance inputs in relation to outputs can have they cannot generalize well. Are there other reasons this is not a good thing?
","['neural-networks', 'bayesian-deep-learning', 'bayesian-neural-networks']","Your intuition is right. The main reason why a deterministic function can be undesirable (or even dangerous, as I will explain below with an example) is that we may not have enough data to learn the correct function, so we may end up learning the incorrect one. Right now, no other reason, from a theoretical point of view, comes to my mind, but below I will mention a few applications/cases where a deterministic function may not be desirable.If we had all data pairs $\{(x_i, y_i)\}$, where $x_i \in \mathcal{X}$ and $y_i = \mathcal{Y}$ are, respectively, an input and output from the unknown function that you want to learn $f: \mathcal{X} \rightarrow \mathcal{Y}$, i.e. $f(x_i) = y_i$, then you could reconstruct $f$: whenever $x_i$ is given, you just need to return $y_i$.Of course, in reality, we almost never have a large enough (training) dataset to approximate out desired (but usually unknown) function. If we learn only one (deterministic) function, then, in principle, you can catastrophically fail, i.e. your approximation of $f$, denoted as $f_\theta$ (where $\theta$ are the parameters of the neural network or any other model), can produce outputs that are completely wrong.Let me try to give you a simple example. Let's say that $f$ is defined as follows$$f: \mathbb{N} \rightarrow \{0, 1\}$$You are given a training labelled dataset $$D = \{(4, 1), (11, 0), (8, 1), (31, 0), (16, 1), (7, 0) \}.$$Apparently, our unknown function is defined as\begin{align}
h_1(x)=
\begin{cases}
1, &x \text{ mod } 2 \equiv 0\\
0, &\text{otherwise}
\end{cases}\tag{1}\label{1}
\end{align}
Given that $D$ is small, your neural network, $f_\theta$, can easily overfit $D$, i.e. learn to output $1$ when $x$ is even and $0$ otherwise.However, what if $f$ is not that function in equation \ref{1} and we collected just a dataset that doesn't represent $f$ well enough? If you look at $D$ more carefully, you will see that another possible hypothesis for $f$ is the following\begin{align}
h_2(x)=
\begin{cases}
1, &x \text{ mod } 4 \equiv 0\\
0, &\text{otherwise}
\end{cases}\tag{2}\label{2}
\end{align}
However, given that your neural network can only compute one of these functions at a time, it could compute the wrong one. Let's say that $f_\theta \approx h_1$, then it should produce $1$ when $x = 6$ (an even number). If the correct unknown function was $h_2$, i.e. $f = h_2$, then $f_\theta(6) = 1$ would be wrong (because $6$ is not a multiple of $4$).Of course, this is just a toy example. However, there are many other cases where this can happen, which may not be desirable, such as healthcare, medicine or self-driving cars, where the wrong prediction can lead to catastrophic consequences, such as the death of a person.If we maintain a probability distribution over the possible functions that are consistent with the observed data so far, we can (partially) avoid this issue. So, continuing with the example above, this probability distribution over functions should be highly uncertain about $x = 6$, whether it produces $0$ or $1$, because it has never seen the label for $x=6$, so a medical doctor or the human driver could intervene in the case of (high) uncertainty.For this reason, in the last decade, people have started to incorporate uncertainty estimation in neural networks. Neural networks that model uncertainty (to some degree) are often called Bayesian neural networks (BNNs), and there are different approaches (such as variational BNNs, MC dropout or Monte Carlo-based approaches). If you are interested in this topic, the paper Weight Uncertainty in Neural Network (2015) is a good start, especially if you are already familiar with VAEs. Given that this is a very new research area, the current solutions are still not very satisfactory. For example, you can find examples in the literature that report that MC dropout can produce very bad estimates of uncertainty (even in my master's thesis I have observed and thus concluded that this is the case), i.e. they can be highly certain when they should be highly uncertain."
Why won't my model train with CTC loss?,"
I am trying to train an LSTM using CTC loss, but the loss does not decrease when I train it. I have created a minimal example of my issue by creating training data where the network simply has to copy the current input element at each time step. Moreover, I have made the length of the label the same as the length of the input sequence and no adjacent elements in the label sequence the same so that both CTC loss and categorical cross-entropy loss can be used. I found that when using categorical cross-entropy loss the model very quickly converges, whereas when using CTC loss it gets nowhere.
I have uploaded by minimal example to colab. Does anyone know why CTC loss is not working in this case?
","['neural-networks', 'deep-learning', 'tensorflow', 'long-short-term-memory', 'ctc-loss']",
What happens if there is no activation function in some layers of a neural network?,"
What if I don't apply an activation function on some layers in a neural network. How will it affect the model?
Take for instance the following code snippet:
def model(x):
    a = Conv2D(64, (3, 3))(x)                         
    x = Conv2D(64, (3, 3), activation = 'relu')(x)
    b = Conv2D(128, (3, 3))(x)
    x = Conv2D(128, (3, 3), activation = 'relu')(b)
    return x, a, b

","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'keras', 'activation-functions']",
How to compute the gradient of the cross-entropy loss function with respect to the parameters with softmax activation function?,"
I've seen plenty of examples of people doing Sigmoid + MSE backpropagation implementations, yet I do not seem to understand how to implement backpropagation as stated in the title in the case of multi-class classifications.
What confuses me mainly are the matrix-vector shapes and their multiplications, and their implementations in code.
","['deep-learning', 'backpropagation', 'math', 'cross-entropy', 'softmax']",
How to train/update neural networks faster without a decrease in performance?,"
I noticed that there are many studies in recent years on how to train/update neural networks faster/quicker with equal or better performance. I find the following methods(except the chips arms race):

using few-shot learning, for instance, pre-taining and etc.
using the minimum viable dataset, for instance using (guided) progressive sampling.
model compression, for instance, efficent transformers
Data echoing, or simply put let the data pass multiple times in the graph(or GPU)

Is there a systematic structure on this topic and how can we update or train a model faster without loss of its capacity?
","['neural-networks', 'machine-learning', 'training', 'efficiency']",
Difference in average rewards between taking random actions and following random policies,"
I wrote two programs that simulated 10000 episodes in gym environment CartPole-v0.
The first program takes random moves in every steps in each episode. The average reward over 10000 episodes is 22.1582.
The second program uses a random policy in each episode. For each episode, initialize a 4 by 2 matrix $M$ with random numbers from a uniform distribution on $[0,1)$ that maps state observations to action values. Then choose the action with higher value in each step. The average reward over 10000 episodes is 46.8291.
The linear mapping given by $M$ covers only a portion of the search space so it seems the way actions are selected in the first program is more ""random"" than the second program. How can we go about explaining the huge discrepancy in the average rewards obtained by the two methods?
","['reinforcement-learning', 'gym']",
Can AI be understood as a generalized statistics tool? [duplicate],"







This question already has answers here:
                                
                            




What is the difference between artificial intelligence and machine learning?

                                (9 answers)
                            


What is artificial intelligence?

                                (8 answers)
                            


What is machine learning?

                                (2 answers)
                            

Closed 2 years ago.



I am a (soon-to-become, to be honest) theoretical physicist. I want to learn a bit about AI. So as you know in physics we develop theories based on as few and as simple basic equations as possible which shall explain as much of the experimental results and observations as possible. I feel that this is kind of not how AI solves problems.
My understanding is that AI can be understood as a very generalized and abstract statistics software package handling input data in a general way to find the ""best fit"" to some form of problem. Is that correct? I know it isn't. But is it vaguely correct?
I give you an example. In weather prediction there is a technique called MOS (model output statistics). It collects output from numerical weather prediction models (simulation software) as well as observational data and finds statistical relations between them to correct the model output for errors. For example, it might be that the intensity of precipitation in London is on average underestimated by the model by 10 %, so MOS will correct for that. Over time, it improves itself, because it collects more and more data. Is this already a form of AI?
","['machine-learning', 'comparison', 'definitions', 'statistical-ai', 'statistics']",
Is object-based representation of the observation space feasible?,"
I just started working on a DRL project from scratch. The state of each episode can be expressed as a state set $S=(S^A, S^B, S^C, S^D)$. Each subset is a feature set of a constituent component of the environment, say, $S^A=(a_1, a_2, a_3)$. To model components, I decided to create four pythonic classes with attributes as features. For example, class A is like:
class A:
    def __init__(self, a1, a2, a3): 
        self.a1 = a1
        self.a2 = a2
        self.a3 = a3

Each class has some methods that help in the interaction with other components (classes) and is used in the environment's step function to generate actions.
I am going to create one instance of class A, 10 instances of class B, 20 instances of class C, and a random number of between 1-10 instances of class D at the beginning of each episode. So, my observation includes 33-42 states of entities.
As far as I know, the observation space is usually encoded in n-dimensional arrays as it is in OpenAI Gym. Is it possible, feasible, or considered good practice to store instances as a sub-state of the whole observation space? In my case, it would be like storing 33-42 instances in an array (list) of 33-42 elements.
Thanks for your time and suggestions!
","['reinforcement-learning', 'python', 'environment', 'state-spaces']",
Aggregating 2D object detections into 3D object detections,"
I have a data set of 3D images with some bounding box annotations. The images are too large to train something like YOLO 3D (would run out of memory), so I instead created slices of the 3D images with corresponding 2D bounding boxes and trained a 2D object detector. During inference, I assemble the 2D detections into 3D detections. I constructed some simple heuristics to do that, which works fine, but I am wondering if there aren't any established methods of doing that.
I appreciate if you could point me in the right direction.
","['computer-vision', 'object-detection', 'yolo']",
How to embed/deploy an arbitrary machine learning model on microcontrollers?,"
Say I have a machine learning model trained on a laptop and I then want to embed/deploy the model on a microcontroller. How can I do this?
I know that TensorflowLite Micro generates a C header to be added in the project and then be embedded, but every example I read shows how it is done with neural networks, which seems legit as TensorFlow is primarily used for deep learning.
But how can I do the same with any type of model, like the ones there is in scikit-learn? So, I'm not interested in necessarily doing this with TensorflowLite Micro, but I'm interested in the general approach to solve this problem.
","['machine-learning', 'tensorflow', 'models']",
Is pre-processing used in deep learning?,"
I'm new to deep learning. I wanted to know: do we use pre-processing in deep learning? Or it is only used in machine learning. I searched for it and its methods on the internet, but I didn't find a suitable answer.
","['machine-learning', 'deep-learning', 'data-preprocessing']",
How do I learn the value function for a POMDP with a single-step horizon (bandit)?,"
Consider a POMDP with a finite number of environment states, $|\mathcal{S}| = N$, but the number of belief states is uncountably infinite. The belief state space is the convex hull of an $N$ simplex. Each turn this space is sampled with a flat probability distribution. As you are sampling from an uncountably infinite set of belief states, the probability of a belief state recurring in a finite number of samples is zero.
Now, let's suppose that there are a finite number of episodes, and each episode ends after 1-time step. At the only time step of the episode, the agent receives a belief state $b(s)$ over some fixed set of contexts $s$, selects a single action, and receives a single reward, before the episode ends.
I understand that the belief state value function, $V(b)$, is piecewise linear and convex, with a single hyperplane for each action (see e.g. [1]).
My question is, given that I only observe the belief states and the sampled rewards, how do I identify the value function, given that a belief state $b(s)$ has an infinitesimally small probability of occuring again?
The expected reward for a given belief state $b$ is just a linear function $\alpha \cdot b$, where $\alpha$ is the vector of the rewards for each state of the environment. But I cannot simply learn a linear model here because $\alpha \cdot b$ gives me the expected reward for a given belief state, but I may never start with this belief state again and so cannot simply calculate the sample mean expected reward.
","['reinforcement-learning', 'value-functions', 'multi-armed-bandits', 'pomdp', 'contextual-bandits']",
How to predict the best from a set of messages - best practice,"
Suppose I have a set of messages A,B,C,D and I want to produce the best message for a website user at a given time.
For training I plan to show random users a random single message [A/B/C/D] and fill these columns (i'm simplifying the data for illustration)

converted before
funnel state (e.g awareness, search, decision)
number of page views
message shown [A-D]
Time to convert (this will be updated later if there is a conversion)

I want to predict what is the best message to show to a specific user in order to maximise the chance of conversion (=min time to convert).
I'm not sure how to represent this for training and inference. Its not a simple prediction like predicting one of the given data points.
One option is to run prediction of time to buy for each of the messages but
1- its not efficient
2- It will prefer messages that are shown closer to purchase time regardless if they fit the current user time.
","['machine-learning', 'training', 'data-preprocessing', 'feature-selection', 'features']",
Is it possible to modify or replace the basic network of YOLO?,"
I have an idea to adapt YOLO algorithm to my application, the original YOLO algorithm is for image classifications, which have 24 convolutional layers with output class of 1000, is it possible to replace the basic network of YOLO with Alexnet or Resnet or the custom network structure designed by myself? Noted that my application have input shape of 500 * 10000 * 1 and only 4 classes for classification.
","['machine-learning', 'deep-learning', 'tensorflow', 'object-detection', 'yolo']","No, the original (or any) YOLO is for object detection. You can easily replace the feature extractor (DarkNet53, if I'm not mistaken) with any other, as long as you maintain the correct number of weights in the detection layer."
Is it possible to train one part of the network with a particular learning rate and the other part with a different one?,"
I have a combined network consisting of two parts: one is for images and the other is for numerical data. Each sample is matched with a numerical case by an ID. For this combined network, a lr of 0.01 was found to be best working via hyperparameter tunings
However, when I trained them as a separate task (binary classification), a lr of 0.001 for images and 0.01 for numerical data were best.
As for an AUC metric, the combined network (0.818) is performing on average of image and numerical networks (0.799 and 0.821, respectively).
Here I thought maybe the combined network's lr is too high for image part and should apply lower lr for that part. But, I don't know if it is possible.
If anyone has any idea of what is what, let me know
","['neural-networks', 'convolutional-neural-networks', 'pytorch', 'hyperparameter-optimization']",
Can a convolutional network predict states for a RL Agent,"
During the course of training a DQN agent, all visited states are stored in a replay buffer. Therefore would it be practically possible for a CNN, given a reasonable amount of data, to predict the next RL state (in the form of an image) for a given action?
For example, take the game of Atari as shown below -

Here the agent can take 2 major actions - go left and go right. Would a CNN be generate the image of the bar going right/left for the respective actions?
My practical knowledge of CNNs is quite limited and therefore I'm trying to gauge the abilities of CNNs before I take up a project.
","['reinforcement-learning', 'convolutional-neural-networks']","It sounds like what you're suggesting is similar to what is done in methods that use a planner. These methods looks to learn the dynamics of the MDP to use to plan during training; that is they want to be able to learn the transition probabilities $p(s'| s, a)$.In this paper that I read recently they note that learning to predict environment dynamics when the state/action space is high dimensional, as is the case with images, is difficult; so whilst it may be possible in theory it would be difficult to do and if you were predicting many steps into the future then the error would compound.A way around this, as is done in the referenced paper, is to use predict environment dynamics in a latent space. This means that they use a latent variable to predict the next state using e.g. Variational Autoencoders."
Are monotonically increasing functions easier to learn?,"
A monotonically increasing function is a function that as x gets bigger so does its output. So, if plotted, it will never go down. Although the outputs might stay constant.
Logically this seems like an easier function to learn when compared to something that can, when plotted, go up or down.
Wikipedia has some example diagrams on monotonic functions.
If I were to say that it is easier for a neural network to learn a monotonic function compared to a non-monotonic function would the statement be correct? If so, is there any reason to it other than 'it only goes one way'?
","['neural-networks', 'deep-learning', 'math', 'function-approximation']",
What is $ \nabla_{\theta_{k-1}} \theta_{k}$ in the context of MAML?,"
I am attempting to fully understand the explicit derivation and computation of the Hessian and how it is used in MAML.  I came across this blog: https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html.
Specifically, could someone help to clarify this for me: is this term in the red box literally interpreted as the gradient at $\theta_{k-1}$ multiplied by the $\theta_k$?

","['math', 'notation', 'meta-learning', 'gradient', 'model-agnostic-meta-learning']","$\nabla_{\theta_{k-1}} \theta_k$ is gradient of $\theta_k$ with respect to $\theta_{k-1}$, it follows chain rule as noted in the side comment in the image. $\nabla_{\theta} \mathcal L(\theta_k)$ is also not a Hessian but a gradient vector."
How to extract parameters from a text using AI/NLP,"
lets say I have three texts:

""make a heading that says hello word""
""make a heading of hello world""
""create heading consist of hello world""

How can I fetch those groups of words using AI which is referring to heading i.e hello world in this case. Which AI frameworks or libraries can do that?
in all examples heading is pointing to hello world (which i am referring as group of words). so basically i want those words which will be a part of heading or in other word there is a relationship between them. another example i can give is ""I am watching Breaking bad"" so there is a relationship between watching and breaking bad and i want to extract what are you watching.
What's the best approach? Do I have to train a model for that or there are some other techniques that can get it done?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'natural-language-processing', 'natural-language-understanding']",
Offline/Batch Reinforcement Learning: when to stop training and what agent to select,"
Context:
My team and I are working on a RL problem for a specific application. We have data collected from user interactions (states, actions, rewards, etc.).
It is too costly for us to emulate agents. We decided therefore to concentrate on Offline RL techniques. For this, we are currently using the RL-Coach library by Intel, which offers support for Batch/Offline RL. More specifically, to evaluate policies in offline settings, we train a DDQN-BCQ model and evaluate the learned policies using Offline Policy Estimators (OPEs).
Problem:
In an Online RL setting, the decision of when to stop the training of an agent generally depends on the goal one wants to achieve (as described in this post: https://stats.stackexchange.com/questions/322933/q-learning-when-to-stop-training). If the goal is to train until convergence (of rewards) but no longer, then you could for example stop when the standard deviation of your rewards over the last n steps drops under some threshold. If the goal is to compare the performance of two algorithms, then you should simply compare the two using the same number of training steps.
However, in the Offline RL setting, I believe the conditions to stop training are not so clear. As stated above, no environement is directly available to evaluate our agents and the evaluation of the quality of the learned policy almost solely relies on OPEs, which are not always accurate.
For me, I believe that there are two different options that would make sense. I am unsure if both those options are actually equivalent though.

The first option would be to stop training when the Q-values have converged/reached a plateau (i.e. when the Q-value network loss has converged) -- if they ever do, as we don't really have any guarantee of this happening with artificial neural networks. If the Q-values do reach a plateau, this would mean that our agent has reached some local optimum (or in the best case, the global optimum).
The second option would be to only look at the OPEs reward estimation, and stop when they reach a plateau. However, different OPEs do not necessarily reach a plateau at the same time, as it can be seen in the figure below. In the Batch-RL tutorial of RL-Coach, it seems that they would simply select the agent at the epoch where the different OPEs give the highest policy value estimation, without checking that the loss of the network had converged or not (but this is only a tutorial, so I suppose we can't rely too much on it). 

Questions:

What would be the best criteria for choosing when to stop the training of an agent in an Offline-RL setting?
Also, the performance of an agent often heavily depends on the seed used for training. To evaluate the general performance, I believe you have to run multiple training with different seeds? However, in the end, you still want only a single agent to deploy. Should you simply select the one having the highest OPEs values among all the runs?

P.S. I am not sure if this question should be splitted into two different posts, so please let me know if this is the case and I will edit the post!
","['training', 'q-learning', 'off-policy-methods', 'batch-learning', 'offline-reinforcement-learning']",
"Why does batch norm standardize with sample mean/variance, when it also learns parameters to scale the mean/variance?","
Batch norm is a normalizing layer that is shown to help deep networks learn faster and with higher generalization accuracy. It normalizes the activations of the previous layer to a mean $\beta$ and variance $\gamma^2$ to prevent things like activations from exploding or shifting during the learning process.
More specifically:
$$\hat{x} = \displaystyle \frac{x - \mu_t}{\sqrt{\sigma_t^2 + \epsilon}}\label{1}\tag{1}$$
$$ BatchNorm_{\mu_t, \sigma_t}(x) = \gamma \hat{x} + \beta \label{2}\tag{2}$$
where

$x$ is the layer input of the layer
$\mu_t, \sigma_t$ is the sample mean and standard deviation at time step $t$
$\epsilon$ is a small constant, and
$\gamma$ and $\beta$ are learnable parameters so that the output is not necessarily standardized to mean $0$ and variance $1$, but possibly to another mean and variance that may be better for the neural network.

My question is, why does BatchNorm first standardize the input $x$ to $\hat{x}$ before applying the learnable parameters $\gamma$ and $\beta$? Isn't this redundant? The parameters $\gamma$ and $\beta$ could learn to standardize the input themselves right?
In fact, as training progresses, $\mu_t$ and $\sigma_t$ becomes updated to new values $\mu_{t+1}$ and $\sigma_{t+1}$, so the learned parameters at that time step, $\gamma_t$ and $\beta_t$, no longer apply for time step $t+1$ since that involves a different standardization process with a different mean and variance. So by adding this standardization step, it may even hurt the convergence of the layer during learning, since it is adding the gradient of $BatchNorm_{\mu_{t+1}, \sigma_{t+1}}(x)$ to $BatchNorm_{\mu_t, \sigma_t}(x)$, which are two different functions right?
Why not just simply make it like this?
$$BatchNorm(x) = \gamma x + \beta \label{3}\tag{3}$$
This would simplify the calculation of the gradients, which would make learning faster to compute.
BatchNorm is one of the most successful developments of deep learning, so I know my intuition on these things is wrong -- I'm just curious as to what I am missing.
","['neural-networks', 'deep-learning', 'batch-normalization', 'normalisation', 'standardisation']",
What are the advantages of RL with actor-critic methods over actor-only methods?,"
In general, what are the advantages of RL with actor-critic methods over actor-only (or policy-based) methods?
This is not a comparison with the Q-learning series, but probably a method of learning the game with only the actor.
I think it's effective to use only actors, especially for sparse rewards. Is that correct?
Please, let me know if you have any specific use cases that use only actors.
","['reinforcement-learning', 'comparison', 'actor-critic-methods', 'policy-based-methods', 'continuous-tasks']",
"Given a 2-layer GCN, can we choose the dimensions of the 2nd weight matrix, such that this architecture has the same capacity as a 1-layer GCN?","
This might be more of a question about nested function classes:
For $k$ class node classification in a graph with $n$ nodes, and $d$ feature vector.
I want to compare
Architecture I: the GCN model of Kipf/ Welling with two graph convolutional layers:
$$
\mathbf{Y}=\operatorname{softmax}\left(\mathbf{A} \xi\left(\mathbf{A X W}_{1}\right) \mathbf{W}_{2}\right)
$$
where

$\mathbf{X}$ is $n \times d,
- $ $\mathbf{Y}$ is $n \times k$
$\mathbf{A}$ is a fixed $n \times n$ graph diffusion matrix,
$\mathbf{W}_{1}, \mathbf{W}_{2}$ are learnable weight matrices of size $d \times d^{\prime}$ and $d^{\prime} \times 2,$ respectively, shared across all nodes, and
$\xi$ is a nonlinearity.

Architecture II: a single-layer graph neural network of the form:
$$
\mathbf{Y}=\operatorname{softmax}\left(\mathbf{A}^{2} \mathbf{X W}\right)
$$
where $\mathbf{W}$ is a learnable weight matrix of size $d \times 2$.

Now I'm wondering

$\operatorname{Can} \xi, d^{\prime}$ be chosen in a way that both architectures have the same expressive power? (i.e. can represent the same class of functions)?

$\operatorname{Can} \xi, d^{\prime}$ be chosen in a way that Architecture II is more expressive?

What would be the advantage in training complexity of Architecture II when applied to large-scale graphs.


","['activation-functions', 'geometric-deep-learning', 'hyper-parameters', 'graph-neural-networks', 'representation-learning']",
"How can an ""architectural motif"" be extracted from a trained MLP?","
I am trying to reproduce the paper Synthetic Petri Dish: A novel surrogate model for Rapid Architecture Search. In the paper, the authors try to reduce the architecture of an MLP model trained on MNIST (2 layers - 100 neurons) by initializing a motif network from it, that is, 2 layers, 1 neuron each, and extracting the sigmoid function. I have been searching a lot, but I have not found the answer of how can someone extract an 'architectural motif' from a trained neural network.
","['neural-networks', 'papers', 'neural-architecture-search']",
variational auto encoder loss goes down but does not reconstruct input. out of debugging ideas,"
My variational autoencoder seems to work for MNIST, but fails on slightly ""harder"" data.
By ""fails"" I mean there are at least two apparent problems:

Very poor reconstruction, for example sample reconstructions from the last epoch on validation set



without any regularization at all.
The last reported losses from console are val_loss=9.57e-5, train_loss=9.83e-5 which I thought would imply exact reconstructions.
validation loss is low (which does not seem to reflect the reconstruction), and always lower than training loss which is very suspicious.



For MNIST everything looks fine (with less layers!).

I will give as much nformation as I can, since I am not sure what I should provide to help anyone help me.

Firstly, here is the full code
You will notice loss calculation and logging is very simple and straight forward and I can't seem to find what's wrong.
import torch
from torch import nn
import torch.nn.functional as F
from typing import List, Optional, Any
from pytorch_lightning.core.lightning import LightningModule
from Testing.Research.config.ConfigProvider import ConfigProvider
from pytorch_lightning import Trainer, seed_everything
from torch import optim
import os
from pytorch_lightning.loggers import TensorBoardLogger
# import tfmpl
import matplotlib.pyplot as plt
import matplotlib
from Testing.Research.data_modules.MyDataModule import MyDataModule
from Testing.Research.data_modules.MNISTDataModule import MNISTDataModule
from Testing.Research.data_modules.CaseDataModule import CaseDataModule
import torchvision
from Testing.Research.config.paths import tb_logs_folder
from Testing.Research.config.paths import vae_checkpoints_path
from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint


class VAEFC(LightningModule):
    # see https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
    # for possible upgrades, see https://arxiv.org/pdf/1602.02282.pdf
    # https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational
    # -auto-encoder
    def __init__(self, encoder_layer_sizes: List, decoder_layer_sizes: List, config):
        super(VAEFC, self).__init__()
        self._config = config
        self.logger: Optional[TensorBoardLogger] = None
        self.save_hyperparameters()

        assert len(encoder_layer_sizes) >= 3, ""must have at least 3 layers (2 hidden)""
        # encoder layers
        self._encoder_layers = nn.ModuleList()
        for i in range(1, len(encoder_layer_sizes) - 1):
            enc_layer = nn.Linear(encoder_layer_sizes[i - 1], encoder_layer_sizes[i])
            self._encoder_layers.append(enc_layer)

        # predict mean and covariance vectors
        self._mean_layer = nn.Linear(encoder_layer_sizes[
                                         len(encoder_layer_sizes) - 2],
                                     encoder_layer_sizes[len(encoder_layer_sizes) - 1])
        self._logvar_layer = nn.Linear(encoder_layer_sizes[
                                           len(encoder_layer_sizes) - 2],
                                       encoder_layer_sizes[len(encoder_layer_sizes) - 1])

        # decoder layers
        self._decoder_layers = nn.ModuleList()
        for i in range(1, len(decoder_layer_sizes)):
            dec_layer = nn.Linear(decoder_layer_sizes[i - 1], decoder_layer_sizes[i])
            self._decoder_layers.append(dec_layer)

        self._recon_function = nn.MSELoss(reduction='mean')
        self._last_val_batch = {}

    def _encode(self, x):
        for i in range(len(self._encoder_layers)):
            layer = self._encoder_layers[i]
            x = F.relu(layer(x))

        mean_output = self._mean_layer(x)
        logvar_output = self._logvar_layer(x)
        return mean_output, logvar_output

    def _reparametrize(self, mu, logvar):
        if not self.training:
            return mu
        std = logvar.mul(0.5).exp_()
        if std.is_cuda:
            eps = torch.FloatTensor(std.size()).cuda().normal_()
        else:
            eps = torch.FloatTensor(std.size()).normal_()
        reparameterized = eps.mul(std).add_(mu)
        return reparameterized

    def _decode(self, z):
        for i in range(len(self._decoder_layers) - 1):
            layer = self._decoder_layers[i]
            z = F.relu((layer(z)))

        decoded = self._decoder_layers[len(self._decoder_layers) - 1](z)
        # decoded = F.sigmoid(self._decoder_layers[len(self._decoder_layers)-1](z))
        return decoded

    def _loss_function(self, recon_x, x, mu, logvar, reconstruction_function):
        """"""
        recon_x: generating images
        x: origin images
        mu: latent mean
        logvar: latent log variance
        """"""
        binary_cross_entropy = reconstruction_function(recon_x, x)  # mse loss TODO see if mse or cross entropy
        # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
        kld_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)
        kld = torch.sum(kld_element).mul_(-0.5)
        # KL divergence Kullback–Leibler divergence, regularization term for VAE
        # It is a measure of how different two probability distributions are different from each other.
        # We are trying to force the distributions closer while keeping the reconstruction loss low.
        # see https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73

        # read on weighting the regularization term here:
        # https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational
        # -auto-encoder
        return binary_cross_entropy + kld * self._config.regularization_factor

    def _parse_batch_by_dataset(self, batch, batch_index):
        if self._config.dataset == ""toy"":
            (orig_batch, noisy_batch), label_batch = batch
            # TODO put in the noise here and not in the dataset?
        elif self._config.dataset == ""mnist"":
            orig_batch, label_batch = batch
            orig_batch = orig_batch.reshape(-1, 28 * 28)
            noisy_batch = orig_batch
        elif self._config.dataset == ""case"":
            orig_batch, label_batch = batch

            orig_batch = orig_batch.float().reshape(
                    -1,
                    len(self._config.case.feature_list) * self._config.case.frames_per_pd_sample
            )
            noisy_batch = orig_batch
        else:
            raise ValueError(""invalid dataset"")
        noisy_batch = noisy_batch.view(noisy_batch.size(0), -1)

        return orig_batch, noisy_batch, label_batch

    def training_step(self, batch, batch_idx):
        orig_batch, noisy_batch, label_batch = self._parse_batch_by_dataset(batch, batch_idx)

        recon_batch, mu, logvar = self.forward(noisy_batch)

        loss = self._loss_function(
                recon_batch,
                orig_batch, mu, logvar,
                reconstruction_function=self._recon_function
        )
        # self.logger.experiment.add_scalars(""losses"", {""train_loss"": loss})
        tb = self.logger.experiment
        tb.add_scalars(""losses"", {""train_loss"": loss}, global_step=self.current_epoch)
        # self.logger.experiment.add_scalar(""train_loss"", loss, self.current_epoch)
        if batch_idx == len(self.train_dataloader()) - 2:
            # https://pytorch.org/docs/stable/_modules/torch/utils/tensorboard/writer.html#SummaryWriter.add_embedding
            # noisy_batch = noisy_batch.detach()
            # recon_batch = recon_batch.detach()
            # last_batch_plt = matplotlib.figure.Figure()  # read https://github.com/wookayin/tensorflow-plot
            # ax = last_batch_plt.add_subplot(1, 1, 1)
            # ax.scatter(orig_batch[:, 0], orig_batch[:, 1], label=""original"")
            # ax.scatter(noisy_batch[:, 0], noisy_batch[:, 1], label=""noisy"")
            # ax.scatter(recon_batch[:, 0], recon_batch[:, 1], label=""reconstructed"")
            # ax.legend(loc=""upper left"")
            # self.logger.experiment.add_figure(f""original last batch, epoch {self.current_epoch}"", last_batch_plt)
            # tb.add_embedding(orig_batch, global_step=self.current_epoch, metadata=label_batch)
            pass
        self.logger.experiment.flush()
        self.log(""train_loss"", loss, prog_bar=True, on_step=False, on_epoch=True)
        return loss

    def _plot_batches(self, orig_batch, noisy_batch, label_batch, batch_idx, recon_batch, mu, logvar):
        # orig_batch_view = orig_batch.reshape(-1, self._config.case.frames_per_pd_sample,
        # len(self._config.case.feature_list))
        #
        # plt.figure()
        # plt.plot(orig_batch_view[11, :, 0].detach().cpu().numpy(), label=""feature 0"")
        # plt.legend(loc=""upper left"")
        # plt.show()

        tb = self.logger.experiment
        if self._config.dataset == ""mnist"":
            orig_batch -= orig_batch.min()
            orig_batch /= orig_batch.max()
            recon_batch -= recon_batch.min()
            recon_batch /= recon_batch.max()

            orig_grid = torchvision.utils.make_grid(orig_batch.view(-1, 1, 28, 28))
            val_recon_grid = torchvision.utils.make_grid(recon_batch.view(-1, 1, 28, 28))

            tb.add_image(""original_val"", orig_grid, global_step=self.current_epoch)
            tb.add_image(""reconstruction_val"", val_recon_grid, global_step=self.current_epoch)

            label_img = orig_batch.view(-1, 1, 28, 28)
            pass
        elif self._config.dataset == ""case"":
            orig_batch_view = orig_batch.reshape(-1, self._config.case.frames_per_pd_sample,
                                                 len(self._config.case.feature_list)).transpose(1, 2)
            recon_batch_view = recon_batch.reshape(-1, self._config.case.frames_per_pd_sample,
                                                   len(self._config.case.feature_list)).transpose(1, 2)

            # plt.figure()
            # plt.plot(orig_batch_view[11, 0, :].detach().cpu().numpy())
            # plt.show()
            # pass

            n_samples = orig_batch_view.shape[0]
            n_plots = min(n_samples, 4)
            first_sample_idx = 0

            # TODO either plotting or data problem
            fig, axs = plt.subplots(n_plots, 1)
            for sample_idx in range(n_plots):
                for feature_idx, (orig_feature, recon_feature) in enumerate(
                        zip(orig_batch_view[sample_idx + first_sample_idx, :, :],
                            recon_batch_view[sample_idx + first_sample_idx, :, :])):
                    i = feature_idx
                    if i > 0: continue  # or scale issues don't allow informative plotting

                    # plt.figure()
                    # plt.plot(orig_feature.detach().cpu().numpy(), label=f'orig{i}, sample{sample_idx}')
                    # plt.legend(loc='upper left')
                    # pass

                    axs[sample_idx].plot(orig_feature.detach().cpu().numpy(), label=f'orig{i}, sample{sample_idx}')
                    axs[sample_idx].plot(recon_feature.detach().cpu().numpy(), label=f'recon{i}, sample{sample_idx}')
                    # sample{sample_idx}')
                    axs[sample_idx].legend(loc='upper left')
                pass
            # plt.show()

            tb.add_figure(""recon_vs_orig"", fig, global_step=self.current_epoch, close=True)

    def validation_step(self, batch, batch_idx):
        orig_batch, noisy_batch, label_batch = self._parse_batch_by_dataset(batch, batch_idx)

        recon_batch, mu, logvar = self.forward(noisy_batch)

        loss = self._loss_function(
                recon_batch,
                orig_batch, mu, logvar,
                reconstruction_function=self._recon_function
        )

        tb = self.logger.experiment
        # can probably speed up training by waiting for epoch end for data copy from gpu
        # see https://sagivtech.com/2017/09/19/optimizing-pytorch-training-code/
        tb.add_scalars(""losses"", {""val_loss"": loss}, global_step=self.current_epoch)

        label_img = None
        if len(orig_batch) > 2:
            self._last_val_batch = {
                ""orig_batch"": orig_batch,
                ""noisy_batch"": noisy_batch,
                ""label_batch"": label_batch,
                ""batch_idx"": batch_idx,
                ""recon_batch"": recon_batch,
                ""mu"": mu,
                ""logvar"": logvar
            }
        # self._plot_batches(orig_batch, noisy_batch, label_batch, batch_idx, recon_batch, mu, logvar)

        outputs = {""val_loss"":  loss, ""recon_batch"": recon_batch, ""label_batch"": label_batch,
                   ""label_img"": label_img}
        self.log(""val_loss"", loss, prog_bar=True, on_step=False, on_epoch=True)
        return outputs

    def validation_epoch_end(self, outputs: List[Any]) -> None:
        first_batch_dict = outputs[-1]

        self._plot_batches(
                self._last_val_batch[""orig_batch""],
                self._last_val_batch[""noisy_batch""],
                self._last_val_batch[""label_batch""],
                self._last_val_batch[""batch_idx""],
                self._last_val_batch[""recon_batch""],
                self._last_val_batch[""mu""],
                self._last_val_batch[""logvar""]
        )
        self.log(name=""VAEFC_val_loss_epoch_end"", value={""val_loss"": first_batch_dict[""val_loss""]})

    def test_step(self, batch, batch_idx):
        orig_batch, noisy_batch, label_batch = self._parse_batch_by_dataset(batch, batch_idx)

        recon_batch, mu, logvar = self.forward(noisy_batch)

        loss = self._loss_function(
                recon_batch,
                orig_batch, mu, logvar,
                reconstruction_function=self._recon_function
        )

        tb = self.logger.experiment
        tb.add_scalars(""losses"", {""test_loss"": loss}, global_step=self.global_step)

        return {""test_loss"": loss, ""mus"": mu, ""labels"": label_batch, ""images"": orig_batch}

    def test_epoch_end(self, outputs: List):
        tb = self.logger.experiment

        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
        self.log(name=""test_epoch_end"", value={""test_loss_avg"": avg_loss})

        if self._config.dataset == ""mnist"":
            tb.add_embedding(
                    mat=torch.cat([o[""mus""] for o in outputs]),
                    metadata=torch.cat([o[""labels""] for o in outputs]).detach().cpu().numpy(),
                    label_img=torch.cat([o[""images""] for o in outputs]).view(-1, 1, 28, 28),
                    global_step=self.global_step,
            )

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self._config.learning_rate)
        return optimizer

    def forward(self, x):
        mu, logvar = self._encode(x)
        z = self._reparametrize(mu, logvar)
        decoded = self._decode(z)
        return decoded, mu, logvar


def train_vae(config, datamodule, latent_dim, dec_layer_sizes, enc_layer_sizes):
    model = VAEFC(config=config, encoder_layer_sizes=enc_layer_sizes, decoder_layer_sizes=dec_layer_sizes)

    logger = TensorBoardLogger(save_dir=tb_logs_folder, name='VAEFC', default_hp_metric=False)
    logger.hparams = config

    checkpoint_callback = ModelCheckpoint(dirpath=vae_checkpoints_path)
    trainer = Trainer(deterministic=config.is_deterministic,
                      # auto_lr_find=config.auto_lr_find,
                      # log_gpu_memory='all',
                      # min_epochs=99999,
                      max_epochs=config.num_epochs,
                      default_root_dir=vae_checkpoints_path,
                      logger=logger,
                      callbacks=[checkpoint_callback],
                      gpus=1
                      )
    # trainer.tune(model)
    trainer.fit(model, datamodule=datamodule)
    best_model_path = checkpoint_callback.best_model_path
    print(""done training vae with lightning"")
    print(f""best model path = {best_model_path}"")
    return trainer


def run_trained_vae(trainer):
    # https://pytorch-lightning.readthedocs.io/en/latest/test_set.html
    # (1) load the best checkpoint automatically (lightning tracks this for you)
    trainer.test()

    # (2) don't load a checkpoint, instead use the model with the latest weights
    # trainer.test(ckpt_path=None)

    # (3) test using a specific checkpoint
    # trainer.test(ckpt_path='/path/to/my_checkpoint.ckpt')

    # (4) test with an explicit model (will use this model and not load a checkpoint)
    # trainer.test(model)



Parameters
I am getting very similar results for any combination of parameters I am (manually) using. Maybe I didn't try something.
num_epochs: 40
batch_size: 32
learning_rate: 0.0001
auto_lr_find: False

noise_factor: 0.1
regularization_factor: 0.0

train_size: 0.8
val_size: 0.1
num_workers: 1

dataset: ""case"" # toy, mnnist, case
mnist:
  enc_layer_sizes: [784, 512,]
  dec_layer_sizes: [512, 784]
  latent_dim: 25
  n_classes: 10
  classifier_layers: [20, 10]
toy:
  enc_layer_sizes: [2, 200, 200, 200]
  dec_layer_sizes: [200, 200, 200, 2]
  latent_dim: 8
  centers_radius: 4.0
  n_clusters: 10
  cluster_size: 5000
case:
  #enc_layer_sizes: [ 1800, 600, 300, 100 ]
  #dec_layer_sizes: [ 100, 300, 600, 1800 ]
  #frames_per_pd_sample: 600

  enc_layer_sizes: [ 10, 600, 300, 300 ]
  dec_layer_sizes: [ 600, 300, 300, 10 ]
  frames_per_pd_sample: 10

  latent_dim: 300
  n_classes: 10
  classifier_layers: [ 20, 10 ] # unused right now.

  feature_list:
    #- V_0_0 # 0, X
    #- V_0_1 # 0, Y
    #- V_0_2 # 0, Z
    - pads_0
  enc_kernel_sizes: [] # for conv
  end_strides: []
  dec_kernel_sizes: []
  dec_strides: []

is_deterministic: False

real_data_pd_dir: ""D:/pressure_pd""
case_dir: ""real_case_20_min""
case_file: ""pressure_data_0.pkl""



Data
For Mnist everything works fine.
When changing to my specific data, results are as above.
The data is a time series, of several features. To dumb this down even more, I am feeding just a single feature, sliced to equal-length chunks, and fed into the input layer as a vector.
The fact that the data is a time series could maybe help modeling in the future, but for now I want to just refer to it as chunks of data, which I believe I am doing.
code:
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
import torch
from Testing.Research.config.ConfigProvider import ConfigProvider
import os
import pickle
import pandas as pd
from typing import Tuple
import numpy as np


class CaseDataset(Dataset):
    def __init__(self, path):
        super(CaseDataset, self).__init__()
        self._path = path

        self._config = ConfigProvider.get_config()
        self.frames_per_pd_sample = self._config.case.frames_per_pd_sample
        self._load_case_from_pkl()
        self.__len = len(self._full) // self.frames_per_pd_sample  # discard last non full batch

    def _load_case_from_pkl(self):
        assert os.path.isfile(self._path)
        with open(self._path, ""rb"") as f:
            p = pickle.load(f)

        self._full: pd.DataFrame = p[""full""]
        self._subsampled: pd.DataFrame = p[""subsampled""]
        self._misc: pd.DataFrame = p[""misc""]

        feature_list = self._config.case.feature_list
        self._features_df = self._full[feature_list].copy()

        # normalize from -1 to 1
        features_to_normalize = self._features_df.columns
        self._features_df[features_to_normalize] = \
            self._features_df[features_to_normalize].apply(lambda x: (((x - x.min()) / (x.max() - x.min())) * 2) - 1)

        pass

    def __len__(self):
        # number of samples in the dataset
        return self.__len

    def __getitem__(self, index: int) -> Tuple[np.array, np.array]:
        data_item = self._features_df.iloc[index * self.frames_per_pd_sample: (index + 1) * self.frames_per_pd_sample, :].values
        label = 0.0
        # plt.figure()
        # plt.plot(data_item[:, 0], label=""feature 0"")
        # plt.legend(loc=""upper left"")
        # plt.show()
        return data_item, label


The amount of time-steps per batch does not seem to affect convergence.
Train test val split
is done like so:
import os
from pytorch_lightning import LightningDataModule
import torchvision.datasets as datasets
from torchvision.transforms import transforms
import torch
from torch.utils.data import DataLoader
from torch.utils.data import Subset
from Testing.Research.config.paths import mnist_data_download_folder
from Testing.Research.datasets.real_cases.CaseDataset import CaseDataset
from typing import Optional


class CaseDataModule(LightningDataModule):
    def __init__(self, config, path):
        super().__init__()
        self._config = config
        self._path = path

        self._train_dataset: Optional[Subset] = None
        self._val_dataset: Optional[Subset] = None
        self._test_dataset: Optional[Subset] = None

    def prepare_data(self):
        pass

    def setup(self, stage):
        # transform
        transform = transforms.Compose([transforms.ToTensor()])
        full_dataset = CaseDataset(self._path)

        train_size = int(self._config.train_size * len(full_dataset))
        val_size = int(self._config.val_size * len(full_dataset))
        test_size = len(full_dataset) - train_size - val_size
        train, val, test = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])

        # assign to use in dataloaders
        self._full_dataset = full_dataset
        self._train_dataset = train
        self._val_dataset = val
        self._test_dataset = test

    def train_dataloader(self):
        return DataLoader(self._train_dataset, batch_size=self._config.batch_size, num_workers=self._config.num_workers)

    def val_dataloader(self):
        return DataLoader(self._val_dataset, batch_size=self._config.batch_size, num_workers=self._config.num_workers)

    def test_dataloader(self):
        return DataLoader(self._test_dataset, batch_size=self._config.batch_size, num_workers=self._config.num_workers)


Questions

I believe having the validation loss consistently lower than the train loss shows something is very wrong here, but I can't put my finger on what, or come up with how to verify this.
How can I just make the model auto-encode the data correctly? Basicaly, I would want it to learn the identity function, and for the loss to reflect that.
The loss does not seem to reflect the reconstruction. I think this is probably the most fundamental issue


My thoughts

Try a convolutional net instead of FC? maybe it would be able to better learn features?
Out of ideas :(


Will provide any lacking information.
","['machine-learning', 'deep-learning', 'autoencoders', 'pytorch', 'variational-autoencoder']",
What place do Agent Communications Language have in Multi-Agent Systems nowadays?,"
I am currently working on implementing a Multi-Agent System for Smart Grids.
There's a lot of literature for that and some things confuse me. I have read that there is FIPA, which aimed to create a unified Agent Communication Language. So multiple Agents are talking to each other and FIPA specifies how the messages should be sent and processed. However, it is pretty old.
In newer papers, where Multi-Agent Reinforcement Learning Algorithms are proposed, FIPA or generally any ACL isn't mentioned. I believe that is because in MARL, communication is done by observing the states of the other agents, rather than communicating explicitly. Also in MARL, the decision making is not based on negotiation like in FIPA but with the learned policy.
I am now super confused if I got it right.
Is FIPA still a thing I should worry about when I design my Multi-Agent System?
Is there any other thing to handle communication in MARL other than sharing states?
Any help would be really appreciated, thank you very much :)
","['reinforcement-learning', 'intelligent-agent', 'multi-agent-systems']",
What is the definition of pre-training?,"
I want to pre-train a model (combined by two popular modules A and B, and both are large blocks), then fine-tune it on downstream tasks.
What if for the weight initialization for pre-training, module A is initialized from some checkpoints, while B's is from random? Can I still claim the process as pre-training? Or the modules in the model must all be initialized from random, and so we can call it pre-training?
If parts of the module's weights are 'contaminated' by checkpoints, it is only can be called fine-tune?
","['deep-learning', 'definitions', 'transfer-learning', 'fine-tuning']",
What is the smoothness assumption in SVMs?,"
In this research paper, we have the following claim

the smoothness assumption that underlies many kernel methods such as Support Vector Machines (SVMs) does not hold for deep neural networks trained through backpropagation

Does smoothness here refer to no sharp rise/fall in gradients?
","['machine-learning', 'terminology', 'papers', 'support-vector-machine']","Smoothness here is the mathematical definition, so as you implied smoothness is ruled out by output data with sharp spikes or discontinuous jumps (and possibly the data of the gradient, the gradient's gradient, ad infinitum, depending on who defines smoothness).By any definition a lot of activation functions are not smooth, for example RELU. This means neural networks in general are not smooth."
"Is a team of ML scientists an ""intelligent agent""?","
I am writing about the role of machine learning scientists in developing a solution. Is there a term for the humans who do learning?
Can we call a ""team of machine learning scientists with their computers working on some ML problem""   an intelligent agent? Is ""cognizer"" the right term? I know that ""learner"" is reserved for an ML algorithm. I just want a shorter term for their role in cognition, learning.
","['machine-learning', 'terminology', 'intelligent-agent']",
How can I use Monte Carlo Dropout in a pre-trained CNN model?,"
In Monte Carlo Dropout (MCD), I know that I should enable dropout during training and testing, then get multiple predictions for the same input $x$ by performing multiple forward passes with $x$, then, for example, average these predictions.
Let's suppose I want to fine-tune a pre-trained model and get MCD uncertainty estimations, how should I add dropout layers?

on the fully-connected layers;
after every convolutional layer.

I've read some papers and implementations where one applies dropout at fully-connected layers only, using a pre-trained model. However, when using a custom model, usually one adds dropout after every convolutional layer. This work builds two configurations:

dropout on the fully-connected layers;
dropout after resnet blocks.

The first configuration performs better, but I'm unsure if this is an actual uncertainty estimation from resnet. The results show that there is a correlation between high uncertainty predictions and incorrect predictions. So, would this be a good way of estimating uncertainty? My shot is ""yes"", because even though there are no nodes being sampled from the backbone, the sampling in the fully-connected layer forces a smooth variation in the backbone, generating a low-variance ensemble. But, I'm quite a beginner on MCD so, any help would be appreciated.
","['deep-learning', 'monte-carlo-methods', 'dropout', 'uncertainty-quantification', 'mc-dropout']",
"In few-shot classification, should I use my custom dataset as the validation dataset and mini-ImageNet as the training dataset?","
I am new to few-shot learning, and I wanted to get a hands-on understanding of it, using Reptile algorithm, applied to my custom dataset.
My custom dataset has 30 categories, with 5 images per category, so this would be a 30 way 5 shot.
Given a new image, I wish to be able to classify it into one of 30 categories. I changed train_shots = 5, classes = 30 in the linked example, and got the training output as
batch 0: train=0.050000 test=0.050000
batch 1: train=0.050000 test=0.050000

Should the custom dataset be used as a validation set, with mini-ImageNet as a training dataset, so that the knowledge is transferred? Or can I use only a custom dataset with only $30*5=150$ images for training?
","['classification', 'datasets', 'meta-learning', 'few-shot-learning', 'reptile-algorithm']",
VAE giving near zero output when latent space dimension is large,"
I'm training a VAE to reconstruct some input (channels picked up by some MIMO BS for context) and I ran an experiment on the training set to see how the performance improves with the latent space dimension.
My VAE structure is as follows : Input : 2048 -> 1024 -> 512 -> Latent space dimension -> 512 -> 1024 -> Output : 2048
Here is what I get in terms of relative error when the latent space dimension goes from 2 to 100 :

Everything works as expected at the beginning, but the error starts rising up at around 50 and I have no idea why. With a large latent space dimension, the output is orders of magnitude smaller than the input, which explains the relative error of value 1.
Here is the same figure when I run the exact same experiment but with a normal autoencoder this time.
This time the results are consistent.
What's wrong with my VAE ?
","['autoencoders', 'variational-autoencoder']",
How and why do state-of-the-art models in medical segmentation differ from general segmentation models?,"
I am just getting into medical image segmentation and have been able to understand the state-of-the-art architectures, like Double UNet, UNet++, and Multiresunet.
What I haven't understood yet: Why are these approaches better for medical segmentation than, for example, HRNet-OCR, which currently tops the rankings of the Cityscapes dataset, and vice versa?
","['comparison', 'models', 'image-segmentation', 'u-net', 'state-of-the-art']",
"Is it really possible to create the ""Perfect Cylinder"" used in Universal Approximation Theorem for 1-hidden layer Neural Network?","
There are proofs for the universal approximation theorem with just 1 hidden layer.
The proof goes like this:

Create a ""bump"" function using 2 neurons.

Create (infinitely) many of these step functions with different angles in order to create a tower-like shape.

Decrease the step/radius to a very small value in order to approximate a cylinder. This is what I'm not convinced of

Using these cylinders one can approximate any shape. (At this point it's basically just a packing problem like this.


In this video, minute 42, the lecturer says

In the limit that's going to be a perfect cylinder. If the cylinder is small enough. It's gonna be a perfect cylinder. Right ? I have control over the radius.

Here are the slides.

Here is a pdf version from another university, so you do not have to watch the video.
Why am I not convinced?
I created a program to plot this, and even if I decrease the radius by orders of magnitude it still has the same shape.
Let's start with a simple tower of radius 0.1:

Now let's decrease the radius to 0.01:

Now, you might think that it gets close to a cylinder, but it just looks like it is approximating a perfect cylinder, because of the zoomed out effect.
Let's zoom in:

Let's decrease the radius to 0.0000001.

Still not a perfect cylinder. In fact, the ""quality"" of the cylinder is the same.
Python code to reproduce (requires NumPy and matplotlib): https://pastebin.com/CMXFXvNj.
So my questions are:
Q1 Is it true that we can get a perfect cylinder solely by decreasing the radius of the tower to 0 ?
Q2 If this true, why is there no difference when I plot it with different radii(0.1, vs 1e-7) ?
Both towers have the same shape
Clarification: What do I mean with: same shape ? Let's say we calculate the volume of an actual cylinder(Vc) with the same raius and height as our tower and divide it by the volume of the tower(Vt) .
Vc = Volume Cylinder
Vt = Volume Tower
ratio(r) = Vc/Vt
What this documents/lectures claim that is the ratio of these 2 volumes depends on the radius but in my view it's just constant.
So what they are saying is that: lim r -> 0 for ratio(r) = 1
But my experiments show that: lim r -> 0 for ratio(r) = const and don't depend on the radius at all.
Q3 Preface
An objection i got multiple times once from Dutta and once from D.W is that just decreasing the radious and plotting it isn't mathematical rigorous.
So let's assume in the limit of r=0 it's really a perfect cylinder.
One possible explanation for this would be that the limit is a special case and one can't approximate towards it
But if that is true this would imply that there is no use for it since it's impossible to have a radius of exactly zero. It would only be useful if we could get gradually closer to a perfect cylinder by decreasing the radius.
Q3 So why should we even care about this then ?
Further Clarifications
The original universal approximation theorem proof for single hidden layer neural networks was done by G. Cybenko. Then I think people tried to make some visual explations for it. I am NOT questioning the paper ! But i am questioning the visual explanation given in the linked lecutre/pdf (made by other people)
","['neural-networks', 'feedforward-neural-networks', 'universal-approximation-theorems']",
"In the Binary Flower Pollination Algorithm (using the sigmoid function), is it possible that no feature is selected?","
I'm trying to use the Binary Flower Pollination Algorithm (BFPA) for feature selection. In the BFPA, the sigmoid function is used to compute a binary vector that represents whether a feature is selected or not. Here are the relevant equations from the paper (page 4).
$$
S\left(x_{i}^{j}(t)\right)=\frac{1}{1+e^{-x_{i}^{j}(t)}} \tag{4}\label{4}
$$
\begin{equation}
x_{i}^{j}(t)=\left\{\begin{array}{ll}
1 & \text { if } S\left(x_{i}^{j}(t)\right)>\sigma \\
0 & \text { otherwise }
\end{array}\right.
\tag{5}\label{5}
\end{equation}
In my case, I noticed that my algorithm sometimes returns a zero vector (i.e. all elements are zeros, such as $[0,0,0,0,0,0,0,0,0]$), which means that no feature is selected (a feature would be selected when it is $1$), which makes the fitness function returns an error.
Is it correct that sigmoid returns result like that?
","['machine-learning', 'feature-selection', 'swarm-intelligence', 'sigmoid', 'binary-flower-pollination-algorithm']",
Is my reward function non-Markovian?,"
I am working on an RL problem where the time when the agent obtains the reward for taking action $a$ in time step $t$ is stochastic. In fact, there is no immediate reward for taking action $a$ in time step $t$, and, for example, the agent may obtain the reward in time step $t+k$ (where $k>1$). I was wondering if this kind of reward function is categorized as a non-Markovian reward function, and which RL method works better (to approximate/find the optimum policy) in this environment?
PS: It differs from the sparse reward problems. In my problem, there is a non-zero reward associated with every action taken. However, the agent does not receive any reward immediately. In fact, once the agent takes an action, like $a$, at a time step like $t$, the agent does not have any control over when he/she will receive the reward associated with that action. The time when the reward is received is stochastic.
","['reinforcement-learning', 'terminology', 'reference-request', 'reward-functions', 'markov-property']",
Multivariate time-series classification with many variables,"
I am attempting to use time-series classification algorithms for fraud detection applications. I have came across several works in the literature that propose novel techniques for multivariate time-series classification, however, most of these approaches treat each feature as an individual signal.
Now, my processing of my data transforms a transactions dataset into a tensor; 1 dimension where each observation is an account, 1 dimension where each element is a transaction and 1 dimension for the transaction attributes. The transactions dataset has a large number of features, many of which are one-hot encoded categorical variables. Therefore, I am not really sure that multivariate time-series classification algorithms such as CNN or LSTM will work in this case, since it will treat every one-hot encoded feature as a signal on its own.
What would be an alternative approach in this case? Would applying PCA on the data to capture the most significant features help instead of the ordinary features?
","['neural-networks', 'classification', 'tensorflow', 'time-series']",
Why does regular Q-learning (and DQN) overestimate the Q values?,"
The motivation for the introduction of double DQN (and double Q-learning) is that the regular Q-learning (or DQN) can overestimate the Q value, but is there a brief explanation as to why it is overestimated?
","['reinforcement-learning', 'q-learning', 'dqn', 'double-dqn', 'double-q-learning']",
"How can I find words in a string that are related to a given word, then associate a sentiment to that found word?","
I came up with an NLP-related problem where I have a list of words and a string. My goal is to find any word in the list of words that is related to the given string.
Here is an example.
Suppose a word from the list is healthy. If the string has any of the following words: healthy, healthier, healthiest, not healthy, more healthy, zero healthy, etc., it will be extracted from the string.
Also, I want to judge whether the extracted word/s is/are bearing positive/negative sentiment.
Let me further explain what I mean by using the previous example.
Our word was healthy. So, for instance, if the word found in the string was healthier, then we can say it is bearing positive sentiment with respect to the word healthy. If we find the word not healthy, it is negative with respect to the word healthy.
","['natural-language-processing', 'sentiment-analysis', 'stemming', 'lemmatization']","There are many ways to solve this problem.  One way is to apply stemming or lemmatization to reduce your words. Using NLTK's Porter stemmer for example on healthy, healthier, healthiest, not healthy, more healthy, and zero healthy gives:This can help make word comparisons easier.Sentiment analysis on the phrases will provide positive, neutral, and negative scores.  There are a lot of algorithms for doing this but a common one is Valence Aware Dictionary and sEntiment Reasoner (VADER).  Here is a recent article with code using NLTK and the VADER lexicon:The following article also does sentiment analysis using NLTK and includes stemming and lemmatization.  Instead of VADER they use a Naive Bayes classifier on a labeled data set of tweets: How To Perform Sentiment Analysis in Python 3 Using the Natural Language Toolkit (NLTK) by Saumik Daityari."
Can DQN outperform DoubleDQN?,"
I found a similar post about this issue, but unfortunately I did not find a proper answer. Are there any references where DQN is better than DoubleDQN, that is DoubleDQN does not improve DQN ?
","['reinforcement-learning', 'dqn', 'deep-rl', 'reference-request', 'double-dqn']",
What algorithm would you advise me to use for my task?,"
I have an image and a mask. I want the image to be the same, but rotated, scaled and positioned like mask. What can I use?
","['image-processing', 'image-generation']",
"What does the parameter $y$ stand for in function $g(y,\mu,\sigma)$ related to REINFORCE algorithm?","
I am wondering what the parameter $y$ in the function $g(y,\mu,\sigma)=\frac{1}{(2\pi)^{1/2}\sigma}e^{-(y-\mu)^{2/2\sigma^2}}$ stands for in Section 6 (page 14) of the paper introducing the REINFORCE family of algorithms.
Drawing an analogy to Equation 4 of the same paper, I would guess that it refers to the outcome (i.e. sample) of sampling from a probability distribution parameterized by the parameters $\mu$ and $\sigma$. However, I am not sure whether that is correct or not.
","['reinforcement-learning', 'papers', 'reinforce', 'notation']","If you take a look at the Wikipedia page related to the normal distribution, you will see the definition of the Gaussian density$$
{\displaystyle f(x)={\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}} \label{1}\tag{1}
$$and you will see that the $y$ in your formula corresponds to the $x$ in equation \ref{1}.I've seen this notation in the context of computer vision and image processing, where the Gaussian kernel is used to blur images.So, as pointed out by someone in a comment, $y$ should indeed be the point where you evaluate the density.Maybe the confusing part is that all parameters are treated equally in terms of their purpose, while $\mu$ and $\sigma$ are clearly the parameters that define the specific density, so they are not the inputs to the specific density.After having read the relevant section of the paper, I now understand why you're confused. The author refers to $y$ as the output (not yet sure why: maybe it's the output of another unit that feeds this Gaussian unit?), but I think that this explanation still applies. The output of the Gaussian density $g$ is not $y$, but the density that corresponds to $y$. In fact, in appendix $B$ of the paper, the author says that $Y$ is the support of $g$ and $y$ is an element of $Y$."
How do you pass the image from one convolutional layer to another in a CNN?,"
I am currently trying to write a CNN from scratch, but I don't understand how to feed the information from a max-pooling layer to the next convolutional layer. Specifically, I don't know what to do with the 6 filtered and pooled images from the first convolutional and max-pooling layers. How do I feed those images into the next convolutional layer?
","['convolutional-neural-networks', 'implementation', 'convolutional-layers', 'pooling']",
"Does there necessarily exist ""dominated actions"" in a MDP?","
In a Markov Decision Process, is it possible that there exists no ""dominated action""?
I define a dominated action the following way:
we say that $(s,a)$ is a dominated action, if $\forall \pi, a \notin \text{argmax}\ q^{\pi}(s,.)$, where $\pi$ are policies.
For now, I am only considering the cases where all q-values are distinct and therefore the max is always unique.
I also only consider the case of deterministic policies (mappings from state space to action space).
We can consider MDP in which each state has at least 2 actions available to get rid of the corner cases where there is only one possible policy.
I am struggling to find a counter-example or a proof.
","['reinforcement-learning', 'markov-decision-process', 'proofs', 'value-functions', 'policies']",
How to use DQN when the action space can be different at different time steps?,"
I would like to employ DQN to solve a constrained MDP problem. The problem has constraints on action space. At different time steps till the end, the available actions are different. It has different possibilities as below.

0, 1, 2, 3, 4
0, 2, 3, 4
0, 3, 4
0, 4

Does this mean I need to learn 4 different Q networks for these possibilities? Also, correct me if I am wrong, it looks like if I specify the action size is 3, then it automatically assumes the actions are 0, 1, 2, but, in my case, it should be 0, 3, 4. How shall I implement this?
","['reinforcement-learning', 'dqn', 'action-spaces', 'constrained-optimization']","There are two relevant neural network designs for DQN:Model q function directly $Q(s,a): \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, so neural network has concatenated input of state and action, and outputs a single real value. This is arguably the more natural fit to Q learning, but can be inefficient.Model all q values for given state $Q(s,\cdot): \mathcal{S} \rightarrow \mathbb{R}^{|\mathcal{A}|}$, so neural network takes input of current state and outputs all action values related to that state as a vector.For the first architecture, you can decide which actions to evaluate by how you construct the minibatch. You pre-filter to the allowed actions for each state.For the second architecture, you must post-filter the action values to those allowed by the state.There are other possibilities for constructing variable-length inputs and outputs to neural networks - e.g. using RNNs. However, these are normally not worth the extra effort. A pre- or post- filter on the actions for a NN that can process the whole action space (including impossible actions) is all you usually need. Don't worry that the neural network may calculate some non-needed or nonsense values."
Are actions deterministic during testing in continuous action space PPO?,"
In a continuous action space (for instance, in PPO, TRPO, REINFORCE, etc.), during training, an action is sampled from the random distribution with $\mu$ and $\sigma$. This results in an inherent exploration. However, during testing, when we no longer need to explore but exploit, the action should be deterministic, i.e. just $\mu$, right?
","['reinforcement-learning', 'proximal-policy-optimization', 'reinforce', 'continuous-action-spaces', 'exploration-strategies']",
"Why is the margin attained with $\Phi=\left[2 x, 2 x^{2}\right]^{T}$ greater than the margin attained with $\Phi=\left[x, x^{2}\right]^{T}$?","
I am trying to understand the solution to part 4 of problem 3 from the midterm exam 6.867 Machine learning: Mid-term exam (October 15, 2003).
For reproducibility, here is problem 3.

We consider here linear and non-linear support vector machines (SVM) of the form:
$$
\begin{equation}
\begin{aligned}
\min w_{1}^{2} / 2 & \text { subject to } y_{i}\left(w_{1} x_{i}+w_{0}\right)-1 \geq 0, \quad i=1, \ldots, n, \text { or } \\
\min \mathbf{w}^{T} \mathbf{w} / 2 & \text { subject to } y_{i}\left(\mathbf{w}^{T} \Phi_{i}+w_{0}\right)-1 \geq 0, \quad i=1, \ldots, n
\end{aligned}
\end{equation}
$$
where $\Phi_{i}$ is a feature vector constructed from the corresponding real-valued input $x_{i}$. We wish to compare the simple linear SVM classifier $\left(w_{1} x+w_{0}\right)$ and the non-linear classifier
$\left(\mathbf{w}^{T} \Phi+w_{0}\right)$, where $\Phi=\left[x, x^{2}\right]^{T}$.

Here is part 4 of the same problem.

In general, is the margin we would attain using scaled feature vectors
$\Phi=\left[2 x, 2 x^{2}\right]^{T}$

greater
equal
smaller
any of the above


The correct answer is the first (greater). Why is that the case?
","['machine-learning', 'classification', 'support-vector-machine', 'homework', 'kernel-trick']",
Is there a reference that describes Recurrent Neural Networks for NLP tasks?,"
I would like some references of works that try to understand the functioning of any kind of RNN in natural language processing tasks. They can be any work that tries to explain the functioning of the model by studying the structure of the model itself. I have the feeling that it is very common for researchers to use models, but there is still little theory about how they work in solving natural language processing tasks.
","['deep-learning', 'natural-language-processing', 'recurrent-neural-networks', 'reference-request']",
How does bootstrapping work with the offline $\lambda$-return algorithm?,"
In Barton and Sutton's book, Reinforcement Learning: An Introduction (2nd edition), an expression, on page 289 (equation 12.2), introduced the form of the $\lambda$-return defined as follows
$$G_t^{\lambda} = (1-\lambda)\sum_{n=1}^{\infty} \lambda^{n-1}G_{t:t+n} \label{12.2}\tag{12.2}$$
with the truncated return defined as
$$ G_{t:t+n} \doteq R_{t+1} +\gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^{n}\hat{v}(S_{t+n}, \mathbf{w}_{t+n-1}) \label{12.1}\tag{12.1}$$
However, slightly later in the text, page 290 (equation 12.4), the update algorithm for the offline $\lambda$-return algorithm is defined as
$$
\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha\left[G_{t}^{\lambda}-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right), \quad t=0, \ldots, T-1 \label{12.4}\tag{12.4}
$$
My question is: how do we bootstrap the truncated returns in the update algorithm?
The way the truncated return is currently defined can not plausibly be used, since we would not have access to $\mathbf{w}_{t+n-1}$, as we are in the process of finding $\mathbf{w}_{t+1}$. I suspect $\mathbf{w}_{t}$ is used for bootstrapping in all returns, but that would alter the definition of the truncated return which I just wanted to clarify.

And as a follow-up question: What weights are used for bootstrapping in the online $\lambda$-return algorithm described on page 298?
I assume it's either $\mathbf{w}_{t-1}^{h}$ or $\mathbf{w}_{h-1}^{h-1}$, it's briefly mentioned that the online $\lambda$-return algorithm performs slightly better than the offline one at the end of the episode which leads me to believe the latter is used otherwise the two algorithms would be identical.
Any insight into either question would be great.
","['reinforcement-learning', 'td-lambda', 'bootstrapping', 'lambda-return', 'lambda-return-algorithm']",
"If the training data are linearly separable, which of the following $L(w)$ has less optimum answer for $w$, when $y = w^Tx$?","
I'm studying machine learning and I came into a challenging question.

The answer is 2. But based on my ML notes, all of them are true. Where are the wrong points?
","['machine-learning', 'classification', 'objective-functions', 'support-vector-machine', 'homework']",
How can a probability density value be used for the likelihood calculation?,"
Consider our parametric model $p_\theta$ for an underlying probabilistic distribution $p_{data}$.
Now, the likelihood of an observation $x$ is generally defined as $L(\theta|x) = p_{\theta}(x)$.
The purpose of the likelihood is to quantify how good the parameters are. How can the probability density at a given observation, $p_{\theta}(x)$, measure how good the parameters $\theta$ are?
Is there any relation between the goodness of parameters and the probability density value of an observation?
","['machine-learning', 'comparison', 'probability-distribution', 'maximum-likelihood']",
Which is the best algorithm to predict the trajectory of a vehicle using lat/lon data?,"
I'm using Kalman Filter approaches and I've just implemented the extended Kalman filter (EKF) with my object 2D trajectory. However, I have a mess of alternative approaches that may fit better like Unscented Kalman Filter (UFK), particle filters, adaptive filtering, etc.
How can I choose the most suitable algorithm for my case? In addition, are there algorithms that can predict more than one step ahead?
","['prediction', 'autonomous-vehicles', 'forecasting', 'kalman-filter']",
How to understand 'losses' in Spacy's custom NER training engine?,"
From the tid-bits, I understand of neural networks (NN), the Loss function is the difference between predicted output and expected output of the NN. I am following this tutorial, the losses are included at line #81 in the nlp.update() function.
I am getting losses in the range 300-100. How to interpret them? What should be the ideal output of this losses variable? I went through Spacy's documentation, but nothing much is written there about losses. Also, please let me know the links to relevant theories to understand this in general.
","['machine-learning', 'natural-language-processing', 'objective-functions', 'spacy', 'named-entity-recognition']","A critical goal of training a neural network is to minimize the loss.  Loss is not explained for spaCy because it is a general concept for machine learning and deep learning.  Loss is not specific to spaCy and although there are some finer details I don't believe that is your inquiry.In general, to understand loss functions, I recommend the following resources:If you like videos watch:"
$\nabla \log \pi$ with respect to some parameters constantly being zero,"
I am new to reinforcement learning. May I ask a simple (and maybe a bit silly) question here? I am trying to use the ""one-step actor-critic"" method to train a robot on a gridworld. Let's focus on the actor as there is nothing puzzling me for the critic.
I used a feedforward ANN with one hidden layer to parameterize the action preference function (i.e. the $h$ function). The ANN has one bias node in the input layer to connect to all the hidden nodes. Therefore, there are three sets of weights associated with the $h$ function -- the weights connecting the inputs to the hidden nodes (let's call it $W1$ matrix), the weights connecting the hidden nodes to the outputs (let's call it $W2$ matrix), and the weights connecting the bias node to the hidden nodes (let's call it $c$ vector).
I used the exponential soft-max as the policy function (i.e. the $\pi$ function). That is,
$$\pi(a|s,W1,W2,c) = \displaystyle\frac{e^{h(a,s,W1,W2,c)}}{\sum_be^{h(b,s,W1,W2,c)}}.$$
The inputs to the ANN are the state/feature vector, and the outputs of the ANN are the action preference values (i.e. the $h$ values). With these action preference values, the $\pi$ function can compute the probabilities for each action to be chosen.
It is easy to derive that
$$\nabla_{W1/W2/c} \log \pi(a|s,W1,W2,c) = \nabla_{W1/W2/c} h(a,s,W1,W2,c)-\sum_b \big[\nabla_{W1/W2/c}h(b,s,W1,W2,c)\pi(b|s,W1,W2,c)\big]$$
In the above, $/$ means ""or"".
My puzzle is, I found that $\nabla_{W2} h(\cdot,s,W1,W2,c) \equiv \sigma$, where $\sigma$ is the vector of the sigmoid activation values. That implies, $\nabla_{W2}$ is independent of actions!? Consequently, $\nabla_{W2} \log \pi(\cdot|s,W1,W2,c) \equiv 0$, which implies that $W2$ will not be updated at all...
Where did I get wrong?
(Actually, the above puzzle of mine extends to any policy gradient method as long as $\nabla \log \pi$ is involved and a feedforward ANN is used to approximate the action preferences.)
","['neural-networks', 'reinforcement-learning', 'policy-gradients']",
How does DDPG algorithm know about my action mapping function?,"
I am using DDPG to solve a RL problem. The action space is given by the Cartesian product $[0,20]^4\times[0,6]^4$. The actor is implemented as a deep neural network with an output dimension equals to $8$ with tanh activation.
So, given a state s, an action is given by a = actor(s) where a contains real numbers in [-1,1]. Next, I map this action a into a valid action valid_a that belongs to the action space $[0,20]^4\times[0,6]^4$. Than, I use valid_a to calculate the reward.
My question is: how does the DDPG algorithm know about this mapping that I am doing? In what part of the DDPG algorithm should I specify this mapping? Should I provide a bijective mapping to guarantee that the DDPG algorithm learns bad from good actions?
","['reinforcement-learning', 'deep-rl', 'ddpg', 'mapping-space']",
Have I understood the loss function from the original U-Net paper correctly?,"
In the original U-Net paper, it is written

The energy function is computed by a pixel-wise soft-max over the final
feature map combined with the cross entropy loss function.
...
$$
E=\sum_{\mathbf{x} \in \Omega} w(\mathbf{x}) \log \left(p_{\ell(\mathbf{x})}(\mathbf{x})\right) \tag{1}\label{1}
$$

where $w(\mathbf{x})$ is a weight map (I'm not interested in that part right now), and $p_{k}(\mathbf{x})$ is
$$
p_{k}(\mathbf{x})=\exp \left(a_{k}(\mathbf{x})\right) /\left(\sum_{k^{\prime}=1}^{K} \exp \left(a_{k^{\prime}}(\mathbf{x})\right)\right)
$$
The pixel-wise softmax with $a_{k}(\mathbf{x})$ being the activation in feature channel $k$ at pixel position $\mathbf{x}$ and $K$ the number of classes. Then $\ell(\mathbf{x})$ from $p_{\ell(\mathbf{x})}$ is the true label of each pixel, i.e. if the pixel at position $\mathbf{x}$ is part of class $1$, then $p_{\ell(\mathbf{x})}$ is equal to $p_1(\mathbf{x})$.
As far as is understand $-E$ should be the cross-entropy function. Right? I've already done the math for the binary case (ignoring $w(\mathbf{x})$) and it seemed to be equal.
","['objective-functions', 'papers', 'image-segmentation', 'u-net', 'cross-entropy']","Yes, $E$ is the cross-entropy function and a direct generalization of the binary case.For the binary case, probability to belong to the class $1$ is given by a sigmoid function $\sigma(x)$ of the output $x$, and the probability to belong to the class $0$ is $1 - \sigma(x)$.Therefore the binary crossentropy will give:
$$
-\sum_i (l_i \log \sigma(x) + (1 - l_i)  \log (1 - \sigma(x))
$$
Where the sum is over all samples in the dataset. Because $l_i$ is a binary variable one of these two terms would be zero. The gradient descent forces the model to predict true label with more confidence.For the multiclass case, now the output is a $K$-vector and the softmax function forces its elements to sum to one:
$$
\sum_{i = 1}^{K} p_i = 0
$$
The true label is one-hot encoded vector, with $1$ on the position of the true label, and $0$ elsewhere. The generalization of the binary case is:
$$
-\sum_i \sum_{j = 1}^{K}(l_{ij} \log \sigma_j(x) + (1 - l_{ij})  \log (1 - \sigma_j(x))
$$
In this case, there will be $K$ non-vanishing contributions.The minization of $E$ forces the classifier to predict the true label more confidently, and all other (to make $\log(1 - \sigma_j(x))$) as small  as possible."
Can you give me a piece of advise of the network sructure that would be suitable for my task?,"
I have 2 small images. They are basically the same, but differ in rotation and size. I should estimate the parameters for affine transform to get them similar. What network structure can be suitable for this task? For example, those based on convolutional networks did badly, because the pictures are too small.
",['image-generation'],
T5 or BERT for sentence correction/generation task?,"
I have sentences with some grammatical errors , with no punctuations and digits written in words... something like below:

As you can observe, a proper noun , winston isnt highlighted with capital in Sample column. 'People' is spelled wrong and there are no punctuations in Sample column. The date in the first row isnt in right format. I have millions of rows like this and want to train a model to learn punctuations and corrections. Can a single BERT or T5 handle this task? or only option is to try 1 model for each task?
Thanks in advance
","['natural-language-processing', 'bert', 'text-generation']",
How is few-shot learning different from transfer learning?,"
To my understanding, transfer learning helps to incorporate data from other related datasets and achieve the task with less labelled data (maybe in 100s of images per category).
Few-shot learning seems to do the same, with maybe 5-20 images per category. Is that the only difference?
In both cases, we initially train the neural network with a large dataset, then fine-tune it with our custom datasets.
So, how is few-shot learning different from transfer learning?
","['deep-learning', 'comparison', 'image-recognition', 'transfer-learning', 'few-shot-learning']",
How to update the observation probabilities in a POMDP?,"
How can I update the observation probability for a POMDP (or HMM), in order to have a more accurate prediction model?
The POMDP relies on observation probabilities that match an observation to a state. This poses an issue as the probabilities are not exactly known. However, the idea is to make them more accurate over time. The simplest idea would be to count the appeared observation as well as the states and use Naive Bayes estimators.
For example, $P(s' \mid a,s)$ is the probability that a subsequent state $s'$ is reached, given that the action $a$ and the previous state $s$ are known: In that simple case I can just count and then apply e.g. Naive Bayes estimators.
But, if I have an observation probability $P(z \mid s')$ (where $z$ is the observation) depending on a state, it's not as trivial to just count up the observation and the states, as I can not say that a state really was reached (Maybe I made an observation, but I was in a different state than wanted). I can just make an observation and hope I was in a certain state. But I can not say if e.g. I was in $s_1$ or maybe $s_2$. I think the update of the observation probability is only possible in the late aftermath.
So, what are good approaches to estimate my state?
","['reinforcement-learning', 'pomdp', 'state-spaces', 'conditional-probability']",
What are the fundamental differences between VAE and GAN for image generation?,"
Starting from my own understanding, and scoped to the purpose of image generation, I'm well aware of the major architectural differences:

A GAN's generator samples from a relatively low dimensional random variable and produces an image. Then the discriminator takes that image and predicts whether the image belongs to a target distribution or not. Once trained, I can generate a variety of images just by sampling the initial random variable and forwarding through the generator.

A VAE's encoder takes an image from a target distribution and compresses it into a low dimensional latent space. Then the decoder's job is to take that latent space representation and reproduce the original image. Once the network is trained, I can generate latent space representations of various images, and interpolate between these before forwarding through the decoder which produces new images.


What I'm more interested is the consequences of said architectural differences. Why would I choose one approach over the other? And why? (for example, if GANs typically produce better quality images, any ideas why that is so? is it true in all cases or just some?)
","['comparison', 'applications', 'generative-adversarial-networks', 'variational-autoencoder', 'image-generation']",
"In OCR, how should I deal with the warped text on the sides of oval objects?","
Consider an image that contains one can (or bottle, or any similar oval object), which has texts all over it. In the image below, I have many bottles, but you can assume that each image only contains one such object.

As we can see, in each can, the text can flow from left to right, and any OCR system may miss the text on the left and right sides of the can, as they are not aligned with the camera angle.
So, is there any solution/s for this, like preprocessing in a certain way, so that we can read the text or make this round object into a straight one? (If there is any Python program that can solve this problem, could you please share it with me?)
","['python', 'image-processing', 'data-preprocessing', 'optical-character-recognition']","There are many papers on this but the following is a good start:You mentioned you do not want to do a panoramic view but that has more than one meaning.  If I assume you mean you do not want to rotate the can while taking multiple photos, or you don't want to take multiple photos from different angles, you could try a pericentric lens.  This would require some image processing to do the unwrapping.  More resolution is needed as the wrapping is much more severe.  The advantage though is that you will have a single image of the full cylindrical surface and won't miss any features or text."
"In the DeepView paper, do they use the same FCN for all depth slices AND all views?","
I'm trying to replicate a paper from Google on view synthesis/lightfields from 2019: DeepView: View Synthesis with Learned Gradient Descent and this is the PDF.
Basically the input to the neural network comes from a set of cameras which number is variable, and the output is a stack of images which number is also variable. For that they use both a Fully Convolutional Network and Learned Gradient Descent.
I don't know if I am understanding this correctly: (in each LGD iteration) They use the same network for all depth slices AND all views. Is this correct?
This is the LGD network, not much important to the question but it helps you understand the setup. You can see at least 3 LGD iterations. Part b) is just the calculation they do in the ""green gradient boxes"" on part a).

This is the inside of the CNNs. On each LGD iteration they use basically the same architecture, but the weights are different per iteration.

For me the confusing part is that they represent each view as a different network, but they don't represent each depth slice as a different network. As you can see in the next image they do say that they use the same parameters for all depth slices, and that the order of the views doesn't matter so it must be that they're also reusing the parameters for all views, right? So if I understand correctly, this is a matter of reusing the same model for all depths and all views. BTW note that the maxpool kind of operation is over each view.
Also I have a question on the practicalities of the implementation. I'll be implementing this with normal 2D convolution layers, so if I want them to run independent of the views and depth slices, I guess I could concatenate views and depth slices in the ""batch"" dimension? I mean, before the maximum k operation, and then reuse the output.
This is what they say:

Thanks
","['convolutional-neural-networks', 'computer-vision', 'fully-convolutional-networks']",
How should I generate datasets for a SARSA agent when the environment is not simple?,"
I am currently working on my master's thesis and going to apply Deep-SARSA as my DRL algorithm. The problem is that there is no datasets available and I guess that I should generate them somehow. Datasets generation seems a common feature in this specific subject as stated in [1]

When a dataset is not available, learning is performed through experience.

I am wondering how to generate datasets when the environment is not as simple as a tic-tac-toe or a maze problem and what the experience means.
PS: The environment consists of 15 mobile users and 3 edge servers, each of which covers a number of mobile users. Each mobile user might generate a computationally heavy-load task and at the beginning of each timestep and can process the task itself or requests its associated edge server to do the processing. If the associated edge server is not capable of processing, due to some reasons, it requests a nearby edge server to lend it a hand. The optimization problem (reward) is to reduce time and energy consumption (multi-objective optimization). Each server has a DRL agent that makes offloading decisions.
I'd really appreciate your suggestions and help.
","['reinforcement-learning', 'datasets', 'environment', 'sarsa', 'on-policy-methods']","I am wondering how to generate datasets when the environment is not as simple as a tic-tac-toe or a maze problemThere is no difference in concept, which is why tic-tac-toe and maze problems are used to teach.As you have noted, the main difference between reinforcement learning (RL) and supervised learning is that RL does not use labeled datasets. If you are using SARSA then you would not expect to use any record of previous experience either because SARSA is designed to work on-policy and online - which means that data needs to be generated during training. Training data for SARSA is typically stored only temporarily before being used, or is used immediately (you might keep a log of it for analysis or to help document your thesis, but that log will not be used for further training by the agent). This is different to Q-learning and DQN, which could in theory make use of longer-term stored experience.You have two main choices for acquiring data:Use a real environment. In your case, set up 15 mobile users and 3 edge servers. Instrument the environment to collect state and reward data for the agent. Implement the agent as the real decision maker in this environment.Simulate the environment. Write a simulation that models user behaviour and server loading. Instrument that to provide state and reward data, and integrate your learning agent with it. Typically the agent will call the environment's step function, passing the action choice as an argument and receiving reward and state data back.If you can simulate the environment, this is likely to be preferable to you since you will likely use less compute resources (than 3 servers and 15 mobile phones) and can run the training faster than real time. Deep reinforcement learning can use a large amount of experience to converge on near-optimal policies, and fast simulations can help because they generate experience faster than reality.You can also do both approaches. Train an initial agent in simulation, then implement the real version once it reaches a good level of performance in simulation. You can even have the agent continue to learn and refine behaviour in production. Given that you are working with SARSA, this may be an important part of the intent of your project, that the agent continues to adapt to changes in user behaviour and server load over time. In fact this is a key advantage of SARSA over Q-learning, that it should be more reliable and safe to use in such a continuous learning scenario deployed to production.and what the experience means.The experience in reinforcement learning is the record of states, actions and rewards that the agent encounters during training."
Semantic segmentation CNN outputs all zeroes,"
I'm using MATLAB 2019, Linux, and UNet (a CNN specifically designed for semantic segmentation). I'm training the network to classify all pixels in an image as either cell or background to get segmentations of cells in microscopic images. My problem is the network is classifying every single pixel as background, and seems to just be outputting all zeroes. The validation accuracy improves a little at the very start of the training but than plateaus at around 60% for the majority of the training time. The network doesn't seem to be training very well and I have no idea why.
Can anyone give me some hints about what I should look into more closely? I just don't even know where to start with debugging this.
Here's my code:
    % Set datapath
    datapath = '/scratch/qbi/uqhrile1/ethans_lab_data';
    
    % Get training and testing datasets
    images_dataset = imageDatastore(strcat(datapath,'/bounding_box_cropped_resized_rgb'));
    load(strcat(datapath,'/gTruth.mat'));
    labels = pixelLabelDatastore(gTruth);
    [imdsTrain, imdsVal, imdsTest, pxdsTrain, pxdsVal, pxdsTest] = partitionCamVidData(images_dataset,labels);
    
    % Weight segmentation class importance by the number of pixels in each class
    pixel_count = countEachLabel(labels); % count number of each type of pixel
    frequency = pixel_count.PixelCount ./ pixel_count.ImagePixelCount; % calculate pixel type frequencies
    class_weights = mean(frequency) ./ frequency; % create class weights that balance the loss function so that more common pixel types won't be preferred
    
    % Specify the input image size.
    imageSize = [512 512 3];
    
    % Specify the number of classes.
    numClasses = 2;
    
    % Create network
    lgraph = unetLayers(imageSize,numClasses);
    
    % Replace the network's classification layer with a pixel classification
    % layer that uses class weights to balance the loss function
    pxLayer = pixelClassificationLayer('Name','labels','Classes',pixel_count.Name,'ClassWeights',class_weights);
    lgraph = replaceLayer(lgraph,""Segmentation-Layer"",pxLayer);
    
    %% TRAIN THE NEURAL NETWORK
    
    % Define validation dataset-with-labels
    validation_dataset_with_labels = pixelLabelImageDatastore(imdsVal,pxdsVal);
    
    % Training hyper-parameters: edit these settings to fine-tune the network
    options = trainingOptions('adam', 'LearnRateSchedule','piecewise', 'LearnRateDropPeriod',10, 'LearnRateDropFactor',0.3, 'InitialLearnRate',1e-3, 'L2Regularization',0.005, 'ValidationData',validation_dataset_with_labels, 'ValidationFrequency',10, 'MaxEpochs',3, 'MiniBatchSize',1, 'Shuffle','every-epoch');
    
    % Set up data augmentation to enhance training dataset
    aug_imgs = {};
    numberOfImages = length(imdsTrain.Files);
    for k = 1 : numberOfImages
        % Apply cutout augmentation
        img = readimage(imdsTrain,k);
        cutout_img = random_cutout(img);imwrite(cutout_img,strcat('/scratch/qbi/uqhrile1/ethans_lab_data/augmented_dataset/img_',int2str(k),'.tiff'));
    end
    aug_imdsTrain = imageDatastore('/scratch/qbi/uqhrile1/ethans_lab_data/augmented_dataset');
    % Add other augmentations
    augmenter = imageDataAugmenter('RandXReflection',true, 'RandXTranslation',[-10 10],'RandYTranslation',[-10 10]);
    % Combine augmented data with training data
    augmented_training_dataset = pixelLabelImageDatastore(aug_imdsTrain, pxdsTrain, 'DataAugmentation',augmenter);
    
    % Train the network
    [cell_segmentation_nn, info] = trainNetwork(augmented_training_dataset,lgraph,options);
    
    save cell_segmentation_nn

","['convolutional-neural-networks', 'training', 'image-segmentation', 'matlab', 'u-net']",
"When computing the ROC-AUC score for multi-class classification problems, when should we use One-vs-Rest and One-vs-One?","
The sklearn's documentation of the method  roc_auc_score states that the parameter multi_class can take the value 'OvR' (which stands for One-vs-Rest) or 'OvO' (which stands for One-vs-One). These values are only applicable for multi-class classification problems.
Does anyone know in what particular cases we would use OvR as opposed to OvO? In the general academic literature, is there a preference given to one?
","['machine-learning', 'metric', 'scikit-learn', 'roc-auc', 'multiclass-classification']",
What are support values in a support vector machine?,"
I started reading up on SVM and very little is defined of what are support values. I reckon it's they are denoted as $\alpha$ in most formulations.
","['machine-learning', 'terminology', 'definitions', 'support-vector-machine']",In the least-squares SVM (LS-SVM) the non-zero Lagrange multipliers ($\alpha$) are the support values.  The corresponding data points are the support vectors.  Johan Suykens explains this in Least Squares Support Vector Machines.
"Can TensorFlow, PyTorch, and other mainstream ML frameworks be used for research-grade work in AI?","
Many authors of research papers in AI (e.g. arXiv) write their neural networks from the ground-up, using low-level languages like C++ to implement their theories. Can existing open source frameworks also be used for this purpose, or are their implementations too limited?
Can, for example, TensorFlow be used to craft an original network architecture that shows improvements on existing benchmarks? Can original mathematical work be coded into a high-level framework like TensorFlow such that original research on network architectures/approaches be demonstrated in a paper?
A quick search reveals many papers using C++ in their implementation:

https://arxiv.org/pdf/1904.04174.pdf
https://arxiv.org/pdf/1701.03980.pdf
https://arxiv.org/pdf/2005.07039.pdf
https://arxiv.org/pdf/2007.14236.pdf
https://arxiv.org/pdf/1908.05858.pdf

","['neural-networks', 'tensorflow', 'python', 'research', 'pytorch']","Your statement that researchers build their network from the ground-up using C++ or some other low level library couldn't be further from the truth.You could take a look at this analysis showing the popularity of these two frameworks in the top ML conferences. The following Figure is taken from there.In CVPR-2020, for example, TensorFlow and pytorch combined for over 500 papers! Furthermore, because the two most active research entities (Google and Facebook) are backing these two frameworks, they are used in some of the most impactful research studies.I want to give some reasons that support the popularity of these frameworks, but first I'm going to rephrase your question a bit:Why use TensorFlow/Pytorch in python rather than build your model on your own using C++?Note: The reason I rephrased the question is because TensorFlow and PyTorch both have a C++ APIs.Some reasons are the followingRapid prototyping. Languages link C++, have bloated syntaxes, require low-level operations (e.g. memory management) and cannot be run interactively. This means it takes someone much less time to create and test a model in python than it does in C++.No need to re-invent the wheel. Some operations are common in most networks (e.g. backpropagation), why re-implement them? Other functionalities are hard to implement on your own (e.g. parallel processing, GPU computation). Do data scientists need to have such a strong technical background to research neural networks?Open-source. They benefit from being opensource and can offer a great deal of tools at your disposal for building neural networks. You want to add batchnorm to your network? No worries, just import it and add it in a single line! Also, they offer the perfect opportunity for sharing pretrained models.They are optimized. These frameworks are optimized to run as fast as possible on GPUs (if available) or CPUs. It would be virtually impossible for someone to write code that runs as fast on his own."
What is the use of the regular convolutional layer in expansion path of U-Net?,"
I was going through the paper on U-Net. U-net consists of a contracting path followed by an expanding path. Both the paths use a regular convolutional layer. I understand the use of convolutional layers in the contracting path, but I can't figure out the use of convolutional layers in the expansive path. Note that I'm not asking about the transpose convolutions, but the regular convolutions in the expansive path.

","['convolutional-neural-networks', 'image-segmentation', 'convolution', 'u-net', 'convolutional-layers']",
Should we use a pre-trained model or a blank model for custom entity training of NER in spacy?,"
Further to my last question, I am training a custom entity of FOODITEM to be recognized by Spacy's  Name Entity Recognition engine. I am following tutorials online, following is the advise given in most of the tutorials;

Load the model or create an empty model


We can create an empty model and train it with our annotated dataset or we can use the existing spacy model and re-train with our annotated data.

But none of the tutorials tell how/why to choose between the two options. Also, I don't understand how will the choice affect my final output or the training of the model.
How do I make the choice between a pre-trained model or a blank model? What are the factors to consider?
","['natural-language-processing', 'spacy', 'named-entity-recognition']","The reason you would load a pre-existing model is that it offers something of value to your task (e.g. named entity recognition for food) and the cost of training it from scratch is not worth it.  For example, to train GPT-3 from scratch would cost several million dollars.  Typically someone will use a model like BERT and fine tune it.  This is called transfer learning.  With spaCy you will typically use en_core_web_sm which was trained on the OntoNotes corpus and includes named entities.  Making a custom food NER using en_core_web_sm should be more accurate than making one from scratch. You should be able to build a good model with and without transfer learning fairly quickly if you have a GPU."
How to express a fully connected neural network succintly using linear algebra?,"
I'm currently reading the paper Federated Learning with Matched Averaging (2020), where the authors claim:

A basic fully connected (FC) NN can be formulated as: $\hat{y} = \sigma(xW_1)W_2$ [...]
Expanding the preceding expression
$\hat{y} = \sum_{i=1}^{L} W_{2, i \cdot } \sigma(\langle x, W_{1,\cdot i} \rangle))$, where $ i\cdot$ and $\cdot i$ denote the ith row and column correspondingly and $L$ is the number of hidden units.

I'm having a hard time wrapping my head around how it can be boiled down to this. Is this rigorous? Specifically, what is meant by the ith row and column? Is this formula for only one layer or does it work with multiple layers?
Any clarification would be helpful.
","['neural-networks', 'papers', 'feedforward-neural-networks', 'linear-algebra', 'federated-learning']","The equation $$\hat{y} = \sigma(xW_\color{green}{1})W_\color{blue}{2} \tag{1}\label{1}$$ is the equation of the forward pass of a single-hidden layer fully connected and feedforward neural network, i.e. a neural network with 3 layers, 1 input layer, 1 hidden layer, and 1 output layer, whereAs an example, suppose that we have $N$ real-valued features, there are $L$ hidden units (or neurons) and $M$ output units, then the elements (feature vector and parameters) of equation \ref{1} would have the following shape$\sigma$ is an activation function that is applicable to all elements of the matrix separately (i.e. component-wise). So, $\hat{y} \in \mathbb{R}^{1 \times M}$.The equation$$
\hat{y} = \sum_{\color{red}{i} = 1}^{L} W_{\color{blue}{2}, \color{red}{i} \cdot} \sigma(\langle x, W_{\color{green}{1}, \cdot \color{red}{i}} \rangle )\label{2}\tag{2}
$$is another way of writing equation \ref{1}.Before going to the explanation, let's try to understand equation \ref{2} and its components.$W_{\color{green}{1}, \cdot \color{red}{i}} \in \mathbb{R}^N$ is the $\color{red}{i}$th column of the matrix that connects the inputs to the hidden neurons, so it is a vector of $N$ elements (note that we sum over the number of hidden neurons, $L$).Similarly, $ W_{\color{blue}{2}, \color{red}{i} \cdot} \in \mathbb{R}^M$ is also a vector, but, in this case, it is a row of the matrix $ W_{\color{blue}{2}}$ (rather than a column: why? because we use $\color{red}{i} \cdot$ instead of $\cdot \color{red}{i}$, which refers to the column).So, $\langle x, W_{\color{green}{1}, \cdot \color{red}{i}} \rangle$ is the dot (or scalar) product between the feature vector $x$ and the $\color{red}{i}$th column of the matrix that connects the inputs to the hidden neurons, so it's a number (or scalar). Note that both $x$ and $W_{\color{green}{1}, \cdot \color{red}{i}}$ have $N$ elements, so the dot product is well-defined in this case.$ W_{\color{blue}{2}, \color{red}{i}} \sigma(\langle x, W_{\color{green}{1}, \color{red}{i}} \rangle))$ is the product between a vector of shape $M$ and a number $\sigma(\langle x, W_{\color{green}{1}, \cdot \color{red}{i}} \rangle )$. This is also well-defined. You can multiply a real-number with a vector, it's like multiplying the real-number with each element of the vector.$\sum_{\color{red}{i} = 1}^{L} W_{\color{blue}{2}, \color{red}{i} \cdot} \sigma(\langle x, W_{\color{green}{1}, \cdot \color{red}{i}} \rangle )$ is thus the sum of $L$ vectors of size $M$, which makes $\hat{y}$ also have size $M$, as in equation \ref{1}.Now, the question is: is equation \ref{2} really equivalent to equation \ref{1}? This is still not easy to see because $xW_\color{green}{1}$ is a vector of shape $L$, but, in equation \ref{2}, we do not have any vector of shape $L$, but we have vectors of shape $N$ and $M$ (and the vectors of shape $M$ are summed $L$ times). First, note that $\sigma(xW_\color{green}{1}) = h\in \mathbb{R}^L$, so $hW_\color{blue}{2}$ are $M$ dot products collected in a vector (i.e. $\hat{y} \in \mathbb{R}^M$), where the $j$th element of $\hat{y}$ was computed as a summation of $L$ elements (a dot product of two vectors is the element-wise multiplication of the elements of the vectors followed by the summation of these multiplications). Ha, still not clear!The easiest way (for me) to see that they are equivalent is to think that $\sigma(xW_\color{green}{1})$ is a vector of $L$ elements $\sigma(xW_\color{green}{1}) = \ell = [l_1, l_2, \dots, l_L]$. Then you know that to multiply this vector with $\ell$ (from the left), you actually perform a dot product between $\ell$ and each column $W_\color{blue}{2}$. A dot product is essentially a sum, and that's why we sum in equation \ref{2}. So, essentially, in equation \ref{2}, we first multiple $l_1$ with the first row of $W_\color{blue}{2}$ (i.e. by all elements of the first row). Then we multiply $l_2$ by the second row of $W_\color{blue}{2}$. We do this for all $L$ rows, then we sum the rows (to conclude the dot product). So, you can think of equation 2 as first perform all multiplications, then summing, rather than dot product-by-dot product.So, in my head, I have the following picture. To simplify the notation, let $A$ denote $W_\color{blue}{2}$, so $A_{ij}$ is the element at the $i$th row and $j$th column of matrix $W_\color{blue}{2}$. So, we have the following initial matrix$$
A = 
\begin{bmatrix}
A_{11} & A_{12} & A_{13} & \dots & A_{1M} \\
A_{21} & A_{22} & A_{23} & \dots & A_{2M} \\
\vdots & \vdots & \vdots & \dots & \vdots \\
A_{L1} & A_{L2} & A_{L3} & \dots & A_{LM} \\
\end{bmatrix}
= 
\begin{bmatrix}
W_{\color{blue}{2}, \color{red}{1} \cdot}  \\
W_{\color{blue}{2}, \color{red}{2}  \cdot} \\
\vdots  \\
W_\color{blue}{2, \color{red}{L}  \cdot} \\
\end{bmatrix}
$$Then, in the first iteration of equation \ref{2}, we do the following$$
\begin{bmatrix}
l_1 A_{11} & l_1 A_{12} & l_1 A_{13} & \dots & l_1 A_{1M} \\
A_{21} & A_{22} & A_{23} & \dots & A_{2M} \\
\vdots & \vdots & \vdots & \dots & \vdots \\
A_{L1} & A_{L2} & A_{L3} & \dots & A_{LM} \\
\end{bmatrix}
$$In the second, we do the following$$
\begin{bmatrix}
l_1 A_{11} & l_1 A_{12} & l_1 A_{13} & \dots & l_1 A_{1M} \\
l_2 A_{21} & l_2 A_{22} & l_2 A_{23} & \dots & l_2 A_{2M} \\
\vdots & \vdots & \vdots & \dots & \vdots \\
A_{L1} & A_{L2} & A_{L3} & \dots & A_{LM} \\
\end{bmatrix}
$$
Until we have$$
\begin{bmatrix}
l_1 A_{11} & l_1 A_{12} & l_1 A_{13} & \dots & l_1 A_{1M} \\
l_2 A_{21} & l_2 A_{22} & l_2 A_{23} & \dots & l_2 A_{2M} \\
\vdots & \vdots & \vdots & \dots & \vdots \\
l_L A_{L1} & l_L A_{L2} & l_L A_{L3} & \dots & l_L A_{LM} \\
\end{bmatrix}
$$
Then we do a reduce sum across the rows to end the dot product (i.e. for each column we sum the elements in the rows). This is exactly equivalent to first performing the dot product between $\ell$ and the first column of $W_\color{blue}{2}$, then the second column, and so on."
How do gradients are flown back into the Siamese network when branching is done?,"
I am curious about the working of a Siamese network. So, let us suppose I am using a triplet loss for my network and I have instantiated single CNN 3 times and there are 3 inputs to the network. So, during a forward pass, each of the networks will give me an embedding for each image, and I can get the distance and calculate the loss and compare it with the output, so that my model is ready to propagate the gradients to update the weights.
The Question: How do these weights get updated during the back propagation? Just because we are using 3 inputs and 3 branches of the same network and we are passing the inputs one by one (I suppose), how do the gradients are updated? Are these series? Like the one branch will update, then the second and then the third. But won't it be a problem because each branch would try to update based on its output? If in parallel, then which branch is responsible for the gradients update? I mean to say that I am unable to get the idea how weights are updated in Siamese network. Can someone please explain in simpler terms?
","['deep-learning', 'convolutional-neural-networks', 'backpropagation', 'facial-recognition', 'siamese-neural-network']",
Why do we use big batch/epoch size in policy gradient methods (vpg specifically)?,"
I am re-implementing vpg and using Spinning Up as reference implementation. I noticed that the default epoch size is 4000. I also see cues in papers that big batch size is quite standard.
My implementation doesn't batch XP together just applies the update after every episode. It turns out my implementation is more sample efficient than the reference implementation on simple problems (like CartPole or LunarLander) even though I haven't added the critic yet! Of course this could be due to number of reasons, for example I've only done parameter search on my implementation.
But it would make sense anyway: bigger batch size is generally considered better only because the GPU is faster processing many samples parallel. Is this the reason here? It would make sense but it is surprising for me as I thought sample efficiency is considered more important than computing efficiency in RL.
","['reinforcement-learning', 'policy-gradients', 'actor-critic-methods', 'reinforce']",
How to know if a real-time classifier is achivable in a low-power emdedded system?,"
Say I have an Machine/Deep learning algorithm I developed on a desktop pc to achieve a real-time classification of time series events from a sensor. Once the algorithm is trained and performs good, I want to implement it on an low power embbeded system, with the same sensor, to classify events in real-time:

How can I know if the low power embedded system is fast enough to allow real-time classification regarding the algorithm (knowing it in advance would avoid to implement and try multiple architectures) ?
Machine/Deep learning algorithm are usually developed in python. Is there easy ways to transfer the code from python to a more embeddable langage ?

","['ai-design', 'computational-complexity', 'embedded-design']",
Correct way to work with both categorical and continuous features together,"
I have a time series with both continuous and categorical features, and I want to do a prediction task.
I will elaborate:
The data is composed of 100Hz sampling of some voltages, kind of like an ecg signal, and of some categorical features such as ""green"", ""na"" and so on.
In total, the number of features can reach 300, of which most are continuous.
The prediction should take in a chunk of frames and predict a categorical variable for this chunk of frames.

I want to create a deep learning model that can handle both categorical and continuous features.
Best I can think of is two separate losses, like MSE and cross entropy, and a hyperparameter to tune between them, kind of like regularization.
Best I could find on this subject was this, with an answer from 2015.
I wonder if something better was invented, since then, or maybe just someone here knows something better.
","['deep-learning', 'reference-request', 'objective-functions', 'categorical-data']",
"For the generalised delta rule in back-propogation, do you subtract the target from the obtained output, or vice versa?","
When I look up the generalised delta rule equation for back-propogation, I am seeing two conflicting equations.
For example, here (slide 20), given $o$ (the output, defined in slide 18), $z$ (the activated output) and a target $t$, defined in slide 17, then:
$\frac{\delta E}{\delta Z} = o(1-o)(o-t)$
When I look for the same equation else, e.g. here, slide 14, it says, given $o$ the output and $y$ the label, then (using slightly different notation $\beta_k$):
$\beta_k = o_k(1-o_k)(y_k-o_k)$
I can see here that these two equations are almost the same, but not quite. One subtracts the output from the target, and one subtracts the target from the output.
The reason why I'm asking this is I'm trying to do question 29 and 30 of this paper, and they are using the second equation ($\beta_k$) but my college notes (that I can't copy and paste due to copyright) define the equation according to the first equation $\frac{\delta E}{\delta Z}$. I'm wondering which way is correct, do you subtract the target from the obtained output, or vice versa?
","['neural-networks', 'backpropagation', 'calculus']",
What are good techniques for continuous learning in production?,"
I was wondering which AI techniques and architectures are used in environments that need predictions to continually improve by the feedback of the user.
So let's take some kind of recommendation system, but not for a number of $n$ products, but for some problem of higher space. It's initially trained, but should keep improving by the feedback and corrections applied by the user. The system should continue to improve its outcomes on-the-fly in production, with each interaction.
Obviously, (deep) RL seems to fit this problem, but can you really deploy this learning process to production? Is it really capable of improving results on-the-fly?
Are there any other techniques or architectures that can be used for that?
I'm looking for different approaches in general, in order to be able to compare them and find the right one for problems of that kind. Of course, there always is the option to retrain the whole network, but I was wondering whether there are some online, on-the-fly techniques that can be used to adjust the network?
","['reinforcement-learning', 'incremental-learning', 'active-learning']",
Research into social behavior in Prisoner's Dilemma,"
I've been working on research into reproducing social behavior using multi-agent reinforcement learning. My focus has been on a GridWorld-style game, but I was thinking that maybe a simpler Prisoner's Dilemma game could be a better approach. I tried to find existing research papers in this direction, but couldn't find any, so I'd like to describe what I'm looking for in case anyone here knows of such research.
I'm looking for research into scenarios where multiple RL agents are playing Iterated Prisoner's Dilemma with each other, and social behaviors emerge. Let me specify what I mean by ""social behaviors."" Most research I've seen into RL/IPD (example) focuses on how to achieve the ideal strategy, and how to get there the fastest, and what common archetypes of strategies emerge. That is all nice and well, but not what I'm interested in.
An agent executing a Tit-for-Tat strategy is giving positive reinforcement to the other player for ""good"" behavior, and negative reinforcement for ""bad"" behavior. That is why it wins. My key point here is that this carrot-and-stick method is done individually rather than in groups. I want to see it evolve within a group.
I want to see an entire group of agents evolve to punish and reward other players according to how they behaved with the group. I believe that fascinating group dynamics could be observed in that scenario.
I programmed such a scenario a decade ago, but by writing an algorithm manually, not using deep RL. I want to do it using deep RL, but first I want to know whether there are existing attempts.
Does anyone know whether such research exists?
","['reinforcement-learning', 'q-learning', 'reference-request', 'research']",
What are the pros and cons of 3D CNN and 2D CNN combined with optical flow for action recognition?,"
For action recognition or similar tasks, one can either use 3D CNN or combine 2D CNN with optical flow. See this paper for details.
Can someone tell the pros/cons of each, in terms of accuracy, cost such as computation and memory requirement, etc.? In other words, is the computation overhead of 3D CNN justified by its accuracy improvement? Under what scenarios would one prefer one over another?
3D CNNs are also used for volumetric data, such as MRI images. Can 2D CNN + optical flow be used here?
I understand 2D CNNs and 3D CNNs, but I do not know about optical flow (my background is not computer-vision).
","['convolutional-neural-networks', 'computer-vision', 'comparison', 'action-recognition']",
Confusion between function learned and the underlying distribution,"
Let us assume that I am working on a dataset of black and white dog images.
Each image is of size $28 \times 28$.
Now, I can say that I have a sample space $S$ of all possible images. And $p_{data}$ is the probability distribution for dog images. It is easy to understand that all other images get a probability value of zero. And it is obvious that $n(S)= 2^{28 \times 28}$.
Now, I am going to design a generative model that sample from $S$ using $p_{data}$ rather than random sampling.
My generative model is a neural network that takes random noise (say, of length 100) and generates an image of the size $28 \times 28$. My function is learning a function $f$, which is totally different from the function $p_{data}$. It is because of the reason that  $f$ is from $R^{100}$ to $S$ and $p_{data}$ is from $S$ to $[0,1]$.
In the literature, I often read the phrases that our generative model learned $p_{data}$ or our goal is to get $p_{data}$, etc.,  but in fact, they are trying to learn $f$, which just obeys $p_{data}$ while giving its output.
Am I going wrong anywhere or the usage in literature is somewhat random?
",['terminology'],
How are nested bounding boxes handled in object detection (and in particular in the case of the SSD)?,"
The basic approach to non-maximum-suppression makes sense, but I am kind of confused about how you handle nested bounding boxes.
Suppose you have two predicted boxes, with one completely enclosing another. What happens under this circumstance (in particular, in the case of the Single Shot MultiBox Detector)? Which bounding box do you select?
","['deep-learning', 'computer-vision', 'object-detection', 'non-max-suppression', 'single-shot-multibox-detector']",
Why do we need importance sampling?,"
I was studying the off-policy policy improvement method. Then I encountered importance sampling. I completely understood the mathematics behind the calculation, but I am wondering what is the practical example of importance sampling.
For instance, in a video, it is said that we need to calculate the expected value of a biased dice, here $g(x)$, in terms of the expected value of fair dice, $f(x)$. Here is a screenshot of the video.

Why do we need that, when we have the probability distribution of the biased dice?
","['reinforcement-learning', 'monte-carlo-methods', 'off-policy-methods', 'importance-sampling']",
What are the disadvantages to using a distance metric in character recognition prediction,"
I am reading this paper, that is discussing the use of distance metrics for character recognition predicton.
I can see the advantages of using a distance metrics in predictions like character recognition: You can model a set of known character's pixel vectors, and then measure a new unseen vector against this model, get the distance, and if the distance is low, predict that this unseen vector belongs to a particular class.
I'm wondering if there's any disadvantages to using distance metrics as the cost function in character recognition? For example, I was thinking maybe the distance calculation is slow for large images (would have to calculate the distance between each item in two long image vectors)?
","['optical-character-recognition', 'metric', 'handwritten-characters']",
Simple DQN too slow to train [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



I have been trying to solve the OpenAI lunar lander game with a DQN taken from this paper
https://arxiv.org/pdf/2006.04938v2.pdf
The issue is that it takes 12 hours to train 50 episodes so something must be wrong.
import os
import random
import gym
import numpy as np
from collections import deque
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Model

ENV_NAME = ""LunarLander-v2""

DISCOUNT_FACTOR = 0.9
LEARNING_RATE = 0.001

MEMORY_SIZE = 2000
TRAIN_START = 1000
BATCH_SIZE = 24

EXPLORATION_MAX = 1.0
EXPLORATION_MIN = 0.01
EXPLORATION_DECAY = 0.99

class MyModel(Model):
    def __init__(self, input_size, output_size):
        super(MyModel, self).__init__()
        self.d1 = Dense(128, input_shape=(input_size,), activation=""relu"")
        self.d2 = Dense(128, activation=""relu"")
        self.d3 = Dense(output_size, activation=""linear"")

    def call(self, x):
        x = self.d1(x)
        x = self.d2(x)
        return self.d3(x)

class DQNSolver():

    def __init__(self, observation_space, action_space):
        self.exploration_rate = EXPLORATION_MAX

        self.action_space = action_space
        self.memory = deque(maxlen=MEMORY_SIZE)

        self.model = MyModel(observation_space,action_space)
        self.model.compile(loss=""mse"", optimizer=Adam(lr=LEARNING_RATE))

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() < self.exploration_rate:
            return random.randrange(self.action_space)
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def experience_replay(self):
        if len(self.memory) < BATCH_SIZE:
            return
        batch = random.sample(self.memory, BATCH_SIZE)
        state_batch, q_values_batch = [], []
        for state, action, reward, state_next, terminal in batch:
            # q-value prediction for a given state
            q_values_cs = self.model.predict(state)
            # target q-value
            max_q_value_ns = np.amax(self.model.predict(state_next)[0])
            # correction on the Q value for the action used
            if terminal:
                q_values_cs[0][action] = reward
            else:
                q_values_cs[0][action] = reward + DISCOUNT_FACTOR * max_q_value_ns
            state_batch.append(state[0])
            q_values_batch.append(q_values_cs[0])
        # train the Q network
        self.model.fit(np.array(state_batch),
                        np.array(q_values_batch),
                        batch_size = BATCH_SIZE,
                        epochs = 1, verbose = 0)
        self.exploration_rate *= EXPLORATION_DECAY
        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)

def lunar_lander():
    env = gym.make(ENV_NAME)
    observation_space = env.observation_space.shape[0]
    action_space = env.action_space.n
    dqn_solver = DQNSolver(observation_space, action_space)
    episode = 0
    print(""Running"")
    while True:
        episode += 1
        state = env.reset()
        state = np.reshape(state, [1, observation_space])
        scores = []
        score = 0
        while True:
            action = dqn_solver.act(state)
            state_next, reward, terminal, _ = env.step(action)
            state_next = np.reshape(state_next, [1, observation_space])
            dqn_solver.remember(state, action, reward, state_next, terminal)
            dqn_solver.experience_replay()
            state = state_next
            score += reward
            if terminal:
                print(""Episode: "" + str(episode) + "", exploration: "" + str(dqn_solver.exploration_rate) + "", score: "" + str(score))
                scores.append(score)
                break
        if np.mean(scores[-min(100, len(scores)):]) >= 195:
            print(""Problem is solved in {} episodes."".format(episode))
            break
    env.close
if __name__ == ""__main__"":
    lunar_lander()

Here are the logs
root@b11438e3d3e8:~# /usr/bin/python3 /root/test.py
2021-01-03 13:42:38.055593: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-01-03 13:42:39.338231: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-01-03 13:42:39.368192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-03 13:42:39.368693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.8095GHz coreCount: 20 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 298.32GiB/s
2021-01-03 13:42:39.368729: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-01-03 13:42:39.370269: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-01-03 13:42:39.371430: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-01-03 13:42:39.371704: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-01-03 13:42:39.373318: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-01-03 13:42:39.374243: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-01-03 13:42:39.377939: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-01-03 13:42:39.378118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-03 13:42:39.378702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-03 13:42:39.379127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-01-03 13:42:39.386525: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3411185000 Hz
2021-01-03 13:42:39.386867: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fb44c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-03 13:42:39.386891: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-01-03 13:42:39.498097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-03 13:42:39.498786: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fdf030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-01-03 13:42:39.498814: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1
2021-01-03 13:42:39.498987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-03 13:42:39.499416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1
coreClock: 1.8095GHz coreCount: 20 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 298.32GiB/s
2021-01-03 13:42:39.499448: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-01-03 13:42:39.499483: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-01-03 13:42:39.499504: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-01-03 13:42:39.499523: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-01-03 13:42:39.499543: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-01-03 13:42:39.499562: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-01-03 13:42:39.499581: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-01-03 13:42:39.499643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-03 13:42:39.500113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-03 13:42:39.500730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-01-03 13:42:39.500772: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-01-03 13:42:39.915228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-03 13:42:39.915298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2021-01-03 13:42:39.915322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2021-01-03 13:42:39.915568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-03 13:42:39.916104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-03 13:42:39.916555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6668 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
Running
2021-01-03 13:42:40.267699: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10

This is the GPU stats
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |
|  0%   53C    P2    46W / 198W |   7718MiB /  8111MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

As you can see, TensorFlow does not compute on the GPU but reserves the memory so I'm assuming it's because the inputs of the neural networks are too small and it uses the CPU instead.
To make sure the GPU was installed properly, I ran a sample from their documentation and it uses the GPU.
Is it an issue with the algorithm or the code? Is there a way to utilize the GPU in this case?
Thanks!
","['reinforcement-learning', 'deep-learning', 'tensorflow', 'q-learning', 'open-ai']",
How to derive compact convex set K and its diameter D to program Accelegrad algorithm in practice?,"
Given the original paper (https://arxiv.org/pdf/1809.02864.pdf), I would like to implement the Accelegrad algorithm for which I report the pseudocode of the paper:

In the pseudocode, the authors refer to a compact convex set $K$ of diameter $D$. The question is whether I can know such elements. I think that they are theoretical conditions to satisfy some theorems. The problem is that the diameter $D$ is used in the learning rate and also the convex set $K$ is used to perform the projection of the gradient descent. How can I proceed?
","['machine-learning', 'deep-learning', 'papers', 'optimization', 'gradient-descent']",
"How can I perform the forward pass in a neural network evolved with NEAT, given that some connections may not exist or there may be loopy connections?","
I have a problem that arose as part of a NEAT (Neuro Evolution Through Augmenting Topologies) implementation that I am writing. I am wanting it to produce topologies or graphs that describe neural networks, similar to the one below.

Here, nodes 0 and 1 are inputs, and 4 is the output node, the rest of the nodes are hidden nodes. Each of these nodes can have some activation function defined for them (not necessary that all the hidden nodes have the same activation function)
Now, I want to perform the forward pass of this neural network with some data, and, based on how well it performed in that task, I assign it with a fitness value, which is used as part of the NEAT evolutionary algorithm to move towards better architectures and weights.
So, as part of the evolution process, I can have connections that can cause internal loops in the hidden layers and there is the possibility that a skip connection is made. Because of this, I feel the regular matrix-based forward pass (of fully connected MLPs) will not work in order to perform the forward pass of these evolved neural networks, and hence I want to know if an algorithm exists that can solve this problem.
In short, I want this neural network to just take the inputs and provide me outputs - no training involved at all, so I'm not interested in the back-propagation part now.
The only way to solve this that I see is to use something on the lines of a job queue (the queue will consist of the nodes that needs processing in order). I feel this is extremely inefficient and I cannot allocate this simulation method a proper stop condition. Or, even when to take output from the neural network graph and consider it.
Can anybody at least point me in the right direction?
","['neural-networks', 'neat', 'neuroevolution', 'fitness-functions', 'forward-pass']",
Is training on single game each time appropriate for an agent to learn to play checkers,"
I was facing a problem I mentioned in a previous question but after a while, I realize that maybe the problem in the dataset not in the learning rate.
I build the dataset from white positions only i.e the boards when it's white's turn.
Each data set consists of one game.
First, I tried to let the agent plays a game then learn from it immediately, but that did not work,
and the agent converges to one state of playing (he only lose or win against itself, or draw in a stupid way).
Second, I tried to let the agent plays about 1000 games against its self then train on each game separately, but that also does not work.
Note: the first and second approach describes one iteration of the learning process, I let the agent repeat them so in total it trained on about 50000 games.
Is my approach wrong? or my dataset must be built in another way? maybe train the agent on several games at once?
My project is here if someone needs to take a closer look at it: CheckerBot
","['machine-learning', 'reinforcement-learning', 'training', 'datasets', 'checkers']",
Training a classifier on different datasets with different image conditions for different labels causes the model to infer using the background,"
I have an interesting problem related to training the model on two different datasets for the target feature on images taken on different conditions, which might affect the model's ability to generalize.
To explain I will give examples of images from the two different datasets.
Dataset 1 sample image:

Dataset 2 sample image:

As you see the images are captured in two completely different conditions. I am afraid that the model will infer from the background information that it shouldn't use to predict the plant diseases, what makes the problem worse is that some plant diseases only exist in one dataset and not the other, if all the diseases are contained in both datasets then I wouldn't think there would be a problem.
I am assuming I need a way to unify the background by somehow detecting the leaf pixels in the images and unifying the background in a way that makes the model focuses on the important features.
I've tried some segmentation methods but the methods I tried don't always give desirable results for all the images.
What is the recommended approach here? All help is appreciated
Further explanation of the problem.
Ok so I will explain one more thing, my model on the two datasets works fine when training and validating, It got a 94% accuracy.
The problem is, even though the model performs well on the datasets I have, I am afraid that when I use the model on real-life conditions (say someone capturing an image with their phone) the model will be heavily biased towards predicting labels in the second dataset (the one with actual background) since the background is similar and it somehow associated the background with the inference process.
I have tried downloading a leaf image of a label that is contained on the first dataset ( the one with the white background), where the image had a real-life background, the model as expected failed to predict the correct label and predicted a label contained in the second dataset, I am assuming it was due to the background. I have tried this experiment multiple times, and the model consistently failed in similar scenarios
I used some Interpretability techniques as well to visualize the important pixels and it seems like the model is using the background for inference, but I am not an expert in interpreting these graphs so I am not 100% sure.




","['deep-learning', 'computer-vision', 'image-recognition', 'image-segmentation', 'domain-adaptation']","I am afraid that the model will infer from the background information that it shouldn't use to predict the plant diseases, what makes the problem worse is that some plant diseases only exist in one dataset and not the otherI am afraid that when I use the model on real-life conditions (say someone capturing an image with their phone) the model will be heavily biased towards predicting labels in the second dataset (the one with actual background) since the background is similar and it somehow associated the background with the inference process.You are right to be concerned about this. However, you are not alone in having a neural network model that relies heavily on the background or other confounding factors for your classification task.A small paper that addresses a similar issue is Context Augmentation for Convolutional Neural Networks. Table 1 from this paper shows how the model trained and tested on only the background performs better than model trained and tested on only the foreground. They address this by making a training set where the foreground objects are given many backgrounds. Given your imbalanced datasets, with some diseases only present in one dataset and not the other, I think context augmentation might work for you. The main barrier here is that you do not have segmentation maps for these. Are your datasets too large to segment yourself? You might consider automatic segmentation networks - especially for the images with white backgrounds.As for your interpretability methods, you should not expect too much from them though some groups are able to get good results with them using GradCAM.Here is an alternative idea that may also solve your problem: If you are trying to make a system that can be used in the field, is it possible to have the technician or photographer put a white board under the diseased leaf?"
Does it make sense to include constant states into reinforcement learning formulation?,"
Does it make sense to incorporate constant states in the Markov Decision Process and employ a reinforcement learning algorithm to solve it?
For instance, for applications of personalization, I would like to include users' attributes, like age and gender, as states, along with other changing states. Does it make sense to do so? I have a sequential problem, so I assume the contextual bandit does not fit here.
","['reinforcement-learning', 'state-spaces']","It is, I suppose, a philosophical question whether data that describes a whole episode and does not respond to events within it is part of the state, or is part of some other structure.However, the practical response is to view such descriptive data as defining an instance of a class of related environments, and to include it in the state features. This may be done for two main reasons:The static data is a relevant parameter of the environment, affecting state transitions and rewards.It is possible to generalise over the population of all values that the parameters can take.In simple environments, generalisation might only be that the same agent can learn about all variations in a single combined training session. You could use a tabular RL method, starting randomly with one of the possible variations until all were sufficiently covered.In more complex environments, generalisation may also occur through functon approximation, in a similar manner to contextual bandits. In your personalisation example, you are not expecting to train the agent for all possible user descriptions, but hope that people with similar age, gender etc descriptions will respond similarly to an agent that personalises content.Philosophically, the contextual data is either part of a larger state space (with a restriction that transitions between different contexts do not happen within an episode), or it is metadata that impacts the ""real"" state transitions and rewards. Pragmatically, to allow the data to influence value functions and policies, it is necessary to use it in the arguments of those functions. Whether you then view it as part of the state feature vector or as something that is concatenated to state features is a personal choice. Most of the literature I have seen assumes without comment that it is part of the state."
Is there any research work that shows that we should explicitly mark the word boundaries for 1D CNNs?,"
I'm doing character embedding for NLP tasks using one-dimensional convolutional neural networks (see Chiu and Nichols (2016) for the motivation). I haven't found any empirical evidence of whether or not marking the word boundaries makes a difference. As an example, a 1-D CNN with kernel size 2 would take ""the"" as input and use {""th"", ""he""} in its filters. But if I explicitly marked the boundaries it would give me {""t"", ""th"", ""he"", ""h""}.
Is there a go-to paper or project that definitively answers this question?
","['neural-networks', 'convolutional-neural-networks', 'natural-language-processing', 'reference-request', 'embeddings']",
What is the smallest upper bound for a number of functions in a range that are computable by a perceptron?,"
I'm reading this book chapter, and I'm looking at the questions on the last page. Can someone explain question 2 on the last page to me, or show me an example of a solution so I can understand it?
The question is:

Consider a simple perceptron with $n$ bipolar inputs and threshold $\theta = 0$. Restrict each of the weights to have the value $−1$ or $1$. Give the smallest upper bound you can find for the number of functions from $\{−1, 1 \}^n$ to $\{−1, 1\}$ which are computable by this perceptron. Prove that the upper bound is sharp, i.e., that all functions are different.

What I understand:

A perceptron is a very simple network with $n$ input nodes, a weight assigned to each input nodes, which are then summed to be above/not above a threshold ($\theta$).

In this example, there are $n$ input nodes, and the value of each input node is either $−1$ or $1$. And we want to map them to outputs of either $−1$ or $1$.


What I'm confused about:
Is it asking how many different ways can you map input values of $\{−1, 1\}$ to $\{−1, 1\}$ output?
For example, is the answer, where each tuple in this list is input1, input2 and label, as described above:
$$[(1,1,1), (1,1,-1), (-1,1,-1), (-1,1,1), (1,-1,1), (1,-1,-1), (-1,-1,-1)]$$
","['machine-learning', 'function-approximation', 'perceptron']",
What is the number of neurons required to approximate a polynomial of degree n?,"
I learned about the universal approximation theorem from this guide. It states that a network even with a single hidden layer can approximate any function within some bound, given a sufficient number of neurons. Or mathematically, ${|g(x)−f(x)|< \epsilon}$, where ${g(x)}$ is the approximation, ${f(x)}$ is the target function and  is $\epsilon$ is an arbitrary bound.
A polynomial of degree $n$ has at maximum $n-1$ turning points (where the derivative of the polynomial changes sign). With each new turning point, the approximation seems to become more complex.
I'm not necessarily looking for a formula, but I'd like to get a general idea on how to figure out the sufficient number of neurons is for a reasonable approximation of a polynomial with a single layer of the neural network (you may consider ""reasonable"" to be $\epsilon = 0.0001$). To ask in other words, how would adding one more neuron affect the model's ability to express a polynomial?
","['neural-networks', 'computational-learning-theory', 'multilayer-perceptrons', 'function-approximation', 'universal-approximation-theorems']",
Why is the derivative of the softmax layer shaped differently than the derivative of other neurons?,"
If the derivative is supposed to give the rate of change of a function at that point, then why is the derivative of the softmax layer (a vector) the Jacobian matrix, which has a different shape than the output/softmax vector? Why is the shape of the softmax vector's derivative (the Jacobian) different than the shape of the derivative of the other activation functions, such as the ReLU and sigmoid?
","['backpropagation', 'objective-functions', 'gradient-descent', 'linear-algebra', 'softmax']","When you use the softmax activation function is usually as a last layer of your network and to get an output that is a vector. Now your confusion is about shapes, so let's review a bit of calculus.
If you have a function $$f:\mathbb{R}\rightarrow\mathbb{R}$$
the derivative is a function on its own and you have $$f':\mathbb{R}\rightarrow\mathbb{R}.$$
If you increase the dimension of the input space have
$$f:\mathbb{R}^n\rightarrow\mathbb{R}.$$
The ""derivative"" in this case is called gradient and it is a vector collecting the $n$ partial derivatives of $f$. The input space of the gradient function is $\mathbb{R}^n$ (the same as for $f$), but the output is the collection of the $n$ derivatives, so the output space is also $\mathbb{R}^n$. In other words
$$\nabla f:\mathbb{R}^n\rightarrow\mathbb{R}^n,$$
which makes sense as for each point $x$ of the input space you get a vector ($\nabla f(x)$) as output.
So far so good, but what happens if you consider a function that takes a vector as input and spits out a vector as output, i.e.
$$f:\mathbb{R}^n\rightarrow\mathbb{R}^m?$$
How to compute the equivalent of the derivative? (This is the softmax case, where you have a vector as input and a vector as output.)
You can reduce this case to the previous case considering $f=(f_1, \dots, f_m)$, where $f_i:\mathbb{R}^n\rightarrow\mathbb{R}.$ Now for each $f_i$ you can compute the gradient $\nabla f_i$ and end up with $m$ gradients. When you evaluate them at a point $x\in\mathbb{R}^n$ you get $m$ $n-$dimensional vectors. These vector can be collected in a matrix, which is the Jacobian; formally
$$Jf:\mathbb{R}^n\rightarrow\mathbb{R}^{m\times n}.$$
Finally, to answer your question, you get a Jacobian ""instead""  of a gradient (they all represent the same concept) because the output of the softmax is not a single number but a vector.
By the way the sigmoid and relu are functions with one dimensional input and output, so they don't really have a gradient but a derivative. The trick is that people write $\sigma(W)$, where $W$ is vector or a matrix, but they mean that $\sigma$ is applied component-wise, as a function from $\mathbb{R}$ to $\mathbb{R}$ (I know, it's confusing).
(I know, I kind of skipped your other question, but this answer was already long and I think you can convince yourself that the dimensions match correctly (with the Jacobian) for the update rule to work. If not I'll edit)"
What is the derivative of a specific output with respect to a specific weight?,"
If I have a neural network, and say the 6th output node of the neural network is:
$$x_6 = w_{16}y_1 + w_{26}y_2 + w_{36}y_3$$
What does that make the derivative of:
$$\frac{\partial x_6}{\partial w_{26}}$$
I guess that it's how is $x_6$ changing with respect to $w_{26}$, so, therefore, is it equal to $y_2$ (since the output, $y_2$, will change depending on the weight added to the input)?
","['neural-networks', 'gradient-descent', 'calculus']","Formally speaking $x_6$ is a function of $w_{16},\ w_{26}$ and $w_{36}$, that is $$x_6 =f(w_{16}, w_{26}, w_{36})=w_{16}y_1 + w_{26}y_2 + w_{36}y_3.$$
The derivative w.r.t. $w_{26}$ is
$$\frac{\partial x_6}{\partial w_{26}}= \frac{\partial w_{16}y_1}{\partial w_{26}} +\frac{\partial w_{26}y_2}{\partial w_{26}}  +\frac{\partial w_{36}y_3}{\partial w_{26}} = 0 +y_2 \frac{\partial w_{26}}{\partial w_{26}} + 0= y_2.$$
The first equality is obtained using the fact that the partial derivative is linear (so the derivative of the sum is the sum of the derivatives); the second equality comes from again from the linearity and from the fact that $w_{16}y_1$ and $w_{36}y_3$ are constants with respect to $w_{26}$, so their partial derivative w.r.t. this variable is $0$.BonusNot really asked in the original question, but since I'm here let me have a bit of fun ;).Let's say $x_6$ is the output of the sixth node after you apply an activation function, that is $$x_6 =\sigma(f(w_{16}, w_{26}, w_{36}))=\sigma(w_{16}y_1 + w_{26}y_2 + w_{36}y_3).$$
You can compute the partial derivative applying the properties illustrated above, with the additional help of the chain rule
$$\frac{\partial x_6}{\partial w_{26}}=\frac{\partial \sigma(w_{16}y_1 + w_{26}y_2 + w_{36}y_3)}{\partial w_{26}}=\sigma'\frac{\partial w_{16}y_1}{\partial w_{26}} +\sigma'\frac{\partial w_{26}y_2}{\partial w_{26}}  +\sigma'\frac{\partial w_{36}y_3}{\partial w_{26}}=y_2\sigma'$$
$\sigma'$ denotes the derivative of sigma with respect to its argument."
How to draw a 3-dimensonal shape's neural network,"
I am reading an exam question about NN (that I cannot publish, for copyright reasons). The question says: 'Construct a rectangle in 2D space. Define the lines, and then define the weights and threshold that will only fire for points inside the rectangle.'
I understand that this is an example of a rectangle drawn as a NN (i.e. this NN will fire, if the point is in the rectangle, where the rectangle is defined by the lines X = 4; X = 1, Y = 2, Y = 5).

In this diagram, since it's a rectangle, the equations of the line in this example are x = 4, x =1, y=2, y=5, so I left the other weights out (as they equal to 0).
I'm now wondering how this could be translated to a 3D structure. For example, if a 3D shape was defined by the points:
(0,0,0), (0,1,0), (0,0,1), (0,1,1), (1,0,0), (1,1,0), (1,0,1), (1,1,1)
I wanted to draw a hyperplane that separates the corner point (1,1,1) from the other points in this cube. Can this 3D shape be drawn similarly to below (maybe it would be easier to understand, if there were other numbers except 1 and 0 in the co-ordinates)?
Would I draw this with 3 nodes in the input layer, still one node in the output layer, I just don't understand what the hidden layer should look like? Would it have 24 nodes? One for each surface of the cube, with relevant X and Y values?
","['neural-networks', 'multilayer-perceptrons']",
What are the general inequalities needed for the logic gate perceptrons?,"
I'm trying to understand how the logic gates (e.g. AND, OR, NOT, NAND) can be built into single-layer perceptrons.
I understand specific examples of weights and thresholds for the gates, but I'm stuck on understanding how to translate these to general inequalities for these gates.
Is my reasoning correct in the table below, or are there cases where these general inequalities do not make sense for this problem? Am I missing other logic gates that can be done in a similar fashion? (e.g., I know XOR cannot)?
In the table below, a perceptron has two input nodes, and one output node. W1 and W2 are the weights (real values) on those input nodes. T is the threshold, above which, the perceptron will fire. I have come up with example values that would work for each logic gate (e.g., for the AND gate, a perceptron with two input weights, W1 = 1 and W2 = 1, and a threshold = 2, will fire, and I'm trying to understand more generally, what is the equation needed for each gate).




Gate
Example W1, W2
Threshold
General inequalities




AND
1,1
2
W1 + W2 >= t, where W1, W2 > 0


OR
1,1
1
W1 > t or W2 > t


NOT
-1
-0.49
W1 > 2(t)


NAND
-2,-2
3
W1 + W2 <= t



 ","['machine-learning', 'perceptron']",
A way to leverage machine learning to reduce DFS/BFS search time on a tree graph?,"
I'm not very knowledgeable in this field but I'm wondering if any research or information already exists for the following situation:
I have some data that may or may not look similar to each other. Each represents a node that represents a vector of size 128.

And they are inserted into a tree graph according to similarity.

and a new node is created with an edge connecting the most similar vertex node found in the entire tree graph.
Except I'm wasting a lot of time searching through the entire graph to insert each new node when I could narrow down my search according to previous information. Imagine a trunk node saying ""Oh, I saw a node like you before, it went down this branch. And don't bother going down that other branch."" I could reduce the cost of searching the entire tree structure if there was a clever way to remember if a similar node went down a certain path.
I've thought about some ways to use caching or creating a look-up table but these are very memory intensive methods and will become slower the longer the program runs on. I have some other ideas I am playing around with but I was hoping someone could point me in the right direction before I started trying out weird ideas.
Edit: added a better (more realistic) graph picture
","['machine-learning', 'deep-learning', 'breadth-first-search', 'depth-first-search']",
How is the latent vector transforming to a feature map in DCGAN (Generator structure)?,"
I'm working on the code trying to generate new images using DCGAN model. The structure of my code is from the PyTorch tutorial here.
I'm a bit confused trying to find and understand how the latent vector is transforming to the feature map from the Generator part (this line of code is what I'm interested in) :
nn.ConvTranspose2d(  nz, ngf * 8, 4, 1, 0, bias=False)

It means the latent vector (nz) of shape 100x1 transforming to the 512 matrices of 4x4 size (ngf=64). How  does it happen? Hence, I can't even clear to myself how the length of the latent vector influence to the generated image.
P.S. The left part of the Genarator structure is clear.
The only idea that I got is :

E.g. there is a latent vector of size 100 as input (100 random values).
We interact each value of the input latent vector with a 4x4 kernel.
In this way we get 100 different 4x4 matrices (each matrix is for one value from latent vector).
Then we summarize all these 100 matrices and get one final matrix - one feature map.
We get necessary number of feature maps taking different kernels.

Is this right? Or does it happen in other way?
","['neural-networks', 'convolutional-neural-networks', 'generative-adversarial-networks', 'dc-gan']","Noise vector (batch_size, 100, 1, 1) is deconvoloved with filter_1 (100, 4, 4). Result is feature_map_1 (1, 4, 4). And since there are 512 filters, so there will be 512 feature maps. Output shape will be (batch_size, 512, 4, 4).I think you need better undersanting for convolutional calculations in general. In this stack it was explained very well."
"When updating the state-action value in the Monte Carlo method, is the return the same for each state-action pair?","
Referring to this post, in the following formula to update the state-action value
$$ Q(s,a) = Q(s,a) + \alpha (G − Q(s,a)),$$
is the value of $G$ (the return) the same for every state-action $(s,a)$ pair?
I am a little confused about this point, so I will thank any clarification.
","['reinforcement-learning', 'monte-carlo-methods', 'return']","The discussion uses poor notation, there should be a time index. You obtain a list of tuples $(s_t, a_t, r_t, s_{t+1})$ and then, for every visit MC, you update$$Q(s_t, a_t) = Q(s_t, a_t) + \alpha (G_t - Q(s_t, a_t))\;;$$where $G_t = \sum_{k=0}^\infty \gamma^k r_{t+k}$, for each $t$ in the episode. You can see that the returns for each time step are calculated for time timestep onwards, and so are not necessarily the same across time steps."
Time series analysis using computer vision principles,"
I'm just starting to explore topics within computer vision and curious if there are any concepts in that area that could be applied to segmenting multivariate time series with the goal of grouping individual data points similar to how a human might do the same.  I know that there are a number of time series segmentation methods, but in-depth explanations of multivariate methods are more scarce and it seems like somewhat of an underdeveloped topic overall. Since segmentation is such a fundamental part of CV and is inherently multidimensional, I'm wondering if concepts there can be modified to apply to time series.
Specifically, I'd like to be able segment a time series and reformulate a prediction problem as something closer to a language processing problem.  The process would look something like this:

Segment a multivariate time series into near-homogenous segments of variable length.  Some degree of preprocessing might be required but I can worry about that separately.
Encode the properties of each segment based on summary statistics (e.g., mean, variance, derivative values, etc.) such that the segments fall into discrete buckets.
Each bucket will represent a ""word"" and the goal of the model will be to predict the next word given a series of words, i.e., the next segment given a series of segments.

In a few days of reading about CV, it seems like there's a ton to learn.  If there are traditional time series segmentation techniques that are more suitable, that would be of interest, but I'd still be curious about a CV approach since that approach likely better aligns with how a person might look at a graph to identify segments.
","['natural-language-processing', 'computer-vision', 'time-series']",
How to detect that the fitness landscape of a genetic algorithm is changing over time?,"
I understand that in each generation of a genetic algorithm, that generation must re-prove it's fitness (and then the fittest of that population is taken for the next population).
In this case, I guess it's a presumption that if you take the fittest of each generation, and use them to form the basis of the next generation, that your population as a whole is getting fitter with time.
But algorithmically, how can I detect this? If there's no end goal known, then I can't measure the error/distance from goal? So how can you tell how much each generation is becoming fitter by?
","['algorithm', 'genetic-algorithms']","There is no exact way to assess that a genetic algorithm has located a global optima. Indeed there may be multiple global optima. You must fall back to heuristic methods. The fitness of a population is the maximum fitness of any individual. Unless specific measures are taken to maintain diversity the population will converge to an optima, local or global. At that point all individuals will, except for mutation, be identical. You could take the fittest individual of such a population as your solution, but you will not know if the solution is a global or local optima.Two reasonable heuristics are these. First, run the algorithm till it converges and maintains its fitness for a number of further generations. Or second run the algorithm multiple times, and take the fittest of all the located solutions. Neither is exact."
Are there any disadvantages to using a variable population size in genetic algorithms?,"
When implementing a genetic algorithm, I understand the basic idea is to have an initial population of a certain size. Then, we pick two individuals from a population, construct two new individuals (using mutation and crossover), repeat this process X number of times and the replace the old population with the new population, based on selecting the fittest.
In this method, the population size remains fixed. In reality in evolution, populations undergo fluctuations in population sizes (e.g. population bottlenecks, and new speciations).
I understand the disadvantages of variable populations sizes from a biological view are, for example, a bottleneck will reduce the population to minimal levels, so not much evolution will occur. Are there disadvantages to using variable population sizes in genetic algorithms, from a programming perspective? I was thinking the numbers per population could follow a distribution of some sort so they don't just randomly fluctuate erratically, but maybe this does not make sense to do.
","['algorithm', 'genetic-algorithms']","Population size is a tricky issue even in pure biological models. Biological population sizes obviously vary. The two great protagonists of the argument were Ronald Fisher and Sewell Wright, with argument being between Fisher favouring few large populations against Wright’s many small interconnected populations. There is evidence that evolution occurs more rapidly in Wright’s model but the evidence is inconclusive. The theory concentrates on the probability that a mutation will occur and then become dominant in a population. In a small population a beneficial mutation is more likely to be selected for reproduction, but premature convergence is a serious danger.  While in a larger population a mutant is less likely to be removed from the population during reproduction. I would strongly recommend a read of Games of life by Karl Sigmund."
Backpropogation rule for the output layer of a multi-layer network - What does the rule do in ambiguous cases?,"
This is the back-propogation rule for the output layer of a multi-layer network:
$$W_{jk} := W_{jk} - C \dfrac{\delta E}{\delta W_{jk}}$$
What does this rule do in the more ambiguous cases such as:
(1) The output of a hidden node is near the middle of a sigmoid curve?
(2) The graph of error with respect to weight is near a maximum or minimum?
","['neural-networks', 'backpropagation']","I assume you are considering a network where the activation function of the last layer is a sigmoid, so the output of your network is $$\tilde{y}=\sigma(W^{L}\cdot f(X, W^1, \dots, W^{L-1})),$$
where $X$ is the input vector, and $f$ is obtained by feeding the input to the network up to the layer $L-1$. Let's also call $Z:= W^{L}\cdot f(X, W^1, \dots, W^{L-1})$.The error term is computed as $$E(y, \tilde{y})=E(y, \sigma(Z)),$$ where $y$ is the actual output. Let's get the derivative of the error with respect to the output of the last node (the input of the sigmoid)
$$\frac{\partial E}{z_i}=\frac{\partial E}{\partial \tilde{y}}\frac{\partial\tilde{y}}{\partial z_i}=\frac{\partial E}{\partial \tilde{y}}\frac{\partial\sigma}{\partial z_i}.$$
The update rule is $$z_i =z_i - C\frac{\partial E}{z_i}= z_i - C\frac{\partial E}{\partial \tilde{y}}\frac{\partial\sigma}{\partial z_i}.$$
Now we can analyse your questions.For instance, what happens if you are close to the center of the sigmoid but also close to an extremum of the loss? You will have a multiplication of $2$ terms, one trying to make the update small and the other trying to make the update large, and what matter are the orders of magnitude involved.
In conclusion, the rules of thumb are as in the points 1. and 2., but they are no guarantee that you won't find any special cases."
What is emperical distribution in MLE?,"
I was reading the book Deep Learning by Ian Goodfellow. I had a doubt in the Maximum likelihood estimation section (Pg 131). I understand till the Eq 5.58 which describes what is being maximized in the problem.
$$
 \theta_{\text{ML}} = \text{argmax}_{\theta} \sum_1^m \log(p_{\text{model}}(x^{(i)};\theta)) 
$$
However the next equation 5.59 restates this equation as:
$$
\theta_{\text{ML}} = \text{argmax}_{\theta}  E_{x \sim \hat{p}_{\text{data}}}(\log(p_{\text{model}}(x;\theta)) 
$$
where $$\hat{p}_{\text{data}}$$ is described as the empirical distribution defined by the training data.
Could someone explain what is meant by this empirical distribution? It seems to be different from the distribution parametrized by theta as that is described by $$ p_{\text{model}} $$
","['probability-distribution', 'maximum-likelihood']","The idea behind this kind of reasoning is that there is a ""true"" distribution (unknown to us, mere mortals) and that the data is generated following this distribution. But what we don't really know the shape of the distribution, all we know is the distribution of the data that we have. This is called the empirical distribution.   Let's see a simple example to illustrate the point.
Let's consider a die. Each number is equally likely to show up if we throw the die, so the true underlying distribution is uniform over the set $\{1, 2,\dots6\}$. Now let's say you ask your friend to throw the die 60 times, what you will see is likely something close to uniform over the set $\{1, 2,\dots6\}$ (not really uniform though, as that would be highly unlikely). This distribution is the empirical one, and as you collect more and more samples it will converge to the actual underlying distribution.
In your case what happened is the following:
$x_1,\dots, x_m$ is your sample (in the example above, the $60$ numbers that you see as your friend throws the die). This sample defines a distribution, $\hat{p}_{data}$. In the example above, $\hat{p}_{data}$ would likely be close to the uniform distribution over $\{1, \dots, 6\}$. Now you can think about the sum over $x_1\dots, x_m$ of $\log(p_{model}(x^{(i)};\theta))$ as the average of $\log(p_{model}(x;\theta))$, where $x$ is drawn according to $\hat{p}_{data}$.
Let me make another example with some actual numbers. Let's say you toss a fair coin $7$ times and you see $$\{H, H, H, T, H, T, H\}.$$
The empirical distribution is $\mathbb{P}(H)=5/7$, $\mathbb{P}(T)=2/7$. So if you compute the expectation $$\mathbb{E}_{x \sim \hat{p}_{data}}[\log(p_{model}(x;\theta)]$$ you get
$$\log(p_{model}(H;\theta)\cdot\mathbb{P}(H) +\log(p_{model}(T;\theta)\cdot\mathbb{P}(T) = \\ \frac{5}{7}\log(p_{model}(H;\theta) +\frac{2}{7}\log(p_{model}(T;\theta)),$$
which is what you will get if you compute the first sum you wrote."
How to design a NLP algorithm to find a food item in menu card list?,"
I am new to NLP and AI in general. I am just expecting springboard information so that I can skip all the introduction to NLP websites.  I have just started studying NLP and want to know how to go about solving this problem. I am creating a chatbot that will take voice input from customers ordering food at restaurants. The customer input I am expecting as;

I want to order Chicken Biryani


Can I have a Veg Pizza, please


Coca-cola  etc

I want to write an algorithm that can separate the name of the food item from the user input and compare it with the list of food items in my menu card and come up with the right item.
I am new to NLP, I am studying it online for this particular project, I can do the required coding, I just need help with the overall algo or sort of flow chart. It will save my time tremendously. Thanks.
","['machine-learning', 'natural-language-processing', 'algorithm']","Since you want a shortcut use the spoonacular API.  Below is a test with your words.  You can see it had trouble with 'Coca' and 'veg'.What you need is 'named-entity recognition' for food.  This is not a new thing but clearly not a solved problem.  The Foodie Favorites repository attempts to solve the problem from scratch.If want to do some research and dig deeper see FoodBase corpus: a new resource of annotated food entities.  From the abstract:It consists of 12,844 food entity annotations describing 2105 unique food entities. Additionally, we provided a weakly annotated corpus on an additional 21,790 recipes. It consists of 274,053 food entity annotations, 13,079 of which are unique."
Where is the mistake in my derivation of the GAN loss function?,"
I was pondering on the loss function of GAN, and the following thing turned out
\begin{aligned}
 L(D, G) 
& = \mathbb{E}_{x \sim p_{r}(x)} [\log D(x)] + \mathbb{E}_{x \sim p_g(x)} [\log(1 - D(x)] \\
& = \int_x \bigg( p_{r}(x) \log(D(x)) + p_g (x) \log(1 - D(x)) \bigg) dx \\ 
& =-\left[CE(p_r(x), D(x))+CE(p_g(x), 1-D(x)) \right] \\
\end{aligned}
Where CE stands for cross-entropy. Then, by using law of large numbers:
\begin{aligned}
L(D, G) 
& = \mathbb{E}_{x \sim p_{r}(x)} [\log D(x)] + \mathbb{E}_{x \sim p_g(x)} [\log(1 - D(x)] \\
& =\lim_{m\to \infty}\frac{1}{m}\sum_{i=1}^{m}\left[1\cdot \log(D(x^{(i)}))+1\cdot \log(1-D(x^{(i)}))\right]\\
& =- \lim_{m \to \infty} \frac{1}{m}\sum_{i=1}^{m} \left[CE(1, D(x))+CE(0, D(x))\right]
\end{aligned}
As you can see, I got a very strange result. This should be wrong intuitively because in the last equation first part is for real samples, and the second is for generated samples. However, I am curious about where are the mistakes?
(Please explain with math).
","['deep-learning', 'objective-functions', 'generative-adversarial-networks', 'generative-model', 'cross-entropy']","I guess the issue is you lost track of where the samples came from and since you requested a math explanation I'll try to go step by step using my notation and without checking other material to avoid being biased by how other authors present itSo we start from$$ L(D,G) = E_{x \sim p_{r}(x)} \log(D(x)) + E_{x \sim p_{g}(x)}\log(1 - D(x)) $$then you apply the definition of $E_{\cdot}(\cdot)$ operator in the continuous case$$ L(D,G) = \int_{x} \log(D(x)) p_{r}(x)dx + \int_{x}\log(1 - D(x))p_{g}(x)dx $$then you Monte Carlo sample it to approximate it$$ L(D,G) = \frac{1}{n} \sum_{i=1}^{n} \log(D(x_{i}^{(r)}))  + \frac{1}{m} \sum_{j=1}^{m}\log(1 - D(x_{j}^{(g)})) $$As you can see here I have kept the samples from the 2 distributions separated and used a notation that allows to track their origin so now you can use the right label in the Cross Entropy$$ L(D,G) = \frac{1}{n} \sum_{i=1}^{n} L_{ce}(1, D(x_{i}^{(r)}))  + \frac{1}{m} \sum_{j=1}^{m} L_{ce}(0, D(x_{j}^{(g)})) $$But you could also have decided to merge the 2 integrals before to have$$ L(D,G) = \int_{x} \left( \log(D(x)) p_{r}(x) + \log(1 - D(x))p_{g}(x) \right) dx $$which is mathematically legit operation, however the issue is when you try to
discretize this with Monte Carlo sampling.You can't just replace the integral with one sum since you are Monte Carlo sampling and here, contrary to what we have done above, you do not have 1 distribution per integral to sample but in the same integral you have 2 distributions and for each sample you have to say what distribution it comes from which is where the issue is in your notation since you lost track of this information and it seems all the samples come from one distribution"
Does the alpha/beta value of parent nodes change if the alpha beta value of the child node changes?,"
I want to do alpha-beta pruning on this tree:

Consider nodes J and K. K is the max. Therefore, node D has an alpha value of 20, node B has a beta value of 20.

Move to Node E. Pass the beta value of 20 to node E. Node L has an alpha value of 30, therefore, at this point 30 (alpha) > 20 (beta) and we can prune the E to M branch.

Now is my question. My original beta value at node B was 20, and the alpha value passed up to node A was 20. Then, in step 2, I changed the alpha value to be 30. Do i then change the beta value at node B to be 30, and the alpha value at node A to be 30? (and therefore pass 30 as the alpha value to node C)? Or do I keep the original value of 20 at node B, A and C?



","['minimax', 'alpha-beta-pruning']",
learning rate and credit assignment problem in checkers,"
I have implemented an AI agent to play checkers based on the design written in the first chapter of 
Machine Learning, Tom Mitchell, McGraw Hill, 1997.
We train the agent by letting it plays against its self.
I wrote the prediction to get how good a board is for white, so when the white plays he must choose the next board with the maximum value, and when black plays it must choose the next board with the minimum value.
Also, I let the agent explore other states by making it choose a random board among the valid next boards, I let that probability to be equal to $0.1$.
The final boards will have training values:

100 if this final board is win for white.


-100 if this final board is lose for white.


0 if this final board is draw.

The intermediate boards will have a training value equal to the prediction of the next board where it is white turn.
The model is based on a linear combination of some features (see the book for full description).
I start by initializing the parameters of the model to random values.
When I train the agent, he lost against himself always or draw in a stupid way, but the error converges to zero.
I was thinking that maybe we should let the learning rate smaller (like 1e-5), and when I do that the agent learns in a better way.
I think that this happened because of the credit assignment problem, because a good move may appear in a loose game, therefore, considered a loose move, so white will never choose it when he plays, but when we let the learning rate to be very small that existence of a good move in a losing game will change its value by a very little amount, and that good move should appear more in win games so its value converges to the right value.
Is my reasoning correct? and if not so what is happened?
","['machine-learning', 'reinforcement-learning', 'intelligent-agent', 'checkers', 'credit-assignment-problem']",
Considerations when doing image classification where the object is not the subject,"
I've come across two types of image classification tasks

cat/dog classification the whole picture is either a cat or a dog. Simple.
this image contains a cat classification. There's a whole chaotic scene, and the image may contain a cat nestled in there somewhere.

Type 2 seems to be way more prevalent in real life application. Here are just some examples:

Determining the sex of an insect. Maybe the male and female look pretty much the same, but the male has a small bump in some location that takes up a tiny part of the image.
Determining the presence of an animal call in an audio spectrogram.
Finding defects in road surfaces.

My question is: for task type 2, what are some key modifications we'd make to the normal approach of post-training on an ImageNet trained architecture like Resnet? Shouldn't the architecture be modified somehow to be more fitted to task type 2?
Before someone mentions using object detection algorithms I'd like to add the rule that we only have global image labels, not bounding box annotation or co-ordinates of any sort.
","['convolutional-neural-networks', 'image-recognition']",
Transformers: How to use the target mask properly?,"
I try to apply Transformers to an unusual use case - predict the next user session based on the previous one. A user session is described by a list of events per second, e.g. whether the user watches a particular video, clicks a specific button, etc. Typical sessions are around 20-30 seconds, I pad them to 45 seconds. Here's a visual example of 2 subsequent sessions:

x axis is time in seconds, y axis is the list of events (black line divides the 2 sessions). I extend the vocabulary with 2 additional tokens - start and end of a session (<sos> and <eos>), where <sos> is a one-hot vector at the very beginning and <eos> - similar vector at the end of the session (which makes this long red line).
Now I use these extended vectors of events as embeddings and want to train a Transformer model to predict the next events in the current session based on previous events in this (target) session and all events in the previous (source) session. So pretty much like seq2seq autoregressive models, but in a bit unusual settings.
Here's the problem. When I train a Transformer using the built-in PyTorch components and square subsequent mask for the target, my generated (during training) output is too good to be true:

Although there's some noise, many event vectors in the output are modeled exactly as in the target. After checking train-val-test split is correct, my best guess is that the model cheats by attending to the same day in the target, which the mask should have prevented. The mask is (5x5 version for brevity):
[[0., -inf, -inf, -inf, -inf],
 [0., -inf, -inf, -inf, -inf],
 [0., 0., -inf, -inf, -inf],
 [0., 0., 0., -inf, -inf],
 [0., 0., 0., 0., -inf]]

Note that since I use <sos> in both - source and target - mask[i, i] is set to -inf (except for mask[0, 0] for numerical reasons), so the output timestamp i should not attend to the target timestamp i.
The code for the model's forward method:
def forward(self, src, tgt):
    memory = self.encoder(src)
    out = self.decoder(tgt, memory, self.tgt_mask.type_as(tgt))
    out = torch.sigmoid(out)
    return out

I also tried to avoid the target mask altogether and set it to all -inf (again, except for the first column for numerical stability), but the result is always the same.
Am I using the mask the wrong way? If the mask looks fine, what other reasons could lead to such a ""perfect"" result?

After shifting the target to the right as suggested in the accepted answer I get the following result:

Which is much more realistic. One suspicious thing is that out[t] now resembles tgt[t - 1], but it can be explained by the fact that the user state tends to be ""sticky"", e.g. if a user watches a video at t - 1, most likely he will watch it at t as well.
","['pytorch', 'transformer']","The main issue during training is that you haven't right-shifted the input of the decoder, which is probably why you set the diagonals of mask to -inf (when it should be $0$).Also, just an FYI, although you haven't focused on evaluation/prediction yet, I will explain the evaluation/prediction here as well for completeness, since it works so differently than training, and also since you will need it when generating the graphs for debugging.Both tgt and tgt_mask should be changed to simulate auto-regressive properties.You are feeding in tgt as the input to the decoder, where tgt is the ground truth target sequence to predict. tgt should have dimension length(sequence) x batchSize x $\|dict\|$. Additionally, you are feeding in mask where the diagonals are -inf.Instead, you should do the following:Note that you only run the decoder once per training batch during training, i.e. the decoder simultaneously predicts the logits of all length(sequence) tokens at the same time. On the other hand, during evaluation/prediction, you must run the decoder length(sequence) times, since you can only use the decoder to predict one token at a time.During evaluation/prediction, the model should generate its own sentence entirely from scratch, one word at a time:During evaluation, you can calculate the validation loss by using the logits of only the final iteration of the decoder (which should have dimension length(sequence)+1 x batchSize x $\|dict\|$), and compare those logits with the ground truth with the <END> token added at the end (resulting in dimensions length(sequence)+1 x batchSize x $\|dict\|$)."
How does general image background removal AI work?,"
I'm well aware of the inner workings of CNN models for object detection, and although I've not worked on a semantic segmentation problem I can imagine how it works.
With these types of models, we need to say ""segment out the humans"", or ""segment out the X"". But what about when I say something like ""segment out the subject of this photo, whatever it happens to be"". For example, see this service: https://removal.ai/
Without too much imagination I might guess that they apply a multiclass segmentation model and just show any foreground pixels, no matter what class they belong to. So we'd hope that the subject is in one of the classes that the model was trained for, and that there are no other class instances in the image that shouldn't be captured. But is there a more general way?
","['computer-vision', 'image-segmentation']",
How do we interpret the images of weights in logistic regression,"
The following images are
a) The weights of a logistic regression model trained on MNIST.
b) The sign of the weights of a logistic regression
How do these images  represent the weights?
Would be grateful for any help.
Source of the research paper

","['weights', 'logistic-regression']",
Is there multi-agent reinforcement learning model in which (some of the) reward is given by other agent and not by the external environment?,"
The traditional setting of multiagent reinforcement learning (MARL) is the mode in which there is set of agents and external environment. And the reward is given to each agent - individually or collectively - by the external environment.
My question is - is there MARL model in which the reward is given by one agent to the other agent, meaning that one agent is incurring costs and other agent - revenue (or maybe even a profit?
Effectively that means distributed supervision: only some agents face the environment with real reward/supervision and then this supervision is more or less effectively propgated to other agents that learn/do their own specialized tasks that are part of collective task ececuted/solved distributively in MARL.
","['reinforcement-learning', 'rewards', 'multi-agent-systems']",
Output volume proof for convolutional neural network,"
As I've been dabbling into the sliding window concept, I stumbled on a question that asked me to find the number of windows needed on a 1D image of $W$ size, knowing the window size $K$ and the stride $S$.
As much as I tried, I couldn't find a formula by myself (the closest I got was this one : $N=\frac{W + x(K-S)}{K}$ where $x$ was the number of overlapping rectangle zones, which seemed to be $x=N-1$ but the reccurence wasn't what I was looking for and it could be wrong as I was reasoning through induction).
I find the right formula on Internet at last (this one : $N=\frac{W-K+2P}{S}+1$ with $P$ the padding but my problem didn't needed one) but I can't find the proof of it.
Is there any place where I could find the proof ?
","['neural-networks', 'convolutional-neural-networks']","You can think about the problem in the following way (without padding, as the padding case is a simple extension of base case with $\tilde{W}:=W + 2P$).
You want to know how many windows are necessary to cover an image of size $W$, given a window of size $K$ and stride $S$. So your image is a vector with indices $1, 2\dots, W$; as you put the first window on the image, the window will cover the indices from $1$ to $K$. As we apply stride (meaning that we translate the window), we will  get a sequence considering the last covered index. The first element of this sequence is $i_1=K$, where the $1$ indexing $i$ is not the number of times we apply stride but the number of covering windows we have. So in this first case applying no stride and we have 1 covering window. Applying stride once to our window, the window will cover the indices form $S$ to $K+S$, so $i_2=K+(2 -1 )S$. In general you get $i_n=K+(n-1)S$.
Now, if we can exactly cover $W$ then there is a number $N$ such that $i_N=W$. This means $$K+(N-1)S=W,$$ which rearranging gives $$N=\frac{W-K}{S} + 1.$$"
Training speed in GPU vs CPU for LSTM,"
I was experimenting seq2seq model which was the bi-LSTM encoder/decoder with attention. When I compared the training times on GPU vs CPU while varying the batch size, I got


CPU on the small batch size (32) is fastest. It's even ~1.5 time
faster than GPU using the same batch size.
When I increase the batch size (upto 2000), GPU becomes faster than CPU due to the
parallelization.

(2) looks reasonable to me. However, I am a bit perplexed by the observation (1). The average sequence length is around 15~20. Even with the batch size 32, I expected that GPU should be faster but it turned out not. I used PyTorch LSTM.
Does it look normal? In RNN-style seq2seq, could CPU be faster than GPU?
","['deep-learning', 'recurrent-neural-networks']",
"In the machine learning literature, what does it mean to say that something is ""embedded"" in some space?","
In the machine learning literature, I often see it said that something is ""embedded"" in some space. For instance, that something is ""embedded"" in feature space, or that our data are ""embedded"" in dot product space, etc. However, I've never actually seen an explanation of what this is supposed to mean. So what does it actually mean to say that something is ""embedded"" in some space?
","['machine-learning', 'terminology', 'definitions', 'embeddings']","Embedding is the process of representing data (from a source domain) in a new (or target) domain. Usually, the source domain is discrete, and the target domain is continuous. For example, embedding words into the continuous vector space can be done by the word2vec method.The main reason behind using the embedding is doing meaningful mathematical computations in the target domain, which is not possible or straightforward in the source domain. For example, summing two words ""brother"" - ""man"" + ""woman"" not meaningful in the word and character levels. However, when using word2vec, embedding(""brother"") - embedding(""man"") + embedding(""woman"") can be meaningful and comparable with other embedded vectors; It should be near the embedded vector of ""sister""."
How can I model a problem as an MDP if the agent does not follow the successive order of states?,"
In my problem, the agent does not follow the successive order of states, but selects with $\epsilon$-greedy the best pair (state, action) from a priority queue. More specifically, when my agent goes to a state $s$ and opens its available actions $\{ a_i \}$, then it estimates each $(s,a)$ pair (regression with DQN) and stores it into the queue. In order for my agent to change to state $s'$, it picks the best pair from the queue instead of following one of the available actions $\{ a_i \}$ of $s$. I note that a state has a partially-different action set from the others.
However, in this way, how can I model my MDP if my agent does not follow the successive order of states?
More specifically, I have a focused crawler that has an input of a few seeds URLs. I want to output as many as possible relevant URLs with the seeds. I model the RL framework as follows.

State: the webpage,
Actions: the outlink URLs of the state webpage,
Reward: from external source I know if the webpage content is relevant.

The problem is that, while crawling, if the agent keeps going forward by following the successive state transition, it can fall into crawling traps or local optima. That is the reason why a priority queue is used importantly in crawling. The crawling agent does not follow anymore the successive order of state transitions. Each state-action pair is added to the priority queue with its estimated action value. For each time, it selects the most promising state-action pair among all pairs in the queue. I note that each URL action can be estimated taking into account the state-webpage where it was extracted.
","['reinforcement-learning', 'dqn', 'markov-decision-process']",
Is there anything that ensures that convolutional filters end up different from one another?,"
I found this question very interesting, and this is a follow up on it.
Presumably, we'd want all the filters to converge towards some complementary set, where each filter fills as large a niche as possible (in terms of extracting useful information from the previous layer), without overlapping with another filter.
A quick thought experiment tells me (please correct me if I'm wrong) that if two filters are identical down to maximum precision, then without adding in any other form of stochastic differentiation between them, their weights will be updated in the same way at each step of gradient descent during training. Thus, it would be a very bad idea to initialise all filters in the same way prior to training, as they would all be updated in exactly the same way (see footnote 1).
On the other hand, a quick thought experiment isn't enough to tell me what would happen to two filters that are almost identical, as we continue to train the network. Is there some mechanism causing them to then diverge away from one another, thereby filling their own ""complementary niches"" in the layer? My intuition tells me that there must be, otherwise using many filters just wouldn't work. But during back-propagation, each filter is downstream, and so they don't have any way of communicating with one another. At the risk of anthropomorphising the network, I might ask ""How do the two filters collude with one another to benefit the network as a whole?""

Footnotes:

Why do I think this? Because the expession for the partial derivative of the $k$th filter weights with respect to the cost $\partial W^k/\partial C$ will be identical for all $k$. From the perspective of back-propagation, all paths through the filters look exactly the same.

","['convolutional-neural-networks', 'gradient-descent']",
"What does ""adding class weights for an imbalanced dataset"" mean in the case of multi-label classification?","
Suppose I have the following toy data set:

Each instance has multiple labels at a time.
You can see I have 2 instances for Label2. However, only one instance for the other labels. It means that we have class imbalanced issues.
I read about adding class weights for an imbalanced dataset. However, I could not understand how it actually works and why it is beneficial.
Can anyone explain this method generally, as well as according to my given toy data set?
In addition to that, how do we handle these missing labels (nan)?
","['machine-learning', 'weights', 'multi-label-classification', 'imbalanced-datasets']",
What's the difference between content-based attention and dot-product attention?,"
I'm following this blog post which enumerates the various types of attention.
It mentions content-based attention where the alignment scoring function for the $j$th encoder hidden state with respect to the $i$th context vector is the cosine distance:
$$
e_{ij} = \frac{\mathbf{h}^{enc}_{j}\cdot\mathbf{h}^{dec}_{i}}{||\mathbf{h}^{enc}_{j}||\cdot||\mathbf{h}^{dec}_{i}||}
$$
It also mentions dot-product attention:
$$
e_{ij} = \mathbf{h}^{enc}_{j}\cdot\mathbf{h}^{dec}_{i}
$$
To me, it seems like these are only different by a factor. If we fix $i$ such that we are focusing on only one time step in the decoder, then that factor is only dependent on $j$. Specifically, it's $1/\mathbf{h}^{enc}_{j}$.
So we could state: ""the only adjustment content-based attention makes to dot-product attention, is that it scales each alignment score inversely with the norm of the corresponding encoder hidden state before softmax is applied.""
What's the motivation behind making such a minor adjustment? What are the consequences?

Follow up question:
What's more, is that in Attention is All you Need they introduce the scaled dot product where they divide by a constant factor (square root of size of encoder hidden vector) to avoid vanishing gradients in the softmax. Any reason they don't just use cosine distance?
","['neural-networks', 'attention', 'seq2seq']",
Can someone explain R1 regularization function in simple terms?,"
I'm trying to understand the R1 regularization function, both the abstract concept and every symbol in the formula.
According to the article, the definition of R1 is:

It penalizes the discriminator from deviating from the Nash Equilibrium via penalizing the gradient on real data alone: when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the GAN game.
$R_1(\psi  ) = \frac{\gamma}{2}E_{pD(x)}\left [ \left \| \bigtriangledown D_{\psi}(x) \right \|^2 \right ]$

I have basic understanding of how GAN's and back-propagation works. I understand the idea of punishing the discriminator when he deviates from the Nash equilibrium. The rest of it gets murky, even if it might be basic math. For example, I'm not sure why it matters if the gradient is orthogonal to the data.
On the equation part, it's even more unclear. The discriminator input is always an image, so I assume $x$ is an image. Then what is $\psi$ and $\gamma$?
(I understand this is somewhat of a basic question, but seems there are no blogs about it for us simple non-researchers, math challenged people who fail to understand the original article )
","['machine-learning', 'generative-adversarial-networks', 'regularization', 'r1-regularization']","Here is how I understand this regularization.$R_1$ is simply the norm of the gradients, which indicates how fast the weights will be updated. Gradient regularization penalizes large changes in the output of some neural network layer.$$
R_{1}\left(\psi\right) = \frac{\gamma}{2}E_{p_{D}\left(x\right)}\left[||\nabla{D_{\psi}\left(x\right)}||^{2}\right]\text{,}
$$where $\psi$ is discriminator weights, $E_{p_{D}\left(x\right)}$ means that we sample data only form the real distribution (i.e. only real images) and $\gamma$ is a hyperparameter.Since we don't know if $G$ can already generate data from real distribution, we apply this regularization to $D$ only on real data, because we don't want the discriminator to create a non-zero gradient without suffering a loss if we are already in a Nash Equilibrium. I guess this also prevents $G$ from updating if it generates data from the real distribution.The authors also investigate which value is best for $\gamma$ by analyzing the eigenvalues of the Jacobian of the the associated gradient vector field, but in my opinion, this value is highly dependent on dataset and architecture.""Gradient orthogonal to the data manifold"" simply means zero gradients. From a GAN perspective, the data manifold is a lower-dimensional latent features manifold embedded in a higher-dimensional space and our goal is to approximate it. Since the gradient vector shows the direction in which we need to update our function, if it is orthogonal to this manifold, we do not need to update the function."
What is the fundamental difference between an ML model and a function?,"
A model can be roughly defined as any design that is able to solve an ML task. Examples of models are the neural network, decision tree, Markov network, etc.
A function can be defined as a set of ordered pairs with one-to-many mapping from a domain to co-domain/range.
What is the fundamental difference between them in formal terms?
","['machine-learning', 'comparison', 'math', 'definitions', 'models']",
How does AlphaZero's MCTS work when starting from the root node?,"
From the AlphaGo Zero paper, during MCTS, statistics for each new node are initialized as such:

${N(s_L, a) = 0, W (s_L, a) = 0, Q(s_L, a) = 0, P (s_L, a) = p_a}$.

The PUCT algorithm for selecting the best child node is $a_t = argmax(Q(s,a) + U(s,a))$, where $U(s,a) = c_{puct} P(s,a) \frac{\sqrt{\sum_b N(s,b)}}{1 + N(s, a)}$.
If we start from scratch with a tree that only contains the root node and no children have been visited yet, then this should evaluate to 0 for all actions $a$ that we can take from the root node. Do we then simply uniformly sample an action to take?
Also, during the expand() step when we add an unvisited node $s_L$ to the tree, this node's children will also have not been visited, and we run into the same problem where PUCT will return 0 for all actions. Do we do the same uniform sampling here as well?
","['deep-rl', 'monte-carlo-tree-search', 'alphazero']",I looked at the Python pseudo-code attached to the Data S1 of the Supplementary Materials of the AlphaZero paper. Here is my findings:
Optimum Discriminator for label smoothed GAN,"
I was reading the paper called Improved Techniques for Training GANs. And, in the one-sided label smoothing part, they said that optimum discriminator with label smoothing is
$$ D^*(x)=\frac{\alpha \cdot p_{data}(x) + \beta \cdot p_{model}(x)}{p_{data}(x) + p_{model}(x)}$$
I could not understand where this is come from. How did we get this result?
Note: By the way, I knew how to find optimal discriminator in vanilla GAN i.e.
$$ D^*(x) = \frac{p_{r}(x)}{p_{r}(x) + p_g(x)} $$
","['deep-learning', 'generative-adversarial-networks', 'generative-model']",
Are inputs into AlphaZero the same during the evaluate step in MCTS and during test time?,"
From the AlphaZero paper:

The input to the neural network is an N × N × (M T + L) image stack that represents state using a concatenation of T sets of M planes of size N × N . Each set of planes represents the board position at a time-step t − T + 1, ..., t, and is set to zero for time-steps less than 1

From the original AlphaGo Zero paper:

Expand and evaluate (Figure 2b). The leaf node $s_L$ is added to a queue for neural network evaluation, $(d_i(p), v) = f_\Theta(d_i(s_L))$, where $d_i$ is a dihedral reﬂection or rotation selected uniformly at random from i∈[1..8]

Ignoring the dihedral reflection, the formula in the original paper $f_\Theta(s_L)$ implies that only the board corresponding to $s_L$ is passed to the neural network for evaluation when expanding a node in MCTS, not including the 7 boards from the 7 previous time steps. Is this correct?
","['deep-rl', 'monte-carlo-tree-search', 'alphazero']",
How can I use this Reformer to extract entities from a new sentence?,"
I have been looking at the NER example with Trax in this notebook. However, the notebook only gives an example for training the model. I can't find any examples of how to use this model to extract entities from a new string of text.
I've tried the following:

Instantiate the model in 'predict' mode. When trying this I get the same error reported in https://github.com/google/trax/issues/556 AssertionError: In call to configurable 'SelfAttention' (<class 'trax.layers.research.efficient_attention.SelfAttention'>)
Instantiate the model in 'eval' mode and then running model(sentence) as I would with other models. In this case the instantiation works but I get the following error when running the model: TypeError: Serial.forward input must be a tuple or list; instead got <class 'numpy.ndarray'>. Presumably this is because in 'eval' mode the model needs 2 entries passed in rather than one sentence

How can I use this Reformer to extract entities from a new sentence?
","['transformer', 'named-entity-recognition', 'inference', 'reformer']",
"Why multiplayer, imperfect information, trick-taking card games are hard for AI?","
AI reached a super-human level in many complex games such as Chess, Go ,Texas hold’em Poker, Dota2 and StarCarft2. However it still did not reach this level in trick-taking card games.
Why there is no super-human AI playing imperfect-information, multi-player, trick-taking card games such as Spades, Whist, Hearts, Euchre and Bridge?
In particular, what are the obstacles for making a super-human AI in those games?

I think those are the reasons that makes Spades hard for AI to master:

Imperfect information games pose two distinct problems: move selection and inference.

The size of the game tree isn't small, however larger games have been mastered.
I. History size: $14!^4 = 5.7\cdot10^{43}$
II. There are $\frac{52!}{13!^4}= 5.4\cdot10^{28}$ possible initial states.
III. Each initial information set can be completed into a full state in $\frac{39!}{13!^3}=8.45\cdot10^{16} $ ways

Evaluation only at terminal states.

Multiplayer games:
I. harder to prune - search algorithms are less effective
II. opponent modeling is hard
III. Goal choosing - several goals are available, need to change goals during rounds according to the reveled information.

Agent need to coordinate with a partner: conventions, signals.


","['gaming', 'state-of-the-art', 'computational-complexity', 'imperfect-information', 'games-of-chance']",
What progress has been made in computerized bridge play?,"
Computer programs have been produced for games such as Chess, Go, Poker, StarCraft 2, Dota. The best ones, Deep Blue and AlphaGo , AlphaZero, Pluribus,... are now considered better than the best human players. More to the point, the computers' game results have been influencing human play.
Apparently, computers are not yet better than human players in Bridge. There can be computer simulations of various hands and hypothetical opposing hands. But what progress have computers made in playing human players in tournaments? Have any new theories of bidding or play evolved as a result of computer-human interaction in Bridge?

This is question was asked at Board & Card Games Q&A however I think here it might get better answer
",['games-of-chance'],
Why is it easier to construct adversarial examples relative to training neural networks?,"
I was having looking at this lecture by Ian Goodfellow and my doubt is around 18:00 timestamp where he explains generation of adversarial examples using FGSM.
He mentions that the there is a linear relationship between the input to the model and the output as the activation functions are piece-wise linear with a small number of pieces. I'm not very clear what he means by input and output. Is he referring to inputs and outputs of a single layer or the input image and final output?
He states that the relation between the parameters (weights) of a model and the output are non-linear which is what makes it difficult to train a neural network, thus it is much easier to find an adversarial example.
Could someone explain what is linear in what? and how linearity helps in adversarial example construction?
EDIT: As per my understanding FGSM method relies on the linearity of the loss function with respect to the input image. It constructs an adversarial example by perturbing the input in the direction of the gradient of the loss function w.r.t image. I am not able to understand why this works?
","['generative-adversarial-networks', 'generative-model']",
What is the space complexity for training a neural network using back-propagation?,"
Suppose that a simple feedforward neural network (FFNN) contains $n$ hidden layers, $m$ training examples, $x$ features, and $n_l$ nodes in each layer. What is the space complexity to train this FFNN using back-propagation?
I know how to find the space complexity of algorithms. I found an answer here, but here it is said that the space complexity depends on the number of units, but I think it must also depend on the input size.
Can someone help me in finding its worst-case space complexity?
","['neural-networks', 'feedforward-neural-networks', 'computational-complexity', 'space-complexity']",
Strategy for playing a board game with Minimax algorithm,"
I want to build a player for the following game:
You have a board where position 1 is your player, position 2 is the rival player, -1 is a blocked cell and some positive value is a bonus. You can move up, down, left, or right. Also, each bonus has a timer until it disappears (number of steps). Furthermore, each move has a timeout limit.  At the end game, when at least one of the players is stuck, we check the scores and announce the winner.
Board example:
 -1 -1  0  0  0 -1 -1  -1
 -1  0 -1 -1 -1  0  0  340
 -1 -1  0  0  0 -1  0   0
 -1  0  0 -1  1 -1  0  -1
 -1  0  0 -1 -1  0  0   0
  0  0 -1 -1 -1  0  2  -1
  0 -1  0  0 -1  0  0  600
 -1 -1  0  0 -1 -1 -1  -1
  0 -1  0  0  0  0 -1  -1

I'm using the MiniMax algorithm with a time limit to play the game. If we got to children, we return $\infty$ for a player win, $-\infty$ for the rival win, and $0$ for a tie. If we got to a specific depth, we calculate the heuristic value. If we got timeout in some place in MiniMax, then we return the last calculated direction.  I'm trying to figure out a good strategy to win this game or get to a tie if no solution is possible.
What heuristic function would you define?
What I thought - four factors:

$f_A$ - The number of steps possible from each direction from the current position.
$f_B$ - The analytical distance from the center.
$f_C=\max_{b\in Bonus}\frac{X * I}{Y}$ - where $X$ is value of the bonus,  $I$ is $1$ if we can get to the bonus, before it disappears (otherwise $0$) and $Y$ is the distance between the bonus and the player.
$F_D$ - The distance between the players.
The final formula:
$$
f(s)=0.5\cdot(9-f_A(s))+0.2\cdot f_C(s)-0.2\cdot f_D(s)-0.1\cdot f_B(s)
$$

I'm not sure if it will be a good strategy for that game or not. How would you define the heuristic function? It should also be quick to calculate it because the game has a timeout for each move.
In order words, what will give us the best indication that our player is going to win/lose/tie?
","['game-ai', 'minimax', 'exploration-strategies', 'heuristic-functions', 'board-games']",
"Unclear definition of a ""leaf"" and diverging UTC values in the Monte Carlo Tree Search","
I have two questions regarding the Selection and Expansion steps in the Monte Carlo Tree Search Algorithm. In order to state the questions, I recall the algorithm that I believe is the one most commonly associated with the MCTS. It is described as a repeated iteration of the following four steps:

Selection: Start from root R. Choose a leaf node L by iteration of some choice algorithm that determines which child node to choose at each branching. UTC being a prominent choice.
Expansion: Create one or more offspring nodes, unless L is terminal. Choose one of them, say C.
Simulation: Play the game from C, randomly or according to some heuristic.
Backpropagation: Update rewards and number of simulations for each node on the branch R-->C.

When implementing this algorithm by myself I was unclear about the following interpretation of step 1 and 2:
Q1. When expanding the choices at the leaf node L, do I expand all, a few or just one child? If I expand all, then the tree grows exponentially large on each MCTS step, I suspect. When I expand one or a few, then either the selection step itself becomes problematic or the term leaf does. The first problem arises, because after the expansion step the node L is no longer a leaf and can never be chosen again during the selection step and in turn all the children that were not expanded will never be probed. If, however, the node L keeps being a leaf node, contrary to graph-theoretic nomenclature, then during the selection step one would need to check at each node, whether there are non-expanded child-nodes. According to which algorithm should one then choose whether to continue down the tree or expand at this non-leaf ""leaf"" some more yet unexpanded children?
Q2. Related to the first question, but slightly more in the direction of the exploitation-exploration part of the selection, I am puzzled about the UTC selection step, which again raises issues for each of the above-mentioned expansion methods: In case that a few or all child-nodes are chosen during expansion at the leaf, one is faced with the problem that some of those nodes will not be simulated in that MCTS step and subsequently will have a diverging UTC value $w_i/n_i + c \sqrt{\frac{\ln{N_i}}{n_i}}\to \infty$, since $n_i\to 0$. On the other hand, in case that only one child is chosen, we are facing the issue that no UTC value can be assigned to the ""unborn"" children on the way. In other words, one cannot use UTC to decide whether to choose a child node according to UTC at each branching or to expand the tree at that node (since all nodes within the tree may have some unexpanded child nodes).
","['monte-carlo-tree-search', 'monte-carlo-methods', 'tree-search']","Q1. When expanding the choices at the leaf node L, do I expand all, a few or just one child?Expanding all nodes or expanding just one node are both possible. There are different advantages and disadvantages. The obvious disadvantage of immediately expanding them all is that your memory usage will grow more quickly. I suppose that the primary advantage is that you no longer need to keep track separately of ""actions that are legal but for which I did not yet create child nodes"" and ""already-created nodes"", which I guess might sometimes lead to better computational efficiency (especially when memory isn't a concern for you, for instance if you do relatively few iterations anyway or if you have a huge amount of memory available).In the more ""modern"" variants of MCTS that also use trained policy networks (like all the work inspired by AlphaGo / AlphaGo Zero / AlphaZero), it typically makes most sense to expand all children at once, because the trained network will immediately give you policy values for all children anyway, so then you can prime them all at once with those policy head outputs and also immediately start using all of those values.In the case where you choose to expand only one child at a time, indeed the terminology of ""leaf"" nodes becomes confusing / incorrect. I never really like the term ""leaf"" node in the context of MCTS anyway; they're really not leaf nodes (unless they happen to represent terminal game states), they're just nodes for which did not yet choose to instantiate all the child nodes. It's a bit more verbose, but I prefer referring to them as ""nodes that have not yet been fully expanded"". That would change the description of the Selection phase to something more like:Selection: Start from root R. Choose a node L that has not yet been fully expanded (or a terminal node) by iteration of some choice algorithm that determines which child node to choose at each branching.In case that a few or all child-nodes are chosen during expansion at the leaf, one is faced with the problem that some of those nodes will not be simulated in that MCTS step and subsequently will have a diverging UTC value $w_i/n_i + c \sqrt{\frac{\ln{N_i}}{n_i}}\to \infty$, since $n_i\to 0$.That's true, but I don't see that as a problem. Suppose we're at a non-fully-expanded node; a node where maybe some legal actions already have corresponding child nodes, or maybe none do, but at least some legal actions still don't have corresponding child nodes. We can view these as having a visit count of $0$, which we can view as leading to a UCB1 value of $\infty$. Since UCT picks nodes (or actions) according to an $\arg\max$ over the UCB1 values, we can simply think of this as UCT always preferring to pick actions that we did not yet expand over actions that we've already expanded and created a child node for. This leads to an implementation where we'll only re-visit a node that we've already visited before if we have already visited each of its possible siblings at least once too."
How is the depth of the filters of convolutional layers determined? [duplicate],"







This question already has answers here:
                                
                            




How is the depth of a convolutional layer determined?

                                (3 answers)
                            

Closed 1 year ago.



I am a bit confused about the depth of the convolutional filters in a CNN.
At layer 1, there are usually about 40 3x3x3 filters. Each of these filters outputs a 2d array, so the total output of the first layer is 40 2d arrays.
Does the next convolutional filter have a depth of 40? So, would the filter dimensions be 3x3x40?
","['convolutional-neural-networks', 'filters', 'convolutional-layers', 'convolution-arithmetic', 'feature-maps']","Does the next convolutional filter have a depth of 40? So, would the filter dimensions be 3x3x40?Yes. The depth of the next layer $l$ (which corresponds to the number of feature maps) will be 40. If you apply $8$ kernels with a $3\times 3$ window to $l$, then the number of features maps (or the depth) of layer $l+1$ will be $8$. Each of these $8$ kernels has an actual shape of $3 \times 3 \times 40$. Bear in mind that the details of the implementations may change across different libraries.The following simple TensorFlow (version 2.1) and Keras programoutputs the followingwhere conv2d has the output shape (None, 26, 26, 40) because there are 40 filters, each of which will have a $3\times 3 \times 40$ shape.The documentation of the first argument (i.e. filters) of the Conv2D saysfilters – Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).and the documentation of the kernel_size parameter stateskernel_size – An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.It doesn't actually say anything about the depth of the kernels, but this is implied from the depth of the layers.Note that the first layer has $(40*(3*3*1))+40 = 400$ parameters. Where do these numbers come from? Note also that the second Conv2D layer has $(8*(3*3*40))+8 = 2888$ parameters. Try to set the parameter use_bias of the first Conv2D layer to False and see the number of parameters again.Finally, note that this reasoning applies to 2d convolutions. In the case of 3d convolutions, the depth of the kernels could be different than the depth of the input. Check this answer for more details about 3d convolutions."
Model-based RL for time series data,"
I have time-series data. When I take an action, it impacts the next state, because my action directly determines the next state, but it is not known what the impact is.
To be concrete: I have $X(t)$ and $a(t-1)$, where $X(t)$ is n-dimensional time-series data and $a(t)$ is 1-dimensional time-series data. At time $t$, they together represent the observation/state space. Also, at time $t$, the agent makes a decision about $a(t)$. This decision (action) $a(t)$ directly defines the next state space $X(t+1)$ and rewards, by some function $f$ & $g$, $f(a(t), X(t)) = X(t+1)$ and $g(a(t), X(t)) = R(t+1)$.
I have to estimate this impact, i.e. where I will end up (what will be the next state). I decided to use a model-based RL algorithm, because, from my knowledge, model-based RL does exactly this.
Can you advise me on a good paper and Github code, to implement this project?
As I noticed, there do not exist many works on Model-based RL.
","['reinforcement-learning', 'reference-request', 'time-series', 'model-based-methods']",
"What is a ""codon"" in grammatical evolution?","
The term codon is used in the context of grammatical evolution (GE), sometimes, without being explicitly defined. For example, it is used in this paper, which introduces and describes PonyGE 2, a Python library for GE, but it's not clearly defined.  So, what is a codon?
","['terminology', 'evolutionary-algorithms', 'genetic-programming', 'grammatical-evolution', 'codon']","To understand what a codon is, we need to understand what GE is, so let me first provide a brief description of this approach.Grammatical evolution (GE) is an approach to genetic programming where the genotypes are binary (or integer) arrays, which are mapped to the phenotypes (i.e. the actual solutions, which can be represented as trees, which, in turn, represent programs or functions), using a grammar (for example, expressed in Backus-Naur form). So, the genotypes (i.e. what is mutated, combined, or searched) and the phenotypes (the actual solutions, which are programs) are different in GE, and the genotype needs to be mapped to the phenotype to get the actual solution (or program), but this is not the case in all GP approaches (for example, in tree-based GP, the genotype and the phenotype are the same, i.e. trees, which represent functions).In GE, a codon is a subsequence of $m$ bits of the genotype (assuming that genotypes are binary arrays). For example, let's say that we have only two symbols in our grammar, i.e. a (a number) and b (another number). In this case, we only need 2 bits to differentiate the two. If we had 3 symbols,  a, b and + (the addition operator), we would need at least a sequence of 2 bits to define each symbol. So, in this case, we could have the following mappingThe operation a+b could then be represented by the binary sequence 001001 (or the integer sequence 021). The 2-bit subsequences 00, 01 and 10 (or their integer counterparts) are the codons.In GE, codons are used to index the specific choice of a production rule. To understand this, let's define a simple grammar, which is composed ofIn this case, the set of production rules $P$ is defined as follows\begin{align}
\langle \text{expr} \rangle & ::= 
\langle \text{expr} \rangle \langle \text{op} \rangle \langle \text{expr} \rangle \; | \;  \langle \text{operand} \rangle \\
\langle \text{op} \rangle & ::= + \;  | \;  - \;  | \; * \; | \; / \\
\langle \text{operand} \rangle & ::= 1 \;  | \; 2 \; | \;  3 \; | \;  4 \;  | \;  \langle \text{var} \rangle \\
\langle \text{var} \rangle & ::= \mathrm{x} \; | \;  \mathrm{y}
\end{align}
So, there are four productions rules. To be clear, $\langle \text{var} \rangle  ::= \mathrm{x} \; | \;  \mathrm{y}$ is a production rule. The symbol $|$ means ""or"", so the left-hand side of each production is a non-terminal (and note that all non-terminals are denoted with angle brackets $\langle \cdot \rangle$), which is defined as (or can be replaced with) one of the right-hand choices, which can be a non-terminal or terminal. The first choice of each production rule is at index $0$. The second choice at index $1$, and so on. So, for example, in the case of the production $\langle \text{var} \rangle  ::= \mathrm{x} \; | \;  \mathrm{y}$, $\mathrm{x}$ is the choice at index $0$ and $\mathrm{y}$ is the choice at index $1$.The codons are the indices that we use to select the production rule's choice while transforming (or mapping) the genotype into a phenotype (an actual program).So, we start with the first production, in this case, $S = \langle \text{expr} \rangle$. If it's a non-terminal, then we replace it with one of its right-hand side choices. In this case, there are two choicesIf our genotype (integer representation) is, for example, $01$ (note that this is a sequence of integers), we would replace $\langle \text{expr} \rangle$ with $\langle \text{expr} \rangle \langle \text{op} \rangle \langle \text{expr} \rangle$, then we would replace the first $\langle \text{expr} \rangle$ in $\langle \text{expr} \rangle \langle \text{op} \rangle \langle \text{expr} \rangle$ with $\langle \text{operand} \rangle$, so we would get $\langle \text{operand} \rangle \langle \text{op} \rangle \langle \text{expr} \rangle$, and so on and so forth, until we get an expression that only contains terminals, or the genotype is terminated. There can be other ways of mapping the genotype to the phenotype, but this is the purpose of codons.The term codon has its origins in biology: a subsequence of 3 nucleotides is known as a codon, which is mapped to an amino acid in order to produce the proteins. The set of all mappings from codons to amino acids is known as genetic code. Take a look at this article for a gentle introduction to the subject. So, in GE, codons also have a similar role to the role of codons in biology, i.e. they are used to build the actual phenotypes (in biology, the phenotypes would be the proteins or, ultimately, the organism).Codons do not have to be 2-bit subsequences, but they can be $m$-bit subsequences, for some arbitrary $m$. The term codon is similar to the term gene, which is also often used to refer to specific subsequences of the genotype (for example, in genetic algorithms), although they may not be synonymous (at least in biology, genes are made up of sequences of codons, so they are not synonymous). Moreover, the binary $m$-bit codons can first be mapped to integers, so codons can also just be integers, as e.g. used here or illustrated in figure 2.2 of this chapter.You can find more info about codons in the book Genetic Programming: An Introduction by Wolfgang Banzhaf et al., specifically sections 9.2.2. (p. 255), 9.2.3. (an example) and 2.3 (p. 39), or in chapter 2 of the book Foundations in Grammatical Evolution by Dempsey et al."
Wasserstein GAN: Implemention of Critic Loss Correct?,"
The WGAN paper concretely proposes Algorithm 1 (cf. page 8). Now, they also state what their loss for the critic and the generator is.
When implementing the critic loss (so lines 5 and 6 of Algorithm 1), they maximize the parameters $w$ (instead of minimizing them as one would normally do) by writing $w \leftarrow w + \alpha \cdot \text{RMSProp}\left(w, g_w \right)$. Their loss seems to be $$\frac{1}{m}\sum_{i = 1}^{m}f_{w}\left(x^{\left(i\right)} \right) - \frac{1}{m}\sum_{i = 1}^{m} f_{w}\left( g_{\theta}\left( z^{\left( i\right)}\right)\right).\quad \quad (1)$$
The function $f$ is the critic, i.e. a neural network, and the way this loss is implemented in PyTorch in this youtbe video (cf. minutes 11:00 to 12:26) is as follows:
critic_real = critic(real_images)
critic_fake = critic(generator(noise))
loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))
My question is: In my own experiments with the CelebA dataset, I found that the critic loss is negative, and that the quality of the images is better if the negative critic loss is higher instead of lower, so $-0.75$ for the critic loss resulted in better generated iamges than a critic loss of $-1.26$ e.g.
Is there an error in the implementation in the youtube video of Eq. (1) and Algorithm 1 of the WGAN paper maybe? In my opinion, the implementation in the video is correct, but I am still confused then on why I get better images when the loss is higher ...
Cheers!
","['papers', 'generative-adversarial-networks', 'implementation', 'wasserstein-gan']",
What is a unified neural network model?,"
In many articles (for example, in the YOLO paper, this paper or this one), I see the term ""unified"" being used. I was wondering what the meaning of ""unified"" in this case is.
","['neural-networks', 'deep-learning', 'terminology', 'architecture', 'yolo']","A unified neural network model consists of one neural network as opposed to other models that rely on two or more neural networks.For example, from page two of the YOLO paper:2. Unified DetectionWe unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and realtime speeds while maintaining high average precision.In the paper by Xu and Wang, they add a branch to handle tracking to the Faster R-CNN architecture, which was designed for object detection.  This is a 'unification' of two models.In the paper by Ebrahimi et al., on pages 42-43, you can see they are fusing multiple neural networks into one unified model:We propose a unified user geolocation method that relies on a fusion of neural networks, incorporating different types of available information: tweet message, users' social relationships, and metadata fields embedded in tweets and profiles."
Validation accuracy higher than training accurarcy,"
I implemented the unet in TensorFlow for the segmentation of MRI images of the thigh. I noticed I always get a higher validation accuracy  by a small gap, independently of the initial split. One example:

So I researched when this could be possible:

When we have an ""easy"" validation set. I trained it for different initial splitting, all of them showed a higher validation accuracy.
Regularization and augmentation may reduce the training accuracy. I removed the augmentation and  dropout regularization and still observed the same gap, the only difference was that it took more epochs to reach convergence.
The last thing I found was that in Keras the training accuracy and loss are averaged over each iteration of the corresponding epoch, while the validation accuracy and loss is calculated from the model at the end of the epoch, which might make the the training loss higher and accuracy lower.

So I thought that if I train and validate on the same set, then I should get the same curve but shifted by one epoch. So I trained only on 2 batches and validated on the same 2 batches (without dropout or augmentation). I still think there is something else happening because they don't look quite the same and at least at the end when the weights are not changing anymore, the training and validation accuracy should be the same (but still the validation accuracy is higher by a small gap). This is the plot:


Is there anything else that can be increasing the loss values, this is the model I am using:
def unet_no_dropout(pretrained_weights=None, input_size=(512, 512, 1)):
inputs = tf.keras.layers.Input(input_size)
conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)
conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)
pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)
conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)
conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)
pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)
conv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)
conv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)
pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)
conv4 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)
conv4 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)
#drop4 = tf.keras.layers.Dropout(0.5)(conv4)
pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)

conv5 = tf.keras.layers.Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)
conv5 = tf.keras.layers.Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)
#drop5 = tf.keras.layers.Dropout(0.5)(conv5)

up6 = tf.keras.layers.Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
    tf.keras.layers.UpSampling2D(size=(2, 2))(conv5))
merge6 = tf.keras.layers.concatenate([conv4, up6], axis=3)
#merge6 = tf.keras.layers.concatenate([conv4, up6], axis=3)
conv6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)
conv6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)

up7 = tf.keras.layers.Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
    tf.keras.layers.UpSampling2D(size=(2, 2))(conv6))
merge7 = tf.keras.layers.concatenate([conv3, up7], axis=3)
conv7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)
conv7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)

up8 = tf.keras.layers.Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
    tf.keras.layers.UpSampling2D(size=(2, 2))(conv7))
merge8 = tf.keras.layers.concatenate([conv2, up8], axis=3)
conv8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)
conv8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)

up9 = tf.keras.layers.Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
    tf.keras.layers.UpSampling2D(size=(2, 2))(conv8))
merge9 = tf.keras.layers.concatenate([conv1, up9], axis=3)
conv9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)
conv9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)
conv9 = tf.keras.layers.Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)
conv10 = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(conv9)

model = tf.keras.Model(inputs=inputs, outputs=conv10)

model.compile(optimizer = Adam(lr = 2e-4), loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Accuracy()])
#model.compile(optimizer=tf.keras.optimizers.Adam(2e-4), loss=combo_loss(alpha=0.2, beta=0.4), metrics=[dice_accuracy])
#model.compile(optimizer=RMSprop(lr=0.00001), loss=combo_loss, metrics=[dice_accuracy])

if (pretrained_weights):
    model.load_weights(pretrained_weights)

return model

and this is how I save the model:
model_checkpoint = tf.keras.callbacks.ModelCheckpoint('unet_ThighOuterSurfaceval.hdf5',monitor='val_loss', verbose=1, save_best_only=True)
model_checkpoint2 = tf.keras.callbacks.ModelCheckpoint('unet_ThighOuterSurface.hdf5', monitor='loss', verbose=1, save_best_only=True)

model = unet_no_dropout()
history = model.fit(genaug, validation_data=genval, validation_steps=len(genval), steps_per_epoch=len(genaug), epochs=80, callbacks=[model_checkpoint, model_checkpoint2])

","['tensorflow', 'keras', 'image-segmentation', 'u-net']",
RL: Encoding action conditioned on previous action,"
I have a card game where on a player's turn, the player sequentially draws two cards. Each card may be drawn from another player's discard stack (face up), or from the deck (face down).
Thinking how to encode this into an action space, I could naively assume the two draws are independent. The action space would simply be a binary vector of 2 * (1 + (number_of_players - 1)), which I could post-filter to limit for empty draw piles (and can't draw from own pile).
However, when playing the game myself, I noticed that it's sometimes advantageous to draw the initial card from the deck, then select the draw pile for the second card based on the value of the first one drawn. But how would this be encoded into an action space? Would it be better to think of these are two separate actions, even thought they are part of the same ""turn""?
",['reinforcement-learning'],"It is hard to say for certain without knowing full details and results of experiments.However, if the game allows for splitting decisions up, it will likely be better for the agent to take advantage of extra knowledge of the value of any previously hidden card just taken from the draw pile.In general, if each player decision is taken sequentially, resulting in changes to state, then it is a separate action on a separate time step according to the MDP theoretical model used in reinforcement learning (RL). You might want to describe/notate the time steps differently so that they match how the game play proceeds. However, for the purposes of RL, each decision point should be on a new time step, and should result in a new state, new value estimates etc.Similarly, whether or not the current choice is the player's first card or second card to be drawn needs to be part of the state. This detail of the state might already be covered by the number of cards in the player's hand, if logically the number of cards is always the same at each stage. However, if hand size can vary for other reasons, it is worth adding an explicit flag for ""first draw choice"" or similar so that the agent can use the information.You have some freedom for encoding the action space. If drawing cards is the only possible action in this game at all stages, then a binary output vector of 1 + (number_of_players - 1) dimensions would be suitable. Other encodings may work well too, it depends if there is any logical structure to the choices or some derived data that encodes useful game information.It may be useful to arrange the action choices so that the index for drawing from each player's discard pile is considered relatively to the current player's turn. That is, instead of actions being arranged $[draw, discard P1, discard P3, discard P4, discard P5]$ for P2, they would be arranged $[draw, discard P3, discard P4, discard P5, discard P1]$ and for P3 would be different: $[draw, discard P4, discard P5, discard P1, discard P2]$ . . . that would inherently allow for the cyclical nature of turns. State representation would need to similarly rotate knowledge about each player to match this. You might not need to do this, but I would recommend it for games where there is a lot of common logic regarding action choices relative to turn position that you could take advantage of. The opposite would apply (and you would use absolute player positions) if there were important differences throughout the game between being P1, P2, P3 etc."
How will the filter size affect the transpose convolution operation?,"
After a series of convolutions, I am up-sampling a compressed representation, I was curious what is the methodology I should follow to choose an optimum kernel size for up-sampling.

How will the filter (or kernel) size affect the transpose convolution operation (e.g. when using ConvTranspose2d)? Will a larger kernel help upsample with better detail or a small-sized kernel? And how would padding fit in this scenario?

At what rate should the depth(Channels i.e number of filters) decrease while upsampling i.e from (Dx24x24) to (D/2 or D/4, 48, 48)
Eg: If i/p to TransposeConvolution is (cxhxw)64x8x8 how o/p quality be different for o/p of shape 32x16x16 and 16x16x16?


","['convolutional-neural-networks', 'pytorch', 'hyper-parameters', 'filters', 'transpose-convolution']",
"Designing Policy-Network for Deep-RL with Large, Variable Action Space","
I am attempting a project involving training an agent to play a game using deep reinforcement learning.
This project has a few features that complicate the design of the neural network:

The action space per state is very large (can be over 1000 per state)
The set of actions available in each state very wildly between states, both in size and the actions available.
The total action-space (the union of each state's action-space) is way too large to enumerate.
The action space is discrete, not continuous.
The game is adversarial, multi-agent.

Most RL neural networks I've seen involve the input of a state, and an output of a constance action size, where each element of the output is either an action's q-score or probability. But since my game's action space non-constant per state, I believe this design will not work for this game.
I have seen an alpha-go style network, which outputs a probability for all actios ever possible, and then zeros out actions not possible in the given state and re-normalized the probabilities. However, since the total action-space in my game is way to large to enumerate, I don't believe this solution will work either.
I have seen several network designs for large, discrete action spaces, including:

design a network to input a state-action pair and output a single score value, and train it via a value-based algorithm (such as q-learning). To select an action given a state, pass in every possible state-action pair to get each action's score, then select the action with the highest score.
(Wolpertinger architecture) have a network output a continous embedding, which is then mapped to a discrete action, and train it via deterministic policy gradient.
divide actions into a sequence of simpler actions, and train an RNN to output a sequence of these simpler actions. Train this network via a value-based algorithm (such as q-learning).

However, all of these solutions are designed for either value-based or deterministic policy gradient algorithms; none of them output probabilities over the action space. This seems to be an issue since at least a very large portion of the multi-agent deep-RL algorithms I've seen involve a network that outputs a probability over the action-space. Therfore, I don't want to limit myself to value-based and deterministic-policy algorithms.
How can I design a neural network that outputs a probability over the action space for my game?
If not, what would be some good solutions to this problem?
","['neural-networks', 'reinforcement-learning', 'policy-gradients', 'multi-agent-systems']",
"Loss randomly changing, incorrect output (even for low loss) when trying to overfit on a single set of input and output","
I am trying to make a neural network framework from scratch in C++ just for fun, and to test my backpropagation, I thought it would be an easy way to test the functionality if I give it one input - a randomized size 10 vector, and one output: a size 5 vector containing all 1s, and train it a bunch of times to see if the loss will decrease. Essentially trying to make it overfit
The problem is that for each run that I do, the loss either shoots up and goes to nan, or reduces a lot, going to 0.000452084 or other similar small values. However even in the low end of things, my output (which should be close to all 1s, as the ""ground truth"") is something like:
0.000263654
1e-07
8.55893e-05
1e-07
0.999651

The only close value close to 1 being the last element.
My network consists of the input layer 10 neurons, one 10 neuron dense layer with RELU activation, and another 5 neuron dense layer for output, with SoftMax activation. I am using categorical cross entropy as my loss function, and I am normalizing my gradient by dividing it by the norm of my gradient if it is over 1.0. I initialize my weights to be random values between -0.1 and 0.1
To calculate the gradient of the loss function, I use -groundTruth/predictedOutput. To calculate
the other derivatives, I dot the derivative of that layer with the gradient of the previous layer with respects to its activation function.
Before this problem I was having exploding gradients, which the gradient scaling fixed, however it was very weird that that would even happen on a very small network like this, which could be related to the problem I am currently having. Is the implementation not correct or am I missing something very obvious?
Any ideas about this weird behavior, and where I should look first? I am not sure how to show a minimal reproduceable example as that would require me to paste the whole codebase, but I am happy to show pieces of code with explanation. Any advice welcomed!!
","['neural-networks', 'machine-learning', 'deep-learning', 'objective-functions', 'gradient-descent']","Softmax activation always adds up to 1, because it's designed to deal with probabilities (in problems of classification, those probabilities represent how likely the network thinks an object belongs to a specific class). You can verify that by summing up the numbers of your output layer. So currently your network is trying to do the impossible, to produce output that sums up to 5, instead of 1. Therefore, loss will never become stable. Since you want your output layer to produce all ones, you need to use some other activation, for example, linear. Linear activation does not have the same constraint that Softmax does."
The last target name is missed in the test set,"
I am training a neural network with a dataset that has 51 classes and 6766 data in it. I used 80% for the training set, 10% for validation, and 10% for the test. After training I got confusion matrix and I find out the last class is missed in the test set. So, I used data augmentation and used 27064 data and 80-10-10 splits again, but the last class name is missed again. I changed the size of the test split but the problem was not solved, and in every trial that I made, only the last class name is missed. How can I solve this?

EDIT: my dataset is images and in the original dataset, I have 104 data from the last class, after augmentation the entire dataset the last class has 416 data.
","['neural-networks', 'classification', 'training']",
How does the support vector machine constraint imply that sample selection bias will not systematically affect the output of the optimisation?,"
I am currently studying the paper Learning and Evaluating Classifiers under Sample Selection Bias by Bianca Zadrozny. In section 3.4. Support vector machines, the author says the following:

3.4. Support vector machines
In its basic form, the support vector machine (SVM) algorithm (Joachims, 2000a) learns the parameters $a$ and $b$ describing a linear decision rule
$$h(x) = \text{sign}(a \cdot x + b),$$
whose sign determines the label of an example, so that the smallest distance between each training example and the decision boundary, i.e. the margin, is maximized. Given a sample of examples $(x_i, y_i)$, where $y_i \in \{ -1, 1 \}$, it accomplishes margin maximization by solving the following optimization problem:
$$\text{minimize:} \ V(a, b) = \dfrac{1}{2} a \cdot a \\ \text{subject to:} \ \forall i : \ y_i[a \cdot x_i + b] \ge 1$$
The constraint requires that all examples in the training set are classified correctly. Thus, sample selection bias will not systematically affect the output of this optimization, assuming that the selection probability $P(s = 1 \mid x)$ is greater than zero for all $x$.

How does the constraint that all examples in the training set are classified correctly imply that sample selection bias will not systematically affect the output of the optimisation? Furthermore, why is it necessary to assume that the selection probability is greater than zero for all $x$? These are not clear to me.
","['classification', 'papers', 'optimization', 'support-vector-machine', 'selection-bias']",
How to obtain the policy in the form of a finite-state controller from the value function vectors over the belief space of the POMDP?,"
I was reading this paper by Hansen.
It says the following:

A correspondence between vectors and one-step policy choices plays an important role in this interpretation of a policy. Each vector in $\mathcal{V}'$ corresponds to the choice of an action, and for each possible observation, choice of a vector in $\mathcal{V}$. Among all possible one-step policy choices, the vectors in $\mathcal{V}'$ correspond to those that optimize the value of some belief state. To describe this correspondence between vectors and one-step policy choices, we introduce the following notation. For each vector $\mathcal{v}_i$ in $\mathcal{V}'$, let $a(i)$ denote the choice of action and, for each possible observation $z$, let $l(i,z)$ denote the index of the successor vector in $\mathcal{V}$. Given this correspondence between vectors and one-step policy choices, Kaelbling et al. (1996) point out that an optimal policy for a finite-horizon POMDP can be represented by an acyclic finite-state controller in which each machine state corresponds to a vector in a nonstationary value function.


I am unable to guess how the left-side finite-state controller is formed from the right side belief space diagram. Does the above text provide enough explanation for the conversion? If yes, I am not really able to fully get it. Can someone please explain?
","['papers', 'value-functions', 'policies', 'pomdp', 'policy-iteration']",
Searching for 3D cad AI synthesis project (New CAD file form two similar CAD model),"
I saw this post: Google AI generates images of 3D models with realistic lighting and reflections.

Is there any project to test this capability for combining two 3d cad files of a similar model and get a new combined model as shown (Hoogle AI ...)? I would like to use software such as Blender, SolidWorks or some project via Google Colab (testable without the need to install any software).
",['image-generation'],
How to do early classification of time series event with small dataset?,"
I would like to build a real-time binary classifier that can predict an event of interest that is occurring as soon as it starts. These are electromyographic signals, and the event classification should be able to classify the event as early as possible. This is because the next stage of the algorithm has to make a decision before the end of the event.
I don't know what kind of algorithm/approach I should use here. I suppose RNN with LSTM cells should do the job, but the dataset is quite small as physiological signals are not easy to gather.
I have seen many algorithms that windowed the signals (from the training set) and labeled each window as an event of interest if at least part of the event is contained in the window. Each window is then fed to a machine learning algorithm. Then, the prediction uses a sliding window in real-time. But this approach doesn't take into account the temporal aspect of the event as there is no link between each window seen by the ML algorithm.
Do you have any tips or resources I could use to solve the problem?
","['machine-learning', 'classification', 'ai-design', 'prediction', 'time-series']",
Is it possible that the fittest individuals in an Artificial Life population may be successful by not actively pursuing the rules of the environment?,"
I'm trying to understand Artificial Life (e.g. here for a simple background) in Computational Evolution.
I understand that in this set of methods, you set up a dynamic environment (e.g. the ecology of the environment) and then you set a series of rules; e.g.:

You need energy to reproduce.
You intake energy from food sources.
For nourishment, you can
eat plants, animals, or steal food.
You must stay alive until you reproduce.
Every action consumes energy.
When you have no energy left, you die.

I think I need a set of rules that govern the survival of an artificial life. You run the environment and see what persists (there's a set of rules instead of a fitness score), and the individuals that survive are said to be successful.
I can imagine a scenario where a successful organism in this environment consumes a lot of food, reproduces, but possibly runs out of energy and dies. I'm wondering if there's ever a situation where an organism does very little (or nothing), and still be successful? I'm not sure if this question makes sense, please let me know if clarification is needed. Given the specified environment, I want to know if the most active organism will always be the most successful. The most active organism would be the one that obtains the most food/energy/reproduces the most. Or is it possible to not be the most active organism and still be successful?
","['genetic-algorithms', 'artificial-life']",
Is it possible to integrate the GPT-3 by OpenAPI inside Unity3D or any game-engine?,"
My company has full access to beta testing for GPT-3. We wanted to try it for some games or game mechanics within Unity3D. Is it possible to use it for dialogues or with unity scripts?
The Documents of OpenAI does not say anything about this possibility, so I'm not sure.
","['game-ai', 'gpt', 'dialogue-systems', 'integration']","Yes, OpenAI will release an API for GPT-3, so any developer can integrate it into their application. I don't believe the document for their API is public yet, so we don't know what the final interface will look like, but it's likely to be a simple REST API. In the future, I imagine your developers can take advantage of their API, or alternatively there will be community-made scripts for you to use/copy.The pricing for using their API is explained here. Note that they charge per token, which might be important in case your game plans to make live calls to GPT-3 during gameplay (as opposed to mining a huge corpus of answers to build an offline database).The use cases of GPT-3 suggests that you can legally use them for commercial products, although I couldn't find a definitive license or user agreement document."
How to derive matrix form of the Bellman operators?,"
Reading the Retrace paper (Safe and efficient off-policy reinforcement learning) I saw they often use a matrix form of the Bellman operators, for example as in the picture below. How do we derive those forms? Could you point me to some reference in which the matter is explained?
I am familiar with the tabular RL framework, but I'm having trouble understanding the steps from operators to this matrix form. For example, why does $Q^{\pi} = (I -\gamma P^{\pi})^{-1}r$? I know that for the value $V$ we can write
\begin{align}
V =  R + \gamma P^{\pi} V \\
V - \gamma P^{\pi} V =  R \\
V (I -\gamma P^{\pi}) =  R \\ 
V = R(I - \gamma P^{\pi})^{-1}
\end{align}
but this seems slightly different.
Picture from Safe and efficient off-policy reinforcement learning:

","['reinforcement-learning', 'reference-request', 'value-functions', 'bellman-equations', 'bellman-operators']","There's not much to derive here it's simply a definition of Bellman operator, it comes from Bellman equation. If you're wondering why
\begin{equation}
Q^{\pi} = (I - \gamma P^{\pi})^{-1}r \tag{1}
\end{equation}
they state that $Q^{\pi}$ is a fixed point which means if you apply Bellman operator to it you get the same value
\begin{equation}
T^{\pi}(Q^{\pi}) = Q^{\pi}
\end{equation}
You can easily check that since from $(1)$
\begin{equation}
r = (I-\gamma P^{\pi})Q^{\pi}
\end{equation}
if you plug it in definition of Bellman operator you get
\begin{align}
T^{\pi}(Q^{\pi}) &= r + \gamma P^{\pi} Q^{\pi}\\
&= (I - \gamma P^{\pi})Q^{\pi}+  \gamma P^{\pi} Q^{\pi}\\
&= Q^{\pi}
\end{align}"
Are Graph Neural Networks generalizations of Convolutional Neural Networks?,"
In lecture 4 of this course, the instructor argues that GNNs are generalizations of CNNs, and that one can recover CNNs from GNNs.
He presents the following diagram (on the right) and mentions that it represents both a CNN and a GNN. In particular, he mentions that if we particularize the graph shift operator (i.e the matrix S, which in the case of a GNN could represent the adjacency matrix or the Laplacian) to represent a directed line graph, then we obtain a time convolutional filter (which I hadn't heard of before watching this, but now I know that all it does is shift the graph signal in the direction of the arrows at each time step).

That part I understand. What I don’t understand is how we can obtain 2D CNNs (the ones that we would for example apply to images) from GNNs. I was wondering if someone could explain.
EDIT: I found part of the answer here. However, it seems that the image convolution as defined is a bit different from what I’m used to. It seems like the convolution considers pixels only to the left and above of the “current” pixel whereas I’m used to convolutions considering both left, right, above, and below
","['neural-networks', 'convolutional-neural-networks', 'comparison', 'graph-neural-networks']",
Understanding example for Improved Policy Iteration for POMDPs,"
I was going through this paper by Hansen. This paper proposes policy improvement by first converting set of $\alpha$ vectors into finite state controller and then comparing them to obtain improved policy. The whole algorithm is summmarised as follows:

Section 4 of this paper explains the algorithm with example. I am unable to get the example, specifically how it forms those states, what are the numbers inside each state and how they are exactly calculated:

","['reinforcement-learning', 'markov-decision-process', 'pomdp']",
What ensemble methods are used in the state-of-the-art models?,"
What ensemble methods are used in the state-of-the-art models?
When I surveyed the state-of-the-art methods of classification and detection, e.g. ImageNet, COCO, etc., I noticed that are few or even no references to the use of ensemble methods like bagging or boosting.
Is it a bad idea to use them?
However, I observed that many use ensemble in Kaggle competitions.
What makes it so different between the two groups of researchers?
","['machine-learning', 'object-detection', 'ensemble-learning', 'boosting']","In my opinion, it is not because ensemble methods are not good, just the state-of-the-art and Kaggle competitions are two different fields.Kaggle competitions can be understood as an industry project where the target (accuracy, distance value, etc) is the most important, and they can select some computationally expensive way such as ensemble methods to reach it.The state-of-the-art models in other ways belong to the research area, where the most important is the contribution for science, you can not just combine a lot of models then call it is the research (and so unfair with some small researcher groups). If you want to contribute something depend on ensemble idea, it should be like this paper."
Can you use machine learning for data with binary outcomes?,"
I am totally new to artificial intelligence and neural networks and have a broad question that I hope is appropriate to ask here.
I am an ecologist working in animal movement and I want to use AI to apply to my field. This will be one of the few times this has been attempted so there is not much literature to help me here.
My dataset is binary. In short, I have the presence (1) and absence (0) of animal locations that are associated with a series of covariates (~20 environmental conditions such as temperature, etc.). I have ~1 million rows of data to train the model on with a ratio of 1:100 (presence:absence).
Once trained, I would like a model that can predict if an animal will be in a location (or give a probability) based on new covariates (environmental conditions).
Is this sort of thing possible using AI?
(If so, where should I be looking for resources? I write in R, should I learn Python?)
","['neural-networks', 'machine-learning', 'applications', 'binary-classification']","Of course, you can use AI (especially Deep Learning) in your application. Your covariates will be the input to your AI model and the model should predict the probability of presence. The model has no problem with binary data and binary data is common in this field.Also, note that the 1:100 ratio is not good and the network will probably learn to output absence for any input (this way it gets 99% accuracy but really it's not doing anything). So, you should probably balance them (using almost the same data, or telling the network to pay more attention to presence data (by weighting the related loss)).I think nowadays you can find Deep Learning in any popular coding language. But most of the DL community uses Python and it's really easy to learn. If you want to learn Deep Learning there are a lot of sources on the internet. But I suggest you the Deep Learning courses of Deeplearning.ai in Coursera (If you have a lot of time) and CS231n of Stanford university on youtube (If you have time)."
What is the equation to update the weights in the perceptron algorithm?,"
I'm trying to understand the solution to question 4 of this midterm paper.
The question and solution is as follows:

I thought that the process for updating weights was:
error = target - guess
new_weight = current_weight + (error)(input)

I do not understand for example, for number 2 below, how that sum is determined. For example, I want to understand whether to update the weight or not. The calculation is:
x1(w1) + x2(w2)
(10)(1) + (10)(1) = 20
20 > 0, therefore update.

But the equation to obtain the same answer in the solution is:
1(10 + 10) 20
20 > 0, therefore update.

I understand that these two equations are essentially the same, but written differently. But for example, in step 5, what do the elements in g5 mean. What do the -8, -16 and -2 represent?
p.s. I know in a previous (now deleted) post of mine, I asked a question related to the use of LaTeX instead for maths equations. If someone can show me a simple way to convert these equations online, I'm more than happy to use it. However, I'm unfamiliar with this software, so I need some sort of converter.
","['machine-learning', 'perceptron']","I will tell you my knowledge, correct me if I am wrong.Perceptron Learning Algorithm (PLA) is a simple method to solve the binary classification problem.Define a function:$$
f_w(x) = w^Tx + b
$$where $x \in \mathbb{R}^n$ is an input vector that contains data points and $w$ is a vector with the same dimension as $x$ which present for the parameters of our model.Call $y=label(x)=\{1,-1\}$ where $1$ and $-1$ are the label of each $x$ vector.The PLA will predict a class like this:$$
y=label(x)=sgn(f_w(x))=sgn(w^Tx+b)
$$(The definition of sgn function can be found in this wiki)We can understand that PLA tries to define a line (in 2D, or a plane in 3D, and hyperplane in more than 3 dimensions coordinate, I will assume it in 2D from now on) which separate our data into two areas. So how can we find that line? Just like every other machine learning problems, define a cost function, then optimize the parameters to have the smallest cost value.Now, let define the cost function first, you can see that if a data point lies in the correct area, $y$ and $f(x)$ have the same sign, which means $y(w^Tx+b) > 0$ and otherwise. Similar to your example, I will define:
$$
g(x)=y(w^Tx+b)
$$We ignore all the points in the safe zone ($g(x)>0$), only update to rotate or move the line to adapt with the misclassified points ($g(x)\le 0$), here, you can understand why we only update if $g(x)\le0$.We need to define a cost function to minimize it, so our cost function will become:
$$
L(w)=\displaystyle\sum_{x_i\in U}(-y_i(w^Tx_i+b))
$$
whereFor each data point, we have the derivative is
$$
\frac{\partial L}{\partial w} = -y_ix_i \\
\frac{\partial L}{\partial b} = -y_i
$$Finally, update them by Stochastic gradient descent (SGD), we get:
$$
w = w - \frac{\partial L}{\partial w} = w + y_ix_i \\
b = b - \frac{\partial L}{\partial b} = b + y_i
$$For your last question, notice that the weight and bias changed from $4$-th updated, so we have:
$$
y_5 = 1, x_5 = (4,8), w = (-2,-2), b = -2 \\
\Rightarrow g_5 = +1 \times (4\times(-2) + 8\times(-2)+ (-2)) = -8 -16 -2
$$"
Appropriate convolutional neural network architecture when the input consists of two distinct signals,"
I have a dataset consisting of a set of samples. Each sample consists of two distinct desctized signals S1(t), S2(t). Both signals are synchronous; however, they show different aspects of a phenomena.
I want to train a Convolutional Neural Network, but I don't know which architecture is appropriate for this kind of data. 
I can consider two channels for input, each corresponding to one of the signals. But, I don't think convolving two signals can produce appropriate features.
I believe the best way is to process each signal separately in the first layers, then join them in the classification layers in the final step. How can I achieve this? What architecture should I use?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']",
DDQN Agent in Othello (Reversi) game struggle to learn,"
This is my first question on this forum and I would like to welcome everyone.
I am trying to implement DDQN Agent playing Othello (Reversi) game. I have tried multiple things but the agent which seems to be properly initialized does not learn against random opponent. Actually the score is about 50-60% won games out of nearly 500. Generally if it gets some score after first 20-50 episodes it stays on the same level. I have doubts on the process of learning and how to decide when the agent is trained. Current flow is as follows:

Initialize game state.
With epsilon greedy policy choose the action to make based on currently available actions depending on game state
Get opponent to make his action
Get the reward as number of flipped places that remain after opponent move.
Save the observation to replay buffer
If number of elements in replay buffer equals or higher than batch size do the training.

What I do not know is when do I know when to stop the training. Previously this agent trained against MinMax algorithm learned how to win 100% games because MinMax played exactly the same every time. I would like the agent to generalize the game. Right now I save the network weights after the game is won but I think it does not matter. I can't see that this agent find some policy and improve over time. Whole code for the environment, agent and training loop can be found here: https://github.com/MikolajMichalski/RL_othello_mgr I would appreciate any help. I would like to understand how the RL works :)
","['reinforcement-learning', 'training', 'dqn', 'deep-rl', 'double-dqn']",
Is there any toy example that can exemplify the performance of double Q-learning?,"
I recently tried to reproduce the results of double Q-learning. However, the results are not satisfying. I have also tried to compare double Q learning with Q-learning in Taxi-v3, FrozenLake without slippery, Roulette-v0, etc. But Q-learning outperforms double Q-learning in all of these environments.
I am not sure whether if there is something wrong with my implementation as many materials about double Q actually focus on double DQN. While at the same time of checking, I wonder is there any toy example that can exemplify the performance of double Q-learning?
","['reinforcement-learning', 'q-learning', 'double-q-learning']",
What is the effect of too harsh regularization?,"
While training a CNN model, I used an l1_l2 regularization (i.e. I applied both $L_1$ and $L_2$ regularization) on the final layers. While training, I saw the training and validation losses are dropping very nicely, but the accuracies aren't changing at all! Is that due to the high regularization rate?
","['convolutional-neural-networks', 'objective-functions', 'accuracy', 'l2-regularization', 'l1-regularization']",
What kind of reinforcement learning method does AlphaGo Deepmind use to beat the best human Go player?,"
In reinforcement learning, there are model-based versus model-free methods. Within model-based ones, there are policy-based and value-based methods.
AlphaGo Deepmind RL model has beaten the best Go human player. What kind of reinforcement model does it use? Why is this particular model appropriate for Go game?
","['reinforcement-learning', 'policies', 'model-based-methods', 'model-free-methods', 'value-based-methods']",
Is there a training data capacity limit for AlphaZero (Chess)?,"
In AlphaZero, we collect ($s_t, \pi_t, z_t$) tuples from self-play, where $s_t$ is the board state, $\pi_t$ is the policy, and $z_t$ is the reward from winning/losing the game. In other DeepRL off-policy algorithms (I'm assuming here that AlphaZero is off-policy (?)) like DQN, we maintain a memory buffer (say, 1 million samples) and overwrite the buffer with newer samples if it's at capacity. Do we do the same for AlphaZero? Or do we continually add new samples without overwriting older ones? The latter option sounds very memory heavy, but I haven't read anywhere that older samples are overwritten.
","['machine-learning', 'deep-rl', 'alphazero']","AlphaZero is on-policy*, which partially answers your question.An on-policy algorithm is not the same as an online policy though, it is not required that updates are made on every step. It is simply required that all data used in the update is taken from the same ""current"" policy.In practice, AlphaZero buffers results from games played with the current policy to create a dataset used to update its neural networks. That buffer is then emptied after the data has been used.From the AlphaZero paper:At the end of the game, the terminal position $s_T$ is
scored according to the rules of the game to compute the game outcome $z: −1$ for a loss, $0$ for a draw, and $+1$ for a win. The neural network parameters $\theta$ are updated so as to minimise the error between the predicted outcome $v_t$ and the game outcome $z$, and to maximise the similarity of the policy vector $p_t$
to the search probabilities $\pi_t$.This implies only a single game is buffered in this way before running each update and then discarding the dataset generated in that game. Theoretically the same approach could be used with any number of games for each update step (provided the training system has capacity to store more moves).* AlphaZero is on-policy because the core algorithm requires using a specific policy and then updating it to match an improved version of the same policy discovered using MCTS for planning during play.It could be possible to construct an off-policy update mechanism using similar MCTS routine. I am not sure why this is not considered, but suspect it would be due to complexity/efficiency of the algorithm compared to the ease of generating new game data."
How can I do video classification while taking into account the temporal dependencies of the frames?,"
I need to solve a video classification problem. While looking for solutions, I only found solutions that transform this problem into a series of simpler image classification tasks. However, this method has a downside: we ignore the temporal relationship between the frames.
So, how can I perform video classification, with a CNN, by taking into account the temporal relationships between frames?
","['deep-learning', 'convolutional-neural-networks', 'reference-request', 'video-classification', 'action-recognition']","Look at spatio-temporal CNNs which extend the image-based CNN in 2D to 3D to handle time.  These are commonly used to detect or classify action in a video.  People have used them to identify specific actions in various sports such as kicking a soccer ball, throwing a baseball or dribbling a basketball.  They have been used to identify fire, smoke, deep fakes, and violence in videos.  Below are some articles to help get started.Source code:"
Interpretation of Inner Product in a two-tower model,"
I have seen at quite a few places the use of two-tower architecture. This(Fig 6) is one of the examples. Each tower computes embedding of a concept which is orthogonal to the concepts in the rest of the towers. Also, I have seen that the next step on getting an n-dimensional embedding from the towers an inner product is taken to measure Cosine similarity.
I would like to know how to interpret this Inner Product. The two n-dimensional embeddings represents different concepts. What is the meaning of similarity here ?
","['neural-networks', 'deep-learning']",
When is using weight regularization bad?,"
Regularization of weights (e.g. L1 or L2) keeps them small and standardized, which can help reduce data overfitting. From this article, regularization sounds favorable in many cases, but is it always encouraged? Are there scenarios in which it should be avoided?
","['neural-networks', 'regularization', 'weights', 'l2-regularization', 'l1-regularization']",
"Recent deep learning textbook (i.e. covering at least GANs, LSTM and transformers and attention)","
I am searching for an academic (i.e. with maths formulae) textbook which covers (at least) the following:

GAN
LSTM and transformers (e.g. seq2seq)
Attention mechanism

The closest match I got is Deep Learning (2016, MIT Press) but it only deals with part of the above subjects.
","['deep-learning', 'reference-request', 'generative-adversarial-networks', 'transformer', 'attention']","There are a few more books that were published after 2016 that cover some of the topics you are interested in. I've not read any of them, so I don't really know whether they are good or not, but I try to summarise if they cover some of the topics you may be interested in.Deep Learning with Python (2017), by Francois Chollet (author of the initial Keras library), which covers GANs in section 8.5 (p. 305), but it does not seem to cover transformers and attention mechanisms, although it covers other intermediate/advanced topics (not sure to which extent), such as text generation with LSTMs, DeepDream, Neural Style Transfer and VAEsGrokking deep learning (2019), by Andrew Trask, which seems to cover some intermediate/advanced topics (such as LSTMs and related tasks), but no transformers or GANs (unless I missed them); you can find the accompanying code hereGenerative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play, by David Foster, which covers many variants of GANs, VAEs and other stuffThe first transformer was published in 2017, so I guess there may not yet be a book that extensively covers it and other related models, such as the GPT models (if you're interested in CV, check this blog post, although it seems to list books that cover mostly traditional CV techniques). The attention mechanisms are older and can probably be found in textbooks that cover machine translation topics (such as seq2seq models with LSTMs), such as this one."
Spectral Networks and Deep Locally Connected Networks on Graphs,"
I’m reading the paper Spectral Networks and Deep Locally Connected Networks on Graphs and I’m having a hard time understanding the notation shown in the picture below (the scribbles are mine):

Specifically, I don’t understand the notation for the matrix F. Why does it include an i and a j?
",['graph-neural-networks'],
Policy gradient: Does it use the Markov property?,"
To derive the policy gradient, we start by writing the equation for the probability of a certain trajectory (e.g. see spinningup tutorial):
$$
\begin{align}
P_\theta(\tau) &= P_\theta(s_0, a_0, s_1, a_1, \dots, s_T, a_T) \\
& = p(s_0) \prod_{i=0}^T \pi_\theta(a_i | s_i) p(s_{i+1} | s_i, a_i)
\end{align}
$$
The expression is based on the chain rule for probability. My understanding is that the application of the chain rule should give up this expression:
$$
p(s_0)\prod_{i=0}^T \pi_\theta(a_i|s_i, a_{i-1}, s_{i-1}, a_{i-2}, \dots, s_0, a_0) p(s_{i+1} | s_i, a_i, s_{i-1}, a_{i-1}, \dots, a_0, s_0)
$$
Then the Markov property should be applicable, producing the desired equality. This should only depend on the latest state-action pair.
Here are my questions:

Is this true?

I watched this lecture about policy gradients, and at this time during the lecture, Sergey says that: ""at no point did we use the Markov property when we derived the policy gradient"", which left me confused. I assumed that the initial step of calculating the trajectory probability was using the Markov property.


","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'markov-property']",
Building a resume recommendation for a job post?,"
There are few challenges I am facing when building a resume recommendation for a particular job positing.
Let's say we convert the resume into a vector on n-dimensions and job description also as an n-dimension vector and in order to see how similar, we can use any similarity metrics like cosine.
Now, for me the biggest problem with such approach is not able to provide more importance to the job title required. Some times, for a cloud engineer position I am getting java developer resume recommended in top 10 just because some of the skills/keywords are overlapping between two so their embeddings becomes similar.
I want to provide more weightage to the job title as well. What are some possible things I can do to make my recommendations consider or put bit emphasis on job title.
Note:- A common job title lookup in resume will fails because people write job titles in multiple ways.
(java engineer (or) java developer) | (cloud engineer or aws engineer) etc.,
How can I overcome this issue?
","['neural-networks', 'machine-learning', 'deep-learning', 'natural-language-processing', 'sequence-modeling']",
What is the Bellman Equation actually telling?,"
What does the Bellman equation actually say? And are there many flavours of that?
I get a little confused when I look for the Bellman equation, because I feel like people are telling slightly different things about what it is. And I think the Bellman Equation is just basic philosophy and you can do whatever you want with that.
The interpretations that I have seen so far:
Let's consider this grid world.
+--------------+
| S6 | S7 | S8 |
+----+----+----+
| S3 | S4 | S5 |
+----+----+----+
| S0 | S1 | S2 |
+----+----+----+


Rewards: S1:10; S3:10
Starting Point: S0
Horizon: 2
Actions: Up, Down, Left, Right (If an action is not valid because there is no space, you remain in your position)

The V-Function/Value:
It tells you how good is it to be in a certain state.
With a horizon of 2, one can reach:
S0==>S3 (Up)   (R 5)
S0==>S0 (Down) (R 0)
S0==>S1 (Right)(R10)
S0==>S0 (Left) (R 0)

From that onwards
S0==>S3 (Up)   (R 5)
S0==>S0 (Down) (R 0)
S0==>S1 (Right)(R10)
S0==>S0 (Left) (R 0)

S1==>S4 (Up)   (R 0)
S1==>S1 (Down) (R10)
S1==>S2 (Right)(R 0)
S1==>S0 (Left) (R 0)

S3==>S6 (Up)   (R 0)
S3==>S0 (Down) (R 0)
S3==>S3 (Right)(R 5)
S3==>S2 (Left) (R10)

Considering no discount, this would mean that it is R=45 good to be in S0, because these are the options. Of course, you can't grab every reward, because you have to decide. Do I need to consider the best next state yet, because this would obviously reduce my expected total reward, but as I can only make two steps it would tell me what is really possible. Not what the overall Reward R(s) in that range is.
The Q-Function/Value
This function takes a state and an action, but I am not sure. If that means that I have a reward function that just considers my actions as well to give me a reward. Because in the previous example I just have to land on a state (It doesn't really matter how I get there). But this time I get a reward, when I choose a certain action. R(s,a)
But otherwise I do not rate the best action and select that next state to calculate the next state. I choose every next step and from that I choose the 2nd next.
Optimization V-function or Q-function
This works the same as V-Function or Q-Function, but it just considers the next best award. Some sort of greedy approach:
First step:
S0==>S3 (Up)   (R 5) [x]
S0==>S0 (Down) (R 0) [x]
S0==>S1 (Right)(R10)
S0==>S0 (Left) (R 0) [x]

Second Step:
S1==>S4 (Up)   (R 0) [x]
S1==>S1 (Down) (R10) 
S1==>S2 (Right)(R 0) [x]
S1==>S0 (Left) (R 0)

So, this would say that is the best I can do in two steps. I know that there is a problem, because when I just follow a greedy approach I risk that I won't get the best result, if I would have had a reward of 1000 on S2 later.
But still, I just want to know, if I have a correct understanding. I know there might be many flavours and interpretations but at least I want to know that is the correct name of these approaches.
","['reinforcement-learning', 'definitions', 'value-functions', 'bellman-equations']",
What is MNLI-(m/mm)?,"
I came across the term MNLI-(m/mm) in Table 1 of the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. I know what MNLI stands for, i.e. Multi-Genre Natural Language Inference, but I'm just unsure about the -(m/mm) part.
I tried to find some information about this in the paper GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding, but this explained only the basic Multi-Genre Language Inference concept. I assume that the m/mm part was introduced later, but this doesn't make any sense because the BERT paper appeared earlier.
It would be nice if someone knows this or has a paper that explains this.
","['terminology', 'transformer', 'bert', 'natural-language-understanding']","as far as I have found out it stands for a different type of task.I have found it here. https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=kTCFado4IrIcIt states that mnli-mm stands for the mismatched version of MNLI
That means that the mnli-m would mean matched version of MNLIOn the following link is more about the MNLI: https://cims.nyu.edu/~sbowman/multinli/Hope this helps you.Cheers."
Hairstyle Virtual Try On,"
I want to help people with cancer who are under chemotherapy, and generally people who have lost their hair to Virtually Try-On Toupees/Wigs on their head.
VTO must support both the frontal and side positions of the head.
At first, I thought I could use traditional deep-learning to find landmarks, then place the hairstyle on the head.
But the results were unrealistic and inaccurate.
2D Placing of Hair:
Some Hairstyles and Portraits fit well together:

But some don't:

They need manual modification:

Sometimes the original hair needs to be erased:

GANs promise more natural results with less manual engineering
changing hairstyle using GAN:

changing hairstyle using GAN:

So, I decided to use GANs for this Task.
Is it a good choice or is there an alternate solution?
","['deep-learning', 'generative-adversarial-networks', 'variational-autoencoder', 'u-net']",
"Is the performance of a neural network, which was trained with encrypted data and weights, affected if the weights are decrypted?","
Suppose that a neural network is trained with encrypted (for example, with homomorphic encryption and, more precisely, with the Paillier partial scheme) data. Moreover, suppose that it is also trained with encrypted weights.
If the neural network's weights are decrypted, is the performance of the neural network theoretically preserved or affected?
","['neural-networks', 'training', 'performance', 'weights']",
"How do I select the (number of) negative cases, if I'm given a set of positive cases?","
We were given a list of labeled data (around 100) of known positive cases, i.e. people that have a certain disease, i.e. all these people are labeled with the same class (disease). We also have a much larger amount of data that we can label as negative cases (all patients that are not on the known positive patient's list).
I know who the positives are, but how do I select negative cases to create a labeled dataset of both positives and negatives, on which to first train a neural network, and then test it?
This is a common problem in the medical field, where doctors have lists of patients that are positive, but, in our case, we were not given a specific list of negative cases.
I argued for picking a number that represents the true prevalence of the disease (around 1-2%). However, I was told that this isn't necessary and to do a 50:50 split of positives to negatives. It seems that doing it this way will not generalize outside our test and train datasets.
What would you do in this case?
","['neural-networks', 'imbalanced-datasets', 'selection-bias', 'training-datasets', 'test-datasets']","To select the proper dataset to construct, you should first figure out a metric to use to compare, and then select the dataset construction that gives the better metric. There is no single best metric, it depends on the task and your interpretation on what type of error is more important.If you believe it is important that errors should not be normalized across class, then use the overall accuracy, and keep your dataset distribution same as the natural distribution (so 1-2% positive cases).If you believe it is important that errors should be normalized across class, then use PR-AUC or ROC-AUC, and re-balance your dataset so that the samples a little more closer to 1:1. The exact ratio will only be determined after testing and comparing the PR-AUC or ROC-AUC metrics.Two popular metrics are ROC-AUC and PR-AUC. ROC curves (Receiver Operating Characteristics) plot the true positive rate vs false positive rate, while PR curves (Precision and Recall) plot the precision vs recall. AUC stands for ""area under curve"", because you can achieve any single point in the curve by specifying the classifier threshold, so the sum of all points (i.e. the entire area under the curve) is the most general way of comparing if one model is doing better than another.Although both ROC curves and PR curves equalize class imbalance at some capacity, PR curves are more sensitive to class imbalance. The paper The Relationship Between Precision-Recall and ROC Curves concludes that if the PR-AUC is good then the ROC-AUC will always also be good, but not the other way around. The difference is due to the fact that if the dataset has huge class imbalance, a false positive hurts PR curves significantly more than ROC curves.On the other hand, total accuracy does not normalize class imbalance at all, so therefore favors the majority class.As a result:If it helps, for most imbalance problems, people usually go for PR curves.By the way, (this paper) studies class imbalance in neural networks by optimizing the ROC curves, and show that you should definitely have equal numbers of positive and negative examples. So if you want the best performance in terms of ROC-AUC, you should do the 50:50 split. I haven't read any similar study that optimizes for PR-AUC, but my intuition tells me that it will have the same conclusion (you should do 50:50 split to optimize for PR-AUC as well)."
What is asymmetric relaxation backpropagation?,"
In Chapter 8, section 8.5.2, Raul Rojas describes how the weights for a layer of a neural network can be calculated using a pseudoinverse of the sigmoid function in the nodes, he explains this is an example of symmetric relaxation.
But the chapter doesn't explain what asymmetric relaxation would be or how it is done.
So, what is asymmetric relaxation and how would it be done in a simple neural network using a sigmoid function in its nodes?
","['neural-networks', 'terminology', 'backpropagation']",
Can I constrain my neurons in a neural network in according to the orders of the input?,"
I'm working with data that is ranked. So the inputs are 1,2,3 etc. This means the smaller numbers (ranks) are preferred to the larger ones. Hence the order is important. I want to estimate a number using regression; however, with the constraint that the order of the numbers must be monotonic non-linear.
Imagine the following input table:
1, 2
2, 3
3, 1

For instance, if the output is 1000 for each input, then the estimation could be:
1 * (800) + 2 * (100) = 1000
2 * (300) + 3 * (60) = 780
3 * (150) + 1 * (400) = 850

Evidently, the estimated Xs are monotonic non-linear decreasing.
For the first column: 1 -> 800, 2 -> 600 (2x300), 3 -> 450 (3x150)
for the second columns: 1 -> 400, 2 -> 200 (2x100), 3-> 180 (2x60)
So here's the question. Can I ensure my model (neural network) enforces the given constraint? I am using Keras.
","['neural-networks', 'machine-learning', 'deep-learning', 'keras']",
"During neural network training, can gradients leak sensitive information in case training data fed is encrypted (homomorphic)?","
Some algorithms in the literature allow recovering the input data used to train a neural network. This is done using the gradients (updates) of weights, such as in Deep Leakage from Gradients (2019) by Ligeng Zhu et al.
In case the neural network is trained using encrypted (homomorphic) input data, what could be the output of the above algorithm? Will the algorithm recover the data in clear or encrypted (as it was fed encrypted)?
","['neural-networks', 'training', 'gradient-descent', 'ai-security', 'training-datasets']","It will recover the encrypted inputs.The algorithm starts with dummy data and dummy labels, and then iteratively optimizes the dummy gradients to be close as to the original. This makes the dummy data close to the real training data:$$\mathbf{x}^{\prime *}, \mathbf{y}^{\prime *}=\underset{\mathbf{x}^{\prime}, \mathbf{y}^{\prime}}{\arg \min }\left\|\nabla W^{\prime}-\nabla W\right\|^{2}=\underset{\mathbf{x}^{\prime}, \mathbf{y}^{\prime}}{\arg \min }\left\|\frac{\partial \ell\left(F\left(\mathbf{x}^{\prime}, W\right), \mathbf{y}^{\prime}\right)}{\partial W}-\nabla W\right\|^{2}$$As the distance is minimized, the algorithm restores the original training data; in the case of encrypted training data - you should get an encrypted input (up to failures when the resulted 'input' isn't close to the original input)."
Is it possible to flip the features and labels after training a model?,"
The goal of this program is to predict a game outcome given a game-reference-id, which is a serial number like so:

id,totalGreen,totalBlue,totalRed,totalYellow,sumNumberOnGreen,sumNumberOnBlue,sumNumberOnRed,sumNumberOnYellow,gameReferenceId,createdAt,updatedAt
1,1,3,2,0,33,27,41,0,1963886,2020-08-07 20:27:49,2020-08-07 20:27:49
2,1,4,1,0,36,110,31,0,1963887,2020-08-07 20:28:37,2020-08-07 20:28:37
3,1,3,2,0,6,33,83,0,1963888,2020-08-07 20:29:27,2020-08-07 20:29:27
4,2,2,2,0,45,58,44,0,1963889,2020-08-07 20:30:17,2020-08-07 20:30:17
5,0,2,4,0,0,55,82,0,1963890,2020-08-07 20:31:07,2020-08-07 20:31:07
6,2,4,0,0,36,116,0,0,1963891,2020-08-07 20:31:57,2020-08-07 20:31:57
7,3,2,1,0,93,16,40,0,1963892,2020-08-07 20:32:47,2020-08-07 20:32:47

Here's the link for a full training dataset.
After training the model, it becomes difficult to use the model to predict the game output, since the game-reference-id is the only independent column, while others are random.
Is there a way to flip the features with the labels during prediction?
","['machine-learning', 'features', 'labels']",
"What is meant by ""the number of examples is reduced"", and why is this the case?","
I am currently studying the paper Learning and Evaluating Classifiers under Sample Selection Bias by Bianca Zadrozny. In section 3.2. Logistic Regression, the author says the following:

3.2. Logistic regression
In logistic regression, we use maximum likelihood to find the parameter vector $\beta$ of the following model:
$$P(y = 1 \mid x) = \dfrac{1}{1 + \exp(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)}$$
With sample selection bias we will instead fit:
$$P(y = 1 \mid x, s = 1) = \dfrac{1}{1 + \exp(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n)}$$
However, because we are assuming that $y$ is independent of $s$ given $x$ we have that $P(y = 1 \mid x, s = 1) = P(y = 1 \mid x)$. Thus, logistic regression is not affected by sample selection bias, except for the fact that the number of examples is reduced. Asymptotically, as long as $P(s = 1 \mid x)$ is greater than zero for all $x$, the results on a selected sample approach the results on a random sample. In fact, this is true for any learner that models $P(y \mid x)$ directly. These are all local learners.

This part is unclear to me:

However, because we are assuming that $y$ is independent of $s$ given $x$ we have that $P(y = 1 \mid x, s = 1) = P(y = 1 \mid x)$. Thus, logistic regression is not affected by sample selection bias, except for the fact that the number of examples is reduced.

What is meant by ""the number of examples is reduced"", and why is this the case?
","['classification', 'papers', 'logistic-regression', 'selection-bias']",
"On learning to rank tasks. Could it be that the input of the Siamese network is a vector, or should it be exclusively raw text?","
I'm developing a method to document and query representation as concept vectors (bag-of-concepts). I want to train a machine learning model on ranking (learning to rank a task). So I have document vector V1 and query vector V2. How should I use these two numerical vectors in learning the way to rank a task? What are the possible scenarios?
Do I calculate relevance (similarity) by cosine and then enter the result as a single feature into a neural network? Is it correct to apply Hadamard to produce a single vector representing the features of a document and query pair, and then train a neural network with it? Can two vectors (document and query vector) be entered into the Siamese network in order to evaluate the relevance? One told me this is not possible because the network only take raw text as input and extracts features. Hence, it is useless to enter a vector that was generated by my vectorization method.
","['machine-learning', 'similarity', 'bag-of-features', 'siamese-neural-network']",
How does uniform offset tiling work with function approximation?,"
I get the fundamental idea of how tilings work, but, in Barton and Sutton's book, Reinforcement Learning: An Introduction (2nd edition), a diagram, on page 219 (figure 9.11), showing the variations of uniform offset tiling has confused me.

I don't understand why all 8 of these figures are instances of uniformly offset tilings. I thought uniformly offset meant ALL tilings have to be offset an equal amount from each other which is only the case for the bottom left figure. Is my understanding wrong?
","['reinforcement-learning', 'function-approximation', 'feature-selection', 'tile-coding', 'coarse-coding']","In this particular diagram, they are showing:So the only difference between the cells in this diagram is the state the $+$ is from.Following this, they show:As such, the authors are showing how asymmetrically offset tilings tend to have less variance in how states are generalized by the tiles (in each tiling) they belong to."
How to build a Neural Network to approximate the Q-function?,"
I am learning reinforcement learning with Q-learning using online resources, like blog posts, youtube videos, and books. At this point, I have learned the underpinning concepts of reinforcement learning and how to update the q values using a lookup table.
Now, I want to create a neural network to replace the lookup table and approximate the Q-function, but I am not sure how to design the neural network. What would be the architecture for my neural network? What are the inputs and outputs?
Here are the two options I can think of.

The input of the neural network is $(s_i, a_i)$ and the output is $Q(s_i,a_i)$

The input is $(s_i)$ and the output is a vector $[Q(s_i,a_1), Q(s_i,a_2), \dots, Q(s_i,a_N)]$


Is there any other alternative architecture?
Also, how to reason about which model would be logically better?
","['neural-networks', 'reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']",
Are there any good alternatives to an LSTM language model for text generation?,"
I have a trained LSTM language model and want to use it to generate text. The standard approach for this seems to be:

Apply softmax function
Take a weighted random choice to determine the next word

This is working reasonably well for me, but it would be nice to play around with other options. Are there any good alternatives to this?
","['natural-language-processing', 'reference-request', 'language-model', 'text-generation']","The current state of the art in natural language generation are all auto-regressive transformer models. Transformers no longer use recurrent neural networks such as LSTM, because the recurrences makes long dependencies messy to calculate. Instead, Transformers only keep the attention layers, and apply attention on all the existing text so far, which can be done in parallel so therefore very fast, while being able to attend to long dependencies (e.g. understanding that ""it"" refers to ""John"" from 3 sentences ago). They are also faster to train than LSTMs (on powerful GPUs at least). The downside is more memory requirement, and you need large models and large datasets (LSTMs work better for small models and small datasets). Here is some background info on how they work.Auto-regressive transformer models only use the decoder for text generation, and removes the encoder. Given an input, they predict the next word.The most well-known one is GPT (GPT-3 has 175B parameters; GPT-2 has 1.5B parameters, and GPT-1 has 175M parameters) GPT is developed by OpenAI and is a commercial, paid-software if you want to use their official model, but I'm sure with a little digging you can find community-trained models that will perform slightly worse but is at least free to use. GPT is basically a vanilla transformer, but trained on a huge, huge dataset with a huge, huge model to achieve state-of-the-art performance.Other auto-regressive transformer models include:If your goal is just to generate English or another commonly researched language, you can use an existing pre-trained language model and avoid doing any training yourself. This saves a lot of time, and there should at least be free community-trained models readily available. Otherwise, for obscure tasks, you'd have to train one yourself, and these state of the art models will take immense resources."
How do I prove that the MSE is zero when all predictions are equal to the corresponding labels?,"
In the back-propogation algorithm, the error term is:
$$
E=\frac{1}{2}\sum_k(\hat{y}_k - y_k)^2,
$$
where $\hat{y}_k$ is a vector of outputs from the network, $y_k$ is the vector of correct labels (and we work out the error by calculating predicted-observed, squaring the answer, and then summing the answers for each $k$ (and dividing by 2).
How do you prove that if this answer is $0$ (i.e., if $E=0$), then $\hat{y}_k=y_k$ for all $k$?
","['deep-learning', 'proofs', 'mean-squared-error']","This is very easy to prove.Let's first prove that, if $\hat{y}_k = y_k$, then the $E = 0$. I will leave all steps, so that it's super clear.\begin{align}
E
&=\frac{1}{2}\sum_k(\hat{y}_k - y_k)^2 \\
&=\frac{1}{2}\sum_k(y_k - y_k)^2\\
&=\frac{1}{2}\sum_k(0)^2\\
&=\frac{1}{2}\sum_k 0\\
&=\frac{1}{2} 0\\
&=0\\
\end{align}To prove the other way around, i.e. if $E = 0$, then $\hat{y}_k = y_k$, you can do as follows\begin{align}
\frac{1}{2}\sum_k(\hat{y}_k - y_k)^2 
&=E\\
&=0
\end{align}
Recall now that any number squared is non-negative (i.e. positive or zero). Given that $(\hat{y}_k - y_k)^2 $ is non-negative, then $\sum_k(\hat{y}_k - y_k)^2$ is a sum of non-negative numbers. The only way that a sum of non-negative numbers is equal to zero is if all numbers are zero, so we must have $\hat{y}_k = y_k$ (because any non-zero number squared is non-zero).(Note that $E$ is the mean squared error, i.e. a loss function, and it's not the back-propagation algorithm, which is just the algorithm that you use to compute partial derivatives of $E$ with respect to the parameters of the model, which are not even visible in the way you wrote $E$)."
How should I define the action space for a card game like Magic: The Gathering?,"
I'm trying to learn about reinforcement learning techniques. I have little background in machine learning from university, but never more than using a CNN on the MNIST database.
My first project was to use reinforcement learning on tic-tac-toe and that went well.
In the process, I thought about creating an AI that can play a card game like Magic: The Gathering, Yu-gi-oh, etc. However, I need to think of a way to define an action space. Not only are there thousands of combinations of cards possible in a single deck, but we also have to worry about the various types of decks the machine is playing and playing against.
Although I know this is probably way too advanced for a beginner, I find attempting a project like this, challenging and stimulating. So, I looked into several different approaches for defining an action space. But I don't think this example falls into a continuous action space, or one in which I could remove actions when they are not relevant.
I found this post on this stack exchange that seems to be asking the same question. However, the answer I found didn't seem to solve any of my problems.
Wouldn't defining the action space as another level of game states just mask the exact same problem?
My main question boils down to:
Is there an easy/preferred way to make an action space for a game as complex as Magic? Or, is there another technique (other than RL) that I have yet to see that is better used here?
","['reinforcement-learning', 'ai-design', 'action-spaces']","There are several different ways you can model the state and action spaces in such sequential (extensive-form) environments/games. For environments with small action spaces or those typically introduced to beginning-RL students, the state space and action space remains constant along an agent's trajectory (termed normal form games when there are multiple agents). In sequential games which can be illustrated as trees, a ""state"" is analogous to ""information set"" which is defined as the sequence (tuple) of actions and observations since the beginning of the game's episode. Terminal states (leaf nodes) exist, and the action space $\mathcal{A}[x]$ at an information set $x$ can be defined as a union of action sequences that can be taken to each terminal state, not counting terminal states that cannot be reached from the current information set.In the above examples, I discussed games such as the examples you stated where more than one agent can interact with the environment, but this is a generalization over RL and can be applied to when only one agent is maximizing its reward."
Are there applications of Grenander's pattern theory in pattern recognition or for implementing algorithms?,"
I came across Grenander's work ""Probabilities on Algebraic Structures"" recently, and I found that much of Grenander's work focused on what he called ""Pattern Theory."" He's written many texts on the matter, and, from what I've seen, they seem like an attempt to unify some mathematical underpinnings of pattern representation. However, I'm not sure what this really means in practice, nor how it relates to results we already have in learning theory. The mathematical aspect of the work is really quite intriguing, but I am skeptical as to its practicality.
Are there any applications of Grenander's pattern theory? Either for getting a better theoretical understanding of certain methods of pattern recognition or for directly implementing algorithms?
Some links to what I'm referring to:

Wikipedia entry,
book,
first book of 3 set volume, and
research group, including fields medalist David Mumford.

",['applications'],
"What is the best algorithm for optimizing profit, rather than making predictions?","
I am new to machine learning, so I am not sure which algorithms to look at for my business problem. Most of what I am seeing in tools like KNIME are geared toward making a prediction/classification, and focusing on the accuracy of that single prediction/classification.
Instead, in general terms, I want to optimize toward maximum profit of a business process/strategy, rather than simply trying to choose the ""best"" transaction from within a set of possible transactions, which is quite different. The latter approach will simply give the best ""transaction success percentage"", without regard for overall profit of the strategy in the aggregate.
This is how the business problem is structured: Each Opportunity is a type of business strategy ""game"" between Entities. Each Entity is unaware, unaffected, and uninterested in conditions or events outside of the Opportunity, such as Observers. Each Opportunity is an independent event with no affect on other concurrent or future Opportunities, and with no effect on decisions unrelated to the Opportunity itself. Each Opportunity will have one and only one Awarded Entity, which is the Entity that ""wins"" the business process.
Observers, however, may create a Market for each Opportunity. Within such an independent, ephemeral Market, the Observers may bid among themselves as to which Entity will be the Awarded Entity for the Opportunity. Each Bid is a fixed-size transaction. A Bid is associated with only one Entity within the Opportunity. Thus, a Bid is a type of vote on the outcome of the Opportunity. There is no limit to the number of Bids that an Observer may place into the Market, but each Observer may only place Bids on a single Entity within the Market. Thus, the total amount of Bids on an individual Entity within the Opportunity represent the confidence level, within that Market, of the prediction.
At the resolution of the Opportunity, one Entity will be the Awarded Entity for the Opportunity. This determination is made based on factors outside of the control of the Observers. The Observers have no influence over the Opportunity nor the Entities within it. When the Awarded Entity is determined, the total value of Bids placed on that Entity are refunded to the Observers that placed them. Additionally, the value of all Bids placed on other Entities are shared among the Observers who bid correctly on the Awarded Entity. Each Observer that placed a correct Bid on the Awarded Entity is entitled to a fraction of the remaining Market value, in equal proportion to the number of fixed Bids placed. In other words, the Market is a zero sum scenario. Bids may be placed at any time during the duration of the Opportunity, from when its Market is created, up until a deadline just shortly before its resolution. The total number of outstanding Bids on each Entity is another data point that is available in real time, and which fluctuates during the duration of the Opportunity, based on total Bids placed and the ratios between Bids on each Entity participating in the Opportunity.
To support the Observers' evaluation and prediction of Awarded Entity within the scope of Opportunities, there are thousands of data points available, as well as extensive history and analytics regarding each Entity involved. Each Observer will employ their own unique strategy to predict Opportunity outcomes. The objective of this algorithm is to optimize a prediction strategy that does not optimize for ""percentage of correct predictions"", but rather ""maximum gain"". Rather than be ""most correct most often"", the model should strive to use the data to create advantages for maximum gain in the aggregate, rather than strive to be the most correct. An Observer is rewarded not for being correct most often, but for recognizing inefficiencies in the Bids within the Market.
I am considering hand-coding a genetic algorithm for this, so that I can write a custom fitness function that computes overall profitability of the strategy, and run the generations to optimize profit instead of individual selection accuracy. However, I'd rather use an out-of-the-box algorithm supported by a tool like KNIME if possible.
Thank you!
","['algorithm', 'optimization']",
What's the difference between architectures and backbones?,"
In the paper ""ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery"", the authors talk about using:

Feature Pyramid Networks (as the architecture)
EfficientNet-B2 (as the backbone)


Performance Measures on the Validation Set. The RF model that only inputs data from the visible Landsat 8 bands achieved the lowest
performance on the validation set, but the incorporation of auxiliary
predictors substantially improved its performance. All of the CNN
models outperformed the RF models. The best performing model, which we
call ForestNet, used an FPN architecture with an EfficientNet-B2
backbone. The use of SDA provided large performance gains on the
validation set, and land cover pre-training and incorporating
auxiliary predictors each led to additional performance improvements.

What's the difference between architectures and backbones? I can't find much online. Specifically, what are their respective purposes? From a high-level perspective, what would integrating the two look like?
","['deep-learning', 'convolutional-neural-networks', 'comparison', 'terminology', 'architecture']",
"In RL as probabilistic inference, why do we take a probability to be $\exp(r(s_t, a_t))$?","
In section 2 the paper Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review the author is discussing formulating the RL problem as a probabilistic graphical model. They introduce a binary optimality variable $\mathcal{O}_t$ which denotes whether time step $t$ was optimal (1 if so, 0 otherwise). They then define the probability that this random variable equals 1 to be
$$\mathbb{P}(\mathcal{O}_t = 1 | s_t, a_t) = \exp(r(s_t, a_t)) \; .$$
My question is why do they do this? In the paper they make no assumptions about the value of the rewards (e.g. bounding it to be non-positive) so in theory the rewards can take any value and thus the RHS can be larger than 1. This is obviously invalid for a probability. It would make sense if there was some normalising constant, or if the author said that the probability is proportional to this, but they don't.
I have searched online and nobody seems to have asked this question which makes me feel like I am missing something quite obvious so I would appreciate if somebody could clear this up for me please.
","['reinforcement-learning', 'probability', 'inference', 'probabilistic-graphical-models']","After doing some further reading, it turns out that negative rewards are an assumption for this distribution to hold. However, the author notes that as long as you don't receive a reward of infinity for any action then it is possible to re-scale your rewards by subtracting the maximum value of your potential rewards so that they are always negative."
"Why can't we combine both training and validation data, given that both types of data are used for developing the model?","
Sorry if I sound confused. I read that data to be fed to a machine are divided into training, validation and test data. Both training and validation data are used for developing the model. Test data is used only for testing the model and no tuning of the model is done using test data.
Why is there a need to separate out training and validation data since both sets of data are for developing/tuning the model? Why not keep things simple and combine both data sets into a single one?
","['machine-learning', 'datasets']",
Regression For Elliptical Curve Public Key Generation Possible?,"
As part of a learning more about deep learning, I have been experimenting with writing ResNets with Dense layers to do different types of regression.
I was interested in trying a harder problem and have been working on a network that, given a private key, could perform point multiplication along ECC curve to obtain a public key.
I have tried training on a dataset of generated keypairs, but am seeing the test loss values bounce around like crazy with train loss values eventually decreasing after many epochs due to what I assume is overfitting.
Is this public key generation problem even solvable with a deep learning architecture? If so, am I doing something wrong with my current approach?
","['deep-learning', 'deep-neural-networks', 'residual-networks']","I don't know of any neural network that can do cryptography well, so you would have to do a little experimenting yourself. The main thing that sticks out to me is that doing operations in the elliptical curve requires the modulus operator since it works in finite field, and I think neural networks have a hard time learning the modulus operator in general. So I would focus on that first. Some things to help the network learn the modulus operator:For rapid iterating, I would test with a much smaller finite field. E.g. use a 8 bit security and see if the neural network can do well with that first before moving on to the full 256-bit security key (or whatever your end goal is).Taking this a step further, I would first test to see if the neural network can even perform a point addition in the elliptical curve well, because if it doesn't then it definitely can't do point multiplication which is needed to compute the public key."
"Bayesian hyperparameter optimization, is it worth it?","
In the Deep Learning book by Goodfellow et al., section 11.4.5 (p. 438), the following claims can be found:

Currently, we cannot unambiguously recommend Bayesian hyperparameter
optimization as an established tool for achieving better deep learning results or for obtaining those results with less effort. Bayesian hyperparameter optimization sometimes performs comparably to human experts, sometimes better, but fails catastrophically on other problems. It may be worth trying to see if it works on a particular problem but is not yet sufficiently mature or reliable

Personally, I never used Bayesian hyperparameter optimization. I prefer the simplicity of grid search and random search.
As a first approximation, I'm considering easy AI tasks, such as multi-class classification problems with DNNs and CNNs.
In which cases should I take it into consideration, is it worth it?
","['deep-learning', 'hyperparameter-optimization', 'bayesian-optimization', 'grid-search', 'random-search']",
Why is there a Uniform and Normal version of He / Xavier initialization in DL libraries?,"
Two of the most popular initialization schemes for neural network weights today are Xavier and He. Both methods propose random weight initialization with a variance dependent on the number of input and output units. Xavier proposes
$$W \sim \mathcal{U}\Bigg[-\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}},\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}}\Bigg]$$
for networks with $\text{tanh}$ activation function and He proposes
$$W \sim \mathcal{N}(0,\sqrt{s/n_{in}})$$
for $\text{ReLU}$ activation. Both initialization schemes are implemented in the most commonly used deep learning libraries for python, PyTorch and TensorFlow.
However, for both versions we have a normal and uniform version. Now the main argument of both papers is about the variance of the information at initialization time (which is dependent on the non-linearity) and that it should stay constant across all layers when back-propagating. I see how one can simply adjust the bounds $[-a,a]$ of a uniform variable in such a way that the random variable has the desired standard deviation and vice versa ($\sigma = a/\sqrt{3}$), but I'm not sure why we need a normal and a uniform version for both schemes? Wouldn't it be just enough to have only normal or only uniform? Or uniform Xavier and normal He as proposed in their papers?
I can imagine uniform distributions are easier to sample from a computational point of view, but since we do the initialization operation only once at the beginning, the computational cost is negligible compared to that from training. Further uniform variables are bounded, so there are no long tail observations as one would expect in a normal. I suppose that's why both libraries have truncated normal initializations.
Are there any theoretical, computational or empirical justifications for when to use a normal over a uniform, or a uniform over a normal weight initialization regardless of the final weight variance?
","['neural-networks', 'training', 'weights', 'weights-initialization']",
Can the attention mechanism improve the performance in the case of short sequences?,"
I am aware that the attention mechanism can be used to deal with long sequences, where problems related to gradient vanishing and, more generally, representing effectively the whole sequence arise.
However, I was wondering if attention, applied either to seq2seq RNN/GRU/LSTM or via Transformers, can contribute to improving the overall performance (as well as giving some sort of interpretability through the attention weights?) in the case of relatively short sequences (let's say around 20-30 elements each).
","['deep-learning', 'natural-language-processing', 'transformer', 'attention', 'performance']",
How to deal with KerasRL DDPG algorithm getting stuck in a local optima?,"
I am using KerasRL DDPG to try to learn a policy on my own custom environment, but the agent is stuck in a local optima although I am adding the OrnsteinUhlenbeck randomization process. I used the exact same DDPG to solve Pendulum-v0 and it works, but my environment is a more complex with a continuous space/action space.
How do you deal with local optima problem in reinforcement learning? is it just an exploitation issue?
More details:
My state space is not pixels, it is numerical, in fact it's a metro line simulator and the state space is the velocity, the position of each train on the line and the number of passengers at each station. I need to control the different trains so I am not trying to control only one train but all the operational trains and each one can have different actions such as speed or not, stay longer on the next station or not etc.
1/ I am using the same ANN architecture for the actor and critic: 3 FC layers with (512, 256, 256) hidden units.
2/ Adam optimizer for both the actor and critic with a small lr=1e-7 and clipnorm=1.
3/ nb_steps_warmup_critic=1000, nb_steps_warmup_actor=1000
4/ SequentialMemory(limit=1000000, window_length=1)
5/ The environement is a simulator of a metro line with a continuous state and action space
","['reinforcement-learning', 'keras', 'open-ai', 'gym', 'ddpg']",
How to normalize for perceptual loss when training neural net from scratch?,"
Let's say we are training a new neural network from scratch. I calculate the mean and standard deviation of my dataset (assume I am training a fully convolutional neural net and my dataset is images) and I standardise each channel of all images based on that mean and standard deviation. My output will be another image.
I want to use for example VGG for perceptual loss (VGG's weights will be frozen). Perceptual loss is when you input your prediction to a pretrained network to extract features from it. Then you do the same for the ground truth and the L2 distance between the features from ground truth and features from prediction is called perceptual loss.
As far as I know, I am supposed to standardise my data based on the mean and standard deviation VGG was trained with (since I am using VGG for inference essentially), which is different than the mean and standard deviation of my dataset. What is the correct way to do this? Should I undo the standardization of my dataset by multiplying standard deviation and adding the original mean to the output of my network, and then restandardise using VGG's statistics to calculate the loss? Or should I continue without restandardising?
","['deep-learning', 'computer-vision']",
"In the update rule of RMSprop, do we divide by a matrix?","
I've been trying to understand RMSprop for a long time, but there's something that keeps eluding me.
Here is a screenshot from this video by Andrew Ng.

From the element-wise comment, from what I understand, $dW$ and $db$ are matrices, so that must mean that $S_{dW}$ is a matrix (or tensor) as well.
So, in the update rule, do they divide a matrix by another matrix? From what I saw on google, no such action exists.
","['deep-learning', 'optimization', 'gradient-descent', 'optimizers']",
Is there a proof to explain why XOR cannot be linearly separable?,"
Can someone explain to me with a proof or example why you can't linearly separate XOR (and therefore need a neural network, the context I'm looking at it in)?
I understand why it's not linearly separable if you draw it graphically (e.g. here), but I can't seem to find a formal proof somewhere, I wanted to try and understand it with either an equation or example written down. I'm wondering if one exists (I guess it has to do with contradictions?), but I can't seem to find it? I have seen this, but it's more a reason than a proof.
","['proofs', 'perceptron', 'xor-problem']","Before proving that XOR cannot be linearly separable, we first need to prove a  lemma:Lemma: If 3 points are collinear and the middle point has a different label than the other two, then these 3 points cannot be linearly separable.Proof: Let us label the points as point $A$, $B$, and $C$. $A$ and $C$ have the same label, and $B$ has a different label. They are all collinear with line $\mathcal{L}$.Assume the contradiction, so a line can linearly separate $A$, $B$, and $C$. This means a line must cross between segment $AB$, and segment $BC$ to linearly separate these three points (by definition of linear separability). Let us label the point where the line crosses segment $AB$ and point $Y$, and the point where the line corsses segment $BC$ and point $Z$.However, since segments $AB$ and $BC$ are collinear to line $\mathcal{L}$, points $Y$ and $Z$ also falls on line $\mathcal{L}$. Since only one unique line can cross 2 points, it must be that the only line that passes segments $AB$ and $BC$ and (therefore separates points $A$, $B$, and $C$) is line $\mathcal{L}$.However, line $\mathcal{L}$ cannot linearly separate $A$, $B$, and $C$, since line $\mathcal{L}$ also crosses them. Therefore, no line exists can separate $A$, $B$, and $C$.Consider these 4 points that represent a XOR table. Let us label them clock-wise, so the top-left point as $A$, top-right point as $B$, bottom-right point as $C$, and bottom-left point as $D$. So $A$ and $C$ have the same label, and $B$ and $D$ have the same label. We want to show that points $A, B, C$ and $D$ cannot be linearly separable.Assume the contradiction, and that there is a line that can separate these 4 points.Imagine a fifth point that lies in the center, and let us label this as point $E$.Since $E$ lies in the center, the three points $A, E$ and $C$ are collinear. Similarly, since $E$ lies in the center, the three points $B, E$ and $D$ are collinear.Because we assume a line can linearly separate $A, B, C$ and $D$, then this line must label point $E$ as some label. If $E$ shares the same label as $A$ and $C$, then the points $B, E$ and $D$ will become ""collinear points where the middle point has a different label"", which by Lemma 1 cannot be linearly separable. Likewise, if $E$ shares the same label as $B$ and $D$, then the points $A, E$ and $C$ will become ""collinear points where the middle point has a different label"", which by Lemma 1 cannot be linearly separable.Therefore it is impossible to give a label to $E$ while satisfying linear separability. As a result, our assumption must be false, and the four points $A, B, C$ and $D$ cannot be linearly separable."
"In attention models with multiple layers, are weight matrices shared across layers?","
In articles that describe neural architectures with multiple attention layers of the same form, are the weight matrices usually the same across the layers?  Consider as an example, ""Attention is all you need"". The authors stack several layers of multi-head self-attention in which each layer has the same number of heads.  Each head $i$ involves a trainable weight matrix $W_{i}^{Q}$.  There is no subscript, superscript, or any other indication that this matrix is different for each layer.  My questions is this: are there separate $W_{i}^{Q}$ for layers $1,2,3,...$ or is this a single matrix shared throughout layers?
My intuition is that the authors of the paper wanted to cut down on notation, and that the matrices are different in different layers.  But I want to be sure I understand this, since I see the same kind of thing in many other papers as well.
","['transformer', 'attention', 'weights']","Weights are not normally shared across Transformer layers in vanilla Transformers. However, there has been research done in testing out sharing weights, and sometimes they improve the scores. Here are some examples:ALBERT is an improvement on BERT (so only uses the encoding side, no decoder), and shows that sharing the attention weights only $\left\{ W_i^Q, W_i^K, W_i^V \right\}$ across all Transformer layers for large networks either result in the same accuracy or slightly improved accuracy, while significantly reducing model size. Sharing the position-wise FFN layer though hindered performance.Text-to-Text Transfer Transformer shows that sharing the weights between encoder and decoder layer of the transformer (so e.g. layer 1 encoding weights = layer 1 decoding weights) barely affected accuracy (it dropped by 0.5%), but the model size is halved.I'm sure there are more papers I have forgotten. Sharing weights is still an active area of research, but for vanilla transformers it is assumed they do not share weights."
"Does the image is logistic regression or SVM, and why? [closed]","







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Does the image is logistic regression or SVM, and why?

","['machine-learning', 'support-vector-machine', 'logistic-regression']","The straight dashed-line shows the typical decision line in logistic regression or any linear classifier. The dashed-circle shows the decision line from SVM.
Obviously, since the data is not linearly separable in the original 2D feature space, if someone makes a higher dimension space by taking into account non-linear interaction of the original 2 features then they can discriminate between x and o data using a linear discriminator applied in higher dimensions. This shows the beauty of kernel methods that can make a linear yet high-dimensional (infinite dimensions indeed) problem from a non-linear low-dimensional problem (finite dimensions actually)."
Stack of Planes as the Action Space Representation for AlphaZero (Chess),"
I have a question regarding the action space of the policy network used in AlphaZero.
From the paper:

We represent the policy π(a|s) by a 8 × 8 × 73 stack of planes encoding a probability distribution over 4,672 possible moves. Each of the 8 × 8 positions identifies the square from which to “pick up” a piece. The first 56 planes encode possible ‘queen moves’ for any piece: a number of squares [1..7] in which the piece will be moved, along one of eight relative compass directions {N,NE,E,SE,S,SW,W,NW}......The policy in Go is represented identically to AlphaGo Zero (29), using a flat distribution over 19 × 19 + 1 moves representing possible stone placements and the pass move. We also tried using a flat distribution over moves for chess and shogi; the final result was almost identical although training was slightly slower.

I don't understand why a stack of planes is used for the action space here. I'm also not entirely sure I understand how this representation is used. My guess is that for Chess, the 8x8 plane represents the board, and each square has a probability assigned to it of picking up a piece on that square (let's assume that all illegal moves haven't been masked yet, so all squares have probability mass on them). From there, we choose from possible 'Queen' type moves or 'Knight' type moves, which total to 73 different types of moves. Is this interpretation correct? How would one go from this representation to sampling a legal move (i.e. how is this used to parametrize a distribution I can actually sample moves from?)
During MCTS when expanding a leaf node, we get $p_a$, the probability of taking some action $a$ from the policy head, so I would also need to be able to go from this 'planes' representation to the probability of taking a specific action.
The paper also mentions trying out 'flat distributions', which I'm not entirely sure what this means either.
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'chess', 'alphazero']",
Multi Armed Bandits with large number of arms,"
I'm dealing with a (stochastic) Multi Armed Bandit (MAB) with a large number of arms.
Consider a pizza machine that produces a pizza depending on an input $i$ (equivalent to an arm). The (finite) set of arms $K$ is given by $K=X_1\times X_2 \times X_3\times X_4$ where $X_j$ denote the set of possible amounts of ingredient $j$.
e.g.
$X_1=\{$ small, medium, large $\}$ (amount of cheese) or
$X_2=\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\}$ (slices of salami)
Thus, running the pizza machine with input $i$ is equivalent to pulling arm $i\in K$. Due to the different permutations, the number of arms $|K|$ is very large (between 100,000 and 1,000,000). Depending on the pulled arm $i$, the machine generates a pizza (associated with a reward that indicates how delicious the pizza is).
However, the machine's rewards are non-static. Pulling an arm $i$ generates a reward according to an unknown (arm specific) distribution $P_i$, with all rewards drawn from $P_i$ beeing i.i.d.. In addition, it is possible to normalize all rewards to the interval [0,1].
The above problem corresponds to the standard problem of a stochastic MAB, but is characterized by the large number of arms. In the case of the pizza machine, several days of computation time are available to determine the best pizza, so the number of itarations is allowed to be large as well.
In my investigation of MAB algorithms addressing a large number of arms, I came across studies that could call up to a few thousand arms.
Are there algorithms that exist in the MAB domain, which specifically deal with large problem instances (e.g.with $|K|>100,000$)?
","['reinforcement-learning', 'multi-armed-bandits', 'epsilon-greedy-policy', 'upper-confidence-bound']",
"In the multi-head attention mechanism of the transformer, why do we need both $W_i^Q$ and ${W_i^K}^T$?","
In the Attention is all you need paper, on the 4th page, we have equation 1, which describes the self-attention mechanism of the transformer architecture
$$
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$
Everything is fine up to here.
Then they introduce the multi-head attention, which is described by the following equation.
$$
\begin{aligned}
\text { MultiHead }(Q, K, V) &=\text { Concat}\left(\text {head}_{1}, \ldots, \text {head}_{\mathrm{h}}\right) W^{O} \\
\text { where head}_{\mathrm{i}} &=\text {Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}
$$
Once the multi-head attention is motivated at the end of page 4, they state that for a single head (the $i$th head), the query $Q$ and key $K$ inputs are first linearly projected by $W_i^Q$ and $W_i^K$, then dot product is calculated, let's say $Q_i^p =  Q W_i^Q$ and $K_i^p = K W_i^K$.
Therefore, the dot product of the projected query and key becomes the following from simple linear algebra.
$$Q_i^p {K_i^p}^\intercal = Q W_i^Q {W_i^K}^T K^T =  Q W_i K^T,$$
where
$$W_i = W_i^Q {W_i^K}^T$$
Here, $W$ is the outer product of query projection by the key projection matrix. However, it is a matrix with shape $d_{model} \times d_{model}$. Why did the authors not define only a $W_i$ instead of $W_i^Q$ and $W_i^K$ pair which have $2 \times d_{model} \times d_{k}$ elements? In deep learning applications, I think it would be very inefficient.
Is there something that I am missing, like these 2 matrices $W_i^Q$ and $W_i^K$ should be separate because of this and that?
","['deep-learning', 'transformer', 'attention', 'weights', 'efficiency']","I'll use notation from the paper you cited, and any other readers should refer to the paper (widely available) for definitions of notation.  The utility of using $W^Q$ and $W^K$, rather than $W$, lies in the fact that they allow us to add fewer parameters to our architecture.  $W$ has dimension $d_{model} \times d_{model}$, which means that we are adding $d_{model}^2$ parameters to our architecture.  $W^Q$ and $W^K$ each have dimension $d_{model} \times d_k$, and $d_k=\frac{d_{model}}{h}$. If we use these two matrices, we only add $2\frac{d_{model}^2}{h}$ parameters to our architecture, even though their multiplication (with the transpose) allows us to have the correct dimensions for matrix multiplication with $Q$ and $K$.We do use $h$ attention heads, which then brings our number of parameters back up, but the multiple heads let the model attend to different pieces of information in our data."
"In AlphaZero, do we need to store the data of terminal states?","
I have a question about the training data used during the update/back-propagation step of the neural network in AlphaZero.
From the paper:

The data for each time-step $t$ is stored as ($s_t, \pi_t, z_t$) where $z_t = \pm r_T$ is the game winner from the perspective of the current player at step $t$. In parallel (Figure 1b), new network parameters $\Theta_i$ are trained from data ($s,\pi, z$) sampled uniformly among all time-steps of the last iteration(s) of self-play

Regarding the policy at time $t$ ($\pi_t$), I understood this as the probability distribution of taking some action that is proportional to the visit count to each child node, i.e. during MCTS, given some parent node (state) at time $t$, if some child node (subsequent state) $a$ is visited $N_a$ times and all children nodes are visited $\sum_b N_b$ times, then the probability of $a$ (and its corresponding move) being sampled is $\frac{N_a}{\sum_b N_b}$, and this parametrizes the distribution $\pi_t$. Is this correct? If this is the case, then for some terminal state $T$, we can't parametrize a distribution because we have no children nodes (states) to visit. Does that mean we don't add ($s_T, \pi_T, z_T$) to the training data?
Also, a followup question regarding the loss function:

$l = (z-v)^2 - \pi^T log\textbf{p} + c||\Theta||^2$

I'm confused about this $\pi^T$ notation. My best guess is that this is a vector of actions sampled from all policies in the $N$ X $(s_t, \pi_t, z_t)$ minibatch, but I'm not sure. (PS the $T$ used in $\pi^T$ is different from the $T$ used to denote a terminal state if you look at the paper. Sorry for the confusion, I don't know how to write two different looking T's)
","['reinforcement-learning', 'deep-rl', 'alphazero', 'chess', 'notation']","I'm not 100% sure whether or not they added any data for terminal game states, but it's very reasonable to indeed make the choice not to include data for terminal game states. As you rightly pointed out, we don't have any meaningful targets to update the policy head towards in those cases, and this is not really a problem because we would also never actually make use of the policy output in a terminal game state. For the value head we could provide meaningful targets to update towards, but again we would never actually have to make use of such outputs; if we encounter a terminal game state in a tree search, we just back up the true value of that terminal game state instead of making a call to the network to obtain a value function approximation.In theory, I could imagine some cases where training the value head on terminal game states might be slightly beneficial despite not being strictly necessary; it could enable generalisation to similar game states that are not terminal (but close to being terminal), and speed up learning for those. For example, if you have a game where the goal is to complete a line of $5$ pieces, training the value head on terminal states where you actually have a line of $5$ pieces and have entirely won the game might generalise and speed up learning for similar game states where you may not yet have $5$ pieces in a line, but are very close to that goal. That said, intuitively I really don't feel like this would provide a big benefit (if any), and we could probably also come up with cases where it would be harmful.In the $\pi^{\text{T}}$ notation, $\pi$ is a vector (for any arbitrary time step, the time step is not specified here) containing a discrete probability distribution over actions (visit counts of MCTS, normalised into a probability distribution), and the $\text{T}$ simply denotes that we take the transpose of that vector. Personally I don't like the notation though, I prefer something like $\pi^{\top}$ which is more clearly distinct from a letter $T$ or $\text{T}$.Anyway, once you understand that to denote the transpose, you'll see that $\pi^{\top}\log(\mathbf{p})$ is a dot product between two vectors, which then ends up being a single scalar."
Why do I get higher average dice accuracy for less data,"
I am working on image segmentation of MRI thigh images with deep learning (Unet). I noticed that I get a higher average dice accuracy over my predicted masks if I have less samples in the test data set.
I am calculating it in tensorflow as
def dice_coefficient(y_true, y_pred, smooth=0.00001):
y_true_f = K.flatten(y_true)
y_pred_f = K.flatten(y_pred)
intersection = K.sum(y_true_f * y_pred_f)
return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

the difference is 0.003 if I have 4x more samples.
I am calculating the dice coefficient over each MRI 2D slice
Why could this be?
This figure shows how the accuracy decreases with the fraction of samples. I start with 0.1 of the data until the whole data set. The splitting of the data was random 
","['tensorflow', 'data-science', 'u-net', 'accuracy']",
Can we use ML to do anything else other than predicting (in the case of mathematical problems)?,"
(The math problem here just serves as an example, my question is on this type of problems in general).
Given two Schur polynomials, $s_\mu$, $s_\nu$, we know that we can decompose their product into a linear combination of other Schur polynomials.
$$s_\mu s_\nu = \sum_\lambda c_{\mu,\nu}^\lambda s_\lambda$$
and we call $c_{\mu,\nu}^\lambda$ the LR coefficient (always an non-negative integer).
Hence, a natural supervised learning problem is to predict whether the LR coefficient is of a certain value or not given the tuple $<\mu, \nu, \lambda>$. This is not difficult.
My question is: can we either use ML/RL to do anything else other than predicting (in this situation) or extract anything from the prediction result? In other words, a statement like ""oh, I am 98% confident that this LR coefficient is 0"" does not imply anything mathematically interesting?
","['neural-networks', 'deep-learning', 'datasets', 'math', 'supervised-learning']","There are quite a few examples of papers where they try and 'teach' neural networks to 'learn' how to solve math problems. Most of the time, sadly, it comes down to training on a large dataset after which the network can 'solve' the sort of basic problems, but is unable to generalize this to larger problems. That is, if you train a neural network to solve addition, it will be inherently constrained by the dataset. It might be able to semi-sufficiently solve addition with 3 or even 4 digits, depending on how big your dataset is, but throw in an addition question containing 2 10 digit numbers, and it will almost always fail.The latest example that I can remember where they tried this is in the General Language Model GPT-3, which was not made to solve equations per se, but does 'a decent job' on the stuff that was in the dataset. Facebook AI made an 'advanced math solver' with a specific architecture that i have not looked into which might disproof my point, but you can look into that.In the end, this comes down to 'what is learning' and 'what do you want to accomplish'. Most agree that these network are not able to generalize beyond their datasets. Some might say that not being able to generalize does not mean that it is not learning. It might just be learning slower. I believe that these models are inherently limited to what is presented in the dataset. Given a good dataset, it might be able to generalize to cases 'near and in-between', but I have yet to see a case where this sort of stuff generalizes to cases 'far outside' the dataset."
Is there any rule of thumb to determine the amount of data needed to train a CNN,"
I am training an AlexNet Convolutional Neural Network to classify images in a dataset. I want to know if there is any general rule for using data augmentation in training a neural network. How can I make sure about the amount of data, and how can I know if I need more data?
","['deep-learning', 'convolutional-neural-networks', 'training', 'datasets', 'data-augmentation']",
Why is KL divergence used so often in Machine Learning?,"
The KL Divergence is quite easy to compute in closed form for simple distributions -such as Gaussians- but has some not-very-nice properties. For example, it is not symmetrical (thus it is not a metric) and it does not respect the triangular inequality.
What is the reason it is used so often in ML? Aren't there other statistical distances that can be used instead?
","['probability-distribution', 'kl-divergence', 'wasserstein-metric', 'total-variational-distance']",
"Is my pseudocode titled ""Monte Carlo Exploring Starts (with model)"" correct?","
Reinforcement Learning: An Introduction second edition, Richard S. Sutton and Andrew G. Barto:


We made two unlikely assumptions above in order to easily obtain this guarantee of
convergence for the Monte Carlo method. ... For now we focus on the assumption that policy evaluation operates on an infinite
number of episodes. This assumption is relatively easy to remove. In fact, the same issue
arises even in classical DP methods such as iterative policy evaluation, which also converge
only asymptotically to the true value function.




There is a second approach to avoiding the infinite number of episodes nominally
required for policy evaluation, in which we give up trying to complete policy evaluation
before returning to policy improvement. On each evaluation step we move the value
function toward q⇡k, but we do not expect to actually get close except over many steps.
We used this idea when we first introduced the idea of GPI in Section 4.6. One extreme
form of the idea is value iteration, in which only one iteration of iterative policy evaluation
is performed between each step of policy improvement. The in-place version of value
iteration is even more extreme; there we alternate between improvement and evaluation
steps for single states.


The original pseudocode:

Monte Carlo ES (Exploring Starts), for estimating $\pi \approx \pi_{*}$
Initialize:
$\quad$ $\pi(s) \in \mathcal{A}(s)$ (arbitrarily), for all $s \in \mathcal{S}$
$\quad$ $Q(s, a) \in \mathbb{R}$ (arbitrarily), for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$
$\quad$ $Returns(s, a) \leftarrow$ empty list, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$
Loop forever (for each episode):
$\quad$ Choose $S_{0} \in \mathcal{S}, A_{0} \in \mathcal{A}\left(S_{0}\right)$ randomly such that all pairs have probability $\geq 0$
$\quad$ Generate an episode from $S_{0}, A_{0},$ following $\pi: S_{0}, A_{0}, R_{1}, \ldots, S_{T-1}, A_{T-1}, R_{T}$
$\quad$ $G \leftarrow 0$
$\quad$ Loop for each step of episode, $t=T-1, T-2, \ldots, 0$
$\quad\quad$ $G \leftarrow \gamma G+R_{t+1}$
$\quad\quad$ Unless the pair $S_{t}, A_{t}$ appears in $S_{0}, A_{0}, S_{1}, A_{1} \ldots, S_{t-1}, A_{t-1}:$
$\quad\quad\quad$ Append $G$ to $Returns\left(S_{t}, A_{t}\right)$
$\quad\quad\quad$ $Q\left(S_{t}, A_{t}\right) \leftarrow \text{average}\left(Returns\left(S_{t}, A_{t}\right)\right)$
$\quad\quad\quad$ $\pi\left(S_{t}\right) \leftarrow \arg \max _{a} Q\left(S_{t}, a\right)$

I want to make the same algorithm but with a model. The book states:


With a model, state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the chapter on DP.


So based on the 1st quote I must use ""stars exploration"" and ""one evaluation — one improvement"" ideas (as well as in model-free version) to make the algorithm converge.
My version of the pseudocode:

Monte Carlo ES (Exploring Starts), for estimating $\pi \approx \pi_{*}$ (with model)
Initialize:
$\quad$ $\pi(s) \in \mathcal{A}(s)$ (arbitrarily), for all $s \in \mathcal{S}$
$\quad$ $V(s) \in \mathbb{R}$ (arbitrarily), for all $s \in \mathcal{S}$
$\quad$ $Returns(s) \leftarrow$ empty list, for all $s \in \mathcal{S}$
Loop forever (for each episode):
$\quad$ Choose $S_{0} \in \mathcal{S}, A_{0} \in \mathcal{A}\left(S_{0}\right)$ randomly such that all pairs have probability $\geq 0$
$\quad$ Generate an episode from $S_{0}, A_{0},$ following $\pi: S_{0}, A_{0}, R_{1}, \ldots, S_{T-1}, A_{T-1}, R_{T}$
$\quad$ $G \leftarrow 0$
$\quad$ Loop for each step of episode, $t=T-1, T-2, \ldots, 1$:
$\quad\quad$ $G \leftarrow \gamma G+R_{t+1}$
$\quad\quad$ Unless $S_{t}$ appears in $S_{0}, S_{1}, \ldots, S_{t-1}:$
$\quad\quad\quad$ Append $G$ to $Returns \left(S_{t}\right)$
$\quad\quad\quad$ $V\left(S_{t}\right)\leftarrow\text{average}\left(Returns\left(S_{t}\right)\right)$
$\quad\quad\quad$ $\pi\left(S_{t-1}\right) \leftarrow \operatorname{argmax}_{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid S_{t-1}, a\right)\left[\gamma V\left(s^{\prime}\right)+r\right]$

— Here I update the policy in $S_{t-1}$ because the step before we update $V(S_{t})$ and changes to $V(S_{t})$ don't affect $\pi (S_{t})$, but affect $ \pi (S_{t-1})$, as $S_{t}$ is in $S'$ for $S_{t-1}$.
Pseudocode as images:

 


","['reinforcement-learning', 'monte-carlo-methods']",
"When we have multiple traces, do we average over traces or the total number of times we have visited that state?","
I am confused about the workings of the first- and every-visit MC.
My first question is, when we have multiple traces, do we average over traces or the total number of times we have visited that state?
So, in the example:
$$\tau_1 = \text{House} +3, \text{House} +2, \text{School} -4, \text{House} +4, \text{School} -3, \text{Holidays}$$
$$\tau_2 = \text{House} -2, \text{House} +3, \text{School} -3, \text{Holidays},$$
where we have states of either House, Holidays, or School, with the numerical values being the immediate rewards.
For every-visit MC to find the state value of HOUSE, with $\gamma$=1, my intuition would be to create a return list, R, that looks like the following
$$R_1=[3+2−4+4−3, 2−4+4−3, 4−3]= [2, −1, 1]$$
$$R_2=[−2+3−3, 3−3]=[−2, 0]$$
$$R_1+R_2=[2,−1, 1,−2, 0]$$
which, when averaged over 5 visits, is 0 and the correct answer, but I would like if you could confirm if the methodology is correct?
However, another approach would be to compute the average returns for each trace. Which is correct?
","['reinforcement-learning', 'markov-decision-process', 'monte-carlo-methods', 'model-based-methods']","For every visit MC you create a list for each state. Every time you enter a state you calculate the returns for the episode and append these returns to a list. Once you have done this for all the episode you want to average over you simply calculate the value of a state to be the average of this list of returns for the state.First visit MC is almost the same except that you only append the returns to the state returns list for the first time you visit the state in an episode.Your workings are correct, you average over the number of times you have visited the state (in every visit MC). So, in your example, you would get the value of HOUSE to be 0, as you stated. If you were doing first visit MC then the returns for episode 1 would be 2 and the returns for episode 2 would be -2, which you would then average over the two first visits to again give you 0 (this would not always be the case that they are equal after 2 episodes but in the limit both methods do converge to the true state value function)."
Can I start with perfect discriminator in GAN?,"
In many implementations/tutorials of GANs that I've seen so far (e.g. this), the generator and discriminator start with no prior knowledge. They continuously improve their performance with training. This makes me wonder — is it possible to use a pre-trained discriminator? I have two motivations for doing so:

Eliminating the overhead of training the discriminator
Being able to use already existing cool models

Would the generator be able to learn just the same, or is it dependent on the fact that they start from scratch?
","['convolutional-neural-networks', 'generative-adversarial-networks']",
Which NN would you choose to estimate a continuous function $f:\mathbb R^2 \rightarrow \mathbb R$?,"
Suppose we want to estimate a continuous function $f:\mathbb R^2 \rightarrow \mathbb R$ based on a sample using a NN (around 1000 examples). This function is not bounded. Which architecture would you choose ? How many layers/neurons ? Which activation functions ? Which loss function ?
Intuitively, I would go with one hidden layer, 2 neurons, $L^2$ loss, and maybe the Bent identity for the output and a sigmoid in the hidden layer ?
What are the advantages of doing something ""fancier"" than that ?
Would you also have chosen to use a NN for this job or would you have considered a regression SVM for example or something else (knowing that precision is the goal)?
","['neural-networks', 'objective-functions', 'activation-functions', 'regression']",
Are the state-action values and the state value function equivalent for a given policy?,"
Are the state-action values and the state value function equivalent for a given policy? I would assume so as the value function is defined as $V(s)=\sum_a \pi(a|s)Q_{\pi}(s,a)$. If we are operating a greedy policy and hence acting optimally, doesn't this mean that in fact the policy is deterministic and then $\pi(a|s)$ is $1$ for the optimal action and $0$ for all others? Would this then lead to an equivalence between the two?
Here is my work to formulate some form of proof where I start with the idea that a policy is defined to be better than a current policy if for all states then $Q_{\pi}(S,\pi^∗(s))\geq Vπ_{\pi}(s)$ :
I iteratively apply the optimal policy to each time step until I eventually get to a fully optimal time step of rewards
$$Vπ_{\pi}(s)≤Q_{\pi}(S,\pi^∗(s))$$
$$=Eπ[R_{t+1}+\gamma V_{\pi}(St+1)|St=s]$$
$$\leq E[Rt+1+\gamma Q_{\pi}(S_{t+1},\pi^∗(S_{t+1})|S_t=s]$$
$$\leq E[Rt+1+\gamma Rt+2+\gamma 2Q \pi^*(S_{t+2},\pi^∗(S_{t+2})|S_t=s]$$
$$\leq E[R_{t+1}+\gamma R_{t+2}+....|S_t=s]$$
$$=V\pi^∗(s)$$
I would say that our final two lines are in fact inequalities, and for me this makes intuitive sense in that if we are always taking a deterministic greedy action our value function and Q function are the same. As detailed here, for a given policy and state we have that $V(s)=\sum_a \pi(a|s)Q_{\pi}(s,a)$ and if the policy is optimal and hence greedy then $\pi(a|s)$ is deterministic.
","['reinforcement-learning', 'comparison', 'value-functions', 'bellman-equations']","In general they are not the same and that should be clear as to why -- mathematically you are conditioning on an extra random variable being known in the state-action value function. You have the correct relationship between them, but I think your understanding of the two may be slightly off. The state-action value function is a function of both $s$ and $a$ whereas the value function is a function of just $s$. As you have noted, the value function is equal to the expected value of the state-action value function, with the expectation taken over the action distribution induced by the policy.However, if you have a deterministic policy, then two are equivalent only if you evaluate the state-action value function at the action that the deterministic policy gives. That is, $v_\pi(s) = Q(s, \pi(s))$. This is because, as you say, for a given state the policy will assign probability one to a certain action and 0 to all other actions in the action space and so the equation I just wrote is what the expectation in your question reduces to.Now, you might think 'well the policy is deterministic so we are always going to choose this action, so my theory was correct'. This is not true. The definition of the state-action value function is the value of the expected future (possibly discounted) returns given that we are in state $s$ and given that we have taken action $a$. This means that we have already chosen our action and we are not choosing it according to the policy -- the policy will only choose all future actions.You also say that ""If we are operating a greedy policy and hence acting optimally"" which is not true. Just because we act greedily it does not imply we are acting optimally. Value functions are functions of a policy. If the policy is completely random then you would get certain value and state-action value functions which you could act greedily to but this would not be optimal."
How to frame this problem using RL?,"
How should this problem be framed in the domain of RL for preventing users from exceeding their bank account balance and being overdrawn?
For example, a user has 1000 in an account, and proceeds to withdraw 300, 400, 500, making the user overdrawn by 200 :((300+400+500) - 1000).
Treating this as a supervised learning problem, I could use logistic regression. The input feature is the transaction amounts. The input features for a training instance, 300,400,500 and the output feature occurs if the account is overdrawn or not overdrawn with corresponding values of 1 and 0 respectively. For simplicity, we will assume the number of transactions is consistent and is always 3.
For RL, a state could be represented as a series of transactions, but how should the reward be assigned?
Update:
Here my RL implementation of the problem:
import torch
from collections import defaultdict
gamma = .1
alpha = 0.1
epsilon = 0.1
n_episode = 2000
overdraft_limit = 1000

length_episode = [0] * n_episode
total_reward_episode = [0] * n_episode

episode_states = [[700,100,200,290,500] , [400,100,200,300,500] , [212, 500,100,100,200,500]]

def gen_epsilon_greedy_policy(n_action, epsilon):
    def policy_function(state, Q):
        probs = torch.ones(n_action) * epsilon / n_action
        best_action = torch.argmax(Q[state]).item()
        probs[best_action] += 1.0 - epsilon
        action = torch.multinomial(probs, 1).item()
        return action
    return policy_function

def is_overdrawn(currentTotal):
    return currentTotal >= overdraft_limit

# Actions are overdrawn or not, 0 - means it is not overdrawn, 1 - means that it will be overdrawn
def get_reward(action, currentTotal):
    if action == 0 and is_overdrawn(currentTotal):
        return -1
    elif action == 0 and not is_overdrawn(currentTotal):
        return 1
    if action == 1 and is_overdrawn(currentTotal):
        return 1
    elif action == 1 and not is_overdrawn(currentTotal):
        return -1
    else :
        raise Exception(""Action not found"") 

def q_learning(gamma, n_episode, alpha,n_action):
    """"""
    Obtain the optimal policy with off-policy Q-learning method
    @param gamma: discount factor
    @param n_episode: number of episodes
    @return: the optimal Q-function, and the optimal policy
    """"""
    Q = defaultdict(lambda: torch.zeros(n_action))
    for ee in episode_states : 
        for episode in range(n_episode):
            state = ee[0]
            index = 0
            currentTotal = 0
            while index < len(ee)-1 :
                currentTotal = currentTotal + state
                next_state = ee[index+1] 
                action = epsilon_greedy_policy(state, Q)
#                 print(action)
                reward = get_reward(action, currentTotal)
                td_delta = reward + gamma * torch.max(Q[next_state]) - Q[state][action]
                Q[state][action] += alpha * td_delta

                state = next_state
                index = index + 1

                length_episode[episode] += 1
                total_reward_episode[episode] += reward
                
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy

epsilon_greedy_policy = gen_epsilon_greedy_policy(2, epsilon)

optimal_Q, optimal_policy = q_learning(gamma, n_episode, alpha, 2)

print('The optimal policy:\n', optimal_policy)
print('The optimal Q:\n', optimal_Q)

This code prints:
The optimal policy:
 {700: 0, 100: 0, 200: 1, 290: 1, 500: 0, 400: 0, 300: 1, 212: 0}
The optimal Q:
 defaultdict(<function q_learning.<locals>.<lambda> at 0x7f9371b0a3b0>, {700: tensor([ 1.1110, -0.8890]), 100: tensor([ 1.1111, -0.8889]), 200: tensor([-0.8889,  1.1111]), 290: tensor([-0.9998,  1.0000]), 500: tensor([ 1.1111, -0.8889]), 400: tensor([ 1.1110, -0.8890]), 300: tensor([-1.0000,  1.0000]), 212: tensor([ 1.1111, -0.8888])})

The optimal policy is to inform us if 700 is added to the balance, then the customer will not overdraw (0). If 200 is added to the balance, then the customer will overdraw(1). What avenues can I explore to improve upon this method as this is quite basic, but I'm unsure as to what approach I should take in order to improve the solution.
For example, this solution just looks at the most recent additions to the balance to determine if the customer is overdrawn. Is this a case of adding new features to the training data?
I'm just requesting a critique on this solution so I can improve it. How can I improve the representation of the state?
","['reinforcement-learning', 'logistic-regression']",
Why do the training and validation loss curves diverge?,"
I was training a CNN model on TensorFlow. After a while I came back and saw this loss curve:

The green curve is training loss and the gray one is validation loss. I know that before epoch 394 the model in heavily overfitted, but I have no idea what happened after that.
Also, this is accuracy curves if it helps:

I'm using categorical cross-entropy and this is the model I am using:

and here is link to PhysioNet's challenge which I am working on: https://physionet.org/content/challenge-2017/1.0.0/
","['convolutional-neural-networks', 'training', 'overfitting', 'loss']",
How to deal with the time delay in reinforcement learning?,"
I have a question regarding the time delay in reinforcement learning (RL).
In the RL, one has state, reward and action. It is usually assumed that (as far as I understand it) when the action is executed on the system, the state changes immediately and that the new state can then be analysed (influencing the reward) to determine the next action. However, what if there is a time delay in this process. For example, when some action is executed at time $t_1$, we can only get its effect on the system at $t_2$ (You can imagine a flow: the actuator is in the upstream region and the sensor is in the downstream region, so that there will be a time delay between the action and the state). How do we deal with this time delay in RL?
","['reinforcement-learning', 'time-series', 'delayed-rewards']",
"$\frac{P(x_1 \mid y, s = 1) \dots P(x_n \mid y, s = 1) P(y \mid s = 1)}{P(x \mid s = 1)}$ indicates that naive Bayes learners are global learners?","
I am currently studying the paper Learning and Evaluating Classifiers under Sample Selection Bias by Bianca Zadrozny. In section 3. Learning under sample selection bias, the author says the following:

We can separate classifier learners into two categories:

local: the output of the learner depends asymptotically only on $P(y \mid x)$
global: the output of the learner depends asymptotically both on $P(x)$ and on $P(y \mid x)$.

The term ""asymptotically"" refers to the behavior of the learner as the number of training examples grows. The names ""local"" and ""global"" were chosen because $P(x)$ is a global distribution over the entire input space, while $P(y \mid x)$ refers to many local distributions, one for each value of $x$. Local learners are not affected by sample selection bias because, by definition $P(y \mid x, s = 1) = P(y \mid x)$ while global learners are affected because the bias changes $P(x)$.

Then, in section 3.1.1. Naive Bayes, the author says the following:

In practical Bayesian learning, we often make the assumption that the features are independent given the label $y$, that is, we assume that
$$P(x_1, x_2, \dots, x_n \mid y) = P(x_1 \mid y) P(x_2 \mid y) \dots P(x_n \mid y).$$
This is the so-called naive Bayes assumption.
With naive Bayes, unfortunately, the estimates of $P(y \mid x)$ obtained from the biased sample are incorrect. The posterior probability $P(y \mid x)$ is estimated as
$$\dfrac{P(x_1 \mid y, s = 1) \dots P(x_n \mid y, s = 1) P(y \mid s = 1)}{P(x \mid s = 1)} ,$$
which is different (even asymptotically) from the estimate of $P(y \mid x)$ obtained with naive Bayes without sample selection bias. We cannot simplify this further because there are no independence relationships between each $x_i$, $y$, and $s$. Therefore, naive Bayes learners are global learners.

Since it is said that, for global learners, the output of the learner depends asymptotically both on $P(x)$ and on $P(y \mid x)$, what is it about $\dfrac{P(x_1 \mid y, s = 1) \dots P(x_n \mid y, s = 1) P(y \mid s = 1)}{P(x \mid s = 1)}$ that indicates that naive Bayes learners are global learners?

EDIT: To be clear, if we take the example given for the local learner case (section 3.1. Bayesian classifiers), then it is evident:

Bayesian classifiers compute posterior probabilities $P(y \mid x)$ using Bayes' rule:
$$P(y \mid x) = \dfrac{P(x \mid y)P(y)}{P(x)}$$
where $P(x \mid y)$, $P(y)$ and $P(x)$ are estimated from the training data. An example $x$ is classified by choosing the label $y$ with the highest posterior $P(y \mid x)$.
We can easily show that bayesian classifiers are not affected by sample selection bias. By using the biased sample as training data, we are effectively estimating $P(x \mid y, s = 1)$, $P(x \mid s = 1)$ and $P(y \mid s = 1)$ instead of estimating $P(x \mid y)$, $P(y)$ and $P(x)$. However, when we substitute these estimates into the equation above and apply Bayes' rule again, we see that we still obtain the desired posterior probability $P(y \mid x)$:
$$\dfrac{P(x \mid y, s = 1) P(y \mid s = 1)}{P(x \mid s = 1)} = P(y \mid x, s = 1) = P(y \mid x)$$
since we are assuming that $y$ and $s$ are independent given $x$. Note that even though the estimates of $P(x \mid y, s = 1)$, $P(x \mid s = 1)$ and $P(y \mid s = 1)$ are different from the estimates of $P(x \mid y)$, $P(x)$ and $P(y)$, the differences cancel out. Therefore, bayesian learners are local learners.

Note that we get $P(y \mid x)$. However, in the global case, it is not clear how we get $P(x)$ and $P(y \mid x)$ (as is required for global leaners) from $\dfrac{P(x_1 \mid y, s = 1) \dots P(x_n \mid y, s = 1) P(y \mid s = 1)}{P(x \mid s = 1)}$.
","['machine-learning', 'terminology', 'papers', 'naive-bayes', 'selection-bias']",
How to handle long sequences with transformers?,"
I have a time series sequence with 10 million steps. In step $t$, I have a 400 dimensional feature vector $X_t$ and a scalar value $y_t$ which I want to predict during inference time and I know during the train time. I want to use a transformer model. I have 2 questions:

If I want to embed the 400 dimensional input feature vector into another space before feeding into the transformer, what are the pros and cons of using let's say 1024 and 64 for the embedding space dimension? Should I use a dimension more than 400 or less?
When doing position embedding, I cannot use a maximum position length of 10 million as that blows up the memory. What is the best strategy here if I want to use maximum position length of 512? Should I chunk the 10 million steps into blocks of size 512 and feed each block separately into the transformer? If so, how can I connect the subsequent blocks to take full advantage of parallelization while keeping the original chronological structure of the sequence data?

","['deep-learning', 'natural-language-processing', 'transformer', 'time-series', 'attention']",
Are there some notions of distance between two policies?,"
I want to determine some distance between two policies $\pi_1 (a \mid s)$ and $\pi_2 (a \mid s)$, i.e. something like $\vert \vert \pi_1 (a \mid s) - \pi_2(a \mid s) \vert \vert$, where $\pi_i (a\mid s)$ is the vector $(\pi_i (a_1 \mid s), \dots, \pi_i(a_n \mid s))$. I am looking for a sensible notion for such a distance.
Are there some standard norms/metrics used in the literature for determining a distance between policies?
","['reinforcement-learning', 'reference-request', 'policies', 'kl-divergence', 'wasserstein-metric']","Given that policies are probability distributions, in principle, you can use any metric or measure of distance that can be used to compare two probability distributions. (Note that notions of distance are not necessarily metrics in a mathematical sense).A common measure is the Kullback–Leibler divergence (which is not a metric, in a mathematical sense, given that it does not satisfy certain required conditions for being a metric). For example, in section 4 of the PPO paper, the KL divergence is used as a regulariser (which is actually quite common, for instance, in the context of variational Bayesian neural networks). The TRPO also uses the KL divergence.The Wasserstein metric has also been used in RL, for instance, in distributional RL (but, in this case, not to compare policies but distributions over value functions).You can find more info about statistical distances here. The specific distance that you use may depend on the problem that you want to solve and the properties that you want your distance to have. For example, the KL divergence is unbounded above, so, if that's not desirable, you could choose another one. The paper On choosing and bounding probability metrics (2002, by Gibbs and Su) may also be useful. Here I also talk about the KL divergence and total variation."
What does the depth of a decision tree depend on?,"
In these notes, we have the following statement

The depth of a learned decision tree can be larger than the number of training examples used to create the tree

This statement is false, according to the same notes, where it is written

False: Each split of the tree must correspond to at least one training example, therefore, if there are $n$ training examples, a path in the tree can have length at most $n$
Note: There is a pathological situation in which the depth of a learned decision tree can be larger than number of training examples $n$ - if the number of features is larger than $n$ and there exist training examples which have same feature values but different labels.

I had written on my notes that the depth of a decision tree only depends on the number of features of the training set and not on the number of training samples. So, what does the depth of the decision tree depend on?
","['machine-learning', 'classification', 'decision-trees', 'features']","As stated in the other answer, in general, the depth of the decision tree depends on the decision tree algorithm, i.e. the algorithm that builds the decision tree (for regression or classification).To address your notes more directly and why that statement may not be always true, let's take a look at the ID3 algorithm, for instance. Here's the initial part of its pseudocode.So, in general, the depth of the tree may not depend on the number of features, but it may just depend on the labels or training examples (which is a degenerate case), although, in most cases, it will also depend on the number of features, because each node represents a split of the training examples based on some condition that needs to be true for some feature (e.g. the height of the people must be less than 150cm)."
What is the difference between the heuristic function and the evaluation function in A*?,"
I am reading college notes on state search space. The notes (which are not publicly available) say:


To do state-search space, the strategy involves two parts: defining a heuristic function, and identifying an evaluation function.

The heuristic is a smart search of the available space. The evaluation function may be well-defined (e.g. the solution solves a problem and receives a score) or may itself be the heuristic (e.g. if chess says pick A or B as the next move and picks A, the evaluation function is the heuristic).

Understand the difference between the heuristic search algorithm and the heuristic evaluation function.



I'm trying step 3 (to understand). Can I check, using the A* search as an example, that the:
Heuristic function: estimated cost from the current node to the goal, i.e. it's a heuristic that's calculating the simplest way to get to the goal (in A*; $h(n)$), so the heuristic function is calculating $h(n)$ for a series of options and picking the best one.
Evaluation function: $f(n) = g(n) + h(n)$.
","['comparison', 'search', 'a-star', 'heuristic-functions', 'evaluation-functions']","What is the difference between the heuristic function and the evaluation function in A*?The evaluation function, often denoted as $f$, is the function that you use to choose which node to expand during one iteration of A* (i.e. decide which node to take from the frontier, determine the next possible actions and which next nodes those actions lead to, and add those nodes to the frontier). Typically, you expand the node $n$ such that $f(n)$ is the smallest, i.e. $n^* = \operatorname{argmin}f(n)$.In the case of informed search algorithms (such as A*), the heuristic function is a component of $f$, which can be written as $f(n) = g(n) + h(n)$, where $h(n)$ is the heuristic function. The heuristic function estimates the cost of the cheapest path from $n$ to the goal. Just for completeness, $g(n)$ is the actual cost from the start node to $n$ (which can be computed exactly during the search). In the case of uninformed search algorithms, you can actually view the evaluation function as just $f(n) = g(n)$, i.e. the heuristic function is always zero.So, you're right.For more details, you can take a look at section 3.5 (p. 92) of the book Artificial Intelligence: A Modern Approach (3rd edition), by Norvig and Russell (you can find the pdf online)."
Why shouldn't batch normalisation layers be learnable during fine-tuning?,"
I have been reading this TensorFlow tutorial on transfer learning, where they unfroze the whole model and then they say:

When you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing training=False when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.

My question is: why? The model's weights are adapting to the new data, so why do we keep the old mean and variance, which was calculated on ImageNet? This is very confusing.
","['convolutional-neural-networks', 'tensorflow', 'transfer-learning', 'batch-normalization', 'fine-tuning']",
How do I infer exploding or vanishing gradients in Keras?,"
It may already be obvious that I am just a practitioner and just a beginner to Deep Learning. I am still figuring out lots of ""WHY""s and ""HOW""s of DL.
So, for example, if I train a feed-forward neural network, or an image classifier with CNNs, or just an OCR problem with GRUs, using something like Keras, and it performs very poorly or takes more time to train than it should be, it may be because of the gradients getting vanished or exploding, or some other problem.
But, if it is due to the gradients getting very small or very big during the training, how do I figure that out? Doing what will I able to infer that something has happened due to the gradient values?
And what are the precautions I should take to avoid it from the beginning (since training DL models with accelerated computing costs money) and if it has happened, how do I fix it?

This question may sound like a duplicate of How to decide if gradients are vanishing?, but actually not, since that question focuses on CNNs, while I am asking about problem with gradients in all kinds of deep learning algorithms.
","['neural-networks', 'convolutional-neural-networks', 'recurrent-neural-networks', 'backpropagation', 'vanishing-gradient-problem']",
Why does my neural network to predict $x$ given $\sin(x)$ not generalize?,"
I made a simple feedforward neural network (FFNN) to predict $x$ from $\sin(x)$. It failed. Does it mean the model has overfitted? Why doesn't it work?

set.seed(1234567890)
Var3 <- runif(500, 0, 20)
mydata3 <- data.frame(Sin=sin(Var3),Var=Var3)
set.seed(1234567890)
winit <- runif(5500, -1, 1)
#hidUnit <- c(9,1)
set.seed(1234567890)
nn3 <-neuralnet(formula = Var~Sin,data = mydata3,
                hidden =c(4,2,1),startweights =winit,
              learningrate = 0.01,act.fct = ""tanh"")

plot(mydata3, cex=2,main='Predicting x from Sin(x)',
     pch = 21,bg=""darkgrey"",
     ylab=""X"",xlab=""Sin(X)"")
points(mydata3[,1],predict(nn3,mydata3), col=""darkred"", 
       cex=1,pch=21,bg=""red"")

legend(""bottomleft"", legend=c(""true"",""predicted""), pch=c(21,21),
       col = c(""darkgrey"",""red""),cex = 0.65,bty = ""n"")

",['r'],"The labels are not unique for the input domain [0,20]. Think about sin(x)=0, x could be 0, pi, 2*pi, 3*pi, ..., n*pi, all are correct from a mathematical point of view, but this is not reflected in your MSE loss. At this point your NN has to guess the correct label from your input data. Predicting the mean of your input data is the safest bet for the network.In essence you're trying to build a arcsin function with a NN. If only consider x values in [-0.5*pi,0.5*pi], the labels are unique and your network should work."
How are these equations of SGD with momentum equivalent?,"
I know this question may be so silly, but I can not prove it.
In Stanford slide (page 17), they define the formula of SGD with momentum like this:
$$
v_{t}=\rho v_{t-1}+\nabla f(x_{t-1}) 
\\ 
x_{t}=x_{t-1}-\alpha v_{t},
$$
where:

$v_{t+1}$ is the momentum value
$\rho$ is a friction, let say it's equal 0.9
$\nabla f(x_{t-1})$ is the gradient of the objective function at iteration $t-1$
$x_t$ are the parameters
$\alpha$ is the learning rate

However, in this paper and many other documents, they define the equation like this:
$$
v_{t}=\rho v_{t-1}+\alpha \nabla f(x_{t-1}) 
\\ 
x_{t}=x_{t-1}- v_{t},
$$
where $\rho$ and $\alpha$ still have the same value as in the previous formula.
I think it should be
$$v_{t}=\alpha \rho v_{t-1}+\alpha \nabla f(x_{t-1})$$
if we want to multiply the learning rate inside the equation.
In some other document (this) or normal form of momentum, they define like this:
$$
v_{t}= \rho v_{t-1}+ (1- \rho) \nabla f(x_{t-1}) 
\\ 
x_{t}=x_{t-1}-\alpha v_{t}
$$
I can not understand how can they prove those equations are similar. Can someone help me?
","['deep-learning', 'comparison', 'optimization', 'stochastic-gradient-descent', 'momentum']","The first two equations are equivalent. The last equation can be equivalent if you scale $\alpha$ appropriately.Consider the equation from the Stanford slide:$$
v_{t}=\rho v_{t-1}+\nabla f(x_{t-1}) 
\\ 
x_{t}=x_{t-1}-\alpha v_{t},
$$Let's evaluate the first few $v_t$ so that we can arrive at a closed form solution:$v_0 = 0 \\
v_1 =  \rho v_0 + \nabla f(x_0) = \nabla f(x_0)\\
v_2 = \rho v_1 + \nabla f(x_1) = \rho \nabla f(x_0) + \nabla f(x_1)\\
v_3 = \rho v_2 + \nabla f(x_2) = \rho^2 \nabla f(x_0) + \rho \nabla f(x_1) + \nabla f(x_2)\\
\dots \\
v_t = \displaystyle \sum_{i=0}^{t-1} \rho^{t-1-i} \nabla f(x_i)
$So the closed form update is:$$x_t = x_{t-1} - \alpha \displaystyle \sum_{i=0}^{t-1} \rho^{t-1-i} \nabla f(x_i)$$Now consider the equation from the paper:
$$
v_{t}=\rho v_{t-1}+\alpha \nabla f(x_{t-1}) 
\\ 
x_{t}=x_{t-1}- v_{t},
$$We again evaluate the first few $v_t$ to arrive at a closed form solution:$v_0 = 0 \\
v_1 =  \rho v_0 + \alpha \nabla f(x_0) = \alpha \nabla f(x_0)\\
v_2 = \rho v_1 + \alpha \nabla f(x_1) = \rho \alpha \nabla f(x_0) + \alpha \nabla f(x_1)\\
v_3 = \rho v_2 + \alpha \nabla f(x_2) = \rho^2 \alpha \nabla f(x_0) + \rho \alpha \nabla f(x_1) + \alpha \nabla f(x_2)\\
\dots \\
v_t = \displaystyle \sum_{i=0}^{t-1} \rho^{t-1-i} \alpha \nabla f(x_i)
$So the closed from update is:$$x_t = x_{t-1} - \displaystyle \sum_{i=0}^{t-1} \rho^{t-1-i} \alpha \nabla f(x_i)$$As you can see, this is equivalent to the previous closed form update. The only difference is if $\alpha$ is inside or outside the summation, but since it is a constant, it doesn't really matter anyways.As for the last equation$$
v_{t}= \rho v_{t-1}+ (1- \rho) \nabla f(x_{t-1}) 
\\ 
x_{t}=x_{t-1}-\alpha v_{t}
$$
Let's do the same thing:$v_0 = 0 \\
v_1 =  \rho v_0 + (1-\rho) \nabla f(x_0) = (1-\rho) \nabla f(x_0)\\
v_2 = \rho v_1 + (1-\rho) \nabla f(x_1) = \rho (1-\rho) \nabla f(x_0) + (1-\rho) \nabla f(x_1)\\
v_3 = \rho v_2 + (1-\rho) \nabla f(x_2) = \rho^2 (1-\rho) \nabla f(x_0) + \rho (1-\rho) \nabla f(x_1) + (1-\rho) \nabla f(x_2)\\
\dots \\
v_t = \displaystyle \sum_{i=0}^{t-1} \rho^{t-1-i} (1-\rho) \nabla f(x_i)
$And so the closed form update is:$$x_t = x_{t-1} - \alpha \displaystyle \sum_{i=0}^{t-1} \rho^{t-1-i} (1-\rho) \nabla f(x_i)$$This equation is equivalent to the other two as long as you scale $\alpha$ by a factor of $\displaystyle \frac{1}{1-\rho}$."
Can a convolutional neural network classify text document images?,"
I know convolutional neural networks are commonly used for image recognition, but I was wondering if they would be able to distinguish between predominantly text-based documents vs something like objects. For example, if you trained using images of the first page of invoices matched to a vendor name, could you get a CNN to predict the vendor based on an image? If not, is there a different AI technique better suited that is purely image-based, or would it require OCR and leveraging the text in the invoice?
Update: based on a comment, my ask my not be clear. I'm not trying to see if the CNN can differentiate between a document (mostly text based image) and a photo image. I want to know if based on a gif/jpeg/png of a document (no OCR performed) a CNN would be able to classify the documents, which basically could be used as a means of identifying the vendor.
","['convolutional-neural-networks', 'image-recognition', 'optical-character-recognition']","Yes, it is possible, CNNs can also be used for OCR (see the MNIST task and this blog), although it's not the common way for OCR because it is considered a bit overkill and inefficient. Furthermore, OCR is considered a solved problem already and don't need deep learning to do well, unless perhaps under unfavorable conditions like dark lighting, complex background of the document, weird fonts, occlusion of text, etc.Breaking down your questions:"
What is different in each head of a multi-head attention mechanism?,"
I have a difficult time understanding the ""multi-head"" notion in the original transformer paper. What makes the learning in each head unique? Why doesn't the neural network learn the same set of parameters for each attention head? Is it because we break query, key and value vectors into smaller dimensions and feed each portion to a different head?
","['neural-networks', 'natural-language-processing', 'papers', 'transformer', 'attention']","The reason each head is different is because they each learn a different set of weight matrices $\{ W_i^Q, W_i^K, W_i^V \}$ where $i$ is the index of the head. To clarify, the input to each attention head is the same. For attention head $i$:\begin{align}
Q_i(x) &= x W_i^Q \\
K_i(x) &= x W_i^K \\
V_i(x) &= x W_i^V \\
\text{attention}_i(x) &= \text{softmax} \left(\frac{Q_i(x) K_i(x)^T}{\sqrt{d_k}} \right) V_i(x).
\end{align}Notice that the input to each head is $x$ (either the semantic + positional embedding of the decoder input for the first decoder layer, or the output of the previous decoder layer). More infoThe question as to why gradient descent learns each set of weight matrices $\{ W_i^Q, W_i^K, W_i^V \}$ to be different across each attention head is very similar to ""Is there anything that ensures that convolutional filters end up the same?"", so maybe you might find the answer there helpful for you:No, nothing really prevents the weights from being different. In practice though they end up almost always different because it makes the model more expressive (i.e. more powerful), so gradient descent learns to do that. If a model has n features, but 2 of them are the same, then the model effectively has n−1 features, which is a less expressive model than that of n features, and therefore usually has a larger loss function."
Why does k-means have more bias than spectral clustering and GMM?,"
I ran into a 2019-Entrance Exam question as follows:

The answer mentioned is (4), but some search on google showed me maybe (1) and (2) is equal to (4). Why would k-means be the algorithm with the highest bias? (Can you please also provide references to valid material to study more?)
","['machine-learning', 'k-means', 'bias-variance-tradeoff', 'spectral-clustering', 'gaussian-mixture-models']",
what is the correct approach for KNN in item based recommendation system?,"
if I make an application for movies and each user in the system can rate the movies. And I want to make a recommendation system to recommend movies to active user based on his rating for other movies. using item based collaborative filtering using KNN. 
when we find the similarities between the movies and pick the top k items, which approach is correct? 
1- calculate the similarities between all movies and then take the top k for every movie the user rated it highly. (the dataset is a matrix represent the rating values for each item from each user)
2- The KNN is applied to all the movies that the user likes, one after the other, and we find the similarity between each movie and the films that the user did not rate, so that for each film we take the top K of similar films (the user not rated yet) for each movie the user rated highly, then show it to the user . (the dataset for each time we apply knn is a matrix contain rating for each item the user rated and all other items that the user not rated yet).
","['machine-learning', 'recommender-system', 'k-nearest-neighbors']",
How to implement RL model with increasing dimensions of state space and action space?,"
I've read in this discussion that ""reinforcement learning is a way of finding the value function of a Markov Decision Process"".
I want to implement an RL model, whose state space and action space dimensions would increase, as the MDP progresses. But I don't know how to define it it terms of e.g. Q-learning or some similar method.
Precisely, I want to create a model, that would generate boolean circuits. At each step, it could perform four different actions:

apply $AND$ gate on two wires,
apply $OR$ gate on two wires,
apply $NOT$ gate on one wire,
add new wire.

Each of the first three actions could be performed on any currently available wires (targets). Also, the number of wires will change over time. It might increase if we perform fourth action, or decrese after e.g. application of an $AND$ gate (taking as input two wires and outputting just one).
","['reinforcement-learning', 'action-spaces', 'discrete-action-spaces']",
What are the differences between Proximal Policy Optimization versions PPO1 and PPO2?,"
When Proximal Policy Optimization (PPO) was released, it was accompanied by a paper describing it.
Later, the authors at OpenAI introduced a second version of PPO, called PPO2 (whereas the original version is now commonly referred to as PPO1). Unfortunately, the several changes made between PPO1 and PPO2 are pretty much undocumented (as stated over here).
Someone associated with OpnenAI's baselines Deep Reinforcement Learning repository commented that the main advancement of PPO2 (compared to PPO1) was the use of a more advanced parallelism strategy, leading to improved performance. Unfortunately, the person omitted naming further changes made.
Now, I was wondering if anyone is aware of a (reliable) source of information or (preferably) even some published literature that lists all the numerous differences between PPO1 and PPO2.
","['reinforcement-learning', 'deep-rl', 'reference-request', 'proximal-policy-optimization']",
How to deal with predictions for data outside the range of the training dataset in neural networks?,"
I’ve set up a neural network model to experiment with predicting foreign exchange rates based on various economic data. The model learned fine and the test data is OK ($R^2 = 0.88$).
But I can't figure out how to input data for scenarios where new data is outside the range of the datasets used to train the model. For example, if US debt is increased (again), then it will be outside the data range used for the training datasets, so, when normalised using the same parameters as the training dataset (0-1 scale), it will be greater than 1, so the model rejects it.
Everything I've read says to normalise the new data using the training data parameters (understandably), but I can't find anything that explains how to use the model to make predictions where new data is outside the range of the training datasets.
In parallel, I've used a regression model, but I'm fairly new to neural networks and would like to find a way of using these for this kind of prediction model. Any help gratefully received.
","['neural-networks', 'machine-learning', 'prediction']",
How robust are deep networks to class imbalance?,"
Before deep learning, I worked with machine learning problems where the data had a large class imbalance (30:1 or worse ratios). At that time, all the classifiers struggled, even after under-sampling the represented classes and creating synthetic examples of the underrepresented classes -- except Random Forest, which was a bit more robust than the others, but still not great.
What are guidelines for class distribution when it comes to deep learning (CNNs, ResNets, transformers, etc)? Must the representation of each class be 1:1? Or maybe it's ""good enough"" as long as it is under some ratio like 2:1? Or is deep learning completely immune to class imbalance as long as we have enough training data?
Furthermore, as a general guideline, should each class have a certain minimum number of training examples (maybe some multiple of the number of weights of the network)?
","['neural-networks', 'deep-learning', 'datasets', 'data-preprocessing', 'imbalanced-datasets']","@nbro pointed out the paper A systematic study of the class imbalance problem in convolutional neural networks, which tested class imbalance LeNet for MNIST, on a custom CNN for CIFAR-10, and on ResNet for ImageNet. The paper found that by artificially creating class imbalance on those data sets, the neural networks are significantly deteriorated. The ROC AUC drops by 5-10%, and accuracy decreases by 20-30%. These effects are worsened on more complex tasks.There are 3 noteworthy class imbalance approaches to partially alleviate this:The paper concludes with the following best practices:Regarding the choice of a method to handle CNN training on imbalanced dataset we conclude the following.The method that in most of the cases outperforms all others with respect to multi-class ROC AUC was oversampling.For extreme ratio of imbalance and large portion of classes being minority, undersampling performs on a par with oversampling. If training time is an issue, undersampling is a better choice in such a scenario since it dramatically reduces the size of the training setTo achieve the best accuracy, one should apply thresholding to compensate for prior class probabilities. A combination of thresholding with baseline and oversampling is the most preferable, whereas it should not be combined with undersampling.Oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance. The higher a fraction of minority classes in the imbalanced training set, the more imbalance ratio should be reduced.Oversampling does not cause overfitting of convolutional neural networks, as opposed to some classical machine learning models.The last point is very interesting, because oversampling is known to cause overfitting in classical machine learning models and many have advised against doing it."
Would it make sense to share the layers (except the last one) of the neural networks in Double DQN?,"
Context: Double Q-learning was introduced to prevent the maximization bias from q-learning. Instead of learning a single Q-network, we can learn two (or in general $K > 1$) and our Q-estimate would be the min across all these Q-networks.
Question:
Does it make sense to share the layers of these Q-networks (except the last layer)?
So, instead of having 2 networks of size [64, 64, 2] (with ~8.5K parameters in total) we can have one network of size [64, 64, 4] (with ~4.3K params).
I couldn't see much of a downside to this, but all the implementations I've seen keep two completely different networks.
","['reinforcement-learning', 'dqn', 'implementation', 'double-dqn']",
How are continuous actions sampled (or generated) from the policy network in PPO?,"
I am trying to understand and reproduce the Proximal Policy Optimization (PPO) algorithm in detail.  One thing that I find missing in the paper introducing the algorithm is how exactly actions $a_t$ are generated given the policy network $\pi_\theta(a_t|s_t)$.
From the source code, I saw that discrete actions get sampled from some probability distribution (which I assume to be discrete in this case) parameterized by the output probabilities generated by $\pi_\theta$ given the state $s_t$.
However, what I don't understand is how continuous actions are sampled/generated from the policy network. Are they also sampled from a (probably continuous) distribution? In that case, which type of distribution is used and which parameters are predicted by the policy network to parameterize said distribution?
Also, is there any official literature that I could cite which introduces the method by which PPO generates its action outputs?
","['reinforcement-learning', 'implementation', 'proximal-policy-optimization', 'continuous-action-spaces']","As long as your policy (propensity) is differentiable, everything's is good. Discrete, continuous, other, doesn't matter! :)A common example for continuous spaces is the reparameterization trick, where your policy outputs $\mu, \sigma = \pi(s)$ and the action is $a \sim \mathcal{N}(\mu, \sigma)$."
Are the relative magnitudes of the learned and optimal state value function the same?,"
I have been reading recently about value and policy iteration. I tried to code the algorithms to understand them better and in the process I discovered something and I am not sure why is the case (or if my code is doing the right thing)
If I compute the expected value until convergence I get the following grid:

If I compute the optimal value with value iteration I will get the following.

As you can see, the values are different but their relative magnitude is similar, i.e. the 3rd column in the last row has the greatest value in both computations.
I believe this makes sense, as the expected value will tend to accumulate values in ""promising"" cells. But I assume that the first computation won't help much because it does not tell us what is the optimal policy, whereas the second, does.
Is my understanding correct?
","['reinforcement-learning', 'value-functions', 'value-iteration']",
Is there anything that ensures that convolutional filters don't end up the same?,"
I trained a simple model to recognize handwritten numbers from the mnist dataset. Here it is:
model = Sequential([
    Conv2D(filters=1, kernel_size=(3,1), padding='valid', strides=1, input_shape=(28, 28, 1)),
    Flatten(),
    Dense(10, activation='softmax')])

I experimented with varying the number of filters for the convolutional layer, while keeping other parameters constant(learning rate=0.0001, number of episodes=2000, training batch size=512). I used 1, 2, 4, 8, and 16 filters, and the model accuracy was 92-93% for each of them.
From my understanding, during the training the filters may learn to recognize various types of edges in the image (e.g, vertical, horizontal, round). This experiment made me wonder whether any of the filters end up being duplicate -- having the same or similar weights. Is there anything that prevents them from that?
","['neural-networks', 'convolutional-neural-networks', 'convolution', 'filters']",
Can Machine Learning be used to synthesize engine sounds?,"
I'm working on a project to equip model locomotives with sound boards. I'm in the process of designing the board at the moment, and the idea is to allow users to load their own sound files onto an SD card plugged into the board.
Conventionally, model locomotive sounds are collected from high-fidelity microphones placed on and around the real engine in question. The engine is started up then put through idle and all of the different notches, as well as dynamic braking, horn and bell sounds, etc. This practice is very expensive because you have to find a willing (usually small) railroad or museum, pay for travel expenses, and diesel fuel ain't exactly cheap at the volumes these engines go through. Secondly, newer engines are hard to record because railroads aren't exactly in the business of letting hobbyists tape microphones all over their money making machines. As such, the main cost for a sound board comes not from the circuit's BOM cost, but from the effort required to get sounds from locomotives.
What there's plenty of are YouTube videos of amateur rail enthusiasts taking videos of locomotive sightings at close(ish) proximity, including startups and shutdowns. My question is - is there a way to take a bunch of different audio recordings of the same engine, remove noise and the doppler effect, and from that create a profile that can be used to simulate what the engine might sound like at different throttle notches? Is machine learning the right tool for this?
","['machine-learning', 'audio-processing']",
What is the process of inventing deep neural network models? How researchers deal with long training times?,"
After reading this topic on GitHub how long time it takes to train YOLOV3 on coco dataset I was wondering how researchers deal with long training times while inventing new architectures.
I imagine that to evaluate the model you need to train it first. How do they make tweaks in their architectures, e.g. tweaking layers, adding pooling, dropout, etc., if training can take a few days? Is it pure art and it is designed roughly or is it a more deliberate process?
What are the steps of engineering new architecture using deep neural networks?
","['deep-learning', 'research']",
Why is Adam trapped in bad/suspicious local optima after the first few updates?,"
In the paper On the Variance of the Adaptive Learning Rate and Beyond, in section 2, the authors write

To further analyze this phenomenon, we visualize the histogram of the absolute value of gradients on a log scale in Figure 2. We observe that, without applying warmup, the gradient distribution is distorted to have a mass center in relatively small values within 10 updates. Such gradient distortion means that the vanilla Adam is trapped in bad/suspicious local optima after the first few updates.

Here is figure 2 from the paper.

Can someone explain this part?

Such gradient distortion means that the vanilla Adam is trapped in bad/suspicious local optima after the first few updates.

Why is this true?
","['deep-learning', 'papers', 'optimization', 'learning-rate', 'adam']",
Why is second-order backpropagation useful?,"
Raul Rojas's book on Neural Networks dedicates section 8.4.3 to explaining how to do second-order backpropagation, that is, computing the Hessian of the error function with respect to two weights at a time.
What problems are easier to solve using this approach rather than first-order backpropagation?
","['neural-networks', 'backpropagation', 'optimization', 'numerical-algorithms']","Second-order optimization algorithms like Hessian optimization have more information on the curvature of the loss function, so converge much, much faster than first-order optimization algorithms like gradient descent. I remember reading somewhere that if you have $n$ weights in the neural network, one iteration of a second-order optimization algorithm will reduce the loss function at approximately the same rate as $n$ iterations of a standard first-order optimization algorithm. However, with recent advancements to gradient descent (momentum, adaptive rates, etc), the difference isn't as large anymore -- @EmmanuelMess pointed out a paper that states:The performance of the proposed first order and second order methods with adaptive gain (BP-AG, CGFR-AG, BFGS-AG) with standard second order methods without gain (BP, CGFR, BFGS) in terms of speed of convergence evaluated in the number of epochs and CPU time. Based on some simulation results, it’s showed that the proposed algorithm had shown improvements in the convergence rate with 40% faster than other standard algorithms without losing their accuracy.Here is a great post explaining the background behind the math of why this is the case.Also, second-order gradients can help the optimizer identify states like saddle points, and help the solver get out of those states. Saddle points give standard gradient descent a lot of issues, as the gradient descent has difficulty and is slow to move out of the saddle point. Fixing the saddle point issue is one of the motivations for improving gradient descent over the last two decades (SDG with momentum, adaptive learning rates, ADAM, etc). More info.The issue though is that to compute the second-order derivative requires a matrix $n^2$ in size, as opposed to gradient descent which requires a matrix $n$ in size. The memory and computation become intractable for large networks, especially if you have millions of weights.Some approaches exist which efficiently approximates the second-order optimizations, solving the tractability problem. A popular one is L-BFGS. I haven't played around with it much, but I believe L-BFGS is not as popular as gradient descent algorithms (such as SGD-M, ADAM) because it is still very memory demanding (requires storing about 20-100 previous gradient evaluations), and cannot work in a stochastic context (you cannot sample mini-batches to train on; you must train the entire dataset in one pass per iteration). If those two are not an issue for you, then L-BFGS works pretty well I believe."
Why is the ideal exploration parameter in the UCT algorithm $\sqrt{2}$?,"
From Wikipedia, in the Monte-Carlo Tree Search algorithm, you should choose the node that maximizes the value:
$${\displaystyle {\frac {w_{i}}{n_{i}}}+c{\sqrt {\frac {\ln N_{i}}{n_{i}}}}},$$
where

${w_{i}}$ stands for the number of wins for the node considered after the $i$-th move,

${n_{i}}$ stands for the number of simulations for the node considered after the $i$-th move,

$N_{i}$ stands for the total number of simulations after the $i$-th move run by the parent node of the one considered

$c$ is the exploration parameter—theoretically equal to$\sqrt{2}$; in practice usually chosen empirically.


Here (and I've seen in other places as well) it claims that the theoretical ideal value for $c$ is $\sqrt{2}$. Where does this value come from?
(Note: I did post this same question on cross-validated before I knew about this (more relevant) site)
","['reinforcement-learning', 'monte-carlo-tree-search', 'exploration-exploitation-tradeoff', 'upper-confidence-bound']",
What is the difference between Attention Gate and CNN filters?,"
Attention models/gates are used to focus/pay attention to the important regions. According to this paper, the authors describe that a model with Attention Gate (AG) can be trained from scratch. Then the AGs automatically learn to focus on the target.
What I am having trouble understanding is that, in the context of computer vision, doesn't a filter from the convolutional layers learn the region of interest?
The authors say that adding Attention Gate reduces complexity when compared with multi-stage CNNs. But the job a trained AG would do is the same as that of a filter in a convolutional layer that would lead to the correct output, right?
","['convolutional-neural-networks', 'computer-vision', 'attention', 'filters']",
Is there any known approach to generate sets of objects?,"
I am looking for some known approach, or some previous work, on the following problem:
Let $\Sigma$ be an alphabet of symbols and $\Sigma^*$ be the set of all the strings that you can compose from this alphabet. Furthermore, let $f:\Sigma^*\rightarrow2^{\Sigma^*}$ be a function that assigns a certain set of $\Sigma$-strings to each $\Sigma$-string. Suppose you have a dataset $\mathcal{D}\subseteq\Sigma^*\times2^{\Sigma^*}$ of input-output pairs.
With this data, the goal is to learn a function $f^\prime:\Sigma^*\rightarrow2^{\Sigma^*}$ that, given a string $\sigma\in\Sigma^*$, gives any superset of $f(\sigma)$, e.g. $f^\prime(\sigma)\supseteq f(\sigma)$. Of course, returning the set of all strings is not a good solution, so $f^\prime(\sigma)$ should not be much larger than $f(\sigma)$ (to give a rough idea, if $|f(\sigma)|=10$, then $|f^\prime(\sigma)|=100$ would still be ok, but $|f^\prime(\sigma)|=10000$ wouldn't). To give an intuitive reason behind this, I have already an algorithm which, given a $\sigma$ and a set $S\supseteq f(\sigma)$, returns $f(\sigma)$. However, this algorithm has a extremely high time-complexity (growing with $|S|$), and I want to use this machine learning approach to narrow down the search.
I would like to use any Machine Learning approach (from Evolutionary Computing to Deep Learning) to solve this problem.
So far my only idea would be to use an encoder-decoder architecture. I construct character embeddings for all symbols in $\Sigma$, and then through some neural architecture (I was thinking about an LSTM) I aggregate them to obtein a string representation. Given this, the decoder generates in sequence all elements of the corresponding set (by a similar, but inverse, fashion).
This is clearly not optimal, because sets lack any meaningful order, and this approach is order-dependent (by nature of LSTMs and decoders in general). Of course I could always sort all sets, but this still imposes a structure to my problem that is not there, and I feel like this could make it harder to solve.
So, in sum, my question is: Is there any known approach to the problem of generating sets of objects from a given input in the literature? If not, how could I improve my approach?
","['reference-request', 'supervised-learning']",
Can we use transformers for audio classification tasks?,"
Since transformers are good at processing sequential data, can we also use them for audio classification problems (same as RNNs)?
","['reference-request', 'applications', 'transformer', 'sequence-modeling', 'audio-processing']","Yes, Transformers can be used to work with audio data, such as audio processing (audio classification, speaker identification, etc) (Audio ALBERT), speech-to-text (Streaming Automatic Speech Recognition with the Transformer Model), and text-to-speech (Neural Speech Synthesis with Transformer Network)."
"When we translate a text from one language to another, how does the frequency of various POS tags change?","
When we translate a text from one language to another, how does the frequency of various POS tags change?
So, let's say we have a text in English, with 10% nouns, 20% adjectives, 15% adverbs, 25% verbs, etc., which we now translate to German, French, or Hindi. Can we say that in these other languages the POS tag frequency will remain the same as earlier?
","['natural-language-processing', 'transformer', 'language-model', 'pos-tagging']",
How to use a conv2d layer after a flatten?,"
I am not familiar with Deep learning and Pytorch. And I want to know how to deal, in general with such a situation. So, I was wondering if I used a pretrained model (EfficientNet for example) if I want to change the _fc attribute and use conv2d in it, how can I recover a 2D structure? Because the pretrained model flattens it just before _fc.
for example, the pretrained model outputs a flattened feature vector of 1280 elements what I did is the following:
self.efficient_net._fc = nn.Sequential(
                nn.Linear(1280, 1225),
                nn.Unflatten(dim=1, unflattened_size=(1, 35, 35)),
                nn.Conv2d(1, 35, kernel_size=1),
                ...,
                )

I didn't have a specific height and width to recover in the 2D structure, so I assumed that h = w = some size and I use a linear layer whose output is equal to the square of the ""some size"". In the example above 35² = 1225. I am not sure if the unflatten is the correct way to do this. Then I added the conv2d. My code works but it doesn't give good results which probably means that the 2D structure I recovered does not capture any meaningful information.
Can anyone enlighten me with general knowledge about how things are done in my situation, or give me some comments? Thank you!
","['neural-networks', 'convolutional-neural-networks', 'python', 'pytorch', 'pretrained-models']","To answer the question in the title, your enclosed method is a valid way to use 2d convs after a flattened feature vector. However, the bad results you experience could come from the structure of your model or from the way you train it. Regarding you last question, it is very hard to give you an advice without knowing your intentions in detail. Regardless, here are my two cents.First of all, in the case you actually want to use this approach, to have a pretrained model and add your layers after one of its layers, you might want to keep the parameters of the original network intact at least until your newly initialized layers get trained properly. To achieve that, you need to use the gradients for updating only the parts you added as described in this comment. (You definitely should read the other comments there as well to get a better picture.)Secondly, it might be worth reconsidering if you really want to add your layers after the very last layers of the pretrained network. Depending on your goals, using the output of some prior layers as the input to your layers might be more beneficial to you. (Just do not forget to keep the parameters of the pretrained model intact as I advised in the prior point.)Lastly, the structure of your layers should also be reconsidered. 2d convs with a kernel size of 1x1 in this situation seems strange to my limited experience, but without knowing what you want to do, its hard to give any solid advice in this regard.Therefore you might be better off splitting your question into smaller parts and work your way through them one by one.(My reputation is not high enough otherwise I would have left a comment.)"
How does noise input size affect fake image generation with GANs?,"
In Generative Adversarial Networks, the Generator takes noise vector as input and feeds it forward to create an image. The noise vector consists of random numbers sampled from the normal distribution. In several examples that I've encountered, the noise vector had 100 numbers (implementation  1, implementation 2). Is there a reason this number is used? How does noise size affect the generation image?
","['neural-networks', 'generative-adversarial-networks', 'hyper-parameters', 'deepfakes']",
"Aside from specific training sets, what distinguishes the capabilities of different AI implementations?","
(Disclaimer: I don't know much about ML/AI, besides some basic ideas behind it all.)
It seems like ML/AI models can often be boiled down to statistics, where certain levers (weights) get fine-tuned based on the specific input of a large set of training data.
Clearly, ML/AI models don't only distinguish themselves in their training data alone, otherwise there would not be so many improvements happening in the field all the time. My question therefore is: What does distinguish different models of the same category?
If I have an AI that completes real-life pictures that have some missing parts, and an AI that completes a painting with missing parts, what key concepts separates the two?
If I have an AI detecting text in an image, and an AI detecting... trees in an image, what key concepts separates the two?
In other words, what is stopping me from ""taking"" an existing implementation of a certain AI category, and just feeding it my specific training set + rewards (i.e. judgement criteria for good vs bad output), in order to solve a specific task?
In yet again other words, if I wanted to use ML/AI to build a new model for a specific task, what concepts and topics would I need to pay extra attention to? (I guess you could say I'm trying to reverse engineer the learning process of the field here. I don't have the time to properly teach myself and become an ""expert"", but find it all very interesting and would still like to use some of the wonderful things people have done.)
","['machine-learning', 'reinforcement-learning', 'models', 'unsupervised-learning', 'supervised-learning']",
Algorithms for training a two motor powered rc car without steering servo,"
Previously, I have build a donkey car where the steering of the two front wheels was done using a motor servo. This project was a success and the car was able to drive autonomously after training was done.
source Donkey Car 2
Now:
I have this  Rc Car kit that has two motors on the back, powering two wheels and a trolley wheel in front.
The steering is supposed to be done by playing around with the two back motors.
My question is:
Is there any method modify the donkey car code, so I can train the model?
Considering that Donkey Car uses the angle of the servo to train the model, and now I just have the information of the two back wheels and no servo steering the vehicle
Not sure if there approach that is specific to this concept.
","['machine-learning', 'python', 'autonomous-vehicles']",
"How is the DQN loss derived from (or theoretically motivated by) the Bellman equation, and how is it related to the Q-learning update?","
I'm doing a project on Reinforcement Learning. I programmed an agent that uses DDQN. There are a lot of tutorials on that, so the code implementation was not that hard.
However, I have problems understanding how one should come up with this kind of algorithms by starting from the Bellman equation, and I don't find a good understandable explanation addressing this derivation/path of reasoning.
So, my questions are:

How is the loss to train the DQN derived from (or theoretically motivated by) the Bellman equation?
How is it related to the usual Q-learning update?

According to my current notes, the Bellman equation looks like this
$$Q_{\pi} (s,a) = \sum_{s'} P_{ss'}^a (r_{s,a} + \gamma \sum_{a'} \pi(a'|s') Q_{\pi} (s',a')) \label{1}\tag{1} $$
which, to my understanding, is a recursive expression that says:
The state-action pair gives a reward that is equal to the sum over all possible states $s'$ with the probability of getting to this state after taking action $a$ (denoted as $P_{ss'}^a$, which means the environment acts on the agent) times the reward the agent got from taking action $a$ in state $s$ + discounted sum of the probability of the different possible actions $a'$ times the reward of the state, action pair $s',a'$.
The Q-Learning iteration (intermediate step) is often denoted as:
$$Q^{new}(s,a) \leftarrow  Q(s,a) + \alpha (r + \gamma \max_a Q(s',a') - Q(s,a)) \label{2}\tag{2}$$
which means that the new state, action reward is the old Q value + learning rate, $\alpha$, times the temporal difference, $(r + \gamma \max_a Q(s',a') - Q(s,a))$, which consists of the actual reward the agent received + a discount factor times the Q function of this new state-action pair minus the old Q function.
The Bellman equation can be converted into an update rule because an algorithm that uses that update rule converges, as this answer states.
In the case of (D)DQN, $Q(s,a)$ is estimated by our NN that leads to an action $a$ and we receive $r$ and $s'$.
Then we feed in $s$ as well as $s'$ into our NN (with Double DQN we feed them into different NNs). The $\max_a Q(s',a')$ is performed on the output of our target network. This q-value is then multiplied with $\gamma$ and $r$ is added to the product. Then this sum replaces the q-value from the other NN. Since this basic NN outputted $Q(s,a)$ but should have outputted $r + \gamma \max_a Q(s',a')$ we train the basic NN to change the weights, so that it would output closer to this temporal target difference.
","['reinforcement-learning', 'q-learning', 'dqn', 'objective-functions', 'bellman-equations']","The Bellman equation in RL is usually defined $$v_\pi(s) = \sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a)\left[r + v_\pi(s')\right] = \mathbb{E}_{s' \sim p, a \sim \pi}\left[r(s, a) + v_\pi(s')\right] \; .$$
The way you have written it is correct, but I just thought I would point this out. Regardless, your intuition is correct in that it expresses a recursive relationship such that the value of your current state $s$ is equal to the sum of the expected reward from this state plus the expected value of the state you transition into.You do, in fact, implement the Q-learning update in Deep Q-Learning. The loss function that you minimise in DQN is
$$ L(\theta) = \mathbb{E}_{(s,a,r,s')\sim U(D)}\left[\left( r + \gamma \max_{a'}Q(s', a'; \theta^-) - Q(s, a; \theta)\right)^2 \right]\;$$
where $U(D)$ denotes uniformly at random from replay buffer $D$ and $\theta$ are your network parameters (the network parameterises the Q-function), and $\theta^-$ are a previous iteration of the parameters that are updated every $c$ episodes to help with convergence of the network.As you can see, the loss function is minimising the 'Bellman error' error from your equation 2. Lets think about why this is.The TD update you provide is gradually shifting the Q value for $(s, a)$ towards $r + \max_a Q(s', a)$ - this is what we want after all since it eventually converges to the optimal Q-function.Now lets think about the Deep Q-learning case. We want our network to approximate $Q(s, a)$ and so if we train the network, using the MSE loss, with $r + \max_a Q(s', a)$ as our target then our network will gradually be shifted towards predicting $r + \max_aQ(s', a)$ (which again would give us optimal Q-values for state-action pairs), just like with the TD update.This is assuming that you know how training of neural networks works so if you don't then I would recommend asking/searching for a relevant question that explains this."
Using an LSTM for model-based RL in a POMDP,"
I am trying to set up an experiment where an agent is exploring an n x n gridworld environment, of which the agent can see some fraction at any given time step. I'd like the agent to build up some internal model of this gridworld.
Now the environment is time-varying, so I figured it would useful to try using an LSTM so the agent can learn potentially useful information about how the environment changes. However, since the agent can only see some of the environment, each observation that could be used to train this model would be incomplete (i.e. the problem is partially-observable from this perspective). Thus I imagine that training such a network would be difficult since there would be large gaps in the data - for example, it may make an observation at position [0, 0] at t = 0, and then not make another observation there until say t = 100.
My question is twofold

Is there a canonical way of working around partial observability in LSTMs? Either direct advice or pointing to useful papers would both be appreciated.
Can an LSTM account for gaps in time between observations?

Thanks!
","['reinforcement-learning', 'long-short-term-memory', 'model-based-methods']",
What would be the AlphaGo's performance in continuous action space?,"
During my research for Google DeepMind's Go-playing program Alpha Go and its successor Alpha Go Zero, I discovered that the system uses a clever pipeline and an interplay of blocks of both policy and value networks to play the game of Go in such a way, that it is able to outperform even the best players in the world. This is in particular remarkable, because the game of Go was considered to be unsolvable a few years ago. This success gained international attention and it was labeled as a breakthrough in the community of AI. It is also not a secret that the research team behind AlphaGo and AlphaGo Zero used lots of computation power to create such a sophisticated system.
But, since each board configuration is considered as a distinct state, where algorithms can be applied really well, and just consider AlphaGo Zero, which uses no prior knowledge and can figure out how the play the game of go from scratch, my question is the following:
Is there any way to state (theoretically) how the performance of AlphaGo would be in continuous action spaces (e.g. self-driving cars)?
","['monte-carlo-tree-search', 'alphazero', 'alphago', 'deepmind', 'continuous-action-spaces']",
Understanding LSTM through example,"
I want to code up one time step in a LSTM. My focus is on understanding the functioning of the forget gate layer, input gate layer, candidate values, present and future cell states.
Lets assume that my hidden state at t-1 and xt are the following. For simplicity, lets assume that the weight matrices are identity matrices, and  all biases are zero.
htminus1 = np.array( [0, 0.5, 0.1, 0.2, 0.6] )
xt = np.array( [-0.1, 0.3, 0.1, -0.25, 0.1] )

I understand that forget state is sigmoid of htminus1 and xt
So, is it?
ft = 1 / ( 1 + np.exp( -( htminus1 + xt ) ) )

>> ft = array([0.47502081, 0.68997448, 0.549834  , 0.4875026 , 0.66818777])

I am referring to this link to implement of one iteration of one block LSTM. The link says that ft should be 0 or 1. Am I missing something here?
How do I get the forget gate layer as per schema given in the below mentioned picture? An example will be illustrative for me.

Along the same lines, how do I get the input gate layer, it and vector of new candidate values, \tilde{C}_t as per the following picture?

Finally, how do I get the new hidden state ht as per the scheme given in the following picture?
A simple, example will be helpful for me in understanding. Thanks in advance.

","['neural-networks', 'python', 'recurrent-neural-networks', 'long-short-term-memory']",
Literature on the advantages of using an auto-encoder for classification,"
Given a supervised problem with X, y input pairs, one can do two things for obtaining the function f that maps X with y with Neural Networks (and in general in machine learning):

Deploy directly a supervised learning algorithm that maps X to y

Deploy a (variational) auto-encoder for learning useful features, and then using these for training the supervised learning algorithm


I would like to be pointed to some papers/blogs that explain which technique is better and when or where they conduct empirical benchmarking experiments.
","['classification', 'reference-request', 'autoencoders', 'supervised-learning', 'transfer-learning']",
How to calculate the distance between the camera and an object using Computer Vision?,"
I want to create a Deep Learning model that measures the distance between the camera and certain objects in an image. Is it possible? Please, let me know some resources related to this task.
","['deep-learning', 'computer-vision', 'reference-request']","In general, calculation of distance between camera and object is impossible if you don't have further scene dependent information.To my knowledge you have 3 options:Stereo VisionIf you have 2 cameras looking at the same scene from a different point of view you can calculate the distance with classical Computer Vision algorithms. This is called stereo vision, or also multiview geometry.
Stereo Vision is the reason why humans can infer the distance to objects around them (because we have 2 eyes).Structure from MotionYou move your camera and therefore change your viewpoint and can essentially do stereo mapping over time. Structure from MotionScene UnderstandingWhy is it then still possible for a one-eyed person to infer depth to some extent? Because humans have lots of scene dependent understanding. If you see a rubber duck that takes half of your field of view, you know it's pretty close because you know a rubber duck is not big. If you don't know the size of rubber ducks it is impossible to know whether you see a big rubber duck that is far away or a small rubber duck that is really close.This is where Deep Learning based models come into play. A recent overview over monocular
depth estimation can be found in Zhao2020"
Why does the loss stops reducing after a point in this Transformer Model?,"
Context
I was making a Transformer Model to convert English Sentences to German Sentences. But the loss stops reducing after some time.
Code
import string
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, RepeatVector, Dense, Dropout, BatchNormalization, TimeDistributed, AdditiveAttention, Input, Concatenate, Flatten
from tensorflow.keras.layers import Activation, LayerNormalization, GRU, GlobalAveragePooling1D, Attention
from tensorflow.keras.optimizers import Adam
from tensorflow.nn import tanh, softmax
import time
from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy
from numpy import array
from tensorflow.keras.utils import plot_model
from sklearn.utils import shuffle
import time
import tensorflow as tf
from numpy import array
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.datasets.imdb import load_data

def load_data(filename):
    file = open(filename, 'r')
    text = file.read()
    file.close()
    return text

def to_lines(text):
    return text.split('\n')

def clean_data(pair):
    pair = 'start_seq_ ' + pair + ' end_seq_'

    re_print = re.compile('[^%s]' % re.escape(string.printable))
    table = str.maketrans('', '', string.punctuation)
    tokens = [token.translate(table) for token in pair.split()]
    tokens = [token.lower() for token in tokens]
    tokens = [re_print.sub('', token) for token in tokens]
    tokens = [token for token in tokens if token.isalpha()]
    return tokens

lines = to_lines(load_data('/content/drive/My Drive/spa.txt'))

english_pair = []
german_pair = []
language = []
for line in lines:
    if line != '':
        pairs = line.split('\t')
        english_pair.append(clean_data(pairs[0]))
        german_pair.append(clean_data(pairs[1]))

        language.append(clean_data(pairs[0]))
        language.append(clean_data(pairs[1]))

english_pair = array(english_pair)
german_pair = array(german_pair)
language = array(language)

def create_tokenizer(data):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(data)
    return tokenizer

def max_len(lines):
    length = []
    for line in lines:
        length.append(len(line))
    return max(length)

tokenizer = create_tokenizer(language)

vocab_size = len(tokenizer.word_index) + 1

max_len = max_len(language)

def create_sequences(sequences, max_len):
    sequences = tokenizer.texts_to_sequences(sequences)
    sequences = pad_sequences(sequences, maxlen=max_len, padding='post')
    return sequences

X1 = create_sequences(english_pair, max_len)
X2 = create_sequences(german_pair, max_len)
Y = create_sequences(german_pair, max_len)


X1, X2, Y = shuffle(X1, X2, Y)

training_samples = int(X1.shape[0] * 1.0)

train_x1, train_x2, train_y = X1[:training_samples], X2[:training_samples], Y[:training_samples]
test_x1, test_x2, test_y = X1[training_samples:], X2[training_samples:], Y[training_samples:]

train_x2 = train_x2[:, :-1]
test_x2 = test_x2[:, :-1]
train_y = train_y[:, 1:].reshape(-1, max_len-1)
test_y = test_y[:, 1:].reshape(-1, max_len-1)

train_x2 = pad_sequences(train_x2, maxlen=max_len, padding='post')
test_x2 = pad_sequences(test_x2, maxlen=max_len, padding='post')

train_y = pad_sequences(train_y, maxlen=max_len, padding='post')
test_y = pad_sequences(test_y, maxlen=max_len, padding='post')

All code above just prepares the Data, so if you want you can skip that part.
Code After this starts implementing the Transformer Model.
class EncoderBlock(tf.keras.layers.Layer):
    def __init__(self, mid_ffn_dim, embed_dim, num_heads, max_len, batch_size):
        super(EncoderBlock, self).__init__()
        # Variables
        self.batch_size = batch_size
        self.max_len = max_len
        self.mid_ffn_dim = mid_ffn_dim
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.attention_vector_len = self.embed_dim // self.num_heads
        if self.embed_dim % self.num_heads != 0:
            raise ValueError('I am Batman!')

        # Trainable Layers
        self.mid_ffn = Dense(self.mid_ffn_dim, activation='relu')
        self.final_ffn = Dense(self.embed_dim)

        self.layer_norm1 = LayerNormalization(epsilon=1e-6)
        self.layer_norm2 = LayerNormalization(epsilon=1e-6)

        self.combine_heads = Dense(self.embed_dim)

        self.query_dense = Dense(self.embed_dim)
        self.key_dense = Dense(self.embed_dim)
        self.value_dense = Dense(self.embed_dim)

    def separate_heads(self, x):
        x = tf.reshape(x, (-1, self.max_len, self.num_heads, self.attention_vector_len))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def compute_self_attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output

    def self_attention_layer(self, x):
        query = self.query_dense(x)
        key = self.key_dense(x)
        value = self.value_dense(x)

        query_heads = self.separate_heads(query)    
        key_heads = self.separate_heads(key)
        value_heads = self.separate_heads(value)

        attention = self.compute_self_attention(query_heads, key_heads, value_heads)

        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) 
        attention = tf.reshape(attention, (-1, self.max_len, self.embed_dim))

        output = self.combine_heads(attention)
        return output
        
    def get_output(self, x):
        attn_output = self.self_attention_layer(x)
        out1 = self.layer_norm1(x + attn_output)

        ffn_output = self.final_ffn(self.mid_ffn(out1))

        encoder_output = self.layer_norm2(out1 + ffn_output)
        return encoder_output

class DecoderBlock(tf.keras.layers.Layer):
    def __init__(self, mid_ffn_dim, embed_dim, num_heads, max_len, batch_size):
        super(DecoderBlock, self).__init__()
        # Variables
        self.batch_size = batch_size
        self.max_len = max_len
        self.mid_ffn_dim = mid_ffn_dim
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.attention_vector_len = self.embed_dim // self.num_heads
        if self.embed_dim % self.num_heads != 0:
            raise ValueError('I am Batman!')

        # Trainable Layers

        self.query_dense1 = Dense(self.embed_dim, name='query_dense1')
        self.key_dense1 = Dense(self.embed_dim, name='key_dense1')
        self.value_dense1 = Dense(self.embed_dim, name='value_dense1')

        self.mid_ffn = Dense(self.mid_ffn_dim, activation='relu', name='dec_mid_ffn')
        self.final_ffn = Dense(self.embed_dim, name='dec_final_ffn')

        self.layer_norm1 = LayerNormalization(epsilon=1e-6)
        self.layer_norm2 = LayerNormalization(epsilon=1e-6)
        self.layer_norm3 = LayerNormalization(epsilon=1e-6)

        self.combine_heads = Dense(self.embed_dim, name='dec_combine_heads')

        self.query_dense2 = Dense(self.embed_dim, name='query_dense2')
        self.key_dense2 = Dense(self.embed_dim, name='key_dense2')
        self.value_dense2 = Dense(self.embed_dim, name='value_dense2')

    def separate_heads(self, x):
        x = tf.reshape(x, (-1, self.max_len, self.num_heads, self.attention_vector_len))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def compute_self_attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output

    def masking(self, x):
        b = []
        for batch in range(x.shape[0]):
            bat = []
            for head in range(x.shape[1]):
                headd = []
                for word in range(x.shape[2]):
                    current_word = []
                    for represented_in in range(x.shape[3]):
                        if represented_in > word:
                          current_word.append(np.NINF)
                        else:
                          current_word.append(0)
                    headd.append(current_word)
                bat.append(headd)
            b.append(bat)
        return b

    def compute_masked_self_attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        score = score + self.masking(score)
        score = tf.convert_to_tensor(score)
                
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output

    def masked_self_attention_layer(self, x):
        query = self.query_dense1(x)
        key = self.key_dense1(x)
        value = self.value_dense1(x)

        query_heads = self.separate_heads(query)    
        key_heads = self.separate_heads(key)
        value_heads = self.separate_heads(value)

        attention = self.compute_masked_self_attention(query_heads, key_heads, value_heads)

        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) 
        attention = tf.reshape(attention, (-1, self.max_len, self.embed_dim))

        output = self.combine_heads(attention)
        return output

    def second_attention_layer(self, x, encoder_output):
        query = self.query_dense2(x)
        key = self.key_dense2(encoder_output)
        value = self.value_dense2(encoder_output)

        query_heads = self.separate_heads(query)    
        key_heads = self.separate_heads(key)
        value_heads = self.separate_heads(value)

        attention = self.compute_self_attention(query_heads, key_heads, value_heads)

        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) 
        attention = tf.reshape(attention, (-1, self.max_len, self.embed_dim))

        output = self.combine_heads(attention)
        return output
      
    def get_output(self, x, encoder_output):
        masked_attn_output = self.masked_self_attention_layer(x)
        out1 = self.layer_norm1(x + masked_attn_output)

        mutli_head_attn_output = self.second_attention_layer(out1, encoder_output)
        out2 = self.layer_norm2(out1 + mutli_head_attn_output)

        ffn_output = self.final_ffn(self.mid_ffn(out2))
        decoder_output = self.layer_norm3(out2 + ffn_output)
        return decoder_output

embed_dim = 512
mid_ffn_dim = 1024

num_heads = 8
max_len = max_len
batch_size = 32

encoder_block1 = EncoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)
encoder_block2 = EncoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)
encoder_block3 = EncoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)

decoder_block1 = DecoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)
decoder_block2 = DecoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)
decoder_block3 = DecoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)

# Define Loss and Optimizer
loss_object = SparseCategoricalCrossentropy()
optimizer = Adam()

embedding = Embedding(vocab_size, embed_dim, name='embedding')
position_embedding = Embedding(vocab_size, embed_dim)

final_transformer_layer = Dense(vocab_size, activation='softmax')

def positional_embedding(x):
    positions = tf.range(start=0, limit=max_len, delta=1)
    positions = position_embedding(positions)
    return x + positions

def train_step(english_sent, german_sent, german_trgt):
    with tf.GradientTape() as tape:
        english_embedded = embedding(english_sent)
        german_embedded = embedding(german_sent)

        english_positioned = positional_embedding(english_embedded)
        german_positioned = positional_embedding(german_embedded)

        # Encoders
        encoder_output = encoder_block1.get_output(english_positioned)
        encoder_output = encoder_block2.get_output(encoder_output)
        encoder_output = encoder_block3.get_output(encoder_output)

        # Decoders
        decoder_output = decoder_block1.get_output(german_positioned, encoder_output)
        decoder_output = decoder_block2.get_output(decoder_output, encoder_output)
        decoder_output = decoder_block3.get_output(decoder_output, encoder_output)

        # Final Output
        transformer_output = final_transformer_layer(decoder_output)

        # Compute Loss
        loss = loss_object(german_trgt, transformer_output)

    variables = embedding.trainable_variables + position_embedding.trainable_variables + encoder_block1.trainable_variables + encoder_block2.trainable_variables
    variables += encoder_block3.trainable_variables + decoder_block1.trainable_variables + decoder_block2.trainable_variables + decoder_block3.trainable_variables
    variables += final_transformer_layer.trainable_variables

    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))

    return float(loss)

def train(epochs=10):
    batch_per_epoch = int(train_x1.shape[0] / batch_size)
    for epoch in range(epochs):
        for i in range(batch_per_epoch):
            english_sent_x = train_x1[i*batch_size : (i*batch_size)+batch_size].reshape(batch_size, max_len)
            german_sent_x = train_x2[i*batch_size : (i*batch_size)+batch_size].reshape(batch_size, max_len)
            german_sent_y = train_y[i*batch_size : (i*batch_size)+batch_size].reshape(batch_size, max_len, 1)

            loss = train_step(english_sent_x, german_sent_x, german_sent_y)

            print('Epoch ', epoch, 'Batch ', i, '/', batch_per_epoch, 'Loss ', loss)

train()


And the Code is done! But the loss stops reducing at around value of 1.2 after some time. Why is this happening?
Maybe Important
I tried debugging the model, by passing random input integers, and the model was still performing the same way it did when I gave real Sentences as input.
When I tried training the model with just 1 training sample, the loss stops reducing at around 0.2. When I train it with 2 training samples, the result was the approximately the same as when I trained it with 1 training sample.
When I stopped shuffling the dataset the loss gone till around 0.7 and again stopped learning.
I tried simplifying the model by removing some encoder and decoder blocks but the results were approximately the same. I even tried making the model more complex but the results were again approximately the same.
","['machine-learning', 'deep-learning', 'tensorflow', 'keras', 'transformer']",
Is there a taxonomy of adversarial attacks?,"
I am a medical doctor working on methodological aspects of health-oriented ML. Reproducibility, replicability, generalisability are critical in this area. Among many questions, some are raised by adversarial attacks (AA).
My question is to be considered from a literature review point of view: suppose I want to check an algorithm from an AA point of view:

is there a systematic methodology approach to be used, relating format of the data, type of models, and AA? Conceptually, is there a taxonomy of AA? If so, practically, are some AA considered as gold standards?

","['machine-learning', 'reference-request', 'adversarial-ml', 'healthcare', 'taxonomy']","There are already a couple of papers in the literature that attempt to provide a taxonomy and survey of adversarial attacks. I will just list the two that I think are reliable enough that you can probably use as a reference.A taxonomy and survey of attacks against machine learning (2019) by Nikolaos Pitropakis et al., published in Computer Science ReviewAdversarial Examples: Attacks and Defenses for Deep Learning (2019) by Xiaoyong Yuan et al., published in IEEE Transactions on Neural Networks and Learning Systems.Needless to say, there are different adversarial attacks, such as the Fast Gradient Signed Method (FGSM), and they can be classified into different categories, such as evasion attacks or poising attacks. You can find a lot more info in these cited papers."
How to graphically represent a RNN architecture implemented in Keras?,"
I'm trying to create a simple blogpost on RNNs, that should give a better insight into how they work in Keras. Let's say:
model = keras.models.Sequential()
model.add(keras.layers.SimpleRNN(5, return_sequences=True, input_shape=[None, 1]))
model.add(keras.layers.SimpleRNN(5, return_sequences=True))
model.add(keras.layers.Dense(1))

I came up with the following visualization (this is only a sketch), which I'm quite unsure about:

The RNN architecture is comprised of 3 layers represented in the picture.
Question: is this correct? Is the input ""flowing"" thought each layer neuron to neuron or only though the layers, like in the picture below. Is there anything else that is not correct - any other visualizations to look into?

Update: my assumptions are based on my understanding from what I saw in Geron's book. The recurrent neurons are connected, see: https://pasteboard.co/JDXTFVw.png ... he then proceeds to talk about connections between different layers, see: https://pasteboard.co/JDXTXcz.png - did I misunderstand him or is it just a peculiarity in keras framework?
","['keras', 'recurrent-neural-networks', 'data-visualization']",
What is the difference between object tracking and trajectory prediction?,"
In autonomous driving, we know that the behaviour prediction module is concerned with understanding how the agents in the environment will behave.
Similarly, in the perception module, the tracking algorithms are responsible for getting an estimate of the object's state over time.
","['comparison', 'autonomous-vehicles']",
Time series prediction using LSTM and CNN-LSTM: which is better?,"
I am working on LSTM and CNN to solve the time series prediction problem.
I have seen some tutorial examples of time series prediction using CNN-LSTM. But I don't know if it is better than what I predicted using LSTM.
Could using LSTM and CNN together be better than predicting using LSTM alone?
","['deep-learning', 'convolutional-neural-networks', 'long-short-term-memory', 'time-series']","Many papers have been published on CNN, LSTM, and CNN-LSTM for time series. From the literature and my experience, I conclude that CNN-LSTM outperforms CNN and LSTM models. Here are two relevant papers on stock price time series forecasting:Wenjie Lu, Jiazheng Li, Yifan Li, Aijun Sun, & Jingyang Wang. (2020). A CNN-LSTM-Based Model to Forecast Stock Prices. Complexity, 2020. https://doi.org/10.1155/2020/6622927The authors' conclusions:This paper takes the relevant data of the Shanghai Composite Index as an example to verify the experimental results. The experimental results show that the CNN-LSTM has the highest forecasting accuracy and the best performance compared with the MLP, CNN, RNN, LSTM, and CNN-RNN.Chen, Y., Fang, R., Liang, T., Sha, Z., Li, S., Yi, Y., Zhou, W., & Song, H. (2021). Stock Price Forecast Based on CNN-BiLSTM-ECA Model. Scientific Programming, 1–20. https://doi.org/10.1155/2021/2446543The authors' conclusions:The proposed model is compared with CNN, LSTM, BiLSTM, CNNLSTM, CNN-BiLSTM, BiLSTM-ECA, and CNN-LSTMECA network models on three datasets. The experimental results show that the proposed model has the highest prediction accuracy and the best performance."
How should I use deep learning to find the rotation of an object from its 2D image?,"
I have 6600 images and I am supposed to know the rotation of the object in each image. So, given an image, I want to regress to a single value.
My attempt: I use Resnet-18 to extract a feature vector of length 1000 from an image. This is then passed to three fully-connected layers: fc(1000, 512) -> fc(512, 64) -> fc(64, 1)
The problem I am facing right now is that my training loss and validation loss immediately go down after the first 5 epochs and then they barely change. But my training and validation accuracy fluctuates wildly throughout.
I understand that I am experiencing over-fitting and I have done the following to deal with it:

data augmentation (Gaussian noise and color jittering)
L1 regularization
dropout

So far, nothing seems to be changing the results much. The next thing I haven't tried is reducing the size of my neural net. Will that help? If so, how should I reduce the size?
","['neural-networks', 'deep-learning', 'deep-neural-networks', 'regression', 'feature-extraction']",
What is the gradient of an attention unit?,"
The paper Attention Is All You Need describes the Transformer architecture, which describes attention as a function of the queries $Q = x W^Q$, keys $K = x W^K$, and values $V = x W^V$:
$\text{Attention(Q, K, V)} = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V \\
= \text{softmax}\left( \frac{x W^Q (W^K)^T x}{\sqrt{d_k}} \right) x W^V$
In the Transformer, there are 3 different flavors of attention:

Self-attention in the Encoder, where the queries, keys, and values all come from the input to the Encoder.
Encoder-Decoder attention in the Decoder, where the queries come from the input to the Decoder, and the keys and values come from the output of the Encoder
Masked self-attention in the Decoder, where the queries, keys and values all come from the input to the Decoder, and, for each token, the $\text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)$ operation is masked out (zero'd out) for all tokens to the right of that token (to prevent look-ahead, which is cheating during training).

What is the gradient (i.e. the partial derivatives of the loss function w.r.t. $x$, $W^Q$, $W^K$, $W^V$, and any bias term(s)) of each of these attention units? I am having a difficult time wrapping my head around derivating a gradient equation because I'm not sure how the softmax function interacts with the partial derivatives, and also, for the Encoder-Decoder attention in the Decoder, I'm not clear how to incorporate the encoder output into the equation.
","['neural-networks', 'natural-language-processing', 'gradient-descent', 'transformer', 'attention']",
What is the cost function of a transformer?,"
The paper Attention Is All You Need describes the transformer architecture that has an encoder and a decoder.
However, I wasn't clear on what the cost function to minimize is for such an architecture.
Consider a translation task, for example, where give an English sentence $x_{english} = [x_0, x_1, x_2, \dots, x_m]$, the transformer decodes the sentence into a French sentence $x_{french}' = [x_0', x_1', \dots, x_n']$. Let's say the true label is $y_{french} = [y_0, y_1, \dots, y_p]$.
What is the object function of the transformer? Is it the MSE between $x_{french}'$ and $y_{french}$? And does it have any weight regularization terms?
","['neural-networks', 'natural-language-processing', 'objective-functions', 'transformer', 'attention']","I took a look at the Tensor2Tensor's source code implementation, and it seems like the loss function is the cross-entropy between the predicted probability matrix $\|\text{sentence length}\| \times \|\text{vocab}\|$ (right before taking the argmax to find the token to output), and the $\|\text{sentence length}\|$-length vector of token IDs as the true label."
"What is the ""contradictory loss"" in the ""Old Photo Restoration via Deep Latent Space Translation"" paper?","
In page 4 of the paper Old Photo Restoration via Deep Latent Space
Translation, it says the encoder $E_{R,X}$ of $VAE_1$ tries to fool the discriminator with a contradictory loss to ensure that $R$ and $X$ are mapped to the same space. What do they mean by ""contradictory loss""?
","['terminology', 'papers', 'generative-adversarial-networks', 'variational-autoencoder', 'loss']",
Finding or creating a dataset for Neural Text Simplification,"
I'm currently starting a research project focused on NLP.
One of the steps involved in this project will be the development of a text simplification system, probably using a neural encoder-decoder architecture.
For most Text Simplification research available, the most commonly used dataset is one derived from pairing Wikipedia entries in both English and Simplified English. My problem arises from the fact that the focus of my research is not on the English Language, but rather in Portuguese, specifically Portugal Portuguese.
There exists no Simple Portuguese Wikipedia page and it seems that there exists no publicly available text simplification dataset in Portugal Portuguese at all. Due to this fact I'm curious if there would be any way of tackling this problem. Maybe having a dataset simply of complex Portuguese and simple portuguese, but with no pairings, although I'm not quite sure how that could be formulated to train a NN with.
So my question is if there are any text simplification datasets in Portugal (or maybe Bazil, as a last resource) Portuguese and, if not, what would be the optimal way to build that dataset.
Thank you.
","['natural-language-processing', 'machine-translation']",
"Is the Decoder mask (triangular mask) applied only in the first decoder block, or to all blocks in Decoder?","
The Decoder mask, also called ""look-ahead mask"", is applied in the Decoder side to prevent it from attending future tokens. Something like this:
[0, 1, 1, 1, 1]
[0, 0, 1, 1, 1]
[0, 0, 0, 1, 1]
[0, 0, 0, 0, 1]
[0, 0, 0, 0, 0]

But is this mask applied only in the first Decoder block? Or to all its blocks?
","['transformer', 'attention']","The masking should be applied to all Decoder blocks, otherwise in some blocks, past words can attend to future words, which would be cheating during training.This is reflected in The Annotated Transformer as well. Notice that in the Decoder class, the forward function applies the same mask to each layer of the decoder:"
How to build a DQN agent with state and action being arrays?,"
I have a Reinforcement-Learning environment where the state is an array of 0s and 1s with length equals to the number of users the agent must satisfy (11 users).
The agent must choose one of 12 resources for the 11 users according to the state array. If state[0] == 1, that means that user0 needs a resource, so the agent must choose a resource out of the 12 resources it has. So, the action array's first element would be, for example: action[0] = 10, which means that resource 10 was allocated to user0.
If the next user (user1) is asking for a resource as well, then the number of resources to choose from is 12 - 1, in other words, because resource10 was already allocated to user0, it cannot be allocated to another user.
If state[X] == 0, it means that userX is not asking for a resource, therefore it must not be allocated any resource.
An example of a state array:
[1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0]

An example of an action array according to the state array example: (resource count starts at 0 | -1 indicates no resource was allocated)
[10, 2, -1, -1, -1, 3, 11, 5, -1, -1, -1]

I'm new to Reinforcement Learning and Deep Learning, and I have no idea how to translate that into a neural network.
","['deep-learning', 'keras', 'dqn', 'deep-rl', 'deep-neural-networks']",
Transformers: how to get the output (keys and values) of the encoder?,"
I was reading the paper Attention Is All You Need.
It seems like the last step of the encoder is a LayerNorm(relu(WX + B) + X), i.e. an add + normalization. This should result in a $n$ x $d^{model}$ matrix, where $n$ is the length of the input to the encoder.
How do we convert this $n$ x $d^{model}$ matrix into the keys $K$ and values $V$ that are fed into the decoder's encoder-decoder attention step?
Note that, if $h$ is the number of attention heads in the model, the dimensions of $K$ and $V$ should both be $n$ x $\frac{d^{model}}{h}$. For $h=8$, this means we need a $n$ x $\frac{d^{model}}{4}$ matrix.
Do we simply add an extra linear layer that learns a $d^{model}$ x $\frac{d^{model}}{4}$ weight matrix?
Or do we use the output of the final Add & Norm layer, and simply use the first $\frac{d^{model}}{4}$ columns of the matrix and discard the rest?
","['natural-language-processing', 'transformer', 'bert', 'attention']","I have read the OpenNMT source code (https://github.com/OpenNMT/OpenNMT-py/blob/cd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e/onmt/modules/multi_headed_attn.py).It seems like an extra linear layer learns the weights $W^{key}$ and $W^{value}$ (plus biases), so to get the output (keys and values), you multiply the output of the encoder's final add + norm layer by $W^{key}$ to get the keys, and by $W^{value}$ to get the values.Additionally, these weights and biases seem to be independent across each of the decoding layer. So you feed the same encoder output (add + norm layer output), but multiply by different $W^{key}$ and $W^{value}$ matrices and add by different biases for each of the decoding layer, resulting in different keys and values for each layer"
Transformers: how does the decoder final layer output the desired token?,"
In the paper Attention Is All You Need, this section confuses me:

In our model, we share the same weight matrix between the two embedding layers [in the encoding section] and the pre-softmax linear transformation [output of the decoding section]

Shouldn't the weights be different, and not the same? Here is my understanding:
For simplicity, let us use the English-to-French translation task where we have $n^e$ number of English words in our dictionary and $n^f$ number of French words.

In the encoding layer, the input tokens are $1$ x $n^e$ one-hot vectors, and are embedded with a $n^e$ x $d^{model}$ learned embedding matrix.

In the output of the decoding layer, the final step is a linear transformation with weight matrix $d^{model}$ x $n^f$, and then applying softmax to get the probability of each french word, and choosing the french word with the highest probability.


How is it that the $n^e$ x $n^{model}$ input embedding matrix share the same weights as the $d^{model}$ x $n^f$ decoding output linear matrix? To me, it seems more natural for both these matrices to be learned independently from each other via the training data, right? Or am I misinterpreting the paper?
","['natural-language-processing', 'transformer', 'bert', 'attention']","I found the answer by reading the paper referenced by that section, Using the output embedding to improve language modelsBased on this observation, we propose threeway weight tying (TWWT), where the input embedding of the decoder, the output embedding of
the decoder and the input embedding of the encoder are all tied. The single source/target vocabulary of this model is the union of both the source
and target vocabularies. In this model, both in the
encoder and decoder, all subwords are embedded
in the same duo-lingual space.It seems like they learned a single embedding matrix ($n^e + n^f$) x $d^{model}$ in dimension."
Can AlphaFold predict proteins with metals well?,"
There are certain proteins that contain metal components, known as metalloproteins. Commonly, the metal is at the active site which needs the most prediction precision. Typically, there is only one (or a few) metals in a protein, which contains far more other atoms. So, the structural data that we could be used to train AlphaFold will contain far less information about the metal elements. Not to mention most proteins don't have metals at all (it is estimated that only 1/2-1/4 of all proteins contain metals [1]).
Given that maybe there is not enough structural data about protein local structure around metal atoms (e.g. Fe, Zn, Mg, etc.), then AlphaFold cannot predict local structure around metals well. Is that right?
I also think that the more complex electron shell of metal also makes the data less useful, since its bounding pattern is more flexible than carbon, etc.
","['machine-learning', 'alpha-fold']",
What's the purpose of layers without biases?,"
I noticed that the TensorFlow library includes a use_bias parameter for the Dense layer, which is set to True by default, but allows you to disable it. At first glance, it seems unfavorable to turn off the biases, as this may negatively affect data fitting and prediction.
What is the purpose of layers without biases?
","['neural-networks', 'tensorflow', 'hidden-layers', 'algorithmic-bias', 'dense-layers']",
Is AlphaFold just making a good estimate of the protein structure?,"
In the news, DeepMind's AlphaFold is said to have solved the protein folding problem using neural networks, but isn't this a problem only optimised quantum computers can solve?
To my limited understating, the issue is that there are too many variables (atomic forces) to consider when simulating how an amino acid chain would fold, in which case only a quantum computer can be used to simulate it.
Is the neural network just making a very good estimate, or is it simulating the actual protein structure?
","['neural-networks', 'deep-learning', 'deepmind', 'quantum-computing', 'alpha-fold']",
DQN Agent with a 2D matrix as input in Keras,"
I have a Reinforcement Learning environment where the state is a 2D matrix with 0s and 1s (only one column with the value of 1 in each row).
Example:
(
 (0, 1, 0),
 (0, 0, 1),
 (1, 0, 0),
 (0, 0, 0),
 (0, 1, 0)
)

The action the agent must take is for each row in the input, choose one resource out of 12 resources the agent has if there is a column with the value of 1 in that row, else choose no resource if the row has 0s only (example: row[3] wouldn't have any resources chosen for it by the agent). The rows correspond to the users the agent must allocate resources to.
In the step() method in the RL environment, the agent would receive a reward or a penalty depending on the action. If the reward is positive, the agent updates the state matrix, putting a 0 instead of 1 in the rows corresponding to the users that were allocated resources, which should be the next state. If the reward is negative, the episode ends, the environment resets and a new state is received by the agent
It came to my understanding that, in a deep learning approach, the DQN agent would receive a 2D matrix of 0s and 1s as input to its neural network (the state matrix), and output a vector with the chosen resources for each row of the input.
The network must choose a resource out of 12 resources for each row if that row has a 1 in it, and no resource is chosen if there is no column with the value of 1 in that row of the input. In other words, the network must choose an element out of 12 and output a vector with the chosen elements, depending on the input matrix.
Is there a way to do this using Deep Q-Learning and neural networks ?
","['deep-learning', 'tensorflow', 'keras', 'deep-rl', 'deep-neural-networks']",
"If $\alpha$ decreases over time, why is Q-learning guaranteed to converge?","
Q-Learning is guaranteed to converge if $\alpha$ decreases over time.
On page 161 of the RL book by Sutton and Barto, 2nd edition, section 8.1, they write that Dyna-Q is guaranteed to converge if each action-state pair is selected an infinite number of times and if $\alpha$ decreases appropriately over time.
It seems that it would be better if $\alpha$ increased over time, as it is the learning rate of the gradient of Q-function values ($R+\gamma\max_aQ(S',a)-Q(S,A)$), and, initially, they start off incredibly inaccurate, because they are initialized arbitrarily and over time converge to the true values, hence you'd want to weight them more as time increases rather than decrease it?
Why is this a convergence criterion?
","['reinforcement-learning', 'q-learning', 'convergence', 'learning-rate', 'dyna']","Why is this a convergence criterion?It is because $R$ and $S'$ are stochastic. A large learning rate applied when these values have variance would not converge to mean, but would wander around typically within some value proportional to $\alpha\sigma$ of the true value, where $\sigma$ is the standard deviation of the term $R + \gamma\text{max}_aQ(S',a)$. If you reduce $\alpha$ towards zero, then this expected error will also reduce to zero.For deterministic environments, it should be possible to prove convergence with large $\alpha$.In the special case of static policy, tabular learning and $\alpha = \frac{1}{N(s,a)}$ where $N(s,a)$ is number of visits to state $s$, action $a$, then the expected error for each Q value is the MSE from basic stats i.e. $\frac{\sigma_{TD}}{\sqrt{N(s,a)}}$"
What difference does it make whether Actor and Critic share the same network or not?,"
I'm learning about Actor-Critic reinforcement learning algorithms. One source I encountered mentioned that Actor and Critic can either share one network (but use different output layers) or they can use two completely separate networks. In this video he mentions that using two separate networks works for simpler problems, such as Mountain Car. However, more complex problems like Lunar Lander works better with a shared network. Why is that? Could you explain what difference that choosing one design over another would that make?
","['neural-networks', 'reinforcement-learning', 'actor-critic-methods']","One can expect the optimal high-level features required to choose the next action and to evaluate a state to be quite similar. Because of that, it is a reasonable idea to share the same network for both policy and value function – you are essentially parameter sharing the feature-extraction part of your neural network, and fine tuning the different heads of your network on the two different tasks: action choice and value prediction.Using two vs one networks is mostly a question of sample efficiency: theoretically, in both case your AC algorithm should work. In practice however, it will usually be useful to have parameter sharing as the representations encouraged by one of the tasks might be highly useful for the other and vice-versa, enabling one task to cause the other task to get unstuck from local optima. Another reason why this might work better is simply because you do not have to learn the same (or at least similar) representations from scratch twice – leading to a more sample efficient training."
Is it possible to use self-supervised learning on different images for the pretext and downstream tasks?,"
I have just come across the idea of self-supervised learning. It seems that it is possible to get higher accuracies on downstream tasks when the network is trained on pretext tasks.
Suppose that I want to do image classification on my own set of images. I have limited data on these images and maybe I can use self-supervised learning to achieve better accuracies on these limited data.
Let's say that I try to train a neural network on a pretext task of predicting the patch position relative to the center patch on different images that are readily available in quantity, such as cats, dogs, etc.
If I try to initialise the weights of my neural network, then do image classification on my own images, which are vastly different from that of the images used in the pretext task, would self-supervised learning work because the images for the pretext and downstream tasks are different?
TLDR: Must the images used in the pretext task and the downstream tasks be the same?
","['computer-vision', 'image-recognition', 'transfer-learning', 'self-supervised-learning', 'pretext-tasks']",
What is the impact of the number of features on the prediction power of a neural network?,"
What is the impact of the number of features on the prediction power of an ANN model (in general)? Does an increase in the number of features mean a more powerful prediction model (for approximation purpose)?
I'm asking these questions because I am wondering if there is any benefit in using two variables (rather than one) to predict one output.
If there is a scientific paper that answers my question, I would thank you.
","['neural-networks', 'deep-learning', 'reference-request', 'feature-selection', 'features']","Based on the clarifications given in the comments on the original question, i will try my best to give an answer.If the number of features increases, does the approximation capability of an ANN improve theoretically speaking?
It depends on whether this additional feature adds 'usefulness' to the data that was already supported. If you add a feature that is already in the dataset, it obviously will not increase the approximation capabilities. If you add a feature that is not yet in the current set of features and does influence the thing you are trying to approximate, then yes! There are some exceptions in this case of course, such as what if features are multicorrelated etc. It is not super important or measurable in ANNs, but statistics has a bunch of theory on this stuff. Simple statistical multiple regression has 4 assumptions and also checks for multicollinearity to see whether it is possible get 'useful' results from the data etc. As ANNs is super general, this is not applicable, but you could argue that similar practices could be looked into when seeking the best possible 'theoretical' validity of the features that you are using.Does adding more features make your ANNs produce more accurate results (in practice)?
Again, it depends. If you indeed added a useful feature, then the possibility exists that your ANN will in practice be able to produce more accurate results. But if your ANN is not able to converge to a solution, then your results will be gibberish. It is more likely that a network is unable to converge if you just throw data at it. More data requires bigger networks etc. So it is highly dependent on your training method. AKA, if your added feature is useful and the method you use is sufficient, then I'd argue that, yes, adding a feature will most likely make your ANNs produce more accurate results."
"AlphaGo Zero: does $Q(s_t, a)$ dominate $U(s_t, a)$ in difficult game states?","
AlphaGo Zero
AlphaGo Zero uses a Monte-Carlo Tree Search where the selection phase is governed by $\operatorname*{argmax}\limits_a\left( Q(s_t, a) + U(s_t, a) \right)$, where:

the exploitation parameter is $Q(s_t, a) = \displaystyle \frac{\displaystyle \sum_{v_i \in (s_t, a)} v_i}{N(s_t, a)}$ (i.e. the mean of the values $v_i$ of all simulations that passes through edge $(s_t, a)$)
the exploration parameter is $U(s_t, a) = c_{puct} P(s_t,a) \frac{\sqrt{\sum_b N(s_t, b)}}{1 + N(s_t, a)}$ (i.e. the prior probability $P(s_t, a)$, weighted by the constant $c_{puct}$, the number of simulations that passes through $(s_t, a)$, as well as the number of simulations that passes through  $s_t$).

The prior probability $P(s_t, a)$ and simulation value $v_i$ are both outputted by the deep neural network $f_{\theta}(s_t)$:

This neural network takes as an input the raw board representation s of the position and its history, and outputs both move probabilities and a value, (p, v) = fθ(s). The vector of move probabilities p represents the probability of selecting each move a (including pass), pa = Pr(a| s). The value v is a scalar evaluation, estimating the probability of the current player winning from position s.

My confusion
My confusion is that $P(s_t, a)$ and $v_i$ are probabilities normalized to different distributions, resulting in $v_i$ being about 80x larger than $P(s_t,a)$ on average.
The neural network outputs $(p, v)$, where $p$ is a probability vector given $s_t$, normalized over all possible actions in that turn. $p_a = P(s_t, a)$ is the probability of choosing action $a$ given state $s_t$. A game of Go has about 250 moves per turn, so on average each move has probability $\frac{1}{250}$, i.e. $\mathbb{E}\left[ P(s_t, a) \right] = \frac{1}{250}$
On the other hand, $v$ is the probability of winning given state $s_t$, normalized over all possible end-game conditions (win/tie/lose). For simplicity sake, let us assume $\mathbb{E} \left[ v_i \right] \ge \frac{1}{3}$, where the game is played randomly and each outcome is equally likely.
This means that the expected value of $v_i$ is at least 80x larger than the expected value of $P(s_t, a)$. The consequence of this is that $Q(s_t, a)$ is at least 80x larger than $U(s_t, a)$ on average.
If the above is true, then the selection stage will be dominated by the $Q(s_t, a)$ term, so AlphaGo Zero should tend to avoid edges with no simulations in them (edges where  $Q(s_t, a) = 0$) unless all existing $Q(s_t, a)$ terms are extremely small ($< \frac{1}{250}$), or the MCTS has so much simulations in them that the $\frac{\sqrt{\sum_b N(s_t, b)}}{1 + N(s_t, a)}$ term in $U(s_t, a)$ evens out the magnitudes of the two terms. The latter is not likely to happen since I believe AlphaGo Zero only uses $1,600$ simluations per move, so $\sqrt{\sum_b N(s_t, b)}$ caps out at $40$.
Selecting only viable moves
Ideally, MCTS shouldn't select every possible move to explore. It should only select viable moves given state $s_t$, and ignore all the bad moves. Let $m_t$ is the number of viable moves for state $s_t$, and let $P(s_t, a)$ = 0 for all moves $a$ that are not viable. Also, let's assume the MCTS never selects a move that is not viable.
Then the previous section is partly alleviated, because now $\mathbb{E} \left[ P(s_t, a) \right] = \frac{1}{m_t}$. As a result, $Q(s_T, a)$ should only be $\frac{m_t}{3}$ times larger than $U(s_t, a)$ on average. Assuming $m_t \le 6$, then there shouldn't be too much of an issue
However, this means that AlphaGo Zero works ideally only when the number of viable moves is small. In a game state $s_t$ where there are many viable moves ($>30$) (e.g. a difficult turn with many possible choices), the selection phase of the MCTS will deteriorate as described in the previous section.
Questions
I guess my questions are:

Is my understanding correct, or have I made mistake(s) somewhere?
Does $Q(s_t, a)$ usually dominate $U(s_t, a)$ by this much in practice when the game state has many viable moves? Is the selection phase usually dominated by $Q(s_t, a)$ during these game states?
Does the fact that $Q(s_t, a)$ and $U(s_t, a)$ being in such different orders of magnitude (when the game state has many viable moves) affect the quality of the MCTS algorithm, or is MCTS robust to this effect and still produces high quality policies?
How common is it for a game state to have many viable moves (>30) in Go?

","['reinforcement-learning', 'monte-carlo-tree-search', 'alphazero', 'alphago-zero', 'alphago']","I don't think you've necessarily made any real mistakes in your calculations or anything like that, that all seems accurate. I can't really confidently answer your questions about ""Does X usually happen?"" or ""How common is X?"", would have to experiment to make sure of that. I think we can also confidently immediately answer the question about whether MCTS is robust and can still produce high quality policies with ""yes"", since we've seen state-of-the-art, superhuman results in a bunch of games using these techniques.But I do think there's a few important details that may change your perception:MCTS does not compare $Q(s, a)$ values to $U(s, a)$ values in its selection phase. It compares $Q(s, a) + U(s, a)$ expressions of actions $a$, to $Q(s, b) + U(s, b)$ expressions for different actions $b$. So, the difference in magnitudes $Q(s, a) - U(s, a)$ is not nearly as important as the difference in magnitude $Q(s, a) - Q(s, b) + U(s, a) - U(s, b)$!For any single given state $s$, it is certainly not the case that we expect the different $Q$-values to be have a nice average like $0.5$ or anything like that. There will likely be plenty of states $s$ where we're already in such a strong position that we can afford to make a mistake or two and still expect to win; all the $Q$ values here will be close to $1.0$. There will also be many states where we're in such a terrible position that we expect to lose no matter what; all the $Q$ values here will be close to $0.0$. And then there will of course be states that a network is not sure about, which will have $Q$ values somewhere in between. I suspect that ""in between"" won't often be a nice mix of all sorts of different values though. If it's something like $0.7$, and there's higher values that attract more attention, during training the MCTS + network will likely become very interested in learning more about that state, and very quickly learn whether that should really just be a $1.0$ or whether it should be lowered. For this reason, I imagine that in unsure states, values will have a tendency to hover around $0.5$.MCTS will only let the $Q(s, a)$ term dominate the selection phase for as long as it believes that this is actually likely to lead to a win. If this is correct and indeed leads to a win, well, that's great, no need to explore anything else! During the tree search, if further investigation of this action leads the MCTS to believe that it actually is a loss, the $Q$ value will drop (ideally towards $0$), and then it will automatically stop being a dominant term. If the tree search fails to adjust for this in time, and we end up wandering down this losing path anyway, we'll get a value signal of $0$ at the end and update our value network and in the future we'll know better than to repeat this mistake."
Should my agent be taking varying number of steps?,"
My environment is set up so that my self-driving agent can take maximum of 400 steps (which is the end goal) before it resets with a completion reward. Despite attaining the end goal during the $\epsilon$-greedy stage, it still kills/crashes itself in subsequent episodes.
I would like to know if this common in RL (D3QN) scenarios.
A graph showing episodes vs steps has been placed below.

As one can see, the agent reaches 400 steps in episode 1000. But, in the subsequent episode, it falls down below 50 steps.
","['reinforcement-learning', 'epsilon-greedy-policy']","Your graph looks to me like a typical learning curve plotted for training process in reinforcement learning.Looking at it in detail I can say:There is clearly some learning occurring.There is a strong random element throughout. As you say the epsilon is reduced to near zero by episode 2000, and I would assume a driving simulation is mostly deterministic but with a lot of state hidden from the agent, this mainly implies that episodes are started in different states.There may be an effect from continued learning that causes the agent performance to vary so much. However, it is more likely towards the end that the agent is encountering new never-seen-before states and making poor decisions in them.The resulting graph matches to my experience of training DQN agents where some of the hyperparameters are off. Probably you need to do some kind of exploration or search through those parameters.From your comments:Perhaps the model is subsequently overfitting? Or maybe there is some catastrophic forgetting?These are possibilities that I cannot rule out by looking at the graph.However, I think it is more likely that you have some agent design or hyperparameters that are not a good fit to the problem. Those hyperparameters could be almost anything - not enough episodes to cover variation, neural network too simple, neural network too complicated, epsilon decay too fast, poor choice of regularisation, experience replay memory too small, poor choice of optimiser, etc etc.It is also possible you have limited the state model to the point that learning is hard or even impossible. A common beginner's mistake here is to use static sensor information such as a single current screenshot on each timestep, so the agent has no way to assess how fast it is going, or which way it is already turning etc (if your input includes direct knowledge of current speed, turning etc then this is probably not a problem for you, I am just raising one of many possibilities).One thing that may help you understand the agent's learning performance better is instead of plotting the learning episode graph, plot a test graph, perhaps once every ~100 episodes, where you run some number of episodes without learning and with $\epsilon = 0$, and take average of the metrics you are interested in. This will remove some of the randomness from the values you are plotting and give you a much better sense of training progress than your current plot.With Q learning, you may also get significant benefit by setting a non-zero minimum $\epsilon$, e.g. $\epsilon = \text{max}(\text{eps_decay}(n), 0.01)$ where $\text{eps_decay}(n)$ is your current epsilon decay function for step $n$. That is because the off-policy updates with exploration are still helpful at all stages of learning."
Training a model to identify certain differences between images?,"
Newbie to CV here so sorry of this is basic. Here's the deal, I have a program that I run many times. and each run I produce a screenshot. I need to compare screenshots from N-1 and N runs and make sure they aren't different in any dramatic way. Of course there are some minor changes like logos and pictures getting updated, etc.
SO far I've used something as simple as absdiff from opencv to highlight the difference regions and then use some sort of threshold to determine whether something passes or not. But I want to make it slightly intelligent but I'm not 100% sure how to proceed. Google hasn't yielded ghe best answers.
Essentially, I want to train the model on many different pairs of images and have the output be binary, yes or no depending on whether it should pass or not. In theory, I should be able to plug in 2 images and based on previous training, it should be able to tell me whether there is significant difference or not. What are some ways I might approach this, particularly with regards to what kinds of models to use.
The requirements here might seem amorphous but that's kinda the nature of the problem. the differences could be, in theory, anything. I am hoping that there will be patterns between different images and that a model would pick up on that. Things like the name of a document is 045 instead of 056 or a logo is slightly updated.
","['neural-networks', 'convolutional-neural-networks', 'computer-vision', 'image-recognition', 'image-processing']",
What happens when the agent faces a state that never before encountered?,"
I have a network with nodes and links, each of them with a certain amount of resources (that can take discrete values) at the initial state. At random time steps, a service is generated, and, based on the agent's action, the network status changes, reducing some of those nodes and links resources.
The number of all possible states that the network can have is too large to calculate, especially since there is the random factor when generating the services.
Let's say that I set the state space large enough (for example, 5000), and I use Q-Learning for 1000 episodes. Afterwards, when I test the agent ($\max Q(s,a)$), what could happen if the agent faces a state that did not encounter during the training phase?
","['reinforcement-learning', 'q-learning', 'state-spaces', 'observation-spaces']","Having too many states to actually visit is a common problem in RL. This is exactly why we often use function approximation. If you replace your q table with a good function approximator such as a neural network, it should be able to generelize well to states it has not yet encountered.If you do not use a function approximator but stick with a table, the agent will have no idea what to do when it encounters a new state. For more information,  see Reinforcement Learning  by Sutton and Barto, chapter 9."
Suppose every-visit MC was used instead of first-visit MC on blackjack. Would you expect the results to be different?,"
This is a question from page 94 of Sutton and Barto's RL book 2020.
I read in someone's compiled GitHub answers to this book's exercises their answer was: ""No because each state in an episode of blackjack is unique.""
I think my answer is more yes, but I'm thinking in terms of casino blackjack, where they have multiple decks shuffled together and add in the dropped cards back into the deck every X games in order to prevent card counting and 1 game can be seen as an episode. I think in this case that first-visit MC and every-visit MC would have drastically different results, given that, at the start of the new episode, the state of the deck, which is only partially observed, will change the value of taking an action given a state (because I believe the cards left in the deck affect the value of an action, but the deck is not totally observable).
If this is blackjack, where the discarded cards are added back in and shuffled every episode, I'll agree that it shouldn't make a difference.
Are there any flaws in my conjecture?
","['reinforcement-learning', 'monte-carlo-methods', 'sutton-barto']",
How to fill NaNs in Cross-Validation?,"
I have been searching this but did not find the answer, so sorry if this is a duplicated question.
I was working with cross-validation, where some doubts came to my mind, and I am not sure which is the correct answer.
Let's say I have a mixed dataset, with numerical and categorical features. I want to perform a K-Fold Cross-Validation with it, with a K=10. Some of these numerical features are missing, so I decided that I will replace those NaNs with the average of that feature.
My steps are the following ones:

Read the entire dataset
Perform One Hot Encoding to categorical features.
Divide my data into different folds. Let's say that I will use 90% for training, 10% for validating.
For every different combination of folds, I replace the missing values from the training and validating sets separately. This means, on one hand, I get the average of the missing values of the training part, and on the other hand the average of the missing values of the validating part.
Normalize the data of the training and validating sets between [0, 1] separately, as I did before.
Train the correspondant model.

So let's put a simple example of a dataset of 20 rows with N columns. Once I do steps 1 and 2, in the first iteration I will select the 18 first rows as a training set, and the last two rows as validating set.
I fill the missing values of the 9 first rows with the average of those 18 rows. Then the same for the 2 last rows.
Then, again, normalize in the same way, separately. And do this for every combination of folds.
I am doing it like this, because otherwise, from my understanding, is that you are training your model with biased data. You should not have access to the validation data, thus you should not be able to do the average with those numbers. Hence I am using only the numbers of the training part. If I do the average with the entire dataset, this will make my model overfitting.
I am not so sure about the normalization step, as I do not really think this will have the same impact. But here I do not really know...
Is this approach correct? Or should I do the average and normalization with the entire dataset? Why?
","['machine-learning', 'data-preprocessing', 'cross-validation']",
"What is the intuition behind equations 10, 11 and 12 of the paper ""Noise2Noise: Learning Image Restoration without Clean Data""?","
Can anyone help me understand these functions described in the paper Noise2Noise: Learning Image Restoration without Clean Data
I have read the portion A.4 in the appendix but need a more detailed and easier to understand explanation. Especially where signum or sign function (sgn) is coming from along with other explanations.
(10),(11),(12)

","['deep-learning', 'objective-functions', 'papers', 'activation-functions']",
Advantage Actor Critic model implementation with Tensorflowjs,"
I am trying to implement an Actor Critic method that controls an RC car. For this I have implemented a simulated environment and actor critic tensorflowjs models.
My intention is to train a model to navigate an environment without colliding with various obstacles.
For this I have the following:
State(continuous):

the sensors distance(left, middle, right): [0..1,0..1,0..1]

Action(discrete):

4 possible actions(move forward, move back, turn left, turn right)

Reward(cumulative):

moving forward is encouraged
being close to an obstacle is penalized
colliding with an obstacle is penalized

The structure of the models:
buildActor() {
      const model = tf.sequential();
      model.add(tf.layers.inputLayer({inputShape: [this.stateSize],}));

      model.add(tf.layers.dense({
        units: parseInt(this.config.hiddenUnits),
        activation: 'relu',
        kernelInitializer: 'glorotUniform',
      }));

      model.add(tf.layers.dense({
        units: parseInt(this.config.hiddenUnits/2),
        activation: 'relu',
        kernelInitializer: 'glorotUniform',
      }));

      model.add(tf.layers.dense({
        units: this.actionSize,
        activation: 'softmax',
        kernelInitializer: 'glorotUniform',
      }));

      this.compile(model, this.actorLearningRate);

      return model;
    }

buildCritic() {
      const model = tf.sequential();

      model.add(tf.layers.inputLayer({inputShape: [this.stateSize],}));

      model.add(tf.layers.dense({
        units: parseInt(this.config.hiddenUnits),
        activation: 'relu',
        kernelInitializer: 'glorotUniform',
      }));

      model.add(tf.layers.dense({
        units: parseInt(this.config.hiddenUnits/2),
        activation: 'relu',
        kernelInitializer: 'glorotUniform',
      }));

      model.add(tf.layers.dense({
        units: this.valueSize,
        activation: 'linear',
        kernelInitializer: 'glorotUniform',
      }));

      this.compile(model, this.criticLearningRate);

      return model;
    }

The models are compiled with an adam optimized and huber loss:
compile(model, learningRate) {
      model.compile({
        optimizer: tf.train.adam(learningRate),
        loss: tf.losses.huberLoss,
      });
    }

Training:
trainModel(state, action, reward, nextState) {
      let advantages = new Array(this.actionSize).fill(0);

      let normalizedState = normalizer.normalizeFeatures(state);
      let tfState = tf.tensor2d(normalizedState, [1, state.length]);
      let normalizedNextState = normalizer.normalizeFeatures(nextState);
      let tfNextState = tf.tensor2d(normalizedNextState, [1, nextState.length]);

      let predictedCurrentStateValue = this.critic.predict(tfState).dataSync();
      let predictedNextStateValue = this.critic.predict(tfNextState).dataSync();

      let target = reward + this.discountFactor * predictedNextStateValue;
      let advantage = target - predictedCurrentStateValue;
      advantages[action] = advantage;
      // console.log(normalizedState, normalizedNextState, action, target, advantages);

      this.actor.fit(tfState, tf.tensor([advantages]), {
        epochs: 1,
      }).then(info => {
          this.latestActorLoss = info.history.loss[0];
          this.actorLosses.push(this.latestActorLoss);
        }
      );

      this.critic.fit(tfState, tf.tensor([target]), {
        epochs: 1,
      }).then(info => {
          this.latestCriticLoss = info.history.loss[0];
          this.criticLosses.push(this.latestCriticLoss);
        }
      );

      this.advantages.push(advantage);
      pushToEvolutionChart(this.epoch, this.latestActorLoss, this.latestCriticLoss, advantage);
      this.epoch++;
    }

You ca give the simulation a spin on https://sergiuionescu.github.io/esp32-auto-car/sym/sym.html .
I found that some behaviors are being picked up - the model learns to prioritize moving forward after a few episodes, but then hits the wall and it reprioritizes spinning - but seems to completely 'forget' that moving forward was ever prioritized.
I've been trying to follow https://keras.io/examples/rl/actor_critic_cartpole/ to a certain extent, but have not found an equivalent of the way back-propagation is handled there - GradientTape.
Is it possible to perform training similar to the Keras example in Tensorflowjs?
The theory i've went through on Actor Critic mentions that the Critic should estimate the reward yet to be obtain until the rest of the episode, but i am training the critic with:
reward + this.discountFactor * predictedNextStateValue where reward is the cumulative reward until the current step.
Should i keep track of a maximum total reward in previous episodes and subtract my reward from that instead?
When i am training the actor i am generating a zero filled advantages tensor:
let advantages = new Array(this.actionSize).fill(0);
let target = reward + this.discountFactor * predictedNextStateValue;
let advantage = target - predictedCurrentStateValue;
advantages[action] = advantage;

All other actions than the taken one will receive a 0 advantage. Could this discourage any previous actions the were proven beneficial?
Should i average out the advantages per state and action?
Thanks for having the patience to go trough all of this.
","['tensorflow', 'keras', 'actor-critic-methods']",
Looking for a good approach for building an automated director for a racing game spectator mode,"
I'm building a tool that should assist a director to broadcast a racing game. I want this tool to suggest the human director which car to focus on and with which camera (among the available ones). I can access quite a lot of data about the current race so I can extrapolate some parameters(like car positions, how many cars near to each other there are, how close they are, last time the camera was switched etc) to be used in the decision making process. I would like the AI to learn from the human director in order to suggest him according to his ""direction style"".
My idea is to split the problem in 2 sub problems: the first is the choice of the car to focus on and the second is the choice of the camera to use (or cameras, since is fairly common to switch cameras while following the same car). My plan was to use some sort of Q-learning, rewarding the AI whenever one of the generated suggestions is chosen by the director but I guess it would be really difficult to define a set of states and moreover it would probably take ages before it would start to give some useful suggestions.
Are there some other good approaches I could consider? I'm also thinking about using a neural network so maybe the learning process would be faster.
","['neural-networks', 'machine-learning', 'reinforcement-learning']",
Why should variance(output) equal variance(input) in Xavier Initialisation?,"
In a lot of explanations online for Xavier Initialization, I see the following:

With each passing layer, we want the variance to remain the same. This helps us keep the signal from exploding to a high value or vanishing to zero. In other words, we need to initialize the weights in such a way that the variance remains the same for x and y. This initialization process is known as Xavier initialization.

Source https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/
However, the intuition behind why var(output) should equal var(inputs) is never explained. Does anyone know why intuitively var(output) should equal var(inputs)?
","['neural-networks', 'deep-learning', 'feedforward-neural-networks', 'weights', 'weights-initialization']",
"For episodic tasks with an absorbing state, why can't we both have $\gamma=1$ and $T= \infty$ in the definition of the return?","
For episodic tasks with an absorbing state, why can't $\gamma=1$ and $T= \infty$?
In Sutton and Barto's book, they say that, for episodic tasks with absorbing states that becomes an infinite sequence, then the return  is defined by:
$$G_t=\sum_{k=t+1}^{T}\gamma^{k-t-1}R_k$$
This allows the return to be the same whether the sum is over the first $T$ rewards, where $T$ is the time of termination or over the full infinite sequence, with $T=\infty$ xor $\gamma=1$.
Why can't we have both? I don't see how they can both be set to those parameters. It seems like, if you have an absorbing state, the rewards from terminal onward will just be 0 and not be affected by $\gamma$ or $T$.
Here's the full section of the book on page 57 in the 2nd edition

I think the reasoning behind this also leads to why for policy evaluation where
$$v_\pi(s)=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]$$
""Has an existence and uniqueness guarantee only if $\gamma < 1$ or termination is guaranteed under $\pi$""(page 74). This part I'm also a bit confused by, but seems related.
","['reinforcement-learning', 'hyper-parameters', 'return', 'discount-factor']","$T = \infty$ and $\gamma = 1$ cannot be both true at the same time because the return defined in equation 3.11 is supposed to be a unified definition of the return for both continuing and episodic tasks. In the case of continuing tasks, $T = \infty$ and $\gamma = 1$ cannot be true at the same time, because the return may not be finite in that case (as I think you already understood).Moreover, note that, in that specific example of the book, they assume that the agent ends up in an absorbing state, so this specific sum is finite, no matter whether $T$ is finite or $\infty$, given that, once you enter the absorbing state, you will always get a reward of $0$. Of course, if you discount those specific rewards, the sum will still be finite. However, in general, if you had a different MDP where the absorbing state is not reachable (i.e. the episode never ends), then the return could not be finite."
Why is it useful to define the return as the sum of the rewards from time $t$ onward rather than up to $t$?,"
Why is it useful to define the return as the sum of the rewards from time $t$ onward rather than up to $t$?
The return for an MDP is usually defined as
$$G_t=R_{t+1}+R_{t+2}+ \dots +R_T$$
Why is this defined as the return? Is there anything useful about this?
It seems like it's more useful to define the return as $$G_t=R_0+ \dots+R_t,$$ because your ""return"", so to speak, is the ""profit from investment"" so it seems like your return will be your accumulated reward from taking actions up to that point.
","['reinforcement-learning', 'definitions', 'markov-decision-process', 'return']","It wouldn't make sense to define the return as you propose, from time 0 to $t$. Once we are in a state at time $t$ we don't care what the returns have been, rather what they will be in the future, thus returns are defined as the total sum of discounted rewards from the current time step onwards. This allows the agent to make decisions about which actions to take based on how valuable taking said action is in the current state at time $t$ -- clearly the rewards previous to this have no effect upon that."
Why does each component of the tuple that represents an action have a categorical distribution in the TRPO paper?,"
I was going through the TRPO paper, and there was a line under Appendix D ""Approximating Factored Policies with Neural Networks"" in the last paragraph which I am unable to understand

The action consists of a tuple $(a_1, a_2..... , a_K)$ of integers $a_k\in\{1, 2,......,N_k\} $ and each of these components is assumed to have a categorical distribution.

I can't seem to get how each component has a categorical distribution. I think it should be the tuple that has a categorical distribution.
I think I am getting something wrong.
","['reinforcement-learning', 'papers', 'trust-region-policy-optimization', 'action-spaces']","I'm not sure specifically which Atari games present this type of action space, but you can imagine a game in which you can perform multiple different types of actions at the same timestep (i.e. the different ""factors"" they mention in the paper).As an example, imagine a game in which you can both move and jump at the same time. In that case, you might have a 4-dimensional discrete action space for moving (NSWE), and a 2-dimensional discrete action space for jumping (yes/no jump), both of which will require a Categorical distribution which has the size of that factor ($N_k$ in the paper).So in this case, you would need to have a categorical distribution for each factor, unless you were to turn these two factors into 1 joint 4*2-dimensional factor and learn a single categorical distribution on that (which would likely be less efficient)."
How did the variance and double summation of the covariance come to the L2 minimization equation?,"
I am trying to understand the last two lines of this math notation (from this paper).

How did the Var and double summation of the Cov come to the equation?
The first two lines I understood something like $(a-b)^2 = a^2 -2ab +b^2$.
","['machine-learning', 'papers', 'objective-functions', 'math']",
Are convolutional neural networks inspired by the human brain?,"
The Deep Learning book by Goodfellow et al. states

Convolutional networks stand out as an example of neuroscientiﬁc principles inﬂuencing deep learning.

Are convolutional neural networks (CNNs) really inspired by the human brain?
If so, how? In particular, what structures within the brain do CNN-like neuron groupings occur?
","['convolutional-neural-networks', 'brain', 'neuroscience', 'neocognitron']","Yes, CNNs are inspired by the human brain [1, 2, 3]. More specifically, their operations, the convolution and pooling, are inspired by the human brain. However, note that, nowadays, CNNs are mainly trained with gradient descent (GD) and back-propagation (BP), which seems not to be a biologically plausible way of learning, but, given the success of GD and BP, there have been attempts to connect GB and BP with the way humans learn [4].The neocognitron, the first convolutional neural network [1], proposed by Kunihiko Fukushima in 1979-1980, and described in the paper Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position, already uses convolutional and pooling (specifically, averaging pooling) layers [1]. The neocognitron was inspired by the work of Hubel and Wiesel described in the 1959 paper Receptive fields of single neurones in the cat's striate cortex.Here is an excerpt from the 1980 Fukushima's paper.The mechanism of pattern recognition in the brain is little known, and it seems to be almost impossible to reveal it only by conventional physiological experiments. So, we take a slightly different approach to this problem. If we could make a neural network model which has the same capability for pattern recognition as a human being, it would give us a powerful clue to the understanding of the neural mechanism in the brain. In this paper, we discuss how to synthesize a neural network model in order to endow it an ability of pattern recognition like a human being.Several models were proposed with this intention (Rosenblatt, 1962; Kabrisky, 1966; Giebel, 1971; Fukushima, 1975). The response of most of these models, however, was severely affected by the shift in position and/or by the distortion in shape of the input patterns. Hence, their ability for pattern recognition
was not so high.In this paper, we propose an improved neural network model. The structure of this network has been suggested by that of the visual nervous system of the vertebrate. This network is self-organized by ""learning without a teacher"", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their position nor by small distortion of their shapes. This network is given a nickname ""neocognitron"", because it is a further extention of the ""cognitron"", which also is a self-organizing multilayered neural network model proposed by the author before (Fukushima, 1975)However, Fukushima did not train the neocognitron with gradient descent (and back-propagation) but with local learning rules (which are more biologically plausible), and that's probably why he doesn't get more credit, as I think he should.You should read at least Fukushima's paper for more details, which I will not replicate here.Section 9.4 of the Deep Learning book also contains details about how CNNs are inspired by neuroscience findings."
"Image classification - Need method to classify ""unknown"" objects as ""trash"" (3D objects)","
We have an image classifier that was built using CNN with faster R-CNN and Yolov5.
It is designated to run on 3D objects.
All of those objects have similar ""features"" structure, but the actual features of each object class are somewhat different one from another. Therefore, we strive to detect the classes based on those differences in features.
In theory there are thousands of different classes, but for now we have trained the model to detect 4 types of classes, by training it on data sets that includes many images from different angles for each of those 4 classes (1,000 images each).
The main problem we face is that whenever the model runs on an ""unknown"" object, it may still classify it as one of our 4 classes, and sometimes it will do it with a high probability score (0.95), which hinders the whole credibility of our model results.
We think it might be since we are using SoftMax, which seems to force the model to assign an unknown object to one of the 4 classes.
We want to know what will the best way to overcome this issue.
We tried adding a new, fifth ""trash"" class, with 1,000 images of ""other"" objects that do not belong to our four classes, but it significantly reduced the confidence level for our test images, so we are not sure if this is at all a progress.
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'classification', 'object-recognition']",
"Why is the policy implied by Q-learning deterministic, when it always chooses the action with highest probability?","
Q-learning uses the maximizing value at each step, which implies that there is a probability distribution and it happens to choose the one with the highest probability. There is no direct mapping between a particular state to ONLY a particular action but a bunch of actions with varying probabilities. I don't understand.
","['reinforcement-learning', 'q-learning']","Q-learning uses the maximizing value at each step,Mostly true. The target policy that Q-learning learns the action values of is the one with the maximum value. While training, Q-learning will take one action randomly (typically with a high probability of taking the action with maximum value), whilst it makes updates to value estimates assuming it will always take the maximising action next.which implies that there is a probability distribution and it happens to choose the one with the highest probability.Not true. This is not implied at all. The action values that Q-learning estimates are not probabilities, but expected sums of future reward. The probability distribution for exploring actions is added outside of the Q table or Q function estimate. There is no implied probability distribution of actions in the target policy. When implementing the target policy, you may decide to break ties randomly, but that is not an important detail.It is worth noting that even if you were working with a table of action probabilities, then an agent that always chose the one with the maximum probability would be deterministic. A list of numbers from a table or non-stochastic function (which is how a Q table or Q function is implemented, and also how policy functions are implemented for methods that do process probabilities), even if it represents probabilities, cannot be random in itself. Instead, it must be interpreted by a process that decides how to use them. A process that includes a random number generator can use the numbers to generate a probability distribution that it samples from.In Q-learning, the process that updates the Q table estimates does not include a random number generator, so as a consequence it is deterministic. However, the choice of which actions to explore is often random, and it is sometimes the case that the environment includes randomness in state transitions or reward values. So Q-learning taken as a whole is a random process."
Autoencoder: predictions missing for nodes in the bottleneck layer,"
I'm using tf.Keras to build a deep-fully connected autoencoder. My input dataset is a dataframe with shape (19947,), and the purpose of the autoencoder is to predict normalized gene expression values. They are continuous values that range from [0,~620000].
I tried different architectures and I'm using relu activation for all layers. To optimize I'm using adam with mae loss.
The problem I have is the network trains successfully (although the train loss is still terrible) but when I'm predicting I notice that although the predictions do make sense for some nodes, there are always a certain number of nodes that only output 0. I've tried changing the number of nodes of my bottleneck layer (output) and it always happens even when I decrease the output number.
Any ideas on what I'm doing wrong?
tf.Keras code:
input_layer = keras.Input(shape=(19947,))
simple_encoder = keras.models.Sequential([
    input_layer,
    keras.layers.Dense(512, activation='relu'),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(16, activation='relu')
])
simple_decoder = keras.models.Sequential([
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(512, activation='relu'),
    keras.layers.Dense(19947, activation='relu')
])
simple_ae = keras.models.Sequential([simple_encoder, simple_decoder])
simple_ae.compile(optimizer='adam', loss='mae')
simple_ae.fit(X_train, X_train,
              epochs=1000,
              validation_data=(X_valid, X_valid),
              callbacks=[early_stopping])

Output of encoder.predict with 16 nodes on the bottleneck layer. 7 nodes predict only 0's and 8 nodes predict ""correctly""

","['python', 'tensorflow', 'keras', 'autoencoders', 'bottlenecks']","My closest guess is because you are using the activation function ReLU, which pushes data to be greater than zero. However, because of your data's nature, the autoencoder is highly dependent on negative calculations to reconstruct your data, but the best it can achieve is zero.In the context of artificial neural networks, the rectifier is an activation function defined as the positive part of its argument:
$$f(x) = x^+ = max(0, x)$$
https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
What is a good way of identifying volatile positions for a checkers game?,"
I am implementing an AI for a mobile checkers game, and have used alpha-beta pruning with Minimax.
Now I have the problem of horizontal effect, and need to do Quiesence search to avoid that.
Any advice on what makes a position volatile for a checkers game?
I want to consider the cases when player can take a piece, and also when any piece can be taken by opponent a volatile position, and continue searching for another depth.
Anything else?
","['game-ai', 'minimax', 'game-theory', 'alpha-beta-pruning', 'checkers']",
How significant is the decoder part of the capsule network?,"
Capsule Networks use an encoder-decoder structure, where the encoder part consists of the capsule layers (PrimiaryCaps and DigitCaps) and is also the part of the capsule network which performs the actual classification. On the other hand, the decoder attempts to reconstruct the original image from the output of the correct DigitCap. The Decoder in the Capsule Network is used as the regularizer of the network as it helps the network learn better features.
I can see how the decoder is helpful for datasets such as MNIST where all image classes have clear differences and the input size if the image is quite small. However, if the input has large dimensions and differences between image classes are quite small, I see the decoder network as overkill, as it will find it hard to reconstruct images for different classes.
In my case, my dataset consists of 3D MRI images of patients which have Alzheimer's Disease and those who do not. I am down-sampling the images and producing 8 3D patches which will be used as input to the network. The patches still have high dimensions considering that these are 3D, and there are not many clear differences between patches of the two image classes.
My questions here are:

How significant is the decoder part of the capsule network? CNNs that perform image classification, usually do not have a decoder part. Why does the capsule network rely on the decoder to learn better features?

Are there any alternatives to the decoder within the capsule network, acting as a regularizer? Can the decoder be ignored completely?


","['machine-learning', 'deep-learning', 'regularization', 'capsule-neural-network']",
What do echo state networks give us over a generic RNN resevoir?,"
Slightly generalizing the definition in Jaeger 2001, let's define a reservoir system to be any system of the form
$$h_{t}=f(h_{t-1}, x_t)$$
$$y_t=g(Wh_t)$$
where $f$ and $g$ are fixed and $W$ is a learnable weight matrix. The idea is we feed a sequence of inputs $x_t$ into the system, which has some fixed initial state $h_0$, and thereby generate the sequence of outputs $y_t$. $f$ is fixed (for example, a randomly generated RNN) we can then attempt to learn $W$ in some way in order to get the system to have the behavior that we want.
Now we add the echo state condition: the system has the echo state condition iff for any left-infinite sequence $...x_{-3}, x_{-2}, x_{-1}, x_0$, there is only one sequence of states $h_t$ consistent with this input sequence.
Seen from this perspective, any training procedure that could be applied to an echo state system could be applied to a generic reservoir system. So what do we get out of the echo state condition? Is there some reason to think echo state systems will generalize better, or be more quickly trainable? Jaeger does not seem to attempt to argue in this direction, he just describes how to train an ESN, but as I've said, nothing about these training methods seems to require the echo state property.
","['neural-networks', 'echo-state-network', 'reservoir-computing']",
How can we derive a Convolution Neural Network from a more generic Graph Neural Network?,"
Convolution Neural Network (CNNs) operate over strict grid-like structures ($M \times N \times C$ images), whereas Graph Neural Networks (GNNs) can operate over all-flexible graphs, with an undefined number of neighbors and edges.
On the face of it, GNNs appear to be neural architectures that can subsume CNNs. Are GNNs really generalized architectures that can operate arbitrary functions over arbitrary graph structures?
An obvious follow-up - How can we derive a CNN out of a GNN?
Since non-spectral GNNs are based on message-passing that employ permutation-invariant functions, is it possible to derive a CNN from a base-architecture of GNN?
","['deep-learning', 'convolutional-neural-networks', 'comparison', 'geometric-deep-learning', 'graph-neural-networks']",
Compressing Parameters of an Response System,"
I have an input-output system, which is fully determined by 256 parameters, of which I know a significant amount are of less importance to the input-output pattern.
The data I have is some (64k in total) input-parameter-output match.
My goal is to compress these 256 parameters to a smaller scale (like 32) using an encoder of some kind while being able to preserve the response pattern.
But I can't seem to find a proper network for this particular problem, because I'm not trying to fit these parameters (they all have a mean of one and variance of 1/4), but rather its influence on the output, so traditional data-specific operations will not work in this case.
","['autoencoders', 'variational-autoencoder', 'dimensionality-reduction']",
Do I need to know in advance all possible number of states in Q-Learning?,"
In Q-learning, is it mandatory to know all possible states that can the agent may end up in?
I have a network with 4 source nodes, 3 sink nodes, and 4 main links. The initial state is the status network where the sink nodes have its resources at its maximum. In a random manner, I generate service requests from the source nodes to the sink nodes. These service requests are generated at random timesteps, which means that, from state to state, the network status may stay the same.
When a service request is launched, the resources from the sink nodes change, and the network status changes.
The aim of the agent is to balance the network by associating each service request to a sink node along with the path.
I know that in MDP you are supposed to have a finite set of states, my question is: if that finite set of states is supposed to be all possible states that can happen, or is just a number that you consider enough to optimize the Q-table?
","['reinforcement-learning', 'q-learning', 'state-spaces']","When you start off learning about Q-learning, you start with a simple example that has a few states. For each of the states, you try to estimate what the 'value' is of that state. Because there are so few states, it is possible to store these values in a table (it is also useful for the intuitiveness of the explanation).However, if you start trying to solve more 'real-life' problems, the number of states can be insanely huge. The essence however will stay the same. You are trying to estimate what you want to do next, based on an estimation of how good each state is that you can end up in. However, now the values are 'most of the times' not stored in a table anymore, as the approach will often come down to using an ANN to estimate the value function.Answer to your question: You are going to run into a lot of problems when training some model with Q-learning if your table is not able to store the values of possible states that it can come across. In practice, most implementations do not use a Q-table and just use an ANN, which alleviates the problem of having to 'define' how many states your problem consists of."
Is it okay to think of any dataset in artificial intelligence as a mathematical set?,"
A dataset is a collection of data points. It is known that the data points in the dataset can repeat. And the repetition does matter for building AI models.
So, why does the word dataset contain the word set? Does it have any relation with the mathematical set, where order and repetition do not matter?
","['datasets', 'math', 'definitions']","It's true that your original dataset can contain duplicates, so it should not be called a set, in order to be consistent with the mathematical definition of a set. There are mathematical objects known as multi-sets that can contain duplicates, but the order of the elements is still not relevant. There are also tuples and sequences, where the order of the elements matters.If you want to get rid of the duplicate elements in your dataset, you could perform a pre-processing step where you remove them. Even if you do that, it is often the case that, if you are learning with mini-batches (i.e. using mini-batch stochastic gradient descent), these mini-batches could contain the same elements, because you may sample the same element in different batches or even in the same batch (this is known as sampling with replacement). Of course, this depends on how you sample your training dataset to build the batches (or mini-batches). So, if you do not want duplicates even in the mini-batches, you need to perform sampling without replacement.Moreover, there are datasets that contain elements whose order in the dataset can be relevant for the predictions, such as datasets of time-series data, while, in mathematical sets and multi-sets, the order of the elements does not matter.So, yes, it is often called a dataset (or data set), but it is not necessarily a set in a mathematical sense. In general, it should just be interpreted as a collection of data. In scenarios where the order of the elements or the existence of duplicates in the dataset (or any other information or property of your collection of data) is relevant, you should probably emphasize/note it."
What constitutes a large space state (in Q-learning)?,"
I know this might be specific to different problems, but does anyone know if there is any rule of thumb or references on what constitutes a large state space?
I know that, according to multiple papers, tabular Q-learning is not suitable for these problems with large state spaces, but I can't find any indication of what would constitute a large state space
I've been working on a problem where my state space is about 30x30 and updating my Q-learning with the tabular method runs and works well. Would 100x100 start to become too big or 400x400?
","['reinforcement-learning', 'q-learning', 'state-spaces']",
How to compute the Retrace target for multi-step off-policy Reinforcement Learning?,"
I am implementing the A3C algorithm and I want to add off-policy training using Retrace but I am having some trouble understanding how to compute the retrace target. Retrace is used in combination with A3C for example in the Reactor.
I often see the retrace update written as
\begin{equation}
\Delta Q(s, a) = \sum_{t' = t}^{T} \gamma^{t'-t}\left(\prod_{j=t+1}^{t'}c_j\right) \delta_{t'}
\end{equation}
with $\delta_{t'} = r(s_{t'}, a_{t'}) + \gamma \mathbb{E}[Q(s_{t'+1}, a_{t'+1})] - Q(s_{t'}, a_{t'})$ and $c_j$ being the Retrace factors $c_j = \lambda \min(c, \frac{\pi(a_j|s_j)}{b(a_j|s_j)})$.
Now, when employing neural networks to approximate $Q_{\theta}(s, a)$ it is often easier to define a loss
\begin{equation}
\mathcal{L}_{\theta} = \left(G_t - Q(s, a)\right)^2
\end{equation}
and let the backward function and the optimizer do the update. How can I write the Retrace target $G_t$ to use in such a setup?
Is it correct to write it as follows?
\begin{equation}
G_t = \sum_{t'=t}^T \gamma^{t'-t} \left(\prod_{j=t+1}^{t'}c_j\right) (r_{t'} + \gamma Q(s_{t'+1}, a_{t'+1}) - Q(s_{t'}, a_{t'}))
\end{equation}
and then compute $\mathcal{L}$ as above, take the gradient $\nabla\mathcal{L}_{\theta}$ and perform the update step $Q(s_t, a_t) = Q(s_t, a_t) + \alpha \nabla\mathcal{L}_{\theta}$ ?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'importance-sampling']",
Statistical method for selecting features for classification,"
I'm working on a classifier for the famous MNIST handwritten data set.
I want to create a few features on my own, and I want to be able to estimate which feature might perform better before actually training the classifier. Lets say that I create the feature which calculates the ratio of ink used between two halves of a digit. Note that with ink I mean how much white there is used (which ranges from 0-255 per pixel).


For example I would calculate the ratio between the total amount of white in the left and right halves (seperated by red line). I could also do the same with the op and bottom half, or seperate the digits diagonally. With this I can calculate the mean and standard deviation.

I imagine that the left / right ratio might give some differences for other numbers. But the ratios might all be closer to the average.
Is there some method for estimating which feature might perform better compared to others? I.e. is there a method which gives a numerical value on how ""seperable"" a data set is?
","['machine-learning', 'data-preprocessing', 'feature-extraction', 'mnist']",
Why is my Keras prediction always close to 100% for one image class?,"
I am using Keras (on top of TF 2.3) to train an image classifier. In some cases I have more than two classes, but often there are just two classes (either ""good"" or ""bad""). I am using the tensorflow.keras.applications.VGG16 class as base model with a custom classifier on top, like this:
input_layer = layers.Input(shape=(self.image_size, self.image_size, 3), name=""model_input"")
base_model = VGG16(weights=""imagenet"", include_top=False, input_tensor=input_layer)
model_head = base_model.output
model_head = layers.AveragePooling2D(pool_size=(4, 4))(model_head)
model_head = layers.Flatten()(model_head)
model_head = layers.Dense(256, activation=""relu"")(model_head)
model_head = layers.Dropout(0.5)(model_head)
model_head = layers.Dense(len(self.image_classes), activation=""softmax"")(model_head)

As you can see in the last (output) layer I am using a softmax activation function. Then I compile the whole model with the categorical_crossentropy loss function and train with one-hot-encoded image data (labels).
All in all the model performs quite well, I am happy with the results, I achieve over 99% test and validation accuracy with our data set. There is one thing I don't understand though:
When I call predict() on the Keras model and look at the prediction results, then these are always either 0 or 1 (or at least very, very close to that, like 0.000001 and 0.999999). So my classifier seems to be quite sure whether an image belongs to either class ""good"" or ""bad"" (for example, if I am using only two classes). I was under the assumption, however, that usually these predictions are not that clear, more in terms of ""the model thinks with a probability of 80% that this image belongs to class A"" - but as said in my case it's always 100% sure.
Any ideas why this might be the case?
","['deep-learning', 'computer-vision', 'keras', 'bayesian-deep-learning', 'uncertainty-quantification']","Traditional neural networks can be over-confident (i.e. give a probability close to $0$ or $1$) even when they are wrong, so you should not interpret the probability that it produces as a measure of uncertainty (i.e. as a measure of how much it is confident that the associated predicted class is the correct one), as that it is essentially wrong. See this and this answers for more details about this.Given that this overconfidence is not desirable in many scenarios (such as healthcare, where doctors also want to know how confident the model is about its predictions, in order to decide whether to give a certain medication to the patient or not), the ML community has been trying to incorporate uncertainty quantification/estimation in neural networks. If you are interested in this topic, you could read the paper Weight Uncertainty in Neural Network (2015) by Blundell et al., which proposes a specific type of Bayesian neural network, i.e. a neural network that models the uncertainty over the actual values of the weights, from which we may also quantify/estimate the uncertainty about the inputs. This paper should not be too difficult to read if you are already familiar with the details of variational-autoencoders.So, the answer to your question is: yes, it's possible that the output probability is close to $1$ because neural networks can be over-confident. (I am assuming that the values returned by tf.keras's predict method are probabilities: I don't remember anymore, so I assumed that you did not make any mistake).A similar question was already asked in the past here. The accepted answer should provide more details about different types of uncertainty and solutions."
What are the conceptual differences between regularisation and optimisation in deep neural nets?,"
I'm trying to wrap my mind around the concepts of regularisation and optimisation in neural nets, especially around their differences.
In my current understanding, regularisation is intended to tackle overfitting whereas optimisation is about convergence.
However, even though regularisation adds terms to the loss function, both approaches seem to do most of their things during the update phase, i.e. they work directly on how weights are updated.
If both concepts are focused on updating weights,

what are the conceptual differences, or why aren't both L2 and Adam, for example, called either optimisers or regularisers?

Can/should I use them together?


","['deep-learning', 'comparison', 'deep-neural-networks', 'optimization', 'regularization']","You are correct.The main conceptual difference is that optimization is about finding the set of parameters/weights that maximizes/minimizes some objective function (which can also include a regularization term), while regularization is about limiting the values that your parameters can take during the optimization/learning/training, so optimization with regularisation (especially, with $L_1$ and $L_2$ regularization) can be thought of as constrained optimization, but, in some cases, such as dropout, it can also be thought of as a way of introducing noise in the training process.You should use regularisation when your neural network is big (where big, of course, is not well-defined) and you have little data (where little is also not well-defined). Why do you want to use regularisation in this case? Because big neural networks have more capacity, so they can memorize the training data more easily (i.e. they can over-fit the training data). If the training data is not representative of the whole data distribution (that you are trying to capture with your neural network), then your neural network may fail on other data from that data distribution, i.e. it may not generalize. Regularization techniques, such as $L_1$ and $L_2$ penalties and dropout, limit the complexity of the functions that can be represented by your neural network (remember that, for a specific set of weights $\theta$, a neural network represents a specific function $f_\theta$), so they prevent your neural network from learning a complicated function that would just over-fit the training data.Of course, you can also use regularisation when you have a very large dataset or your neural network is not that big. However, in principle, the case mentioned above is the case where you will likely need some kind of regularisation. So, as a rule of thumb, the bigger your NN is and the smaller your training dataset is, the more likely you will need some kind of regularisation.The typical way to visualize that your neural network is over-fitting is to look at the evolution of the loss function (a type of objective function, which you want to minimize) for the training dataset and the validation dataset (a dataset that you do not use for training, i.e. updating the weights of the neural network). If the training loss is very small, while the validation loss is bigger (stays constant or even increases), as you train more the neural network, that's a good sign of over-fitting, and it suggests that you may need some kind of regularisation. However, note that, even with regularisation, it is not guaranteed that you will find a good set of weights that is able to generalize to all data (not seen during training), but it's more likely.There are other forms of regularization, such as the KL divergence in the context of Bayesian neural networks or variational auto-encoders, but these are more advanced topics that you don't need to know now. In any case, the KL divergence has the same role as the other regularisation techniques that I mentioned above, i.e., in some way, it restricts/limits the possible functions that you can learn."
Mining repeated subsequences in a given sequence,"
Given an alphabet $I=\left\{i_1,i_2,\dots,i_n\right\}$ and a sequence $S=[e_1,e_2,\dots,e_m]$, where items $e_j \in I$, I am interested in finding every single pattern (subsequence of $S$) that appears in $S$ more than $N$ times, and that has a length $L$ between two limits: $m<L<M$. In case of overlapping between patterns, only the longest pattern should be considered.
For example, given $N=3$, $m=3$ and $M=6$, an alphabet $I=\left\{A,B,C,D,E,F\right\}$, and the following sequence $S$ (the asterisks simply mark the positions of the patterns in this example):
$$
S=[A,A,*F,F,A,A*,B,D,E,F,E,D,*F,F,A,A*,F,C,*C,A,B,D,D*,C,C,*C,A,B,D,D*,C,A,C,B,E,A,B,C,*F,F,A,A*,E,A,B,C,A,D,E,*F,F,A,A*,B,C,D,A,E,A,B,*C,A,B,D,D*,]
$$
The sought algorithm should be able to return the following patterns:
$$
[C,A,B,D,D], [F,F,A,A]
$$
together with their respective positions within $S$.
An available Python implementation would be very desirable (and a parallel implementation, even more so).
I was reading about BIDE algorithms, but I think this is not the correct approach here. Any ideas? Thanks in advance!
","['python', 'pattern-recognition', 'sequence-modeling', 'data-mining']","You can do this similar to the BIDE approach. It can be done like this:The result is:Your second pattern occurs exactly 3 times and so doesn't fullfill the requirement of > 3 occurances ([C,A,B,D,D]).To make it processable in parallel you can do a slide modification. Just create another method in TreeNode, that allows to merge nodes. Like this:Now you can call find_patterns on subtrees, but you need to take into account, that you may not split the input sequence normally. You need to define some overlapping sequence part, so that patterns, that begin at the end of one sequence-fragment can be completed but that this overlapping sequences need to be counted differently (otherwise you get double-counts, which lead to wrong inputs). So you have to make sure, that only patterns, which began in the sequence fragment are continued with the overlap part, but that no new patterns are started in the overlap part, because you count them elsewere:Now you can do:This yields the same result as above. To make it a bit clearer, what I mean with the overlap part:With find_patterns(S1, 3, 3, 6) you would only search for repeated patterns in S1, so it would not consider patterns which begin with the part BDD (starting at the end of S1) and are continued within S2. With find_patterns(S1, 3, 3, 6, overlap=S1_overlap) I consider these patterns as well."
Object Detection as a means of Anomaly Detection,"
Is it possible to train an Object Detector (e.g. SSD), to detect when something is not in the image. Imagine an assembly line that transports some objects. Each object needs to have 5 screws. If the Object Detector detects 4 screws, we know that one is missing, hence there is an anomaly.
Actually this is an Anomaly Detection task where there is something else than a screw (e.g. a hole), but unsupervised anomaly detectors are hard to train and not as stable as object detectors.
Is my assumption correct, that even though it is not really an object detection task, one can use such methods?
","['deep-learning', 'computer-vision', 'object-detection', 'anomaly-detection']",
Face detection and replacement in photos,"
I have 2 photos, and my goal is to detect the face in one and place it on the face of the person in the other photo- basically face detection and replacement. It's not deep fakes. It's more of a computer vision-based approach for smoothing the edges and stitching.
How can we achieve that? What are the approaches and techniques to do that? Some tutorials or code or github repos would be very helpful.

","['computer-vision', 'image-recognition', 'object-detection', 'image-processing', 'face-recognition']",
Why does off-policy learning outperform on-policy learning?,"
I am self-studying about Reinforcement Learning using different online resources. I now have a basic understanding of how RL works.
I saw this in a book:

Q-learning is an off-policy learner. An off-policy learner learns the value of an optimal policy independently of the agent’s actions, as long as it explores enough.


An on-policy learner learns the value of the policy being carried out by the agent, including the exploration steps.

However, I am not quite understanding the difference. Secondly, I came across that off-policy learner works better than the on-policy agent. I don't understand why that would be i.e. why off-policy would be better than the on-policy.
","['reinforcement-learning', 'comparison', 'q-learning', 'off-policy-methods', 'on-policy-methods']",
What should the value of epsilon be in the Q-learning?,"
I am trying to understand Reinforcement Learning and already explored different Youtube videos, blog posts, and Wikipedia articles.
What I don't understand is the impact of $\epsilon$. What value should it take? $0.5$, $0.6$, or $0.7$?
What does it mean when $\epsilon = 0$ and $\epsilon = 1$? If $\epsilon = 1$, does it mean that the agent explores randomly? If this intuition is right, then it will not learn anything - right? On the other hand, if I set $\epsilon = 0$, does this imply that the agent doesn't explore?
For a typical problem, what is the recommended value for this parameter?
","['reinforcement-learning', 'q-learning', 'hyperparameter-optimization', 'hyper-parameters', 'epsilon-greedy-policy']",
Can One-Hot Vectors be used as Inputs for Recurrent Neural Networks?,"
When using an RNN to encode a sentence, one normally takes each word, passes it through an embedding layer, and then uses the dense embedding as the input into the RNN.
Lets say instead of using dense embeddings, I used a one-hot representation for each word, and fed that sequence into the RNN. My question is which of these two outcomes is correct:

Due to the way in which an RNN combines inputs, since these vectors are all orthogonal, absolutely nothing can be combined, and the entire setup does not make sense.

The setup does make sense and it will still work, but not be as effective as using a dense embedding.


I know I could run an experiment and see what happens, but this is fundamentally a theoretical question, and I would appreciate if someone could clarify so that I have a better understanding of how RNNs combine inputs. I suspect that the answer to this question would be the same regardless of whether we are discussing a vanilla RNN or an LSTM or  GRU, but if that is not the case, please explain why.
Thank you.
","['natural-language-processing', 'recurrent-neural-networks', 'long-short-term-memory', 'word-embedding', 'gated-recurrent-unit']",
Neural network architecture with inputs and outputs being an unkown function each,"
I am trying to set up a neural network architecture that is able to learn the points of one function (blue curves) from the points of an other one (red curves). I think that it could be somehow similar to the problem of learning a functional like it was described in this question here. I don't know at all what this (let's call it) functional looks like, I just see the 'blue' response of it to the 'red' input.

The inputs of the network would be the (e.g. 100) points of a red curve and the outputs would probably be the (e.g. 50) points of the blue curve. This is where my problem begins. It tried to implement a simple dense network with two hidden layers and around 200-300 neurons each. Obviously it didn't learn much.
I have the feeling that I somehow need to tell the network that the points next to each other (e.g. input points $x_0$ and $x_1$) are correlated and the function they belong to is differentiable. For the inputs this could be archieved by using convolutional layers, I suppose. But I don't really now how to specify, that the output nodes are correlated with each other as well.
At the beginning I had high hopes in the approach using Generalized Regression Neural Networks as presented here, where a lowpass filter is implemented with NNs. However, as I understood, only the filter coefficients are predicted. As I don't know anything about the general structure of my functional, this will not help me here ...
Do you have any other suggestions for NN architectures that could be helpful for this problem? Any hint is appreciated, thank you!
","['neural-networks', 'regression', 'function-approximation']",
How to use and update a shared/global policy between Reinforcement Learning Agents,"
I would be grateful for some guidance on a RL problem I am trying to solve where multiple RL agents use a common/global policy at the initial state of an episode in the RL Environment, and then update this common/shared policy once the episode is completed.
Below is an example of the problem scenario:

An alert triggers a RL agent to execute a ""episode"" in the Environment
Multiple alerts (e.g., episodes) can occur at the same time, or, one alert may still be being processed (e.g., the episode has not finished) before another alert is triggered (e.g., another episode begins).

Below are the conditions of the Environment and desired behaviour of the RL Agent:

Multiple episodes can run at once (e.g., another episode starts before another finishes).
For each episode a ""instance"" of the RL agent uses the latest version of a common policy.
After each episode the RL agent updates the common policy.
Common policy updates are ""queued"" using versioning in code to prevent race conditions.

Q: How can multiple RL agents in this case use a common policy at the beginning of an episode and then update a common policy after completing it? - All I have found are discussions related to Q-Learning, where agents can update a shared Q-table, or later update a ""global"" Q-table without any examples of how this can be achieved and whether there are also methods for other approaches such as TD rather than only Q-Learning, for example
Q: Does this sound like a traditional multi-agent scenario, at least conceptually? If so, how might one go about implementing this, any examples would be really helpful.
Any help on this is greatly appreciated!
EDIT:
Since doing more investigation I have found this reference on Mathworks:
Link, which is similar to the above problem, but not exact.
","['reinforcement-learning', 'environment', 'policies']","This sounds like distributed RL, and most of the work goes in building the distributed system; the actual RL part is just a DQN (+some tricks from Rainbow DQN).
NB: multi-agent RL arises when the agents interact with each other in the same environment (like Hanabi the card game), while in this case we have multiple agents that collect experiences in parallel.
Here's a possible design from the Ape-X paper:
"
What is the impact of changing the crossover and mutation rates?,"
What is the impact of using a:

low crossover rate

high crossover rate

low mutation rate

high mutation rate


","['genetic-algorithms', 'hyperparameter-optimization', 'crossover-operators', 'mutation-operators', 'genetic-operators']",
"Is this referring to the true underlying distribution, or the distribution of our sample?","
I am currently studying the paper Learning and Evaluating Classifiers under Sample Selection Bias by Bianca Zadrozny. In the introduction, the author says the following:

One of the most common assumptions in the design of learning algorithms is that the training data consist of examples drawn independently from the same underlying distribution as the examples about which the model is expected to make predictions. In many real-world applications, however, this assumption is violated because we do not have complete control over the data gathering process.
For example, suppose we are using a learning method to induce a model that predicts the side-effects of a treatment for a given patient. Because the treatment is not given randomly to individuals in the general population, the available examples are not a random sample from the population. Similarly, suppose we are learning a model to predict the presence/absence of an animal species given the characteristics of a geographical location. Since data gathering is easier in certain regions than others, we would expect to have more data about certain regions than others.
In both cases, even though the available examples are not a random sample from the true underlying distribution of examples, we would like to learn a predictor from the examples that is as accurate as possible for this distribution. Furthermore, we would like to be able to estimate its accuracy for the whole population using the available data.

It's this part that I am confused about:

In both cases, even though the available examples are not a random sample from the true underlying distribution of examples, we would like to learn a predictor from the examples that is as accurate as possible for this distribution.

What exactly is ""this distribution""? Is it referring to the true underlying distribution, or the distribution of our sample (which, as was said, is not necessarily a ""good"" reflection of the underlying distribution, since it is not a random sample)?
","['machine-learning', 'classification', 'probability-distribution', 'selection-bias']","In both cases, even though the available examples are not a random sample from the true underlying distribution of examples, we would like to learn a predictor from the examples that is as accurate as possible for this distribution.""The true underlying distribution"" is the closest ""distribution"" that is explicitly mentioned as such in the part of the text preceding the phrase ""this distribution"", so that's what it's referring to. For clarity, I've put the two things that are ""the same"" in bold in the above quote."
Minimax algorithm with only partial visibility,"
I'm trying to implement the minimax algorithm with alpha beta pruning on a game that works like this:

Player 1 plays (x1, y1).

Player 2 can only see the x-value (x1) that Player 1 played (and not the y-value y1). Player 2 plays (x2, y2).

An action event happens, which may change the heuristic of the current game state.

Player 2 plays (x3, y3).

Player 1 can only see the x-value (x3) that Player 2 played (and not the y-value y3). Player 1 plays (x4, y4).

Action event. The game continues with alternating starting players for a maximum depth of 10.


To do so, I have been treating each turn as you regularly would with the minimax algorithm, with each player making moves given the set of moves already played, including the possibly hidden move from the turn before. However, I've noticed that my algorithm will return moves for Player 2 that assume that Player 1 plays a certain way when, in a ""real"" game, it may be the case that Player 1 plays something else (and vice versa). For example, if Player 2 could guarantee a win on a given turn under all circumstances (when Player 1 plays first for that series), it might not play optimally when it assumes Player 1 will not play its maximum-strength move.
I believe it is doing this precisely because it assumes that all moves are visible (a fully visible game state). And indeed, if that were the case, the sequence of moves it returns would be optimal.
How can I remedy this?
I do not believe that a probability-based algorithm (e.g. Expectiminimax) is necessary, since the game is entirely deterministic. The partial visibility part is making things difficult, though.
Something tells me that changing the turn order in my algorithm might be a solution to this problem, since the action event is the only time the game heuristic is changed.
","['algorithm', 'minimax', 'game-theory', 'alpha-beta-pruning', 'optimality']",
"If the reward function of an environment depends on some initial conditions, should I create a separate environment for each condition?","
I would like some guidance on how to design an Environment for a Reinforcement Learning agent where the stopping conditions and rewards for the environment change based on an initial set of input parameters.
For example, let's say that a system generated alert triggers the instantiation of the RL environment, whereby the RL agent is launched to make decisions in the environment, based on the alert. The alert has two priorities ""HIGH"" and ""LOW"", when the priority is ""HIGH"" the stopping condition is a reward of ""100"" and when the priority is ""LOW"", the stopping condition is a reward of ""1000"".
In this scenario, is it preferable to create two separate environments based on the priority (input parameter) of the alert? Or is this a common requirement that should be designed into the environment/agent? If so, how? Note that I have simplified the scenario, so there could be multiple conditions (e.g., alert, system type, etc), but I am just trying to find a basic solution for the general case.
","['reinforcement-learning', 'environment', 'reward-design', 'reward-functions']","In this scenario, is it preferable to create two separate environments based on the priority (input parameter) of the alert?It is difficult to make a hard rule here.If the resulting environments can be cleanly sorted into a few different categories, and the ideal behaviour and/or the states visited are radically different in each scenario, then maybe a few different agents optimised for each scenario could work well.A more general approach however, is to include the episode start data as part of the state that the agent observes on each time step. A single agent can then in theory learn the different behaviours required depending on the initial values, plus still generalise from anything shared between the multiple scenarios.The alert has two priorities ""HIGH"" and ""LOW"", when the priority is ""HIGH"" the stopping condition is a reward of ""100"" and when the priority is ""LOW"", the stopping condition is a reward of ""1000"".This may work against you. RL agents do not respond to the absolute values of rewards, other than how they compare to other rewards also available within the same episode (or continuing environment).If there is only ever one issue to solve at a time, and no conflict between solving either of the ""HIGH"" or ""LOW"" priority problems (such as splittig resources or effort between them), the different reward system seems redundant. Solved is solved. You might rate the usefulness of an agent that solves the ""LOW"" priority issue well higher, but it seems to me that this describes what you should work on first, not the goals of the agent. To influence the goals of the agent, both rewards would need to be available within the same episode or continuing environment, requiring the agent to make a choice between them."
What is the difference between the positional encoding techniques of the Transformer and GPT?,"
I know the original Transformer and the GPT (1-3) use two slightly different positional encoding techniques.
More specifically, in GPT they say positional encoding is learned. What does that mean? OpenAI's papers don't go into detail very much.
How do they really differ, mathematically speaking?
","['comparison', 'transformer', 'gpt', 'positional-encoding']","The purpose of introduction of positional encoding  is to insert a notion of location of a given token in the sequence. Without it, due to the permutation equivariance (symmetry under the token permutation) there will be no notion of relative order inside a sequence.Given a token at $\text{pos}$-th position we would like to make the model understand, that this token is at particular position. See pretty nice blog here - https://kazemnejad.com/blog/transformer_architecture_positional_encoding/.Fixed encodingIn the original Transformer one uses a fixed map from the token position $i$ to the embedding vector added to the original embedding:
$$
\begin{aligned}
PE(\text{pos}, 2i) &= \sin(\text{pos} / 10000^{2i / d_{\text{model}}}) \\
PE(\text{pos}, 2i + 1) &= \cos(\text{pos} / 10000^{2i / d_{\text{model}}})
\end{aligned}
$$Here $\text{pos}$ is an index of the token in sequence, and $2i, 2i+1$ correspond to the dimension inside the embedding.Learned encodingAnother strategy is to make map for $\text{pos}$ to the embedding vector of dimension $d_{\text{model}}$ learnable. One initializes somehow for each position in the sequence vector of positional embedding for each position from $0$ to $\text{max_length}$ and during the training these vectors are updated by gradient descent."
Multi-armed bandit problem without getting rewards,"
In a 2-armed-bandit problem, an agent has an opportunity to see n reward for each action. Now the agent should choose actions m times and maximize the expected reward in these m decisions. but it cant see the reward of them. what is the best approach for this?
","['reinforcement-learning', 'multi-armed-bandits']",
Semantic segmentation failing in small instance detection,"
I performed semantic segmentation with U-net. My dataset consists of grayscale images of defects. After training the dataset for I got an metric accuracy of only 0.3 - 0.4 IOU. Eventhough it is merely low it performs well enough to identify instances that are huge means the prediction performs well enough in places where there is a standard intensity change(color change) and they are bigger instances. There are many other instances where there is no color change and it occoupies only few pixels in image(smaller instances) and the prediction rate is almost 0 on these instances.
I also tried Resdiual connection in the downsampling part of U-net likewise in ResNet. But still its the same and for smaller instances I used dilated convolution blocks in between the skip connections for encoder and decoder of U-net based on some papers. But still I cannot have a higher accuracy in my network and prediction rate for smaller instances are really poor. Although I use only 350 images for training with Data Augmentations. My image size is also 256,256.
Is there any other method I can try to increase the accuracy and prediction rate for smaller instances?
Any suggestion would be helpful.
","['deep-learning', 'convolutional-neural-networks', 'image-segmentation', 'u-net', 'binary-classification']",
What is the difference between Stochastic Hill Climbing and Simulated Annealing?,"
I am reading about local search: hill climbing, and its types, and simulated annealing
One of the hill climbing versions is ""stochastic hill climbing"", which has the following definition:

Stochastic hill climbing does not examine for all its neighbor before moving. Rather, this search algorithm selects one neighbor node at random and decides whether to choose it as a current state or examine another state

Some sources mentioned that it can be used to avoid local optima.
Then I was reading about simulated annealing and its definition:

At every iteration, a random move is chosen. If it improves the situation then the move is accepted, otherwise it is accepted with some probability less than 1

So, what is the main difference between the two approaches? Does the stochastic choose only random (uphill) successor? If it chooses only (uphill-successors), then how does it avoid local optima?
","['comparison', 'simulated-annealing', 'meta-heuristics', 'local-search', 'stochastic-hill-climbing']","Russell and Norvig's book (3rd edition) describe these two algorithms (section 4.1.1., p. 122) and this book is the reference that you should generally use when studying search algorithms in artificial intelligence. I am familiar with simulated annealing (SA), given that I implemented it in the past to solve a combinatorial problem, but I am not very familiar with stochastic hill climbing (SHC), so let me quote the parts of the book that describe SHC.Stochastic hill climbing chooses at random from among the uphill moves; the probability of selection can vary with the steepness of the uphill move. This usually converges more slowly than steepest ascent, but in some state landscapes, it finds better solutions.So, SHC chooses at random one ""uphill move"", i.e. a move that improves the objective function (for example, if you're trying to solve the travelling salesman problem, a ""uphill move"" could be any change to the current Hamiltonian cycle, a solution, so that the new Halmitonian cycle has a shorter cost) among the uphill moves (so among some set of moves that improve the objective).In simulated annealing, you perform some move. If that move leads to a better solution, you always keep the better solution. If it leads to a worse solution, you accept that worse solution with a certain probability. There are other details, such as how you accept the worse solution (which you can find in Russell and Norvig's book), but this should already clarify that SA is different from SHC: SA can accept worse solutions in order to escape from local minima, while SHC accepts only uphill moves."
Hierarchical reinforcement learning for combinatorial complexity,"
I want to try a hierarchical reinforcement learning (HRL) approach to hard logical problems with combinatorial complexity, i.e. games like chess or Rubik's cube. The majority of HRL papers I have found so far focus either on training a control policy or they tackle quite simple games.
By HRL I mean all methods that (among others):

split hard and complex problem into a series of simpler ones
create desired intermediate goals (or spaces of such goals)
somehow think in terms of 'what to achieve' rather than 'how to achieve'

Do you know any examples of solving logically hard problems with HRL or maybe just any promising approaches to such problems?
","['reinforcement-learning', 'combinatorial-games', 'hierarchical-rl']",
How are afterstate value functions mathematically defined?,"
In this answer, afterstate value functions are mentioned, and that temporal-difference (TD) and Monte Carlo (MC) methods can also use these value functions. Mathematically, how are these value functions defined? Yes, they are a function of the next state, but what's the Bellman equation here? Is it simply defined as $v(s') = \mathbb{E}\left[ R_t \mid S_t = s, A_t = a, S_{t+1} = s' \right]$? If yes, how can we define it in terms of the state, $v(s)$, and state-action, $q(s, a)$, value functions, or as a Bellman (recursive) equation?
Sutton & Barto's book (2nd edition) informally describe afterstate value functions in section 6.8, but they don't provide a formal definition (i.e. Bellman equation in terms of reward or other value functions), so that's why I am asking this question.
","['reinforcement-learning', 'value-functions', 'bellman-equations']","Based on this and this resources, let me give an answer to my own question, but, essentially, I will just rewrite the contents of the first resource here, for reproducibility, with some minor changes to the notation (to be consistent with Sutton & Barto's book, 2nd edition). Note that I am not fully sure if this formulation is universal (i.e. maybe there are other ways of formulating it), but the contents of the first resource seem to be consistent with the contents in the second resource.Let's assume that we have an infinite-horizon MDP$$\mathcal{M} = (\mathcal{S}, \mathcal{Y}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma),$$
whereLetThe transition function $\mathcal{T}$ for $\mathcal{M}$ is defined as\begin{align}
\mathcal{T}(s, a, s^{\prime}) 
&\doteq P ( s^{\prime} \mid f(s, a)) \\
&= P ( s^{\prime} \mid y)
\end{align}A transition is composed of 2 stepsSo, I have denoted afterstates with a different letter, $y$, because afterstates are reached with a deterministic function $f$, while other states, $s$ or $s'$, are reached with $P$.After having taken the action $a$ in the state $s$, we get a reward (i.e. we get a reward in step 1), but we do not get a reward after the stochastic step (given that no action is taken).So, we can define the reward function $\mathcal{R}$ for this MDP as follows$$
\mathcal{R} (s, a, s^{\prime} ) \doteq \mathcal{R}(s, a)
$$The situation is illustrated by the following diagramSo, here, $P$ is the stochastic transition function (i.e. a probability distribution) as used above. Note that, here, $r_t$ is a specific realization of $R_t$ (the random variable) in the formulas below.Let's recall the definition of the state value function $v_\pi(s)$ for a given policy $\pi$ (as defined in Sutton & Barto, section 3.5)\begin{align}
v_{\pi}(s) 
&\doteq 
\mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right] \\
&=
\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right],
\end{align}
for all $s \in \mathcal{S}$ and\begin{align}
G_{t} 
&\doteq 
\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \\
&=
R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3}+ \cdots \\
&=
\mathcal{R}(s_t, a_t) + \gamma \mathcal{R}(s_{t+1}, a_{t+1})+\gamma^{2} \mathcal{R}(s_{t+2}, a_{t+2}) +\cdots,
\end{align}
where $\pi(s_t) = a_t$ and $\mathcal{R}(s_t, a_t) = R_{t+1}$, for $t=0, 1, 2, \dots$. (So, note that $\mathcal{R} \neq R_t$: the first is the reward function, while the second is a random variable that represents the reward received after having taken action $a_t$ in step $s_t$)The optimal state value function is defined as$$
v_{*}(s) \doteq \max _{\pi} v_{\pi}(s)
$$Similarly, we will define the afterstate value function, but we will use the letter $w$ just to differentiate it from $v$ and $q$.\begin{align}
w_{\pi}\left(y\right)
&\doteq 
\mathbb{E}_{\pi}\left[G_{t+1} \mid Y_{t}=y\right] 
\\
&=
\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} \mid Y_{t}=y\right] 
\\
&=
\mathbb{E}_{\pi}\left[
R_{t+2} + \gamma R_{t+3}+\gamma^{2} R_{t+4} + \cdots
 \mid Y_{t}=y\right] 
\\
&=
\mathbb{E}_{\pi}\left[
\mathcal{R}(s_{t+1}, a_{t+1})+\gamma \mathcal{R}(s_{t+2}, a_{t+2}) + \gamma^{2} \mathcal{R}(s_{t+3}, a_{t+3}) + \cdots
 \mid Y_{t}=y\right] ,
\end{align}
where $\mathcal{R}(s_{t+1}, a_{t+1}) = R_{t+2}$, for all $t$.In other words, the value of an afterstate $y$ (at time step $t$, i.e. given $Y_t = y$) is defined as the expectation of the return starting from the state that you ended up in after the afterstate $y$.This seems reasonable to me and is similar to my proposal for the definition of the afterstate value function in the question, although I was not considering any deterministic functions in a potential formulation, and I was also not thinking of afterstates as intermediate states, reached by a deterministic step, between the usual states.Similarly to the optimal state value function, we also define the optimal afterstate value function$$
w_{*}(y) \doteq \max _{\pi} w_{\pi}(y)
$$We can define the afterstate value function in terms$$
w_{*}(y) 
= 
\sum_{s^{\prime}} P (s^{\prime} \mid y ) v_{*} ( s^{\prime} )
$$
In other words, $w_{*}(y)$ is defined as an expectation over the value of next possible states $s'$ from the afterstate $y$.This seems to be correct and consistent with the above definitions.In this and this resources, the state value function is also defined in terms of afterstate value function as follows$$v_{*}(s)=\max_{a}\left(\mathcal{R}(s, a)+\gamma w_{*}(f(s, a))\right)$$The Bellman equation for afterstate value function (from which an update rule can be derived) is given by$$
w_{*}(y)
=
\sum_{s^{\prime}} 
P(s^{\prime} \mid y ) 
\max_{a} ( 
\mathcal{R} (s^{\prime}, a)
+
\gamma 
w_{*}(f ( s^{\prime}, a ))),
$$
which is really similar to the Bellman equation for the state value function.Finally, we can also express the state-action value function in terms of the afterstate value function$$
q_\pi(s_t, a_t)
=
\mathcal{R}\left(s_{t}, a_{t}\right)+\gamma w_{\pi}\left(f\left(s_{t}, a_{t}\right)\right)
$$Given that this answer is already quite long, see the resource for more details (including an algorithm based on the afterstate Bellman equation).If you are the type of person that understands the concepts by looking at the code, then this Github project, which implements a Monte Carlo method that uses afterstates to play tic-tac-toe, may be useful. Afterstates are useful in tic-tac-toe because it is a 2-player game, where two agents take actions in turn, so we can estimate the action that you should take deterministically (as if it was the $f$ above) before the other agent takes an action (probabilistically), at least, this is my current interpretation of the usefulness of afterstates in this game (and similar games/problems)."
How do self-driving cars perform lane changes?,"
I am a bit stuck trying to understand how a lane change is performed from an operational point of view.
Let's assume a self-driving car uses an occupancy grid map for local planning, this map may even have the detected lane boundaries. It's following a slow car and decides to overtake, but how does it know where the centre of the adjacent lane is? Does it use a separate map, or is there a separate data structure that is used to keep the lane information which informs the car where the centre of the adjacent lane is?
Alternatively, does the car just decide to start drifting off to the side until it picks up the lane boundaries and then centres itself?
","['autonomous-vehicles', 'control-theory']",
"Does this diagram represent several LSTMs, or one through several timesteps?","
I'm trying to read this paper describing Google's LSTM architecture for machine translation. It features this diagram on page 4:

I'm interested in the encoder block, on the left. Apparently, the pink and green cells are LSTMs. However, I can't tell if the x-axis is space or time. That is, are the LSTM cells on a given row all the same cell, with time flowing forward from left to right? The diagram on the next page in the paper seems to suggest that.
","['neural-networks', 'deep-learning', 'long-short-term-memory', 'papers', 'google']","are the LSTM cells on a given row all the same cell, with time flowing forward from left to right?Yes this is correctThe x-axis on this figure is basically the time axis. Essentially all pink boxes in the same row are the same LSTM cell, with different inputs from the same sequence. At each timestep, the cell takes an input and produces an output which is fed to the next layer. At the 8-th layer, the outputs over all timesteps are inputted at the same time to the attention layer."
Has the logistic map ever been used as an activation function?,"
I find the logistic map absolutely fascinating. Both in itself (because I love fractal) and because it is observed in nature (see: https://www.youtube.com/watch?v=ovJcsL7vyrk).
I'm wondering if anyone tried it as an activation function in some way or another with any kind of success.
I like it because it has some kind of ""I'm not sure what to do"" above ~3.0 and the less confidence the more chaotic the response is. It gives the possibility to explore some other solution to escape a local optimum (not sure I use this word correctly).  And below 3 it's still a nice and smooth activation function like, eg, a tanh.
Eg : the reward i got isn't the reward i expect, and the higher the difference the more i'll explore other solution. But it's still gradual, from 1 choice, to 2 choice, 4, 8, 16, ... until it become chaotic. (giving the possibility to experiment some pseudo-random choice). And below this threshold it still act as a usable ""good old"" activation function.
Another good side is that it's gpu-friendly and don't need many iteration for this application since a little bit of uncertainty (even below the threshold) isn't undesirable. see : https://upload.wikimedia.org/wikipedia/commons/6/63/Logistic_Map_Animation.gif
Edit : so, ok, i tested it on my extremely naive racetrack. (feedforward, no feedback, no error, no fitness, only genetic selection for the car that didn't crash). It does work, for sure. I don't see any advantage in practive but with such a naive NN, there isn't much i can tell.
My implementation :
def logi(r):
    x = .6  # the initial population doesn't matter so i took .6
    for _ in range(random.randrange(10,40)):
        x = r * x * (1 - x)
    return x

The activation take 8% of my laptop cpu (while is was invisible on my radar with leaky leru)

","['neural-networks', 'activation-functions']",
Is the expected value we sample in TD-learning action-value Q or state-value V?,"
Both MC and TD are model-free and they both follow a sample trajectory (in the case of TD, the trajectory is cut-short) to estimate the return (we basically are sampling Q values). Other than that, the underlying structure of both algorithms is exactly the same. However, from the blogs and texts I read, the equations are expressed in terms of V and NOT Q. Why is that?
","['reinforcement-learning', 'monte-carlo-methods', 'temporal-difference-methods', 'value-functions', 'return']","However, from the blogs and texts I read, the equations are expressed in terms of V and NOT Q. Why is that?MC and TD are methods for associating value estimates to time step based on experienced gained in later time steps. It does not matter what kind of value estimate is being associated across time, because all value functions are expressing the same thing in general, which is the expected return conditioned on a ""current position"" within the MDP. In MC the association is directly with a sampled return, in TD with a sampled combination of immediate reward and a later value estimate - most commonly in TD the same kind of value estimate (e.g. matching later state value estimates to state values).Both approaches can be analysed and used from the perspective of both state value (V) and action value (Q) functions. They also apply to other value functions - e.g. afterstate values.It is quite common for textbooks and tutorials to use the slightly simpler state value function to explain how MC or TD learning work in general, outside of being used for any purpose. You can also use the state value function for model-free policy evaluation in MC and TD.However, without a model, you cannot use state value function for control (i.e. to learn an optimal policy). To pick the best action using state values, you need to do something like this:$$\pi(s) = \text{argmax}_a [ \sum_{r,s'} p(r,s'|s,a)(r + \gamma v(s'))]$$The problem here is that $p(r,s'|s,a)$ is a model of the environment. So, if it is needed, the control method would not be model-free.Hence when you learn about MC or TD in a control scenario, model-free methods to learn optimal policies, then you generally need to use an action value (sometimes you can use an afterstate value, if the action involves choosing the next state directly).With an action value function, the greedy policy becomes:$$\pi(s) = \text{argmax}_a q(s, a)$$This does not refer to any model of the environment. So it can be used when you have none."
How to translate sudoku XV boards in CNF format?,"
I'm trying to implement the logic for a Sudoku XV puzzle, that it's essentially a standard sudoku with the addition of X and V markers between some pairs of squares. X markers in adjacent pairs requires that the sum of the two values is 10. Similarly, the V marks requires that the sum of the values is equal to 5.
(Assume that $$ S_{xyz} $$ stands for [digit][row][column])
I've written the following CNF formulae that handle the logic of a standard Sudoku puzzle:
There is at least one number in each entry:
$$ \bigwedge_{x=1}9\bigwedge_{y=1}9\bigwedge_{z=1}9S_{xyz} $$
Each number appears at most once in each row:
$$ \bigwedge_{y=1}9\bigwedge_{z=1}9\bigwedge_{x=1}{8\bigwedge_{i=x+1}9}{(\lnot S}_{xyz\ }\vee\lnot S_{iyz\ }) $$
Each number appears at most once in each column:
$$ \bigwedge_{x=1}9\bigwedge_{z=1}9\bigwedge_{y=1}{8\bigwedge_{i=x+1}9}{(\lnot S}_{xyz\ }\vee\lnot S_{xiz\ }) $$
Each number appears at most once in each 3x3 sub-grid:
$$
\bigwedge_{z=1}9\bigwedge_{i=0}2\bigwedge_{j=0}{2\bigwedge_{x=1}2\bigwedge_{y+1}3\bigwedge_{k=x+1}3\bigwedge_{l=1,\ \ y \neq l}3}{(\lnot S}_{(3i+x)(3j+y)z\ }\vee\lnot S_{(3i+k)(3j+l)z\ })
$$
Unfortunately, I'm stuck, and I don't really know how I can express the logic for X and V markers, and most importantly how to invalidate squares that contain neither an X nor a V marker that have digits summing to 5 or 10.
","['ai-design', 'logic', 'sudoku', 'conjunctive-normal-form']",
React on train-validation curve after trening,"
I have a regression task that I tray to solve with AI.
I have around 6M rows with about 30 columns. (originally there was 100, but I reduce it with drop feature importance)
I understand basic principle: Look if model overfit or underfit - according change the parameters. In theory.
I would ask for help with two graphs:

If I understand correctly what is going on
How would you attack the situation.

1. Graph


I use LightGBM
learning_rate = 1
max_depth = 3
num_leaves = 2**15,
number of iterations = 4000

If I understand this model is Underfitting. The validation and training is falling, but not very much...
BUT: The number of iteration is now too large and place higher number is not ok. Learning rate is 1 (as hight as it gets). Only the max_depth is low, but if it is higher (I try 30) the graph is same, just the values are worse.
So, what to do, so that model would not underfit.

Graph



I use Neural Nets
epochs=200,
batch_size=64

The model
i = Input(shape=(100,), name='input')
x = Dense(128)(i)
x = Dense(64)(i)
o = Dense(1, activation='relu', name='output')(x)

Here I am not sure. This doesn't really looks like underfit, but more that doesn't converged.
Is this right?
So, should I create more complex model (more neurones or more layers?)?
And how much epochs do I need to see this behaviour? Because, in the beginning I use only 10 epochs, for faster development, and I thought that model is overfitting. Only when I use more epochs I see that I was wrong.
How would you start to ""debug"" this neural net? What would be the plan of attack?
","['neural-networks', 'overfitting', 'convergence', 'gradient-boosting', 'validation-loss']",
Generating fake faces containing specific features with GANs,"
I'm trying to understand how DeepFakes are generated and so far I understood that they're mostly generated through the usage of GANs and autoencoders.
The autoencoders part is understandable, but what I cannot understand is how to generate faces with GANs that match destination face.
GANs consist of a generator and a discriminator. The generator is getting noise input which is randomly selected from a normal distribution and feedback from the discriminator. The discriminator is taught how the real data looks like and just classifies if the data fed to him is real or fake. Depending on the answer - one of them (generator/discriminator) updates its model. If the discriminator guesses right, the generator is getting updated if not, then the discriminator is the one that is updating its model.
So after the training part is over, we can feed the generator more noise to achieve more fake data. In DeepFake videos, we normally try to swap the destination face with the input face. My problem with that is that the destination face has specific features for example it has closed eyes, smiles, rotates its head. If we feed the generator noise, how can we control the process to achieve similar facial features that are in the destination face?
I've found papers about GANs that can control some of the features of generated faces (StyleGANs). Although I'm not sure how would it be possible to extract ""special features"" of destination face and generate them with StyleGANs.
I will be extremely grateful for any help in understanding the concept of DeepFake with GANs.
Thanks a lot.
","['machine-learning', 'generative-adversarial-networks', 'generative-model']",
Why does PPO lead to a worse performance than TRPO in the same task?,"
I am training an agent with an Actor-Critic network and update it with TRPO so far. Now, I tried out PPO and the results are drastically different and bad. I only changed from TRPO to PPO, the rest of the environment and rewards are the same. PPO is just a more efficient method compared to TRPO and has proven to be a state-of-the-art method in RL. So, why shouldn't it work? I just thought to ask if someone knows roughly how to transform configuration parameters from TRPO to PPO.
Here some more details about my configurations.
TRPO

Actor loss:  $-\log(\pi) * A$ where $A$ are advantages
Critic Loss: MSE(predicted_values, discounted return)
Desired KL Divergence for Actor and Critic: 0.005
Conjugate gradient iterations: 20
Residual tolerance in conjugate gradient: 1e-10
Damping coefficient for Fisher Product: 1e-3

  
PPO

Actor and Critic optimizer and learning rate: Adam with 0.0001
Actor loss: negative minimum of either:

$\frac{\pi}{\pi_{old}} * A$
$clamp(\frac{\pi}{\pi_{old}}, 1-0.1, 1+0.1) * A$


Critic loss: MSE(predicted_values, discounted_rewards)
Optimization iterations: 10

  

The rest of my problem set-up is absolutely the same. But somehow I get completely different results while training, as you can see on the plots above. I also changed learning rates and optimization iterations, gradient clipping, optimizing with mini-batches and $-log(\pi) * A$ as Loss for PPO, but neither helped. Taking importance sampling $\frac{\pi}{\pi_{old}} * A$  as loss for TRPO gives the same results there.
Can someone please help me to understand where could be the problem? Or which parameters I would need to change in PPO?
","['reinforcement-learning', 'policy-gradients', 'hyperparameter-optimization', 'proximal-policy-optimization', 'trust-region-policy-optimization']",
"Making generated texts from ""data-to-text"" more variable","
I am diving in data-to-text generation for long articles (> 1000 words). After creating a template and fill it with data I am currently going down on paragraph level and adding different paragraphs, which are randomly selected and put together. I also added on a word level different outputs for date, time and number formats.
The challenge I see is, that when creating large amounts of such generated texts they become boring to read as the uniqueness for the reader goes down.
Furthermore, I also think it's easy to detect that such texts have been autogenerated. However, I still have to validate this hypotheses.
I was wondering if there is an even better method to bring in variability in such a text?
Can you suggest any methods, papers, resources or share your experience within this field.
I highly appreciate your replies!
","['natural-language-processing', 'text-generation', 'natural-language-generation']","You could handwrite different templates and choose probabilistically, according to writing style or pragmatic effects like irony and so on, but that very much depends on the domain.
If you have tabular data, from which you want to generate text, you should probably forget about GPT and so on. You only have few control (despite copy mechanism) over the generation process, you actually predict the next most probable word sequence for a given length - GPTs don't author coherent text across paragraphs, especially not when text length is more than a few hundred words.Check the linguistic counterpart (https://arxiv.org/abs/1703.09902) to end2end generation systems. Breaking up the networks, pipelining again and using networks for controllable tasks.
e.g. build a module that selects which attributes to produce. Create RDF triples from the column head words and values in your data base. Take a Text to text Transfer model (Google's T5) and transform into surface text. You should also have a look into the webNLG challenge (https://webnlg-challenge.loria.fr/challenge_2020/). This might help. Much of this is still open research. I am quite busy in this topic, so feel free to ask."
Why are shallow networks so prevalent in RL?,"
In deep learning, using more layers in a neural network adds the capacity to capture more features. In most RL papers, their experiments use a 2 layer neural network. Learning to Reset, Constrained Policy Optimization, Model-based RL with stability guarantees just to name a few - these are papers I personally remember but there are definitely many others.
I came across this question whose answers generally agree that yes, RL using a shallow network is considered deep RL, but the reason for preference of shallow networks was not part of the question.
In MetaMimic (2018), the authors trained the largest neural net RL algorithm at the time (a residual net with 20 convolution layers) for one-shot imitation learning. The paper demonstrates that larger networks as policy approximators generalize better and represent many behaviours.
So, why are shallow 2 layer networks so widely used?
","['neural-networks', 'reinforcement-learning', 'deep-learning', 'deep-rl']",
How does NN follows law of energy conservation?,"
Communication requires energy, and using energy requires communication. According to Shannon, the entropy value of a piece of information provides an absolute limit on the shortest possible average length of a message without losing information as it is transmitted. (https://towardsdatascience.com/entropy-the-pillar-of-both-thermodynamics-and-information-theory-138d6e4872fa)
I don't know whether Neural Network actually deals with information flow or not. This information flow is taken from the idea of entropy. Since I haven't found any paper or ideas based on the law of energy for neural networks. The law of energy states that energy can neither be created nor destroyed. If it is creating information (energy) (e.g. in the case of a generative model), then some information may be lost while updating weights. How is Neural Network ensuring this energy conservation?
","['neural-networks', 'generative-model', 'information-theory', 'minimum-description-length', 'entropy']",
How should we interpret this figure that relates the perceptron criterion and the hinge loss?,"
I am currently studying the textbook Neural Networks and Deep Learning by Charu C. Aggarwal. Chapter 1.2.1.2 Relationship with Support Vector Machines says the following:

The perceptron criterion is a shifted version of the hinge-loss used in support vector machines (see Chapter 2). The hinge loss looks even more similar to the zero-one loss criterion of Equation 1.7, and is defined as follows:
$$L_i^{svm} = \max\{ 1 - y_i(\overline{W} \cdot \overline{X}_i), 0 \} \tag{1.9}$$
Note that the perceptron does not keep the constant term of $1$ on the right-hand side of Equation 1.7, whereas the hinge loss keeps this constant within the maximization function. This change does not affect the algebraic expression for the gradient, but it does change which points are lossless and should not cause an update. The relationship between the
perceptron criterion and the hinge loss is shown in Figure 1.6. This similarity becomes particularly evident when the perceptron updates of Equation 1.6 are rewritten as follows:
$$\overline{W} \Leftarrow \overline{W} + \alpha \sum_{(\overline{X}, y) \in S^+} y \overline{X} \tag{1.10}$$
Here, $S^+$ is defined as the set of all misclassified training points $\overline{X} \in S$ that satisfy the condition $y(\overline{W} \cdot \overline{X}) < 0$. This update seems to look somewhat different from the perceptron, because the perceptron uses the error $E(\overline{X})$ for the update, which is replaced with $y$ in the update above. A key point is that the (integer) error value $E(X) = (y − \text{sign}\{\overline{W} \cdot \overline{X} \}) \in \{ −2, +2 \}$ can never be $0$ for misclassified points in $S^+$. Therefore, we have $E(\overline{X}) = 2y$ for misclassified points, and $E(X)$ can be replaced with $y$ in the updates after absorbing the factor of $2$ within the learning rate.

Equation 1.6 is as follows:

$$\overline{W} \Leftarrow \overline{W} + \alpha \sum_{\overline{X} \in S} E(\overline{X})\overline{X}, \tag{1.6}$$
where $S$ is a randomly chosen subset of training points, $\overline{X} = [x_1, \dots, x_d]$ is a data instance (vector of $d$ feature variables), $\overline{W} = [w_1, \dots, w_d]$ are the weights, $\alpha$ is the learning rate, and $E(\overline{X}) = (y - \hat{y})$ is an error value, where $\hat{y} = \text{sign}\{ \overline{W} \cdot \overline{X} \}$ is the prediction and $y$ is the observed value of the binary class variable.

Equation 1.7 is as follows:

$$L_i^{(0/1)} = \dfrac{1}{2} (y_i - \text{sign}\{ \overline{W} \cdot \overline{X_i} \})^2 = 1 - y_i \cdot \text{sign} \{ \overline{W} \cdot \overline{X_i} \} \tag{1.7}$$

And figure 1.6 is as follows:



Figure 1.6 looks unclear to me. What is figure 1.6 showing, and how is it relevant to the point that the author is trying to make?
","['objective-functions', 'support-vector-machine', 'perceptron', 'binary-classification', 'hinge-loss']",
"Why doesn't the set $\{ -2, +2 \}$ in $E(X) = (y − \text{sign}\{\overline{W} \cdot \overline{X} \}) \in \{ −2, +2 \}$ include $0$?","
I am currently studying the textbook Neural Networks and Deep Learning by Charu C. Aggarwal. Chapter 1.2.1.2 Relationship with Support Vector Machines says the following:

The perceptron criterion is a shifted version of the hinge-loss used in support vector machines (see Chapter 2). The hinge loss looks even more similar to the zero-one loss criterion of Equation 1.7, and is defined as follows:
$$L_i^{svm} = \max\{ 1 - y_i(\overline{W} \cdot \overline{X}_i), 0 \} \tag{1.9}$$
Note that the perceptron does not keep the constant term of $1$ on the right-hand side of Equation 1.7, whereas the hinge loss keeps this constant within the maximization function. This change does not affect the algebraic expression for the gradient, but it does change which points are lossless and should not cause an update. The relationship between the
perceptron criterion and the hinge loss is shown in Figure 1.6. This similarity becomes
particularly evident when the perceptron updates of Equation 1.6 are rewritten as follows:
$$\overline{W} \Leftarrow \overline{W} + \alpha \sum_{(\overline{X}, y) \in S^+} y \overline{X} \tag{1.10}$$
Here, $S^+$ is defined as the set of all misclassified training points $\overline{X} \in S$ that satisfy the condition $y(\overline{W} \cdot \overline{X}) < 0$. This update seems to look somewhat different from the perceptron, because the perceptron uses the error $E(\overline{X})$ for the update, which is replaced with $y$ in the update above. A key point is that the (integer) error value $E(X) = (y − \text{sign}\{\overline{W} \cdot \overline{X} \}) \in \{ −2, +2 \}$ can never be $0$ for misclassified points in $S^+$. Therefore, we have $E(\overline{X}) = 2y$ for misclassified points, and $E(X)$ can be replaced with $y$ in the updates after absorbing the factor of $2$ within the learning rate.

Equation 1.7 is as follows:

$$L_i^{(0/1)} = \dfrac{1}{2} (y_i - \text{sign}\{ \overline{W} \cdot \overline{X_i} \})^2 = 1 - y_i \cdot \text{sign} \{ \overline{W} \cdot \overline{X_i} \} \tag{1.7}$$

And figure 1.6 is as follows:



It is said that we are dealing with the case of binary classification, where $y \in \{ -1, +1 \}$. But the author claims that $E(X) = (y − \text{sign}\{\overline{W} \cdot \overline{X} \}) \in \{ −2, +2 \}$, which doesn't include the case of $0$. So shouldn't the $\{ -2, +2 \}$ in $E(X) = (y − \text{sign}\{\overline{W} \cdot \overline{X} \}) \in \{ −2, +2 \}$ be $\{ -2, 0, +2 \}$?
","['objective-functions', 'support-vector-machine', 'perceptron', 'binary-classification']",
What is the meaning of these equations in Noise2Noise paper?,"
I am trying to understand what is meant by following equations in the Noise2Noise paper by Nvidia.

What is meant by the equation in this image? What is $\mathbb{E}_y\{y\}$? And how should I try to visualize these equations?
","['papers', 'supervised-learning', 'notation', 'expectation', 'mean-squared-error']","The equation you are referring to is called Mean Squared Error (or $L_2$ loss) and it is used for regression tasks, where the goal is to predict a real value given some input.In your case, the inputs are measurements of temperature $y$, either at a certain point in time or point in space or both or none, this is not clear from the image. Now, the goal would be to predict the temperature at a new point in space, time, or both, where we don't have access to a measurement. That is we would like to find a function $f$ (e.g. a simple linear function) which we can use for prediction. But how can we measure which function is ""best""? We introduce a loss function $L(f,y)$, another function which tells us how good our proposed function is.Visually it looks like this (image source):Red crossed are measurements, the black line is our function we use for prediction and the green dotted lines are the errors (the distance from our prediction to the real measurement). In this example salary depends on experience.Now, the paper introduces the constant mean of all measurements as $y$, $z = \mathbb{E}_y\{y\} = \frac{1}{N}\sum_i^N y_i$, as our function $f$, which is known to be the minimizer for the $L_2$ loss in the case where there is no dependence on other variables (e.g. time or space)."
DQN fails to learn useful policy for the Taxi environment (Dietterich 200),"
I'm building an agent to solve the Taxi environment. I've seen this problem solved with Q-Learning algorithms but my DQN consistently fails to learn anything. The environment has a discrete observation space, I one-hot encode the state before feeding it to the DQN. I also went ahead to implement Hindsight Experience Replay to help the learning process but the DQN still doesn't learn anything. What can I do to fix this?
I've heard that DQN doesn't excel at environments that require planning to succeed, if that's the case, which algorithms would work well for this environment?
EDIT
When I posted this question, my DQN was learning from only 2 step transitions, since this environment can go on for several timesteps without any positive reward, I updated the agent to use transitions of 200 steps. Since I'm using Hindsight Experience Replay, my agent is sure to receive rewards within 200 timesteps even if it didn't meet the goal. I tried this and my agent still hasn't improved, it continually performs worse than the random agent baseline. I checked the contents of the buffer, I observed transitions that do lead to several rewards because their goals have been modified during HER and yet the DQN agent doesn't learn anything.
Also, I'm using TensorFlow's tf_agents for my implementation. Here's a link to the code. I repurposed this example.
I hope this helps
","['reinforcement-learning', 'dqn']",
Predict next event based on previous events and discrete reward values,"
Suppose, I have several sequences that include a series of text (the length of sequence can be varied). Also, I have some related reward value. however, the value is not continuous like the text. It has many missing values. Here is an example of the dataset.
Sequence 1        Sequence 2 .............. Sequence n
------------      ----------                -------------
Action  Reward    Action  Reward            Action  Reward
  A                 C                          D
  B       5         A                          B      6
  C                 A       7                  A       
  C       6         B       10                 D           
  A                 C                          A           
  B       2         A                          B           
  ..                ...                        ...
 ...               .....                      .....
  D       5         C      4                   D          

Now I want to predict the next action based on the reward value. The idea is I want to predict the actions that leads to more rewards. Previously, I used only action data to predict the next action using LSTM and GRU. However, how could I use this reward value in this prediction? I was if thinking Reinforcement learning (MDP) could solve the problem. However, as the rewards are discrete, I am not sure if RL could do that. Also, is it possible to solve this problem with Inverse RL? I have some knowledge of deep learning. However, I am new to reinforcement learning. If anyone gives me some suggestion or provide me useful papers link regarding this problem, it could help me a lot.
","['reinforcement-learning', 'deep-learning', 'natural-language-processing', 'prediction']","Your problem does look like it could be a good match to reinforcement learning, or at least the related idea of contextual bandits. Whether or not it would be a good match to the full reinforcement learning algorithm depends on whether any of the data you are processing could be considered part of an environment state, and whether or not that state evolves based on rules that an agent could learn to take advantage of.Previously, I used only action data to predict the next action using LSTM and GRU. However, how could I use this reward value in this prediction?There are a few different ways to do this using reinforcement learning theory. The simplest is to build a regression predictor that approximates a sum of future rewards (also known as the return or utility) depending on a proposed action from the current state. Then you could use the value function approximator (formal name for the predictor you just built) to predict results from each possible action and pick the maximising one. It is possible to learn such a value function from a historical dataset using methods such as Q learning.The subject is too complex to teach from scratch in a single answer here. A good learning resource is Reinforcement Learning: An Introduction by Sutton & Barto, which the authors have made available for free.However, as the rewards are discrete, I am not sure if RL could do that.Yes it can. Reinforcement learning just requires that rewards in each situation follow a consistent distribution of values. Always returning discrete values is not a problem, neither is always returning the same value in the same situation. Randomness in the reward value - such as sometimes returning a discrete value and other times not in the same situation - is also OK. You can treat missing values as zero, since you are concerned only with the sum of received rewards, using zero when no value is available has no effect on what will be considered the optimal solution.Also, is it possible to solve this problem with Inverse RL?Probably not. Inverse RL is concerned with figuring out the parameters that an existing agent is working with by observing it. For instance you could use it to observe a creature's behaviour and figure out which rewards were more valuable to it. In your case you have the reward values, so you don't need to figure them out.Caveat: You need to figure out what constitutes state in your environment. If there is some state that can be used for predictions, but the agent's behaviour never changes the state, then you may want to spend some time modelling your problem as a contextual bandit instead. Bandit algorithms are introduced in the same book, Reinforcement Learning: An Introduction, but only as much is needed to teach about the full RL problem - bandit solvers can get far more sophisticated than the book considers.Note that if the history of agent actions impacts the reward (e.g. it is a matter of timing when to take the right action), then that history is part of the state, and you likely do have a full reinforcement learning problem to solve."
"How should I change the hyper-parameters of the C51 algorithm, in order to obtain higher reward?","
I have a scenario where, in an ideal situation, the greedy approach is the best, but when non-idealities are introduced which can be learned, DQN starts doing better. So, after checking what DQN achieved, I tried C51 using the standard implementation from tf.agents (link). A very nice description is given here. But, as shown in the image, C51 does extremely bad.

As you can see, C51 stays at the same level throughout. When learning, the loss right from the first iteration is around 10e-3 and goes on to 10e-5, which definitely impacts the change in the weights. But I am not sure how this can be solved.
The scenario is

1 episode consists of 10 steps and the episode only ends after the 10th step, the episode never ends earlier.

states at each step are integer values and can take values between 0 and 1. In the image, states are of shape 20*1.

actions have the shape 20*1

learning rate = 10e-3

exploration factor $\epsilon$ starts out at 0.2 and decays up to 0.01


C51 has 3 additional parameters, which help it to learn the distribution of q-values
num_atoms = 51 # u/param {type:""integer""} 
min_q_value = -20 # u/param {type:""integer""} 
max_q_value = 20 # u/param {type:""integer""}

num_atoms is the number of support that the learned distribution will have, and min_q_value and max_q_value are the endpoints of the q-value distribution. I set them as 51 (the first paper and other implementations keep it as 51 and hence the name 51), and the min and max are set as the min and max possible rewards.
So, if anyone could help me with fine-tuning the parameters for C51 to work, I would be very grateful.
","['reinforcement-learning', 'dqn', 'hyperparameter-optimization', 'performance', 'c51']","Here is what I discovered empirically, trial and error. Since tuning the parameters are going to be environment specific, I'll lay out mine to give a better understanding of what I found to work for my case. Hopefully someone with better understanding of the algorithm will weigh in:Environment: A 2D map where an agent controls a simulated PC mouse pad and must navigate from a random spawn point to a random reward point.Action Space: Discrete(24) -- This was flattened from the original implementation to allow for use in the Q* algorithms.Distance normalization: Trial and error led me to normalizing the space. This also brings the benefit of allowing the same trained algorithm to be used for different world sizes, eg a 2x3 vs a 3x2 vs a 3x3 on up to large common monitor dimensions. Specifically, distances are normalized between -1 and 1 for the height and the width of the space. So no matter the dimensions all points map to x and y between -1 and 1. In the case the height and the width differ the smaller is scaled to the size of the larger.Reward space: After having agents reward exploit iteration after iteration of reward scheme, Rewards are given when an agent moves closer to the target - with closer taking into consideration both ""birds eye"" and Euclidian distance. After first, took ""theoretical"" maximum sum of the total rewards summed up to 1 and were assigned by taking the maximum distance an agent might move in a given 2D space to gain a reward. This works out to be the diagonal of the 2D space. I say ""theoretical"" because unless the agent randomly spawns exactly in one corner and the reward is placed exactly in the other corner, the actual total reward that can be achieved is lower. So in practice the maximum reward for a given episode is the sum of normalized distances between the agent and the reward.To avoid reward exploitation, only .70% of the reward if the agent moves closer vertically or horizontally when a diagonal move was available. This is to get the agent to take the shortest path moving diagonally with only the sqrt(2) bird's eye reward. Otherwise and agent might move up, then left and collect a reward of 2 normalized units instead of the ~=1.4142 normalized reward units collected for moving diagonally. The .70% keeps the reward for an vertical then horizontal (or vice versa) move to 1.4 which is less than the ~=1.4142 diagonal reward.Originally, the agent could collect up to a total reward of up to 1 and then 3 additional reward points for reach the target. Using the stock TensorFlow c51 DQN agent against this reward scheme, using the default min_q_value=-10 and max_q_value=10 values (I believe they are referred to as VMIN and VMAX in the literature) I found learning achieved the following results on very small world sizes:As can be seen, even for a simple 6x6 world C-51 DQN did not converge even after over 4 hours of wall time on GTX-3080 sampling 1,671,400 steps so I killed it.So I modified the total possible reward to for the agent moving to sum up to .5. This was simply a matter of dividing the old reward scheme in half (or rather *.5).I then changed the target reached reward from 3 to .5.The theoretical maximum reward then totaled to 1, so I changed min_q_value=0 and max_q_value=1 to match the sum of the rewards an agent might achieve. This resulted in wall times ever larger than before.My latest attempt I used min_q_value=0 and max_q_value=.5 to match the maximum cumulative reward the agent could recieve before the collecting the .5 target reached reward, (which BTW is also a terminal state in my environment).The new rewards, normalized to total to 1 run orders of a magnitude faster for the larger spaces. The 6x6 world ""converged"" after 05 minutes, 43 secs using 88721 total steps in  88721 in 4624 total episodes.I still find this slow, but clearly the q values matching the range of cumulative rewards is an improvement.FYI: The paper states that Transitions to a terminal state are handled with γt = 0.Update:The 9x9 environment took 294,897 steps, in 37,025 episodes over 51 minutes and 38 seconds. It may be the case that the max_q_value=1 works better in the larger world sizes where an agent might collect more movement reward. In any case, these values are nearly quadratically better than the default values from the tutorial.
I will experiment with them more.Additionally, tuning n-steps might help. My implementation may also have an issue using the replay buffer memories from smaller world sizes as I am changing the world size dynamically an currently not clearing the buffer."
Which models can I use for supervised learning with images?,"
I have to do a project that detects fabric surface errors and I will use machine learning methods to deal with it. I have a dataset that includes around six thousand fabric surface images with the size 256x256. This dataset is labeled, one thousand of it was labeled as NOK that means fabric surface with error, and the rest was labeled as OK which means fabric surface without an error. 
I read a lot of papers about fabric surface error detection with machine learning methods, and I saw that ""autoencoders"" are used to do it. But as I saw that the autoencoders are used in unsupervised learning models without labels. I need to do it with supervised learning models. Is there any model that can I use for fabric surface error detection with images in the supervised learning? Can be autoencoders used for it or is there any better model to do it?
","['neural-networks', 'machine-learning', 'unsupervised-learning', 'autoencoders', 'supervised-learning']",
Why is TD(0) not converging to the optimal policy?,"
I am trying to implement the basic RL algorithms to learn on this 10x10 GridWorld (from REINFORCEJS by Kaparthy).
Currently I am stuck at TD(0). No matter how many episodes I run, when I am updating the policy after all episodes are done according to the value function I dont get the optimal value function which I obtain when toggle td learning on the grid from the link I provided above.
The only way I am getting the optimal policy is when I am updating the policy in each iteration and then following the updated policy when calculating the TD target. But according to the algorithm from Sutton and Barto a given policy (which is fixed over all episodes - see line 1 below) should be evaluated.

Using alpha=0.1 and gamma=0.9 after 1000 episodes my Td(0) algo finds the following value function
[[-0.04 -0.03 -0.03 -0.05 -0.08 -0.11 -0.09 -0.06 -0.05 -0.07]
 [-0.08 -0.04 -0.04 -0.06 -0.1  -0.23 -0.11 -0.06 -0.07 -0.11]
 [-0.13  -inf  -inf  -inf  -inf -0.58  -inf  -inf  -inf -0.25]
 [-0.24 -0.52 -1.23 -2.6   -inf -1.4  -1.28 -1.12 -0.95 -0.62]
 [-0.28 -0.49 -0.87 -1.28  -inf -2.14 -2.63 -1.65 -1.38 -1.04]
 [-0.27 -0.42 -0.64 -0.94  -inf  0.97 -1.67 -2.01 -2.79 -1.62]
 [-0.26 -0.36 -0.69 -0.93  -inf -1.17 -1.72 -1.92 -2.75 -1.82]
 [-0.25 -0.38 -0.67 -2.27  -inf -2.62 -2.74 -1.55 -1.31 -1.14]
 [-0.23 -0.31 -0.66 -1.2  -0.98 -1.24 -1.48 -1.02 -0.7  -0.7 ]
 [-0.2  -0.29 -0.43 -0.62 -0.64 -0.77 -0.87 -0.67 -0.54 -0.48]]

where -infare walls in the grid. If I update the policy according to that value function I am getting.
[['e   ' 'e   ' 'w   ' 'w   ' 'w   ' 'w   ' 'e   ' 'e   ' 's   ' 'w   ']
 ['e   ' 'n   ' 'n   ' 'w   ' 'w   ' 'e   ' 'e   ' 'e   ' 'n   ' 'w   ']
 ['n   ' 'XXXX' 'XXXX' 'XXXX' 'XXXX' 'n   ' 'XXXX' 'XXXX' 'XXXX' 'n   ']
 ['n   ' 'w   ' 'w   ' 's   ' 'XXXX' 'n   ' 'e   ' 'e   ' 'e   ' 'n   ']
 ['n   ' 'w   ' 'w   ' 'w   ' 'XXXX' 's   ' 'n   ' 'n   ' 'n   ' 'n   ']
 ['s   ' 'w   ' 'w   ' 'w   ' 'XXXX' 's   ' 'w   ' 'n   ' 'n   ' 'n   ']
 ['s   ' 'w   ' 'w   ' 'w   ' 'XXXX' 'n   ' 'w   ' 's   ' 's   ' 's   ']
 ['s   ' 'w   ' 'w   ' 'w   ' 'XXXX' 'n   ' 'e   ' 's   ' 's   ' 's   ']
 ['s   ' 'w   ' 'w   ' 's   ' 's   ' 's   ' 's   ' 'e   ' 's   ' 's   ']
 ['n   ' 'w   ' 'w   ' 'w   ' 'w   ' 'w   ' 'e   ' 'e   ' 'e   ' 'w   ']]

where (n, w, s, e) = (north, west, south, east). According to the result from Andrey Kaparthys simulation (from here) the final policy should look like this

Notes:

I did not use any exploration
when the agent ends up in the final state [5, 5] I used the value of the starting state [0, 0] as the value of its successor state V(S_{t+1}). The episode is then finished and the agent starts again in the starting state.
In every state the agent takes a random action taken from north, west, south or east. If he ends in a wall the value of the next state is just the value where the agent currently is in. And it stays in its state and takes a random action again.

I am scratching my head on this for a while now but I dont understand what I am missing.

The value function has to converge. Meaning my policy should be the same as on the website (picture 2)?
Only the value of my final state is positive while on the website simulation the whole optimal trajectory has positive values. I know that this is because on the website they update the policy in every step. But shouldn't it also work without updating it iteratively like I did it?
Since I am taking a random action (from n,w,s,e) in every step in every episode for example state [6, 5] or [6, 6] (the one below the terminal state) can not really take advantage of the positivity of the terminal state since they are surrounded by more negative-reward-states than this positive-reward-state. This is why after so many iterations the values are getting negative.

I appreciate any help. Thanks in advance.
","['reinforcement-learning', 'reinforce', 'td-lambda']",
"In GradCAM, why is activation strength considered an indicator of relevant regions?","
In the GradCAM paper section 3 they implicitly propose that two things are needed to understand which areas of an input image contribute most to the output class (in a multi-label classification problem). That is:

$A^k$ the final feature maps
$\alpha_k^c$ the average pooled partial derivatives of the output class scores $y^c$ with respect to the the final feature maps $A_k$.

The second point is clear to me. The stronger the derivative, the more important the $k$th channel of the final feature maps is.
The first point is not, because the implicit assumption is that non-zero activations have more significance than activations close to zero. I know it's tempting to take that as a given, but for me it's not so obvious. After all, neurons have biases, and a bias can arbitrarily shift the reference point, and hence what 0 means. We can easily transform two neurons [0, 1] to [1, 0] with a linear transformation.
So why should it matter which regions of the final feature maps are strongly activated?

EDIT
To address a comment further down, this table explains why I'm thinking about magnitude rather than sign of the activations.

It comes from thinking about the possible variations of
$$
L_{Grad-CAM}^c = ReLU\bigl( \sum_k \alpha_k^c A^k \bigr)
$$
","['neural-networks', 'deep-neural-networks', 'explainable-ai', 'grad-cam']","I think you are misreading the relevant passage here.Since you do not specify exact excerpt(s), I take that by ""implicit assumption"" you refer to the equation (2) (application of a ReLU) and the corresponding text explanation (bold emphasis mine):We apply a ReLU to the linear combination of maps because we are only
interested in the features that have a positive influence on the class
of interest, i.e. pixels whose intensity should be increased in order
to increase $y^c$. Negative pixels are likely to belong to other
categories in the image. As expected, without this ReLU, localization maps sometimes highlight more than just the desired class and perform worse at localization.The first thing to notice here is that this choice is not at all about activations close to zero, as you seem to believe,  but about negative ones; and since negative activations are indeed likely to belong to other categories/classes than the one being ""explained"" at a given trial, it is very natural to exclude them using a ReLU.Grad-CAM maps are essentially localization ones; this is apparent already from the paper abstract (emphasis mine):Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’  in  a classification  network  or  a  sequence  of  words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept.and they are even occasionally referred to as ""Grad-CAM localizations"" (e.g. in the caption of Fig. 14); taking a standard example figure from the paper, e.g. this part of Fig. 1:it is hard to see how including the negative values of the map (i.e. removing the thresholding imposed by the ReLU) would not lead to maps that include irrelevant parts of the image, hence resulting in a worse localization.A general remark: while your claim thatAfter all, neurons have biases, and a bias can arbitrarily shift the reference point, and hence what 0 meansis correct as long as we treat the network as an arbitrary mathematical model, we can no longer treat a trained network as such. For a trained network (which Grad-CAM is all about), the exact values of both biases & weights matter, and we cannot transform them arbitrarily.UPDATE (after comments):Are you pointing out that it's called ""localization"" and therefore must be so?It is called ""localization"" because it is localization, literally (""look, here is the ""dog"" in the picture, not there"").I could make a similar challenge ""why does positive mean X and why does negative mean Y, and why will this always be true in any trained network?""It is not at all like that; positive means X and negative means not-X in the presence of class X (i.e. a specific X is present), in the specific network, and all this in a localization context; notice that Grad-CAM for ""dog"" is different from the one for ""cat"" in the picture above.Why is it that a trained network tends to make 0 mean ""insignficant"" in the deeper layers? [...] why is it that a network should invariably make the positive activations be the ones that support the prediction rather than the negative ones? Couldn't a network just learn it the other way around but use a negative sign on the weights of the final dense layer (so negative flips to positive and thus supports the highest scoring class)?Again, beware of such symmetry/invariability arguments when such symmetries/invariabilities are broken; and they are indeed broken here for a very simple reason (albeit hidden in the context), i.e. the specific one-hot encoding of the labels: we have encoded ""cat"" and ""dog"" as (say) [0, 1] and [1, 0] respectively, so, since we are interested in these 1s (which indicate class presence), it makes sense to look for the positive activations of the (late) convolutional layers. This breaks the positive/negative symmetry. Should we had chosen to encode them as [0, -1] and [-1, 0] respectively (""minus-one-hot encoding""), then yes, your argument would hold, and we would be interested in the negative activations. But since we take the one-hot encoding as given, the problem is no longer symmetric/invariant around zero - by using the specific label encoding, we have actually chosen a side (and thus broken the symmetry)..."
"How to use sigmoid as transfer function when input is not (0,1) range in ANN?","
I am building my first ANN from scratch. I know that I need a transfer function and I want to use the sigmoid function as my teacher recommended that. That function can be between 0 and 1, but my input values for the network are between -5 and 20. Someone told me that I need to scale the function so that it is in the range of -5 and 20 instead of 0 and 1. Is this true? Why?
","['neural-networks', 'activation-functions', 'sigmoid']",
Is it possible to pre-train a CNN in a self-supervised way so that it can later be used to solve an instance segmentation task?,"
I would like to use self-supervised learning (SSL) to learn features from images (the dataset consists of similar images with small differences), then use the resulting trained model to bootstrap an instance segmentation task.
I am thinking about using Faster R-CNN, Mask R-CNN, or ResNet for the instance segmentation task, which is pre-trained in an SSL way by solving a pretext task, with the aim that this will lead to higher accuracy and also teach the CNNs with fewer examples during the downstream task.
Is it possible to use SSL to pre-train e.g. a faster R-CNN on a pretext task (for example, rotation), then use this pre-trained model for instance segmentation with the aim to get better accuracy?
","['transfer-learning', 'r-cnn', 'self-supervised-learning', 'instance-segmentation', 'pretext-tasks']",
How to use unmodified input in neural network?,"
My question may be a bit hard to explain...
My neural network learns a categorical distribution, which serves as an index. This index will look up the value (= action_mean) in Input 2.
From this action_mean, I create a normal distribution where the network has to learn to adjust the standard deviation. The output of the network is a sample of this normal distribution.
Since the value of action_mean is directly taken from the input, somehow the gradient can't be computed or gives Nones, respectively, because the output of the net is not completely connected with the input.
Would there be a way to link my action_mean with the input value, without changing the input values itself? To describe my problem, I attached a simplified computational graph how tensorboard shows it.
I would be very thankful for any help!

","['neural-networks', 'machine-learning', 'reinforcement-learning', 'gradient-descent', 'graphs']",
What kind of problems is DQN algorithm good and bad for?,"
I know this is a general question, but I'm just looking for intuition. What are the characteristics of problems (in terms of state-space, action-space, environment, or anything else you can think of) that are well solvable with the family of DQN algorithms? What kind of problems are not well fit for DQNs?
","['neural-networks', 'reinforcement-learning', 'dqn', 'applications']","I don't currently have much practical experience with DQN, but I can partially answer this question also based on my theoretical knowledge and other info that I found.DQN is typically used fordiscrete action spaces (although there have been attempts to apply it to continuous action spaces, such as this one)discrete and continuous state spacesproblems where the optimal policy is deterministic (an example where the optimal policy is not deterministic is rock-paper-scissors)off-policy learning (Q-learning is an off-policy algorithm, but the point is that, if you have a problem/application where data can only be or has been gathered by a policy that is unrelated to the policy you want to estimate, then DQN is suitable, though there are other off-policy algorithms, such as DDPG)This guide also states that DQN is slower to train, but more sample efficient than other approaches, due to the use of the experience replay buffer.Moreover, if you have a small state and action space, it is probably a good idea to just use tabular Q-learning (i.e. no function approximation), given that it is guaranteed to converge to the optimal value function.See also this and this questions and this article (which compares DQN with policy gradients)."
Why does Monte Carlo policy evaluation relies on action-value function rather than state-value function?,"
Here is David Silver's lecture on that. Look at 9:30 to 10:30.
He says that, since it is model-free learning, the environment's dynamics are unknown, so the action-value function $Q$ is used.

But then state-values are already calculated (via first-visit or every-visit). So, why aren't these values used?

Secondly, even if we were to use $Q$, we have $Q^{\pi}(s,a) = R(s) + \gamma \sum_{s'}P(s'|s,a)V^{\pi}(s')$, so we still need to know the transition model, which is unknown.


What am I missing here?
","['reinforcement-learning', 'monte-carlo-methods', 'value-functions', 'model-based-methods', 'model-free-methods']","In Model Based Reinforcement learning, state and state-action values for all states can be calculated based on the bellman equations. The equations are taken from Andrew Ng's Algorithms for Inverse Reinforcement Learning $$V^{\pi}(s) = R(s) + \gamma \sum_{s'}P(s'|s,a)V^{\pi}(s') \\ Q^{\pi}(s,a) = R(s) + \gamma \sum_{s'}P(s'|s,a)V^{\pi}(s')$$In this setting, $Q^{\pi}$ can be obtained from $V^{\pi}$ because we have access to the transition model $P(s'|s,a)$. The $Q^{\pi}$ values allow us to carry out a step in $\textbf{policy improvement}$ as in policy iteration.To answer the first bullet point, the first visit or every state visit policy evaluation in the model free setting for $\textbf{state values}$ is not helpful in determining how to carry out model free control because we cannot compute $Q^{\pi}(s,a)$ from $V^{\pi}$ in the model free case.The update for SARSA in model free control is $$Q(s,a) \rightarrow Q(s,a) + \alpha (r(s) + \gamma Q(s',a') - Q(s,a))$$Even though we do not know the transition model, we are essentially $\textbf{sampling}$ from $P(s'|s,a)$ by allowing the environment to provide us the possible next states $s'$ that we may end up in. The following update for SARSA is equivalent to computing $$Q^{\pi}(s,a) = R(s) + \gamma E_{s' \sim P(s'|s,a)}[Q^{\pi}(s',a')]$$
Essentially this should give the same $Q^{\pi}(s,a)$ values when we have the ground truth $P(s'|s,a)$ values for the model free case."
Why would a VAE train much better with batch sizes closer to 1 over batch size of 100+?,"
I've been training a VAE to reconstruct human names and when I train it on a batch size of 100+ after about 5 hours of training it tends to just output the same thing regardless of the input and I'm using teacher forcing as well. When I use a lower batch size for example 1 it super overfitted and a batch size of 16 tended to give a much better generalization. Is there something about VAEs that would make this happen? Or is it just my specific problem?
","['variational-autoencoder', 'batch-size', 'batch-learning']",
Is it possible to improve the average precision of YOLO trained on Open Images Dataset by fine-tuning it with COCO?,"
I consider pre-training a YOLOv5 with Google Open Images Object Detection dataset. The dataset includes general domain categories with ~15 M box samples. After the pre-training is done, I will fine-tune the model on MS COCO dataset.
I would like to do it, if I can improve AP by ~7%. Do you think that it is possible, and I have a logical expectation? Unfortunately, I could not find anywhere anyone has tried an Open Images pre-trained object detector with MSCOCO training.
","['object-detection', 'yolo', 'pretrained-models', 'fine-tuning']",
Why do we use the tree-search version of breadth-first search or A*?,"
In Artiﬁcial Intelligence A Modern Approach, search algorithms are divided into tree-search version and graph-search version, where the graph-search version keeps an extra explored set to avoid expanding nodes repeatedly.
However, in breadth-first search or A* search, I think we still need to keep the expanded nodes in the memory so that we can track the path from the root to the goal. (The node structure contains its parent node which can be used to extract the solution path, but only if we have those nodes kept in the memory)
So, if I'm right, why do we need the tree-search version in BFS and A*, given that the expanded nodes still need to be stored? Why not just use the graph-search version then?
If I'm wrong, so how do we track the solution path given that the expanded nodes have been discarded?
","['search', 'a-star', 'breadth-first-search', 'graph-search', 'tree-search']",
"Why don't neural networks project the data into higher dimensions first, then reduce the size of each layer thereafter?","
Background
From my understanding (and following along with this blog post), (deep) neural networks apply transformations to the data such that the data's representation to the next layer (or classification layer) becomes more separate. As such, we can then apply a simple classifier(s) to the representation to chop up the regions where the different classes exist (as shown by this blog post).
If this is true and say we have some noisy data where the classes are not easily separable, would it make sense to push the input to a higher dimension, so we can more easily separate it later in the network?
For example, I have some tabular data that is a bit noisy, say it has 50 dimensions (input size of 50). To me, it seems logical to project the data to a higher dimension, such that it makes it easier for the classifier to separate. In essence, I would project the data to say 60 dimensions (layer out dim = 60), so the network can represent the data with more dimensions, allowing us to linearly separate it. (I find this similar to how SVMs can classify the data by pushing it to a higher dimension).
Question
Why, if the above is correct, do we not see many neural network architectures projecting the data into higher dimensions first then reducing the size of each layer thereafter?
I learned that if we have more hidden nodes than input nodes, the network will memorize rather than generalize.
","['neural-networks', 'overfitting', 'multilayer-perceptrons', 'generalization']","To better understand this you should think in terms of capacity. Capacity is a theoretical notion that shows how much information your network can model.The capacity of a network (given sufficient training) ties in directly with the bias/variance tradeoff:At some point a network reaches a point where it has a high enough capacity to memorize the whole training set!Now, by increasing the number of hidden neurons you essentially increase the capacity of your network. If the network already has enough capacity to learn the problem, then by increasing the neurons you are giving the network the capability of overfitting more easily.Note: all the above assume that the network is trained sufficiently (i.e. no early stopping, etc)."
Does the order in which the features are concatenated to create the state (or observation) matter?,"
I'm experimenting with an RL agent that interacts with the following environment. The learning algorithm is double DQN. The neural network represents the function from state to action. It's build with Keras sequential model and has two dense layers. The observation in the environment consists of the following features

the agent's position in an N-dimensional grid,
metrics that represent the hazards of adjacent cells (temperatures, toxicity, radiation, etc.) of adjacent cells, and
some parameters that represent the agent's current characteristics (health, mood, etc.).

There are patterns to the distribution of hazards and the agent's goal is to learn to navigate safely through space.
I am concatenating these features, in the aforementioned order, into a tensor, which is fed into the double DQN.
Does the order in which the features are concatenated to create the state (or observation) matter? Is it possible to group the features in some way to increase the learning speed? If I mix up the features randomly, would that have any effect or it doesn't matter to the agent?
","['reinforcement-learning', 'deep-rl', 'feature-selection', 'state-spaces', 'observation-spaces']",
Is there some known pattern for selecting a batch of candidates for the next generation?,"
I'm a beginner with a classic ""racing car"" sandbox and a homemade simple neural network.
My pattern:

Copy the ""top car"" (without mutation) to the next generation

If there are some cars still running (because simulation reached the 30s win condition), then copy a mutated version of them for the next generation.

Fill the rest of the pool with mutation of the ""top car"".


But this is just some dumb intuitive pattern I made on the fly while playing with my code. Perhaps I should copy the cars that are still running as-is instead of mutating them. Or, perhaps, some selection method I don't know about.
A new random track is generated at each new generation. a ""top car"" may be good on a track and crash immediately on the next track. I just feel that basing everything on the top car is wrong because of the track randomness.
Is there some known pattern for selecting a batch of candidates? (paper, google-fu keyword, interesting blog, etc.)
I don't know what to search for. I don't even know the name of my network or any vocabulary related to AI.
","['neural-networks', 'reference-request', 'genetic-algorithms', 'genetic-operators']","The most general descriptive frameworks covering what you are trying to do are:Sequential decision making (article is a stub, but the term a good launching point to discover different wys of modelling and solving these kind of problems)Optimal controlThese put some context around your problem, and might give you some pointers. For instance, reinforcement learning is an alternative approach to the evolutionary system you are trying to build.The specific AI system you appear to be building is a genetic algorithm, and more specific still you are attempting to find a neural network that is optimal at a task by searching for the best network using a system of population generation, selection and mutation which repeats.There are lots of ways to set up a system like this, so your approach is not necessarily wrong. However, I think there are two key things that would improve what you have built so far:Use a fitness function for selection. Score each car, perhaps by how far it got before crashing when the episode ends. To reduce luck factor on random courses, you could make this score the mean result from e.g. 3 different courses (it is not necessary, but may address your concern that selection is too random in your case). Select some fraction of top scoring cars, or look into other selection approaches - e.g. weighted selection based on fitness score or ranking.Add ""sex"", more properly known as genome crossover between selected population members. Mutating individuals is limiting because it silos improvements to a single line of ancestry - if there are two good mutations found at random you rely on that single line finding both of them. Whilst crossover allows sharing of good mutations between lines, making it much more likely that two good mutations will end up in the same individual.There is a framework called NEAT which covers the issues above plus has other features useful for evolving neural networks. It often does well at control scenarios like the one you are considering. You may want to look into it, if your focus is mainly on solving the control problem. However, it is relatively advanced from where you are, so if your current focus to learn by building from scratch you may get more initially from implementing fitness functions and crossover yourself."
"Given two optimal policies, is an affine combination of them also optimal?","
If there are two different optimal policies $\pi_1, \pi_2$ in a reinforcement learning task, will the linear combination (or affine combination) of the two policies $\alpha \pi_1 + \beta \pi_2, \alpha + \beta = 1$ also be an optimal policy?
Here I give a simple demo:
In a task, there are three states $s_0, s_1, s_2$, where $s_1, s_2$ are both terminal states. The action space contains two actions $a_1, a_2$.
An agent will start from $s_0$, it can choose $a_1$, then it will arrive $s_1$，and receive a reward of $+1$. In $s_0$, it can also choose
$a_2$, then it will arrive $s_2$, and receive a reward of $+1$.
In this simple demo task, we can first derive two different optimal policy $\pi_1$, $\pi_2$, where $\pi_1(a_1|s_0) = 1$, $\pi_2(a_2 | s_0) = 1$. The combination of $\pi_1$ and$\pi_2$ is
$\pi: \pi(a_1|s_0) = \alpha, \pi(a_2|s_0) = \beta$. $\pi$ is an optimal policy, too. Because any policy in this task is an optimal policy.
","['reinforcement-learning', 'proofs', 'policies', 'optimal-policy', 'optimality']","Two policies are different if they take different actions in a specific state $s$ (or they give different probabilities of taking those actions in $s$). There can be more than one optimal policy for a given value function: this only happens when two actions have the same value in a given state. Nevertheless, both policies lead to the same expected return. So, although they take different actions, those actions lead to the same expected return, so it doesn't matter which one you take: both actions are optimal.There are a few important points that need to be understood before understanding that an affine combination of optimal policies is also optimal.A policy $\pi$ is optimal if and only if $v_\pi(s) \geq v_{\pi'}(s)$, for all states $s \in S$ and $\pi' \neq \pi \in \Pi$ [1];In that case, we denote $\pi$ as $\pi_*$ and $\pi_* = \pi \geq \pi'$, for all $\pi' \neq \pi \in \Pi$.In simple words, a policy is optimal if it leads to more or equal expected return, in all states, with respect to all other policiesOptimal policies share the same state and state-action value functions [1, 2], i.e. $v_*$ and $q_*$, respectivelyConsequently, two optimal policies $\pi_1$ and $\pi_2$ can differ in state $s$ (i.e. $\pi_1$ takes action $a_1$ and $\pi_2$ takes action $a_2$ and $a_1 \neq a_2$) if and only if there exist actions $a_1$ and $a_2$ in $s$ such that
\begin{align}
v_{*}(s) 
&= q_{\pi_1}(s, a_1) \\ 
&= q_{\pi_1}(s, a_2) \\ 
&= q_{\pi_2}(s, a_2) \\
&= q_{\pi_2}(s, a_1) \\
&= \max _{a \in \mathcal{A}(s)} q_{\pi_{*}}(s, a) \\
&= \max _{a \in \mathcal{A}(s)} q_{\pi_1}(s, a) \\
&= \max _{a \in \mathcal{A}(s)} q_{\pi_2}(s, a) \tag{1} \label{1}
\end{align}This holds for deterministic (i.e. policies that always take the same action in a given state, i.e. they give probability $1$ to one action) and stochastic (give non-zero probability only to optimal actions) optimal policiesSo, two different optimal policies $\pi_1$ and $\pi_2$ lead to the same expected return, for all states. Given that optimality is defined in terms of expected return, then, if $a_1 = \pi_1(s) \neq \pi_2(s) = a_2$, for some state $s$, then, it doesn't matter whether you take $a_1$ or $a_2$, because both lead to the same expected return. So, as written in this answer, you can either take action $a_1$ or $a_2$: both are optimal in terms of expected returns and this follows from equation \ref{1} above.In this simple demo task, we can first derive two different optimal policy $\pi_1$, $\pi_2$, where $\pi_1(a_1|s_0) = 1$, $\pi_2(a_2 | s_0) = 1$. The combination of $\pi_1$ and$\pi_2$ is
$\pi: \pi(a_1|s_0) = \alpha, \pi(a_2|s_0) = \beta$. $\pi$ is an optimal policy, too. Because any policy in this task is an optimal policy.Yes, correct. The reason is simple. In your case, $\pi_1$ and $\pi_2$ give probability $1$ to one action, $a_1$ and $a_2$ respectively, so they must give probability $0$ to any other actions. $\pi$ will give a probability $\alpha$ to action $a_2$ and probability $\beta$ to action $a_1$, but, given that $a_1$ and $a_2$ lead to the same expected return (i.e. they are both optimal), it doesn't matter whether you take $a_1$ or $a_2$, even if $\alpha \ll \beta$ (or vice-versa)."
Can the (sparse) categorical cross-entropy be greater than one?,"
I am using AlexNet CNN to classify my dataset which contains 10 classes and 1000 data for each class, with 60-30-10, splits for train, validation, and test. I used different batch sizes, learning rates, activation functions, and initializers. I'm using the sparse categorical cross-entropy loss function.
However, while training, my loss value is greater than one (almost equal to 1.2) in the first epoch, but until epoch 5 it comes near 0.8.
Is it normal? If not, how can I solve this?
","['convolutional-neural-networks', 'training', 'object-recognition', 'categorical-crossentropy', 'alexnet']","Both the sparse categorical cross-entropy (SCE) and the categorical cross-entropy (CCE) can be greater than $1$. By the way, they are the same exact loss function: the only difference is really the implementation, where the SCE assumes that the labels (or classes) are given as integers, while the CCE assumes that the labels are given as one-hot vectors.Here is the explanation with examples.Let $(x, y) \in D$ be an input-output pair from the labelled dataset $D$, where $x$ is the input and $y$ is the ground-truth class/label for $x$, which is an integer between $0$ and $C-1$. Let's suppose that your neural network $f$ produces a probability vector $f(x) = \hat{y} \in [0, 1]^C$ (e.g. with a softmax), where $\hat{y}_i \in [0, 1]$ is the $i$th element of $\hat{y}$.The formula for SCE is (which is consistent with the TensorFlow implementation of this loss function)$$
\text{SCE}(y, \hat{y}) = - \ln (\hat{y}_{y}) \label{1}\tag{1},
$$where $\hat{y}_{y}$ is the $y$th element of the output probability vector $\hat{y}$ that corresponds to the probability that $x$ belongs to class $y$, according to $f$.Actually, the equation \ref{1} is also the definition of the CCE with one-hot vectors as targets (which behave as indicator functions). The only difference between CCE and SSE is really just the representation of the targets, which can slightly change the implementation under the hood. Moreover, note that this is the definition of the CE for only $1$ training pair. If you have multiple pairs, you have to compute the CE for all pairs, then average these CEs (for a reference, see equation 4.108, section 4.3.4 Multiclass logistic regression of Bishop's book PRML).Let's have a look at a concrete example with concrete numbers. Let $C=5$, $y = 3$, $\hat{y} = [0.2, 0.2, 0.1, 0.4, 0.1]$, then the SCE is\begin{align}
\text{SCE}(y, \hat{y}) 
&= 
- \ln (0.4) \approx 0.92,
\end{align}If $\hat{y} = [0.2, 0.2, 0.2, 0.1, 0.3]$, so $\hat{y}_{y} = 0.1$, and we still have $y = 3$, then the CCE is $2.3 > 1$.You can execute this Python code to check yourself.To answer the following question more directly.However, while training, my loss value is greater than one (almost equal to 1.2) in the first epoch, but until epoch 5 it comes near 0.8. Is it normal? If not, how can I solve this?Yes. It can happen, as explained above. (However, this does not mean that you do not have mistakes in your code.)"
How is input defined for a biaxial lstm network for generating music?,"
I am reading Composing Music With Recurrent Neural Networks by Daniel D. Johnson. But I am really confused about the input passed to this network. If we pass notes of music along the time axis, then what is passed along the note axis?

The author says:

If we make a stack of identical recurrent neural networks, one for each output note, and give each one a local neighborhood (for example, one octave above and below) around the note as its input, then we have a system that is invariant in both time and notes: the network can work with relative inputs in both directions.

This might mean that the inputs passed to the network along the note axis are fixed representations of notes in the vocabulary. But I am not sure.
I am also having a hard time understanding the input passed to this network as the author explains a few paragraphs below. (Position, Pitchclass, Previous Vicinity, Previous Context, Beat).
Also, at some point, the author talks about RNN along the note axis. But in the architecture, there only seems to be RNN along the time axis. I would really appreciate is anyone could give me some more information to understand how this Biaxial Network is setup. This article by Deep Dark Learning was a little helpful but I am still not fully sure what is going on here.
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'language-model']",
When does IDA* consider the goal has been found?,"
I was reading about IDA* and I found this link explaining IDA* and providing an animation for it.
Here is a picture of the solution.

I know what is the cutoff condition (it depends on F), and the search is like DFS if the value of (f) of the node is less or equal to the cutoff, and like IDF it is iterative
My question is:
In the animation, when the threshold is 7 and after expanding the parent of the goal (14), they stated that a solution has been found, so, if we found the goal after expanding a node which is <= the cutoff value, can we consider it the solution without applying any condition-test on it? (it's F <= threshold): for example, if there was another level where there is a goal and it can be found with value 13 (less than 14), like the following pic:

when the threshold is 7, 11 will not be expanded so we will never get (13)
So, what is the correct solution?
","['search', 'ida-star', 'stopping-conditions']","According to Artificial Intelligence: A Modern Approach 4th edition in IDA* the cutoff is the $f$-cost($g+h$); at each iteration, the cutoff value is the smallest $f$-cost of any node that exceeded the cutoff on the previous iteration.In other words, each iteration exhaustively searches an $f$-contour, finds a node just beyond that contour, and uses that node's $f$-cost as the next contour.And we must test if the node is a goal node when it was selected for expansion, otherwise, the algorithm is not optimal anymore (the proof is similar to that one in A*).I think the animation that the site provided is misleading because in the code which is written in the last of the same site we have that:And in the previous code, we test if the node is the goal node only when it was selected for expansion."
Can $Q$-learning or SARSA be thought of a Markov Chain?,"
I might just be overthinking a very simple question but nonetheless the following has been bugging me a lot.
Given an MDP with non-trivial state and action sets, we can implement the SARSA algorithm to find the optimal policy or the optimal state-action-value function $Q^*(s,a)$ by the following iteration:
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t) + \alpha(r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)).$$
Assuming each state-action pair is visited infinitely often, fix one such pair $(s,a)$ and denote the time sequence of visiting the said pair as $t_1 < t_2 < t_3 < \dots t_n\dots.$ Also, let $Q_{t_n}(s,a) = X_n$ for ease of notation and consider the sequence of random variables:
$$X_0, X_1, \dots X_n,\dots $$
Can $\{X_n\}_{n\geq 0}$ be though of a discrete-time Markov Chain on $\mathbb{R}$ ? My intuition says no, because the recurrence equation will look like:
$$X_{n+1} = (1-\alpha)X_n + \alpha(r_{t_n} +\gamma Q_{t_n}(s',a'))$$
and that last term $Q_{t_n}(s',a')$ will be dependent on the path even if we condition on $X_n = x.$
However, I am not quite able to rigorously write an answer in the either direction. I will greatly appreciate if someone can resolve this issue that I am having in either direction.
","['reinforcement-learning', 'markov-decision-process', 'sarsa', 'markov-chain', 'markov-property']",
Loss function decays linearly in segmentation MRI fascia,"
I am working on a segmentation of MRI images of the thigh. I am trying to segment the fascia, there is a slight imbalance between the background and the mask. I have about 1400 images from 30 patients for training and 200 for validation. I am working with keras. The loss function is combination of weighted cross entropy and dice loss (smooth factor of dice loss = 2)
def combo_loss(y_true, y_pred,alpha=0.6, beta=0.6): # beta before 0.4
    return alpha*tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, pos_weight=beta)+((1-alpha)*dice_coefficient_loss(y_true, y_pred))

When I use a value alpha greater thatn 0.5 (the weighted cross entropy) the loss rapidly decreases during the first epoch. Afterwards if slowly decreases in a linear manner.

Why is this happening? What would be a good approach reasonable approach to choose the values of alpha and beta?
","['tensorflow', 'keras', 'objective-functions', 'image-segmentation']",
Which machine learning technique can I use to match one set of data points to another?,"
I have two measuring devices. Both measure the same thing. One is accurate, the other is not, but does correlate with a non-fixed offset, some outliers, and some noise.
I won't always be using the accurate device. The nonfixed offset makes things difficult, but I'm certain there is sufficient similarity to make a link using a machine learning (or AI) technique and to convert one set of numbers to a good approximation of the other.
One is a footbed power meter and gives power in Watts every second. The other is a crank-based power meter, also outputting Watts at 1Hz. The footbed power is much less than the crank (which I know to be accurate), but it does track the increases and decreases in power, just with more noise and, as I say, a non-fixed offset (and by non-fixed I mean, at low power the offset is different to that at high power, I don't mean it isn't consistent, it is consistent). Both measure cadence which may be a useful metric to help find a pattern.
I will be collecting sets of data from both and hoped to plug the footbed data in as a column of values with the crank data as another column representing the truth, so after training, the model would be able to transform the footbed data to an approximation of the crank data.
Anyway, I'm completely lost as to how to begin. I've tried searching, but, clearly, I'm using the wrong keywords. Does anyone have any pointers, please?
","['neural-networks', 'machine-learning', 'linear-regression']",
How to extract the main text from a formatted text file?,"
My idea is to model and train a neural network that receives a text version of a PDF file as the input and gives the content text as output.
Take the scenario:

One prints a PDF file to a text file (the text file does not have images, but has the main text, headings, page numbers, some other footer text, and so on, and keeps the same number of columns  - two for instance - of text);

This text file is submitted to a tool that strips everything that is not the main content of the text in one single text column (one text stream), keeping the section titles, paragraphs, and the text in a readable form (does not mix columns);

The tool generates a new version of the original text file containing only the main text portion, ready to be used for other purposes where the striped parts would be considered noise.


How to model this problem in a way a neural network can handle it?
Update 1
Here are some clarifications on the problem.
PDF file
The picture below shows two pages of a pdf version of a scientific paper. This is just to set the context, the PDF file is not the input for this problem, it is just to understand where the actual input data comes from.

The color boxes show some parts of interest for this discussion. Red boxes are headers and footers. We are not interested in them. Blue and green boxes are content text blocks. Different colors were used to emphasize the text is organized in columns and that is part of the problem. Those blue and green boxes are what we actually want.
Text file
If a use the ""save as text file"" feature of my free PDF reader, I get a text file similar to the image below.

The text file is continuous, but I put the equivalent of the first two pages of the PDF file side-by-side just to make things easier to compare. We can see the very same colored boxes. In terms of words, those boxes contain the same text as in the PDF version.
Understanding the problem
When we read a paper, we are usually not very interested in footers or headers. The main content is what we actually read and that will provide us with the knowledge we are looking for. In this case, the text is inside blue and green boxes.
So, what we want here is to generate a new version of the input (text) file organized in one single text stream (one column if you will), with the text laid-out in a form someone can read it, which means, alternating the blue and the green boxes.
However, if the original PDF has no footers, it should work in the same way, providing the main text content. If the text comes in three of four columns, the final product must be a text in good condition to be read without losing any information.
Any pictures will be simply stripped off the text version of the paper and we are fine with that.
",['neural-networks'],
How does a GCN handle new input graphs?,"
Quick questions to see whether I understand GCNs correctly.
Is it correct that, if I have trained a GCN, it can take arbitrary graphs as input, assuming the feature size is the same?
I can't seem to find explicit literature on this.
","['geometric-deep-learning', 'graphs', 'graph-neural-networks']",
What is the difference between the uniform-cost search and Dijkstra's algorithm?,"
Every computer science student (including myself, when I was doing my bachelor's in CS) probably encountered the famous single-source shortest path Dijkstra's algorithm (DA). If you also took an introductory course on artificial intelligence (as I did a few years ago, during my bachelor's), you should have also encountered some search algorithms, in particular, the uniform-cost search (UCS).
A few articles on the web (such as the Wikipedia article on DA) say that DA (or a variant of it) is equivalent to the UCS. The famous Norvig and Russell's book Artificial Intelligence: A Modern Approach (3rd edition) even states

The two-point shortest-path algorithm of Dijkstra (1959) is the origin of uniform-cost search. These works also introduced the idea of explored and frontier sets (closed and open lists).

How exactly is DA equivalent to UCS?
","['comparison', 'search', 'uniform-cost-search', 'dijkstras-algorithm', 'shortest-path-problem']","The answer to my question can be found in the paper Position Paper: Dijkstra's Algorithm versus Uniform Cost Search or a Case Against Dijkstra's Algorithm (2011), in particular section Similarities of DA and UCS, so you should read this paper for all the details.DA and UCS are logically equivalent (i.e. they process the same vertices in the same order), but they do it differently. In particular, the main practical difference between the single-source DA and UCS is that, in DA, all nodes are initially inserted in a priority queue, while in UCS nodes are inserted lazily.Here is the pseudocode (taken from the cited paper) of DAHere is the pseudocode of the best-first search (BFS), of which UCS is just a particular case. Actually, this is the pseudocode of UCS where $g(n)$ is the cost of the path from the source node to $n$ (although the title indicates that this is the pseudocode of BFS)."
"In a face database containing multiple images per subject, how do we determine the face image which is most suited for face recognition?","
Let us imagine a face database with several subjects, each subject having multiple face images. How do we determine which is the best face suitable for face recognition purposes?
","['deep-learning', 'face-recognition']",
Why do we need to provide false labels to the discriminator on purpose to train GANs?,"
This is the tutorial that I used to learn about GANs. In this tutorial, it taught us to intentionally provide false labels to ""fool"" the discriminator, but does it make the discriminator actually inaccurate? I don't quite understand his explanation, can anyone help me?
","['computer-vision', 'training', 'keras', 'generative-adversarial-networks']",
Understanding neural network achitectures in policy gradient reinforcement learning for continuous state and action space,"
I am trying to train a neural network using reinforcement learning / policy gradient methods. The states, i.e. the inputs, as well as the actions I am trying to sample are vectors with each element being a real number like in this question: https://math.stackexchange.com/questions/3179912/policy-gradient-reinforcement-learning-for-continuous-state-and-action-space
The answer that was given there already helped me a lot. Also, I have been trying to understand Chapter 13: Policy Gradient Methods from ""Reinforcement Learning: An Introduction"" by Sutton et al. and in particular Section 13.7: Policy Parametrization for Continous Actions.
My current level of understanding that I can use the weights in the network to calculate the mean/means and the standard deviation/covariance matrix. I can then use them to define a multivariate Gaussian distribution and randomly sample an action from there.
For now, I have one main question: In the book it says, that I have to split the weights, i.e. the policy's parameter vector, into two parts: $\theta = [\theta_{\mu}, \theta_{\sigma}]$. I can then use each part together with a feature vector to calculate the means and the covariance matrix. However, I was wondering, how this is usually done? Do I train two separate networks? I am not sure how an architecture of this will look like. Also, I am not sure what the output nodes will be in this case. Do they have any meaning like for supervised learning?
So far, I have only found papers that talk about this issue rather theoretically like it is presented in the book. I would be very happy to understand how this is actually implemented. Thank you very much!
","['neural-networks', 'reinforcement-learning', 'policy-gradients']",
GAN model predictions before training is predictable,"
I have a dataset of 3000 8x8 images, and I would like to train a GAN for an image generation purpose.
I am planning to start with a simple GAN model and see if it overfits. Before training, I try to do a comparison of the discriminator model prediction using real image input against the whole GAN model prediction using random seed input. My thought process is that since this model is not trained yet, the output for real images and fake images by the discriminator should not be predictable.
However, the discriminator model prediction using real image input always returns a value very close to 1.0, and the whole GAN model prediction using random seed input always returns a value near 0.5 with a small deviation. I suspect that during training, the model would simply pull the 0.5 value near 0.0 and would never actually learn from the dataset.
I try to increase the training parameters and different initializers, but the output is still the same.
By ruling out the possibility a bad dataset, what could be the reason for this situation?
This is some sneak peek of the generator and discriminator model building: https://pastebin.com/ehMDP7k6
",['generative-adversarial-networks'],"I took a look at your model. It seems you have incorrect architecture. The Conv2D layers in your D should have following params: (n_filters, kernel=3, padding='same'), where n_filters is the number of filters and it usually should be doubled as per DCGAN architecture. You can also use strides but since your images are small it won't make any sense.The D network also should not include UpSampling2D layer. This layer can be used in G network instead of Conv2DTranspose.Your images should be normalized to the range of [-1, 1] and the activation function of G should be tanh.You also included m.add(keras.layers.Reshape((8, 8, 3))) to your G network which is the final size of your data. Since you have the final size in the first layers you don't have to include upsampling Conv2DTranspose layers.Your final model should look likeNote that I did not test the model, so you should adapt it to the shape of your data.I encourage you to follow this tutorial.Update regarding overfittingGANs do not have overfitting problem in the classical sense. Instead, vanilla GAN have other problems like vanishing gradient. It happens when the D becomes overconfident regarding fake samples. In that case, D stops providing useful information to the G, since its error becomes 0. To find out if training fell into this problem, you should plot D and G loss. It will look as follows:Another problem is the mode collapse. In that case, the G tricks the D by producing only one type of samples which looks realistic but G won't represent the real data distribution. Therefore, the generated samples will be homogeneous.For more information, see Improved Techniques for Training GANs and Wasserstein GAN."
How to compute dominant colors in an image?,"
I was trying Google Cloud's Vision API, and how the dominant colors part shows. I uploaded a sample image, and here is the results for the dominant colors. I realized it doesn't simply count pixel colors and cluster them. The background has many gray pixels which are not included.
How does it perform dominant colors? How can I do something similar?

","['machine-learning', 'computer-vision', 'object-detection', 'image-processing', 'google-cloud']","You can find an explanation here (github of the googleapi):My current understanding of a color's score is a combination of two things:For example, given the following image:The focus is clearly the cat, and therefore the color annotation for this image with the highest score (0.15) will be RGB = (232, 183, 135) which is the beige color:The green of the grass (despite having more pixels in the image dedicated to it) has a much lower score by virtue of the algorithm's detection that it's the background and not the focus of the image.In other words, higher ""scores"" means higher confidence that the color in question is prominent in the central focus of the image.It is analogous to your case. Therefore, using a background removal this can help to find the focus of the image, and then make the histogram of the remaining objects' color. An example of the background removal using deep learning can be found in this post."
"When learning off-policy with multi-step returns, why do we use the current behaviour policy in importance sampling?","
When learning off-policy with multi-step returns, we want to update the value of $Q(s_1, a_1)$ using rewards from the trajectory $\tau = (s_1, a_1, r_1, s_2, a_2, r_2, ..., s_n, a_n, r_n, s_n+1)$. We want to learn the target policy $\pi$ while behaving according to policy $\mu$. Therefore, for each transition $(s_t, a_t, r_t, s_{t+1})$, we apply the importance ratio $\frac{\pi(a_t | s_t)}{\mu(a_t | s_t)}$.
My question is: if we are training at every step, the behavior policy may change at each step and therefore the transitions of the trajectory $\tau$ are not obtained from the current behavior policy, but from $n$ behavior policies. Why do we use the current behavior policy in the importance sampling? Should each transition use the probability of the behavior policy of the timestep at which that transition was collected? For example by storing the likelihood $\mu_t(a_t | s_t)$ along with the transition?
","['reinforcement-learning', 'off-policy-methods', 'value-functions', 'importance-sampling', 'return']",
"Why are ""Transformers"" called this way?","
What is the reason behind the name ""Transformers"", for Multi Head Self-Attention-based neural networks from Attention is All You Need?
I have been googling this question for a long time, and nowhere I can find any explanation.
","['neural-networks', 'terminology', 'transformer', 'attention']",
Optimal mixed strategy in two player zero sum games,"
I am currently studying game theory based on Peter Norvig's 3rd edition introduction to artificial intelligence book. In chapter 17.5, the two player zero sum game can be solved by using the $\textbf{minimax}$ theorem
$$max_x \, min_y \, x^TAy = min_x \, max_y \, x^TAy = v$$
where $x$ is  the probability distribution of actions by the max player (in the left equation) and the min player (in the right equation).
Regarding the minimax theorem, I have 2 questions.

Do both the min and the max players have the same probability distribution of actions ? In the book by Peter Norvig, the book demonstrated that in the game of $\textbf{Morra}$, both the min and max player had $[\frac{7}{12}:one, \frac{5}{12}:two]$ for the probability distributions.

Also, regarding minimax game tree, is the difference between minimax game tree and the zero sum game the fact that for minimax game tree, the opponent can react to the first player's move whereas for zero sum game defined in 17.5, both players are unaware of each other's move ?


","['minimax', 'game-theory', 'zero-sum-games']","I will answer the first question question based on information I have gathered so far. The probability of each action for the $\textbf{two player zero sum game}$ need not be the same for both players. It turns out that in the game of Morra, the probability vectors just turn out to be the same value.In general to determine $\textbf{optimal mixed strategies}$ for two player two action game: Suppose we have 2 actions $a_1$ and $a_2$. In a mixed strategy, we let the probability of the row player taking action $a_1$ be $p$. The probability of the row player taking $a_2$ is then $1-p$. Now, we will try to find the probability $p$ such that the column player is indifferent to either action $a_1$ or $a_2$. (I.e the expected payoff for column player if row player plays $a_1$ with probability $p$ is the same for either actions).Solving for $p$, we know that the column player can play any mixed strategy since any strategy played by the column player will yield the same payoff (since expected payoff is the same for either $a_1$ and $a_2$). Since column player can play any mixed strategy, we want to find a probability distribution over $a_1$ and $a_2$ for the column player such that the row player is indifferent to either action $a_1$ or $a_2$. We let the column player take action $a_1$ with probability $q$. It turns out we can repeat the same process for the column player and find the values of $p$. The game value for the row player can be computed from $x^TAy$ and this turns out to be the same value for the column player as well. (Only in zero sum games)Doing this process yields a $\textbf{mixed strategy Nash Equilibrium}$, using such an approach does not always work however in the scenario when one action choice always dominates another action choice. (I.e in the case of the prisoner's dilemma of Alice and Bob, no matter what mixed strategy Bob tries to use, there will never be a mixed strategy such that Alice is indifferent to refusing or testifying). She will always testify.I am not too sure about how to solve zero sum games involving more than 2 actions. I think linear programming would have to be used but i am not to familiar with how it can be applied."
What are the benefits of using ELU over other activation functions in CNNs?,"
I have come up with some examples of CNNs (segmentation CNNs) that use ELU (exponential linear unit) as an activation function.
What are the benefits of this activation function over others, such as RELU or leaky RELU?
","['convolutional-neural-networks', 'activation-functions', 'relu', 'leaky-relu', 'elu']",
Object detection noise filtering,"
In my project, I am detecting only one class, which is ""airplane"", using yolov5. However, at some frames, the neural network labels some of the buildings as airplanes, which obviously are not. This noise happens like 1 frame among 60 frames. How should I treat this issue? Which algorithms can be applied to filter out?
","['object-detection', 'yolo']",
Find repeating patterns in sequence data,"
I have database of sequential events for multiple animals.
The events are represented by integers so it looks something like:
Animal A: [1,6,4,2,5,7,8] 
Animal B: [1,6,5,4,1,6,7]
Animal C: [5,4,2,1,6,4,3]

I can manually see that for each event 6 event 1 first happens.
And event 4 happens quickly after a 1,6 combination.
But these are easy to spot in such a small dataset, the real lists are 10000+ events per animal.
Is there a way to use an algorithm or machine learning to search for these kinds of patterns?
","['machine-learning', 'pattern-recognition']",
Thompson sampling with Bernoulli prior and non-binary reward update,"
I am solving a problem for which I have to select the best possible servers (level 1) to hit for a given data. These servers (level 1) in turn hit some other servers (level 2) to complete the request. The level 1 servers have the same set of level 2 servers integrated with them. For a particular request, I am getting success or failure as a response.
For this, I am using Thompson Sampling with Bernoulli prior. On success, I am considering reward as 1 and, for failure, it is 0. But in case of failure, I am receiving errors as well. In some error, it is evident that the error is due to some issue at the server (level 1) end, and hence reward 0 makes sense, but some error results from request data errors or issue at level 2 servers. For these kinds of errors, we can't penalize the level 1 servers with reward 0 nor can we reward them with value 1.
Currently, I am using 0.5 as a reward for such cases.
Exploring over the Internet, I couldn't find any method/algorithm to calculate the reward for such cases in a proper (informed) way.
What could be the possible way to calculate reward in such cases?
","['reinforcement-learning', 'reward-design', 'reward-functions', 'bayesian-statistics', 'thompson-sampling']",
How can I discourage the RL agent from drawing in a zero-sum game?,"
My agent receives $1, 0, -1$ rewards for winning, drawing, and losing the game, respectively. What would be the consequences of setting reward to $-1$ for draws? Would that encourage the agent to win more or will it have no effect at all? Is it appropriate to do so?
","['reinforcement-learning', 'reward-design', 'reward-functions', 'zero-sum-games']",
"If random rotations are included in the data augmentation process, how are the new bounding boxes calculated?","
When studying bounding box-based detectors, it's not clear to me if data augmentation includes adding random rotations.
If random rotations are added, how is the new bounding box calculated?
","['deep-learning', 'object-detection', 'object-recognition', 'yolo', 'data-augmentation']",
What is the appropriate way of passing a list of integers that represents the environment to a neural network's dense layer?,"
I'm training an RL agent using the DQN algorithm to do a specific task. The environment is represented by a list of $10$ integer numbers from $0$ to $20$. An example would be $[5, 15, 8, 8, 0, \dots]$.
Is it okay to pass the list as floats to the dense layer, or would that impede the learning process? What is the right way to go about passing integer numbers to the neural network?
","['neural-networks', 'reinforcement-learning', 'environment']",
"Why aren't exploration techniques, such as UCB or Thompson sampling, used in full RL problems?","
Why aren't exploration techniques, such as UCB or Thompson sampling, typically used in bandit problems, used in full RL problems?
Monte Carlo Tree Search may use the above-mentioned methods in its selection step, but why do value-based and policy gradient methods not use these techniques?
","['reinforcement-learning', 'multi-armed-bandits', 'upper-confidence-bound', 'thompson-sampling', 'exploration-strategies']","You can indeed use UCB in the RL setting. See e.g. section 38.5 Upper Confidence Bounds for Reinforcement Learning (page 521) of the book Bandit Algorithms by Csaba Szepesvari and Tor Lattimore for the details.However, compared to $\epsilon$-greedy (widely used in RL), UCB1 is more computationally expensive, given that, for each action, you need to recompute this upper confidence bound for every time step (or, equivalently, action taken during learning).To see why, let's take a look at the UCB1 formula$$
\underbrace{\bar{x}_{j}}_{\text{value estimate}}+\underbrace{\sqrt{\frac{2 \ln n}{n_{j}}}}_{\text{UCB}},
$$
whereSo, at each time step (or new action taken), we need to recompute that square root for each action, which depends on other factors that evolve during learning.So, the higher time complexity than $\epsilon$-greedy is probably the first reason why UCB1 is not so much used in RL, where interaction with the environment can be the bottleneck. You could argue that this recomputation (for each action) also needs to be done in bandits. Yes, it's true, but, in the RL problem, you have multiple states, so you need to compute value estimates for each action in all states (i.e. the full RL problem is more complex than bandits or contextual bandits).Moreover, $\epsilon$-greedy is so conceptually simple that everyone can easily implement it in less than $5$ minutes (though this is not really a problem, given that both are simple to implement).I am currently not familiar with Thompson sampling, but I guess (from some implementations I have seen) it's also not as cheap as $\epsilon$-greedy, where you just need to perform an argmax (can be done in constant time if you keep track of the highest value) or sample a random integer (it's also relatively cheap). There's a tutorial on Thompson sampling here, which also includes a section dedicated to RL, so you may want to read it."
Can we use Q-learning update for policy evaluation (not control)?,"
For policy evaluation purposes, can we use the Q-learning algorithm even though, technically, it is meant for control?
Maybe like this:

Have the policy to be evaluated as the behaviour policy.
Update the Q value conventionally (i.e. updating $Q(s,a)$ using the action $a'$ giving highest $Q(s',a')$ value)
The final $Q(s,a)$ values will reflect the values for the policy being evaluated.

Am I missing something here, given that I have not seen Q-learning being used anywhere for evaluation purposes?
","['reinforcement-learning', 'q-learning', 'policy-evaluation']","For off-policy learning you must have two policies - a behaviour policy and a target policy. If the two policies are the same, then you end up with SARSA, not Q learning.You cannot use Q learning directly for evaluating a fixed target policy, because it directly learns optimal value function as the target policy, regardless of the behaviour policy. Instead you must use another variant of off-policy learning that can evaluate an arbitrary target policy.Your suggested algorithm is:This will not work for evaluating the behaviour policy. If the behaviour policy was stochastic and covered all possible state/action choices, then it will still be Q learning and converge on the optimal value function - maybe very slowly if the behaviour policy did not get to important states very often.The ""trick"" to off-policy is that the environment interaction part uses the behaviour policy to collect data, and the update step uses the target policy to calculate estimated returns. In general for off-policy updates, there can be corrections required to re-weight the estimated returns. However, one nice thing about single-step TD methods is that there are no such additional corrections needed.So this gives a way to do off-policy TD learning, using an approach called Expected SARSA. To use Expected SARSA, you will need to know the distribution of action choices i.e. know $\pi(a|s)$ for the target policy.This is the variant of your description that will work to evaluate your target policy $\pi(a|s)$:Worth noting that Expected SARSA with a target policy of $\pi(s) = \text{argmax}_a Q(s,a)$ is exactly Q learning. Expected SARSA is a strict generalisation of Q learning that allows for learning the value function of any target policy. You may not see it used as much as Q learning, because the goal of learning an optimal value function is more common in practice."
Is Deep SARSA learning a feasible approach?,"
I noticed that SARSA has been rarely used in the deep RL setting. Usually, the training for DQN is done off-policy. I think one of the major reasons for this is due to greater sample efficiency in training due to the reuse of experiences in training off-policy. For SARSA, I would think that at every time step of an update, a stochastic gradient update of that sample would have to be performed and then the sample would have to be thrown away.
The approach while might take longer to train, might still allow the agent to do relatively well. Would deep SARSA implementation perform as well as DQN in terms of final performance (since SARSA would definitely take longer to train)?
","['reinforcement-learning', 'dqn', 'deep-rl', 'off-policy-methods', 'sarsa']",
How is the shape of the anchor boxes predefined in YOLO algorithm?,"
I am not sure if I really understand how anchor boxes are defined. As far as I understand, in YOLO algorithm you define a set of ""good"" shapes (anchor boxes) that may contain the object you are trying to detect for each cell.
However, I don't really understand how do you predefine the shape of these anchor boxes. As far as I have seen, there are examples in which the algorithm outputs bx, by, bh and bw values for each anchor box. Are you actually giving ""freedom"" to the algorithm to define these four values, or somehow is it being defined a fixed ratio between bh and bw for each of the anchor boxes? And how is this ratio defined in the output?
","['object-detection', 'yolo']",
"In DQN, is it possible to make some actions more likely?","
In a general DQN framework, if I have an idea of some actions being better than some other actions, is it possible to make the agent select the better actions more often?
","['reinforcement-learning', 'dqn', 'exploration-strategies']","For single-step Q learning, the behaviour policy can be any stochastic policy without any further adjustment to the update rules.You don't have to use $\epsilon$-greedy based on current Q function approximation, although that is a common choice because it works well in general cases. However, you should always allow some chance of taking all actions if you want the algorithm to converge - if you fixed things so that bad actions were never taken, the agent would never learn that they had low value.Probably the simplest way to use your initial idea of best actions is to write a function that returns your assessment of which action to take, and use that with some probability in preference to a completely random choice. At some point you will also want to stop referencing the helper function (unles it is guaranteed perfect) and use some form of standard $\epsilon$-greedy based on current Q values.I have done similar with a DQN learning to play Connect 4, where the agent would use a look-ahead search function that could see e.g. 7 steps ahead. If that was inconclusive it would use argmax of current Q values. Both these fixed action choices could be replaced, with probability $\epsilon$, with a random action choice to ensure exploration. It worked very well. You could replace the look-ahead search in my example with any function that returned ""best"" actions for any reason.There are some other ways you can skew action selection towards better looking action choices. You could look into Boltzmann exploration or upper confidence bounds (UCB) as other ways to create behaviour policies for DQN."
What are the use-cases of self-replicating neural networks?,"
I started researching the subject of self-replication in neural networks, and unexpectedly I saw that there is not much research on this subject. I should mention I am new in the field of NNs.
This idea seems to be very appealing, but now I am having problems coming up with an actual use case. The paper Neural Network Quine from 2018 seems to be one of the main ones addressing this topic.
So, what are the use-cases of self-replicating neural networks? Why isn't this subject more thoroughly researched?
","['neural-networks', 'applications', 'self-replicating-machines']",
What adapts an algorithm to continuous or to discrete action spaces?,"
Some RL algorithms can only be used for environments with continuous action spaces (e.g TD3, SAC), while others only for discrete action spaces (DQN), and some for both
REINFORCE and other policy gradient variants have the choice of using a categorical policy for discrete actions and a Gaussian policy for continuous action spaces, which explains how they can support both. Is that interpretation completely correct?
For algorithms that learn a Q function or a Q function and a policy, what places this restriction for their use on either discrete or continuous action spaces environments?
In the same regard, if an algorithm suited for discrete spaces is to be tuned to handle continuous action spaces, or vice versa. What does such a configuration involve?
","['reinforcement-learning', 'dqn', 'reinforce', 'continuous-action-spaces', 'discrete-action-spaces']",
Difference between Neural Compute Stick 2 and Google Coral USB for edge computing [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I am trying understand machine learning inferece, and i would like to know what exactly is the difference between Google Coral USB and Movidius Intel Neural Compute Stick 2. From what i could gather the google coral USB speeds up the frame rate, but that doesn't look clear to me. My questions are:
What exactly is the benefit from both of them in units? Like, is it frame rate? prediction speed? Are both visual processing units? And lastly, do i need to keep my neural network in a single computer board for training or can i have it at a cloud?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'inference']","I do not know much about the Neural Compute Stick but I can tell you a little bit about the Coral Edge TPU, since I used it myself (which I think also applies to the Neural Compute Stick).The Edgte TPU is a specialized ASIC which is very efficient at the main calculations (convolutions, relu etc.) for neural network inferencing. That means it cannot be used for training* a neural network but for deploying a neural network in production after it has been trained and optimized/quantized (precision reduced from float32 to int8). But I think you already new that, as it seems from your question.Now to your actual question in terms of speed: You cannot really compare speed of such chip in terms of framerate alone, nor can you call it a visual processing unit or not. The google coral is a general ASIC that is very fast at doing convolutions/relus etc. For what you gonna use your neural network (like Image recognition, or maybe stock predicitons) is complitly up to you, and especially up to your neural network for which task it was trained for. The only limitation you have here is regarding the supported layer operations. E.g. it is not possible to do operations like 3D convolutions or some fancy new non-linear activation functions (There was an overview of supported operations in the docs that I cannot find right now).Furthermore the framerate also depends on your NN architecture, image resolution etc. so a comparison here is completly misleading. If you want to have a general indication of speed look at how many int8 operations it can handle per second (TOPS) also under what energy consuption (Watts), if you care.The main advantage of this unit compared to a GPU that is usually used for training+inferencing is much lower energy consumption and unit cost. With roughly 4 Watts on the Edge TPU (as far as I can remember) compared to e.g. 250 Watts GPU. Energy consumption also dictates the necessary cooling solution, which can just be a passive for the Edge Tpu in many cases. Regarding the unit costs, I guess you can easily sees that yourself, if you keep in mind that you can get ""nearly"" similar inferencing speeds than with a full workstation class GPU. Furthermore, such unit has a much smaller formfactor and weight, which makes it perfect for applications at the edge. (I should also add that the efficiency gain is also a lot due to the precision reduction to int8 (quantization). Quantization is also possible on GPUs.)And lastly, do i need to keep my neural network in a single computer board for training or can i have it at a cloud?I am not quite sure what you mean with this. The general workflow would look like this: you use a GPU for training your neural network, after training you optimize/quantize your network, and lastly you deploy your neural network to the ""computer board"" of your choice that just needs to have a CPU and the Edge TPU and do inferencing operations/predictions for your task.*Transfer learning is possible of e.g. the last layer. But no backpropagation for full NN training."
Keras DQN Model with Multiple Inputs and Multiple Outputs [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



I am trying to create a DQN agent where I have 2 inputs: the agent's position and a matrix of 0s and 1s.
The output is composed of the agent's new chosen position, a matrix of 0s and 1s (different from the input matrix), and a vector of values.
The first input is fed to an MLP network, the second input (matrix) is fed to a convolutional layer, then their outputs are fed to a FC network, or at least that's the idea.
This is my attempt so far, having this tutorial as a reference.
Here is the code:
First, create the MLP network
def create_mlp(self, arr, regress=False): # for the position input
        # define MLP network
        print(""Array"", arr)
        model = Sequential()
        model.add(Dense(env.rows * env.cols, input_shape=(len(arr)//2, len(arr)), activation=""relu""))
        model.add(Dense((env.rows * env.cols)//2, activation=""relu""))
        
        # check to see if the regression node should be added
        if regress:
            model.add(Dense(1, activation=""linear""))
            
        # return our model
        return model

Then, the CNN
def create_cnn(self, width, height, depth=1, regress=False): # for the matrix
        # initialize the input shape and channel dimension
        inputShape = (height, width, depth)
        output_nodes = 6e2
        
        # define the model input
        inputs = Input(shape=inputShape)

        # if this is the first CONV layer then set the input
        # appropriately
        x = inputs
        
        input_layer = Input(shape=(width, height, depth))
        conv1 = Conv2D(100, 3, padding=""same"", activation=""relu"", input_shape=inputShape) (input_layer)
        pool1 = MaxPooling2D(pool_size=(2,2), padding=""same"")(conv1)
        flat = Flatten()(pool1)
        hidden1 = Dense(200, activation='softmax')(flat) #relu

        batchnorm1 = BatchNormalization()(hidden1) 
        output_layer = Dense(output_nodes, activation=""softmax"")(batchnorm1) 
        output_layer2 = Dense(output_nodes, activation=""relu"")(output_layer) 
        output_reshape = Reshape((int(output_nodes), 1))(output_layer2)
        model = Model(inputs=input_layer, outputs=output_reshape)

        # return the CNN
        return model

Then, concatenate the two
def _build_model(self):
        # create the MLP and CNN models
        mlp = self.create_mlp(env.stateSpacePos)
        cnn = self.create_cnn(3, len(env.UEs))
        
        # create the input to our final set of layers as the *output* of both
        # the MLP and CNN
        combinedInput = concatenate([mlp.output, cnn.output])
        
        # our final FC layer head will have two dense layers, the final one
        # being our regression head
        x = Dense(len(env.stateSpacePos), activation=""relu"")(combinedInput)
        x = Dense(1, activation=""linear"")(x)
        
        # our final model will accept categorical/numerical data on the MLP
        # input and images on the CNN input, outputting a single value
        model = Model(inputs=[mlp.input, cnn.input], outputs=x)
        
        opt = Adam(lr=self.learning_rate, decay=self.epsilon_decay)
        model.compile(loss=""mean_absolute_percentage_error"", optimizer=opt)
        
        print(model.summary())
        
        return model

I have an error:
A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 32, 50), (None, 600, 1)]

The line of code that gives the error is:
combinedInput = concatenate([mlp.output, cnn.output])

This is the MLP summary

And this is the CNN summary

I'm a beginner at this, and I'm not where my mistakes are, the code does not work obviously but I do not know how to correct it.
","['convolutional-neural-networks', 'python', 'tensorflow', 'dqn', 'multilayer-perceptrons']","Firstly, concatenate only works on identical output shape of the axis. Otherwise, the function will not work.
Now, your function output size is (None, 32, 50) and (None, 600, 1). Here, '32' and '600' must be same when you want to concatenate.I would like to suggest some advice based on your problem. You can flatten both of them first and then concatenate. Because you need to flatten feature to use dense layer later.And just remove the reshape layer in create_cnn function.
(output shape should be = (None, 600)).then concatenate two modelcombinedInput = concatenate([mlp.output, cnn.output])  ## output shape =(None, 2200)Later you can just use Dense layer as your code. I don't know how can you used dense (next to concatenate layer) without flatten the feature in create_mlp function.Your code should work this way.
You can read this simple one for better understanding."
How to interpret the variance calculation in a Guassian process,"
I answered another question here about the mean prediction of a GP, but I have a hard time coming up with an intuitive explanation of the variance prediction of a GP.
Thew specific equation that I am speaking of is equation 2.26 in the Gaussian process book...
$$
\mathbb{V}[f_*] = k(x_*, x_*) - k_{x*}^TK_{xx}^{-1}k_{x*}
$$
I have a number of questions about this...

if $k(x_*, x_*)$ is the result of the kernel function with a single point $x_*$, then won't this value always be 1 (assuming an RBF kernel) since the kernel will give 1 for a covariance with itself ($k(x, x) =\exp\{-\frac{1}{2}|| x - x ||^2\}$)

If the kernel value $k(x_*, x_*)$ is indeed one for any single arbitrary point, then how can I interpret the last multiplication on the RHS? $K_{xx}^{-1}k_{x*}$ is the solution to $Ax = b$, which is the vector which $K_{xx}$ projects into $k_{x*}$, but then my intuition breaks down and I cannot explain anymore.

If the kernel value $k(x_*, x_*)$ is indeed one for any single arbitrary point, then can we view the whole term as the prior variance being reduced by the some sort of similarity between the test point and the training points?

Is it every possible for this variance to be greater than 1? Or is the prior variance of 1 seen as the maximum, which can only be reduced by observing more data?


","['linear-algebra', 'gaussian-process']",
What are the implications of storing the alternative situation (that could have been experienced) in the replay buffer?,"
Consider an environment where there are 2 outcomes (e.g. dead and alive) and a discrete set of actions. For example, a game where the agent has 2 guns $A$ and $B$ to shoot a monster (the monster dies only if the correct gun is used).
Let's say we store the experience $e_1 = (s,a_1, r_1, s'_1)$ in the replay buffer $D$, where

$s$: we have the monster to kill
$a_1$: choose and use gun $A$
$r_1$: $-1$ (the wrong gun)
$s'_1$: monster is alive

But we also save the alternative situation $e_2 = (s, a_2, r_2, s'_2)$ in the replay buffer $D$, where

$s$: the same state (we have the monster)
$a_2$: choose and use gun $B$
$r_2$: $(-1) * r = 1$
$s'_2$: the monster is dead

I can't find a topic about this technique, or don't know what to look for.
","['reinforcement-learning', 'dqn', 'deep-rl', 'experience-replay']",
Initialising DQN with weights from imitation learning rather than policy gradient network,"
In AlphaGo, the authors initialised a policy gradient network with weights trained from imitation learning. I believe this gives it a very good starting policy for the policy gradient network. the imitation network was trained on labelled data of (state, expert action) to output a softmax policy denoting the probability of actions for each state.
In my case, I would like to use weights learnt from the imitation network as initial starting weights for a DQN. Initial rewards from running the policy are high but start to decrease (for a while) as the DQN trains and later increases again.
This could suggest that the effect of initialising weights from the imitation network has little effect, since it kind of undo's the effect of initialising weights from the imitation network.
","['reinforcement-learning', 'dqn', 'deep-rl', 'alphago', 'imitation-learning']","My understanding is that you are first training a policy network using imitation learning. Then you are adjusting that trained network in some way to be a value network for DQN. The most obvious change would be to remove softmax activation whilst keeping the network layer sizes identical. This would then present Q estimates for all actions from any given state.The initial estimates would not be trained Q values though, they would be the ""preferences"" or the logits for probabilities to support a near optimal action choice. The main thing that will be likely in the new network is that for the one near optimal action choice, the network would predict the highest action value. As you derive the target policy by taking the maximising action, initially this looks good. However, the problem is that the Q values that this network predicts can have little to no relation to the real expected returns experienced by the agent under the target policy.Initial rewards from running the policy are high but start to decrease (for a while) as the DQN trains and later increases again.I think what is happening is that initially the greedy policy derived from your Q network is very similar to the policy learned during imitation learning. However, the value estimates are very wrong. This leads to large error values, large corrections needed, and radical changes to network weights throughout in order to change the network from an approximate policy function to an approximate action value function. The loss of performance occurs because there is not a smooth transition between the two very different functions that also maintains correct maximising actions.I don't think this can be completely fixed. However you might get some insight into potential work-arounds by considering that you are not just doing imitation learning here. Instead you are performing both imitation learning (to copy a near optimal policy) and transfer learning (to re-use network weights on a related task).Approaches that help with transfer learning may also help here. For instance, you could freeze the layers closer to input features, or reduce the learning rate for those layers. You do either of these things on the assumption that the low-level derived features (in the hidden layers) that the first network has learned are still useful for the new task."
Is average pooling equivalent to a strided convolution with a specific constant kernel?,"
It seems to me that average pooling can be replaced by a strided convolution with a constant kernel. For instance, a 3x3 pooling would be equivalent to a strided convolution (of stride $3$) with a $3 \times 3$ matrix of constants, with each entry being $\frac{1}{9}$.
However, I haven't found any mention of this fact online (perhaps it's too trivial of an observation)? Why then are explicit pooling layers needed if they can be realized by convolutions?
","['neural-networks', 'convolutional-neural-networks', 'implementation', 'convolution', 'pooling']","Is average pooling equivalent to a strided convolution with a specific constant kernel?Yes.Why then are explicit pooling layers needed if they can be realized by convolutions?It is probably because the convolution is more expensive than the usual/natural implementation (i.e. just summing and then dividing).To see why, let's consider your example. If you implement pooling with a convolution (or cross-correlation), we would need to perform $9$ multiplications, then $8$ summations, for a total of $17$ operations. If we implement pooling as usual, we would need to perform $8$ summations and $1$ division (or multiplication), for a total of $9$ operations.Moreover, convolution may also be more prone to numerical instability (multiplications of numbers in the range $[0, 1]$ are not nice), but I am not completely sure about this, given that we multiply always by the same numbers."
Are there any downsides of using a fixed seed for a neural network's weight initialization?,"
For example, if we set the random seed to be 0, will we run into any problems? For example, maybe for seed 0, we can only reach a certain training error, but other seeds will converge to a much lower error
I'm specifically concerned about supervised learning on point cloud data, but curious about whether it matters in general whenever you use a neural network.
","['neural-networks', 'training', 'weights-initialization']",
How is MuZero's second binary plane for chess defined?,"
From the MuZero paper (Appendix E, page 13):

In chess, 8 planes are used to encode the action. The first one-hot plane encodes which position the piece was moved from. The next two planes encode which position the piece was moved to: a one-hot plane to encode the target position, if on the board, and a second binary plane to indicate whether the target was valid (on the board) or not. This is necessary because for simplicity our policy action space enumerates a superset of all possible actions, not all of which are legal, and we use the same action space for policy prediction and to encode the dynamics function input. The remaining five binary planes are used to indicate the type of promotion, if any (queen, knight, bishop, rook, none).

Is the second binary plane all zeros or all ones?  Or, something else?
How is it known if the move is off the board?  For my game, I know if it is a legal move on the board, but do not know if the move is off the board.
","['reinforcement-learning', 'deep-rl', 'papers', 'muzero']",
"Which ML approach could determine that a number greater than 5 is not prime, knowing that a number is not prime if it ends with an even digit or 5?","
I have started studying ML just a short while ago, so that my questions will be very elementary. That being so, if they are not welcome, just tell me and I'll stop asking them.
I gave myself a homework project which is to make an ML algorithm be able to learn that, if the last digit of a number $n$ is $0, 2, 4, 5, 6$ or $8$, then it cannot be a prime, provided $n > 5$. Note that, if a number $n$ ends with $0, 2, 4, 6, 8$, then it is even, so it is divisible by $2$, hence not prime. Similarly, numbers ending in $5$ are divisible by $5$, so they cannot be prime.
Which ML approach should I choose to solve this problem? I know that I don't need ML to solve this problem, but I am just trying to understand which ML approach I could use to solve this problem.
So far, I have only learned about two ML approaches, namely linear regression (LR) and $k$-nearest neighborhoods, but they both seem inappropriate in this case since LR seems to be a good choice in finding numerical relations between input and output data and KNN seems to be good at finding clusters, and ""primality"" has neither of these characteristics.
","['machine-learning', 'classification', 'homework', 'primality-test']",
