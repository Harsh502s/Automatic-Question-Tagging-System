Head,Body,Tags,First Answer
Are there any rules of thumb for having some idea of what capacity a neural network needs to have for a given problem?,"
To give an example. Let's just consider the MNIST dataset of handwritten digits. Here are some things which might have an impact on the optimum model capacity:

There are 10 output classes
The inputs are 28x28 grayscale pixels (I think this indirectly affects the model capacity. eg: if the inputs were 5x5 pixels, there wouldn't be much room for varying the way an 8 looks)

So, is there any way of knowing what the model capacity ought to be? Even if it's not exact? Even if it's a qualitative understanding of the type ""if X goes up, then Y goes down""?
Just to accentuate what I mean when I say ""not exact"": I can already tell that a 100 variable model won't solve MNIST, so at least I have a lower bound. I'm also pretty sure that a 1,000,000,000 variable model is way more than needed. Of course, knowing a smaller range than that would be much more useful!
","['neural-networks', 'computational-learning-theory', 'regularization', 'vc-dimension', 'capacity']","Rather than providing a rule of thumb (which can be misleading, so I am not a big fan of them), I will provide some theoretical results (the first one is also reported in paper How many hidden layers and nodes?), from which you may be able to derive your rules of thumb, depending on your problem, etc.The paper Learning capability and storage capacity of two-hidden-layer feedforward networks proves that a 2-hidden layer feedforward
network ($F$) with $$2 \sqrt{(m + 2)N} \ll N$$ hidden neurons can learn any $N$ distinct samples $D= \{ (x_i, t_i) \}_{i=1}^N$ with an arbitrarily small error, where $m$ is the required number of output neurons. Conversely, a $F$ with $Q$ hidden neurons can store at least $\frac{Q^2}{4(m+2)}$ any distinct data $(x_i, t_i)$ with
any desired precision.They suggest that a sufficient number of neurons in the first layer should be $\sqrt{(m + 2)N} + 2\sqrt{\frac{N}{m + 2}}$ and in the second layer should be $m\sqrt{\frac{N}{m + 2}}$. So, for example, if your dataset has size $N=10$ and you have $m=2$ output neurons, then you should have the first hidden layer with roughly 10 neurons and the second layer with roughly 4 neurons. (I haven't actually tried this!)However, these bounds are suited for fitting the training data (i.e. for overfitting), which isn't usually the goal, i.e. you want the network to generalize to unseen data.This result is strictly related to the universal approximation theorems, i.e. a network with a single hidden layer can, in theory, approximate any continuous function.There are also the concepts of model selection and complexity control, and there are multiple related techniques that take into account the complexity of the model. The paper Model complexity control and statistical learning theory (2002) may be useful. It is also important to note regularisation techniques can be thought of as controlling the complexity of the model [1].You may also want to take a look at these related questionsHow to choose the number of hidden layers and nodes in a feedforward neural network?How to estimate the capacity of a neural network?(I will be updating this answer, as I find more theoretical results or other useful info)"
What is the difference between sensitivity analysis and parameter tuning?,"
I tried different values of genetic algorithm operators:

many crossover rates from 20% to 80%
many crossover rates from 1% to 20%
varying the population size

The study of different parameter values is called quantitative parameter tuning or sensitivity analysis. What is the difference between the two terms?
","['comparison', 'terminology', 'genetic-algorithms', 'evolutionary-algorithms', 'hyperparameter-optimization']",
DQN layers when state space and action space are multi dimensional,"
I have built my own RL environment, where a state is composed of two elements: the agent's position and a matrix of 0s and 1s (1 if a user has requested a service from the agent, 0 otherwise); an action is composed of 3 elements: the movement the agent chooses (up, down, left or right), a matrix of 0s and 1s (1 if a resource has been allocated to a user, 0 otherwise), and a vector representing the allocation of another type of resource (the vector contains the values allocated to the users).
I am currently trying to build a Deep Q Learning agent, I am a bit confused however as to what model (example Sequential), what type of layers (example Dense layers), how many layers, what activation mode I should use, and what the state and action sizes are. (Taking this code as a reference cartpole dqn agent)
I also do not know what my inputs and outputs should be.
The examples I have come across are rather simple and I don't know how to approach setting it all up for my agent.
","['deep-learning', 'python', 'keras', 'deep-rl', 'deep-neural-networks']","This is not necessarily the only way to do this but it would be the approach I'd take.Assuming your agents position is a vector in $\mathbb{R}^d$, then I would have the network take as input this position vector and pass it through a fully connected layer. I would also take as input the matrix and pass it through a convolutional layer(s) and flatten the output so it is now also a vector in $\mathbb{R}^{d'}$. I would then concatenate these together so you have a vector in $\mathbb{R}^{d + d'}$ and pass this through some fully connected layers as usual.As for how many layers and which activation to use this is something you'll have to do by trial and error as this really is problem specific.Your output would typically be a score for each of the action combinations, though as one of your outputs is a matrix this could potentially make things expensive to compute as you would need $2^{n \times m}$ binary representations just for the matrix ($n$ is the number of rows and $m$ is the number of columns)."
How does the implementation of the VAE's objective function equate to ELBO?,"
For a lot of VAE implementations I've seen in code, it's not really obvious to me how it equates to ELBO.
$$L(X)=H(Q)-H(Q:P(X,Z))=\sum_ZQ(Z)logP(Z,X)-\sum_ZQ(Z)log(Q(Z))$$
The above is the definition of ELBO, where $X$ is some input, $Z$ is a latent variable, $H()$ is the entropy. $Q()$ is a distribution being used to approximate distribution $P()$, which in the above case both $P()$ and $Q()$ are discrete distributions, because of the sum.
A lot of the times when VAEs are built for reconstructing discrete data types, let's say for example an image, where each pixel can be black or white or $0$ or $1$. The main steps of a VAE that I've seen in code are as follows:

$\text{Encoder}(Y) \rightarrow Z_u, Z_{\sigma}$
$\text{Reparameterization Trick}(Z_\mu, Z_\sigma) \rightarrow Z$
$\text{Decoder}(Z) \rightarrow \hat{Y}$
$L(Y)= \text{CrossEntropy}(\hat{Y}, Y) - 0.5*(1+Z_{\sigma}-Z_{\mu}^2-exp(Z_\sigma))$

where

$Z$ represents the latent embedding of the auto-encoder
$Z_\mu$ and $Z_\sigma$ represent the mean and standard deviation for sampling for $Z$ from a Gaussian distribution.
$Y$ represents the binary image trying to be reconstructed
$\hat{Y}$ represents its reconstruction from the VAE.

As we can see from the ELBO, it's the entropy of the latent distribution being learned, $Q()$, which is a Gaussian, and the cross entropy of the latent distribution being learned $Q()$ and the actual distribution $P()$ with $Z$ intersected with $X$.
The main points that confuse me are

how $\text{CrossEntropy}(\hat{Y}, Y)$ equates to the CE of the distribution for generating latents and its Gaussian approximation, and
how $(0.5*(1+Z_{\sigma}-Z_{\mu}^2-exp(Z_\sigma)))$ equates to the entropy

Is it just assumed the CE of $Y$ with $\hat{Y}$ also leads to the CE of the latent distribution with it's approximation, because they're part of $\hat{Y}$'s generation? It still seems a bit off because you're getting the cross entropy of $Y$ with it's reconstruction, not the Gaussian distribution for learning latents $Z$.
Note: $Z_\sigma$ is usually not softplused to be strictly positive as required by a Gaussian distribution, so I think that's what $exp(Z_\sigma)$ is for.
","['implementation', 'variational-autoencoder', 'cross-entropy', 'evidence-lower-bound', 'categorical-crossentropy']","I don't want to think about the correctness of your supposed ELBO equation now. Nevertheless, it's true that the ELBO can be rewritten in different ways (e.g. if you expand the KL divergence below, by applying its definition, you will end up with a different but equivalent version of the ELBO). I will use the most common (and definitely most intuitive, at least to me) one, which you can find in the paper that originally introduced the VAE, which you should use as a reference, when you are confused (although that paper may require at least 2 readings before you fully understand it).Here's the most common form of the ELBO (for the VAE), which immediately explains the common VAE implementations (including yours):$$\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)=
\underbrace{
\color{red}{
-D_{K L}\left(q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \| p_{\boldsymbol{\theta}}(\mathbf{z})\right)
}
}_{\text{KL divergence}}
+
\underbrace{
\color{green}{
\mathbb{E}_{q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)}\left[\color{blue}{ \log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right)}\right]
}
}_{\text{Expected log-likelihood}},
$$where$p_{\boldsymbol{\theta}}(\mathbf{z})$ is the prior over $\mathbf{z}$ (i.e. the latent variable); in practice, the prior is not actually parametrized by $\boldsymbol{\theta}$ (i.e. it's just a Gaussian with mean zero and variance one, or whatever, depending on your assumptions about $\mathbf{z}$!), but in the paper they assume that $\mathbf{z}$ depends on $\boldsymbol{\theta}$ (see figure 1).$q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)$ is the encoder parametrized by $\boldsymbol{\phi}$$p_{\boldsymbol{\theta}}$ is the decoder, parametrized by $\boldsymbol{\theta}$If you assume that $p_{\boldsymbol{\theta}}(\mathbf{z})$ and $q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) $ are Gaussian distributions, then it turns out that the KL divergence has an analytical form, which is also derived in appendix B of the VAE paper (here is a detailed derivation)$$
\color{red}{
-D_{K L}\left(q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \| p_{\boldsymbol{\theta}}(\mathbf{z})\right)}
= 
\color{red}{
\frac{1}{2} \sum_{j=1}^{J}\left(1+\log \left(\left(\sigma_{j}\right)^{2}\right)-\left(\mu_{j}\right)^{2}-\left(\sigma_{j}\right)^{2}\right)
}
$$Hence this implementation, which should be equivalent to your term $0.5*(1+Z_{\sigma}-Z_{\mu}^2-exp(Z_\sigma))$The expected log-likelihood is actually an expectation, so you cannot compute it exactly, in general. Hence, you can approximate it with Monte Carlo sampling (aka sampling averages: remember the law of large numbers?). More concretely, if you assume that you have a Bernoulli likelihood, i.e. $p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right)$ is a Bernoulli, then its definition is (again from the VAE paper, Appendix C.1)$$
\color{blue}{ \log p(\mathbf{x} \mid \mathbf{z})}= \color{blue}{ \sum_{i=1}^{D} x_{i} \log y_{i}+\left(1-x_{i}\right) \cdot \log \left(1-y_{i}\right) }\tag{1}\label{1},
$$
where $\mathbf{y}$ is the output of the decoder (i.e. the reconstruction/generation of the original input).This formula should be very familiar to you if you are familiar with the cross-entropy. In fact, minimizing the cross-entropy is equivalent to maximizing the log-likelihood (this may still be confusing because of the flipped signs in the ELBO above, but just remember that maximizing a function is equivalent to minimizing its negative!). Hence this loss.To answer/address some of your questions/doubts directlyhow $(0.5*(1+Z_{\sigma}-Z_{\mu}^2-exp(Z_\sigma)))$ equates to the entropy?I answered this above. That's just the analytical expression for the KL divergence (by the way, the KL divergence is also known as relative entropy). See this answer for a derivation.It still seems a bit off because you're getting the cross entropy of $Y$ with it's reconstruction, not the Gaussian distribution for learning latents $Z$.As you can see from the definition of the ELBO above (from the VAE paper), the expected log-likelihood is, as the name suggests, an expectation, with respect to the encoder (i.e. the Gaussian, in case you choose a Gaussian). However, the equivalence is between the log-likelihood (which is the term inside the expectation) and the cross-entropy, i.e. once you have sampled from the encoder, you just need to compute the term inside the expectation (i.e. the cross-entropy). Your term $\text{CrossEntropy}(\hat{Y}, Y)$ represents the CE but after you have sampled a latent variable from the encoder (or Gaussian), otherwise, you could not have obtained the reconstruction $\hat{Y}$ (i.e. the reconstruction depends on this latent variable, see figure 1).In equation \ref{1}, note that there is no expectation. In fact, in the implementations, you may just sample once from the encoder, and then immediately compute the ELBO. I have seen this also in the implementations of Bayesian neural networks (basically, normal neural networks with the same principles of the VAE). However, in principle, you could sample multiple times from the Gaussian encoder, compute \ref{1} multiple times, then average it, to compute a better approximation of the expected log-likelihood.Hopefully, some of the information in this answer is clear. Honestly, I don't have much time to write a better answer now (maybe I will come back later). In any case, I think you can find all answers to your questions/doubts in the VAE paper (although, as I said, you may need to read it at least twice to understand it).By the way, the simplest/cleanest implementation of the VAE that I have found so far is this one."
Bias-variance tradeoff and learning curves for non-deep learning models,"
I am following a course on machine learning and am confused about the bias-variance trade-off relationship
to learning curves in classification.
I am seeing some conflicting information online on this.
The scikit-learn learning curve looks like the top 2 curves here: 
(source: scikit-learn.org)
What I don't understand is: how do we read bias from this? If we look at this image
where each blue dot is a model. I think the bias would be the green curve being high. But high bias
indicates underfitting, right? So shouldn't the red curve be high then too?

High variance would be the gap between green and red, is this correct?
My question is how do the red and green curves relate to underfitting and overfitting,
and how do learning curves fit with the figure with the concentric circles? Is bias purely related to the red curve, or is a model with a low validation score and high train score also a high bias model?
","['bias-variance-tradeoff', 'learning-curve']",
DQN rgb input channels problem using pytorch,"
I've been trying to learn about CNN's and reinforcement learning and I found this project to play with: https://github.com/adityajn105/flappy-bird-deep-q-learning
I've been trying to change the code to work with RGB input instead of grayscale. Pre-precosessing part is fine, but I'm having a problem with state and next_state I guess because they're deque and when deque is appended shape is (4,H,W) because it's appended 4 times (4 frames). Problem I'm having is when I append frames to deque which are RGB it becomes something like (4,H,W,3). I tried some stuff that came to mind and that I googled and read about online, but I still had problems with dimensions. What should be done so that it works with RGB instead of grayscale?
","['convolutional-neural-networks', 'dqn', 'double-dqn']",
How to calculate probability from fuzzy membership grade?,"
Suppose we have the fuzzy membership grade for a person $x$ with a set $S = \text{set of tall people}$ be $0.9$, i.e. $\mu_S(x)=0.9$.
Does this mean that the probability of person $x$ being tall is $0.9$?
","['probability', 'fuzzy-logic']","No, you can't extract any probability from a fuzzy membership grade. The uncertainty expressed by fuzzy logic is about partial truth, not about probability. $ \mu_S(x) = 0.9 $ doesn't mean that ""$ x $ is tall"" is true with a probability of 0.9, but that ""$ x $ is tall"" is 90% true (notice the difference in semantics). You have to think about fuzzy logic as an extension of logic (as its name implies), rather than an extension of probability.It's true, however, that fuzzy logic is flexible and lets you define how the membership grades are combined in logic formulae, to the extend you can replicate probability theory within the fuzzy logic framework. Wikipedia has a good overview on this: https://en.wikipedia.org/wiki/Fuzzy_logic#Comparison_to_probability.However, please understand that, in general, fuzzy membership $ \neq $ probability. How we come up with the fuzzy membership grade is subjective and application-dependent. Conversely, probabilities have a well-defined and unambiguous interpretation. The point of being fuzzy is to replicate our reasoning process which, even if is not necessarily formal and rigorous, is often very accurate. To do so it needs a set of (admittedly arbitrary) rules on how to calculate the ""truthfulness"" of logical formulae. This may turn to be very useful in applications where manipulating probabilities or coming up with them in the first place is not tractable."
Explain the difference in graphical patterns between discriminator fake loss and generator loss in GAN,"
In GAN (generative adversarial networks), let us take ""binary cross-entropy"" as the loss function for discriminator $$(overall \; loss = -\sum log(D(x_i)) -\sum log(1-D(G(z_i))) $$
$$ where \; x_i = real \; image \; pixel \; matrix$$
$$ and \; z_i = a \; vector \; from \; latent \; space$$.
Let us define discriminator real loss and fake loss:
$$ d_{fake \; loss} = -\sum log(1-D(G(Z)))$$
$$ d_{real \; loss} = -\sum log(D(x))$$
$$ d_{fake \; loss} \; implies \; discriminator \; loss \; against \; fake \; images$$
$$ d_{real \; loss} \; implies \; discriminator \; loss \; against \; real \; images$$
Generator Loss :
$$ g_{loss} = -\sum log(D(G(z_i)))$$
Since the functions are similar, we should be expecting some similarity in graphical patterns (i.e since none of the functions are inherently oscillatory, I expect that if one comes out to be oscillatory, the other one should be the same as well). But, If you refer to chapter 10 of the book ""Generative Adversarial Networks with python by Jason Brownlee"", we find some difference. The following are the graphs published in the book


Can anyone explain the difference in the plots between discriminator fake loss and generator loss (mathematically)?
","['objective-functions', 'generative-adversarial-networks']",
Why do we minimise the loss between the target Q values and 'local' Q values?,"
I have a question regarding the loss function of target networks and current (online) networks. I understand the action value function. What I am unsure about is why we seek to minimise the loss between the qVal for the next state in our target network and the current state in the local network. The Nature paper by Mnih et al. is well explained, however, I am not getting from it the purpose of the above. Here is my training portion from a script I am running:
for state, action, reward, next_state, done in minibatch:
    target_qVal = self.model.predict(state)

    # print(target_qVal)

    if done:
        target_qVal[0][action] = reward #done
    else:
        # predicted q value for next state from target model
        pred = self.target_model.predict(next_state)[0]
        target_qVal[0][action] = reward + self.gamma * np.amax(pred)

    # indentation position?
    self.model.fit(np.array(state), 
                   np.array(target_qVal), 
                   batch_size=batch_size,
                   verbose=0, 
                   shuffle=False, 
                   epochs=1)

I understand that the expected return is the immediate reward plus the cumulative sum of discounted rewards looking into the future $s'$ (correct me if I'm wrong in my understanding) when following a given policy.
My fundamental misunderstanding is the loss equation:
$$L = [r + \gamma \max Q(s',a'; \theta') - Q(s,a; \theta)],$$
where $\theta'$ and $\theta$ are the weights of the target and online neural networks, respectively.
Why do we aim to minimize the Q value of the next state in the target model and the Q value of the current state in the online model?
A bonus question would be, in order to collect $Q(s,a)$ values for dimensionality reduction (as in Mnih et al t-sne plot), would I simply collect the target_qVal[0] values during training and feed them into a list after each step to accumulate the Q values over time?
","['reinforcement-learning', 'dqn', 'deep-rl', 'double-dqn']",
What are ways to learn a classifier for labelling a series of images rather than individual images?,"
... and how do I reword my question in the title?
I have a dataset where each ""instance"" has a ""series"" of multiple photos taken from different angles. I need to classify each instance as a 0 or a 1.
A little over half of the images in each series probably do not contain the information required for a classification. Only some of the images are taken from an angle where the relevant clue is visible.
For training I have many such series and they are labelled at a series level, but not at an image level.
My current approach is to use a standard architecture like ResNet. I pass each image through the CNN then I combine the features by averaging, then put that through a sigmoid activated layer. I'm concerned that the network won't be able to learn because the ""clue"" is so buried among everything else.
Questions:

Is there a better/standard way to do this? Would going RNN help? What if the images are not really in a  meaningful sequence?

If my way is good, is arithmetic averaging the right way to combine the features?

Would it be worth spending the time to label each image as ""has positive clue""/""does not have positive clue""? Should I add a ""not possible to tell""? What if it is possible to tell but it's just humans that can't tell?


","['convolutional-neural-networks', 'classification', 'image-recognition']",
"In Soft Actor Critic, why is the action sampled from current policy instead of replay buffer on value function update?","
While reading the original paper of Soft Actor Critic, I came across on page number 5, under equation (5) and (6)
$$
J_{V}(\psi)=\mathbb{E}_{\mathbf{s}_{t} \sim \mathcal{D}}\left[\frac{1}{2}\left(V_{\psi}\left(\mathbf{s}_{t}\right)-\mathbb{E}_{\mathbf{a}_{t} \sim \pi_{\phi}}\left[Q_{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log \pi_{\phi}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right]\right)^{2}\right]
\tag{5}\label{5}
$$
$$
\hat{\nabla}_{\psi} J_{V}(\psi)=\nabla_{\psi} V_{\psi}\left(\mathbf{s}_{t}\right)\left(V_{\psi}\left(\mathbf{s}_{t}\right)-Q_{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log \pi_{\phi}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)
\tag{6}\label{6}
$$
The following quote:

where the actions are sampled according to the current policy, instead of the replay buffer

In the context of deriving the formulation of the (estimated) gradient for the value function square residual error (Equation 5 in the paper)
I'm having a hard time understanding why they use the action sampled from the current policy instead of the replay buffer. My intuition tells me that this is because SAC is an off policy Reinforcement Learning algorithm, and Q-learning uses $\max Q$ in one-step Q-value function update (to keep it off-policy), but why would sampling one action from the current policy still make it off-policy?
I first asked a friend of mine (researcher in RL) and the answer I got was

""If the action is sampled with the current policy given any state the update is on-policy.""

I've checked SpinningUpRL by OpenAI's explanation of SAC but they only make it more clear which action is sampled from the current policy, and which one is from the replay buffer, but does not specify why.
Does this have anything to do with the stochastic policy? Or the entropy term in the update equation?
So I'm still quite confused. Link/references to explanation are also appreciated!
","['reinforcement-learning', 'policy-gradients', 'value-functions', 'off-policy-methods', 'soft-actor-critic']",
Is automated feature engineering a path to general AI?,"
I recently came across the featuretools package, which facilitates automated feature engineering.  Here's an explanation of the package:
https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219

Automated feature engineering aims to help the data scientist by
automatically creating many candidate features out of a dataset from
which the best can be selected and used for training.

I only have limited experience with ML/AI techniques, but general AI is something that I'd been thinking about for a while before exploring existing ML techniques.  One idea that kept popping up was the idea of analyzing not just raw data for patterns but derivatives of data, not unlike what featuretools can do.  Here's an example:

It's not especially difficult to see the figure above as two squares, one that is entirely green and one with a blue/green horizontal gradient.  This is true despite the fact that the gradient square is not any one color and its edge is the same color as the green square (i.e., there is no hard boundary).
However, let's say that we calculate the difference between each pixel and the pixel to its immediate left.  Ignoring for a moment that RGB is 3 separate values, let's call the difference between each pixel column in the gradient square X.  The original figure is then transformed into this, essentially two homogenous blocks of values.  We could take it one step further to identify a hard boundary (applying a similar left-to-right transformation again) between the two squares.

Once a transformation is performed, there should be some way to assess the significance of the transformation output.  This is a simple and clean example where there are two blocks of homogenous values (i.e., the output is clearly not random).  If it's true that our minds use any kind of similar transformation process, the number of transformations that we perform would likely be practically countless, even in brief instances of perception.
Ultimately, this transformation process could facilitate finding the existence of order in data.  Within this framework, perhaps ""intelligence"" could be defined simply as the ability to detect order, which could require applying many transformations in a row, a wide variety of types of transformations, an ability to apply transformations with a high probability of finding something significant, an ability to assess significance, etc.
Just curious if anyone has thoughts on this, if there are similar ideas out there beyond simple automated feature engineering, etc.
","['superintelligence', 'feature-selection', 'feature-engineering']",
How can I improve the performance on unseen data for semantic segmentation using an auto-encoder?,"
I am using simple autoencoders for the task of semantic segmentation on the VOC2012 dataset. I am currently using a simple autoencoder based model. It is trained on adam optimizer with cross-entropy loss on 21 classes 0 - 20. You can find the code here: https://github.com/parthv21/VOC-Semantic-Segmentation
My Architecture:
   self.encoder = nn.Sequential(
        nn.Conv2d(3, 64, 3, stride=2, padding=1),
        nn.LeakyReLU(),
        nn.Conv2d(64, 128, 3, stride=2, padding=1),
        nn.LeakyReLU(),
        nn.Conv2d(128, 256, 3, stride=2, padding=1),
        nn.LeakyReLU(),
        nn.Conv2d(256, 512, 3, stride=2, padding=1),
    )

    self.decorder = nn.Sequential(
        nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1),
        nn.LeakyReLU(),
        nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),
        nn.LeakyReLU(),
        nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),
        nn.LeakyReLU(),
        nn.ConvTranspose2d(64, 21, 3, stride=2, padding=1, output_padding=1),
    )

After 200 iterations I am getting the following output
Training Data

Validation Data

Is a more complex architecture the only way I can fix this problem? Or can I fix this with a different loss function like dice or more regularization? The same issue happened after training for 100 iterations. So the model is not generalizing for some reason.
Edit
I also tried adding weights to CrossEntropy such that w_label = 1 - frequency(label). The idea was that 0 label for the background which was more common would contribute less to the loss, and other labels which were rare, would contribute more to the loss. But that did not help:

Another thing I tried was ignoring label 0 for background in the loss. But that created horrible results even for training data:

","['deep-learning', 'convolutional-neural-networks', 'autoencoders', 'image-segmentation']",
How can I reconstruct sparse one-hot encodings using an RBM?,"
I am currently working with a categorical-binary RBM, where there are 50 categorical visible units and 25 binary hidden units. The categorical visible units are expressed in one-hot encoding format, such that if there is 5 categories, then the visible units are expressed as a $50 \times 5$ array, where each row is the one-hot encoding of a category from 1 to 5.
Ideally, the RBM should be able to reconstruct the visible units. However, since the visible units are in one-hot encoding, then the visible units array contains a lot of zeros. This means the RBM quickly learns to guess all zeros for the entire array to minimize the reconstruction loss. How can I force the RBM to not do this and to instead guess 1's where the category occurs and 0's otherwise?
Note that I would still have this problem with a regular autoencoder.
","['overfitting', 'variational-autoencoder', 'restricted-boltzmann-machine']",
Why should the weight updates be proportional to input?,"
I'm reading the book Grokking Deep Learning. Regarding weight updates during training, it has the following code and explanation:
direction_and_amount = (pred - goal_pred) * input
weight = weight - direction_and_amount

It explains the motivation behind multiplying the prediction difference with input using three cases: scaling, negative reversal and stopping.

What are scaling, negative reversal, and stopping? These three attributes have the combined effect of translating the pure error into the absolute amount you want to change weight.  They do so by addressing three major edge cases where the pure error isn’t sufficient to make a good modification to weight.

These three cases are:

Negative input,
zero input and
the value of input (scaling).

Negative and zero cases are very obvious. However, I didn't understand scaling. Regarding scaling, there's the following explanation:

Scaling is the third effect on the pure error caused by multiplying it by input. Logically, if the input is big, your weight update should also be big. This is more of a side effect, because it often goes out of control. Later, you’ll use alpha to address when that happens.

But I didn't understand it. Considering the linear regression problem, why weight update should be big if the input is big?
","['neural-networks', 'deep-learning', 'training', 'backpropagation', 'gradient-descent']",
How is Google Translate able to convert texts of different lengths?,"
According to my experience with Tensorflow and many other frameworks, neural networks have to have a fixed shape for any output, but how does Google translate convert texts of different lengths?
","['natural-language-processing', 'recurrent-neural-networks', 'machine-translation', 'google-translate', 'seq2seq']",
Why is the completeness of UCS guaranteed only if the cost of every step exceeds some small positive constant?,"
I was reading Artificial Intelligence: A Modern Approach 3rd Edition, and I have reached to the UCS algorithm.
I was reading the proof that UCS is complete.
The book state that:

Completeness is guaranteed provided the cost of every step exceeds some small positive constant $\epsilon .$

And that's because UCS will be stuck if there is a path with an infinite
sequence of zero-cost actions.
Why the step cost must exceed $\epsilon$? Isn't enough for it to be greater than zero?
","['search', 'uniform-cost-search', 'completeness']","Let's consider a problem where all edge costs are greater than zero, but not above some $\epsilon$:Image a problem where we have an infinite path where the first edge is cost $\frac{1}{2}$, the next is $\frac{1}{4}$, the following is $\frac{1}{8}$, and so on forever. Every edge is greater than zero, meeting the condition being proposed in the question. However, this path overall has finite cost (1) even through there are an infinite number of states on that path. So, on this problem UCS will never reach paths with cost greater than 1. Thus, if the solution cost is 2, UCS will not find any solution to this problem, and thus it would not be a complete algorithm. So, all edges being greater than zero is not sufficient.For most search algorithms to be complete, there must be a finite number of states with any given cost. (To be slightly more precise, there must exist some fixed $\epsilon$ such that in each range size $\epsilon$ there are a finite number of states.)"
Can models get 100% accuracy on solved games?,"
I had a question today that I feel it must have an answer already, so I'm shopping around.
If we ask a model to learn the binary OR function, we get perfect accuracy with every model (as far as I know).
If we ask a model to learn the XOR function we get perfect accuracy with some models and an approximation with others (e.g. perceptrons).
This is due to the way perceptrons are designed -- it's a surface the algorithm can't learn. But again, with a multi-layered neural network, we can get 100% accuracy.
So can we perfectly learn a solved game as well?
Tic-tac-toe is a solved game; an optimal move exists for both players in every state of the game. So in theory our model could learn tic-tac-toe as well as it could a logic function, right?
","['machine-learning', 'function-approximation']",
"Where can I find pre-trained agents able to play games with multiple stages like exploration, dialog, combat?","
My goal is to create an ML model to be able to classify different game stages, e.g., dialog with a non-player character, exploration, combat with enemy, in-game menu etc.
In order to do that, I am looking for an agent pre-trained on such a game. I am intending to develop a model using this pre-trained agent to produce a data set (frames-labels) and finally I will use that data set to train a model to classify those different stages.
I could only find a pre-trained model for the Doom, however, it is not much appropriate for my case because it does not have different game stages (it is merely based on running & shooting).
Training my own Reinforcement Learning Agent is a whole another workload in terms of both time and GPU such a game needs.
Any single idea could help me a lot. Thanks!
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'deep-learning', 'classification']",
Multi-armed bandits: reducing stochastic multi-armed bandits to bernoulli bandits,"
Agrawal and Goyal (http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf page 3) discussed how we can extend Thompson sampling for bernoulli bandits to Thompson sampling for stochastic bandits in general by simply Bernoulli sampling with the received reward $r_t \in [0,1]$.
My question is whether such extension from Bernoulli bandits to general stochastic bandits hold in general and not only for Thompson sampling. E.g. can I prove properties such as lower bounds on regret for Bernoulli bandits and always transfer these results to general stochastic bandits?
","['reinforcement-learning', 'multi-armed-bandits', 'thompson-sampling']",
"If uniform cost search is used for bidirectional search, is it guaranteed the solution is optimal?","
If uniform cost search is used for both the forward and backward search in bidirectional search, is it guaranteed the solution is optimal?
","['search', 'proofs', 'uniform-cost-search', 'bidirectional-search', 'optimality']","Let's first recall that the uniform-cost search (UCS) is optimal (i.e. if it finds a solution, which is not guaranteed unless the costs on the edges are big enough, that solution is optimal) and it expands nodes with the smallest value of the evaluation function $f(n) = g(n)$, where $g(n)$ is the length/cost of the path from the goal/start node to $n$.The problem of bidirectional search with UCS for the forward and backward searches is that UCS does not proceed layer-by-layer (as breadth-first search does, which ensures that when the forward and backward searches meet, the optimal path has been found, assuming they both expand one level at each iteration), so the forward search may explore one part of the search space while the backward search may explore a different part, and it could happen (although I don't have the proof: I need to think about it a little bit more!), that these searches do not meet. So, I will consider both cases:when the forward and backward searches do not ""meet"" (the worst case, in terms of time and space complexity)when they meet (the non-degenerate case)Let's consider the case when the forward search does not meet the backward search (the worst/degenarate case).If we assume that the costs on the edges are big enough and the start node $s$ is reachable from $g$ (or vice-versa), then bidirectional search eventually degenerates to two independent uniform-cost searches, which are optimal, which makes BS optimal too.Let's consider the case when the forward search meets the backward search.To ensure optimality, we cannot just stop searching when we take off both the frontiers the same $n$. To see why, consider this example. We take off the first frontier node $n_1$ with cost $N$, then we take off the same frontier node $n_2$ with cost $N+10$. Meanwhile, we take off the other frontier node $n_2$ with cost $K$ and the node $n_1$ with cost $K + 1$. So, we have two paths: one with cost $N+(K + 1)$ and one with cost $(N+10)+K$, which is bigger than $N+(K + 1)$, but we took off both frontiers $n_2$ first.See the other answer for more details and resources that could be helpful to understand the appropriate stopping condition for the BS."
"In the MINE paper, why is $\hat{G}_B$ biased, and how does the exponential moving average reduce the bias?","
While reading the Mutual Information Neural Estimation (MINE) paper [1] I came across section 3.2 Correcting the bias from the stochastic gradients. The proposed method requires the computation of the gradient
$$\hat{G}_B = \mathbb{E}_B[\nabla_{\theta}T_{\theta}] - \frac{\mathbb{E_B}[\nabla_{\theta}T_{\theta}e^{T_{\theta}}]}{\mathbb{E}_B[e^{T_{\theta}}]},$$
where $\mathbb{E}_B$ denotes the expectation operation w.r.t. a minibatch $B$, and $T_{\theta}$ is a neural network parameterized by $\theta$. The authors claim that this gradient estimation is biased and that can be reduced by simply performing an exponential moving average filtering.
Can someone give me a hint to understand these two points:

Why is $\hat{G}_B$ biased, and
How does the exponential moving average reduce the bias?

","['neural-networks', 'deep-learning', 'papers', 'generative-adversarial-networks', 'stochastic-gradient-descent']","The lower bound in MINE is as follows:$$\widehat{I(X;Z)}_n = \sup_{\theta\in\Theta} \mathbb{E}_{\mathbb{P}_{XZ}^{(n)}}[T_\theta] - \log{\mathbb{E}_{\mathbb{P}_X^{(n)} \otimes \hat{\mathbb{P}}_Z^{(n)}}[e^{T_\theta}]}$$Here $\mathbb{\hat{P}^{(n)}}$ denotes the empirical distribution that we get from n i.i.d samples of $\mathbb{P}.$Note that in the above equation, the first term is calculated from the joint distribution while the second term from the marginals of $X$ and $Z$. In the implementation of MINE, these statistics are calculated over the data from a minibatch. The marginal distribution is obtained by shuffling the values of Z (or X) along the batch dimension. Hence, in this case, the gradient is as follows.$$\hat{G}_B = \mathbb{E}_B[\nabla_{\theta}T_{\theta}] - \frac{\mathbb{E_B}[\nabla_{\theta}T_{\theta}e^{T_{\theta}}]}{\mathbb{E}_B[e^{T_{\theta}}]},$$"
What is the difference between exploitation and exploration in the context of optimization?,"
In the paper Moth-flame optimization algorithm: A novel nature-inspired heuristic paradigm (2015, published in Knowledge-Based Systems)

The test functions are divided to three groups: unimodal, multi-modal, and composite. The unimodal functions ($F1 - F7$) are suitable for benchmarking the exploitation of algorithms since they have one global optimum and no local optima. In contrary, multi-modal functions ($F8 - F13$) have a massive number of local optima and are helpful to examine exploration and local optima avoidance of algorithms

I imagine that exploration means it goes searching for something in unknown regions from a starting point. But exploitation would search more around the starting (or current point).
It is more or less that? What else differentiates both concepts?
","['comparison', 'optimization', 'exploration-exploitation-tradeoff', 'meta-heuristics', 'moth-flame-optimization']",
What is the space complexity of bidirectional search?,"
Is the space complexity of the bidirectional search, where the breadth-first search is used for both the forward and backward search, $O(b^{d/2})$, where $b$ is the branching factor and $d$ the length of the optimal path (assuming that there is indeed one)?
","['search', 'branching-factors', 'breadth-first-search', 'space-complexity', 'bidirectional-search']","Norvig & Russell's book (section 3.5) states that the space complexity of the bidirectional search (which corresponds to the largest possible number of nodes that you save in the frontier)$$O(2b^{d/2}) = O(b^{d/2}).$$The intuition behind this result is that (as opposed to e.g. uniform-cost search or breadth-first search, which have space (and time) complexity of $O(b^{d})$) is that the forward and backward searches only have to go half way, so you will not eventually need to expand all $b^{d}$ leaves, but only half of them.However, this space complexity is correct if you use a breadth-first search for the forward and backward searches (which is your scenario!), given that breadth-first search, assuming a finite branching factor, expands one level at a time, so it's guaranteed that both the forward and backward searches meet in the middle. This can be seen in figure 3.17 of the same book, where you can see that both searches have the same ""radius"". Moreover, the only nodes that you need to store in the frontier are the ones on the circumference (not all nodes that you see in the image)However, if you used another search algorithm to perform the forward and backward searches, the space complexity may be different. This is true if e.g. the searches do not meet and then they end up exploring all the state space."
Off-policy full-random training in easy-to-explore environment,"
Let say we are in an environment where a random agent can easily explore all the states of an environment (for example: tic-tac-toe).
In those environments, using off-policy algorithm, is it a good practice to train using exclusively random actions, instead or epsilon-greedy, Boltzmann or whatever ?
For my mind, it seems logical, but I have never heard about it before.
","['reinforcement-learning', 'dqn', 'policies', 'off-policy-methods', 'epsilon-greedy-policy']",
Can most of the basic machine learning models be easily represented as simple neural network architectures?,"
I am currently studying the textbook Neural Networks and Deep Learning by Charu C. Aggarwal. In chapter 1.2.1 Single Computational Layer: The Perceptron, the author says the following:

Different choices of activation functions can be used to simulate different types of models used in machine learning, like least-squares regression with numeric targets, the support vector machine, or a logistic regression classifier. Most of the basic machine learning models can be easily represented as simple neural network architectures.

I remember reading something about it being mathematically proven that neural networks can approximate any function, and therefore any machine learning method, or something along these lines. Am I remembering this correctly? Would someone please clarify my thoughts?
","['neural-networks', 'machine-learning', 'activation-functions', 'universal-approximation-theorems']","I think the author refers to both different choices of activation function and loss. It is explained in more detail in chapter 2. In particular 2.3 is ilustrative of this point.I don't think there is a relation between this argument and universal approximation theorems, which state that certain classes of neural networks can approximate any function in certain domains, rather than any learning algorithm."
What is the space complexity of breadth-first search?,"
When using the breadth-first search algorithm, is the space complexity $O(b^d)$, where $b$ is the branching factor and $d$ the length of the optimal path (assuming that there is indeed one)?
","['search', 'breadth-first-search', 'branching-factors', 'space-complexity']","The space complexity of the breadth-first search algorithm is $O(b^d$) in the worst case, and it corresponds to the largest possible number of nodes that may be stored in the frontier at once, where the frontier is the set of nodes (or states) that you are currently considering for expansion.You can take a look at section 3.5 (page 74) of the book Artificial Intelligence: A Modern Approach (3rd edition, by Norvig and Russell) for more info about the time and space complexity of BFS."
What is the space complexity of iterative deepening search?,"
When using iterative deepening, is the space complexity, $O(d)$, where $b$ is the branching factor and $d$ the length of the optimal path (assuming that there is indeed one)?
","['search', 'branching-factors', 'homework', 'space-complexity', 'iddfs']","As stated in my other answers here and here, the space complexity of these search algorithms is calculated by looking at the largest possible number of nodes that you may need to save in the frontier during the search.Iterative deepening search (IDS) is a search algorithm that iteratively uses depth-first search (DFS), which has a space complexity of $O(bm)$, where $m$ is the maximum depth, with progressively bigger depths, and it turns out that its space complexity is $O(bd)$.(Note that there is also the informed version of IDS, which uses the informed search algorithm A*, rather than DFS, known as iterative deepening A*)."
"In CNNs, why do we sum the filter derivatives w.r.t the loss function to get the final gradient?","
In a Convolutional Neural Network, unlike the fully connected layers, the same filter is used multiple times on the input while convolving - so during backpropagation, we get multiple derivatives for the filter parameters w.r.t the loss function. My question is, why do we sum all the derivatives to get the final gradient? Because, we don't sum the output of the convolution during forward pass. So, isn't it more sensible to average them? What is the intuition behind this?
PS: although I said CNN, what I'm actually doing is correlation for simplicity of learning.
","['convolutional-neural-networks', 'backpropagation', 'convolution', 'filters']","I really liked the question. Yes, we sum over derivatives. First of all think what backpropagation is trying to do: finding the affect of each parameter on the loss.So as you said:the same filter is used multiple times on the input while convolvingmeaning that each kernel affects the final loss in several ways, so those affects should be summed together, not averaged."
How is the performance of a model affected by adding a ReLU to fully connected layers?,"
How significant is adding a ReLU to fully connected (FC) layers? Is it necessary, or how is the performance of a model affected by adding ReLU to FC layers?
","['neural-networks', 'deep-neural-networks', 'activation-functions', 'performance', 'relu']","ReLU is piecewise linear function that outputs the received input directly if it's positive, or outputs a zero. i.e., $max(0, x)$ReLU, being an activation function, will determine what the output of the nodes in your FCs are. Since it's a non-linear function, one significance is it will allow the nodes in your model to learn  complex mappings between the inputs and the outputs. Compared to using a linear function, it will allow back-propagation since it has a derivative, allowing the neural network to have the advantage of stacking of multiple FC layers.ReLU (and non-linear activation functions in general) will introduce non-linear properties in a neural network that enables it to learn more complex arbitrary structures in the inputs. Without activation functions between the layers, your neural network will simply be a linear function, regardless of the number of layers it has. Why? A linear function + linear function gives a linear function. Additionally, see this answer.Compared to no activation function at all, it will be slower to train but will only behave like a linear regression model. So ReLU will increase the power of the model by making it non-linear.
However, compared to other non-linear activation functions like tanH, ReLU will speed up training as (1) its computation step is cheaper i.e., $0.0$ or $x$ without additional operations (2) its gradient just depends on the sign of the input $x$. See this answer"
Is it possible to retrieve the optimal policy from the state value function?,"
One can easily retrieve the optimal policy from the action value function but how about obtaining it from the state value function?
","['reinforcement-learning', 'dynamic-programming']","You can obtain the optimal policy from the optimal state value function if you also have the state transition and reward model for the environment $p(s',r|s,a)$ - the probability of receiving reward $r$ and arriving in state $s'$ when starting in state $s$ and taking action $a$.This looks like:$$\pi^*(s) = \text{argmax}_a [\sum_{s',r} p(s',r|s,a)(r + \gamma v^*(s'))]$$There are variations of this function, depending on how you represent knowledge of the environment. For instance, you don't actually need the full distribution model for reward, an expected reward function and separate distribution model for state transition rules would also work.Without at least an approximate model of the environment, you cannot derive a policy from state values. If all you have is state values, then to pick an optimal action, you absolutely need the ability to look ahead a time step at what the next state might be for each action choice."
"Is which sense was AlphaGo ""just given a rule book""?","
I was told that AlphaGo (or some related program) was not explicitly taught even the rules of Go -- if it was ""just given the rulebook"", what does this mean? Literally, a book written in English to read?
","['reinforcement-learning', 'alphazero', 'alphago-zero', 'alphago', 'muzero']",
How can reinforcement learning be applied when the goal location or environment is unknown?,"
I am studying RL. I was thinking whether a new state value or the observation is provided by the environment before the agent actually implements the action.
Take the maze problem as an example. Each state consists of all the available cells information, provided by the environment. But what if the environment is unknown? For example, there is a maze with an unknown destination cell. The agent needs to find the destination cell. The state is 1 or 0, meaning the destination reached or not. But the environment, which is the maze, can only provide the state at cell $i$ which is 0 or 1 only when the agent reaches cell $i$.
Can this still be solved by RL? I am confused about the environment setup.
","['reinforcement-learning', 'environment']",
What exactly is the eigenspace of a graph (in spectral clustering)?,"
When we find the eigenvectors of a graph (say in the context of spectral clustering), what exactly is the vector space involved here? Of what vector space (or eigenspace) are we finding the eigenvalues of?
","['graphs', 'clustering', 'linear-algebra', 'graph-theory', 'spectral-analysis']","In spectral clustering we not find the eigenvectors of a graph (a graph is not a matrix) but the eigenvalues/eigenvectors of the Laplacian matrix related to the adjacency matrix of the graph:graph => adjacency matrix => Laplacian matrix => eigenvalues (spectrum).The adjacency matrix describes the ""similarity"" between two graph vertexs. In the most simple case (undirected unweighted simple graph), a value ""1"" in the matrix means two vertex joined by an edge, a value ""0"" means no edge between these vertex.So, the space under the adjacency matrix is the space of connectivity, being row ""i"" of a column vector a measure of the connectivity with vertex ""i"". In other words, the adjacency and Laplacian matrix map from vertexs to vertex connectivity.ExampleAssume a simple graph with 3 vertex {1,2,3} and edges (1,2) and (2,3). The respective Laplacian matrix is:$$
A=\begin{pmatrix}
1 & -1 & 0\\
-1 & 2 & -1\\
0 & -1 & 1
\end{pmatrix}
$$a) vertex 1, than in vertex space is (1,0,0) maps to:$$
A\begin{pmatrix}
1\\
0\\
0
\end{pmatrix}
=
\begin{pmatrix}
1\\
-1\\
0
\end{pmatrix}
$$if we analyze the product result, component by component, it means:b) the set of vertexs 1 and 2, that is represented in vertex space as (1,1,0), maps to:$$
A\begin{pmatrix}
1\\
1\\
0
\end{pmatrix}
=
\begin{pmatrix}
0\\
1\\
-1
\end{pmatrix}
$$meaning that:Finally, see what happens if multiply (inner/scalar product) previous result by the vertex vector again:$$
\begin{pmatrix}
1 & 1 & 0
\end{pmatrix}
A\begin{pmatrix}
1\\
1\\
0
\end{pmatrix}
= 1
$$it gives the number of edges that connects the set of nodes {1,2} with the remainder graph."
"When training deep learning models for object detection in images, do you need a large number of images, or a large number of training samples?","
I am training a deep learning model for object detection. The consensus is that the more images that you have, the better the results will be. All the tutorials that I have seen say that more images are key.
I am labeling objects in my images with Label-Img, which provides the algorithm with specific training samples on the images. For my images, I am using photos with dimensions of 1100 x 1100 pixels. In my case, I could generate anywhere between 50-100 high-quality training samples per image. For example:

In cases such as this where large numbers of training samples can be generated from a single image, do you really need several hundred images? Or can you lessen the number of images because of the number of training samples?
","['deep-learning', 'training', 'object-detection']",
"Intuitively, how does it make sense to take an action $A'$ when the environment already ended? [duplicate]","







This question already has answers here:
                                
                            




How should I handle action selection in the terminal state when implementing SARSA?

                                (2 answers)
                            

Closed 2 years ago.



The update equation for SARSA is $Q(S,A) = R + \gamma Q(S',A')$. Consider this: I take an action $A$ that leads to the terminal state. Now my $S'$ would be one of the terminal states. So...

Intuitively, how does it make sense to take an action $A'$ when the environment already ended? Or is this something you just do anyway?

Once a terminal state-action pair is reached, you update the previous state-action pair and then start the game loop all over again. But this means that the terminal state-action pair ($Q(S',A')$ in my example) is never updated. So, if your initial estimate of $Q(S',A')$ was wrong, you would never be able to fix it which would be very problematic. (And you can't set all the terminal values to zero because you are using function approximators)


So, how do I resolve these issues?
","['reinforcement-learning', 'sarsa']","It doesn't make sense, in that nothing can happen once the agent reaches a terminal state. However, it is often modelled as an ""absorbing state"" where the action is unimportant (either null or value ignored) with value by definition of $0$.And you can't set all the terminal values to zero because you are using function approximatorsThe value is zero by definition. There is no need to approximate it. So don't use function approximators for action values in terminal states.  When $S'$ is terminal, the update becomes:$Q(S,A) \leftarrow Q(S,A) + \alpha(R - Q(S,A))$Look at any implementation of Q learning and you will see a conditional calculation for the update value, that uses some variant of the above logic when $S'$ is terminal. For OpenAI Gym environments for instance, it will use the done flag."
Is the 3d convolution associative given that it can be represented as matrix multiplication?,"
I'm trying to understand if a 3D convolution of the sort performed in a convolutional layer of a CNN is associative. Specifically, is the following true:
$$
X \otimes(W \cdot Q)=(X \otimes W) \cdot Q,
$$
where

$\otimes$ is a convolution,
$X$ is a 3D input to a convolution layer,
$W$ is a 4D weights matrix reshaped into 2 dimensions,
and $Q$ is a PCA transformation matrix.

To elaborate: say I take my 512 convolutional filters of shape ($3 \times 3 \times 512$), flatten across these three dimensions to give a ($4096 \times 512$) matrix $W$, and perform PCA on that matrix, reducing it to say dimensions of ($4096 \times 400$), before reshaping back into ($400$) 3d filters and performing convolution.
Is this the same as when I convolve $X$ with $W$, and then perform PCA on that output using the same transformation matrix as before?
I know that matrix multiplication is associative i.e. $A(BC)=(AB)C$, and I have found that convolution operations can be rewritten as matrix multiplication.
So my question is, if I rewrite the convolution as matrix multiplication, is it associative with respect to the PCA transformation (another matrix multiplication)?
For example, does $X' \cdot (W' \cdot Q) = (X' \cdot W') \cdot Q$, where $X'$ and $W'$ represent the matrices necessary to compute the convolution in matrix multiplication form?
To try and figure it out, I looked to see how convolutions could be represented as matrix multiplications, since I know matrix multiplications are associative. I've seen a few posts/sites explaining how 2D convolutions can be rewritten as matrix multiplication using Toeplitz matrices (e.g. in this Github repository or this AI SE post), however, I'm having trouble expanding on it for my question.
I've also coded out simple convolutions with a $W$ matrix of $4 \times 3$, an $X$ matrix of $4 \times 2$, and using sklearn's PCA to reduce $W$ to $4 \times 2$. If I do this both ways, the output is not the same, leading me to think this kind of associativity does not exist. But how can I explain this with linear algebra?
Can anyone explain whether this is or is not the case, with a linear algebra explanation?
","['convolutional-neural-networks', 'convolution', 'linear-algebra', 'principal-component-analysis', 'convolutional-layers']",
CNN to detect presence/absense of label on images with mixed labels,"
Here's my problem: I work with medical image classification, and currently I have 3 classes:

class A: images with lesion 1 only; and images with lesion 1 and N other lesions
class B: images with 2 other lesions (no lesion 1)
class C: images with no lesion

The goal is to classify into ""lesion 1"", ""other lesion"", ""no lesion"". I'd like to know some approach/method/paper/clue for this classification. I think the presence of other lesions on both class A and B is confusing the model (the validation accuracy and f1-score are very low).
Thanks in advance.
","['deep-learning', 'convolutional-neural-networks', 'classification', 'multi-label-classification', 'image-recognition']",
Why is an embedding of dimension 400 enough to represent 70000 words?,"
I am learning PyTorch on Udacity. In lesson 8, section 11: Training the Model, the instructor writes:

Then I have my embedding and hidden dimension. The embedding dimension is just a smaller representation of my vocabulary of 70k words and I think any value between like 200 and 500 or so would work, here. I've chosen 400. Similarly, for our hidden dimension, I think 256 hidden features should be enough to distinguish between positive and negative reviews.

There are more than 70000 different words. How could those more than 70000 unique words be represented by just 400 embeddings? How does an embedding look like? Is it a number?
Moreover, why would 256 hidden features be enough?
","['recurrent-neural-networks', 'hyperparameter-optimization', 'word-embedding', 'hyper-parameters', 'sentiment-analysis']","The specific term you are looking for is ""word embedding"" and not just ""embedding"".Neural networks (typically) require as inputs (and produce as outputs) numerical data (i.e. numbers, vectors, matrices, or higher-dimensional arrays). So, when processing textual data, we first need to encode (or convert) the text into a numerical representation. There are different ways to do it, such asone-hot encoding (in that case, if you have 70000 words, you would have sparse vectors with 70000 entries where only one of those entries is equal to $1$ and all other entries are $0$: see this article  for more info)map each word to a number (in this case, you would have 70000 numbers, one for each word)word embeddingsEach of these representations has different benefits and drawbacks. For instance, if you map each word to a number, then you just need to keep track of $70000$ numbers. In the case of one-hot encoding or word embeddings, you will need more memory. However, nowadays, word embeddings are widely used in natural language processing/understanding/generation tasks (and given that your question is about word embeddings), so let me briefly describe them.There are different word embedding techniques (such as word2vec). However, they are all based on the same ideasWords that are similar (or related) in meaning should be mapped to vectors (i.e. the ""word embeddings"") that are also similar in some sense (for instance, their cosine similarity should be high). For instance, the words ""man"" and ""boy"" should be mapped to vectors that are similar.These word embeddings are learned (rather than hard-coded or manually specified) given the dataThe size of the word embeddings is a hyper-parameter (this should answer your question!)To answer your question(s) more directly, the choice of the dimension of the embeddings or the number of ""hidden features"" (which are both hyper-parameters) was probably more or less arbitrary or based on the instructor's experience. In general, it is difficult to determine the optimal choice of any hyper-parameter. Sometimes you can just use numbers that other people have used in the past and have noticed that work ""well enough"". If you really want to find more appropriate values of the hyper-parameters, you could use some hyper-parameter optimization technique, such as Bayesian optimization or a simple grid search.You can find many resources online that explain the concept of ""word embeddings"" more in detail. For instance"
Is case-based reasoning a machine learning technique?,"
A few years ago when I was in university, I had implemented (for my final year project) an Itinerary Planning System, which incorporates an AI technique called ""case-based reasoning"".
Is case-based reasoning a machine learning technique or an AI technique (that is not machine learning)?
","['machine-learning', 'definitions', 'ai-field', 'case-based-reasoning', 'instance-based-learning']","The book Machine Learning (1997) by Tom Mitchell covers case-based reasoning (CBR), a form of instance-based learning (nearest neighbor is the typical example of IBL) in chapter 8 (p. 230).T. Mitchell  writesInstance-based methods such as $k$-NEAREST NEIGHBOR and locally weighted regression share three key properties. First, they are lazy learning methods in that they defer the decision of how to generalize beyond the training data until a new query instance is observed. Second, they classify new query instances by analyzing similar instances while ignoring instances that are very different from the query. Third, they represent instances as real-valued points in an $n$-dimensional Euclidean space. Case-based reasoning (CBR) is a learning paradigm based on the first two of these principles, but not the third. In CBR, instances are typically represented using more rich symbolic descriptions, and the methods used to retrieve similar instances are correspondingly more elaborate. CBR has been applied to problems such as conceptual design of mechanical devices based on a stored library of previous designs (Sycara et al. 1992), reasoning about new legal cases based on previous rulings (Ashley 1990), and solving planning and scheduling problems by reusing and combining portions of previous solutions to
similar problems (Veloso 1992).He then goes on and gives the example of a CBR system: the CADET system. He also formulates CBR as a learning problem and uses the term ""learn"" to refer to a search process that CADET goes through, which is similar to what k-NN does.He then writesTo summarize, case-based reasoning is an instance-based learning method in which instances (cases) may be rich relational descriptions and in which the retrieval and combination of cases to solve the current query may rely on knowledge-based reasoning and search-intensive problem-solving methods.To conclude, yes, CBR can be considered a machine learning technique (if you also consider k-NN a learning algorithm, which people often do), even though it may rely on knowledge-based reasoning and search-intensive problem-solving methods.You may also be interested in the paper Representation in case-based reasoning (2005) by Ralph Bergmann et al. Moreover, the famous AIMA book (3rd edition) mentions case-based reasoning in chapter 19 (p. 799), which is dedicated to knowledge and learning."
"What is a ""center loss""?","
I have seen that a center loss is beneficial in computer vision, especially in face recognition. I have tried to understand this concept from the following material

A Discriminative Feature Learning Approach for Deep Face Recognition
https://www.slideshare.net/JisungDavidKim/center-loss-for-face-recognition

However, I could not understand the concept clearly. If someone can explain with the help of an example, that would be appreciated.
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'objective-functions', 'face-recognition']",
How to verify classification model trained on classification dataset on a detection dataset for classification purpose?,"
I am working on a problem that involves two tasks - detection and classification. There is no single dataset for both tasks. I am training two models, separate on detection dataset and another on classification dataset. I use the images from the detection dataset as input and get classification predictions on top of detected bounding boxes.
Dataset description :

Classification - Image of the single object (E.g. Car) in the center with a classification label.
Detection - Image with multiple objects (E.g. 4 Cars) with bounding box annotations.

Task - Detect objects(e.g. cars) from detection datasets and classify them into various categories.
How do I verify whether the classification model trained on the classification dataset is working on images from detection dataset? (In terms of classification accuracy)
I cannot manually label the images from the detection dataset for individual class labels. (Need expert domain knowledge)
How do I verify my classification model?
Is there any technique to do this ? Like domain transfer or any weakly-supervised method ?
","['deep-learning', 'convolutional-neural-networks', 'object-detection', 'image-processing']","The ProblemWe can see from the question that existing information on detection and
classification in the small automotive vehicle domain has been
located (in the form of two independent sets of vectors usable for
machine training), and there is no already existing mapping or
other correspondence between the elements of one set and the
elements of the other.  They were obtained independently, remain
independent, and are linked only by the conventions of the domain
(today's aesthetically acceptable and thermodynamically workable
forms of small vehicles).The goal stated in the question is to create a computer vision system
that both detects cars and classifies them leveraging the
information contained in the two distinct sets.In the vision systems of mammals, there are also two distinct equivalences
of sets; one arising from a genetic algorithm, the DNA that is
expressed during the formation of the neural net geometry and
bio-electro-chemistry of the visual system in early development;
and the cognitive and coordinative pathways in the cerebrum and
cerebellum.If a robot, wheelchair, or other vehicle is to avoid traffic, we must
produce a system that in some way matches or exceeds the
collision avoidance performance of mammals.
In crime prevention, toll collection, sales lot inventory,
county traffic analysis, and other like applications,
performance will again be expected to match or exceed the
performance of biological systems.
If a person can record the make, model, year, color, and
license plate strings, so should the machine we employ in
these capacities.Consequently, this question is pertinent beyond academic curiosity, as
it is applicable in current research and development of products.That this question author notices the lack of a unified data
set that can be used to train it to detect and characterize
in a single network objects of interest is apropos and
key to the challenge of finding a solution.Approach
The simplest approach would be to compose the system of two functions.The four dimensions of input for $\mathcal{D}$, the detector, are horizontal position, vertical position,
rgb index, and brightness to decribe the pixelized image; and the output are bounding boxes as two ""corner"" coordinates corresponding to each identified vehicle, the second coordinate being either relative to the first or to a specific corner of the entire frame.
The categorizer, $\mathcal{C}$, receives as input bounding boxes and produces as output the index
or code that maps to the categories corresponding to the labels of the training set available for
categorization.
The system can then be described as follows.$\quad\quad\mathcal{S}: \mathcal{C} \circ \mathcal{D}$If the system is not color, subtract one from the above dimensionality of the input.  If the system processes video, add one to the dimensionality of the input and consider using LSTM or GRU cell types.The above substitution represented by ""$\circ$"" appears to be what is meant by, ""I use the images from the detection dataset as input
and get classification predictions on top of detected bounding boxes.""The interrogative, ""How do I verify whether the classification model trained on the
classification dataset is working on images from detection dataset?
(In terms of classification accuracy),""
appears to refer to the fact that labels do not exist for the second set that correspond to
input elements of the first set, so an accuracy metric cannot be directly obtained.
Since there is no obvious automatic way of generating labels for the vehicles in the pre-detected images
containing potentially multiple vehicles,
there is no way to check actual results against expected results.
Composing multiple vehicle images from the categorization set to use as test input to
the entire system $\mathcal{S}$ will only be useful in evaluating an aspect
of the performance of $\mathcal{D}$, not $\mathcal{C}$.SolutionThe only way to evaluate the accuracy and reliability of $\mathcal{C}$ is with portions
of the set used to train it that were excluded from the training and trust
that the vehicles depicted in those images were sufficiently representative
of the concept ""car"" to provide consistency of accuracy and reliability across
the range of those detected by $\mathcal{D}$ in the application of $\mathcal{S}$.
This means that the leveraging of the information, even if optimized to the degree
possible by any arbitrary algorithm or parallelism in the set of all possible
algorithms or parallelisms, is limited by the categorization training set.
The number of set elements and the comprehensiveness and distribution of categories
within that set must be sufficient to achieve an approximate equality between
these two accuracy metrics.With Additional ResourcesOf course this discussion is in a particular environment, that of the system
defined as the two artificial networks, one involving convolution based
recognition and the other involving feature extraction, and the two training
sets.
What is needed is a wider environment where known vehicles are in view so that
performance data of $\mathcal{S}$ is evaluated and a tap on the transfer
of information between $\mathcal{D}$ and $\mathcal{C}$ can be used
to differentiate between mistakes made on either side of the tap point.Unsupervised ApproachAnother course of action could be to not use the training set for categorization
on the training of $\mathcal{C}$ at all, but rather use feature extraction and
auto-correlation in an ""unsupervised"" approach, and then evaluate the results of
on the basis of the final convergence metrics at the point when stability in
categorization is detected.  In this case, the images in the bounding boxes
output by $\mathcal{D}$ would be used as training data.The auto-trained network realizing $\mathcal{C}$ can then be further evaluated
using the entire categorization training set.Further ResearchHybrids of these two approaches are possible. Also, the independent training only in the rarest of cases leads to optimal performance. Understanding feedback as originally treated with rigor by MacColl in chapter 8 of his Fundamental Theory of Servomechanisms, later applied to the problem of linearity and stability of analog circuitry, and then to training, first in the case of GANs, may lead to effective methods to bi-train the two networks.That evolved biological networks are trained in situ is an indicator that the most optimal performance can be gained by finding training architectures and information flow strategies that create optimality in both components simultaneously.  No biological niche has ever been filled by a neural component that is first optimized and then inserted or copied in some way to a larger brain system.  That is no proof that such component-ware can be optimal, but there is also no proof that the DNA driven systems that have emerged are not nearly optimized for the majority of terrestrial conditions."
Will changing the dimension reduction size of a neural network (i.e. SSD ResNet-50) change the overall outcome and accuracy of the model?,"
I am training a convolutional neural network to detect objects (weeds amongst crops, in my case) using TensorFlow. The original dimensions of the raw training photos are 4000 x 3000 pixels, which must be resized to become workable. The idea here is to label objects in the training images (using Label-Img), train the model, and use it to detect weeds in certain situations.
According to TensorFlow 2 Detection Model Zoo, there are algorithms designed for different speeds, which involves initially resizing the images to a specified dimension. Although this is not a coding question, here is an example of SSD ResNet-50, which initially resizes the input images to 1024 x 1024 pixels:
model {
  ssd {
    num_classes: 1
    image_resizer {
      fixed_shape_resizer {
        height: 1024
        width: 1024
      }
    }
    feature_extractor {
      type: ""ssd_resnet50_v1_fpn_keras""
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 0.00039999998989515007
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.029999999329447746
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.996999979019165
          scale: true
          epsilon: 0.0010000000474974513
        }
      }
      override_base_feature_extractor_hyperparams: true
      fpn {
        min_level: 3
        max_level: 7
      }
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 0.00039999998989515007
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0
              stddev: 0.009999999776482582
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.996999979019165
            scale: true
            epsilon: 0.0010000000474974513
          }
        }
        depth: 256
        num_layers_before_predictor: 4
        kernel_size: 3
        class_prediction_bias_init: -4.599999904632568
      }
    }
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        scales_per_octave: 2
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993922529e-09
        iou_threshold: 0.6000000238418579
        max_detections_per_class: 100
        max_total_detections: 100
        use_static_shapes: false
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 2.0
          alpha: 0.25
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    encode_background_as_zeros: true
    normalize_loc_loss_by_codesize: true
    inplace_batchnorm_update: true
    freeze_batchnorm: false
  }
}
train_config {
  batch_size: 64
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_crop_image {
      min_object_covered: 0.0
      min_aspect_ratio: 0.75
      max_aspect_ratio: 3.0
      min_area: 0.75
      max_area: 1.0
      overlap_thresh: 0.0
    }
  }
  sync_replicas: true
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.03999999910593033
          total_steps: 100000
          warmup_learning_rate: 0.013333000242710114
          warmup_steps: 2000
        }
      }
      momentum_optimizer_value: 0.8999999761581421
    }
    use_moving_average: false
  }
  fine_tune_checkpoint: ""PATH_TO_BE_CONFIGURED""
  num_steps: 100000
  startup_delay_steps: 0.0
  replicas_to_aggregate: 8
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
  fine_tune_checkpoint_type: ""classification""
  use_bfloat16: true
  fine_tune_checkpoint_version: V2
}
train_input_reader {
  label_map_path: ""PATH_TO_BE_CONFIGURED""
  tf_record_input_reader {
    input_path: ""PATH_TO_BE_CONFIGURED""
  }
}
eval_config {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""PATH_TO_BE_CONFIGURED""
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: ""PATH_TO_BE_CONFIGURED""
  }
}

Because I will be labeling many pictures in the future, I need to decide on a dimension to resize my original ones to (literature review says 1100 x 1100 has been used in previous projects).
If I were to change the image resizer in the code above to 1100 x 1100, for example, would that have any effect on model accuracy/training loss? Would it even run? I'm fairly new to this, so any insights on this would be greatly appreciated!
Note: I am using a NVIDIA GPU, so that helps speed the process quite a bit. Google Colab also can be used.
","['neural-networks', 'convolutional-neural-networks', 'training', 'object-detection']","That depends! You can try it. But if you change the sizes, you have to ensure that you do not mismatch the shapes. As far as size is concerned it won't affect the accuracy much unless you are significantly changing it.Since the default is 1024x1024, and you are making 1100x1100, there wont be any issues.Remember, there is a tradeoff in terms of speed and the amount of information you can derive from the image. The larger the image size, the higher the computation time and the image information you have."
What exactly are deep learning primitives?,"
I came across the concept of ""deep learning primitives"" from the Nvidia talk Jetson AGX Xavier New Era Autonomous Machines (on slide 44).
There doesn't seem to be a lot of articles in the community on this concept. I was able to find one definition from here, where it defined deep learning primitives as the ""fundamental building blocks of deep networks"" like fully connected layers, convolutions layers, etc.
I was curious to find out if a self-attention layer is a primitive, I came across this OpenDNN issue and one person explained that self-attention layers can be built by other primitives like inner product, concat, etc.
So my question is what exactly are primitives in deep learning? What makes a convolution layer a primitive and a self-attention layer not a primitive?
","['deep-learning', 'architecture']",
What is the impact of scaling the KL divergence and reconstruction loss in the VAE objective function?,"
Variational autoencoders have two components in their loss function. The first component is the reconstruction loss, which for image data, is the pixel-wise difference between the input image and output image. The second component is the Kullback–Leibler divergence which is introduced in order to make image encodings in the latent space more 'smooth'. Here is the loss function:
\begin{align}
\text { loss }
&=
\|x-\hat{x}\|^{2}+\operatorname{KL}\left[N\left(\mu_{x}, \sigma_{x}\right), \mathrm{N}(0,1)\right] \\
&=
\|x-\mathrm{d}(z)\|^{2}+\operatorname{KL}\left[N\left(\mu_{x^{\prime}} \sigma_{x}\right), \mathrm{N}(0,1)\right]
\end{align}
I am running some experiments on a dataset of famous artworks using Variational Autoencoders. My question concerns scaling the two components of the loss function in order to manipulate the training procedure to achieve better results.
I present two scenarios. The first scenario does not scale the loss components.

Here you can see the two components of the loss function. Observe that the order of magnitude of the Kullback–Leibler divergence is significantly smaller than that of the reconstruction loss. Also observe that 'my famous' paintings have become unrecognisable. The image shows the reconstructions of the input data.

In the second scenario I have scaled the KL term with 0.1. Now we can see that the reconstructions are looking much better.


Question

Is it mathematically sound to train the network by scaling the components of the loss function? Or am I effectively excluding the KL term in the optimisation?

How to understand this in terms of gradient descent?

Is it fair to say that we are telling the model ""we care more about the image reconstructions than 'smoothing' the latent space""?


I am confident that my network design (convolutional layers, latent vector size) have the capacity to learn parameters to create proper reconstructions as a Convolutional Autoencoder with the same parameters is able to reconstruct perfectly.
Here is a similar question.
Image Reference:
https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
","['objective-functions', 'gradient-descent', 'variational-autoencoder', 'kl-divergence']",
How to classify anomalies between two sound datasets?,"
I have two sound datasets and each one has 80% normal and 20% anomalous data points. The first one is a rock song and the second one is a mellow indie song. I use half of the normal data as a baseline in each dataset. I identify anomalies using isolation forest in each dataset and found 25 anomalies in the first rock song dataset and 12 in the mellow threshold. Now my question is how can I classify an anomaly as a rock song specific one? Do you think building a simple linear regression classifier should work?
","['classification', 'anomaly-detection']",
WGAN-GP Loss formalization,"
I have to write the formalization of the loss function of my network, built following the WGAN-GP model. The discriminator takes 3 consecutive images as input (such as 3 consecutive frames of a video) and must evaluate if the intermediate image is a possible image between the first and the third.

I thought something like this, but is it correct to identify x1, x2 and x3 coming from Pr even if they are 3 consecutive images? Only the first is chosen randomly, the others are simply the next two.
EDIT:

EDIT 2:
I replaced Pr with p_r(x1, x3) and p_r(x1, x2, x3) to reinforce the fact that x2 and x3 are taken after x1, so they depend on the choice of x1. Is it more correct this way?

","['deep-learning', 'generative-adversarial-networks', 'loss', 'wasserstein-metric', 'wasserstein-gan']",
Isn't evolutionary theory the essence of intelligence after all? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



The theory of evolution seems to be intelligent as it creates life
The mechanism of evolutionary theory consists of mutation, recombination, and natural selection like a genetic algorithm.
Isn't this evolutionary mechanism itself the same as the essence of human intelligence?
","['philosophy', 'evolutionary-algorithms']","The theory of evolution seems to be intelligent as it creates lifeWhen you say ""seems to be intelligent"" that begs the question: How are you defining ""intelligent""? Which of course is still one of the big issues in AI research.I think there are some flaws with the argument that ""creates life"" = ""intelligent"":Evolution does not create life. It operates on entities where there is a copy mechanism which is not 100% reliable, plus a selective environment that also impacts likelihood of further copies being made. Some form of proto-life (an initial Darwinian ancestor or Ida) needed to exist before evolution started.The process of creating the first proto-life capable of undergoing evolution is generally thought to be a large semi-random search through chemical combinations. Random search is sometimes used in optimisation problems, and might be studied as part of AI search topics. However, it would normally be considered something of a baseline algorithm, and definitley not tick the boxes for all the general traits of intelligence.Isn't this evolutionary mechanism itself the same as the essence of human intelligence?The Wikipedia article on artificial intellegence lists challenges faced by researchers and developers in AI. The categories chosen there are:Reasoning, problem solving; Knowledge representation; Planning; Learning; Natural language processing; Perception; Motion and manipulation; Social intelligence; General intelligenceTogether, these are mainly traits of mammalian, avian and a few other multicellular species, with a few traits such as language heavily focused on humans.I think it is important to separate out the mechanism whereby these traits arose naturally - which is generally agreed to be via an evolutionary process - from how those traits function. Artifical intelligence may use a little bit of reverse engineering from the natural traits in order to inspire design, but most AI systems do not use theory of evolution directly.When used directly, evolutionary algorithms can be used to solve search and optimisation problems. Also they can be used to solve simplified problems in perception and motion/manipulation. However, we are not able to scale up such algorithms to solve all aspects of general intelligence. Instead, systems like machine learning are designed to work from analysis of the problem, inspired in part by working natural systems. These work far more efficiently than evolutionary algorithms. There are no competitive evolutionary variants of AlphaZero, Watson, GPT-3 or neural-networks used in image processing.Evolutionary algorithms have their place in AI in practice and research. However, they do not define or encapsulate a form of general intelligence."
Is Webpage Semantic Segmentation possible nowadays?,"
I'm trying to do some research about semantic segmentation for webpages, in particular e-commerce webpages. I found some articles which provide some solutions based on very old dataset and those solutions in my opinion can't be effective for modern websites, in particular e-commerce. I would like to semantically infer the images bounding box, text, price etc..
Another problem is related with the size of webpage screenshot which are huge, I resized to 1024x512, but I think that I can't resize the image more otherwise I loose quality.
I built a very complex neural network in order to semantically infer text, images and background, (not classification but just segmentation), and the results are not so bad, but they are far from my expectations which seems strange to me, as we have many DNN able to do semantic segmentation of road, building, car etc for example. One problem is for sure the lack of a dataset with detailed labels. I didn't find any dataset that can satisfy my requests.
QUESTION: Any idea to help the network learn better the structure of a webpage just with a screenshot?
My DNN essentially is built as an auto-encoder architecture based on Segnet, with some modifications, skip connections, unpooling etc, I think that it is a good network.
references:
https://clgiles.ist.psu.edu/pubs/CVPR2017-connets.pdf
https://link.springer.com/chapter/10.1007/978-981-13-0020-2_33
","['deep-learning', 'image-segmentation']",
Is there a pretrained (NLP) transformer that uses subword n-gram embeddings for tokenization like fasttext?,"
I know that several tokenization methods that are used for tranformer models like WordPiece for Bert and BPE for Roberta and others. What I was wondering if there is also a transformer which uses a method for tokenization similarly to the embeddings that are used in the fasttext library, so based on the summations of embeddings for the n-grams the words are made of.
To me it seems weird that this way of creating word(piece) embeddings that can function as the input of a transformer isn't used in these new transformer architectures. Is there a reason why this is not tried yet? Or is this question just an result of my inability to find the right papers/repo's.
","['transformer', 'word-embedding', 'bert']",
"Why does the accuracy drop while the loss decrease, as the number of epochs increases?","
I've been trying to find the optimal number of epochs that I should train my neural network (that I just implemented) for.
The visualizations below show the neural network being run with a variable number of epochs. It is quite obvious that the accuracy increases with the number of epochs. However, at 75 epochs, we see a dip before the accuracy continues to rise. What is the cause of this?

","['neural-networks', 'overfitting', 'loss', 'accuracy', 'epochs']",
How do I create a custom gym environment based on an image?,"
I am trying to create my own gym environment for the A3C algorithm (one implementation is here). The custom environment is a simple login form for any site. I want to create an environment from an image. The idea is to take a screenshot of the web page and create an environment from this screenshot for the A3C algorithm. I know the doc and protocol for creating a custom environment. But I don't understand how to create an environment, exactly, based on a screenshot.
If I do so
self.observation_space = gym.spaces.Box(low=0, high=255, shape=(128, 128, 3), dtype=np.uint8)

I get a new pic.
Here's the algorithm that I am trying to implement (page 39 of the master's thesis Deep Reinforcement Learning in Automated User Interface Testing by Juha Eskonen).

","['reinforcement-learning', 'open-ai', 'gym', 'a3c']",
What is the definition of a loss function in the context of neural networks?,"
I have read what the loss function is, but I am not sure if I have understood it. For each neuron in the output layer, the loss function is usually equal to the square of the difference value of the neuron and the result we want. Is that correct?
","['neural-networks', 'objective-functions', 'mean-squared-error']","A loss function is what helps you ""train"" your neural network to do what you want it to do. A better way to word it to begin with would be an ""objective"" function. This function describes what objective you'd like your neural network to fit to (or to be good at).The loss function that you've described is ""squared error"", which, as the name suggests, is the squared difference between the expected output and the output from the neural network. This trains the network to match the expected output value.Other loss (or ""objective"") functions could train your network to look for different things. For example, training on cross entropy loss helps your network learn certain probabilities. That's why it's usually used for classification, like when you want to determine which digit from 0-9 was fed into your MNIST classifier."
How to design my Neural Network for Game AI,"
For my school project, I have to develop an agent to play my game.

The base I have is a 'GameManager' which call 2 AIs, each taking a random move to do.
To make my AI perform, I decided to make a deep RL algorithm.
Here is how I've designed my solution.
1st : the board is a 8x8 board. making 112 possible lines to draw.
2nd : on each decision, my Agent has to choose 1 line in the remaining one.
3rd : each decision the Agent take is one among 112 possible.
I read some codes on the internet, the most relevant for me was a 'CartPole' example, which is a cart we have to slide to prevent a mass to fall.
I made an architecture which is this one:
a game is simulated:
the board is clean, making all 112 possibilities available.
Our Agent is interroged by the gameManager to make a move passing him the actual state of the game
(the state shape is a 112*1 vector of Boolean values, 1 means a line can be drawn, 0 means there is already a line on this position)
(the action shape is a vector of 112*1 Boolean values, All values are set to 'False' except the line we want to draw)
So, our Agent return his move decision.
Each time our agent perform a move, i store the initial state, the action we take, the reward we get performing the action, the state we reach and a boolean to know if the game is done or not.
The rewards I choose are:
+1 if our action make us close a box,
-1 if our action make other close a box,
+10 if our action make us win the game,
-10 if our action make us loose the game
The point is it's my 1st Deep learning project and I'm not sure about the mecanism i'm doing.
when i launch a simulation, the Neural Network is running, but the move he does seems not to be better and better.
I give you the code I've wrote:
Here is the gameManager code:
while True:
hasMadeABox = False
gameIsEnd = False
rewardFCB = 0
doneFCB = False


if GRAPHIC_MODE:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            pygame.quit()
            sys.exit()
    disp_board()

if playerTurns==""1"":
    stateFCB = possibilitiesToBoolList(possible_moves[0])

boolArrayPossibleMoves = possibilitiesToBoolList(possible_moves[0])

theAction = players[playerTurns].play(boxes, possible_moves[0], boolArrayPossibleMoves, False)
#print(possible_moves[0])
#print(theAction)
if playerTurns ==""1"":
    actionFCB = theAction

if playerTurns==""1"":
    is_box = move(True, theAction)

elif playerTurns==""2"":
    is_box = move(False, theAction)

if is_box:
    if playerTurns ==""1"":
        #rewardFCB = 1
        rewardFCB = 1
        pass
    else:
        rewardFCB = -1
    hasMadeABox = True

if check_complete():
    gameIsEnd = True
    rewardFCB += 10 if score[0]>score[1] else -10 #does loosing is a reward null or negativ ?
    queueOfLastGame.pop(0)

    #Scotch pour affichage winrate
    isWin = 1 if score[0]>score[1] else -1
    queueOfLastGame.append(isWin)
    if queueOfLastGame.count(-1)+queueOfLastGame.count(1) > 0:
        print(queueOfLastGame.count(1)/(queueOfLastGame.count(-1)+queueOfLastGame.count(1)) * 100 , "" % Winrate"")

    doneFCB = True


if playerTurns==""1"" and hasMadeABox:
    #si c'est notre IA vient de faire un carré
    #on connait directement l'état qui succede
    nextStateFCB = possibilitiesToBoolList(possible_moves[0])

if playerTurns==""2"":
    nextStateFCB = possibilitiesToBoolList(possible_moves[0])


if nextStateFCB is not None:
    bufferSARS.append([stateFCB, actionFCB, rewardFCB, nextStateFCB, doneFCB])
    #ai_player_1.remember(stateFCB, actionFCB, rewardFCB, nextStateFCB, doneFCB)
    rewardFCB = 0
    nextStateFCB = None

if gameIsEnd:
    flushBufferSARS()
    reset()
    continue

#switch user to play if game is not end
if not hasMadeABox:
    playerTurns=""1"" if playerTurns == ""2"" else ""2""

And here's my code about the Agent:
class Agent:
def __init__(self, name, possibleActions, stateSize, actionSize, isHuman=False, alpha=0.001, alphaDecay=0.01, batchSize=2048, learningRate=0.1, epsilon= 0.9, gamma = 0.996, hasToTrain=True):

    self._memory = deque(maxlen=100000)
    self._actualEpisode=1
    self._episodes=7000
    self._name=name
    self._possibleAction=possibleActions
    self._isHuman=isHuman
    self._epsilon=epsilon
    self._epsilonDecay = 0.99
    self._epsilonMin = 0.05
    self._gamma=gamma
    self._stateSize=stateSize
    self._actionSize=actionSize
    self._alpha=alpha
    self._alphaDecay=alphaDecay
    self._hasToTrain=hasToTrain
    self._batchSize=batchSize

    self._totalIllegalMove = 0
    self._totalLegalMove = 0

    self._path = ""./modelWeightSave/""

    self._model = self._buildModel()


def save_model(self):
    self._model.save(self._path)

def getName(self):
    return self._name

def _buildModel(self):
    model = Sequential()

    model.add(Dense(128, input_dim=self._stateSize, activation='relu'))
    model.add(Dense(256, kernel_initializer='normal', activation='relu'))
    model.add(Dense(256, kernel_initializer='normal', activation='relu'))
    model.add(Dense(256, kernel_initializer='normal', activation='relu'))
    model.add(Dense(128, kernel_initializer='normal', activation='relu'))
    model.add(Dense(self._actionSize, kernel_initializer='normal', activation='relu'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self._alpha), metrics=['accuracy'])
    #if os.path.isfile(self._path):
        #model.load_weights(self._path)
    return model

def act(self, UNUSED_state, stateAsBool):
    playableIndexes = []
    for i in range(len(stateAsBool[0])):
        if stateAsBool[0][i] == 1:
            playableIndexes.append(i)
    indexForRand = playableIndexes[random.randint(0, len(playableIndexes) - 1)]

    if np.random.random() <= self._epsilon:
        action= [0]*self._actionSize
        action[indexForRand]=1

    else:
        arrayState = np.array(stateAsBool)

        action = self._model.predict(arrayState)
        #Set index of max esperence to 1, we play this line.
        tmp=[0]*self._actionSize
        tmp[np.argmax(action)] = 1
        action = tmp

        isLegalMove = True
        if sum(action) != 1:
            isLegalMove = False
        for i in range(len(action)):
            if action[i] == 1:
                if stateAsBool[0][i] == 0:
                    isLegalMove = False
                    break

        if isLegalMove:
            pass
            #print(""Legal move"")
        else:
            #print(""Illegal move"")
            #AI try to play on an already draw line, we choose a random line in remainings
            self._totalIllegalMove+=1
            action = [0] * self._actionSize
            action[indexForRand] = 1

    #print(""My AI took action : "",action)
    return action

def remember(self, state, action, reward, nextState, done):
    self._memory.append((state.copy(), action, reward, nextState, done))

    self._actualEpisode+=1
    if self._actualEpisode > self._episodes:
        self._actualEpisode = 0
        self.replay(self._batchSize)

def replay(self, batchSize):
    x_batch, y_batch = [], []
    minibatch = random.sample(self._memory, min(len(self._memory), self._batchSize))
    for state, action, reward, next_state, done in minibatch:
        actionIndex = np.argmax(action)
        y_target = self._model.predict(state)
        y_target[0][actionIndex] = reward if done else reward + self._gamma * np.max(self._model.predict(next_state)[0])
        x_batch.append(state[0])
        y_batch.append(y_target[0])
    self._model.fit(np.array(x_batch), np.array(y_batch),epochs=10, batch_size=len(x_batch), verbose=1)

    if self._epsilon > self._epsilonMin:
        self._epsilon *= self._epsilonDecay

    self.save_model()

def play(self, board, state, statesAsBool, player):
    actionTaken= self.act(state, statesAsBool)
    return actionTaken

def callBackOnPreviousMove(self, state, action, reward, nextState, done):
    self.remember(state, action, reward, nextState, done)

Example of output i have during fit method:
Epoch 1/10 

1/1 [==============================] - 0s 0s/step - loss: 109.9612 - accuracy: 0.8867
 
Epoch 2/10 

1/1 [==============================] - 0s 998us/step - loss: 109.9467 - accuracy: 0.8867 

Epoch 3/10 

1/1 [==============================] - 0s 0s/step - loss: 109.9456 - accuracy: 0.8867 

Epoch 4/10 

1/1 [==============================] - 0s 0s/step - loss: 109.9332 - accuracy: 0.8867 

Epoch 5/10 

1/1 [==============================] - 0s 998us/step - loss: 109.9339 - accuracy: 0.8867 

Epoch 6/10 

1/1 [==============================] - 0s 0s/step - loss: 109.9337 - accuracy: 0.8867 

Epoch 7/10 

1/1 [==============================] - 0s 997us/step - loss: 109.9305 - accuracy: 0.8867 

Epoch 8/10 

1/1 [==============================] - 0s 0s/step - loss: 109.9314 - accuracy: 0.8867 

Epoch 9/10 

1/1 [==============================] - 0s 0s/step - loss: 109.9306 - accuracy: 0.8867 

Epoch 10/10

1/1 [==============================] - 0s 0s/step - loss: 109.9301 - accuracy: 0.8867

My questions are:

Is my architecture good
(inputs = [0,0,1,1,0,0,1,0.....,1,0] (112x1 shape) to represent the state, and
output = [0,0,0,0,0,0,0,0,0,1,0,0,0...0,0,0,0] (112x1 shape with only one '1') )
to represent an action ?

How to nicely choose the architecture of the Neural Network model (self._model) (I have only the basics of Neural Network, so I don't really know all activation fonction, how to design the hiden layers, choose a loss...)

To train my NN, is it good to call the 'fit' function with (state, action) as parameter to make it learn?

Is there something really important I forget in my design to make it work?


","['machine-learning', 'ai-design', 'python', 'tensorflow', 'keras']",
"How am I supposed to code equation 4.57 from the book ""Machine Learning: An Algorithmic Perspective""?","
Consider the equation 4.57 (p. 108) from section 4.6 of the Book Machine Learning: An Algorithmic Perspective, where the derivative of the softmax function is explained
$$\delta_o(\kappa) = (y_\kappa - t_\kappa)y_\kappa(\delta_{\kappa K} - y_K),$$
which is derived from equation 4.55 (p. 107)
$$y_{\kappa}(1 - y_{\kappa}),$$
which is to compute the diagonal of the Jacobian, and equation 4.56  (p. 107)
$$-y_{\kappa}y_K$$
In the book, it is not explained how they go from 4.55 and 4.56 to 4.57, it is just given, but I cannot follow how it is derived.
Moreover, in equation 4.57, the Kronecker's delta function is used, but how would one handle cases $i=j$, then we must have some for loop? Does having an $i$ and $j$ imply we need a nested for loop?
Also, I have tried to just compute the derivative of softmax according to the $i=j$ case only, and my model was faster (since we're not computing the jacobian) and accurate, but this assumes the error function is logarithmic, which I would like to code the general case.
","['machine-learning', 'python', 'backpropagation', 'implementation', 'softmax']",
Should we also shuffle the test dataset when training with SGD?,"
When training machine learning models (e.g. neural networks) with stochastic gradient descent, it is common practice to (uniformly) shuffle the training data into batches/sets of different samples from different classes. Should we also shuffle the test dataset?
","['machine-learning', 'training', 'datasets', 'stochastic-gradient-descent', 'testing']","Shuffling affects learning (i.e. the updates of the parameters of the model), but, during testing or validation, you are not learning. So, it should not make any difference whether you shuffle or not the test or validation data (unless you are computing some metric that depends on the order of the samples), given that you will not be computing any gradient, but just the loss or some metric/measure like the accuracy, which is not sensitive to the order or the samples you use to compute it. However, the specific samples that you use affects the computation of the loss and these quality metrics. So, how you split your original data into training, validation and test datasets affects the computation of the loss and metrics during validation and testing.Let me describe how gradient descent (GD) and stochastic gradient descent (SGD) are used to train machine learning models and, in particular, neural networks.When training ML models with GD, you have a loss (aka cost) function $L(\theta; D)$ (e.g. the cross-entropy or mean squared error) that you are trying to minimize, where $\theta \in \mathbb{R}^m$ is a vector of parameters of your model and $D$ is your labeled training dataset.To minimize this function using GD, you compute the gradient of your loss function $L(\theta; D)$ with respect to the parameters of your model $\theta$ given the training samples. Let's denote this gradient by $\nabla_\theta L(\theta; D) \in \mathbb{R}^m$. Then we perform a step of gradient descent$$
\theta \leftarrow \theta - \alpha \nabla_\theta L(\theta; D) \label{1}\tag{1}
$$You can also minimize $L$ using stochastic gradient descent, i.e. you compute an approximate (or stochastic) version of $ \nabla_\theta L(\theta; D)$, which we can denote as $\tilde{\nabla}_\theta L(\theta; B) \approx \nabla_\theta L(\theta; D)$, which is typically computed with a subset of $B$ of your training dataset $D$, i.e. $B \subset D$ and $|B| < |D|$. The step of SGD is exactly the same as the step of GD, but we use $\tilde{\nabla}_\theta L(\theta; B)$$$
\theta \leftarrow \theta - \alpha \tilde{\nabla}_\theta L(\theta; B) \label{2}\tag{2}
$$
If we split $D$ into $k$ subsets (or batches) $B_i$, for $i=1, \dots, k$ (and these subsets usually have the same size, i.e. $|B_i| = |B_j|, \forall i$, apart from one of them, which may contain fewer samples), then the SGD step needs to be performed $k$ times, in order to go through all training samples.Given that $\tilde{\nabla}_\theta L(\theta; B_i) \approx \nabla_\theta L(\theta; D), \forall i$, it should be clear that the way you split the samples into batches can affect learning (i.e. the updates of the parameters).For instance, you could consider your dataset $D$ as an ordered sequence/list, and just split it into $k$ sub-sequences. Without shuffling this ordered sequence before splitting, you will always get the same batches, which means that, if there's some information associated with the specific ordering of this sequence, then it may bias the learning process. That's one of the reasons why you may want to shuffle the data.So, you could uniformly choose samples from $D$ to create your batches $B_i$ (and this is a way of shuffling, in the sense that you will be uniformly building these batches at random), but you can also sample differently and you could also re-use the same samples in different batches (i.e. sampling with replacement). Of course, all these approaches can affect how learning proceeds.Typically, when analyzing the convergence properties of SGD, you require that your samples are i.i.d. and that the learning rate $\alpha$ satisfies some conditions (the Robbins–Monro conditions). If that's not the case, then SGD may not converge to the correct answer. That's why sampling or shuffling can play an important role in SGD.During testing or validation, you are just computing the loss or some metric (like the accuracy) and not a stochastic gradient (i.e. you are not updating the parameters, by definition: you just do it during training). The way you compute the loss or accuracy should not be sensitive to the order of the samples, so shuffling should not affect the computation of the loss or accuracy. For instance, if you use the mean squared error, then you will need to compute\begin{align}
L(\theta; D_\text{test}) 
&= \operatorname {MSE} \\
&= {\frac {1}{n}}\sum _{i=1}^{n}(f_\theta(x_i)-{\hat {y_{i}}})^{2}\\
&= {\frac {1}{n}}\sum _{i=1}^{n}(y_{i}-{\hat {y_{i}}})^{2}
\end{align}whereThis is an average, so it doesn't really matter whether you shuffle or not. Of course, it matters which samples you use though!Here you can find some informal answers to the question ""Why do we shuffle the training data while training a neural network?"". There are other papers that partially answer this and/or other related questions more formally, such as this or this."
Can any area of math come into play in Machine Learning Research?,"
As I read online following areas in mathematics come into play in ML research

Linear Algebra
Calculus
Differential Equations
Probability
Statistics
Discrete Mathematics
Optimization
Analytic Geometry
Topology
Numerical and Real Analysis

Can / Are any other areas of math used in ML research? If so what other areas? ex: Number theory
","['machine-learning', 'math', 'research']",
Can XGBoost solve XOR problem?,"
I've read that decision trees are able to solve XOR operation so I conclude that XGBoost algorithm can solve it as well.
But my tests on the datasets (datasets that should be highly ""xor-ish"") do not produce good results, so I wanted to ask whether XGBoost is able to solve this type of problem at all, or maybe I should use a different algorithm like ANN?
EDIT: I found a similar question with a negative answer here.
Could someone please confirm that XGBoost cannot perform XOR operation due to ""greedy approach"" and whether this can maybe be changed in parameters?
","['machine-learning', 'problem-solving', 'gradient-boosting', 'xor-problem']",
Why does a negative reward for every step really encourage the agent to reach the goal as quickly as possible?,"
If we shift the rewards by any constant (which is a type of reward shaping), the optimal state-action value function (and so optimal policy) does not change. The proof of this fact can be found here.
If that's the case, then why does a negative reward for every step encourage the agent to quickly reach the goal (which is a specific type of behavior/policy), given that such a reward function has the same optimal policy as the shifted reward function where all rewards are positive (or non-negative)?
More precisely, let $s^*$ be the goal state, then consider the following reward function
$$
r_1(s, a)=
\begin{cases}
-1, & \text{ if }  s \neq s^*\\
0, & \text{ otherwise}
\end{cases}
$$
This reward function $r_1$ is supposed to encourage the agent to reach $s^*$ as quickly as possible, so as to avoid being penalized.
Let us now define a second reward function as follows
\begin{align}
r_2(s, a) 
&\triangleq r_1(s, a) + 1\\
&=
\begin{cases}
0, & \text{ if }  s \neq s^*\\
1, & \text{ otherwise}
\end{cases}
\end{align}
This reward function has the same optimal policy as $r_1$, but does not incentivize the agent to reach $s^*$ as quickly as possible, given that the agent does not get penalized for every step. So, in theory, $r_1$ and $r_2$ lead to the same behavior. If that's the case, then why do people say that $r_1$ encourage the agents to reach $s^*$ as quickly as possible? Is there a proof that shows that $r_1$ encourages a different type of behaviour than $r_2$ (and how is that even possible given what I have just said)?
","['reinforcement-learning', 'proofs', 'reward-shaping', 'reward-functions']",
Generating data from a High-Res. RGB image for a CNN,"
Say I want to build a detection model that detects the existence of X or NO X.
The only piece of information I have, though, is a high res. RGB image, say 100k (width) x ~1000 pixels (height).
Let's also assume I cannot browse the internet to grab more data. I am stuck with this High resolution image. Can I somehow ""slice"" this image into multiple images and use said images as input data for my CNN?
How would I do so?
","['convolutional-neural-networks', 'computer-vision']",
"If REINFORCE agent suddenly drops, how do I verify if it's due to catastrophic forgetting?","
I am using the default implementations of REINFORCE, DQN and c51 available from the tf.agents repo (links). As you can see, DQN manages to improve performance while REINFORCE seems to suffer from catastrophic forgetting. OTOH, c51 is not able to learn much and performs like a random policy throughout.
The environment looks like this -

action = [66, 1]
states = [20, 1]
max possible state value = 20
steps per episode = 20
Hidden Layer dimension = (128, 128)
learning rate = 0.001 (constant throughout)
Epsilon (exploration factor) = 0.2 with decay of 0.05 every 4000 episodes
Discount factor = 0.9
relay memory size = 10,000

Every episode runs for 20 steps and the rewards are collected for every step.

actual episode value is the plot is the x-axis value multiplied by 50
What could be the possible reasons for such a performance of c51 and DQN? And based on the state space, are my hyperparameters correct or some of them need more tuning? I will increase the replay memory size but other than that to check for catastrophic forgetting, I am not sure how to diagnose other issues.
","['reinforcement-learning', 'deep-learning', 'dqn', 'reinforce', 'catastrophic-forgetting']",
How to choose the first action in a Monte Carlo Tree Search?,"
I'm working on reimplementing the MuZero paper. In the description of the MCTS (page 12), they indicate that a new node with associated state $s$ is to be initialized with $Q(s,a) = 0$, $N(s,a) = 0$ and $P(s,a) = p_a$. From this, I understand that the root node with state $s_0$ will have edges with zero visits each, zero value and policy evaluated on $s_0$ by the prediction network.
So far so good. Then they explain how actions are selected, according to the equation (also on page 12):

But for the very first action (from the root node) this will give a vector of zeros as argument to the argmax: $Q(s_0,a) = 0$ and $\sum_bN(s_0,b)=0$, so even though $P(s_0,a)$ is not zero, it will be multiplied by a zero weight.
Surely there is a mistake somewhere? Or is it that the very first action is uniformly random?
","['monte-carlo-tree-search', 'muzero']",
"How can I improve the performance of my approach to solving a 1-player version of the card game ""The Game"" by Steffen Benndorf?","
I would like to create an AI for the 1 player version of the card game called ""The Game"" by Steffen Benndorf (rules here: https://nsv.de/wp-content/uploads/2018/05/the-game-english.pdf).
The game works with four rows of cards. Two rows are in ascending order (numbers 1–99), and two rows are in descending order (numbers 100–2). The goal is to lay as many cards as possible, all 98 if possible, in four rows of cards. The player can have a maximum of 8 cards in his hand and has to play at least 2 cards before drawing again. He can only play a greater value on an ascending row and a smaller value on a descending row with one single exception that lets him play in the reverse order: whenever the value of the number card is exactly 10 higher or lower.
I already implemented a very simple hard-coded AI that just picks the card with the smallest difference and prioritizes a +10/-10 play when possible. With some optimizations, I can get the AI to score 20 points (the number of cards left) on average which is decent (less than 10 points in an excellent score) but I'm stuck there and I would like to go further.
As there is randomness because of the draw pile, I was wondering if it was possible to implement a robust and not hard-coded AI to play this game.
Currently, my AI is playing piecemeal with a very simple heuristic. I do not see how to improve this heuristic, so I am wondering if it is possible to improve the performance by having a view over several turns for example. But I don't see how to simulate the next rounds since they will depend on the cards drawn.
","['game-ai', 'monte-carlo-tree-search', 'algorithm-request', 'minimax', 'heuristics']","There are a few different ways to improve on your simple heuristic approach, but they mostly resolve to these three things:Find a better heuristic. This could be done by calculating probabilities of results, or running loads of training simulations and somehow tuning the heuristic function.Look-ahead search/planning. There are many possible search algorithms. Most rely on you being able to simulate the impact of future decisions before taking them.Take account of more player knowledge. So far your simple heuristic does not take account of which cards have already been played (thus which values remain to be drawn).Currently my AI is playing piecemeal with a very simple heuristic. I do not see how to improve this heuristic so I am wondering if it is possible to improve the performance by having a view over several turns for example. But I don't see how to simulate the next rounds since they will depend on the cards drawn.I think the main conceptual barrier you have to improvements is how to account for the complex behaviour of probabilities for drawing specific useful cards. There are a few ways to do this, but I think the simplest would be some kind of rollout (simulated look ahead), which might lead to more sophisticated algorithm such as Monte Carlo Tree Search (MCTS).Here's how a really simple variant might work:For each possible choice of play in the game that you are currently looking at:Simulate the remaining deck (shuffle a copy of the known remaining cards)Play a simulation (a ""rollout"") to the end of game against the simulated deck using a simple heuristic (your current greedy choice version should be good as long as it is fast enough, but even random choices can work). Take note of the final score.Repeat 1.1 and 1.2 as many times as you can afford to (given allowed decision time). Average the result and save it as a score for the choice of play being considered.Instead of choosing the next play by your heuristic, choose the one that scores best out of all the simulations.This statistical mean of samples works in a lot of cases because it avoids the complexity and time-consuming calculations that would be required to make a perfect decision analytically from probability theory. The important things it does in your case is look-ahead planning plus taking account of additional knowledge that the player has about the state of the game.MCTS is like the above but nested so that the simulations are made from multiple starting points.In terms of robustness, provided you run enough rollouts per decision to be confident about the mean scores, then it should be OK."
Why not use the target network in DQN as the predictor after training,"
Target network in DQN is known to make the network more stable, and the loss is like ""how good I'm now compared to using the target"". What I don't understand is, if the target network is the stable one, why do we keep using/saving the first model as the predictor instead of the target?
I see in the code everywhere:

Model
Target model
Train model
Copy to target
Get loss between them

At the end, the model is saved and used for prediction and not the target.
","['reinforcement-learning', 'dqn', 'deep-rl']",
"If neurons performed the operation of an entire layer, would that make the neural network more effective?","
(I have a very primitive understanding of neural networks, so please forgive the lack of technicality here.)
I am used to seeing a neuron in a neural network as something that-

Takes the inputs and multiplies them by their weights,
then sums them up,
and after that it applies the activation function to the sum.

Now, what if it was ""smarter""? Say, a single neuron could do the function of an entire layer in a network, could that make the network more effective? This comes from an article I was reading at Quanta, where the author says:

Later, Mel and several colleagues looked more closely at how the cell might be managing multiple inputs within its individual dendrites. What they found surprised them: The dendrites generated local spikes, had their own nonlinear input-output curves and had their own activation thresholds, distinct from those of the neuron as a whole. The dendrites themselves could act as AND gates, or as a host of other computing devices.


...realised that this meant that they could conceive of a single neuron as a two-layer network. The dendrites would serve as nonlinear computing subunits, collecting inputs and spitting out intermediate outputs. Those signals would then get combined in the cell body, which would determine how the neuron as a whole would respond.

My thoughts: I know that Backpropagation is used to ""teach"" the network in the normal case, and the fact that neurons are simply activation buttons is somehow related to that. So, if neurons were to be more complicated, it would reduce efficiency. However, I am not sure of this: why would complex individual components make the network less effective?
","['neural-networks', 'artificial-neuron', 'hidden-layers']","Say, a single neuron could do the function of an entire layer in a network, could that make the network more effective?That depends what you mean by ""more effective"". In terms of number of neurons to achieve the same result, then you should need fewer units. In terms of being able to calculate an end result for any specific problem, then no, because you can generally solve a problem using simpler units by adding more of them.If this could somehow be done using less resources, it might reduce overall costs. In a biological system, there are possibly overheads per cell in order to maintain it on top of the costs for calculation, so it may be better to do more than the simplest calculation in each cell. Further to that, there may be an optimal amount of processing that each cell could do (this is all conjecture on my part).In an artifical neural network, the calculations are the only thing being considered, there is no separate overhead per neuron.There are neural network architectures with complex ""sub-units"". Probably the most well known are the recurrent neural network designs for LSTM and gated recurrent units (GRU), plus ""skip connections"" in residual neural networks. For efficiency these are normally processed in groups per layer with matrix processing functions, but you can also view them as per-neuron complexities.I am not sure of this- why would complex individual components make the network less effective?If the complexity was not used, or not really needed, in some of the units, then it would be wasted capacity. In a biological system, this might correspond to maintaining cells larger than they needed to be. In an artificial system, it would mean using memory and CPU to calculate interim values that were not needed for the task at hand."
Are there any meaningful books entirely written by an artificial intelligence?,"
Are there any meaningful books entirely written by an artificial intelligence? I mean something with meaning, unlike random words or empty books.
Something that can be charactersed as fiction literature.
If yes, then I think it is also interesting to know if any of those books is available for sale. Is there a specific name for such books? Like ""robot books""?
","['natural-language-processing', 'applications', 'natural-language-understanding', 'natural-language-generation']",
How does MuZero learn to play well for both sides of a two-player game?,"
I'm coding my own version of MuZero. However, I don't understand how it supposed to learn to play well for both players in a two-player game.
Take Go for example. If I use a single MCTS to generate an entire game (to be used in the training stage), couldn't MuZero learn to play badly for black in order to become good at predicting a win for white? What is forcing it to play well at every turn?
","['reinforcement-learning', 'self-play', 'muzero']",
Single-Shot Learning for Object Re-Identification,"
I am looking for a way to re-identify/classify/recognize x real life objects (x < 50) with a camera. Each object should be presented to the AI only once for learning and there's always only one of these objects in the query image. New objects should be addable to the list of ""known"" objects. The objects are not necessarily part of ImageNet nor do I have a training dataset with various instances of these objects.
Example:

In the beginning I have no ""known"" objects. Now I present a
smartphone, a teddy bear and a pair of scissors to the system. It
should learn to re-identify these three objects if presented in the
future. The objects will be the exact same objects, i.e. not a different phone, but definitely in a different viewing angle, lighting etc.

My understanding is that I would have to place each object in an embedding space and do a simple nearest neighbor lookup in that space for the queries. Maybe just use a trained ResNet, cut off the classification and simply use the output vector for each object? Not sure what the best way would be.
Any advice or hint to the right direction would be highly appreciated.
","['deep-learning', 'convolutional-neural-networks', 'classification', 'object-recognition', 'one-shot-learning']","I have put my initial idea to a test and used a small pretrained CNN (MobileNet) to compute features for reference images and stored the feature vectors in a ""database"". Query images go through the exact same network and the resulting feature vector is used for nearest neighbor retrieval in the DB.At least with my 5 test reference images and several query images it worked without a single failed ""classification"". However, I am not sure if it will stand up to a larger test..."
How can I predict an anomaly based on FFT of multiple signals?,"
I have 999 signals, each with separate day timestamp, each T=10s long, sampled with fs=25kHz. This gives N=250,000 samples in total.
My task was to obtain the averaged magnitude spectrum for each signal. For example, for k=100, the signal is divided into k-equal fragments, 0.1s and 2500 samples long. Then FFT is computed on all fragments and mean value is calculated for all spectral component (mean for each frequency from DC to Nyquist Frequency).
The averaged spectrum for each signal for k=100 contains 1251 values and 1251 frequency points (0-fs/2).
My question is, how can prepare the train dataset for multiple Machine Learning models based on that data, so I can predict when is the threshold time, before the failure of the machine occurs?
Do i treat each spectral component (frequency) as separate feature? Or there is a different approach ?
",['python'],
Is there a way to provide multiple masks to BERT in MLM task?,"
I'm facing a situation where I've to fetch probabilities from BERT MLM for multiple words in a single sentence.
Original : ""Mountain Dew is an energetic drink""
Masked : ""[MASK] is an energetic drink""

But BERT MLM task doesn't consider two tokens at a time for the MASK. I strongly think that there should be some sort of work around that I'm unable to find other than fine-tuning.
","['natural-language-processing', 'bert', 'language-model']","I've found the answer in the original BERT git repo***** New May 31st, 2019: Whole Word Masking Models *****This is a release of several new models which were the result of an
improvement the pre-processing code.In the original pre-processing code, we randomly select WordPiece
tokens to mask. For example:The new technique is called Whole Word Masking. In this case, we
always mask all of the the tokens corresponding to a word at once. The
overall masking rate remains the same.The training is identical -- we still predict each masked WordPiece
token independently. The improvement comes from the fact that the
original prediction task was too 'easy' for words that had been split
into multiple WordPieces.This can be enabled during data generation by passing the flagto create_pretraining_data.py."
Find object's location in an area using computer vision,"
I'm trying to see how to detect the location of a soccer ball in the field using the live camera. What are some ways to achieve this?
1- Assuming we have a fixed camera with a wide shot. How to find the ball location on the actuall field?
2- A camera is zooming into the ball. But we know the location of the camera and maybe it's turning angle. Can we estimate the ball location on the field using this info? Or maybe we need additional info? Can we do it with two cameras as reference points?
Any thoughts would be helpful.


","['neural-networks', 'computer-vision', 'object-detection', 'object-recognition']",
Should binary feature be in one or two columns in deep neural networks?,"
Let's assume I have a simple feedforward neural network whose input contains binary 0/1 features and output is also binary two classes.
Is it better, worse, or maybe totally indifferent, for every such binary feature to be in just one column or maybe it would be better to split one feature into two columns in a way that the second column will have the opposite value, like that:
feature_x (one column scenario) 

[0]

[1]

[0]


feature_x (two columns scenario)

[0, 1]

[1, 0]

[0, 1]

I know this might seem a bit weird and probably it is not necessary, but I have a feeling like there might be a difference for a network especially for its inner workings and how neurons in the next layers see such data. Has anyone ever researched that aspect?
","['deep-learning', 'deep-neural-networks', 'data-preprocessing', 'geometric-deep-learning', 'binary-classification']","You're simply adding a redundant feature by having it as two: $X_2 = 1 - X_1$. It would be equally useful to duplicate the first column. At best this will not improve your model, at worst it will decrease accuracy."
Should the exploration rate be updated at the end of the episode or at every step?,"
My agent uses an $\epsilon$-greedy strategy to learn. The exploration rate (i.e. $\epsilon$) decays throughout the training. I've seen examples where people update $\epsilon$ every time an action is taken, while others update it at the end of the episode. If updated at every action, $\epsilon$ is more continuous. Does it matter? Is there a standard? Is one better than another?
","['reinforcement-learning', 'dqn', 'exploration-exploitation-tradeoff', 'epsilon-greedy-policy']",
Get object's orientation or angle after object detection,"
I'm trying to get a detected car's orientation when object detection is applied. For instance, when we apply object detection on a car and get a bounding box, is there any ways or methods to calculate where the heading is or the orientation or direction of the car (just 2D plane is fine)?
Any thoughts or ideas would be helpful.

","['neural-networks', 'ai-design', 'object-detection', 'object-recognition', 'yolo']",
Could the data augmentation lead to the model learning features which corresponds to data augmented data and not to the real data?,"
I am trying to train a Unet network with Synthetic data to do binary segmentation due to the fact that is is not easy to collect real data.
And there is something in the training process that I do not understand.
I have a gap in the IoU metrics between the training and the validation (despite having really similar data).
My training Iou is around 95 % and my validation is around 70 %.
And the dice loss is around 0.007.
The IoU is calculated on the inverted mask used for the loss.
So I do not understand why there is this gap whereas the images in validation has been created from the same background dataset and the same object dataset which has been randomly placed on background ( + rotation and rescaled randomly). The only difference is an aggressive data augmentation used for training dataset.
In my opinion, it is not overfitting since the loss value and comportment is very similar for train and val. Moreover, it seems very unlikely to me that the model overfit with same backgrounds and objects or at least model should have very good IoU for train and val if it was overfitting.
So could the data augmentation  lead to the model learning features which corresponds to data augmented data (even if loss is similar) and not to the real data explaining the gap in IoU between train and val ?
","['training', 'objective-functions', 'image-segmentation', 'metric', 'u-net']",
Why do we resize images before using them for object detection?,"
In object detection, we can resize images by keeping the ratio the same as the original image, which is often known as ""letterbox"" resize.
My questions are

Why do we need to resize images? If we resize images to have all the same dimensions, given that some original images are too long vertically or horizontally, we will lose a lot of features in those images.

If the ""letterbox"" method is better than ""normal resize"" (i.e. without keeping the aspect ratio, e.g. the result of the application of OpenCV's resize function with the parameter interpolation set to cv2.INTER_AREA), why don't people apply it in the classification task?


","['classification', 'computer-vision', 'object-detection', 'image-processing', 'data-preprocessing']",
Distinguishing between handwritten compound fraction and subtraction,"
I am working in a project named ""Handwritten Math Evaluation"". So what basically happens in this is that there are 11 classes of (0 - 9) and (+, -) each containing 50 clean handwritten digits in them. Then I trained a CNN model for it with 80 % of data used in training and 20 % using in testing of model which results in an accuracy of 98.83 %. Here is the code for the architecture of CNN model:
import pandas as pd 
import numpy as np 
import pickle 
np.random.seed(1212) 
import keras 
from keras.models import Model 
from keras.layers import *
from keras import optimizers 
from keras.layers import Input, Dense 
from keras.models import Sequential 
from keras.layers import Dense 
from keras.layers import Dropout 
from keras.layers import Flatten 
from keras.layers.convolutional import Conv2D 
from keras.layers.convolutional import MaxPooling2D 
from keras.utils import np_utils 
from keras import backend as K  
from keras.utils.np_utils import to_categorical 
from keras.models import model_from_json
import matplotlib.pyplot as plt
model = Sequential() 
model.add(Conv2D(30, (5, 5), input_shape =(28,28,1), activation ='relu')) 
model.add(MaxPooling2D(pool_size =(2, 2))) 
model.add(Conv2D(15, (3, 3), activation ='relu')) 
model.add(MaxPooling2D(pool_size =(2, 2))) 
model.add(Dropout(0.2)) 
model.add(Flatten()) 
model.add(Dense(128, activation ='relu')) 
model.add(Dense(50, activation ='relu')) 
model.add(Dense(12, activation ='softmax')) 
# Compile model 
model.compile(loss ='categorical_crossentropy', 
            optimizer ='adam', metrics =['accuracy']) 
model.fit(X_train, y_train, epochs=1000)

Now each image in dataset is preprocessed as follows:
import cv2
im = cv2.imread(path)
im_gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, im_th = cv2.threshold(im_gray, 90, 255, cv2.THRESH_BINARY_INV)
ctrs, hier = cv2.findContours(im_th.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
rects = [cv2.boundingRect(ctr) for ctr in ctrs]
rect = rects[0]
im_crop =im_th[rect[1]:rect[1]+rect[3],rect[0]:rect[0]+rect[2]]
im_resize = cv2.resize(im_crop,(28,28))
im_resize = np.array(im_resize)
im_resize=im_resize.reshape(28,28)

I have made an evaluation function which solves simple expression like 7+8 :-
def evaluate(im):
    s = ''
    data = []
    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
    ret, im_th = cv2.threshold(im_gray, 90, 255, cv2.THRESH_BINARY_INV)
    ctrs, hier = cv2.findContours(im_th.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    sorted_ctrs = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])
    boundingBoxes = [cv2.boundingRect(c) for c in ctrs]
    look_up = ['0','1','2','3','4','5','6','7','8','9','+','-'] 
    i=0
    for c in ctrs:
        rect = boundingBoxes[i]
        im_crop = im_th[rect[1]:rect[1]+rect[3], rect[0]:rect[0]+rect[2]]
        im_resize = cv2.resize(im_crop,(28,28))
        im_resize = np.array(im_resize)
        im_resize = im_resize.reshape(28,28,1)
        data.append(im_resize)
        i+=1
    data = np.array(data)
    predictions = model.predict(data)
    i=0
    while i<len(boundingBoxes):
        rect = boundingBoxes[i]
        print(rect[2],rect[3])
        print(predictions[i])
        s += look_up[predictions[i].argmax()]
        i+=1
    return s

I need help extending this to compound fractions, but the problem is that the vinculum / is identical to the subtraction sign - when resized to (28, 28). So I need help in distinguishing between them.
This is my first question, so please let me know if any details are left.
","['computer-vision', 'tensorflow', 'python', 'keras', 'opencv']",
What are the state-of-the-art results in OpenAI's gym environments?,"
What are the state-of-the-art results in OpenAI's gym environments? Is there a link to a paper/article that describes them and how these SOTA results were calculated?
","['reinforcement-learning', 'reference-request', 'gym', 'state-of-the-art']",
How to manually draw a $k$-NN decision boundary with $k=1$ given the dataset and labels?,"
How to manually draw  a $k$-NN decision boundary with $k=1$ knowing the dataset

the labels are

and the euclidean distance between two points is defined as

","['machine-learning', 'k-nearest-neighbors']","This is a rather involved task. What to do from a high-level theoretical perspective might be easy to see, but it's difficult putting that into code from scratch.Doing this in Python using existing libraries in not too complicated, though.
See for example this tutorial or this StackOverflow post.Edit:Theoretically, I would first plot (draw) the points from your dataset in a graph and then watch out for ""decision points"" half-way in between any two near-by points (from the dataset) from distinct classes.For the next step, keep in mind those decision points. Given close-by data points from classes (e.g.) A and B, imagine a straight line connecting these two points from separate classes. Next, take the point half-way along that imaginary line (i.e. your decision point) and draw a ""soft"" line (maybe using pencil instead of pen) orthogonal/perpendicular to that imaginary line which intersects the imaginary line in the decision point.
Do that for all combinations of ""reasonably"" close points from different classes.Parts of the lines you have just drawn will define the final decision boundary. Next, think of each line as consisting of multiple elements, which are separated from one another by means of the points of intersection with other drawn lines. In other words, split lines into elements wherever they intersect other lines. Now, decide which of these elements to outline as the eventual decision boundary (finally using a pen instead of pencil). This step simply involves human intelligence and is difficult to describe. Whenever a line's element accurately separates (logically speaking) two classes, indicate it using a pen. Otherwise, if it does not contribute to separating two classes, don't indicate it.After having indicated the final decision boundary using a pen, simply erase the pencil drawings. Now, you should be left with a decision boundary.I hope this is clear and accurate enough."
Are there any rules for choosing batch size? [duplicate],"







This question already has answers here:
                                
                            




How do I choose the optimal batch size?

                                (3 answers)
                            

Closed 2 years ago.



I am training a CNN with a batch size of 128, but I have some fluctuations in the validation loss, which are greater than one. I want to increase my batch size to 150 or 200, but, in the code examples I have come across, the batch size is always something like 32, 64, 128, or 256. Is it a rule? Can I use other values for it?
","['machine-learning', 'convolutional-neural-networks', 'training', 'batch-size']",
How do I derive the gradient of the log-likelihood of an RBM?,"
In a Restricted Boltzmann Machine (RBM), the likelihood function is:
$$p(\mathbf{v};\mathbf{\theta}) = \frac{1}{Z} \sum_{\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}$$
Where $E$ is the energy function and $Z$ is the partition function:
$$Z = \sum_{\mathbf{v},\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}$$
The log-likelihood function is therefore:
$$ln(p(\mathbf{v};\mathbf{\theta})) = ln\left(\sum_{\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}\right) - ln\left(\sum_{\mathbf{v},\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}\right)$$
Since the log-likelihood function cannot be computed, its gradient is used instead with gradient descent to find the optimal parameters $\mathbf{\theta}$:
$$\frac{\partial ln(p(\mathbf{v};\mathbf{\theta}))}{\partial \mathbf{\theta}} = -\frac{1}{\sum_{\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}} \sum_{\mathbf{h}} \left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \cdot e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}\right] + \frac{1}{\sum_{\mathbf{v},\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}} \sum_{\mathbf{v},\mathbf{h}} \left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \cdot e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}\right]$$
Since:
$$p(\mathbf{h}|\mathbf{v}) = \frac{p(\mathbf{v},\mathbf{h})}{p(\mathbf{v})} = \frac{\frac{1}{Z} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}}{\frac{1}{Z} \sum_{\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}} = \frac{e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}}{\sum_{\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}}$$
Then:
$$\frac{\partial ln(p(\mathbf{v};\mathbf{\theta}))}{\partial \mathbf{\theta}} = -\sum_{\mathbf{h}} \left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \cdot p(\mathbf{h}|\mathbf{v}) \right] + \frac{1}{\sum_{\mathbf{v},\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}} \sum_{\mathbf{v},\mathbf{h}} \left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \cdot e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}\right]$$
Also, since:
$$ \frac{e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}}{Z} = \frac{e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}}{\sum_{\mathbf{v},\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\mathbf{\theta})}} = p(\mathbf{v},\mathbf{h})$$
Then:
$$\begin{align} \frac{\partial ln(p(\mathbf{v};\mathbf{\theta}))}{\partial \mathbf{\theta}} &= -\sum_{\mathbf{h}} \left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \cdot p(\mathbf{h}|\mathbf{v}) \right] + \sum_{\mathbf{v},\mathbf{h}} \left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \cdot p(\mathbf{v},\mathbf{h})\right] \\ &= -\mathbb{E}_{p(\mathbf{h}|\mathbf{v})}\left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \right] + \mathbb{E}_{p(\mathbf{v},\mathbf{h})}\left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \right] \end{align}$$
Since both of these are expectations, they can be approximated using Monte Carlo integration:
$$ \frac{\partial ln(p(\mathbf{v};\mathbf{\theta}))}{\partial \mathbf{\theta}} \approx -\frac{1}{N} \sum_{i = 1}^{N} \left[\frac{\partial E(\mathbf{v},\mathbf{h}_i;\mathbf{\theta})}{\partial \mathbf{\theta}} \right] + \frac{1}{M} \sum_{j=1}^{M} \left[\frac{\partial E(\mathbf{v}_j,\mathbf{h}_j;\mathbf{\theta})}{\partial \mathbf{\theta}} \right] $$
The first term can be computed beacuse it is easy to sample from $p(\mathbf{h}|\mathbf{v})$. However, it is difficult to sample from $p(\mathbf{v},\mathbf{h})$ directly, but since it is easy to sample from $p(\mathbf{v}|\mathbf{h})$, then Gibbs sampling is used to sample from both $p(\mathbf{h}|\mathbf{v})$ and $p(\mathbf{v}|\mathbf{h})$ to approximate a sample from $p(\mathbf{v},\mathbf{h})$.
My questions are:

Is my understanding and math correct so far?
In the expression for the gradient of the log-likelihood, can expectations be interchanged with partial derivatives such that:

$$\begin{align} \frac{\partial ln(p(\mathbf{v};\mathbf{\theta}))}{\partial \mathbf{\theta}} &= -\mathbb{E}_{p(\mathbf{h}|\mathbf{v})}\left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \right] + \mathbb{E}_{p(\mathbf{v},\mathbf{h})}\left[\frac{\partial E(\mathbf{v},\mathbf{h};\mathbf{\theta})}{\partial \mathbf{\theta}} \right] \\ &= - \frac{\partial}{\partial \mathbf{\theta}} \mathbb{E}_{p(\mathbf{h}|\mathbf{v})}\left[E(\mathbf{v},\mathbf{h};\mathbf{\theta}) \right] + \frac{\partial}{\partial \mathbf{\theta}} \mathbb{E}_{p(\mathbf{v},\mathbf{h})}\left[E(\mathbf{v},\mathbf{h};\mathbf{\theta}) \right] \\ &= \frac{\partial}{\partial \mathbf{\theta}} \left(\mathbb{E}_{p(\mathbf{v},\mathbf{h})}\left[E(\mathbf{v},\mathbf{h};\mathbf{\theta}) \right] - \mathbb{E}_{p(\mathbf{h}|\mathbf{v})}\left[E(\mathbf{v},\mathbf{h};\mathbf{\theta}) \right] \right) \\ &\approx \frac{\partial}{\partial \mathbf{\theta}} \left(\frac{1}{M} \sum_{j=1}^{M} \left[E(\mathbf{v}_j,\mathbf{h}_j;\mathbf{\theta}) \right] - \frac{1}{N} \sum_{i = 1}^{N} \left[E(\mathbf{v},\mathbf{h}_i;\mathbf{\theta}) \right] \right) \end{align}$$

After approximating the gradient of the log-likelihood, the update rule for the parameter vector $\mathbf{\theta}$ is:

$$\mathbf{\theta}_{t+1} = \mathbf{\theta}_{t} + \epsilon \frac{\partial ln(p(\mathbf{v};\mathbf{\theta}))}{\partial \mathbf{\theta}}$$
Where $\epsilon$ is the learning rate. Is this update rule correct?
","['machine-learning', 'training', 'math', 'restricted-boltzmann-machine', 'probabilistic-graphical-models']",
Getting bounding box/boundaries from segmentations in UNet Nuclei Segmentation,"
From my understanding, in a tissue where nuclei are present and need to be detected, we need to predict bounding boxes (either rectangular/circular or in the shape of the nucleus, i.e. as in instance segmentation). However, a lot of research papers start with semantic segmentation. Again, what I understood is semantic segmentation won't give the location, bounding box or count of nuclei. It will just tell that some stuff is probably nuclei and rest is probably background.
So, what is the bridging that I am missing when trying to detect nuclei from semantic segmentation. I have personally done semantic segmentation but I can't seem to count/predict bounding boxes because I can't understand how to do that (for example if semantic segmentation gave a probable region for nuclei which is actually a mixture of 3 nuclei overlapping). Semantic segmentation (in the example) just stops right there.

Thresholding algorithm like Watershed might not work in some cases as demonstrated in [Nuclei Detection][1] at 23:30 onwards.
Edge detection between segmented nuclei and background would not separate overlapping nuclei.
Finding local maxima and putting a dot there might give rise to false positives.
Finding IoU but what if the output of segmentation is not a region of classification (1s and 0s) but a continuous probability map from values between 0 to 1.
Isn't finding contours and getting bounding boxes from masks using opencv a parametric method? What I mean is, it being an image processing technique, there are chances it will work for some images and won't work for some.

","['object-detection', 'image-segmentation', 'u-net', 'mask-rcnn']",
Why do we add additional axis in CNN autoencoder while denoising?,"
I am currently learning about autoencoders and I follow https://www.tensorflow.org/tutorials/generative/autoencoder
When denoising images, authors of tutorial add an additional axis to the data and I cannot find any explanation why... I would appreciate any answer or suggestion :)
x_train = x_train[..., tf.newaxis]
x_test = x_test[..., tf.newaxis]

Then the encoder is built from the following layers:
 self.encoder = tf.keras.Sequential([
      layers.Input(shape=(28, 28, 1)), 
      layers.Conv2D(16, (3,3), activation='relu', padding='same', strides=2),
      layers.Conv2D(8, (3,3), activation='relu', padding='same', strides=2)])
    
 self.decoder = tf.keras.Sequential([
      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),
      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),
      layers.Conv2D(1, kernel_size=(3,3), activation='sigmoid', padding='same')])

","['convolutional-neural-networks', 'tensorflow', 'autoencoders']",
Evolved networks fail to solve XOR,"
My implementation of NEAT consistently fails to solve XOR completely. The species converge on different sub-optimal networks which map all input examples but one correctly (most commonly (1,1,0)). Do you have any ideas as to why that is?
Some information which might be relevant:

I use a plain logistic activation function in each non-input node 1/(1 + exp(-x)).
Some of the weights seem to grow quite large in magnitude after a large number of epochs.
I use the sum squared error as the fitness function.
Anything over 0.5 is considered a 1 (for comparing the output with the expected)

Here is one example of an evolved network. Node 0 is a bias node, the other red node is the output, the green are inputs and the blue ""hidden"". Disregard the labels on the connections.

EDIT: following the XOR suggestions on the NEAT users page of steepening the gain of the sigmoid function, a network that solved XOR was found for the first time after ca 50 epochs. But it still fails most of the time. Here is the network which successfully solved XOR:

","['neural-networks', 'activation-functions', 'neat', 'neuroevolution']","The problem was due to the following issues in my implementation:Another thing that previously caused issues was the network.activate function. Make sure that you wait for the network to stabilize when doing classification tasks, so all signals have time to propagate through the network."
Determining observation and state spaces for viterbi algorithm in a simple word recognition system using HMM,"
The system I'm trying to implement is a microcontroller with a connected microphone which have to recognise single words. the feature extraction is done using MFCC (and is working).

the system have to recognise [predefined, up to 20] single words each one up to 1 seconds length
input audio is sampled with a frequency of 10KHz and 8 bits resolution
the window is 256 sample wide (25.6 ms), hann windowed with a 15ms step (overlaying windows)
the total MFCC features representing each window, is about 18 features

I've done the above things, and tested the outputs for accuracy and computation speed so there is not much concern about the computations. now I have to implement a HMM for word recognition. I've read about the HMM and I think these parameters need to be addressed:

the hidden states are the ""actual"" pieces of the word with 25.6ms length represented in 18 MFCC features. and they count up to maximum of 64 sets in a single word (because the maximum length for input word is 1sec and each window is (25.6 - 10)millisecs)
I should use Viterbi algorithm to find out the most probable word spoken untill the current state. so, if the user is saying ""STOP"", the Viterbi can suggest it (with proper learning of course) when the user has spoken ""STO.."" . so it's some kind of prediction too.
I have to determine the other HMM parameters like the emission and transition. the wikipedia page for Viterbi which has written the algorithm, shows the input/output as:


from the above:

what is observation space? the user may talk anything so it seems indefinite to me
the state space is obviously the set containing all the possible MFCC feature sets used in the learned word set. how I learn or hardcode that ?

thanks for reading this long question patiently.
","['speech-recognition', 'hidden-markov-model']",
Are there deep neural networks that have inputs connected with deeper hidden layers?,"
Are there any architectures of deep neural networks that connect input neurons not only with the first hidden layer but also with deeper ones (red lines on the picture)?

If so could you give some names or links to research papers?
","['neural-networks', 'deep-learning', 'deep-neural-networks', 'architecture', 'hidden-layers']","This type of connections are called skip or residual connections. There are numerous works which employs this type of mechanism, for example: ResNet, SkipRNN. In addition here you can find a paper that empirically explores the skip connections for sequential tagging, or this one for speech enhancement."
Genetic algorithm stuck and cannot find an optimal solution,"
I'm working on SLAP (storage location assignment problem) using genetic algorithm implemented manually in the C++ programming language. The problem is fairly simple, we do have N products, which we want to allocate to M warehouse location slots (N might and might not be equal to M).
Let's begin with the encoding of the chromosomes. The chromosome length is equal to number of products (i.e. each product is one gene). Each product has one integer value (allele value), representing the location its allocated to.
Let me show you on simple example.
Products         Average picking rate        Location slots       Location number
Prod1            0.4                         Location 1, slot 1   (1)   // 3rd best
Prod2            0.3                         Location 1, slot 2   (2)   // 4th best
Prod3            0.2                         Location 2, slot 1   (3)   // The best
Prod4            0.1                         Location 2, slot 2   (4)   // 2nd best

We aim for optimal allocation of products (Prod1-4) to location slots (1-4). The better the allocation is, the faster we can process all the products in customer orders. Now let's say the Location 2 is closer to the warehouse entrance/exit, so its more attractive, and the lower the location slot number is, the faster we can pick product out of the location slot. So the optimal allocation should be:
Product     Location number
Prod1       3
Prod2       4
Prod3       1
Prod4       2

And expressed as the chromosome:
+---+---+---+---+
| 3 | 4 | 1 | 2 |
+---+---+---+---+

This allocation will lead to the best warehouse performance. Now let me show you my crossover operator (based on TSP crossover https://www.permutationcity.co.uk/projects/mutants/tsp.html):
void crossoverOrdered(std::vector<int32_t>& lhsInd, std::vector<int32_t>& rhsInd)
{
    int32_t a, b;
    int32_t pos =  0;
    int32_t placeholder = -1;
    int32_t placeholderCount = 0;

    std::vector<int32_t> o1, o1_missing, o1_replacements;
    std::vector<int32_t> o2, o2_missing, o2_replacements;

    while(true)
    {
        do
        {
            a = randomFromInterval(pos, constants::numberDimensions);
            b = randomFromInterval(pos, constants::numberDimensions);
        }
        while(a == b);

        if(a > b) std::swap(a, b);

        // Insert from first parent
        for(int32_t i = pos; i < a; ++i)
        {
            o1.push_back(lhsInd.at(i));
            o2.push_back(rhsInd.at(i));
        }

        // Insert placeholders
        for(int32_t i = a; i < b; ++i)
        {
            ++placeholderCount;
            o1.push_back(placeholder);
            o2.push_back(placeholder);
        }

        if(b >= constants::numberDimensions - 1)
        {
            for(int32_t i = b; i < constants::numberDimensions; ++i)
            {
                o1.push_back(lhsInd.at(i));
                o2.push_back(rhsInd.at(i));
            }

            break;
        }
        else
        {
            pos = b;
        }
    }

    // Find missing elements
    for(int32_t i = 0; i < constants::problemMax; ++i)
    {
        if(std::find(o1.begin(), o1.end(), i) == o1.end()) o1_missing.push_back(i);
        if(std::find(o2.begin(), o2.end(), i) == o2.end()) o2_missing.push_back(i);
    }

    // Filter missing elements and leave only those which are in the second parent (keep the order)
    for(int32_t i = 0; i < static_cast<int32_t>(rhsInd.size()); i++)
    {
        if(std::find(o1_missing.begin(), o1_missing.end(), rhsInd.at(i)) != o1_missing.end()) o1_replacements.push_back(rhsInd.at(i));
    }

    // Filter missing elements and leave only those which are in the second parent (keep the order)
    for(int32_t i = 0; i < static_cast<int32_t>(lhsInd.size()); i++)
    {
        if(std::find(o2_missing.begin(), o2_missing.end(), lhsInd.at(i)) != o2_missing.end()) o2_replacements.push_back(lhsInd.at(i));
    }

    // Replace placeholders in offspring 1
    for(int32_t i = 0; i < placeholderCount; ++i)
    {
            auto it = std::find(o1.begin(), o1.end(), placeholder);
            *it     = o1_replacements.at(i);
    }

    // Replace placeholders in offspring 2
    for(int32_t i = 0; i < placeholderCount; ++i)
    {
            auto it = std::find(o2.begin(), o2.end(), placeholder);
            *it     = o2_replacements.at(i);
    }

    // Assign new offsprings
    lhsInd.assign(o1.begin(), o1.end());
    rhsInd.assign(o2.begin(), o2.end());
}

My mutation operator(s):
void mutateOrdered(std::vector<int32_t>& ind)
{
    int32_t a, b;

    do
    {
        a = randomFromInterval(0, constants::numberDimensions);
        b = randomFromInterval(0, constants::numberDimensions);
    }
    while(a == b);

    std::rotate(ind.begin() + a, ind.begin() + b, ind.begin() + b + 1);
}

void mutateInverse(std::vector<int32_t>& ind)
{
    int32_t a, b;

    do
    {
        a = randomFromInterval(0, constants::numberDimensions);
        b = randomFromInterval(0, constants::numberDimensions);
    }
    while(a == b);

    if(a > b) std::swap(a, b);

    std::reverse(ind.begin() + a, ind.begin() + b);
}

I tried to use roulette, truncate, tournament and rank selection alorithms, but each with similar results.
This is my configuration:
populationSize = 20
selectionSize = 5
eliteSize = 1
probabilityCrossover = 0.6
probabilityMutateIndividual = 0.4
probabilityMutateGene = 0.2

My fitness function is fairly simple, since it's real number returned by simulation program which simulates picking of orders on the current allocation we gave it. Unfortunately I cannot provide this program as its confidential. It's just a real number representing how good the current allocation is, the better the allocation is, the lower the number is (i.e. its minimization problem).
The problem
This genetic algorithm can find better solutions than just random allocation, the problem is, it gets ""stuck"" after lets say few thousand generations and it fails to improve furthermore, even though there are better solutions, and it will go even 20k generations with exact same ellite chromosom (don't improve at all). I tried to increase crossover/mutation probability and population size but none of it worked. Thanks for any help.
","['genetic-algorithms', 'c++']",
What is the justification for Kaiming He initialization?,"
I've been trying to understand where the formulas for Xavier and Kaiming He initialization come from. My understanding is that these initialization schemes come from a desire to keep the gradients stable during back-propagation (avoiding vanishing/exploding gradients).
I think I can understand the justification for Xavier initialization, and I'll sketch it below. For He initialization, what the original paper actually shows is that that initialization scheme keeps the pre-activation values (the weighted sums) stable throughout the network. Most sources I've found explaining Kaiming He initialization seem to just take it as ""obvious"" that stable pre-activation values will somehow lead to stable gradients, and don't even mention the apparent mismatch between what the math shows and what we're actually trying to accomplish.
The justification for Xavier initialization (introduced here) is as follows, as I understand it:

As an approximation, pretend the activation functions don't exist and we have a linear network. The actual paper says we're assuming the network starts out in the ""linear regime"", which for the sigmoid activations they're interested in would mean we're assuming the pre-activations at every layer will be close to zero. I don't see how this could be justified, so I prefer to just say we're disregarding the activation functions entirely, but in any case that's not what I'm confused about here.

Zoom in on one edge in the network. It looks like $x\to_{w} y$, connecting the input or activation value $x$ to the activation value $y$, with the weight $w$. When we do gradient descent we consider $\frac{\partial C}{\partial w}$, and we have:
$$\frac{\partial C}{\partial w}=x\frac{\partial C}{\partial y}$$
So if we want to avoid unstable $\frac{\partial C}{\partial w}$-s, a sufficient (not necessary, but that's fine) condition is to keep both those factors stable - the activations and the gradients with respect to activations. So we try to do that.

To measure the ""size"" of an activation, let's look at its mean and variance (where the randomness comes from the random weights). If we use zero-mean random weights all i.i.d. on each layer, then we can show that all of the activation values in our network are zero-mean, too. So controlling the size comes down to controlling the variance (big variance means it tends to have large absolute value and vice versa). Since the gradients with respect to activations are calculated by basically running the neural network backwards, we can show that they're all zero-mean too, so controlling their size comes down to controlling their variance as well.

We can show that all the activations on a given layer are identically distributed, and ditto for the gradients with respect to activations on a given layer. If $v_n$ is the variance of the activations on layer $n$, and if $v'_n$ is the variance of the gradients, we have
$$v_{n+1}=v_n k_n \sigma^2$$
$$v'_n=v_{n+1} k_{n+1} \sigma^2$$


where $k_i$ is the number of neurons on the $i$-th layer, and $\sigma^2$ is the variance of the weights between the $n$-th and $n+1$-th layers. So to keep either of the growth factors from being too crazy, we would want $\sigma^2$ to be equal to both $1/k_n$ and $1/k_{n+1}$. We can compromise by setting it equal to the harmonic mean or the geometric mean or something like that.

This stops the activations from exploding out of control, and stops the gradients with respect to activations from exploding out of control, which by step (2) stops the gradients with respect to the weights (which at the end of the day are the only things we really care about) from growing out of control.

However, when I look at the paper on He initialiation, it seems like almost every step in this logic breaks down. First of all, the math, if I understand correctly, shows that He initialization can control the pre-activations, not the activations. Therefore, the logic from step (2) above that this tells us something about the gradients with respect to the weights fails. Second of all, the activation values in a ReLU network like the authors are considering are not zero-mean, as they point out themselves, but this means that even the reasoning as to why we should care about the variances, from step (3), fails. The variance is only relevant for Xavier initialization because in that setting the mean is always zero, so the variance is a reasonable proxy for ""bigness"".
So while I can see how the authors show that He initialization controls the variances of the pre-activations in a ReLU network, for me the entire reason as to why we should care about doing this has fallen apart.
","['deep-learning', 'weights-initialization']",
How do you measure multi-label classification accuracy?,"
Multi-label assignment is the task in machine learning to assign to each input value a set of categories from a fixed vocabulary where the categories need not be statistically independent, so precluding building a set of independent classifiers each classifying the inputs as belong to each of the categories or not.
Machine learning also needs a measure by which the model may be evaluated. So this is the question how do we evaluate a multi-label classifier?
We can’t use the normal recall,  accuracy and  F measures since they require a binary is it correct or not measure of each categorisation. Without such a measure we have no obvious means to evaluate models nor to measure concept drift.
","['reference-request', 'performance', 'metric', 'multi-label-classification']",
How does one stack multiple observations in the input layer of a convolutional neural network?,"
The paper, Deep Recurrent Q-Learning for Partially Observable MDPs, talks about stacking multiple observations in the input of a convolutional neural network.
How does this exactly work? Do the convolutional filters loop over each observation (image)?
(I know this isn't the right group to request this, but I'll highly appreciate if someone could also suggest a framework that helps with this.)
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'pomdp']",
Applications of Information Theory in Machine Learning,"
How is information theory applied to machine learning, and in particular to deep learning, in practice? I'm more interested in concepts that yielded concrete innovations in ML, rather than theoretical constructions.
Note that, I'm aware that basic concepts such as entropy is used for training decision trees, and so on. I'm looking for applications which use slightly more advanced concepts from information theory, whatever they are.
","['machine-learning', 'deep-learning', 'information-theory']","Apart from the entropy and the cross-entropy, which are widely used in deep learning and you seem to be aware of, there is also the Kullback-Leibler divergence (also known as relative entropy), which is widely used in the context of variational Bayesian neural networks and variational auto-encoders, given that it's often part of the loss function that is minimized, i.e. the Evidence Lower BOund, which is a proxy objective function for the KL divergence between the prior and posterior distributions (which actually corresponds to the minimum description length needed to encode the data: huh?). See this answer for more details. There is also the mutual information, which has also been used as a measure of uncertainty in the context of Bayesian neural networks."
Unable to meet desired mean squared error,"
I wish to get MSE < 0.5 on test data (https://easyupload.io/zr7xf3) which is 20% of given data chosen randomly. But I am reaching 0.73 using both plain Ridge Regression as well as a neural network with about 6 layers with some elementary regularization, dropout and choice of other parameters. Overfitting also occurs.
Suggest. I believe a Bayesian optimization or a genetic algorithm for parameters is required.
I did no feature selection (as top 4 features showed no improvement) and non-linear methods exploration.
My solutions -
Ridge - Alpha = 0.002 (Grid searched)
Neural Network efforts =
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=10, min_lr=0.001)


es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)


model_b = Sequential()


model_b.add(Dense(2048, kernel_initializer='he_uniform',input_dim = X.shape[1], activation='relu', kernel_regularizer=regularizers.l2(l2=1e-6)))
model_b.add(BatchNormalization(beta_regularizer = regularizers.l2(0.00001)))


# The Hidden Layers :
model_b.add(Dense(1024, kernel_initializer='lecun_normal',activation='selu',kernel_regularizer=regularizers.l2(l2=1e-6)))


model_b.add(BatchNormalization(beta_regularizer = regularizers.l2(0.00001)))


model_b.add(Dense(1024, kernel_initializer='lecun_normal',activation='selu',kernel_regularizer=regularizers.l2(l2=1e-6)))


model_b.add(BatchNormalization(beta_regularizer = regularizers.l2(0.00001)))


model_b.add(Dropout(0.5))


model_b.add(Dense(512, kernel_initializer='normal',activation='relu'))


model_b.add(Dense(512, kernel_initializer='normal',activation='relu'))


model_b.add(Dense(256, kernel_initializer='normal',activation='relu'))

# The Output Layer :


model_b.add(Dense(1, kernel_initializer='normal',activation='linear'))
optimizer = SGD(lr = 0.0001)


model_b.compile(loss='mean_squared_error', optimizer= optimizer)


model_b.fit(X_train, y_train, batch_size=70,
              epochs=256,
              validation_data=(X_test, y_test),callbacks = [es])


predb = model_b.predict(X_test)

If anyone has free time, may answer.
Best
","['neural-networks', 'keras', 'genetic-algorithms', 'hyperparameter-optimization', 'principal-component-analysis']",
What is the reason for taking tuples as vectors rather than points?,"
Across the literature of artificial intelligence, especially machine learning, it is normal to treat the tuples of datasets as vectors.
Although there is a convention to treat them as data points. Treating them as vectors is also considerable.
It is easy to understand the tuples of datasets as points over space. But what is the purpose of treating them as vectors?
","['machine-learning', 'terminology', 'datasets', 'vectors']",
Why is the perceptron criterion function differentiable?,"
I'm reading chapter one of the book called Neural Networks and Deep Learning from Aggarwal.
In section 1.2.1.1 of the book, I'm learning about the perceptron. One thing that book says is, if we use the sign function for the following loss function: $\sum_{i=0}^{N}[y_i - \text{sign}(W * X_i)]^2$, that loss function will NOT be differentiable. Therefore, the book suggests us to use, instead of the sign function in the loss function, the perceptron criterion which will be defined as:
$$ L_i = \max(-y_i(W * X_i), 0) $$
The question is: Why is the perceptron criterion function differentiable? Won't we face a discontinuity at zero? Is there anything that I'm missing here?
","['optimization', 'gradient-descent', 'perceptron']","$\max(-y_i(w x_i), 0)$ is not partial derivable respect $w$ if $w x_i=0$.Loss functions are problematic when not derivable in some point, but even more when they are flat (constant) in some interval of the weights.Assume $y_i = 1$ and $w x_i < 0$ (that is, an error of type ""false negative"").In this case, function $[y_i - \text{sign}(w x_i)]^2 = 4$. Derivative on all interval $w x_i < 0$ is zero, thus, the learning algorithm has no any way to decide if it is better increase or decrease $w$.In same case, $\max(-y_i(w x_i), 0) = - w x_i$, partial derivative is $-x_i$. The learning algorithm knows that it must increase $w$ value if $x_i>0$, decrease otherwise. This is the real reason this loss function is considered more practical than previous one.How to solve the problem at $w x_i = 0$ ? simply, if you increase $w$ and the result is an exact $0$, assign to it a very small value, $w=\epsilon$. Similar logic for remainder cases."
Support Vector Machine Convert optimisation problem from argmax to argmin,"
I'm new to the AI Stackexchange and wasn't certain if this should go here or to Maths instead but thought the context with ML may be useful to understand my problem. I hope posting this question here could help another student learning about Support Vector Machines some day.
I'm currently learning about Support Vector Machines at university and came across a weird step I could not understand. We were talking about basic SVMs and formulated the optimisation problem $\max_{w,b} \{ \frac{1}{||w||} \min_n(y^{(n)}f(x^{(n)}))\}$ which we then simplified down to $\max_{w,b} \{ \frac{1}{||w||}\}$ by introducing $\kappa$ as a scaling factor for $w$ and $b$ according to the margin of the SVM. Now our lecturer converted it without explanation into a quadratic optimisation problem as $\min_{w,b}\{\frac{1}{2} ||w||^2\}$ which I could not explain myself. I hope someone with context can help me how this is possible and what math or trick is behind this approach?

Notation information:

$w$ - weight matrix
$b$ - bias (sometimes denoted $w_0$ I believe?)
$x^{(n)}$ - Independent variable (vector)
$y^{(n)}$ - Dependent variable (scalar classifying the input in a binary classifcation as $y=1$ or $y=-1$)

Thank you very much!
","['machine-learning', 'classification', 'support-vector-machine', 'binary-classification']",
Purpose of the hidden variables in a Restricted Boltzmann Machine,"
From the part titled Introducing Latent Variables under subsection 2.2 in this tutorial:

Introducing Latent Variables. Suppose we want to model an $m$-dimensional
unknown probability distribution $q$ (e.g., each component of a sample corresponds to one of m pixels of an image). Typically, not all variables $\mathbf{X} = (X_v)_{v \in V}$ in an MRF need to correspond to some observed component, and the number of nodes is larger than $m$. We split $\mathbf{X}$ into visible (or observed) variables $\mathbf{V} = (V_1,...,V_m)$ corresponding to the components of the observations and latent (or hidden) variables $\mathbf{H} = (H_1,...,H_n)$ given by the remaining $n = |\mathbf{V}| − m$ variables. Using latent variables allows to describe complex distributions over the visible variables by means of simple (conditional) distributions. In this case the Gibbs distribution of an MRF describes the joint probability distribution of $(\mathbf{V},\mathbf{H})$ and one is usually interested in the marginal distribution of $\mathbf{V}$ which is given by:
$$p(\mathbf{v}) = \sum_{\mathbf{h}} p(\mathbf{v},\mathbf{h}) = \frac{1}{Z} \sum_{\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h})}$$
where $Z = \sum_{\mathbf{v},\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h})}$. While the visible variables correspond to the components of an observation, the latent variables introduce dependencies between the visible variables (e.g., between pixels of an input image).

I have a question about this part:

While the visible variables correspond to the components of an observation, the latent variables introduce dependencies between the visible variables (e.g., between pixels of an input image).

Given a set of nodes $\mathbf{X}$ in a Markov Random Field $G$, the joint distribution of all the nodes is given by:
$$p(\mathbf{X}) = \frac{1}{Z} \prod_{c \in C} \phi(c)$$
Where $Z$ is the partition function and $C$ is the set of cliques in $G$. To ensure that the joint distribution is positive, the following factors can be used:
$$\phi(c) = e^{-E(c)}$$
Such that:
$$p(\mathbf{X}) = \frac{1}{Z} e^{-\sum_{c \in C} E(c)}$$
Where $E$ is the energy function.
I am not sure why there is a need to introduce hidden variables and express $p(\mathbf{v})$ as a marginalization of $p(\mathbf{v},\mathbf{h})$ over $\mathbf{h}$. Why can't $p(\mathbf{v})$ be expressed as:
$$p(\mathbf{v}) = \frac{1}{Z} e^{-\sum_{v \in \mathbf{v}} E(v)}$$
directly? I think it may be because the factors only encode dependencies between variables in cliques, and so may not be able to encode dependencies between variables that are in two separate cliques. The purpose of the hidden variables are then to encode these ""long-range"" dependencies between visible variables not in cliques. However, I am not sure about this reasoning.
Any help would be greatly appreciated.
By the way, I am aware of this question, but I think the answer is not specific enough.
","['restricted-boltzmann-machine', 'probabilistic-graphical-models']",
Deep Learning based image restoration using multiple frames,"
Suppose we have a sequence of still images each of which has been contaminated by some particles(ex, dust/sand/smoke) making the images very poor in certain areas.
What architecture would be best to teach image regeneration using multiple frames? The simplest technique is to simply find a way to detect what parts of the image are contaminated and uncontaminated and pull uncontaminated sections from each frame.
","['deep-learning', 'computer-vision', 'signal-processing']",
Are FSA and FSTs used in NLP nowadays?,"
Finite state automata and transducers are computational models that were widely used decades before in natural language processing for morphological parsing and other nlp tasks. I wonder if these computational models are still used in NLP nowadays for significant purposes. If these models are in use, can you give me some examples ?
","['natural-language-processing', 'computational-linguistics']","Both are used, for example,  in the GATE framework, which is still widely used. I suspect that this also applies to many other applications.I would think that many recent academic publications are now on other approaches, as FSAs and FSTs are fairly established and mature technologies, but I've been out of academia for a while now."
What is the difference between step_model and train_model in the OpenAI implementation of the A2C algorithm?,"
I'm struggling a little with understanding the OpenAI implementation of A2C in the baselines (version 2.9.0) package. From my understanding, one step_model acts in different parallel environments and gathers experiences (calculates the gradients, I think), and sends them to the train_model that trains with them. After this, the step_model gets updated from the train_model.
What I am unsure about is if both step_model and train_model are actor-critic models or if step_model is actor and train_model is a critic (or vice versa). Does the step_model use the advantage function or is it just the train_model?
","['reinforcement-learning', 'open-ai', 'implementation', 'advantage-actor-critic']",
Why are there two versions of softmax cross entropy? Which one to use in what situation?,"
I have seen 2 forms of softmax cross-entropy loss and are confused by the two. Which one is the right one?
For example in this Quora answer, there are 2 answers:

$L(\mathbf{w})=\frac{1}{N} \sum_{n=1}^{N} H\left(p_{n}, q_{n}\right)=-\frac{1}{N} \sum_{n=1}^{N}\left[y_{n} \log \hat{y}_{n}+\left(1-y_{n}\right) \log \left(1-\hat{y}_{n}\right)\right]$

$\mathrm{L}(y, \hat{y})=-\Sigma y(i) \log \hat{y}(i)$, which is only the first part of the version one.


","['machine-learning', 'comparison', 'cross-entropy', 'softmax', 'categorical-crossentropy']","It's the same thing, first version is the special case of the more general one. In the first case you only have two classes, it's binary cross-entropy, and they also included iteration over batch of samples. In the second case you have multiple classes and in the current form it's only for a single sample.In the first case there is only one output, if you had two outputs it would have been
\begin{equation}
-\frac{1}{N} \sum_{n=1}^N \sum_{j=1}^2 y_{n,j} \log(\hat y_{n,j})
\end{equation}
where $n$ iterates over batch samples, and $j$ over two classes. The reason why it was written like that is that if you have two classes you can have only one output because you can immediately conclude about probability of the second class if you have the probability of the first class, it would simply be $p_1 = 1-p_0$.In the second case, with batch samples included, it would be
\begin{equation}
 -\frac{1}{N} \sum_{n=1}^N \sum_{j=1}^c y_{n,j} \log(\hat y_{n,j})
\end{equation}
wheren $n$ iterates over batch samples, and $j$ over $c$ output classes."
Is reinforcement learning only about determining the value function?,"
I started reading some reinforcement learning literature, and it seems to me that all approaches to solving reinforcement learning problems are about finding the value function (state-value function or action-state value function).
Are there any algorithms or methods that do not try to calculate the value function but try to solve a reinforcement learning problem differently?
My question arose because I was not convinced that there is no better approach than finding the value functions. I am aware that given the value function we can define an optimal policy, but are there not other ways to find such an optimal policy?
Also, is the reason why I don't encounter any non value-based methods that these are just less successful?
","['reinforcement-learning', 'comparison', 'actor-critic-methods', 'policy-based-methods', 'value-based-methods']","There are many algorithms that are not based on finding a value function. The most common ones are policy gradients. These methods attempt to map states to actions through a neural network. They learn the optimal policy directly, not through a value function.The important part of the image is when Model-Free RL splits into  Policy Optimization (which includes policy gradients) and Q-Learning. Later you can see the two sections coming back together in algorithms that are a mix of both techniques. Even the bottom three methods in policy optimization involve some form of learning a value function. The best and most advanced algorithms use value function learning and policy optimization. The value function is only for training. Then when the agent is tested, it only uses the policy.The most likely reason you have only heard of value function methods is because policy gradients are more complicated. There are many algorithms more advanced than ones that only use value functions and policy gradients can learn to operate in continuous actions spaces (an action can be between -1 and 1, like when moving a robot arm) while value functions can only operate with discrete action spaces (move 1 right or 1 left).Summary: Yes, there are other methods that learn the optimal policy without a value function. The best algorithms use both types of reinforcement learning.The SpinningUp website has a lot of information about reinforcement learning algorithms and implementations. You can learn more about direct policy optimization there. That is also where I got the image from.This answer is specific to the most common types of Model-Free RL. There are other algorithms related to the RL problem that do not learn value functions, like inverse reinforcement learning and imitation reinforcement learning."
How to find a parameter combination for a black box using AI?,"
I am working on a project where I encountered a component which takes 96 arguments (all integer values) and outputs 12 float values.
I would like to find a useful combination of these 96 values to receive the output that I want while avoiding random guessing, so the desired behavior would be that I provide the outcome and receive the 96 inputs to use them in my component.
Unfortunately I am not that experienced in that field. If I think about how I can implement this, my first thought was a kind of classification task, since I could build a dataset but the problem here is that I need integer values.
A second guess was a regression but would that be possible for y as an output vector?
Are there some other approaches that could fit to my use case?
","['machine-learning', 'classification', 'ai-design', 'regression', 'algorithm-request']",
"What do we mean by ""infrequent features""?","
I am reading this blog post: https://ruder.io/optimizing-gradient-descent/index.html. In the section about AdaGrad, it says:

It adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features.

But I am not sure about the meaning of infrequent features: is it that the value of a given feature changes rarely?
","['deep-learning', 'terminology', 'optimizers']","We will describe the input to the network as a vector, called features vector. Each component of this vector is usually related to some ""real world"" information, by example ""age of the person"", ""number of atoms"", ""..."".In very usual situations, a specific component of the input vector will have near than always the same value. This is more usual in binary components or components that has a small set of possible values.However, usually in the cases where this component has a value different from the most usual one, this component is very important and informative.These values of this kind of components are called infrequent features.(example: ""rains?"" is in my city 99.9% of time ""false"". However, when it is true, it is a key factor to all questions about the behavior of the population).The problem with these features: as unusual values are infrequent, the net has few chances to learn from them, and some learning algorithms could fail to give them the weight that they must (taken into account that, as has been said, these components are very important when they take a value different to the most frequent one).Some adaptive learning rate algorithms as AdaGrad tries to solve this issue."
Why do we have two similar action selection strategies for UCB1?,"
In the literature, there are at least two action selection strategies associated with the UCB1's action selection strategy/policy. For example, in the paper Algorithms for the multi-armed bandit problem (2000/2014), at time step $t$, an action is selected using the following formula
$$
a^*(t) \doteq \arg \max _{i=1 \ldots k}\left(\hat{\mu}_{i}+\sqrt{\frac{2 \ln t}{n_{i}}}\right) \tag{1}\label{1},
$$
where

$\hat{\mu}_{i}$ is an estimate of the expected return for arm $i$
$n_i$ is the number of times the action $i$ is selected
$k$ is the number of arms/actions

On the other hand, Sutton & Barto (2nd edition of the book) provide a slightly different formula (equation 2.10)
$$
a^*(t) \doteq \arg \max _{i=1 \ldots k}\left(\hat{\mu}_{i}+c\sqrt{\frac{\ln t}{n_{i}}}\right) \tag{2}\label{2},
$$
where $c > 0$ is a hyper-parameter that controls the amount of exploration (as explained in the book or here).
Why do we have these two formulas? I suppose that both are ""upper confidence bounds"" (and, in both cases, they are constants, though one is a hyper-parameter), but why (and when) would we use one over the other? They are not equivalent because $c$ only needs to be greater than $0$, i.e. it can be arbitrarily large (although, in the mentioned book, the authors use $c=2$ in one experiment/figure). If $c = \sqrt{2}$, then they are the same.
The answer to my question can probably be found in the original paper that introduced UCB1 (which actually defines the UCB1 as in \ref{1}),  or in a paper that derives the bound, in the sense that the bound probably depends on some probability of error, but I have not fully read it yet, so, if you know the answer, feel free to derive both bounds and relate the two formulas.
","['reinforcement-learning', 'policies', 'sutton-barto', 'multi-armed-bandits', 'upper-confidence-bound']",
How can I determine whether a video's frame is realistic (was recorded by a camera) or contains computer-generated graphics?,"
Given a video, I'm trying to classify whether it is a graphical (computer-generated) or realistic scene. For instance, if it contains computer-generated graphics, credit, moving bugs, blue screen, etc. it will be computer-generated graphics, and if it is a realistic scene captured by camera, it will be a realistic scene.
How can we achieve that with AI? Do we have any working solutions available?
Some examples of graphical scenes:



","['deep-learning', 'classification', 'ai-design', 'object-detection', 'scene-classification']","As per your requirements, I would suggest that you start with any simple CNN network.CNNs take advantage of the hierarchical pattern in data and assemble
more complex patterns using smaller and simpler patterns. Therefore,
on the scale of connectedness and complexity, CNNs are on the lower
extreme.Here is a Keras example:where image_shape is the resolution and number of channels of images (e.g. 128x128x3 for RGB images). I also suggest you downscaling the images to a lower resolution. You will also have to crop the images as they must all be the same image_shape.Also take a look at the MaxPooling2D  and BatchNormalization layers.Since you only have real and CGI images, this becomes a binary classification problem. Therefore you can have a single output (0 - CGI, 1 - real). Such problems can be solved with BinaryCrossentropy loss.Finally, you can fit your modelYou can find a complete example here.Please note that depending on your data, the model can become biased if your dataset is unbalanced. That is, if all of your CGI images have text, and only a small fraction of the real images also have text, they might be misclassified. Therefore, I recommend that you visualize your model to better understand what it has learned. Here is an example of such a problem we faced at our university.There are also more advanced CNN architectures such as ResNet, VGG or YOLO. You can also extend your model with time series (i.e. video) using LSTM or GRU architecture."
Is it possible to perform neuroevolution without a fitness function?,"
My question is about neuroevolution (genetic algorithm + neural network): I want to create artificial life by evolving agents. But instead of relying on a fitness function, I would like to have the agents reproduce with some mutation applied to the genes of their offspring and have some agents die through natural selection. Achieve evolution in this manner is my goal.
Is this feasible? And has there been some prior work on this? Also, is it somehow possible to incorporate NEAT into this scheme?
So far, I've implemented most of the basics in amethyst (a parallel game engine written in Rust), but I'm worried that the learning will happen very slowly. Should I approach this problem differently?
","['neural-networks', 'genetic-algorithms', 'neuroevolution', 'fitness-functions', 'artificial-life']","You do not always need an explictly coded fitness function to perform genetic algorithm searches. The more general need is for a selection process that favours individuals that perform better at the core tasks in an environment (i.e. that are ""more fit""). One way of assessing performance is to award a numerical score, but other approaches are possible, including:Tournament selection where two or more individuals compete in a game, and the winner is selected.Opportunity-based selection, where agents in a shared environment - typically with limited resources and chances to compete - may reproduce as one of the available actions, provided they meet some criteria such as having collected enough of some resource. I was not able to find a canonical name for this form of selection, but it is commonly implemented in artificial life projects.A key distinction between A-life projects and GA optimisation projects is that in A-life projects there is no goal behaviour or target performance. Typically A-life projects are simulations with an open ended result and the developer runs a genetic algorithm to ""see what happens"" as opposed to ""make the best game-player"". If your project is like this then you are most likely looking for the second option here.To discover more details about this kind of approach, you could try searching ""artifical life genetic algorithms"" as there are quite a few projects of this type published online, some of which use NEAT.Technically, you could view either of the methods listed above as ways of sampling comparisons between individuals against an unknown fitness function. Whether or not a true fitness function could apply is then partly a matter of philosophy. More importantly for you as the developer, is that you do not have to write one. Instead you can approximately measure fitness using various methods of individual selection.So far I've implemented most of the basics in amethyst (a parallel game engine written in rust), but I'm worried that the learning will happen very slowly. Should I approach this problem differently?It is difficult to say whether you should approach the problem differently. However, the biggest bottlenecks against successful GA approaches are:Time/CPU resources needed to assess agents.Size of search space for genomes.Both of these can become real blockers for ambitious a-life projects. It is common to heavily simplify agents and environments in attempts address these issues."
"Has ""deep vs. wide"" been resolved?","
All else being equal, including total neuron count, I give the following definitions:

wide is a parallel ensemble, where good chunks of the neurons have the same inputs because the inputs are shared and they have different outputs.
deep is a series ensemble, where for the most part neurons have as input the output of other neurons and few inputs are shared.

For CART ensembles the parallel (wide) ensemble is a random forest while the series (deep) ensemble is a gradient boosted machine.  For several years the GBM was the ""winningest"" on kaggle.
Is there a parallel of that applied to Neural networks?  Is there some reasonable measure that indicates whether deep outperforms wide when it comes to neural networks?  If I had the same count of weights to throw at a tough problem, all else being equal should they be applied more strongly in parallel or in series?
","['deep-neural-networks', 'gradient-boosting']",
Is there a common way to build a neural network that seeks to extract spatial and temporal information simultaneously?,"
Is there a common way to build a neural network that seeks to extract spatial and temporal information simultaneously? Is there an agreed up protocol on how to extract this information?
What combination of layers works: convolution + LSTM? What would be the alternatives?
","['neural-networks', 'convolutional-neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'multilayer-perceptrons']","Yes, there are different ways. What I think you are looking for is under the research field of Localization and Mapping. Which divides in the following subfields:
Here it is the amazing survey that links you to tons of papers with different models for each category. If you want to know what are the most common architectural blocks (LSTM, ConvLSTM, RNN...) used for your problem, read the most promising papers under your target category.References:Survey: https://arxiv.org/abs/2006.12567"
Is there an equivalent model to the Hidden Markov Model for continuous hidden variables?,"
I understand that Hidden Markov Models are used to learn about hidden variables $z_i$ with the help of observable variables $\xi_i$. On Wikipedia, I read that while the $\xi_i$'s can be continuous (say Gaussian), the $z_i$'s are discrete. Is this necessary, and why? Are there ways in which I could extend this to continuous domains?
","['machine-learning', 'hidden-markov-model', 'kalman-filter']","Kalman filter is what you're looking for.According to Wikipedia:The Kalman filter may be regarded as analogous to the hidden Markov model, with the key difference that the hidden state variables take values in a continuous space (as opposed to a discrete state space as in the hidden Markov model)."
Q-learning agent stuck at taking same actions,"
I have created my own RL environment where I have a 2-dimensional matrix as a state space, the rows represent the users that are asking for a service, and 3 columns representing 3 types of users; so if a user U0 is of type 1 is asking for a service, then the first row would be (0, 1, 0) (first column is type 0, second is type 1...).
The state space values are randomly generated each episode.
I also have an action space, representing which resources were allocated to which users.
The action space is a 2-dimensional matrix, the rows being the resources that the agent has, and the columns represent the users. So, suppose we have 5 users and 6 resources, if user 1 was allocated resource 2, then the 3rd line would be like this: ('Z': a value zero was chosen, 'O': a value one was chosen)
(Z, O, Z, Z, Z)
The possible actions are a list of tuples, the length of the list is equal to the number of users + 1, and the length of each tuple is equal to the number of users.
Each tuple has one column set to 'O', and the rest to 'Z'. (Each resource can be allocated to one user only). So the number of the tuples that have one column = 'O', is equal to the number of users, and then there is one tuple that has all columns set to 'Z', which means that the resource was not allocated to any users.
Now, when the agent chooses the action, for the first resource it picks an action from the full list of possible action, then for the second resource, the action previously chosen is removed from the possible actions, so it chooses from the actions left, and so on and so forth; and that's because each user can be allocated one resource only. The action tuple with all 'Z' can always be chosen.
When the agent allocates a resource to a user that didn't request a service, a penalty is given (varies with the number of users that didn't ask for a service but were allocated a resource), otherwise, a reward is given (also varies depending on the number of users that were satisfied).
The problem is, the agent always tends to pick the same actions, and those actions are the tuple with all 'Z' for all the users. I tried to play with the q_values initial values; q_values is a dictionary with 2 keys: 1st key: the state being a tuple representing each possible state from the state space, meaning (0, 0, 0) & (1, 0, 0) & (0, 1, 0) & (0, 0, 1), combined with each action from the possible actions list.
I also tried different learning_rate values, different penalties and rewards etc. But it always does the same thing.


","['reinforcement-learning', 'python', 'q-learning']","I am confused. For the initial $Q$-values, you generate one for each possible row $(1, 0, 0), (0,0,0), \ldots$ so you would have 4 states.However, from the first paragraph it seems that the states themselves are matrices (one row for each user), so the state space is a set of such matrices.That means that your $Q$-table should have a row for each possible matrix, and a column for each possible total assignment of items to users."
Did Alphago zero actually beat Alphago 100 games to 0?,"
tl;dr
Did AlphaGo and AlphaGo play 100 repetitions of the same sequence of boards, or were there 100 different games?
Background:
Alphago was the first superhuman go player, but it had human tuning and training.
AlphaGo zero learned to be more superhuman than superhuman.  Its supremacy was shown by how it beat AlphaGo perfectly in 100 games.
My understanding of AlphaGo and AlphaGo are that they are deterministic, not stochastic.
If they are deterministic, then given a board position they will always make the same move.
The way that mathematicians count the possible games in chess is to account for different board positions.  As I understand it, and I could be wrong, if they have the exact same sequence of board positions then it does not count as a different game.
If they make the same sequence of moves 100 times, then they did not play 100 different games, but played one game for 100 repetitions.
Question:
So, using the mathematical definition, did AlphaGo and AlphaGo Zero play only one game for 100 iterations or did they play 100 different games?
References:

https://www.scientificamerican.com/article/ai-versus-ai-self-taught-alphago-zero-vanquishes-its-predecessor/
https://deepmind.com/blog/article/alphago-zero-starting-scratch
https://mathworld.wolfram.com/Stochastic.html

","['alphago-zero', 'alphago', 'deterministic-policy', 'stochastic-policy']",
What is the best neural network model to classify an x(t) signal according two classes?,"
I am a beginner in AI methods. I have a collection of x(t) data, where x are some signal amplitudes and t is a time. My testing data are divided into two classes, say those from good and bad experimental samples. I need to classify the signals from unknown samples as good or bad according to their similarity to these two classes. What kind of a neural network is the best in this case? Could you recommend me some example in the literature where such a problem is considered?
","['neural-networks', 'classification']",
What is meant by subspace clustering in MFA?,"

The basic idea of MFA is to perform subspace clustering by assuming the covariance structure for each component of the form, $\Sigma_i = \Lambda_i \Lambda_i^T + \Psi_i$, where $\Lambda_i \in \mathbb{R}^{D\times d}$, is the factor loadings matrix with $d < D$ for parsimonious representation of the data, and $Ψ_i$ is the diagonal noise matrix. Note that the mixture of probabilistic principal component analysis (MPPCA) model is a special case of MFA with
the distribution of the errors assumed to be isotropic with $Ψ_i = Iσ_i^2$.

What is meant by subspace clustering here, and how does $\Sigma_i = \Lambda_i \Lambda_i^T + \Psi_i$ accomplish the same? I understand that this is a dimensionality reduction technique since $\text{rank}(\Lambda_i) \leq d < D$. It'd be great if someone could help me understand more, and/or suggest resources I could look into for learning about this as an absolute beginner.
From what I understand, $x = \Lambda z + u$ is one factor-analyzer (right?), i.e. the generative model in maximum likelihood factor analysis. This paper goes on to define a mixture of factor-analyzers indexed by $\omega_j$, where $j = 1,...,m$. The generative model now obeys the distribution $$P(x) = \sum_{i=1}^m \int P(x|z,\omega_j)P(z|\omega_j)P(\omega_j)dz$$ where, $P(z|\omega_j) = P(z) = \mathcal{N}(0,I)$. How does this help/achieve the desired objective? Why take the sum from $1$ to $m$? Where is subspace clustering happening, and what's happening on a high-level when we are using this mixture of factor-analyzers?
","['clustering', 'dimensionality-reduction', 'statistics']",
Are there any known models/techniques to determine whether a person in a store is a customer or a store representative?,"
Are there any known models/techniques to determine whether a person in a store is a customer or a store representative?
For example, customer representatives can wear uniforms and then one possible way to identify customer representatives is by the color of their uniform, texture, etc. On the other hand, a customer can also wear the same color clothes as that of a customer representative. Likewise, a customer representative could be wearing ""normal clothes."" So the main problems that may occur could be:

A customer becomes misclassified as a customer representative.
A customer representative representative becomes misclassified as a customer

So using clothing as the only proxy to classify people as customers or customer representatives seems to be flaky. Any other known ideas?
","['computer-vision', 'image-recognition', 'reference-request', 'object-detection']",
"How to tell a neural network that: ""your i-th input is special""","
Assume that I have a fully connected network that takes in a vector containing 1025 elements. First 1024 elements are related to the input image of size 32 x 32 x 1, and the last element in the vector (1025-th element) is a control bit that I call it special input.
When this bit is zero, the network should predict if there is a cat in the image or not, and when this bit is one, it should predict if there is a dog in the image or not.
So how can I tell the network that your 1025-th element should be special to you and you should pay more attention to it?
Note that it's just an example and the real problem is more complex than this. So please don't bypass the goal of this question by using tricks special to this example. Any idea is appreciated.
","['deep-learning', 'tensorflow', 'keras', 'feedforward-neural-networks']","The main benefit of deep learning is that you don't have to manually design features.Classic Machine Learning algorithms always include the Feature engineering step, whereas neural networks are able to extract features automatically during learning. The classic example is CNN. In the first layer, it creates simple features that representing lines, the last layers represent abstract features. Of course, some tasks do require feature engineering (e.g. signal processing).In your case, if you want to take advantage of the CNN network, you can also add an additional input layer for the flag (e.g. as one-hot vector). Here is an illustration taken from this answer."
How does vanish gradient restrict RNN to not work for long range dependencies?,"
I am really trying to understand deep learning models like RNN, LSTMs etc. I have gone through many tutorials of RNN and have learned that RNN cannot work for long Range dependencies, like:
Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.
it comes due to vanish gradient problem. However, I could not understand that how to vanish gradient creates an issue for RNN to not work for long-range dependencies.
Since, as I know that vanish gradient usually comes when we have many hidden layers and the gradient for the first layer usually produced too low and that affects the training process. However, everyone connects this issue with vanish gradient, technically what is the relationship RNN (long-range dependencies) with vanish gradient?
I am really sorry if it is a weird question
","['deep-learning', 'recurrent-neural-networks', 'gradient-descent', 'vanishing-gradient-problem']",
Why is tree search/planning used in reinforcement learning?,"
In AlphaGo Zero, MCTS is used along with policy networks. Some sources say MCTS (or planning in general) increases the sample efficiency.
Assumed the transition model is known and the computational cost of interacting through planning is the same as interacting with the environment, I do not see the difference between playing many games versus playing a single game, but plan at each step.
Furthermore, given a problem with a known transition model, how do we know combining learning and tree search will likely be better than pure learning?
","['reinforcement-learning', 'deep-rl', 'monte-carlo-tree-search', 'alphago-zero']",
XOR problem with bipolar representation,"
I am taking a course in Machine Learning and the Professor introduced us to the XOR problem.
I understand the XOR problem is not linearly separable and we need to employ Neural Network for this problem.
However, he mentioned XOR works better with Bipolar representation(-1, +1) which I have not really understand.
I am wondering what Bipolar representation would be better than Binary Representation? Whats the rationale for saying so?
","['neural-networks', 'deep-learning', 'backpropagation']",
How is the error calculated with multiple output neurons in the neural network?,"
Machine Learning books generally explains that the error calculated for a given sample $i$ is:
$e_i = y_i - \hat{y_i}$
Where $\hat{y}$ is the target output and $y$ is the actual output given by the network. So, a loss function $L$ is calculated:
$L = \frac{1}{2N}\sum^{N}_{i=1}(e_i)^2$
The above scenario is explained for a binary classification/regression problem. Now, let's assume a MLP network with $m$ neurons in the output layer for a multiclass classification problem (generally one neuron per class).
What changes in the equations above? Since we now have multiple outputs, both $e_i$ and $y_i$ should be a vector?
","['neural-networks', 'backpropagation', 'feedforward-neural-networks', 'multilayer-perceptrons']",
How can I evaluate a reinforcement learning algorithm over an entire problem space?,"
I am working on implementing an RL agent and I want to demonstrate its effectiveness over a bounded problem space. The setting is essentially a queueing network and so it can be represented as a graph. I want to consider the agent's performance over all graphs up to order $n$ and with average degree from $0$ (edgeless) to $n-1$ (fully connected).
I have looked into generating random graphs using the Erdős–Rényi model, for example. My thought is that I could show the average performance of my agent for different settings of number of nodes and edge probability (under this particular graph generation model).
Are there any established techniques that are along the lines of this approach?
","['reinforcement-learning', 'graph-theory']",
How do multiple coordinate systems help in capturing invariant features?,"
I've been reading this paper that formulates invariant task-parametrized HSMMs. The task parameters are represented in $F$ coordinate systems defined by $\{A_j,b_j\}_{j=1}^F$, where $A_j$ denotes the rotation of the frame as an orientation matrix and $b_j$ represents the origin of the frame. Each datapoint $\xi_t$ is observed from the viewpoint of $F$ different experts/frames, with $\xi_t^{(j)} = A_j^{-1}(\xi_t - b_j)$ denoting the datapoint w.r.t. frame $j$. I quote from the abstract:

""Generalizing manipulation skills to new situations requires extracting invariant patterns from demonstrations. For example, the robot needs to understand the demonstrations at a higher level while being invariant to the appearance of the objects, geometric aspects of objects such as its position, size, orientation and viewpoint of the observer in the demonstrations.""


""The algorithm takes as input the demonstrations with respect to different coordinate systems describing virtual landmarks or objects of interest with a task-parameterized formulation, and adapt the segments according to the environmental changes in a systematic manner.""

Though it makes some intuitive sense, I'm not fully convinced why working with multiple coordinate systems would help us capture invariant patterns in demonstrations, and leave aside the scene-specific details. That is the goal, right? On a very high level, I see that having access to more ""viewpoints"" may help the robot understand the environment better, and neglect viewpoint-specific biases to focus on invariant patterns across different frames. However, this is very handwavy - and I'd love to know specific details about why using multiple viewpoints is a good idea in this case.
Thanks!
","['robotics', 'hidden-markov-model', 'imitation-learning']",
How do I find the data-point with respect to a given frame?,"
I've been reading this paper that formulates invariant task-parametrized HSMMs. In section 3.1 (Model Learning), the task parameters are represented in $F$ coordinate systems defined by $\{A_j,b_j\}_{j=1}^F$, where $A_j$ denotes the rotation of the frame as an orientation matrix and $b_j$ represents the origin of the frame. Each datapoint $\xi_t$ is observed from the viewpoint of $F$ different experts/frames, with $\xi_t^{(j)} = A_j^{-1}(\xi_t - b_j)$ denoting the datapoint w.r.t. frame $j$.
How is $\xi_t^{(j)} = A_j^{-1}(\xi_t - b_j)$ derived? I understand that we must subtract $b_j$, but I'm not sure if I should pre-multiply by $A_j$ or $A_j^{-1}$, so it'd be great if someone could help me understand this better. Since $A_j$ is an orientation matrix, I'd guess that it's orthogonal, and so $A_j^{-1} = A_j^T$ - and it may just be a matter of convention (i.e. depending on how $A_j$ is defined). The details aren't clear from the paper though, and I'd appreciate any help!
","['math', 'robotics']",
How to have closer validation loss and training loss in training a CNN,"
I am using an AlexNet architecture as my Convolutional Neural Network.
A learning rate of 0.00007 and 128 batch_size.
I have 20000 data and 10% test, 40% validation, and 50% for training.
I used 100 epochs to train my network and here are my results for Loss and Accuracy.
I would like to ask how can I get closer validation and training loss in these plots?
At first, I guess the number of epochs was not enough, but I tried more epochs and my results didn't change.
Can I say my training process is complete with this distance between train and validation loss?
Is there any way to have closer loss plots?


","['convolutional-neural-networks', 'training', 'convergence']",
How do LSTMs work if the following two matrices are not able to be multiplied?,"

In the above diagram, the shape of some of the matrices can be seen in the yellow highlight. For instance:
The hidden state at timestep t-1 ($h_{t-1}$) has shape $(na, m)$
The input data at timestep t ($x_{t}$) has shape $(nx, m)$
$Z_{t}$ has shape $(na+nx, m)$ since the hidden state and input data are concatenated in LSTMs.
$W_{c}$ has shape $(na, na+nx)$
$W_{c}$ • $Z_{t}$ has shape $(na, m)$ = $i_{t}$
$W_{i}$ • $Z_{t}$ has shape $(na, m)$ = $ĉ_{t}$
When working through the network to the point $i_{t}$ and $ĉ_{t}$, how can these two be dot producted when the multiplication is not of the form (m x n)(n x p) as per the matrix multiplication definition?:

","['neural-networks', 'machine-learning', 'recurrent-neural-networks', 'long-short-term-memory']",
Improving DQN with fluctuations,"
Hello :) I'm pretty new to this community, so let me know if I posted anything incorrectly and I'll try to change it.
I'm working on the project which aim is to create self-driving agent in CARLA. I built a neural network Xception (decaying  ε-greedy). The other parameters are:

EPISODES: 100
GAMMA: 0.3
EPSILON_DECAY: 0.9
MIN_EPSILON: 0.001
BATCH: 16

Due to the limited computer resources I chose 100 or 300 epochs to train the model, but it generates much fluctuations:



EPISODES: 100
GAMMA: 0.7
EPSILON_DECAY: 0.9
MIN_EPSILON: 0.001
BATCH: 16


Can anyone suggest how can I improve my results? Or it is only the issue of small number of epochs?
","['neural-networks', 'reinforcement-learning', 'dqn']",
how to handle highly imbalanced multilabel classification?,"
I am working on a multilabel classification in which I am having 206 labels. When I saw the percentage of the number of 1's in each label they are way less than 0.1% for each label. The maximum percentage of ones in labels is 0.034%.
Below is the distribution of percentage of one's in each labels

If I simply build a multilabel classification single model. The score it gives may be high but it got biased towards zeros very much so, it doesn't give probability of a label to be one very high. And if I want to build for each label different model, I can treat it as a bunch of imbalanced data and apply smote algorithm to each model, But I have a doubt whether can smote produce a good amount of data to balance because we know how imbalance my data is. Now, doubt is can I gave a try to autoencoders, which I heard good at fraud detection when the data is having a percentage of one's less than 1% or such. Will it perform better in my case? because if it can work well, then I will study autoencoders.
","['neural-networks', 'machine-learning', 'autoencoders', 'multi-label-classification']",
How should I define the reward function to solve the Wumpus game with deep Q-learning?,"
I'm writing a DQN agent for the Wumpus game.
Is the reward function to train the Q-networks (target network and policy) the same as the score of the game, i.e. +1000 for picking up gold, -1000 for falling in pits and dying from the wumpus, -1 each move?
This is naturally cumulative, in that the score changes after each action taken by the agent. Alternatively, is it just a +1 for win, -1 for a loss and 0 in all other situations?
","['reinforcement-learning', 'dqn', 'deep-rl', 'reward-design', 'reward-functions']",
Which reinforcement learning approach to use when there are 2 collaborative agents?,"
Suppose we are training an environment with 2 collaborative agents with Reinforcement Learning. We define the following example: There is a midfielder and a striker. The midfielder's reward depends on how many goals are scored, which however depends on the attacker's performance. And the striker's performance depends on how good the midfielder is at making his passes.
For this type of problem, what do you recommend to study?
","['reinforcement-learning', 'reference-request']",
How to decide if gradients are vanishing?,"
I am trying to debug a convolutional neural network. I am seeing gradients close to zero.
How can I decide whether these gradients are vanishing or not? Is there some threshold to decide on vanishing gradient by looking at the values?
I am getting values close to $4$ decimal places (e.g. $0.0001$) and, in some cases, close to $5$ decimal places (e.g. $0.00001$).

The CNN seems not to be learning since the histogram of weight is also quite similar in all epochs.
I am using the ReLU activation function and Adam optimizer. What could be the reason for the vanishing gradient in the case of the ReLU activation function?
If it is possible, please, point me to some resources that might be helpful.
","['convolutional-neural-networks', 'activation-functions', 'relu', 'vanishing-gradient-problem', 'adam']",
Dynamically adapting activation function,"
I am training a network through reinforcement learning. The policy network learns rotations, but depending on the actual input (state), the output of the network should be restricted to be in certain bounds otherwise it mostly fails to reach these bounds. I am using tanh as last activation function. So, I wonder if there could be a way to modify this last activation function s.th. it can adaptively change bounds depending on input? Or would this have a negative impact in learning?
I would also be open for papers or publications tackling these kind of problems. Thank you for your help!
","['neural-networks', 'reinforcement-learning', 'activation-functions', 'policies', 'state-of-the-art']",
What is the computational complexity in terms of Big-O notation of a Gated Recurrent Unit Neural network?,"
I have been digging up of articles across the internet in context of computational complexity of GRU. Interestingly, I came across this article, http://cse.iitkgp.ac.in/~psraja/FNNs%20,RNNs%20,LSTM%20and%20BLSTM.pdf, where it takes the following notations:
Let I be the number of inputs, K be the number of outputs and H be the number of cells in the hidden layer
And then goes on to explain the computational complexity of FNNs, RNNs, BRNNs, LSTM and BLSTM computational complexity is O(W) i.e., the total number of edges in the network.
where

For FNN: $W = IH + HK$ ( I get this part as, for fully connected networks, we have connections from each input to each node in hidden and subsequently for hidden to output nodes)

For RNN: $W = IH + H^2$ + HK ( The formula is pretty same as is it for FNN but where does this $H^2$ come into picture?)

For LSTM : $W = 4IH + 4H^2 + 3H + HK$ (It becomes more difficult as it comes down to LSTM as to where the 4's and 3's come into the equation? )


Continuing with these notations, can I get a similar notation for GRU as well? This can be very helpful for understanding.
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'computational-complexity', 'gated-recurrent-unit']",
How efficient is SCAWI weight initialization method?,"
I'm currently in the middle of a project (for my thesis) constructing a deep neural network. Since I'm still in the research part, I'm trying to find various ways and techniques to initialize weights. Obviously, every way will be evaluated and we will choose the one that fits best with our data set and our desired outcome.
I'm all familiar with the Xavier initialization, the classic random one, the He initialization, and zeros. Searching through papers I came across the SCAWI one (Statistically Controlled Activation Weight Initialization). If you have used this approach, how efficient is it?
(Also, do you know any good sources to find more of these?)
","['neural-networks', 'reference-request', 'weights', 'weights-initialization']",
Can the hidden layer prior to the ouput layer have less hidden units than the output layer?,"
I attended an introductory class about neural network and I had a question regarding how to choose the number of hidden units per hidden layer.
I remember that the Professor saying that there is no rule for choosing the number of hidden units and that having many of them along with many hidden layers can cause the network to overfit the data and under learn.
However, I still have this question where assuming that we have a network with an input layer of n  input nodes, a first hidden layer of 4 hidden units, a second layer of X hidden units and an output layer of 5 units. Now if I follow the Professor's saying, it would mean that I am allowed to have X = 3 or X = 4 in layer 2.
Is that actually allowed? Won't we have some sort of information gain passing from 4 (or 3) nodes to 5?  The example is illustrated below.

","['neural-networks', 'feedforward-neural-networks', 'hidden-layers']","A layer with bigger number of nodes than previous one is something very common. Some examples are:strategies encoder-decoder (autoencoders) where the encoder typically  has layers with a decreasing number of nodes (until the compressed/encoded data) and the decoder has layers increasing in number of nodes.bidirectional recurrent networks where in the forward direction number nodes decreases and in the backward increases.generators, that from a random vector generates, by example, a full image.As general rule: decrease number of nodes forces the net to filter/resume/abstract/summarize the internal signal information (discarding useless information or noise) while increase number of nodes means apply current information to generate an answer value for a specific question/target.Allow me a strongly simplified example: assume you want a system that, from a photo of an animal, answers the questions: number of legs? has beak ? flies ? . Net inputs are images of birds and dogs.The net architecture can have layers of decreasing size until a single node that will decide ""is bird or dog ?"". From this single item of information (the only one need to answer all the questions) the output layer will have 3 nodes, each one answering one of the specific target questions: number or legs ? 4 if dog, 2 if bird, etc ."
Transfer Learning of Numerical Data,"
It seems like transfer learning is only applicable to neural networks. Is this a correct assumption?
While I was looking for examples of Transfer Learning, most seemed to be based on image data, audio data, or text data. I was not able to find an example of training a neural network on numerical data.
I want to use transfer learning in this particular scenario: I have a lot of numerical data from an old environment, with binary classification labels, and utilizing this, want to train on a new environment to do the same binary classification.
The dataset would look something like this 
Is this possible? What would the model look like?
",['transfer-learning'],"It seems like transfer learning is only applicable to neural networks. Is this a correct assumption?No. Wiki page give you pointers of several examples in other methodologies.While I was looking for examples of Transfer Learning, most seemed to be based on image data, audio data, or text data. I was not able to find an example of training a neural network on numerical data.All the cases you say are converted to numerical data. Image and audio usually via sampling, text via one-hot encoding.I want to use transfer learning in this particular scenario: I have a lot of numerical data from an old environment, with binary classification labels, and utilizing this, want to train on a new environment to do the same binary classification.That is not transfer learning. Transfer learning applies when there are a change in the domain (input features) or in the task (output labels).The dataset would look something like this Sample Table
Is this possible? What would the model look like?For a simple case as the one you present, probably a simple network with one hidden layer will be enough. Train it with original pairs of {features,label} or, if not available, use the current predictor to obtain the label from the features."
SeqGAN - Policy gradient objective function interpretation,"
Could someone clear my doubt on the loss function used in SeqGAN paper . The paper uses policy gradient method to train the generator which is a recurrent neural network here.

Have I interpreted the terms correctly?
What are we summing over? The entire vocabulary of words?

Loss function - my interpretation:

","['reinforcement-learning', 'papers', 'generative-adversarial-networks', 'reinforce', 'seq2seq']",
Is my interpretation of the mathematics of the CBOW and Skip-Gram models correct?,"
I am a mathematics student who is learning NLP, so I have paid a high amount of attention on the mathematics used in the subject, but my interpretations may or may not be right sometimes. Please correct me if any of them are incorrect or do not make sense.
I have learned CBOW and Skip-Gram models.
I think I have understood the CBOW model, and here is my interpretation: First, we fix a number of neighbors of the unknown center word which we would like to predict; let the number be $m$. We then input the original characteristic vectors (vectors of zeros and ones only) of those $2m$ context words. By multiplying those vectors by a matrix, we obtain $2m$ new vectors. Next, we take the average of those $2m$ vectors and this is our hidden layer, namely $v$. We finally multiply $v$ with another matrix, and that is the ""empirical"" result.
I tried to follow the logic to Skip-Gram similarly, but I have been stuck. I understand that Skip-Gram is kind of a ""reversal"" of CBOW, but the specific steps have given me a hard time. So, in Skip-Gram, we only have a center word, and based upon that we are trying to predict $2m$ context words. By similar steps, we obtain a hidden layer, which is again a vector. The final process also involves multiplication with a matrix, but I don't know how we can get $2m$ new vectors based upon one, unless we have $2m$ different matrices?
","['natural-language-processing', 'math', 'word2vec', 'cbow', 'skip-gram']",
Is it possible to have a variable-length latent vector in an autoencoder?,"
I'm trying to have a simple autoencoder but with variable latent length (the network can produce variable latent lengths with respect to the complexity of the input), but I've not seen any related work to get idea from. Have you seen any related work? Do you have any idea to do so?
Actually, I want to use this autoencoder for transmitting the data over a noisy channel, so having a variable-length may help.
","['deep-learning', 'reference-request', 'autoencoders', 'latent-variable']",
Smallest possible network to approximate the $sin$ function,"
The main goal is: Find the smallest possible neural network to approximate the $sin$ function.
Moreover, I want to find a qualitative reason why this network is the smallest possible network.
I have created 8000 random $x$ values with corresponding target values $sin(x)$. The network, which am currently considering, consists of 1 input neuron, 3 neurons in two hidden layers, and 1 output neuron:
Network architecture:

The neural network can be written as function
$$y = sig(w_3 \cdot sig(w_1 \cdot x) + w_4 \cdot sig(w_2 \cdot x)),$$
where $\text{sig}$ is the sigmoid activation function.
$tanh$ activation function:
When I use $tanh$ as an activation function, the network is able to hit the 2 extrema of the $sin$ function:
$tanh$ activation function"" />
Sigmoid activation function:
However, when I use the sigmoid activation function $\text{sig}$, only the first extremum is hit. The network output is not a periodic function but converges:

My questions are now:

Why does one get a better approximation with the $tanh$ activation function? What is a qualitative argument for that?
Why does one need at least 3 hidden neurons? What is the reason that the approximation with $tanh$ does not work anymore, if one uses only 2 hidden neurons?

I really appreciate all your ideas on this problem!
","['neural-networks', 'activation-functions', 'function-approximation', 'universal-approximation-theorems']","Before anything, the function you have wrote for the network lacks the bias variables (I'm sure you used bias to get those beautiful images, otherwise your tanh network had to start from zero).Generally I would say it's impossible to have a good approximation of sinus with just 3 neurons, but if you want to consider one period of sinus, then you can do something. for clarity look at this picture:I've write the code for this task in colab and you can find it here, and you can play with it if you want.If you run the network several times you may get different results (because of different initializations) and you can see some of them at the Results section of the link above. What you showed us in the images above are just two possibilities. But it's interesting that you can get better results with tanh rather than sigmoid and if you want to know why, I highly recommend you to look at this lecture of CS231n. In summary it's because tanh has the negative part and the network can learn better with it.But actually their power of approximation are almost similar because 2*sigmoid(1.5*x) - 1 almost looks the same as tanh(x) and you can find it by looking the picture below:
So why you can't get the same results as tanh? that's because tanh suits the problem better and if the network wants to get the same result as tanh with sigmoid it should learn their transformation parameters and learning these parameters makes the learning task harder. So It's not impossible to get the same result with sigmoid but it's harder. And to show you that its possible, I have set the parameters of the network using sigmoid manually and got the result below (you can get better results if you have more time):
At last if you want to know why you can't get the same result with 2 neurons instead of 3 neurons, it's better to understand what does the network do with 3 neurons.
If you look at the output of the first layer, you may see something like this (which are outputs of two neurons it has):Then the next layer gets the difference between the output of these two neurons (which is like sinus) and applies sigmoid or tanh to it, and that's how you get a good result. But when you have just one neuron in the first layer, you can't imagine some scenario like this and approximating one period of sinus is out of it's ability (underfitting)."
How to create a Q-Learning agent when we have a matrix as an action space?,"
I have a 2-dimentional matrix as an action space, the rows being a resource to be allocated, and the columns are the users that we will allocate the resources to. (I built my own RL environment)
The possible actions are 'Zero' or 'One'. One if the resource was allocated to the user, Zero if not.
I have a constraint related to the resource allocation, which states that each resource can be allocated to one user only, and the resource should only be allocated to users who have requested a resource to be allocated to them, and that would be the state space which is another matrix.
A penalty would be applied if the agent violates the constraints and the episode would end and the reward would equal the penalty. Otherwise, the reward would equal the sum of all the users that were satisfied with the allocation.
I am struggling with the implementation. The agent starts by exploring, then little by little it starts exploiting. When it gets to be more exploitative, I've noticed that the action matrix's values are all set to 'One', and the penalty always has the same value from episode to episode.
","['reinforcement-learning', 'python', 'q-learning']",
How to define Agar.io state and action space?,"
I am trying to implement an AI bot for my Agar.io clone using deep neural network.
However, I am struggling with the state and action space of the AI bot.
Because the bot can take real number for position and velocity, can I say the state space is continuous?
For the action space, I am thinking something like (velocityX, velocityY, ""split to half"", ""eject mass"").
What should be the number of input nodes in the input layer for my Neural network? And what are those input(observations, rewards)?
As the number of players and AI bots are changing, how can I train a dynamic network with changing input node number?
For the outputs, how can I get a continuous action output like velocity?
As a reference, you can learn about the game rules from this short youtube video:
20 Rules and Game Mechanism of Agar (How to Play Agar.io)
","['neural-networks', 'reinforcement-learning', 'deep-learning', 'game-ai']",
"In the definition of the state-action value function, what is the random variable we take the expectation of?","
I know that
$$\mathbb{E}[g(X) \mid A] = \sum\limits_{x} g(x) p_{X \mid A}(x)$$
for any random variable $X$.
Now, consider the following expression.
$$\mathbb{E}_{\pi} \left[ \sum \limits_{k=0}^{\infty} \gamma^{k}r_{t+k+1} \mid s_t = s, a_t = a \right]$$
It is used for the calculation of Q values.
I can understand the following

$A$ is $\{s_t = s, a_t = a\}$ .i.e., agent has been performed action $a$ on state $s$ at time step $t$ and

$g(X)$ is $\sum\limits_{k=0}^{\infty} \gamma^{k}r_{t+k+1}$ i.e., return (long run reward).


What I didn't understand is what is $X$ here. i.e., what is the random variable on which we are calculating long-run rewards?
My guess is policy function. It is averaging long-run rewards over all possible policy functions. Is it true?
","['reinforcement-learning', 'q-learning', 'value-functions', 'notation', 'random-variable']","I am using the convention of uppercase $X$ for random variable and lowercase $x$ for an individual observation. It is possible your source material did not do this, which might be causing your confusion. However, it is the convention used in Sutton & Barto's Reinforcement Learning: An Introduction.What I didn't understand is what is 𝑋 here. i.e., what is the random variable on which we are calculating long-run rewards?The random variable is $R_t$, the reward at each time step. The distribution of $R_t$ in turn depends on the distribution of $S_{t-1}$ and $A_{t-1}$ plus the policy and state progression rules. There is no need to include the process that causes the distribution of each $R_t$ in every equation. Although sometimes it is useful to do so, for example when deriving the Bellman equations for value functions.My guess is policy function. It is averaging long-run rewards over all possible policy functions. Is it true?No, this is not true. In fact, it is the more usual assumption that the policy function $\pi(a|s)$ remains constant over the expectation, and this is what the subscript $\pi$ in $\mathbb{E}_{\pi}[...]$ means.The expectation is over randomness due to the policy $\pi$, plus randomness due to the environment, which can be described by the function $p(r, s'|s, a)$ - the probability of observing reward $r$ and next state $s'$ given starting in state $s$ and taking action $a$. These two functions combine to create the distribution of $R_t$. It is possible that both functions are deterministic in practice, thus $R_t$ is also deterministic. However, RL theory works on the more general stochastic case, which is also used to model exploratory actions, even if the target policy and environment are deterministic."
Why does the training time of SVMs dramatically decrease after applying dimensionality reduction to the features?,"
Training an SVM with an RBF kernel model with c = 5.5 and gamma = 1.06, for a 5-class classification problem on the NSL-KDD train data-set with 122 features using one vs rest strategy takes $2162$ seconds. Also, considering binary classification (c = 10, gamma = 4), it takes $520.56$ seconds.
After dimensionality reduction, from 122 to 30, using a sparse auto-encoder, the training time falls dramatically, from $2162$ to $240$ and $520$ to $170$, while using the same hyperparameters for the RBF-kernel.
What is the reason for that? Is it not true that using kernel neutralized the effect of high dimensions?
","['machine-learning', 'training', 'autoencoders', 'support-vector-machine', 'feature-selection']","SVM complexity is $O(\max(n,d)\min(n,d)^2)$ according to Chapelle, Olivier. ""Training a support vector machine in the primal."" Neural Computation 19.5 (2007): 1155-1178.$n$ is the number of instances and $d$ is the number of dimensions. I'm assuming that you have more instances than dimensions giving a complexity of $O(nd^2)$. Hopefully this explains fully why reducing the number of dimensions will reduce the training time."
How to define loss function for Discriminator in GANs?,"
To train the discriminator network in GANs we set the label for the true samples as $1$ and $0$ for fake ones. Then we use binary cross-entropy loss for training.
Since we set the label $1$ for true samples that means $p_{data}(x) = 1$ and now binary cross-entropy loss is:
$$L_1 = \sum_{i=1}^{N} P_{data}(x_i)log(D(x)) + (1-P_{data}(x_i))log(1-D(x))$$
$$L_1 = \sum_{i=1}^{N} P_{data}(x_i)log(D(x))$$
$$L_1 = E_{x \sim P_{data}(x)}[log(D(x))]$$
For the second part, since we set the label $0$ for fake samples that means $p_{z}(z) = 0$ and now binary cross-entropy loss is:
$$L_2 = \sum_{i=1}^{N} P_{z}(z_i)log(D_{G}(z)) + (1-P_{z}(z_i))log(1-D_{G}(z))$$
$$L_2 = \sum_{i=1}^{N} 1-P_{z}(z_i)log(1-D_{G}(z))$$
$$L_2 = E_{z \sim \bar{P_{z}(z)}}[log(1-D_{G}(z))]$$
Now we combine those two losses and get:
$$L_D = E_{x \sim P_{data}(x)}[log(D(x))] + E_{z \sim \bar{P_{z}(z)}}[log(1-D_{G}(z))]$$
When I was reading about GANs I saw that the loss function for discriminator is defined as:
$$L_D = E_{x \sim P_{data}(x)}[log(D(x))] + E_{z \sim P_{z}(z)}[log(1-D_{G}(z))]$$
Should not it be $E_{z \sim \bar{P_{z}(z)}}$ instead of $E_{z \sim P_{z}(z)}$ ?
","['deep-learning', 'objective-functions', 'generative-adversarial-networks', 'generative-model', 'discriminator']",
What is the right way to train a generator in a GAN?,"
I am not fully understanding how to train a GAN's generator. I have a few questions below, but let me first describe what I am doing.
I am using the MNIST dataset.

I generate a batch of random images (the faked ones) with the generator.

I train the discriminator with the set composed of faked images and real MNIST images.

After the training phase, the discriminator modifies the weights in the direction of recognizing fake (probability 0) from real (probability 1) ones.

At this point, I have to consider the combined model of generator and discriminator (keep untrainable the discriminator) and put in the generator as input the faked images with the tag of 1s (as was real one).


My questions are:
Why do I have to set to real these fake images, and what fake images are these? The one generated in the first round from the generator itself? Or only the one classified as faked by the discriminator? (Then they could be both real images classified wrongly or fake images classified in the right way). Finally, what the generator does to these faked images?
","['training', 'generative-adversarial-networks', 'generative-model', 'mnist']",
What is the difference between neural networks and other ways of curve fitting?,"
For simplicity, let's assume we want to solve a regression problem, where we have one independent variable and one dependent variable, which we want to predict. Let's also assume that there is a nonlinear relationship between the independent and dependent variables.
No matter the way we do it, we just need to build a proper curved line based on existing observations, such that the prediction is the best.
I know we can solve this problem with neural networks, but I also know other ways to create such curves. For example:

splines

kriging

lowess

Something I think would also work (do not know if exists): fitting curve using a series of Fourier sine waves, and so on


My questions are:

Is it true that neural networks are just one of the ways to fit a non-linear curve to the data?

What are the advantages and disadvantages of choosing a neural network over other approaches? (maybe it becomes better when I have many independent variables, and another little guess: maybe the neural network is better in omitting the effect of linear dependent input variables?)


","['neural-networks', 'machine-learning', 'comparison', 'non-linear-regression', 'curve-fitting']","In some sense, you're right that a neural net is just another tool to fit data. However, it's quite the tool! There's this universal approximation theorem saying that, under decent conditions, a neural network can get as close as you want to a wide class of functions. This means that you can get the network to give you complicated shapes with squiggles all over if that's the right trend.The universal approximation theorem is a big upside. You don't have to specify that you want to model with sine curves or a particular type of spline. You just let the computer figure that out for you. The result is the ability to model complex patterns and make accurate predictions. The drawback is that the modeling can pick up on coincidences in the data that look like a trend but are not. This causes overfitting. When your goal is to make accurate predictions, a model that has badly overfit does nothing for you. A second drawback is that neural networks are hard to interpret. A third drawback is that they can take a long time to train, while a linear regression is just a matrix inversion and a few matrix products (the $\hat{\beta}=(X^TX)^{-1}X^Ty$)."
"In LSTMs, how does the additive property enables better balancing of gradient values during backpropagation?","
There are two sources that I'm using to to try and understand why LSTMs reduce the likelihood of the vanishing gradient problem associated with RNNs.
Both of these sources mention the reason LSTMs are able to reduce the likelihood of the vanishing gradient problem is because

The gradient contains the forget gate's vector of activions
The addition of four gradient values help balance gradient values

I understand (1), but I don't understand what (2) means.

Sources are SLIDE 119 on http://cs231n.stanford.edu/slides/2020/lecture_10.pdf

Sentence beginning ""Another important property to notice is that the cell state"" on https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577


Any insight would greatly be appreciated!
","['recurrent-neural-networks', 'long-short-term-memory', 'backpropagation', 'vanishing-gradient-problem', 'bptt']",
Training while predicting on dataset [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I have been trying to figure out whether if I train a model and then while predicting is it possible to train images too just like humans
Somehow converting valid images to the dataset by asking us when an object is shown
Like the Google Photos somehow they ask us if they predicted a face correctly and then reinforces on it
","['reinforcement-learning', 'training', 'ai-milestones']",
Are Markov Random Fields and Conditional Random Fields still used in computer vision?,"
Back before deep learning, there were a lot of different attempts at computer vision. Some involved Conditional Random Fields and Markov Random Fields, which were both computationally difficult and hard to understand/implement.
Are these areas still being developed in the computer vision domain? What was the end result of this line of study? I haven't seen any papers on this topic be cited in top-performing benchmarks, so I assume nobody cares about them anymore, but I wanted to ask.
","['computer-vision', 'state-of-the-art', 'conditional-random-field']",
What is the formal terminology for emotion recognition AI?,"
I'm researching the use of emotion recognition in Intelligent Tutoring Systems and trying to more effectively find and formally reference materials. My question is whether this is the most formal terminology (i.e. ""emotion recognition""), because I've also seen ""affect recognition"" and ""affective computing"". Maybe it's a matter of taste, but I know sometimes the market terminology is different from the engineering terminology and I'd like to be more in tune with the engineers.
Maybe there is a leading classification system of related technologies (e.g. facial recognition, sentiment analysis, etc.)?
I'm seeing the ""affective-computing"" tag now, but not sure if these tags reflect a formal classification system in the field of AI.
","['terminology', 'facial-recognition', 'affective-computing', 'emotion-recognition']",
Bechmark models for Text Classification / Sentiment Classification,"
I am currently working on a novel application in NLP where I try to classify empathic and non-empathic texts. I would like to compare the performance of my model to some benchmark models. As I am working with models based on Word2Vec embeddings, the benchmark models should also be based on Word2Vec, however I am looking for some relatively easy, quick to implement models.
Do you have any suggestions?
","['natural-language-processing', 'word-embedding', 'text-classification', 'word2vec', 'benchmarks']",
GAN for specific face attribute modification,"
A recent paper ""MagGAN High Resolution Face Attribute Editing with Mask Guided GAN"" published this month (October 2020) describe how an approach has been developed to deal with specific face attribute editing.
The thing is that in this paper and related work (StarGAN, CycleGAN, AttGAN, STGAN ...) seems to tackle the process of adding / editing a face attribute (e.g. adding a hat or mustache...), but not really modifying the face attribute (like eyes, nose, lips ...)
Is there anyway we can make a model that can edit for example the nose type/size or any related works already published?
",['generative-adversarial-networks'],
What exactly does meta-learning in reinforcement learning setting mean?,"
We can use DDPG to train agents to stack objects. And stacking objects can be viewed as first grasping followed by pick and place. In this context, how does meta-reinforcement learning fit? Does it mean I can use grasp, pick and place as training tasks and generalize to assembling objects?
","['reinforcement-learning', 'definitions', 'meta-learning']",
When should we use separable convolution?,"
I was reading the ""Deep Learning with Python"" by François Chollet. He mentioned separable convolution as following

This is equivalent to separating the learning of spatial features and
the learning of channel-wise features, which makes a lot of sense if
you assume that spatial locations in the input are highly correlated,
but different channels are fairly independent.

But I could not understand what he meant by saying ""correlated spatial locations"". Can some explain what he means or the purpose of separable convolutions? (except performance-related part).
Edit: Separable convolution means that first depthwise convolution is applied then pointwise convolution is applied.
","['convolutional-neural-networks', 'convolution']","Context of the questionThis is a link to the text cited in the question.It refers to the usage of SeparableConv2D (tf, keras name). A related question on StackOverflow is ""What is the difference between SeparableConv2D and Conv2D layers"". This answer points to this excellent article by Chi-Feng Wang:A Basic Introduction to Separable ConvolutionsAnswer to the questionIn image processing, a separable convolution converts a NxM convolution to two convolutions with kernels Nx1 and 1xM. Using this idea, in NN a SeparableConv2D converts a WxHxD convolution (width x height x depth, where depth means number of incoming features ) to two convolutions with kernels WxHx1 and 1x1xD.Note the first kernel doesn't handles information across features, thus, it is ""learning of spatial features"". The 1x1xD kernel doesn't handles different points, it is ""learning of channel-wise features"".About the phrase ""spatial locations in the input are highly correlated"", my understanding of what the author means is: Assume we have a channel (feature) image that each pixel measures the ""distance to the background"". When we pass from one pixel to a neighbors one, it is expected some continuity in the value (except for edge pixels): correlation. Instead, if we have a channel that measures ""brightness"" and another one that measures ""distance to background"" the two values for one specific pixel has little correlation.Finally, about title question ""When should we use separable convolution?"" : if the final output must depend of some features of one pixel and some other features of neighbors pixels in a very unpredictable way, a complete WxHxD convolution must be used. However if, as is more usual, you can handle first spatial dependencies (neighborhood) to extract pixel features and next handle pixel-by-pixel these features to get the output, better use a WxHx1 followed by 1x1xD, saving lots of network parameters, thus, saving training time."
How to determine if Q-learning has converged in practice？,"
I am using Q-learning and SARSA to solve a problem. The agent learns to go from the start to the goal without falling in the holes.
At each state, I can choose the action corresponding to the maximum Q value at the state (the greedy action that the agent would take). And all the actions connect some states together. I think that would show me a road from start to goal, which means the result converges.
But some others think that as long as the agent learns how to reach the goal, the result converges. Sometimes the success rate is very high but we cannot get the road from Q table. I don't know which one means the agent is trained totally and what the converged result means.
","['reinforcement-learning', 'q-learning', 'convergence', 'temporal-difference-methods', 'sarsa']",
Should the range and initial values of weights and biases be adjusted to fit input and output data?,"
As a routine (in typical everyday  tasks) of a data scientist, should they usually decide about weights and biases range and initial values as a function of which data they are planning to insert as an input, and which type of data they expect to get in the output? Or we usually do not deal with such fine-tuning, and let the algorithm to do it?
One could answer that normalizing inputs solves the problem and no need to fit weights and biases, but I guess they depend also on expected output.
To summarize:

is it common to deal with weights and biases in everyday tasks or in most of the cases existing algorithms do it well?

what are the rules of thumb for how to decide about range and initial values of weights and biases?


","['neural-networks', 'machine-learning', 'deep-learning', 'hyper-parameters', 'weights']","is it common to deal with weights and biases in everyday tasks or in most of the cases existing algorithms do it well?No; and it is no coincidence that you will not be able to find any reference to such a practice in any course or tutorial about neural networks. Such a practice would require a whole additional level of (business/SME) know-how in order to meaningfully apply neural networks to real-world problems, and fortunately this is not necessary.The desired situation is for both weights & biases to remain in a relatively small* range around zero, among other reasons because this avoids the exploding & vanishing gradient problems, which is catastrophic for learning; trying to adjust for the scale of our inputs & outputs by scaling accordingly the model weights & biases is the wrong approach, and never followed.[*How small? Well, see here and here for what a change in just the standard deviation of a zero-mean weight initialization from 1.0 and 0.1 to 0.01 can do to the model performance]It is a well-established fact by now that neural nets work with normalized inputs, and, depending on the problem, normalized outputs as well; which answers to your objection:One could answer that normalizing inputs solves the problem and no need to fit weights and biases, but I guess they depend also on expected output.It depends indeed, but the solution here is to scale the output(s) as well, and not the weights.Now, it's true that de-scaling the outputs back to their original range in order to be able to meaningfully compare them with the ground truth (and possibly compute more meaningful metrics, like the error on our true output range, and not the scaled one) is AFAIK seldom mentioned in introductory expositions; but this is indeed the correct thing to do (especially for regression problems, where metrics like MSE are scale-sensitive), instead of trying to manually intervene on the weights range. For details, see own answers in How to interpret MSE in Keras Regressor and ANN regression accuracy and loss stuck.what are the rules of thumb for how to decide about range and initial values of weights and biases?Range aside, the general rule, as already implied, is to initialize the biases with zeros and the weights with small random values around zero. Nevertheless, the exact details are an area of active research. Currently used initialization schemes that are already integrated into the relevant frameworks (Tensorflow, Keras, Pytorch etc) are the Glorot (or Xavier) and He initializations (for a nice overview, see Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming).Beyond these routinely-used approaches that have already reached the practitioner's workbench, and moving closer to the front of active theoretical research, the Lottery Ticket Hypothesis (finding ""winning"" weight initializations that require minimal training) is an ultra-hot topic lately."
Evaluate model multiple times in loss function? Is this reinforcement learning?,"
I am interested in models that exhibit behavior. My goal is a model that survives indefinitely on a two dimensional resource landscape. One dimension represents the location (0 to 1) and the second says if there is a resource available at that location (-1 = resource one, 0 = no resource, 1 = resource two).
The landcape looks like this:
location = [0, 0.2, 0.4, 0.6, 0.8, 1]
resource = [-1, 0,   0,   0,   0,  1] (I added spaces so the elements line up)

My model represents an organism deciding if it will move or rest on the landscape at each time step. The organism has reserves of each resource. The organism fills its reserve of a resource if it rests on the resource and loses 1 unit of both resources at each time step. I am considering neural networks to represent my organisms. The input would be 4 values; The location on the landscape, the resource value at that location, and the reserve levels of resource one and two. The output would be 3 values; move right, rest, move left. The highest value decides what happens. To survive indefinitely the model will have to bounce between the ends of the landscape, briefly resting on the resource. Model evaluation would go like this: start the model in the middle of the landscape with full resource reserves. Allow time to pass until one of the resource reserves is depleted (the organism dies).
My question is this: Can my loss function be evaluating the model until it dies? 1/survival time could be the loss value to be minimized by gradient descent. Is this a reinforcement learning problem (I don't think so..?) Thanks!!
","['neural-networks', 'reinforcement-learning', 'tensorflow', 'keras', 'objective-functions']","Can my loss function be evaluating the model until it dies? 1/survival time could be the loss value to be minimized by gradient descent.In order to use backpropagation and gradient descent, you have to relate the loss function directly to the output of the neural network. Your proposed loss function is too indirect, it is not possible to turn it directly into a gradient that could be used to alter the neural network weights.In addition, the specific function of time you have chosen will be difficult to optimise, as incrememental improvements from e.g. 10 to 11 time steps surviving will provide a much lower signal for adjusting behaviour than the improvement from  e.g. 2 to 3 lifetime. If the environment has enough randomness (and typically these kind of a-life scenarios do), then the signal here could be swamped by random events and very hard to optimise, requiring a larger number of samples in order to extract expected improvements to the loss function.Is this a reinforcement learning problem (I don't think so..?)It is very close to a definition of reinforcement learning (RL) problem. For a RL problem you need the following things:An environment in which an agent exists, and which has a measurable state.A set of actions = sequential decisions that need to be made, and that have consequences.The consequences of any action are:In your problem definition you don't have a reward signal, but it would be easy to add one. A suitable one would be $+1$ per time step.Technically your problem would also be partially observable (sometimes called a POMDP), in that the agent does not get to see resources available in other locations. It only knows its current location, its internal state and resources available at its current location. This is not a major issue, although you should note that adding some kind of memory (either open-ended memory as in a recurrent neural network, or explicitly added to the state) would allow for more efficient agents. That's not a RL issue as such, any learning process without ability to form or use memories would be limited in this environment, and you might want to look into that as a later experiment.How RL helps is that it provides a framework to convert your problem definition into measurements and gradients for the neural network to improve its performance.As you have set up your neural network to predict best action choice, this would naturally lend itself to policy gradient methods in RL, such as REINFORCE. As it is a simple problem, I would expect REINFORCE with baseline to perform well enough for it.I will not explain the algorithm here in full detail, but the basic approach is to have the agent act with the current network for a few episodes, collecting data on its choices and performance. You will get a dataset of (state, action, return = sum of rewards to end of episode). You then use that as a labelled dataset to train a minibatch as if the action choice was correct ground truth, but multiply each gradient by (return - baseline) where the baseline is typically the average return seen so far from that state. You may need a second neural network during training to estimate that expected return (aka state value). After using a minibatch once, you will need to discard it, as it represents results for the previous iteration of the network before the weight updates. There are ways around this, but not typically done in REINFORCE - instead the approach is to just keep generating data and train on the new mini-batch as fast as you can go.RL offers other methods which may work just as well to solve your problem, but I suspect REINFORCE will serve you well since it will allow the agent to randomise direction choice which is important when it cannot see where the resources are and has no memory of where it has searched.You don't have to use RL for this problem. An alternative that may work for you is using genetic algorithms to tune the network architecture and weights. It avoids using gradients, but I would still recommend a simple fitness function equal to number of time steps survived. There is a framework called NEAT which is ideal for this sort of a-life control problem."
Computer vision - Can you put more weight on a specific part of the object?,"
Let's say I'm looking for any item that has a certain shape (outline) in a photo. but I can further classify it only according to particular features, that most of them are expected to be shown only in a smaller area of the the object itself.
How may I give more weight, in the model, to that particular area, in order to avoid wrong classification issues?
What is the flow, and are there specific tools that should be used for that purpose?
Example:
I want to detect all triangles in the image, and try to classify them like this:
If triangle has 3 lines in its corner, it's A type. if only two lines, it's B type.
So the triangles outline composes 100% of the object, but we can see that the area where the red lines are present, is only about 10% of the object area. How may I give more weight and tell the model to carefully look for the details in that area, so it doesn't confuse A with B or vice versa, just because the other 90% of the shape is similar.
And Of course, I want the certainty level to be as close as possible to 100%, for both A and B, and to be distinguished from the other option.
So my goal is the get this output:
Purple Triangle ==> Type A, certainty, 99%. Type B: certainty: 50%
Green Triangle ==> Type B, certainty, 99%. Type A: certainty: 50%

","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'computer-vision']",
Is it possible to know the distance objects are from camera based on only knowing one object's height?,"
I am doing a project where I have to know distance a particular object is from camera. In the photo I only know one of the object's height, but I don't know how far away that object is and I don't know how tall are other objects. Is it possible to write a code or do some geometry to know other objects distances from camera using only the height of one object? For example I have an image where 5 meters away there is a box which is 1 meter high, I wanna know the distance to human who is 12 meters away, or to know a distance to a dog who is 7 meters away. Maybe you guys know any datasets or models which deal with the same problem as I am facing. Any help will be appreciated.
","['neural-networks', 'convolutional-neural-networks', 'datasets', 'math']",
How Restricted Boltzman Machine (RBM) generates hand-written digit?,"
I am reading RBMs from this paper. In Fig1 they show an example of generating hand-written digit using RBMs. This is the figure they are showing:

In the learning step first we sample $h$ from $h \sim P(h|v)$ and then reconstructe $v$ from $v \sim P(v|h)$. Now in the generating step they are sampling $v$ from $v \sim P(v|h)$. My question is, in generating step if we do not sample $h$ from $h \sim P(h|v)$ how can we get $P(v|h)$?
","['deep-learning', 'restricted-boltzmann-machine']",
Role of autoencoder in Hierarchical Extreme Learning Machine,"
I want to build HELM neural network that consists of autoencoder (AE) and one class classification (OC).
HELM with AE and OC have following shape:

That is, hidden layer output of AE is input of OC.
Training of HELM consists of training AE and OC separately. In order to train each neural network in HELM, first there are generated random weights and biases between input and hidden layers, and then based of them and activation function (for example sigmoid), only weights between hidden and output layers are trained. But then what's the point in training weights between hidden and output layers in AE, since only output of its hidden layer is provided as input into one-class classifier? What is point in use AE in HELM if weights between input and hidden layers of AE are basicaly random?
Following paper (page 6):
https://arxiv.org/pdf/1810.05550.pdf
confirms that output of hidden layer of AE is input for OC, but also on the contrary in Algorithms 2 and 3 (pages 6, 7), there is shown that input of OC is AE input vector multiplied by matrix of weights between hidden and output layer, what sounds weird for me.
","['classification', 'training', 'autoencoders']",
Linear output layer back propagation,"
So I'm stack to something that it's probably very easy but I can't get my head around it. I'm building a Neural Network that will consist of many layers with non-linear activation functions (probably ReLUs) and the last output layer will be linear because we are trying to catch a specific number and not a probability. I've done the forward propagation calculations but I'm stuck at the back propagation ones.
Let's say that I'm gonna use the cross entropy loss function: (we will implement the MSE as well)   $-(y \log (a)+(1-y) \log (1-a))$. (I understand that this is not a good option for a regression problem)
So we can easily find the dJ/dA $d A=\frac{\partial J}{\partial A}=-\left(\frac{y}{A}-\frac{1-y}{1-A}\right)$ of the last layer and we can start going backwards finding the $\frac{\partial J}{\partial Z^{[L]}}$ which we can calculate from the equation: $d Z^{[L]}=d A*g^{\prime}\left(Z^{[L]}\right)$ The problem lies at the second part of this equation where the derivative of g is.
What will be the outcome since we have a linear activation function which derivative is equal with 1? (Activation function: f(x) = x, f'(x) = 1)
Will it be an identity matrix with the shape of Z[L] or a matrix full of ones with the same shape again? I'm asking about the term $g^{\prime}\left(Z^{[L]}\right)$.
Many thanks.
","['neural-networks', 'backpropagation', 'linear-regression']",
How are the lower and upper bound values of the moths determined in the Moth-Flame Optimization algorithm?,"
I am currently implementing the Moth-Flame Optimization (MFO) Algorithm, based on the paper: Moth-Flame Optimization Algorithm: A Novel Nature-inspired Heuristic Paradigm.
To calculate the values of the Moths, it uses two arrays of values, which contain upper and lower values for each variable. However, as far as I can see. it mentions nothing about what these values are. Quoting from the paper:

As can be seen, there are two other arrays called $ub$ and $lb$. These matrixes define the upper and lower bounds of the variables as follows:
$ub = [ub_1, ub_2, ub_3, \dots,ub_{n-1}, ub_n]$
where $ub_i$ indicates the upper bound of the $i$-th variable.
$lb = [lb_1, lb_2, lb_3, \dots, ub_{n-1}, ub_n]$
where $lb_i$ indicates the lower bound of the $i$-th variable

After that, it says nothing more about this matter
So, if anyone has any idea of how these bound values are determined, please tell me!
","['optimization', 'papers', 'meta-heuristics', 'moth-flame-optimization']",
What is the need for so many filters in a CNN?,"
Consider the following coding line related to CNNS
Conv2D(64, (3,3), strides=(2, 2), padding='same')

It is a convolution layer with filter size $3 \times 3$ and step size of $2\times 2$.
I am confused about the need for $64$ filters.
Are they doing the same task? Obviously, it is no. (one is enough in this case)
Then how do each filter differ by? Is it in hovering over the input matrix? Or is it in the values contained by filter itself? Or differs in both hovering and content?
I am finding difficulty in visualizing it.
","['convolutional-neural-networks', 'computer-vision', 'keras', 'filters', 'convolutional-layers']","Then how do each filter differ by? Is it in hovering over the input matrix? Or is it in the values contained by filter itself? Or differs in both hovering and content?The filters (aka kernels) are the learnable parameters of the CNN, in the same way that the weights of the connections between the neurons (or nodes) are the learnable parameters of a multi-layer perceptron (or feed-forward neural network).So, the value of these filters is not fixed or pre-determined, but will depend on how you train the CNN, i.e. the learning algorithm, the objective function and the data. If you use gradient descent as the learning algorithm, you will be minimizing a loss (aka cost or error) function (e.g. the cross-entropy, in the case of classification problems). To do that, you need to find the gradient of the loss function with respect to the filters. You then apply a step of gradient descent (i.e. you add a scaled version of the gradient of the loss function with respect to the parameters to the parameters), so that this loss decreases.To answer your question more directly, the only thing that usually changes is just the value of the filters. The convolution (or cross-correlation) operation is the same for all filters.Why do you use more than one filter? The usual explanation is that each filter, when convolved with the input, will extract different features from it, and the specific features that they will extract will depend on the specific values of the filters, which, in turn, depend on the data, so we can say that CNNs are data-driven feature extractors. If you are familiar with image processing techniques, then you know that different filters, when convolved with the same image, can have different effects (e.g. blurring or de-noising)."
Is Monte Carlo tree search guaranteed to converge to the optimal solution in two player zero-sum stochastic games?,"
I'm aware that convergence proofs for Monte Carlo tree search exist in the case of deterministic zero sum games and Markov decision processes.
I have come across research which applies MCTS to zero-sum stochastic games, however I was unable to find proof that such an approach is guaranteed to converge to the optimal solution.
If anyone is able to provide references or an explanation explaining why or why not MCTS is guaranteed to converge to the optimal solution in this setting I would appreciate it a lot.
","['monte-carlo-tree-search', 'convergence', 'game-theory']",
Computation of initial adjoint for NODE,"
I'm reading the paper Neural Ordinary Differential Equations and I have a simple question about adjoint method. When we train NODE, it uses a blackbox ODESolver to compute gradients through model parameters, hidden states, and time. It uses another quantity $\mathbf{a}(t) = \partial L / \partial \mathbf{z}(t)$ called adjoint, which also satisfies another ODE. As I understand, the authors build a single ODE that computes all the gradients $\partial L / \partial \mathbf{z}(t_{0})$ and $\partial L / \partial \theta$ by solving that single ODE. However, I can't understand how do we know the value $\partial L / \partial \mathbf{z}(t_1)$ which corresponds to the initial condition for the ODE corresponds to the adjoint. I'm using this tutorial as a reference, and it defines custom forward and backward methods for solving ODE. However, for the backward computation (especially ODEAdjoint class in the tutorial) we need to pass $\partial L / \partial \mathbf{z}$ for backpropagation, and this enables us to compute $\partial L / \partial \mathbf{z}(t_i)$ from $\partial L / \partial \mathbf{z}(t_{i+1})$, but we still need to know the adjoint value $\partial L / \partial \mathbf{z}(t_N)$. I do not understand well about how pytorch's autograd package works, and this seems to be a barrier to understand this. Could anyone explain how it operates, and where $\partial L / \partial \mathbf{z}(t_1)$ (or $\partial L / \partial \mathbf{z}(t_N)$ if this is more comfortable) comes from? Thanks in advance.

Here's my guess for the initial adjoint from simple example. Let $d\mathbf{z}/dt = Az$ be a 2-dim linear ODE with given $A \in \mathbb{R}^{2\times 2}$. If we use Euler's method as a ODE solver, then the estimate for $z(t_1)$ is explicitly given as $$\hat{\mathbf{z}}(t_1) = \mathrm{ODESolve}(\mathbf{z}(t_0), f, t_0, t_1, \theta))= \left(I + \frac{t_1 - t_0}{N}A\right)^{N} \mathbf{z}(t_0) $$ where $N$ is the number of steps for Euler's method (so that $h = (t_1 - t_0) /N$ is the step size). If we use MSE loss for training, then the loss will be
$$
L(\mathbf{z}(t_1)) = \Bigl|\Bigl| \mathbf{z}_1 - \left(I + \frac{t_1 - t_0}{N}A\right)^N\mathbf{z}(t_0)\Bigr|\Bigr|_2^2
$$
where $\mathbf{z}_1$ is the true value at time $t_1$, which is $\mathbf{z}_1 = e^{A(t_1 - t_0)}\mathbf{z}(t_0)$. Since adjoint $\mathbf{a}(t) = \partial L / \partial \mathbf{z}(t)$ satisfies $$\frac{d\mathbf{a}(t)}{dt} = -\mathbf{a}(t)^{T} \frac{\partial f(\mathbf{z}(t), t, \theta)}{\partial \mathbf{z}} = \mathbf{0},$$
$\mathbf{a}(t)$ is constant and we get $\mathbf{a}(t_0) = \mathbf{a}(t_1)$. So we do not need to use augmented ODE for computing $\mathbf{a}(t)$. However, I still don't know what $\mathbf{a}(t_1) = \partial L / \partial \mathbf{z}(t_1)$ should be.  If my understanding is correct, since $L = ||\mathbf{z}_1 - \mathbf{z}(t_1)||^{2}_{2}$, it seems that the answer might be
$$
\frac{\partial L}{\partial \mathbf{z}(t_1)} = 2(\mathbf{z}(t_1) - \mathbf{z}_1).
$$
However, this doesn't seem to be true: if it is, and if we have multiple datapoints at $t_1, t_2, \dots, t_N$, then the loss is
$$
L = \frac{1}{N} \sum_{i=1}^{N}||\mathbf{z}_i  -\mathbf{z}(t_i)||_{2}^{2}
$$
and we may have
$$
\frac{\partial L}{\partial \mathbf{z}(t_i)} = \frac{2}{N} (\mathbf{z}(t_i) - \mathbf{z}_i),
$$
which means that we don't need to solve ODE associated to $\mathbf{a}(t)$.
","['neural-networks', 'backpropagation']",
How can I train a CNN to detect when a person is smoking outside of shop given images from a video camera?,"
My friend is working at a pizza shop. He takes cigarette breaks in an area that is covered by the public webcam of our town.
I now want to train a convolutional neural network to be able to detect when he is smoking.
Can somebody direct me in the right direction, which tools/tutorials I should look at for this classification task? I already saved 18 hours worth of pictures, one a minute. He is in 28 of these images, I will probably save a few more, maybe 2-3 days. But I don't really know how to start this.
","['convolutional-neural-networks', 'classification', 'computer-vision', 'image-recognition', 'object-recognition']",
What happens when an opponent a neural network is playing with does not obey the rules of the game (i.e. cheats)?,"
For example, if AlphaZero plays with an opponent who has a right to move chess figures any way she wants, or make more than 1 move in a turn? Will a neural network adapt to that, as it adapted to an absurd move made by Lee Sedol in 2015?
","['neural-networks', 'reinforcement-learning', 'game-ai', 'chess', 'alphazero']",
"Reinforcement learning simple problem: agent not learning, wrong action","
I am pretty new to RL and I am trying to code a simple RL task with pytorch.
The goal/task is the following:
The initial state is $t_o$ and the agent takes an action $\Delta_t$: $t_o +\Delta_t = t_1$.
If $t_1$ equals 450 or 475 then it gets a reward, else he does not get a reward.
I am training the agent with DQN algorithm on a NN ( with: 2 Linear layes: fist layer n_in=1 n_out 128 and second layer n_in=128 and n_out=5):

observation space($t_i$) is 700 --> $t_i \in [0,700[$
action space ($\Delta_t$) is 5 --> ($\Delta_t \in [-50,-25,0,25,50]$)


epsilon_start=0.9#e-greedy threshold start value
epsilon_end=0.01#e-greedy threshold end value
epsilon_decay=200#e-greedy threshold decay learning_rate=0.001# NN
optimizer learning rate batch_size=64#Q-learning batch size

Unfortunately it does not seem to converge to the values $t_i=$ 450 or 475. I doesn't seem to care about getting a reward.
How can I improve my code so that the agent learns what I am trying to teach him?
I put my code below in case the explanations were not clear enough:

import gym
from gym import spaces

class RL_env(gym.Env):
    metadata = {'render.modes': ['human']}

    
    def __init__(self):
        super(RL_env, self).__init__()
        
        n_actions_delta = 1 #delta_t
        self.action_space = spaces.Discrete(5)
        
        n_observations = 1 #time
    
        self.observation_space = spaces.Discrete(700)
       
        #initial time
        self.time = 0
        
        self.done = 0
        self.reward = 0

    def reset(self):
        self.reward = 0
        self.done = False
        return self.reward
       
    def step(self,delta_t):
        print('self time',self.time)
        d_t = np.arange(-50,70,25)
        
        self.time = (self.time + d_t[delta_t])%700
        print('delta time',d_t[delta_t],'-->','self time',self.time)
        
       
        
        if self.time == 475 or self.time == 450:
            self.reward = 1
            
            
        else:
            self.reward += 0
        
            
        info = {}
        print('total reward',self.reward)
        print('\n')
        return self.time,self.reward, self.done, info
    

    
    
    def render(self, mode='human', close=False):
        print()

import gym
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
from torch.distributions import Categorical
dtype = torch.float
device = torch.device(""cpu"")
import random
import math
import sys
if not sys.warnoptions:#igrnore warnings
    import warnings
    warnings.simplefilter(""ignore"")

#hyper parameters
epsilon_start=0.9
#e-greedy threshold start value
epsilon_end=0.01#e-greedy threshold end value
epsilon_decay=200#e-greedy threshold decay
learning_rate=0.001# NN optimizer learning rate
batch_size=64#Q-learning batch size 

env = RL_env()


#use replay memory (-> to stabilize and improve our algorithm)for training: store transitions observed by agent,
#then reuse this data later
#sample from it randomly (batch built by transitions are decorrelated)
class ReplayMemory:#allowes the agent to learn from earlier memories (speed up learning and break undesirable temporal correlations)
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []
    def push(self, transition):#saves transition
        self.memory.append(transition)
        if len(self.memory)>self.capacity:#if length of memory arra is larger than capacity (fixed)
            del self.memory[0]#remove 0th element

    def sample(self, batch_number):#samples randomly a transition to build batch
        return random.sample(self.memory, batch_number)

    def __len__(self):
        return len(self.memory)
    
#Dqn NN (we want to maximize the discounted, cumulative reward)
#idea of Q-learning: we want to approximate with NN maximal Q-function (gives max return of action in given state)
#training update rule: use the fact that every Q-function for some policy obeys the Bellman equation
#difference between the two sides of the equality is known as the temporal difference error (we want to min -> Huber loss)
#calculate over batch of transitions sampled from the replay memory
class DqnNet(nn.Module):
    def __init__(self):
        super(DqnNet, self).__init__()
        
        state_space = 1
        action_space = env.action_space.n
        num_hid = 128
        self.fc1 = nn.Linear(state_space, num_hid)
        self.fc2 = nn.Linear(num_hid, action_space)
        self.gamma=0.5 #Q-learning discount factor (ensures that reward sum converges, 
                        #makes actions from far future less important)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.sigmoid(self.fc2(x))
        return x

#select action accordingly to epsilon greedy policy
#sometimes we use model for choosing action, other times sample uniformly 
#probability of choosing a random action will start at epsilon_start and will decay (epsilon_decay) exponentially
#towards epsilon_end
steps_done=0
def predict_action(state):
    global steps_done
    sample=random.random()#random number
    eps_threshold=epsilon_end+(epsilon_start-epsilon_end)*math.exp(-1.*steps_done/epsilon_decay)
    steps_done += 1
    if sample>eps_threshold:  
        x  = eps_threshold,model(Variable(state,).type(torch.FloatTensor)).data.max(0)[1].view(1, 1)
        return x#chose action from model
    
    else:
        x = eps_threshold,torch.tensor([[random.randrange(env.action_space.n)]])
        return x#choose random action uniformly

#wtih the update_policy function we perform a single step of the optimization
#first sample a batch, concatenate all the tensors into a single one, compute Q-value and max Q-value, 
#and combine them into loss
def update_policy():
    if len(memory)<batch_size:#we want to sample a batch of size 64
        return
    transitions = memory.sample(batch_size)#take random transition batch from experience replay memory
    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)#convert batch-array of Transitions
                                                                              #to Transition of batch-arrays   
    #-->zip(*) takes iterables as arguments and return iterator
    
    batch_state = Variable(torch.cat(batch_state))#concatenate given sequence tensors in the given dimension
    batch_state = batch_state.resize(batch_size,1)
    batch_action = Variable(torch.cat(batch_action))
    batch_next_state = Variable(torch.cat(batch_next_state))
    batch_next_state = batch_next_state.reshape(batch_size,1)
    batch_reward = Variable(torch.cat(batch_reward))
    
    #print('model batch state',model(Variable(batch_state[0])))
    current_q_values = model(batch_state).gather(1, batch_action)#current Q-values estimated for all actions,
                                                                 #compute Q, then select the columns of actions taken,
                                                                 #these are the actions which would've been taken
                                                                 #for each batch state according to policy_net
    max_next_q_values = model(batch_next_state).detach().max(1)[0]#predicted Q-values for non-final-next-states
                                                                  #(-> gives max Q)
    expected_q_values = batch_reward + (model.gamma * max_next_q_values)

    #loss is measured from error between current and newly expected Q values (Huber Loss)
    loss = F.smooth_l1_loss(current_q_values, expected_q_values)

    # backpropagation of loss to NN --> optimize model
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss, np.sum(expected_q_values.numpy())
    

def train(episodes):
    scores = []
    Losses = []
    Bellman = []
    Epsilon = []
    Times = []
    Deltas = []
    
    
    
    for episode in range(episodes):  
        state=env.reset()#reset environment
        print('\n')
        print('episode',episode)
           
        epsilon_action = predict_action(torch.FloatTensor([state]))
        
        action = epsilon_action[1] #after each time step predict action

        next_state, reward, done,info = env.step(action.item())#step through environment using chosen action
        
        epsilon = epsilon_action[0]
        Epsilon.append(epsilon)
        print(reward,'reward')
              
        state=next_state
        Times.append(state)
        scores.append(reward)            

    
        memory.push((torch.FloatTensor([state]),action,torch.FloatTensor([next_state]),
                         torch.FloatTensor([reward])))#action is already a tensor
        up = update_policy()#update_policy()#update policy
            
        if up != None:
            Losses.append(Variable(up[0]))
            print('loss',Variable(up[0]))
            Bellman.append(up[1])

        #calculate score to determine when the environment has been solved
        mean_score=np.mean(scores[-50:])#mean of score of last 50 episodes
        #every 50th episode print score
        if episode%50 == 0:
            print('Episode {}\tScore: {}\tAverage score(last 50 episodes): {:.2f}'.format(episode,scores[-50:],mean_score))

    
    #print('Losses',Losses)
    Losses = torch.stack(Losses).numpy()
    #print('Losses',Losses)
    plt.plot(np.arange(len(Losses)),Losses)
    plt.xlabel('Training iterations')
    plt.ylabel('Loss')
    plt.show()
    
    Bellman = np.array(Bellman)
    #print('Bellman',Bellman,'\n')
    plt.plot(np.arange(len(Bellman)),Bellman)
    plt.xlabel('Training iterations')
    plt.ylabel('Bellman target')
    plt.show()
    
    #print('scores',scores)
    plt.plot(np.arange(len(scores)),scores)
    plt.xlabel('Training iterations')
    plt.ylabel('Reward')
    plt.show()
    
    #print('epsilon',Epsilon)
    plt.plot(np.arange(len(Epsilon)),Epsilon)
    plt.xlabel('Training iterations')
    plt.ylabel('Epsilon')
    plt.show()
    
    print('Times',Times[-25:])
    print('Deltas',Deltas[-25:])
    
    Times = np.array(Times)
    print('Times',Times)
    #plt.figure(figsize=(31,20))
    plt.figure(figsize=(9,7))
    plt.plot(np.arange(len(Times)),(np.array(Times)))
    plt.xlabel('Training iterations')
    plt.ylabel('t')
    plt.show()
     
    Times_1 = np.array(Times[-300:])
    print('t',Times)
    plt.figure(figsize=(9,7))
    plt.plot(np.arange(len(Times_1)),(np.array(Times_1)))
    plt.xlabel('Last 300 Training iterations')
    plt.ylabel('t')
    plt.ylim(0,1000)
    plt.show()
    
model = DqnNet()#policy         
memory = ReplayMemory(20000)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

train(10000)

","['reinforcement-learning', 'q-learning', 'dqn']",
What is the advantage of using cross entropy loss & softmax?,"
I am trying to do the standard MNIST dataset image recognition test with a standard feed forward NN, but my network failed pretty badly. Now I have debugged it quite a lot and found & fixed some errors, but I had a few more ideas. For one, I am using the sigmoid activation function and MSE as an error function, but the internet suggests that I should rather use softmax for the output layer, and cross entropy loss as an error function. Now I get that softmax is a nice activation function for this task, because you can treat the output as a propability vector. But, while being a nice thing to have, that's more of a convinience thing, isn't it? Easier to visualize?
But when I looked at what the derivative of softmax & CEL combined is (my plan was to compute that in one step and then treat the activation function of the last layer as linear, as not to apply the softmax derivative again), I found:
$\frac{δE}{δi}$ = $t$ − $o$
(With $i$ being the input of the last layer, $t$ the one hot target vector and $o$ the prediction vector).
That is the same as the MSE derivative. So what benefits does softmax + CEL actually have when propagating, if the gradients produced by them are exactly the same?
","['neural-networks', 'gradient-descent', 'cross-entropy', 'mean-squared-error', 'softmax']",
How does back-propagation through time work for optimizing the weights of a bidirectional RNN?,"
I am aware that back-propagation through time is used for training the recurrent neural network. But I am not able to understand how this happens for the bi-directional versions of the recurrent neural networks?
So, I was hoping if anyone help me with:

Understanding with an example the training of bi-directional recurrent neural networks using back-propagation through time? (I tried following the original paper https://ieeexplore.ieee.org/document/650093, but it was kind of confusing for me when they perform the backward pass for training)

","['machine-learning', 'deep-learning', 'training', 'recurrent-neural-networks', 'backpropagation']",
Measuring novel configuration of points,"
I am trying to implement Novelty search; I understand why it can work better than the standard Genetic Algorithm based solution which just rewards according to the objective.
I am working on a problem which requires to generate a fixed number of points in a 2d box centered at the origin.
In this problem, how can I identify which is a novel configuration of points?
Note: I have thought of one way of doing this: We call the mean of one configuration of points to be the mean of all points in that configuration (let's say this tuple is $(m_x, m_y)$, we store the mean of all configurations generated till now, now for a new configuration it's novelty can be defined as the distance of the mean of this new configuration with $(m_x, m_y)$.
But I think it will not work greatly as some very different configuration of points can also have the same mean.
","['genetic-algorithms', 'optimization', 'novelty-search']",
Which hyperparameters in neural network are accesible to users adjustment,"
I am new to Neural Networks and my questions are still very basic.
I know that most of neural networks allow and even ask user to chose hyper-parameters like:

amount of hidden layers
amount of neurons in each layer
amount of inputs and outputs
batches and epochs steps and some stuff related to back-propagation and gradient descent

But as I keep reading and youtubing, I understand that there are another important ""mini-parameters"" such as:

activation functions type

activation functions fine-tuning (for example shift and slope of sigmoid)


whether there is an activation funciton in the output

range of weights (are they from zero to one or from -1 to 1  or -100 to +100 or any other range)

are the weights normally distributed or they just random


etc...
Actually the question is:
Part a:
Do I understand right that most of  neural networks do not allow to change those ""mini-parameters"", as long as you are using ""readymade"" solutions?
In other words if I want to have an access to those ""mini-parameters"" I need to program the whole neural network by myself or there are  ""semi-finished products""
Part b:(edited)
For someone who uses neural network as an everyday routine tool to solve problems(Like data scientist), How common and how often do those people deal with fine tuning things which I refer to as ""mini-parameters""? Or those parameters are usually adjusted by a neural network developers who create the frameworks like pytorch, tensorflow etc?
Thank you very much
","['activation-functions', 'hyper-parameters', 'weights', 'pretrained-models']","In general, many of the parameters you mentioned are called hyperparameters. All hyperparameters are user-adjusted (or user-programmed) in training phase. Some hyperparameters are:To answer your (a) part of your question, there are obsiously many frameworks and libraries, for example in python; TensorFlow, pytorch and so on. You might never create a net from the very beginning; maybe only in order to understand the forward and backpropagation algorithms. When we call from scatch networks, we mean that these networks are trained from scratch, with learnable weights and chosen hyperparameters; with no transfer learning.To answer your (b) part of your question, I can understand from it that you mean when a net is good enough. Dependently of your data, of course, a neural network is good enough, when it is trained adequately on them. That is, you should be aware of overfitting, underfitting, and in general of the model you are trying to train with all its parameters and hyperparameters.Since, you are at the very beginning with Machine Learning, I propose you read some books, in order to get everything needed, in terms of Mathematical and Computer Science aspects."
What is the return-to-go in reinforcement learning?,"
In reinforcement learning, the return is defined as some function of the rewards. For example, you can have the discounted return, where you multiply the rewards received at later time steps by increasingly smaller numbers, so that the rewards closer to the current time step have a higher weight. You can also have $n$-step returns or $\lambda$-returns.
Recently, I have come across the concept of return-to-go in a few research papers, such as Prioritized Experience Replay (appendix A. Prioritization Variants, p. 12) or Being Optimistic to Be Conservative: Quickly Learning a CVaR Policy (section Theoretical Analysis, p. 3).
What exactly is the return-to-go? How is it mathematically defined? In which situations do we need to care about it? The name suggests that this is the return starting from a certain time step $t$, but wouldn't this be the same thing as the return (which is defined starting from a certain time step $t$ and often denoted as $G_t$ for that same reason)?
There is also the concept of reward-to-go. For example, the reward-to-go is analyzed in the paper Learning the Variance of the Reward-To-Go, which states that the expected reward-to-go is the value function, which seems to be consistent with this explanation of the reward-to-go, where the reward-to-go is defined as
$$\hat{R}_{t} \doteq \sum_{t^{\prime}=t}^{T} R\left(s_{t^{\prime}}, a_{t^{\prime}}, s_{t^{\prime}+1}\right)$$
We also had a few questions that involve the reward-to-go: for example, this or this. How is the return-to-go related to the reward-to-go? Are they the same thing? For example, in this paper, the return-to-go seems to be used as a synonym for reward-to-go (as used in this article), i.e. they call $R(t)$ the ""return to-go"" (e.g. on page 2), which should be the return starting from time step $t$, which should actually be the reward-to-go.
","['reinforcement-learning', 'comparison', 'definitions', 'reward-to-go']",
Difficulty in agent's learning with increasing dimensions of continuous actions,"
I have been working on some RL project, where the policy is controlling the robot using its joint angles.Throughout the project I have noticed some phenomenon, which caught my attention. I have decided to create a very simplified script to investigate the problem. There it goes:
The environment
There is a robot, with two rotational joints, so 2 degrees of freedom. This means its continuous action space (joint rotation angle) has a dimensionality of 2. Let's denote this action vector by a. I vary the maximum joint rotation angle per step from 11 to 1 degrees and make sure that the environment is allowed to do a reasonable amounts of steps before the episode is forced to terminate on time-out.
Our goal is to move the robot by getting its current joint configuration c closer to the goal joint angle configuration g (also two dimensional input vector).
Hence, the reward I have chosen is e^(-L2_distance(c, g)).
The smaller the L2_distance, the exponentially higher the reward, so I am sure that the robot is properly incentivised to reach the goal quickly.
Reward function (y-axis: reward, x-axis: L2 distance):

So the pseudocode for every step goes like:

move the joints by predicted joint angle delta

collect the reward

if time-out or joint deviates too much into some unrealistic configuration: terminate.


Very simple environment, not to have too many moving parts in our problem.
RL algorithm
I use Catalyst framework to train my agent in the actor-critic setting using TD3 algorithm. By using a tested framework, which I am quite familiar with, I am quite sure that there are no implementational bugs.
The policy is goal-driven so the actor consumes the concatenated current and goal joint configuration a= policy([c,g])
The big question
When the robot has only two degrees of freedom, the training quickly converges and the robots learns to solve the task with high accuracy (final L2 distance smaller than 0.01).
Performance of the converged 2D agent. y-axis: joint angle value, x-axis: no of episodes. Crosses denote the desired goal state of the robot.:

However, if the problem gets more complicated - I increase the joint dimensions to 4D or 6D, the robot initially learns to approach the target, but it never ""fine-tunes"" its movement. Some joints tend to oscillate around the end-point, some of them tend to overshoot.
I have been experimenting with different ideas: making the network wider and deeper, changing the action step. I have not tried optimizer scheduling yet. No matter how many samples the agent receives or how long it trains, it never learns to approach targets with required degree of accuracy (L2_distance smaller than 0.05).
Performance of the converged 4D agent. y-axis: joint angle value, x-axis: no of episodes. Crosses denote the desired goal state of the robot.:

Training curve for 2D agent (red) and 4D agent (orange). 2D agent quickly minimises the L2 distance to something smaller than 0.05, while the 4D agent struggles to go below 0.1.:

Literature research
I have looked into papers which describe motion planning in joint space using TD3 algorithm.
There are not many differences from my approach:
Link 1
Link 2
Their problem is much more difficult because the policy needs to also learn the model of the obstacles in joint space, not only the notion of the goal. The only thing which is special about them is that they use quite wide and shallow networks. But this is the only peculiar thing.
I am really interested, what do you guys would advise me to do, so that the robot can reach high accuracy in higher joint configuration dimensions? What am I missing here?!
Thanks for any help in that matter!
","['reinforcement-learning', 'python', 'robotics']",
What is the time complexity for training a gated recurrent unit (GRU) neural network using back-propagation through time?,"
Let us assume we have a GRU network containing $H$ layers to process a training dataset with $K$ tuples, $I$ features, and $H_i$ nodes in each layer.
I have a pretty basic idea how the complexity of algorithms are calculated, however, with the presence of multiple factors that affect the performance of a GRU network including the number of layers, the amount of training data (which needs to be large), number of units in each layer, epochs and maybe regularization techniques, training with back-propagation through time,  I am messed up. I have found an intriguing answer for neural networks complexity out here What is the time complexity for training a neural network using back-propagation?, but that was not enough to clear my doubt.
So, what is the time complexity of the algorithm, which uses back-propagation through time, to train GRU networks?
","['backpropagation', 'gradient-descent', 'time-complexity', 'gated-recurrent-unit', 'bptt']",
Is the state transition matrix known to the agents in a Markov decision processes?,"
The question is more or less in the title.
A Markov decision process consists of a state space, a set of actions, the transition probabilities and the reward function. If I now take an agent's point of view, does this agent ""know"" the transition probabilities, or is the only thing that he knows the state he ended up in and the reward he received when he took an action?
","['reinforcement-learning', 'markov-decision-process', 'model-based-methods']","In reinforcement learning (RL), there are some agents that need to know the state transition probabilities, and other agents that do not need to know. In addition, some agents may need to be able to sample the results of taking an action somehow, but do not strictly need to have access to the probability matrix. This might be the case if the agent is allowed to backtrack for instance, or to query some other systems that simulates the target environment.Any agent that needs to have access to the state transition matrix, or look-ahead samples of the environment is called model-based. The model in this case can either be a distribution model i.e. the state transition matrix, or it can be a sampling model that simulates the outcome from a given state/action combination.The state transition function $p(r, s'|s, a)$ which returns the probability of observing reward $r$ and next state $s'$ given the start state $s$ and action $a$, is another way to express the distribution model. It often maps simply to the state transition matrix, but can be a more complete description of the model.One example model-based approach is Value Iteration, and that requires access to the full distribution model in order to process value update steps. Also, any reinforcement learning that involves planning must use some kind of model. MCTS, as used in AlphaGo, uses a sampling model for instance.Many RL approaches are model-free. They do not require access to a model. They work by sampling from the environment, and over time learn the impact on expected results due to behaviour of the unknown state transition function. Example methods that do this are Monte Carlo Control, SARSA, Q learning, REINFORCE.It is possible to combine model-free and model-based methods by using observations to build an approximate model of the environment, and using it in some form of planning. Dyna-Q is an approach which does this by simply remembering past transitions and re-using them in the background to refine its value estimates. Arguably, the experience replay table in DQN is a similar form of background planning (the algorithm is essentially the same). However, more sophisticated model-learning and reuse is not generally as successful, and is not seen commonly in practice. See How can we estimate the transition model and reward function?In general, model-based methods on the same environment can learn faster than model-free methods, since they start with more information that they do not need to learn. However, it is quite common need to learn without having an accurate model available, so there is lots of interest in model-free learning. Sometimes an accurate model is possible in theory, but it would be more work to calculate predictions from the model than to work statistically from the observations."
What is the difference between text-based image retrieval and natural language object retrieval?,"
I'm working on creating a model that locates the object in the scene (2D image or 3D scene) using a natural language query. I came across this paper on natural language object retrieval, which mentions that this task is different from text-based image retrieval, in the sense that natural language object retrieval requires an understanding of objects in the image, spatial configurations, etc. I am not able to see the difference between these two approaches. Could you please explain it with an example?
","['machine-learning', 'deep-learning', 'natural-language-processing', 'comparison', 'object-detection']",
When do the ensemble methods beat neural networks?,"
In many applications and domains, computer vision, natural language processing, image segmentation, and many other tasks, neural networks (with a certain architecture) are considered to be by far the most powerful machine learning models.
Nevertheless, algorithms, based on different approaches, such as ensemble models, like random forests and gradient boosting, are not completely abandoned, and actively developed and maintained by some people.
Do I correctly understand that the neural networks, despite being very flexible and universal approximators, for a certain kind of tasks, regardless of the choice of the architecture, are not the optimal models?
For the tasks in computer vision, the core feature, which makes CNNs superior, is the translational invariance and the encoded ability to capture the proximity properties of an image or some sequential data. And the more recent transformer models have the ability to choose which of the neighboring data properties is more important for its output.
But let's say I have a dataset, without a certain structure and patterns, some number of numerical columns, a lot of categorical columns, and in the feature space (for classification task) the classes are separated by some nonlinear hypersurface, would the ensemble models be the optimal choice in terms of performance and computational time?
In this case, I do not see a way to exploit CNNs or attention-based neural networks. The only thing that comes to my head, in this case, is the ordinary MLP. It seems that, on the one hand, it would take significantly more time to train the weights than the trees from the ensemble. On the other hand, both kinds of models work without putting prior knowledge to data and assumptions on its structure. So, given enough amount of time, it should give a comparable quality.
Or can there be some reasoning that neural network is sometimes bound to give rather a poor quality?
","['convolutional-neural-networks', 'transformer', 'ensemble-learning', 'random-forests', 'gradient-boosting']",
How do AI researchers imagine higher dimensions?,"
We can visualize single, two, and three dimensions using websites or imagination.
In the context of AI and, in particular, machine learning, AI researchers often have to deal with multi-dimensional random vectors.
Suppose if we consider a dataset of human faces, each image is a vector in higher dimensional space and needs to understand measures on them.
How do they imagine them?
I can only imagine with 3D and then approximating with higher dimensions. Is there any way to visualize in higher dimensions for research?
","['research', 'academia', 'dimensionality-reduction', 'data-visualization']",
When are multiple hidden layers necessary?,"
I know that my question probably seems like being asked many times, but Ill try
to be more speciffic:
Limitations to my question:

I am NOT asking about convolutional neural networks, so please, try not to mention this as an example or as an answer as long as it is possible. (maybe only in question number 3)

My question is NOT about classification using neural networks

I am asking about a ""simple"" neural network designed to solve the regression type of problem. Let's say it has 2 inputs and 1 output.


Preambula:
As far as I understood, from the universal approximation theorem, in such a case, even if the model is nonlinear, only one hidden layer can perfectly fit a nonlinear model, as shown here
http://neuralnetworksanddeeplearning.com/chap4.html.
Question 1
In this specific case, is there any added value in using extra layers?
(maybe the model will be more precise, or faster training?)
Question 2
Suppose in 1st question the answer was there is no added value. In such a case will the added value appear if I enlarge inputs from two inputs as described above, to some larger number?
Question 3
Suppose in 2nd question the answer was there is no added value. I am still trying to pinpoint the situation where it STARTS making sense in adding more layers AND where it makes NO sense at all using one layer.
","['neural-networks', 'regression', 'hidden-layers', 'non-linear-regression', 'universal-approximation-theorems']","A very wide but shallow neural network is going to be harder to train.
You can check that with the playground of tensorflow or with the MPG example in Google Colab.
The relationship between architecture and learning capabilities is not fully understood, but, empirically, thats what you see.
But making the network too deep creates more problems:It is for that reason that humans and neural networks are the one deciding a good architecture."
How to use residual learning applied to fully connected networks?,"
Is there any reason why skip connections would not provide the same benefits to fully connected layers as it does for convolutional?
I've read the ResNet paper and it says that the applications should extend to ""non-vision"" problems, so I decided to give it a try for a tabular data project I'm working on.
Try 1: My first try was to only skip connections when the input to a block matched the output size of the block (the block has depth - 1 number of layers with in_dim nodes plus a layer with out_dim nodes :
class ResBlock(nn.Module):
    def __init__(self, depth, in_dim, out_dim, act='relu', act_first=True):
        super().__init__()
        self.residual = nn.Identity()
        self.block = block(depth, in_dim, out_dim, act)
        self.ada_pool = nn.AdaptiveAvgPool1d(out_dim)
        self.activate = get_act(act)
        self.apply_shortcut = (in_dim == out_dim)
        
    def forward(self, x):
        if self.apply_shortcut:
            residual = self.residual(x)
     
            x = self.block(x)
            return self.activate(x + residual)
        return self.activate(self.block(x))

The accompanying loss curve:

Try 2: I thought to myself ""Great, it's doing something!"", so then I decided to reset and go for 30 epochs from scratch. I don't have the image saved, but this training only made it 5 epochs and then the training and validation loss curves exploded by several orders of magnitude:

Try 3: Next, I decided to try to implement the paper's idea of reducing the input size to match the output when they don't match: y = F(x, {Wi}) + Mx. I chose average pooling in place of matrix M to accomplish this, and my loss curve became.

The only difference in my code is that I added average pooling so I could use shortcut connections when input and output sizes are different:
class ResBlock(nn.Module):
    def __init__(self, depth, in_dim, out_dim, act='relu', act_first=True):
        super().__init__()
        self.residual = nn.Identity()
        self.block = block(depth, in_dim, out_dim, act)
        # squeeze/pad input  to output size
        self.ada_pool = nn.AdaptiveAvgPool1d(out_dim) 
        self.activate = get_act(act)
        self.apply_pool = (in_dim != out_dim)
        
    def forward(self, x):
        # if in and out dims are different apply the padding/squeezing:
        if self.apply_pool:
            residual = self.ada_pool(self.residual(x).unsqueeze(0)).squeeze(0)
        else: residual = self.residual(x)
            
        x = self.block(x)
        return self.activate(x + residual)
)

Is there a conceptual error in my application of residual learning? A bug in my code? Or is resididual learning just not applicable to this kind of data/network?
","['feedforward-neural-networks', 'residual-networks', 'binary-classification']",
How to train the NN of simple agents given a reward system?,"
I'm not an expert in AI or NN, I gathered most of the information I have from the internet, and I'm looking for advice and guidance.
I'm trying to design a NN that is going to be used by all the agents of my simulation (each agent will have its own matrix of weights). This is what I plan to have:

The NN will have 1 input layer and 1 output layer (no hidden layers).
The number of inputs will always be greater than the number of outputs.
The outputs represent the probability of an action being taken by the agent (the output node with the highest value will identify the action that will be taken). Which means there are as many output nodes are there are actions.

When an agent takes an action it receives a reward: a number that represents how well the agent performed. This happens ""online"" that is, the agent is trained on the fly.
What I would like to know if how to best train the NN: that is, how to update the weights of my matrix to maximise the rewards long term.
From the research I made it seems this is close to the concept of Reinforcement Learning, but even if it was, it's not clear to me how to apply it to such a simple NN shape.
","['neural-networks', 'reinforcement-learning', 'training', 'intelligent-agent', 'multi-agent-systems']",
Loss function for better class separability in multi class classification,"
So I am trying to enforce better separability in my deep learning model and was wondering what I can use besides cross entropy loss to do that? Could maybe using logarithm with different basis in cross entropy (i.e. using lower basis of logarithm than $e$ to gain steeper losses on small values, or bigger basis of logarithm to enforce plateaued losses). What would you suggest on doing?
","['classification', 'objective-functions', 'loss', 'cross-entropy']",
What exactly is an interpretable machine learning model?,"
From this page in Interpretable-ml book and this article on Analytics Vidhya, it means to know what has happened inside an ML model to arrive at the result/prediction/conclusion.
In linear regression, new data will be multiplied with weights and bias will be added to make a prediction.
And in boosted tree models, it is possible to plot all the decisions as trees that results in a prediction.
And in feed-forward neural networks, we will have weights and biases just like linear regression and we just multiply weights and add bias at each layer, limiting values to some extent using some kind of activation function at every layer, arriving finally at prediction.
In CNNs, it is possible to see what happens to the input after having passed through a CNN block and what features are extracted after pooling (ref: what does a CNN see?).
Like I stated above, one can easily know what happens inside an ML model to make a prediction or conclusion. And I am unclear as to what makes them un-interpretable!. So, what exactly makes an algorithm or it's results un-interpretable or why are these called black box models? Or am I missing something?
","['machine-learning', 'deep-learning', 'deep-neural-networks', 'explainable-ai', 'black-box']","In a simple linear model of the form $y = \beta_0 + \beta_1 x $ we can see that increasing $x$ by a unit will increase the prediction on $y$ by $\beta_1$. Here we can completely determine what the effect on the models prediction will be by increasing $x$. With more complex models such as neural networks it is much more difficult to tell due to all the calculations that a single data point is involved in. For instance, in a CNN as you mentioned, if I changed the value of a pixel in an image we were passing through the CNN you wouldn't really be able to tell me exactly the effect this would have on the prediction like you can with the linear model."
"What are some alternatives to ""Papers with Code""?","
There are lots of research papers available that are worth reading. We can read papers easily, but the associated code (not necessarily the official one developed by the authors of the paper) is often not available.
Papers with Code (and the associated Github repo) already lists many research papers and often there is a link to the associated Github repo with the code, but sometimes the code is missing. So, are there alternatives to Papers with Code (for such cases)?
","['papers', 'resource-request', 'implementation']",
Would performance of atomic models matter in ensemble methods?,"
Suppose I have two fitted ensemble models $F_1 := (f_1, f_2, f_3, \cdots f_n)$ and $G_1 := (g_1, g_2, g_3, \cdots g_n)$.
And they were using the same ensemble methods (boosting or bagging).
And I am using some measurement for model performance $M: f_i \to \mathbb{R}^+$, higher the better.
And I know beforehand $M(f_i) \gt M(g_i), \forall i \in [1,n]$, can I conclude $M(F) \gt M(G) $ ?
","['machine-learning', 'performance', 'ensemble-learning']",
Is training a CNN object detector on an image containing multiple targets that are not all annotated will teach it to miss targets?,"
I want to train a convolutional neural network for object detection (say YOLO) to detect faces. Consider this image:

In this training image, I have many people, but only 2 of them are annotated. Is having this kind of images (where target classes are not all annotated) will train the network to ignore positives?
If yes, are there any techniques to solve the issue apart from annotating the data (I don't have enough resources to do that).
","['computer-vision', 'training', 'object-detection', 'supervised-learning', 'semi-supervised-learning']",
"Why is the fraction of time spent in state $s$, $\mu(s)$, not in the update rule of the parameters?","
I am reading ""Reinforcement Learning: An Introduction (2nd edition)"" authored by Sutton and Barto. In Section 9, On-policy prediction with approximation, it first gives the mean squared value error objective function in (9.1):
$\bar{VE}(\boldsymbol{w}) = \sum_{s \in S} \mu(s)[v_{\pi}(s) - \hat{v}(s,\boldsymbol{w})]^2$. (9.1)
$\boldsymbol{w}$ is a vector of the parameterized function $\hat{v}(s,\boldsymbol{w})$ that approximates the value function $v_{\pi}(s)$. $\mu(s)$ is the fraction of time spent in $s$, which measures the ""importance"" of state $s$ in $\bar{VE}(\boldsymbol{w})$.
In (9.4), it states an update rule of $\boldsymbol{w}$ by gradient descent:
$\boldsymbol{w}_{t+1} = \boldsymbol{w} -\frac{1}{2}\alpha \nabla[v_{\pi}(S_t) - \hat{v}(S_t,\boldsymbol{w})]^2$. (9.4)
I have two questions regarding (9.4).

Why $\mu(s)$ is not in (9.4)?
Why is it the ""minus"" instead of ""+"" in (9.4)? In other words, why is it $\boldsymbol{w} -\frac{1}{2}\alpha \nabla[v_{\pi}(S_t) - \hat{v}(S_t,\boldsymbol{w})]^2$ instead of $\boldsymbol{w} +\frac{1}{2}\alpha \nabla[v_{\pi}(S_t) - \hat{v}(S_t,\boldsymbol{w})]^2$?

","['reinforcement-learning', 'gradient-descent', 'function-approximation', 'sutton-barto']","$\mu(s)$ is not in equation (9.4) because we are assuming that the examples by which we update our parameter $w$, i.e. the frequency of which we will observe the states during online training, is the same. That is, it is a constant with respect to $w$ and since we are differentiating it can be somewhat disregarded as a constant of proportionality -- it essentially can be 'absorbed' by $\alpha$.The minus is there because we are performing gradient descent. For more information on this, see e.g. the wikipedia page"
How do we derive the expression for average reward setting in continuing tasks?,"
In the average reward setting we have:
$$r(\pi)\doteq \lim_{h\rightarrow\infty}\frac{1}{h}\sum_{t=1}^{h}\mathbb{E}[R_{t}|S_0,A_{0:t-1}\sim\pi]$$
$$r(\pi)\doteq \lim_{t\rightarrow\infty}\mathbb{E}[R_{t}|S_0,A_{0:t-1}\sim\pi]$$
How is the second equation derived from the first?
","['reinforcement-learning', 'rewards', 'function-approximation', 'sutton-barto']","We assume that our MDP is ergodic. Loosely speaking, this means that wherever the MDP starts (i.e. no matter which state we start in) or any actions the agent takes early on can only have a limited effect on the MDP and in the limit (as $t \rightarrow \infty$) the expectation of being in a given state depends only on the policy $\pi$ and the transition dynamics of the MDP.This means that, eventually, $\mathbb{E}[R_t] = \mathbb{E}[R_{t+1}]$ for some large $t$. Therefore, as we take the average of our expected values of the rewards received for an infinitely long period of time, this will have converged due to what I just mentioned of $\mathbb{E}[R_t] = \mathbb{E}[R_{t+1}]$. To see why the two are equal, recall that the reward received is dependent on the current state and the action taken -- to better emphasise this I will briefly denote the reward at time step $t+1$ as $R(S_t, A_t)$. If we are in the steady state distribution, that is, the state distribution is now fixed, and our actions are still taken according to our policy, then the expected value of $R(S_t, A_t)$ will be the same for all future $t$ since neither the policy nor the state distribution are changing (recall that the average rewards are a way of evaluating a policy in the average-reward setting so for sure this does not change).A way to think of this is that since we know that, eventually, $\mathbb{E}[R_t]$ will equal $\mathbb{E}[R_{t+1}]$, and so if we keep have an infinite number of these, the average of them will of course converge to the same value. Imagine if I gave you the sequence 1, 2, 3, 4, 4, 4, 4, ........, 4 and asked you to take the average - if we had an infinite amount of 4's then the average would of course be 4."
When should one prefer using Total Variational Divergence over KL divergence in RL,"
In RL, both the KL divergence (DKL) and Total variational divergence (DTV) are used to measure the distance between two policies. I'm most familiar with using DKL as an early stopping metric during policy updates to ensure the new policy doesn't deviate much from the old policy.
I've seen DTV mostly being used in papers giving approaches to safe RL when placing safety constraints on action distributions. Such as in Constrained Policy Optimization and Lyapunov Approach to safe RL.
I've also seen that they are related by this formula:
$$
D_{TV} = \sqrt{0.5 D_{KL}}
$$
When you compute the $D_{KL}$ between two polices, what does that tell you about them, and how is it different from what a $D_{TV}$ between the same two policies tells you?
Based on that, are there any specific instances to prefer one over the other?
","['reinforcement-learning', 'comparison', 'probability-distribution', 'kl-divergence', 'total-variational-distance']","I did not read those two specified linked/cited papers and I am not currently familiar with the total variation distance, but I think I can answer some of your questions, given that I am reasonably familiar with the KL divergence.When you compute the $D_{KL}$ between two polices, what does that tell you about themThe KL divergence is a measure of ""distance"" (or divergence, as the name suggests) between two probability distributions (i.e. probability measures) or probability densities. In reinforcement learning, (stochastic) policies are probability distributions. For example, in the case your Markov decision process (MDP) has a discrete set of actions, then your policy can be denoted as $$\pi(a \mid s),$$which is the conditional probability distribution over all possible actions, given a specific state $s$. Hence, the KL divergence is a natural measure of how two policies are similar or different.There are 4 properties of the KL divergence that you always need to keep in mindand how is it different from what a $D_{TV}$ between the same two policies tells you?$D_{TV}$ is also a measure of the distance between two probability distributions, but it is bounded, specifically, in the range $[0, 1]$ [1]. This property may be useful in some circumstances (which ones?). In any case, the fact that it lies in the range $[0, 1]$ potentially makes its interpretation more intuitive. More precisely, if you know the maximum and minimum values that a measure can give you, you can have a better idea of the relative difference between probability distributions. For instance, imagine that you have p.d.s $q$, $p$ and $p'$. If you compute $D_{TV}(q, p)$ and $D_{TV}(q, p')$, you can have a sense (in terms of percentage) of how much $p'$ and $p$ differ with respect to $q$.The choice between $D_{TV}$ and $D_{KL}$ is probably motivated by their specific properties (and it will probably depend on a case by case basis, and I expect the authors of the research papers to motivate the usage of a specific measure/metric). However, keep in mind that there is not always a closed-form solution not even to calculate the KL divergence, so you may need to approximate it (e.g. by sampling: note that the KL divergence is defined as an expectation/integral so you can approximate it with a sampling technique). So, this (computability and/or approximability) may also be a parameter to take into account when choosing one over the other.By the way, I think that your definition of the total variational divergence is wrong, although the DTV is related to the DKL, specifically, as follows [1]\begin{align}
D_{TV} \leq \sqrt{\frac{1}{2} D_{KL}}
\end{align}So the DTV is bounded by the KL divergence. Given that the KL divergence is unbounded (e.g. it can take very big values, such as 600k, this bound should be very loose).Take a look at the paper On choosing and bounding probability metrics
 (2002, by Alison L. Gibbs and  Francis Edward Su) or this book for information about $D_{TV}$ (and other measures/metrics)."
Determining if an entity in free text is 'present' or 'absent'; what is this called in NLP?,"
I'm processing a semi-structured scientific document and trying to extract some specific concepts. I've actually made quite good progress without machine-learning so far, but I got to a block of true free text and I'm wondering whether a very narrow sense NLP/learning algorithm can help.
Specifically, there are concepts I know to be important that are discussed in this section, but I'll need some NLP to get the 'sentiment'. I thought this might 'entity sentiment' analysis, however, I'm not trying to capture the writer's emotion about a concept. It's literally whether the writer of the text thinks the entity is present, absent, or is uncertain about an entity.
Simple example. ""First, there are horns. And second, the sheer size of this enormous fossil record argues for an herbivore or omnivore. The jaws are large, but this is not a carnivore.""
And say my entities are horns (presence or absence), and type of dinosaur (herbivore, omnivore, carnivore). Desired output:
Horns (present)
Carnivore (absent)
Herbivore (possible/present) -- fine if it thinks 'present'
Omnivore (possible/present) -- fine if it thinks 'present'

What is the class of NLP analysis that takes an explicit input entity (or list of entities) and tries to assess based on context whether that entity is present or absent according to the writer of the input text? It's actually fine if this isn't a learning algorithm (maybe better). Bonus if you have suggestions for python packages that could be used in this narrow sense. I've looked casually through NLTK and spacy packages but they're both vast, wasn't obvious which class of model or functions I'd need to solve this problem.
","['natural-language-processing', 'python', 'named-entity-recognition']",
Root finding in Deep Equilibrium Models,"
In the Deep Equilibrium Model the neural network can be seen as ""infinitely deep"". Training learns a nonlinear function as usual. But there is no forward propagation of input data through layers. Instead, a root finding problem is solved when data comes in.
My question is, what is actually the function for which roots are searched, I'm struggling to see what would be unknown when data is available and parameters have been found in training?
","['optimization', 'prediction']",
Why do all states appear identical under the function approximation in the Short Corridor task?,"
This is the Short Corridor problem taken from the Sutton & Barto book. Here it's written:

The problem is difficult because all the states appear identical under the function approximation

But this doesn't make much sense as we can always choose states as 0,1,2 and corresponding feature vectors as
x(S = 0,right) = [1  0  0  0  0  0]
x(S = 0 , left)  = [0  1  0  0  0  0]
x(S = 1,right) = [0  0  1  0  0  0]
x(S = 1 , left) = [0  0  0  1  0  0]
x(S = 2,right) = [0  0  0  0  1  0]
x(S = 2 , left) = [0  0  0  0  0  1]\
So why is it written that all the states appear identical under the function approximation?

","['reinforcement-learning', 'environment', 'function-approximation', 'sutton-barto']","You can choose those states, but is the agent aware of the state it is in? From the text, it seems that the agent cannot distinguish between the three states. Its observation function is completely uninformative.This is why a stochastic policy is what is needed. This is common for POMDPs, whereas for regular MDPs we can always find a deterministic policy that is guaranteed to be optimal."
Why is the E step in expectation maximisation algorithm called so?,"
The E step on the EM algorithm asks us to set the value of the variational lower bound to be equal to the posterior probability of the latent variable, given the data points and parameters. Clearly we are not taking any expectations here, then why is it called the Expectation step? Am I missing something here?
","['bayesian-probability', 'expectation-maximization', 'bayesian-neural-networks']","In expectation step, firstly we calculate the posterior of latent variable $Z$ and then the $Q(θ | θ^{(t)})$ is defined as the expected value of the log likelihood of $θ$, with respect to the current conditional contribution of $Z$ given $X$ and the current estimates of $θ^{(t)}$. In maximization step, we update $θ$ using the argmax on $Q$, with respect to $θ$.$$Q(θ | θ^{(t)}) = E_{Z|X,θ^{(t)}}[logL(θ;Χ,Z)]$$To be more intuitive, think of k-means as a special case of EM, where in expectation step the $Z$ variables are defined, that is the latent variables indicating the membership in a cluster, and calculated in a hard assignment way. In maximization step the $μ$s of the clusters are updated. If you want to see the corresponding relation for $Q$ in k-means, I suggest you read the chapter 9.3.2 in C.Bishop's book: Pattern Recognition and Machine Learning."
What is the status of the capsule networks?,"
What is the status of the capsule networks?
I got an impression that capsule networks turned out not to be so useful in applications more complicated than the MNIST (at least according to this reddit discussion​).
Is this really the case? Or can they be a promising research direction (and if so, is there any specific application for which they seem the most promising)?
","['neural-networks', 'deep-learning', 'capsule-neural-network']",
How to find the optimal pokemon team,"
Pokemon is a game where 2 players each select 6 Pokemon (a team) at the beginning of the game without knowing the other player's team. Every Pokemon has one or two types. Every type is either weak, neutral or strong against every other type. This means that every 2 Pokemon matchup will either have a winner or be a tie. This also means that any team can be ranked against any other team based on the number of winning matchups they have.
I want to write a program that can find the optimal Pokemon team out of a set of 70 provided Pokemon.
A team is considered optimal if it has the greatest number of winning matchups against any other team. Basically, I want to calculate which team will have the most amount of favorable matchups if you were to battle it against every other possible team.
What algorithm would be best for doing this? It is not feasible to compute matchups for every possible team. Can I do some sort of A* search with enough pruning to make it computationally feasible?
","['ai-design', 'game-ai']","After my initial comment (where I suggest that it might not be enough info) I believe I actually came up with an idea.Start with the full set of pokemon. For every possible type, identify the count of pokemon that are strong against that type. For this, you'll end up with a List<(pokemonId, types, List<weakAgainst>)>.Minimize List<weakAgainst>.Count() and from the possible set of pokemonIds, select one at random. Without knowing anything else besides type, this pokemon is as good as any other with the same weakness count (this is the point of my original comment).From the list of weaknesses that this selected pokemon has, select a pokemon from your list that is strong against the weakness, minimizing the amount of weaknesses again. Likely more than one will match this criteria, again, select one at random.Keep repeating this pattern until you obtain the 6 in your team. This is, statistically speaking, one of the best teams that you can gather.For all the combinations that you might find here, some teams will have less weaknesses, since we're ""randomly"" walking down a tree of possibilities. This very much sounds like a minimax-prunning algorithm, where each pokemon selection (minimizing your weaknesses) can be met with potential opponents that will maximize your weak points.Simplified, put together:From this algorithm is it not obvious where the ""max"" portion is. We're minimizing our losses (weaknesses) but we're considering all possible opponent teams equally, so there is no real maximization of the opponent choices. For a set of ideas, check below.Note that this algorithm will give you a set of ""teams"" that are equally good in the sense that they'll have the same amount of minimized weaknesses and maximized strengths against other possible teams. But even if pokemon are different, the numbers will be the same, just different types.For a more complex approach, you might want to consider how prevalent some pokemon are (you might not need to optimize against a super rare mythical type, but rather the very common types available in the game), how likely is it that certain pokemon can have better / faster attacks, what is the probability of battle IVs, how frequent can a trainer switch pokemon in battle, etc. Again, I know this is not what you asked for, but for the sake of the example, this will become so complex that instead of a search algorithm, a simulation (Monte Carlo?) approach might be simpler to build teams out of statistical testing."
Why is automated theorem proving so hard?,"
The problem of automated theorem proving (ATP) seems to be very similar to playing board games (e.g. chess, go, etc.): it can also be naturally stated as a problem of a decision tree traversal. However, there is a dramatic difference in progress on those 2 tasks: board games are successfully being solved by reinforcement learning techniques nowadays (see AlphaGo and AlphaZero), but ATP is still nowhere near to automatically proving even freshman-level theorems. What does make ATP so hard compared to board games playing?
","['automated-theorem-proving', 'automated-reasoning']",
What is the best algorithm to solve the regression problem of predicting the number of languages a Wikipedia article can be translated to?,"
I'm doing a student project where I construct a model predicting the number of languages that a given Wikipedia article is translated into (for example, the article TOYOTA is translated into 93 languages). I've tried extracting basic info (article length, number of links, etc.) to create a simple regression model, but can't get the $R^2$ value above $0.25$ or so.
What's the most appropriate NLP algorithm for regression problems? Almost all examples I find online are classification problems. FYI I'm aware of the basics of NLP preprocessing (tokenization, lemmatization, bag of words, etc).
","['natural-language-processing', 'reference-request', 'regression']",
What is the difference between derivation and entailment?,"
In section 7.3 of the book Artificial Intelligence: A Modern Approach (3rd edition), it's written

An inference algorithm that derives only entailed sentences is called sound or truth-preserving.


The property of completeness is also desirable: an inference algorithm is complete if it can derive any sentence that is entailed.

However, this does not make much sense to me. I'd like someone to kindly elaborate on this.
","['comparison', 'logic', 'norvig-russell']",
Any comparison between transformer and RNN+Attention on the same dataset?,"
I am wondering what is believed to be the reason for superiority of transformer?
I see that some people believe because of the attention mechanism used, it’s able to capture much longer dependencies. However, as far as I know, you can use attention also with RNN  architectures as in the famous paper attention is introduced(here)).
I am wondering whether the only reason for the superiority of transformers is because they can be highly parallelized and trained on much more data?
Is there any experiment comparing transformers and RNN+attention trained on the exact same amount of data comparing the two?
","['natural-language-processing', 'recurrent-neural-networks', 'long-short-term-memory', 'transformer', 'attention']","If you go through the main introductory paper of the transformer (""Attention is all you need""), you can find the comparison of the model with other state-of-the-art machine translation method:For example, Deep-Att + PosUnk is a method that has utilized RNN and attention for the translation task. As you can see, the training cost for the transformer with self-attention is $2.3 \cdot 10^{19}$ (FLOPs) and $1.0 \cdot 10^{20}$ (FLOPs) for the ""Deep-Att + PosUnk"" method (the transformer is 4 times faster) on ""WMT14 English-to-French"" dataset.Please note that the BLEU is a crucial factor here (not merely training cost). Hence, you can see the BLEU‌ value of the transformer superior to the ByteNet (Neural Machine Translation in Linear Time). Although the ByteNet has not adopted the RNN, you can find the comparison of the ByteNet with other ""RNN + Attention"" methods in its original paper:Hence, by transitivity property of the BLEU score, you can find that the transformer has already outperformed other ""RNN‌ + Attention"" methods in terms of the BLEU score (please check their performance on ""WMT14"" dataset)."
Is it ok to perform transfer learning with a base model for face recognition to perform one-shot learning for object classification?,"
I am trying to create a model that is using a one-shot learning approach for a classification task. We do this because we do not have a lot of data and it also seems like a good way to learn this approach (it is going to be a university project). The task would be to classify objects, probably from drone/satellite image (of course zoomed one).
My question is, do you think it would be ok to use a model for face recognition, such as DeepFace or OpenFace, and, using transfer learning, retrain it on my classes?
","['object-recognition', 'transfer-learning', 'one-shot-learning', 'deep-face', 'open-face']",
How does dimensionality reduction occur in Self organizing Map (SOM)?,"
We have n dimension input for SOM and the output 2-D clusters. How does it happen?
","['machine-learning', 'unsupervised-learning', 'self-organizing-map']","SOM (Self-Organinizing Map) is a type of artificial neural network (ANN), introduced by the Finnish professor Teuvo Kohonen in the 1980s, that is trained using unsupervised learning to produce a low-dimensional, discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction.SOM produces a mapping from a multidimensional input space onto a lattice of clusters, i.e. neurons, in a way that preserves their topology, so that
neighboring neurons respond to “similar” input patterns.It uses three basic processes:In competition, each neuron is assigned a weight vector with the same
dimensionality d as the input space. Any given input pattern is compared to the weight vector of each neuron and the closest neuron is declared the winner.In cooperation, the activation of the winning neuron is spread to neurons in its immediate neighborhood, and as a result this allows topologically close neurons to become sensitive to similar patterns. The size of the neighborhood is initially large, but shrinks over time, where an initially large neighborhood promotes a topology-preserving mapping and smaller neighborhoods allows neurons to specialize in the latter stages of training.In adaptation, the winner neuron and its topological neighbors are adapted to make their weight vectors more similar to the input pattern that caused the activation.So, a Self-Organizing Map (SOM) is to encode a large set of input vectors $\textbf{x}$ by finding a smaller set of “representatives” or
“prototypes” or “code-book vectors” $\textbf{w}$ that provide a good approximation to the original input space. This is the basic idea of vector quantization theory, the motivation of which is dimensionality reduction or data compression. Performing a gradient descent style minimization on SOM's loss function (eg. the sum of the Euclidean distances between the input sample and each neuron) does lead to the SOM weight update algorithm, which confirms that it is generating the best possible discrete low dimensional approximation to the input space (at least assuming it does not get trapped in a local minimum of the error function).To answer your question, you should take into consideration that the Dimensionality reduction takes place in fields that deal with large numbers of observations and/or large numbers of variables. Thus, SOM helps finding good ""prototypes"", in a way that each input pattern belongs to exactly one of them. As a result, the training instances are mapped to the training ""prototypes"" and the whole training set is mapped to a new one with less instances.In addition, the ""prototypes"" neurons resulted by SOM can often be used as good centers in RBF networks or to classify patterns with the LVQ family algorithms."
What is the purpose of Decoder mask (triangular mask) in Transformer?,"
I'm trying to implement transformer model using this tutorial.  In the decoder block of the Transformer model, a mask is passed to ""pad and mask future tokens in the input received by the decoder"". This mask is added to attention weights.
import tensorflow as tf

def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask

Now my question is, how is doing this step (adding mask to the attention weights) equivalent to revealing the words to model one by one? I simply can't grasp the intuition of it's role. Most tutorials won't even mention this step like it's very obvious. Please help me understand. Thanks.
","['natural-language-processing', 'transformer', 'attention']","The Transformer model presented in this tutorial is an auto-regressive Transformer. Which means that prediction of next token only depends on it's previous tokens.So in order to predict next token, you have to make sure that only previous token are attended. (If not, this would be a cheating because model already knows whats next).So attention mask would be like this
[0, 1, 1, 1, 1]
[0, 0, 1, 1, 1]
[0, 0, 0, 1, 1]
[0, 0, 0, 0, 1]
[0, 0, 0, 0, 0]For example: If you are translating English to Spanish
Input: How are you ?
Target: < start > Como estas ? < end >
Then decoder will predict something like this
< start > (it will be given to decoder as initial token)
< start > Como
< start > Como estas
< start > Como estas ?
< start > Como estas ? < end >Now compare this step by step prediction sequences to attention mask given above, It would make sense now to you"
How to find distance between 2 points when dimensions are all of different nature?,"
I have a dataset with four features:

the x coordinate
the y coordinate
the velocity magnitude
angle

Now, I want to measure the distance between two points in the dataset, taking into account the facts that the angle dimension is toroidal, and taking into account the difference in nature of the dimensions (2 of them are distances, one of them is velocity magnitude, and the other an angle).
What kind of distance function would suit this need?
If I have to go for an $L^p$ norm, can I determine which value of $p$ would be apt by some means?
Also, if you are aware, please, let me know how such problems have been solved in various applications.
","['datasets', 'objective-functions', 'linear-algebra']",
Variance of the Gaussian policy is not decreasing while training the agent using Soft Actor-Critic method,"
I've written my own version of SAC(v2) for a problem with continuous action space. While training, the losses for the value network and both q functions steadily decrease down to 0.02-0.03. The loss for my actor/agent is negative and decreases to about -0.25 (I've read that it doesn't matter whether it is negative or not, but I'm not 100% sure). Despite that, the output variance from the Gaussian policy is way too high (make all the outcomes nearly uniformly likely) and is not decreasing during training.
Does anyone know what can be a cause of that?
My implementation is mostly based on https://github.com/keiohta/tf2rl/blob/master/tf2rl/algos/sac.py, but I resigned from computing td_errors.
Here is the code (in case you need it).
import tensorflow as tf
from tensorflow.keras.layers import *
from src.anfis.anfis_layers import *
from src.model.sac_layer import *
from src.anfis.anfis_model import AnfisGD

hidden_activation = 'elu'
output_activation = 'linear'


class NetworkModel:
    def __init__(self, training):

        self.parameters_count = 2
        self.results_count = 1
        self.parameters_sets_count = [3, 4]
        self.parameters_sets_total_count = sum(self.parameters_sets_count)

        self.models = {}
        self._initialise_layers()  # initialises self.models[]

        self.training = training

        self.train()

    def _initialise_layers(self):
        # ------------
        # LAYERS & DEBUG
        # ------------

        f_states = Input(shape=(self.parameters_count,))
        f_actions = Input(shape=(self.results_count,))

        # = tf.keras.layers.Dense(10)# AnfisGD(self.parameters_sets_count)
        #f_anfis = model_anfis(densanf)#model_anfis(f_states)
        f_policy_1 = tf.keras.layers.Dense(5, activation=hidden_activation)(f_states)
        f_policy_2 = tf.keras.layers.Dense(5, activation=hidden_activation)(f_policy_1)
        f_policy_musig = tf.keras.layers.Dense(2, activation=output_activation)(f_policy_2)
        f_policy = GaussianLayer()(f_policy_musig)

        #self.models[""anfis""] = tf.keras.Model(inputs=f_states, outputs=f_anfis)
        #self.models[""forward""] = tf.keras.Model(inputs=f_states, outputs=model_anfis.anfis_forward(f_states))

        self.models[""actor""] = tf.keras.Model(inputs=f_states, outputs=f_policy)

        self.models[""critic-q-1""] = generate_q_network([f_states, f_actions])
        self.models[""critic-q-2""] = generate_q_network([f_states, f_actions])

        self.models[""critic-v""] = generate_value_network(f_states)
        self.models[""critic-v-t""] = generate_value_network(f_states)

        # self.models[""anfis""].compile(
        #     loss=tf.losses.mean_absolute_error,
        #     optimizer=tf.keras.optimizers.SGD(
        #         clipnorm=0.5,
        #         learning_rate=1e-3),
        #     metrics=[tf.keras.metrics.RootMeanSquaredError()]
        # )
        # self.models[""forward""].compile(
        #     loss=tf.losses.mean_absolute_error,
        #     optimizer=tf.keras.optimizers.SGD(
        #         clipnorm=0.5,
        #         learning_rate=1e-3),
        #     metrics=[tf.keras.metrics.RootMeanSquaredError()]
        # )
        self.models[""actor""].compile(
            loss=tf.losses.mean_squared_error,
            optimizer=tf.keras.optimizers.Adam(
                learning_rate=1e-3),
            metrics=[tf.keras.metrics.RootMeanSquaredError()]
        )

    def act(self, din):
        data_input = tf.convert_to_tensor([din], dtype='float64')
        data_output = self.models[""actor""](data_input)[0]
        return data_output.numpy()[0]

    def train(self):
        self.training.train(self, hybrid=False)


def mean(y_true, y_pred): #ignore y_pred
    return tf.reduce_mean(y_true)


def generate_value_network(inputs):
    # SAC Critic Value (Estimating rewards of being in state s)
    f_critic_v1 = tf.keras.layers.Dense(5, activation=hidden_activation)(inputs)
    f_critic_v2 = tf.keras.layers.Dense(5, activation=hidden_activation)(f_critic_v1)
    f_critic_v = tf.keras.layers.Dense(1, activation=output_activation)(f_critic_v2)
    m_value = tf.keras.Model(inputs=inputs, outputs=f_critic_v)
    m_value.compile(
        loss=tf.losses.mean_squared_error,
        optimizer=tf.keras.optimizers.Adam(
            learning_rate=1e-3),
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )
    return m_value


def generate_q_network(inputs):
    # SAC Critic Q (Estimating rewards of taking action a while in state s)
    f_critic_q_concatenate = tf.keras.layers.Concatenate()(inputs)
    f_critic_q1 = tf.keras.layers.Dense(5, activation=hidden_activation)(f_critic_q_concatenate)
    f_critic_q2 = tf.keras.layers.Dense(5, activation=hidden_activation)(f_critic_q1)
    f_critic_q = tf.keras.layers.Dense(1, activation=output_activation)(f_critic_q2)

    m_q = tf.keras.Model(inputs=inputs, outputs=f_critic_q)
    m_q.compile(
        loss=tf.losses.mean_squared_error,
        optimizer=tf.keras.optimizers.Adam(
            learning_rate=1e-3),
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )
    return m_q;


from src.model.training import Training
import numpy as np
import tensorflow as tf
from src.constructs.experience_holder import ExperienceHolder


class SACTraining(Training):

    def __init__(self, environment):
        super().__init__()
        self.environment = environment
        self.models = None
        self.parameters_sets_count = None
        self.parameters_sets_total_count = 0
        self.parameters_count = 0

        self.gamma = 0.99
        self.alpha = 1.0
        self.beta = 0.003
        self.tau = 0.01

        self.experience = ExperienceHolder(capacity=10000, cells=5)  # state, action, reward, state', done

    def train(self, simulation_model, **kwargs):
        self.models = simulation_model.models
        self.parameters_count = simulation_model.parameters_count
        self.parameters_sets_count = simulation_model.parameters_sets_count
        self.parameters_sets_total_count = simulation_model.parameters_sets_total_count

        self.train_sac(
            self.models,
            epochs=300, max_steps=200, experience_batch=128, simulation=self.environment)

    def train_sac(self, models, epochs, max_steps, experience_batch, simulation):

        # deterministic random
        np.random.seed(0)

        history = []
        epoch_steps = 128
        simulation.reset()
        update_net(models['critic-v'], models['critic-v-t'], 1.0)

        for i in range(epochs):
            print(""epoch: "", i)
            episode_reward = 0
            reset = False
            j = 0
            while not(j > epoch_steps and reset):
                j += 1
                reset = False
                # ---------------------------
                # Observe state s and select action according to current policy
                # ---------------------------

                # Get simulation state
                state_raw = simulation.get_normalised()
                # state_unwound = [[i for t in state for i in t]]

                state = [state_raw[0]]  # TODO
                state_tf = tf.convert_to_tensor(state)

                # Get actions distribution from current model
                # and their approx value from critic
                actions_tf, _, _ = models['actor'](state_tf)
                actions = list(actions_tf.numpy()[0])

                # ---------------------------
                # Execute action in the environment
                # ---------------------------
                reward, done = simulation.step_nominalised(actions)
                episode_reward += reward

                # ---------------------------
                # Observe next state
                # ---------------------------

                state_l_raw = simulation.get_normalised()
                state_l = [state_l_raw[0]]  # TODO

                # ---------------------------
                # Store information in replay buffer
                # ---------------------------

                self.experience.save((state, actions, reward, state_l, 1 if not done else 0))

                if done or simulation.step_counter > max_steps:
                    simulation.reset()
                    reset = True

            # ---------------------------
            # Updating network
            # ---------------------------
            if self.experience.size() > 500:  # update_counter_limit:
                exp = self.experience.replay(min(experience_batch, int(self.experience.size() * 0.8)))
                states_tf = tf.convert_to_tensor(exp[0], dtype='float64')
                actions_tf = tf.convert_to_tensor(exp[1], dtype='float64')
                rewards_tf = tf.convert_to_tensor(exp[2], dtype='float64')
                states_l_tf = tf.convert_to_tensor(exp[3], dtype='float64')
                not_dones_tf = tf.convert_to_tensor(exp[4], dtype='float64')

                with tf.GradientTape(watch_accessed_variables=True, persistent=True) as tape:

                    q_1_current = models['critic-q-1']([states_tf, actions_tf])
                    q_2_current = models['critic-q-2']([states_tf, actions_tf])
                    v_l_current = models['critic-v-t'](states_l_tf)

                    q_target = tf.stop_gradient(rewards_tf + not_dones_tf * self.gamma * v_l_current)
                    q_1_loss = tf.reduce_mean((q_target - q_1_current) ** 2)
                    q_2_loss = tf.reduce_mean((q_target - q_2_current) ** 2)

                    v_current = models['critic-v'](states_tf)
                    actions, policy_loss, sigma = models['actor'](states_tf)
                    q_1_policy = models['critic-q-1']([states_tf, actions_tf])
                    q_2_policy = models['critic-q-2']([states_tf, actions_tf])
                    q_min_policy = tf.minimum(q_1_policy, q_2_policy)

                    v_target = tf.stop_gradient(q_min_policy - self.alpha * policy_loss)
                    v_loss = tf.reduce_mean((v_target - v_current)**2)

                    a_loss = tf.reduce_mean(self.alpha * policy_loss - q_min_policy)

                backward(tape, models['critic-q-1'], q_1_loss)
                backward(tape, models['critic-q-2'], q_2_loss)
                backward(tape, models['critic-v'], v_loss)
                update_net(models['critic-v'], models['critic-v-t'], self.tau)

                backward(tape, models['actor'], a_loss)

                del tape
           
                print('Loss:\n\tvalue: {}\n\tq1   : {}\n\tq2   : {}\n\tactor (ascent): {}'.format(
                     tf.reduce_mean(v_loss),
                     tf.reduce_mean(q_1_loss),
                     tf.reduce_mean(q_2_loss),
                     tf.reduce_mean(a_loss) #Gradient ascent

                ))
                print('Episode Reward: {}'.format(episode_reward))
                print('Batch sigma: {}'.format(tf.reduce_mean(sigma)))


def update_net(model, target, tau):
    len_vars = len(model.trainable_variables)
    for i in range(len_vars):
        target.trainable_variables[i] = tau * model.trainable_variables[i] + (1.0 - tau) * target.trainable_variables[i]


def backward(tape, model, loss):
    grads = tape.gradient(loss, model.trainable_variables)
    model.optimizer.apply_gradients(
        zip(grads, model.trainable_variables))


from tensorflow.keras import Model
import tensorflow as tf
import tensorflow_probability as tfp


class GaussianLayer(Model):
    def __init__(self, **kwargs):
        super(GaussianLayer, self).__init__(**kwargs)

    def call(self, inputs, **kwargs):
        mu, log_sig = tf.split(inputs, num_or_size_splits=2, axis=1)

        log_sig_clip = tf.clip_by_value(log_sig, -20, 2)
        sig = tf.exp(log_sig_clip)

        distribution = tfp.distributions.Normal(mu, sig)
        output = distribution.sample()
        actions = tf.tanh(output)

        return actions, \
            distribution.log_prob(output) - \
            tf.reduce_sum(tf.math.log(1 - actions ** 2 + 1e-12), axis=1, keepdims=True), \
            tf.stop_gradient(tf.keras.backend.abs(actions - tf.tanh(mu)))

","['reinforcement-learning', 'tensorflow', 'actor-critic-methods', 'loss']",
Why aren't the BERT layers frozen during fine-tuning tasks?,"
During transfer learning in computer vision, I've seen that the layers of the base model are frozen if the images aren't too different from the model on which the base model is trained on.
However, on the NLP side, I see that the layers of the BERT model aren't ever frozen. What is the reason for this?
","['natural-language-processing', 'computer-vision', 'bert', 'transfer-learning', 'fine-tuning']",
Machine Learning Techniques for Objects Location/Orientation in Images,"
what Machine Learning tool can understand in which location and orientation a picture was taken from?
That is from pictures of similar objects, say for example pictures of car interiors.
So given a side vent picture it will show me all the pictures with a side vent with similar views (if a picture shows a vent from the cockpit it will not show me pictures of vents taken from outside the car with open door).
If the problem is too complicated for just one tool, could you address me to which particular field of Artificial Intelligence should I research in?
this goes close to what I am looking for:
https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html
But I thought to ask if there are more specific appropriate examples.
","['machine-learning', 'deep-learning', 'image-recognition', 'object-detection']",
Should I use additional empty category in some categorical problems?,"
I try to create autonomous car using keyboard data so this is a multi class classification problem. I have keys W,A,S and D. So I have four categories. My model should decide what key should be pressed based on the screenshot (or some other data, I have some ideas). I have some API that I can use to capture keyboard data and screen (while gathering data) and also to simulate keyboard events (in autonomous mode when the car is driven by neural network).
Should I create another category called for example ""NOKEY""? I will use sigmoid function on each output neuron (instead of using softmax on the all neurons) to have probabilities from 0 to one for each category. But I could have very low probabilities for each neuron. And it can mean either that no key should be pressed or that the network doesn't know what to do. So maybe I should just create additional ""artificial"" category?
What is the standard way to deal with such situations?
","['categorical-data', 'sigmoid', 'multi-label-classification', 'softmax']","In short: yes, you must allow ""do nothing"" decision as a first level result.Your system must decide the action to be taken, including ""do nothing"" action. This is different to low network outputs, that can be translated as ""don't know what to do"".In other words, the network can result in:Kind regards."
speech comment detection by deep speech mozilla for data set [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I want to create a system so that when a human being says a word or command through a microphone, such as ""shut down"", the system can execute that command ""shut down"".
I used the Deep Speech algorithm on the Persian language database, which takes audio data through a microphone and returns the text. The problem I have now is what I have to do from this point on. I need to explain that my system has to work offline and also the number of commands I have is limited.
","['machine-learning', 'deep-learning', 'natural-language-processing', 'speech-recognition']",
What's the difference between estimation and approximation error?,"
I'm unable to find online, or understand from context - the difference between estimation error and approximation error in the context of machine learning (and, specifically, reinforcement learning).
Could someone please explain with the help of examples and/or references?
","['machine-learning', 'computational-learning-theory', 'bias-variance-tradeoff', 'approximation-error']","Section 5.2 Error Decomposition of the book Understanding Machine Learning: From Theory to Algorithms (2014) gives a description of the approximation error and estimation error in the context of empirical risk minimization (ERM) and, in particular, in the context of the bias-complexity tradeoff (which is strictly related to the bias-variance tradeoff).The expected risk (error) of a hypothesis $h_S \in \mathcal{H}$ selected based on the training dataset $S$ from a hypothesis class $\mathcal{H}$ can be decomposed into the approximation error, $\epsilon_{\mathrm{app}}$, and the estimation error, $\epsilon_{\mathrm{est}}$, as follows\begin{align}
L_{\mathcal{D}}\left(h_{S}\right)
&=
\epsilon_{\mathrm{app}}+\epsilon_{\mathrm{est}} \\
&=
\epsilon_{\mathrm{app}}+ \left( L_{\mathcal{D}}\left(h_{S}\right)-\epsilon_{\mathrm{app}} \right) \\
&=
\left( \min _{h \in \mathcal{H}} L_{\mathcal{D}}(h)\right) + \left( L_{\mathcal{D}}\left(h_{S}\right)-\epsilon_{\mathrm{app}} \right) \label{1}\tag{1}
\end{align}The approximation error (AE), aka inductive bias, defined as$$\epsilon_{\mathrm{app}} = \min _{h \in \mathcal{H}} L_{\mathcal{D}}(h) $$is the error due to the specific choice of hypothesis class (or set) $\mathcal{H}$. So, $\min _{h \in \mathcal{H}} L_{\mathcal{D}}(h)$ is minimal risk/error that can be achieved with a hypothesis class $\mathcal{H}$. In other words, if you limit yourself to $\mathcal{H}$ and you select the ""best"" hypothesis in $\mathcal{H}$, then $\min _{h \in \mathcal{H}} L_{\mathcal{D}}(h)$ is the expected risk of that hypothesis.Here are some properties.The larger $\mathcal{H}$ is, the smaller this error is (because it's more likely that a larger hypothesis class contains the actual hypothesis we are looking for). So, if $\mathcal{H}$ does not contain the actual hypothesis we are searching for, then this error could not be zero.This error does not depend on the training data. You can see in the formula above that there's no $S$ (the training dataset), but only on $D$ (the distribution over the space of inputs and labels from which $S$ was assumed to have been sampled)The estimation error (EE) is the difference between the approximation error $\epsilon_{\mathrm{app}}$ and the training error $L_{\mathcal{D}}\left(h_{S}\right)$, i.e.\begin{align}
\epsilon_{\mathrm{est}}
&=L_{\mathcal{D}}\left(h_{S}\right)-\epsilon_{\mathrm{app}} \\
&= L_{\mathcal{D}}\left(h_{S}\right) - \min _{h \in \mathcal{H}} L_{\mathcal{D}}(h) 
\end{align}Here are some properties.The estimation error depends on the training dataset $S$. You can see $S$ in the formula above.$\epsilon_{\mathrm{est}}$ also depends on the choice of the hypothesis class (given that it is defined as a function of $\epsilon_{\mathrm{app}}$).If we increase the size and complexity of the hypothesis class, the approximation error decreases, but the estimation error may increase (i.e. we may have over-fitting). On the other hand, if we decrease the size and complexity of the hypothesis class, the estimation error may decrease, but the bias may increase (i.e. we may have under-fitting). So, we have a bias-complexity trade-off (where the bias refers to the approximation error or inductive bias) and the complexity refers to the complexity of the hypothesis class.In section 4.1 of this book also describes a similar (but equivalent) error decomposition, which is called error excess because it's a difference between the expected risk and the Bayes error (which is sometimes called inherent error or irreducible error), which they denote by $R^{*}$, while the other book above, which also points out that there's this equivalent error excess decomposition, denote it by $\epsilon_{\mathrm{Bayes}}$. So, here's the error excess$$R(h)-R^{*}=\underbrace{\left(\inf _{h \in \mathcal{H}} R(h)-R^{*}\right)}_{\text {approximation excess}} + \underbrace{\left(R(h)-\inf _{h \in \mathcal{H}} R(h)\right)}_{\text {estimation error}}$$So, if you take $R^{*} = \epsilon_{\mathrm{Bayes}}$ from both sides of the equation, you end up with$$R(h)=\underbrace{\left(\inf _{h \in \mathcal{H}} R(h) \right)}_{\epsilon_{\mathrm{app}}} + \underbrace{\left(R(h)-\inf _{h \in \mathcal{H}} R(h)\right)}_{\epsilon_{\mathrm{est}}} \label{2}\tag{2}$$which is equivalent to equation \ref{1}, whereA nice picture that illustrates the relationship between these terms can be found in figure 4.1 of this book (p. 62).Here, the red points are specific hypotheses. In this illustration, we can see that the best hypothesis (the Bayes hypothesis) lies outside our chosen hypothesis class $\mathcal{H}$. The distance between the risk of $h \in \mathcal{H}$ and the risk of $h^* = \operatorname{arg inf} _{h \in \mathcal{H}} R(h)$ is the estimation error, while the distance between $h^*$ and the Bayes hypothesis (i.e. the hypothesis that achieves the Bayes error) is the approximation excess in equation \ref{2}."
Is it better to split sequences into overlapping or non-overlapping training samples?,"
I have $N$ (time) sequences of data with length $2048$. Each of these sequences correseponds to a different target output. However, I know that only a small part of the sequence is needed to actually predict this target output, say a sub-sequence of length $128$.
I could split up each of the sequences into $16$ partitions of $128$, so that I end up with $16N$ training smaples. However, I could drastically increase the number of training samples if I use a sliding window instead: there are $2048-128 = 1920$ unique sub-sequences of length $128$ that preserve the time series. That means I could in fact generate $1920N$ unique training samples, even though most of the input is overlapping.
I could also use a larger increment between individual ""windows"", which would reduce the number of sub-sequences but it could remove any autocorrelation between them.
Is it better to split my data into $16N$ non-overlapping sub-sequences or $1920N$ partially overlapping sub-sequences?
","['neural-networks', 'sample-complexity']",
Image Classification for watermarks with poor results,"
Just starting learning things about tensorflow and NN.
As an exercise I decided to create a dataset of images, watermarked and not, in order to binary classify these. First of all, the dataset ( you can see it here ) was created artificially by me applying some random watermarks.
First doubt, in the dataset I don't have both images one watermarked and one not, would be better to have?
Second, frustrating: model stand on 0.5 accuracy, so it just produce random output :(
Model I tried is this:
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(16,(1,1), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Conv2D(32,(3,3), activation='relu'),
    tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),
    tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='elu'),
    tf.keras.layers.Dense(64, activation='elu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1,activation=""sigmoid"")

and then compiled as this:
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics = ['accuracy'])

Here below the fit:
history = model.fit(train_data,
                              validation_data=valid_data,
                              steps_per_epoch=100,
                              epochs=15,
                              validation_steps=50,
                              verbose=2)

As for any other details, code is here.
I already checked for technical issues, I'm pretty sure image enter properly, train and validation dataset are 80/20, about 12K images for training. However accuracy bounches up and down around .5 while fitting. How can I improve?
","['convolutional-neural-networks', 'tensorflow', 'keras', 'image-recognition', 'binary-classification']","Well probably the response is that previous approach was a little naive.
I managed to fave some interesting result with this kernel that allow me to have an accuracy of 0.969  and a validation accuracy of 0.931.
Model I used is based on ResNet50 with the following additional layers ( and the last one just for binary classification ):Even if the net is already trained, I trained each layer again otherwise I did not have any sensible accuracy.Training history is like this:Still far to be really good, but progress."
What are the disadvantages of actor-only methods with respect to value-based ones?,"
While the advantages of actor-only algorithms, the ones that search directly the policy without the use of the value function, are clear (possibility of having a continuous action space, a stochastic policy, etc.), I can't figure out the disadvantages, if not general statements about less stability or bigger variance with respect to critic-only methods (with which I refer to methods that are based on the value function).
","['reinforcement-learning', 'comparison', 'policy-gradients', 'policy-based-methods', 'value-based-methods']",
Can we stop training as soon as epsilon is small?,"
I'm new to reinforcement learning.
As it is common in RL, $\epsilon$-greedy search for the behavior/exploration is used. So, at the beginning of the training, $\epsilon$ is high, and therefore a lot of random actions are chosen. With time, $\epsilon$ decreases and we often choose the best action.

I was wondering, e.g. in Q-Learning, if $\epsilon$ is small, e.g. 0.1 or 0.01, do the Q-values really still change? Do they just change their direction, i.e. the best action remains the best action but the Q-values diverge further, or do the values really change again so that the best action always changes for a given state?

If the Q-values really do still change strongly, is it because of the remaining random actions, which we still have at $\epsilon>0$ or would it still change at $\epsilon=0$?


","['reinforcement-learning', 'q-learning', 'value-functions', 'exploration-exploitation-tradeoff', 'epsilon-greedy-policy']",
How to use validation dataset in my logistic regression model?,"
I am new to machine learning and recently I joined a course where I was given a logistic regression assignment in which I had to split 20% of the training dataset for the validation dataset and then use the validation dataset to capture the minimum possible loss and then use the test dataset to find the accuracy of the model.
Below is my code for implementing logistic regression
class LogReg(LinReg):
    def __init__(self, n_dim, bias=True):
        if bias:
            n_dim = n_dim + 1
        super(LogReg, self).__init__(n_dim)
        self.bias = bias
    
  def __call__(self, x):
      return x.mm(self.theta).sigmoid()

  def compute_loss(self, x, y, lambda_reg):
      # The function has a generic implementation, and can also work for the neural nets!
      predictions = self(x)
      loss = -(y * torch.log(predictions) + (1-y) * torch.log(1 - predictions)).mean()
      regularizer = self.theta.transpose(0, 1).mm(self.theta)
      return loss + regularizer.mul(lambda_reg)
  @staticmethod
  def add_bias(x):
      ones = torch.ones((x.size(0), 1), dtype=torch.float32)
      x_hat = torch.cat((ones, x), dim=-1)
      return x_hat

  def fit(self, x, y, num_iter=10, mb_size=32, lr=1e-1, lambda_reg=1e-2, reset=True):
      N = x.size(0)
      losses = []
      x_hat = x
      # Adding a bias term if needed
      if self.bias:
          x_hat = self.add_bias(x)
      if reset:
          self.reset() # Very important if you want to call fit multiple times
      num_batches = x.size(0) // mb_size
      # The outer loop goes over `epochs`
      # The inner loop goes over the whole training data
      for it in range(num_iter):
          loss_per_epoch = 0
          for batch_it in range(num_batches):
              # has been implemented for the linear model
              self.zero_grad()

              ind = torch.randint(0, N, (mb_size, 1)).squeeze()
              x_mb, y_mb = x_hat[ind, :], y[ind, :]

              loss = self.compute_loss(x_mb, y_mb, lambda_reg)

              loss.backward()
              self.theta.data = self.theta.data - lr*self.grad().data
              loss_per_epoch += loss.item()
          
          loss_per_epoch /= num_batches
          losses.append(loss_per_epoch)
      
      return losses

How should I use the validation set at the level of epoch to find the best loss?
","['python', 'pytorch', 'regression']","So generally, when you seperate your training data to 80%-20% then you fit method should get 2 x,y. better to call them x_train,y_train, x_val, y_val or something similar.Now its important you do the split before entering the fit, and not do it for each epoch or something alike.Once you do that and the fit method should be something like:Then you should, at the end of each epoch, test the performance of the model on the validation set entirely and calculate the desired metric for evaluation. If you improved it's better to save the current model. This is done repeatedly for each epoch until the end of the training and you will guarantee to have the model who gave you the best results on the validation set rather than on the training set, which might be overfitting it.I will do it in a separate method with the following flow:And if the average result is better from some previously saved one, save the new model"
What is the loss for policy gradients with continuous actions?,"
I know with policy gradients used in an environment with a discrete action space are updated with $$
\Delta \theta_{t}=\alpha \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) v_{t}
$$
where $v_t$ could be many things that represent how good the action was. And I know that this can be calculated by performing cross entropy loss with the target being what the network would have outputted if it were completely confident in its action (zeros with the index of the action chosen being one). But I don’t understand how to apply that to policy gradients that output the mean and variance of a Gaussian distribution for a continuous action space. What is the loss for these types of policy gradients?
I tried keeping the variance constant and updating the output with mean squared error loss and the target being the action it took. I thought this would end up pushing the mean towards actions with greater total rewards but it got nowhere in OpenAI’s Pendulum environment.
It would also be very helpful if it was described in a way with a loss function and a target, like how policy gradients with discrete action spaces can be updated with cross entropy loss. That is how I understand it best but it is okay if that is not possible.
Edit: for @Philipp. The way I understand it is that the loss function is the same with a continuous action space and the only thing that changes is the distribution that we get the log-probs from. In PyTorch we can use a Normal distribution for continuous action space and Categorical for discrete action space. The answer from David Ireland goes into the math but in PyTorch, that looks like log_prob = distribution.log_prob(action_taken) for any type of distribution. It makes sense that for bad actions we would want to decrease the probability of taking the action. Below is working code for both types of action spaces to compare them. The continuous action space code should be correct but the agent will not learn because it is harder to learn the right actions with a continuous action space and our simple method isn't enough. Look into more advanced methods like PPO and DDPG.
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions.categorical import Categorical #discrete distribution
import numpy as np
import gym
import math
import matplotlib.pyplot as plt

class Agent(nn.Module):
    def __init__(self,lr):
        super(Agent,self).__init__()
        self.fc1 = nn.Linear(4,64)
        self.fc2 = nn.Linear(64,32)
        self.fc3 = nn.Linear(32,2) #neural network with layers 4,64,32,2

        self.optimizer = optim.Adam(self.parameters(),lr=lr)

    def forward(self,x):
        x = torch.relu(self.fc1(x)) #relu and tanh for output
        x = torch.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))
        return x

env = gym.make('CartPole-v0')
agent = Agent(0.001) #hyperparameters
DISCOUNT = 0.99
total = []

for e in range(500): 
    log_probs, rewards = [], []
    done = False
    state = env.reset()
    while not done:
        #mu = agent.forward(torch.from_numpy(state).float())
        #distribution = Normal(mu, SIGMA)
        distribution = Categorical(agent.forward(torch.from_numpy(state).float()))
        action = distribution.sample()
        log_probs.append(distribution.log_prob(action))
        state, reward, done, info = env.step(action.item())
        rewards.append(reward)
        
    total.append(sum(rewards))

    cumulative = 0
    d_rewards = np.zeros(len(rewards))
    for t in reversed(range(len(rewards))): #get discounted rewards
        cumulative = cumulative * DISCOUNT + rewards[t]
        d_rewards[t] = cumulative
    d_rewards -= np.mean(d_rewards) #normalize
    d_rewards /= np.std(d_rewards)

    loss = 0
    for t in range(len(rewards)):
        loss += -log_probs[t] * d_rewards[t] #loss is - log prob * total reward

    agent.optimizer.zero_grad()
    loss.backward() #update
    agent.optimizer.step()

    if e%10==0:
        print(e,sum(rewards)) 
        plt.plot(total,color='blue') #plot
        plt.pause(0.0001)    


def run(i): #to visualize performance
    for _ in range(i):
        done = False
        state = env.reset()
        while not done:
            env.render()
            distribution = Categorical(agent.forward(torch.from_numpy(state).float()))
            action = distribution.sample()
            state,reward,done,info = env.step(action.item())
        env.close()

Above is the discrete action space code for CartPole and below is the continuous action space code for Pendulum. Sigma (variance or standard deviation) is constant here but adding it is easy. Just make the final layer have two neurons and make sure sigma is not negative. Again, the pendulum code won't work because most environments with continuous action spaces are too complicated for such a simple method. Making it work would probably require a lot of testing for hyper parameters.
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions.normal import Normal #continuous distribution
import numpy as np
import gym
import math
import matplotlib.pyplot as plt
import keyboard

class Agent(nn.Module):
    def __init__(self,lr):
        super(Agent,self).__init__()
        self.fc1 = nn.Linear(3,64)
        self.fc2 = nn.Linear(64,32)
        self.fc3 = nn.Linear(32,1) #neural network with layers 3,64,32,1

        self.optimizer = optim.Adam(self.parameters(),lr=lr)

    def forward(self,x):
        x = torch.relu(self.fc1(x)) #relu and tanh for output
        x = torch.relu(self.fc2(x))
        x = torch.tanh(self.fc3(x)) * 2
        return x

env = gym.make('Pendulum-v0')
agent = Agent(0.01) #hyperparameters
SIGMA = 0.2
DISCOUNT = 0.99
total = []

for e in range(1000): 
    log_probs, rewards = [], []
    done = False
    state = env.reset()
    while not done:
        mu = agent.forward(torch.from_numpy(state).float())
        distribution = Normal(mu, SIGMA)
        action = distribution.sample().clamp(-2.0,2.0)
        log_probs.append(distribution.log_prob(action))
        state, reward, done, info = env.step([action.item()])
        #reward = abs(state[1])
        rewards.append(reward)
        
    total.append(sum(rewards))

    cumulative = 0
    d_rewards = np.zeros(len(rewards))
    for t in reversed(range(len(rewards))): #get discounted rewards
        cumulative = cumulative * DISCOUNT + rewards[t]
        d_rewards[t] = cumulative
    d_rewards -= np.mean(d_rewards) #normalize
    d_rewards /= np.std(d_rewards)

    loss = 0
    for t in range(len(rewards)):
        loss += -log_probs[t] * d_rewards[t] #loss is - log prob * total reward

    agent.optimizer.zero_grad()
    loss.backward() #update
    agent.optimizer.step()

    if e%10==0:
        print(e,sum(rewards)) 
        plt.plot(total,color='blue') #plot
        plt.pause(0.0001)
        if keyboard.is_pressed(""space""): #holding space exits training
            raise Exception(""Exited"")


def run(i): #to visualize performance
    for _ in range(i):
        done = False
        state = env.reset()
        while not done:
            env.render()
            distribution = Normal(agent.forward(torch.from_numpy(state).float()), SIGMA)
            action = distribution.sample()
            state,reward,done,info = env.step([action.item()])
        env.close()

David Ireland also wrote this on a different question I had:

The algorithm doesn't change in this situation. Say your NN outputs the mean parameter of the Gaussian, then logπ(at|st) is just the log of the normal density evaluated at the action you took where the mean parameter in the density is the output of your NN. You are then able to backpropagate through this to update the weights of your network.

","['neural-networks', 'reinforcement-learning', 'policy-gradients', 'deterministic-policy']","This update rule can still be applied in the continuous domain.As pointed out in the comments, suppose we are parameterising our policy using a Gaussian distribution, where our neural networks take as input the state we are in and output the parameters of a Gaussian distribution, the mean and the standard deviation which we will denote as $\mu(s, \theta)$ and $\sigma(s, \theta)$ where $s$ shows the dependancy of the state and $\theta$ are the parameters of our network.I will assume a one-dimensional case for ease of notation but this can be extended to multi-variate cases. Our policy is now defined as
$$\pi(a_t | s_t) = \frac{1}{\sqrt{2\pi \sigma(s_t, \theta)^2}} \exp\left(-\frac{1}{2}\left(\frac{a_t - \mu(s_t, \theta)}{\sigma(s_t, \theta)}\right)^2\right).$$As you can see, we can easily take the logarithm of this and find the derivative with respect to $\theta$, and so nothing changes and the loss you use is the same. You simply evaluate the derivative of the log of your policy with respect to the network parameters, multiply by $v_t$ and $\alpha$ and take a gradient step in this direction.To implement this (as I'm assuming you don't want to calculate the NN derivatives by hand) then you could do something along the lines of the following in Pytorch.First you want to pass your state through your NN to get the mean and standard deviation of the Gaussian distribution. Then you want to simulate $z \sim N(0,1)$ and calculate $a = \mu(s,\theta) + \sigma(s, \theta) \times z$ so that $a \sim N( \mu(s, \theta), \sigma(s, \theta))$ -- this is the reparameterisation trick that makes backpropagation through the network easier as it takes the randomness from a source that doesn't depend on the parameters of the network. $a$ is your action that you will execute in your environment and use to calculate the gradient by simply writing the code torch.log(normal_pdf(a, \mu(s, \theta), \sigma(s, \theta)).backward() -- here normal_pdf() is any function in Python that calculates the pdf of a normal distribution for a given point and parameters."
Why neural networks tend to be trained to recognize multiple things instead of just one?,"
I was watching this series: https://www.youtube.com/watch?v=aircAruvnKk
The series demonstrates neural networks by building a simple number recognizing network.
It got me thinking: Why neural networks try to recognize multiple labels instead of just one? In the above example, the network tries to recognize numbers from 0 to 9. What is the benefit of trying to recognize so many things simultaneously? Wouldn't it make it easier to reason about if there would be 10 different neural networks which would specialize to recognize only one number at a time?
",['neural-networks'],
Is stable learning preferable to jumps in accuracy/loss,"
A stable/smooth learning validation curve often seems to keep improving over more epochs than an unstable learning curve. My intuition is that dropping the learning rate and increasing the patience of a model that produces a stable learning curve could lead to better validation fit.
The counter argument is that jumps in the curve could mean that the model has just learned something significant, but they often jump back down or tail off after that.
Is one better than the other? Is it possible to take aspects of both to improve learning?
","['machine-learning', 'training', 'learning-rate']",
Why would the lookup table (of a table-driven artificial agent) need to store data at pixel precision?,"
While reading the book AI A modern approach, 4th ed, I came across the section of ""Agent program"" with following text:

It is instructive to consider why the table-driven approach to agent
construction is doomed to failure. Let $P$ be the set of possible
percepts and let $T$ be the lifetime of the agent (the total number of
percepts it will receive).
The lookup table will contain $\sum_{i=1}^T |P|^T$ entries.
Consider the automated taxi: the visual input from a single camera
(eight cameras is typical) comes in at the rate of roughly 70 mb per
sec. (30 frames per sec, 1080 X 720 pixels, with 24 bits of color
information).
This gives a lookup table with over $10^{600,000,000,000}$ for an
hour's driving.

Could someone please explain how the lookup table number is derived? (or what the author's point is which I am missing). If I were to multiply all of the numbers $30 × 1080 × 720 × 24 × 8 × 3600$, then I get $1.6124314e+13$ which comes very close I think, but can't get what would be the reason to build a table (even though a theoretic one) in such a way - something which is obviously intractable
edit:
My core question is this:
Assuming $10^{600,000,000,000}$ is derived from $30 × 1080 × 720 × 24 × 8 × 3600$, what is the purpose of storing data in the look up table at pixel precision? Wouldn't storing higher level of details be enough to solve these kind of problems (ie, autonomous driving)? Coming more from standard software database systems, I am missing that point. Thanks
","['math', 'intelligent-agent', 'norvig-russell']","A tabular system for agent decisions is a direct and simple map of percept to control choice. For each percept received, the agent looks up the percept and cross-references it to the action it should take. In order to construct this, you need to list all percepts in full detail, with the associated control choice.Clearly that is not going to be feasible for the automated taxi example. No-one would think to build such a table to handle natural image inputs. That is the author's point.However, a tabular structure is a reasonable theoretical construct for mapping an arbitrary discrete function, and also is practical for simple environments.To answer your extended question:Assuming $10^{600,000,000,000}$ is derived from $30 × 1080 × 720 × 24 × 8 × 3600$, what is the purpose of storing data in the look up table at pixel precision?It is the only way to get a map from percept to control using a tabular system.By proposing any kind of summarisation or approximation of the input-to-output function to solve this, you have gone beyond the capability of a tabular system. That again is the author's point.Wouldn't storing higher level of details be enough to solve these kind of problems (ie, autonomous driving)?If it is really obvious to you that this is the solution, then that's a good thing as you are thinking ahead. However, you should also consider what that means in terms of what you might be giving up, from a theoretical perspective. For instance, a tabular system can make radically different decisions based on very minor differences between percepts, whilst any form of processing of the inputs to make them easier to handle is necessarily going to remove information that might be important."
Training a CNN for semantic segmentation of large 4600x4600px images,"
I am trying to implement a CNN (U-Net) for semantic segmentation of similar large grayscale ~4600x4600px medical images. The area I want to segment is the empty space (gap) between a round object in the middle of the picture and an outer object, which is ring-shaped, and contains the round object. This gap is ""thin"" and is only a small proportion of the whole image.
In my problem having a small a gap is good, since then the two objects have a good connection to each other.
My questions:

Is it possible to feed such large images on a CNN?
Downscaling the images seems a bad idea since the gap is thin and most of the relevant information will be lost. From what I've seen CNN are trained on much smaller images.

Since the problem is symmetric in some sense is it a good idea to split the image in 4 (or more) smaller images?

Are CNNs able to detect such small regions in such a huge image? From what I've seen in the literature, mostly larger objects are segmented such as organs etc.


I would appreciate some ideas and help. It is my first post on the site so hopefully I didn't make any mistakes.
Cheers
","['neural-networks', 'convolutional-neural-networks', 'training', 'image-recognition', 'image-segmentation']",
Which 6-bit string would represent an optimal solution for trap-3 in the Linkage Learning Genetic Algorithm?,"
I am struggling to learn certain Evolutionary algorithm concepts and also relations between each of them. I am going through the Linkage Learning Genetic Algorithm (LLGA) right now and came across this question:
Which 6-bit string of LLGA would represent an optimal solution for trap-3?
Can anyone give me an answer or explain it?
","['genetic-algorithms', 'evolutionary-algorithms']",
"What is meant by ""ground truth"" in the context AI?","
What does ""ground truth"" mean in the context of AI especially in the context of machine learning?
I am a little confused because I have read that the ground truth is the same as a label in supervised learning. And I think that's not quite right. I thought that ground truth refers to a model (or maybe the nature) of a problem. I always considered it as something philosophical (and that's what also the vocabulary 'ground truth' implies), because in ML we often don't build a describing model of the problem (like in classical mechanics) but rather some sort of a simulator that behaves like it is a describing model. That's what we/I call sometimes black box.
What is the correct understanding?
","['machine-learning', 'terminology']",
Why is it necessary to divide the priority range according to the batch size in Prioritized Experience Replay?,"
According to DeepMinds's paper Prioritized Experience Replay (2016), specifically Appendix B.2.1 ""Proportional prioritization"" (p. 13), one should equally divide the priority range $[0, p_\text{total}]$ into $k$ ranges, where $k$ is the size of the batch, and sample a random variable within these sub-ranges. This random variable is then used to sample an experience from the sum-tree according to its priority (probability).
Why do we need to do that? Why not simply sampling $k$ random variables in $[0, p_\text{total}]$ and getting $k$ variables from the sum-tree without dividing the priority range into $k$ different ranges? Isn't this the same?
","['reinforcement-learning', 'papers', 'experience-replay']",
"Why don't ensembling, bagging and boosting help to improve accuracy of Naive bayes classifier?","

You might think to apply some classifier combination techniques like ensembling, bagging and boosting but these methods would not help. Actually, “ensembling, boosting, bagging” won’t help since their purpose is to reduce variance. Naive Bayes has no variance to minimize.

The above paragraph is mentioned in this article.

How can they say the purpose of these methods is to reduce variance?
Naive Bayes has no variance, is it true?

Thanks in advance
","['machine-learning', 'naive-bayes', 'bias-variance-tradeoff', 'ensemble-learning', 'boosting']",
Model output segmentation maps which are not full,"
I created a VGG based U-Net in order to perform image segmentation task on yeast cells images obtained by a microscope.
There are a couple of problems with the data:

There is inhomogeneity in the amount of yeast in the images. 1 image can have hundred of yeast cells while others can have less the one hundred.
The GT segmentation map is also incomplete, and some of the cells are not labeled.

All in all the model, given the above problem, is able to learn in some manner. My problem is that the segmentation maps seem incomplete.
For example:


My loss function contains BCE, I was wondering if there is a way to force the model to create a 'fuller' segmentation maps. Something like using Random fields of some sort. Or maybe to enhance my loss function to overcome the above-mentioned problems.
I wish to stay in the domain of simple architectures rather than using more sophisticated ones such as RCNN.
Would appreciate any suggestions
","['convolutional-neural-networks', 'image-segmentation', 'u-net', 'semi-supervised-learning']",
How to mathematically describe the convolution operation (with a Gaussian kernel)?,"
I have to build a model where I pre-process the data with a Gaussian kernel. The data are an $n\times n$ matrix (i.e one channel), but not an image, thus I can't refer to this matrix as an image and to its elements as pixels. The Gaussian kernel is built by the following function (more i.e. here)
$$\begin{equation}
\begin{aligned}
g(x,y,\sigma) = 
\dfrac{1}{2\pi\sigma^2} e^{\dfrac{-(x^2+y^2)}{2\sigma^2}}.
\end{aligned}
\end{equation}$$
This kernel is moving one by one element and doing convolution. In my case, most of the elements are zero, the matrix is sparse.
How can I describe/understand the process of convolving the original data with a Gaussian kernel?
I have been looking for some articles, but I am unable to find any mathematical explanations, only explanation in words or pseudo-code.
","['convolutional-neural-networks', 'math', 'image-processing', 'data-preprocessing', 'convolution']","Mathematically, the convolution is an operation that takes two functions, $f$ and $g$, and produces a third function, $h$. Concisely, we can denote the convolution operation as follows$$f \circledast g = h$$In the context of computer vision and, in particular, image processing, the convolution is widely used to apply a so-called kernel (aka filter) to an input (typically, an image, but this does not have to be the case). The input (e.g. an image), the kernel, and the output of the convolution, in this context, is usually a matrix or a tensor. In image processing, the convolution is typically used to e.g. blur images or maybe to remove noise.However, in the beginning, I said that the convolution is an operation that takes two functions (and not matrices) and produces a third one, so these two explanations of the convolution do not seem to be consistent, right?The answer to this question is that the two explanations are consistent with each other. More precisely, if you have a function $f : X \rightarrow Y$ (assuming that $X$ is discrete/countable), you can represent it in a vector form as follows $\mathbf{f} = [y_1, y_2, \dots, y_n]$, i.e. $\mathbf{f}$ is a vector that contains all outputs of the function $f$ (for all possible inputs).In image processing, an image and a kernel can also be thought of as a function with a discrete domain (i.e. the pixels), so the matrices that represent the image or the kernel are just the vector forms of the corresponding functions. See this answer for more details about representing an image as a function.Once you understand that the convolution in image processing is really the convolution operation as defined in mathematics, then you can simply look up the mathematical definition of the convolution operation.In the discrete case (i.e. you can think of the function as vectors, as explained above), the convolution is defined as$${\displaystyle h[n] =  (f  \circledast g)[n]=\sum _{m=-M}^{M}f[n-m]g[m].} \tag{1}\label{1}$$You can read equation $1$ as follows$$
\mathbf{g} = 
\frac{1}{273}
\begin{bmatrix}
1 & 4 & 7 & 4 & 1 \\
4 & 16 & 26 & 16 & 4 \\
7 & 26 & 41 & 26 & 7 \\
4 & 16 & 26 & 16 & 4 \\
1 & 4 & 7 & 4 & 1
\end{bmatrix}
\label{2}\tag{2}
$$Here are some notes:The kernel \ref{2} is symmetric around the $x$ and $y$ axes: this actually implies that the convolution is equal to the cross-correlation, so you don't even have to worry about their equivalence or not (in case you have ever worried about it, which would have happened only if you already came across the cross-correlation). See this question for more info.The kernel \ref{2} is the vector form of the function form of the 2d Gaussian kernel (the one in your question): more precisely, an integer-valued approximation of the 2D Gaussian kernel when $\sigma = 1$ (as stated in your slides).The convolution can be implemented as matrix multiplication. This may not be useful now, but it's something useful to know if you want to implement it. See this question for more info.Question for you: what is the result of the application of this Gaussian kernel to any input? What does this kernel intuitively do? Once you fully understand the convolution, you can answer this question."
Why scaling down the parameter many times during training will help the learning speed be the same for all weights in Progressive GAN?,"
The title is one of the special things in Progressive GAN, a paper of the NVIDIA team. By using this method, they introduced that

Our approach ensures that the dynamic range, and thus the learning speed, is the same for all weights.

In details, they inited all learnable parameters by normal distribution $N(0,1)$. During training time, each forward time, they will scale the result with per-layer normalization constant from He's initializer
I reproduced the code from pytorch GAN zoo Github's repo
def forward(self, x, equalized):
    # generate He constant depend on the size of tensor W
    size = self.module.weight.size()
    fan_in = prod(size[1:])
    weight = math.sqrt(2.0 / fan_in)
    '''
    A module example:

    import torch.nn as nn
    module = nn.Conv2d(nChannelsPrevious, nChannels, kernelSize, padding=padding, bias=bias) 
    '''
    x = self.module(x)

    if equalized:
        x *= self.weight
    return x

At first, I thought the He constant will be $c = \frac{\sqrt{2}}{\sqrt{n_l}}$ as He's paper. Normally, $n_l > 2$ so $w_l$ can be scale up which lead to the gradient in backpropagation is increase as the formula in ProGan's paper $\hat{w}_i=\frac{w_i}{c}$ $\rightarrow$ prevent vanishing gradient.
However, the code shows that $\hat{w}_i=w_i*c$.
In summary, I can't understand why to scale down the parameter many times during training will help the learning speed be more stable.
I asked this question on some communities e.g: StackOverflow, mathematics, Data Science, and still haven't had an answer.
Please help me explain it, thank you!
","['training', 'optimization', 'gradient-descent', 'papers', 'generative-adversarial-networks']",
How does the MCTS tree look like?,"
I have come across the Monte Carlo tree search (MCTS) algorithm, but I can't find what the tree should look like. For example, does it still represent a minimax process, i.e. player 1 from the root has its child nodes as probabilities of moves, then from those child nodes, the next move is player 2, etc.? Is this how the tree looks like, so when we backpropagate we update only the winning player nodes?
","['search', 'monte-carlo-tree-search', 'minimax']",
Why do my rewards reduce after extensive training using D3QN?,"
I am running a drone simulator for collision avoidance using a slight variant of D3QN. The training is usually costly (runs for at least a week) and I have observed that reward function gradually increases during training and then drastically drops. In the simulator, this corresponds to the drone exhibiting cool collision avoidance after a few thousand episodes. However, after training for more iterations it starts taking counterintuitive actions such as simply crashing into a wall (I have checked to ensure that there is no exploration at play over here).
Does this have to do with overfitting? I am unable to understand why my rewards are falling this way.
","['reinforcement-learning', 'q-learning', 'dqn', 'rewards']","It is not 100% clear, but this seems like an instance of catastrophic forgetting. This is something that often impacts reinforcement learning.I have answered a very similar question on Data Science stack exchange, and reproduce the same answer here.This is called ""catastrophic forgetting"" and can be a serious problem in many RL scenarios.If you trained a neural network to recognise cats and dogs and did the following:Train it for many epochs on a full dataset until you got a high accuracy.Continue to train it, but remove all the cat pictures.Then in a relatively short space of time, the NN would start to lose accuracy. It would forget what a cat looks like. It would learn that its task was to switch the dog prediction as high as possible, just because on average everything in the training population was a dog.Something very similar happens in your DQN experience replay memory. Once it gets good at a task, it may only experience success. Eventually, only successful examples are in its memory. The NN forgets what failure looks like (what the states are, and what it should predict for their values), and predicts high values for everything.Later on, when something bad happens and the NNs high predicted value is completely wrong, the error can be high. In addition the NN may have incorrectly ""linked"" features of the state representation so that it cannot distinguish which parts of the feature space are the cause of this. This creates odd effects in terms of what it learns about values of all states. Often the NN will behave incorrectly for a few episodes but then re-learn optimal behaviour. But it is also possible that it completely breaks and never recovers.There is lots of active research into catastrophic forgetting and I suggest you search that term to find out some of the many types of mitigation you could use.For Cartpole, I found a very simple hack made the learning very stable. Keep aside some percentage of replay memory stocked with the initial poor performing random exploration. Reserving say 10% to this long term memory is enough to make learning in Cartpole rock solid, as the NN always has a few examples of what not to do. The idea unfortunately does not scale well to more complex environments, but it is a nice demonstration. For a more sophisticated look at similar solutions you could see the paper ""The importance of experience replay database composition in deep reinforcement learning"""
Transformer Language Model generating meaningless text,"
I currently learning on Transformers, so check my understanding I tried implementing a small transformer-based language model and compare it to RNN based language model. Here's the code for transformer. I'm using PyTorch inbuilt layer for Transformer Encoder
class TransformerLM_1(nn.Module):

    def __init__(self, head, vocab_size, embedding_size, dropout = 0.1, device = 'cpu', 
                 pad_idx = 0, start_idx = 1, end_idx = 2, unk_idx = 3):
      
        super(TransformerLM_1, self).__init__()
      
        self.head = head
        self.embedding_size = embedding_size
        self.vocab_size = vocab_size
        self.device = device
        self.embed = WordEmbedding(self.vocab_size, self.embedding_size, pad_idx)
        self.postional_encoding = PostionalEncoding(embedding_size, device)
        self.decoder = nn.TransformerEncoderLayer(self.embedding_size, self.head)
        self.out_linear = nn.Linear(self.embedding_size, vocab_size)
        self.dropout = dropout
        self.pad_idx = pad_idx
        self.start_idx = start_idx
        self.end_idx = end_idx
        self.unk_idx = unk_idx
        self.device = device

    
    def make_src_mask(self, src_sz):
        mask = (torch.triu(torch.ones(src_sz, src_sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, 10e-20).masked_fill(mask == 1, float(0.0))
        mask = mask.to(self.device)
        return mask

    def forward(self, x):
        dec_in = x.clone()[:, :-1]
        src_mask = self.make_src_mask(dec_in.size()[1])
        src = self.embed(dec_in)
        src = self.postional_encoding(src) 
        src = src.transpose(0,1)
        transformer_out = self.decoder(src, src_mask)
        out = self.out_linear(transformer_out)
        return out

I'm using teacher forcing to make it converge faster. From what I saw from the results, the text generated by the RNN model is better than transformer's.
Here is sample generated text with the expected
Expected: you had to have been blind not to see the scenario there for what it was and is and will continue to be for months and even years a part of south carolina that has sustained a blow that the red cross expects will cost that organization alone some $ n million <eos> 
Predicted: some <unk> been the been <unk> not be $ the total has was the may has <unk> the that that be to the <unk> the 

Expected: citicorp and chase are attempting to put together a new lower bid <eos> 
Predicted: a are <unk> carries n't to the together with <unk> jersey than 

Expected: it ' s amazing the amount of money that goes up their nose out to the dog track or to the tables in las vegas mr . katz says <eos> 
Predicted: <unk> ' s <unk> comeback money of the in mr to their <unk> and of <unk> <unk> or or <unk> the money 

Expected: moreover while asian and middle eastern investors <unk> gold and help <unk> its price silver does n't have the same <unk> dealers say <eos> 
Predicted: the production the routes <unk> of its 

Expected: a board of control spokesman said the board had not seen the claim and declined to comment <eos> 
Predicted: the board said declined of said 

Expected: property capital trust said it dropped its plan to liquidate because it was n't able to realize the value it had expected <eos> 
Predicted: the claims markets said its was n <unk> to sell insolvent of was n't disclosed to sell its plan 

Expected: similarly honda motor co . ' s sales are so brisk that workers <unk> they have n't had a saturday off in years despite the government ' s encouragement of more leisure activity <eos> 
Predicted: the honda ' credit . s s <unk> 

Expected: we expect a big market in the future so in the long term it will be profitable <eos> 
Predicted: it can it <unk> board 

Expected: u . k . composite or <unk> insurers which some equity analysts said might be heavily hit by the earthquake disaster helped support the london market by showing only narrow losses in early trading <eos> 
Predicted: the . s . s trading sell said which <unk> traders market said the be able in the the earthquake 

Expected: this will require us to define and <unk> what is necessary or appropriate care <eos> 
Predicted: <unk> is be the $ <unk> <unk> <unk> <unk> is the to <unk> and or 

As you can see Transformer fails to grasp grammar compared to RNN. Is there anything wrong with my understanding?
EDIT
This is one example that caught my eye
Expected: also the big board met with angry stock specialists <eos> 
Predicted: also met specialists board met the stock big with after 

Most of the words predicted have is from the expected but in a different order. I have read that transformers are permutation invariant which is the reason why we include positional encoding with the word embedding.
","['natural-language-processing', 'pytorch', 'transformer', 'text-generation']",
Are Autoencoders for noise-reduction only suited to deal with salt-and-pepper kind of noise?,"
I'm currently looking at NN to deal with noisy data. I like the Autoencoder approach https://medium.com/@aliaksei.mikhailiuk/unsupervised-learning-for-data-interpolation-e259cf5dc957 because it seems to be adaptive and does not require to be trained on specific training data.
However, as it is described in this article it seems to rely on having none-noise samples in the input data that are true to the ground truth, so I wonder if an autoencoder also could work in the case of white or blue noise instead of salt-and-pepper noise?
",['autoencoders'],
"To solve chess with deep RL and MCTS, how should I represent the input (the state) to a neural network?","
I'm wanting to build a NN that can create a policy for each possible state. I want to combine this with MCTS to eliminate randomness so when expansion occurs, I can get the probability of the move to winning.
I am confident (I believe) in how to code the neural network, but the input shape is the hardest part here. I am firstly wanting to try with 2 player chess and then expand to 3 player chess.
What is the best vector/matrix to use for the input in a chess game? How should the input be fed into a neural network to output the most promising move from the position? In addition, what format should it look like (i.e [001111000], etc.)?
","['reinforcement-learning', 'deep-rl', 'monte-carlo-tree-search', 'chess', 'state-spaces']",
Should I prefer cropped images or realistic images for object detection?,"
I am new to the field of AI but due to the high level of abstraction that comes with services such as Google VisionAI I got motivated to write an application that detects symbols in photos based on tensorflow.js and a custom model trained in Google Vision AI.
My App is about identifying symbols in photos, very similar to traffic signs or logo detection. Now I wonder if

I should train the model based on real, distorted and complex photos that contain those symbols and lots of background noise
if it was enough to train the model based on cropped, clean symbols
A hybrid of both

I started with option a and it works fine, however it was a lot of work to create the training dataset. Does the model need the distorted background to work?
","['tensorflow', 'object-detection']",
How are weight matrices in attention learned?,"
I have been looking into transformers lately and have been reading tons of tutorials. All of them address the intuition behind attention, which I understand. However, they treat learning the weight matrices (for query, key, and value) as it is the most trivial thing.
So, how are these weight matrices learned? Is the error just backpropagated, and the weights are updated accordingly?
","['deep-learning', 'natural-language-processing', 'backpropagation', 'transformer', 'attention']",
What is human-level performance for semantic segmentation?,"
I see so many papers claim to have an algorithm that beats 'human-level performance' for semantic segmentation tasks, but I can't find any papers reporting on what the human-level performance actually is. An analysis of the similarity between segmentations drawn by multiple different human experts would be good. Could someone point me towards a paper that reports on something like that?
","['image-segmentation', 'performance']",
How to let the agent choose how to populate a state space matrix in RL (using python),"
I have an agent (drone) that has to allocate subchannels for different types of User Equipment.
I have represented the subchannel allocation with a 2-dimentional binary matrix, that is initialized to all zeros as there is no requests at the beginning of the episode.
When the agent chooses an action, it has to choose which subchannels to allocate to which UEs, hence populating the matrix with 1s.
I have no idea how to do it.
","['reinforcement-learning', 'python', 'environment']",
Are there any examples of state-of-the-art NLP applications that are still n-gram based and use Naive Bayes?,"
As far as I can tell, most NLP tasks today use word embeddings and recurrent networks or transformers.
Are there any examples of state-of-the-art NLP applications that are still n-gram based and use Naive Bayes?
","['natural-language-processing', 'reference-request', 'state-of-the-art', 'naive-bayes']",
Why weighting by lambda that sums to 1 ensures convergence in eligibility trace?,"
In Sutton and Barto's Book in chapter 12, they state that if weights sum to 1, then an equation's updates have ""guaranteed convergence properties"". Actually why it ensures convergence?
There is a full citation from the mentioned fragment in Richard S. Sutton and Andrew G. Barto. Second Edition:

Now we note that a valid update can be done not just toward any n-step return, but toward any average of n-step returns for different ns. For example, an update can be done toward a target that is half of a two-step return and half of a four-step return: $\frac{1}{2}G_{t:t+2} + \frac{1}{2}G_{t:t+4}$. Any set of n-step returns can be averaged in this way, even an infinite set, as long as the weights on the component returns are positive and sum to 1. The composite return possesses an error reduction property similar to that of individual n-step returns (7.3) and thus can be used to construct updates with guaranteed convergence properties.

","['reinforcement-learning', 'eligibility-traces']",
How to design an observation(state) space for a simple `Rock-Paper-Scissor` game?,"
For weeks I've been working with this toy game of Rock-Paper-Scissor. I want to use a PPO agent learn to beat a computer opponent whose logic is defined as the code bellow.
For short, this computer opponent, named abbey, uses a strategy that tracks all two consecutive plays of the agent, and gives the opposite play of the most likely guess of the agent's next play, according to the agent's last play.
I design the agent(using gym env) to have an internal state, keeping track of all counts of its two consecutive plays, in a 3x3 matrix. And then I normalized each row of the matrix to be an observation of the agent, representing the probabilities of the second play given the previous one. So the agent will get the same knowledge as what abbey knows.
Then I copied an PPO network algorithm from some RL book, which works well with CartPole. Then I did some minor changes which are commented in the code bellow.
But the algorithm does not converge even a little, and abbey always wins the agent about 60% of the time from first run to the last.
I doubt the state and observation space I designed is the reason why it does not converge. All I get is that the agent maybe should find something from the histories of its own successes and fails, and find its way out.
Can you give me some advice for the designing of a state space?
Thank you very much.
### define a Rock-Paper-Scissor opponent

abbey_state = []
play_order=[{
              ""RR"": 0,
              ""RP"": 0,
              ""RS"": 0,
              ""PR"": 0,
              ""PP"": 0,
              ""PS"": 0,
              ""SR"": 0,
              ""SP"": 0,
              ""SS"": 0,
          }]
def abbey(prev_opponent_play,
          re_init=False):
    if not prev_opponent_play:
        prev_opponent_play = 'R'
    global abbey_state, play_order
    if re_init:
        abbey_state = []
        play_order=[{
              ""RR"": 0,
              ""RP"": 0,
              ""RS"": 0,
              ""PR"": 0,
              ""PP"": 0,
              ""PS"": 0,
              ""SR"": 0,
              ""SP"": 0,
              ""SS"": 0,
          }]
    abbey_state.append(prev_opponent_play)
    last_two = """".join(abbey_state[-2:])
    if len(last_two) == 2:
        play_order[0][last_two] += 1
    potential_plays = [
        prev_opponent_play + ""R"",
        prev_opponent_play + ""P"",
        prev_opponent_play + ""S"",
    ]
    sub_order = {
        k: play_order[0][k]
        for k in potential_plays if k in play_order[0]
    }
    prediction = max(sub_order, key=sub_order.get)[-1:]
    ideal_response = {'P': 'S', 'R': 'P', 'S': 'R'}
    return ideal_response[prediction]


### define the gym env
import gym
from gym import spaces
from collections import defaultdict
import numpy as np

ACTIONS = [""R"", ""P"", ""S""]
games = 1000

class RockPaperScissorsEnv(gym.Env):
  metadata = {'render.modes': ['human']}

  def __init__(self):
    super(RockPaperScissorsEnv, self).__init__()
    self.action_space = spaces.Discrete(3)
    self.observation_space = spaces.Box(low=0.0, high=1.0,
                                        shape=(3,3), dtype=float)
    self.reset()

  def step(self, actions):
    assert actions == 0 or actions == 1 or actions == 2
    opponent_play = self.opponent_play()

    self.prev_plays[self.prev_actions * 3 + actions] += 1
    reward = self.calc_reward(actions, opponent_play)
    terminal = False

    self.calc_state(self.timestep, opponent_play, actions)
    self.prev_actions = actions
    self.prev_opponent_play = opponent_play
    self.timestep += 1
    return self.get_ob(), reward, terminal, None

  def reset(self):
    self.opponent = abbey
    self.timestep = 0
    self.prev_opponent_play = 0
    self.prev_actions = 0
    self.prev_plays = defaultdict(int)
    self.init_state = np.zeros((3,3), dtype=int)
    # the internal state
    self.state = np.copy(self.init_state)
    self.results = {""win"": 0, ""lose"": 0, ""tie"": 0}
    return self.get_ob()

  def render(self, mode='human'):
    pass

  def close (self):
    pass

  def calc_reward(self, actions, play):
    if self.timestep % games == games - 1:
      pass
    if actions == play:
      self.results['tie'] += 1
      return 0
    elif actions == 0 and play == 1:
      self.results['lose'] += 1
      return -0.3
    elif actions == 1 and play == 2:
      self.results['lose'] += 1
      return -0.3
    elif actions == 2 and play == 0:
      self.results['lose'] += 1
      return -0.3
    elif (actions == 1 and play == 0) or (actions == 2 and play == 1) or (actions == 0 and play == 2):
      self.results['win'] += 1
      return 0.3
    else:
      raise NotImplementedError('calc_reward something get wrong')

  def opponent_play(self):
    re_init = (self.timestep == 0)
    opp_play = self.opponent(ACTIONS[self.prev_actions], re_init=re_init)
    return ACTIONS.index(opp_play)

  def calc_state(self, timestep, opponent_play, actions):
    self.state[self.prev_actions][actions] += 1

  def get_ob(self):
    '''return observations'''
    state0 = self.state[0]
    sum0 = state0.sum()
    state1 = self.state[1]
    sum1 = state1.sum()
    state2 = self.state[2]
    sum2 = state2.sum()
    init = np.ones(3, dtype=float) / 3.0
    ob = np.array([
      state0 / sum0 if sum0 else init,
      state1 / sum1 if sum1 else init,
      state2 / sum2 if sum2 else init,
    ])
    # print(ob)
    return ob



### Learning Algo copied from some book

import  matplotlib
from    matplotlib import pyplot as plt
matplotlib.rcParams['font.size'] = 18
matplotlib.rcParams['figure.titlesize'] = 18
matplotlib.rcParams['figure.figsize'] = [9, 7]
matplotlib.rcParams['axes.unicode_minus']=False

plt.figure()

import  gym,os
import  numpy as np
import  tensorflow as tf
from    tensorflow import keras
from    tensorflow.keras import layers,optimizers,losses
from    collections import namedtuple
env = RockPaperScissorsEnv()
env.seed(2222)
tf.random.set_seed(2222)
np.random.seed(2222)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
assert tf.__version__.startswith('2.')



gamma = 0.98
epsilon = 0.2
batch_size = 32

Transition = namedtuple('Transition', ['state', 'action', 'a_log_prob', 'reward', 'next_state'])

class Actor(keras.Model):
    def __init__(self):
        super(Actor, self).__init__()
        self.fc1 = layers.Dense(18, kernel_initializer='he_normal') # I changed 100 to 18
        self.fc2 = layers.Dense(3, kernel_initializer='he_normal') # I changed 4 to 3

    def call(self, inputs):
        x = tf.nn.relu(self.fc1(inputs))
        x = self.fc2(x)
        x = tf.nn.softmax(x, axis=1)
        return x

class Critic(keras.Model):
    def __init__(self):
        super(Critic, self).__init__()
        self.fc1 = layers.Dense(18, kernel_initializer='he_normal') # I changed 100 to 18
        self.fc2 = layers.Dense(1, kernel_initializer='he_normal')

    def call(self, inputs):
        x = tf.nn.relu(self.fc1(inputs))
        x = self.fc2(x)
        return x




class PPO():
    def __init__(self):
        super(PPO, self).__init__()
        self.actor = Actor()
        self.critic = Critic()
        self.buffer = []
        self.actor_optimizer = optimizers.Adam(1e-3)
        self.critic_optimizer = optimizers.Adam(3e-3)

    def select_action(self, s):
        s = tf.constant(s, dtype=tf.float32)
        # s = tf.expand_dims(s, 0)   # I removed this line, otherwise we will get a (1,3,3) tensor and later we will get an error
        prob = self.actor(s)
        a = tf.random.categorical(tf.math.log(prob), 1)[0]
        a = int(a)
        return a, float(prob[0][a])

    def get_value(self, s):
        s = tf.constant(s, dtype=tf.float32)
        s = tf.expand_dims(s, axis=0)
        v = self.critic(s)[0]
        return float(v)

    def store_transition(self, transition):
        self.buffer.append(transition)

    def optimize(self):
        state = tf.constant([t.state for t in self.buffer], dtype=tf.float32)
        action = tf.constant([t.action for t in self.buffer], dtype=tf.int32)
        action = tf.reshape(action,[-1,1])
        reward = [t.reward for t in self.buffer]
        old_action_log_prob = tf.constant([t.a_log_prob for t in self.buffer], dtype=tf.float32)
        old_action_log_prob = tf.reshape(old_action_log_prob, [-1,1])

        R = 0
        Rs = []
        for r in reward[::-1]:
            R = r + gamma * R
            Rs.insert(0, R)
        Rs = tf.constant(Rs, dtype=tf.float32)

        for _ in range(round(10*len(self.buffer)/batch_size)):

            index = np.random.choice(np.arange(len(self.buffer)), batch_size, replace=False)

            with tf.GradientTape() as tape1, tf.GradientTape() as tape2:

                v_target = tf.expand_dims(tf.gather(Rs, index, axis=0), axis=1)

                v = self.critic(tf.gather(state, index, axis=0))
                delta = v_target - v
                advantage = tf.stop_gradient(delta)
                a = tf.gather(action, index, axis=0)
                pi = self.actor(tf.gather(state, index, axis=0)) 
                indices = tf.expand_dims(tf.range(a.shape[0]), axis=1)
                indices = tf.concat([indices, a], axis=1)
                pi_a = tf.gather_nd(pi, indices)
                pi_a = tf.expand_dims(pi_a, axis=1)
                # Importance Sampling
                ratio = (pi_a / tf.gather(old_action_log_prob, index, axis=0))
                surr1 = ratio * advantage
                surr2 = tf.clip_by_value(ratio, 1 - epsilon, 1 + epsilon) * advantage
                policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))
                value_loss = losses.MSE(v_target, v)
            grads = tape1.gradient(policy_loss, self.actor.trainable_variables)
            self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))
            grads = tape2.gradient(value_loss, self.critic.trainable_variables)
            self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))

        self.buffer = []


def main():
    agent = PPO()
    returns = []
    total = 0
    for i_epoch in range(500):
        state = env.reset()
        for t in range(games):
            action, action_prob = agent.select_action(state)
            if t == 999:
              print(action, action_prob)
            next_state, reward, done, _ = env.step(action)
            # print(next_state, reward, done, action)
            trans = Transition(state, action, action_prob, reward, next_state)
            agent.store_transition(trans)
            state = next_state
            total += reward
            if done:
                if len(agent.buffer) >= batch_size:
                    agent.optimize()
                break
        print(env.results)

        if i_epoch % 20 == 0:
            returns.append(total/20)
            total = 0
            print(i_epoch, returns[-1])

    print(np.array(returns))
    plt.figure()
    plt.plot(np.arange(len(returns))*20, np.array(returns))
    plt.plot(np.arange(len(returns))*20, np.array(returns), 's')
    plt.xlabel('epochs')
    plt.ylabel('total return')
    plt.savefig('ppo-tf.svg')


if __name__ == '__main__':
    main()
    print(""end"")


```

","['tensorflow', 'deep-rl', 'actor-critic-methods', 'proximal-policy-optimization']",
Is there a complement to GPT/2/3 that can be trained using supervised learning methods?,"
This is a bit of a soft question, not sure if it's on topic, please let me know how I can improve it if it doesn't meet the criteria for the site.
GPT models are unsupervised in nature and are (from my understanding) given a prompt and then they either answer the question or continue the sentence/paragraph. They also seem to be the most advanced models for producing natural language, capable of giving outputs with correct syntax and (to my eye at least) indistinguishable from something written by a human (sometimes at least!).
However if I have a problem where I have an input (could be anything, but lets call it an image or video) and a description of the image or video as the output I could in theory train a model with convolutional filters to identify the object and describe the image (assuming any test data is within the bounds of the training data). However when I've seen models like this in the past the language is either quite simple or 'feels' like it's been produced by a machine.
Is there a way to either train a GPT model as a supervised learning model with inputs (of some non language type) and outputs (of sentences/paragraphs); or a similar type of machine learning model that can be used for this task?
A few notes:
I have seen the deep learning image captioning methods - these are what I mention above. I'm more looking for something that can take an input-output pair where the output is text and the input is any form.
","['natural-language-processing', 'models', 'natural-language-understanding', 'text-generation']",
How are Energy Based models really connected to Statistical Mechanics?,"
From statistical mechanics, the Boltzmann distribution over a system's energy states arises from the assumption that many replicas of the system are exchanging energy with each other.  The distribution of these replicas in each energy level is the maximum entropy distribution subject to the constraint that their total energy is fixed, and that any one assignment of energy levels to each replica, a ""microstate"", satisfying this constraint, is equally probable.
From machine learning, the so-called Energy-based model defines a Hamiltonian (energy function) to its various configurations, and uses Boltzmann's distribution to convert an ""energy"" to a probability over these configurations.  Thus, an EBM can model a probability distribution over some data domain.
Is there some viewpoint by which one can interpret the EBM as a ""system"" exchanging energy with many other replicas of that system?  What semantic interpretation of EBMs connects them to the Boltzmann distribution's assumptions?
",['boltzmann-machine'],
How do weak learners become strong in boosting?,"
Boosting refers to a family of algorithms which converts weak learners to strong learners. How does it happen?
","['machine-learning', 'gradient-boosting', 'boosting']","As @desertnaut mentioned in the commentNo weak learner becomes strong; it is the ensemble of the weak learners that turns out to be strongBoosting is an ensemble method that integrates multiple models(called as weak learners) to produce a supermodel (Strong learner).Basically boosting is to train weak learners sequentially,  each trying to correct its predecessor. For boosting, we need to specify a weak model (e.g. regression, shallow decision trees, etc.), and then we try to improve each weak learner to learn something from the data.AdaBoost is a boosting algorithm where a decision tree with a single split is used as a weak learner. Also, we have gradient boosting and XG boosting."
What is the difference between parametric and non-parametric models?,"
A model can be classified as parametric or non-parametric. How are models classified as parametric and non-parametric models? What is the difference between the two approaches?
","['machine-learning', 'comparison', 'definitions', 'models']","A parametric approach (Regression, Linear Support Vector Machines) has a fixed number of parameters and it makes a lot of assumptions about the data. This is because they are used for known data distributions, i.e., it makes a lot of presumptions about the data.A non-parametric approach (k-Nearest Neighbours, Decision Trees) has a flexible number of parameters, there are no presumptions about the data distribution. The model tries to ""explore"" the distribution and thus has a flexible number of parameters.Comparatively speaking, parametric approaches are computationally faster and have more statistical power when compared to non-parametric methods."
Is it legal to license and sell the output of a neural network that was trained on data that you don't own the license to?,"
Is it legal to license and sell the output of a neural network that was trained on data that you don't own the license to?  For example, suppose you trained WaveNet on a collection of popular music. Could you then sell the audio that the WaveNet produces? There are copyright restrictions on using samples to produce music, but the output of a generative neural network might not include any exact replicas from the training data, so it's not clear to me whether those laws apply.
","['neural-networks', 'generative-model', 'legal']","Disclaimer: I am not an attorney and this does not constitute formal legal advice.In this case, almost certainly the human who utilizes the algorithm†.  There was a recent US patent case ""Dabus"" [U.S. Patent Application No.: 16/524,350] where the human programmers tried to claim an AI as an inventor, which was rejected by USPTO.  This is largely because an inventor is defined as a ""natural person"".  But it's an interesting challenge to the notion of inventorship and authorship.An argument regarding output specifically as a salable commodity would be that most human creative endeavor is result of previous work that forms the basis for the novel arrangement of elements in the form of an original work.Music is more specific because melodies are mathematical, and what are traditionally protected, although Blurred Lines case blurred this line, in that the ruling there was the production aspect, the ""feel"", was what was plagiarized.And, in fact, algorithmic music generation, which is heavily utilized in pop music, depends on combinatorial functions producing novel sequences, not protected by prior copyright.Samples are clear cut—it is excerpting from copyrighted work.  A wave form is then copyrightable, in that a work of recorded music is simply an arrangement of waveforms in some combination and sequence.If the output is procedural, and a process that can be automated, that would enter the domain of patent law, utility patents specifically.Design output would also be in the realm of design patents, which are specific/generalized arrangements of elements in products that are not processes.A note on liability:Law is a process, where precedent plays a major factor.  Because law uses natural language, there is ambiguity, and jurisprudence is the process of clarifying the meaning and application via challenges.  Especially when in uncharted territory ""no one knows"" until a case has been ruled on, and rulings can be challenged all the way up to the Supreme Court, which may or may not accept a suit.Damages for copyright are 3x, but damages can be very hard to prove.  Intellectual Property litigation is also enormously expensive, thus unlikely to be pursued unless there is a potential financial benefit.  (The first step in any potential copyright violation is typically a ""cease and desist"" letter, with no legal action if acceded to. In a case such as ""Blurred Lines"", where there is significant financial return to the alleged infringer, the award or settlement presumably exceeds the cost of litigation.)However, deep-pocket players can use litigation, or threat of litigation, to disincentivize competitors, and, if the target has limited resources, can produce the desired outcome regardless of the strength of the claim.  (It's not uncommon in law in general to file frivolous suits as a strategy, although there is typically a financial penalty  such as reimbursement of the defendant's legal fees if the suit is found to be frivolous and dismissed.) ""Patent trolling"" became so much of a problem before 2013, the entire philosophy of patentability, what is patentable, had to be revisited.†Who has rights to an algorithmic process is also a legal question.Software is patentable, being simply a type of computer, regardless of medium, but most software is not patented, and protected instead as ""trade secrets"" and via non-disclosure agreements.  (This may be due to the glacial pace of the patent process compared to the software development process, but also per the from the necessity of making patents public.  i.e. if Google patented their search algorithm, it would be an instruction manual on how to exploit it.)Software is copyrightable as the specific lines of code.  Copyright naturally resides with the creator, but the process of copyright is be utilized to document the right against potential infringement."
Is there a connection between the bias term in a linear regression model and the bias that can lead to under-fitting?,"
Here is a linear regression model
$$y = mx + b,$$
where $b$ is known as $y$-intercept, but also known as the bias [1], $m$ is the slope, and $x$ is the feature vector.
As I understood, in machine learning, there is also the bias that can cause the model to underfit.
So, is there a connection between the bias term $b$ in a linear regression model and the bias that can lead to under-fitting in machine learning?
","['machine-learning', 'linear-regression', 'bias-variance-tradeoff', 'underfitting', 'inductive-bias']","In machine learning, the term bias can refer to at least 2 related conceptsA (learnable) parameter of a model, such as a linear regression model, which allows you to learn a shifted function. For example, in the case of a linear regression model $y = f(x) = mx + b$, the bias $b$ allows you to shift the straight-line up an down: without the bias, you would only be able to control the slope $m$ of the straight-line. Similarly, in a neural network, you can have a neuron that performs a linear combination of the inputs, then it uses a bias term to shift the straight-line, and you could also use the bias after having applied the activation function, but this will have a different effect.Anything that guides the learning algorithm (e.g. gradient descent with back-propagation) towards a specific set of solutions. For example, if you use regularization, you are biasing the learning algorithm to choose, typically, smoother or simpler functions. The bias term in the linear regression model is also a way of biasing the learning algorithm: you assume that the straight-line function does not necessarily go through zero, and this assumption affects the type of functions that you can learn (and this is why these two concepts of bias are related!). So, there are many ways of biasing a learning algorithm.The bias does not always lead you to the best solutions and can actually lead to the wrong solutions, but it is often useful in many ways, for example, it can speed up learning, as you restrict the number of functions that can be learned and searched. The bias (as described in point 2) is often discussed in the context of the bias-variance trade-off, and both the bias and the variance are related to the concepts of generalization, under-fitting, and over-fitting. The linked Wikipedia article explains these concepts quite well and provides an example of how the bias and variance are decomposed, so you should probably read that article for more details."
Why is regret so defined in MABs?,"
Consider a multi-armed bandit(MAB). There are $k$ arms, with reward distributions $R_i$ where $1 \leq i \leq k$. Let $\mu_i$ denote the mean of the $i^{th}$ distribution.
If we run the multi-armed bandit experiment  for $T$ rounds, the ""pseudo regret"" is defined as $$\text{Regret}_T = \sum_{t=1}^T \mu^* - \mu_{it},$$ where $\mu^*$ denotes the highest mean among all the $k$ distributions.
Why is regret defined like this? From what I understand, at time-step $t$, the actual reward received is $r_t \sim R_{it} $ and not $\mu_{it}$ - so shouldn't that be a part of the expression for regret instead?
","['reinforcement-learning', 'definitions', 'rewards', 'multi-armed-bandits', 'regret']","In short, you don't regret your bad luck that you could do nothing about, you regret your bad choices that you could have done something about if only you knew.The point of regret as a metric therefore is to compare your choices with the ideal choices. This makes sense in MABs, because although the primary goal is to gain the most reward, the learning part of the goal is to calculate from experience what are the best choices - usually whilst sacrificing as little as possible in the process.The formula captures that concept, so does not concern itself with individual rewards in the past that could have been due to good or bad luck. Hence it uses expected (or mean) rewards."
Comparing a large/general CNN to a smaller more specialized one?,"
I am still somewhat a novice in the ML world, but I had a strange idea about CNNs and wanted to ask if this would be a valid way to check the robustness of a general CNN that classifies certain images.
Let's say that I make a CNN that takes in many different images of sports players performing a certain action (basketball shot, football kick, freestyle in swimming, flip in gymnastics, etc). Firstly, would it be possible for such a CNN to distinguish between such varied images and classify them accurately? And if so, can it be a good idea to compare this ""larger"" CNN to multiple ""smaller"" more specialized ones that take in images from one particular sport?
In other words, I want to know that if I have a ""larger"" CNN that gives me an output like ""football being kicked"", is there a way to then double-check that output with a smaller CNN that only focuses on football moves? In essence, could we create a  system where once you obtain an output from a general CNN, it automatically classifies the same image through a more specialized CNN, and then if the results are of similar accuracy, you know for sure that CNN works?
Kind of like having a smaller CNN as a ""ground-truth"" for the bigger one? In my head it kind of goes like this:
large_net_output = 'Football kick identified with 95.56% confidence' 

for sport in large_net:
    if sport == 'football':
        access = small_net_for_football
        return small_net_for_football_output

    elif sport == 'swimming':
        access = small_net_for_swimming
        return small_net_for_swimming_output

    elif sport == 'baseball':
        access = small_net_for_baseball
        return small_net_for_baseball_output

# and so on....
>>> small_net_for_football_output = 'Football kick identified with 97.32% confidence'

robustness_check = large_net_output - small_net_for_football_output
print(robustness_check)

>>> 'Your system is accurate within a good range of 1.76%'
     

I hope this makes sense, and that this question does not cause any of your to cringe. Would appreciate any feedback on this!
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'classification']","After reading your question I can relate it to the Representation Learning papers such as SimCLR and SwAV. These models use a ""Big Task agnostic CNN"" to obtain smaller representations of the images and then they train another CNN for classification. I suggest you read Big Self-Supervised Models are Strong Semi-Supervised Learners by Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi and Geoffrey Hinton. The code for the following can be found here. But I feel that training such a model would take up a lot of computational resources."
How does Hartigan & Wong algorithm compare to Lloyd's and Macqueen's algorithm in K-means clustering?,"
As far I know, this is how the latter two algorithms work...
Lloyd's algorithm

Choose the number of clusters.
Choose a distance metric (typically squared euclidean).
Randomly assign each observation to a cluster and compute the cluster centroids.
Iterate below until convergence (i.e. until cluster centroids stop changing):


Assign each observation point to the cluster whose centroid is closest.
Update cluster centroids only after a complete pass through all observations.

Macqueen's Algorithm

Choose the number of clusters.
Choose a distance metric (typically squared euclidean).
Randomly assign each observation to a cluster and compute the cluster centroids.
Perform a complete pass of below (i.e. go through all observations):


Assign an observation to a cluster whose centroid is closest.
Immediately update the centroids for the two affected clusters (i.e. for the cluster that lost an observation and for the cluster that gained it).


Update centroids after a complete pass.

How does the Hartigan & Wong algorithm compare to these two above? I read this paper in an effort to understand but it's still not clear to me. The first three steps is the same as Lloyd's and Macqueen's algorithm (as described above), but then what does the algorithm do? Does it update the centroids as often as Macqueen's algorithm does, or as often as Lloyd's algorithm does? At what point does it take into consideration the within-cluster sum of squares and how does it fit into the algorithm?
I'm generally confused when it comes to this algorithm and would very much appreciate a step-wise explanation as to what's going on.
","['machine-learning', 'algorithm', 'clustering', 'k-means']",
Relation between a value function of an MDP and a value function of the corresponding latent MDP,"
In paper ""DeepMDP: Learning Continuous Latent Space Models for Representation Learning"", Gelada et al. state in the beginning of section 2.4

The degree to which a value function of $\bar{\mathcal M}$, $\bar{V}^{\bar\pi}$ approximates the value function $V^\bar\pi$ of ${\mathcal M}$ will depend on the Lipschitz norm of $\bar V^\bar\pi$ .

where $\mathcal M$ is the Markov Decision Process(MDP) defined in the original state space $\mathcal S$ and $\bar{\mathcal M}$ is the MDP defined in the corresponding latent space $\bar{\mathcal S}$. $\bar\pi$ is a policy defined on the latent space, which can be applied to $\mathcal M$ by first mapping $s\in \mathcal S$ to $\bar{\mathcal S}$.
My question is how they draw the connection between ""The degree to which a value function of $\bar{\mathcal M}$, $\bar{V}^{\bar\pi}$ approximates the value function $V^\bar\pi$ of ${\mathcal M}$"" and "" the Lipschitz norm of $\bar V^\bar\pi$""?
","['reinforcement-learning', 'markov-decision-process']",
What is a wavefront algorithm?,"
I am designing and researching algorithms which I call of a wavefront nature. It is image analsyis agorithms when every pixel may change many times during the processing. I have heard this name before, but it seems it is not used widely. Are there other ""wavefront algorithms""?
","['computer-vision', 'image-processing', 'robotics']",
Why is sine activation function not used frequently since we know from fourier transforms that sine functions can combine to fit any function?,"
Pretty much the title.
I'm no expert but from what I know, if you add up enough sine functions with proper amplitudes and frequencies you can get any function you want as a result. With that knowledge, wouldn't it make sense to have neuron's activation function be a sine function?
","['neural-networks', 'activation-functions']",
Formal definition of the Object Detection problem,"
For many problems in computer science, there is a formal, mathematical problem defition.
Something like: Given ..., the problem is to ...
How can the Object Detection problem (i.e. detecting objects on an image) be formally defined?
Given a set of pixels, the task is to decide

which pixels belong to an object at all,
which pixels belong to the same object.

How can this be put into a formula?
","['computer-vision', 'math', 'object-detection', 'information-theory']","This is just an ideaGiven a set of pixels, the task is to decide:Formula, consider this is a 2D image, call $(x,y)$ is the horizontal and vertical coordinate and $(w_i,h_i)$ is the size of bouding box of object $i$:$\text{For }m \in[x,x+w_i] \text{ and } n\in[y,y+h_i]$$c_i(m,n) = \begin{cases}
    1, \text{if pixel at position (m,n) is belongs to object i,}\\
    0, \text{else}
\end{cases}$"
Estimating $\sigma_i$ according to maximum likelihood method,"
Let be a Bayesian multivariate normal distribution classifier with distinct covariance matrices for each class and isotropic, i.e. with equal values over the entire diagonal and zero otherwise, $\mathbf{\Sigma}_i=\sigma_i^2\mathbf{I},~\forall i$.
How can I compute the equation for estimating the parameter $\sigma_{i}$ by the maximum likelihood method? Here $\sigma_{i,j}$ is is the covariance between $x_i$ and $x_j$. So $\sigma_i$ is just the variance of $x_i$.
Attempt:
Suppose $\mathcal{X}_i = \{x^t_i\}^N_{t=1}$ i.i.d, $x_i^t$ is in the class $C_i$ and $x_i^t \sim \mathcal{N}(\mu, \sigma^2)$.
Do I have to find the log-likelihood under $p(x) = \frac{1}{\sqrt{2 \pi} \sigma} \exp \left[ -\frac{(x-\mu)^2}{2 \sigma^2}\right]$, find the derivative and put it equal to $0$ to find the maximum?
EDIT
Suppose my data points are $m$-dimensional, and I have $K$ classes.
","['classification', 'probability', 'probability-distribution', 'maximum-likelihood']",
How does a neural network that has been trained keep learning while in a real world scenario,"
Say I trained a Neural Network (not RNN or CNN) to classify a particular data set.
So I train using a specific data set & then I test using another and get an accuracy of 95% which is good enough.
I then deploy this model in a production level environment where it will then be processing real world data.
My question is, will this trained NN be constantly learning even in a production scenario? I can't figure out how it will because say it processes a dataset such as this:
[ [1,2,3] ] and gets an output of [ 0, 0.999, 0 ]
In a training scenario it will compare the predicted output to the actual output and back propagate but in a real world scenario it will not know the actual value.
So how does a trained model learn in a real world scenario?
I am still very much a beginner in this field and I am not sure if the technology used is going to affect the answer to this question, but I am hoping to use Eclipse Deeplearning4J to create a NN. That being said the answer does not need to be restricted to this technology in particular as I am hoping more for the theory behind it and how it works.
","['neural-networks', 'deep-learning', 'online-learning', 'incremental-learning']","You are right. If you don't continuously train the neural network after you have deployed it, there is no way it can continuously learn or be updated with more information. You need to program the neural network to learn even after it has been deployed. There is no such thing as a neural network that decides what it does without a human deciding first what it needs to do: this is a very common misconception (probably caused by the media and science fiction movies). It's also true that you need to label your data if you intend to train the neural network in a supervised fashion, but there are other ways to train neural networks (e.g. by reinforcement learning), depending also on the problem you want to solve.If you want to develop neural networks that can learn continually, you probably want to look into continual learning techniques for neural networks. Another term that you may be looking for is online machine learning."
"How does one know that a problem is ""model-free"" in reinforcement learning?","
Consider this slide from a Stanford lecture on reinforcement learning. It states that a model is

the agent's representation of how the world changes in response to the agent's action.

I've been experimenting with Q-learning for simple problems such as OpenAI's FrozenLake and Mountain Car, which both are amenable to the Q-learning framework (the latter upon discretization). I consider the topologies of the lake and the mountain to be the ""worlds"" (aka. environments) in the two cases, respectively.
Q-learning is said to be ""model-free"". Given the two examples above, is it because neither the lake's topology nor that of the mountain are changed by the actions taken?
","['reinforcement-learning', 'q-learning', 'dqn']","Q-learning is said to be ""model-free"". Given the two examples above, is it because neither the lake's topology nor that of the mountain are changed by the actions taken?No. That's not why Q-learning is model-free. Q-learning assumes that the underlying environment (FrozenLake or MountainCar, for example) can be modelled as a Markov decision process (MDP), which is a mathematical model that describes problems where decisions/actions can be taken and the outcomes of those decisions are at least partially stochastic (or random). More precisely, an MDP is composed ofA model-free algorithm is any algorithm that does not use or estimate this $p$. Q-learning, if you look at its pseudocode, does not make use of this model. Q-learning estimates the value function $q(s, a)$ by interacting with the environment (taking actions and receiving rewards), but, meanwhile, it does not know or keep track of the dynamics (i.e. $p$) of the environment, and that's why it's model-free.And, no, the value function is not what we mean by ""model"" in reinforcement learning. The value function is, as the name suggests, a function.How does one know that a problem is ""model-free"" in reinforcement learning?A problem is not model-free or model-based. An algorithm is model-free or model-based. Again, a model-free algorithm does not use or estimate $p$, a model-based one uses (and/or estimates) it.Given the two examples above, is it because neither the lake's topology nor that of the mountain are changed by the actions taken?No. As stated in the other answer, you could apply the model-based algorithm Dyna-Q to these environments."
Can someone explain and help to understand this fuzzy diagram?,"
Could someone help me to understand in detail each step of this fuzzy diagram, because I am lost?

","['machine-learning', 'fuzzy-logic']",
Flatten image using Neural network and matrix transpose,"
I have read a lecture note of Prof. Andrew Ng. There was something about data normalization like how can we flatten an image of (64x64x3) into a (64x64x3)*x1 vector. After that there is pictorial representation of flatten

As per the picture height, length and width of the picture is 64 , 64, 3. I think nx is a row vector which is then transpose to a column vector. If there is 3 pictures I think nx contains {64,64,3,64,64,3,64,64,3}. Am I right?
To use a 64x64x3 image as an input to our neuron, we need to flatten the image into a (64x64x3)x1 vector. And to make Wᵀx + b output a single value z, we need W to be a (64x64x3)x1 vector: (dimension of input)x(dimension of output), and b to be a single value. With N number of images, we can make a matrix X of shape (64x64x3)xN. WᵀX + b outputs Z of shape 1xN containing z’s for every single sample, and by passing Z through a sigmoid function we get final ŷ of shape 1xN that contains predictions for every single sample. We do not have to explicitly create a b of 1xN with the same value copied N times, thanks to Python broadcasting.
As per my understanding, Wᵀ = nx and x= nxᵀ.
Is it Wᵀ= [64,64,3,64,64,3,64,64,3] and x = [64,64,3,64,64,3,64,64,3]ᵀ?
In that case there product will be a symmetry matrix.
Is there any significance of symmetry matrix?
I just messed up all the things while flatten the image. If anyone has any idea please share with me.
Thank you in advance.
","['neural-networks', 'gradient-descent', 'thought-vectors', 'normalisation']","Yes, if you have 3 images (and by images I assume you mean samples) the flatten layer will be of the shape $12288*3$ ($64*64*3=12288$). The size of $W$ however does not change, and nor does the size of $b$ as these are parameters and are independent of the amount of samples passed through the network.ETA: I only answered the ""Am I right?"" part of your question because that's the only part of your questions that's actually a question. I don't know what you're trying to ask in the second half of your question"
Why is the learning rate generally beneath 1?,"
In all examples I've ever seen, the learning rate of an optimisation method is always less than $1$. However, I've never found an explanation as to why this is. In addition to that, there are some cases where having a learning rate bigger than 1 is beneficial, such as in the case of super-convergence.
Why is the learning rate generally less than 1? Specifically, when performing an update on a parameter, why is the gradient generally multiplied by a factor less than 1 (absolutely)?
","['machine-learning', 'optimization', 'gradient-descent', 'learning-rate', 'stochastic-gradient-descent']","If the learning rate is greater than or equal to $1$ the Robbins-Monro condition
$$\sum _{{t=0}}^{{\infty }}a_{t}^{2}<\infty\label{1}\tag{1},$$where $a_t$ is the learning rate at iteration $t$, does not hold (given that a number bigger than $1$ squared becomes a bigger number), so stochastic gradient descent is not generally guaranteed to converge to a minimum [1] (although the condition \ref{1} is a sum from $t=0$ to $t=\infty$, but, of course, we only iterate for a finite number of iterations). Moreover, note that, if the learning rate is bigger than $1$, you are essentially giving more weight to the gradient of the loss function than to the current value of the parameters (you give weight $1$ to the parameters).This is probably the main reason why the learning rate is usually in the range $(0, 1)$ and there are methods to decay the learning rate, which can be beneficial (and there are several explanations of why this is the case [2])."
How to quantify the amount of information lost by the decoder NN in an AE?,"
Is there a way to quantify the amount of information lost in the lossy part of an autoencoder where the original input is compressed to a representation with less degrees of freedom?
I was thinking maybe to use somehow the mutual information either in the image or frequency domain.
$$
\mathrm{I}(X ; Y)=\sum_{y \in \mathcal{Y}} \sum_{x \in \mathcal{X}} p_{(X, Y)}(x, y) \log \left(\frac{p_{(X, Y)}(x, y)}{p_{X}(x) p_{Y}(y)}\right)
$$
where $p_{(X,Y)}$ is the joint probability density function which is deduced somehow empirically from a set of $N$ input and output to the network.
Maybe it's not even an interesting question since the loss function evaluates exactly that?

",['autoencoders'],
How parameter adjustment works in Gradient Descent?,"
I am trying to comprehend how the Gradient Descent works.
I understand we have a cost function which is defined in terms of the following parameters,
$J(𝑤_{1},𝑤_{2},.... , w_{n}, b)$
the derivative would tell us which direction to adjust the parameters.
i.e. $\dfrac{dJ(𝑤_{1},𝑤_{2},.... , w_{n}, b)}{d(𝑤_{1}}$ is the rate of change of the cost w.r.t $𝑤$
The lecture kept on saying this very valuable as we are asking the question how should I change $𝑤$ to improve the cost?
But then the Lecturer presented $w_{1}$, $w_{2}$, ... as scaler value, How can we differentiate a scalar value.
I am fundamentally missing what is happening.
Can anyone please guide me to any blog post, a book that I should read to understand better?
","['backpropagation', 'gradient-descent', 'linear-regression']",
Is this ML task possible?,"
What I want to do is from an Internet challenge to transform any given image into the Polish flag using the available filters and crop tool on the iPhone camera app. Here's an example.
There aren't nearly enough of these videos to train a neural network using a labeled dataset, and (while I haven't ruled it out) I don't think automatically inserting a polish flag into an image then adding random filters to it to create my own dataset would work out.
My thinking is that I would feed a neural network the image and it would output a value for each filter & cropping coordinates. Then, I could easily calculate the loss by comparing the resulting picture to a picture of the polish flag. The obvious problem here is that you don't know how each of the neurons in the last layer affects the loss so you can't perform back propagation.
Is my best bet to mathematically calculate the loss (by this I mean as opposed to using high level libraries, which would be difficult but I'm sure it's possible) so I can find the partial derivative of each last layer neuron with respect to the loss function and then backpropagate? Would this even work? Are there any alternatives that you recommend?
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'deep-learning']",
What are trap functions in genetic algorithms? [duplicate],"







This question already has answers here:
                                
                            




What is a trap function in the context of a genetic algorithm?

                                (2 answers)
                            

Closed 2 years ago.



What are trap functions in genetic algorithms? Suppose you ran a GA with a trap function and examined the population midway through the run. Can someone explain what you would expect the population to look like?
","['genetic-algorithms', 'evolutionary-algorithms', 'trap-functions']","Traps are functions that are designed to have a very obvious gradient that leads to basically the second-best solution, with the best one being very far removed from that, often the complement of the second-best.Take the 1-max problem. You have N bits, and the fitness of each individual is just the number of 1 bits in the string. If you run a GA on that, you'd expect to see an initial random distribution of 1s and 0s across the population fairly quickly start to converge to strings of all 1s. Now let's add one more clause to the fitness criteria. Instead of f(0)=0, let f(0)=N+1. This gives you a trap function. The ""trap"" is the solution of all 1s. Everything in the search space points to this being the best solution, but it's actually maximally far away from the best solution.In terms of dyanmics, now the optimal solution is the string of all 0s. But unless you happen to very quickly stumble across that string, you likely never will. That is, there's some slight chance that your initial population included 00001111 and 11110000 and you crossed them over in the middle and got the optimum. Or maybe you had 00001000 and you got lucky and mutated that one bit to a zero. But if you don't do that immediately, you're screwed, because the algorithm is going to pretty relentless drive all the zero bits out of the population. No matter what it does, flipping a 0 to a 1 is always better unless it would otherwise lead to the single string of all 0s, so there's constant pressure to find more and more 1 bits. A few generations in, you might have 11110110, but you're never realistically going to randomly mutate all six of those 1s to 0s. And you have to get it all in one shot. Any subset of fewer than six of those bits being flipped will have worse fitness than you started with, and be selected against."
What's the threshold to call something 'machine learning'?,"
For example, if I use some iterative solvers to find a solution to a non-linear least squares problem, is that already considered machine learning?
","['machine-learning', 'definitions']",
Why one unit in the layers of neural network is not enough?,"
In a deep connected network, when every unit gets all the input features(X) so it has one parameter for every feature and every unit tweaks its parameters for loss optimization. What if we use only one unit and that one unit will have all the parameters which it can tweak for loss optimization. Is there a reason or benefit of using multiple units in every layer except the output layer?
","['neural-networks', 'deep-learning', 'artificial-neuron', 'hyper-parameters']",
How do you make a regression model from a binary labeled dataset?,"
Suppose I have a dataset with hand images. Hand completely opened is labeled as 0 and hand completely closed (fist) are labeled as 1. I also have a bunch of unlabeled images of hands which, if properly labeled would have values between 0 and 1. Because they are not completely opened and not completely closed.


Extra info I have is the ordering between all the pairs of unlabeled images. For example, given image A and B, I can tell you which image should be predicted with higher value, but I cannot tell you what exactly is the value. The unlabeled dataset is collected by recording a video of a closing hand from completely opened to completely closed.
What are some machine learning techniques that I can use to give the label or predict the values of hands not completely closed and not completely opened? I expect it to be based on ordering or ranking system. If it doesn't even require the ordering label (A > B?) then that would be a very smart algorithm.
I want the values between 0 and 1. If there's a code for it that would be a plus. Thank you.
","['machine-learning', 'deep-learning', 'classification', 'computer-vision', 'regression']",
Is it possible to classify the subject of a conversation?,"
I would like to classify the subject of a conversation. I could classify each messages of the conversation, but I will loose some imformation because of related messages.
I also need to do it gradually and not at the end of the conversation.
I searched near recurrent neural network and connectionist classification but I'm not sure it answer really well my issue.
","['natural-language-processing', 'recurrent-neural-networks', 'text-summarization', 'natural-language-understanding']","Thank you very much for your help, all of you.I finally find on the Internet key words : ""Dialog act classification"".I don't know yet how to implement it, but it's a good start !"
Do we need non-linear activation function in neural networks whose task isn't classification?,"
While researching why we need non linear activation functions, all the explanations revolve around neural network being able to separate values that aren't linearly separable.
So I wonder, if we have a neural network whose task is something else, say predicting an output value of a time series, is it still important to have an activation function that is non linear?
","['neural-networks', 'classification', 'activation-functions', 'time-series']",
Deep learning based physics engine,"
Ridgid body simulation is a well known field with well established methods. It's still fairly computationally expensive to simulate things.
I am interested in approaches to training deep learning networks to predict rigid body dynamics and interactions to reduce the computational load associated with simulations.
Has this been done before and what approaches have been used?
",['deep-learning'],
Mapping given probabilities to empirical probabilities,"
Consider following problem statement:

You have given $n$ actions. You can perform any of them. Each action gives you success with some probability. The challenge is to perform given finite number of actions to get maximum successes.

Here, we can perform actions and slowly decide upon possible probabilities of each action for success. I have no doubts in this problem.
Now consider following variant of the problem:

You have given $n$ actions. You can perform any of them. Each action gives you success with some probability. Also you are given set of $n$ probabilities, but you are not told which probability is associated with which action. The challenge is to utilise this additional information to perform given finite number of actions to get maximum successes.

I have doubt in this problem that how we can map probabilities to actions? I can do some enough number of actions to gather empirical probability and them try to associate given probabilties with actions having closest empirical probabities. But, is there any algorithm for such problem in the literature?
","['reinforcement-learning', 'math', 'statistical-ai', 'multi-armed-bandits']",
What would be a typical pre-processing and data normalization pipeline for time series data (for non-linear models such as neural networks)?,"
I've started to work on time series. I was wondering what would be the best data normalizing and pre-processing technique for non-linear models, specifically, neural networks.
One I can think of is min-max normalization
$$z = \frac{x - min(x)}{max(x) - min(x)}$$
","['neural-networks', 'machine-learning', 'time-series', 'data-preprocessing', 'normalisation']",
Does replacing 3x3 filters with 3x1 and 1x3 filters improve the performance?,"
Recently I have come up with a VGG16 model for my binary classification task. I have relatively simple signal images

Therefore (maybe?) other deeper models like resnet18 and Inceptionv3 were not as good. As known, VGG uses 3x3 filters for convolving the images to make feature maps. I have tried several hyper-parameters to get a desired performance. However, there are still some things I need to do. I was thinking of replacing the 3x3 conv filters with 3x1 followed by 1x3 filters to reduce the compute. I think it will definitely do so considering the multiplications (9 operations for 3x3and 6 for 3x1 followed by 1x3).
Then I came to think: If I replace all the 3x3 filters with separable filters, will I get any performance improvement?
What are the benefits of replacing 3x3 filters with separable ones?
Thanks
","['deep-learning', 'filters', 'vgg']",
How to implement or avoid masking for transformer?,"
When it comes to using Transformers for image captioning is there any reason to use masking?
I currently have a resnet101 encoder and am trying to use the features as the input for a transformer model in order to generate a caption for the image, is there any need to use masking? and what would I mask if I did need to?
Any help would be much appreciated
Thanks in advance.
","['convolutional-neural-networks', 'natural-language-processing', 'python', 'pytorch', 'transformer']",
What are the strategies for computationally heavy environments or long-time waiting environments?,"
I have an environment that is computationally heavy (takes several seconds to get a reward and next state). This limits reinforcement capability, due to poor sampling of the problem. There is any strategy that could be used to address the problem (e.g. If I can use the environment in parallel, then I could use a multi-agent approach)
","['reinforcement-learning', 'environment']",
Is it possible to express attention as a Fourier convolution?,"
Convolutions can be expressed as a matrix-multiplication (see e.g. this post) and as an element-wise multiplication using the Fourier domain (https://en.wikipedia.org/wiki/Convolution_theorem).
Attention utilizes matrix multiplications, and is as such $O(n^2)$. So, my question is, is it possible to exploit the Fourier domain for attention mechanisms by turning the matrix multiplication of attention into a large convolution between the query and the key matrices?
","['convolution', 'attention']",
What is the time complexity for training a single-hidden layer auto-encoder?,"
What is the time complexity for training a single-hidden layer auto-encoder, for 1 epoch?
You can assume that there are $n$ training examples, $m$ features, and $k$ neurons in the hidden layer, and that we use gradient descent and back-propagation to train the auto-encoder.
","['training', 'autoencoders', 'time-complexity']",
How much overfitting is acceptable?,"
I have a deep learning configuration in which I obtain good results on the validation set but even better results in the training set. From my understanding this means that there is overfitting to some extent. What does this mean in practice? Does it mean that my model is not good and that I should not use it? If I decrease the gap between the validation and training accuracy (decreasing the overfitting) but at the same time decrease the validation accuracy, which of the two models is better?
Below are some images to illustrate the two situations outlined previously:


","['machine-learning', 'overfitting']",
What is the goal of weight initialization in neural networks?,"
This is a simple question. I know the weights in a neural network can be initialized in many different ways like: random uniform distribution, normal distribution, and Xavier initialization. But what is the weight initialization trying to achieve?
Is it trying to allow the gradients to be large so it can quickly converge? Is it trying to make sure there is no symmetry in the gradients? Is it trying to make the outputs as random as possible to learn more from the loss function? Is it only trying to prevent exploding and vanishing gradients? Is it more about speed or finding a global maximum? What would the perfect weights (without being learned parameters) for a problem achieve? What makes them perfect? What are the properties in an initialization that makes the network learn faster?
","['neural-networks', 'machine-learning', 'gradient-descent', 'weights']","The aim of weight initialization is to make sure that we don't converge to a trivial solution. That's why we have different kinds of initialization depending on the dataset type. So, Yes it is trying to avoid symmetry.The time it takes to converge, is I think a property of the optimizer and not of the weights initialization. Of course, the manner in which we initialize our weights matters but I think Optimization Algorithms contribute more towards convergenceGlorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network. Incidentally, when they trained deeper networks that used ReLUs, it was found that a 30-layer CNN using Xavier initialization stalled completely and didn’t learn at all. Thus, it depends on the particular problem at hand."
Estimating an $n$-Gram model using on bigrams,"
One of the main arguments against $n$-gram models is that, as $n$ increases, there is no way to compute $P(w_n|w_1,\cdots,w_{n-1})$ from training data (since the chance of visiting $w_n,...,w_1$ is practically zero).
Wondering why we cannot estimate $P(w_n|w_1,\cdots,w_{n-1})$ using the following:
Let $P_i(u|v)$ be the probability of having sequences where word $u$ comes exactly $i$ words after word $v$ (This is easy to compute).
Then we can esitmate $P(w_n|w_1,\cdots,w_{n-1})$ as a function of $P_i(u|v)$. I could not find any reference to such approach in the literature. The most similar approach is the smoothing/backoff methods.
Is there any reason why no-one used this approach? Or if one can share some previous work about this approach.
P.S.1. The disadvantage of this approach, comparing with standard $n$-gram model, is its running time.
P.S.2. We could use bucketing idea: Instead of computing/storing/using $P_i$, for every $i$, we can compute/store/use $PB_{i}=P_{2^i}$ . Then $P_i(u|v) \approx PB_{\log i}(u|v)$.
","['machine-learning', 'natural-language-processing']",
What are some good models to use for spelling corrections?,"
I used OCR to extract text from an image, but there are some spelling mistakes in it :
The text is as follows :
'gaRBOMATED WATER\n\nSFMEETENED CARBONATED 6\nBSREDERTS: CARBONATED WATER,\nSUGAR. ACIOITY REGULATOR (338),\n\nCFFENE. CONTAINS PERMITTED NATURAL\nCOLOUR (1506) AMD ADDED FLAVOURS QUcTURAL,\nSATIRE: OENTICAL AND ARTIFICIAL PLIVOUREE\n\nCOLA\nl 1187.3 PIRANGUT, TAL. MULSHI,\nGBST. PUME 612111, MAHARASHTRA.\nHELPLINE: 1800- 180-2653\ntet indishetptine@cocs-cola.com\nAUTHORITY OF THE COCA-COLA\n‘COCA-COLA PLAZA, ATLANTA, GA 36313, USA\nme DATE OF MANUFACTURE. BATCH NO. &\nLP CNL. OF ae TAXES}:\nSE BOTTOM OF CAN.\n\nTST Fone Sor MOTHS FROM\nWe, RE WHEN STORED ft.\n\nY PLACE.\nChe coca conn\nnee\n\n| BRA License uo:\n‘ eS wo:\n\n \n\x0c'

I would like to know if there are some NLP models/libraries that I can use to correct spelling mistakes(like correcting gaRBOMATED to CARBONATED
","['neural-networks', 'machine-learning', 'deep-learning', 'natural-language-processing']",
"What is the meaning of ""exploration"" in reinforcement and supervised learning?","
While exploration is an integral part of reinforcement learning (RL), it does not pertain to supervised learning (SL) since the latter is already provided with the data set from the start.
That said, can't hyperparameter optimization (HO) in SL be considered as exploration? The more I think about this the more I'm confused as to what exploration really means. If it means exploring the environment in RL and exploring the model configurations via HO in SL, isn't its end goal ""mathematically"" identical in both cases?
","['reinforcement-learning', 'terminology', 'supervised-learning', 'hyperparameter-optimization', 'exploration-exploitation-tradeoff']","In reinforcement learning, exploration has a specific meaning, which is in contrast with the meaning of exploitation, hence the so-called exploration-exploitation dilemma (or trade-off). You explore when you decide to visit states that you have not yet visited or to take actions you have not yet taken. On the other hand, you exploit when you decide to take actions that you have already taken and you know how much reward you can get. It's like in life: maybe you like cereals $A$, but you never tried cereals $B$, which could be tastier. What are you going to do: continue to eat cereals $A$ (exploitation) or maybe try once $B$ (exploration)? Maybe cereals $B$ are as tasty as $A$, but, in the long run, $B$ are healthier than $A$.More concretely, recall that, in RL, the goal is to collect as much reward as you can. Let's suppose that you are in state $s$ and, in the past, when you were in that state $s$, you had already taken the action $a_1$, but not the other actions $a_2, a_3$ and $a_4$. The last time you took action $a_1$, you received a reward of $1$, which is a good thing, but what if you take action $a_2, a_3$ or $a_4$? Maybe you will get a higher reward, for example, $10$, which is better. So, you need to decide whether to choose again action $a_1$ (i.e. whether to exploit your current knowledge) or try another action that may lead to a higher (or smaller) reward (i.e. you explore the environment). The problem with exploration is that you don't know what's going to happen, i.e. you are risking if you already get a nice amount of reward if you take an action already taken, but sometimes exploration is the best thing to do, given that maybe the actions you have taken so far have not led to any good reward.In hyper-parameter optimization, you do not need to collect any reward, unless you formulate your problem as a reinforcement learning problem (which is possible). The goal is to find the best set of hyper-parameters (e.g. the number of layers and neurons in each layer of the neural network) that performs well, typically, on the validation dataset. Once you have found a set of hyper-parameters, you usually do not talk about exploiting it, in the sense that you will not continually receive any type of reward if you use that set of hyper-parameters, unless you conceptually decide that this is the case, i.e., whenever you use that set of hyper-parameters you are exploiting that model to get good performance on the test sets that you have. You could also say that when you are searching for new sets of hyper-parameters you are exploring the search space, but, again, the distinction between exploitation and exploitation, in this case, is typically not made, but you can well talk about it.It makes sense to talk about the exploitation-exploration trade-off when there is stochasticity involved, but in the case of the hyper-parameter optimization there may not be such a stochasticity, but it's usually a deterministic search, which you can, if you like, call exploration."
Can we apply transfer learning between any two different CNN architectures?,"
There are many types of CNN architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet, etc. Can we apply transfer learning between any two different CNN architectures? For instance, can we apply transfer learning from AlexNet to GoogLeNet, etc.? Or even just from a ""conventional"" CNN to one of these other architectures, or the other way around? Is this possible in general?
EDIT: My understanding is that all machine learning models have the ability to perform transfer learning. If this is true, then I guess the question is, as I said, whether we can transfer between two different CNN architectures – for instance, what was learned by a conventional CNN to a different CNN architecture.
","['convolutional-neural-networks', 'transfer-learning']","No, transfer learning cannot be applied ""between"" different architectures, as transfer learning is the practice of taking a neural network that has already been trained on one task and retraining it on another task with the same input modality, which means that only the weights (and other trainable parameters) of the network change during transfer learning but not the architecture.In my understanding, transfer learning is also only really effective in deep learning, but I could be wrong, considering that this Google search seems to yield some results.You might otherwise be thinking of knowledge distillation, which is a related but different concept, where an already trained network acts as a teacher and teaches another network (a student network) with possibly a different architecture (or a machine learning model not based on neural networks at all) the correct outputs for a bunch of input examples."
How many types of variational auto-encoders are there?,"
I have been studying about auto-encoders and variational auto-encoders. I would like to know how many variants of VAEs are there today.
If there are many variants, can they be used for feature extraction for complex reinforcement learning tasks like self-driving cars?
","['reinforcement-learning', 'reference-request', 'variational-autoencoder']",
What are proxy reward functions?,"
The understanding I have is that they somehow adjust the objective to make it easier to meet, without changing the reward function.

... the observed proxy reward function is the approximate solution to a reward design problem

(source: Inverse Reward Design)
But I have trouble getting how they fit the overall reward objective and got confused by some examples of them.
I had the idea of them being small reward functions (as in the case of solving for sparse rewards) eventually leading to the main goal. But the statement below, from this post, made me question that.

Typical examples of proxy reward functions include “partial credit” for behaviors that look promising; artificially high discount rates and careful reward shaping;...


What are they, and how would one go about identifying and integrating proxy rewards in an RL problem?

In the examples above, how would high discount rates form a proxy reward?


I'm also curious about how they are used as a source of multiple rewards
","['reinforcement-learning', 'definitions', 'papers', 'reward-functions']","In the paper that you cite, Inverse Reward Design (2017), the authors actually define what they mean by ""proxy reward function"".We formalize this in a probabilistic model that relates the proxy (designed) reward to the true rewardSo, the proxy reward function is the reward function designed by the human, which may not necessarily be the reward function that he/she intended (i.e. it may be a misspecified reward function), given that the human may have forgotten to model/incorporate certain (unpredicted by the human) scenarios or situations that the agent may face. This usage of the word ""proxy"" is thus consistent with the general usage of the word in computer science, i.e. a ""proxy reward function"" is a reward function that is used instead of the intended (optimal) reward function."
"Is it possible to have a negative output using only ReLU activation functions, but not in the final layer?","
I know that if you use an ReLU activation function at a node in the neural network, the output of that node will be non-negative. I am wondering if it is possible to have a negative output in the final layer, provided that you do not use any activation functions in the final layer, and all the activation functions in the previous hidden layers are ReLU?
","['neural-networks', 'relu']",
DQN not learning and step not stepping towards target,"
I am trying to create a simple Deep Q-Network with 2d convolutional layers.
I can't figure out what I am doing wrong, and the only thing I can see that doesn't seem right is when I get the model prediction for a state after the optimizer step it doesn’t seem to get closer to the target.
I am using pixels from pong in OpenAI's gym with single-channel 90x90 images, a batch size of 32, and replay memory.
As an example, if I try with a batch size of 1, and try running self(states) again right after the optimizer step the output is as follows:
current_q_values -> -0.16351485  0.29163417  0.11192469 -0.08969332  0.11081569  0.37215832
q_target ->         -0.16351485  0.5336551   0.11192469 -0.08969332  0.11081569  0.37215832
self(states) ->     -0.8427617   0.6415581   0.44988257 -0.43897176  0.8693738   0.40007943

Does this look as what would be expected for a single step?
The network with loss and optimizer:
    self.in_layer = Conv2d(channels, 32, 8)
    self.hidden_conv_1 = Conv2d(32, 64, 4)
    self.hidden_conv_2 = Conv2d(64, 128, 3)
    self.hidden_fc1 = Linear(128 * 78 * 78, 64)
    self.hidden_fc2 = Linear(64, 32)
    self.output = Linear(32, action_space)

    self.loss = torch.nn.MSELoss()
    self.optimizer = torch.optim.Adam(
        self.parameters(), lr=learning_rate) # lr is 0.001

def forward(self, state):
    in_out = fn.relu(self.in_layer(state))
    in_out = fn.relu(self.hidden_conv_1(in_out))
    in_out = fn.relu(self.hidden_conv_2(in_out))
    in_out = in_out.view(-1, 128 * 78 * 78)
    in_out = fn.relu(self.hidden_fc1(in_out))
    in_out = fn.relu(self.hidden_fc2(in_out))
    return self.output(in_out)

Then the learning block:
        self.optimizer.zero_grad()

        sample = self.sample(self.batch_size)
        states = torch.stack([i[0] for i in sample])
        actions = torch.tensor([i[1] for i in sample], device=device)
        rewards = torch.tensor([i[2] for i in sample], dtype=torch.float32, device=device)
        next_states = torch.stack([i[3] for i in sample])
        dones = torch.tensor([i[4] for i in sample], dtype=torch.uint8, device=device)

        current_q_vals = self(states)
        next_q_vals = self(next_states)
        q_target = current_q_vals.clone()
        q_target[torch.arange(states.size()[0]), actions] = rewards + (self.gamma * next_q_vals.max(dim=1)[0]) * (~dones).float()

        loss = fn.smooth_l1_loss(current_q_vals, q_target)
        loss.backward()

        self.optimizer.step()
```

","['reinforcement-learning', 'q-learning', 'dqn', 'pytorch', 'convolutional-layers']",
Is there a UCB type algorithm for linear stochastic bandit with lasso regression?,"
Why is there no upper confidence bound algorithm for linear stochastic bandits that uses lasso regression in the case that the regression parameters are sparse in the features?
In particular, I don't understand what is hard about lasso regression that makes it hard to be used in a UCB type algorithm whereas there is a lot of work on ridge regression based UCB algorithms see e.g. Yadkori et al.
I looked up some works e.g. Bastani and Bayati, Kim and Paik but they all do not a UCB-type algorithm, instead, they propose forced or probabilistic sampling to satisfy the compatibility condition (see Lemma EC.6. of Bastani and Bayati).
","['machine-learning', 'reinforcement-learning', 'linear-regression', 'multi-armed-bandits', 'contextual-bandits']",
What is the search depth of AlphaGo and AlphaGo Zero?,"
I cannot find reliable sources but someone says it is 40 moves and someone else says it is 50+ moves. I read their papers and they use value function (NN) and policy function to trim the tree, so more layers can be searched while spending less time searching less different positions.
My question is, is the search depth a fixed preset parameter? If so, approximately how much is it back to 2016 (AlphaGo) and 2018 (AlphaGo Zero)?
","['neural-networks', 'reinforcement-learning', 'monte-carlo-tree-search', 'alphago-zero', 'alphago']",
How much can an inclusion of the number of iterations have on the training of an MLP?,"
My doubt is like this :

Suppose we have an MLP. In an MLP, as per the backprop algorithm (back-propagation algorithm), the correction applied to each weight is :

$$ w_{ij} := -\eta\frac{\partial E}{\partial w_{ij}}$$
($\eta$ = learning rate, $E$ = error in the output, $w_{ij}$ = $i^{\text{th}}$ neuron in the $j^{\text{th}}$ row or layer)

Now, if we put an extra factor in the correction as:

$$ w_{ij} := -k\eta  \frac{\partial E}{\partial w_{ij}}$$ ($k$ denotes the number of iterations at the time of correction)

how much will that factor affect the learning of the network ? Will it affect the convergence of the  network such that it takes time to fit to the data ?

NB : I am only asking this as a doubt. I haven't tried any ML projects recently, so this is not related to anything I am doing.

","['neural-networks', 'backpropagation', 'convergence']","If anything, you want the learning rate to decrease as the number of iterations increases.When you're looking for a good spot and you're clueless, take large steps. When you've found a pretty good spot, take small steps, so you don't end up far away.In other fields of machine learning, there are studies of how the learning rate should scale. For example, in traditional reinforcement learning methods, if $\alpha_i$ is the learning rate at step $i$, then we want to have the following two criteria, to make sure we get convergence to the optimal policy:A typical choice here is $\alpha_i = \frac{1}{1+i}$, which fits both criteria.I am unaware of similar criteria for MLPs, but if you're going to modify the step sizes, I would follow a similar approach. Make the step sizes decrease, but not too fast."
Which loss function to choose for imbalanced datasets?,"
For imbalanced datasets (either in the context of computer vision or NLP), from what I learned, it is good to use a weighted log loss. However, in competitions, the people who are in top positions are not using weighted loss functions, but treating the classification problem as a regression problem, and using MSE as the loss function. I want to know which one should I use for imbalanced datasets? Or maybe should I combine both?
the weighted loss I am talking is:: 
neg_weights=[]
pos_weights=[]
for i in tqdm(range(5)):##range(num_classes)
    neg_weights.append(np.sum(y_train[:,i],axis=0)/y_train.shape[0])
    pos_weights.append(np.sum(1-y_train[:,i],axis=0)/y_train.shape[0])
def customloss(y_true,y_pred):
    y_true=tf.cast(y_true,dtype=y_pred.dtype)
    loss=0.0
    loss_pos=0.0
    loss_neg=0.0
    for i in range(5):
        loss_pos+=-1*(K.mean(pos_weights[i]*y_true[:,i]*K.log(y_pred[:,i]+1e-8)))
        loss_neg+=-1*(K.mean(neg_weights[i]*(1-y_true[:,i])*K.log(1-y_pred[:,i]+1e-8)))
    loss=loss_pos+loss_neg
    return loss

the competition I was talking about is https://www.kaggle.com/c/aptos2019-blindness-detection/discussion/109594
","['deep-learning', 'datasets', 'objective-functions']",
Multiple Inertia sensors system based for gestures recognition,"
I am a newbie to Machine Learning field as I am engaging to a personal project that I am trying to use the 6 degree of freedom Inertial Measurement Units(IMUs) measuring the Acceleration acting on 3 axes(x-y-z) and the Angular velocity around the same 3 axis(x-y-z). One sensor generates a set of 6 raw variables of: Acc_x, Acc_y, Acc_z, Gyro_x, Gyro_y, Gyro_z.
Initially I have 2 of those sensors that used to be attached on to the arm (one to the part above the elbow and one to the part bellow the elbow) together they spit out a dataset of 12 raw variables that represent a specific movement of the arm, I save them as a the csv file. This is the point where I really get overwhelmed with a huge amount of data that I don't know how to process this kind of data and extract the features to differentiate the gestures.
My dataset of the first movement I recorded looks like this:

I denoted 1 for the first sensor above the elbow and 2 for the sensor below the elbow.
Looking forward to hearing the opinions from the experts and seniors on this.
Thank you in advanced.
Let me know if my question is inappropriate and lack of information as it is my first time.
","['machine-learning', 'data-preprocessing', 'supervised-learning']",
"Would it be possible to use AI to measure pupil dilation diameters and fluctuation, on video films on a regular webcam?","
I've been researching the topic of Cognitive Load Measurement through pupil dilation measurement. All solutions to pupil dilation measurement require some kind of special hardware setup. I was wondering if it would be possible to use AI on a regular webcam record and do those measurements later. If yes, I'd love some pointers to resources of what I need to know to be able to implement it.
","['computer-vision', 'image-recognition', 'image-processing']",
What framework for a project with a custom environment? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I'm planning an RL project and I have to decide which RL framework do I use if any at all. The project has a highly custom environment, and testing different algorithms will be required to obtain optimal results. Furthermore, it will use a custom neural network, not implemented in the popular TensorFlow/PyTorch ML frameworks. Therefore, the framework should allow for customization with regard to approximation function (1) and the environment (2). The problem is that to my current knowledge, most of the framework allows only to work with a built-in environment. Does anybody know a framework that meets the two conditions (1) and (2)? Or anybody knows a review that contains information about framework in the context of those conditions?
","['reinforcement-learning', 'python']",
Enforcing sparsity constraints that make use of spatial contiguity,"
I have a deep learning network that outputs grayscale image reconstructions. In addition to good reconstruction performance (measured through mean squared error or some other measure like psnr), I want to encourage these outputs to be sparse through a regularization term in the loss function.
One way to do this is to add an L1 regularization term that penalizes the sum of the absolute value of pixel intensities. While this is a good start, is there any penalization that take adjacency and spatial contiguity into account? It doesn't have to be a commonly used constraint/regularization term, but even potential concepts or papers that go in this direction would be extremely helpful. In natural images, sparse pixels tend to form regions or patches as opposed to being dispersed or scattered. Are there ways to encourage regions of contiguous pixels to be sparse as opposed to individual pixels?
","['deep-learning', 'objective-functions', 'image-processing', 'autoencoders', 'regularization']",
What are most commons methods to measure improvement rate in a meta-heuristic?,"
When I run a meta-heuristics, like a Genetic Algorithm or a Simulated Annealing, I want to have a termination criterion that stops the algorithms when there is not any significant fitness improvement.
What are good methods for that?
I tried something like
$$improvement=\frac{fit(Solution_{new})}{fit(Solution_{old})}$$
and
$$improvement={fit(Solution_{new})}-{fit(Solution_{old})}$$
Both options don't seem to be good, because as the old solutions get better and newer solutions even if they are good don't improve so much compare to the old.
","['genetic-algorithms', 'simulated-annealing', 'meta-heuristics', 'stopping-conditions', 'numerical-algorithms']",
Should I remove the text overlaying some images in the dataset before training the CNN?,"
If I am attempting to train a CNN on some image data to perform image classification, but some of the images have pieces of text overlaying them (for the purpose of description to humans), then is it better for the CNN to remove the text? And if so, then how do I remove the text? Furthermore, is it a good idea to use both the images with text overlaying them and the images with the removed text for training, since it might act as a form of data augmentation?
","['convolutional-neural-networks', 'image-processing', 'data-preprocessing', 'data-augmentation', 'image-recognition']","Removing the overlayed text might increase accuracy, but you'd need to train a different model to do this, and that is an entirely different task as it is no longer classification, but generation. There are easier ways to augment your data and probably get similar benefits to your accuracy. However, if you would still like to do this, there is a lot of examples you can find by simple searching ""Watermark removal machine learning"" in google. Here's an example I found.Overall, a CNN will be able to look past the overlayed text without issue, and perform classification like it would without the overlayed text. There is the possibility that it actually learns relationships between the overlayed text and the expected output, but that depends on the data, and is likely a harder task then simply identifying features.The only issue you might run into is if the real data this model will be used on is different to the data provided, as in the real word images do not contain overlayed text describing what the image is."
Literature on computational modelling involving neuronal ensemblies,"
Straying from the current trends in deep learning, there is an, arguably, interesting idea of neuronal ensembles possibly providing an alternative to the current ""layered feature detectors"" framework for neural network construction by being considered a basic computational unit instead of one feature detecting neuron. This idea certainly has at least some presence in neuroscience circles, but I found it hard to find studies, attempting to obtain a working computational ensemble based model, which would try to solve any of the existing computer vision/NLP tasks or anything of the sort. This may be just due to me looking in the wrong places, but in any case, I would appreciate any references to papers, exploring building neural network architectures with neuronal ensemblies involvement.
Just to be clear, I would be interested in any papers on computational modelling of ensemblies even if they are not trying to solve any particular ML task, but it would be better, if the topic of the research is more closely aligned with computer science instead of neurobiology even if CS connection is of a more exotic kind; for example, paper trying to see, whether you can store different concepts and their relations in the ensemble based network is more desirable than paper, trying to accurately model individual neuron and synaptic plasticity dynamics and see, that ensembles emerge, if you scale the system. But again, I would be glad to get references to research in both of these example topics and many more.
","['neural-networks', 'machine-learning', 'recurrent-neural-networks', 'reference-request', 'models']",
Does it make sense to train images (for object detection algorithms) with cameras that will not be used to collect future data?,"
I am training an algorithm to identify weeds within crops using the YOLOv5 algorithm. This algorithm will  be used in the future to identify weeds in images collected by unmanned aircraft (drones) after making an orthomosaic images. Using the open-source LabelImg software, I am labeling images for object detection that were collected with both UAV and hand-held digital cameras. Using both platforms, I collected many images of weeds that will need to be identified.
My question is this: Does it make sense to collect training samples from the hand-held digital camera, since it will be of much higher resolution than the UAV imagery (and thus not used for future imagery collections after the model is trained)? My initial thought is that it would be best to only use the UAV imagery, since it will be the most similar to what will be collected in the future. However, I do not want to throw out the hand-held digital imagery if it could help in the image classification process.
","['deep-learning', 'training']","I think this can only be used for pretraining/some kind of transfer learning. This would be useful if the ratio of real training data to pretraining data is really low. You could then pretrain on digital data, and fine-tune on your UAV data.How much of this is really useful I can't really say, this depends on how close the digital data is to UAV data. If it is significantly different, you are training on a different distribution and sample space than your UAV images, which is pointless."
"Which loss function should I use in REINFORCE, and what are the labels?","
I understand that this is the update for the parameters of a policy in REINFORCE:
$$
\Delta \theta_{t}=\alpha \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) v_{t},
$$
where $v_t$  is usually the discounted future reward and  $\pi_{\theta}\left(a_{t} \mid s_{t}\right)$ is the probability of taken the action that the agent took at time $t$. (Tell me if something is wrong here)
However, I don't understand how to implement this with a neural network.
Let's say that probs = policy.feedforward(state) returns the probabilities of taking each action, like [0.6, 0.4]. action = choose_action_from(probs) will return the index of the probability chosen. For example, if it chose 0.6, the action would be 0.
When it is time to update the parameters of the policy network, what should we do? Should we do something like the following?
gradient = policy.backpropagate(total_discounted_reward*log(probs[action])
policy.weights += gradient

And I only backpropagate this through one output neuron?
Which loss function should I use in this case? What would the labels be?
If you need more explanation, I have this question on SO.
","['reinforcement-learning', 'backpropagation', 'policy-gradients', 'reinforce', 'cross-entropy']",The loss function you are looking for is cross entropy loss. The 'label' that you use is the action you took at the time point you are updating for.
How much is currently invested in artificial general intelligence research and development?,"
How much is currently invested in artificial general intelligence research and development worldwide?
Feel free to add company or VC names, but this is not the point. The point is to get an idea of the economics around artificial general intelligence.
","['reference-request', 'agi', 'research', 'economics']",
Back propagation approach to logistic regression: why is cost diverging but accuracy increasing?,"
Background I have tried to fit a logistic regression model - written using a forward / back propagation approach (as part of Andrew Ng's deep learning course) - to a very non-linear data set (see picture below). Of course, it totally fails; in Andrew Ng's course, the failure of logistic regression to fit to this motivates developing a neural net - which works quite nicely.  But my question concerns what my logistic model is doing and why.

The problem My logistic regression model's cost increases, even after massively reducing the learning rate. But at the same time my accuracy (slowly) increases. I simply cannot see why.
To confuse matters even more - if I resort to a negative learning rate (essentially trying to force the calibration to higher cost values) the cost then decreases for a time until the accuracy hits 50%. After this point, the cost then inexorably increases - but the accuracy stays equal to 50%. The solution so found is to set all points to either red or blue (a reasonable fit given logistic regression simply cannot work on this data).
My questions and thoughts on answers I have reproduced the Python code below - hopefully it's clear. My questions are:

Is there a mistake in the model that explains why negative learning rates seem to work better?
On the topic of why the cost increases even as accuracy asymptotes to 50%: is the issue that once the model has discovered the ""all points equal to either red or blue"" solution the parameters ""w"" and ""b"" just get larger and larger (in absolute terms) - driving all of the predictions closer to 1 (or conversely if it predicts all points are 0)?

To explain this second question a bit more: imagine red points are defined by y = 1. Suppose parameters w, b are chosen such that the probability for every point equals 0.9. Then the model predicts all points are red - which is correct for half the points. The model can then improve half the predictions by driving w and b up (so that sigmoid ( w*x + b) --> 1). But of course, this makes half the predictions (the blue points) more and more wrong - which causes the cost function for those points - log(1 - prob) - to diverge. I don't truly see why gradient descent would do this but it's all I can think of for the peculiar behaviour of the algorithm.
Hope this all makes sense. Hit me up if not.
import numpy as np
import matplotlib.pyplot as plt


# function to create a flower-like arrangement of 1s and 0s
def load_planar_dataset():
    np.random.seed(1)
    m = 400 # number of examples
    N = int(m/2) # number of points per class
    D = 2 # dimensionality / i.e. work in 2d plane - so X is a set of (x,y) coordinate points
    X = np.zeros((m,D)) # data matrix where each row is a single example
    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)
    a = 4 # maximum ray of the flower

    for j in range(2):
        ix = range(N*j,N*(j+1))
        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta / random element mixes up some of the petals so you get mostly blue with some red petals and vice-versa
        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius / again random element alters  shape of flower slightly
        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
        Y[ix] = j
        
    X = X.T # transpose so columns = training example as per standard in lectures
    Y = Y.T

    return X, Y

# function to plot the above data plus a modelled decision boundary - works by applying model to grid of points and colouring accordingly
def plot_decision_boundary(model, X, y):
    # Set min and max values and give it some padding
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)


# sigmoid function as per sandard linear regression
def sigmoid(z):
    """"""
    Compute the sigmoid of z

    Arguments:
    z -- A scalar or numpy array of any size.

    Return:
    s -- sigmoid(z)
    """"""

    s = 1. / (1. + np.exp(-z))
    
    return s


# 
def propagate(w, b, X, Y):
    """"""
    Implement the cost function and its gradient for the propagation explained above

    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)
    Y -- true ""label"" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)

    Return:
    cost -- negative log-likelihood cost for logistic regression
    dw -- gradient of the loss with respect to w, thus same shape as w
    db -- gradient of the loss with respect to b, thus same shape as b
    """"""

    m = X.shape[1];

    # forward prop
    Z = np.dot(w.T, X) + b; 
    A = sigmoid(Z); # activiation = the prediction of the model


    # compute cost
    cost =  - 1. / m * np.sum( (Y * np.log(A) + (1. - Y) * np.log(1. - A)  ) )

    #back prop for gradient descent

    da = - Y / A + (1. - Y) / (1. - A)  
    dz = da * A * (1. - A) # = - Y (1-A) + (1. - Y) A =  A - Y  
    dw = 1. / m * np.dot( X, dz.T )
    db = 1. / m * np.sum(dz)

    grads = {""dw"": dw,
                ""db"": db}

    return grads, cost


def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    """"""
    This function optimizes w and b by running a gradient descent algorithm
    
    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of shape (num_px * num_px * 3, number of examples)
    Y -- true ""label"" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
    num_iterations -- number of iterations of the optimization loop
    learning_rate -- learning rate of the gradient descent update rule
    print_cost -- True to print the loss every 100 steps
    
    Returns:
    params -- dictionary containing the weights w and bias b
    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.
    
    """"""
    costs = []

    for i in range(num_iterations):
        
        # cost /gradient calculation
        grads, cost = propagate(w, b, X, Y)

        #retrieve derivatives
        dw = grads[""dw""]
        db = grads[""db""]

        # update values according to gradient descent algorithm
        w = w - learning_rate * dw
        b = b - learning_rate * db

        # record the costs
        if i % 100 == 0:
            costs.append(cost)

            # Print the cost every 100 training iterations
            if print_cost:
                print(""Cost after iteration %i: %f"" %(i, cost))



    params = {  ""w"": w,
                ""b"": b}

    grads = {   ""dw"": dw,
                ""db"": db}

    return params, grads, costs


def predict(w, b, X):
    '''
    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)
    
    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)
    
    Returns:
    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X
    '''
    Z = np.dot(w.T, X) + b
    A = sigmoid(Z)


    Y_prediction = (A >= 0.5).astype(int)

    return Y_prediction




np.random.seed(1) # set a seed so that the results are consistent

X, Y = load_planar_dataset()

# Visualize the data:

plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral); # s = size of points; cmap are nicer colours
plt.show()

shape_X = X.shape
shape_Y = Y.shape
m = shape_Y[1]  # training set size
n = shape_X[0] # number of features (2)


# initialise parameters
w = np.random.rand(n, 1)
b = 0

# print accuracy of initial parameters by comparing prediction to 
print(""train accuracy: {} %"".format(100 - np.mean(np.abs(predict(w, b, X) - Y)) * 100))


# fit model and print out costs every 100 iterations of the forward / back prop
parameters, grads, costs = optimize(w, b, X, Y, num_iterations = 10000, learning_rate = 0.000005, print_cost = True)


# return the prediction
Y_prediction = predict(parameters[""w""], parameters[""b""], X)

# print accuracy of fitted model
print(""train accuracy: {} %"".format(100 - np.mean(np.abs(Y_prediction - Y)) * 100))


# print parameters for interest
print( parameters[""w""] , parameters[""b""] )

# plot decision boundary
plot_decision_boundary(lambda x: predict(parameters[""w""], parameters[""b""], x.T), X, Y)
plt.show()

List item

","['deep-learning', 'backpropagation', 'objective-functions', 'logistic-regression']",
What is the current artificial general intelligence technology valuation?,"
That is, if AGI were an existing technology, how much would it be valued to?
Obviously it would depend on its efficiency, if it requires more than all the existing hardware to run it, it would be impossible to market.
This question is more about getting a general picture of the economy surrounding this technology.
Assuming a specific definition of AGI and that we implemented that AGI, what is its potential economical value?
Current investments in this research field are also useful data.
","['agi', 'economics']",
Is GPT-3 an early example of strong AI in a narrow setting?,"
In GPT-2, the large achievement was being able to generate coherent text over a long-form while maintaining context. This was very impressive but for GPT-2 to do new language tasks, it had to be explicitly fine-tuned for the new task.
In GPT-3 (From my understanding), this is no longer the case. It can perform a larger array of language tasks from translation, open domain conversation, summarization, etc., with only a few examples. No explicit fine-tuning is needed.
The actual theory behind GPT-3 is fairly simple, which would not suggest any level of ability other than what would be found in common narrow intelligence systems.
However, looking past the media hype and the news coverage, GPT-3 is not explicitly programmed to ""know"" how to do these wider arrays of tasks. In fact, with limited examples, it can perform many language tasks quite well and ""learn on the fly"" so to speak. To me, this does seem to align fairly well with what most people would consider strong AI, but in a narrow context, which is language tasks.
Thoughts? Is GPT-3 an early example of strong AI but in a narrower context?
","['philosophy', 'agi', 'gpt']",
Are there transformer-based architectures that can produce fixed-length vector encodings given arbitrary-length text documents?,"
BERT encodes a piece of text such that each token (usually words) in the input text map to a vector in the encoding of the text. However, this makes the length of the encoding vary as a function of the input length of the text, which makes it more cumbersome to use as input to downstream neural networks that take only fixed-size inputs.
Are there any transformer-based neural network architectures that can encode a piece of text into a fixed-size feature vector more suitable for downstream tasks?
Edit: To illustrate my question, I’m wondering whether there is some framework that allows the input to be either a sentence, a paragraph, an article, or a book, and produce an output encoding on the same, fixed-sized format for all of them.
","['natural-language-processing', 'reference-request', 'autoencoders', 'transformer', 'bert']",
How differentiable programming and programming language supporting it will potentially help the development towards AGI?,"
After the state of the art Deep Learning techniques/algorithms being implemented in low-level languages like Objective-C, C++, etc to high-level languages like Python, JS, etc. and with the help of huge libraries like Tensorflow, Pytorch, Scikit-Learn, etc.
Now, Swift: Google's bet on differentiable programming, they are making Swift differential programming ready see this manifesto and they are building TensorFlow from the ground up in swift S4TF.
So, how differentiable programming and programming language supporting it will potentially help the development towards AGI?
","['machine-learning', 'deep-learning', 'agi', 'research', 'programming-languages']",
Research paths/areas for improving the performance of CNNs when faced with limited data,"
I've been reading through the research literature for image processing, computer vision, and convolutional neural networks. For image classification and object recognition, I know that convolutional neural networks deliver state-of-the-art performance when large amounts of data are available. Furthermore, I know that Hinton et al. created ""capsule networks"" to try and overcome some of the fundamental limitations of CNN architecture (such as them not being rotationally invariant). However, my understanding is that capsule networks have been a failure (so far), and most people expect them to go nowhere. And CNNs have progressively been improved in various ways (Bayesian optimisation for hyper parameter tuning, new convolution kernels, etc.). It seems to me that, at the moment, and for the foreseeable future, CNNs are the best architecture available for image-related stuff.
But, as I said, CNNs, like other Deep Learning architectures, require large amounts of data. So my question is as follows:
What are the research areas/topics for improving CNNs in the sense of making them work more effectively (that is, have greater performance) with less data (working with small datasets)?
I know that there is various research looking at approaches to increasing data (such as data augmentation, generative networks, etc.), but I am primarily interested in fundamental modifications to CNNs themselves, rather than purely focusing on changes to the data itself.
And to expand upon my question, using my above definition of ""performance"", I am interested in these two categories:

""Computational methods"" for increasing CNN performance. This would be the non-mathematical stuff that I've read about, such as just increasing the number of layers and making the CNN deeper/wider (and I think another one had to do with just making the size of the convolution kernel smaller, so that it looks at smaller pieces of the image at any one time, or something like that?).

""Mathematical methods"" for increasing CNN performance. This would be the cutting-edge mathematical/statistical stuff that I've read about: things like algorithms (such as Bayesian optimization); I've come across a lot of geometric stuff; and I guess the cutting-edge convolution kernels created by the image processing people would also fall under this category.


Obviously, this ""list"" is not exhaustive, and it's probably incorrect; I'm a novice to this research, so I'm trying to find my way around.
I am interested in studying both of the above categories, but I will primarily be working from the mathematical/statistical side. And I want to work on research that is still practical and can be put to use in industry for improved performance (even if it might still be ""advanced""/complex for most people in industry) – not the the highly theoretical stuff related.
Related (but unanswered): Are there any good research papers on image identification with limited data?
","['convolutional-neural-networks', 'math', 'research', 'image-processing', 'statistics']","Some research areas that come to mind which can be useful when faced with a limited amount of data:Regularization: Comprises different methods to prevent the network from overfitting, to make it perform better on the validation data but not necessarily on the training data. In general, the less training data you have, the stronger you want to regularize. Common types include:Injecting noise in the network, e.g., dropout.Adding regularization terms to the training loss, e.g., L1 and L2 regularization of the weights, but also confident output distributions can be penalized.Reducing the number of parameters in the network to make it unable to fit the training data completely and thus unable to overfit badly. Interestingly, increasing the number of parameters for large models can also improve the validation performance.Early stopping of training. For example, if one part of the training set is set aside and not used to update the weights, training can be stopped when the observed loss on this part of the training set is observed to start to increase.Generating new training data:Data augmentation: Ways to augment existing training examples without removing the semantics, e.g., slight rotations, crops, translations (shifts) of images.Data interpolation, e.g., manifold mixup.Using synthetic data, e.g., frames from video games or other CGI.Transfer learning: When you take a neural network that has already been trained on another, much larger dataset of the same modality (images, sounds, etc.) as your dataset and fine-tune it on your data.Multitask learning: Instead of training your network to perform one task, you give it multiple output heads and train it to perform many tasks at once, given that you have that the labels for the additional tasks. While it may seem that this is a more difficult for the network, the extra tasks have a regularizing effect.Semi-supervised learning: If you have much unlabeled data that labeled data, you can combine supervised learning with unsupervised learning. Much like with multitask learning, the extra task introduced by the unsupervised learning also has a regularizing effect.Other interesting methods can be found in systems that perform one-shot learning, which inherently implies very little training data. These systems often uses slightly modified network architectures. For example, facial recognition systems can learn to recognize a face from only a single photo, and usually use a triplet loss (or similar) of a vector encoding of the face, instead of cross-entropy loss of the output of a softmax layer normally used for image classification.Zero-shot learning also exists (e.g., zero-shot machine translation), but this is a completely different type of problem setup and requires multiple data modalities."
What is wrong with equation 7.3 in Sutton & Barto's book?,"
Equation 7.3 of Sutton Barto book:
$$\text{Equation: } max_s|\mathbb{E}_\pi[G_{t:t+n}|S_t = s] - v_\pi| \le \gamma^nmax_s|V_{t+n-1}(s) - v_\pi(s)| $$
$$\text{where }G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + .....+\gamma^{n-1} R_{t+n} + \gamma^nV_{t+n-1}(S_{t+n})$$
Here $V_{t+n-1}(S_{t+n})$ is the estimate of $V_\pi(S_{t+n})$
But the Left Hand Side of the above equation should be zeros as, for any state s, $G_{t:t+n}$ is an unbiased estimate of $v_\pi(s)$ hence $\mathbb{E}_\pi[G_{t:t+n}|S_t = s] = v_\pi(s)$.
","['reinforcement-learning', 'value-functions', 'sutton-barto', 'expectation', 'return']","In general, $\mathbb{E}_\pi[G_{t:t+n}|S_t = s] \neq v_\pi(s)$. $v_\pi(s)$ is defined as $\mathbb{E}_\pi[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]$, so you should be able to see why the two are not equal when the LHS is an expectation of the $n$th step return. They would only be equal as $n \rightarrow \infty$."
What are some examples of functions that machine learning models compute?,"
My simple understanding of AI is that it is based on a mathematical model of a problem. If I understood correctly, the model is a polynomial equation and its weights are calculated by training the model with data sets.
I am interested to see a few example polynomial equations (trained models) which are used in certain problem areas. I tried to search it, but so far could not find any simple answers.
Can anyone list a few examples here?
","['machine-learning', 'training', 'datasets', 'models', 'weights']","If I understood correctly, the model is a polynomial equationNo, it's not true that all machine learning (ML) models compute (or represent) a polynomial function. For example, a sigmoid is not a polynomial, but, for example, in a neural network, you can combine many sigmoids to build complicated functions that may not necessarily be polynomials.We usually distinguish between linear (straight-lines) and non-linear functions (rather than polynomials and non-polynomials). In some cases, it is straightforward to visualize the function that your model computes: for example, in the case of linear regression, once you learned the coefficients (i.e. the slope and y-intercept), you can plot the learned straight-line function. In other cases, for example, in the case of neural networks, it is not fully clear how to visualize the function that your model computes, given that it is the composition of many non-linear functions (typically, ReLUs, sigmoids or hyperbolic tangents).If you are interested in solving problems with polynomials, take a look at polynomial regression.and its weights are calculated by training the model with data sets.Yes, in machine learning, we want to find a function that ""fits the given data"", and the specific meaning of ""fitting the data"" depends on the specific machine learning technique.For simplicity, let's focus on supervised learning, a machine learning technique where we are given a labelled dataset, i.e. a dataset of pairs $D = \{(x_1, y_1), \dots, (x_N, y_N)\}$, where we assume that $f(x_i) = y_i$, for some typically unknown function $f$, and $y_i$s are the labels (the outputs of $f$) and $x_i$ the inputs of $f$. The goal is to find function $g_{\theta}$ that approximates well $f$. I will soon describe what the subscript $\theta$ represents.For simplicity, let's assume that $f$ is a linear function (i.e. a straight-line). So, we can define a linear model $g_{\theta}$ that we can use to find a function that approximates well $f$. Here is the linear model$$g_{\theta}(x) = ax + b,$$whereWhy is this a model? I call this a model because, depending on the specific values of the parameters $\theta$, we have different specific functions. So, I am using the term ""model"" as a synonym for a set of functions, which, in this case, are limited by the definition $ax + b$ and the specific values that $a$ and $b$ (i.e. $\theta$) can take.So, what do we do with this linear model? We want to find a specific set of parameters $\hat{\theta}$ (note that I use the $\hat{ }$ to emphasize that this is a specific configuration of the variable $\theta$) that corresponds to a linear function (a straight-line) that approximates $f$ well.  In other words, we need to find the parameters $\hat{\theta}$, such that $g_\hat{\theta} \approx f$, where $\approx$ means ""approximately computes"".How do we do that? We typically don't know $f$, but we know (or assumed) that $f(x_i) = y_i$, so the labeled dataset $D$ contains information about our unknown function $f$. So, the idea is that we can use the dataset $D$ to find a specific set of parameters $\hat{\theta}$ that corresponds to some function that approximates $f$ according to the information in $D$.This process of finding $\hat{\theta}$ based on $D$ is often denoted as ""fitting the model to the data"". There different ways of fitting the model to the data that differ in the way they compute some notion of distance between the information in $D$ and $g_{\hat{\theta}}$. I will not explain them here because this answer is already quite long. If you want to know more about it, you should take a book about the topic and read it.What are some examples of functions that machine learning models compute?I don't have specific examples, but you can easily try to fit a linear regression model to some labelled data, then plot the function that you found. You could use the Python library sklearn to do that."
Can a neural network be trained on a dataset containing only values for true output for a classification problem?,"
I am using a dataset from Google which contains 1,27,000 data points on simulated concentrations of the atmosphere of exoplanets which can sustain life. So, the output label of all these data points is 1 i.e, probability of life existing there is 1. If I train my neural network on this data, and test it on data points with concentrations other than these, can I expect to get probability values at the output? Asking because the model knows no false labelled value.
","['neural-networks', 'deep-learning', 'classification', 'datasets', 'data-preprocessing']","Yes, you can and the answer is One-Class Classification. A well-written resource to understand is this."
How do I label images for deep learning classification?,"
I have roughly 30,000 images of two categories, which are 'crops' and 'weeds.' An example of what I have can be found below:

The goal will use my training images to detect weeds among crops, given an orthomosaic GIS image of a given field. I guess you could say that I'm trying to detect certain objects in the field.
As I'm new to deep learning, how would one go about generating training labels for this task? Can I just label the entire photo as a 'weed' using some type of text file, or do I actually have to draw bounding boxes (around weeds) on each image that will be used for training? If so, is there an easier way than going through all 30,000 of my images?
I'm very new to this, so any specific details would really help a lot!
","['deep-learning', 'object-detection']","If each photo is intended to show a photo of weed or crops you should give one label. If your task is different where you also try to localize weed or crops in the image, then you need to label accordingly. My understanding is you are trying to do the first case, therefore, there should be one label for each image."
NLP Bible verse division problem: Whats the best model/method?,"
I'm working on a project compiling various versions of the Bible into a dataset. For the most part versions separate verses discreetly. In some versions, however, verses are combined. Instead of verse 16, the marker will say 16-18. I wonder if, given I have a lot of other versions that separate them discretely, I can train an NLP model (I have about 30 versions that could act as a training set which would constitute  to separate those combined verses into discrete verses. I'm fairly new at deep learning, having done a few toy projects. I wonder how to think about this problem? What kind of problem is it? I think it might be similar to auto-punctuation problems and it seems the options there are seq2seq and classifier. This makes more sense to me as a classification problem, but maybe my inexperience is what drives me that direction. Can people suggest ways to think about this problem and resources I might use?
In answer to questions in the comment, I am dealing only with text, not images. An example might be like this:
Genesis 2, New Revised Standard Version:

5 when no plant of the field was yet in the earth and no herb of the field had yet sprung up—for the Lord God had not caused it to rain upon the earth, and there was no one to till the ground; 6 but a stream would rise from the earth, and water the whole face of the ground— 7 then the Lord God formed man from the dust of the ground, and breathed into his nostrils the breath of life; and the man became a living being.

Genesis 2, The message version:

5-7 At the time God made Earth and Heaven, before any grasses or shrubs had sprouted from the ground—God hadn’t yet sent rain on Earth, nor was there anyone around to work the ground (the whole Earth was watered by underground springs)—God formed Man out of dirt from the ground and blew into his nostrils the breath of life. The Man came alive—a living soul!

The goal then would be to divide the message version into discrete verses in the way that the NRSV is. Certainly, a part of the guide would be that a verse always ends in some kind of punctuation, though while necessary it is not sufficient to assign a distinct verse.
","['natural-language-processing', 'sequence-modeling', 'text-classification']",
Are there any python implementations of GGP games or how to use game logic written in GDL in python?,"
I'm writing a General Game Playing (GGP) AI in python. I'd like to test it on some GGP games. So are there any python implementations of GGP games?
I found in http://games.ggp.org/base games written in Game Description Language (GDL). How to use them in python if it is possible to do so?
","['python', 'game-ai', 'combinatorial-games']",
Are linear approximators better suited to some tasks compared to complex neural net functions?,"
Model based RL attempts to learn a function $f(s_{t+1}|s_t, a_t)$ representing the environment transitions, otherwise known as a model of the system. I see linear functions are still being used in model-based RL such as in robotic manipulation to learn system dynamics, and can work effectively well. (Here, I mean in learning the model, not as an optimization method for the controller selecting the best actions).
In model-based RL, are there situations where a learning a linear model such as using a Lyapunov function would be better suited than using a neural network, or are the examples of problems framed to use linear models when addressing them using model-based RL?
","['reinforcement-learning', 'control-problem', 'model-based-methods', 'linear-programming']",
"What does $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ mean in the article Hindsight Experience Replay, section 2.1?","
Taken from section 2.1 in the article:

We consider the standard reinforcement learning formalism consisting of an agent interacting with an environment. To simplify the exposition we assume that the environment is fully observable. An environment is described by a set of states $S$, a set of actions $A$, a distribution of initial states $p(s_0)$, a reward function $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, transition probabilities $p(s_{t+1} \mid s_t, a_t)$, and a discount factor $\gamma \in [0, 1]$.*

How should one interpret the maths behind it?
","['reinforcement-learning', 'math', 'papers', 'hindsight-experience-replay']","This answer assumes that you only have a problem with this notation from the article:$r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$This is a standard notation, used in many disciplines, for defining a function and its input and output domains. It is a bit like the method signature for the function - it does not fully define it, but does enough to show how it can interact with other expressions.All functions can be thought of as maps between the input domain and output domain. You provide an input value, and it returns an output value. The values can be arbitrary mathematical objects. To show what kind of objects the inputs and outputs are allowed to be, the notation for sets is used.Importantly the symbol $\mathbb{R}$ at the end does not refer to the set of possible rewards in the environment (although it is a reward function, and that will be its output), but the set of all real numbers, because a reward is always a real number*.As a concrete example, if you had the function $f(x) = x^2 - 2x + 7$ defined for a real number $x$, then its equivalent notation might be $f : \mathbb{R} \rightarrow \mathbb{R}$. If you allowed $x$ to be complex then it would be $f : \mathbb{C} \rightarrow \mathbb{C}$, because $\mathbb{C}$ is the standard symbol for the set of all complex numbers.So now we can break down the notation $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$$r$The function is called $r$$:$It has an input domain of . . .$\mathcal{S} \times \mathcal{A}$The cartesian product of the set of all possible states $\mathcal{S}$ and the set of possible actions $\mathcal{A}$.That is much the same as saying the function has a signature $r(s, a)$ where $s \in \mathcal{S}$ and $a \in \mathcal{A}$$\rightarrow$It has an output domain of . . .$\mathbb{R}$any single real number.* This choice (of declaring the more general $\mathbb{R}$ instead of specific $\mathcal{R}$) is made partly because operators like $+$ and $\times$ are well defined for real numbers. This is a useful thing to assert about the behaviour of the reward function output when defining how value functions work for instance. Of course you could be more specific, defining $\mathcal{R}$ as some subset of $\mathbb{R}$, that would be correct and more precise definition, but it is not needed for general theory in reinforcement learning. The less precise definition is fine for nearly all purposes."
What is the “Hello World” problem of Unsupervised Learning?,"
As a followup to this question, I'm interested in what the typical ""Hello World"" problem (first easy example problem) is for unsupervised learning.
A quick Google search didn't find any obvious answers for me.
",['unsupervised-learning'],"I disagree with the context that MNIST is the ""hello world"" of supervised learning. It is definitely, though, the ""hello world"" of image classification, which is a very specific sub-field of supervised learning.I'd consider the Iris dataset a better candidate for the ""hello world"" of supervised learning, with other close candidates such as the Wine, Wisconsin breast cancer or Pima indians datasets. However, as an even simpler and more alternative choice, a lot of people prefer generating their own 2-dimensional datasets so that can more intuitively understand what the different algorithms are doing. An example of this is TensorFlow playground.Equivalently, in unsupervised learning there are a lot of different tasks. I personally think that clustering is probably the task that is easier for people to understand and as such the most common intro to unsupervised learning. Here there are, as well, two options:"
Could the neural network automatically calculate and get different one-to-many quantities relative to their parent quantity?,"
Let's say I have a primary dataset that its secondary dataset is hundreds to match and group like an one-to-many relationship.
I'm new in this world of the AI but my problem is that many child groups contain the same elements or even different combinations to result the parent data but in this case more to avoid duplication is get those duplications and the some way add up the data.
This is an example of what secondary data can look like and what I want to get from grouping it.
Parent data
  ID        FIELD1       FIELD2 FIELD3  FIELD4      FIELD5
  90148001  BLABLA       40     0       35896.89479 35896.89479

Child data
  ID        FIELD1       FIELD2 FIELD3  FIELD4      FIELD5
* 90148001  BLABLA       1      1770    1769.572665 1769.572665
* 90148001  DESCRIPTION2 1      13146   13146.45284 13146.45284
* 90148001  BLABLA       1      2176    2176.435074 2176.435074
* 90148001  BLABLA       1      2306    2305.716285 2305.716285
* 90148001  BLABLA       1      2531    2531.271196 2531.271196
* 90148001  BLABLA       1      1147    1146.803622 1146.803622
* 90148001  BLABLA       1      1991    1990.613246 1990.613246
* 90148001  BLABLA       1      3641    3641.394446 3641.394446
* 90148001  BLABLA       1      2471    2470.8253   2470.8253
* 90148001  BLABLA       1      2247    2246.984815 2246.984815
* 90148001  BLABLA       1      2471    2470.8253   2470.8253

Could a neural network be able to process, aggregate, and group those quantities?
","['neural-networks', 'datasets', 'math']",
How to construct input dependent convolutional filter?,"
I am constructing a convolutional variational autoencoder for images, starting out with mnist digits. Typically I would specify convolutional layers in the following way:
input_img = layers.Input(shape=(28,28,1))
conv1 = keras.layers.Conv2D(32, (3,3), strides=2, padding='same', activation='relu')(input_img)
conv2 = keras.layers.Conv2D(64, (3,3), strides=2, padding='same', activation='relu')(conv1) 
...

However, I would also like to construct a convolutional filter/kernel that is fixed BUT dependent on some content related to the input, which we can call an auxiliary label. This could be a class label or some other piece of relevant information corresponding to the input. For example, for MNIST I can use the class label as auxiliary information and map the digit to a (3,3) kernel and essentially generate a distinct kernel for each digit. This specific filter/kernel is not learned through the network so it is fixed, but it is class dependent. This filter will then be concatenated with the traditional convolutional filters shown above.
input_img = layers.Input(shape=(28,28,1))
conv1 = keras.layers.Conv2D(32, (3,3), strides=2, padding='same', activation='relu')(input_img)

# TODO: add a filter/kernel that is fixed (not learned by model) but is class label specific
# Not sure how to implement this?
# auxiliary_conv = keras.layers.Conv2D(1, (3,3), strides=2, padding='same', activation='relu')(input_img)

I know there are kernel initializers to specify initial weights https://keras.io/api/layers/initializers/, but I'm not sure if this is relevant and if so, how to make this work with a class specific initialization.
In summary, I want a portion of the model's weights to be input content dependent so that some of the trained model's weights vary based on the auxiliary information such as class label, instead of being completely fixed regardless of the input. Is this even possible to achieve in Keras/Tensorflow? I would appreciate any suggestions or examples to get started with implementation.
","['convolutional-neural-networks', 'tensorflow', 'keras', 'variational-autoencoder', 'filters']","Here is one way of achieving this. This network is an autoencoder, with extra auxiliary_convs. The active convolution depends on the input image's class, since each convolution layer's output is multiplied with the one-hot encoded class input.It is a bit simpler if you don't sum auxiliary tensors together, but simply concat them:However in this case the last Conv2D layer would have redundant parameters to train, since x has dim * num_classes dimensions after the K.concatenate.If you don't want to train the input-dependent part you can freeze the layers, but I don't know why you would want to do that.I also tested a variation of this for fun, having a constrained convolutional network but modifying the autoencoded feature based on the image's class:The scatter plot on the left shows the codes (aka. embeddings), and the image's class is varied on the decoded examples on the right. Higher the l2_reg is, the tighter the classes' distribution is. Although I don't know what is the utility of this network :D"
What is the purpose of the DAMSM loss for the generators in AttnGAN?,"
I am confused about the training part in AttnGan.
If you observe page 3. There are two types of losses for generator network: one involving the Deep Attentional
Multimodal Similarity Model (DAMSM) loss $(L_{DAMSM})$ and the others for individual generator $(L_{G_i})$ for $i= 1, 2, 3$.
My doubt is: if each generator has its own loss function that is useful in training, what is the purpose in using $L_G$, i.e., with DAMSM loss function? Is my assumption wrong?
","['objective-functions', 'papers', 'generative-adversarial-networks', 'attn-gan']",
"What's wrong with my answer to this constraint satisfaction problem, which needs to be solved the AC-3 algorithm?","
I was watching the video Constraint Satisfaction: the AC-3 algorithm, and I tried to solve this question:

Given the variables A, B, C and D, with domain {1, 2, 3, 4} in each of them and restrictions A> B, B = C and D ≠ A, use the AC algorithm.

But the teacher told me that my answer below is wrong!
He gave me a tip: Domain D will not be changed!
Below, I is my answer step by step. If someone can help me find the error, I appreciate it!
To solve this exercise, it is first necessary to organize the data in order to separate what is the domain, agenda and arc.

Soon after, we will analyze the first item on the agenda “A> B” with domain A, in order to eliminate unnecessary elements from the domain.

Analyze domain B with the agenda item “B <A”

Analyze domain B with the agenda item “B = C” and add the constraint “A> B”

Analyze domain D with the agenda item “D ≠ A” and add the constraint “B <A”

Analyze domain A with the agenda item “A ≠ D”

Analyze domain A with the agenda item “A> B”

Analyze domain B with the agenda item “B = C”

Analyze domain B with the agenda item “B <A”

Result

","['constraint-satisfaction-problems', 'homework']",
"Is continuous learning possible with a deep convolutional neural network, without changing its topology?","
In general, is continuous learning possible with a deep convolutional neural network, without changing its topology?
In my case, I want to use a convolutional neural network as a classifier of heartbeat types. The ECG signal is split, and a color image is created using feature extraction. These photos (the inputs) are fed into a deep CNN, but they must be labeled by someone first.
Are there ways to implement continuous learning in a deep neural network for image recognition? Does such an implementation make sense if the labels have to be specially prepared in advance?
","['deep-learning', 'convolutional-neural-networks', 'image-recognition', 'incremental-learning', 'online-learning']","In general, is continuous learning possible with a deep convolutional neural network, without changing its topology?Your intuition that it is possible to perform incremental (aka continual, continuous or lifelong) learning by changing the NN's topology is correct. However, dynamically adapting the NN's topology is just one approach to continual learning (a specific example of this approach is DEN). So, there are other approaches, such asregularization approaches (examples are LwF, EWC and SI)ensemble approaches (examples are Learn++, FEL and PathNet)rehearsal approaches (an example is iCaRL)For more details about these and other approaches (and problems related to continual learning and catastrophic forgetting in neural networks), take a look at this very nice review of continual learning approaches in neural networks. You should also check this answer.Are there ways to implement continuous learning in a deep neural network for image recognition?Yes. Many of the approaches focus on image recognition and classification, and often the experiments are performed on MNIST or similar datasets (e.g. see this paper).Does such an implementation make sense if the labels have to be specially prepared in advance?Yes, you can prepare your dataset in advance, and then later train incrementally (in fact, in the experiments I have seen in some of these papers, they usually do this to simulate the continual learning scenario), but I am not sure about the optimality of this approach. Maybe with batch learning (i.e. the usual offline learning where you train on all data), you would achieve higher performance."
"What is the ""Hello World"" problem of Reinforcement Learning?","
As we all know, ""Hello World"" is usually the first program that any programmer learns/implements in any language/framework.
As Aurélien Géron mentioned in his book that MNIST is often called the Hello World of Machine Learning, is there any ""Hello World"" problem of Reinforcement Learning?
A few candidates that I could think of are multi armed bandits problem and Cart Pole Env.
","['machine-learning', 'reinforcement-learning']",
Appropriate metric and approach for natural language generation for small sentences,"
I am trying to create a language generation model to generate very short sentences/words, like a rapper name generator. The sentences in my dataset are anywhere between 1 word and 15 words (3-155 characters). So far, I have tried LSTM's with 1-3 layers and inputs as subwords and characters. The results so far are not that great, I am getting ~0.5 crossentropy loss and ~50% accuracy.
My inputs are like a sliding window with prepadding, (eg. (for a batch) Inputs = [[0,0,0,1], [0,0,1,2]...[n-4,..n-1]], outputs=[[0,0,1,2], ...[n-3,n-2,n-1,n]]) where 0 is padding, 1 is the start token and n is the end token. Outputs are 1 hot encoded.
The model is an embedding layer, few lstm and dropout layers, followed by time distributed dense and then a dense layer.
My doubt is, is accuracy a right metric, I am using it because at the end, I am making a classification for 4 output values. Another one is, will a transformer be suitable for this, since I want to generate small sentences, (which are nouns) and models like GPT/ Bert are more suitable for capturing dependency between long sentences.
","['natural-language-processing', 'long-short-term-memory', 'attention', 'language-model']",
How much can/should the non-player character know about the game's world?,"
I'm about to write a non-player character (NPC). I wonder how much the AI should know about the game's world. So, my question isn't about the amount of training data the AI has to collect. I'm interested in how much the AI is allowed to know about what's going on in the game's world.
For example, can (shall) it have knowledge about the build queue of the player?
To provide more details: while a human plays a game against another human, not all information of what the opponent is doing is available (e.g. the queue of the units your opponent is building). This could give you an advantage (so that you can prepare for a rush, when he's building many cheap units). Theoretically, an NPC could access and make use of that knowledge and, in addition, spare resources for scouting/spying/exploring.
But is this the way of constructing an NPC AI? Or should this data also be restricted? I have never done anything like this before.
I don't know where to ask else wise or what more information I could provide. So, if something in my question is unclear or unfit, please let me know what exactly.
",['game-ai'],
"Fundamentally, what is a perfect language model?","
Suppose that we want to generate a sentence made of words according to language $L$:
$$
W_1 W_2 \ldots W_n
$$
Question: What is the perfect language model?
I ask about perfect because I want to know the concept fundamentally at its fullest extent. I am not interested in knowing heuristics or shortcuts that reduce the complexity of its implementation.

1. My thoughts so far
1.1. Sequential
One possible way to think about it is moving from left to right.  So, 1st, we try to find out value of $W_1$. To do so, we choose the specific word $w$ from the space of words $\mathcal{W}$ that's used by the language $L$.  Basically:
$$
w_1 = \underset{w \in \mathcal{W}}{\text{arg max }} \Pr(W_1 = w)
$$
Then, we move forward to find the value of the next word $W_2$ as follows
$$
w_2 = \underset{w \in \mathcal{W}}{\text{arg max }} \Pr(W_2 = w | W_1 = w_1)
$$
Likewise for $W_3, \ldots, W_n$:
$$
w_3 = \underset{w \in \mathcal{W}}{\text{arg max }} \Pr(W_3 = w | W_1 = w_1, W_2=w_2)
$$
$$
\vdots
$$
$$
w_n = \underset{w \in \mathcal{W}}{\text{arg max }} \Pr(W_n = w | W_1 = w_1, W_2=w_2, \ldots W_{n-1}=w_{n-1})
$$
But is this really perfect?  I personally doubt.  I think while language is read and written usually from a given direction (e.g. left to right), it is not always done so, and in many cases language is read/written possibly in a funny order as we always do.  E.g. even when I wrote this question, I jumped back and forth, then went to edit it (as I'm doing now).  So I clearly didn't write it from left to right!  Similarly, you, the reader; you won't really read it in a single pass from left to right, will you?  You will probably read it in some funny order and go back and forth for awhile until you conclude an understanding.  So I personally really doubt that the sequential formalism is perfect.
1.2. Joint
Here we find all the $n$ words jointly. Of course ridiculously expensive computationally (if implemented), but our goal here is to only know what is the problem at its fullest.
Basically, we get the $n$ words as follows:
$$
(w_1, w_2, \ldots, w_n) = \underset{(w_1,w_2,\ldots,w_n) \in \mathcal{W}^n}{\text{arg max }} \Pr(W_1 = w_1, W_2=w_2, \ldots W_n=w_n)
$$
This is a perfect representation of language model in my opinion, because its answer is gauranteed to be correct.  But there is this annoying aspect which is that its words candidates space is needlessly large!
E.g. this formalism is basically saying that the following is a candidate words sequence: $(., Hello, world, !)$ even though we know that in (say) English a sentence cannot start by a dot ""."".
1.3. Joint but slightly smarter
This is very similar to 1.2 Joint, except that it deletes the single bag of all words $\mathcal{W}$, and instead introduces several bags $\mathcal{W}_1, \mathcal{W}_2, \ldots, \mathcal{W}_n$, which work as follows:

$\mathcal{W}_1$ is a bag that contains words that can only appear as 1st words.
$\mathcal{W}_2$ is a bag that contains words that can only appear as 2nd words.
$\vdots$
$\mathcal{W}_n$ is a bag that contains words that can only appear as $n$th words.

This way, we will avoid the stupid candidates that 1.2. Joint evaluated by following this:
$$
(w_1, w_2, \ldots, w_n) = \underset{w_1 \in \mathcal{W}_1,w_2 \in \mathcal{W}_2,\ldots,w_n \in \mathcal{W}_n) \in \mathcal{W}^n}{\text{arg max }} \Pr(W_1 = w_1, W_2=w_2, \ldots W_n=w_n)
$$
This will also guarantee being a perfect representation of a language model, yet it its candidates space is smaller than one in 1.2. Joint.
1.4. Joint but fully smart
Here is where I'm stuck!
Question rephrase (in case it helps): Is there any formalism that gives the perfect correctness of 1.2. and 1.3., except for also being fully smart in that its candidates space is smallest?
",['language-model'],"One of your hypothesis is very close to the truth, it's 1.2So, a language model measures the probability of a given sentence in a language $L$. The sentences can have any length and the sum of probabilities of all the sentences in the language $L$ is 1. It's very difficult to compute, thus people use some simplifications, like say if the words are located far enough from each other, then the occurrence of a current word doesn't depend on a word which was occurred far away in the past.
Each sentence is a sequence $w_1, \dots, w_n$ and a language model computes the probability of the sequence $p([w_1, \dots w_n])$ (it's not joint distrribution yet). It can be decomposed into a joint distribution with some special tokens added $p(BOS, w_1, \dots w_n, EOS])$. BOS is begin of the sentence and EOS is end of sentence. Then this joint distribution can be decomposed using the chain rule $p(BOS, w_1, \dots w_n, EOS]) = p(BOS) p(w_1 | BOS) \Big[ \prod\limits_{i=1}^n p(w_i | BOS, w_1, \dots, w_{i-1}) \Big] p(EOS | BOS, w_1, \dots, w_n)$. There are 2 types of probabilities that are usually modelled differently: a prior probability $p(BOS)$ which is always equal to 1, because you always have BOS as the first token in the augmented sequence. Then conditional probabilities can be computed as follows $p(w_i | BOS, w_1, \dots, w_{i-1}) = \frac{c(BOS, w_1, \dots, w_{i-1}, w_i)}{\sum_{w_i \in W} c(BOS, w_1, \dots, w_{i-1}, w_i)}$. Where $c$ is a counter function that measures how many times a given sequence occured in the dataset you specified to train your model. You can notice it's a maximum likelihood estimate of the unknown conditional probabilities. Obviously if you're using a certain dataset you compute a model of that dataset, not of a language, but that's the to approximate true probabilities of sentences in a language.
The EOS token is needed to make difference between a probability of a non-finished yet sequence and that which has finished, because if you take those counters from above and forget about adding the EOS into your dataset in the end of all sentences, you'll get probabilities that don't sum into 1 (which is bad)."
How to perform prediction when some features have missing values?,"
Sorry if this is too noob question, I'm just a beginner.
I have a data set with companies' info. There are 2 kinds of features: financial (revenue and so on) and general info (like the number of employees and date of registration)
I have to predict the probability of default. And the data has gaps: about the half of the companies have no financial data at all. But general features are 100% filled.
What is the best practice for such a situation?
Will be great if you can give some example links to read.
","['machine-learning', 'prediction', 'data-preprocessing', 'feature-engineering']",
Why would the reward of A3C with LSTM suddenly drop off after many episodes?,"
I am training an A3C with stacked LSTM.
During initial training, my model was giving descent +ve reward. However, after many episodes, its reward just goes to zero and is continuing for a long time. Is it because of LSTM?
Is it normal?
Should I expect it to work after the training is over or just terminate the training and increase the density of my network?
","['reinforcement-learning', 'deep-rl', 'pytorch', 'actor-critic-methods', 'a3c']",
Extract features with CNN and pass as sequence to RNN,"
I read an article about captioning videos and I want to use solution number 4 (extract features with a CNN, pass the sequence to a separate RNN) in my own project.
But for me, it seems really strange that in this method we use the Inception model without any retraining or something like that. Every project has different requirements and even if you use pretrained model instead of your own, you should do some training.
And I wonder how to do this? For example, I created a project where I use the network with CNN layers and then LSTM and Dense layers. And in every epoch, there is feed-forward and backpropagation through the whole network, all layers. But what if you have CNN network to extract features and LSTM network that takes sequences as inputs. How to train CNN network if there is no defined output? This network should only extract features but the network doesn't know what features. So the question is: How to train CNN to extract relevant features and then passing these features to LSTM?
","['neural-networks', 'deep-learning', 'tensorflow', 'python', 'long-short-term-memory']","The approach that you don't train the whole net, but just the latter part of it (all starting with lstm in our case), can actually work. The idea is that the inception was already pretrained a very large dataset (imagenet for instance). And it's capable of extracting some useful information from it. Actually there are different domains of images in the imagenet and the inception net needed to capture a vast variety of input information to classify images well. The idea is that the pretrained inception is already capable to extract almost everything what could possibly be useful (unless your images aren't something completely different from imagenet, but that a rare case). Then you adapt the lstm layers and the fully connected layers to correctly process that information. Maybe you aren't going to get the perfect score with this approach and maybe it's better to train the whole large net including the inception part on the new data to lower the distributional shift and that's what people usually do in fact, but it takes more time to train and if you don't have enough data you won't be able to achieve results that are significantly better than those with a frozen CNN part."
Can we use genetic algorithms to evolve datasets?,"
Genetic algorithms are used to solve many optimization tasks.
If I have a dataset, can I evolve it with a genetic algorithm to create an evolved version of the same dataset?
We could consider each feature of the initial dataset as a chromosome (or individual), which is then combined with other chromosomes (features) to find more features. Is this possible? Has this been done?
I will like to edit the details with an example so that it is easier to understand.
Example: In practice cyber-security attacks evolve over time since it finds a new way to breach a system. The main draw-back of intrusion detection model is that it needs to be trained every time attack evolves. So I was hoping if genetic algorithm can be used on the present benchmarked datasets (like NSL-KDD) to come up with a futuristic type dataset maybe after X-number of generations.
And check if a model is able to classify that generated dataset as well.
","['datasets', 'genetic-algorithms', 'genetic-operators']",
"As an AI researcher, what subjects do you find yourself referring to most often? [closed]","







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 2 years ago.







                        Improve this question
                    



This is a bit of a weird question.
I am hoping to create an online reference since I have some downtime.  I know some about statistics but very little about computer science.  As a result, the reference guide I am hoping to create will be very statistics oriented - even though I wish that it could be a reference for someone who wants to start from scratch and work their way to AI.
While I would love to be involved with AI, from what I have read about ML and AI, seems like AI does not involve much statistics.  (A lot of statistical theory is based off normal assumption and math, and ML seems to bypass that by not requiring strong assumptions nor analytical results). CS seems to be more relevant.
And so my question is, since my guide will mostly cover statistics, how relevant would it be for someone who wants to get into AI?  If it's not relevant, then I guess I'll just make my guide for someone who wants to get into stats/data science, as opposed to someone who wants to be an AI researcher.
I guess another way to phrase my question is, as an AI researcher, when you ""google"" stuff, wikipedia things, or go to your notes, what subjects are you looking at and what exactly are you googling?  Are you getting a refresher on how to code back propagation?  Or are you getting a refresher on the pros and cons of L1 vs. L2?  Do you ever look at how to implement a boosting tree or NN using a pre-existing package?
Basically, I know that what I can provide will be relevant to HS/college stats and data science students.  But what really want to do is create something useful for aspiring/current AI researchers.  The former is realistic, the latter is a dream.  I want to see if my dream is realistic.
Thanks!
",['research'],"I think AI researcher mostly google some new papers because now there's just a crazy amount of them published. Sometimes people just forget the new concept which was introduced in a paper they read several months ago and they google that concept. Sometimes I forget some loss functions (and intuition behind them) used in a specific area like computer vision, natural language processing or audio processing. Like dice loss or contrastive losses. So these are more advanced things than pros and cons of L1 and L2 losses and how to code backprop. Usually I find the answers on https://distill.pub/ or https://towardsdatascience.com/. Have a look at those, I think they represent correctly current interests and topics for refreshment in AI research community. From my experience I can say not much statistics is used in contemporary AI research (unless you're doing research in statistical learning theory). Sometimes I google some statistical tests to prove results of my experiments are statistically significant and I think that's it."
Zero shot learning available labels in testing set,"
As we all know, zero shot learning involves a model predicting classes that it has not seen. But we are given all the attributes each class might have.
Is it fair to assume that we are ""aware"" of all the class labels a dataset might have ? (Including the test set)
","['neural-networks', 'deep-learning', 'zero-shot-learning']",
How to use a NN for seq2seq tasks?,"
I am trying to make a NN(probably with dense layers) to map a specific input to a specific output (or basically sequence2sequence). I want the model to learn the relation between the sequences and predict the output of any other input I give it.
I have 2 files - one with the inputs and another with all the corresponding outputs and would probably use a bunch of Dense Layers with word embeddings to vectorize it into higher dimensions. However, I cannot find any good resources out there for that.
Does anyone know how to accomplish such an NN? Which architectures are best for pattern matching? examples, links, and other resources would be very welcome. I was considering using RNN's but found them not very good in the pattern matching tasks so had ditched them. I would still consider them if someone can provide a plausible explanation...
","['machine-learning', 'deep-learning', 'prediction', 'pattern-recognition', 'architecture']",
How to train a model to predict the number of people at a certain bus stop before they cumulate in large numbers?,"
Each person probably uses an app that tracks his/her position periodically and sends it to our servers. What I want is to use these data to train a model to predict the rush hours of each bus-stop on the map, so we can send extra buses to handle the predicted cumulation before it happens.
I have no experience in AI nor machine learning. So, which model should I use to do this?
","['machine-learning', 'ai-design', 'prediction', 'ai-development']",
What are some suitable positive functions as activations of neural networks?,"
I am working on a deep Q-learning project. My project is different than normal deep Q-learning. The rewards of my neural network must be positive because I need their values to importance sample actions. I know that I can't use ReLU as the activation function of my neural network. So the only suitable functions which I know are sigmoid, softmax and exponential function. I tried working with sigmoid and softmax but they generate wrong results and the loss function diverges.
There are two terminal states in my model. Their rewards are 1 and 0. All other states don't have any immediate rewards.
","['neural-networks', 'reinforcement-learning']","First of all: an activation function is usually placed after a linear operation and you can have a lot of them (maybe different) in your nn. That's why it would be better to say $\bf a$ activation function of the neural net and not $\bf the$ activation function. So if you meant by activation function the last operation which is going to make your outputs non-negative, then you're right, ReLU isn't a good choice. Usually when people need to output some positive values, they take exponent as the last operation. I used it several times and everything was just fine."
"BERT: After pretraining 880000 step, why fine-tune not work? [closed]","







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



I am using pretraining code from https://github.com/NVIDIA/DeepLearningExamples
Pretrain parameters:
 15:47:02,534: INFO tensorflow 140678508230464   init_checkpoint: bertbase3layer-extract-from-google
 15:47:02,534: INFO tensorflow 140678508230464   optimizer_type: lamb
 15:47:02,534: INFO tensorflow 140678508230464   max_seq_length: 64
 15:47:02,534: INFO tensorflow 140678508230464   max_predictions_per_seq: 5
 15:47:02,534: INFO tensorflow 140678508230464   do_train: True
 15:47:02,535: INFO tensorflow 140678508230464   do_eval: False
 15:47:02,535: INFO tensorflow 140678508230464   train_batch_size: 32
 15:47:02,535: INFO tensorflow 140678508230464   eval_batch_size: 8
 15:47:02,535: INFO tensorflow 140678508230464   learning_rate: 5e-05
 15:47:02,535: INFO tensorflow 140678508230464   num_train_steps: 10000000
 15:47:02,535: INFO tensorflow 140678508230464   num_warmup_steps: 10000
 15:47:02,535: INFO tensorflow 140678508230464   save_checkpoints_steps: 1000
 15:47:02,535: INFO tensorflow 140678508230464   display_loss_steps: 10
 15:47:02,535: INFO tensorflow 140678508230464   iterations_per_loop: 1000
 15:47:02,535: INFO tensorflow 140678508230464   max_eval_steps: 100
 15:47:02,535: INFO tensorflow 140678508230464   num_accumulation_steps: 1
 15:47:02,535: INFO tensorflow 140678508230464   allreduce_post_accumulation: False
 15:47:02,535: INFO tensorflow 140678508230464   verbose_logging: False
 15:47:02,535: INFO tensorflow 140678508230464   horovod: True
 15:47:02,536: INFO tensorflow 140678508230464   report_loss: True
 15:47:02,536: INFO tensorflow 140678508230464   manual_fp16: False
 15:47:02,536: INFO tensorflow 140678508230464   amp: False
 15:47:02,536: INFO tensorflow 140678508230464   use_xla: True
 15:47:02,536: INFO tensorflow 140678508230464   init_loss_scale: 4294967296
 15:47:02,536: INFO tensorflow 140678508230464   ?: False
 15:47:02,536: INFO tensorflow 140678508230464   help: False
 15:47:02,536: INFO tensorflow 140678508230464   helpshort: False
 15:47:02,536: INFO tensorflow 140678508230464   helpfull: False
 15:47:02,536: INFO tensorflow 140678508230464   helpxml: False
 15:47:02,536: INFO tensorflow 140678508230464 **************************

Pretrain loss: (I remove nsp_loss)
{'throughput_train': 1196.9646684552622, 'mlm_loss': 0.9837073683738708, 'nsp_loss': 0.0, 'total_loss': 0.9837073683738708, 'avg_loss_step': 1.200513333082199, 'learning_rate': '0.00038143058'}
{'throughput_train': 1230.5063662500734, 'mlm_loss': 1.3001925945281982, 'nsp_loss': 0.0, 'total_loss': 1.3001925945281982, 'avg_loss_step': 1.299936044216156, 'learning_rate': '0.00038143038'}
{'throughput_train': 1236.4348949169155, 'mlm_loss': 1.473339319229126, 'nsp_loss': 0.0, 'total_loss': 1.473339319229126, 'avg_loss_step': 1.2444063007831574, 'learning_rate': '0.00038143017'}
{'throughput_train': 1221.2668264552692, 'mlm_loss': 0.9924975633621216, 'nsp_loss': 0.0, 'total_loss': 0.9924975633621216, 'avg_loss_step': 1.1603020071983337, 'learning_rate': '0.00038142994'}

Fine-tune code:
self.train_op = tf.train.AdamOptimizer(0.00001).minimize(self.loss, global_step=self.global_step)

Fine-tune accuracy: (restore from my ckpt pretrained from https://github.com/NVIDIA/DeepLearningExamples)
epoch 1:
training step 895429, loss 4.98, acc 0.079
dev loss 4.853, acc 0.092

epoch 2:
training step 895429, loss 4.97, acc 0.080
dev loss 4.823, acc 0.092

epoch 3:
training step 895429, loss 4.96, acc 0.081
dev loss 4.849, acc 0.092

epoch 4:
training step 895429, loss 4.95, acc 0.082
dev loss 4.843, acc 0.092

Without restore the pretrained ckpt:
epoch 1:
training step 10429, loss 2.48, acc 0.606
dev loss 1.604, acc 0.8036

Restore the google's BERT-Base pretrained ckpt. Or restore from a pretrained ckpt pretrained from https://github.com/guotong1988/BERT-GPU
epoch 1:
training loss 1.89, acc 0.761
dev loss 1.351, acc 0.869

","['transformer', 'bert', 'transfer-learning', 'pretrained-models', 'fine-tuning']",changeto
What are other examples of theoretical machine learning books?,"
I am looking for a book about machine learning that would suit my physics background. I am more or less familiar with classical and complex analysis, theory of probability, сcalculus of variations, matrix algebra, etc. However, I have not studied topology, measure theory, group theory, and other more advanced topics. I try to find a book that is written neither for beginners, nor for mathematicians.
Recently, I have read the great book ""Statistical inference"" written by Casella and Berger. They write in the introduction that ""The purpose of this book is to build theoretical statistics (as different from mathematical statistics) from the first principles of probability theory"". So, I am looking for some ""theoretical books"" about machine learning.
There are many online courses and brilliant books out there that focus on the practical side of applying machine learning models and using the appropriate libraries. It seems to me that there are no problems with them, but I would like to find a book on theory.
By now I have skimmed through the following books

Pattern Recognition And Machine Learning
It looks very nice. The only point of concern is that the book was published in 2006. So, I am not sure about the relevance of the chapters considering neural nets, since this field is developing rather fast.

The elements of statistical learning
This book also seems very good. It covers most of the topics as well as the first book. However, I am feeling that its style is different and I do not know which book will suit me better.

Artificial Intelligence. A Modern Approach
This one covers more recent topics, such as natural language processing. As far as I understand, it represents the view of a computer scientist on machine learning.

Machine Learning A Probabilistic Perspective
Maybe it has a slight bias towards probability theory, which is stated in the title. However, the book looks fascinating as well.


I think that the first or the second book should suit me, but I do not know what decision to make.
I am sure that I have overlooked some books.
Are there some other ML books that focus on theory?
","['machine-learning', 'reference-request', 'computational-learning-theory', 'books']","Some of the books that you mention are often used as reference books in introductory courses to machine learning or artificial intelligence.For example, if I remember correctly, in my introductory course to machine learning, the professor suggested the book Pattern Recognition And Machine Learning (2006) by Bishop, although we never used it during the lessons. This is a good book, but, in my opinion, it covers many topics, such as variational inference or sampling methods, that are not suited for an introductory course.The book Artificial Intelligence. A Modern Approach, by Norvig and Russell, definitely does not focus on machine learning, but it covers many other aspects of artificial intelligence, such as search, planning, knowledge representation, machine learning, robotics, natural language processing or computer vision. This is probably the book that you should read and use if you want to have an extensive overview of the AI field. Although I never fully read it, I often used it as a reference, as I use the other mentioned book. For instance, during my bachelor's and, more specifically, an introductory course to artificial intelligence, we had used this book as the reference book, but note that there are other books that provide an extensive overview of the AI field.The other two books are not as famous as these two, but they are probably also good books, although their focus may be different.There are at least three other books that I think you should also be aware of, given that they also cover the actual theory of learning, aka (computational) learning theory, before diving into more specific topics, such as kernel methods.You can find more books on learning theory here."
How could I convolve a 4D image and a 4D filter with stride?,"
I want to create a CNN in Python, specifically, only with NumPy, if possible. For optimizing the time of convolution (actually correlation) in the network, I wanna try to use FFT-based convolution. The data that needs to be convoluted (correlated) is a 4D image tensor with shape [batch_size, width, height, channels] and 4D filter tensor [filter_width, filter_height, in_channel, out_channel]. I read a lot of articles about FFT-based convolution, but they aren't doing it in my way. Thus, I need your help.
How could I fft-convolve a 4D image and a 4D filter with stride?
","['neural-networks', 'convolutional-neural-networks', 'python', 'convolution']",
Can we use NLP to understand/parse/compile programming code?,"
I wonder if we can use Natural Language Processing (NLP) to process programming code:
Given a piece of code, can we

Translate it to human language to understand what it does? The input could be a function definition（normally lack of documentation) in Python and the output could be the documentation for that function.
Compile or translate it to another programming language? Compile Python code to C or machine code, or translate C code to Python code?

","['natural-language-processing', 'machine-translation']",
Strategy to input and get large images in VGG neural networks,"
I'm using a transfert-style based deep learning approach that use VGG (neural network). The latter works well with images of small size (512x512pixels), however it provides distorted results when input images are large (size > 1500px). The author of the approach suggested to divide the input large image to portions and perform style-transfert to portion1 and then to portion2 and finally concatenate the two portions to have a final large result image, because VGG was made for small images... The problem with this method is that the resulting image will have some inconsistent regions at the level of areas where the portions were ""glued"". How can I correct these areas ? Is there an alternative approach to this dividing method ?
thanks
","['deep-learning', 'python', 'vgg']",
How to implement RL policies learned on a finite horizon?,"
I am modelling a ride-hailing system where passenger requests continuously arrive into the system. An RL model is developed to learn how to match those requests with drivers efficiently.
Basically, the system can run infinitely as long as there are requests arriving (infinite horizon reality). However, in order for the RL training to conduct, the episode length should be restricted to some finite duration, say $[0,T]$ (finite horizon training).

My question is how to implement the learned policy based on finite horizon $[0,T]$ to the real system with infinite horizon $[0,\infty]$?

I expect there would be a conflict of objectives. The value function near $T$ is partially cut off in a finite horizon and would become an underestimate and affect policy performance in an infinite horizon implementation. To this end, I doubt the applicability of the learned policy.
","['machine-learning', 'reinforcement-learning']","A normal way to deal with training on infinite horizon (aka ""continuing"" or ""non-episodic"") problems is to use TD learning or other bootstrapping methods (of which Q-learning in DQN is one example), and to treat the cutoff at $T$ for pseudo-episodes as a training artefact.If the state at time $T$ was really a terminal state, the TD target would be just $r$ because $q(s^T,\cdot) = 0$ by definition, but that doesn't apply in your case.So always use the bootstrapped TD target - e.g. $r + \gamma \text{max}_{a'} \hat{q}(s',a',\theta)$ for single step TD target with a Q estimate having $\theta$ as learned params - and don't treat the horizon data any differently.If you do this, then your concerns about under-estimates should not be an issue. Your pseudo-episodes do need to be long enough to observe the impact of multiple requests in the long term is the main issue (setting $T$ too low so that the system does not reach any kind of equilibrium would be a problem).You could also use an average reward setting and differential value functions. It is slightly better from a theoretical standpoint, but Q-learning and DQN is fine if you don't want to be bothered with that. The same basic rule applies - ignore ""episode end"" for constructing TD targets because it is just a training artefact. Also ensure you set $T$ high enough that the long term impacts of a policy are observable.If your starting state is special (e.g. cars all in fixed places, and no requests in progress) this could also be an issue, because the real world system will rarely be in that state but you will have many episode starts with it. If you cannot start the system in a reasonable random initial state for your problem, you may also want to discard data from episode starts and allow a run-in time before using data for training from the pseudo-episodes."
What are the rules behind vector product in gradient?,"
Let's suppose we have calculated the gradient and it came out to be $f(WX)(1-f(W X))X$, where $f()$ is the sigmoid function, $W$ of order $2\times2$ is the weight matrix, and $X$ is an input vector of order $2\times 1$. For ease let $f(WX)(1-f(W X))=\Bigg[
\begin{array}{c}
0.3 \\
0.8 \\
\end{array}\Bigg]$ and $X=\Bigg[
\begin{array}{c}
1 \\
0 \\
\end{array}\Bigg]$. When we multiply these vectors we will multiply them as $f(WX)(1-f(W X))\times X^T$ i.e $\Bigg[
\begin{array}{c}
0.3 \\
0.8 \\
\end{array}\Bigg]\times[1 \quad0]$. I do this because I know that we need this gradient to update a $2\times 2$ weight matrix, hence, the gradient should have size $2\times 2$. But, I don't know the law/rule behind this, if I was just given the values and had no knowledge that we need the solution to update the weight matrix, then, I might have done something like $[0.3 \quad 0.8]\times\Bigg[
\begin{array}{c}
1 \\
0 \\
\end{array}\Bigg]$ which will return a scalar. For a long chain of such operations (multiple derivatives in applying chain rule, resulting in many vectors), how do we know if the multiplication of two vectors should return a vector or matrix (dot or cross product)?
","['neural-networks', 'recurrent-neural-networks', 'backpropagation', 'deep-neural-networks', 'gradient-descent']","It helps to think of each output dimension separately.You have $X$ which is an $(2 \times 1)$ vector, and $W_1$ is a $(1 \times 2 )$ vector. Their product is a scalar, of which we then take the sigmoid to get our output $Y_1$.The gradient of this w.r.t. $W_1$ will be $f(WX) (1 - f(WX)) X^T$, which has the appropriate dimensions.Then, you just stack these for all your outputs, giving you the shape you got."
Hyper-plane in logistic regression vs linear regression for same number of features,"
Geometric interpretation of Logistic Regression and Linear regression is considered here.
I was going through Logistic regression and Linear regression. In the optimization equation of both following term is used. $$W^{T}.X$$
W is a vector which holds weights of the hyper-plane.
I realized following about the dimensions of the fitted hyper-plane. Want to confirm it.
Let,
d = Number of features for both Logistic Regression and Linear Regression.
Logistic Regression case:
Fitted hyper-plane is d-dimensional.
Linear Regression case:
Fitted hyper-plane is (d + 1) dimensions.
Example
d = 2
feature 1 : weight,
feature 2 : height
Logistic Regression:
Its a 2 class classification.
y : {obsess, normal)
Linear Regression:
y: blood pressure (real value)
Here,

Logistic Regression will fit a 2-D line.
Linear Regression will fit a 3-D plane.

Please confirm if this understanding is correct and same happens even in higher dimensions.
","['linear-regression', 'logistic-regression']",
"What is a ""learned emulator""?","
In this article, the term ""learned emulator"" is used.

Recently, scientists have started creating ""learned emulators"" using
AI neural network approaches, but have not yet fully explored the
advantages and potential pitfalls of these surrogates.

What is a ""learned emulator""?  I believe it is related to neural networks. Where can I read more?
","['machine-learning', 'terminology', 'applications']",
How can I build a recommendation system that takes into account some constraints or the context?,"
I am building a recommendation system that recommends relevant articles to the user. I am doing this using simple similarity-based techniques (with the Jaccard similarity) using as features the page title, the tags, and the article content.
Now my problem is I have different ""adult articles"" and some are articles that expire (for example, an article about a movie in Jan 2019 would not be relevant in Dec 2019).
I want to keep these adult articles separate, as a person who is reading about history does not want to be led to an adult article and not recommend articles that have expired or would not be relevant in the present moment.
Should I just improve the quality of my features or tags? Or is there any other way to achieve this?
","['machine-learning', 'ai-design', 'recommender-system', 'jaccard-similarity']",
Are mult-adds and FLOPs equivalent?,"
I am comparing different CNN architectures for edge implementation. Some papers describing architectures refer to mult-adds, like the MobileNet V1 paper, where it is claimed that this net has 569M mult-adds, and others refer to floating-point operations (FLOPs), like the CondenseNet paper claims 274M FLOPs.
Are these comparable? Is 1 multiply-add equivalent to 2 floating-point operations? Any direction will be greatly appreciated.
","['convolutional-neural-networks', 'terminology', 'papers', 'complexity-theory']",
Is there any way where you can train a Neural Network with only one data point in the dataset?,"
I was working on a project involving the search for biosignatures (signs of life) on exoplanets and the probability of that planet harboring life. In this case, we know that Earth is the only planet confirmed to have life on it. So the parameters of atmospheric conditions, radius, temperature, distance from the star for planets confirmed to have life is one (Earth).
Is there any way to use NNs to predict the probability of an exoplanet harboring life if we have the data of all these parameters for that planet?
","['neural-networks', 'deep-learning']",
How much should we augment our training data?,"
I am wondering how much I should extend my training set with data augmentation. Is there somewhere a pre-defined number I can go with?
Suppose I have 10000 images, can I go as far as 10x or 20x times, to get 100000 and 200000, respectively, images? I am wondering how will this impact model training. I am using a mask R-CNN.
","['machine-learning', 'data-preprocessing', 'data-augmentation', 'mask-rcnn']",
Neural Network for locating shifting resonant frequencies,"
I have multiple FFT's taken from a sample at different pressures, through different analysis I can see that the resonant frequencies are shifting in the spectrum for each FFT at a different pressure.
Using conventional peak tracking has been difficult as the peaks increase/decrease in magnitude within the FFT as well as shifting in the spectrum.
Is it possible for a neural network to 'detect'/'pick out' these frequency values?
Any help or guidance is appreciated :)
Thanks!
","['neural-networks', 'machine-learning', 'feature-extraction']",
What is the intuition behind the number of filters/channels for each convolutional layer?,"
After having chosen the number of layers for a convolutional neural network, we must also choose the number of filters/channels for each convolutional layer.
The intuition behind the filter's spatial dimension is the number of pixels in the image that must be considered to perform the recognition/detection task.
However, I still can't find the intuition behind the number of filters. The numbers 128 and 256 are often used in the literature, but why?
","['convolutional-neural-networks', 'hyper-parameters', 'filters', 'convolutional-layers']","The channel sizes 32, 128, etc. are used because of memory and efficiency. There is nothing holy about these numbers.The intuition behind choosing the number of channels is as follows-
The initial layers extract low-level features- they consist of edge detectors, etc. There aren't many such features. So, we won't gain much by adding a lot of filters (of course, if we use a 3x3 filters on an RGB image, we would have $2^{27}$ different filters even if our neurons have only 0 and 1 as their values. However, most of them are quite similar/meaningless for our job). Using a lot of filters might even lead to overfitting.The latter layers are responsible for detecting more nuanced features, like elbows/nose shape from the lower level features extracted previously. So, we might do better if we increase the number of channels. Also, note that the resultant layers become more and more sparse as we go deeper.Though it might differ in applications like super resolution image, in general, the number of channels stays the same or increases when we go deeper.A nice experiment would be to try and increase the number of channels until you get no more benefit from it. I believe there was a paper that did exactly this (please cite it if someone remembers). You could even try to visualise the filters at this stage and see if the filters are similar or not."
How do I write production systems?,"
I understand that I can draw a state-space graph for any problem. However, here is the problem: I can't really figure out how to make production systems.
I am solving the FWGC (Farmer, Wolf, Goat, Cabbage) River Crossing Puzzle using a state-space search. So, my tasks are that:

Represent the state-space graph (which I know how to do)

Write production systems.


My questions: How do I write production systems?
The thing that confused me, was the production system example in Rich's book (about the water jug problem), where he has imagined all the states possible and wrote the next state for them.
Here in the FWGC problem, I see some problems while writing the production system.
For instance, for a given state, there are multiple possible next states, i.e. a farmer can take Goat, Cabbage, Wolf, or go alone to the other side (assuming that all states are safe, just for the sake of simplicity).
So, how would I represent the same state going to multiple next states in production systems?
What I have tried-:
Then, I googled a pdf
https://www.cs.unm.edu/~luger/ai-final2/CH4_Depth-.%20Breadth-,%20and%20Best-first%20Search.pdf

Is that what I call the production system for this case?
But, here are my reasons why it should not be called a production system:

There might be other possible states as well.

It is showing only 1 solution.


So, how do I actually learn to create production rules (I know how to make state-space representation better as I have read that J.Nillison's book which was GOLD in this matter)? And, what would be the production rules in this case?
","['expert-systems', 'prolog', 'production-systems']",
"Is batch learning with gradient descent equivalent to ""rehearsal"" in incremental learning?","
I am learning about incremental learning and read that rehearsal learning is retraining with old data. In essence, isn't this the exact same thing as batch learning (with stochastic gradient descent)? You train a model by passing in batches of data and redo this with a set number of epochs.
If I'm understanding rehearsal learning correctly, you do the exact same thing but with ""new"" data. Thus, the only difference is inconsistencies in the epoch number across data batches.
","['neural-networks', 'deep-learning', 'gradient-descent', 'incremental-learning', 'batch-learning']",
Should image augmentation be applied before or after image resizing?,"
For the purposes of training a Convolutional Neural Network Classifier, should image augmentation be done before or after resizing the training images?
To reduce file size and speed up training time, developers often resize training images to a set height and width using something like PIL (Python Imaging Library).
If the images are augmented (to increase training set size), should it be done before or after resizing the members of the set?
For simplicity sake, it would probably be faster to augment the images after resizing, but I am wondering if any useful data is lost in this process. I assume it may depend on the method used to resize the images (cropping, scaling technique, etc.)
","['convolutional-neural-networks', 'python', 'image-processing']","As it is told in PIL documentationIt uses some filters to resize images.And those filters are explained here
uses mostly numerical methods as I see. So it is approximating the image data input. Which means you are right about data loss. But here might be the question would it change the data so much If it is done after augmentation or before?Since in numerical methodic approaches the more values means more valid approximation in general.So it might be beneficial to augment and then resize.But one should look for its real answer. Mine is just a thought experiment. You will get better proven answers"
Do the rows of the design matrix refer to the observations or predictors?,"
I attempt to understand the formulation of dictionary learning for this paper:

Depression Detection via Harvesting Social Media: A Multimodal Dictionary Learning Solution
Multimodal Task-Driven Dictionary Learning for Image Classification

Both papers used the exact formulation in two different domains.
Based on my understanding, in common machine learning, we formulate our matrices, from vectors, as rows to be observations, columns to be predictors.
Given a matrix, $A$:
\begin{array}{lcccccc} 
& p_1 & p_2 &  p_3 &  p_4 &  p_5 & \text { label } \\
o_1 & 1 & 2 & 3 & 4 & 1 & 1 \\
o_2 & 2 & 3 & 4 & 5 & 2 & 1 \\
o_3 & 3 & 4 & 5 & 6 & 2 & 0 \\
o_4 & 4 & 5 & 6 & 7 & 3 & 0
\end{array}
So, using a math notation and excluding the label, I can define this matrix, $A = [o_1, o_2, o_3, o_4]  ∈ R^{4×5}$, as $A = [{(1, 2, 3, 4, 1), (2, 3, 4, 5, 2), (3, 4, 5, 6, 2), (4, 5, 6, 7, 3)}]$, and in numpy:
import numpy as np

A = np.array([[1, 2, 3, 4, 1],
              [2, 3, 4, 5, 2],
              [3, 4, 5, 6, 2],
              [4, 5, 6, 7, 3]])

A.shape
# (4, 5)

Am I right?
","['machine-learning', 'math', 'papers', 'data-preprocessing', 'notation']","Based on my understanding, in common machine learning, we formulate our matrices, from vectors, as rows to be observations, columns to be predictors.The rows (or, in general, the first dimension of your tensor) are typically the observations. For example, in TensorFlow, the first dimension of the input tensor typically refers to the batch size, i.e. the number of observations. If you are using Pandas (a Python library to manipulate data), the rows are typically the observations and the columns are the predictors.However, in general, it does not really matter which convention you use, provided that you use one of the conventions consistently in your implementation (i.e. you choose one of the conventions and you stick with it throughout all your code, to avoid complexity), and make it clear in the documentation. So, you can have a matrix where either the rows or columns are observations and, consequently, the columns or, respectively, rows are the features (aka predictors or independent variables).Anyway, it is probably a good idea to be consistent with existing literature and implementations/libraries, so you should probably use the rows for the observations."
How to identify if 2 faces contain the same person?,"
I have got numerous frames and I've detected all the faces in all the frames using Retinaface. However I need to track the faces of people over frames.
For this purpose, I assumed I could try finding the landmarks from the face using libraries like dlib and maybe compare these landmarks to check if they are infact the face of the same person.
I would like to know if there are other methods or some useful resources I could refer for the same. Thanks a lot in advance.
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'facial-recognition']",
Use of virtual worlds (e.g. Second Life) for training Artificial General Intelligence agents?,"
There is emerging effort for Third Wave Artificial Intelligence (Artificial General Intelligence) (http://hlc.doc.ic.ac.uk/3AI_HLC_2019.html and https://www.darpa.mil/work-with-us/ai-next-campaign) and it covers the open-ended life-long machine learning as well. Currently machine learning agents are being run on quite immutable and simple games like Atari and Go. But what about the efforts to build and run machine learning adaptable agents (or even teams of them) in the virtual worlds (like Second Life) which are complex, expanding and in which the interaction with human representatives happens? Are there efforts to do that?
I have found some articles from 2005-2009, but Google gives no recent literature on queries like Reinforcement Learning Second Life etc.
So - maybe there are some efforts to do this, but I can not just Google it.
My question is - are there references for machine learning agents for virtual worlds and if not - what are the obstacles for trying to build them? There are little risks or costs for building them for virtual worlds?
https://meta-guide.com/embodiment/secondlife-npc-artificial-intelligence is some bibliography and it is lacking recent research, for example.
","['reinforcement-learning', 'reference-request', 'game-ai', 'agi', 'intelligent-agent']",
What's the difference between RNNs and Feed Forward Neural Networks if a fixed size vector can preserve sequential information?,"
I was watching a Youtube video in which the problem of trying to predict the last word in  a sentence was posed. The sentence was ""I took my cat for a"" and the last word was ""walk"". The lecturer in this video stated that whilst sentences (the sequence) can be of varying lengths, if we take a really large fixed window we can model the whole sentence. In essence she said that we can convert any sentence into a fixed size vector and still preserve the order of the sentence (sequence). I was then wondering why do we need RNNs if we can just use FFNNs? Also does a fixed size vector really preserve sequential order information?
Thank You for any help!
","['recurrent-neural-networks', 'time-series', 'feedforward-neural-networks', 'sequence-modeling', 'word2vec']",
How could facts be distinguished from opinions?,"
As a software engineer, I am searching for an existing solution or, if none exists, willing to create one that will be able to process texts (e.g. news from online media) to extract/paraphrase dry facts from them, leaving all opinions, analysis, speculations, humor, etc., behind.
If no such solution exists, what would be a good way to start creating it (considering that I have zero experience in AI/machine learning)?
It would be no problem to manually create a set of examples (pairs of original news + dry facts extracted), but is that basically what it takes? I doubt so.
(This knowledge domain is already huge, so which parts of it need to be learned first and foremost to figure out how to achieve the goal?)
","['natural-language-processing', 'reference-request', 'natural-language-understanding', 'text-summarization']","I will be starting my PhD in natural language processing in a few days and this is very similar to my proposed topic. It's an open problem that ties NLP and AI into philosophy of science and epistemology and is, I think, extremely interesting. I say all this to drive home the point that this is not a simple problem.Two major theoretical concerns come to my mind:What is a ""fact""? Is it a universal truth, if there is such a thing? Or is it a generally accepted theory, and if so how do you measure acceptance? That is, accepted by whom, where, when?Are there any linguistic markers of opinions vs. facts? Only in rare cases, such as when the speaker prefaces their statement with something like ""I believe"". In most cases, I think, opinions will be stated linguistically similarly to facts. For example, compare ""Cats are felines."" (a ""fact"") with ""Cats are aliens."" (an opinion some may hold). They have the exact same syntactic structure. The difference here is deeply semantic, and probably relates to the speaker's intention. I'd venture that often people state their opinions with the intention of communicating a ""fact"".Some more practical concerns are:Information extraction (also called relationship extraction, text mining, etc.), which for the most part assumes that the ""facts"" given in the labeled datasets are correct, is far from a solved problem. E.g. the state of the art model developed for a task released in 2010 has an F1 of only 76! What you propose adds significant uncertainty to these types of tasks.I suspect that even if you were able to compile a dataset of facts and opinions with corresponding labels you would encounter a number of modeling problems. Given the linguistic similarity between the statements of facts and opinions, I'd guess that your model will simply memorize the dataset, making it generalize poorly to your test set. Either that or it would would pick up on random, hidden correlations in the data to solve the problem (neural nets are really good at this), perhaps generalizing to the test set, but failing to apply to any other data.Fact vs. opinion is something that is embedded in a cultural milieu, so a model would, I think, need access to some proxy for what is culturally accepted in order to make this distinction, perhaps a via knowledge base. This may be feasible for limited, highly curated domains (e.g. biomedicine), but there is currently nothing suitable for a general-purpose fact finder.tldr: No, it is not enough to simply create a dataset of facts vs. opinions. This problem poses major theoretical concerns related to epistemology, linguistics, and cognitive science. Additionally, there are more mundane (but non-trivial!) modeling issues to consider. @Sceptre is right that it will be impossible to start this without knowledge of AI/ML/NLP, especially a rather deep knowledge of what current AI systems are really capable of."
Can entire neural networks be composed of only activation functions?,"
Inverse Reinforcement Learning based on GAIL and GAN-Guided Cost Learning(GAN-GCL), uses a discriminator to classify between expert demos and policy generated samples.
Adversarial iRL, build upon GAN-GCL, has its discriminator $D_{\theta, \phi}$ as a function of a state-only reward approximator $f_{\theta, \phi}$.
$$
D_{\theta, \phi}\left(s, a, s^{\prime}\right)=\frac{\exp \left\{f_{\theta, \phi}\left(s, a, s^{\prime}\right)\right\}}{\exp \left\{f_{\theta, \phi}\left(s, a, s^{\prime}\right)\right\}+\pi(a \mid s)},
$$
where $f_{\theta,\phi}$ is expressed as:
$$f_{\theta,\phi} = g_{\theta} (s) + γh_φ (s\prime ) − h_φ (s).$$
The optimal $*g(s)$ tries to recover the optimal reward function $r^*(s)$. While the $h(s)$ tries to recover the optimal value funtion $V^*(s)$, which makes $f_{\theta,\phi}$ interpretable as the advantage.
My question comes from the network architecture used for $h(s)$ in the original paper.

... we use a 2-layer ReLU network for the shaping term h. For the policy, we use a two-layer (32 units) ReLU gaussian policy.

What is meant by the quoted text in bold, because my interpretation of that text, (shown below) doesn't seem viable
h = nn.Sequential([nn.ReLu(), nn.ReLu()])

","['neural-networks', 'reinforcement-learning', 'papers', 'activation-functions', 'inverse-rl']",
"In Alpha(Go)Zero, why is the policy extracted from MCTS better than the network one?","
I've read through the Alpha(Go)Zero paper and there is only one thing I don't understand.
The paper on page 1 states:

The MCTS search outputs probabilities π of playing each move. These search probabilities usually select much stronger moves than the raw move probabilities p of the neural network fθ(s);

My question: Why is this the case? Why is $\pi$ usually better than $p$? I think I can imagine why it's the case but I'm looking for more insight.
what $\pi$ and $p$ are:
Say we are in state $s_1$. We have a network that takes the state and produces $p_1$ (probabilities for actions) and $v_1$ (a value for the state).
We then run MCTS from this state and extract a policy $\pi(a|s_1) = \frac{N(s_1,a)^{1/\tau}}{\sum_b N(s_1,b)^{1/\tau}}$.
The paper is saying that $\pi(-|s_1)$ is usually better than $p_1$.
","['reinforcement-learning', 'monte-carlo-tree-search', 'alphazero', 'alphago-zero']",
Are there neural networks with 3-dimensional topologies?,"
The topologies (or architectures) of the neural networks that I have seen so far are only 2-dimensional. So, are there neural networks whose topology is 3-dimensional (i.e. they have a width, height, and depth)?
","['neural-networks', 'convolutional-neural-networks', 'reference-request']","Yes. Convolutional neural networks are usually 3-dimensional. In fact, they usually deal with images (e.g. RGB images), which can already be 3-dimensional."
Why does CNN forward pass take longer compared to MLP forward pass? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



Let's take a 32 x 32 x 3 NumPy array and convolve with 10 filters of size 2 x 2 x 3 with stride 2 to produce feature maps of volume 16 x 16 x 10. The total number of operations - 16 * 16 * 10 * 2 * 2 * 2 * 3 = 61440 operations. Now, let's take an input array of length 3072 (flattening the 32 * 32 * 3 array) and dot it with a weight matrix of size 500 x 3072. The total number of operations - 500 * 3072 * 2 = 3072000 operations. The convolution takes 4-5 times longer than np.dot(w, x) even though number of operations is less.
Here's my code for the convolution operation:
for i in range(16):
    for j in range(16):
        for k in range(10):
            v[i, j, k] = np.sum(x[2 * i:2 * i + 2, 2 * j:2 * j + 2] * kernels[k]) 

Is np.dot(w, x) optimized or something? Or are my calculations wrong? Sorry if this is a silly question...
","['convolutional-neural-networks', 'time-complexity']",
Why do we need to go back to policy evaluation after policy improvement if the policy is not stable?,"

Above is the algorithm for Policy Iteration from Sutton's RL book. So, step 2 actually looks like value iteration, and then, at step 3 (policy improvement), if the policy isn't stable it goes back to step 2.
I don't really understand this: it seems like, if you do step 2 to within a small $\Delta$, then your estimate of the value function should be pretty close to optimal for each state.
So, why would you need to visit it again after policy improvement?
It seems like policy improvement only improves the policy function, but that doesn't affect the value function, so I'm not sure why you'd need to go back to step 2 if the policy isn't stable.
","['reinforcement-learning', 'value-iteration', 'policy-iteration', 'policy-evaluation', 'policy-improvement']",
What are some programming-oriented resources for reinforcement learning?,"
I have been reading: Reinforcement Learning: An Introduction by Sutton and Barto. I admit it's a good read for learning RL whereas it's more theoretical with detailed algorithms.
Now, I want something more programming oriented resource(s) maybe a course, book, etc. I have been exploring Kaggle, Open-source RL projects.
I need this to learn and grasp a deeper understanding of RL from the perspective of a developer i.e optimized way of writing code, explanation about using the latest RL libraries, cloud services, etc.
","['reinforcement-learning', 'python', 'resource-request']",
What are the mathematical prerequisites needed to understand research papers on neural networks? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I know we have developed some mathematical tools to understand deep neural networks, gradient descent for optimization, and basic calculus. Recently, I encountered arxiv paper that describes higher mathematics for neural networks, such as functional analysis. For example, I remember universal approximation theorem was proved with the Hann-Banach theorem, but I lost the link of that article, so I need to find similar papers or articles to develop my understanding of neural networks mathematically (like with functional analysis, in short, I need to learn more advanced math for research), can you suggest some books or arxiv papers or articles or any other source that describes mathematics for deep neural networks?
","['neural-networks', 'deep-neural-networks', 'research', 'resource-request']",
Value Iteration failing to converge to optimal value function in Sutton-Barto's Gambler problem,"
In Example 4.3:Gambler's Problem of Sutton and Barto's book whose code is given here.
In this code the value function array is initialized as np.zeros(states) where states $\in[0,100]$ and the value function for optimal policy which is returned after solving it with value iteration is same as the one given in the book, but, if we only change the initialization of the value function in the code, suppose to np.ones(states) then the optimal value function returned changes too, which means that the value iteration algorithm converges in both the cases but to different optimal value functions,but two different optimal value function is impossible in a MDP. So why is the value iteration algorithm not converging to optimal value function?
PS: If we change the initialization of value function array to -1*np.random.rand(states), then the converged optimal value function also contains negative numbers which should be impossible as rewards>=0, hence value iteration fails to converge to optimal value function.
","['reinforcement-learning', 'value-functions', 'sutton-barto', 'value-iteration', 'numpy']","So, naturally, if you've observed something that contradicts the theoretical properties of Value Iteration, something's wrong, right?Well, the code you've linked, as it is, is fine. It works as intended when all the values are initialized to zero. HOWEVER, my guess is that you're the one introducing an (admittedly very subtle) error. I think you're changing this:for this:So, you see, this is wrong. And the reason why it's wrong is that both GOAL (which is 100 in the example) and 0 must have an immutable and fixed values, because they're terminal states, and their values are not subject to estimation. The value for GOAL is 1.0, as you can see in the original code. If you want initial values other than 0, then you must do this:In the first case (changing the initial values to 1) what you were seeing was, essentially, an ""I don't care policy"". Whatever you do, you'll end with a value of 1. In the second case, with the random values, you saw the classic effects of ""garbage in, garbage out""."
Bert for Sentiment Analysis - Connecting final output back to the input,"
I have not found a lot of information on this, but I am wondering if there is a standard way to apply the outputs of a Bert model being used for sentiment analysis, and connect them back to the initial tokenized string of words, to gain an understanding of which words impacted the outcome of the sentiment most.
For example, the string ""this coffee tastes bad"" outputs a negative sentiment.  Is it possible to analyze the output of the hidden layers to then tie those results back to each token to gain an understanding of which words in the sentence had the most influence on the negative sentiment?
The below chart is a result at my attempt to explore this, however I am not sure it makes sense and I do not think I am interpreting it correctly. I am basically taking the outputs of the last hidden layer, which in this case has shape (1, 7, 768), [CLS] + 5 word tokens + [SEP], and looping through each token summing up their values (768) and computing the average. The resulting totals are outputted in the below graph.

Any thoughts around if there is any meaning to this or if i am way off on approach, would be appreciated.  Might be my misunderstanding around the actual output values themselves.
Hopefully this is enough to give someone the idea of what i am trying to do and how each word can be connected to positive or negative associations that contributed to the final classification.
","['bert', 'sentiment-analysis']",
What is the efficiency of trained neural networks?,"
Training neural networks takes a while. My question is, how efficient is a neural network that is completely trained (assuming it's not a model that is constantly learning)?
I understand that this is a vague and simply difficult question to answer, so let me be more specific: Imagine we have a trained Deep Neural Net, and even to be more specific it's a GPT-3 model.
Now, we put the whole thing on a Raspberry Pi. No internet access. The whole process takes place locally.

Will it run at all? Will it have enough RAM?

Now let's say we give it some text to analyze. Then we ask it a question. Will it take milliseconds to answer? Or is it going to be in the seconds? Minutes?


What I'm trying to understand, once a model is trained is it fairly performant because it's just essentially a bunch of very simple function calls on top of each other, or is it very heavy to execute? (perhaps due to the sheer number of these simple function calls)
Please correct any misunderstanding about how the whole process works if you spot any. Thank you.
","['neural-networks', 'gpt', 'efficiency', 'computational-complexity', 'benchmarks']",
What are some programming related topics that can be solved using NLP?,"
I've been working on the Punctuation Restoration Problem for my Master's Thesis, however, me being primarily a programmer at heart, I wish I could use some of my NLP skills to solve issues related to programming in general.
I know Microsoft does lots of research in NLP and I think after they acquired Github, they have an immense dataset to work with for any problems related to programming they want to tackle. Most recently I think they did a great job on their new python suggestion extension on VSCode.
So, could you suggest to me some issues you think are interesting research topics? This is something that I would like to work with, but I have no idea where to start yet.
",['natural-language-processing'],
Can in principle GPT language models learn physics?,"
Does anyone know of research involving the GPT models to learn not only regular texts, but also learn from physics books with the equations written in latex format?
My intuition is that the model might learn the rules relating equations and deductions, as they can learn statistically what correlates with what. I understand that the results can also be a little nonsensical, like the sometimes surreal paragraphs written by these models.
Have there been any attempts to do this?
","['gpt', 'natural-language-understanding']",
Handling a Large Discrete Action Space in Deep Q Learning,"
I am attempting to solve a timetabling problem using deep Q learning. It could be thought of as a resource allocation problem to obtain some certificate of 'optimality'. However, how to define and access the action space is alluding me. Any help, thoughts, or direction towards the literature would be appreciated. Thanks!
The problem is entirely deterministic, the pair of the current state and action is isomorphic to the resulting state. The Q network is therefore being set up to approximate a Q value (a scalar) for the resulting state, i.e. for the current state and proposed action.
I have so far assumed that the action space should be randomly sampled during training to generate some approximation of the Q table. This seems highly inefficient.
I am open to reinterpretations of the action space. The problem involves a set of n individuals and at any given state a maximum of b can be 'active' and, of the remaining 'inactive' individuals, f can be made 'active' by an action. An action will need to involve making some reallocation to active individuals made up of those who are already active and the other f available people.
To give you a sense over the numbers that I will ultimately use, $n=17, b=7$, and $f$ will hover somewhere around 7-10 (but depends on the allocations). At first this sounds tractable, but a (very) rough approximation of the cardinality of the set of actions is 17 choose 7 = 19448.
Does anyone know a more efficient way to encode this action space? If not, is there a more sensible way to sample it (as is my current plan) than uniformly extracting actions from the space? Also when sampling the space is it valid to enforce some cap on the number of samples drawn (say 500). Please feel free to ask for further clarification.
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'deep-learning', 'q-learning']",
Can we identify only the objects in specific parts of an image with computer vision?,"
I am studying computer vision for the past 3 months. I have come across the object identification problem, where given an image, CV would identify various parts in the image.
If I give an image, and a rectangle coordinates, can CV identify the parts' names within that rectangle? For example, can I train a model to identify the parts in the below image (mountain, river, in this case)? The model should not identify other parts like flowers, sky, etc., as they come outside the rectangle).
I tried searching but could not find similar problems. Can anyone give me a direction to solve this problem?

","['machine-learning', 'computer-vision', 'image-recognition']",
What's a good neural network for this problem?,"
I am very new to the field of AI so please bear with me.
Say there is a dice with three sides, -1,0 and 1, and I want to predict which side it lands on (so only one output is needed I guess). The input variables are numerous but not that many, maybe 7-10.
These input variables are certain formulae that involve calculations to do with wind, time, angle, momentum etc, and each formula returns which side it thinks the dice will like roll. Let's say that intuitively, by looking at these variables, I can make a very good guess at which side the dice lands on. If for example 6 out of 7 input variables say it likely that the dice will land on 1 but the 7th input suggests that it will land on 0, I would guess it lands on 1. As a human, I'm essentially consulting these inputs as a kind of ""brains trust"", and I act as a judge to make the final decision based on the brains trust. Of course in that example, my logic as a judge was simply majority rules, but what if some other more complicated non-linear method of judging was needed?
I essentially want my neural network to take this role as a judge. I have read that feedforward nns have limitations regarding control flow and loops, so I'm not sure if that structure will be appropriate. I'm not sure if recurrent nn will be appropriate either as I don't care what the previous inputs were.
Thanks
","['neural-networks', 'reinforcement-learning', 'convolutional-neural-networks', 'recurrent-neural-networks', 'feedforward-neural-networks']","A simple feed-forward neural network with at least one hidden layer would suffice in your problem, and can deal with arbitrary non-linear relationships between input and output. If you expect relationships to be highly non-linear then additional layers might be required, but from your description of the problem, I would be surprised if you needed more than few layers, and a relatively small network.However, I note that:The input variables are numerous but not that many, maybe 7-10.This gives you $3^{10} = 59049$ possible inputs. That's not much in terms of amount of data needed for ML statistical models. Assuming that even the best predictions are still probabilistic, then you may only need a million or so examples to create a reasonably accurate lookup table, not needing a neural network at all.The strength of a neural network is to be able to generalise well from less examples than that. Of course, this is not perfect, but it would be able to do things such as notice if inputs 1,2 and 3 agree then that is always the most likely answer. If that turns out to be true (and not an accident of having low numbers of samples), then the NN could learn that useful pattern using far less data than a table-based approach.I have read that feedforward nns have limitations regarding control flow and loops, so I'm not sure if that structure will be appropriate.This is true, but does not impact your situation, because there is no control flow or loops involved. You have described a simple function. Whilst you or I might inspect the data and look backwards and forwards across it before coming to a decision, a neural network approximating a function does not need to do that, and in simple cases there is usually no benefit to doing so - a statistical summary of the correct mapping from input to output is more than sufficient and likely the best that can be done.I'm not sure if recurrent nn will be appropriate either as I don't care what the previous inputs were.As all your inputs represent the same kind of thing, you could implement as a RNN with a single input, -1, 0 or +1, always feeding in the predictions by type in the same order. It might resemble how you are thinking about the problem as a human (at least a better analogy than the direct statistical match in a feed-forward network), especially if you implemented a variant of attention. However, I don't think there would be any benefit to that in improved accuracy, and it would be a significant challenge to build that if you are new to AI."
Is there a way to make my neural network discard inputs with bad results from learning?,"
What I want to achieve is this: If my desired outputs are [1, 2, 3, 4] I would rather have my network produce this output:
[0.99, 2.01, 999, 4.01]
than say this:
[0.94, 1.88, 3.12, 4.1]
So I'd rather have a few very accurate outputs and the rest completely off, than have them all be decent but no more than that. My question is, is there a known way to do this? If not, would it make sense to remove the inputs that produce poor outputs, and redo the learning phase?
","['neural-networks', 'machine-learning']",
Why is domain adaptation and generative modelling for knowledge graphs still not applied widely in enterprise data? What are the challenges?,"
I see that domain adaptation and transfer learning has been widely adopted in image classification and semantic segmentation analysis. But it's still lacking in providing solutions to enterprise data, for example, solving problems related to business processes?
I want to know what characteristics of the data determine the applicability or non-applicability with respect to generating models for prediction where multiple domains are involved within an enterprise information database?
","['neural-networks', 'machine-learning', 'applications', 'transfer-learning', 'domain-adaptation']",
"What is the dimension of my output of the form (2n + 1, 2n + 1, #filters) after a MaxPooling layer","
I'm trying to white board the different mechanisms behind a convolutional neural network.
I have on question regarding the dimension of my volume after using a max pooling layer.
Let's suppose I have a (21,21,#filtres) volume's dimension. If Max Pooling divide by 2 the height and width of my volume, what will be the dimension after the Max Pooling layer ?
If odd numbers are a problem when using max pooling layer, How do I fix it ?
Thank you !
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']",
How to normalise image input to backpropogation algorithm?,"
I am implementing a simple backpropagation neural network for classifying images. One set of images are cars another set of images are buildings (houses). So far I have used Sobel Edge detector after converting the images from black and white. I need a way to remove the offset (in other words normalise the input) of where the car or where the house is in the image.
Will taking the discrete Fourier cosine transform remove the offset? (so the input to the neural network will be the coefficients of the discrete cosine Fourier transform). To be clear, when I mean offset I mean a pair of values (across the number of pixels, and vertically the number of pixels) determining where the car or the building is in the 2D image from the origin.
","['computer-vision', 'backpropagation', 'image-processing', 'edge-detection', 'image-recognition']",
Bellman optimality equation in semi Markov decision process,"
I wrote a Python program for a simple inventory control problem where decision epochs are equally divided (every morning) and there is no lead time for orders (the time between submitting an order until receiving the order). I use the Bellman equations, and solve them by policy iteration (Dynamic Programming).
Now I want to consider lead time (between 1 to 3 days with equal probabilities). As I understand, the problem is defined by Semi Markov Decision Process for considering sojourn time in each state. I am confused about the Bellman equations in this scenario because we don't know exactly when the order will be received and is it necessary to discount the reward for day two or three?
","['reinforcement-learning', 'markov-decision-process', 'dynamic-programming', 'semi-mdp']","The core problem here is state representation, not estimating return due to delayed response to actions on the original state representation (which is no longer complete for the new problem). If you fix that, then you can solve your problem as a normal MDP, and base calculations on single timesteps. This allows you to continue using dynamic programming to solve it, provided the state space remains small enough.What needs to change is the state representation and state transitions. Instead of orders resulting in immediate change of stock levels, they become pending changes, and for each item you will have state representation for the amount of current stock, plus amount of stock in each lead time category. State transitions will modify expected lead time for each amount of pending stock as well as amount of current stock.Your lead time categories will depend on whether the agent knows the lead time immediately after making an order:If lead times are known, track remaining time until items arrive 1,2 or 3 days. These categories will be assigned by the enviroment following the order, then lead time will transition down on each day deterministically. A 1 day lead time will transition to in stock, 2 day lead will transition to 1 day etc.If lead times are not known, but probabilities of them are, track time since the order was made. This will be 0, 1 or 2 days. Although you don't know when an order will arrive, you know the probabilities for state transition - e.g. items in 0 days have a 1 in 3 chance of transitioning to ""in stock"" and a 2 in 3 chance of transitioning to 1 days.This makes the state space larger, but is less complex than moving to the Semi MDP representation. For instance, doing it this way means that you can still work with single time step transitions and apply dynamic programming in a standard way.In general, if the environment has a delayed response to actions, then the  best way to maintain Markov trait is to add relevant history of actions taken to the state. The added state variables can either be a direct list of the relevant actions, or something that tracks the logical consequence of those actions."
Which type of feature extractor do you suggest to classify sensor data?,"
I have IMU (Inertial Measurment Unit- 6 axis) sensor data. The sensor attached on a car and 7 different drivers wipe on same path. I want to extract features and classify drivers. Which type of feature extractor do you guys suggest? I am planning to use PCA and Autoencoders but what do you think about classical signal properties to classify drivers?
","['classification', 'autoencoders', 'time-series', 'feature-extraction', 'signal-processing']",There could be multiple possible ways to extract the features. One would be to use RNNs for a temporal relationship as the input data is time-series.
"Is my ""Insane Mind"" design for a classifier novel or effective?","
This question is in relation to a previous doubt of mine :
Are there neural networks where nodes are randomly selected from among a set of nodes (in random orders and a random number of times)?
I have made a bit of progress from there, refurbished my code, and got things ready.
What I intend to make is 'Insane Mind', a model which forms random linear neural networks from a set of nodes at random times ( I made out the 'linear neural network' part from a bit of Google searches).
The basic process involved is :


The system forms nodes of random weights . These nodes also have the Sigmoid function (the logistic fuction : $f(x) = \frac{1}{1 + e^{-x}}$ ) , and I termed these 'Gravitons' (because of the usage of the word 'weights' in them - sorry if my terminology work seems ambiguous...😅)
The input enters the system via one of the gravitons.
The node processes it and either passes the output to the next node or to itself .
Step 3 is repeated a certain number of times as the number of gravitons made for use.
The output of the final graviton is given as the output of the whole system.


One thing I'm sure of this model is that this model can transform an input vector into an output vector.
I am not sure whether this is ambiguous or similar to  previously discovered model. Plus, I'd like to know if this will be effective in any situation (I believe it will be of help in classification problems).

Note : I made this out of my imagination , which means this may be useless one way or the other, but still it seemed to work.

Here's the training algorithm I made for this model :


In my Python implementation of this model, I had added a provision in the 'Graviton' class to store the derivative of the output of the graviton. Using this, the gravitons are ordered in the increasing order of the derivatives of their outputs.
The first graviton is taken, and its weight is modified by the error in the output.
The error is modified by the product of the graviton's output derivative and its weight after editing.
Steps 2 through 3 are done for the other gravitons as well. The final error (given by the error variable ) will be the product of the derivatives, the edited weights and the error in the output.
The set of gravitons thus formed is the next set subjected to this training.


For extra reference, here's the code:

Insane_Mind.py :

from math import *
from random import *
 
class MachineError(Exception):
    '''standard exception in the API'''
    def __init__(self, stmt):
        self.stmt = stmt
        
def sig(x):
    '''Sigmoid function'''
    try :
        return exp(x)/(exp(x) + 1)
    except OverflowError:
        if x > 0 :
            return 1
        elif x < 0:
            return 

class Graviton:
    def __init__(self, weight, marker):
        '''Basic unit in 'Insane Mind' algorithm'''
        self.weight = weight
        self.marker = marker + 1
        self.input = 0
        self.output = 0
        self.derivative = 0

    def process(self, input_to_machine):
        '''processes the input'''
        self.input = input_to_machine
        self.output = sig(self.weight * self.input)
        self.derivative = self.input * self.output * (1- self.output) 
        return self.output
    
    def get_derivative_at_input(self):
        '''returns the derivative of the output'''
        return self.derivative

    def correct_self(self, learning_rate, error):
        '''edits the weight'''
        self.weight += -1 * error * learning_rate * self.get_derivative_at_input() * self.weight
        
class Insane_Mind_Base:
    '''Insane_Mind base class - this is what we're gonna use to build the actual machine'''
    def __init__(self, number_of_nodes):
        '''initialiser for Insane_Mind_Base class.
           arguments : number_of_nodes : the number of nodes you want'''
        self.system = [Graviton(random(),i) for i in range(number_of_nodes)] # the actual system
        self.system_size = number_of_nodes # number of nodes , or 'system size'
        
    def  output_sys(self, input_to_sys):
        '''system output'''
        self.output = input_to_sys
        for i in range(self.system_size):
            self.output = self.system[randint(0,self.system_size - 1 )].process(self.output)
        return self.output
    
    def train(self, learning_rate, wanted):
        '''trains the system'''
        self.cloned = []
        order = []
        temp = {}
        for graviton in self.system:
            temp.update({str(graviton.derivative): self.system.index(graviton)})
        order = sorted(temp)
        i = 0
        error = wanted - self.output
        for value in order:
            self.cloned.append(self.system[temp[value]])
            self.cloned[i].correct_self(learning_rate, error)
            error *= self.cloned[i].derivative * self.cloned[i].weight
            i += 1
        self.system = self.cloned

    def details(self):
        '''gets the weights of each graviton'''
        for graviton in self.system:
            print(""Node : {0}, weight : {1}"".format(graviton.marker , graviton.weight))

class Insane_Mind:
    
    '''Actaul Insane_Mind class'''
    def __init__(self, number_of_gravitons):
        '''initialiser'''
        self.model = Insane_Mind_Base(number_of_gravitons)
        self.size = number_of_gravitons
        
    def get(self, input):
        '''processes the input'''
        return self.model.output_sys(input)
    
    def train_model(self, lrate, inputs, outputs, epoch):
        '''train the model'''
        if len(inputs) != len(outputs):
            raise MachineError(""Unequal sizes for training input and output vectors"")
        epoch = str(epoch)
        if epoch.lower() == 'sys_size':
            epoch = int(self.model.system_size)
        else:
            epoch = int(epoch)
        for k in range(epoch):
            for j in range(len(inputs)):
                    val = self.model.output_sys(inputs[j])
                    self.model.train(1/val if str(lrate).lower() == 'output' else lrate, outputs[j])
    
    def details(self):
        '''details of the machine'''
        self.model.details()



Insane_Mind_Test.py :

from Insane_Mind import *
from statistics import *

input_data = [3,4,3,5,4,4,3,6,5,4] # list of forces using which the coin is tossed
output_data = [1,0,0,1,1,0,0,0,1,1] # head or tails in binary form (0 = tail (= not head), 1 = head)
wanteds = output_data.copy()
model = Insane_Mind(2) # Insane Mind model
print(""Before Training:"")
print(""----------------"")
model.details() # fetches you weights of the model

def normalize(x):
    cloned = x.copy()
    meanx = mean(x)
    stdevx = stdev(x)
    for i in range(len(x)):
        cloned[i] = (cloned[i] - meanx)/stdevx
    return cloned

def random_catch(range_of_catches, sample_length):
    # sample data generator. I named it random catch as part of using it in testing whether my model 
    # ' catches the correct guess'. :)
    return [randint(range_of_catches[0], range_of_catches[1]) for i in range(sample_length)]

input_data = normalize(input_data)
output_data = normalize(output_data)

model.train_model('output', input_data, output_data, 'sys_size')
# the argument 'output' for the argument 'lrate' (learning rate) was to specify that the learning rate at # each step is the inverse of the output, and the use of 'sys_size' for the number of times to be trained
# is used to tell the machine that the required number of epochs is equal to the size of the system or 
# the number of nodes in it.

print(""After Training:"")
print(""----------------"")
model.details() # fetches you weights of the model

predictions = [model.get(i) for i in input_data]

threshold = mean(predictions)
predictions = [1 if i >= threshold else 0 for i in predictions]

print(""Predicted : {0}"".format(predictions))
print(""Actual:{0}"".format(wanteds))
mse_array = [(wanteds[j] - predictions[j])**2 for j in range(len(input_data))]
print(""Mean squared error:{0}"".format(mean(mse_array)))

accuracy = 0
for i in range(len(predictions)):
    if predictions[i] == wanteds[i]:
        accuracy += 1

print(""Accuracy:{0}({1} out of {2} predictions correct)"".format(accuracy/len(wanteds), accuracy, len(predictions)))

print(""______________________________________________"")

print(""Random catch test"")
print(""-----------------"")

times = int(input(""No. of tests required : ""))
catches = int(input(""No. of catches per test""))
mse = {}
for m in range(times):
    wanted = random_catch([0,1] , catches)
    forces = random_catch([1,10], catches)
    predictions = [model.get(k) for k in forces]
    threshold = mean(predictions)
    predictions = [1 if value >= threshold else 0 for value in predictions]
    mse_array = [(wanted[j] - predictions[j])**2 for j in range(len(predictions))]
    print(""Mean squared error:{0}"".format(mean(mse_array)))
    mse.update({(m + 1):mean(mse_array)})
    accuracy = 0
    for i in range(len(predictions)):
        if predictions[i] == wanted[i]:
            accuracy += 1
    print(""Accuracy:{0}({1} out of {2} predictions correct)"".format(accuracy/len(wanteds), accuracy, len(predictions)))
    

I tried running 'Insane_Mind_Test.py', and the results I got are :

The formula I used from MSE is (please correct me if I was wrong):
$$ MSE = \frac{\sum_{i = 1}^n (x_i - x'_i)^2}{n}$$
where,
$$ x_i = \text{Intended output}$$
$$ x'_i = \text{Output predicted}$$
$$ n = \text{Number of outputs}$$
My main intention was to make a guess system.

Note : Here, I had to think differently. I decided to classify the forces as those yielding a head and those that yield a tail (unlike what I say in the comments in the program).

Thanks for all help in advance.
Edit: Here's the training data :
Forces         Head(1) or not head(0)[rather call it tail] 
_______        ______________________
3                  1
4                  0
3                  0
5                  1
4                  1
4                  0
3                  0
6                  0
5                  1
4                  1

","['neural-networks', 'classification']",
Is there any network/paper used to analyse music scores?,"
As I am curious on music theory I would like to know that If is there any such network that analyse like labeling chords, or doing a roman numeral analysis.
Like an example below:

Source
It does not seem to be a difficult task.
Some other examples are given here[external link]
Also I am curious that If it is a possible task for AI to accomplish.
","['neural-networks', 'computer-vision', 'generative-adversarial-networks']","I have not come across music labeling algorithms but upon a google scholar search, I found a couple of papers that aim to do quite the same task.In general, if you have a labeled dataset then you can take an approach of a general speech recognition model. It should work fine for music labeling too, but you might need to tweak certain parameters."
What makes Google Translate fail on the Latin language?,"
As it is discussed here, and I saw it on other Latin language forums too, everybody complains about how Google Translate fails to translate the Latin language. From my personal experience, it is not that much bad on other languages, including romance languages.
So, what makes Google Translate fail so much to translate the Latin language? Is it about its syntax and grammar or lack of data?
","['natural-language-processing', 'google-translate']","I don't know what model Google is using for their translations, but it's highly likely that they're using one of today's SOTA deep learning models.The latest NLP models are trained on data scraped from the web, e.g. OpenAI's GPT-2 was trained on a dataset of 8 million web pages, Google's BERT was trained on the BookCorpus (800M words) and English Wikipedia (2.500M words) pages.Now think about the amount of latin web pages and notice that there are over 6 million english wikipedia articles but less than 135.000 in latin (see here).As you can see, massive amounts of data are crucial for neural machine translation and I assume there is simply not enough out there for latin. Plus latin is one of the most complex and complicated languages, this makes the task not easier. Maybe Google and Co also focus less on a 'dead' language which is not spoken anymore and has it's right to exist more for educational purposes."
How does PCA work when we reduce the original space to 2 or higher-dimensional space?,"
How does PCA work when we reduce the original space to a 2 or higher-dimensional space? I understand the case when we reduce the dimensionality to $1$, but not this case.
$$\begin{array}{ll} \text{maximize} & \mathrm{Tr}\left( \mathbf{w}^T\mathbf{X}\mathbf{X}^T\mathbf{w} \right)\\ \text{subject to} & \mathbf{w}^T\mathbf{w} = 1\end{array}$$
","['math', 'optimization', 'linear-algebra', 'principal-component-analysis']",
How to avoid over-fitting using early stopping when using R cross validation package caret,"
I have a data set with  36 rows and 9 columns. I am trying to make a model to predict the 9th column
I have tried modeling the data using  a range of models using caret to perform cross-validation and hyper parameter tuning: 'lm', random forrest (ranger) and GLMnet, with range of different folds and hyper-parameter tuning, but the modeling has not been very successful.
Next I have tried to use some of the neural-network models. I tried the 'monmlp'. During hyper parameter tuning I could see that the RMSE drops to a level when using ~ 6 hidden units. The problem I observe using this model is

Prediction is almost equal to data
When doing a ""manual"" cross validation by removing a single datapoint and using the trained model to predict, it has no predictive power

I have tried to use a range of different hidden units, but i think the problem is that the model is overfitted despite using  caret cross validation feature.
There two feedbacks I would appreciate

Is there a way to prevent overfitting, by chosen optimal number of training iterations ( optimal RMSE on out of sample ). Can this by done using caret or some other package
Am I using the right model?

I am relatively unexperienced with ML and choosing a good model is tough: when you look at the available packages it is overwhelming:
https://topepo.github.io/caret/train-models-by-tag.html
","['machine-learning', 'overfitting', 'cross-validation', 'r', 'early-stopping']",
Why would DDPG with Hindsight Experience Replay not converge?,"
I am trying to train a DDPG agent augmented with Hindsight Experience Replay (HER) to solve the KukaGymEnv environment.  The actor and critic are simple neural networks with two hidden layers (as in the HER paper).
More precisely, the hyper-parameters I am using are

The actor's hidden layer's sizes: [256, 128] (using ReLU activations and a tanh activation after the last layer)
Critic's hidden layer's sizes: [256, 128] (Using ReLU activations)
Maximum Replay buffer size: 50000
Actor learning rate: 0.000005 (Adam Optimizer)
Critic learning rate: 0.00005 (Adam Optimizer)
Discount rate: 0.99
Polyak constant : 0.001
The transitions are sampled in batches of 32 from the replay buffer for training
Update rate: 1 (target networks are updated after each time step)
The action selection is stochastic with the noise being sampled from a normal distribution of mean 0 and standard deviation of 0.7

I trained the agent for 25 episodes with a maximum of 700 time-steps each and got the following reward plot:

The reward shoots up to a very high value of about 8000 for the second episode and steeply falls to -2000 in the very next time step, never to rise again. What could be the reason for this behavior and how can I get it to converge?
PS : One difference I observed while training this agent from while training a simple DDPG agent is that for simple DDPG, the episode would usually terminate at around 450 time-steps, thus never reaching the maximum specified timesteps. However, here, no episode terminated before the specified 700 maximum time steps. This might have something to do with the performance.
","['reinforcement-learning', 'deep-rl', 'rewards', 'ddpg', 'hindsight-experience-replay']",
Why does using a higher representation space lead to performance increase on the training data but not on the test data?,"
I read the following from a book:

You can intuitively understand the dimensionality of your representation space as “how much freedom you’re allowing the model to have when learning internal representations.” Having more units (a higher-dimensional representation space) allows your model to learn more-complex representations, but it makes the model more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data).

Why does using a higher representation space lead to performance increase on the training data but not on the test data?
Surely the representations/patterns learnt in the training data will be found too in the test data.
","['neural-networks', 'machine-learning', 'deep-learning', 'deep-neural-networks']","The answer to your question is that the capacity of your model (i.e. the number and type of function that your model can compute) generally increases with the number of parameters. So, a bigger model can potentially approximate better the function represented by your training data, but, at the same time, it may not take into account the test data, a phenomenon known as over-fitting the training data (i.e. fitting ""too much"" the training data).In theory, you want to fit the training data perfectly, so over-fitting should not make sense, right? The problem is that, if we just fit all the (training) data, there is no way of empirically checking that our model will perform well on unseen data, i.e. will it generalize to data not seen during training?
We split our data into training and test data because of this: we want to understand whether our model will perform well also on unseen data or not.There are also some theoretical bounds that ensure you that, probabilistically and approximately, you can generalize: if you have more training data than a certain threshold, the probability that you perform badly is small. However, these theoretical bounds are often not taken into account in practice because, for example, we may not be able to collect more data to ensure that the bounds are satisfied.Surely the representations/patterns learnt in the training data will be found too in the test data.This is possibly the wrong assumption and the reason why you are confused. You may assume that both your training data and test data come from the same distribution $p(x, y)$, but it does not necessarily mean that they have the same patterns. For example, I can sample e.g. 13 numbers from a Gaussian $N(0, 1)$, the first 10 numbers could be very close to $0$ and the last $3$ could be close to $1$. If you split this data so that your training data contains different patterns than the test data, then it is not guaranteed that you will perform well also on the test data.Finally, note that, in supervised learning, our ultimate goal when we fit models to labeled data is to learn a function (or a probability distribution over functions), where we often assume that both the training and test data are input-output pairs from our unknown target function, i.e. $y_i = f(x_i)$, where $(x_i, y_i) \in D$ (where $D$ is your labelled dataset), and $f$ is the unknown target function (i.e. the function we want to compute with our model), so, if our model performs well on the training data but not on the test data and we assume that both training and test data come from the same function $f$, there is no way that our model is computing our target function $f$ if it performs badly on the test data."
"Given the same features, do logistic regression and neural networks produce the same output?","
I have a binary classification problem. I have variables (features) var1, var2, var3, ..., var14.
Using these variables (aka features) in a logistic regression, I get their weights.
If I use the same set of variables in a neural network:

Should I get a different output?
or

Should I get the same output?


I developed a ROC Curve, and I have both lines overlaying on each other. I am not sure if I am missing something here.
","['neural-networks', 'comparison', 'logistic-regression']",
Why don't we use auto-encoders instead of GANs?,"
I have watched Stanford's lectures about artificial intelligence, I currently have one question: why don't we use autoencoders instead of GANs?
Basically, what GAN does is it receives a random vector and generates a new sample from it. So, if we train autoencoders, for example, on cats vs dogs dataset, and then cut off the decoder part and then input random noise vector, wouldn't it do the same job?
","['computer-vision', 'generative-adversarial-networks', 'autoencoders']","Auto-encoders are widely used and maybe even more used than GANs (in fact, auto-encoders are older than GANs, although the main general idea behind GANs is quite old). For example, auto-encoders are used in World Models, for drug design (e.g. see this paper) and many other tasks that involve data compression or generation.So, if we train autoencoders, for example, on cats vs dogs dataset, and then cut off the decoder part and then input random noise vector, wouldn't it do the same job?Yes, the encoder part of the auto-encoder produces a vector that represents the input in a compressed form. You may also be interested in denoising auto-encoders, but there are other variations, such as convolutional auto-encoders or variational auto-encoders."
Is it feasible to train a DQN with thousands of input ports?,"
I designed a DQN architecture for some problem. The problem has a parameter $m$ as the number of clients. In my situation, $m$ is large, $m\in\{100,200,\ldots,1000\}$. For this situation, the number of input ports of the DQN is some few thousand, $\{1000, 2000, \ldots, 10000\}$. For some fixed $m$, I would like to see the performance of deep Q learning on the performance. So I have to train the DQN for every change that occurs on $m$ and this should handle thousands of inputs ports for each training. Is this situation familiar in DQN and if not how to solve this issue?
","['q-learning', 'dqn', 'deep-rl', 'curse-of-dimensionality']",
Can I use one-hot vectors for text classification?,"
For an upcoming project I'm trying to write a text classifier for the IMDb sentiment analysis dataset. This needs to vectorize words using an embedding layer and then reduce the dimensions of the output with global average pooling. This is proving however to be very difficult for my low experience level, and I am struggling to wrap my head around the dimensionality involved, bearing in mind I must avoid libraries such as tensorflow that would make it very basic exercise. I am hoping that I could make it easier by encoding each word in the reviews as a one-hot vector, and passing it through a few regular dense layers. Would this work and yield decent results?
","['neural-networks', 'machine-learning', 'text-classification']","One hot encoding is a good strategy to apply with categorical variables that assume few possible values. The problem with text data is that you easily end up with corpora with a really large vocabulary. If I remember correctly the IMDb dataset contains around 130.000 unique words, which means that you should create a network with an input matrix of size 130.000 x max_length where max_length is the fixed maximum length allowed for each review. Apart from the huge size, this matrix would also be extremely sparse, and that's another big issue in using one-hot encoding with text.For these reasons, I really doubt you would achieve any good results with a simple one-hot encoding. Embeddings where actually designed precisely to overcome all these issues, they have fixed reasonable size, they assume continue values between 0 and 1, which is desirable for deep neural networks, and they can be treated as ""extra"" trainable weights of a network.If you really want to avoid embeddings I would suggest you to use (or implement, I don't think it will be so hard) a term frequency–inverse document frequency vectoriser. It is closer to one-hot encoding in the fact that it is based on the creation of a huge co-occurances matrix between words, but at least the values are continuous and not dichotomous. Nevertheless I would not expect high performances with the tf-idf either, simply because this type of encoding works best with shallow models like the Naive Bayes rather than deep models."
What are examples of problems where neural networks have achieved human-level or higher performance?,"
What are examples of problems where neural networks have been used and have achieved human-level or higher performance?
Each answer can contain one or more examples. Please, provide links to research papers or reliable articles that validate your claims.
","['neural-networks', 'reference-request', 'applications', 'social']",
How can I classify houses given a dataset of houses with descriptions?,"
I have a dataset with a number of houses, for each house, I have a description. For example ""The house is luxuriously renovated"" or ""The house is nicely renovated"". My aim is to identify for each house whether it is luxuriously, well or poorly renovated. I am new to NLP so any tips on how to approach this problem would be much appreciated.
","['natural-language-processing', 'classification', 'datasets']","It all depends on what kind of annotations or other variables are present in your dataset. I see 2 possible scenarios here:your dataset is made only of houses descriptions, without any indication of their luxury level.you have annotations regarding the luxury level or another similar variables from which you can infer the luxury level (like the house price for example).In the first case there's not much you can do except for trying to apply some unsupervised algorithms or transfer learning. Usually in NLP unsupervised techniques are used for tasks like Topic or language modeling, both of which are not really helpful for your specific application since they work at a really abstract level trying to learn relationships between words or documents in corpora containing huge variety of texts. The best you could try could be preprocessing the data to extract specific terms like entities (cities names, furnitures names, etc.) and adjectives from each description, and then try cluster them into n clusters were n is the number of classes you're interest in by applying for example Latent Dirichlet Allocation. Even though everything is possible with the right time and patience, I would never follow this road, especially because it relies on a perfect preprocessing, that involves already transfer learning, for the name entity recognition part for example. And even tough libraries like SpaCy offer really good models to perform these tasks, once you have these kind of annotations a rule based approach would probably be faster and easier to build than another unsupervised model, e.g. creating a simple dictionary contacting names of luxurious furnitures name and adjectives that indicate if the house is expensive would probably be sufficiently good.In the second case, the story change completely because if you have annotations you could rely on supervised learning. If you already have explicit annotations like ""luxuriously renovated"" ""nicely renovated"" nothing stops you from trying to train whatever architecture you feel more comfortable with on this classification task. Even simple architecture like CNN that are easy and fast to train usually achieve good results in classification, and you definitely want to leverage some pre-trained embedding vectors like GloVe as an input feature (every deep learning framework like tensorflow or pytorch already implement the possibility to use them).To conclude, if you don't have annotations you might try LDA just to check if you're lucky but if I were you I would start annotating data as quick as I can. A good practice in this case is to also ask someone else to perform the annotations, not only to be faster in the creation of the dataset but also to then calculate the Inter Annotator Agreement score, that gives an indication about the quality of the annotations (if the score is low, the dataset quality is poor and no model will be able to learn something from it)."
How to understand this NN architecture?,"
I was reading a paper Multi-Agent Reinforcement Learning for Adaptive
User Association in Dynamic mmWave Networks and I was stuck understanding the deep neural network architecture that was used. The authors gave it in Fig. 3 (on top of page 6) and they state the following (on page 9):

This architecture comprises 2 multi-layers perceptron (MLP) of 32 hidden units, one RNN layer (a long short memory term - LSTM) layer with 64 memory cells followed by another 2 MLPs of 32 hidden units. The network then branches off in two MLPs of 16 hidden units to construct the duelling network.

According to Fig. 3 there is one MLP, one RNN and one MLP. So why the authors said 2 MLPs?
Assuming it is 2 MLPs, does this mean we have 2 hidden layers of 32 neurons each? So, at the end we will have:
one input layer - one hidden layer with 32 neurons - another hidden layer with 32 neurons - one RNN layer with 64 cells - one hidden layer with 32 neurons - another hidden layer with 32 neurons - one hidden layer with 16 neurons - another hidden layer with 16 neurons - one output layer.
","['neural-networks', 'reinforcement-learning', 'deep-neural-networks', 'papers']",
How do non-local neural networks relate to attention and self-attention?,"
I've been reading non-local neural networks as explained in the original paper. My understanding is that they solve the restrained reception of local filters. I see how they are different from convolutions and fully connected networks.
How do they relate to attention (specifically self-attention)? How do they integrate this attention?
","['neural-networks', 'computer-vision', 'papers', 'attention']",
How to calculate the GPU memory need to run a deep learning network?,"
In general, how do I calculate the GPU memory need to run a deep learning network?
I'm asking this question because my training for some network configuration is getting out of memory.
If the TensorFlow only store the memory necessary to the tunable parameters, and if I have around 8 million, I supposed the RAM required will be:
RAM = 8.000.000 * (8 (float64)) / 1.000.000 (scaling to MB)
RAM = 64 MB, right?
The TensorFlow requires more memory to store the image at each layer?
By the way, these are my GPU Specifications:

Nvidia GeForce 1050 4GB

Networking topology

Unet
Input Shape (256,256,4)

Model: ""functional_1""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 256, 256, 4) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 256, 256, 64) 2368        input_1[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 256, 256, 64) 0           conv2d[0][0]                     
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 256, 256, 64) 36928       dropout[0][0]                    
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 128, 128, 64) 0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 128, 128, 128 0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 128, 128, 128 147584      dropout_1[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_3[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 64, 64, 256)  0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 64, 256)  590080      dropout_2[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_5[0][0]                   
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 32, 512)  0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 512)  2359808     dropout_3[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 64, 64, 256)  524544      conv2d_7[0][0]                   
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 64, 64, 512)  0           conv2d_transpose[0][0]           
                                                                 conv2d_5[0][0]                   
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 64, 64, 256)  1179904     concatenate[0][0]                
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 64, 64, 256)  0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 64, 64, 256)  590080      dropout_4[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 128, 128, 128 131200      conv2d_9[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 128, 256 0           conv2d_transpose_1[0][0]         
                                                                 conv2d_3[0][0]                   
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 128, 128, 128 295040      concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 128, 128, 128 0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 128, 128, 128 147584      dropout_5[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 256, 256, 64) 32832       conv2d_11[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 256, 256, 128 0           conv2d_transpose_2[0][0]         
                                                                 conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 256, 256, 64) 0           conv2d_12[0][0]                  
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 256, 256, 64) 36928       dropout_6[0][0]                  
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 256, 256, 1)  65          conv2d_13[0][0]                  
==================================================================================================
Total params: 7,697,921
Trainable params: 7,697,921
Non-trainable params: 0

This is the error given.
---------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
<ipython-input-17-d4852b86b8c1> in <module>
     23 # Train the model, doing validation at the end of each epoch.
     24 epochs = 30
---> 25 result_model = model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)

~\Anaconda3\envs\tf23\lib\site-packages\tensorflow\python\keras\engine\training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

~\Anaconda3\envs\tf23\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-> 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

~\Anaconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

~\Anaconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\def_function.py in _call(self, *args, **kwds)
    838         # Lifting succeeded, so variables are initialized and we can run the
    839         # stateless function.
--> 840         return self._stateless_fn(*args, **kwds)
    841     else:
    842       canon_args, canon_kwds = \

~\Anaconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\function.py in __call__(self, *args, **kwargs)
   2827     with self._lock:
   2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2830 
   2831   @property

~\Anaconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\function.py in _filtered_call(self, args, kwargs, cancellation_manager)
   1846                            resource_variable_ops.BaseResourceVariable))],
   1847         captured_inputs=self.captured_inputs,
-> 1848         cancellation_manager=cancellation_manager)
   1849 
   1850   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~\Anaconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1922       # No tape is watching; skip to running the function.
   1923       return self._build_call_outputs(self._inference_function.call(
-> 1924           ctx, args, cancellation_manager=cancellation_manager))
   1925     forward_backward = self._select_forward_and_backward_functions(
   1926         args,

~\Anaconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    548               inputs=args,
    549               attrs=attrs,
--> 550               ctx=ctx)
    551         else:
    552           outputs = execute.execute_with_cancellation(

~\Anaconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

ResourceExhaustedError:  OOM when allocating tensor with shape[8,64,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
     [[node gradient_tape/functional_1/conv2d_14/Conv2D/Conv2DBackpropInput (defined at <ipython-input-17-d4852b86b8c1>:25) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
 [Op:__inference_train_function_17207]

Function call stack:
train_function

Is there any type of mistake in the network definition? How could I improve the network to solve this problem?
","['deep-learning', 'tensorflow', 'training', 'memory']",
How is the data labelled in order to train a region proposal network?,"
I don't get how the training of the RPN works. From the forward propagation, I have $W \times H \times k$ outputs from the RPN.
How is the training data labeled such that I can use the loss function and update the weights through bach propagation?  Is the training data labeled in the same shape of the output, as there are $W \times H \times k$ anchor boxes and we use the loss function directly or what?
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'computer-vision', 'r-cnn']",
What is the weight matrix in self-attention?,"
I've been looking into self-attention lately, and in the articles that I've been seeing, they all talk about ""weights"" in attention. My understanding is that the weights in self-attention are not the same as the weights in a neural network.
From this article, http://peterbloem.nl/blog/transformers, in the additional tricks section, it mentions,
The query is the dot product of the query weight matrix and the word vector,
ie, q = W(q)x and the key is the dot product of the key weight matrix and the word vector, k = W(k)x and similarly for the value it is v = W(v)x. So my question is, where do the weight matrices come from?
","['neural-networks', 'transformer', 'attention']",
Why are neural networks preferred to other classification functions optimized by gradient decent,"
Consider a neural network, e.g. as presented by Nielsen here. Abstractly, we just construct some function $f: \mathbb{R}^n \to [0,1]^m$ for some $n,m \in \mathbb{N}$ (i.e. the dimensions of the input and output space) that depends on a large set of parameters, $p_j$. We then just define the cost function $C$ and calculate $\nabla_p C$ and just map $p \to p - \epsilon \nabla_p C$ repeatedly.
The question is why do we choose $f$ to be what it is in standard neural networks, e.g. a bunch of linear combinations and sigmoids? One answer is that there a theorem saying any suitably nice function can be approximated using neural networks. But the same is true of other types of functions $f$. The Stone-Weierstrass theorem gives that we could use polynomials in $n$ variables: $$f(x) =  c^0_0 + (c^1_1 x_1 + c^1_2 x_2 + \cdots + c^1_n x_n) + (c^2_{11}x_1 x_1 + c^2_{12} x_1x_2 + \cdots + c^2_{1n} x_1 x_2 + c^2_{21} x_2x_1 + c^2_{22} x_2x_2 + \cdots) + \cdots,$$
and still have a nice approximation theorem. Here the gradient would be even easier to calculate. Why not use polynomials?
","['neural-networks', 'machine-learning', 'comparison', 'function-approximation']","You can indeed fit a polynomial to your labelled data, which is known as polynomial regression (which can e.g. be done with the function numpy.polyfit). One apparent limitation of polynomial regression is that, in practice, you need to assume that your data follows some specific polynomial of some degree $n$, i.e. you assume that your data has the form of the polynomial that you choose, which may not be true.When you use a neural network to solve a classification or regression problem, you also need to choose the activation functions, the number of neurons, how they are connected, etc., so you also need to limit the number and type of functions that you can learn with neural networks, i.e. the hypothesis space.Now, it is not necessarily a bad thing to limit the hypothesis space. In fact, learning is generally an ill-posed problem, i.e. in simple terms, there could be multiple solutions or no solutions at all (and other problems), so, actually, you often need to limit the hypothesis space to find some useful solutions (e.g. solutions that generalise better to unseen data). Regularisations techniques are ways of constraining the learning problem, and the hypothesis space (i.e. the set of functions that your learning algorithm can choose from), and thus making the learning problem well-posed.Neural networks are not preferred over polynomial regression because they are theoretically more powerful. In fact, both can approximate any continuous function [1], but these are just theoretical results, i.e. these results do not give you the magical formula to choose the most appropriate neural network or polynomial that best approximates the desired unknown function.In practice, neural networks have been proven to effectively solve many tasks (e.g. translation of natural language, playing go or atari games, image classification, etc.), so I would say that this is the main reason they are widely studied and there is a lot of interest in them. However, neural networks typically require large datasets to approximate well the desired but unknown function, it can be computationally expensive to train or perform inference with them, and there are other limitations (see this), so neural networks are definitely not perfect tools, and there is the need to improve them to make them more efficient and useful in certain scenarios (e.g. scenarios where uncertainty estimation is required).I am not really familiar with research on polynomial regression, but it is possible that this and other tools have been overlooked by the ML community. You may want to have a look at this paper, which states that NNs are essentially doing polynomial regression, though I have not read it, so I don't know the details about the main ideas and results in this paper."
Are there neural networks where nodes are randomly selected from among a set of nodes (in random orders and a random number of times)?,"
I am trying to make a classifier.
I am new to AI (even if I know the definition and all such a bit) , and also I have no idea of how to implement it properly by myself even if I know a bit of Python coding (in fact, I am fifteen years old !🙄🙄), but my passion for this has made me ask this (silly, probably) question.
Are there neural networks where nodes are randomly selected from among a set of nodes (in random orders and a random number of times)? I know this is from ML (or maybe deep learning, I suppose), but I have no idea how to recognize such a thing from the presently available algorithms. It will be great if you all could help me, because I am preparing to release an API for programming a model which I call the 'Insane Mind' on GitHub, and I want some help to know if my effort was fruitless.
And for reference, here's the code :
from math import *
from random import *
 
class MachineError(Exception):
    '''standard exception in the API'''
    def __init__(self, stmt):
        self.stmt = stmt
def sig(x):
    '''Sigmoid function'''
    return (exp(x) + 1)/exp(x)

class Graviton:
    def __init__(self, weight, marker):
        '''Basic unit in 'Insane Mind' algorithm
           -------------------------------------
           Graviton simply refers to a node in the algorithm.
           I call it graviton because of the fact that it applies a weight
           on the input to transform it, besides using the logistic function '''
        self.weight = weight # Weight factor of the graviton
        self.marker = marker # Marker to help in sorting
        self.input = 0 # Input to the graviton
        self.output = 0 # Output of the graviton
        self.derivative = 0 # Derivative of the output

    def process(self, input_to_machine):
        '''processes the input (a bit of this is copied from the backprop algorithm'''
        self.input = input_to_machine
        self.output = (sig(self.weight * self.input) - 1)/(self.marker + 1)
        self.derivative = (sig(self.input * self.weight) - 1) * self.input *self.output * (1- self.output) 
        return self.output
    
    def get_derivative_at_input(self):
        '''returns the derivative of the output'''
        return self.derivative

    def correct_self(self, learning_rate, error):
        '''edits the weight'''
        self.weight += -1 * error * learning_rate * self.get_derivative_at_input() * self.weight
        
class Insane_Mind:

    def __init__(self, number_of_nodes):
        '''initialiser for Insane_Mind class.
           arguments : number_of_nodes : the number of nodes you want in the model'''
        self.system = [Graviton(random(),i) for i in range(number_of_nodes)] # the actual system
        self.system_size = number_of_nodes # number of nodes , or 'system size'
        
    def  output_sys(self, input_to_sys):
        '''system output'''
        self.output = input_to_sys
        for i in range(self.system_size):
            self.output = self.system[randint(0,self.system_size - 1 )].process(self.output)
        return self.output
    
    def train(self, learning_rate, wanted):
        '''trains the system'''
        self.cloned = [] # an array to keep the sorted elements during the sorting process below
        order = [] # the array to make out the order of arranging the nodes
        temp = {} # a temporary dictionary to pick the nodes from
        for graviton in self.system:
            temp.update({str(graviton.derivative): graviton.marker})
        order = sorted(temp)
        i = 0
        error = wanted - self.output
        for value in order:
            self.cloned.append(self.system[temp[value]])
            self.cloned[i].correct_self(learning_rate, error)
            error *= self.cloned[i].derivative
            i += 1
        self.system = self.cloned

Sorry for not using that MachineError exception anywhere in my code (I will use it when I am able to deploy this API).
To tell more about this algorithm, this gives randomized outputs (as if guessing). The number of guesses vary from 1 (for a system with one node), 2 (for two nodes) and so on to an infinite number of guesses for an infinite number of nodes.
Also, I wanna try and find how much it can be of use (if this is something that has never been discovered, if it is something that can find a good place in the world of ML or Deep Learning) and where it can be used.
Thanks in advance.
Criticisms (with a clear reason) are also accepted.
","['neural-networks', 'machine-learning']","It is difficult to prove a negative, but I do not think there is any classifier (neural network or otherwise) that fully matches to your idea.I suspect that you will not be able to take the idea of random connections and loops at run time, and make a useful classifier out of it. That's not to say the idea is completely without merit, sometimes it is good to explore blue sky ideas and just see what happens. However, I think it might be a frustrating excercise to build anything on top of your idea without some basic foundation work first. I recommend that you look into the theory and implementation of logistic regression as a starting point, which is a good stepping stone to understanding neural networks.There are some neural network components and architectures that make use of random behaviour at the activation level:Dropout. This is a method used during training which zeroes outputs from randomly selected neurons. It often gives an effective boost to neural network stability (acting to prevent overfitting to input data) and can improve accuracy of classifiers too due to behaving similarly to having multiple simpler classifiers.Boltzmann machines, and restricted Boltzmann machines (RBMs) output 0 or 1 randomly from each ""neuron"" unit, with the probability decided by sum of inputs. They are used to create generative models, not classifiers though. Another difference is that the randomness is applied both during training and during inference, whilst dropout is most often applied to augment training. Early on in the days of deep learning, RBMs were used to pre-train layers in a deep neural network. This was effective, but other simpler methods were discovered later and are nowadays preferred in most cases.A variant of dropout call called Monte Carlo dropout is used at inference time. This can be used to measure uncertainty in a model's individual predictions, which is otherwise hard to obtain.Although not quite as freeform as your random connections on a per neuron basis. If you applied dropout to a recurrent neural network, that might be quite close to your idea, because the existence of loops between neurons in each time step would be random. This could be applied in language modelling and classifiers for sequence data. The same motivations apply here as for dropout in simpler feed forward classifiers - it can in theory make a classifier more robust against noise in the inputs and more accurate."
is it ok to take random actions while training a3c as in below code,"
i am trying to train an A3C algorithm but I am getting same output in the multinomial function.
can I train the A3C with random actions as in below code.
can someone expert comment.
while count<max_timesteps-1:
            value, action_values, (hx, cx) = model((Variable(state.unsqueeze(0)), (hx, cx)))
            prob = F.softmax(action_values,dim = -1)
            log_prob = F.log_softmax(action_values, dim=-1)
            print(log_prob.shape)
            print(""log_prob: "",log_prob)
            entropy = -(log_prob * prob).sum(1, keepdim=True)
            entropies.append(entropy)
            actn = np.random.randn(3)
            action = actn.argmax()
            log_prob = log_prob[0,action]
            # print(""log_prob "",log_prob)
            # print(""action "",action)
            state, reward, done = env.step(action)
            done = (done or count == max_timesteps-2)
            reward = max(min(reward, 1), -1)

","['reinforcement-learning', 'pytorch', 'a3c']",
How can I fix jerky movement in a continuous action space,"
I am training an agent to do object avoidance. The agent has control over its steering angle and its speed. The steering angle and speed are normalized in a $[−1,1]$ range, where the sign encodes direction (i.e. a speed of −1 means that it is going backwards at the maximum units/second).
My reward function penalises the agent for colliding with an obstacle and rewards it for moving away from its starting position. At a time $t$, the reward, $R_t$, is defined as
$$
R_t=
\begin{cases}
r_{\text{collision}},&\text{if collides,}\\
\lambda^d\left(\|\mathbf{p}^{x,y}_t-\mathbf{p}_0^{x,y}\|_2-\|\mathbf{p}_{t-1}^{x,y}-\mathbf{p}_0^{x,y}\|_2 \right),&\text{otherwise,}
\end{cases}
$$
where $\lambda_d$ is a scaling factor and $\mathbf{p}_t$ gives the pose of the agent at a time $t$. The idea being that we should reward the agent for moving away from the inital position (and in a sense 'exploring' the map—I'm not sure if this is a good way of incentivizing exploration but I digress).
My environment is an unknown two-dimensional map that contains circular obstacles (with varying radii). And the agent is equipped with a sensor that measures the distance to nearby obstacles (similar to a 2D LiDAR sensor). The figure below shows the environment along with the agent.

Since I'm trying to model a car, I want the agent to be able to go forward and reverse; however, when training, the agent's movement is very jerky. It quickly switches between going forward (positive speed) and reversing (negative speed). This is what I'm talking about.
One idea I had was to penalise the agent when it reverses. While that did significantly reduce the jittery behaviour, it also caused the agent to collide into obstacles on purpose. In fact, over time, the average episode length decreased. I think this is the agent's response to the reverse penalties. Negative rewards incentivize the agent to reach a terminal point as fast as possible. In our case, the only terminal point is obstacle collision.
So then I tried rewarding the agent for going forward instead of penalising it for reversing, but that did not seem to do much. Evidently, I don't think trying to correct the jerky behaviour directly through rewards is the proper approach. But I'm also not sure how I can do it any other way. Maybe I just need to rethink what my reward signal wants the agent to achieve?
How can I rework the reward function to have the agent move around the map, covering as much distance as possible, while also maintaining smooth movement?
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'rewards', 'reward-shaping']","I think you should try to reason in terms of total ""area"" explored by the agent rather than ""how far"" it moves from the initial point, and also you should add some reward terms to push the agent steering more often. I think that the problem with your setting is more or less this: The agent go as straight as it can because you're rewarding it for it, it start sensing an obstacle so it stops, there is no reward for steering so the best strategy to go away from the obstacle and not end the episode is just to go backwards.Considering that you have information about the grid points at any time you could rewrite the reward function in terms of grid squared explored by checking at each move if the agent end up in a new square grid:$$
R_t=
\begin{cases}
r_{\text{collision}}\\
\lambda^d\left(\|\mathbf{p}^{x,y}_t-\mathbf{p}_0^{x,y}\|_2-\|\mathbf{p}_{t-1}^{x,y}-\mathbf{p}_0^{x,y}\|_2 \right) + r_{new-squared-explored}
\end{cases}
$$Moreover it would be useful to add some reward terms also related to how the agent avoid the obstacle, for example a penalisation when the sensor goes and remain under a certain threshold (to make the agent learn to not go and stay too close to an obstacle) but also a rewarding term when an obstacle is detected and the agent manage to maintain a certain distance from it (even though if not well tuned this term could lead the agent to learn to just run in circles around a single obstacle, but if tuned properly I think it might help to make the agent movements smoother)."
Understanding GLIE conditions for epsilon greedy approach,"
I was going through this course on reinforcement learning (the course has two lecture videos and corresponding slides) and I had a doubt. On slide 18 of this pdf, it states following condition for an algorithm to have regret sublinear in T (T being number of pulls of multi arm bandit).

C2 - Greedy in the Limit: Let exploit(T) denote the number of pulls that are that are greedy w.r.t the empirical mean up to horizon $T$. For sub-linear regret, we need
$$\lim_{T\rightarrow\infty}\frac{\mathbb{E}[exploit(T)]}{T}=1 $$

Here, $exploit(T)$ denote the total number of ""exploit"" rounds performed in the first $T$ pulls. Given that expectation is defined as ""the weighted sum of the outcome values, where the weights correspond to the probabilities of realizing that value"",
(Q1) how exactly mathematically we define $\mathbb{E}[exploit(T)]$?
In second video (at 24:44), instructor has said that $\mathbb{E}[exploit(T)]$ is the number of exploit steps.
(Q2) Then how it equals ""weighted sum of outcome values""?
(note that instructor assumes that the pulling of arm may give reward which correspond to outcome value of 1 and may not give reward which correspond to ourcome value of 0)
Also in slide 27, for GLIE-ifying $\epsilon_T$-first strategy, he selects $\epsilon_T=\frac{1}{\sqrt{T}}$. Then, the instructor counts $\sqrt{T}$ exploratory pulls and $T-\sqrt{T}$ exploitory pulls. Then to show that this satisfies condition C2, instructor states $$\mathbb{E}[exploit(T)]\geq \frac{T-\sqrt{T}}{T}$$.
Here, $\frac{T-\sqrt{T}}{T}$ is a fraction of exploitory pulls.
(Q3) So, by above equation does the instructor mean, number of exploitory pulls is greater than equal to fraction of number of exploitory pulls?
(Q4) How can we put 2nd equation in first equation and still prove limit in first equation still holds, that is, how following is the case:
$$\lim_{T=\rightarrow\infty}\frac{\frac{T-\sqrt{T}}{T}}{T}=1$$
I guess I am missing some basic concept of expectation here.
","['reinforcement-learning', 'multi-armed-bandits', 'epsilon-greedy-policy']",
How do I design the rewards and penalties for an agent whose goal it is to explore a map,"
I am trying to train an agent to explore an unknown two-dimensional map while avoiding circular obstacles (with varying radii). The agent has control over its steering angle and its speed. The steering angle and speed are normalized in a $[-1, 1]$ range, where the sign encodes direction (i.e. a speed of $-1$ means that it is going backwards at the maximum units/second).
I am familiar with similar problems where the agent must navigate to a waypoint, and in which case the reward is the successful arrival to the target position. But, in my case, I can't really reward the agent for that, since there is no direct 'goal'.
What I have tried
The agent is penalised when it hits an obstacle; however, I am not sure how to motivate the agent to move. Initially, I was thinking of having the agent always move forward, meaning that it only has control over the steering angle. But, I want the ability for the agent to control its speed and be able to reverse (since I'm trying to model a car).
What I have tried is to reward the agent for moving and to penalise it for remaining stationary. At every timestep, the agent is rewarded ${1}/{t_\text{max}}$ if the absolute value of the speed is above some epsilon, or penalised that same amount if otherwise. But, as expected, this doesn't work. Rather than motivating the agent to move, it simply causes it to jitter back and forth. This makes sense since 'technically' the most optimal strategy if you want to avoid obstacles is to remain stationary. If the agent can't do that then the next best thing is to make small adjustements in the position.
So my question: how can I add in an exploration incentive to my agent? I am using proximal policy optimization (PPO).
","['reinforcement-learning', 'rewards', 'reward-design']","Measure what you want to achieve as directly as possible, and reward that. Later you can add more sophisticated incentives for the type of motion etc, but the key to a good reward signal is that it measures the quality of a solution at a high level, without specifying how to achieve that solution.If you want your simulated car to explore, you will want to give it a reward signal based on it encountering new unexplored areas. There are lots of reasonable choices here. I suspect a good one will depend on what sensors you can reasonably code for the car, and what you consider to count as exploration - e.g. is it a thorough search of an area, moving far from the original position, experiencing different ""views""?One likely component you will need to give your agent and incorporate into the state representation is a memory. In order to understand whether the agent is exploring, something will need to know whether the agent has experienced something before and how much. A very simple kind of memory would be to add counters to a grid map and allow the agent to know how many time steps it has spent in each position on the map. The reward signal can then be higher when the agent enters a point on the map that it has not been in recently. If you want a non-episodic or repeating tour of exploration you might decay the values over time, so that an area that has not been visited for a long time counts the same as a non-visited one.A related concept that you might be able to borrow ideas from is curiousity. There have been some interesting attempts to encourage an agent to seek new/interesting states by modifying action selection. Curiosity-driven Exploration by
Self-supervised Prediction is one such attempt, and might be of interest to you. In that case, the authors use an intrinsic model of curiousity that can solve some environments even when there is no external reward signal at all!Alternatively, if you don't care to get involved in technical solution, you could create a maybe acceptable behaviour for your vehicle by setting a random goal position, then granting a reward and moving it to a new random location each time the car reaches it."
"Why is the target called ""target"" in Monte Carlo and TD learning if it is not the true target?","
I was going through Sutton's book and, using sample-based learning for estimating the expectations, we have this formula
$$
\text{new estimate} = \text{old estimate} + \alpha(\text{target} - \text{old estimate})
$$
What I don't quite understand is why it's called the target, because since it's the sample, it’s not the actual target value, so why are we moving towards a wrong value?
","['machine-learning', 'reinforcement-learning', 'terminology', 'monte-carlo-methods', 'temporal-difference-methods']",
Effect of adding an Independent Variable in Multiple Linear Regression,"
I am new in machine learning and learning linear regression concept. Please help with answers to below queries.
I want to understand effect on existing independent variable(X1) if I add a new independent variable(X2) in my model.
This new variable is highly correlated with dependent variable(Y)

Will it have any effect on beta coefficient of X1?
Will relationship between X1 and Y become insignificant?
Can adjusted R-square value decrease?

","['machine-learning', 'linear-regression']",
Is it feasible using today's technology to use an AI training algorithm to custom teach a robot to do common household cores?,"
Like making a bed, washing dishes, taking out the garbage, etc., by training it on the video of specific individuals doing those cores in their own unique environments?
I have researched what machine learning is capable of doing at this point in time, and it seems this may be now feasible when done on a customer-specific basis and enable by an A.I. enhanced, full articulated, robot along the lines of an enhanced InMoov. https://en.wikipedia.org/wiki/InMoov
If it's feasible, what are the AI algorithms I should be considering to train my robot to do these tasks? Isn't deep learning the most promising of these selections:  https://www.ubuntupit.com/machine-learning-algorithms-for-both-newbies-and-professionals/?
","['neural-networks', 'robotics', 'state-of-the-art', 'humanoid-robots']","I would suggest using a neural network with back propagation. From what I know, they can be applied to many different circumstances and work well. For your more simpler and repetitive tasks like moving an object, you can just use simpler regression methods."
How are we calculating the average reward ($r(\pi)$) if the policy changes over time?,"
In the average reward setting, the quality of a policy is defined as:
$$ r(\pi) = \lim_{h\to\infty}\frac{1}{h} \sum_{j=1}^{h}E[R_j] $$
When we reach the steady state distribution, we can write the above equation as follows:
$$ r(\pi) = \lim_{t\to\infty}E[R_t | A \sim \pi] $$
We can use the incremental update method to find $r(\pi)$:
$$ r(\pi) = \frac{1}{t} \sum_{j=1}^{t} R_j = \bar R_{t-1} + \beta (R_t - \bar R_{t-1})$$
where $ \bar R_{t-1}$ is the estimate of the average reward $r(\pi)$ at time step $t-1$.
We use this incremental update rule in the SARSA algorithm:

Now, in this above algorithm, we can see that the policy will change with respect to time. But to calculate the $r(\pi)$, the agent should follow the policy $\pi$ for a long period of time. Then how we are using $r(\pi)$ if the policy changes with respect to time?
","['reinforcement-learning', 'deep-rl', 'sarsa']",
What is the optimal exploration-exploitation trade-off in Q*bert?,"
I am training an RL agent with Deep Q-learning + Experience Replay on the Q*bert Atari environment. After 400,000 frames, my agent appears to have learned strategic information about the game, but none about the environment. It has learned that a good immediate strategy is to simply jump down both diagonals and fall of the board, thus completing a large portion of the first level. However, it remains to understand neither the boundaries of the board to prevent jumping off, nor anything about avoiding enemies. I’m asking this here, instead of Stack Overflow because it is a more general question with less of a need in terms of programming understanding. Simply, I am asking whether or not this is a matter of a pore exploration policy (which I presume). If you agree, what should be a better exploration policy for Q*bert that would facilitate my agent’s learning experience?
As per the request of a comment:

Could you add what your current exploration approach is, and what options you are using for your Deep Q Learning implementation (e.g. replay size, batch size, NN architecture, steps per target network copy, or if you are using a different update mechanism for the target network). Also if you are using any other approach different to the classic DQN paper such as in state representation.

Here are my parameters:

Exploration policy: epsilon = min(1.0, 1000 / (frames + 1))
Replay Memory = 20,000 frames
Batch size = 32 transitions
NN architecture: Conv2D(64, 3, 2), Dropout(0.2), Dense(32, relu), Dense(32, relu), Dense(num_actions, linear)
Steps per target network copy: 100

","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'exploration-exploitation-tradeoff']","I can spot three, maybe four, things in your implementation that could be contributing to incomplete learning that you are observing.I think you have correctly identified that exploration could be an issue. In off-policy learning (which Q-learning is an instance of), it is usual to set a minimum exploration rate. It is a hyperparameter that you need to manage. Set too high, the agent will never experience the best rewards as it will make too many mistakes. Set too low, the agent will not explore enough to find the correct alternative actions when the opportunity to learn them occurs.I would suggest for you something like:You can choose numbers other than 0.01, but I think that is a reasonable start for many Atari games. You could try higher, up to 0.1 in games which are more forgiving of mistakes.I am not sure why, but I always have problems with dropout in RL neural networks. Try removing the dropout layer.Convolutional layers are very efficient generalisers for vision and grid-based problems. You won't really benefit much from having a single layer though. I would add another two, increase the number of output channels.It is not clear from your description whether you are using a single colour frame for the state representation, or stacked greyscale frames for the last 3 inputs. It should be the latter, and if you want to more closely replicate the orginal DQN Atari paper, you should take the previous 4 frames as input.In addition, you should be normalising the input into range $[0,1]$ or $[-1,1]$. The native image range $[0,255]$ is tricky for neural networks to process, and quite common for value functions to get stuck if you don't normalise."
Examples of single player games that use modern ML techniques in the AI?,"
Are there any examples of single player games that use modern ML technique in its games? By this I mean AI that plays with or against the human player, and not just play the game by itself (like Atari).
""Modern ML techniques"" is a vague term, but for example, Neural Networks, Reinforcement Learning, or probabilistic methods. Basically anything that goes above and beyond traditional search methods that most games use nowadays.
Ideally, the AI would be:

widely available (i.e. not like the OpenAI Five, which was only available for a limited amount of time and requires a high amount of computational power)
human level (not overpowered)

Ideally, the game would be:

symmetrical (the AI has the same agent capabilities as the player, though answers similar to The Director would be very interesting as well)
""complex environment"" (more complex than, say, a board game, but a CIV5 game might work)

But any answer would be appreciated, as some of the criteria above are quite vauge.
Edit: the ideal cases listed above are not meant to discourage other answers, nor are they intended to be of strictly inclusionary (ie: any game would need to satisfy all of the above requirements)
","['game-ai', 'reference-request']",
How to apply Q-learning when rewards is only available at the last state?,"
I have a scheduling problem in which there are $n$ slots and $m$ clients. I am trying to solve the problem using Q-learning so I have made the following state-action model.
A state $s_t$ is given by the current slot $t=1,2,\ldots,n$ and an action $a_t$ at slot $t$ is given by one client, $a_t\in\{1,2,\ldots,m\}$. In my situation, I do not have any reward associated with a state-action pair $(s_t,a_t)$ until the terminal state which is the last slot. In other words, for all $s_t\in\{1,2,\ldots,n-1\}$, the reward is $0$ and for $s_t=n$ I can compute the reward given $(a_1,a_2,\ldots,a_n)$.
In this situation, the Q table, $Q(s_t,a_t)$, will contain only zeros except for the last row in which it will contain the updated reward.
Can I still apply Q-learning in this situation? Why do I need a Q table if I only use the last row?
","['reinforcement-learning', 'q-learning', 'reward-functions', 'sparse-rewards', 'combinatorial-optimization']","Having only a non-zero reward at the very end is not uncommon. When rewards are sparse, it becomes a bit harder to learn compared to having lots of different rewards along the way, but for your problem, the goal state is always reached, so that should not be a problem. (The real problem with sparse rewards is that, if an agent can do a lot of exploration without every finding the goal, it essentially receives no feedback and will behave randomly, until it happens to stumble upon the very rare reward state.)What concerns me more about your problem is that the final reward depends not just on the last state visited, but also on the chain of actions taken so far. That means that, to make this a proper MDP, you need to keep the chain of action in the state. So, your state would be something of the type $(s_k, [a_1, a_2, \ldots, a_{k-1}])$.This kind of combinatorial problem is not what RL is really great at. RL is really good when the state and action together give a lot of information about the next state. Here it seems that, in your formulation, the next state is independent of the previous action.Instead of seeing this as a RL problem, you might want to express this as sequences of actions with an associated reward, and look at it as a combinatorial optimization problem."
Why is sampling non-uniformly from the replay memory an issue? (Prioritized experience replay),"
I can't seem to understand why we need importance sampling in prioritized experience replay (PER). The authors of the paper write on page 5:

The estimation of the expected value with stochastic updates relies on those updates corresponding to the same distribution as its expectation. Prioritized replay introduces bias because it changes this distribution in an uncontrolled fashion, and therefore changes the solution that the estimates will converge to (even if the policy and state distribution are fixed).

My understanding of this statement is that sampling non-uniformly from the replay memory is an issue.
So, my question is: Since we are working 1-step off-policy, why is it an issue? I thought that in an off-policy setting we don't care how transitions are sampled (at least in the 1-step case).
The one possibility for an issue that came to my mind is that in the particular case of PER, we are sampling transitions according to the errors and rewards, which does seem a little fishy.
A somewhat related question was asked here, but I don't think it answers my question.
","['reinforcement-learning', 'deep-learning', 'q-learning', 'experience-replay']","The problem is not that we need importance sampling because the learning is off-policy -- you are correct in that for one step off-policy algorithms such as $Q$-learning we don't need importance sampling, see e.g. here for an explanation why. The reason we need the importance sampling is due to the loss used to train the network.In the original DQN paper, the loss is defined as
$$L_i(\theta_a) = \mathbb{E}_{(s,a,r,s') \sim \mbox{U}(D)} \left[ \left( r + \gamma \max_{a'} Q(s',a' ; \theta_i^-) - Q(s,a;\theta_i) \right)^2 \right ]\;.$$
You can see here the expectation over the loss is taken according to a uniform distribution over the replayed buffer $D$. If we started randomly sampling non-uniformly, as is the case in PER, then the expectation wouldn't be satisfied and would introduce bias. Importance sampling is used to correct this bias. Note that the main reason that we sample uniformly at random is that gradient descent usually assumes that the data is i.i.d. You can argue that in an RL problem the data will never be fully i.i.d, but you can probably tell that sampling uniformly at random from a large buffer of experience is 'closer' to being i.i.d, whereas using a priority without importance sampling will likely lead to non-i.i.d. data (e.g. there is probably high correlation between experience that has high TD error, if the priority is chosen according to this).Note that in the paper they mention that the bias isn't as much of an issue at the start of learning and hence they use a decaying $\beta$ that only makes the importance sampling weights the 'correct' weights to use at the end of learning - this means that the estimate of the loss is asymptotically unbiased."
Why does (not) the distribution of states depend on the policy parameters that induce it?,"
I came across the following proof of what's commonly referred to as the log-derivative trick in policy-gradient algorithms, and I have a question -

While transitioning from the first line to the second, the gradient with respect to policy parameters $\theta$ was pushed into the summation. What bothers me is how it skipped over $\mu (s)$, the distribution of states - which (the way I understand it), is induced by the policy $\pi_\theta$ itself! Why then does it not depend on $\theta$?
Let me know what's going wrong! Thank you!
","['reinforcement-learning', 'policy-gradients', 'proofs']","The proof you are given in the above post is not wrong. It's just they skip some of the steps and directly written the final answer. Let me go through those steps:I will simplify some of the things to avoid complication but the generosity remains the same. Like I will think of the reward as only dependent on the current state, $s$, and current action, $a$. So, $r = r(s,a)$First, we will define the average reward as:
$$r(\pi) = \sum_s \mu(s)\sum_a \pi(a|s)\sum_{s^{\prime}} P_{ss'}^{a} r $$
We can further simplify average reward as:
$$r(\pi) = \sum_s \mu(s)\sum_a \pi(a|s)r(s,a) $$
My notation may be slightly different than the aforementioned slides since I'm only following Sutton's book on RL. Our objective function is:
$$ J(\theta) = r(\pi) $$
We want to prove that:
$$ \nabla_{\theta} J(\theta) = \nabla_{\theta}r(\pi) = \sum_s \mu(s) \sum_a \nabla_{\theta}\pi(a|s) Q(s,a)$$Now let's start the proof:
$$\nabla_{\theta}V(s) = \nabla_{\theta} \sum_{a} \pi(a|s) Q(s,a)$$
$$\nabla_{\theta}V(s) = \sum_{a} [Q(s,a) \nabla_{\theta} \pi(a|s)  + \pi(a|s) \nabla_{\theta}Q(s,a)]$$
$$\nabla_{\theta}V(s) = \sum_{a} [Q(s,a) \nabla_{\theta} \pi(a|s)  + \pi(a|s) \nabla_{\theta}[R(s,a) - r(\pi) + \sum_{s^{\prime}}P_{ss^{\prime}}^{a}V(s^{\prime})]]$$
$$\nabla_{\theta}V(s) = \sum_{a} [Q(s,a) \nabla_{\theta} \pi(a|s)  + \pi(a|s) [- \nabla_{\theta}r(\pi) + \sum_{s^{\prime}}P_{ss^{\prime}}^{a}\nabla_{\theta}V(s^{\prime})]]$$
$$\nabla_{\theta}V(s) = \sum_{a} [Q(s,a) \nabla_{\theta} \pi(a|s)  + \pi(a|s) \sum_{s^{\prime}}P_{ss^{\prime}}^{a}\nabla_{\theta}V(s^{\prime})] - \nabla_{\theta}r(\pi)\sum_{a}\pi(a|s)$$
Now we will rearrange this:
$$\nabla_{\theta}r(\pi) = \sum_{a} [Q(s,a) \nabla_{\theta} \pi(a|s)  + \pi(a|s) \sum_{s^{\prime}}P_{ss^{\prime}}^{a}\nabla_{\theta}V(s^{\prime})] - \nabla_{\theta}V(s)$$
Multiplying both sides by $\mu(s)$ and summing over $s$:
$$\nabla_{\theta}r(\pi) \sum_{s}\mu(s)= \sum_{s}\mu(s) \sum_{a} Q(s,a) \nabla_{\theta} \pi(a|s)  + \sum_{s}\mu(s) \sum_a \pi(a|s) \sum_{s^{\prime}}P_{ss^{\prime}}^{a}\nabla_{\theta}V(s^{\prime}) - \sum_{s}\mu(s) \nabla_{\theta}V(s)$$
$$\nabla_{\theta}r(\pi) = \sum_{s}\mu(s) \sum_{a} Q(s,a) \nabla_{\theta} \pi(a|s)  + \sum_{s^{\prime}}\mu(s^{\prime})\nabla_{\theta}V(s^{\prime}) - \sum_{s}\mu(s) \nabla_{\theta}V(s)$$
Now we are there:
$$\nabla_{\theta}r(\pi) = \sum_{s}\mu(s) \sum_{a} Q(s,a) \nabla_{\theta} \pi(a|s)$$
This is the policy gradient theoram for average reward formulation (ref. Policy gradient)."
What's the optimal policy in the rock-paper-scissors game?,"
A deterministic policy in the rock-paper-scissors game can be easily exploited by the opponent - by doing just the right sequence of moves to defeat the agent. More often than not, I've heard that a random policy is the optimal policy in this case - but the argument seems a little informal.
Could someone please expound on this, possibly adding more mathematical details and intuition? I guess the case I'm referring to is that of a game between two RL agents, but I'd be happy to learn about other cases too. Thanks!
EDIT: When would a random policy be optimal in this case?
","['reinforcement-learning', 'game-theory', 'optimal-policy']","For this, we will need game theory.In game theory, an optimal strategy is one that cannot be exploited by the opponent even if they know your strategy.Let's say you want a strategy where your move selection is not based on what happened before (so you are not trying to model your opponent, or trick them into believing you will always play scissors and then throw them off, anything like that).
A strategy will look like $(P, S, R)$, where $P, S, R \in [0, 1], P+S+R = 1$. You select paper with probability $P$, scissors with probability $S$, rock with probability $R$.
Now, if your probabilities are a bit uneven (for example $(0.5, 0.2, 0.3)$) an opponent can abuse that strategy. If your opponent plays with probabilities $(p, s, r)$, their expected reward (counting +1 for win, -1 for loss, 0 for draw) would be $0.5(s - r) + 0.2(r - p) + 0.3(p - s) = 0.1p + 0.2s - 0.3r$. If they wish to maximize their wins, they would play scissors all the time against you, and expect to have a distinct advantage over you.In general, for a strategy $(P, S, R)$ for you and $(p, s, r)$ for your opponent, your opponent's winnings would be $P(s - r) + S(r - p) + R(p - s) =  p(R-S) + s(P-R) + r(S - P)$. If all the partial derivatives of this, with respect to $p$, $s$ and $r$ are 0, the opponent has no way to maximize his winnings; they would have no incentive to play a particular move over any other move.
This occurs when $P = S = R = \frac13$.That's basically how to approach game theory: find a strategy so your opponent has no incentive to choose one action over another. The approach seems a bit counter-intuitive at first (you're trying to find the optimal strategy for your opponent instead of for yourself) but it works for many similar problems."
What are some other real-life examples of simple policies but complex value functions?,"
Hado van Hasselt, a researcher at DeepMind, mentioned in one of his videos (from 7:20 to 8:20) on Youtube (about policy gradient methods) that there are cases when the policy is very simple compared to the value function - and it makes more sense to learn the policy directly rather than first learning the value function and then doing control. He gives a very simple example at minute 7:20.
What are some other real-life examples (even just one example) of simple policies but complex value functions?
By real-life example I mean an example that is not as simple as a robot in a grid world, but some relatively complex real-world situations (say, autonomous driving).
","['reinforcement-learning', 'policy-gradients', 'policies', 'value-functions', 'applications']",
How to calculate v min and v max for C51 DQN,"
Background: In C51 DQNs you must specify a v-min/max to be used during training. The way this is generally done is you take the max score possible for the game and set that to v-max, then v-min is just negative v-max. For a game like Pong deciding the v-min/max is simple because the max score possible is 20, therefore, v_min=-20 and v_max=20.
Question: In a game like Space Invaders, there is no max score, so how would I calculate the v-min/max for a C51 DQN?
","['reinforcement-learning', 'dqn', 'probability-distribution']","If you're using a discount factor less than 1, you should be able to compute a maximum return (likewise, a minimum return) based on the max (min) reward you can earn at each timestep. However, this issue you bring up is usually cited as a difficulty with C51. I think people tend to simply use fixed values for the min/max return (or just make rough estimates). If you want to avoid this, I recommend looking into the QR-DQN algorithm which circumvents this issue altogether and is more theoretically sound."
Generation of 'new log probabilities' in continuous action space PPO,"
I have a conceptual question for you all that hopefully I can convey clearly. I am building an RL agent in Keras using continuous PPO to control a laser attached to a pan/tilt turret for target tracking. My question is how the new policy gets updated. My current implementation is as follows

Make observation (distance from laser to target in pan and tilt)
Pass observation to actor network which outputs a mean (std for now is fixed)
I sample from a gaussian with the mean output from step 2
Apply the command and observe the reward (1/L2 distance to target)
collect N steps of experience, compute advantage and old log probabilities,
train actor and critic

My question is this. I have my old log probabilities (probabilities of the actions taken given the means generated by the actor network), but I dont understand how the new probabilities are generated. At the onset of the very first minibatch my new policy is identical to my old policy as they are the same neural net. Given that in the model.fit function I am passing the same set of observations to generate 'y_pred' values, and I am passing in the actual actions taken as my 'y_true' values, the new policy should generate the exact same log probabilities as my old one. The only (slight) variation that makes the network update is from the entropy bonus, but my score
np.exp(new_log_probs-old.log_probs)  is nearly identically 1 because the policies are the same.
Should I be using a pair of networks similar to DDQN so there are some initial differences in the policies between the one used to generate the data and the one used for training?
","['reinforcement-learning', 'keras', 'objective-functions', 'policy-gradients', 'proximal-policy-optimization']",
"Have agents that ""dream"" been explored in Reinforcement Learning?","
I was reading this article about the question ""Why do we dream?"" in which the author discusses dreams as a form of rehearsal for future threats, and presents it as an evolutive advantage. My question is whether this idea has been explored in the context of RL.
For example, in a competition between AIs on a shooter game, one could design an agent that, besides the behavior it has learned in a ""normal"" training, seeks for time in which is out of danger, to then use its computation time in the game to produce simulations that would further optimize its behavior. As the agent still needs to be somewhat aware of its environment, it could alternate between processing the environment and this kind of simulation. Note that this ""in-game"" simulation has an advantage with respect to the ""pre-game"" simulations used for training; the agent in the game experiences the behavior of the other agents, which could not have been predicted beforehand, and then simulates on top of these experiences, e.g. by slightly modifying them.
For more experienced folks, does this idea make sense? has something similar been explored?
I have absolutely no experience in the field, so I apologize if this question is poorly worded, dumb or obvious. I would appreciate suggestions on how to improve it if this is the case.
","['reinforcement-learning', 'reference-request', 'model-based-methods', 'imagination']","Yes, the concept of dreaming or imagining has already been explored in reinforcement learning.For example, have a look at Metacontrol for Adaptive Imagination-Based Optimization (2017) by Jessica B. Hamrick et al., which is a paper that I gave a talk/presentation on 1-2 years ago (though I don't remember well the details anymore).There is also a blog post about the topic Agents that imagine and plan (2017) by DeepMind, which discusses two more recent papers and also mentions Hamrick's paper.In 2018, another related and interesting paper was also presented at NIPS, i.e. World Models, by Ha and Schmidhuber.If you search for ""imagination/dreaming in reinforcement learning"" on the web, you will find more papers and articles about this interesting topic."
What type of model should I fit to increase accuracy?,"
Currently, I'm working on 6-axis IMU(Inertial Measurment Unit) dataset. This dataset contain 6 axis IMU data of 7 different drivers. The Imu sensor attached on vehicle. The drivers drives same path. So, the dataset include 6 feature columns and a label column.
I tried multiple neural network models.The sensor data is a sequential data so I tried LSTM(Long Short Term Memory) & classical fully-connected layers.
Some of my architecture(in keras framework):

Layer (type)                 Output Shape              Param #   

lstm_4 (LSTM)                (None, 1, 128)            69120     
_________________________________________________________________
lstm_5 (LSTM)                (None, 1, 64)             49408     
_________________________________________________________________
lstm_6 (LSTM)                (None, 1, 32)             12416     
_________________________________________________________________
dense_8 (Dense)              (None, 1, 64)             2112      
_________________________________________________________________
dropout_2 (Dropout)          (None, 1, 64)             0         
_________________________________________________________________
dense_9 (Dense)              (None, 1, 7)              455       


2nd Architecture:

=================================================================
dense_10 (Dense)             (None, 32)                224       
_________________________________________________________________
dense_11 (Dense)             (None, 64)                2112      
_________________________________________________________________
dense_12 (Dense)             (None, 128)               8320      
_________________________________________________________________
dense_13 (Dense)             (None, 256)               33024     
_________________________________________________________________
dropout_3 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_14 (Dense)             (None, 512)               131584    
_________________________________________________________________
dense_15 (Dense)             (None, 256)               131328    
_________________________________________________________________
dense_16 (Dense)             (None, 128)               32896     
_________________________________________________________________
dense_17 (Dense)             (None, 64)                8256      
_________________________________________________________________
dropout_4 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_18 (Dense)             (None, 128)               8320      
_________________________________________________________________
dense_19 (Dense)             (None, 7)                 903     

The best accuracy in my models was %70 which is not good. How style of layers should I use to handle this data? Or, which type of model would increase accuracy?
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'deep-neural-networks']",after reading some literature in the area I'd recommend the following:
Why does the output shape of a Dense layer contain a batch size?,"
I understand that the batch size is the number of examples you pass into the neural network (NN). If the batch size is 10, it means you feed the NN 10 examples at once.
Assuming I have an NN with a single Dense layer. This Dense layer of 20 units has an input shape (10, 3). This means that I am feeding the NN 10 examples at once, with every example being represented by 3 values. This Dense layer will have an output shape of (10, 20).
I understand that the 20 in the 2nd dimension comes from the number of units in the Dense layer. However, what does the 10 (Batch Size) in the first dimension mean? Does this mean that the NN learns 10 separate sets of weights (with each set of weights corresponding to one example, and one set of weights being a matrix of 60 values:3 features x 20 units)?
","['neural-networks', 'keras', 'hidden-layers', 'dense-layers', 'batch-learning']","The Dense layers outputs 20 values per example. And since you have 10 examples in the batch the output is (10, 20) (one set of 20 values per example in the batch).
The nn doesn't learn 10 separate sets of weights. Each set of 20 values is computed with the same weight (and bias if you have any). So if say example 2 and 5 had the same input values, they'll always have the same output values."
Compressing text using AI by sending only prediction rank of next word,"
Is there any effort made to compress text (and maybe other media) using prediction of next word and thus sending only the order number of the word/token which will be predicted on the client side
i.e
Server text: This is an example of a long text example, custom word flerfom inserted to confuse, that may appear on somewhere
Compressed Text transmitted : This [choice no 3] [choice no 4] [choice no 1] [choice no 6] [choice no 1] [choice no 3] [choice no 1], custom word flerfom [choice no 4] inserted [choice no 4] confuse [choice no 5] [choice no 4] [choice no 6] [choice no 5] on somewhere
(Note: of course [choice no 3] will be shortened to [3] to save bytes and also maybe we can do much better in some cases by sending the first letter of the word)
of course it means that the client side neural network has to be static or only updated in a predictable fasion, so the server knows for sure that the client neural network's predictions will follow the given choice orders. I tried example with https://demo.allennlp.org/next-token-lm, but the prediction is not that good. maybe gpt-3 can do better . but its too heavy for use in a normal pc / mobile device
In more details, the process is
Deploy the same model on both sides
Predict the next word after the starting word
Keep the prediction limit say 100
For any word which have more than 2 characters we do the prediction
If the current word is predicted within the top 100 predictions of the model , we can essentially replace it with a numeric char between 0-99 (inclusive) so we are replacing a say , 5 character word with a 2 character numerical char..
And if the word is not predicted in top 100 we send the word as it is..
As much better the model predicts, that much better the compression
And under no scenario it will work worse than the existing method..
","['natural-language-processing', 'reference-request', 'prediction', 'game-theory', 'information-theory']",
"How are the ""Link Strength true"", ""Link Strength blind"" and ""Mutual Information"" calculated in this report on Bayesian networks?","
I'm trying to understand how to calculate the strength of every arc in a Bayesian Network.
I came across this report Measuring Connection Strengths and Link Strengths in Discrete Bayesian Networks, but I got lost in the calculation.
In particular, how are the values of Link Strength true, Link Strength blind, and Mutual Information computed in Table 1?

","['papers', 'bayesian-networks']",
Mathematical calculation behind decision tree classifier with continuous variables,"
I am working on a binary classification problem having continuous variables (Gene expression Values). My goal is to classify the samples as case or control using gene expression values (from Gene-A, Gene-B and Gene-C) using decision tree classifier. I am using the entropy criteria for node splitting and is implementing the algorithm in python. The classifier is easily able to differentiate the samples.
Below is the sample data,
sample training set with labels
Gene-A    Gene-B    Gene-C    Sample
   1        0         38       Case
   0        7         374      Case
   1        6         572      Case
   0        2         538      Control
   33       5         860      Control

sample testing set labels
Gene-A    Gene-B    Gene-C    Sample
  1         6        394       Case
  13        4        777       Control

I have gone through a lot of resources and have learned, how to mathematically calculate Gini-impurity, entropy and information gain.
I am not able to comprehend how the actual training and testing work. It would be really helpful if someone can show the calculation for training and testing with my sample datasets or provide an online resource?
","['machine-learning', 'math', 'decision-trees']","Of course, it depends on what algorithm you use. Typically, a top-down algorithm is used.You gather all the training data at the root. The base decision is going to be whatever class you have most of. Now, we see if we can do better.We consider all possible splits. For categorical variables, every value gets its own node. For continuous variables, we can use any possible midpoint between two values (if the values were sorted). For your example, possible splits are Gene-A < 0.5, Gene-A < 17, Gene-B < 1, Gene-B < 3.5, and so on. There is a total of 10 possible splits.For each of those candidate splits, we measure how much the entropy decreases  (or whatever criterion we selected) and, if this decrease looks significant enough, we introduce this split.
For example. Our entropy in the root node is $-0.4 \log_2 0.4 - 0.6 \log_2 0.6 \approx 0.97$. If we introduce the split Gene-A < 0.5, we get one leaf with entropy $1$ (with 2 data points in it), and one leaf with entropy $0.918$ (with 3 data points). The total decrease of entropy is $0.97 - (\frac25 \times 1 + \frac35 \times 0.918) \approx 0.02$.
For the split Gene-A < 17 we get a decrease of entropy of about $0.3219$.The best splits for the root are Gene-B < 5.5 and Gene-C < 456. These both reduce the entropy by about $0.42$, which is a substantial improvement.When you choose a split, you introduce a leaf for the possible outcomes of the test. Here it's just 2 leaves: ""yes, the value is smaller than the threshold"" or ""no, it is not smaller"". In every leaf, we collect the training data from the parent that corresponds to this choice. So, if we select Gene-B < 5.5 as our split, the ""yes"" leaf will contain the first, fourth and fifth data points, and the ""no"" leaf will contain the other data points.Then we continue, by repeating the process for each of the leaves. In our example, the ""yes"" branch can still be split further. A good split would be Gene-C < 288, which results in pure leaves (they have 0 entropy).When a leaf is ""pure enough"" (it has very low entropy) or we don't think we have enough data, or the best split for a leaf is not a significant improvement, or we have reached a maximum depth, you stop the process for that leaf. In this leaf you can store the count for all the classes you have in the training data.If you have to make a prediction for a new data point (from the test set), you start at the root and look at the test (the splitting criterion). For example, for the first test point, we have that Gene-B < 5.5 is false, so we go to the 'no' branch. You continue until you get to a leaf.In a leaf, you would predict whatever class you have most of. If the user wants, you can also output a probability by giving the proportion.
For the first test point, we go to the ""no"" branch of the first test, and we end up in a leaf; our prediction would be ""Case"". For the second test point, we go to the ""yes"" branch of the first test. Here we test whether 777 < 288, which is false, so we go to the ""no"" branch, and end up in a leaf. This leaf contains only ""Control"" cases, so our prediction would be ""Control""."
Why does Simulated Annealing not take worse solution if the energy difference becomes higher?,"
In Simulated Annealing, a worse solution is accepted with this probability:
$$p=e^{-\frac{E(y)-E(x)}{kT}}.$$
If that understanding is correct: Why is this probability function used? This means that, the bigger the energy difference, the smaller the probability of accepting the new solution. I would say the bigger the difference the more we want to escape a local minimum. I plotted that function in Matlab in two dimensions:

","['optimization', 'simulated-annealing', 'meta-heuristics']",
How classification neural nets are different from simple dimension reduction + clustering?,"
I know the training of neural nets involves some sort of dimension manipulation to separate classes of different features.
If there is no variation of features, no matter for neural nets or simple dimension reduction methods (e.g. PCA, LDA) + clustering, neither of them are going to distinguish different classes.
In such sense, I would like to know the true power of neural nets:
How classification neural nets are different from simple dimension reduction + clustering?
or rephrase the question:
What value do neural nets add to solving classification problems in terms of its algorithmic architecture compared with simple dimension reduction + clustering?
","['neural-networks', 'deep-learning', 'dimensionality-reduction']",
Is the stride applied both in the horizontal and vertical directions in convolutional neural networks?,"
In the convolutional layer for CNNs, when you specify the stride of a filter, typical notes show some examples of this but only for the horizontal panning. Is this same stride applied for the vertical direction too when you're done with the current row?
In other words, say our input volume is 7x7, and we apply a stride of 1 for a 3x3 filter. Is the output volume 5x5? (which would mean you applied the stride in both the horizontal and vertical panning).
Is it possible to apply a different stride for each direction?
","['convolutional-neural-networks', 'convolutional-layers', 'stride']",
Is there a logical method of deducing an optimal batch size when training a Deep Q-learning agent with experience replay?,"
I am training an RL agent using Deep-Q learning with experience replay. At each frame, I am currently sampling 32 random transitions from a queue which stores a maximum of 20000 and training as described in the Atari with Deep RL paper. All is working fine, but I was wondering whether there is any logical way to select the proper batch size for training, or if simply using a grid search is best. At the moment, I’m simply using 32, for its small enough that I can render the gameplay throughout training at a stunning rate of 0.5fps. However, I’m wondering how much of an effect batch size has, and if there is any criteria we could generalize across all Deep Q-learning tasks.
","['q-learning', 'dqn', 'deep-rl']","There is no special calculation you can do to determine the optimal batch size for any situation, so you kinda have to do a bit of testing to determine what batch size will work best. But there are some common trends you can take into account to make your testing easier.According to the paper Accelerated Methods for Deep Reinforcement Learning you get the best performance from DQNs (on average) with a batch size of 512. The problem with this is that is is much slower than the usual batch size of 32 and most of the time the performance improvement doesn't warrant it.If you are just trying to test out your agents it is generally best to stick with a batch size of 32 or 64 so that you can train the agent quickly yet still get an idea of what it is capable of. But if getting the best performance is your top priority and waiting longer isn't a problem, then you should go for a batch size of 512 (higher can actually lead to worse performance) or something near that."
Why doesn't value iteration use $\pi(a \mid s)$ while policy evaluation does?,"
I was looking at the Bellman equation, and I noticed a difference between the equations used in policy evaluation and value iteration.
In policy evaluation, there was the presence of $\pi(a \mid s)$, which indicates the probability of choosing action $a$ given $s$, under policy $\pi$. But this probability seemed to be omitted in the value iteration formula. What might be the reason? Maybe an omission?
","['reinforcement-learning', 'policies', 'value-iteration', 'policy-iteration', 'bellman-equations']","You appear to comparing the value table update steps in policy iteration and value iteration, which are both derived from Bellman equations.In policy iteration, a policy lookup table is generated, which can be arbitrary. It usually maps a deterministic policy $\pi(s): \mathcal{S} \rightarrow \mathcal{A}$, but can also be of the form $\pi(a|s): \mathcal{A} \times  \mathcal{S} \rightarrow \mathbb{R} = Pr\{A_t = a |S_t =s\}$. Policy iteration then alternately evaluates then improves that policy, with the improvement always being to act greedily with respect to expected return. Because the policy function can be arbitrary, and also the current value estimates during evaluation might not relate to it directly, the function $\pi(s)$ or $\pi(a|s)$ needs to be shown.Typically with policy iteration, you will see this update rule:$$V(s) \leftarrow \sum_{r,s'} p(r,s'|s,\pi(s))(r + \gamma V(s'))$$The above rule is for evaluating a deterministic policy, and is probably more commonly used. There is no real benefit in policy iteration to working with stochastic policies.For completeness, the update rule for an arbitrary stochastic policy is:$$V(s) \leftarrow \sum_a \pi(a|s) \sum_{r,s'} p(r,s'|s,a)(r + \gamma V(s'))$$In value iteration, the current policy to evaluate is to always take the greedy action with respect to the current evaluations. As such, it does not need to be explicity written, because it can be derived from the value function, and so can the terms in the Bellman equation (specifically the Bellman equation for the optimal value function is used here, which usually does not refer the policy). What you would typically write for the update step is:$$V(s) \leftarrow \text{max}_a \sum_{r,s'} p(r,s'|s,a)(r + \gamma V(s'))$$However, you can write this out as if there was a policy table:$$\pi(s) \leftarrow \text{argmax}_a \sum_{r,s'} p(r,s'|s,a)(r + \gamma V(s'))$$
$$a \leftarrow \pi(s)$$
$$V(s) \leftarrow \sum_{r,s'} p(r,s'|s,a)(r + \gamma V(s'))$$This is not the usual way to implement it though, because of the extra maximum value search required to identify the action. In simple value iteration it does not matter what the interim action choices and policies actually are, and you can always derive them from the value function if you want to know.You will find other algorithms that drive the current policy direct from a value function, and when they are described in pseudo-code they might not have an explicit policy function. It is still there, only the Bellman update is easily calculated directly from the value function, so the policy is not shown in the update step. Descriptions of SARSA and Q-learning are often like that."
How does the RL agent understand motion if it gets only one image as input?,"
Basic deep reinforcement learning methods use as input an image for the current state, do some convolutions on that image, apply some reinforcement learning algorithm, and it is solved.
Let us take the game Breakout or Pong as an example. What I do not understand is: how does the agent understand when an object is moving towards it or away from it? I believe that the action it chooses must be different in these two scenarios, and, from a single image as input, there is no notion of motion.
","['reinforcement-learning', 'deep-rl', 'data-preprocessing', 'atari-games']","In the article Playing Atari with Deep Reinforcement Learning, Mnih et al, 2013, which was a major outbreak in Deep Reinforcement learning (especially in Deep Q learning), they don't feed only the last image to the network. They stack the 4 last images :For the experiments in this paper, the function φ from algorithm 1
applies this preprocessing to the last 4 frames of a history and stacks
them to produce the input to the Q-functionSo they add the motion through sequentiality. From various articles and own coding experiences, this seems to me to be the main common approach. I don't know if other techniques have been implemented.One thing we could imagine would be to compute the Cross-correlation between a previous frame and the last one, and then feed the cross correlation product to the net.Another idea would be to train previously a CNN to extract motion features from a sequence of frames, and feed these extracted features to your net. This article (Performing Particle Image Velocimetry using Artificial Neural Networks: a proof-of-concept), Rabault et al, 2017 is an example of a CNN to extract motion features."
Feeding YOLOv4 image data into LSTM layer?,"
How would one extract the feature vector from a given input image using YOLOv4 and pass that data into an LSTM to generate captions for the image?
I am trying to make an image captioning software in PyTorch using YOLO as the base object classifier and an LSTM as the caption generator.
Can anyone help me figure out what part of the code I would need to call and how I would achieve this?
Any help is much appreciated.
","['natural-language-processing', 'long-short-term-memory', 'pytorch', 'yolo']",
How to handle a changing in the Reinforcement Learning environment where there is increasing or decreasing in number of agents?,"
I'm working in A2C and I have an environment where there is increasing or decreasing in the number of agents. The action space in the environment will not change but the state will change when new agents join or leave the game.
I have tried
encoder-decoder model with attention but the problem is that the state and the model will change when the number of agents is changing.
I also tried this way where they use LSTM to get the Q value for the agent but I got this message
Cannot interpret feed_dict key as Tensor: Tensor Tensor(""state:0"", shape=(137,), dtype=float32) is not an element of this graph.

or error like this because of changing of state size
ValueError: Cannot feed value of shape (245,) for Tensor 'state:0', which has shape '(161,)'

(1) Are there any reference papers that deal with such a problem?
(2) What is the best way to deal with the new agents that join or leave the game?
(3) How to deal with the changing of state space?
","['reinforcement-learning', 'tensorflow', 'actor-critic-methods', 'environment', 'multi-agent-systems']",
"Based on the Turing test, what would be the criteria for an agent to be considered smart?","
Based on the Turing test, what would be the criteria for an agent to be considered smart?
","['intelligence-testing', 'turing-test']","If an artificial agent (AA) passes the (standard) Turing test (i.e. where you have to imitate a human that speaks), then, on average, the AA should be able to imitate any human in any situation that mainly requires the conversation abilities and common-sense knowledge of a human, without being ever recognized as an AA.For example, if you want to talk about football, you don't expect the AA only to say ""I don't know"" or to clearly avoid a topic (e.g. by redirecting you to a search engine) when it doesn't know something (although some humans behave in this way), but you expect it to have common-sense knowledge, such as that Messi, Cristiano Ronaldo, Pelé, Maradona, etc., are among the best players of all time, and this should be precious information that the AA should have in any case, even if it doesn't know much about football.You also expect it to be emotional and have a personality, given that humans are emotional and have personalities. So, in the example above, maybe the AA could say that Maradona is its favorite player, and then it could explain why in an emotional way (e.g. by changing the tone of the voice).The AA should also be able to keep track of (almost) everything you and it said, and it should be able to contextualize very well, as humans do. Some personal assistants already take context into account, but they don't do this very well or just do it to a little extent.The AA should also be able to reason given the current situation. For example, if you explain something to the AA, you expect it to infer or predict something based on the information it has acquired and the common-sense knowledge.Moreover, when you speak or write to the AA, you don't expect it to regularly hear badly or not understand what you say or ask (and ask you to repeat), but you expect it to understand well what you say almost always, provided you don't talk or write trash. You also don't expect big delays and you expect the AA to at least say something while it searches for a more appropriate answer, although not all humans behave in this way, but I think that interjections or words such as ""hm"", ""well"", ""let me think..."", etc., will be very important to make the AA look or sound like a human. In general, the AA should be as interactive as a human.These are some traits that the AA absolutely needs to have in order to pass the Turing test (and be considered intelligent according to that test), but there are probably many others."
How to train a hierarchical DQN to play the Montezuma's Revenge game?,"
Would anybody share the experience on how to train a hierarchical DQN to play the Montezuma's Revenge game? How should I design the reward function? How should I balance the anneal rate of the two-level?
I've been trying to train an agent to solve this game. The agent is with 6 lives. So, every time when the agent fetches the key and loses his life for the instability or power of the sub-goal network, the agent restart at the original location and simply go through the door, thus gets a huge reward. With an $\epsilon$-greedy rate 0.1, the agent is possible to choose the subgoal key network and fetch the key, so the agent always chooses the door as the subgoal.
Would anyone show me how to train this agent in the setting of one life?
","['dqn', 'atari-games', 'hierarchical-rl']",
How to combine specific CNN models that work better at slightly different tasks?,"
I'm not sure how to describe this in the most accurate way but I'll give it a shot.
I've developed a Inception-Resnet V2 model for detecting audio signals via spectrogram.  It does a pretty good job but is not exactly the way I'd like it to be.
Some details:  I use 5 sets of data to evaluate my model during training.  They are all similar but slightly different.  Once I get to a certain threshold of F1 Scores for each training set I stop training.  My overall threshold is pretty hard to get to.  Every time training develops a model that produces a ""best yet"" of one of these data sets I save the model.
What I've noticed is that, during training, some round will produce a high F1 Score for one particular set while the other sets languish as mediocre.  Then, several dozen rounds later, another data set will peak while the others are mediocre.  Overall the entire model gets better but there are always some models that work better for some data sets.
What I would like to know is, given I might have 5 different models that each work better for a particular subset of data, is there a way that I can combine these models (either as a whole or better yet their particular layers) to produce a single model that works the best for all my data validation subsets?
Thank you.  Mecho
","['convolutional-neural-networks', 'models', 'audio-processing']",
What is the difference between active learning and online learning?,"
The definitions for these two appear to be very similar, and frankly, I've been only using the term ""active learning"" the past couple of years. What is the actual difference between the two? Is one a subset of the other?
","['machine-learning', 'comparison', 'terminology', 'online-learning', 'active-learning']","Active learning (AL) is a weakly supervised learning (WSL) technique where you can have both labelled and unlabelled data [1]. The main idea behind AL is that the learner (or learning algorithm) can query an ""oracle"" (e.g. a human) to label some unlabelled instances. AL is similar to semi-supervised learning (SSL), which is also a WSL technique, given that both deal with unlabelled and labeled data, but do that differently (i.e. SSL does not use an oracle).Online learning are machine learning techniques that update the models as new data is collected or arrives sequentially, as opposed to batch learning (or offline learning), where you first collect a dataset of multiple instances and then you train a model once (although you can later update it when you update your dataset). Batch learning is currently the common way of training machine learning models, given that it avoids problems like the known catastrophic interference (aka catastrophic forgetting) problem, which can occur if you learn online. For example, neural networks are known to face this problem when learning online. There are incremental learning (aka lifelong learning) algorithms that attempt to address this catastrophic interference problem."
How can traditional edge detection algorithms be implemented on a GPU?,"
How can edge detection algorithms, which are not based on deep learning, such as the canny edge detector, be implemented on a GPU? For example, how are non-edge pixels removed from an image once it detects all the edges?
The reason why I am asking this question is that when writing data to memory the GPU cores can't see what memory locations the other cores are writing to, so I am interested in knowing how traditional edge detectors can be implemented in on GPU.
","['computer-vision', 'image-processing', 'gpu', 'edge-detection', 'canny-edge-detector']",
Why isn't medical imaging improving faster with AI?,"
Researcher here. I just read this piece about medical imaging ai with object recognition and it left me wondering why there are still 100,000+ deaths a year in the US due to misdiagnosis - anyone out there working on these problems? Vinod Khosla famously said that he'd rather get surgery from AI than from a human - so where are we at with that?
","['computer-vision', 'image-recognition', 'object-recognition']",
How is BERT different from the original transformer architecture?,"
As far as I can tell, BERT is a type of Transformer architecture. What I do not understand is:

How is Bert different from the original transformer architecture?

What tasks are better suited for BERT, and what tasks are better suited for the original architecture?


","['natural-language-processing', 'comparison', 'transformer', 'bert']","The original transformer, proposed in the paper Attention is all you need (2017), is an encoder-decoder-based neural network that is mainly characterized by the use of the so-called attention (i.e. a mechanism that determines the importance of words to other words in a sentence or which words are more likely to come together) and the non-use of recurrent connections (or recurrent neural networks) to solve tasks that involve sequences (or sentences), even though RNN-based systems were becoming the standard practice to solve natural language processing (NLP) or understanding (NLU) tasks. Hence the name of the paper ""Attention is all you need"", i.e. you only need attention and you don't need recurrent connections to solve NLP tasks.Both the encoder-decoder architecture and the attention mechanism are not novel proposals. In fact, previous neural network architectures to solve many NLP tasks, such as machine translation, had already used these mechanisms (for example, take a look at this paper).  The novelty of the transformer and this cited paper is that it shows that we can simply use attention to solve tasks that involve sequences (such as machine translation) and we do not need recurrent connections, which is an advantage, given that recurrent connections can hinder the parallelization of the training process.The original transformed architecture is depicted in figure 1 of the cited paper. Both the encoder and decoder are composed ofThe decoder part is also composed of a linear layer followed by a softmax to solve the specific NLP task (for example, predict the next word in a sentence).BERT stands for Bidirectional Encoder Representations from
Transformers, so, as the name suggests, it is a way of learning representations of a language that uses a transformer, specifically, the encoder part of the transformer.BERT is a language model, i.e. it represents the statistical relationships of the words in a language, i.e. which words are more likely to come after another word and stuff like that. Hence the part Representations in its name, Bidirectional Encoder Representations from
Transformers.BERT can be trained in an unsupervised way for representation learning, and then we can fine-tune BERT on the so-called downstream tasks in a supervised fashion (i.e. transfer learning). There are pre-trained versions of BERT that can be already fine-tuned (e.g. this one) and used to solve your specific supervised learning task. You can play with this TensorFlow tutorial to use a pre-trained BERT model.On the other hand, the original transformed was not originally conceived to be a language model, but to solve sequence transduction tasks (i.e. converting one sequence to another, such as machine translation) without recurrent connections (or convolutions) but only attention.BERT is only an encoder, while the original transformer is composed of an encoder and decoder. Given that BERT uses an encoder that is very similar to the original encoder of the transformer, we can say that BERT is a transformer-based model. So, BERT does not use recurrent connections, but only attention and feed-forward layers. There are other transformed-based neural networks that use only the decoder part of the transformer, for example, the GPT model.BERT uses different hyper-parameters than the ones used in Attention is all you need to achieve the best performance. For example, it uses 12 and 16 ""attention heads"" (please, read the transformer paper to know more about these ""attention heads"") rather than 8 (although in the original transformer paper the authors experimented with a different number of heads).BERT also uses segment embeddings, while the original transformer only uses word embeddings and positional encodings.There are probably other small differences that I missed, but, after having read the paper Attention is all you need and quickly read some parts of the BERT paper, these seem to be the main differences.Although I never used them, I would say that you want to use BERT whenever you want to solve an NLP task in a supervised fashion, but your labeled training dataset is not big enough to achieve good performance. In that case, you start with a pre-trained BERT model, then fine-tune it with your small labeled dataset. You probably need to add specific layers to BERT to solve your task."
Why scaling reward drastically affects performance?,"
I have devised an gridworld-like environment where a RL agent is tasked to cover all the blank squares by passing through them. Possible actions are up, down, left, right. The reward scheme is the following: +1 for covering a blank cell, and -1 per step. So, if the cell was colored after a step, the summed reward is (+1) + (-1) = 0, otherwise it is (0) + (-1) = -1. The environment is a tensor whose layers encode the positions to be covered and the position of the agent.
Under this reward scheme, DQN fails to find a solution (implementation: stable_baselines3). However, when the rewards are reduced by a factor of 10 to +0.1/-0.1, then the algorithm learns an optimal path.
I wonder why that happens. I have tried reducing learning rate and gradient clipping (by norm) for the first case to see whether it will improve the learning, but it does not.
The activation function used is ReLU
","['neural-networks', 'reinforcement-learning', 'dqn', 'deep-rl']",
How homographs is an NLP task can be treated?,"

A homograph - is a word that shares  the same written form as another word but has a different meaning.

They can be even different parts of speech. For example:

close(verb) - close(adverb)
lead(verb) - lead(noun)
wind(noun) - wind(verb)

And there is rather a big list https://en.wikipedia.org/wiki/List_of_English_homographs.
As far as I understand, after processing the text data in any conventional way, lemmatization, building an embedding, these words, despite having different meaning, and appearing in different contexts, would be absolutely the same for the algorithm, and in the end we would get some averaged context between two or more meainings of the word. And this embedding would be meaningless.
How is this problem treated or these words are regarded to be too rare to have a significant impact on the quality of resulting embedding?
I would appreciate comments and references to the papers or sources
","['natural-language-processing', 'word-embedding', 'natural-language-understanding']",
"What are the variables that need to be saved and loaded, so that a DQN model starts where it left off?","
TensorFlow allows users to save the weights and the model architecture, however, that will be insufficient unless the values of certain other variables are also stored. For instance, in DQN, if $\epsilon$ is not stored the model will start exploring from scratch and a new model will have to be trained.
What are the variables that need to be saved and loaded, so that a DQN model starts where it left off? Some pseudocode will be highly appreciated!
Here is my current model with code
## Slightly modified from the following repository - https://github.com/gsurma/cartpole

from __future__ import absolute_import, division, print_function, unicode_literals

import os
import random
import gym
import numpy as np
import tensorflow as tf

from collections import deque
from tensorflow.models import Sequential
from tensorflow.layers import Dense
from tensorflow.optimizers import Adam


ENV_NAME = ""CartPole-v1""

GAMMA = 0.95
LEARNING_RATE = 0.001

MEMORY_SIZE = 1000000
BATCH_SIZE = 20

EXPLORATION_MAX = 1.0
EXPLORATION_MIN = 0.01
EXPLORATION_DECAY = 0.995

checkpoint_path = ""training_1/cp.ckpt""


class DQNSolver:

    def __init__(self, observation_space, action_space):
        self.exploration_rate = EXPLORATION_MAX

        self.action_space = action_space
        self.memory = deque(maxlen=MEMORY_SIZE)

        self.model = Sequential()
        self.model.add(Dense(24, input_shape=(observation_space,), activation=""relu""))
        self.model.add(Dense(24, activation=""relu""))
        self.model.add(Dense(self.action_space, activation=""linear""))
        self.model.compile(loss=""mse"", optimizer=Adam(lr=LEARNING_RATE))

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() < self.exploration_rate:
            return random.randrange(self.action_space)
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def experience_replay(self):
        if len(self.memory) < BATCH_SIZE:
            return
        batch = random.sample(self.memory, BATCH_SIZE)
        for state, action, reward, state_next, terminal in batch:
            q_update = reward
            if not terminal:
                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))
            q_values = self.model.predict(state)
            q_values[0][action] = q_update
            self.model.fit(state, q_values, verbose=0)
        self.exploration_rate *= EXPLORATION_DECAY
        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)


def cartpole():
    env = gym.make(ENV_NAME)
    #score_logger = ScoreLogger(ENV_NAME)
    observation_space = env.observation_space.shape[0]
    action_space = env.action_space.n
    dqn_solver = DQNSolver(observation_space, action_space)
    checkpoint = tf.train.get_checkpoint_state(os.getcwd()+""/saved_networks"")
    print('checkpoint:', checkpoint)
    if checkpoint and checkpoint.model_checkpoint_path:
        dqn_solver.model = keras.models.load_model('cartpole.h5')
        dqn_solver.model = model.load_weights('cartpole_weights.h5')
        
    run = 0
    i = 0
    while i<5:
        i = i + 1
        #total = 0
        run += 1
        state = env.reset()
        state = np.reshape(state, [1, observation_space])
        step = 0
        while True:
            step += 1
            #env.render()
            action = dqn_solver.act(state)
            state_next, reward, terminal, info = env.step(action)
            #total += reward
            reward = reward if not terminal else -reward
            state_next = np.reshape(state_next, [1, observation_space])
            dqn_solver.remember(state, action, reward, state_next, terminal)
            state = state_next
            dqn_solver.model.save('cartpole.h5')
            dqn_solver.model.save_weights('cartpole_weights.h5')
            if terminal:
                print(""Run: "" + str(run) + "", exploration: "" + str(dqn_solver.exploration_rate) + "", score: "" + str(step))
                #score_logger.add_score(step, run)
                break
            dqn_solver.experience_replay()


if __name__ == ""__main__"":
    cartpole()

","['reinforcement-learning', 'dqn', 'deep-rl', 'implementation']","Typically you would need to save the network weights, hyper-parameters and the replay buffer if you wanted to stop training and then come back at a later date and carry on training. Usually, I do this by writing it all as a class in Python (the agent, the memory buffer, hyper-parameters etc.) and saving the final object with Pickle.Looking at your code, the only thing I would personally have done different would be to define the model outside of the class and have the class take as input a network; however I usually use PyTorch as opposed to Keras/Tensorflow so I'm not sure which method works better.As per OP's request in the comments, here is a snippet of code I used for Car-pool."
"What does the notation $\mathcal{N}(z; \mu, \sigma)$ stand for in statistics?","
I know that the notation $\mathcal{N}(\mu, \sigma)$ stands for a normal distribution.
But I'm reading the book ""An Introduction to Variational Autoencoders"" and in it, there is this notation:
$$\mathcal{N}(z; 0, I)$$
What does it mean?
picture of the book:

","['terminology', 'variational-autoencoder', 'notation', 'random-variable', 'bayesian-statistics']",It means that $z$ has a (multivariate) normal distribution with 0 mean and identity covariance matrix. This essentially means each individual element of the vector $z$ has a standard normal distribution.
"How can I be sure that the final model, trained on all data, is correct?","
The 'by the book' method of delivering final machine learning models is to include all data in the final training (including validation and test sets). To check robustness of my model I use randomly chosen population for training and validation sets with each training (no set random seed). The results on validation and then test sets are pretty satisfactory for my case however they are always different each time, precision spans between 0.7 and 0.9. This is due to fact that each time different data points fall to set with which model is trained.
My question is: how do I know that final training will also generate good model and how to estimate its precision when I do not have anymore unseen data?
","['machine-learning', 'training', 'models', 'testing']",
How is exponential moving average computed in deep Q networks?,"
In normal Q-learning, the update rule is an implementation of the exponential moving average, which then converges to the optimal true Q values. However, looking at DQN, how exactly is the exponential moving average implemented in deep networks?
","['machine-learning', 'reinforcement-learning', 'deep-learning', 'dqn', 'deep-rl']",
Combine DQN with the Average Reward setting,"
I have to deal with a non-episodic task, where there is addittionally a continuous state space and more specifically in each time step there is always a new state that has never been seen before. I want to use DQN algorithm. As it is referred in Sutton's book (Chapter 10), the average reward setting, that is the undiscounted setting with differential function, should be preferred for non-episodic tasks with function approximation.
(a) Are there any reference papers that use DQN with the average reward setting?
(b) Why should the classic discounted setting (with no average reward) fail in such tasks, comparing to the average reward setting, taking into account that the highest reward that my agent can gain in a time step is 1.0 and thus the max $G_t = \frac{1}{1-γ}$ and not infinite ?
","['reinforcement-learning', 'deep-learning', 'dqn', 'reference-request', 'discount-factor']",
Is the error function known or unknown?,"
What is the error function? Is it the same as the cost function?
Is the error function known or unknown?
When I get the outcome of a neural net I compare it with the target value. The difference between both is called the error. When I get mutiple error values e.g. when I pass a batch through the NN I will get as many error value as the size of my batch. Is the error function the plot of the points? If yes, to me the error function would be unknown. I would only know some point on the graph of the error function.
","['neural-networks', 'terminology', 'objective-functions']",
How is weighted average computed in Deep Q networks,"
I was going through the Sutton book and they said the update formula for Q learning comes from the weighted average of the returns
I.e
New estimate= old estimate +alpha*[returns- old estimate]
So by the law of large numbers this will converge to the optimal true q value
Now when we go to Deep Q networks,how exactly is the weighted average computed, all they simply did was try to reduce the error between the target and the estimate, and keep in mind this isn’t the true target, it’s just an unbiased estimate,since it’s an unbiased estimate how is the weighted average computed , which is the expectation?
Can someone help me out here??
Thanks in advance
","['reinforcement-learning', 'deep-learning', 'q-learning', 'dqn', 'deep-neural-networks']",
Why are large models necessary when we have a limited number of training examples?,"
In Goodfellow et al. book Deep Learning chapter 12.1.4 they write

These large models learn some function $f(x)$, but do so using many more parameters than are necessary for the task. Their size is necessary only due to the limited number of training examples.

I am not able to understand this. Large models are expressive, but if you train them on few examples they should also overfit.
So, what do the authors mean by saying large models are necessary precisely because of the limited number of training examples?
This seems to go against the spirit of using more bias when training data is limited.
","['neural-networks', 'overfitting', 'bias-variance-tradeoff']",
Choosing an AI method to recreate a given binary 2D image,"
If the title wan not very clear, I want a method to take an input image like this,
[[0, 0, 0, 0],
 [1, 1, 1, 0],
 [1, 1, 1, 0],
 [0, 1, 1, 0]]

and output the 2D coordinates of the 1s of the image (So that I can recreate the image)
The application is a robot creating the image using some kind of building blocks, placing one block after the other
I want the output to be sequential because I need to reconstruct the input image pixel by pixel and there are some conditions on the image construction order (e.g. You cannot place a 1 somewhere when it is surrounded by 1s)
The image can change and the number of 1s in the image too.

What is an appropriate AI method to apply in this case?
How should I feed the image to the network? (Will flattening it to 2D affect my need of an output order?)
Should I get the output coordinates one by one or as an ordered 2xN matrix?
If one by one, should I feed the same image for each output or the image without the 1s already filled?


I have tried to apply ""NeuroEvolution of Augmenting Topologies"" for this using neat-python but was unsuccessful. I am currently looking at RNNs but I am not sure if it is the best choice either.
","['neural-networks', 'ai-design', 'recurrent-neural-networks', 'evolutionary-algorithms', 'neuroevolution']",
"How does the Ornstein-Uhlenbeck process work, and how it is used in DDPG?","
In section 3 of the paper Continuous control with deep reinforcement learning, the authors write

As detailed in the supplementary materials we used an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) to generate temporally correlated exploration for exploration efficiency in physical control problems with inertia (similar use of autocorrelated noise was introduced in (Wawrzynski, 2015)).

In section 7, they write

For the exploration noise process we used temporally correlated noise in order to explore well in physical environments that have momentum. We used an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) with θ = 0.15 and σ = 0.2. The Ornstein-Uhlenbeck process models the velocity of a Brownian particle with friction, which results in temporally correlated values centered around 0.

In a few words, what is the Ornstein-Uhlenbeck process? How does it work? How exactly is it used in DDPG?
I want to implement the Deep Deterministic Policy Gradient algorithm, and, in the initial actions, noise has to be added. However, I cannot understand how this Ornstein-Uhlenbeck process works. I have searched the internet, but I have not understood the information that I found.
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'papers', 'ddpg']",
What Classification Algorithm Do I need to Use to Solve this Problem?,"
I am trying to solve the following problem it is to classify the the red points
and green points in image 1 into two cases.  The cluster of green or
red points can be anywhere and there can be any number of green or red
clusters; different coloured clusters do not mix or bleed into each other;
at least one green and one red cluster always exists.
An example of points to classify is given in image 1.
So I guess there are two ways to do this

Classify then with boundaries as shown in image 2, with some algorithm, then use a some post
processing step to link the separate classes that have the same colour.

Use some algorithm to directly find the classification boundaries as shown in image 3.


So my question is what is the  algorithm or algorithm can I use for 1) and 2)?
It seems it can be solved using the MLE algorithm in some way.



",['machine-learning'],
How to train the images of various sizes?,"
I am practicing with an image dataset which is having different dimensions. 
If I simply crop and pad them to 1024X1024(the original images having smallest width is around 300 and largest is around 2400 and widths and heights of the images are not the same) I am not getting good val_accuracy. It's just giving 49% accuracy.
How to do image processing to these images because the brightness of the images is also changing. My task is to classify them into 5 classes.
","['deep-learning', 'computer-vision']",
Are neural networks really used apart from specific hi-tech organisations?,"
This is a generic question. Still posting it to get insights from experts in the field.
I am interested in knowing if Neural Networks are used in general apart from specific hi-tech organizations.
If so, which type of NN is used in which industry and for what purpose?
","['neural-networks', 'applications']",
When to convert data to word embeddings in NLP,"
When training a network using word embeddings, it is standard to add an embedding layer to first convert the input vector to the embeddings.
However, assuming the embeddings are pre-trained and frozen, there is another option. We could simply preprocess the training data prior to giving it to the model so that it is already converted to the embeddings. This will speed up training, since this conversion need only be performed once, as opposed to on the fly for each epoch.
Thus, the second option seems better. But the first choice seems more common. Assuming the embeddings are pre-trained and frozen, is there a reason I might choose the first option over the second?
","['natural-language-processing', 'data-preprocessing', 'word-embedding']","If you have to move a lot of data around during training (like retrieving batches from disk/network/what have you), it's much faster to do so as a rank-3 tensor of [batches, documents, indices] than as a rank-4 tensor of [batches, documents, indices, vectors]. In this case, while the embedding is O(1) wherever you put it, it's more efficient to do so as part of the graph."
Why is the mean used to compute the expectation in the GAN loss?,"
From Goodfellow et al. (2014), we have the adversarial loss:
$$ \min_G \, \max_D V (D, G) = \mathbb{E}_{x∼p_{data}(x)} \, [\log \, D(x)]  + \, \mathbb{E}_{z∼p_z(z)} \, [\log \, (1 − D(G(z)))] \, \text{.} \quad$$
In practice, the expectation is computed as a mean over the minibatch. For example, the discriminator loss is:
$$
\nabla_{\theta_{d}} \frac{1}{m} \sum_{i=1}^{m}\left[\log D\left(\boldsymbol{x}^{(i)}\right)+\log \left(1-D\left(G\left(\boldsymbol{z}^{(i)}\right)\right)\right)\right]
$$
My question is: why is the mean used to compute the expectation? Does this imply that $p_{data}$ is uniformly distributed, since every sample must be drawn from $p_{data}$ with equal probability?
The expectation, expressed as an integral, is:
$$
\begin{aligned}
V(G, D) &=\int_{\boldsymbol{x}} p_{\text {data }}(\boldsymbol{x}) \log (D(\boldsymbol{x})) d x+\int_{\boldsymbol{z}} p_{\boldsymbol{z}}(\boldsymbol{z}) \log (1-D(g(\boldsymbol{z}))) d z \\
&=\int_{\boldsymbol{x}} p_{\text {data }}(\boldsymbol{x}) \log (D(\boldsymbol{x}))+p_{g}(\boldsymbol{x}) \log (1-D(\boldsymbol{x})) d x
\end{aligned}
$$
So, how do we go from an integral involving a continuous distribution to summing over discrete probabilities, and further, that all those probabilities are the same?
The best I could find from other StackExchange posts is that the mean is just an approximation, but I'd really like a more rigorous explanation.
This question isn't exclusive to GANs, but is applicable to any loss function that is expressed mathematically as an expectation over some sampled distribution, which is not implemented directly via the integral form.
(All equations are from the Goodfellow paper.)
","['deep-learning', 'objective-functions', 'generative-adversarial-networks', 'expectation']","It seems your question is concerned with how an empirical mean works.It is indeed true that, if all $x^{(i)}$ are independent identically distributed realisations of a random variable $X$, then $\lim_{n \rightarrow \infty} \frac{1}{n}\sum_{i=1}^n f(x^{(i)}) = \mathbb{E}[f(X)]$. This is a standard result in statistics known as the law of large numbers."
Which method of tree searching should be used for this board game?,"
Suppose the following properties of a board game:

High branching factor in the beginning of the game (~500) which slowly tends towards 0 at the end of the game

Evaluation of the any given board state isn't hard to create and can be quite accurate


And that we want to create an AI to play such board game.

What method of tree searching should be applied for the AI?

Considering the absurd branching factor (at least for most of the game), the Monte Carlo method of search is appealing. The problem is that from what I've seen usually monte carlo search methods are used on games with both high branching factor and no easy evaluation function. However that is not the case for this board game as previously stated.
I'm simply curious how this property of evaluation should influence my decision. For example: Should I replace simulations and playouts with an evaluation function? At that point, would alpha-beta pruning minimax work better? Is there some hybrid which would be optimal?
","['algorithm', 'optimization', 'search']",
How is dropout applied to the embedding layer's output?,"
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(1000, 16, input_length=20), 
    tf.keras.layers.Dropout(0.2),                           # <- How does the dropout work?
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

I can understand when dropout is applied between Dense layers, which randomly drops and prevents the former layer neurons from updating parameters. I don't understand how dropout works after an embedding layer.
Let's say the output shape of the Embedding layer is (batch_size,20,16) or simply (20,16) if we ignore the batch size. How is dropout applied to the embedding layer's output?
Randomly dropout rows or columns?
","['natural-language-processing', 'tensorflow', 'recurrent-neural-networks', 'long-short-term-memory', 'word-embedding']","It doesn't drops rows or columns, it acts directly on scalars. The Dropout Layer keras documentation explains it and illustrates it with an example :The Dropout layer randomly sets input units to 0 with a frequency of rateAfter an Dense Layer, the Dropout inputs are directly the outputs of the Dense layer neurons, as you said. After your embedding layer, in your case, you should have rate * (16 * input_length) = 0.2 * 20 * 16 = 64 inputs set to 0 out of the 320 scalars inputs. These 64 dropped inputs are randomly selected in the 20x16 grid. Note that the Dropout rescales the non dropped inputs by multiplicating them by a factor $\frac{1}{1-rate}$."
Why is the convolution layer called Conv2D?,"
When I build a convolution layer for image processing, the filter parameters should have 3 dimensions, (filter_length, filter_width, color_depth) is that correct?
Why is this convolution layer called Conv2D? Where does the 2 come from?
","['convolutional-neural-networks', 'terminology', 'tensorflow', 'keras', '2d-convolution']",
What is the best activation function for the embedding layer in a deep auto-encoder?,"
I am designing a deep autoencoder for graph embedding (exactly node embedding) following this paper SDNE. In the original paper, they used the sigmoid activation for all hidden layers in the autoencoder model, even for the embedding layer.
However, I think the embedding layer should use the tanh activation and the reconstruction layer should be used ReLU activation. Because, embedding is in the range $[-1, 1]$ and reconstruction layer is in the range $[0, x]$, which generates better results due to a larger range for representation and directed graph. Instead of in the range $[0,1]$ from sigmoid will lead to a lack of embedding information.
So, what is the best activation function for deep autoencoders to capture good information about the structure of graph?
","['machine-learning', 'activation-functions', 'autoencoders', 'hyperparameter-optimization', 'hyper-parameters']",
"How to calculate the attention loss in the paper ""Tell Me Where to Look: Guided Attention Inference Network""?","
I have been reading the research paper Tell Me Where to Look: Guided Attention Inference Network.
In this paper, they calculate the attention loss, but I didn't understand how to calculate it. Do we have to calculate it like outcome[c]? If it is so, then why do arrows connect with each other from the middle FC and the last FC?
Here is the image:

","['machine-learning', 'computer-vision', 'objective-functions', 'papers']",
Human intuition behind SVD in case of recommendation system,"
This does not answer my question. I struggled very hard to understand the SVD from a linear-algebra point of view. But in some cases I failed to connect the dots. So, I started to see all the application of SVD. Like movie recommendation system, Google page ranking system, etc.
Now in the case of movie recommendation system, what I had as a mental picture is...
The SVD is a technique that falls under collaborative filtering. And what the SVD does is factor a big data matrix into two smaller matrix. And as an input to the SVD we give an incomplete data matrix. And SVD gives us a probable complete data matrix. Here, in the case of a movie recommendation system we try to predict ratings of users. Incomplete input data matrix means some users didn't give ratings to certain movies. So the SVD will help to predict users' ratings. I still don't know how the SVD breaks down a large matrix to smaller pieces. I don't how the SVD determines the dimensions of the smaller matrices.
It would be helpful if anyone could judge my understanding. And I will very much appreciate any resources which can help me to understand the SVD from scratch to its application to Netflix recommendation systems. Also for the Google Page ranking system or for other applications.
I am looking forward to seeing an explanation more from human-intuition level and from a linear-algebra point of view. Because I am interested in using this algorithm in my research, I need to understand as soon as possible: how does the SVD work deep down from the core?
","['machine-learning', 'recommender-system', 'linear-algebra', 'singular-value-decomposition']",
What's the nearest neighbor algorithm used for upsampling?,"

Additionally, by default, the UpSampling2D layer will use a nearest neighbor algorithm to fill in the new rows and columns. This has the effect of simply doubling rows and columns, as described and is specified by the ‘interpolation‘ argument set to ‘nearest‘. Alternately, a bilinear interpolation method can be used which draws upon multiple surrounding points. This can be specified via setting the ‘interpolation‘ argument to ‘bilinear‘.

How exactly does the nearest neighbor algorithm mentioned above work? Also, what does interpolation mean in this context (nearest and bilinear)?
Source: Section on Upsampling2D layer
","['neural-networks', 'convolutional-neural-networks', 'interpolation']",
How can we teach a neural net to make arbitrary data associations?,"
Let's say I have pairs of keys and values of the form $(x_1, y_1), \dots, (x_N, y_N)$. Then I give a neural net a key and a value, $(x_i, y_i)$. For example, $x_i$ could be $4$ and $y_i$ could be $3$, but this does not have to be the case.
Is there a way to teach the neural net to output the $y_i$ variable every time it receives the corresponding $x_i$?
By the way, how do our brains perform this function?
","['neural-networks', 'machine-learning', 'deep-learning', 'supervised-learning']","In a nutshell : Memorizing is not LearningSo, first let's just remind the classical use of a neural net, in Supervised Learning :Can this solve your question ? Well, I don't think so. With this scheme, your neural net will learn an appropriate mapping from the set $X$ to the set $Y$, but this mapping is appropriate according to your loss function , not to your $(x_{train}, y_{train})$ pairs.Imagine that a small part of the data is wrongly labelled. A properly trained net learns to extract relevant features and thus will predict correctly the label, not like you did. So the net doesn't memorize your pairs, it infers a general law from the data, and this law may not respect each $(x_{train}, y_{train})$.
So classical Supervised Deep Learning should not memorize $(x_{train}, y_{train})$  pairs.However, you could memorize using a net with too many parameters :  it's Overfitting !But as long as you want only to memorize, and not to learn, a overfitted net may be the a solution. An other solution for memorization may be Expert Systems, I don't know them enough to explain them, but you may check that if you want.What about the brain ?The matter in answering this question is that we don't really know how does the brain work. I highly recommend this article discussing neural networks and the brain.Some thoughts to start :"
Were AI strategies identified at go or starcraft games and how?,"
When an AI is trained to play an opposing game, such as chess or go, it can become very strong.
I have read in an article (non-scientific) the claim that AI strategies were identified by scientists while an AI was bound to play go games, as well as starcraft games. However it did not tell what these strategies actually were, how they were identified, nor did it explain the configuration in which AI played (AI vs AI? AI vs human?)
Can someone explain it to me? I am familiar with go, not with starcraft, so an explanation about go is appreciated.
I also note that the chess game is not mentioned. Is there any specific feature for chess that makes them inappropriate for strategies? Or is it the behavior of an AI in the chess game that does not allow to identify strategy?
I understand there are plenty of definitions for strategy, and the article did not give one. So let's focus on following significance: Strategy is a group of principles that tell which fields are important to fight for and which are not. A strategy gives long term rewards, which is opposite to tactics with short term rewards obtained thanks to calculation on a specific issue. With this definition, go game stand for a strategic game with a few, well known tactical situations such as line versus line.
","['game-ai', 'chess', 'go']",
How to modify the Actor-Critic policy gradient algorithm to perform Safe exploration in Reinforcement Learning,"
I am trying to implement safe exploration technique in [Ref.1]. I am using Soft Actor-Critic algorithm to teach an agent to introduce a bias between 0 and 1 to a specific state of interest in my environment.
I would like to ask for your help in order to modify the critic update equation -which is originally  based on the return or the RL cost function- from this:

which is based on the return functions:
  =

to be based on the following cost function to make the RL objective risk-sensitive and avoid large-valued actions (bias values) at the start of the agent's learning,

How can I include the second part of the cost function - in which the variance of the reward is evaluated- in the update equation?
[Ref.1] Heger, M., Consideration of risk in reinforcement learning. 11th International Machine Learning Conference (1994)
","['actor-critic-methods', 'exploration-exploitation-tradeoff']",
Does a better discriminator in GANs mean better sample generation by the generator?,"
Since the discriminator defines how the generator is updated, then building a discriminator with a higher number of parameters/more layers should lead to a better quality of generated samples. So, assuming that it won't lead to overwhelming the generator (discriminator loss toward 0) or mode collapse, when engineering a GAN, I should build a discriminator as good as possible?
","['deep-learning', 'generative-adversarial-networks']",
How should I compute the target for updating in a DQN at the terminal state if I have pseudo-episodes?,"
I'm training a DQN in a real environment where I do not have a natural terminal state, so I've built the episode in an artificial way (i.e. it starts in a random condition and after T steps it ends). My question is about the terminal state: should I consider it when I have to compute $y$ (so using only the reward) or not?
","['reinforcement-learning', 'dqn', 'environment']","If the episode does not terminate naturally, then if you are breaking it up into pseudo-episodes for training purposes, the one thing you should not do is use the TD target $G_{T-1} = R_T$ used for an end of episode, which assumes a return of 0 from any terminal state $S_{T}$. Of course that is because it is not the end of the episode.You have two ""natural"" options to tweak DQN to match to theory at the end of a pseudo-episode:Store the state, action, reward, next_state tuple as normal and use the standard one step TD target $G_{t:t+1} = R_{t+1} + \gamma \text{max}_{a'} Q(S_{t+1}, a')$Completely ignore the last step and don't store it in memory. There is no benefit to this as opposed to the above option, but it might be simpler to implement if you are using a pre-built RL library.Both these involve ignoring any done flag returned by the environment for the purposes of calculating TD targets. You still can use that flag to trigger the end of a loop and a reset to new starting state.You should also take this approach if you terminate an episodic problem early after hitting a time step limit, in order to reset for training purposes.As an aside (and mentioned in comment by Swakshar Deb), you can also look into the average reward setting for non-episodic environments. This solves the problem of needing to pick a value for $\gamma$. If you have no reason to pick a specific $\gamma$ in a continuing problem, then it is common to pick a value close to 1 such as 0.99 or 0.999 in DQN - this is basically an approximation to average reward."
How to take the optimal batch_size for training a model?,"
I have an image dataset, which is composed of 113695 images for training and 28424 images for validation. Now, when I use ImageDataGenerator and flow_from_dataframe, it as the parameter batch_size.
How can I take the correct number for batch_size because both numbers cannot be divided by the same number? Should I need to drop four images in the validation data to make them batch_size of 5? Or is there another way?
","['deep-learning', 'computer-vision', 'tensorflow', 'data-preprocessing']",
"What does ""convolve k filters"" mean in the AlphaGo paper?","
On page 27 of the DeepMind AlphaGo paper appears the following sentence:

The first hidden layer zero pads the input into a $23 \times 23$ image, then convolves $k$ filters of kernel size $5 \times 5$ with stride $1$ with the input image and applies a rectifier nonlinearity.

What does ""convolves $k$ filters"" mean here?
Does it mean the following:

The first hidden layer is a convolutional layer with $k$ groups of $(19 \times 19)$ neurons, where there is a kernel of $(5 \times 5 \times numChannels + 1)$ parameters (input weights plus a bias term) used by all the neurons of each group. $numChannels$ is 48 (the number of feature planes in the input image stack).
All $(19 \times 19 \times k)$ neurons' outputs are available to the second hidden layer (which happens to be another convolutional layer, but could in principle be fully connected).

?
","['convolutional-neural-networks', 'convolution', 'alphago', 'filters', 'convolutional-layers']",
How are neural networks built in practice?,"
I am curious to know how neural networks are built in practice.
Are they hand coded using weight matrices, activation functions etc OR are there ways to build the NN by mentioning the number of layers, number of neurons in each layer, activation to be used, etc as parameters?
Similar question on training, once built is there a ‘fit’ method or does the training need to be hand coded?
Any reference for understanding these basics will be of great help.
",['neural-networks'],
What's the difference between domain randomization and domain adaptation?,"
In my understanding, domain randomization is one method of diversifying the dataset to achieve a better shot at domain adaptation. Am I wrong?
","['machine-learning', 'comparison', 'transfer-learning', 'domain-adaptation']",
What are the best techniques to perform text simplification?,"
I'm evaluating the state of the art techniques to translate legal text to simple text, what are the best approaches for a non-English language (Portuguese)?
","['natural-language-processing', 'python', 'text-summarization']",
How to determine the number of hidden layers and units of a deep auto-encoder?,"
I am using a deep autoencoder for my problem. However, the way I choose the number of hidden layers and hidden units in a hidden layer is still based on my feeling.
The size of the model that indicates the number of hidden layers and units should not be too much or too few for the model can capture useful features from the dataset.
So, how do I choose the right size of the deep autoencoder model is enough to good?
","['deep-learning', 'autoencoders', 'hyperparameter-optimization', 'hidden-layers', 'hyper-parameters']","You are right!1- the number of hidden layers shouldn't be too high! Because of the gradient descent when the number of layers is too large, the gradient effect on the first layers become too small! This is why the Resnet model was introduced.2- the number of hidden layers shouldn't be too small to extracts good features. It's proved that in CNN networks the first layers extract very simple elements like lines and curves but last layers extracts more complex features.3- number of hidden units is a hyper-parameters and usually you should find it by testing or based on your background knowledge.But what can you do at all?
As you can tests different parameters and compare their results there is some other options! One option is grid search you can check this tutorial https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e"
Do we assume the policy to be deterministic when proving the optimality?,"
In reinforcement learning, when we talk about the principle of optimality, do we assume the policy to be deterministic?
","['reinforcement-learning', 'proofs', 'policies', 'deterministic-policy']",
What do the state features of KukaGymEnv represent? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I trying to use DDPG augmented with Hindsight Experience Replay (HER) on pybullet's KukaGymEnv.
To formulate the feature vector for the goal state, I need to know what the features of the state of the environment represent. To be precise, a typical state vector of KukaGymEnv is an object of the numpy.ndarray class with a shape of (9,).
What do each of these 8 elements represent, and how can I formulate the goal state vector for this environment? I tried going through the source code of the KukaGymEnv, but was unable to understand anything useful.
","['reinforcement-learning', 'ddpg', 'hindsight-experience-replay', 'state-spaces', 'pybullet']","Here's an incomplete answer, but it may help.Your state is read by the function getExtendedObservation(). This function makes two things : it calls the function getObservation() from this source code, gets a state, and extend this state with three components :relative x,y position and euler angle of block in gripper spaceBut what are the 5 first components returned by getObservation()? From what I read, there are positions, then euler angles describing the orientation. But that would make 6 + 3 = 9 features, so there is either only 2 positions, or only 2 euler angles. You may know kuka better than me and know the answer of this one :).So, to sum up :(Either Z or Beta is absent)"
Why does loss and accuracy for a multi label classification ann does not change overtime?,"
I have run into a strange behavior of my multi label classification ANN
model = Sequential()
model.add(Dense(6, input_shape=(input_size,), activation='elu'))
#model.add(BatchNormalization(axis=-1))
model.add(Dropout(0.2))
#model.add(BatchNormalization(axis=-1))
model.add(Dense(6, activation='elu'))
model.add(Dropout(0.2))
#model.add(BatchNormalization(axis=-1))
model.add(Dense(6, activation='elu'))
model.add(Dropout(0.2))

# model.add(keras.layers.BatchNormalization(axis=-1))
model.add(Dense(6, activation='sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='nadam',
              metrics=['accuracy'])
history = model.fit(X_train, Y_train,batch_size=64 ,epochs=300,
                    validation_data = (X_test, Y_test), verbose=2)

The result is quite strange, I have a feeling that my model could not improve any more. Why does the loss and the accuracy does not change overtime ?
P/S For clarification, I have 6 output and the value of each output is 0 or 1 that is
output 1: can be 0 or 1
output 2: can be 0 or 1
output 3: can be 0 or 1
output 4: can be 0 or 1
output 5: can be 0 or 1
output 6: can be 0 or 1

","['neural-networks', 'accuracy', 'loss']",
Need some reviews in PEAS descriptions,"
Here is the Question:
Describe the PEAS descriptions for the following agents:
a) A grocery store scanner that digitally scans a fruit or vegetable and
identifies it.
b) A GPS system for an automobile. Assume that the destination has been
preprogrammed and that there is no ongoing interaction with the driver.
However, the agent might need to update the route if the driver misses a turn.
c) A credit card fraud detection agent that monitors an individual’s transactions
and reports suspicious activity.
d) A voice activated mobile-phone assistant
For each of the agents described above, categorize it with respect to the six dimensions
of task environments as described on pages 41-45 (Section 2.3.2 of AIMA). Be sure
that your choices accurately reflect the way you have specified your environment,
especially the sensors and actuators. Give a short justification for each property
Here is what i thinks that the answers of above questions might be this. Can you guyz correct me if i answered wrong at any point.

","['intelligent-agent', 'environment']",
What do we mean by 'principal angle between subspaces'?,"
I came across the term 'principal angle between subspaces' as a tool for comparing objects in images. All material that I found on the internet seems to deal with this idea in a highly mathematical way and I couldn't understand the real physical meaning behind the term.
I have some knowledge of linear algebra. Any help to understand the physical significance of this term and its application in object recognition would be appreciated.
","['computer-vision', 'terminology', 'object-detection', 'linear-algebra']",
How should I choose the target's update frequency in DQN?,"
I have been dealing with a problem that I'm trying to solve with DQN. A general question that I have is regarding the target's update frequency. How should it change? Depending on what factor do we increase or decrease this hyperparameter?
","['reinforcement-learning', 'deep-learning', 'dqn', 'hyperparameter-optimization', 'hyper-parameters']",
"How to deal with Unix timestamps features of sequences, which will be classified with RNNs?","
I want to use RNN for classifying whole sequences of events, generated by website visitors. Each event has some categorical properties and a Unix timestamp:
sequence1 = [{'timestamp': 1597501183, 'some_field': 'A'}, {'timestamp': 1597681183, 'some_field': 'B'}]
sequence2 = [{'timestamp': 1596298782, 'some_field': 'B'}]
sequence3 = [{'timestamp': 1596644362, 'some_field': 'A'}, {'timestamp': 1596647951, 'some_field': 'C'}]

Unfortunately, they can't be treated as classic time series, because they're of variable length and irregular, so timestamps contain essential information and cannot be ignored. While categorical features can be one-hot encoded or made into embeddings, I'm not sure what to do with the timestamps. It doesn't look like a good idea to use them raw. I've come up with two options so far:

Subtract the minimum timestamp from every timestamp in the sequence, so that all sequences start at 0. But in this case the numbers can still be high, because the sequences run over a month.
Use offsets from previous event instead of absolute timestamps.

I'm wondering if there are common ways to deal with this? I haven't found much on this subject.
","['recurrent-neural-networks', 'data-preprocessing', 'time-series', 'feature-extraction', 'feature-engineering']",I would say offsets from previous N-event is necessary in this case. You can also encode their max and avg as other features in order to compress the irregular sampling times into a vector.
Using DDPG for control in multi-dimensional continuous action space?,"
I am relatively new to reinforcement learning, and I am trying to implement a reinforcement learning algorithm that can do continuous control in a custom environment. The state of the environment is composed of 5 sequential observations of 11 continuous values that represent cells (making the observation space 55 continuous values total), and the action is 11 continuous values representing multipliers of those cell totals.
After preliminary research, I decided to use Deep Deterministic Policy Gradient (DDPG) as my control algorithm because of its ability to deal with both discrete states and actions. However, most of the examples, including the one that I am basing my implementation off of, have only a single continuously valued action as the output. I have tried to naively change the agent network from outputting a single value to output a vector of values, but the agent does not improve as all, and the  set of outputs seems to split into two groups near either the maximum value or the minimum value (I believe the tanh activation on the output has something to do with it) with the values in those groups changing in unison.
I have two questions about my problems.

First, is it even possible to use DDPG for multi-dimensional continuous action spaces? My research leads me to believe it is, but I have not found any code examples to learn from and many of the papers I have read are near the limit of my understanding in this area.

Second, why might my actor network be outputting values clustered near its max/min values, and why would the values in either cluster all be the same?


Again, I am fairly new to reinforcement learning, so any advice or recommendations would be greatly appreciated, thanks.
","['reinforcement-learning', 'keras', 'ddpg', 'control-problem']","First, is it even possible to use DDPG for multi-dimensional
continuous action spaces?Yes, DDPG was primarily developed to deal with continuous action space you can find out more here, here and here.I have not found any code examples to learn from and many of the
papers I have read are near the limit of my understanding in this
area.You can check kerasRL ddpg implementation it is quite easy to understand and can solve your problem of a model with multiple outputs..why might my actor network be outputting values clustered near its
max/min values, and why would the values in either cluster all be the
same?I think it is just a convergence problem try out small learning rates and/or small target_model_update param.If this doesn't work for you check TRPO and/or PPO.."
Learning only using off-policy samples,"
When training policies, is there a reason we need on-policy samples? For expensive simulations, it makes sense to try and reuse samples. Say we're interested in hyperparameter tuning. Can we collect a bunch of episodes using randomly sampled actions (or maybe by following an old policy) one time, and train multiple policies using this set of samples to find the most effective hyperparameters? Every time we train a new policy, does it make sense to replay all the episodes generated by the previous policy? I'm mostly interested in actor-critic methods.
","['reinforcement-learning', 'actor-critic-methods', 'off-policy-methods']",
Can the normal equation be used to optimise the RNN's weights?,"
I have made an RNN from scratch in Tensorflow.js. In order to update my weights (without needing to calculate the derivatives), I thought of using the normal equation to find the optimal values for my RNN's weights. Would you recommend this approach and if not why?
","['neural-networks', 'recurrent-neural-networks', 'backpropagation']","Unfortunately, this is not possible. The normal equation can only directly optimise a single layer that connects input and output. There is no equivalent for multiple layers such as those in any neural network architecture."
Can you convert a MDP problem to a Contextual Multi-Arm Bandits problem?,"
I'm trying to get a better understanding of Multi-Arm Bandits, Contextual Multi-Arm Bandits and Markov Decision Process.
Basically, Multi-Arm Bandits is a special case of Contextual Multi-Arm Bandits where there is no state(features/context). And Contextual Multi-Arm Bandits is a special case of Markov Decision Process, where there is only one state (features, but no transitions).
However, since MDP has Markov property, I wonder if every MDP problem can also be converted into a Contextual Multi-Arm Bandits problem, if we simply treat each state as a different input context (features)?
","['reinforcement-learning', 'comparison', 'markov-decision-process', 'multi-armed-bandits', 'contextual-bandits']",
What are the differences between Q-Learning and A*?,"
Q-learning seems to be related to A*. I am wondering if there are (and what are) the differences between them.
","['reinforcement-learning', 'comparison', 'q-learning', 'a-star']","Q-learning and A* can both be viewed as search algorithms, but, apart from that, they are not very similar.Q-learning is a reinforcement learning algorithm, i.e. an algorithm that attempts to find a policy or, more precisely, value function (from which the policy can be derived) by taking stochastic moves (or actions) with some policy (which is different from the policy you want to learn), such as the $\epsilon$-greedy policy, given the current estimate of the value function. Q-learning is a numerical (and stochastic optimization) algorithm that can be shown to converge to the optimal solution in the tabular case (but it does not necessarily converge when you use a function approximator, such as a neural network, to represent the value function). Q-learning can be viewed as a search algorithm, where the solutions are value functions (or policies) and the search space is some space of value functions (or policies).On the other hand, A* is a general search algorithm that can be applied to any search problem where the search space can be represented as a graph, where nodes are positions (or locations) and the edges are the weights (or costs) between these positions. A* is an informed search algorithm, given that you can use an (informed) heuristic to guide the search, i.e. you can use domain knowledge to guide the search. A* is a best-first search (BFS) algorithm, which is a family of search algorithms that explore the search space by following the next best location according to some objective function, which varies depending on the specific BFS algorithm. For example, in the case of A*, the objective function is $f(n) = h(n) + g(n)$, where $n$ is a node, $h$ the heuristic function and $g$ the function that calculates the cost of the path from the starting node to $n$. A* is also known to be optimal (provided that the heuristic function is admissible)"
Best/quickest approach for tuning the hyperparameters of a restricted boltzmann machine,"
I have an RBM model which takes extremely long to train and evaluate because of the large number of free parameters and the large amount of input data. What would be the most efficient way of tuning its hyperparameters (batch size, number of hidden units, learning rate, momentum and weight decay)?
","['hyperparameter-optimization', 'restricted-boltzmann-machine']",
Black Box Explanations: Using LIME and SHAP in python,"
Recently, I came across the paper Robust and Stable Black Box Explanations, which discusses a nice framework for global model-agnostic explanations.
I was thinking to recreate the experiments performed in the paper, but, unfortunately, the authors haven't provided the code. The summary of the experiments are:

use LIME, SHAP and MUSE as baseline models, and compute fidelity score on test data. (All the 3 datasets are used for classification problems)

since LIME and SHAP give local explanations, for a particular data point, the idea is to use K points from the training dataset, and create K explanations using LIME. LIME is supposed to return a local linear explanation. Now, for a new test data point, using the nearest point from K points used earlier and use the corresponding explanation to classify this new point.

measure the performance, using fidelity score (% of points for which $E(x) = B(x)$, where $E(x)$ is the explanation of the point and $B(x)$ is the classification of the point using the black box.


Now, the issue is, I am using LIME and SHAP packages in Python to achieve the results on baseline models.
However, I am not sure how I'll get a linear explanation for a point (one from the set K), and use it to classify a new test point in the neighborhood.
Every tutorial on YouTube and Medium discusses visualizing the explanation for a given point, but none talks about how to get the linear model itself and use it for newer points.
","['machine-learning', 'explainable-ai', 'black-box']",
"Given two neural networks that compute two functions $f(x)$ and $g(x)$, how can I create a neural network that computes $f(x)g(x)$?","
I have two functions $f(x)$ and $g(x)$, and each of them can be computed with a neural network $\phi_f$ and $\phi_g$.
My question is, how can I write a neural net for $f(x)g(x)$?
So, for example, if $g(x)$ is constant and equal to $c$  and $\phi_f = ((A_1,b_1),...(A_L,b_L))$, then $\phi_{fg} = ((A_1,b_1),...,(cA_L,cb_L))$.
Actually, I need to show it for $f(x)=x$ and $g(x)=x^2$ if this make something easier.
","['neural-networks', 'ai-design']",
Why does REINFORCE work at all?,"
Here's a screenshot of the popular policy-gradient algorithm from Sutton and Barto's book -

I understand the mathematical derivation of the update rule - but I'm not able to build intuition as to why this algorithm should work in the first place. What really bothers me is that we start off with an incorrect policy (i.e. we don't know the parameters $\theta$ yet), and we use this policy to generate episodes and do consequent updates.
Why should REINFORCE work at all? After all, the episode it uses for the gradient update is generated using the policy that is parametrized by parameters $\theta$ which are yet to be updated (the episode isn't generated using the optimal policy - there's no way we can do that).
I hope that my concern is clear and I request y'all to provide some intuition as to why this works! I suspect that, somehow, even though we are sampling an episode from the wrong policy, we get closer to the right one after each update (monotonic improvement). Alternatively, we could be going closer to the optimal policy (optimal set of parameters $\theta$) on average.
So, what's really going on here?
","['reinforcement-learning', 'policy-gradients', 'reinforce']","The key to REINFORCE working is the way the parameters are shifted towards $G \nabla \log \pi(a|s, \theta)$.Note that $ \nabla \log \pi(a|s, \theta) = \frac{ \nabla \pi(a|s, \theta)}{\pi(a|s, \theta)}$. This makes the update quite intuitive - the numerator shifts the parameters in the direction that gives the highest increase in probability that the action will be repeated, given the state, proportional to the returns - this is easy to see because it is essentially a gradient ascent step. The denominator controls for actions that would have an advantage over other actions because they would be chosen more frequently, by inversely scaling with respect to the probability of the action being taken; imagine if there had been high rewards but the action at time $t$ has low probability of being selected (e.g. 0.1) then this will multiply the returns by 10 leading to a larger update step in the direction that would increase the probability of this action being selected the most (which is what the numerator controls for, as mentioned).That is for the intuition -- to see why it does work, then think about what we've done. We defined an objective function, $v_\pi(s)$, that we are interested in maximising with respected to our parameters $\theta$. We find the derivative of this objective with respect to our parameters, and then we perform gradient ascent on our parameters to maximise our objective, i.e. to maximise $v_\pi(s)$, thus if we keep performing gradient ascent then our policy parameters will converge (eventually) to values that maximise $v$ and thus our policy would be optimal."
How can I implement 2D CNN filter with channelwise-bound kernel weights?,"
I would like to bind kernel parameters through channels/feature-maps for each filter. In a conv2d operation, each filter consists of HxWxC parameters I would like to have filters that have HxW parameters, but the same (HxWxC) form.
The scenario I have is that I have 4 gray pictures of bulb samples (yielding similar images from each side), which I overlay as channels, but a possible failure that needs to be detected might only appear on one side (a bulb has 4 images and a single classification). The rotation of the object when the picture is taken is arbitrary. Now I solve this by shuffling the channels at training, but it would be more efficient if I could just bind the kernel parameters. Pytorch and Tensorflow solutions are both welcome.
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'filters']",
Time Series Forecasting - Recurrent Neural Networks (tensorflow),"
I am attempting to forecast a time series using tensorflow with the following code:
X = mytimeseries
scaler = MinMaxScaler()
scaled = scaler.fit_transform(X)

length = len(X)-1
generator = TimeseriesGenerator(scaled,scaled,
                            length=length,batch_size=1)

model = Sequential()
model.add(LSTM(units=100,activation='relu',input_shape=(length,n_features)))
model.add(Dense(units=100))
model.add(Dense(units=1))

model.fit(generator,epochs=20)

Then I just run a loop to forecast, but it's giving me nothing more than a straight line after a few points, as observed below.
Obviously there is a trend for the data to go down, and I would expect to see that.
Is this because my architecture is not sophisticated enough / not the right one to pick up on the general decline of the known data? Have I inappropriately chosen any parameters?
I have tried increasing the number of neurons in the dense layer, units in the LSTM cell, etc. At the moment, the thing that looks like to most effect the resultant curve is to change the length parameter in my code above. But all this does is make the predictions more sinusoidal.

Thanks for your help!
","['recurrent-neural-networks', 'time-series']",
What is the scope of real-world deep learning applications in 2020?,"
2015 was a milestone year for AI--""deep learning"" was validated in a very public way with AlphaGo.  However, at the time, the question was raised: ""What else is deep learning good for?""
5 years later, I want to gauge:

How is deep learning applied to real world problems in 2020?  What real world applications is it currently used for?

","['deep-learning', 'applications']",
Is there any real-time computer vision system that can learn to detect new objects of new classes?,"
Suppose you have a ground plane and can use a stereo vision system to detect things that are possibly separate objects.
Suppose also your robot or agent can attempt to pick up and move these objects around in real-time.
Is there any current system in computer vision that allows new objects to be learned in real-time?
","['machine-learning', 'computer-vision', 'online-learning', 'incremental-learning']",
What is the most appropriate ML algorithm for creating recommendations,"
I am trying to find the best algorithm to create a list of recommendations for a user based on the interests of all other users.
Say I have a list of of samples:
$samples = [
    ['hot dog', 'big mac', 'whopper'],
    ['hot dog', 'big mac'],
    ['hot dog', 'whopper'],
    ['big mac', 'dave single'],
    ['whopper', 'mcnuggets', 'mcchicken'],
    ['mcchicken', 'original chicken sandwich'],
    ['mcchicken', 'mcrib']
];

And we will say each array in the sample list is unique user's food preferences.
Let's say now I have a user with this food preference:
['hot dog', 'mcchicken']

I want to be able to recommend to this user other foods that other users have in their preferences.
So in the simplest terms, it should return:
['whopper', 'big mac', 'original chicken sandwich', 'mcrib', 'mcnuggets']

Obviously I will also introduce other variables such as how each user rates each item in their preference list and also the percentage of users that need to have that item in order to use their other food items as recommendations.
But I would like to find the best algorithm to start working on it.
At first I thought Apriori might be my best guess, but I wasn't having luck once I introduced multiple items.
","['machine-learning', 'recommender-system']","You can use Collaborative Filtering, and specifically its memory based approach.
The problem that you have discussed in the question should probably be solved using User-Item collaborative filtering which will calculate similarity between users and then recommend the item. The similarity can be be calculated using cosine similarity or Pearson's similarity formulae. 
The best thing about this approach is that no training is required here but if you wish to have very large data, this approach's performance decreases."
Is value iteration stopped after one update of each state?,"
In section 4.4 Value Iteration, the authors write

One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called value iteration.

After that, they provide the following pseudo-code

It is clear from the code that updates of each state occur until $\Delta$ is sufficiently small. Not one update of each state as the authors write in the text. Where is the mistake?
","['reinforcement-learning', 'value-iteration', 'policy-evaluation', 'pseudocode', 'policy-improvement']","Where the author mentions the policy evaluation being stopped after one state, they are referring to the part of the algorithm that evaluates the policy -- the pseudocode you have listed is the pseudocode for Value Iteration, which consists of iterating between policy evaluation and policy improvement.In normal policy evaluation, you would apply the update $v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1})|S_t = s]$ until convergence. In the policy iteration algorithm, you perform policy evaluation until the value functions converge in each state, then apply policy improvement, and repeat. Value iteration will perform policy evaluation for one update, i.e. not until convergence, and then improve the policy, and repeat this until the value functions converge.The line$$V(s) \leftarrow \max_a \sum_{s', r} p(s',r|s,a)[r + \gamma V(s')]$$perform both the early stopping policy evaluation and policy improvement. Lets examine how:The $\sum_{s', r} p(s',r|s,a)[r + \gamma V(s')]$ is the same as the expectation I wrote earlier, so we can see clearly that is policy evaluation for just one iteration. Then, we take a max over the actions -- this is policy improvement. Policy improvement is defined as (for a deterministic policy)
\begin{align}
\pi'(s) &= \arg\max_a q_\pi(s,a) \\ &= \arg\max_a \sum_{s', r} p(s',r|s,a)[r + \gamma V(s')]\;.
\end{align}
Here, we assign the action that satisfies the $\mbox{argmax}$ to the improved policy in state $s$. This is essentially what we are doing in the   line from your pseudo when we take the max. We are evaluating our value function for a policy that is greedy with respect to said value function.If you keep applying the line from the pseudocode of value iteration it will eventually converge to the optimal value function as it will end up satisfying the Bellman Optimality Equation."
How to compute the target for double Q-learning update step?,"
I've already read the original paper about double DQN but I do not find a clear and practical explanation of how the target $y$ is computed, so here's how I interpreted the method (let's say I have 3 possible actions (1,2,3)):

For each experience $e_{j}=(s_{j},a_{j},r_{j},s_{j+1})$ of the mini-batch (consider an experience where $a_{j}=2$) I compute the output through the main network in the state $s_{j+1}$, so I obtain 3 values.

I look which of the three is the highest so: $a^*=arg\max_{a}Q(s_{j+1},a)$, let's say $a^*=1$

I use the target network to compute the value in $a^*=1$ , so  $Q_{target}(s_{j+1},1)$

I use the value at point 3 to substitute the value in the target vector associeted with the known action $a_{j}=2$, so: $Q_{target}(s_{j+1},2)\leftarrow r_{j}+\gamma Q_{target}(s_{j+1},1)$, while $Q_{target}(s_{j+1},1)$ and $Q_{target}(s_{j+1},3)$, which complete the target vector $y$, remain the same.


Is there anything wrong?
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'double-dqn']",
Finding the optimal policy from a set of fixed policies in reinforcement learning,"
This is an open-ended question.Suppose I have a reinforcement learning task that is being solved using many different fixed policies, one of which is optimal. The goal of the agent is not to figure out what the optimal policy is but rather which policy (from a set of predefined fixed policies) is the optimal one.
Are there any algorithms/methods that handle this?
I was wondering if meta learning is the right area to look into?
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'meta-learning']","The quickest way to do this would be to use policy evaluation methods. Most of the standard optimal control algorithms consist of policy evaluation plus a rule for updating the policy.It may not be possible to rank arbitrary policies by performance when considering all states. So you will want to rank them according to some fixed distribution of state values. The usual distribution of start states would be a natural choice (this is also the objective when learning via policy gradients in e.g. Actor-Critic).One simple method would be to run multiple times for each policy, starting each time according to the distribution of start states, and calculate the return (discounted sum of rewards) from each one. A simple Monte Carlo run from each start state would be fine, and is very simple to code. Take the mean value as your estimate, and measure the variance too so you can establish a confidence for your selection.Then simply select the policy with the best average value in start states. You can use the variance to calculate a standard error for this, so you will have a feel for how robust your selection is.If have a large number of policies to select between, you could do a first pass through with a relatively low number of samples, and try to rule out policies that perform badly enough that even adding say 3 standard errors to the estimated value would not cause them to be preferred. Other than that, the more samples you can take, the more accurate your estimates of mean starting value for each policy will be, and the more likely you will be to select the right policy.I was wondering if meta learning is the right area to look into?In general no, but you might want to consider meta learning if:You have too many policies to select between by testing them all thoroughly.The policies have some meaningful low dimension representation that is driving their behaviour. The policy function itself would normally be too high dimensional.You could then use some form of meta-learning to predict policy performance directly from the representation, and start to skip evaluations from non-promising policies. You may need your fixed policies to number in the thousands or millions before this works though (depending on the number of parameters in the representation and complexity of mapping between parameters and policy function), plus you will still want to thoroughly estimate performance of candidates selected as worth evaluating by the meta-learning.In comments you suggest treating the list of policies as context-free bandits, using a bandit solver to pick the policy that scores the best on average. This might offer some efficiency over evaluating each policy multiple times in sequence. A good solver will try to find best item in the list using a minimal number of samples, and you could use something like UCB or Gibbs distribution to focus more on the most promising policies. I think the main problem with this will be finding the right hyperparameters for the bandit algorithm. I would suggest if you do that to seed the initial estimates with an exhaustive test of each policy multiple times, so you can get a handle on variance and scale of the mean values."
Why are policy iteration and value iteration studied as separate algorithms?,"
In Sutton and Barto's book about reinforcement learning, policy iteration and value iterations are presented as separate/different algorithms.
This is very confusing because policy iteration includes an update/change of value and value iteration includes a change in policy. They are the same thing, as also shown in the Generalized Policy Iteration method.
Why then, in many papers as well, they (i.e. policy and value iterations) are considered two separate update methods to reach an optimal policy?
","['reinforcement-learning', 'comparison', 'value-iteration', 'policy-iteration']",
What does the number of required expert demonstrations in Imitation Learning depend on?,"
I just read the following points about the number of required expert demonstrations in imitation learning, and I'd like some clarifications. For the purpose of context, I'll be using a linear reward function throughout this post (i.e. the reward can be expressed as a weighted sum of the components of a state's feature vector)

The number of expert demonstrations required scales with the number of features in the reward function.

I don't think this is obvious at all - why is it true? Intuitively, I think that as the number of features rises, the complexity of the problem does too, so we may need more data to make a better estimate of the expert's reward function. Is there more to it?

The number of expert demonstration required does not depend on -

Complexity of the expert’s optimal policy $\pi^{*}$
Size of the state space


I don't see how the complexity of the expert's optimal policy plays a role here - which is probably why it doesn't affect the number of expert demonstrations we need; but how do we quantify the complexity of a policy in the first place?
Also, I think that the number of expert demonstrations should depend on the size of the state space. For example, if the train and test distributions don't match, we can't do behavioral cloning without falling into problems, in which case we use the DAGGER algorithm to repeatedly query the expert and make better decisions (take better actions). I feel that a larger state space means that we'll have to query the expert more frequently, i.e. to figure out the expert's optimal action in several states.
I'd love to know everyone's thoughts on this - the dependence of the number of expert demonstrations on the above, and if any, other factors. Thank you!

Source: Slide 20/75
","['reinforcement-learning', 'apprenticeship-learning', 'inverse-rl', 'imitation-learning']","The answer to your question can be found in the original paper that introduced the max-margin and projection imitation learning (IL) algorithms: Apprenticeship Learning via Inverse Reinforcement Learning (by Abbel and Ng, 2004, ICML). Specifically, theorem 1 (section 4, page 4) statesLet an $\text{MDP} \setminus R$, features $ \phi : S \rightarrow [0, 1]^k$, and any $\epsilon > 0$ be given. Then the apprenticeship learning algorithm (both max-margin and projection versions) will terminate with $t^{(i)} \leq \epsilon$ after at most$$n=O\left(\frac{k}{(1-\gamma)^{2} \epsilon^{2}} \log \frac{k}{(1-\gamma) \epsilon}\right)$$
iterations.Here $k$ is the dimension of the feature vectors, so it's clear that the number of iterations needed for these algorithms to terminate scales with $k$. The proof of this theorem can be found in appendix A of the same paper (and all other terms are defined in the paper, which you should read to understand all the details). Of course, this result holds (only) for these specific IL algorithms (which are the algorithms the author of your slides, Abbel, is referring to). See also theorem 2 and the experiments section (in particular, figure 4, which shows the performance as a function of the number of trajectories) of the same paper.
These slides provide a nice overview of the contents of this paper, so I suggest that you read them too."
"What is the surrogate loss function in imitation learning, and how is it different from the true cost?","
I've been reading A Reduction of Imitation Learning and Structured Prediction
to No-Regret Online Learning lately, and I can't understand what they mean by the surrogate loss function.
Some relevant notation from the paper -

$d_\pi$ = average distribution of states if we follow policy $\pi$ for $T$ timesteps
$C(s,a)$ = the expected immediate cost of performing action a in state s for the task we are considering (assume $C$ is bounded in [0,1]
$C_\pi(s) = \mathbb{E}_{a\sim\pi(s)}[C(s,a)]$ is the expected immediate cost of $π$ in $s$.
$J(π) = T\mathbb{E}_{s\sim d_\pi}[C_\pi(s)]$ is the total cost of executing policy $\pi$ for $T$ timesteps


In imitation learning, we may not necessarily know or observe true costs $C(s,a)$ for the particular task. Instead, we observe expert demonstrations and seek to bound $J(π)$
for any cost function $C$ based on how well $π$ mimics the expert’s policy $π^{*}$. Denote $l$ the observed surrogate loss function we minimize instead of $C$. For instance, $l(s,π)$ may be the expected 0-1 loss of $π$ with respect to $π^{*}$ in state $s$, or a squared/hinge loss of $π$ with respect to $π^{*}$ in $s$. Importantly, in many instances, $C$ and $l$ may be the same function – for instance, if we are interested in optimizing the learner’s ability to predict the actions chosen by an expert.

I don't understand how exactly the surrogate loss is different from the true costs, and what are the possible cases in which both are the same. It'd be great if someone could throw some light on this. Thank you!
","['reinforcement-learning', 'papers', 'imitation-learning']","A surrogate loss is a loss than you use ""instead of"", ""in place of"", ""as a proxy for"" or ""as a substitute for"" another loss, which is typically the ""true"" loss.Surrogate losses are actually common in machine learning (although almost nobody realizes that they are surrogate losses). For example, the empirical risk (which the mean squared error is an instance of) is a surrogate for the expected risk, which is incomputable in almost all cases, given that you do not know the underlying probability distribution. See An Overview of Statistical Learning Theory by V. N. Vapnik for more details. In fact, discussions on generalization arise because of this issue, i.e. you use surrogate losses rather than true losses.The term ""surrogate"" is also used in conjunction with the term ""model"", i.e. ""surrogate model"", for example, in the context of Bayesian optimization, where a Gaussian process is the surrogate model for the unknown model/function that you want to know about, i.e. you use the Gaussian process to approximate the unknown function/model.Regarding the excerpt you are quoting and your specific concerns, although I didn't read the paper and I am not an expert in imitation learning, let me try to explain what I understand from this excerpt. Essentially, in imitation learning, you use the expert's policy $\pi^*$ to train the agent, rather than letting him just explore and exploit the environment. So, what you know is $\pi^*$ and you can calculate the ""loss"" between $\pi^*$ and $\pi$ (the current agent's policy), denoted by $l$. However, this loss $l$ that you calculate is not necessarily the ""true"" loss (i.e. it is a surrogate loss), given that our goal is not really to imitate the ""expert"" but to learn an optimal policy to behave in the environment. If the goal was to just imitate the ""expert"", then $C$ and $l$ would coincide, because, in that case, $l$ would represent the ""discrepancy"" or ""loss"" between $\pi$ and the expert's policy $\pi^*$."
What are the pros and cons of sparse and dense rewards in reinforcement learning?,"
From what I understand, if the rewards are sparse the agent will have to explore more to get rewards and learn the optimal policy, whereas if the rewards are dense in time, the agent is quickly guided towards its learning goal.
Are the above thoughts correct, and are there any other pros and cons of the two contrasting settings? On a side-note, I feel that the inability to specify rewards that are dense in time is what makes imitation learning useful.
","['reinforcement-learning', 'comparison', 'reward-functions', 'sparse-rewards', 'dense-rewards']","What are the pros and cons of sparse and dense rewards in reinforcement learning?It is unusual to refer to this difference as ""pros and cons"" because that term is often used to make comparisons between difference choices. Assuming you have a specific problem to solve, then whether or not the rewards are naturally sparse or dense is not a choice. You cannot say ""I want to solve MountainCar, I will use a dense reward setting"", because MountainCar has (relatively, for a starting problem) sparse rewards. You can only say ""I won't attempt MountainCar, it is too difficult"".In short however, your assessment is correct:if the rewards are sparse the agent will have to explore more to get rewards and learn the optimal policy, whereas if the rewards are dense in time, the agent is quickly guided towards its learning goalThere is not really any other difference at the top level. Essentially, sparser rewards make for a harder problem to solve. All RL algorithms can cope with sparse rewards to some degree, the whole concept of returns and value backup is designed to deal with sparseness at a theoretical level. In practical terms however, it may take some algorithms an unreasonable amount of time to determine a good policy beyond certain levels of sparseness.On a side-note, I feel that the inability to specify rewards that are dense in time is what makes imitation learning useful.Imitation learning is one of many techniques available to work around or deal with problems that have sparse reward structure. Others include:Reward shaping, which attempts to convert a sparse reward scheme to a dense one using domain knowledge of the researcher.Eligibility traces, which back up individual TD errors across multiple time steps.Prioritised sweeping, which focuses updates on ""surprising"" reward data.Action selection planning algorithms that look ahead from the current state.""Curiousity"" driven reinforcement learning that guides exploration to new state spaces independently of any reward signal."
Customized food for persons based on their profile using Reinforcement learning,"
I am newbie to Reinforcement Learning, this is my idea - Agent(food provider) has to select a food based on the environment(based on the user profile). Here the reward will be given to the agent based on the user's feedback. This is for single person, what if I wanted to make it for multiple persons. I wanted the system to learn on its own and create a policy such that it can identify certain group of people based on their profile and what type of food will be suitable for them.

Is this possible to implement in Reinforcement learning?
If so what type of problem is this and what type of solution I can use to solve this.

","['reinforcement-learning', 'q-learning', 'policy-gradients', 'markov-decision-process', 'multi-armed-bandits']",
"In DQN, when do the parameters in the Neural Network update based on the reward received?","
I'm aware that we back-propagate after computing the loss between:
The Neural Network Q values and the Target Network Q values
However, all this is doing is updating the parameters of the Neural Network to produce an output that matches the Target Q values as close as possible.
Suppose one epoch is run and the reward is +10, surely we need to update the parameters using this too to tell the Network to push the probability of these actions, given these parameters up.
How does the algorithm know +10 is good? Suppose the reward range is -10 for loss and +10 for win.
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'deep-learning', 'dqn']","However, all this is doing is updating the parameters of the Neural Network to produce an output that matches the Target Q values as close as possible.Yes. That is all it needs to do because we have defined the policy around the Q values like so:$$\pi(s) = \text{argmax}_a \hat{q}(s,a,\theta)$$Where $\theta$ is the neural network weights.Therefore, if the estimates of Q are approximately the same as the action value of the optimal policy, the policy in DQN is approximately the optimal policy.How does the algorithm know +10 is good?It does not, at least not directly. The algorithm knows, approximately, what the action values are if it acts consistently with their current estimates by always choosing the maximising action at each step.The learning process will learn that +10 is relatively good in your scenario because it never finds anything better when exploring."
Why are Target Networks used in Deep Q-Learning as opposed to the Expected Value equation?,"
I understand we use a target network because it helps resolve issues regarding stability, however, that's not what I'm here to ask.
What I would like to understand is why a target network is used as a measure of ground truth as opposed to the expectation equation.
To clarify, here is what I mean. This is the process used for DQN:

In DQN, we begin with a state $S$
We then pass this state through a neural network which outputs Q values for each action in the action space
A policy e.g. epsilon-greedy is used to take an action
This subsequently produces the next state $S_{t+1}$
$S_{t+1}$ is then passed through a target neural network to produce target Q values
These target Q values are then injected into the Bellman equation which ultimately produces a target Q value via the Q-learning update rule equation
MSE is used on 6 and 2 to compute the loss
This is then back-propagated to update the parameters for the neural network in 2
The target neural network has its parameters updated every X epochs to match the parameters in 2

Why do we use a target neural network to output Q values instead of using statistics. Statistics seems like a more accurate way to represent this. By statistics, I mean this:
Q values are the expected return, given the state and action under policy π.
$Q(S_{t+1},a) = V^π(S_{t+1})$ = $\mathbb{E}(r_{t+1}+ γr_{t+2}+ (γ^2)_{t+3} + ...    \mid S_{t+1}) = {E}(∑γ^kr_{t+k+1}\mid S_{t+1})$
We can then take the above and inject it into the Bellman equation to update our target Q value:
$Q(S_{t},a_t) + α*(r_t+γ*max(Q(S_{t+1},a))-Q(S_{t},a))$
So, why don't we set the target to the sum of diminishing returns? Surely a target network is very inaccurate, especially since the parameters in the first few epochs for the target network are completely random.
","['machine-learning', 'reinforcement-learning', 'deep-learning', 'dqn', 'deep-rl']",
What is Precision@K for link prediction in graph embedding meaning?,"
I am trying to re-implement the SDNE algorithm for graph embedding by PyTorch.
I get stuck at some issues about evaluation metric Precision@K.

precision@k is a metric which gives equal weight to the returned instance. It is defined as follows
$$precision@k(i) = \frac{\left| \, \{ j \, | \, i, j \in V, index(j) \le k, \Delta_i(j) = 1 \} \, \right|}{k}$$
where $V$ is the vertex set, $index(j)$ is the ranked index of the $j$-th vertex and $\Delta_i(j) = 1$ indicates that $v_i$ and $v_j$ have a link.

I don't understand what ""ranked index of the $j$-th vertex"" means.
Beside, I am also confused about the MAP metric in section 4.3. I don't understand how to calculate it.

Mean Average Precision (MAP) is a metric with good discrimination and stability. Compared with precision@k, it is
more concerned with the performance of the returned items ranked ahead. It is calculated as follows:
$$AP(i) = \frac{\sum_j precision@j(i) \cdot \Delta_i(j)}{\left| \{ \Delta_i(j) = 1 \} \right|}$$
$$MAP = \frac{\sum_{i \in Q} AP(i)}{|Q|}$$
where $Q$ is the query set.

If anyone is familiar with these metrics, could you help me to explain them?
","['deep-learning', 'graphs', 'precision']","These measures are used for evaluating how ""good"" an embedding of a graph is or how ""good"" the graph reconstructed from the embedding resembles the original.Given the embedding and vertex $i$, it seems to be that the rank of the vertices is dependent on the probability of there being a link between vertex $i$ and vertex $j$ in the original graph. If there is a higher probability of there being a link between $i$ and $j$ in the original graph, $j$ has a lower rank.In other words, $precision@k(i)$ is the proportion of vertices $j$ that vertex $i$ has a link to in the original graph out of the $k$ vertices for which vertex $i$ has the highest probability of having a link to, recovered from the embedding.This matches up with the common definition of $precision@n$ used in evaluating information/document retrieval, defined as the proportion of relevant documents out of the $n$ best retrieved documents.The average precision of a vertex, $AP(i)$, is the average of $precision@j$ over all $j$ such that there is a link between vertex $i$ and vertex $j$. Perhaps a more clear definition would have been $$AP(i) = \frac{\sum_{j \in S_i} precision@j(i)}{\left| S_i \right|}$$where $S_i = \{j \, |\,  \Delta_i(j) = 1 \}$, the set of all $j$ such that there is a link from $i$ to $j$.$MAP$ for a query set $Q$ is then the mean of the average precision ($AP$) over all vertices in $Q$."
Do all filters of the same convolutional layer need to have the same dimensions and stride?,"
In Convolutional Neural Networks, do all filters of the same convolutional layer need to have the same dimensions and stride?
If they don't, then it would seem the channel produced by each filter would have different sizes. Or is there some way to get around that?
","['convolutional-neural-networks', 'filters', 'convolutional-layers', 'stride']",
"When using experience replay in reinforcement learning, which state is used for training?","
I'm slightly confused about the experience replay process. I understand why we use batch processing in reinforcement learning, and from my understanding, a batch of states is input into the neural network model.
Suppose there are 2 valid moves in the action space (UP or DOWN)
Suppose the batch size is 5, and the 5 states are this:
$$[s_1, s_2, s_3, s_4, s_5]$$
We put this batch into the neural network model and output Q values. Then we put $[s_1', s_2', s_3', s_4', s_5']$ into a target network.
What I'm confused about is this:
Each state in $[s_1, s_2, s_3, s_4, s_5]$ is different.
Are we computing Q values for UP and DOWN for ALL 5 states after they go through the neural network?
For example, $$[Q_{s_1}(\text{UP}), Q_{s_1}(\text{DOWN})],
\\ [Q_{s_2} (\text{UP}), Q_{s_2}(\text{DOWN})], \\ 
[Q_{s_3}(\text{UP}), Q_{s_3}(\text{DOWN})], \\ 
[Q_{s_4}(\text{UP}), Q_{s_4}(\text{DOWN})], \\ 
[Q_{s_5}(\text{UP}), Q_{s_5}(\text{DOWN})]$$
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'q-learning', 'dqn']","The way the states are used is as follows:Typically your $Q$-network will state a state as input and output scores over the action space. I.e. $Q : \mathcal{S} \rightarrow \mathbb{R}^{|\mathcal{A}|}$. So, in your replay buffer you should store $s_t, a_t, r_{t+1}, s_{t+1}, \mbox{done}$ (note that done just represents where the episode ended on this transition and I add for completeness.Now, when you are doing your batch updates you sample uniformly at random from this replay buffer. This means you get $B$ tuples of $s_t, a_t, r_{t+1}, s_{t+1}, \mbox{done}$. Now, I will assume $B=1$ as it is easier to explain and the extension to $B > 1$ should be easy to see.For our state-action tuple $s_t, a_t$ we want to shift what the network predicts for this pair to be closer to $r_{t+1} + \gamma \arg\max_a Q(s,a)$. However, our neural network only takes the state as input, and outputs a vector of scores for each action. That means we want to shift the output of our network for the state $s_t$ towards the target I just mentioned, but only for the action $a_t$ that we took. To do this we just calculate the target, i.e. we calculate $r_{t+1} + \gamma \arg\max_a Q(s,a)$, and then we do gradient ascent like we would a normal neural network where the target vector is the same as the predicted vector everywhere except the $a_t$th element, which we will change to $r_{t+1} + \gamma \arg\max_a Q(s,a)$. This way, our network moves closer to our Q-learning update for only the action we want, in line with how Q-learning works.It is also worth nothing that you can parameterise your Neural Network to be a function $Q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ which would make training more in line with how tabular Q-learning but is seldom used in practice as it becomes much more expensive to compute (you have to do a forward pass for each action, rather than one forward pass per state)."
Why is depth-first search an artificial intelligence algorithm?,"
I'm new to the artificial intelligence field. In our first chapters, there is one topic called ""problem-solving by searching"". After searching for it on the internet, I found the depth-first search algorithm. The algorithm is easy to understand, but no one explains why this algorithm is included in the artificial intelligence study.
Where do we use it? What makes it an artificial intelligence algorithm? Is every search algorithm is an AI algorithm?
","['search', 'ai-field', 'depth-first-search']","This is a fundamentally a philosophical question. What makes AI AI? But first things, why would DFS be considered an AI algorithm?In its most basic form, DFS is a very general algorithm that is applied to wildly different categories of problems: topological sorting, finding all the connected components in a graph, etc. It may be also used for searching. For instance, you could use DFS for finding a path in a 2D maze (although not necessarily the shortest one). Or you could use it to navigate through more abstract state spaces (e.g. between configuration of chess or in the towers of Hanoi). And this is where the connection to AI arises. DFS can be used on its own for navigating such spaces, or as a basic subroutine for more complex algorithms. I believe that in the book Artificial Intelligence: A Modern Approach (which you may be reading at the moment) they introduce DFS and Breadth-First Search this way, as a first milestone before reaching more complex algorithms like A*.Now, you may be wondering why such search algorithms should be considered AI. Here, I'm speculating, but maybe the source of the confusion comes from the fact that DFS does not learn anything. This is a common misconception among new AI practitioners. Not every AI technique has to revolve around learning. In other words, AI != Machine Learning. ML is one of the many subfields within AI. In fact, early AI (around the 50s-60s) was more about logic reasoning than it was about learning.AI is about making an artificial system behave ""intelligently"" in a given setting, whatever it takes to reach that intelligent behavior. If what it takes is applying well-known algorithms from computer science like DFS, then so be it. Now, what is it that intelligent means? This is where we enter more philosophical grounds. My interpretation is that ""intelligence"" is a broad term to define the large set of techniques that we use to approach the immense complexity that reality and certain puzzle-like problems have to offer. Often, ""intelligent behavior"" revolves around heuristics and proxy methods away from the perfect, provable algorithms that work elsewhere in computer science. While certain algorithms (like DFS or A*) may be proven to give optimal answers if infinitely many resources can be devoted to the task at hand, only in sufficiently constrained settings would such techniques be affordable. Fortunately, we can make them work in many situations (like A* for chess or for robot navigation, or Monte Carlo Tree Search for Go), but only if reasonable assumptions and constraints over the state space are imposed. For all the rest is where learning techniques (like Markov Random Fields for image segmentation, or Neural Nets paired with Reinforcement Learning for situated agents) may come handy.Funny enough, even if intelligence is often regarded as a good thing, my interpretation can be summed up as imperfect modes of behavior to address immensely complex problems for which no known perfect solution exists (with rare exceptions in sufficiently bounded problems). If we had a huge table that, for each chess position, gives the best possible move you can make, and put that table inside a program, would this program be intelligent? Maybe you'd think so, but in any case it seems more arguable than a program that makes real-time reasoning and spits a decision after some reasonable time, even if it's not the best one. Similarly, do you consider sorting algorithms intelligent? Again, the answer is arguable, but the fact is that algorithms exist with optimal time and memory complexities, we know that we can't do better than what those algorithms do, and we do not have to resort to any heuristic or any learning to do better (disclaimer: I haven't actually checked if there's some madman out in the wild applying learning to solve sorting with better average times)."
My Deep Q-Learning Network does not learn for OpenAI gym's cartpole problem,"
I am implementing OpenAI gym's cartpole problem using Deep Q-Learning (DQN). I followed tutorials (video and otherwise) and learned all about it. I implemented a code for myself and I thought it should work, but the agent is not learning. I will really really really appreciate if someone can pinpoint where I am doing wrong.
Note that I have a target neuaral network and a policy network already there. The code is as below.
import numpy as np
import gym
import random
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense
from collections import deque

env = gym.make('CartPole-v0')

EPISODES = 2000
BATCH_SIZE = 32
DISCOUNT = 0.95
UPDATE_TARGET_EVERY = 5
STATE_SIZE = env.observation_space.shape[0]
ACTION_SIZE = env.action_space.n
SHOW_EVERY = 50

class DQNAgents:
    
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.replay_memory = deque(maxlen = 2000)
        self.gamma = 0.95
        self.epsilon = 1
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.model = self._build_model()
        self.target_model = self.model
        
        self.target_update_counter = 0
        print('Initialize the agent')
        
    def _build_model(self):
        model = Sequential()
        model.add(Dense(20, input_dim = self.state_size, activation = 'relu'))
        model.add(Dense(10, activation = 'relu'))
        model.add(Dense(self.action_size, activation = 'linear'))
        model.compile(loss = 'mse', optimizer = Adam(lr = 0.001))
        
        return model

    def update_replay_memory(self, current_state, action, reward, next_state, done):
        self.replay_memory.append((current_state, action, reward, next_state, done))
        
    def train(self, terminal_state):
        
        # Sample from replay memory
        minibatch = random.sample(self.replay_memory, BATCH_SIZE)
        
        #Picks the current states from the randomly selected minibatch
        current_states = np.array([t[0] for t in minibatch])
        current_qs_list= self.model.predict(current_states) #gives the Q value for the policy network
        new_state = np.array([t[3] for t in minibatch])
        future_qs_list = self.target_model.predict(new_state)
        
        X = []
        Y = []
        
        # This loop will run 32 times (actually minibatch times)
        for index, (current_state, action, reward, next_state, done) in enumerate(minibatch):
            
            if not done:
                new_q = reward + DISCOUNT * np.max(future_qs_list)
            else:
                new_q = reward
                
            # Update Q value for given state
            current_qs = current_qs_list[index]
            current_qs[action] = new_q
            
            X.append(current_state)
            Y.append(current_qs)
        
        # Fitting the weights, i.e. reducing the loss using gradient descent
        self.model.fit(np.array(X), np.array(Y), batch_size = BATCH_SIZE, verbose = 0, shuffle = False)
        
       # Update target network counter every episode
        if terminal_state:
            self.target_update_counter += 1
            
        # If counter reaches set value, update target network with weights of main network
        if self.target_update_counter > UPDATE_TARGET_EVERY:
            self.target_model.set_weights(self.model.get_weights())
            self.target_update_counter = 0
    
    def get_qs(self, state):
        return self.model.predict(np.array(state).reshape(-1, *state.shape))[0]
            

''' We start here'''

agent = DQNAgents(STATE_SIZE, ACTION_SIZE)

for e in range(EPISODES):
    
    done = False
    current_state = env.reset()
    time = 0 
    total_reward = 0
    while not done:
        if np.random.random() > agent.epsilon:
            action = np.argmax(agent.get_qs(current_state))
        else:
            action = env.action_space.sample()
        
        next_state, reward, done, _ = env.step(action)

        agent.update_replay_memory(current_state, action, reward, next_state, done)
        
        if len(agent.replay_memory) < BATCH_SIZE:
            pass
        else:
            agent.train(done)
            
        time+=1    
        current_state = next_state
        total_reward += reward
        
    print(f'episode : {e}, steps {time}, epsilon : {agent.epsilon}')
    
    if agent.epsilon > agent.epsilon_min:
        agent.epsilon *= agent.epsilon_decay

Results for first 40ish iterations are below (look for the number of steps, they should be increasing and should reach a maximum of 199)
episode : 0, steps 14, epsilon : 1
episode : 1, steps 13, epsilon : 0.995
episode : 2, steps 17, epsilon : 0.990025
episode : 3, steps 12, epsilon : 0.985074875
episode : 4, steps 29, epsilon : 0.9801495006250001
episode : 5, steps 14, epsilon : 0.9752487531218751
episode : 6, steps 11, epsilon : 0.9703725093562657
episode : 7, steps 13, epsilon : 0.9655206468094844
episode : 8, steps 11, epsilon : 0.960693043575437
episode : 9, steps 14, epsilon : 0.9558895783575597
episode : 10, steps 39, epsilon : 0.9511101304657719
episode : 11, steps 14, epsilon : 0.946354579813443
episode : 12, steps 19, epsilon : 0.9416228069143757
episode : 13, steps 16, epsilon : 0.9369146928798039
episode : 14, steps 14, epsilon : 0.9322301194154049
episode : 15, steps 18, epsilon : 0.9275689688183278
episode : 16, steps 31, epsilon : 0.9229311239742362
episode : 17, steps 14, epsilon : 0.918316468354365
episode : 18, steps 21, epsilon : 0.9137248860125932
episode : 19, steps 9, epsilon : 0.9091562615825302
episode : 20, steps 26, epsilon : 0.9046104802746175
episode : 21, steps 20, epsilon : 0.9000874278732445
episode : 22, steps 53, epsilon : 0.8955869907338783
episode : 23, steps 24, epsilon : 0.8911090557802088
episode : 24, steps 14, epsilon : 0.8866535105013078
episode : 25, steps 40, epsilon : 0.8822202429488013
episode : 26, steps 10, epsilon : 0.8778091417340573
episode : 27, steps 60, epsilon : 0.8734200960253871
episode : 28, steps 17, epsilon : 0.8690529955452602
episode : 29, steps 11, epsilon : 0.8647077305675338
episode : 30, steps 42, epsilon : 0.8603841919146962
episode : 31, steps 16, epsilon : 0.8560822709551227
episode : 32, steps 12, epsilon : 0.851801859600347
episode : 33, steps 12, epsilon : 0.8475428503023453
episode : 34, steps 10, epsilon : 0.8433051360508336
episode : 35, steps 30, epsilon : 0.8390886103705794
episode : 36, steps 21, epsilon : 0.8348931673187264
episode : 37, steps 24, epsilon : 0.8307187014821328
episode : 38, steps 33, epsilon : 0.8265651079747222
episode : 39, steps 32, epsilon : 0.8224322824348486
episode : 40, steps 15, epsilon : 0.8183201210226743
episode : 41, steps 20, epsilon : 0.8142285204175609
episode : 42, steps 37, epsilon : 0.810157377815473
episode : 43, steps 11, epsilon : 0.8061065909263957
episode : 44, steps 30, epsilon : 0.8020760579717637
episode : 45, steps 11, epsilon : 0.798065677681905
episode : 46, steps 34, epsilon : 0.7940753492934954
episode : 47, steps 12, epsilon : 0.7901049725470279
episode : 48, steps 26, epsilon : 0.7861544476842928
episode : 49, steps 19, epsilon : 0.7822236754458713
episode : 50, steps 20, epsilon : 0.778312557068642

","['reinforcement-learning', 'dqn', 'deep-neural-networks', 'open-ai', 'gym']","There is a really small mistake in here that causes the problem: Since np.max(future_qs_list) should be np.max(future_qs_list[index]) since you're now getting the highest Q of the entire batch.
Instead of the getting the highest Q from the current next state.It's like this after changing that (remember an epsilon of 1 means that you get 100% of your actions taken by the a dice roll so I let it go for a few more epochs, also tried it with the old code but indeed didn't get more then 50 steps (even after 400 epochs/episodes))"
"Implementing Gradient Descent Algorithm in Python, bit confused regarding equations","
I'm following the guide as outlined at this link: http://neuralnetworksanddeeplearning.com/chap2.html
For the purposes of this question, I've written a basic network 2 hidden layers, one with 2 neurons and one with one neuron. For a very basic task, the network will learn how to compute an OR logic gate so the training data will be:
X = [[0, 0], [0, 1], [1, 0], [1, 1]]
Y = [0, 1, 1, 1]

And the diagram:

For this example, the weights and biases are:
w = [[0.3, 0.4], [0.1]]
b = [[1, 1], [1]]

The feedforward part was pretty easy to implement so I don't think I need to post that here. The tutorial I've been following summarises calculating the errors and the gradient descent algorithm with the following equations:
For each training example $x$, compute the output error $\delta^{x, L}$ where $L =$ Final layer (Layer 1 in this case). $\delta^{x, L} = \nabla_aC_x \circ \sigma'(z^{x, L})$ where $\nabla_aC_x$ is the differential of the cost function (basic MSE) with respect to the Layer 1 activation output, and $\sigma'(z^{x, L})$ is the derivative of the sigmoid function of the Layer 1 output i.e. $\sigma(z^{x, L})(1-\sigma(z^{x, L}))$.
That's all good so far and I can calculate that quite straightforwardly. Now for $l = L-1, L-2, ...$, the error for each previous layer can be calculated as
$\delta^{x, l} = ((w^{l+1})^T \delta^{x, l+1}) \circ \sigma(z^{x, l})$
Which again, is pretty straight forward to implement.
Finally, to update the weights (and bias), the equations are for $l = L, L-1, ...$:
$w^l \rightarrow w^l - \frac{\eta}{m}\sum_x\delta^{x,l}(a^{x, l-1})^T$
$b^l \rightarrow b^l - \frac{\eta}{m}\sum_x\delta^{x,l}$
What I don't understand is how this works with vectors of different numbers of elements (I think the lack of vector notation here confuses me).
For example, Layer 1 has one neuron, so $\delta^{x, 1}$ will be a scalar value since it only outputs one value. However, $a^{x, 0}$ is a vector with two elements since layer 0 has two neurons. Which means that $\delta^{x, l}(a^{x, l-1})^T$ will be a vector even if I sum over all training samples $x$. What am I supposed to do here? Am I just supposed to sum the components of the vector as well?
Hopefully my question makes sense; I feel I'm very close to implementing this entirely and I'm just stuck here.
Thank you
[edit] Okay, so I realised that I've been misrepresenting the weights of the neurons and have corrected for that.
weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]

Which has the output
[array([[0.27660583, 1.00106314],
   [0.34017727, 0.74990392]])
array([[ 1.095244  , -0.22719165]])

Which means that layer0 has a weight matrix with shape 2x2 representing the 2 weights on neuron01 and the 2 weights on neuron02.
My understanding then is that $\delta^{x,l}$ has the same shape as the weights array because each weight gets updated indepedently. That's also fine.
But the bias term (according to the link I sourced) has 1 term for each neuron, which means layer 0 will has two bias terms (b00 and b01) and layer 1 has one bias term (b10).
However, to calculate the update for the bias terms, you sum the deltas over x i.e $\sum_x \delta^{x, l}$; if delta has the size of the weight matrix, then there are too many terms to update the bias terms. What have I missed here?
Many thanks
","['neural-networks', 'python', 'gradient-descent']","There seems to be a mismatch between the weights you provide and your network diagram. Since w[0] (the yellow connections) is meant to transform $ x \in \mathbb{R}^2 $ into the layer 0 activations which are $ \mathbb{R}^2 $, w[0] should be a matrix $ \in \mathbb{R}^{2 \times 2} $, not a vector in $\mathbb{R}^2 $ as you have. Likewise, your w[1] (the red connections) should be a vector $ \in \mathbb{R^2} $ and not a scalar. Finally, if you are indeed scaling the output of layer 1 (the blue connection), then you'll need an additional scalar value. However, the blue connection confuses me a bit as usually the activated output is used directly in the loss function, not a scaled version of it. Unless the blue connection stands for the loss function.In short, I believe if you change the shapes of your weight matrices to actually represent your network diagram, your update equations will work. I'll go through the network below to make sure I illustrate my point.$ x \in \mathbb{R}^{2} $, an input example$ W^0 \in \mathbb{R}^{2 \times 2} $, the yellow connections$ W^1 \in \mathbb{R}^2 $, the red connections$ z^0 = xW^0 \in \mathbb{R}^{2} $, the weighted inputs to the layer 0 nodes. The dimensions of this should match the number of nodes at layer 0.$ a^0 = \sigma(z^0) \in \mathbb{R}^{2} $, the output of the layer 0 nodes. The dimensions of this should match the number of nodes at layer 0.$ z^1 = a^0 W^1 \in \mathbb{R} $, the weighted inputs to the layer 1 nodes. The dimensions of this should match the number of nodes at layer 1.$ a^1 = \sigma(z^1) \in \mathbb{R} $, the output of the layer 1 nodes and thus the output of the network. The dimensions of this should match the number of nodes at layer 1.Weight UpdatesAs you say before your edit, $\delta^1$, as the product of two scalars $\nabla_a C$ and $\sigma'(z^1)$, is also a scalar. Since $a^0$ is a vector in $\mathbb{R}^2$, then
$\delta^1(a^0)^T$ is also a vector in $\mathbb{R}^2$. This matches what we expect, as it should match the dimensions of $W^1$ to allow the element-wise subtraction in the weight update equation.NB. It is not the case, as you say in your edit, that the shape of $\delta^l$ should match the shape of $W^l$. It should instead match the number of nodes, and it is the shape of $\delta^l(a^{l-1})^T$ that should match the shape of $W^l$. You had this right in your original post.Bias UpdatesThis brings us to the bias updates.
There should be one bias term per node in a given layer, so the shapes of your biases are correct (i.e. $\mathbb{R}^2$ for layer 0 and $\mathbb{R}$ for layer 1). Now, we saw above that the shape of $\delta^l$ also matches the number of nodes in layer $l$, so again the element-wise subtraction in your original bias update equation works.I also tried using this book to learn backprop, but I had a hard time connecting the variables with the different parts of the network and the corresponding code. I finally understood the algorithm in depth only after deriving all the update equations by hand for a very small network (2 inputs, one output, no hidden layers) and working my way up to larger networks, making sure to keep track of the shapes of the inputs and outputs along the way. If you're having trouble with the update equations I highly recommend this.A final piece of advice that helped me: drop the $x$ and the summations over input examples from your formulations and just treat everything as matrices (e.g. a scalar becomes a matrix in $\mathbb{R}^{1 \times 1}$, $X$ is a matrix in $\mathbb{R}^{N \times D}$). First, this allows you to better interpret matrix orientations and debug issues such as a missing transpose operation. Second, this is (in my limited understanding) how backprop should actually be implemented in order to take advantage of optimized linalg libraries and GPUs, so it's perhaps a bit more relevant."
How do RNN's for sentiment classification deal with different sentence lengths?,"
I have been doing a course which teaches you about Deep Neural Networks, during one of the exercises I was made to make an RNN for sentiment classification which I did, but I did not understand how an RNN is able to deal with sentences of different lengths while conducting sentiment classification.
","['neural-networks', 'recurrent-neural-networks', 'text-classification', 'sentiment-analysis', 'multi-label-classification']","One of the essential pre-processing we do on the corpus involves treating the variable-length sentences to a fixed length. There are various ways in which we can do this:This involves reducing the length of all the sentences to the length of the shortest sentence in the corpus. This is generally not done as it reduces the amount of information that we can learn from the corpus. This image shows pre sequence truncation, where we remove from the back to make the sentences of the same length.This is the most preferred method when it comes to handling the problem of variable length sentences. In this approach, we increase the size of each vector to the longest sentence in the corpus. There are two ways to this:Effect of Padding on LSTMs and CNNs by Dwarampudi Mahidhar Reddy and N V Subba Reddy, et al."
What is the representational capacity of a learning algorithm? [duplicate],"







This question already has answers here:
                                
                            




What is the difference between hypothesis space and representational capacity?

                                (3 answers)
                            

Closed 2 years ago.



The definition I see for representational capacity is ""the family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective."" (Goodfellow's Deep learning book).
However, to me this seems to be the same as the definition of the hypothesis space. Is the key difference the ""in order to reduce a training objective"" in that some functions may not be chosen in reducing a training objective? Or are these identical definitions.
","['deep-learning', 'terminology', 'computational-learning-theory']",
What is the time complexity of the upsampling stage of the U-net?,"
I am trying to determine the complexity of the neural network we use. The neural network is a U-net generator with an input shape of NxN (not an image but image-like data) and output of the same shape. There is 7x downsampling and 7x upsampling. Downsampling is a simple convolutional layer, where I have no problem to determine complexity as stated here:
$$
O\left(\sum_{l=1}^{d} n_{l-1} \cdot s_{l}^{2} \cdot n_{l} \cdot m_{l}^{2}\right)
$$
I however cannot find what is big O complexity for the upsampling stage, where the UpSampling2D layer is used before convolution.
Any idea what is the time complexity of the upsampling convolutional layer, or where I might find information? Thanks in advance!
","['convolutional-neural-networks', 'time-complexity', 'u-net', 'computational-complexity', 'pooling']","After further investigating the problem I have found the answer:U-net generators' up-sampling stage consists of two steps:The UpSampling2D layer is in the keras documentation described as:Repeats the rows and columns of the data by size[0] and size[1] respectively.From this information, we can calculate the time cost for UpSampling2D alone. Lets set size to (2,2), as is set in basic configuration of the U-net generator. The output of the UpSampling2D is then doubled. In case we started with (4,4,3), where the last index corresponds to number of channels, the output shape will be 8,8,3. We can see that each row and column need to be copied twice in each channel. From this we can define time complexity of a single up-sampling as:$$
O\left(2 \cdot c \cdot n \cdot s\right)
$$Where c corresponds to number of channels, n corresponds to input length (one side of a matrix) and s is equal to filter size. Assuming that length and filter size have square shape, the complexity is multiplied by 2. Since in this case the the filter size is known, equal to (2,2), the notation can be simplified to:$$
O\left(4 \cdot c \cdot n \right) = O\left(c \cdot n \right)
$$In my case, with only 1 channel, the complexity is simply$$
O\left(n \right)
$$Which means the up-sampling stage is linear, and the only important feature is input size, which is negligible to the complexity of the following convolutional layer and can be ignored."
"Given the daily stock prices of the last 3 years, how should I sample the training data for episodic RL?","
I am playing around with a stock trading agent trained via (deep) reinforcement learning, including memory replay. The agent is trained for 1000 episodes, where each episode consists of 180 timesteps (e.g. daily stock prices).
My question is concerning the sampling of episodes for training.
Assuming I've got daily stock prices going back 3 years, that's about 750 trading days/prices.
How should I sample this data set to get enough episodes for training?
With an episode length of 180 and an episode count of 1000, I'd need 180k ""days"" to choose from, if I wouldn't want any duplication.
Do I even need to sample 1000 non-overlapping windows from my dataset or can I sample my episodes using a sliding window approach? Could I even just randomly sample the dataset for episodes? For example, calculate a random date and build the episode from the 180 days following that random starting date?
The reward for each action is calculated as follows, p are the prices and t is the current timestep of the episode.

CASH: 0
BUY:  p(t+1) - p(t) - fee
HOLD: p(t+1) - p(t)

","['training', 'deep-rl', 'reward-functions', 'algorithmic-trading']",
How can I derive n-step off-policy temporal difference formula?,"
I was reading the book ""Reinforcement Learning: An Introduction"" by Sutton and Barto. In section 7.3, they write the formula for n-step off-policy TD as
$$V(S_t) = V(S_{t-1}) + \alpha \rho_{t:t+n-1}[G_{t:t+n} - V(S_{t-1})],$$
where $V(S_{t})$ is state value function of the state $S$ at time $t$ and $ G_{t:t+n} \doteq \sum_{i=t}^{t+n-1}\gamma^{i-t}R_{i+1} + \gamma^n V(S_{t+n})$ and $\rho_{t:t+n-1}$ is the importance sampling ratio.
I tried to prove this equation for $n = 1$ using the incremental update of the value function. Now I end up with this formula:
$$V(S_t) = \frac{1}{t} \sum_{j=1}^{t} \rho_{j}G_{j} $$
$$V(S_t)= \frac{1}{t}(\rho_{t}G_{t} + \sum_{j=1}^{t-1}\rho_{j}G_{j}) $$
$$V(S_t) = \frac{1}{t}(\rho_t G_t + (t-1)V(S_{t-1}))$$
$$V(S_t)=V(S_{t-1}) + \frac{1}{t}(\rho_{t}G_{t} - V(S_{t-1}))$$
I know I'm wrong because this does not match with the above equation. But can anyone please show me where I am wrong?
","['reinforcement-learning', 'proofs', 'temporal-difference-methods', 'off-policy-methods', 'importance-sampling']",
What is layer freezing in transfer learning?,"
Transfer learning consists of taking features learned on one problem and leveraging them on a new, similar problem.
In the Transfer Learning, we take layers from a previously trained model and freeze them.
Why is this layer freezing required and what are the effects of layer freezing?
",['transfer-learning'],"Why is this layer freezing required?It's not.What are the effects of layer freezing?
The consequences are:(1) Should be faster to train (the gradient will have far less components)(2) Should require less data to train onIf you do unfreeze the weights, I'd think your performance would be better because you are adjusting (i.e., fine-tuning) the parameters to your specific problem at hand. I am not sure what the marginal improvements are in practice, as I have not experiemented much with fine-tuning (like are the improvements typically a 0.01% reduction in error rate? Not sure.)"
Should I use the discounted average reward as objective in a finite-horizon problem?,"
I am new to reinforcement learning, but, for a finite horizon application problem, I am considering using the average reward instead of the sum of rewards as the objective. Specifically, there are a total of $T$ maximally possible time steps (e.g., the usage rate of an app in each time-step), in each time-step, the reward may be 0 or 1. The goal is to maximize the daily average usage rate.
Episode length ($T$) is maximally 10. $T$ is the maximum time window the product can observe about a user's behavior of the chosen data. There is an indicator value in the data indicating whether an episode terminates. From the data, it is offline learning, so in each episode, $T$ is given in the data. As long as an episode doesn't terminate, there is a reward of $\{0, 1\}$ in each time-step.
I heard if I use an average reward for the finite horizon, the optimal policy is no longer a stationary policy, and optimal $Q$ function depends on time. I am wondering why this is the case.
I see normally, the objective is defined maximizing
$$\sum_t^T \gamma^t r_t$$
And I am considering two types of average reward definition.

$1/T(\sum^𝑇_{t=0}\gamma^t r_t)$, $T$ varies is in each episode.

$1/(T-t)\sum^T_{i=t-1}\gamma^i r_i$


","['reinforcement-learning', 'q-learning', 'rewards', 'stationary-policy']",
How does YOLO handle non-class objects?,"
I have been reading more about computer vision and I'm bothered by YOLO and similar deep learning architectures.
The thing I am confused about is how non-class image sections are dealt with. In particular, it's not clear to me at all why YOLO doesn't consider every part of an image a possible class.
What actually sets the cutoff for detection and then classification?
","['deep-learning', 'computer-vision', 'object-detection', 'yolo']",
How can we prevent AGI from doing drugs?,"
I recently read some introductions to AI alignment, AIXI and decision theory things.
As far as I understood, one of the main problems in AI alignment is how to define a utility function well, not causing something like the paperclip apocalypse.
Then a question comes to my mind that whatever the utility function would be, we need a computer to compute the utility and reward, so that there is no way to prevent AGI from seeking it to manipulate the utility function to always give the maximum reward.
Just like we humans know that we can give happiness to ourselves in chemical ways and some people actually do so.
Is there any way to prevent this from happening? Not just protecting the utility calculator physically from AGI (How can we sure it works forever?), but preventing AGI from thinking of it?
","['reinforcement-learning', 'agi', 'rewards', 'aixi', 'reward-hacking']","This is known as reward hacking in the literature; see, e.g., https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd for discussion and further links."
How can Transformers handle arbitrary length input?,"
The transformer, introduced in the paper Attention Is All You Need, is a popular new neural network architecture that is commonly viewed as an alternative to recurrent neural networks, like LSTMs and GRUs.
However, having gone through the paper, as well as several online explanations, I still have trouble wrapping my head around how they work. How can a non-recurrent structure be able to deal with inputs of arbitrary length?
","['natural-language-processing', 'recurrent-neural-networks', 'long-short-term-memory', 'transformer']","Actually, there is usually an upper bound for inputs of transformers, due to the inability of handling long-sequence. Usually, the value is set as 512 or 1024 at current stage.However, if you are asking handling the various input size, adding padding token such as [PAD] in BERT model is a common solution. The position of [PAD] token could be masked in self-attention, therefore, causes no influence. Let's say we use a transformer model with 512 limit of sequence length, then we pass a input sequence of 103 tokens. We padded it to 512 tokens. In the attention layer, positions from 104 to 512 are all masked, that is, they are not attending or being attended."
"Is there a problem for ""Sound Source Identification in Video Footage""?","
I've been considering starting a project for some time on sound source identification.
To be more specific, my goal is to be able to identify the ""sources"" for sound in videos. Moving parts clanging, lips speaking, hands clapping etc. I'd like to think a model trained to be able to do this might be helpful for:

Identifying who is saying what in a crowd
Discovering noise sources caught on video (ex. a carpenter's saw as he is talking to someone)
Extending this to design a model for reading lips, to discern speech in silent video.


(Taken from https://www.youtube.com/watch?v=8ch_H8pt9M8)
You might think of this task like grounding in NLP, expect for sound/speech instead. I'm sure this has been done before, and I'd like to conduct a literature review. So is there a name for this kind of sound-source identification?
I've tried Googling ""Sound Source Identification"", but it only returns Speech Classification results (Is this sound a car or a truck etc.)
","['classification', 'computer-vision', 'audio-processing']",
What is the Preferred Mathematical Representation for a Forward Pass in a Neural Network?,"
I know this may be a question of semantics but I always see different articles explain forward pass slightly different. e.g. Sometimes they represent a forward pass to a hidden layer in a standard neural network as np.dot(x, W) and sometimes I see it as np.dot(W.T, x) and sometimes np.dot(W, x).
Take this image for example. They represent the input data as a matrix of [NxD] and weight data as [DxH] where H is the number of neurons in the hidden layer. This seems the most natural since input data will often be in tabular format with rows as samples and columns as features.

Now an example from the CS231n course notes. They talk about this below example and cite the code used to compute it as:
f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3, 1) # random input vector of three numbers (3x1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)
out = np.dot(W3, h2) + b3 # output neuron (1x1)

Where W is [4x3] and x is [3x1]. I would expect the weight matrix to have dimensions equal to [n_features, n_hidden_neurons] but in this example it just seems like they transposed it naturally before it was used.

I guess I am just confused about general nomenclature in how data should be shaped and used consistently when computing neural network forward passes. Sometimes I see transpose, sometimes I don't. Is there a standard, preferred way to represent data in accordance to a diagram like these This question may be silly but I just wanted to discuss it a bit. Thank you.
","['neural-networks', 'python', 'feedforward-neural-networks', 'forward-pass']",
Where can I find short videos of examples of RL being used?,"
I would like to add a short ~1-3 minute video to a presentation, to demonstrate how Reinforcement Learning is used to solve problems. I am thinking something like a short gif of an agent playing an Atari game, but for my audience it would probably be better to have something more manufacturing/industry based.
Does anyone know any good sources where I could find some stuff like this?
",['reinforcement-learning'],
Alternatives to Hierarchical RL for centralized control tasks?,"
Consider a problem where the agent must learn to control a hierarchy of agents acting against another such agent in a competitive environment.  The agents on each team need to learn cooperate in order to compete with the other agents.
A hierarchical RL algorithm would seem to be ideal for such a problem, learning a policy that includes sub-policies for sub-agents.  But are there are other types of algorithms that could be used for this kind of task, perhaps ones that are involved centralized cooperation but aren't considered hierarchical RL?
","['reinforcement-learning', 'learning-algorithms', 'hierarchical-rl']",
DDPG doesn't converge for MountainCarContinuous-v0 gym environment,"
I am trying to implement Deep Deterministic policy gradient algorithm by referring to the paper Continuous Control using Deep Reinforcement Learning on the MountainCarContinuous-v0 gym environment. I am using 2 hidden Linear layers of size 32 for both the actor and the critic networks with ReLU activations and a Tanh activation for the output layer of the actor network. However, for some reason, algorithm doesn't seem to converge for some reason. I tried tuning the hyperparameters to no success.

Code

import copy
import random
from collections import deque, namedtuple

import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim

""""""
Hyperparameters:

actor_layer_sizes
critic_layer_sizes
max_buffer_size
polyak_constant
max_time_steps
max_episodes
actor_lr
critic_lr
GAMMA
update_after
batch_size
""""""

device = torch.device(""cpu"")
dtype = torch.double

Transition = namedtuple(
    ""Transition"", (""state"", ""action"", ""reward"", ""next_state"", ""done"")
)


class agent:
    def __init__(
        self,
        env,
        actor_layer_sizes=[32, 32],
        critic_layer_sizes=[32, 32],
        max_buffer_size=2500,
    ):
        self.env = env
        (
            self.actor,
            self.critic,
            self.target_actor,
            self.target_critic,
        ) = self.make_models(actor_layer_sizes, critic_layer_sizes)
        self.replay_buffer = deque(maxlen=max_buffer_size)
        self.max_buffer_size = max_buffer_size

    def make_models(self, actor_layer_sizes, critic_layer_sizes):
        actor = (
            nn.Sequential(
                nn.Linear(
                    self.env.observation_space.shape[0],
                    actor_layer_sizes[0],
                ),
                nn.ReLU(),
                nn.Linear(actor_layer_sizes[0], actor_layer_sizes[1]),
                nn.ReLU(),
                nn.Linear(
                    actor_layer_sizes[1], self.env.action_space.shape[0]
                ), nn.Tanh()
            )
            .to(device)
            .to(dtype)
        )

        critic = (
            nn.Sequential(
                nn.Linear(
                    self.env.observation_space.shape[0]
                    + self.env.action_space.shape[0],
                    critic_layer_sizes[0],
                ),
                nn.ReLU(),
                nn.Linear(critic_layer_sizes[0], critic_layer_sizes[1]),
                nn.ReLU(),
                nn.Linear(critic_layer_sizes[1], 1),
            )
            .to(device)
            .to(dtype)
        )

        target_actor = copy.deepcopy(actor)    # Create a target actor network

        target_critic = copy.deepcopy(critic)   # Create a target critic network

        return actor, critic, target_actor, target_critic

    def select_action(self, state, noise_factor):         # Selects an action in exploratory manner
      with torch.no_grad():
        noisy_action = self.actor(state) + noise_factor * torch.randn(size = self.env.action_space.shape, device=device, dtype=dtype)
        action = torch.clamp(noisy_action, self.env.action_space.low[0], self.env.action_space.high[0])

        return action

    def store_transition(self, state, action, reward, next_state, done):             # Stores the transition to the replay buffer with a default maximum capacity of 2500
        if len(self.replay_buffer) < self.max_buffer_size:
            self.replay_buffer.append(
                Transition(state, action, reward, next_state, done)
            )
        else:
            self.replay_buffer.popleft()
            self.replay_buffer.append(
                Transition(state, action, reward, next_state, done)
            )

    def sample_batch(self, batch_size=128):                                            # Samples a random batch of transitions for training
      return Transition(
            *[torch.cat(i) for i in [*zip(*random.sample(self.replay_buffer, min(len(self.replay_buffer), batch_size)))]]
        )


    def train(
        self,
        GAMMA=0.99,
        actor_lr=0.001,
        critic_lr=0.001,
        polyak_constant=0.99,
        max_time_steps=5000,
        max_episodes=200,
        update_after=1,
        batch_size=128,
        noise_factor=0.2,
    ):
        
        self.train_rewards_list = []
        actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)
        critic_optimizer = optim.Adam(
            self.critic.parameters(), lr=critic_lr
        )
        print(""Starting Training:\n"")
        for e in range(max_episodes):
            state = self.env.reset()
            state = torch.tensor(state, device=device, dtype=dtype).unsqueeze(0)
            episode_reward = 0
            for t in range(max_time_steps):
                #self.env.render()
                action = self.select_action(state, noise_factor)               
                next_state, reward, done, _ = self.env.step(action[0])         # Sample a transition
                episode_reward += reward

                next_state = torch.tensor(next_state, device=device, dtype=dtype).unsqueeze(0)
                reward = torch.tensor(
                    [reward], device=device, dtype=dtype
                ).unsqueeze(0)
                done = torch.tensor(
                    [done], device=device, dtype=dtype
                ).unsqueeze(0)

                self.store_transition(                               
                    state, action, reward, next_state, done
                )                # Store the transition in the replay buffer

                state = next_state
                
                sample_batch = self.sample_batch(128)

                with torch.no_grad():                 # Determine the target for the critic to train on
                  target = sample_batch.reward + (1 - sample_batch.done) * GAMMA * self.target_critic(torch.cat((sample_batch.next_state, self.target_actor(sample_batch.next_state)), dim=1))
                
                # Train the critic on the sampled batch
                critic_loss = nn.MSELoss()(
                    target,
                    self.critic(
                        torch.cat(
                            (sample_batch.state, sample_batch.action), dim=1
                        )
                    ),
                )

                critic_optimizer.zero_grad()
                critic_loss.backward()
                critic_optimizer.step()

                actor_loss = -1 * torch.mean(
                  self.critic(torch.cat((sample_batch.state, self.actor(sample_batch.state)), dim=1))
                  )

                #Train the actor  
                actor_optimizer.zero_grad()
                actor_loss.backward()
                actor_optimizer.step()
                

                #if (((t + 1) % update_after) == 0):
                for actor_param, target_actor_param in zip(self.actor.parameters(), self.target_actor.parameters()):
                  target_actor_param.data = polyak_constant * actor_param.data + (1 - polyak_constant) * target_actor_param.data
                  
                for critic_param, target_critic_param in zip(self.critic.parameters(), self.target_critic.parameters()):
                  target_critic_param.data = polyak_constant * critic_param.data + (1 - polyak_constant) * target_critic_param.data

                if done:
                    print(
                        ""Completed episode {}/{}"".format(
                            e + 1, max_episodes
                        )
                    )
                    break

            self.train_rewards_list.append(episode_reward)

        self.env.close()
        print(self.train_rewards_list)

    def plot(self, plot_type):
        if (plot_type == ""train""):
            plt.plot(self.train_rewards_list)
            plt.show()
        elif (plot_type == ""test""):
            plt.plot(self.test_rewards_list)
            plt.show()
        else:
            print(""\nInvalid plot type"")


Train code snippet

import gym

env = gym.make(""MountainCarContinuous-v0"")

myagent = agent(env)
myagent.train(max_episodes=150)
myagent.plot(""train"")

The figure below shows the plot for episode reward vs episode number:

","['reinforcement-learning', 'python', 'policy-gradients', 'ddpg', 'gym']",I had to change the actions selection function for this and tune some hyper-parameters. Here's what I did to make it converge:This is the plot that I get now after training it for 75 episodes :
When do SARSA and Q-Learning converge to optimal Q values?,"
Here's another interesting multiple-choice question that puzzles me a bit.

In tabular MDPs, if using a decision policy that visits all states an infinite number of times, and in each state, randomly selects an action, then:

Q-learning will converge to the optimal Q-values
SARSA will converge to the optimal Q-values
Q-learning is learning off-policy
SARSA is learning off-policy


My thoughts, and question: Since the actions are being sampled randomly from the action space, learning definitely seems to be off-policy (correct me if I'm wrong, please!). So that rules 3. and 4. as incorrect. Coming to the first two options, I'm not quite sure whether Q-learning and/or SARSA would converge in this case. All that I'm able to understand from the question is that the agent explores more than it exploits, since it visits all states (an infinite number of times) and also takes random actions (and not the best action!). How can this piece of information help me deduce if either process converges to the optimal Q-values or not?

Source: Slide 2/55
","['reinforcement-learning', 'q-learning', 'convergence', 'sarsa']","The true answers are 1 and 3.1 is true because the required conditions for tabular Q-learning to converge is that each state action pair will be visited infinitely often, and Q-learning learns directly about the greedy policy, $\pi(a|s) := \arg \max_a Q_\pi(s,a)$,  and because Q-learning converges to the optimal Q-value function we know that the policy will be optimal (because the optimal policy is the greedy policy wrt the optimal Q-function).3 is true because Q-learning is by definition an off-policy algorithm, because we learn about the greedy policy whilst following some arbitrary policy.2 is false because SARSA is on-policy, so it will be learning the Q-function under the random policy.4 is false because SARSA is strictly on-policy, for reasons analogous to why Q-learning is off-policy."
experiences on using genetic algorithms as a way to improve neural networks?,"
I wonder if there is research, patents, or libraries using Genetic algorithms (GA) to improve Neural Networks. I don't find anything in the subject. For example:

use GA to find better parameters in a NN. So the chromosome will be [learning rate, activation function, layers number, layers size, dropout factor] and the fit function minimize computational cost to reach NN 95% accuracy.
use GA to mix your NN input data and generate new data to adjust.
use GA to mix several small NN, different types, and find the perfect mix for better predictions.

","['neural-networks', 'genetic-algorithms']",
Can a computer make a proof by induction?,"
Can a computer solve the following problem, i.e. make a proof by induction? And why?

Prove by induction that $$\sum_{k=1}^nk^3=\left(\frac{n(n+1)}{2}\right)^2, \, \, \, \forall n\in\mathbb N .$$

I'm doing a Ph.D. in pure maths. I love coding when I wanna have some fun, but I've never got too far in this field. I say my background because maybe there's someone who wants to explain this in a more abstract language there's a chance that I will understand it.
","['math', 'proofs', 'automated-theorem-proving']","It is possible for some classes of problems. For instance, WolframAlpha can generate an induction proof to the problem posed in the question.According to the author of this proof generator, he built a library of pattern-matched proofs to generate the proofs. More details about his approach can be find in his write-up about the problem.Other alternative (thought not induction-based) for automatically verifying these kind of identities (in special, hypergeometric identities) is by using algorithms such as Zeilberger's method along with the HYPER algorithm, both described in the excellent book A=B, currently available for free by one of its co-authors."
Understanding graphs of the mean square error: relationships between val loss and train loss,"
I am currently working with some models aimed at predicting time series (89 days for training, 22 for testing), including a CNN LSTM and a convLSTM.
When training these models, I had the following scenario:


In the first case, it is possible to see the val loss moving more sharply away from the train loss. In the second case, it seems to me that this also happens, but in a much smoother way.
What do these graphs mean? What causes these situations to occur? If they are problematic situations, is it possible to correct them? If so, how?
","['neural-networks', 'deep-learning', 'training', 'recurrent-neural-networks']",
What are some (deep) reinforcement learning books for beginners? [duplicate],"







This question already has answers here:
                                
                            




What introductory books to reinforcement learning do you know, and how do they approach this topic?

                                (3 answers)
                            

Closed 2 years ago.



What are some books on reinforcement learning (RL) and deep RL for beginners?
I'm looking for something as friendly as the head first series, that breaks down every single thing.
","['reinforcement-learning', 'deep-rl', 'reference-request', 'resource-request']","Reinforcement Learning: An Introduction by Richard Sutton and Andrew Barto is undoubtedly one of the best books, to begin with. Despite its age, the book is still the canonical introduction to reinforcement learning. It does require some patience, but I think it's very approachable and rigorous at the same time!"
Get Neural Network to predict a tag/class on a certain word using the surrounding words as context [PyTorch]?,"
I am somewhat a novice at the topic of Neural Netoworks and PyTorch.
I am trying to create a model that takes a word (that I have modified very slightly) and a 'window' of context around it and predicts one of 5 tags (the tags relate to what sort of action I should perform on that word to get its correct form).
For example, here's what I would call a window of size 7 and it's tag (what it means isn't too important, it's just the 'target'):
        Sentence                    Label
here is a sentence for my network     N

sentence is the word that I want the network to predict the label for, but the 3 words on either side provide contextual meaning. My problem is, how would I get a network to know I want it to predict for that central word but not outright ignore the others? I am familiar with more normal NLP tasks such as NMT and character level classification.
I have already gotten my dataset 'padded' out so they're all of equal size.
Any help is appreciated
","['neural-networks', 'natural-language-processing', 'python', 'pytorch']","You may want to take a look at this article, but I'll summarize. You can use BERT (or some other tool) to make embeddings of every word in every sentence. Then for each word, make a contextualized embedding vector using the rest of the sentence. bert-embedding does all of this itself. Then keep the embedding vector for the important words.For each important word, you would then have two pieces of information: the embedding vector and the correct label (which could easily be made into an integer from $0$ to $4$). Depending on the size of the embedding vectors, you could use PCA to reduce the size, although this may not be needed. Using this data, you can then train a neural network or use a k-nearest neighbors classifier.There is more information in the article, which I suggest that you read. They do a better job explaining than me and also have some actual code you may want to look at."
How does DQN convergence work in reinforcement learning,"
In supervised learning we have an unbiased target value, but in reinforcement learning this isn’t the case
The network predicts its own target value, now how exactly does it converge if the network predicts its target value
Can someone explain this to me ??

","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']",
How to run a Monte Carlo Tree Search MCTS for stochastic environment?,"
For MCTS there is an expansion phase where we make a move and list down all the next states. But this is complicated by the fact that for some games, after making the move, there is a stochastic change to the environment. Consider the game 2048, after I make a move, random tile is generated. So the state of the world after my next move is a mix of possibilities!
How does MCTS work in a stochastic environment? I am having trouble understanding how to keep track of the expansion, do I expand all stochastic possibilities and weight the return via their chance of happening?
",['monte-carlo-tree-search'],
What is the computational complexity of the forward pass of a convolutional neural network?,"
How do I determine the computational complexity (big-O notation) of the forward pass of a convolutional neural network?
Let's assume for simplicity that we use zero-padding such that the input size and the output size are the same.
","['convolutional-neural-networks', 'time-complexity', 'computational-complexity', 'space-complexity', 'forward-pass']",
"How to combine two differently equally important signals into the reward function, that have different scales?","
I have two signals that I want to use to model my reward.
The first one is the CPU TIME: running mean from this diagram:

The second one is the MAX RESIDUAL from this diagram:

Since they are both equally important, I can weight them together like this:
$r = w_\rho \rho + w_\tau \tau$
where $r$ is the reward function, $\tau$ is the CPU TIME: running mean, and $\rho$ is the MAX RESIDUAL. The problem is, how to set the weights $w_\tau,w_\rho$ to make the contributions equally important if $\rho$ and $\tau$ are on very different scales?
RL algorithms will learn policies based on increases/decreases of the reward, and if one signal has values that are much smaller than the other, it will influence the reward less, if this is done in a wrong way.
On the other hand, if the algorithm converges, they must be on different scales, as I want the CPU time to go ideally to $0$, and residuals $\rho$ to be minimized as well.
Modeling the reward function is a crucial RL step, because it decides what the algorithm will in fact optimize. How are examples like these handled? Are there any best practices for this? Also, what happens when there are $n$ such signals, that have to be combined with ""equally important"" weighting into a reward function?
Basing the weights $w$ on the current signal values is possible to define their reward, but then the reward contributions won't see $\max(\rho), \min(\rho)$ and $\max(\tau), \min(\tau)$ over time.
So, how do you do feature scaling for reward signals?
","['reinforcement-learning', 'deep-rl', 'reward-design', 'reward-functions']",
Why does TD Learning require Markovian domains?,"
One of my friends and I were discussing the differences between Dynamic Programming, Monte-Carlo, and Temporal Difference (TD) Learning as policy evaluation methods - and we agreed on the fact that Dynamic Programming requires the Markov assumption while Monte-Carlo policy evaluation does not.
However, he also pointed out that Temporal Difference Learning cannot handle non-Markovian domains, i.e. it depends on the Markov assumption. Why is it so?
The way I understand it, the TD learning update is, in essence, the same as the Monte-Carlo update, except for the fact that the return instead of being calculated using the entire trajectory, is bootstrapped from the previous estimate of the value function, i.e. we can update the value as soon as we encounter a $(s,a,r,s')$ tuple, we don't have to wait for the episode (if finite) to terminate.
Where is the Markov assumption being used here, i.e the future is independent of the past given the present?
","['reinforcement-learning', 'monte-carlo-methods', 'temporal-difference-methods', 'markov-property', 'dynamic-programming']","The Markov assumption is used when deriving the Bellman equation for state values:$$v(s) = \sum_a \pi(a|s)\sum_{r,s'} p(r,s'|s,a)(r + \gamma v(s'))$$One requirement for this equation to hold is that $p(r,s'|s,a)$ is consistent. The current state $s$ is a key argument of that function. There is no adjustment for history of previous states, actions or rewards. This is the same as requiring the Markov trait for state, i.e. that $s$ holds all information necessary to predict outcome probabilities of the next step.The one step TD target that is sampled in basic TD learning is simply the inner part of this:$$G_{t:t+1} = R_{t+1} + \gamma \hat{v}(S_{t+1})$$which when sampled is equal to $v(s)$ in expectation *, when $S_t = s$. That is, when you measure a single instance of the TD target and use it to update a value function, you implicitly assume that the values or $r_{t+1}$ and $s_{t+1}$ that you observed occur with probabilities determined by $\pi(a|s)$ and $p(r,s'|s,a)$ as shown by the Bellman equation.So the theory behind TD learning uses the Markov assumption, otherwise the sampled TD targets would be incorrect.In practice you can get away with slightly non-Markov environments - most measurements of state for machinery are approximations that ignore details at some level, for instance, and TD learning can solve optimal control in many robotics environments. However, Monte Carlo methods are more robust against state representations that are not fully Markov.* Technically this sample is biased because $\hat{v}(S_{t+1})$ is not correct when learning starts. The bias reduces over time and multiple updates. So the expected value during learning is approximately the same as the true value as shown by the Bellman equation."
Why are state-values alone not sufficient in determining a policy (without a model)?,"

""If a model is not available, then it is particularly useful to estimate action values (the
values of state-action pairs) rather than state values. With a model, state values alone are
sufficient to determine a policy; one simply looks ahead one step and chooses whichever
action leads to the best combination of reward and next state, as we did in the chapter on
DP. Without a model, however, state values alone are not sufficient. One must explicitly
estimate the value of each action in order for the values to be useful in suggesting a policy.""

The above extract is from Sutton and Barto's Reinforcement Learning, Section 5.2 - part of the chapter on Monte Carlo Methods.
Could someone please explain in some more detail, as to why it is necessary to determine the value of each action (i.e. state-values alone are not sufficient) for suggesting a policy in a model-free setting?

P.S.
From what I know, state-values basically refer to the expected return one gets when starting from a state (we know that we'll reach a terminal state, since we're dealing with Monte Carlo methods which, at least in the book, look at only episodic MDPs). That being said, why is it not possible to suggest a policy solely on the basis of state-values; why do we need state-action values? I'm a little confused, it'd really help if someone could clear it up.
","['reinforcement-learning', 'monte-carlo-methods', 'model-free-methods']","why is it not possible to suggest a policy solely on the basis of state-values; why do we need state-action values?A policy function takes state as an argument and returns an action $a = \pi(s)$, or it may return a probability distribution over actions $\mathbf{Pr}\{A_t=a|S_t=s \} =\pi(a|s)$.In order to do this rationally, an agent needs to use the knowledge it has gained to select the best action. In value-based methods, the agent needs to identify the action that has the highest expected return. As an aside, whilst learning it may not take that action because it has decided to explore, but if it is not capable of even identifying a best action then there is no hope of it ever finding an optimal policy, and it cannot even perform $\epsilon$-greedy action selection, which is a very basic exploration approach.If you use an action value estimate, then the agent can select the greedy action simply:$$\pi(s) = \text{argmax}_a Q(s,a)$$If you have state values, then the agent can select the greedy action directly only if it knows the model distribution $p(r,s'|s,a)$:$$\pi(s) = \text{argmax}_a \sum_{r,s'}p(r,s'|s,a)(r + \gamma V(s'))$$In other words, to find the best action to take the agent needs to look ahead a time step to find out what the distribution of next states would be following that action. If the only values the agent knows are state values, this is the only way the agent can determine the impact of any specific action.Although there are alternatives to this specific equation, there is no alternative that does not use a model in some form. For instance, if you can simulate the environment, you could simulate taking each action in turn, and look over multiple simulation runs to see which choice ends up with the best $(r + \gamma V(s'))$ on average. That would be a type of planning, and perhaps the start of a more sophisticated approach such as MCTS. However, that simulation is a model - it needs access to the transition probabilities in some form in order to correctly run.It is possible to have an entirely separate policy function that you train alongside a state value function. This is the basis of Actor-Critic methods, which make use of policy gradients to adjust the policy function, and one of the value-based methods, such as TD learning, to learn a value function that assists with calculating the updates to the policy function. In that case you would not be using a value-based method on its own, so the quote from that part of Sutton & Barto does not apply."
Which is the best RL algo for continuous states but discrete action spaces problem,"
I am trying to train an AI with an environment where the states are continuous but the actions are discrete, that means I can not apply DDPG or TD3.
Can someone please help to let know what should be the best algorithm for discrete action spaces and is there any version of DDPG or TD3 which can be applied to discrete action spaces on partially observable MDPs.
","['ai-design', 'deep-rl', 'ddpg', 'td3']",
Why is the reward in reinforcement learning always a scalar?,"
I'm reading Reinforcement Learning by Sutton & Barto, and in section 3.2 they state that the reward in a Markov decision process is always a scalar real number. At the same time, I've heard about the problem of assigning credit to an action for a reward. Wouldn't a vector reward make it easier for an agent to understand the effect of an action? Specifically, a vector in which different components represent different aspects of the reward. For example, an agent driving a car may have one reward component for driving smoothly and one for staying in the lane (and these are independent of each other).
","['reinforcement-learning', 'rewards', 'reward-functions', 'multi-objective-rl']","If you have multiple types of rewards (say, R1 and R2), then it is no longer clear what would be the optimal way to act: it can happen that one way of acting would maximize R1 and another way would maximize R2.  Therefore, optimal policies, value functions, etc., would all be undefined.  Of course, you could say that you want to maximize, for example, R1+R2, or 2R1+R2, etc.  But in that case, you're back at a scalar number again.It can still be helpful for other purposes to split up the reward into multiple components as you suggest, e.g., in a setup where you need to learn to predict these rewards.  But for the purpose of determining optimal actions, you need to boil it down into a single scalar."
How to overfit GANs with a single image,"
When designing CNN for image recogition a commonly used sainty check to see if a model is working/designed fine is to see if we are able to overfit the model with a very small subset of images.
I am trying out GANs. While designing GAN I took a dataset with just one image(full black image). I used the DCGAN implementation in pytorch websitecode link.
I tried training the model with this just one black image and even after training for 100s-1000 epochs. I am not able to overfit the model ie generate a black (or something close). All what is generated are random noise image as below
However the model does work well for celeba dataset(the one used in the tutorial). Which means the model is good. Can anybody help me why overfitting is very difficult/impossible when using a single image.
","['generative-adversarial-networks', 'overfitting']",
What is the state-of-the-art algorithm for neural style transfer?,"
I've read the paper A Neural Algorithm of Artistic Style by Gatys et. al. and I find the application of neural style transfer very fun.
I also read that Exploring the structure of a real-time, arbitrary neuralartistic stylization network by Ghiasi et. al. is a more modern approach to NST.
My question is whether the above paper by Ghiasi et. al. is still the state-of-the-art method in NST, or maybe new algorithms perform even more efficiently.
I shall precise that my goal is to deploy some NST algorithm on a web page as a fun project to apply some deep learning and learn about backend-frontend interactions.
","['deep-learning', 'computer-vision', 'deep-neural-networks', 'image-processing', 'image-generation']",
Is it a good idea to change the learning rate at each training step as a function of the loss?,"
Is it a good idea to change the learning rate at each training step as a function of the loss? i.e. for points with high loss value, put a high learning rate and for low loss value a low learning rate (using a tailored function)?
I know that the update of the parameters is done via $\gamma \nabla L$, where $\nabla L$ is the gradient and $\gamma$ the learning rate, and that points with high loss should correspond to a high gradient. Hence the dependency of the update of the parameters on the value of the loss should be already contained, although in a more indirect way. Is doing what I propose dangerous and/or useless?
","['machine-learning', 'learning-rate']",
Would a different learning rate for every neuron and layer mitigate or solve the vanishing gradient problem?,"
I'm interested in using the sigmoid (or tanh) activation function instead of RELU. I'm aware of RELU advantages on faster computation and no vanishing gradient problem. But about vanishing gradient, the main problem is about the backpropagation algorithm going to zero quickly if using sigmoid or tanh. So I would like to try to compensate this effect that affects deep layers with a variable learning rate for every layer, increasing the coefficient every time you go a layer deeper to compensate the vanishing gradient.
I have read about adaptive learning rate, but it seems to refer to a technique to change the learning rate on every epoch, I'm looking for a different learning rate for every layer, into any epoch.

Based on your experience, do you think that is a good effort to try?

Do you know some libraries I can use that already let you define the learning rate as a function and not a constant?

If such function exists, it will be better to define a simple function lr=(a*n)*0.001 where n is layer number, and a a multiplier based on experience, of we will need the inverse of the activation function to compensate enough the gradient vanishing?


","['deep-learning', 'backpropagation', 'activation-functions', 'learning-rate', 'vanishing-gradient-problem']",
Can I resize my images after labeling them?,"
Is it okay if I label my images with their original size and then resize them, or should I first resize them and then label them?
I mean do I need to recalibrate my labels if I resized my images?
","['object-detection', 'object-recognition', 'data-preprocessing']",
Why is the optimal policy for an infinite horizon MDP deterministic?,"
Could someone please help me gain some intuition as to why the optimal policy for a Markov Decision Process in the infinite horizon case (agent acts forever) is deterministic?
","['reinforcement-learning', 'markov-decision-process', 'policies', 'optimal-policy', 'optimality']","Suppose you learned your action-value function perfectly. Recall that the action-value function measures the expected return after taking a given action in a given state. Now, the goal when solving an MDP is to find a policy that maximizes expected returns. Suppose you're in state $s$. According to your action-value function, let's say actions $a$ maximizes the expected return. So, according to the goal of solving an MDP, the only action you would ever take from state $s$ is $a$. In other words $\pi(a'\mid s) = \mathbf{1}[a'=a]$, which is a deterministic policy.Now, you might argue that your action-value function will never be perfect. However, this just means you need more exploration, which can manifest itself as stochasticity in the policy. However, in the limit of infinite data, the optimal policy will be deterministic since the action-value function will have converged to the optimum."
Is there a neural network that accepts both the current input and previous output?,"
I am quite new to neural networks. I am trying to implement in Python a neural network having only one hidden layer with $N$ neurons and $1$ output layer.
The point is that I am analyzing time series and would like to use the output layer as the input of the next unit: by feeding the network with the input at time $t-1$ I obtain the output $O_{t-1}$ and, in the next step, I would like to use both the input at time $t$ and $O_{t-1}$, introducing a sort of auto-regression. I read that recurrent neural network are suitable to address this issue.
Anyway I cannot imagine how to implement a network in Keras that involves multilayer recurrence: all the references I found are linked to using the output of a layer as input of the same layer in the next step. Instead, I would like to include the output of the last layer (the output layer) in the inputs of the first hidden layer.
","['neural-networks', 'python', 'keras', 'recurrent-neural-networks']","You could just do this; concatenate your input_vector with zero's vector that has the size of your output. Then in the first pass you concatenate with the output instaid of the zero's vector. After that repeat.. At the end just compare (compute the loss) your entire output from t0 to t1 to your target and backprop.You might want to look into recurrent layers, these are layers that have connections back to themselves so that the network can learn what to ""remember"".
These have some problems with longer sequences, so the ""newer"" versions try to deal with that. (LSTM and GRU)
You can also use attention mechanisms if you're dealing with sequences. (basically you learn what parts of your input sequence to look at given a certain ""query"", in your case maybe the last timestep) (generally used in natural language processing) But it's a bit more exotic and complicated."
Keras model accuracy not improving beyond threshold,"
I am currently working on a public project for the National Weather Model. We are experimenting with using a recurrent neural network to replace the output of a quadratic formula that is in use. The aim of the experiment is to get a speedup in the computation by using a neural network to essentially mimic the output of the quadratic formula. We have achieved an accuracy of about +-.02 but would like to see that improve to +-.001 or so in order to make the outputs indiscernible from a usage standpoint. Despite changing or increasing the training data size, validation data size, number of layers, size of layers, optimizer, batch size, epoch number, normalizations, etc. we cannot seem to move past this level of accuracy. We have changed and tested every standard metric we can find on how to improve the model, but nothing improves the accuracy beyond that threshold.
The main question we have is whether or not Keras is rounding at some point between each layer or has some limiting factor on the backend limiting the model's significant figures in the output. The training data resolution should allow for a finer level of accuracy, but as stated before, any changes made the model cannot improve past what has been achieved. Any insight on what is holding the model back would be greatly appreciated and could help with applying this method elsewhere. The Github has a readme file explaining what is occurring in each file and how to run the model as this is still a work in progress. I would be happy to dive deeper into any aspect of the model as well.
https://github.com/NOAA-OWP/t-route/tree/testing/src/lookup_routing
","['deep-learning', 'tensorflow', 'keras', 'recurrent-neural-networks', 'accuracy']",
How much computing power does it cost to run GPT-3? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I know it cost around $4.3 million dollars to train, but how much computing power does it cost to run the finished program? IBM Watson chatbot AI only costs a few cents per chat message to use, OpeenAI Five seemed to run on a single gaming PC setup. So I'm wondering how much computing power does it need to run the finished ai program.
","['gpu', 'gpt']","I can't anwser your question on how much computing power you might need, but you'll need atleast a smallgrid to run the biggest model just looking at the memory requirments (175B parameters so 700GB of memory). The biggest gpu has 48 GB of vram  I've read that gtp-3 will come in eigth sizes, 125M to 175B parameters. So depending upon which one you run you'll need more or less computing power and memory. (https://lambdalabs.com/blog/demystifying-gpt-3/) 
For an idea of the size of the smallest, ""The smallest GPT-3 model is roughly the size of BERT-Base and RoBERTa-Base."""
How do I classify whether a document is legal or not given a set of keywords that appear only in legal documents?,"
Let's say that I want to classify whether a document is a legal document or not. I have a list of keywords that will be presented only in legal documents.
What is the proper way or algorithm to calculate probability based on this list?
","['machine-learning', 'classification']",
Does the selective search algorithm in object detection learn?,"
I am trying to get a better grasp of how object detection works. I (almost) completely understand the concept behind RPNs. However, I am a little bit confused with the selective search algorithm part. This algorithm does not really learn anything, as far as I understand.
So, for example, when I have an image containing people (even though my network does not need to classify these), will the selective search still propose these people to my network?
Of course, my CNN  has not learned to classify a human and will output a very low probability for every class (it did learn), and thus, this way, the human will not contain a bounding box.
Also, in further iterations of the R-CNN model, they proposed using regressors to improve the bounding box.
Does this mean that this part of the model got the CNNs feature maps, and, based on this, learned to output a bounding box (this way smaller instances of a detected object would get a smaller bounding box)?
So, in this first iteration, they probably did not need bounding boxes in the training data (since there was no way to learn the size of the bounding boxes and thus no need to find a loss function for this problem)?
Lastly, I understand that the selective search algorithm is an improvement on the sliding window algorithm. It tries to have a high recall, so having false positives is not bad, as long as we have all the true positives. Again, I do not seem to understand HOW this algorithm knows when it has the object it needs without really learning. Any intuïtive explanation or visual (I am a visual learner at first) on how this algorithm works is greatly appreciated.
","['convolutional-neural-networks', 'computer-vision', 'object-detection', 'r-cnn', 'selective-search']",
Can I use ML to discover via videos the best place to shoot in foosball?,"
I am a programmer, but just now attempting to enter the world of ML. I'm eyeballing a potential project/problem related to foosball.
Pro foosball is a thing believe it or not and I'm wondering if I can use decades worth of game footage to determine where defensive holes are most likely to be.
The way shooting works in pro foosball is you front pin the ball and walk it back and forth in front of the goal. The defense meanwhile is attempting to randomly move two men in front of the goal. Of course, our human brains are not truly random and this I'd like ML to help me understand and exploit.
Questions like, if I walk the ball left, then step right, historically where is the open hole likely to be?
If you'd like to better understand the nature of shooting, here is video of pro foosball: https://www.youtube.com/watch?v=uOdnqmwOQhA&t=16s
So what ML topics should I research and what strategies and tools do you recommend for creating such a model?
","['machine-learning', 'video-classification', 'games-of-chance']",
What ML algorithm should I use that suits this data?,"
What if I have some data, let's say I'm trying to answer if education level and IQ affect earnings, and I want to analyze this data and put in a regression model to predict earnings based on the IQ and education level. My confusion is, what if the data is not linear or polynomial? What if it's a mess but there are still patterns that the linear plane algorithm can't capture? How do I figure out if plotting all of the independent variables will form a line or a polynomial curve like here?

I mean, with one dependent and one independent variable it's easy because you can plot it and see, but in a situation with multiple independent variables... how do I figure out if the relationship is linear or something like this? How do I figure out if I should use a regression model?
Let's say I want to predict a store's daily revenue based on the day of the week, weather and the number of people arrived in the city. My data would look something like this:
+-----------+---------+----------------+---------+
| DAY       | WEATHER | PEOPLE ARRIVED | REVENUE |
+-----------+---------+----------------+---------+
| Monday    | Sunny   | 1115           | $500    |
+-----------+---------+----------------+---------+
| Tuesday   | Cloudy  | 808            | $250    |
+-----------+---------+----------------+---------+
| Wednesday | Sunny   | 450            | $300    |
+-----------+---------+----------------+---------+

I'm a bit confused about what ML algorithm I should use in such a scenario. I can represent the days of the week as (Monday - 1, Tuesday - 2, Wednesday - 3, etc.) and the weather as (Sunny - 1, Cloudy - 2, Normal - 3, etc.) but would a regression model work? I'm skeptical because I'm not sure if there's a linear relationship between the variables and I'm not sure if a hyperplane can create accurate representation of what's going on.
","['prediction', 'regression', 'linear-regression', 'non-linear-regression']","What you should do as part of your exploration is to learn various models of increasing complexity. Start from a simple linear model, ending in multi-layer neural networks (with non-linear activations of course). If the nonlinear models are better then that implies that your data do not follow a linear hyperplane.Also check this out for recent trends: https://machinelearningmastery.com/auto-sklearn-for-automated-machine-learning-in-python/"
How to generate labels for self-supervised training?,"
I've been reading a lot lately about self-supervised learning and I didn't understand very well how to generate the desired label for a given image.
Let's say that I have an image classification task, and I have very little labeled data.
How can I generate the target label from the other data in the dataset?
","['deep-learning', 'datasets', 'supervised-learning', 'self-supervised-learning', 'representation-learning']",
Why do the standard and deterministic Policy Gradient Theorems differ in their treatment of the derivatives of $R$ and the conditional probability?,"
I would like to understand the difference between the standard policy gradient theorem and the deterministic policy gradient theorem. These two theorem are quite different, although the only difference is whether the policy function is deterministic or stochastic.  I summarized the relevant steps of the theorems below. The policy function is $\pi$ which has parameters $\theta$.
Standard Policy Gradient
$$
\begin{aligned}
\dfrac{\partial V}{\partial \theta} &= \dfrac{\partial}{\partial \theta} \left[ \sum_a \pi(a|s) Q(a,s) \right]  \\
&= \sum_a \left[ \dfrac{\partial \pi(a|s)}{\partial \theta}  Q(a,s) + \pi(a|s) \dfrac{\partial Q(a,s)}{\partial \theta} \right] \\
&= \sum_a \left[ \dfrac{\partial \pi(a|s)}{\partial \theta}  Q(a,s) + \pi(a|s) \dfrac{\partial}{\partial \theta} \left[ R + \sum_{s'} \gamma p(s'|s,a) V(s') \right] \right]  \\
&= \sum_a \left[ \dfrac{\partial \pi(a|s)}{\partial \theta}  Q(a,s) + \pi(a|s) \gamma \sum_{s'} p(s'|s,a) \dfrac{\partial V(s') }{\partial \theta} \right]
\end{aligned}
$$
When one now expands next period's value function $V(s')$ again one can eventually reach the final policy gradient:
$$
\dfrac{\partial J}{\partial \theta} = \sum_s \rho(s) \sum_a \dfrac{\pi(a|s)}{\partial \theta}  Q(s,a)
$$
with $\rho$ being the stationary distribution. What I find particularly interesting is that there is no derivative of $R$ with respect to $\theta$ and also not of the probability distribution $p(s'|s,a)$ with respect to $\theta$. The derivation of the deterministic policy gradient theorem is different:
Deterministic Policy Gradient Theorem
$$
\begin{aligned}
\dfrac{\partial V}{\partial \theta} &= \dfrac{\partial}{\partial \theta} Q(\pi(s),s)  \\
&= \dfrac{\partial}{\partial \theta} \left[ R(s, \pi(s)) + \gamma \sum_{s'} p(s'|a,s) V(s') \right] \\
&= \dfrac{R(s, a)}{\partial a}\dfrac{\pi(s)}{\partial \theta} + \dfrac{\partial}{\partial \theta} \left[\gamma \sum_{s'} p(s'|a,s) V(s')   \right]  \\
&= \dfrac{R(s, a)}{\partial a}\dfrac{\pi(s)}{\partial \theta} + \gamma \sum_{s'}  \left[p(s'|\mu(s),s) \dfrac{V(s')}{\partial \theta} + \dfrac{\pi(s)}{\partial \theta} \dfrac{p(s'|s,a)}{\partial a} V(s')   \right]  \\
&= \dfrac{\pi(s)}{\partial \theta} \dfrac{\partial}{\partial a} \left[ R(s, a) +  p(s'|s,a) V(s') \right] +  \gamma  p(s'|\pi(s),s) \dfrac{V(s')}{\partial \theta}  \\
&= \dfrac{\pi(s)}{\partial \theta} \dfrac{\partial Q(s, a)}{\partial a} +  \gamma  p(s'|\pi(s),s) \dfrac{V(s')}{\partial \theta}  \\
\end{aligned}
$$
Again, one can obtain the finaly policy gradient by expanding next period's value function. The policy gradient is:
$$
\dfrac{\partial J}{\partial \theta} = \sum_s \rho(s) \dfrac{\pi(s)}{\partial \theta} \dfrac{\partial Q(s,a))}{\partial a}
$$
In contrast to the standard policy gradient, the equations contain derivatives of the reward function $R$ and the conditional probability $p(s'|s, a,)$ with respect to $a$.
Question
Why do the two theorems differ in their treatment of the derivatives of $R$ and the conditional probability? Does determinism in the policy function make such a difference for the derivatives?
","['reinforcement-learning', 'policy-gradients', 'policy-gradient-theorem', 'deterministic-pg-theorem', 'theorems']","In the policy gradient theorem, we don't need to write $r$ as a function of $a$ because the only time we explicitly 'see' $r$ is when we are taking the expectation with respect to the policy. For the first couple lines of the PG theorem we have
\begin{align}
\nabla v_\pi(s) &= \nabla \left[ \sum_a \pi(a|s) q_\pi (s,a) \right] \;, \\ &= \sum_a \left[ \nabla \pi(a|s) q_\pi(s,a) + \pi(a|s) \nabla\sum_{s',r} p(s',r|s,a)(r+ v_\pi(s')) \right] \; ;
\end{align}
you can see that we are taking expectation of $r$ with respect to the policy, so we don't need to write something like $r(s,\pi(a|s))$ (especially because this notation doesn't really make sense for a stochastic policy). This is why we don't need to take the derivative of $r$ with respect to the policy parameters. Now, the next line of the PG theorem is
$$\nabla v_\pi(s) = \sum_a \left[ \nabla \pi(a|s) q_\pi(s,a) + \pi(a|s)\sum_{s'} p(s'|s,a) \nabla v_\pi(s') \right] \; ;$$
so now we have an equation similar to the bellman equation in terms of the $\nabla v_\pi(s)$'s, so we can unroll this repeatedly meaning we never have to take an explicit derivative of the value function.For the deterministic gradient, this is a bit different. In general we have
$$v_\pi(s) = \mathbb{E}_\pi[Q(s,a)] = \sum_a \pi(a|s) Q(s,a)\;,$$
so for a deterministic policy (denoted by $\pi(s)$ which represents the action taken in state $s$) this becomes
$$v_\pi(s) = Q(s,\pi(s))$$
because the deterministic policy has 0 probability for all actions except one, where it has probability one.Now, in the deterministic policy gradient theorem we can write
$$\nabla v_\pi(s) = \nabla Q(s,\pi(s)) = \nabla \left(r(s, \pi(s)) + \sum_{s'} p(s'|s,a)v(s') \right)\;.$$We have to write $r$ explicitly as a function of $s,a$ now because we are not taking an expectation with respect to the actions because we have a deterministic policy. Now, if you replace where I have written $\nabla$ with the notation you have used for the derivatives you will arrive at the same result and you'll see why you need to use the chain rule, which I believe you understand because your question was more why don't we use the chain rule for the normal policy gradient, which I have hopefully explained -- it is essentially because of how an expectation over the action space works with a deterministic policy vs. a stochastic policy.Another way to think of this is as follows -- the term you're concerned with is obtained by expanding $\nabla q_\pi(s,a) = \nabla \sum_{s', r}p(s',r|s,a)(r(s,a) + v_\pi(s'))$. Because, by definition of the $Q$ function, we have conditioned on knowing $a,s$ then $a$ is completely independent of the policy in this scenario - we could even condition on an action that the policy would have 0 probability for - thus the derivative of $r$ with respect to the policy parameters is 0.However, in the deterministic policy gradient we are taking $\nabla q_\pi(s, \pi(s)) = \nabla \left(r(s, \pi(s)) + \sum_{s'} p(s'|s,a) v_\pi(s')\right)$ -- here $r$ clearly depends on the policy parameters because the action taken was the deterministic action given by the policy in the state $s$, thus the derivative wrt the policy parameters is not necessarily 0!"
"When past states contain useful information, does A3C perform better than TD3, given that TD3 does not use an LSTM?","
I am trying to build an AI that needs to have some information about the past states as well. Therefore, LSTMs are suitable for this.
Now, I want to know that for a problem/game like Breakout, where we require previous states as well, does A3C perform better than TD3, given that TD3 does not have LSTM?
Or without the LSTM TD3 should perform better than A3C despite the fact that A3C has LSTM in it.
","['reinforcement-learning', 'comparison', 'long-short-term-memory', 'actor-critic-methods', 'a3c']",
"How to deal with very, very small time-series?","
I have an ensemble of 231 time series, the largest among them being 14 lines long. The task at hand is to try to predict these time-series. But I'm finding this difficult due to the very small size of the data. Any suggestions about what algorithm to use? I'm thinking about going for a hidden markov model, but I don't know if that's a wise choice.
","['python', 'time-series', 'hidden-markov-model']","Of course depends on your type of data, but Holt-Winter models can have different degree of complexity and use moving average, trend, and seasonality. This is most useful if the data is not hierarchical, meaning that the time-series are independent from each other.
If time-series are relatives of each other then you can also try aggregating them, predict at aggregate level and then disaggregate. The following can be a good resource:https://otexts.com/fpp2/"
What are some best practices when trying to design a reward function?,"
Generally speaking, is there a best-practice procedure to follow when trying to define a reward function for a reinforcement-learning agent? What common pitfalls are there when defining the reward function, and how should you avoid them? What information from your problem should you take into consideration when going about it?
Let us presume that our environment is fully observable MDP.
","['reinforcement-learning', 'reward-design', 'reward-functions', 'reward-shaping', 'inverse-rl']","Designing a reward function is sometimes straightforward, if you have knowledge of the problem. For example, consider the game of chess. You know that you have three outcomes: win (good), loss (bad), or draw (neutral). So, you could reward the agent with $+1$ if it wins the game, $-1$ if it loses, and $0$ if it draws (or for any other situation).However, in certain cases, the specification of the reward function can be a difficult task [1, 2, 3] because there are many (often unknown) factors that could affect the performance of the RL agent. For example, consider the driving task, i.e. you want to teach an agent to drive e.g. a car. In this scenario, there are so many factors that affect the behavior of a driver. How can we incorporate and combine these factors in a reward function? How do we deal with unknown factors?So, often, designing a reward function is a trial-and-error and engineering process (so there is no magic formula that tells you how to design a reward function in all cases). More precisely, you define an initial reward function based on your knowledge of the problem, you observe how the agent performs, then tweak the reward function to achieve greater performance (for example, in terms of observable behavior, so not in terms of the collected reward; otherwise, this would be an easy problem: you could just design a reward function that gives infinite reward to the agent in all situations!). For example, if you have trained an RL agent to play chess, maybe you observed that the agent took a lot of time to converge (i.e. find the best policy to play the game), so you could design a new reward function that penalizes the agent for every non-win move (maybe it will hurry up!).Of course, this trial-and-error approach is not ideal, and it can sometimes be impractical (because maybe it takes a lot of time to train the agent) and lead to misspecified reward signals.It is well known that the misspecification of the reward function can have unintended and even dangerous consequences [5]. To overcome the misspecification of rewards or improve the reward functions, you have some options, such asLearning from demonstrations (aka apprenticeship learning), i.e. do not specify the reward function directly, but let the RL agent imitate another agent's behavior, either toIncorporate human feedback [9] in the RL algorithms (in an interactive manner)Transfer the information in the policy learned in another but similar environment to your environment (i.e. use some kind of transfer learning for RL [10])Of course, these solutions or approaches can also have their shortcomings. For example, interactive human feedback can be tedious.Regarding the common pitfalls, although reward shaping (i.e. augment the natural reward function with more rewards) is often suggested as a way to improve the convergence of RL algorithms, [4] states that reward shaping (and progress estimators) should be used cautiously.  If you want to perform reward shaping, you should probably be using potential-based reward shaping (which is guaranteed not to change the optimal policy).The MathWorks' article Define Reward Signals discusses continuous and discrete reward functions (this is also discussed in [4]), and addresses some of their advantages and disadvantages.Last but not least, the 2nd edition of the RL bible contains a section (17.4 Designing Reward Signals) completely dedicated to this topic.Another similar question was also asked here."
Why would the loss increase on a single fixed input?,"
I'm training a neural network on some input data. I know that loss increasing may be related to:

overfitting, if the loss increases on test data (while still decreases on training data)
oscillations near the optimal point, if the learning rate is too big

However, I find that while for some input data the net makes good predictions, for other data the loss continues to increase, even if I only train on one data point and if the learning rate is fairly low. To me, it's quite strange that performing the training on only one point the loss continues to increase and not decreases; in fact, the only reason I can find for this is a big learning rate.
Can you think about some other reason?
","['neural-networks', 'objective-functions']",
How can I find a specific word in an audio file?,"
I'm trying to train and use a neural network to detect a specific word in an audio file. The input of the neural network is an audio of 2-3 seconds duration, and the neural network must determine whether the input audio (the voice of a person) contains the word ""hello""  or not.
I do not know what kind of network to use. I used the SOM network, but I did not get the desired result. My training data contains a large number of voices that contain the word ""hello"".
Is there any python code for dis problem?
","['neural-networks', 'machine-learning', 'deep-learning', 'python', 'audio-processing']",
How to make sense of label propagation formula in graph neural networks?,"
In the label propagation algorithm in section 3.2.3, we know the label of some nodes and we want to predict the label for the rest of the nodes whose labels we don't know. The update formula for this is the following:
$$F(t+1) = \alpha SF(t) + (1-\alpha)Y $$
where $F(t)$ is predicted label from timestep t and $S$ can be considered as an adjacency matrix, $Y$ is the label for both the unlabeled data and labeled data. In the case of labeled data, we initialize $Y$ with ground truth and for the unlabeled data, we randomly initialize their label and assign it to $Y$.
Now, the most problematic part is I think the $Y$ matrix. Since I do not know the label of some nodes, so we initialize with some random value and keep Y as a constant throughout this iterative process.
We can calculate the optimal value of F directly using:
$$F^{*} = (I - \alpha S)^{-1}Y$$
But my question is, if we keep Y as a constant ( assign random numbers to unknown nodes as labels) what kind of sense does it make?
","['deep-learning', 'computer-vision', 'graph-theory', 'probabilistic-graphical-models']",
What are some solutions for dealing with time series data that are recorded at uneven intervals?,"
Let's say I have a time series data which is a bunch of observations that occur at different time stamps and intervals. For example, my observations come from a camera located at a traffic intersection. It only records when something occurs, like a car passes, a pedestrian crosses, etc... But otherwise doesn't record information.
I want to produce a LTSM NN (or some other memory based NN for time series data), but since my features don't occur at even time intervals, I am not sure how having a memory would help. For example let's consider two sequences:
Scenario 1:

At 1PM, I recorded a car passing.
At 105, some people cross.
At 150 some more people cross.
At 2PM, another car passes.

Scenario 2:

At 1 PM a car passes
At 2 PM a car passes

In the first scenario, the last car passed 3 observations ago. In the second scenario, the last car passed 1 observation ago. Yet in both scenarios, the last car passed 1 hour ago. I am afraid that any model would treat the last car passing in scenario 1 as 4 time periods ago and the last car passing in scenario 2 as 1 time period ago, even though in reality, the time difference is the same. My hypothesis is that the time difference is a very important feature, probably more so than the intermediate activity between the two car passing. In other words, knowing that the last car passed 1 hour ago is equal or likely more important than knowing that there were some people crossing in the last hour. With that said, knowing that people crossed is important too so I can't just remove that feature.
Another example of my issue can be scene below:
Scenario 1

1PM Car passes
2PM Car passes

Scenario 2

1PM Car passes
10PM Car passes

Once again, in my data set, this would be treated as adjacent observations, but in reality, the time gap is vastly different and thus, the two scenarios should be viewed as very dissimilar.
What are some ways to solve these issues?

I've thought of just expanding the data set by creating a row for every possible time stamp, but I don't think this is the right choice as it would make my dataset humongous and most rows would have 0s across the board. I have observations that occur in microseconds so it would just become a very sparse dataset.
It would be nice to include time difference as a feature, but I am not sure if there's a way to include a dynamic feature in your data set. For example, in the first scenario, at 105, the 1PM observation needs a feature that says this occurred 5 minute ago. But at 150, that feature needs to be changed to this occurred 50 minutes ago, and then at 2PM, that feature needs to now say that it occurred 1 hour ago.
Would the solution to just give the NN the raw data and not worry about it? When building a NN on word prediction, I guess if you give the model enough data, it'll learn the language even if the relevant word happened 10 paragraphs ago...  However, I am not sure if there are enough examples of the exact same sequences (even with the amount of data) for it to obtain the predictability I want.

Any ideas on ways to solve this problem while keeping in mind that the goal is to build a NN?  Another way to think about it is, the time when a data point occurred relative to when the prediction will be made, in my situation, is a crucial piece of information for prediction.
","['neural-networks', 'deep-learning', 'recurrent-neural-networks', 'time-series', 'feature-engineering']",
Why is tic-tac-toe considered a non-deterministic environment?,"
I have been reading about deterministic and stochastic environments, when I came up with an article that states that tic-tac-toe is a non-deterministic environment.
But why is that?
An action will lead to a known state of the game and an agent has full knowledge of the board and of its enemies' past moves.
","['game-ai', 'markov-decision-process', 'game-theory', 'pomdp', 'tic-tac-toe']","The game of TIC-TAC-TOE can be modelled as a non-deterministic Markov decision process (MDP) if, and only if:The opponent is considered part of the environment. This is a reasonable approach when the goal is to solve playing against a specific opponent.The opponent is using a stochastic policy. Stochastic policies are a  generalisation that include deterministic policies as a special case, so this is a reasonable default assumption.An action will lead to a known state of the game and an agent has full knowledge of the board and of Its enemies past moves.Whilst this is true, the next state and reward as observed by an agent may not be due to the postion it plays in (with the exception being if it wins or draws on that move), but the position after the opponent plays.It is also possible to frame TIC-TAC-TOE as a partially observed MDP (POMDP) if you consider the opponent to not have a fixed policy, but to be reacting to play so far, perhaps even learning from past games. In which case, the internal state of the opponent is the unknown part of the state. In standard game playing engines and in games of perfect information, this is resolved by assuming the opponent will make the best possible (or rational) move, which can be determined using a search process such as minimax. When there is imperfect information, such as in poker, it becomes much harder to allow for an opponent's action."
Is increasing software complexity the most likely bottleneck to the AI singularity?,"
From Wikipedia:

According to the most popular version of the singularity hypothesis, called intelligence explosion, an upgradable intelligent agent will eventually enter a ""runaway reaction"" of self-improvement cycles, each new and more intelligent generation appearing more and more rapidly, causing an ""explosion"" in intelligence and resulting in a powerful superintelligence that qualitatively far surpasses all human intelligence.

But what if the complexity of the problem of self-improving the software grows at a faster rate than the AGI intelligence self-improvement?
From experience we know that problems tend to be harder to solve at every iteration, with diminishing returns. Take as an example the theory of gravitation. Newtonian physics is relatively easy to formulate and covers the majority of high level gravitation phenomenas.
A more refined picture, like General Relativity, fill few holes in the theory with a huge increase in complexity. To describe black holes and primordial cosmology we need a theory of quantum gravity, which appears to require a further step in complexity.
What ""saved"" us so far is the economic growth of our civilisation, which allowed more and more scientist to focus on solving the next problems. It's true that the first AGI will have the luxury of being duplicated, being a software base intelligence, but at the same time is likely that the first AGI will be extremely compute intensive. But even assuming that we (or maybe it would better to say, they) have the hardware to run $10^{2}$ instances, if the complexity of every substantial self-improvement grows say by $10^{d}$x with $d=3$ while the improvement to the intelligence is only $10^{l}$x with $l=1$, the self-improvement cycle will quickly slow down.
So is increasing software complexity the most likely bottleneck to the AI singularity? And what are likely values for $d$ and $l$?
","['agi', 'singularity', 'kolmogorov-complexity']",
How to input dataset with multi-value properties?,"
I'm trying to learn to use AI, and so I've followed some basic tutorials like training an MLP to predict the price of a car given properties like its age and manufacturer. Now I want to see if I can do it myself, so I thought it'd be fun to predict what score I would give a movie given some data scraped off IMDB.
I immediately got stuck, because how do you deal with the cast? A single property with multiple values, where a particular actor may impact the final score (or a combination of actors - that's for the neurons to suss out).
I haven't found a way to do this when googling, but it may just be that I'm unfamiliar with the terminology. Or have I accidentally chosen a really difficult problem?
Note that I'm completely new to all of this, so if you have suggestions, please try to put it as simply as possible.
","['neural-networks', 'training']","You could use scikit-learn's MultiLabelBinarizer. It's essentially the multi-label equivalent of one-hot encoding. For each movie, create a vector of zeros, where each zero is associated with a particular actor. If an actor is in that movie, change their zero to a one. In the context of a neural network, think of it as each actor having their own input neuron, which will fire only if they are in that movie.The caveat is that to represent all actors, you'd need a rediculously long vector. In such cases, it's often sufficient to only look at the most common, say, 100 and ignore the rest. It intuitively makes sense that having a big actor who's done a lot of movies says more about the quality of a movie than whoever's playing Unimportant Bystander #3. This is actually how natural language processing represents words in the English language - take the top n and ignore the rest."
How can deep Q-learning converge if the targets may not be correct?,"
In deep Q-learning, $Q(s, a)$ and $Q'(s, a)$ are predicted or estimated by the neural network itself. In supervised learning, the target value is a true unbiased value. However, this isn't the case in reinforcement learning. So, how can we be sure that deep Q-learning converges? How do we know that the target Q values are accurate?
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'convergence']",
What does self-play in reinforcement learning lead to?,"

Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?

Above is an extract from Reinforcement Learning: An Introduction by Andrew Barto and Richard S. Sutton, and I wasn't quite sure about what the answer to the question would be, so thought of posting it here. The algorithm being referred to is the one for playing the game tic-tac-toe.
In my opinion, if the same algorithm plays both sides, it may end up assisting itself to win every time - and not really learn anything. What do you think?
","['reinforcement-learning', 'self-play']",
Can AlphaZero considered as Multi-Agent Deep Reinforcement Learning?,"
Can AlphaZero considered as Multi-Agent Deep Reinforcement Learning?
I could not find a clear answer on this. I would say yes it is Multi Agent Learning, as there are two Agents playing against each other.
","['deep-learning', 'deep-rl', 'multi-agent-systems', 'alphazero']","On one hand, you have an agent playing in an environment with another agent also evolving. This falls under the definition of Multi-Agent Learning, as can be seen with works such asMichael Bowling and Manuela Veloso. Multiagent learning using a variable learning
rate. Artificial Intelligence, 136(2):215 – 250, 2002.Michael Bowling. Convergence and no-regret in multiagent learning. In Proceedings of
the 17th International Conference on Neural Information Processing Systems, NIPS’04,
pages 209–216, Cambridge, MA, USA, 2004. MIT Press.M. D. Awheda and H. M. Schwartz. Exponential moving average q-learning algorithm. In
2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning
(ADPRL), pages 31–38, April 2013.Sherief Abdallah and Victor Lesser. A multiagent reinforcement learning algorithm with
non-linear dynamics. Journal of Artificial Intelligence Research, 33:521–549, 2008.However, you can also claim that you simply have a single agent learning on a non-stationary environment (the environment contains both the game rules and the opponent), and you simply learn on that basis. From this perspective, there is no multi-agent learning at all."
Is it possible to have a fixed trajectory size in the vanilla policy gradient algorithm?,"
In the concept of the vanilla policy gradient algorithm, is it possible for our trajectory size to be fixed?
For example, my environment is the space of embedded images (using a pre-trained encoder to take images in a space with smaller dimensions), the action I am performing is clustering via k-means algorithm and the reward is the silhouette score metric applied on the clustered images.
I am thinking to set batches of size 100 (dataset is MNIST and its trainset size is 60000). Then take the mean of them and consider this as one observation. Then feed this into the policy network to give me its logits which is an array of size 20 (20 discrete actions). These actions tell me the number of k clusters in the k-means clustering algorithm. One value for k is sampled and the k-means algorithm is applied on these 100 images and then reward is calculated.
I can set a constant number for trajectory sizes, for example, 20 and sum the rewards to get the R(trajectory). Is this possible in the context of the RL and policy gradient, or the trajectory's size cannot be fixed? Also, the action that our policy gives us must lead us to the next observation in the environment, but here images are independent of the policy network's parameters.
I wonder if I can utilize RL to implement this. I appreciate any hints.
","['reinforcement-learning', 'policy-gradients', 'pytorch']",
How to use text as an input for a neural network - regression problem? How many likes/claps an article will get,"
I am trying to predict the number of likes an article or a post will get using a NN.
I have a dataframe with ~70,000 rows and 2 columns: ""text"" (predictor - strings of text) and ""likes"" (target - continuous int variable). I've been reading on the approaches that are taken in NLP problems, but I feel somewhat lost as to what the input for the NN should look like.
Here is what I did so far:

Text cleaning: removing html tags, stop words, punctuation, etc...
Lower-casing the text column
Tokenization
Lemmatization
Stemming

I assigned the results to a new column, so now I have ""clean_text"" column with all the above applied to it. However, I'm not sure how to proceed.
In most NLP problems, I have noticed that people use word embeddings, but from what I have understood, it's a method used when attempting to predict the next word in a text. Learning word embeddings creates vectors for words that are similar to each other syntax-wise, and I fail to see how that can be used to derive the weight/impact of each word on the target variable in my case.
In addition, when I tried to generate a word embedding model using the Gensim library, it resulted in more than 50k words, which I think will make it too difficult or even impossible to onehot encode. Even then, I will have to one hot encode each row and then create a padding for all the rows to be of similar length to feed the NN model, but the length of each row in the new column I created ""clean_text"" varies significantly, so it will result in very big onehot encoded matrices that are kind of redundant.
Am I approaching this completely wrong? and what should I do?
","['neural-networks', 'machine-learning', 'deep-learning', 'natural-language-processing', 'recurrent-neural-networks']","I'll answer in a couple of stages.I feel somewhat lost as to what the input for the NN should look like.Your choices boil down to two options, each with their own multitude of variants:Vector Representation: Your input is a vector of the same size as your vocabulary where the elements represent the tokens in the input example. The most basic version of this is a bag-of-words (BOW) encoding with a 1 for each word that occurs in the input example and a 0 otherwise. Some other variants are (normalized) word counts or TF-IDF values. With this representation padding will not be necessary as each example will be encoded as a vector of the same size as the vocabulary. However, it suffers from a variety of issues: the input is high-dimensional and very sparse making learning difficult (as you note), it does not encode word order, and the individual word representations have little (TF-IDF) to no (BOW, counts) semantic information. It also limits your NN architecture to a feed-forward network, as more ""interesting"" architectures such a RNNs, CNNs, and transformers assume a matrix-like input, described below.Matrix Representation: Here your input representation is a matrix with each row being a vector (i.e. embedding) representation of the token at that index in the input example. How you actually get the pretrained embeddings into the model depends on a number of implementation-specific factors, but this stackoverflow question shows how to load embeddings from gensim into PyTorch. Here padding is necessary because the input examples will have variable numbers of tokens. This stackoverflow answer shows how to add zero padding in PyTorch.
This representation will be significantly better than the vector representation as it is relatively low-dimensional and non-sparse, it maintains word order, and using pretrained word-embeddings means your model will have access to semantic information. In fact, this last point leads to your next question.Learning word embeddings creates vectors for words that are similar to each other syntax-wise, and I fail to see how that can be used to derive the weight/impact of each word on the target variable in my case.Word embeddings are based on the assumptions of distributional semantics, the core tenet of which is often quoted as ""a word is characterized by the company it keeps"". That is, the meaning of a word is how it relates to other words. In the context of NLP, models can make better decisions because similar words are treated similarly from the get-go.For example, say that articles about furry pets get a lot of likes (entirely plausible if you ask me). However, the mentions of furry pets in these articles will be varied, including words like ""dog"", ""cat"", ""chinchilla"", ""poodle"", ""doggo"", ""good boy"", etc. An input representation that treats these mentions as completely distinct (such as BOW) will need to learn individual correlations between each word and the number of likes (that's a lot of learning). A well-trained word embedding, on the other hand, will be able to immediately group these mentions together and learn general correlations between groups of similar words and likes. Fair warning, this is a very imprecise description of why word embeddings work, but I hope it gives you some intuitive understanding.Finally, since you're doing regression, make sure you choose your objective function accordingly. Mean squared error would be my first try."
What are some applications where tree models perform better than neural networks?,"
Neural networks are known to be generally better modeling techniques as compared to tree-based models (such as decision trees). Are there any exceptions to this?
","['neural-networks', 'comparison', 'decision-trees', 'random-forests', 'gradient-boosting']",
How to avoid running out of solutions in genetic algorithm due to selection?,"
The genetic algorithm consists of 5 phases of which 4 are repeated:

Initial population (initially)
Fitness function
Selection
Crossover
Mutation

In the selection phase, the number of solutions decreases. How is it avoided to run out of the population before reaching a suitable solution?
","['genetic-algorithms', 'genetic-operators', 'selection-operators']",
How to handle images that don’t pertain to image classifier at all?,"
I am trying to create a CNN model that classifies if a person is wearing a seatbelt or not to verify they drive safely. I know to get images of people wearing seatbelts and people not wearing seatbelts, but I have a problem.
What if the person doesn’t submit a picture of them in a car at all? How do I construct the rest of the dataset to determine if that picture is an actual picture of a person wearing a seatbelt?
Do I insert completely random pictures in a different category? Do I classify images that don’t have a high confidence score as ""wrong"" images? Or leave it?
","['deep-learning', 'convolutional-neural-networks', 'training']","From what I understood, you want to be able to determine whether the input to your classifier is a valid picture or not. Where:For that you could build a Bayesian model from your current deep-learning model. Check out Pyro (from pytorch). The main idea behind it, is that the model will always predict a class: person wearing or not wearing a seatbelt. But since it is a Bayesian model it will also tell you the confidence of the prediction in terms of the similarity of the input with the input distribution for which the model was trained. In other words, it will also tell you ""how valid"" is that prediction, or ""how valid"" is the input for that prediction.Check out this post where it is very well explained. Hope it helps!"
Are there fundamental learning theories for developing an AI that imitates human behavior?,"
Most, if not all, AI systems do not imitate humans. Some of them out-perform humans. Examples include using AI to play a game, classification problems, auto-driving, and goal-oriented chatbots. Those tasks usually come with an easily and clearly defined value function, which is the objective function for the AI to optimize.
My question is: how is deep reinforcement learning, or related techniques, to be applied to an AI system that is designed to just imitate humans but not outperform humans?
Note this is different from a human-like system. Our objective here is to let the AI become a human rather than a superintelligence. For example, if a human consistently makes a mistake in image identification, then the AI system must also make the same mistake. Another example is the classic chatbot to pass the Turing test.
Is deep reinforcement learning useful in these kinds of tasks?
I find it is really hard to start with because the value function cannot be easily calculated.
What is some theory behind this?
","['deep-learning', 'reference-request', 'human-like']",
How to choice CNN architecture for stitching images,"
I decided to start learning neural networks by creating a bot for the game. One of the intermediate steps is to create a global map from a series of inaccurate overlapping sub-maps. This task can be solved using OpenCV, but this solution will be too limited and sensitive (in the future I intend to complicate the task and work directly with the map image, instead of binary masks).
I've tried the following options:

predict the position of a new map area within the global map. (as a probability distribution)

predict the new state of the global map from the old and new minimap.


I've tried a lot of options formulation of the problem of network architecture, including the idea of conjoined networks, but nothing gave any relevant results.
Some articles about solving similar problems:

Automatic Field-of-View Expansion using Deep Features and Image Stitching

JigsawNet: Shredded Image Reassembly using Convolutional Neural Network and Loop-based Composition

 Robust Cylindrical Panorama Stitching for Low-Texture Scenes Based on Image Alignment Using Deep Learning and Iterative Optimization


Here is an example of one of the options statement of the problem:

","['convolutional-neural-networks', 'image-processing']",
Finding whether an input column is missing,"
I am working on a problem similar to this one:(supervised, artificial data)
x=np.random.uniform(-100,100,10000)
y=np.random.uniform(-100,100,10000)
z=np.random.uniform(-100,100,10000)
a=np.random.uniform(-100,100,10000)
b=np.random.uniform(-100,100,10000)

i= x**2/1000 + 2*x*y/1000 + 4*np.sqrt(abs(z)) + z*a + 4*a + abs(b)**1.5 -1/((b+1)**2) * a + 10*y

Since I am not creating the data myself I want to make sure, that my customer provided all the relevant input features. Is there a way to find out, whether the input is complete and not lacking a feature, say ""a""? Obviously if the input is the same and the output differs it would be evidence of missing data but it isn't guaranteed that any two input samples are the same. Another way I thought of would be to use an autoencoder to find the dimension of the dataset(including the output) and hope it is exactly the input dimension but in my case it is also possible that there are redundant features. Is there any other way to check whether a function is computable from the given inputs?
","['machine-learning', 'supervised-learning', 'dimensionality']",
Is gradient descent scale invariant or not?,"
I know we should scale the input and output (assuming regression task) before we feed it to the neural network. Then the gradient descent will give the better minima much faster. But I have subtle confusion whether gradient descent with feature scale and without feature scale gives the same result or just gradient descent is not scale-invariant.
","['machine-learning', 'deep-learning']",
Is it a good idea to train a neural network to classify images without base-hypothesis?,"
I'm a relative beginner in deep learning (understand by that, I'm doing my first Kaggle competition right now, and I have loads to learn still) and I was just wondering something.
Let's say you have pathology/biopsy tissue images from patients dying from a disease A and patients dying from other causes (whatever causes actually but not related to disease A).
To date, I think we can say that nobody actually really knows what causes at the level of a biopsy the disease A.
My idea, as my group could have actually a lot of these biopsies for both groups, would be to use them to fuel a neural network.
Why would I do that? Biopsies images are rather complex images, and maybe some fine details are hard to guess for a human being, or maybe the sum of some details is actually important to tell whether disease A kills the patient or not. But again, I don't think anybody could come and say: on those tissue biopsy, the sign(s) for disease A are x, y, z.
My question then becomes a bit more theoretical: given the fact that you have enough data to actually give chances to the algorithm to find differences, is it a good idea to train a neural network without having actually any idea of what could differentiate the two groups?
Do you know examples of such a strategy? How hard is it afterwards - in the case of a rather good accuracy - to understand what makes it so recognisable?
","['neural-networks', 'deep-learning', 'image-recognition', 'research']",
What happens if our target network overestimates the value?,"
When we use DDQN, we often use the target network in case our online network overestimates a value, but this doesn't make sense to me, because

What happens if our target network is the one that overestimated a value, then we’d keep using that overestimated value

Why can't we use both target network for selection and evaluation


","['reinforcement-learning', 'dqn', 'deep-rl']",
How could logistic loss be used as loss function for an ANN?,"
Normally, in practice, people use those loss functions with minima, e.g. $L_1$ mean absolute loss, $L_2$ mean squared error, etc. All those come with a minimum to optimize to.

However, there's another thing, logistic loss, I'm reading about, but don't get it why the logistic function could be used as a loss function, given that it has the so-called minimum at infinity, but that isn't a normal minimum. Logistic loss function (black curve):

How can an optimizer minimize the logistic loss?
","['objective-functions', 'optimization', 'logistic-regression', 'neural-networks']","I see why you might be confused. First, the logistic-loss or log-loss is technically called cross-entropy loss. This function is very simple:$CE = -[y \log(p) + (1 - y) \log(1 - p)]$This tells basically if the predicted class $y$ was right $y=1$ then the loss is $CE=-\log(p)$, if the predicted class was not the right one then the loss is $CE=-\log(1-p)$.If we look at the function as a pure math concept we see that:$CE = f(x) = - \log(x)$And as you point out, that function is minima-unbounded as its domain is $D(f(x)) = [0, +\infty]$. You can check that in here:However the trick is that the inputs must be bounded, meaning, the inputs to the loss function must be in range $[0, 1]$. This bounding is achieved by applying a sigmoid activation function as the final ""layer"" of the network. Then if the inputs to the loss function are bounded the function has a clear minima.Check how the function looks in reality from one of the most important papers in loss function in the AI world: Focal Loss (I really encourage you to read it as the first section explains in detail the cross-entropy loss). The blue curve is the one you are looking for.Finally, you might want to review your log-loss/CE function since it should have an asymptote for $f(x=0) = \infty$"
"Who first coined the term ""deep learning""?","
AFAIK, deep learning became popular in 2012 with the victory of ImageNet Competition - Large Scale Visual Recognition Challenge 2012 where winners of this contest actually used deep learning techniques for optimizing the solution for object recognition.
Who first coined the term deep learning? Is there any published research paper that first used that term?
","['deep-learning', 'terminology', 'history']","The term was introduced to the machine learning and computer science community by Rina Dechter in Learning While Searching in Constraint-Satisfaction-Problems (1986) [1], where she writesDiscovering all minimal conflict-sets amounts to acquiring all the possible information out of a dead-end. Yet, such deep learning may require considerable amount of work.When deep learning is used in conjunction with restricting the level of learning we get deep first-order learning (identifying minimal conflict sets of size 1) and deep second-order learning (i.e. identifying minimal conflict-sets of sizes 1 and 2).Our experiments (implemented in LISP on a Symbolits LISP Machine) show that in most cases both performance measures improve as we move from shallow learning to deep learning and from first-order to second order.However, note that the term deep learning was used before 1986 in other contexts, for example, in [2]. Moreover, note that Rina Dechter did not use the term in the context of neural networks, which was probably used later."
Why are the Bellman operators contractions?,"
In these slides, it is written
\begin{align}
\left\|T^{\pi} V-T^{\pi} U\right\|_{\infty} & \leq \gamma\|V-U\|_{\infty} \tag{9} \label{9} \\
\|T V-T U\|_{\infty} & \leq \gamma\|V-U\|_{\infty} \tag{10} \label{10}
\end{align}
where

$F$ is the space of functions on domain $\mathbb{S}$.
$T^{\pi}: \mathbb{F} \mapsto \mathbb{F}$ is the Bellman
policy operator
$T: \mathbb{F} \mapsto \mathbb{F}$ is the Bellman
optimality operator

In slide 19, they say that equality $9$ follows from
\begin{align}
{\scriptsize
\left\| 
T^{\pi} V-T^{\pi} U 
\right\|_{\infty}
= 
\max_{s} \gamma \sum_{s^{\prime}} \operatorname{Pr}
\left(
s^{\prime} \mid s, \pi(s)
\right)
\left|
V\left(s^{\prime}\right) - U 
\left(s^{\prime}\right)
\right| \\
\leq \gamma \left(\sum \operatorname{Pr} \left(s^{\prime} \mid s, \pi(s)\right)\right) \max _{s^{\prime}}\left|V\left(s^{\prime}\right)-U\left(s^{\prime}\right)\right| \\
\leq \gamma\|U-V\|_{\infty}
}
\end{align}
Why is that? Can someone explain to me this derivation?
They also write that inequality \ref{10} follows from
\begin{align}
{\scriptsize
\|T V-T U\|_{\infty}
= \max_{s} 
\left| 
\max_{a}
\left\{ 
R(s, a) + \gamma \sum_{s^{\prime}} \operatorname{Pr}
\left(
s^{\prime} \mid s, a
\right) V
\left(
s^{\prime}
\right)
\right\}
-\max_{a} \left\{R(s, a)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} \mid s, a\right) U\left(s^{\prime}\right)\right\} \right| \\
\leq \max _{s, a}\left|R(s, a)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right)
-R(s, a)-\gamma \sum \operatorname{Pr}\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right) \right| \\
= 
\gamma \max _{s, a}\left|\sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} \mid s, a\right)\left(V\left(s^{\prime}\right)-U\left(s^{\prime}\right)\right)\right| \\
\leq \gamma\left(\sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} \mid s, a\right)\right) \max _{s^{\prime}}\left|\left(V\left(s^{\prime}\right)-U\left(s^{\prime}\right)\right)\right| \\
\leq 
\gamma\|V-U\|_{\infty}
}
\end{align}
Can someone explain to me also this derivation?
","['reinforcement-learning', 'proofs', 'policy-iteration', 'bellman-equations', 'bellman-operators']",
NLP: What is expected from the output of a perfect coreference system?,"
For instance, consider the following piece of text:
'The father of Richard is a very nice guy. He was born in a poor family. Because of that, Richard learnt very good values. Richard is also a very nice guy. However, Richard's mother embarasses the family. She was born rich and she does not know the real value of the money. She did not have to be a hard worker to succeed in life.""
How should a perfect coreference should work? Is this solution the perfect solution?
Cluster 1:
'The father of Richard' (first sentence) <-> 'He' (second sentence)

Cluster 2:
'Richard' (third sentence) <-> 'Richard' (forth sentence)

Cluster 3:
'Richard\'s mother' (fifth sentence) <-> She (sixth sentence) <-> she (sixth sentence) <-> She (seventh sentence)  

If I use the coreference library of spacy (neuralcoref), I get these clusters:
Clusters:
[Richard: [The father of Richard, He, Richard, Richard, Richard], a poor family: [a poor family, the family], Richard's mother: [Richard's mother, She, she, She]]

Note that this output says that ""Richard"" is the same for sentences that is true. However, ""He"" in the second sentence is not related to ""Richard"", but to his father. Also ""Richard"" and the ""Father of Richard"" are together in the same cluster. Furthermore, ""poor family"" and ""family"" should not come together. However, this is realy difficulty since in this case there is some level of ambiguity.'
I know that this is a very difficult problem. The point is not criticize this fantastic library. I am just trying to understand what I should expect as perfect result.
If I change a little the text:
'The mother of Richard is a very nice woman. She was born in a poor family. Because of that, Richard learnt very good values. Richard is a very nice guy. However, Richard's father embarasses the family. He was born rich and he does not know the real value of the money. He did not have to be a hard worker to succeed in life.'
The clusters are:
[Richard: [The mother of Richard, She, Richard, Richard, Richard, He, he, He], a poor family: [a poor family, the family]]

In this case, the clusters become stranger, since ""She"" and ""Richard"" are in the same cluster. Furthermore, the ""He"" related to the ""father of Richard"" belongs to the cluster, but not ""Richard's father"".
So, my question is:
What is the perfect result that I should expect from a ""perfect"" coreference system?
",['natural-language-processing'],
What exactly is the advantage of double DQN over DQN?,"
I started looking into the double DQN (DDQN). Apparently, the difference between DDQN and DQN is that in DDQN we use the main value network for action selection and the target network for outputting the Q values.
However, I don't understand why would this be beneficial, compared to the standard DQN. So, in simple terms, what exactly is the advantage of DDQN over DQN?
","['comparison', 'q-learning', 'dqn', 'deep-rl', 'double-dqn']","In $Q$-learning there is what is known as a maximisation bias. That is because the update target is $r + \gamma \max_a Q(s,a)$. If you slightly overestimate your $Q$-value then this error gets compounded (there is a nice example in the Sutton and Barto book that illustrates this). The idea behind tabular double $Q$-learning is to have two $Q$-networks, $Q_1,Q_2$, and you choose an action $a$ from them, e.g. from $Q_1 + Q_2$. You then flip a coin to decide which to update. If you choose to update $Q_1$ then the update target becomes $r + \gamma Q_2(s', \arg\max_a Q_1(s',a))$.The idea is that if you overshoot your estimate on one $Q$ network then having the second will hopefully control this bias when you would take the max.In Deep Double $Q$-learning the idea is essentially the same but instead of having to maintain and train two $Q$-networks, they use the target network from vanilla DQN to provide the target. To make this more concrete, the update target they use is
$$r + \gamma Q(s', \arg\max_aQ(s',a;\theta);\theta^-)\;,$$
where $Q(s,a;\theta^-)$ denotes the target network whose parameters are only updated to the current networks every $C$ time steps.As before, the idea is that if we have overestimated our value of being state $s'$ in our current network when taking the max action, using the target network to provide the target will help control for this bias.I will here explain maximisation bias from the simple example given from the Sutton and Barto book.The Markov Decision Process in the image is defined as follows: we start in state A and can take the 'right' action which gives us 0 reward and immediately leads to termination. If we choose 'left' we get 0 immediate reward where we then move to state B. From there, we have an arbitrary number of action we can take where they all lead to the terminal state and the reward is drawn from a Normal(-0.1,1) distribution.Clearly, the optimal action is always to move to the right from state A as this gives 0 expected future returns. Taking the left action will give a $\gamma \times -0.1$ expected future returns (the $\gamma$ is our discount factor).Now, if we got into state $B$ and took some random action our initial reward could be bigger than 0 -- after all it is drawn from a Normal(-0.1,1) distribution.Now, consider we are updating our $Q$-function for state A and taking the left action. Our update target will be $0 + \gamma \max_a Q(B,a)$. Because we are taking the max over all possible actions, this will lead to a positive reward and so we are backing up the belief of our expected future rewards from taking action left in state A to be something positive -- clearly this is wrong since we know it should be -0.1. This is what is known as the maximisation bias, because it gives us a kind of 'optimistic' estimate of the action value!I've attached an image below that shows the %age of time the agent chose the left action, which it shouldn't be choosing). As you can see, it takes normal $Q$-learning along time to even start to correct itself, whereas double $Q$-learning corrects the mistake almost immediately."
How to cluster data points such that the number of clusters is kept minimal and each cluster projects well onto a lower-dimensional subspace?,"
If I want to find a (linear) subspace onto which a data-set projects well, I can simply use PCA. However, often the data can project with much smaller error if I first separate it into a couple of classes and then perform PCA for each class individually. But what If I don't know what kind of classes there might be in my data and into how many classes it would make sense to split the data? What kind of machine learning algorithm can do this well?
Example:

If I'd just cluster first based on distance in the high-dimensional space, I would arrive at the bad clustering. There are 5 clusters and the green and red clusters don't project very well onto a 2D subspace.
As a human looking at the data, I see however that if I separate the data as indicated, red and blue will project very well onto a plane each and green will project very well onto a line, so I can run PCA for each group individually.
How can I automate this clustering based on how well it will project onto as low-imensional subspaces as possible?
Something like minimize E = SumOverClusters(SumOverPoints(SquaredDist(projected_point, original_point)) * (number_dims_projected / number_dims_original)) + C * number_of_clusters
What technique is well suited to do that?
(edit: while the example shows a 3d space, I'm more interested in doing that in about 64dimensional spaces)
","['clustering', 'dimensionality-reduction']",
What is the best way to make a deep reinforcement learning environment with a continuous 2D action space?,"
I understand that the actor-critic method is probably where I want to start because of how it works with continuous action spaces.
However, the problem I am trying to solve would require the action be a vector of 11 continuous values. When I go to design my training environment and the architecture of my DRL network, I am not sure how to map a vector of values to the state for the state-action pairs.
I am trying to use this article as a jumping off point, but I am not sure where to go: https://medium.com/@asteinbach/actor-critic-using-deep-rl-continuous-mountain-car-in-tensorflow-4c1fb2110f7c
","['machine-learning', 'reinforcement-learning', 'keras']",
What is eager learning and lazy learning?,"
What is the difference between eager learning and lazy learning?
How does eager learning or lazy learning help me build a neural network system? And how can I use it for any target function?
","['neural-networks', 'comparison', 'definitions']",
"How to predict the ""word"" based on the meaning in a document?","
What I mean to say is

For example, if I give the meaning of Apple from the dictionary as input to the program, it should give output as Apple.
Or I say My day to day job involves monitoring and managing the resources - the output should be Project management.

The meaning and the word could be a dictionary or it could be custom. I am looking for ideas and tools to go further on this.
","['natural-language-processing', 'training', 'python', 'natural-language-understanding']",
How are mujoco environments used for meta-rl?,"
Afaik, investigating meta reinforcement learning algorithms requires a collection of two or more environments which have similar structure but are still different enough.
When I read this paper it was unclear to me what the meta-training and meta-testing environments were.
For eg., a graph is given for Ant-Fwd-Bkwd showing its performance with number of gradient steps. I'm guessing these are the meta-testing performances. So, which environment was it 'meta-trained' on?
Was it meta-trained on the same Ant-Fwd-Bkwd environment?
","['reinforcement-learning', 'meta-learning']","According to this paper (PEARL):These locomotion task families require adaptation across reward
functions (walking direction for Half-CheetahFwd-Back, Ant-Fwd-Back,
Humanoid-Direc-2D, target velocity for Half-Cheetah-Vel, and goal
location for Ant-Goal2D) or across dynamics (random system parameters
for Walker-2D-Params).It looks like different versions of the same environment with differing reward functions are used. For eg., Forward direction might be rewarded positively in one version, Negative in another, Both directions rewarded positively in yet another."
Strange behavior of Q-learning agent after being trained,"
I built a simple X*Y grid world environment to learn and then trained my agent over it. All worked fine and the agent learned as well. Let me give some detail about the environment.
Environment:

A 4x4 grid world with episode starting at (0,0) and terminal state (3,3)
Four actions: Left, Up, Right, Down
The reward of -1 for moving into new state from the previous state to a new state. The reward of 0 when reaching the terminal state. The reward of -2 for bouncing off of the boundary
Epsilon-greedy scheme for action selection.

All works fine, and the following are the learning results of the agent.

Later I ran a test run of my TRAINED QL-agent where I used greedy action selection. All I could see in every episode was that my agent start from (0,0), take right to move to (1,0), and then take left to move back to (0,0) again and this goes on and on and on... I check the Q table and it makes sense because the Q-values for these actions justifies such behaviour. But this is not a practical agent should be doing.
","['reinforcement-learning', 'q-learning', 'environment', 'testing']",
How to train an LSTM with varying length input?,"
I have a dataset where each of the training instances is different in the length and the data is sequential. So, I design an LSTM but I am thinking about how to train the LSTM. In fixed-length data, we just keep all of the input in an array and pass it to the network, but here the case is different. I can not store varying length data in an array and I do not want to use padding to make it fixed length. Now, should I train the LSTM where each training instance are varying in length?
","['deep-learning', 'natural-language-processing', 'tensorflow']",
Multilabel stratified split for images/object detection,"
I am working on an object detection model and have thought of looking into stratified splits for the dataset.
Now since I am doing object detection I have a variable number of ""labels"" for every image because in each image there is variable number of occurrences for each object I am looking for (car, truck, motorbike, etc.).
Obviously single-label stratification does not apply.
From what I understand multi-label stratification is only applicable if there are basically label ""features"" that we know are always present, which does not seem the case here.
My question is... is there a way to perform stratified split in this case so that in each split there is roughly the same number of cars/trucks/bikes/etc.? (Or is it going to actually improve the results at all?)
","['datasets', 'data-preprocessing']",
What is Cognitive Intelligence?,"
Similarly to the question, What is artificial intelligence?

Cognitive Intelligence, as well as being a part of Artificial Intelligence, is an area that mainly covers the technology and tools that allow our apps, websites, and bots to see, hear, speak and understand the needs of the user through natural language

What is the definition of Cognitive Intelligence?
","['terminology', 'definitions', 'history', 'academia']","The term it comes from cognitive science - depending on the paradigm, it can have many meanings correlated with neuroscience as well as the brain.Activities based on this sphere is :The mentioned cognitive processes - activate the appropriate regions in the brain narrow cognitive intelligence - it's an effective response to in particular in unforeseen and uncertain situations.However, it requires the ability to set goals, learn, plan and think about your own thinking."
Using transformer but masking in reverse direction/smart sampling for desired final word?,"
I'm trying to generate rhymes, so it would be very helpful to have a language model where I could input a final word, and have it output a sequence of words that ends with that word.
I could train my own model and reverse the direction of the mask, but I was wondering if there was any way I could use a pretrained model but apply a different mask to achieve this goal.
If this isn't possible, what's the best way to sample a forward-predicting model to achieve the highest probability sentence ending with a particular word?
Thanks!
","['natural-language-processing', 'transformer']",
Looking for the proper algorithm to compress many lowres images of nearby locations,"
I have an optimization problem that I'm looking for the right algorithm to solve.
What I have: A large set of low-res 360 images that were taken on a regular grid within a certain area. each of these images is quite sparsely sampled and each of these 360 images has an accurate XYZ position assigned of its center. There are millions of these small images, clusters of close-by images obviously share a lot of information while images farther apart can be completely different.
What I want to do is to compress these small 360 images.
If two 360 images are close by each other, they can be 'warped' into each other by projecting it onto a sphere of finite distance and then moving that sphere (so a closeby 360 image can be a good aproximation of another 360 image when it has been warped that way).
Based on this idea, I want to compress these small low-res 360 images by replacing each of them with:

N (N being something like 2-5) indices into an archive of M (M being something like 50-500) different 'prototype' images (of possibly higher resolution than the low res 360 images), each of which has an XZY location assigned plus a radius
N blend weights

Such that if I want to reconstruct one of the small, sparsely sampled 360 images I take the N indices stored for this image, look-up the corresponding prototype images from the archive, warp them based on the radius of the archive image and the delta vector of archive XZY and compressed image XYZ location, and then blend the N prototype images based on the N blend weights (and possibly scale down in the prototype images are higher res)
I guess this goes into the direction of Eigen Faces, but with Eigen faces each compressed face has a weight stored for each eigen-face, whereas I want that each compressed sphere only has N non-zero weights.
So my input is:
a lot of small 360 images plus a XYZ location each
my output should be:

an archive of M ""prototype"" images, each assigned an XYZ location and a projection radius
all compressed spheres, with each sphere compressed to N indices and N weights

This seems to be some non-linear least squares problem, but I wonder if someone can point me into the right direction on how to solve this?
As a completely alternative approach I also looked into spherical harmonics, but with those I only get enough high-frequency details at l=6 which takes 36 coefficients which is too much and also too slow to decompress.
","['autoencoders', 'principal-component-analysis']",
How can I fetch ​exploration decay rate of an iterable Q-table in Python?,"
I have done creating the virtual environment, creating the Q-table, initializing the q-parameters, then I made a training module and stored it in a numpy array. After completion of training, I have updated the q-table and now I get the plots for the explorations But how can I code for rate decay? Here is my sample code for every step of the training module,
for step in range(max_steps): 
        exploration_rate_threshold = random.uniform(0,1)

        if exploration_rate_threshold > exploration_rate:
            action = np.argmax(q_table[state,:])
        else:
            action = env.action_space.sample()

",['q-learning'],Here is one way to calculate the exploration rate decay:
How can I update my Q-table in Python?,"
I want to implement this function on a voice searching application:
$$
Q(S, A) \leftarrow Q(S, A)+\alpha\left(R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right)
$$
And also restricted to use epsilon-greedy policy based on a given Q-function and epsilon. I simply need a $\epsilon$-greedy policy for updating my q-table.
",['q-learning'],"Just try returning a function that takes the state as an input and returns the probabilities for each action in the form of a numpy array of length of the action space (set of possible actions). Here, is one attempt:"
Why does a neuron in a multi-layer network need several input connections?,"
For example, if I have the following architecture:


Each neuron in the hidden layer has a connection from each one in the
input layer.
3 x 1 Input Matrix and a 4 x 3 weight matrix (for the backpropagation we have of course the transformed version 3 x 4)

But until now, I still don't understand what the point is that a neuron has 3 inputs (in the hidden layer of the example). It would work the same way, if I would only adjust one weight of the 3 connections.
But in the current case the information flows only distributed over several ""channels"", but what is the point?
With backpropagation, in some cases the weights are simply adjusted proportionally based on the error.
Or is it just done that way, because then you can better mathematically implement everything (with matrix multiplication and so on)?
Either my question is stupid or I have an error in my thinking and assume wrong ideas. Can someone please help me with the interpretation.
In tensorflow playground for example, I cut the connections (by setting the weight to 0), it just compansated it by changing the other still existing connection a bit more:

","['neural-networks', 'backpropagation', 'architecture', 'dense-layers']",
What is the memory complexity of the memory-efficient attention in Reformer?,"
When I read the paper, Reformer: The Efficient Transformer, I cannot get the same complexity of the memory-efficient method in Table 1 (p. 5), which summarizes time/memory complexity of scaled dot-product, memory efficient, and LSH attention.
The memory complexity of the memory-efficient method is as follow:
$$\max \left(b n_{h} l d_{k}, b n_{h} l^{2}\right)$$
$b$: batch size
$l$: sequence length
$n_h$: the number of attention head
$d_k$: the dimension of query or key
To the best of my knowledge, the memory-efficient method will do a loop for each query, therefore, the whole attention matrix will not show up.
So, shouldn't the memory complexity be $\max(b n_h l d_k, b n_h l)=(b n_h l d_k)$ instead of $\max(b n_h l d_k,b n_h l^2)$?
","['deep-learning', 'papers', 'transformer', 'space-complexity', 'reformer']",
Does Algorithmic Mechanism Design come under the field of AI?,"
I see many papers in AAMAS talk about artificial intelligence and mechanism design simultaneously. I was wondering, for the sake of being pedantic, is mechanism design could be classified under AI.
","['classification', 'terminology', 'game-theory']",
Is there a way to reduce the RMSE error when training a neural network to recognise MNIST digits using ANFIS?,"
I wanted to build a digit recognition neural network using MATLAB ANFIS kit.
I started by using the MNIST database and I figured out it's almost impossible to classify 784 dimension data using ANFIS. So, I reduced my data dimension from 784 to 13, using an autoencoder in Python. With the new data, I had about 80 percent accuracy in classification using a sequential model. I implemented my data in MATLAB too.
Since MATLAB treats the problem as a regression problem, I had about 1.5 RMSE after 10 epoch of learning, in both grid partitioning and subtractive clustering, and also the error curve almost seems constant in the process.
Is there any way that I can have less error?
","['neural-networks', 'training', 'matlab', 'mnist']",
How large should the corpus be to optimally retrain the GPT-2 model?,"
I just started working with the GPT-2 models and want to retrain one on a pretty narrow topic, so I have problems finding training material.
How large should the corpus be to optimally retrain the GPT-2 model? And what is the bare minimum size? Should it simply be as large as possible or can it flip over and make the model worse in some way?
I am also not certain how many steps you should let the retraining run. I have been using 6000 steps when testing, and it seems not much happens after that, loss only moved from 0.2 to 0.18 last 1000 steps.
","['deep-learning', 'natural-language-processing', 'tensorflow', 'training', 'gpt']",
Recommendations or resources for neural network/deep learning for time series application?,"
I know there are quite a few good deep learning books out there, but most explain neural networks and deep learning via application on images.  If there are examples/code, they are often done on the MNIST data set.
I was wondering if there's a book out there that goes in depth into neural networks and is equally well written but explains it on non-image data.  I am particularly interested in time series application, although some talk on cross-sectional application would be helpful as well.  I am particularly interested in learning about:

What types of layers/functions/structure are better suited for time-series data
Various models for time series and pros/cons/applications of each (Convoluted NN, LTSM, etc...)
Typical structures of your neural network (depth, sparse connections, etc...) that seem to work well on time series data
Special considerations or settings you should have in your neural network when working with time series data
Maybe some talk/examples on how time series prediction using traditional models like ARIMA, can be reproduced or done better using neural networks.  Or side by side comparison of pros/cons of using one vs the other.

Thanks!
","['neural-networks', 'recurrent-neural-networks', 'reference-request', 'time-series']",
Does this $\max$ mean that we need to maximize the regret in this regret formula?,"
I found that the regret in Online Machine Learning is stated as:
$$\operatorname{Regret}_{T}(h)=\sum_{t=1}^{T} l\left(p_{t}, y_{t}\right)-\sum_{t=1}^{T} l\left(h(x), y_{t}\right),$$
where $p_t$ is the answer of my algorithm to the question $x$ and $y_t$ is the right answer, while $h()$ is one of the hypotheses in the hypothesis space. Intuitively, as denoted in the paper, our objective is to minimize this Regret in order to optimize our algorithm, but in the following formula
$$
\operatorname{Regret}_{T}(\mathcal{H})=\max _{h^{\star} \in \mathcal{H}} \operatorname{Regret}_{T}\left(h^{\star}\right)
$$
they maximize this value. Am I interpreting the $max$ wrongly?
","['machine-learning', 'optimization', 'computational-learning-theory', 'online-learning']","Yes, you're interpreting the $\max$ there wrongly. In your second formula$$
\operatorname{Regret}_{T}(\mathcal{H})=\max _{h^{\star} \in \mathcal{H}} \operatorname{Regret}_{T}\left(h^{\star}\right) \label{1}\tag{1}
$$The sign $=$ means ""is defined as"", so maybe the following notation is less confusing$$
\operatorname{Regret}_{T}(\mathcal{H}) \triangleq \max _{h^{\star} \in \mathcal{H}} \operatorname{Regret}_{T}\left(h^{\star}\right)
$$In fact, in section 2.1 of the same paper, there's a similar but more detailed formula that should clarify the meaning of $\max$
$$
\operatorname{Regret}_{T}(\mathcal{H})=\max _{h \in \mathcal{H}}\left(\sum_{t=1}^{T}\left|p_{t}-y_{t}\right|-\sum_{t=1}^{T}\left|h\left(\mathbf{x}_{t}\right)-y_{t}\right|\right)
$$Note that $\operatorname{Regret}_{T}(\mathcal{H})$ is defined for $\mathcal{H}$, while your first formula is defined for $x$. So, \ref{1} is the regret for the whole hypotheses class $\mathcal{H}$, which is thus the maximum regret that you can have across all possible hypotheses $h \in \mathcal{H}$. This should make sense."
Do I need a large pool of training data to train a bot to play the 'pegging' game in the cribbage card game,"
The game of cribbage https://en.wikipedia.org/wiki/Cribbage is a two-player card game played over a series of deal with the goal to reach 121 points.
The game's elements are:

the discard. There are three hands, one for each player of 4 cards selected from six. The discards go face down to a hand which belongs to the dealer (the crib). On average, because non-dealer is optimising one hand, his hand scores slightly more points, say 8 points vs 7.8 for dealer, while the crib scores around 4 points, because one player aims to toss trash (while not damaging his own hand), while the other aims to toss good cards (while likewise maximising his own hand)
pegging. Here the dealer expects to peg on average around 4 points (he plays second, so has an advantage in responding to non-dealer's lead), and non-dealer around 2 points
the cut. This is a fifth card which belongs to all three hands. If the cut is a Jack, dealer scores 2 points, for free.
the order of play. This is:

the cut. Averages 2/13 = 0.15 points for dealer (if a Jack is cut)
pegs (played one card at a time, scoring for each). Dealer will peg at least 1 point, and non-dealer will peg 0 or more. Averages 4 and 2 points.
non-dealer shows his hand. Averages 8 points
dealer shows his hand. Averages 8 points.
dealer shows his crib. Average 4 points.



As should be obvious the hands are generally the main source of points, so players will tend to optimise their hands to maximise points there. This problem is OUTSIDE OF THE SCOPE of this post, since there are exhaustive analyses that have solved this problem reasonably well. E.g.,  https://cliambrown.com/cribbage/
There are 6C4 = 15 discards possible for each hand, and often the correct play will be unambiguous. For example, if we had the hand A49QKK with mixed suits (no more than 3 of any 1 suit) and we are non-dealer, then the correct hold is obvious - A4KK (this hand has two ways to make 15 A4K and A4K for 2 points each, and a pair for 2 points, plus it improve swith cuts of A, 4, 5, T, J, Q K)
After we've held the card we must then 'play the pegging game'. The scope of this post/question is therefore limited to how to play a given 4-card hand that we have already selected using an exhaustive combinatorial analysis, hence I'm assuming that the bots inputs are a given 4 card hand, as well as the three dead cards (discard and cut), and the replies to each play from our opponent.
The scoring for pegging is:

each pair scores 2 for the player playing the second card, 6 for 3 of a kind, 12 for 4 of a kind
each time the count reaches 15, the player playing the card scores 2
each time the count reaches 31, the player reaching scores 2 (it is not permitted to exceed 31, and if you have no card that will keep the count at/below 31, you must call 'go', and if both players call 'go', the last player scores 1 point). After this the cards are turned over, and play continues from zero
each time a run of 3 or more cards is visible (between the cards played by both players), then you score 3, 4, 5, 6, 7, or even 8 points according to the length of the run.

Follows discussion about pegging strategy, which I have blockquoted for those who find this tl;dr

The optimum play for a given hand is not necessarily clear-cut. For
example, with the hand A4KK, then we could lead:

K - on the basis that if dealer replies with another K (for 2 points), then we reply with the third K (for 6 points), and then most
likely dealer does not have an Ace, so we score 2 points for the 31 (K
= 10, A = 1), AND dealer must lead the next round of pegging, which is a considerable disadvantage.
not K - because there's a bias to holding 5s, so if dealer was dealt a 5, then he can reply with the 5 scoring 2 points for the 15, to
which we have no scoring reply. In addition, non-dealer generally has
a bias to leading from a pair, so if we lead the K, then in general a
smart dealer says 'he is likely to have a pair of Ks'. So even if
dealer has a K, he may decline to peg it, especially if he doesn't
hold the A himself
4 - this is the most popular lead in cribbage because 15 cannot be scored on it. As indicated above there are 16 10 cards (TJQK), as
opposed to 4 of every other denomination, so it's more likely to
receive a ten back than any other denomination, and we hold an Ace, so
we would score 2 points following a ten card reply
A - this has the same advantage that dealer cannot reach 15, and again if we get a ten card back we can score 15-2.
It might be that the A is better to lead because the 4 is such a popular lead that dealer is more likely to pair it (risking that we
hold the third 4 for 6 points), in which case we have no good reply.
If we lead the A, then the dealer is [probably!] less likely to reply
with an A (which doesn't help us).
We might prefer to lead the 4 if we think that the A is more likely to allow us to play the last card before 31, in which case dealer is
forced to lead first next round, which is a disadvantage.

During pegging we have considerable information:

which cards we hold and discarded. For example, clearly if we hold 3 4s, and the 4th was cut, then there is no chance that we get one
played against us.
which cards have been played and therefore which cards are likely to remain, based on the hand selection process. For example, if a 4 is
led then x% of the time that will be from A4 TJQK, y% it will be from
4456, etc. As more cards are played, then this becomes more obvious.
For example, if we've seen the 3 cards 5TK, then it's VERY likely that
the 4th card is another TJQK card, and somewhat likely that it's a 5.
It could be ANY card because maybe of the six cards the player was
dealt he ended up with an unconnected card. But we can say that the
chance of this is low.

In terms of exactly what cards are held, then if we analysed millions
of games, we could calculate in general knowing one, two or three
cards what the remaining cards could be.
Although there are in theory 270,725 4 card hands, in pegging terms
(ignoring suits), there are only 1820  distinct hands
(https://docs.google.com/spreadsheets/d/1fxkLBkWC2LA6J06zhku21jcG2ATHqE1RNHlPSDcc4fQ/edit#gid=834958733)
For any given hand, if we for example held the cards 358TTT and we
were non-dealer, then we would choose to hold 5TTT and toss 38. We
would then lead the T. In this spot, clearly of the 1820 possible
hands, then combinations such as TT xy are no longer possible.
As another example if we were dealt A778JK, then we'd toss the JK.
Here we'd likely lead the 7.
Before we play the 7 it's relatively simple to calculate the odds that
dealer was dealt two 7s. Since he had six cards, that is
2/461/456C2, which is 1.45%. The chance that he chose to hold those
two 7s is a different number, and we could theoretically calculate it
by figuring out for each of the 45C6 (but simplifiable!) combinations,
which hold he would make. However it won't be too far from this number
of 1.45%.
HOWEVER, once we have played a 7 and dealer replies with a second 7,
then this number is now very far away from accurate. Firstly because
it's now a simple conditional probability where one condition is
already satisfied, so the chance of any of 5 unknown cards being a
specific card (e.g., the 7 of diamonds) is now almost 1/9. Of course
dealer has 3 cards, not 5, so again the chances of this are not that
number, but not too far off, because of those hands where dealer was
dealt 77, and he holds one seven, then he is seldom going to toss the
other one.
In terms of our possible plays, we have at the count of 14:

play the Ace and score 2 points for the count of 15.
play the 7 and bring the count to 21, which is 6 points. However, dealer is fairly likely to have a TJQK (bringing the count to 31,
scoring 2 points, and making us lead the next deal). In addition,
given that dealer has a similar analysis process to us, in hands where
he holds say 78 then he probably replies to the 7 lead with the 8, as
it doesn't allow us to score 6 points. So given that dealer has
replied with 7, this increases the chance that he holds the fourth 7
as well.  It's hard to say what this chance is, but for example there
is an x% chance of 31-2, a y% chance of 28-12, and a z% chance of
something else. Or we could consider the chance of any given card.
probably not play the 8, because at 778 the count is 22, and dealer has many scoring replies: 6 is a run (3 points), 9 is a run AND 31 (5
points), and 8 is a pair (2 points)

In general it can be seen that there are:

up to 1820 unique 4-card hands
up to 455 3-card hands
up to 91 2-card hands
up to 13 1-card hands

Clearly we could weight each hand by the chance it has to be held, and
this would work well BEFORE a card has been played.
However after 1 or more cards has been played this approach is going
to be hopelessly naive. For example, let's say we led the 4. If dealer
is holding a hand like 5JJK then he's definitely not going to reply
with the 5, because we can play a 3 for 3 points (run), or a 6 for 5
points (run and count of 15).
Further, against non beginner-level players, it will be obvious that
we often hold hands like A4 JK or 23 KK, etc. So if we lead the 3, and
dealer replies with a ten card (TJQK), then it's likely he's NOT
holding cards such as 789, which a non-beginner player would likely
prefer to reply with here. It's possible of course that dealer has the
same cards. For example, if we hold 23JK, dealer might be holding
23QQ. In this case, the Q reply might be preferred by dealer, because
if the play goes 3-Q-2, then dealer can pair our 2, and perhaps dealer
doesn't like to pair our 3 lead, for fear we have a third 3, while he
lacks the fourth.
In addition, while for pegging terms suits don't matter at all, suit
information is during important, so long as the player has a flush.
For example if we were dealt 2468h Jd Js, we'd hold the 2468 of
hearts, because that's a flush worth 4 points. So if during we play,
we've seen 468h from our opponent, then the probability distribution
of possible hands is going to contain a good weight for every heart
card. Whereas if we've seen 4h 6d 8h, then we know after two cards
that he does NOT have a flush, so this weights likely hands towards
hand such as 4568, 4678 and so on, and it's unlikely that the
remaining card is, say, a K.

In general the goal of a pegging bot could be seen as:

score most points or
concede fewest points or
maximise net points


Nuance: In early game it's likely max net points is the best approach.
But if we are at a score like 117-117 (121 is the winning mark) as
dealer, then it's (let's say) 80% likely that non-dealer has 4 points
in his hand, which means he wins if we do not peg 4 points. In this
case non-dealer would try to hold cards that score 4 points (as a
hand), and if there are multiple options, try to hold cards that
reduce dealer's chance of pegging. Meanwhile dealer's realistic route
to victory would be to hold the 4 cards that give him the best chance
of pegging 4 points (remembering that dealer will on average score
more pegs, while non-dealer scores his hand first). If the score was
113-113, then dealer would play differently as he has NO chance of
pegging 8 points, but there's perhaps a 40% chance that non-dealer
fails to score 8 points from pegging and his hand. So in this case
dealer would try to stop non-dealer pegging anything.
So it seems that an AI would need to take into account the current score
to decide how to peg.

I have read a couple of papers on cribbage AI, but they have been superficial and tended to concentrate on the discard game, which can be optimised at least for discarding purposes (without considering the relative pegging value), by simply iterating through the possible hands.

http://r6.ca/cs486/ (does not cover pegging)
https://helda.helsinki.fi/bitstream/handle/10138/273545/SeanLang_MS_thesis.pdf (addresses pegging very superficially by maximising the score of the next card, not considering that that might result in very poor total scores)

Now the question is to what extent this is a problem of machine learning, and to what extent this is an exhaustive analysis? If we ignore the discard problem, and we say that we will input four-card hands to our bot using the output of a process similar to the one here https://cliambrown.com/cribbage/methodology.php then for example:

we could calculate the exact probability that our opponent holds any given four card hand, by iterating through the 45C6 hands (subject to appropriate simplifications) that our opponent could have, and then producing a weight for each of the (up to) 1820 possible hands, and for the 715 different hands of 4 unique denominations, the further chance for each to be a flush
I am not quite clear how computationally expensive this is, but it seems to me that should be calculate the weights in a reasonable amount of time

So we have 4 cards, and we have weighted possibilities for each of 1820 hands.
Clearly it's not appropriate for us to simply randomly iterate through the hands. I.e. there are four choices for our first card, likewise four for our opponent, then 3 for the next. Roughly there will be 4!*4! choices in this way (roughly because the issue of the count resetting at 31 means that the card order after the third card is not always the samme). But our opponent's reply is not random.  If we lead the 5, then he will certainly reply with a TJQK if he has one. He won't possibly reply with a 5.
So it seems to me that some kind of learning process is appropriate. But I am not really familiar with AI to say how this should work. Do we need a pool of human training data? Or do we allow the bot to play against itself?
And what role do probability tables have in this process? Is it going to be productive on a hand level to iterate through  possible hands that our opponent might have, or is a Monte Carlo process just as good?
I should note that humans might make sub-optimal play. So for example, if we play PERFECTLY, then if we hold the hand 6699, and the 4 is led, then we SHOULD reply with the 9, rather than risk a 5 on our 6, conceding 5 points. So the play of a 6 on a 4 SHOULD indicate that we hold a hand such as 5566, in which case then we are informed accordingly. But clearly the chance that we hold 6699 and just made a blunder is not zero. So the bot cannot ever be completely certain about our holds.
Likewise it might be we choose to make 'sub-optimal' plays in order to avoid becoming predictable ourselves. For example, the question 'will our opponent pair our lead' is an important one - if we have a pair, we generally want him to. But sometimes we will hold a hand such as A4 JK, in which case we don't want him to pair our lead. Some players will play more aggressively than others, and against an aggressive player, he might pair our lead every time, and a more defensive player might almost never pair our lead.
","['ai-design', 'game-ai', 'deep-rl']",
Is a policy in reinforcement learning analogous to a field such as APF?,"
If a policy maps states to actions in reinforcement learning, then for a path planning with obstacles, can't we simply use Artificial Potential Field fields for path planning and model policy mathematically as a field where the obstacles form repulsive field and goal form attractive field?
So, technically, is a policy simply a field?
","['reinforcement-learning', 'markov-decision-process', 'policies', 'path-planning']","Since most policies depend solely on actions and states/observations, then if you model the space of this field as the Cartesian Product of your state and action spaces, then the policy learns a surface over this combined space, similar to the way a field is parameterized.The policy an agent learns could exhibit the same behavior as the field you describe above (obstacles form an repulsive field, and goal(s) form an attractive field).  However, unlike the field described above, it is not guaranteed that the learned policy will capture this behavior - the policy learned depends on:To sum this answer up, I believe you could train the policy in such a way (using the mechanisms above) such that it resembles the field you describe."
What is the difference between artificial neural networks and deep learning?,"
I have read many mixed definitions around these two terms. For example, is it right to say deep learning is any ANN with more than two hidden layers?
What are formal definitions for these two?
","['neural-networks', 'deep-learning', 'comparison', 'definitions']",
"Why does not the deepAR model of Amazon require the time series being stationary, as opposed to ARMA model?","
As what the title said. Does not deepAR require the time series being stationary?
","['deep-neural-networks', 'time-series']",
Most of state-action pairs remain unvisited in the q-table,"
In building my first Q-learning algorithm for OpenAI gym's CartPole problem, many of my states remain unvisited. I believe it is the reason that my agent does not learn.
Can I be told of the reasons I can look into why that may happen? I have read and seen tutorials and I know a lot has already been done for this problem. My goal here is to learn and hence this simple implementation with Q-learning.
PS. The specific question to my problem is asked here.
PSS. As an edit, I am inserting my whole code in the following.
import numpy as np
import gym
import matplotlib.pyplot as plt

env = gym.make('MountainCar-v0')

EPISODES = 5000
LEARNING_RATE = 0.1
SHOW_AFTER = 1500
DISCOUNT_FACTOR = 0.95
EPSILON = 0.1
NUMBER_OF_BINS = 10
OBSERVATION_SPACE = 2
MAX_STATES = 100 # e.g. 23 means obs one is 2, obs two is 3

'''This function breaks the continuous states into discrete form'''
def digitize_states():
    bins = np.zeros((OBSERVATION_SPACE,NUMBER_OF_BINS))
    bins[0] = np.linspace(-1.2, 0.6, NUMBER_OF_BINS)
    bins[1] = np.linspace(-.07, 0.07, NUMBER_OF_BINS)
    return bins

'''This function assign the observations into discrete bins using 
   digitize function and the bins that we created using digitize_states()
'''
def assign_obs_to_bins(obs, bins):
    states = np.zeros((OBSERVATION_SPACE))
    states[0] = np.digitize(obs[0], bins[0]) 
    states[1] = np.digitize(obs[1], bins[1]) 
    return states

'''This function merely make the states in form of the strings so that we can
   later use those strings (i.e. number of states) as the KEYs in our q-table 
   dictionary.
'''
def get_all_states_as_strings():
    states = []
    for i in range(MAX_STATES):
        states.append(str(i).zfill(OBSERVATION_SPACE))
    return states

'''Convert the current state into string so that it can be used as key for dictionary '''
def current_state_to_string(state):
    current_state = ''.join(str(int(e)) for e in state)
    return current_state

'''This function iniquation the q-table to zeros'''
def initialize_q():
    states = get_all_states_as_strings()
    q = {}
    for state in states:
        q[state] = {}
        for action in range(env.action_space.n):
            q[state][action] = 0 
    return q

def initialize_Q():
    Q = {}

    all_states = get_all_states_as_strings()
    for state in all_states:
        Q[state] = {}
        for action in range(env.action_space.n):
            Q[state][action] = np.random.uniform(-.5, 0.5, 1)
    return Q

'''This function returns the maximum Q-value from Q-table'''
def return_max_from_dict(dict_var, action = None):
    '''    Arguments
    # dict_var: Dictionary variable, which represent the q-table.
    
    # Return
    # max_key: Best Action
    # max_val: max q-value for the current state, taking best action
    '''
    if(action == None):
        max_val = float('-Inf')
        for key, val in dict_var.items():
            if val > max_val:
                max_val = val
                max_key = key
        return max_key, max_val
    else:
        return dict_var[action]   
        

'''Main code starts here'''

bins = digitize_states()
all_states = get_all_states_as_strings()

q = initialize_Q()    

Total_reward_matrix = []
_testing_action_matrix = []
_testing_state_matrix = [] 
_testing_states = []
_testing_random = 0
_testing_greedy = 0

for episode in range(EPISODES):
    
    done = False
    cnt = 0
    
    # Reset the observations -> then assign them to bins
    obs = env.reset()
    
    if episode%SHOW_AFTER == 0:
        print(episode)
    
    total_reward = 0
    
    while not done:
        current_state = current_state_to_string(assign_obs_to_bins(obs, bins))
        _testing_state_matrix.append(int(current_state))

        if np.random.uniform() < EPSILON:
            act = env.action_space.sample()
            best_q_value = return_max_from_dict(q[current_state], action = act)
            _testing_random+=1
        else:
            act, best_q_value = return_max_from_dict(q[current_state])
            _testing_greedy+=1
        
        obs, reward, done, _  = env.step(act)
        _testing_action_matrix.append(act)
        
        q[current_state][act] = (1-LEARNING_RATE)*q[current_state][act] + LEARNING_RATE * (reward + DISCOUNT_FACTOR * best_q_value)
        cnt+=1
        total_reward += reward
    
        if done and cnt > 200:
            print(f'reached at episode: {episode} in count {cnt}') 
            Total_reward_matrix.append(total_reward)
        elif done:
#             print('Failed to reach flag in episode ', episode)
            Total_reward_matrix.append(total_reward)
        
    
env.close()

","['reinforcement-learning', 'q-learning', 'open-ai', 'gym']",
OpenAI gym's CartPole problem system does not learn,"
My OpenAI CartPole-v0 problem's implementation using basic Q-learning does not learn at all.
I am a beginner and have implemented my first ever Q-learning from scratch after learning from tutorials.
Can anyone suggest what is going wrong?
I have seen through testing that the problem may be that most of the states are remain unvisited even after 10,000 runs. Hence, Q-table remains mostly unchanged at the end of all episodes. I have seen other things in the implementation and they all seem fine to me, at least. Any tip where I should start looking at?
The reward is -200 flat, for all the episodes! which suggests that the improvement is NILL/NADDA/NONE!
Some relevant images are given at the end.
The q-learning part of code is given below:
env.reset()
while not done:    
    current_state = current_state_to_string(assign_obs_to_bins(obs, bins))

    if np.random.uniform() < EPSILON:
        act = env.action_space.sample()
        best_q_value = return_max_from_dict(q[current_state], action = act)
    else:
        act, best_q_value = return_max_from_dict(q[current_state])

    obs, reward, done, _  = env.step(act)
    q[current_state][act] += LEARNING_RATE * (reward + DISCOUNT_FACTOR * best_q_value - q[current_state][act])
    cnt+=1
    total_reward += reward



","['q-learning', 'open-ai', 'gym']",
Combining clustering and deep learning for computer vision,"
Is there any recent work on combining clustering approaches (k-means, or gaussian mixture or PGM) with deep learning for computer vision?
In particular I'm interested in if anyone has used the first few layers of a deep learning network as feature extractors in conjunction with clustering algorithms which have been engineered to induce things like translation and rotation invariance while preserving basic object structure?
Taking the max value out of the output of each feature and fitting them to a gaussian mixture is the easiest approach but I'm interested in seeing other ways you could structure the clustering algorithm. For example I'm interested in seeing how you might learn structure between features that includes position information.
","['deep-learning', 'computer-vision', 'clustering']",
Why L2 loss is more commonly used in Neural Networks than other loss functions?,"
Why L2 loss is more commonly used in Neural Networks than other loss functions?
What is the reason to L2 being a default choice in Neural Networks?
","['neural-networks', 'deep-learning', 'objective-functions', 'regularization', 'loss']","I'll cover both L2 regularized loss, as well as Mean-Squared Error (MSE):MSE:L2 Regularization:Using L2 regularization is equivalent to invoking a Gaussian prior (see https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior) on your model/estimator.  If modeling your problem as a Maximum A Posteriori Inference (MAP) problem, if your likelihood model (p(y|x)) is Gaussian, then your posterior distribution over parameters (p(x|y)) will also be Gaussian.  From Wikipedia: ""If the likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure that the posterior distribution is also Gaussian"" (source: https://en.wikipedia.org/wiki/Conjugate_prior).As in the case above, L2 loss is continuously-differentiable across any domain, unlike L1 loss."
How can a de-noising auto-encoder act as an anomaly detection model?,"
In some research papers, I have seen that, for training the autoencoders, instead of giving the non-anomalous input images, they add some anomalies to the normal input images, and train the auto-encoders with this anomalous images.
And, during testing, they take and pass an anomalous image and get the output, take their pixel-wise difference, and, based on a threshold, they detect if it is an anomaly or not.
If we are adding noise or anomalies to the training set, are we generalizing the model's capability to recreate the original normal input?
How does it help to detect the anomaly?
My understanding is that we should train using only normal data without adding any noise, then give an anomaly image at test, take the loss as a threshold.
","['neural-networks', 'papers', 'autoencoders', 'anomaly-detection', 'denoising-autoencoder']",
In what situations ELUs should be used instead of RELUs?,"
I always use RELUs actication functions when I need to and I understand limitations of ELUs. So in what situation do I need to consider ELUs over RELUs?
","['neural-networks', 'deep-learning', 'activation-functions']","ELU does not suffer from dying neurons issue, unlike ReLU. While ELU can help you to achieve better accuracy, it is slower than ReLU because of its non-linearity in its negative range.Choosing the right activation function totally depends on the situation but you need to also consider other similar types of activation functions such as leaky ReLU.Check this link out. It could be useful."
Embedding Layer into Convolution Layer,"
I'm looking to encode PDF documents for deep learning such that an image representation of the PDF refers to word embeddings instead of graphic data
So I've indexed a relatively small vocabulary (88 words). I've generated images that replace graphic data with word indexed (1=cat, 2=dog, etc) data. Now I'm going to my NN model
right_input = Input((width, height, 1), name='right_input')
right = Flatten()(right_input)
right = Embedding(wordspaceCount, embeddingDepth)(right)
right = Reshape((width, height, embeddingDepth))(right)
right = vgg16_model((width, height, embeddingDepth))(right)

Image data is positive-only and embedding outputs negative values though so I'm wondering if it is necessary to normalize the embedding layer with something like this after the Embedding layer
right = Lambda(lambda x: (x + 1.)/2.)(right)

The word indexed image looks like this:

Also, is this a problematic concept generally?
","['natural-language-processing', 'tensorflow', 'keras', 'convolution']",
Is it good practice to save NLP Transformer based pre-trained models into file system in production environment,"
I have developed a multi label classifier using BERT. I'm leveraging Hugging Face Pytorch implementation for transformers.
I have saved the pretrained model into the file directory in dev environment. Now, the application is ready to be moved the production environment.
Is it a good practice to save the models into file system in prod ?
Can I serialize the model files and word embeddings into any DB and read again ?
","['models', 'pytorch', 'word-embedding', 'transformer', 'pretrained-models']",
Why do DQNs tend to forget?,"
Why do DQNs tend to forget? Is it because when you feed highly correlated samples, your model (function approximation) doesn't give a general solution?
For example:

I use level 1 experiences, my model $p$ is fitted to learn how to play that level.

I go to level 2, my weights are updated and fitted to play level 2 meaning I don't know how to play level 1 again.


","['reinforcement-learning', 'q-learning', 'dqn', 'experience-replay', 'catastrophic-forgetting']",You are referring to catastrophic forgetting which could be an issue in any neural net. More specifically for DQN refer to this article.
What is the difference between Bayes-adaptive MDP and a Belief-MDP in Reinforcement Learning?,"
I have been reading a few papers in this area recently and I keep coming across these two terms. As far as I'm aware, Belief-MDPs are when you cast a POMDP as a regular MDP with a continuous state space where the state is a belief (distribution) with some unknown parameters.
Are they not the same thing?
","['reinforcement-learning', 'comparison', 'markov-decision-process', 'pomdp']",
What is the advantage of using experience replay (as opposed to feeding it sequential data)?,"
Let's suppose that our RL agent needs to play a game with different levels. If we train our RL agent sequentially or with sequential data, our agent will learn how to play level 1, but then it will learn to play level 2 differently, because our agent learns how to play level 2 and forgets how to play level 1, since now our model is fitted using only experiences from level 2.
How does an experience replay buffer change this? Can you explain this in simple terms?
","['reinforcement-learning', 'experience-replay']",
Plotting loss vs number of updates made and plotting loss vs run time,"
I wanted to plot a graph to show the effect of increasing the batch size on loss calculated (MNIST dataset). But I am not able to decide if I should show change in loss over training time of the neural network or number of updates made to weights and biases (iterations and epochs basically, but for large differences in batch sizes, I think number of updates made makes more sense?). I am confused about what makes more sense (or neither of them makes sense, idk). 
With Loss vs training time graph, I can show that for any training time, the loss for large batch is more. From what I have read on wiki, with Loss vs number of updates made graph, I can show that change in loss is smoother for larger batches (rate of convergence). But can't the same conclusion be made when plotted against time? (Smooth convergence means lesser spikes right?)
","['machine-learning', 'plotting']",
What is the difference between vanilla policy gradient with a baseline as value function and advantage actor-critic?,"
What is the difference between vanilla policy gradient (VPG) with a baseline as value function and advantage actor-critic (A2C)?
By vanilla policy gradient I am specifically referring to spinning up's explanation of VPG.
","['reinforcement-learning', 'comparison', 'policy-gradients', 'actor-critic-methods', 'advantage-actor-critic']","The difference between Vanilla Policy Gradient (VPG) with a baseline as value function and Advantage Actor-Critic (A2C) is very similar to the difference between Monte Carlo Control and SARSA:The value estimates used in updates for VPG are based on full sampled returns, calculated at the end of episodes.The value estimates used in updates for A2C are based on temporal difference bootstrapped from e.g. a single step difference, and the Bellman function.This leads to the following practical differences:A2C can learn during an episode which can lead to faster refinements in policy than with VPG.A2C can learn in continuing environments, whilst VPG cannot.A2C relies on initially biased value estimates, so can take more tuning to find hyperparameters for the agent that allows for stable learning. Whilst VPG typically has higher variance and can require more samples to achieve the same degree of learning."
"Why can we use a network to estimate $Q_\pi(s, a)$ in Actor-Critic Method?","
According to deep Q learning, we want to learn $Q^*(s,a)$, which is the optimal action-value function. It does make sense because we assume there is only one optimal function so the algorithm will converge supposedly.
But when it comes to actor-critic method, we use critic network (also called  value network) to estimate $Q_\pi(s, a)$. This is what confused me. Since our policy $\pi$ will change through time, the target $Q_\pi(s, a)$ of value network will also change. What will happen for a network to learn a changing function?
","['machine-learning', 'reinforcement-learning', 'deep-learning']",
What activation functions are currently popular?,"
I am not asking what activation function is better. I want to know what activation functions are more used in research or deployment. Also, are they used in combination? E.g., ReLU, ELUs, etc. I'd appreciate any statistics or insight on this.
","['neural-networks', 'deep-learning', 'activation-functions']","Currently, both ReLU and ELUs are the most popular activation functions (AF) used in neural nets (NNs). This is because they eliminate the vanishing gradient problem that causes major problems in the training process and degrades the accuracy and performance of NN models.Also, these AFs, more specifically ReLU, are very fast learning AF which makes them even more useful in research.However, depending on the type of NN you working on, it's always a good practice to pay some attention to new studies."
How can I build a deep reinforcement learning model that can be trained with multiple time series datasets,"
I built a DRL model to trade stocks in the financial market but the number of observations is relatively small and I would like to increase it by training the same model with stocks from several different companies. My problem is that I don't know what is the correct way to do this since the price series is a time series. Someone to enlighten me? I have read articles that show that this is possible but none that say how.
","['reinforcement-learning', 'training', 'dqn', 'deep-rl', 'time-series']",
Reinforcement learning with action consisting of two discrete values,"
I'm new to reinforcement learning. I have a problem where an action is composed of an order (rod with a required length) and an item from a warehouse (an existing rod with a certain length, which will be cut to the desired length and the remainder put back to the warehouse).
I imagine my state as two lists of a defined size: orders and warehouse, and my action as an index from the first list and an index from the second list. However, I have only worked with environments where it was only possible to pick single action and I'm not sure how to deal with two indexes. I'm not sure how DQN architecture should look like to give me such action.
Can anyone validate my general idea and help me find a solution? Or maybe just point me to some papers where similar problems are described?
","['reinforcement-learning', 'q-learning', 'dqn', 'markov-decision-process', 'environment']","You would still be picking a single action. Your action space is now $\mathcal{A} = \mathcal{O} \times \mathcal{I}$ where I've chosen $\mathcal{O}$ to be the set of possible orders from your problem and $\mathcal{I}$ to be the set of possible items.Provided both of these sets are finite, then you should still be able to approach this problem with DQN. Theoretically, this should be easy to see, as any element from $\mathcal{A}$ is still a single element it just happens that this element is now a tuple.From a programming point of view, let's consider the simple example of cartpole, where the possible actions are left and right. Your $Q$-function obviously won't know the meanings of 'left' and 'right', you just assign it to an element of a vector, i.e. your $Q$-function would output a vector in $\mathbb{R}^2$ with e.g. the first element corresponding to the score for 'left' and the second element corresponding to the score for 'right'. This is still the case in your problem formulation, you will just have a $Q$-function that outputs a vector in $\mathbb{R}^d$ where $d = |\mathcal{A}|$ - you would just have to make sure you know which element corresponds to which action.Also, there is the possibility that this approach could leave you with a large dimensional vector output, which I imagine would probably mean you'd need more simulations to properly explore the action space.Hope this helps."
How is the F1 score calculated in a question-answering system?,"
I have an NLP model for answer-extraction. So, basically, I have a paragraph and a question as input, and my model extracts the span of the paragraph that corresponds to the answer to the question.
I need to know how to compute the F1 score for such models. It is the standard metric (along with Exact Match) used in the literature to evaluate question-answering systems.
","['natural-language-processing', 'natural-language-understanding', 'metric', 'question-answering']",
"If the performance of an RL agent in a partially observable environment is ""good"", is this likely only accidental?","
In my research, I remember to have read that, in case of an environment which can be modeled by partially observable MDP, there are no convergence guarantees (unfortunately, I do not find the paper anymore and I would appreciate if someone can post the link to the reference).
If the performance of an RL agent in a partially observable environment is ""good"" (i.e. the agent does pretty well in achieving its goal), is this likely only accidental or due to chance?
","['reinforcement-learning', 'markov-decision-process', 'convergence', 'pomdp']",
"What exactly are the ""parameters"" in GPT-3's 175 billion parameters and how are they chosen/generated?","
When I studied neural networks, parameters were learning rate, batch size etc. But even GPT3's ArXiv paper does not mention anything about what exactly the parameters are, but gives a small hint that they might just be sentences.

Even tutorial sites like this one start talking about the usual parameters, but also say ""model_name: This indicates which model we are using. In our case, we are using the GPT-2 model with 345 million parameters or weights"". So are the 175 billion ""parameters"" just neural weights? Why then are they called parameters? GPT3's paper shows that there are only 96 layers, so I'm assuming it's not a very deep network, but extremely fat. Or does it mean that each ""parameter"" is just a representation of the encoders or decoders?

An excerpt from this website shows tokens:

In this case, there are two additional parameters that can be passed
to gpt2.generate(): truncate and include_prefix. For example, if each
short text begins with a <|startoftext|> token and ends with a
<|endoftext|>, then setting prefix='<|startoftext|>',
truncate=<|endoftext|>', and include_prefix=False, and length is
sufficient, then gpt-2-simple will automatically extract the shortform
texts, even when generating in batches.

So are the parameters various kinds of tokens that are manually created by humans who try to fine-tune the models? Still, 175 billion such fine-tuning parameters is too high for humans to create, so I assume the ""parameters"" are auto-generated somehow.
The attention-based paper mentions the query-key-value weight matrices as the ""parameters"". Even if it is these weights, I'd just like to know what kind of a process generates these parameters, who chooses the parameters and specifies the relevance of words? If it's created automatically, how is it done?
","['recurrent-neural-networks', 'open-ai', 'transformer', 'attention', 'gpt']","Parameters is a synonym for weights, which is the term most people use for a neural networks parameters (and indeed in my experience it is a term that machine learners will use in general whereas parameters is more often found in statistics literature). Batch size, learning rate etc. are hyper-parameters which basically means they are user specified, whereas weights are what the learning algorithm will learn through training."
How do AIs like Siri and Alexa respond to their names being called?,"
AIs like Siri and Alexa respond to their names being called. How does the system recognize the name by ignoring all the other words that have been said before their name? For example, ""Hey Siri"" would trigger Siri to start listening for commands, but if a user said ""hey how are you hey Siri"" the system will ignore ""hey how are you"" but trigger the system to ""hey Siri"". Is it because their listening function reloads in milliseconds or even nanoseconds, or is there a different way it works?
","['applications', 'speech-recognition', 'siri']","Is it because their listening function reloads in milliseconds or even nanosecondsYes, it expects the keyword to start every moment of time and it ignores the rest.Overall, the algorithm is described here, you can read for details:https://machinelearning.apple.com/research/hey-siri"
What is the reason for mode collapse in GAN as opposed to WGAN?,"
In this article I am reading:

$D_{KL}$ gives us inifity when two distributions are disjoint. The value of $D_{JS}$ has sudden jump, not differentiable at $\theta=0$. Only Wasserstein metric provides a smooth measure, which is super helpful for a stable learning process using gradient descents.

Why is this important for a stable learning process? I have also the feeling this is also the reason for mode collapse in GANs, but I am not sure.
The Wasserstein GAN paper also talks about it obviously, but I think I am missing a point. Does it say JS does not provide a usable gradient? What exactly does that mean?
","['neural-networks', 'generative-adversarial-networks', 'kl-divergence', 'wasserstein-metric', 'wasserstein-gan']",
When to apply reward for time series data?,"
Reading the paper 'Reinforcement Learning for FX trading 'at https://stanford.edu/class/msande448/2019/Final_reports/gr2.pdf it states:

While our end goal is to be able to make decisions on a universal time
scale, in order to apply a reinforcement learning approach to this
problem with rewards that do not occur at each step, we formulate the
problem with a series of episodes. In each episode, which we designate
to be one hour long, the agent will learn the make decisions to
maximize the reward (return) in that episode, given the time series
features we have.

This may be a question for the authors but is it not better in RL to apply rewards at each time step instead of ""rewards that do not occur at each step""? If apply rewards at each time step then the RL algorithm will achieve better convergence properties as a result of learning at smaller time intervals rather than waiting for ""one hour"". Why not apply rewards at each time step?
",['reinforcement-learning'],
What kind of neural network can be trained to recognise patterns?,"
Is there a type of neural network that can be fed patterns to train itself on to complete new patterns that it has not seen before?
What I'm trying to do is train a neural network to transform an image into another image. The image may be slightly different each time (denoted with different lines in the shapes) but a human would get the idea of how the new images should look. I'd like to make a network that can learn how to learn what comes next and then predict the rest of the sequence from the first part of a new sequence.
Taking the picture below as an example. The neural network would be fed the patterns in grey and learn how to predict the next ones in the sequence. Then the user would put the blue shapes into the network and hope to get the green ones out.
Is there a neural network that could perform this type of function of completing a pattern based on only a small number of examples to start the pattern based on the other patterns it has seen?

EDIT: Corrected image and added more context
","['neural-networks', 'pattern-recognition']",
What is a multi channel supervised classifier?,"
I came across a paper that describes its model architecture in the following way.

Our TRIL network is a two-channel network jointly trained to predict the expert’s action given state and the system’s next state transition given state and expert action. The training procedure of TRIL is similar to that of a multi- channel supervised classifier with regularization. Let $\theta_{π_0}$ be the parameters of TRIL and $L_{ce}$ be the cross entropy loss for predicting expert’s action and $L_{mse}$ be the mean squared error loss on predicting next state given current state and the expert’s action

The loss function is given in the following manner
$$L(\theta_{\pi_0}) = L_{ce}(a, \pi_0(s)) + \lambda L_{mse}(T_{\pi_0}(s,a),s')$$

TRIL is a dual- channel network that shares certain hidden layers and jointly predicts expert action(a) and state transitions(s’)

I am not sure what a dual channel network means and what does it mean when it is able to jointly predict two outputs ? It seems something similar to a multi-task learning since there is shared hidden layers and different ""task"" prediction but i am not too sure of that either.
","['reinforcement-learning', 'deep-learning', 'supervised-learning']",
Why can we perform graph convolution using the standard 2d convolution with $1 \times \Gamma$ kernels?,"
Recently I was reading this paper Skeleton Based Action RecognitionUsing Spatio Temporal Graph Convolution. In this paper, the authors claim (below equation (\ref{9})) that we can perform graph convolution with the following formula
$$
\mathbf{f}_{o u t}=\mathbf{\Lambda}^{-\frac{1}{2}}(\mathbf{A}+\mathbf{I}) \mathbf{\Lambda}^{-\frac{1}{2}} \mathbf{f}_{i n} \mathbf{W} \label{9}\tag{9}
$$
using the standard 2d convolution with kernels of shape $1 \times \Gamma$ (where $\Gamma$ is defined under equation 6 of the paper), and then multiplying it with the normalised adjacency matrix
$$\mathbf{\Lambda}^{-\frac{1}{2}}(\mathbf{A}+\mathbf{I}) \mathbf{\Lambda}^{-\frac{1}{2}}$$
For the past few days, I was thinking about his claim but I can't find an answer. Does anyone read this paper and can help me to find it out, please?
","['computer-vision', 'geometric-deep-learning', 'convolution', 'graph-neural-networks']",
Extending patch based image classification into image classification,"
I am trying to classify tampered, pristine images from set of images, in that I have built a network in which I would divide the image into multiple overlapping patches and then classify them into pristine or fake(based on the probability outputs), but now I want extend the same to Image level. That is I want to build some model or some rule over output probabilities of patches of each image to get probability that the image is fake or pristine.
ways I am thinking to do is -

Build a shallow network over the probabilities of the patch probabilities. In this case problem is all images are of different shape
Apply a ML classifier (something like Logistic Regression), with output probabilities by appending zeros to the output probability vector generated so that all image has same sized probability vector as input
generate a mask using patches and then build  a simple classification network over the masks using original image labels.

I can't really say which among the above three is better or worse, I don't even know the possibility of the above three. (Kind of hit a roadblock in thinking)
Now the question am I thinking in right direction, what would be better among the ideas I am considering and why. Is there anything better than what I am thinking. It would helpful in suggesting some resources.
","['convolutional-neural-networks', 'classification', 'image-segmentation', 'representation-learning']",
Is there an optimal way to split the text into small parts when working with co-reference resolution?,"
I am working with co-reference resolution in a large text. Is there an optimal way to split the text into small parts? Or the best correct procedure is to use the entire text?
Just for reference, I am using the library spacy-neuralcoref in Python that is based on Deep Reinforcement Learning for Mention-Ranking Coreference Models by Kevin Clark and Christopher D. Manning, EMNLP 2016.
Why am I asking about splitting the text?
I am applying coreference to chapters of books (roughly 30 pages of text). All the examples I have seen show situations of coreference applied to small pieces of texts. I applied to a chapter and I found strange results. However, this is not a clear justification for that since the state of art in coreference is about 60%. Am I right?
I didn't check all databases that people use to test coreference but the ones I took a look (like MUC 3 and MUC 4 Data Sets), if I understand well, they were composed by a collection of a small number of paragraphs.
A test Example:

TST1-MUC3-0001
GUATEMALA CITY, 4 FEB 90 (ACAN-EFE) -- [TEXT] THE GUATEMALA ARMY
DENIED TODAY THAT GUERRILLAS ATTACKED THE ""SANTO TOMAS"" PRESIDENTIAL
FARM, LOCATED ON THE PACIFIC SIDE, WHERE PRESIDENT CEREZO HAS BEEN
STAYING SINCE 2 FEBRUARY.
A REPORT PUBLISHED BY THE ""CERIGUA"" NEWS AGENCY -- MOUTHPIECE OF
THE GUATEMALAN NATIONAL REVOLUTIONARY UNITY (URNG) -- WHOSE MAIN
OFFICES ARE IN MEXICO, SAYS THAT A GUERRILLA COLUMN ATTACKED THE FARM
2 DAYS AGO.
HOWEVER, ARMED FORCES SPOKESMAN COLONEL LUIS ARTURO ISAACS SAID
THAT THE ATTACK, WHICH RESULTED IN THE DEATH OF A CIVILIAN WHO WAS
PASSING BY AT THE TIME OF THE SKIRMISH, WAS NOT AGAINST THE FARM, AND
THAT PRESIDENT CEREZO IS SAFE AND SOUND.
HE ADDED THAT ON 3 FEBRUARY PRESIDENT CEREZO MET WITH THE
DIPLOMATIC CORPS ACCREDITED IN GUATEMALA.
THE GOVERNMENT ALSO ISSUED A COMMUNIQUE DESCRIBING THE REBEL REPORT
AS ""FALSE AND INCORRECT,"" AND STRESSING THAT THE PRESIDENT WAS NEVER
IN DANGER.
COL ISAACS SAID THAT THE GUERRILLAS ATTACKED THE ""LA EMINENCIA""
FARM LOCATED NEAR THE ""SANTO TOMAS"" FARM, WHERE THEY BURNED THE
FACILITIES AND STOLE FOOD.
A MILITARY PATROL CLASHED WITH A REBEL COLUMN AND INFLICTED THREE
CASUALTIES, WHICH WERE TAKEN AWAY BY THE GUERRILLAS WHO FLED TO THE
MOUNTAINS, ISAACS NOTED.
HE ALSO REPORTED THAT GUERRILLAS KILLED A PEASANT IN THE CITY OF
FLORES, IN THE NORTHERN EL PETEN DEPARTMENT, AND BURNED A TANK TRUCK.

",['natural-language-processing'],
How are training hyperparameters determined for large models?,"
When training a relatively small DL model, which takes several hours to train, I typically start with some starting points from literature and then use a trial-and-error or grid-search approach to fine-tune the values of the hyper-parameters, in order to prevent overfitting and achieve sufficient performance.
However, it is not uncommon for large models to have training time measured in days or weeks [1], [2], [3].
How are hyperparameters determined in such cases?
","['neural-networks', 'deep-learning', 'training', 'hyperparameter-optimization', 'hyper-parameters']","In general, it is definitely very computationally expensive, so an exhaustive search is not performed in practice. However, there are some recent approaches for determining whether the architecture is ""fine"" without training the neural network first - by looking at the covariance matrix after forwarding the data, for example, in a recent paper Neural Architecture Search without Training. However, such an approach is very limited."
"What is meant by ""arranging the final features of CNN in a grid"" and how to do it?","
In the paper What You Get Is What You See: A Visual Markup Decompiler, the authors have proposed a method to extract the features from the CNN and then arrange those extracted features in a grid to pass into an RNN encoder. Here's an illustration.

I can easily extract features from either the existing model, like ResNet, VGG, or make a new CNN model easily as they have described in the paper.
For example, let us suppose, I do this
features = keras.applications.ResNet()(images_array) # just hypothetical

How can I convert these images to the grid?? I am supposed to feed the output of the changed grid to an LSTM Encoder as:
keras.layers.LSTM()(grid) # again, hypothetical type

I just want to know what the author means from changing the output in the grid format.
","['neural-networks', 'deep-learning', 'computer-vision', 'terminology', 'papers']",
Can I think of the graph convolution operation as a regular 2D convolution for images?,"
Kipf et al. described in his paper that we can write graph convolution operation like this:
$$H_{t+1} = AH_tW_t$$
where $A$ is the normalized adjacency matrix, $H_t$ is the embedded representation of the nodes and $W_t$ is the weight matrix.
Now, can I imagine the same formula as first performing 2D convolution with fixed-size kernel over the whole feature space then multiply the result with the adjacency matrix?
If this is the case, I think I can create a graph convolution operation just using the Conv2D layer then performing simple matrix multiplication with adjacency matrix using PyTorch.
","['deep-learning', 'geometric-deep-learning', 'convolution', 'graph-neural-networks']",
Which neural network should I use to distinguish between different types of defects?,"
I want to teach a neural network to distinguish between different types of defects. For that, I generated images of fake-defects. The images of the fake-defect types are attached.






I tried many different network architectures now:

resnet18
squeezenet
own architectures: a narrow network with broad layers and high dropout rates.

I have to say that some of these defects have really random shapes, like the type single-dirt or multi-dirt. I imagine that the classification should not be as easy as I thought before, due to the lack of repetitive features within the defects. But I always feel like the network is learning some ""weird"" features, which do not occur in the test set, and the results are really frustrating. I felt like teaching binary images had way better results, which should IMO be not the case.
Still, I feel like a neural network should be able to learn to distinguish them.
Which kind of network architecture would you recommend to classify the images in the attachment?
","['deep-learning', 'classification', 'image-recognition']",
Why does every neuron in hidden layers of a multi-layer perceptron typically have the same activation function? [duplicate],"







This question already has an answer here:
                                
                            




Do all neurons in a layer have the same activation function?

                                (1 answer)
                            

Closed 2 years ago.



Why does every neuron in a hidden layer of a multi-layer perceptron (MLP) typically have the same activation function as every other neuron in the same or other hidden layers (so I exclude the output layer, which typically has a different activation function) of the MLP? Is this a requirement, are there any advantages, or maybe is it just a rule of thumb?
","['neural-networks', 'deep-learning', 'activation-functions', 'multilayer-perceptrons', 'hyper-parameters']","As you stated, it's popular to have some form of a rectified linear unit (ReLU) activation in hidden layers and the output layer is often a softmax or sigmoid (depending also on the problem: multi-class or binary classification, respectively), which provides an output that can be viewed as a probability distribution.You could generalize this further to blocks of different activation functions within the same layer. This is something I've thought about, haven't done, but imagine has been attempted. In some sense, the idea here would be to allow for a subsection of the network to develop a representation that may not be feasible otherwise. These different representations within the same layer would then be unified by subsequent layers as we move closer to the output."
Is the Bellman equation that uses sampling weighted by the Q values (instead of max) a contraction?,"
It is proved that the Bellman update is a contraction (1).
Here is the Bellman update that is used for Q-Learning:
$$Q_{t+1}(s, a) = Q_{t}(s, a) + \alpha*(r(s, a, s') + \gamma  \max_{a^*} (Q_{t}(s',
   a^*)) - Q_t(s,a)) \tag{1} \label{1}$$
The proof of (\ref{1}) being contraction comes from one of the facts (the relevant one for the question) that max operation is non expansive; that is:
$$\lvert \max_a f(a)- \max_a g(a) \rvert \leq \max_a \lvert f(a) -  g(a) \rvert \tag{2}\label{2}$$
This is also proved in a lot of places and it is pretty intuitive.
Consider the following Bellman update:
$$ Q_{t+1}(s, a) = Q_{t}(s, a) + \alpha*(r(s, a, s') + \gamma  SAMPLE_{a^*} (Q_{t}(s', a^*)) - Q_t(s,a)) \tag{3}\label{3}$$
where $SAMPLE_a(Q(s, a))$ samples an action with respect to the Q values (weighted by their Q values) of each action in that state.
Is this new Bellman operation still a contraction?
Is the SAMPLE operation non-expansive? It is, of course, possible to generate samples that will not satisfy equation (\ref{2}). I ask is it non-expansive in expectation?
My approach is:
$$\lvert\,\mathbb{E}_{a \sim Q}[f(a)] - \mathbb{E}_{a \sim Q}[g(a)]\, \rvert \leq \,\,\mathbb{E}_{a \sim Q}\lvert\,\,[f(a) - g(a)]\,\,\rvert \tag{4} \label{4} $$
Equivalently:
$$\lvert\,\mathbb{E}_{a \sim Q}[f(a) - g(a)] \, \rvert \leq \,\,\mathbb{E}_{a \sim Q}\lvert\,\,[f(a) - g(a)]\,\,\rvert$$
(\ref{4}) is true since:
$$\lvert\,\mathbb{E}[X] \, \rvert \leq \,\,\mathbb{E} \,\,\lvert\,\,[X]\,\,\rvert $$
But, I am not sure if proving (\ref{4}) proves the theorem. Do you think that this is a legit proof that (\ref{3}) is a contraction.
(If so; this would mean that stochastic policy q learning theoretically converges and we can have stochastic policies with regular q learning; and this is why I am interested.)
Both intuitive answers and mathematical proofs are welcome.
","['reinforcement-learning', 'q-learning', 'proofs', 'convergence', 'bellman-equations']",
Classification or regression for deep Q learning,"
DQN implemented at https://github.com/PacktPublishing/PyTorch-1.x-Reinforcement-Learning-Cookbook/blob/master/Chapter07/chapter7/dqn.py uses the mean square error loss function for the neural network to learn the state -> action mapping :
self.criterion=torch.nn.MSELoss()

Could cross-entropy be used instead as the loss function? Cross entropy is typically used for classification, and mean squared error for regression.
As the actions are discrete (the example utilises the mountain car environment - https://github.com/openai/gym/wiki/MountainCar-v0) and map to [0,1,2] can cross-entropy loss be used instead of mean squared error? Why use regression as the state -> action function approximator for deep Q learning instead of classification?
Entire DQN src from https://github.com/PacktPublishing/PyTorch-1.x-Reinforcement-Learning-Cookbook/blob/master/Chapter07/chapter7/dqn.py :
'''
Source codes for PyTorch 1.0 Reinforcement Learning (Packt Publishing)
Chapter 7: Deep Q-Networks in Action
Author: Yuxi (Hayden) Liu
'''

import gym
import torch

from torch.autograd import Variable
import random


env = gym.envs.make(""MountainCar-v0"")



class DQN():
    def __init__(self, n_state, n_action, n_hidden=50, lr=0.05):
        self.criterion = torch.nn.MSELoss()
        self.model = torch.nn.Sequential(
                        torch.nn.Linear(n_state, n_hidden),
                        torch.nn.ReLU(),
                        torch.nn.Linear(n_hidden, n_action)
                )
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)


    def update(self, s, y):
        """"""
        Update the weights of the DQN given a training sample
        @param s: state
        @param y: target value
        """"""
        y_pred = self.model(torch.Tensor(s))
        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


    def predict(self, s):
        """"""
        Compute the Q values of the state for all actions using the learning model
        @param s: input state
        @return: Q values of the state for all actions
        """"""
        with torch.no_grad():
            return self.model(torch.Tensor(s))



def gen_epsilon_greedy_policy(estimator, epsilon, n_action):
    def policy_function(state):
        if random.random() < epsilon:
            return random.randint(0, n_action - 1)
        else:
            q_values = estimator.predict(state)
            return torch.argmax(q_values).item()
    return policy_function


def q_learning(env, estimator, n_episode, gamma=1.0, epsilon=0.1, epsilon_decay=.99):
    """"""
    Deep Q-Learning using DQN
    @param env: Gym environment
    @param estimator: DQN object
    @param n_episode: number of episodes
    @param gamma: the discount factor
    @param epsilon: parameter for epsilon_greedy
    @param epsilon_decay: epsilon decreasing factor
    """"""
    for episode in range(n_episode):
        policy = gen_epsilon_greedy_policy(estimator, epsilon, n_action)
        state = env.reset()
        is_done = False

        while not is_done:
            action = policy(state)
            next_state, reward, is_done, _ = env.step(action)
            total_reward_episode[episode] += reward

            modified_reward = next_state[0] + 0.5

            if next_state[0] >= 0.5:
                modified_reward += 100
            elif next_state[0] >= 0.25:
                modified_reward += 20
            elif next_state[0] >= 0.1:
                modified_reward += 10
            elif next_state[0] >= 0:
                modified_reward += 5

            q_values = estimator.predict(state).tolist()

            if is_done:
                q_values[action] = modified_reward
                estimator.update(state, q_values)
                break

            q_values_next = estimator.predict(next_state)

            q_values[action] = modified_reward + gamma * torch.max(q_values_next).item()

            estimator.update(state, q_values)

            state = next_state


        print('Episode: {}, total reward: {}, epsilon: {}'.format(episode, total_reward_episode[episode], epsilon))

        epsilon = max(epsilon * epsilon_decay, 0.01)

n_state = env.observation_space.shape[0]
n_action = env.action_space.n
n_hidden = 50
lr = 0.001
dqn = DQN(n_state, n_action, n_hidden, lr)


n_episode = 1000

total_reward_episode = [0] * n_episode

q_learning(env, dqn, n_episode, gamma=.9, epsilon=.3)



import matplotlib.pyplot as plt
plt.plot(total_reward_episode)
plt.title('Episode reward over time')
plt.xlabel('Episode')
plt.ylabel('Total reward')
plt.show()

","['reinforcement-learning', 'python', 'objective-functions', 'pytorch']",
How Graph Convolutional Neural Networks forward propagate?,"
In the basic variant of GCN we have the following:

Here we aggregate the information from the adjacent node and pass it to a neural network, then transform our own information and add them all.
But the main question is: how can we ensure that $W_{k}(\sum(\frac{h_k}{N(V)})$ will be the same size as $B_{k}h_{v}$ and does $B_{k}$ emply another neural network?
","['geometric-deep-learning', 'graph-neural-networks']",
How can I predict the true label for data with incomplete features based on the trained model with data with more features?,"
Suppose I have a model that was trained with a dataset that contains the features (f1, f2, f3, f4, f5, f6). However, my test dataset does not contain all features of the training dataset, but only (f1, f2, f3). How can I predict the true label of the entries of this test dataset without all features?
","['machine-learning', 'deep-learning', 'training', 'datasets', 'testing']",
Why is DDPG not learning and it does not converge?,"
I have used a different setting, but DDPG is not learning and it does not converge. I have used these codes 1,2, and 3 and I used different optimizers, activation functions, and learning rate but there is no improvement.
    parser.add_argument('--actor-lr', help='actor network learning rate', default=0.001)
    parser.add_argument('--critic-lr', help='critic network learning rate', default=0.0001)
    parser.add_argument('--gamma', help='discount factor for critic updates', default=0.95)

    parser.add_argument('--tau', help='soft target update parameter', default=0.001)
    parser.add_argument('--buffer-size', help='max size of the replay buffer', default=int(1e5))
    parser.add_argument('--minibatch-size', help='size of minibatch for minibatch-SGD', default=64)

    # run parameters
    # parser.add_argument('--env', help='choose the gym env- tested on {Pendulum-v0}', default='MountainCarContinuous-v0')
    parser.add_argument('--random-seed', help='random seed for repeatability', default=1234)
    parser.add_argument('--max-episodes', help='max num of episodes to do while training', default=200)
    parser.add_argument('--max-episode-len', help='max length of 1 episode', default=100)


I have trained in the same environment with A2C and it converged.

Which parameters should I change to make the DDPG converge? Can anyone help me with this?
","['reinforcement-learning', 'tensorflow', 'python', 'convergence', 'ddpg']",
Forcing a neural network to be close to a previous model - Regularization through given model,"
I'm wondering, has anyone seen any paper where one trains a network but biases it to produce similar outputs to a given model (such as one given from expert opinion or it being a previously trained network).
Formally, I'm looking for a paper doing the following:
Let $g:\mathbb{R}^d\rightarrow \mathbb{R}^D$ be a model (not necessarily, but possibly, a neural network) trained on some input/output data pairs $\{(x_n,y_n)\}_{n=1}^N$ and train a neural network $f_{\theta}(\cdot)$ on
$$
\underset{\theta}{\operatorname{argmin}}\sum_{n=1}^N \left\|
f_{\theta}(x_n) - y_n
\right\| + \lambda \left\|
f_{\theta}(x_n) - g(x_n)
\right\|,
$$
where $\theta$ represents all the trainable weight and bias parameters of the network $f_{\theta}(\cdot)$.
So put another way...$f_{\theta}(\cdot)$ is being regularized by the outputs of another model...
","['neural-networks', 'training', 'reference-request', 'regularization', 'algorithmic-bias']",
Is it necessary to standardise the expected output,"
Normalisation transform data into a range:
$$X_i = \dfrac{X_i - Min}{Max-Min}$$
Practically, I found out that the model doesn't generalise well when using normalisation of input data, instead of standardisation (another formula shown below).
Before training a neural net, data are usually standardised or normalised. Standardising seems good as it makes the model generalise better, while normalisation may make the model not working with values out of training data range.
So I'm using standardisation for input data (X), however, I'm confusing whether I should standardise the expected output values too?
For a column in input data:
$$X_i = \dfrac{(X_i - Mean)}{Standard\ Deviation\ of\ the\ Column}$$
Should I apply this formula to the expected output values (labels) too?
","['neural-networks', 'machine-learning', 'data-preprocessing', 'normalisation', 'standardisation']","It depends, as mentioned in the comments, on your model and labels. For example, how would you use standardization on a multi-classification problem?Generally, standardization is more favorable for input data as its mean is around 0.I assume you have a regression model and in that case, using standardization could be better than normalization."
Can residual neural networks use other activation functions different from ReLU?,"
In many diagrams, as seen below, residual neural networks are only depicted with ReLU activation functions, but can residual NNs also use other activation functions, such as the sigmoid, hyperbolic tangent, etc.?

","['neural-networks', 'convolutional-neural-networks', 'activation-functions', 'relu', 'residual-networks']","The problem with certain activation functions, such as the sigmoid, is that they squash the input to a finite interval (i.e. they are sometimes classified as saturating activation functions). For example, the sigmoid function has codomain $[0, 1]$, as you can see from the illustration below.This property/behaviour can lead to the vanishing gradient problem (which was one of the problems that Sepp Hochreiter, the author of the LSTM, was trying to solve in the context of recurrent neural networks, when developing the LSTM, along with his advisor, Schmidhuber).Empirically, people have noticed that ReLU can avoid this vanishing gradient problem. See e.g. this blog post. The paper Deep Sparse Rectifier Neural Networks provides more details about the advantage of ReLUs (aka rectifiers), so you may want to read it. However, ReLUs can also suffer from another (opposite) problem, i.e. the exploding gradient problem. Nevertheless, there are several ways to combat this issue. See e.g. this blog post.That being said, I am not an expert on residual networks, but I think that they used the ReLU to further avoid the vanishing gradient problem. This answer (that I gave some time ago) should give you some intuition about why residual networks can avoid the vanishing gradient problem."
How artificial intelligence will change the future? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 1 year ago.







                        Improve this question
                    



AI is the emerging field and biggest business opportunity of the next decade. It's already automating manual and repetitive tasks. And in some areas, it can learn faster than humans, if not yet as deeply.
From the Forbes article

In the AI-enabled future, humans will be able to converse and interact with each other in the native language of choice, not having to worry about miscommunicating intentions.

I would like to know more about how artificial intelligence will change the future?
",['social'],"Here is something I've noticed about humans: We're bad at projecting the future with all of its 2nd, 3rd ... N order effects, and we're REALLY bad a projecting and quantifying risk. So, I'm not sure that you'll get an answer that is anything more than either trivially true (""Chatbots will be commonplace"") or a correct but wasn't justified (""We'll explore the stars by sending out swarms of AI-enhanced satellites that will use their computing power to improve themselves since they are always on but have copious amounts of down-time"").But if you want to limit the timeframe to the next decade and don't mind the previous caveat, here goes:I don't hold a lot of hope out for communication without error of to speakers of different languages as you can have two native fluent speakers who have known each other for years speak to each and still have them miscommunicate."
What is the best resources to learn Graph Convolutional Neural Networks?,"
For the past few days, I am trying to learn graph convolutional networks. I saw some of the lectures on youtube. But I can not able to get any clear concept of how those networks are trained. I have a vague understanding of how to perform convolution, but I can not understand how we train them. I want a solid mathematical understanding of graph convolutional networks. So, can anyone please suggest me how to start learning graph convolutional network from start to expert level?
","['resource-request', 'geometric-deep-learning', 'graph-neural-networks']",I believe Graph Representation Learning book by William L. Hamilton is a great resource to start
Can GANs be used to generate something other than images?,"
AFAIK, GANs are used for generating/synthesizing near-perfect human faces (deepfakes), gallery arts, etc., but can GANs be used to generate something other than images?
","['deep-learning', 'applications', 'generative-adversarial-networks', 'deepfakes']","They can indeed. Although generally they are kept to images because at the moment, they are the best at that, but not the best in other areas that you might consider.GANs can be used for audio generation, with many examples such as GANsynth and GAN voice generation. But each of these tasks are outperformed by other methods. With music generation, WaveNet is the best (last I checked, and it also performs very well at speech synthesis), and a more powerful model for voice generation is achieved through the use of a VAE.This is only looking at one area that you could use GANs for, because in reality you could use them for any kind of generation if you wanted to, but at the moment the vast majority of the research into GANs is into image generation, and as such other areas do not compete with the current SOTA techniques, unless there's some big paper I've missed within the last few months."
Choosing a policy improvement algorithm for a continuing problem with continuous action and state-space,"
I'm trying to decide which policy improvement algorithm to use in the context of my problem. But let me emerge you into the problem
Problem
I want to move a set of points in a 3D space. Depending on how the points move, the environment gives a positive or negative reward. Further, the environment does not split up into episodes, so it is a continuing problem. The state space is high-dimensional (a lot of states are possible) and many states can be similar (so state aliasing can appear), also states are continuous. The problem is dense in rewards, so for every transition, there will be a negative or positive reward, depending on the previous state.
A state is represented as a vector with dimension N (initially it will be something like ~100, but in the future, I want to work with vectors up to 1000).
In the case of action, it is described by a matrix 3xN, where N is the same as in the case of the state. The first dimension comes from the fact, that action is 3D displacement.
What I have done so far
Since actions are continuous, I have narrowed down my search to policy gradient methods. Further, I researched methods, that work with continuous state spaces. I found a deep deterministic policy gradient (DDPG) and the Proximal Policy Gradient (PPO) would fit here. Theoretically, they should work but I'm unsure and any advice would be gold here.
Questions
Would those algorithms be suitable for the problem (PPO or DDPG)?
There are other policy improvement algorithms that would work here or a family of policy improvement algorithms?
","['reinforcement-learning', 'policy-gradients']",
What should the output of a neural network that needs to classify in an unsupervised fashion XOR data be?,"
XOR data, without labels:
[[0,0],[0,1],[1,0],[1,1]]

I'm using this network for auto-classifying XOR data:
H1  <-- Dense(units=2, activation=relu)    #any activation here
Z   <-- Dense(units=2, activation=softmax) #softmax for 2 classes of XOR result
Out <-- Dense(units=2, activation=sigmoid) #sigmoid to return 2 values in (0,1)

There's a logical problem in the network, that is, Z represents 2 classes,
however, the 2 classes can't be decoded back to 4 samples of XOR data.
How to fix the network above to auto-classify XOR data, in unsupervised manner?
","['neural-networks', 'machine-learning', 'deep-learning', 'unsupervised-learning', 'autoencoders']","How to fix the network above to auto-classify XOR data, in unsupervised manner?This cannot be done, except accidentally.Unsupervised learning cannot replace or emulate supervised learning.As a thought experiment, consider why you would expect the network to discover XOR, when simply considering outputs rounded to binary, you could equally find AND, OR, NAND, NOR or any of the 16 possible mapping functions from input to output. All of the possible maps are equally valid functions, and there is no reason why a discovered function mapping should become any one of them by preference.Unsupervised learning approaches typically find patterns that optimise some measure across the dataset without using labelled data. Clustering is a classic example, and auto-encoding is sometimes considered unsupervised because there is no separate label (although the term self-supervised is also used, because there is still technically a label used in training, it happens to equal the input).You cannot use auto-encoding approaches here anyway, because XOR needs to map $\{0,1\} \times \{0,1\} \rightarrow \{0,1\}$You could potentially use a loss function based on how close to a 0 or 1 any output is. That should cause the network to converge to one of the 16 possible binary functions, based on random initialisation. For example, you could use $y(1-y)$ as the loss."
"Why is the policy loss the mean of $-Q(s, \mu(s))$ in the DDPG algorithm?","
I am trying to implement the DDPG algorithm based on this paper.
The part that confuses me is the actor network's update.
I don't understand why the policy loss is simply the mean of $-Q(s, \mu(s))$, where $Q$ is the critic network and $\mu$ is the policy network.
How does one arrive at this?
","['reinforcement-learning', 'policy-gradients', 'pytorch', 'ddpg']","This is not quite the loss that is stated in the paper.For standard policy gradient methods the objective is to maximise $v_{\pi_\theta}(s_0)$ -- note that this is analogous to minimising $-v_{\pi_\theta}(s_0)$. This is for a stochastic policy. In DDPG the policy is now assumed to be deterministic.In general, we can write
$$v_\pi(s) = \mathbb{E}_{a\sim\pi}[Q(s,a)]\;;$$
to see this note that
$$Q(s,a) = \mathbb{E}[G_t | S_t = s, A_t=a]\;;$$
so if we took expectation over this with respect to the distribution of $a$ we would get
$$\mathbb{E}_{a\sim\pi}[\mathbb{E}[G_t|S_t=s, A_t=a]] = \mathbb{E}[G_t|S_t=s] = v_\pi(s)\;.$$However, if our policy is deterministic then $\pi(\cdot|s)$ is a point mass (a distribution which has probability 1 for a specific point and 0 everywhere else) for a certain action, so $\mathbb{E}_{a\sim\pi}[ Q(s,a)] = Q(s,a=\pi(s)) = v_\pi(s)$. Thus the objective is still to maximise $v_\pi(s)$ it is just that now we know the policy is deterministic we say we want to maximise $Q(s,a=\pi(s))$.The policy gradient of this term was shown to be
\begin{align}
\nabla_\theta Q(s,a=\pi_\theta(s)) & \approx \mathbb{E}_{s \sim \mu}[\nabla_\theta Q(s,a=\pi_\theta(s))]\;; \\ & =  \mathbb{E}_{s\sim\mu}[\nabla_aQ(s,a=\pi(s)) \nabla_\theta \pi_\theta(s)]\;;
\end{align}where if we put a minus at the front of this term then we would arrive at the loss from the paper. Intuitively this makes sense, you want to know how much the action-value function changes with respect to the parameter of the policy, but this would be difficult to directly calculate so you use the chain rule to see how much the action-value function changes with $a$ and in turn how much $a$ (i.e. our policy) changes with the parameter of the policy.I realise I have changed notation from the paper you are reading so here $\pi$ is our policy as opposed to $\mu$ and here where I have used $\mu$ I take this to be the state distribution function."
Are tabular reinforcement learning methods obsolete (or getting obsolete)?,"
While learning RL, I came across some problems where the Q-matrix that I need to make is very very large. I am not sure if it is ever practical. Then I research and came to this conclusion that using the tabular method is not the only way, in fact, it is a very less powerful tool as compared to other methods such as deep RL methods.
Am I correct in this understanding that with the increasing complexity of problems, tabular RL methods are getting obsolete?
","['reinforcement-learning', 'deep-learning']","Am I correct in this understanding that with the increasing complexity of problems, tabular RL methods are getting obsolete?Individual problems don't get any more complex, but the scope of solvable environments increases due to research and discovery of better or more apt methods.Using deep RL methods with large neural nets can be a lot less efficient for solving simple problems. So tabular methods still have their place there.Practically, if your state/action space (number of states times number of actions) is small enough to fit a Q table in memory, and it is possible to visit all relevant state/action pairs multiple times in a relatively short time, then tabular methods offer guarantees of convergence that approximate methods cannot. So tabular approaches are often preferred if they are appropriate.Many interesting, cutting edge problems that are relevant to AI, such as autonomous robots acting in the real world, do not fit the tabular approach. In that sense, the approach is ""obsolete"" in that it no longer provides challenging research topics for practical AI (there are still unanswered theoretical questions, such as proof of convergence for Monte Carlo control).It is still worth understanding tabular value-based methods in detail, because they form the foundations of the more complex deep learning methods. In some sense they represent ideal solutions that deep RL tries to approximate, and the design of tabular solutions can be the inspiration for changes and adjustments to neural-network methods."
How does the target network in double DQNs find the maximum Q value for each action?,"
I understand the fact that the neural network is used to take the states as inputs and it outputs the Q-value for state-action pairs. However, in order to compute this and update its weights, we need to calculate the maximum Q-value for the next state $s'$. In order to get that, in the DDQN case, we input that next state $s'$ in the target network.
What I'm not clear on is: how do we train this target network itself that will help us train the other NN? What is its cost function?
","['reinforcement-learning', 'training', 'dqn', 'deep-rl', 'double-dqn']","Both in DQN and in DDQN, the target network starts as an exact copy of the Q-network, that has the same weights, layers, input and output dimensions, etc., as the Q-network.The main idea of the DQN agent is that the Q-network predicts the Q-values of actions from a given state and selects the maximum of them and uses the mean squared error (MSE) as its cost/loss function. That is, it performs gradient descent steps on$$\left(Y_{t}^{\mathrm{DQN}} -Q\left(s_t, a_t;\boldsymbol{\theta}\right)\right)^2,$$where the target $Y_{t}^{\mathrm{DQN}}$ is defined (in the case of DQN) as$$
Y_{t}^{\mathrm{DQN}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}^{-}\right)
$$$\boldsymbol{\theta}$ are the Q-network weights and $\boldsymbol{\theta^-}$ are the target network weights.After a usually fixed number of timesteps, the target network updates its weights by copying the weights of the Q-network. So, basically, the target network never performs a feed-forward training phase and, thus, ignores a cost function.In the case of DDQN, the target is defined as$$
Y_{t}^{\text {DDQN}} \equiv R_{t+1}+\gamma Q\left(S_{t+1}, \underset{a}{\operatorname{argmax}} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right) ; \boldsymbol{\theta}_{t}^{-}\right)
$$This target is used to decouple the selection of the action (i.e. the argmax part) from its evaluation (i.e. the computation of the Q value at the next state with this selected action), as stated the paper that introduced the DDQN)The max operator in standard Q-learning and DQN, in (2) and (3), uses the same values both to select and to evaluate an action. This makes it more likely to select overestimated values, resulting in overoptimistic value estimates. To prevent this, we can decouple the selection from the evaluation"
What is the expectation of an empirical model in model based RL?,"
In the paper - ""Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems"", on page 1083, on the 6th line from the bottom, the authors define expectation of the empirical model as
$$\hat{\mathbb{E}}_{s,s',a}[V(s')] = \sum_{s' \in S} \hat{P}^{a}_{s, s'}V(s').$$
I didn't understand the significance of this quantity since it puts $V(s')$ inside an expectation while assuming the knowledge of $V(s')$ in the definition on the right.
A clarification in this regard would be appreciated.
EDIT:
The paper defines $\hat{P}^{a}_{s, s'}$ as,
$$\hat{P}^{a}_{s, s'} = \frac{|(s, a, s', t)|}{|(s, a, t)|}.$$
Where $|(s, a, t)|$ is the number of times state $s$ was visited and action $a$ was taken and $|(s, a, s', t)|$ as the number of times among the $|(s, a, t)|$ times $(s, a)$ was visited when the next state landed in was $s'$ during model learning.
No explicit definition for $V$ is provided however, $V^{\pi}$ is defined as the usual expected discounted return, using the same definition as Sutton and Barto or other sources.
","['model-based-methods', 'pac-learning']","If I understand your question correctly, the significance of this is due to the fact that $s'$ is random. In the RHS of the equation it is assumed that $V(\cdot)$ is known for each state, but the quantity is measuring the expected value of the next state given the current state and action."
Ways to keep up with the latest developments in Machine Learning and AI?,"
With over 100 papers published in the area of artificial intelligence, machine learning and their subfields every day (source), accounting for ~3% of all publications world wide per year (source) and dozens of annual conferences like NeurIPS, ICML, ICLR, ACL, ... I wonder how you keep up with the current state of the art and latest developments? The field is progressing very fast, models that were considered SOTA not even a decade ago are now (almost) outdated (Attention Is All You Need). A lot of this progress is driven by big tech companies (source), e.g. 12% of all papers accepted at NeurIPS 2019 have at least one author from Google and DeepMind (source).
My strategy is to read blogs and articles to maintain a general overview and not to miss any important breakthroughs. To be up to date in subfields of my own interest, I read specific papers once in a while. What are your personal strategies? Continuous education is a big keyword here. It's not about understanding every detail and being able to reproduce results, but rather maintaining a bird's eye view, having an idea about the direction of research and knowing whats already possible.
To name a few of my preferred sources there are the research blogs of the big players: OpenAI, DeepMind, Google AI, FAIR. Further there are very good personal blogs with a more educational character, like the well known one of Christopher Olah, the recently started one of Yoshua Bengio and the one from Jay Alammar. Unfortunately finding personal blogs is hard, it often depends on luck and referrals, also the update frequency is generally lower since these people have (understandably) other important things to do in life as well.
Therefore I'm always looking for new sources, which I can bookmark and read later, if I like to avoid doing other stuff.
Can you name any other personal / corporate research blogs or news websites that publish latest advances in ML & AI?
","['machine-learning', 'research', 'papers', 'state-of-the-art', 'ai-development']",
Is a reward given at every step or only given when the RL agent fails or succeeds?,"
In reinforcement learning, an agent can receive a positive reward for correct actions and a negative reward for wrong actions, but does the agent also receive rewards for every other step/action?
","['reinforcement-learning', 'reward-design', 'reward-functions', 'reward-shaping', 'dense-rewards']","In reinforcement learning (RL), an immediate reward value must be returned after each action, along with the next state. This value can be zero though, which will have no direct impact on optimality or setting goals.Unless you are modifying the reward scheme to try and make an environment easier to learn (called reward shaping), then you should be aiming for a ""natural"" reward scheme. That means granting reward based directly on the goals of the agent.Common reward schemes might include:+1 for winning a game or reaching a goal state granted only at the end of an episode, whilst all other steps have a reward of zero. You might also see 0 for a draw and -1 for losing a game.-1 per time step, when the goal is to solve a problem in minimum time steps.a reward proportional to the amount of something that the agent produces - e.g. energy, money, chemical product, granted on any stop where this product is obtained, zero otherwise. Potentially a negative reward based on something else that the agent consumes in order to produce the product, e.g. fuel."
How AlphaGo Zero is learning from $\pi_t$ when $z_t = -1$?,"
I have questions on the way AlphaGo Zero is trained.
From original AlphaGo Zero paper, I knew that AlphaGo Zero agent learns a policy, value functions by the gathered data $\{(s_t, \pi_t, z_t)\}$ where $z_t = r_T \in \{-1,1\}$.
However, the fact that the agent tries to learn a policy distribution when $z_t = -1$ seems to be counter-intuitive (at least to me).
My assertion is that the agent should not learn the policy distribution of when it loses (i.e, gets $z_t=-1$), since such a policy will guide it to lose.
I think I have missed some principles and resulted in that assertion. Or is my assertion reasonable, either?
","['reinforcement-learning', 'monte-carlo-tree-search', 'alphago-zero', 'alphago']","Intuitively I think there's definitely something to be said for your idea, but it's not a 100% clear case, and there are also some arguments to be made for the case that we should also be training the policy from data where $z_t = -1$.So, first let's establish that if we do indeed choose to discard any and all data where $z_t = -1$, we are in fact discarding a really significant part of our data; we're discarding 50% of all the data we generate in games like Go where there are no draws (less than that in games like Chess where there are many draws, but still a significant amount of data). So this is not a decision to be made lightly (it has a major impact on our sample efficiency), and we should probably only do it if we really believe that policy learning from any data where $z_t = -1$ is actually harmful.The primary idea behind the self-play learning process in AlphaGo Zero / AlphaZero may intuitively be explained as:Of course, there can be exceptions to point 1. if we get unlucky, but on average we expect that to be true. Crucially for your question, we don't expect this to only be true in games where we actually won, but still also be true in games that we ultimately end up losing. Even if we still end up losing the game played according to the MCTS search, we expect that we at least put up a slightly better fight with the MCTS + $\pi_t$ combo than we would have done with just $\pi_t$, and so it may still be useful to learn from it (to at least lose less badly).On top of this, it is important to consider that we intentionally build in exploration mechanisms in the self-play training process, which may ""pollute"" the signal $z_t$ without having polluted the training target for the policy. In self-play, we do not always pick the action with the maximum visit count (as we would in an evaluation match / an important tournament game), but we pick actions proportionally to the MCTS visit counts. This is done for exploration, to introduce extra variety in the experience that we generate, to make sure that we do not always learn from exactly the same games. This can clearly affect the $z_t$ signal (because sometimes we knowingly make a very very bad move just for the sake of exploration), but it does not affect the policy training targets encountered throughout that game; MCTS still tries to make the best that it can out of the situations it faces. So, these policy training targets are still likely to be useful, even if we ""intentionally"" made a mistake somewhere along the way which caused us to lose the game."
Is it common to have extreme policy's probabilities?,"
I have implemented several policy gradient algorithms (REINFORCE, A2C, and PPO) and am finding that the resultant policy's action probability distributions can be rather extreme. As a note, I have based my implementations on OpenAI's baselines. I've been using NNs as the function approximator followed by a Softmax layer. For example, with Cartpole I end up with action distributions like $[1.0,3e-17]$. I could understand this for a single action, potentially, but sequential trajectories end up having a probability of 1. I have been calculating the trajectory probability by $\prod_i \pi(a_i|s_i)$. Varying the learning rate changes how fast I arrive at this distribution, I have used learning rates of $[1e-6, 0.1]$. It seems to me that a trajectory's probability should never be 1.0 or 0.0 consistently, especially with a stochastic start. This also occurs for environments like LunarLander.
For the most part, the resulting policies are near-optimal solutions that pass the criteria for solving the environments set by OpenAI. Some random seeds are sub-optimal
I have been trying to identify a bug in my code, but I'm not sure what bug would be across all 3 algorithms and across environments.
Is it common to have such extreme policy's probabilities? Is there a common way to handle an update so the policy's probabilities do not end up so extreme? Any insight would be greatly appreciated!
","['reinforcement-learning', 'policy-gradients', 'policies']","Your policy gradient algorithms appear to be working as intended. All standard MDPs have one or more deterministic optimal solutions, and those are the policies that solvers will converge to. Making any of these policies more random will often reduce their effectiveness, making them sub-optimal. So once consistently good actions are discovered, the learning process will reduce exploration naturally as a consequence of the gradients, much like a softmax classifier with a clean dataset.There are some situations where a stochastic policy can be optimal, and you could check your implementations can find those:A partially observable MDP (POMDP) where one or more key states requiring different optimal actions are indistinguishable to the agent. For example, the state could be available exits in a corridor trying to get the end in a small maze, where one location secretly (i.e. without the agent having any info in the state representation that the location is different) reverses all directions, so that progressing along it is not possible for a deterministic agent, but a random agent would eventually get through.In opposing guessing games where a Nash equilibrium occurs for specific random policies. For example scissor, paper, stone game where the optimal policy in self-play should be to choose each option randomly with 1/3 chance.The first example is probably easiest to set up a toy environment to show that your implementations can find stochastic solutions when needed. A concrete example of that kind of environment is in Sutton & Barto: Reinforcement Learning, An Introduction chapter 13, example 13.1 on page 323.Setting up opposing agents in self-play is harder, but if you can get it to work and discover the Nash equilibrium point for the policies, it would be further proof that you have got something right."
How do we make our outputs to have the same size as the true mask?,"
When we are doing multi-label segmentation tasks, our y_true (the mask) will be (w, h, 3), but, in our model, at the last layer, we will be getting (w, h, number of classes) as output.
How do we make our outputs to have the same size as the true mask so that to apply the loss function, given that, currently, the shapes are not equal? Also, if we are done with applying the loss function and trained the model, how do I make results in the shape of (w, h, 3) from (w, h, number of classes)?
","['deep-learning', 'image-segmentation']",
How should I implement the backward pass through a flatten layer of a CNN?,"
I am making a NN library without any other external NN library, so I am implementing all layers, including the flatten layer, and algorithms (forward and backward pass) from scratch. I know the forward implementation of the flatten layer, but is the backward just reshaping it or not? If yes, can I just call a simple NumPy's reshape function to reshape it?
","['neural-networks', 'convolutional-neural-networks', 'backpropagation', 'implementation']","Yes, a simple reshape would do the trick. A flattening layer is just a tool for reshaping data/activations to make them compatible with other layers/functions. The flattening layer doesn't change the activations themselves, so there is no special backpropagation handling needed other than changing back the shape."
How can one be sure that a particular neural network architecture would work?,"
Traditionally, when working with tabular data, one can be sure(or at least know) that a model works because the included features could explain a target variable, say ""Price of a ticket"" good. More features can be then be engineered to explain the target variable even better.
I have heard people say, that there is no need to hand-engineer features when working with CNNs or RNNs or Deep Neural Networks, provided all the advancements in AI and computation. So, my question is, how would one know, before training, why a particular architecture worked(or would work) when it did or why it didn't when the performance isn't acceptable or very bad. And also that not all of us would have the time to try out all possible architectures, how can one know or at least be sure that something would work for the problem in hand. Or to say, what are the things one needs to follow when designing an architecture to train for a problem, to ensure that an architecture will work?
","['convolutional-neural-networks', 'ai-design', 'recurrent-neural-networks', 'deep-neural-networks', 'architecture']",
Why is symbolic AI not so popular as ANN but used by IBM's Deep Blue?,"
Everybody is implementing and using DNN with, for example, TensorFlow or PyTorch.
I thought IBM's Deep Blue was an ANN-based AI system, but this article says that IBM's Deep Blue was symbolic AI.
Are there any special features in symbolic AI that explain why it was used (instead of ANN) by IBM's Deep Blue?
","['neural-networks', 'machine-learning', 'comparison', 'symbolic-ai', 'deep-blue']","ANNs as used today need 1. a lot of data 2. a lot of computational power. Before we had any of the above two, we didn't really know how to properly build ANNs since we didn't quite have the means to train the network, and thus couldn't evaluate it.""Symbolic AI"" on the other hand, is very much just a bunch of if-else/logical conditions, much like regular programming. You don't need to think too much about the whole ""symbolic"" part of it. The main/big breakthrough is that you had a lot of clever ""search algorithms"" and a lot of computation power relative to before.Point being, is just that symbolic AI was the main research program at the time, and people didn't really bother with ""connectionist"" methods."
Are there examples of agents that use a more modest number of parameters on Pendulum (or similar environments)?,"
I'm looking at some baseline implementations of RL agents on the Pendulum environment. My guess was to use a relatively small neural net (~100 parameters).
I'm comparing my solution with some baselines, e.g. the top entry on the Pendulum leaderboard. The models for these solutions are typically huge, i.e. ~120k parameters. What's more, they use very large replay buffers as well, like ~1M transitions. Such model sizes seem warranted for Atari-like environments, but for something as small as the Pendulum, this seems like complete overkill to me.
Are there examples of agents that use a more modest number of parameters on Pendulum (or similar environments)?
","['reinforcement-learning', 'architecture', 'environment', 'ddpg', 'weights']",
How can the FCNN reduce the dimensions of the input from $1048 \times 100$ to $523 \times 100$ with max-pooling?,"
I am trying to implement a paper on Image tempering detection and localization, the paper is Image Manipulation Detection and Localization Based on the Dual-Domain Convolutional Neural Networks, I was able to implement the SCNN, the one surrounded by red dots, I could not quite understand the FCNN, the one that is surrounded with blue dots.
The problem I am facing is: How the network made features vector from (1048 x 100) to (523 x 100) through max-pooling (instead of 524 x 100), and from (523 x 100) to (260 x 100) and then (260 x 100) to (256, ).

It appears that the given network diagram might be wrong, but, if it is wrong, how could it be published in IEEE. Please, help me understand how the FCNN is constructed.
","['neural-networks', 'convolutional-neural-networks', 'image-segmentation', 'convolutional-layers', 'pooling']",
Why is GPT-3 such a game changer?,"
I've been hearing a lot about GPT-3 by OpenAI, and that it's a simple to use API with text in text out and has a big neural network off 175B parameters.
But how did they achieve this huge number of parameters, and why is it being predicted as one of the greatest innovations?
","['reinforcement-learning', 'open-ai', 'gpt']",
NEAT can't solve XOR completely,"
I'm currently implementing the NEAT algorithm. But problems occur when testing it with problems which don't have a linear solution(for example xor). My xor only produces 3 correct outputs once at a time:
1, 0 -> 0.99
0, 0 -> 0
1, 1 -> 0
0, 1 -> 0

My genome class works fine, so I guess that the problem occurs on breeding or that my config is wrong.
Config
const size_t population_size = 150;
const size_t inputs = 3 (2 inputs + bias);
const size_t outputs = 1;
double compatibility_threshold = 3;
double steps = 0.01;
double perturb_weight = 0.9;
double mutate_connection = 0.05;
double mutate_node = 0.03;
double mutate_weights = 0.8;
double mutate_disable = 0.1;
double mutate_enable = 0.2;
double c_excess = 1;
double c_disjoint = 1;
double c_weight = 0.4;
double crossover_chance = 0.75;

Does anyone has an idea what the problem might be? I proof read my code multiple times, but wasnt able the figure it out.
Here is the github link to my code(not documented): click
","['machine-learning', 'evolutionary-algorithms', 'unsupervised-learning', 'neat']",
Is there a place where people can share (or buy) ready made neural networks?,"
Is there a place where people can share (or buy) ready made neural networks instead of creating them themselves? Something like a Wikipedia for DNNs?
","['neural-networks', 'resource-request']",
Is there a way to get landmark features automatically learned by a neural network?,"
Is there a way to get landmark features automatically learned by a neural network without having to manually pre-label them in the images that are being fed into the network?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'self-supervised-learning', 'automated-machine-learning']",
Why can't we train neural networks in a peer-to-peer manner?,"
I have recently been exposed to the concept of decentralized applications,
I know that neural networks require a lot of parallel computing infra for training.
What are the technical difficulties one may face for training neural networks in a p2p manner?
","['neural-networks', 'training', 'distributed-computing']",
How can I change observation states' values in OpenAI gym's cartpole environment?,"
I am learning with the OpenAI gym's cart pole environment.
I want to make the observation states discrete (with small stepsize) and for that purpose, I need to change two of the observations from [$
-\infty, \infty$] to some finite upper and lower limits. (By the way, these states are velocity and pole velocity at the tip).
How can I change these limits in the actual gym's environment?
Any other suggestions are also welcome.
","['reinforcement-learning', 'q-learning', 'open-ai', 'gym']",
"In continuous action spaces, how is the standard deviation, associated with Gaussian distribution from which actions are sampled, represented?","
I have a question about implementing policy gradient methods for problems with continuous action spaces.
Assume that actions are sampled from a diagonal Gaussian distribution with mean vector $\mu$ and standard deviation vector $\sigma$. As far as I understand, we can define a neural network that takes the current state as the input and returns a $\mu$ as its output. According to OpenAI Spinning Up, the standard deviation $\sigma$ can be represented in two different ways:

I don't completely understand the first method. Does it mean that we must set the log standard deviations to fix numbers? Then how do we choose these numbers?
","['reinforcement-learning', 'policy-gradients', 'probability-distribution']",
Is there an upper limit to the maximum cumulative reward in a deep reinforcement learning problem?,"
Is there an upper limit to the maximum cumulative reward in a deep reinforcement learning problem?
For example, you want to train a DQN agent in an environment, and you want to know what the highest possible value you can get from the cumulative reward is, so you can compare this with your agents performance.
","['reinforcement-learning', 'deep-rl', 'q-learning', 'dqn', 'rewards']",
Is A* with an admissible but inconsistent heuristic optimal?,"
I understand that, in tree search, an admissible heuristic implies that $A*$ is optimal. The intuitive way I think about this is as follows:
Let $P$ and $Q$ be two costs from any respective nodes $p$ and $q$ to the goal. Assume $P<Q$. Let $P'$ be an estimation of $P$. $P'\le P \Rightarrow P'<Q$. It follows from uniform-cost-search that the path through $p$ must be explored.
What I don't understand, is why the idea of an admissible heuristic does not apply as well to ""graph-search"". If a heuristic is admissible but inconsistent, would that imply that $A*$ is not optimal? Could you please provide an example of an admissible heuristic that results in a non-optimal solution?
","['search', 'heuristics', 'a-star', 'admissible-heuristic', 'consistent-heuristic']",
When would bias regularisation and activation regularisation be necessary?,"
For Keras on TensorFlow, a layer class constructor comes with these:

kernel_regularizer=...
bias_regularizer=...
activity_regularizer=...

For example, Dense layer:
https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense#arguments_1
The first one, kernel_regularizer is easy to understand, it regularises weights, makes weights smaller to avoid overfitting on training data only.
Is kernel_regularizer enough? When should I use bias_regularizer and activity_regularizer too?
","['neural-networks', 'keras', 'regularization', 'weights']",Regularizer's are used as a means to combat over fitting.They essentially create a cost function penalty which tries to prevent quantities from becoming to large.  I have primarily used kernel regularizers. First I try to control over fitting using dropout layers. If that does not do the job or leads to poor training accuracy I try the Kernel regularizer. I usually stop at that point. I think activity regularization would be my next option to prevent outputs from becoming to large. I suspect weight regularization effectively can pretty much achieve the same result.
Why would the learning rate curve go backwards?,"
I'm working on recognizing the numbers 3 and 7 using the MNIST data set. I'm using cnn_learner() function from fastai library.
When I plotted the learning rate, the curve started going backward after a certain value on X-axis. Can someone please explain what does it signify?

","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'learning-rate']","I have not used fastai library but this also happens on tensorboard when you have more than one training being recorded on the same plot.Looking at the picture, I think this is a very special type of graph because for a single LR value you have 2 loss values associated. Put in other words, you have the same LR value for different loss values. My guess it that some time-dependent issue is messing things here.Another intuition that may help to solve the issue is representing data differently. If you rotate the graph 90º left-wise you could see how the LR is evolving with different loss values. LR should be decreasing along with loss value, but in this case it is not like that either. So review how you are setting the LR too!My list of to-check would be:Hope it helps"
"How does an episode end in OpenAI Gym's ""MountainCar-v0"" environment? [closed]","







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 2 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I am working on OpenAI's ""MountainCar-v0"" environment.  In this environment, each step that an agent takes returns (among other values) the variable named done of type boolean. The variable gets a True value when the episode ends. However, I am not sure how each episode ends. My initial understanding was that an episode should end when the car reaches the flagpost. However, that is not the case.
What are the states/actions under which the episode terminates in this environment?
","['reinforcement-learning', 'environment', 'gym']","The episode ends when either the car reaches the goal, or a maximum number of timesteps has passed. By default the episode will terminate after 200 steps. You can customize this with the _max_episode_steps attribute of the environment."
"What kind of policy evaluation and policy improvement AlphaGo, AlphaGo Zero and AlphaZero are using","
I'm trying to find out what kind of policy improvement and policy evaluation AlphaGo, AlphaGo Zero, and AlphaZero are using. By looking into their respective paper and SI, I can conclude that it is a kind of policy gradient actor-critic approach, where the policy is evaluated by a critic and is improved by and actor. Yet still can't fit it to any of the known policy gradient algorithms.
","['reinforcement-learning', 'policy-gradients', 'alphazero']",
Advantages of training Neural Networks based on analytic success criteria,"
What is the reason to train a Neural Network to estimate a task's success (i.e. robotic grasp planning) using a simulator that is based on analytic grasp quality metrics?
Isn't a perfectly trained NN going to essentially output the same probability of task success as the analytic grasp quality metrics that were used to train it? What benefits does this NN have with respect to just directly using said analytic grasp quality metrics to determine whether a certain grasp candidate is good or bad? Analytic metrics are by definition deterministic, so I fail to understand the reason for using them to train a NN that will ultimately output the same result.
This approach is used in high-caliber works like the Dex-Net2 from Berkeley Automation. I am rather new to the field and the only reason I can think of is computational efficiency in production?
","['deep-learning', 'deep-neural-networks', 'supervised-learning']",
"CIFAR-10 can't get above 10% Accuracy with MobileNet, VGG16 and ResNet on Keras","
I'm trying to train the most popular Models (mobileNet, VGG16, ResNet...) with the CIFAR10-dataset but the accuracy can't get above 9,9%. I want to do that with the completely model (include_top=True) and without the weights from imagenet.
I have tried increasing/decreasing dropout and learning rate and I changed the optimizers but I become always the same accuracy.
with weights='imagenet' and include_top=False I achieve an accuracy of over 90% but I want to train the model without those parameters.
Is there any solution to solve this? It is possible, that the layers of those Models are not set to be trainable?
train_generator = ImageDataGenerator(
                                    rotation_range=2, 
                                    horizontal_flip=True,
                                    zoom_range=.1 )
val_generator = ImageDataGenerator(
                                    rotation_range=2, 
                                    horizontal_flip=True,
                                    zoom_range=.1)

train_generator.fit(x_train)
val_generator.fit(x_val)

base_model_1 = MobileNet(include_top=True,weights=None,input_shape=(32,32,3),classes=y_train.shape[1])

batch_size= 100
epochs=50

learn_rate=.001

sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)
adam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

model_1.compile(optimizer=adam,loss='sparse_categorical_crossentropy',metrics=['accuracy'])

model_1.fit_generator(train_generator.flow(x_train,y_train,batch_size=batch_size),
                      epochs=epochs,
                      steps_per_epoch=x_train.shape[0]//batch_size,
                      validation_data=val_generator.flow(x_val,y_val,batch_size=batch_size),validation_steps=250,
                      verbose=1)

Results of MobileNet:
    Epoch 1/50
350/350 [==============================] - 17s 50ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1021
Epoch 2/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1030
Epoch 3/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1016
Epoch 4/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1014
Epoch 5/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1040
Epoch 6/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1009
Epoch 7/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1035
Epoch 8/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1013
Epoch 9/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1029
Epoch 10/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1023
Epoch 11/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1017
Epoch 12/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1020
Epoch 13/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1020
Epoch 14/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1033
Epoch 15/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1011
Epoch 16/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1016
Epoch 17/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1024
Epoch 18/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1024
Epoch 19/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1041
Epoch 20/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1010
Epoch 21/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1022
Epoch 22/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1014
Epoch 23/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1035
Epoch 24/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1032
Epoch 25/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1012
Epoch 26/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1018
Epoch 27/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1022
Epoch 28/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1031
Epoch 29/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1022
Epoch 30/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1015
Epoch 31/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1028
Epoch 32/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1015
Epoch 33/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1030
Epoch 34/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1003
Epoch 35/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1044
Epoch 36/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1012
Epoch 37/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1022
Epoch 38/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1021
Epoch 39/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1028
Epoch 40/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1012
Epoch 41/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1035
Epoch 42/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1009
Epoch 43/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1034
Epoch 44/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1024
Epoch 45/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1016
Epoch 46/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1028
Epoch 47/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1016
Epoch 48/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1033
Epoch 49/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1018
Epoch 50/50
350/350 [==============================] - 17s 49ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.1023

<tensorflow.python.keras.callbacks.History at 0x7fa30b188e48>

","['neural-networks', 'machine-learning', 'deep-learning', 'tensorflow', 'keras']",
Prioritised Remembering in Experience Replay (Q-Learning),"
I'm using Experience Replay based on the original Prioritized Experience Replay (PER) paper. In the paper authors show ~ an order of magnitude increase in data efficiency from prioritized sampling. There is space for further improvement, since PER remembers all experiences, regardless of their importance.
I'd like to extend PER so it remembers selectively based on some metric, which would determine whether the experience is worth remembering or not. The time of sampling and re-adjusting the importance of the experiences increases with the number of samples remembered, so being smart about remembering should at the very least speed-up the replay, and hopefully also show some increase in data efficiency.
Important design constrains for this remembering metric:

compatibility with Q-Learning, such as DQN
computation time, to speed up the process of learning and not trade off one type of computation for another
simplicity


My questions:

What considerations would you make for designing such a metric?
Do you know about any articles addressing the prioritized experience memorization for Q-Learning?

","['reinforcement-learning', 'q-learning', 'dqn', 'experience-replay']",
How is AI helping humanity?,"

There was a lot of Negative news on Artificial Intelligence. Most people were first exposed to the idea of artificial intelligence from Hollywood movies, long before they ever started seeing it in their day-to-day lives. This means that many people misunderstand the technology. When they think about common examples that they’ve seen in movies or television shows, they may not realize that the killer robots they’ve seen were created to sell emotional storylines and drive the entertainment industry, rather than to reflect the actual state of AI technology.

There are few questions on our SE on how AI impacts/harms humankind. For example, How could artificial intelligence harm us? and Could artificial general intelligence harm humanity?
However, now, I'm looking for the positive impacts of AI on humans. How could AI help humankind?
","['philosophy', 'social']","For good or bad, AI is the next step in automation. The impact which is already visible, and trends show will continue in the future, is the eradication of repetitive and body-straining labor.Hopefully, the transformation will be gradual enough for the global labor market to re-adjust, otherwise, we'll face a problem of growing unemployment. It seems to me that we've become aware enough to foresee bad outcomes of our inventions, hence in almost every dimension affected by AI, a plausible and either positive or negative future can be presented, depending on the sentiment of the storyteller.Regardless of what different experts and sci-fi writers tell us about the future, actually predicting it is a futile endeavour. Considering that predictions made for a dynamic system, even when we have a lot of data and good models (like about the weather), become unreliable just for a few weeks ahead."
Why does the number of channels in the PointNet increase as we go deeper?,"
For example, in PointNet, you see the 1D convolutions with the following channels 64 -> 128 -> 1024. Why not e.g. 64 -> 1024 -> 1024 or 1024 -> 1024 -> 1024?
","['convolutional-neural-networks', 'convolution', 'filters', 'convolutional-layers', '1d-convolution']",
How can a learning rate that is too large cause the output of the network (and the error) to go to infinity?,"
It happened to my neural network, when I use a learning rate of <0.2 everything works fine, but when I try something above 0.4 I start getting ""nan"" errors because the output of my network keeps increasing.
From what I understand, what happens is that if I choose a learning rate that is too large, I overshoot the local minimum. But still, I am getting somewhere, and from there I'm moving in the correct direction. At worst my output should be random, I don't understand what is the scenario that causes my output and error to approach infinity every time I run my NN with a learning rate that is too large (and it's not even that large)

How does the red line go to infinity ever? I kind of understand it could happen if we choose a crazy high learning rate, but if the NN works for 0.2 and doesn't for 0.4, I don't understand that
","['learning-rate', 'mean-squared-error']",
Why is the expected return in Reinforcement Learning (RL) computed as a sum of cumulative rewards?,"
Why is the expected return in Reinforcement Learning (RL) computed as a sum of cumulative rewards?
Would it not make more sense to compute $\mathbb{E}(R \mid s, a)$ (the expected return for taking action $a$ in the given state $s$) as the average of all rewards recorded for being in state $s$ and taking action $a$?
In many examples, I've seen the value of a state computed as the expected return computed as the cumulative sum of rewards multiplied by a discount factor:
$V^π(s)$ = $\mathbb{E}(R \mid s)$ (the value of state s, if we follow policy π is equal to the expected return given state s)
So, $V^π(s)$ = $\mathbb{E}(r_{t+1}+ γr_{t+2}+ (γ^2)_{t+3} + ...    \mid s) = {E}(∑γ^kr_{t+k+1}\mid s)$
as $R=r_{t+1}+ γr_{t+2}+ {γ^2}r_{t+3}, + ...   $
Would it not make more sense to compute the value of a state as the following:
$V^π(s)$ = $(r_{t+1} + γr_{t+2} + (γ^2)_{t+3}, + ...    \mid s)/k = {E}(∑γ^kr_{t+k+1}\mid s)/k $ where k is the number of elements in the sum, thus giving us the average reward for being in state s.
Reference for cumulative sum example: https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations/
","['reinforcement-learning', 'q-learning', 'rewards', 'value-functions', 'return']",
Can I apply AdaBoost on a random forest?,"
I know the random forest is a bagging technique. But what if my random forest overfits on a dataset, so I reduce the depth of the decision tree and now it is underfitting. In this scenario, can I take the under-fitted random forest with little depth and try to boost it?
","['machine-learning', 'decision-trees', 'random-forests']",
What is the purpose of a Neural Network in Reinforcement Learning when we have a Q-learning update rule?,"
I'm confused as to the purpose of training a neural network (NN) for reinforcement learning (RL) tasks such as Gridworld. In RL tasks, namely q-learning, we have a q-learning update rule, which is designed to take some state and action and compute the value of that state-action pair.
Performing this process several times will eventually produce a table of states and what action will likely lead to a high reward.
In RL examples, I've seen them train a neural network to output q-values and a loss function like MSE to compute the loss between the q-learning update rule q-value and the NN's q value.
So:
(a) Q-learning update rule-> outputs target Q-values
(b) NN -> outputs Q values
MSE to compute the loss between (a) and (b)
So, given we already know what the target Q-value is from a, why do we need to train a NN?
","['neural-networks', 'reinforcement-learning', 'deep-learning', 'training', 'q-learning']",
"Why do we use $X_{I_t,t}$ and $v_{I_t}$ to denote the reward received and the at time step $t$ and the distribution of the chosen arm $I_t$?","
I'm doing some introductory research on classical (stochastic) MABs. However, I'm a little confused about the common notation (e.g. in the popular paper of Auer (2002) or Bubeck and Cesa-Bianchi (2012)).
As in the latter study, let us consider an MAB with a finite number of arms $i\in\{1,...,K\}$, where an agent choses at every timestep $t=1,...,n$ an arm $I_t$ which generates a reward $X_{I_t,t}$ according to a distribition $v_{I_t}$.
In my understanding, each arm has an inherent distribution, which is unknown to the agent. Therefore, I'm wondering why the notation $v_{I_t}$ is used instead of simply using $v_{i}$? Isn't the distribution independent of the time the arm $i$ was chosen?
Furthermore, I ask myself: Why not simply use $X_i$ instead of $X_{I_t,t}$ (in terms of rewards). Is it because the chosen arm at step $t$ (namely $I_t$) is a random variable and $X$ depends on it? If I am right, why is $t$ used twice in the index (namely $I_t,t$)?
Shouldn't $X_{I_t}$ be sufficient, since $X_{I_t,m}$ and $X_{I_t,n}$ are drawn from the same distribution?
","['papers', 'notation', 'multi-armed-bandits', 'upper-confidence-bound']",
What is the Turing test?,"
I'm looking for intuition in simple words but also some simple insights (I don't know if the latter is possible). Can anybody shed some light on the Turing test?
","['agi', 'definitions', 'intelligence-testing', 'turing-test']","The Turing test is a test proposed by Alan Turing (one of the founders of computer science and artificial intelligence), described in section 1 of paper Computing Machinery and Intelligence (1950), to answer the questionCan machines think?More precisely, the Turing test was originally framed as an interactive quiz (denoted as the imitation game by Turing) where a human interrogator $C$ asks multiple questions to two entities, $A$ (a computer) and $B$ (a human), which stay in different rooms than the room of the interrogator, so the interrogator cannot see them, in order to figure out which one is $A$ (the computer) and which one is $B$ (the human). $A$ and $B$  can only communicate in written form or any form that avoids them being easily recognized by $C$. The goal of the computer is to fool the interrogator and make him/her believe that it is a human and the goal of $B$ is to somehow help him and make him believe that he/she is the actual human.If the computer is able to fool the interrogator and make him/her believe that it is a human, then that would be an indication that machines can think. However, note that even Turing called this game the imitation game, so Turing was aware of the fact that this game would only really show that a machine can imitate a human (unless he was using the term ""imitation"" differently than its current meaning).Nowadays, there are different variations of the Turing test and some people use the term Turing test to refer to any test that attempts to tell humans and computers apart. For example, some people consider the CAPTCHA test a Turing test. In fact, CAPTCHA stands for ""Completely Automated Public Turing Test To Tell Computers and Humans Apart"".The Turing test also has different interpretations and meanings. Some people think that the Turing test is sufficient to test that a machine can actually think and possesses consciousness, other people think that this only tests human-like intelligence (and there could be other intelligences) and some people (like me) think that this test is limited and only tests the conversational skills (and maybe other properties too) of the machine. Even Turing attempted to address these issues in the same paper (section 2), where he discusses some advantages and disadvantages of his imitation game. In any case, we can all agree that, if machines (in particular, programs like Siri, Google Home, Cortana, or Alexa) were always able to pass the Turing test, they would be a lot more useful, interesting and entertaining than they are now."
Detect data in tables of roughly the same structure,"
I would like to train a model that serializes a table of nutrition facts into it's values.
The tables can vary in form and colour, but always contain the same set of keys (e.g. carbs, fats).
Examples for these tables can be found here.
The end goal is to be able to take a picture of such a table and have it's values added to a database.
My initial idea was to train a model on finding subpictures of the individual key/value pairs and then using OCR to find out which value it actually is.
As I am relatively new to ML, I would love to have some ideas about how one could try to build this, so I can do further research on it.
Thanks
","['image-recognition', 'object-detection', 'models', 'optical-character-recognition']","Assuming all of the tables will be oriented in similar ways (label and value running horizontally) and that all writing will be printed rather than handwritten, one solution method would be to use an image segmentation method such as edge detection to segregate these horizontal (label, value) pairs and then use a library like Tesseract for OCR.There are many types of image segmentation methods that may all have value, but if my assumption holds regarding the neat, structured nature of the tables, then I think simple edge detection methods could be sufficient."
"Do we have to use the IOB format on labels in the NER dataset? If so, why?","
Do we have to use the IOB format on labels in the NER dataset (such as B-PERSON, I-PERSON, etc.) instead of using the usual format (PERSON, ORGANIZATION, etc.)? If so, why? How will it affect the performance of the model?
","['machine-learning', 'natural-language-processing', 'named-entity-recognition']",
Would it be possible to implement the principals of the K means clustering algorithm in a Neural Network,"
During a Machine Learning course which I have done I have learnt about the K means algorithm. Is it possible to use the principals of K means within a neural network?
","['neural-networks', 'clustering', 'k-means']",
What is the amount of test data needed to evaluate a CNN?,"
I have an image dataset of about 400 images. 70% of these data points were used for training, 15% for validation, and 15% for testing. I am using the 70% to train a CNN-based binary classifier. I augmented the training data to around 8000 images. That makes my test set really small in comparison. Is that ok, and what is considered a decent size of images for a test set?
","['neural-networks', 'convolutional-neural-networks', 'training', 'testing', 'data-augmentation']",
Why do we need target network in deep Q learning? [duplicate],"







This question already has an answer here:
                                
                            




Why does DQN require two different networks?

                                (1 answer)
                            

Closed 3 years ago.



I already know deep RL, but to learn it deeply I want to know why do we need 2 networks in deep RL. What does the target network do? I now there is huge mathematics into this, but I want to know deep Q-learning deeply, because I am about to make some changes in the deep Q-learning algorithm (i.e. invent a new one). Can you help me to understand what happens during executing a deep Q-learning algorithm intuitively?
","['deep-learning', 'q-learning', 'dqn', 'deep-rl', 'deep-neural-networks']",
Correct dimensionality of parameter vector for solving an MRP with linear function approximation?,"
I'm in the process of trying to learn more about RL by shadowing a course offered collaboratively by UCL and DeepMind that has been made available to the public. I'm most of the way through the course, which for auditors consists of a Youtube playlist, copies of the Jupyter notebooks used for homework assigments (thanks to some former students making them public on Github), and reading through Sutton and Barto's wonderful book Reinforcement Learning: An Introduction (2nd edition).
I've gone a little more than half of the book and corresponding course material at this point, thankfully with the aid of public solutions for the homework assignments and textbook exercises which have allowed me to see which parts of my own work that I've done incorrectly. Unfortunately, I've been unable to find such a resource for the last homework assignment offered and so I'm hoping one of the many capable people here might be able to explain parts of the following question to me.

We are given a simple Markov reward process consisting of two states and with a reward of zero everywhere.  When we are in state $s_{0}$, we always transition to $s_{1}$.  If we are in state $s_{1}$, there is a probability $p$ (which is set to 0.1 by default) of terminating, after which the next episode starts in $s_{0}$ again.  With a probability of $1 - p$, we transition from $s_{1}$ back to itself again.  The discount is $\gamma = 1$ on non-terminal steps.
Instead of a tabular representation, consider a single feature $\phi$, which takes the values $\phi(s_0) = 1$ and $\phi(s_1) = 4$.  Now consider using linear function approximation, where we learn a value $\theta$ such that $v_{\theta}(s) = \theta \times \phi(s) \approx v(s)$, where $v(s)$ is the true value of state $s$.
Suppose $\theta_{0} = 1$, and suppose we update this parameter with TD(0) with a step size of $\alpha = 0.1$.  What is the expected value of $\mathbb{E}[ \theta_T ]$ if we step through the MRP until it terminates after the first episode, as a function of $p$?  (Note that $T$ is random.)

My real point of confusion surrounds $\theta_{0}$ being given as 1. My understanding was that the dimensionality of the parameter vector should be equal to that of the feature vector, which I've understood as being (1, 4) and thus two-dimensional. I also don't grok the idea of evaluating $\mathbb{E}[ \theta_T ]$ should $\theta$ be a scalar (as an aside I attempted to simply brute-force simulate the first episode using a scalar parameter of 1 and, unless I made errors, found the value of $\theta$ to not depend on $p$ whatsoever). If $\theta$ is two-dimensional, would that be represented as (1, 0), (0, 1), or (1, 1)?
Neither the 1-d or 2-d options make intuitive sense to me so I hope there's something clear and obvious that someone might be able to point out. For more context or should someone just be interested in the assignment, here is a link to the Jupyter notebook:
https://github.com/chandu-97/ADL_RL/blob/master/RL_cw4_questions.ipynb
","['reinforcement-learning', 'markov-decision-process', 'function-approximation']",
"What is convergence analysis, and why is it needed in reinforcement learning?","
While reading a paper about Q-learning in network energy consumption, I came across the section on convergence analysis. Does anyone know what convergence analysis is, and why is convergence analysis needed in reinforcement learning?
","['reinforcement-learning', 'q-learning', 'papers', 'convergence']",
Do the order of the features ie channel matter for a 1d convolutional network?,"
Do the test dataset feature order and inference (real world) feature order have to be the same as the training dataset? For example, if features are in the order (a,c,b,e,d) for the training dataset, does that particular order have to match for the inference and test dataset?
","['deep-learning', 'convolutional-neural-networks']","Generally, order matters. A (trained) Neural Network (NN) is just a mathematical function trained on taking some given input and producing the corresponding output. So, if you train a certain node on producing large output if (and only if) an animal is present in a picture (for example), but later you give it the numeric evidence for a car being present in the image, it will still produce large output, indicating an animal being present in the image. This is simply because a node doesn't know what it is receiving or supposed to detect. It just follows its standard mathematical procedure.So, if you train your network on one kind of input data, you must provide the same kind of input data during testing (or at least you cannot simply exchange inputs or outputs of certain nodes without manually correcting for that in the remainder of the network).In the most simple case, consider whether you could simply swap the inputs to that simple function: $f(x,y) = x^2 + y$. If you swap the inputs, the output will be different. The exactly same applies to NNs.Addition:
I think this post explains and especially illustrates the intuition nicely."
What does the term $|\mathcal{A}(s)|$ mean in the $\epsilon$-greedy policy?,"
I've been looking online for a while for a source that explains these computations but I can't find anywhere what does the $|A(s)|$ mean. I guess $A$ is the action set but I'm not sure about that notation:
$$\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} Q^{\pi}(s, a)+(1-\varepsilon) \max _{a} Q^{\pi}(s, a)$$
Here is the source of the formula.
I also want to clarify that I understand the idea behind the $\epsilon$-greedy approach and the motivation behind the on-policy methods. I just had a problem understanding this notation (and also some other minor things). The author there omitted some stuff, so I feel like there was a continuity jump, which is why I didn't get the notation, etc. I'd be more than glad if I can be pointed towards a better source where this is detailed.
","['reinforcement-learning', 'monte-carlo-methods', 'notation', 'on-policy-methods', 'epsilon-greedy-policy']","This expression: $|\mathcal{A}(s)|$ means$|\quad|$ the size of$\mathcal{A}(s)$ the set of actions in state $s$or more simply the number of actions allowed in the state.This makes sense in the given formula because $\frac{\epsilon}{|\mathcal{A}(s)|}$ is then the probability of taking each exploratory action in an $\epsilon$-greedy policy. The overall expression is the expected return when following that policy, summing expected results from the exploratory and greedy action."
What is meant by degrees of freedom of latent variables?,"

...Designing such a likelihood function is typically challenging; however, we observe that features like spectrogram are effective when latent variables have limited degrees of freedom. This motivates us to infer latent variables via methods like Gibbs sampling, where we focus on approximating the conditional probability of a single variable given the others.

Above is an excerpt from a paper I've been reading, and I don't understand what the author means by degrees of freedom of latent variables. Could someone please explain with an example, or add more details?

References
Shape and Material from Sound (31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA)
","['terminology', 'papers', 'generative-model', 'latent-variable']","A good example is the degree of freedom in Student's distribution:‌ The degrees of freedom refers to the number of independent observations in a set of data.For example:When estimating a mean score or a proportion from a single sample, the number of independent observations is equal to the sample size minus one.e.g, if we have 100 observation $X_1, \ldots, X_{100}$ and we want to estimate their mean $\bar{X}$, as $\bar{X} = \frac{X_1 + \cdots + X_{100}}{100}$, if we know the mean, just we need to find the value of 99 variables from $X_1, \ldots, X_{100}$. Hence, here the degree of freedom is 99.Your referenced paragraph is a general explanation in the paper as well. However, base on the above example, the degree of freedom in the paragraph depends on the likelihood function and number of observations that we have from spectrograms.Now, as the DoF of latent variables is not high, using Gibbs sampling we will approximate some observations, and then using them we will compute the value of the latent variables."
Why does learning rate reduce train-test generalization gap?,"
In this blog post: http://www.argmin.net/2016/04/18/bottoming-out/
Prof Recht shows two plots:


He says one of the reasons the plot below has a lower train-test gap is because that model was trained with a lower learning rate (and he also manually drops the learning rate at 120 epoch).
Why would a lower learning rate reduce overfitting?
","['neural-networks', 'machine-learning', 'generalization', 'learning-rate']",
Should the training data be the same in each epoch?,"
Should the training data be the same in each epoch?
If the training data is generated on the fly, for example, is there a difference between training 1000 samples with 1 epoch or training 1000 epochs with 1 sample each?
To elaborate further, samples do not need to be saved or stay in memory if they are never used again. However, if training performs best by training over the same samples repeatedly, then data would have to be stored to be reused in each epoch.
More samples is generally considered advantageous. Is there a disadvantage to never seeing the same sample twice in training?
","['neural-networks', 'machine-learning']",
How to optimize neural network parameters with REINFORCE,"
I've seen a few mentions in papers that neural network parameters can be found using REINFORCE algorithm. It was mentioned in the context of nondifferentiable operations involving e.g. step function which appears in ""hard attention"" or weight pruning. Unfortunately, I haven't seen how to really do this. In Markov Decision Process we have, states (S), actions (A) and rewards (R) so what is what in case of neural net? I don't see how can we find parameters or neural net if the gradient is not well defined.
Any code sample or explanation?
","['reinforcement-learning', 'optimization', 'policy-gradients']",
"How to solve the ""dangerous feedback loops"" in machine learning?","
From the article Dangerous Feedback Loops in ML

Let’s say our model has leads from Facebook, Google, and Bing. If our first model decides that the probability of conversion is 3%, 5%, and 1% from these given sources, and we have finite amount of callbacks we can make, we will only callback the 5% probability. Now fast forward two months. The second model finds these probabilities are now: 0.5%, 8.5%, and 0%. What happened?


Because we started only calling Google leads back, we increased our chances of converting these leads, and likewise, because we stopped calling Facebook and Bing leads, these leads never converted because we never called them. This is an example of a real world feedback loop

How can we solve this problem?
",['machine-learning'],
Do correlations matter when building neural networks?,"
I am new to working with neural networks. However, I have built some linear regression models in the past. My question is, is it worth looking for features with a correlation to my target variable as I would normally do in a linear regression or is it better to feed the neural network with all the data I have?
Assuming that the data I have is all related to my target variable of course. I am working with this dataset and building a neural network regressor for it.
https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv
Here is a snippet of the data. The target variable is the concrete strength rate given a certain combination of materials for that concrete sample.

I greatly appreciate any tips and explanations. I excuse me if this is too noob of a question but unfortunately I did not find any info about it on google. Thanks again!
","['neural-networks', 'python', 'keras', 'linear-regression']","If there is some correlation between features, that is what the network will ideally find out on its own and learn to utilize. So, in general, don't take correlated samples or features out of the training loop only because they look correlated. After all, they could convey a lot of valuable information.When it comes to correlation between data samples during training, this correlation is commonly broken up by training a network on randomly selected mini-batches of training data samples. So, you randomly sample e.g. 16 or 32 (or so) training examples based on which you apply a single update of the weights using some Stochastic Gradient Descent variant. Since the members of a mini-batch are sampled at random, chances for finding highly correlated training samples in some mini-batch shall be sufficiently minimized in order not to negatively affect the training outcome.Having said that, if you are concerned about overfitting of your model or weights that would overly weight just a small subset of all available input features, you could try applying regularization techniques like L1 (encouraging sparse representations) or L2 (encouraging low weights in general) regularization or dropout.
In your particular case, since the main concern is an excessive contribution of only a small set of input features, L2 shall yield better results (avoiding excessively large weights that would be required to excessively much weight just a small number of features).Besides that: Commonly, you split your training dataset into 3 parts:The final evaluation on the test dataset shall reveal then the generalization ability of your trained model to novel data.So, with regularization in place during training and relatively low error rates on the validation and test datasets, you are pretty much save even without checking for correlated data beforehand. Only when you really struggle decreasing the validation loss, it might be worth to further inspect what exactly is going wrong in terms of correlations and such."
Loss Function In Units Of Bits?,"
Where can I find a machine learning library that implements loss functions measuring the Algorithmic Information Theoretic-friendly quantity ""bits of information""?
To illustrate the difference between entropy, in the Shannon information sense of ""bits"" and the algorithmic information sense of ""bits"", consider the way these two measures treat a 1 million character string representing $\pi$:
Shannon entropy ""bits"" ($6$ for the '.'):  $\lceil 1e6*\log_2(10) \rceil+6$
Algorithmic ""bits"":  The length, in bits, of the shortest program that outputs 1e6 digits of $\pi$ .
All statistical measures of information, such as KL divergence, are based on Shannon information.  By contrast, algorithmic information permits representations that are fully dynamical as in Chomsky type 0, Turing Complete, etc. languages.  Since the world in which we live is dynamical, algorithmic models are at least plausibly more valid in many situations than are statistical models.  (I recognize that recursive neural nets can be dynamical and that they can be trained with statistical loss functions.)
For a more authoritative and formal description of these distinctions see the Hutter Prize FAQ questions Why aren't cross-validation or train/test-set used for evaluation? and Why is Compressor Length superior to other Regularizations? For a paper-length exposition on the same see ""A Review of Methods for Estimating Algorithmic Complexity: Options, Challenges, and New Directions"".
From what I can see, machine learning makes it difficult to relate loss to algorithmic information.  Such an AIT-friendly loss function must, by definition, measure the number of bits required to reconstruct, without loss, the original training dataset.
Let me explain with examples of what I mean by AIT-friendly loss functions, starting with the baby-step of classification loss (usually measured as cross-entropy):
Let's say your training set consists of $P$ patterns belonging to $C$ classes. You can then construct a partial AIT loss function providing the length of the corrections to the model's classifications with a $P$-length vector, each element containing a $0$ if the model was correct for that pattern, or the class if not.  These elements would each have a bit-length of $\lceil \log_2(C+1) \rceil$, and be prefixed by a variable length integer storing $P$.  The more $0$ elements, the more compressible this correction vector until, in the limit, a single run-length code for $P$ $0$'s is stored as the correction, prefixed by $P$ and the length of the binary for the RLE algorithm itself. The bit-length of these, taken together, would comprise this partial loss function.
This is a reasonable first cut at an AIT-friendly loss function for classification error.
So now let's go one step further to outputs that are numeric, the typical approach is a summation of a function of individual error measures, such as squaring or taking their absolute value or whatever -- perhaps taking their mean.  None of these are in units of bits of information.  To provide the correction on the outputs to reproduce the actual training values requires, again, a vector of corrections.  This time it would be deltas, the precision of which must be adequate to the original data being losslessly represented, hence requiring some sort of adaptive variable length quantity representation(s).  These deltas would likely have a non-uniform distribution so they can be arithmetically encoded.  That seems like a reasonable approach to another AIT-friendly loss function.
But now we get to the ""model parameters"" and find ourselves in the apparently well-defined but ill-founded notions like ""L2 regularization"", which are defined in terms of ill-defined ""parameters"", e.g. ""parameter counts"" aren't given in bits.
I'll grant that L2 regularization sounds like it is heading in the right direction by squaring the weights and summing them up, but when one looks at what is actually being done, it is:

applying additional functions to the sum such as mean
asking for a scaling factor to apply
applying the regularization on a per-layer basis rather than the entire model

I'm sure I missed some of the many ways L2 regularization fails to be AIT-friendly.
Finally, there is the model's pseudo-invariance, measured, not simply in terms of its hyperparameters but in terms of the length of the (compressed archive of the) actual executable binary running on the hardware.  I say 'pseudo' because there is nothing that says one cannot vary, say, the number of neurons in a neural network during learning -- nor even change to another learning paradigm than neural networks during learning (in the most general case).
So that's pretty much the complete loss function down to the Universal Turing Machine iron, but I'd be happy to see just a reference to an existing TensorFlow or another library that tries to do even a partial loss function for AIT-theoretic learning.
","['objective-functions', 'information-theory']",
"What is meant by ""generate the data"" in describing the difference between on-policy and off-policy?","
From the book:
Sutton, Richard S.,Barto, Andrew G.. Reinforcement Learning (Adaptive Computation and Machine Learning series) (p. 100). The MIT Press. Kindle Edition. ""
following is stated:
""On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a policy different from that used to generate the data.""
Looking at off policy:

and on-policy:

What is meant by ""generate the data""? I'm confused as to what 'data' means in this context.
Does ""generate the data"" translate to the actions generated by the policy ? or Does ""generate the data"" translate to the Q data state action mappings?
","['reinforcement-learning', 'policies', 'off-policy-methods']","In the book, the phrase ""generate the data"" refers to the data from observations about states, actions, next states and rewards, that then get used to make value estimate updates.In both the SARSA and Q learning pseudocode from the book, there is a behaviour policy that selects the next action to take. Other than the initial start state, this policy drives the observations that the learning process must handle - every action is directly selected by it, and due to how MDPs work, every next state and reward are indirectly influenced by it. It is this behaviour policy therefore that must ""generate the data"".You can see from the pseudocode that both algorithms describe this behaviour policy as ""policy derived from Q (e.g. $\epsilon$-greedy)"".The difference in the two algorithms is in which target polcy is being used:In SARSA, the update is based on the Bellman equation for the same policy that generated the most recent set of data for S, A, R, S' and A' - all of these values will be directly or indirectly caused by the behaviour policy. So the whole equation uses the same data that was generated. Another way to put that is behaviour policy and target policy are the same.In Q learning, the $\text{max}_a Q(S',a)$ from the Bellman optimality equation removes the need to use A' in the update. Effectively it is a local search for actions to find one that is potentially better than A', and runs the update as if that one was the one that was taken. This revision of A' is what makes the off-learning evaluate a different target policy to the behaviour policy that generated the rest of the data used in the update."
Is the size of a neural network directly linked with an increase in its inteligence?,"
Just came across this article on GPT-3, and that lead me to the question:
In order to make a certain kind of neural network architecture smarter all one needs to do is to make it bigger?
Also, if that is true, how does the importance of computer power relates with the importance of fine-tuning/algorithmic improvement?
","['neural-networks', 'algorithm', 'architecture', 'gpt']","First of all, there is no real 'intelligence' innate to artificial Neural Networks (NNs).
All they do is trying to approximate a mathematical function with a certain degree of generalization (hopefully without learning a given dataset by heart, i.e. hopefully without overfitting).The more nodes (or neurons) you include into the network, the more complex a function can be that a network can learn to approximate. It's similar to high-school math: The higher the degree of some polynomial, the better the polynomial can be adjusted to fit some observation to be modeled; with the only difference being that NNs commonly include non-linearities and are trained via some kind of stochastic gradient descent.So, yes. The more nodes a model possesses, the higher the so-called model capacity, i.e. the higher the degree of freedom a NN-model has to fit some function. After all, NN are said to be universal function approximators - given they have enough internal nodes in their hidden layer(s) to fit some given function.In practice, however, you don't want to blow up a model architecture unnecessarily, since this commonly results in overfitting if it doesn't cause some instabilities of the training procedure instead.Generally, the larger the model to be trained, the higher the computational cost to train the network.A common suggestion is to reduce the number of nodes in a network at the expense of increasing a network's depth, i.e. the number of hidden layers. Often, that can help reduce the demand for excessively many nodes."
Could a quantum computer perform vectorized forward propagation in deep networks?,"
Forward propagation in Deep Neural Networks
In the ""Forward Propagation in a Deep Network"" video on Coursera, Andrew NG mentions that there's no way to avoid a for loop to loop through the different layers of the network during forward propagation.
See image showing a deep network with 4 layers, and the requirement of a forloop to compute activations for each layer during forward propagation: https://nimb.ws/CkRVLT
This makes intuitive sense since each layer's activation depends on the previous layer's output.
Warning: start of speculation
My rudimentary understanding of quantum computing is that it somehow ""magically"" can bypass computing intermediate states -> this is why supposedly quantum computers can break cryptography... or something like that.
I'm wondering if a quantum computer could perform vectorized forward propagation on an L layer deep neural network.
","['neural-networks', 'machine-learning', 'deep-learning', 'quantum-computing', 'forward-pass']",
Why are these same neural network architecture giving different results?,"
I tried the first neural network architecture and the second one, but keeping all other variables constants, I am getting better results with the second architecture. Why are these same neural network architecture giving different results? Or am I making some mistakes?
First one:
def __init__(self, state_size, action_size, seed, hidden_advantage=[512, 512],
             hidden_state_value=[512,512]):
    super(DuelingQNetwork, self).__init__()
    self.seed = torch.manual_seed(seed)
    hidden_layers = [state_size] + hidden_advantage
    self.adv_network = nn.Sequential(nn.Linear(hidden_layers[0], hidden_layers[1]),
                                     nn.ReLU(),
                                     nn.Linear(hidden_layers[1], hidden_layers[2]),
                                     nn.ReLU(),
                                     nn.Linear(hidden_layers[2], action_size))

    hidden_layers = [state_size] + hidden_state_value
    self.val_network = nn.Sequential(nn.Linear(hidden_layers[0], hidden_layers[1]),
                                     nn.ReLU(),
                                     nn.Linear(hidden_layers[1], hidden_layers[2]),
                                     nn.ReLU(),
                                     nn.Linear(hidden_layers[2], 1))                                                           
def forward(self, state):
    """"""Build a network that maps state -> action values.""""""
    # Perform a feed-forward pass through the networks
    advantage = self.adv_network(state)
    value = self.val_network(state)
    return advantage.sub_(advantage.mean()).add_(value)

Second one:
def __init__(self, state_size, action_size, seed, hidden_advantage=[512, 512],
             hidden_state_value=[512,512]):
    super(DuelingQNetwork, self).__init__()
    self.seed = torch.manual_seed(seed)

    hidden_layers = [state_size] + hidden_advantage
    advantage_layers = OrderedDict()
    for idx, (hl_in, hl_out) in enumerate(zip(hidden_layers[:-1],hidden_layers[1:])):
        advantage_layers['adv_fc_'+str(idx)] = nn.Linear(hl_in, hl_out)
        advantage_layers['adv_activation_'+str(idx)] = nn.ReLU()

    advantage_layers['adv_output'] = nn.Linear(hidden_layers[-1], action_size)

    self.network_advantage = nn.Sequential(advantage_layers)

    value_layers = OrderedDict()
    hidden_layers = [state_size] + hidden_state_value

    # Iterate over the parameters to create the value network
    for idx, (hl_in, hl_out) in enumerate(zip(hidden_layers[:-1],hidden_layers[1:])):
        # Add a linear layer
        value_layers['val_fc_'+str(idx)] = nn.Linear(hl_in, hl_out)
        # Add an activation function
        value_layers['val_activation_'+str(idx)] = nn.ReLU()

    # Create the output layer for the value network
    value_layers['val_output'] = nn.Linear(hidden_layers[-1], 1)

    # Create the value network
    self.network_value = nn.Sequential(value_layers)

def forward(self, state):
    """"""Build a network that maps state -> action values.""""""

    # Perform a feed-forward pass through the networks
    advantage = self.network_advantage(state)
    value = self.network_value(state)
    return advantage.sub_(advantage.mean()).add_(value)

","['neural-networks', 'machine-learning', 'deep-learning']",
Can text-independent writer identification be done without multi-sentence training datasets for each writer?,"
I am trying to learn more about text-independent writer identification and was hoping for some advice.
I have a folder with 100k images, each of them with a different handwritten sentence. All of the images have sentences of different lengths. They range from about 30 to 80 English characters. The file names start at 1.png and go up to 100k.png. That's it, as far as input data. 95% of the sentences are written by different writers. 5% are written by the same writers. Some writers might have written 2 sentences, while others 300+.
Does anyone know of an identification method that would be able to determine what images were written by the same writer?
I know that most methods require each writer to have provided a full page of sample writing for training, but, of course, I do not have that.
","['machine-learning', 'convolutional-neural-networks', 'ai-design', 'natural-language-understanding']",
How can I formulate a prediction problem (given labeled data) as an RL problem and solve it with Q-learning?,"
One of my friends sent me a problem he was working on lately, and I couldn't help but I wonder how could it be solved using Q-learning. The statement is as follows:

Given the following datasets, the objective is to find a suitable strategy per customer contract to maximize Gain and minimize Cost according to the characteristics of the customer.


train.csv: 5000 independent rows, 33 columns.


Columns description:



Day (1, 2 or 3): on which day the strategy was applied.
28 variables (A, B, C, ..., Z, AA, BB): characteristics of the individual;
Gain: the gain for this individual for the corresponding strategy;
Cost: the cost for this individual for the corresponding strategy;
Strategy (0, 1 or 2): the strategy applied on this individual;
Success: 1 if the strategy succeeded, 0 otherwise.
If Success is 1, then the net gain is Gain - Cost, and if Success is 0, consider a standardized cost of 5.




test.csv: 2 000 independent rows, 31 columns.



Columns description:



Index: 0 to 1999, unique for each row.
Day (4): on which day the strategy will be applied.
28 variables (A, B, C, ..., Z, AA, BB): characteristics of the client;
Gain: the gain for this individual for the corresponding strategy;
Cost: the cost for this individual for the corresponding strategy;


From what I understood, the train.csv file is used to build a Q-Learning model, and the test one for generating a strategy and predicting a Success.
My main question is:
How to formulate this problem as an RL problem? How to define an episode? Since the training data is labeled, this could be clearly a classification problem (predicting the strategy), but I have no idea how to solve it using RL (Q-learning ideally). Any ideas will be helpful.
","['reinforcement-learning', 'ai-design', 'q-learning', 'datasets']",
Why does this Keras implementation of the DDPG algorithm update the critic's network using the gradient but the pseudocode doesn't?,"
I'm trying to understand the DDPG algorithm using Keras
I found the site and started analyzing the code, I can't understand 2 things.
The algorithm used to write the code presented on the page

In the algorithm image, updating the critic's network does not require gradient
But the gradient is implemented in the code, why?
with tf.GradientTape() as tape:
    target_actions = target_actor(next_state_batch)
    y = reward_batch + gamma * target_critic([next_state_batch, target_actions])
    critic_value = critic_model([state_batch, action_batch])
    critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))

critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)
critic_optimizer.apply_gradients(zip(critic_grad, critic_model.trainable_variables))

The second question is why in the photo of the algorithm when calculating the actor's policy gradient are 2 gradients multiplied by themselves and in the code only one gradient is calculated for the critic's network and it's not multiplied by the second gradient?
with tf.GradientTape() as tape:
    actions = actor_model(state_batch)
    critic_value = critic_model([state_batch, actions])
    # Used `-value` as we want to maximize the value given
    # by the critic for our actions
    actor_loss = -tf.math.reduce_mean(critic_value)

actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)
actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))

","['reinforcement-learning', 'keras', 'gradient-descent', 'ddpg', 'pseudocode']",
Why isn't the loss of my neural network reduced after 2500 iterations?,"
I have developed a basic feedforward neural network from scratch to classify whether image is of cat or not cat. It works fine, but after 2500 iterations, my cost function is not reducing properly.
The loss function which I am using is
$L(\hat{y},y) = -ylog\hat{y}-(1-y)log(1-\hat{y})$
Can you please point out where I am going wrong the link to the notebook is
https://www.kaggle.com/sidcodegladiator/catnoncat-nn?
","['neural-networks', 'deep-learning', 'feedforward-neural-networks']",
Can I use augmented data in the validation set?,"
I am trying to predict nursing activity using mobile accelerometer data. My dataset is a CSV file containing x, y, z component of acceleration. Each frame contains 20-second data. The dataset is highly imbalance, so I perform data augmentation and balance the data. In the data augmentation technique, I only use scaling and my assumption is, if I scale down or up a signal the activity remains the same. Using this assumption I augmented the data and my validation set not only contain the original signals but also the augmented (scaling) signals. Using this process, I am getting quite a good accuracy that I never being expected using only data augmentation. So, I am thinking that I performed a terrible mistake somewhere. I check the code, everything is right. So now I think, since my validation set has augmented data, that's the reason of getting this high accuracy (maybe the augmented data is really easy to classify).
","['machine-learning', 'deep-learning', 'computer-vision']",
How can I classify policy gradient methods in RL?,"
In the book of Barto and Sutton, there are 3 methods presented that solve an RL problem: DP, Monte Carlo, and TD. But which category does policy gradient methods (or actor-only methods) classify in? Should I classify them as the 4th method of solving a reinforcement learning problem?
","['reinforcement-learning', 'terminology', 'policy-gradients']","DP, Monte Carlo, and TD are methods of estimating returns. Policy gradient describes methods of learning a policy. So policy gradients serve a different purpose than the other things you mentioned. For clarity, you can use Monte Carlo or TD methods to estimate returns to construct the loss that you get your policy gradient from."
What is a heatmap in the CornerNet paper?,"
I have been working on understanding how CornerNet works, but I couldn't figure out a few parts about the architecture.
First, the authors mention that there are 3 distinct parts to be predicted as a heatmap, embedding, and offset.
Also, in the paper, it is stated that the network was trained on the COCO dataset, which has bounding box and class annotations.
As far as I am concerned, since CornerNet is based on detecting the top-left and bottom-right corners, the ground-truth labels for heatmap should be composed of top-left and bottom-right pixel locations of bounding boxes with the class score (but I might be wrong). What is the heatmap used for?
Moreover, for the embedding part, authors used the pull&push loss at the ground-truth pixel locations to find out which corner pairs belong to which object, but I don't understand how to backpropagate this loss. How do I back-propagate the embedding loss?
","['deep-learning', 'computer-vision', 'object-detection', 'papers']",
Binary mode or Multi-label mode is correct when using binary crossentropy and sigmoid output function on multi-label classification,"
I would like to ask a question about the relationship of accuracy with the loss function.
My experiment is a multiclass text classification problem, and I have built a Keras neural network to tackle it. My labels are something like
array([array([0, 0, 0, 0, 0, 1, 0, 1]), array([0, 1, 1, 0, 0, 0, 0, 1])])
For the final output layer I use the 'sigmoid' activation function and for loss the 'binary crossentropy', however, I am a bit confused about the metric. I am using the F1_score metric because Accuracy it's not a metric to count on when there are many more negative labels than positive labels. So, since the problem is multilabel classification shall I use the multi-label mode like tfa.metrics.F1_score(mode=""micro""). Is that correct? or since I use binary_crossentropy and sigmoid activation function should I use the standard binary f1-score because every label-tag Is independent to the others and has a different Bernoulli distribution?
I would really like to get your input on this. My humble opinion is that I should you the binary standard mode of binary f1-score and not the multi-label micro approach even though my experiment is multi-label text classification.
My current approach (using micro F1-score since my y_train is Multi-label
model_for_pruning.compile(optimizer='adam',
                          loss='binary_crossentropy',
                          metrics=[tfa.metrics.F1Score(y_train[0].shape[-1], average=""micro"")])

My alternative approach (based on the binary_crossentropy and the sigmoid activation function, despite I have multi-label y_train)
model_for_pruning.compile(optimizer='adam',
                          loss='binary_crossentropy',
                          metrics=[tfa.metrics.F1Score(y_train[0].shape[-1], average=None)])

Τhe reason why I use sigmoid and not softmax as the output layer
relevant link
Why Sigmoid and not Softmax in the final dense layer?
In the final layer of the above architecture, sigmoid function as been used instead of softmax. The advantage of using sigmoid over Softmax lies in the fact that one synopsis may have many possible genres. Using the Softmax function would imply that the probability of occurrence of one genre depends on the occurrence of other genres. But for this application, we need a function that would give scores for the occurrence of genres, which would be independent of occurrences of any other movie genre.
relevant link 2
Binary cross-entropy rather than categorical cross-entropy.
This may seem counterintuitive for multi-label classification; however, the goal is to treat each output label as an independent Bernoulli distribution and we want to penalize each output node independently.
Please check my reasoning behind this and I would be happy if we can contradict this explanation. To better explain my experiment, I want to predict movie genres. So a movie can belong to 1 or more genres ['Action', 'Comedy', 'Children'], so when I use softmax the probability sum to 1, while when I use sigmoid its single probability of a class has a range between (0,1). Thus, if the predictions are correct the genres with the highest probabilities are those assigned to the movie. So imagine that my vector of prediction probabilities are something like [0.15, 0.12, 0.54, 0.78, 0.99] sum()> 1, and not something like [0.12, 0.43, 11, 0.32,  0.01, 0.01) sum() = 1.
","['machine-learning', 'python', 'keras']",
Is my understanding of back-propogation correct?,"
I am trying to learn backpropagation and this is what I know so far.
To update the weights of the neural network you have to figure out the partial derivative of each of the parameters on the loss function using the chain rule. List all of these partial derivatives in a column vector and you have your gradient vector of your current parameter's on the loss function. Then by taking the negative of the gradient vector to descend the loss function and multiplying it by the learning rate (step size) and adding it to your original gradient vector, you have your new weights.
Is my understanding correct? Also, how can this be done in iterations over training examples?
","['neural-networks', 'machine-learning', 'deep-learning', 'backpropagation', 'gradient-descent']",
Why does adding another network help in double DQN? [duplicate],"







This question already has an answer here:
                                
                            




Why does DQN require two different networks?

                                (1 answer)
                            

Closed 2 years ago.



What is the idea behind double DQN?
The target in double DQN is computed as follows
$$
Y_{t}^{\text {DoubleQ }} \equiv R_{t+1}+\gamma Q\left(S_{t+1}, \underset{a}{\operatorname{argmax}} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right) ; \boldsymbol{\theta}_{t}^{\prime}\right),
$$
where

$\boldsymbol{\theta}_{t}^{\prime}$ are the weights of the target network
$\boldsymbol{\theta}_{t}$ are the weights of the online value network
$\gamma$ is the discount factor

On the other hand, the target in DQN is computed as
$$Y_{t}^{\mathrm{DQN}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}^{-}\right),$$
where $\boldsymbol{\theta}_{t}^{-}$ are the weights of the target network.
The target network for evaluating the action is updated using weights of the online network and the value fed to the target value is basically the old q value of the action.
Any ideas on how or why adding another network based on weights from the first network helps? Any example?
","['reinforcement-learning', 'dqn', 'deep-rl', 'double-dqn']",
Inaccurate masks with Mask-RCNN: Stairs effect and sudden stops,"
I've been using matterport's Mask R-CNN to train on a custom dataset. However, there seem to be some parameters that i failed to correctly define because on practically all of the images, the bottom or top of the object's mask is cut off:

As you can see, the bounding box is fine since it covers the whole blade, but the mask seems to suddenly stop in a horizontal line on the bottom.
On another hand, there is a stair-like effect on masks of larger and curvier objects such as this one (in addition to the bottom and top cut-offs):


The original images are downscaled to IMAGE_MIN_DIM = IMAGE_MAX_DIM = 1024 using the ""square"" mode.
USE_MINI_MASK is set to true with MINI_MASK_SHAPE = (512, 512) (somehow if i set it off, RAM gets filled and training chrashes).
RPN_ANCHOR_SCALES = (64, 128, 256, 512, 1024) since the objects occupy a large space of the image.

It doesn't feel like the problem comes from the amount of training. These two predictions come from 6 epochs of 7000 steps per epoch (took around 17 hours). And the problem appears from early stage and persists along all the epochs.
I posted the same question on stack overflow, and an answer pointed out that this issue is common when using mask r-cnn. It also suggested to look at PointRend, an implementation to mask r-cnn that addresses this issue.
Nevertheless I feel like I could still optimize my model and use the full potential of mask r-cnn before looking for an alternative.
Any idea on what changes to make ?
","['neural-networks', 'deep-learning', 'python', 'keras', 'r-cnn']",
When is adding a feature useless?,"
I'm building a model, where, from a feature set A, I want to predict a target set C. I need to understand if another feature set B, together with A, can improve my model performances, instead of using only A.
Now I want to check if I can predict B directly from A, since, in my understanding, this would mean that info on B is already inside A.
If I get good predictions when testing the model A -> B, is it true then that adding B to A in predicting C is completely useless?
And furthermore, are there smarter ways to decide if/when a feature is useless?
","['neural-networks', 'deep-learning', 'prediction', 'feature-selection', 'features']","Now I want to check if I can predict B directly from A, since, in my understanding, this would mean that info on B is already inside A.This will help inform you how much redundancy there is between A and B. However, even if you can predict B with 100% accuracy from A, you may still be better off using A+B (instead of A alone) to predict C.If I get good predictions when testing the model A -> B, is it true then that adding B to A in predicting C is completely useless ?It is an indicator that adding B probably won't make great improvements to your prediction of C.The only way to be sure is to make the model that uses A+B and compare its performance against a model that uses only A. If collecting B costs time or other resources, then perform this check by limiting both models to only learn from the subset of data where you have all of A, B, C available.And furthermore, are there smarter ways to decide if/when a feature is useless ?Another thing you can do is to try and predict C from B alone. It doesn't need to score well, but may indicate that something useful is in the data if you get better than chance results. Scoring badly unfortunately doesn't rule it out for working well in combination with A.Generally, if you cannot reason it clearly one way or another from theory, the accepted method is to build variations of your model with and without a feature and test them. You do have to be aware of the chances of spurious correlation though, so usually you need to have some basis or motivation for considering any new feature, from your domain knowledge that applies to the model."
How does batch normalisation actually work?,"
I actually went through the Keras' batch normalization tutorial and the description there puzzled me more.
Here are some facts about batch normalization that I read recently and want a deep explanation on it.

If you froze all layers of neural networks to their random initialized weights, except for batch normalization layers, you can still get 83% accuracy on CIFAR10.

When setting the trainable layer of batch normalization to false, it will run in inference mode and will not update its mean and variance statistics.


","['machine-learning', 'transfer-learning', 'batch-normalization']","I'm not sure how just training the batch normalisation layer, you can get an accuracy of 83%. The batch normalisation layer parameters $\gamma^{(k)}$ and $\beta^{(k)}$, are used to scale and shift the normalised batch outputs. These parameters are learnt during the back-propagation step. For the $k$th layer, $$y^{(k)} = \gamma^{(k)}\hat{x}^{(k)} + \beta^{(k)}$$ The scaling and shifting are done to ensure a non - linear activation being outputted by each layer. Because batch normalisation scales outputs between 0-1, some activation functions are linear within that range (Eg. $tahh$ and $sigmoid$)Regarding the second fact however, the difference between training and inference mode is this. During training mode, the statistics of each batch norm layer $\mu_B$ and $\sigma^2_B$ is computed. This statistic is used in scaling and normalising the outputs of the batch norm layer to have 0 mean and unit variance. At the same time, the current batch statistic computed is also used to update the running mean and running variance of the population. $\mu_B[t]$ represents the current batch mean, $\sigma^2_B[t]$ represents the current batch variance, while $\mu'_B[t]$ and $\sigma'_B[t]$ represent the accumulated means and variance from the previous batches. The running mean and variance of the population is then updated as $$\mu'_B[t]=\mu'_B[t]× momentum+ \mu_B[t]×(1−momentum)$$
$$\sigma'^2_B[t]=\sigma'^2_B[t] × momentum + \sigma^2_B[t]×(1−momentum)$$In inference mode, the batch normalisation uses the running mean and variance computed during training mode to scale and normalise inputs in the batch norm layer instead of the current batch mean and variance."
Finding the 'ultimate resolution' of an ANN,"
I want to use a neural network to predict the refractive index of a solution. My thinking is, instead of immediately training on many samples, I will first find the 'ultimate resolution' of the network given the experimental apparatus I am using. What I mean is I will make two different solutions which have refractive indices near the middle of the range of which I am interested. Then I will train the network to classify these two solutions based on reflectance measured from the solution. If it works, say with at least 95% accuracy, then I will make two different solutions in which the difference in refractive index is smaller than before. I will repeat this until the ANN classifies, say below 95%.
Will this method of finding the 'resolution' by classification extrapolate well to regression with many more training examples?
","['classification', 'training', 'regression']",
Monte Carlo Exploring Starts broke for 2048 game AI,"
I implemented a MCES for 2048 (the game), with a quality function implemented as a neural net of a single layer.
The starts are created with 6 cells filled with values between 64 and 1024, two cells are 1024 an ther other 8 cells are filled with 0. The game is then progressed until the AI loses or wins and another start is created.
After 10 wins the max cell created in the start is reduced in half. Thus, after the first 10 wins, the max cell created in the start is 512.
The issue I am having is that after the first 10 wins, the AI gets stuck, it can run around 3 million steps but doesn't get any more wins.
How should I create the starts for it to actually learn?
Code for reward (complete code here):
        ArrayList<DataSet> dataSets = new ArrayList<>();
        double gain = 0;

        for(int i = rewards.size()-1; i >= 0; i--) {
            gain = gamma * gain + rewards.get(i);

            double lerpGain = reward(gain);
            INDArray correctOut = output.get(i).putScalar(actions.get(i).ordinal(), lerpGain);
            dataSets.add(new DataSet(input.get(i), correctOut));
        }

        Qnetwork.fit(DataSet.merge(dataSets));  

Code:
public class SimpleAgent {
    private static final Random random = new Random(SEED);

    private static final MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(SEED)
            .weightInit(WeightInit.XAVIER)
            .updater(new AdaGrad(0.5))
            .activation(Activation.RELU)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .weightDecay(0.0001)
            .list()
            .layer(new DenseLayer.Builder()
                    .nIn(16).nOut(4)
                    .build())
            .layer(new OutputLayer.Builder()
                    .nIn(4).nOut(4)
                    .lossFunction(LossFunctions.LossFunction.SQUARED_LOSS)
                    .build())
            .build();


    public SimpleAgent() {
        Qnetwork.init();
        ui();
    }

    private static final double gamma = 0.02;

    private final ArrayList<INDArray> input = new ArrayList<>();
    private final ArrayList<INDArray> output = new ArrayList<>();
    private final ArrayList<Double> rewards = new ArrayList<>();
    private final ArrayList<GameAction> actions = new ArrayList<>();

    private MultiLayerNetwork Qnetwork = new MultiLayerNetwork(conf);
    private GameEnvironment oldState;
    private GameEnvironment currentState;
    private INDArray oldQuality;
    private double epsilon = 1;

    public void setCurrentState(GameEnvironment currentState) {
        this.currentState = currentState;
    }

    public GameAction act() {
        if(oldState != null) {
            double reward = currentState.points - oldState.points;

            if (currentState.lost) {
                reward = 0;
            }

            input.add(oldState.boardState);
            output.add(oldQuality);
            rewards.add(reward);

            epsilon -= (1 - 0.01) / 1000000.;
        }

        oldState = currentState;
        oldQuality = Qnetwork.output(currentState.boardState);

        GameAction action;

        if(random.nextDouble() < 1-epsilon) {
            action = GameAction.values()[oldQuality.argMax(1).getInt()];
        } else {
            action = GameAction.values()[new Random().nextInt(GameAction.values().length)];
        }

        actions.add(action);

        return action;
    }

    private final int WINS_TO_NORMAL_GAME = 100;
    private int wonTimes = 0;

    public void setHasWon(boolean won) {
        if(won) {
            wonTimes++;
        }
    }

    public boolean playNormal() {
        return wonTimes > WINS_TO_NORMAL_GAME;
    }

    public boolean shouldRestart() {
        if (currentState.lost || input.size() == 20) {
            ArrayList<DataSet> dataSets = new ArrayList<>();
            double gain = 0;

            for(int i = rewards.size()-1; i >= 0; i--) {
                gain = gamma * gain + rewards.get(i);

                double lerpGain = reward(gain);
                INDArray correctOut = output.get(i).putScalar(actions.get(i).ordinal(), lerpGain);
                dataSets.add(new DataSet(input.get(i), correctOut));
            }

            Qnetwork.fit(DataSet.merge(dataSets));

            input.clear();
            output.clear();
            rewards.clear();
            actions.clear();

            return true;
        }

        return false;
    }

    public Game2048.Tile[] generateState() {
        double lerped = lerp(wonTimes, WINS_TO_NORMAL_GAME);
        int filledTiles = 8;

        List<Integer> values = new ArrayList<>(16);

        for (int i = 0; i < 16-filledTiles; i++) {
            values.add(0);
        }

        for (int i = 16-filledTiles; i < 14; i++) {
            values.add((int) (7-7*lerped) + random.nextInt((int) (2- 2*lerped)));
        }

        values.add((int) ceil(10-10*lerped));
        values.add((int) ceil(10-10*lerped));

        Collections.shuffle(values);

        return values
                .stream()
                .map((value) -> (value == 0? 0: 1 << value))
                .map(Game2048.Tile::new)
                .toArray(Game2048.Tile[]::new);
    }

    private static double reward(double x) {
        return x/ 2048;
    }

    private static double lerp(double x, int maxVal) {
        return x/maxVal;
    }

    private void ui() {
        UIServer uiServer = UIServer.getInstance();
        StatsStorage statsStorage = new InMemoryStatsStorage();
        uiServer.attach(statsStorage);
        Qnetwork.setListeners(new StatsListener(statsStorage));
    }
}

","['game-ai', 'monte-carlo-methods']",
"Which paper introduced the term ""softmax""?","
Nowadays, the softmax function is widely used in deep learning and, specifically, classification with neural networks. However, the origins of this term and function are almost never mentioned anywhere. So, which paper introduced this term?
","['neural-networks', 'deep-learning', 'classification', 'history', 'softmax']","The paper that appears to have introduced the term ""softmax"" is Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters (1989, NIPS) by John S. Bridle.As a side note, the softmax function (with base $b = e^{-\beta}$)$$\sigma (\mathbf {z} )_{i}={\frac {e^{-\beta z_{i}}}{\sum _{j=1}^{K}e^{-\beta z_{j}}}}{\text{ for }}i=1,\dotsc ,K {\text{ and }}\mathbf {z} =(z_{1},\dotsc ,z_{K})\in \mathbb {R} ^{K}$$is very similar to the Boltzmann (or Gibbs) distribution$$
p_i=\frac{e^{- {\varepsilon}_i / k T}}{\sum_{j=1}^{M}{e^{- {\varepsilon}_j / k T}}}
$$which was formulated by Ludwig Boltzmann in 1868, so the idea and formulation of the softmax function is quite old."
Should forecasting with neural networks only be treated as a supervised learning (regression) problem?,"
I have recently made a work about the application of neural networks to time series forecasting, and I treated this as a supervised learning (regression) problem. I have come across the suggestion of treating this problem as an unsupervised, semi-supervised, or reinforcement learning problem. The ones that made this suggestion didn't know how to explain this approach and I haven't found any paper of this. So I found myself now trying to figure it out without any success. To my understanding:
Unsupervised learning problems (clustering and segmentation reduction) and semi-supervised learning problems (semi-supervised clustering and semi-supervised classification) can be used to decompose the time series but not forecast it.
Reinforcement learning problems (model-based and non-model-based on/off-policy) is to decision taken problems, not to forecast.
It is possible to treat forecasting time series with neural networks as an unsupervised, semi-supervised, or reinforcement learning problem? How it is done?
","['neural-networks', 'reinforcement-learning', 'unsupervised-learning', 'forecasting', 'semi-supervised-learning']","I think the choice of technique strongly depends on how fine-grained your forecast-predictions need to be.When it comes to forecasting by Reinforcement Learning (RL), one prominent example is the stock-trading RL agent. The agent must decide which stock to buy or sell, thereby drawing upon predictions concerting the expected future development of some stock.
Given this approach, you would not necessarily let the RL agent explicitly generate estimates of how stock prices are gonna develop at any point, but instead you would only observe the predicted decision concerning whether to buy or sell etc.But if you think hard enough, I am certain that you could come up with setups of RL agents that would allow you to explicitly generate future estimates of values to be forecast. In this case, the final buy/sell decision would have to depend on the explicit future stock price predictions to enforce accurate predictions.Concerning unsupervised learning, you could cluster data points (training samples) with respect to how some value(s) of interest changed $t$ time steps in the future (after having observed the training sample). Then, you could associate clusters with rough forecast-estimates. After all, you would treat the forecast value as a label associated with data points.
Afterwards, you could use some kind of nearest neighbor approach to determine which cluster is closest to some novel data sample. Then, you take as a prediction for the new data sample the forecast prediction (i.e. label) that is associated with the closest cluster/prototype etc. But strictly speaking, as soon as you start turning forecast values (which were previously part of some unlabeled time-series dataset) into labels, you turn the training procedure of course into a supervised technique again.How well especially the latter training approach would work, I can't tell since I have never heard anyone using this method. But if training data is too scarce to employ some deep learning method, why not giving it at least a try if accuracy doesn't have to be too precise?After all, it's just a matter of creativity and testing which method works best given your specific machine learning problem at hand."
"How to ""forward"" updated NN model to a transferred model?","
I've trained a robot to walk in a straight line for as long as it can (using TD3), and now I'm using that pre-trained model for two new models with separate purposes: 1. Walk to a specific point and halt (adding target position to the NN inputs); 2. Walk straight at a specified velocity (adding a target velocity to NN inputs).
Now let's say I retrain the original model again to walk properly after changing, say, the mass of the robot. How can I approach ""forwarding"" this update to the two transfer-learned models? The purpose of this is to minimize re-training time for all future models transfer-learned from the original.
(What strikes me as particularly challenging is the fact that the input layer of the transfer-learned models have additional features, so this may re-wire the majority of the NN, making a ""forwarded update"" completely incompatible...)
","['reinforcement-learning', 'transfer-learning']","I think there is no simple way to transfer knowledge changes between different models.If you take your initial model and create a new version of it which you use to learn some other task (like ""Walk to a specific location""), then the values copied from the first (original) model change in the second model.
From that moment on, training the former model on another task will have different effects on its weights than continuing the training of the second model, whose parameter have been changed already.Consider, for example, that you had changed the mass of the robot and trained the initial model on that new task already. Then, if you took all the re-trained parameters from the first model and implanted them into the second model (trained on walking to a certain location), then you would essentially overwrite the additional knowledge the second model had gained already during its initial transfer-learning-process (not even taking into consideration any additional parameters appended to the list of parameters in the second model).So, you will have to re-train all three models (the original one and the two transfer-learning models) if you change the mass of the robot.Edit:There might be an option to apply the same knowledge changes to another model architecture if you refrain from pure transfer learning. This can be achieved with a more modularized model architecture.Consider that you train your first model on walking straight head. Let's call this model $m_{walk}$.Then you intend to recycle $m_{walk}$ for another task, like walking straight ahead to a given location. Such a model architecture could be realized in two ways:In the second case, your overall model architecture (let's call it $m_{go\_to}$) consists of both one model used for walking (i.e. $m_{walk}$) and one model architecture which is used for predicting $go$ vs. $stop$ (i.e. $m_{navigator}$).
The idea then is that the robot executes the actions suggested by $m_{walk}$ until $m_{navigator}$ suggests stopping, upon which prediction the suggestions by $m_{walk}$ will be ignored.Then, whenever you retrain your model $m_{walk}$ (e.g. because the mass of some robot changes), you can simply apply the changes to $m_{go\_to}$ by replacing $m_{walk}$ by a new version, leaving the rest of $m_{go\_to}$ intact.If you generalize $m_{walk}$ to not only walk straight ahead, but also to take turns etc. and you generalize $m_{navigator}$ to predict going $left$, $right$, $straight\ ahead$, $back$, or $stop$ (being predicted when a certain destination has been reached), you can generalize $m_{go\_to}$ to walk where-ever you want it to go."
"Do I need to rotate the masks, if I also rotate the images and the masks are generated from the input?","
I am training a neural network that takes an input (H, W, 3) and has the output of size (H', W', C). Now, to augment my dataset, since I only have 45k images, I am using the following in my custom data generator
def Generator():
img=cv2.imread(trainDir+'\'+imgpath)
img=tf.keras.preprocessing.image.random_rotation(img,20m,row_axis=0,col_axis=1,channel_axis=2)

output_mask=np.load(trainDir+'\'+maskpath)

yield(img/255-.5,output_mask)

Since I am rotating my input images and the output_masks are generated from information about the input (specifically, heat maps around the joint locations) do I also need to rotate the masks as well?
","['neural-networks', 'convolutional-neural-networks', 'data-preprocessing', 'data-augmentation']","Yes! This is crucial.If you rotate your input images for segmentation, you need to rotate the output masks as well. Otherwise the loss of your network will not be correctly calculated and your network will not learn how to generalize to rotated input images.If you use keras, you can use two ImageDataGenerator classes, one for the images and one for the masks, with the same random seed and augmentation parameters. It looks something like this:"
Isn't it true that using max over a softmax will be much slower because there is not a smooth gradient?,"
Isn't it true that using max over a softmax will be much slower because there is not a smooth gradient?
Max basically zeros out the gradients of all the non-maximum values. Especially at the beginning of training, this means it is zeroing out potentially useful features simply because of random weight initialization. Wouldn't this drastically slow down the training in the beginning?
","['neural-networks', 'comparison', 'training', 'gradient-descent', 'softmax']",
Neural Network is not learning a very simple task,"
I am a complete beginner in the area. I implemented my first neural network following the online book ""Neural Networks and Deep Learning"" by Micheal Nielsen. It works fine with classifying handwritten digits. Achieving ~9500/10000 accuracy on the test data.
I am trying to train the network to determine whether $x > 5$ where $x$ is in the interval $[0,10)$, which should be a much simpler task than classifying handwritten digits. However, no learning happens and the accuracy ever the test data stays exactly the same with every epoch. I tried different structures and different learning rates but always the same thing happened. Here is the code I wrote that uses libraries in Nielsen's book:
import networkCopy
import numpy as np
# Creating training data
x = []
y = []
for n in range(1000):
    to_add = 100*np.random.rand()
    x.append(np.array([to_add]).reshape(1,1))
    y.append(np.array([float(to_add > 50)]).reshape(1,1))
training_data = zip(x, y)
# Creating test data
tx = []
ty = []
for n in range(1000):
    to_add = 100*np.random.rand()
    tx.append(np.array([to_add]).reshape(1,1))
    ty.append(np.array([float(to_add > 50)]).reshape(1,1))
test_data = zip(tx, ty)

# Creating and training the network
net = networkCopy.Network([1, 5, 1])  # [1, 5, 1] contains the number of neurons for each layer
net.SGD(training_data, 300, 100, 5.0, test_data=test_data)
# 300 is the number of epochs, 100 is the mini batch size
#5.0 is the learning rate 

The way I generated the data may not be optimal, it is an ad hoc solution to make the data in the proper form for the network. This is my first question so I apologize for any mistakes that might be in the format of the question.
","['neural-networks', 'machine-learning']",
Can rewards be decomposed into components?,"
I'm training a robot to walk to a specific $(x, y)$ point using TD3, and, for simplicity, I have something like  reward = distance_x + distance_y + standing_up_straight, and then it adds this reward to the replay buffer. However, I think that it would be more efficient if it can break the reward down by category, so it can figure out ""that action gave me a good distance distance_x, but I still need work on distance_y and standing_up_straight"".
Are there any existing algorithms that add rewards this way? Or have these been tested and proven not to be effective?
","['reinforcement-learning', 'reward-design', 'reward-functions', 'multi-objective-rl', 'reward-hypothesis']","If I understood correctly you're looking at a Multi-Objective Reinforcement Learning (MORL). Keep in mind however  that many scientist will often follow the reward hypothesis (Sutton and Barto) which says thatAll of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)The argument for a scalar reward could be that even if you define your policy using some objective vector (as in MORL) - you will find a pareto bound of optimal policies, some of which favour one component of the objective over the other - leaving you (the scientist) responsible for making the ultimate decision concerning the objectives' tradeoff - thus eventually degenerating the reward objective into scalar.In your example there might be two different ""optimal"" policies - one which results in a very high value of distance_x but relatively poor distance_y and a one that favours distance_y instead. It'll be up to you to find the sweet spot and collapse a reward function back to a scalar."
"If the transition model is available, why would we use sample-based algorithms?","
Sample-based algorithms, like Monte Carlo Algorithms and TD-Learning, are often presented as useful since they do not require a transition model.
Assuming I do have access to a transition model, are there any reasons one might want to use sample-based methods instead of performing a full Bellman update?
","['reinforcement-learning', 'monte-carlo-methods', 'temporal-difference-methods', 'bellman-equations', 'dynamic-programming']",
How can I create an embedding layer to convert words to a vector space from scratch?,"
For an upcoming project, I am trying to build a neural network for classifying text from scratch, without the use of libraries. This requires an embedding layer, or a way to convert words to some vector representation. I understand the gist, but I can't find any deep explanations or tutorials that don't start with importing TensorFlow. All I'm really told is that it works by context using a few surrounding words, but I don't understand exactly how.
Is it much different from a classic network, with weights and biases? How does it figure out the loss?
If someone could point me towards a guide to how these things work exactly I would be very grateful.
","['neural-networks', 'implementation', 'word-embedding']",
Solving multi-armed bandit problems with continuous action space,"
My problem has a single state and an infinite amount of actions on a certain interval (0,1). After quite some time of googling I found a few paper about an algorithm called zooming algorithm which can solve problems with a continous action space. However my implementation is bad at exploiting. Therefore I'm thinking about adding an epsilon-greedy kind of behavior.
Is it reasonable to combine different methods?
Do you know other approaches to my problem?
","['reinforcement-learning', 'multi-armed-bandits']",
How to quickly change hand-drawn shapes to symmetrical polished shapes?,"
Given a hand-drawn shape, I'd like to generate the corresponding symmetrical polished shapes such as circle, rectangle, triangle,  trapezoid, square, parallelogram, etc.
A short video demonstration
Here below we can see a parallelogram, trapezoid, triangle and a circle.
I was wondering how can I transform it into symmetrical polished shapes?

At first, I tried a simple approach with traditional computer vision algorithms, with OpenCV (no neural networks were involved), by counting the number of corners, but it failed miserably, since there are many edge cases within a user's doodler.
So, I was thinking to delve into CNN specifically U-Net for segmentation.
Can somebody please give me some suggestions on how to approach this kind of problem? I'd like to read some relevant articles and code about this subject for getting a better grasp of this kind of problem.
","['convolutional-neural-networks', 'object-recognition', 'image-segmentation', 'u-net']",
How is depth perception (e.g. in autonomous driving) addressed without using a Lidar or Radar unit?,"
For practical applications, like autonomous driving, depth perception is needed to make useful decisions.
How is this normally addressed without using a LIDAR or RADAR unit (but using a camera)?
","['deep-learning', 'computer-vision', 'applications']",
Is it necessary to label the background when generating the labelled dataset for semantic segmentation?,"
When I label images for semantic segmentation (using u-net, if that matters), is labeling the background (anything I am not interested in) necessary? Will it improve the network's performance?
","['deep-learning', 'computer-vision', 'training', 'image-segmentation', 'u-net']",
"If the normal equation works, why do we need gradient descent?","
Recently, I followed the open course CS229,
http://cs229.stanford.edu/notes/cs229-notes1.pdf
This lecturer introduces an alternative approach to gradient descent that is called ""Normal Equation"" and the equation is as follows:
$$\theta=\left(X^{T} X\right)^{-1} X^{T} \vec{y}$$
The normal equation can directly compute the $\theta$.
If the normal equation works, why do we need gradient descent? What is the trade-off between these two methods?
","['machine-learning', 'optimization', 'gradient-descent']",That normal equation is sometimes called the closed-form solution.The short answer to your question is that the closed-form solution may be impractical or unavailable in certain cases or the iterative numerical method (such as gradient descent) may be more efficient (in terms of resources).This answer gives you more details and an example.
Best ROC threshold for classifier?,"
Suppose I have a neural network $N$ that produces the output probabilities $[0.3, 0.8]$. Normally, I would specify a threshold of 0.5 for the argmax of the prediction, let's say, second arg > 0.5 means that the image is attractive, and if both probabilities are lesser than 0.5 we don't have that good of a prediction.
My question is, can we plot this threshold on a ROC curve so we can figure out the best value?
","['machine-learning', 'deep-learning', 'computer-vision']",
How can the expectation-maximization improve the classification?,"
I am learning the expectation-maximization algorithm from the article  Semi-Supervised Text Classification Using
EM. The algorithm is very interesting. However, the algorithm looks like doing a circular inference here.

I don't know am I understanding the description right or wrong, what I perceived is:
Step 1: train NB classifier on labeled data.
Repeat
Step 2 (E-step): use trained NB to add label to unlabeled data.
Step 3 (M-step): train NB classifier by using labeled data and unlabeled data(with tags from step 2) to get a new classifier
Until convergent.
Here is the question:
In step 2, the label is tagged by the classifier trained on the labeled data, which is the only source containing the knowledge on a correct prediction. And the step-3 (M-step) is actually updating the classifier on the labels generated from step 2. The whole process is relying on the labeled data, so how can the EM classifier improve the classification? Can someone explain it to me?
","['machine-learning', 'naive-bayes', 'semi-supervised-learning', 'expectation-maximization']",
Does the input layer have bias and are there bias neurons?,"
I have seen two different representations of neural networks when it comes to bias. Consider a ""simple"" neural network, with just an input layer, a hidden layer and an output layer. To compute the value of a neuron in the hidden layer, the weights and neurons from the input layer are multiplied, shifted by a bias and then activated by the activation function. To compute the values in the output layer, you may choose not to have a bias and have an identity activation function on this layer, so that this last calculation is just ""scaling"".
Is it standard to have a ""scaling"" layer? You could say that there is a bias associated with each neuron, except those in the input layer correct (and those in the output layer when it is a scaling layer)? Although I suppose you could immediately shift any value you're given. Does the input layer have a bias?
I have seen bias represented as an extra unchanging neuron in each layer (except the last) having value 1, so that the weights associated with the connections from this neuron correspond to the biases of the neurons in the next layer. Is this the standard way of viewing bias? Or is there some other way to interpret what bias is that is more closely described by ""a number that is added to the weighted sum before activation""?
",['neural-networks'],"The purpose of the input layer is just to conceptually represent the input and, in case it is necessary, define the dimensions of the input that the neural network expects. In fact, some neural networks, such as multi-layer perceptrons, expect a fixed-size input, but not all of them: fully convolutional networks can deal with inputs of different dimensions.The input layer doesn't contain neurons (although in the diagrams that you will come across they are usually represented as circles, like the neurons, and that's probably why you are confused!), so it also does not contain biases, linear transformations, and non-linearities. In fact, in the context of neural networks, you could define a neuron as some unit/entity that performs a linear or non-linear transformation (to which you can add a bias). Note that the hidden and output layers can contain biases because they contain neurons that perform a linear or non-linear transformation.However, although I have never seen it (or I don't recall having seen it), I would not exclude the existence of an input layer that transforms or augments the inputs before passing them to the next layer. For example, one could implement a neural network that first scales the input to a certain range, and the input layer could do this, although, in practice, this is typically done by some object/class that does not belong to the neural network (e.g. tf.data.Dataset)."
Is it possible that every class has a higher recall than precision for multi-class classification?,"
I am a student learning machine learning recently, and one thing is keep confusing me, I tried multiple sources and failed to find the related answer.
As following table shows (this is from some paper):

Is it possible that every class has a higher recall than precision for multi-class classification?
Recall can be higher than precision over some class or overall performance which is common, but is it possible to keep recall greater than precision for every class?
The total amount of test data is fixed, so, to my understanding, if the recall is greater than the precision for one class, it is a must that the recall must be smaller than the precision for some other classes.
I tried to make a fake confusion matrix to simulate the result, but I failed. Can someone explain it to me?
this is a further description:
Assume we have classified 10 data into 3 classes, and we have a confusion matrix like this,

if we want to keep recall bigger than precision over each class (this case 0,1,2) respectively, we need to keep:
x1+x2 < x3+x5
x3+x4 < x1+x6
x5+x6 < x2+x4
There is a conflict, because the sum of the left side equals to the sum of the right side in these inequalities, and the sum(x1...x6) = 10 - sum(a,b,c) in this case.
Hence, I think to get recall higher than precision on all classes is not feasible, because the quantity of the total classification is fixed.
I don't know am I right or wrong, please tell me if I made a mistake.
","['machine-learning', 'text-classification', 'metric', 'precision']",
What is the big fuzz about SHA-RNN versus Transformers?,"
In his paper introducing SHA-RNN (https://arxiv.org/pdf/1911.11423.pdf) Stephen Merity states that neglecting one direction of research (in this case LSTMs) over another (transformers) merily because the SOTA in transformers are due to using more computing power is not the way to go.
I agree that finding neat tricks in AI/ML is equally (if not more) important than just throwing more computing power at the problem. However I am a little bit confused.
The main difference (since they both use attention units) between his SHA-RNN and transformers seem to be the fact that SHA-RNN uses LSTMs to ""encode the position of words"", where transformers do position encoding by using cosine and sine functions.
My confusion comes from the fact that LSTMs need to be handled sequentially and thus they cannot use this large advantage of GPUs, being able to compute things in parallel, whilst transformers can. Wouldn't this mean that (assuming LSTMs and positional encoding are able to acquire the same results), training using LSTMs would take longer than transformers and thus need more computing power, thus defeating the initial puporse of this paper? Or am I misinterpreting this?
Basically my question comes down to ""Why would an SHA-RNN be less computationally expensive than a transformer?""
","['recurrent-neural-networks', 'long-short-term-memory', 'transformer', 'attention']",
How should we interpret all the different metrics in reinforcement learning?,"
I'm trying to train some deep RL agents using policy gradient methods like AC and PPO. While training, I have a ton of different metrics being monitored.
I understand that the ultimate goal is to maximize the reward or return per episode.
But there are a ton of other metrics that I don't understand what they are used for.
In particular, how should one interpret the mean and standard deviation curves of the policy loss, value, value loss, entropy, and reward/return over time while training?
What does it mean when these values increase or decrease over time? Given these curves, how would one decide how to tune hyperparameters, see where the training is succeeding and failing, and the like?
","['reinforcement-learning', 'training', 'policy-gradients', 'metric']",
How is the reward in reinforcement learning different from the label in supervised learning problems?,"
How is the notion of immediate reward used in the reinforcement learning different from the notion of a label we find in the supervised learning problems?
","['reinforcement-learning', 'comparison', 'rewards', 'supervised-learning']","Reward in reinforcement learning (RL) is entirely different from a supervised learning (SL) label, but can be related to it indirectly.In a RL control setting, you can imagine that you had a data oracle that gave you SL training example and label pairs $x_i, y_i$ where $x_i$ represents a state and $y_i$ represents the correct action to take in that state in order to maximise the expected return. For simplicity I will use $G_t = \sum_{k=1}^{\infty} \gamma^k R_{t+k+1}$ for return here (where $G_t$ and $R_t$ are random variables), there are other definitions, but the argument that follows doesn't change much for them.You can use the oracle to reduce the RL training process to SL, creating a policy function $\pi(s): \mathcal{S} \rightarrow \mathcal{A}$ learned from a dataset that the oracle output. This clearly relates SL with RL, but how do $x_i, y_i$ from SL relate to $s_t, a_t$ from RL in terms of reward values?The states can relate directly (as input):$$x_i \equiv s_t$$The action from the policy function is more indirect, if you want to see how reward is involved:$$y_i \equiv \pi^*(s_t) = \text{argmax}_a \mathbb{E}_{A \sim \pi^*}[\sum_{k=1}^{\infty} \gamma^k R_{t+k+1} | S_t=s_t, A_t=a]$$Note the oracle is represented by the optimal policy function $\pi^*(s_t)$, and the expectation is conditional both on the start conditions of state and action plus following the optimal policy from then on (which is what $A \sim \pi^*$ is representing).In practice the optimal policy function is unknown when starting RL, so the learning process cannot be reduced to a SL problem. However, you can get close in some circumstances by creating a dataset of action choices made by an expert at the problem. In that case a similar relationship applies - the label (of which action to take) and immediate reward are different things but can be related by noting that the expert behaviour is close to the $\text{argmax}$ over actions of expected sums of future reward.Another way to view the difference:In SL, the signal from the label is an instruction - ""associate these two values"". Data is supplied to the learning process by some other independent process, and can be learned from directlyIn RL, the signal from the reward is a consequence - ""this is the value, in context, of what you just did"", and needs to be learned from indirectly. Data is not supplied separately from the learning process, but must be actively collected by it - deciding which state, action pairs to learn from is part of the agent's learning task"
"In PyTorch, why does the sequence length need to be provided as the first dimension of the input tensor for an RNN?","
I am confused as to why the sequence length is the first dimension of the input tensor for an RNN, while the batch size is the first dimension for any other kind of network (linear, CNN, etc.).
This makes me think that I haven't fully grasped the concept of RNN batches. Is each independent batch a different sequence? And is the same hidden state across batches? Is the hidden state maintained between timesteps for a given sequence (for vanilla/truncated BPTT)?
","['neural-networks', 'recurrent-neural-networks', 'pytorch']",
How can I identify bigrams and trigrams that represent concepts?,"
I have many text documents and I want to identify concepts in these documents in an unsupervised manner. One of my problems is that the concepts can be bigrams, trigrams, or even longer.
So, for example, out of all the bigrams, how can I identify the ones that are more likely to represent a concept?
A concept could be ""machine learning"".
Are you aware of any standard approaches to solve this problem?
Edit: The corpus I am working with consists of papers accessed from web of science. That is, they are all in some given domain niche. I want to extract words, bigrams, trigrams... that represent common concepts/buzzwords from these papers. These could be ""Automated machine learning"", ""natural language processing"" et cetera. I need to be able to distinguish these from other common n-grams such as ""New York"", ""Barack Obama"",...
I know that I could do this using a NER approach but this would require hand-labelling. Are you aware of any unsupervised ways to approach this problem? Or even a semi-superised method with little labelled data?
",['natural-language-processing'],
What is the score used to visualize attention in this paper?,"
I'm reading this paper Global-Locally Self-Attentive Dialogue State Tracker and follow through the implementation published in GLAD.
I was wondering if someone can clarify what variable or score is used to calculate the global and local self-attention scores in Figure 4 (the heatmap).
For me, it is not really clear how to derive these scores. The only score that would match the given dimension would be in the scoring module  $p_{utt}=softmax(a_{utt})$. However, I do not see in their implementation that anything is done with this value.
So, what I did was the following:
 q_utts = []
 a_utts=[]
 for c_val in C_vals:
     q_utt, a_utt = attend(H_utt, c_val.unsqueeze(0).expand(len(batch), *c_val.size()), lens=utterance_len)
     q_utts.append(q_utt)
     a_utts.append(a_utt)
attention_score= torch.mean(torch.stack(a_utts,dim=1),dim=1)

But the resulting attention score differs very much from what I expect.
","['natural-language-processing', 'papers', 'implementation', 'dialogue-systems']",
Should the importance sampling ratio be updated at the end of the for loop in the off-policy Monte Carlo control algorithm?,"
I'm studying RL with Sutton and Barto's book. I'd like to ask about the order of execution of a statement in the algorithm below.

Here, $W$ (importance sampling ratio) is updated at the end of the For loop.
But, I think that updating should be located after calculating $G$ (the return) and before updating $C(s,a)$ (cumulative of $W$). This seems to be right considering the second picture below, which I found in http://incompleteideas.net/book/first/ebook/node56.html.

Is Sutton and Barto's book wrong? Or the two algorithms in Sutton and Barto's book and seconds picture are actually the same, and I am wrong? Is there any difference between the two when implemented? If I am wrong, can you explain the reason?
","['reinforcement-learning', 'monte-carlo-methods', 'off-policy-methods', 'importance-sampling']",
Network design to learn multiple sequences of multiple categories,"
For learning a single sequence, LSTM only should suffice.
However, my situation is different here. I have a list of sequences to learn:

The sale volumes of 12 months, these are the sequences

And each sequence above belongs to a category.
I'm trying it out by consider [category,sequence] as a sequential sample, the loss can be reduced to 1% but it gives wrong values in inferring real data.
The second try is considering [category,sequence] as a sample of 2 inputs:

X1 = sequence
X2 = category

Feed the sequence thru' LSTM layers to get H, and then concat with X2, and feed again the pair [H,X2] thru' some dense layers, the results aren't better.
Any popular solutions (network shape, network design) for learning this kind of data: sequential data in different categories?
","['neural-networks', 'long-short-term-memory', 'sequence-modeling', 'categorical-data', 'network-design']",
Why the cost/loss starts to increase for some iterations during the training phase?,"
I am trying to build a recurrent neural network from scratch. It's a very simple model. I am trying to train it to predict two words (dogs and gods). While training, the value of cost function starts to increase for some time, after that, the cost starts to decrease again, as can be seen in the figure.

I am using the gradient descend method for optimization. Decreasing the step size/learning rate does not change the behavior. I have checked the code and math, again and again, I don't think there is an error (I could be wrong).
Why is the cost function not decreasing monotonically? Could there be a reason other than an error in my code/math? If there is an error, do you think that it is just a coincidence that each time the system finally converges to a very small value of error?
I am a beginner in the field of machine learning, hence, many questions I have asked may seem foolish to you. I am saving the values after every 100 iterations so the figure is actually for 15000 iterations.
About training:
I am using one-hot encoding. As the training data has only two samples (""gods"" and ""dogs""), where each alphabet is represented as d=[1,0,0,0],o=[0,1,0,0],g=[0,0,1,0],s=[0,0,0,1]. The recurrent neural network (RNN) goes back to a maximum of 3 time units, (e.g for dogs, the first input is 'd', then 'o', followed by 'g' and s). So, for the second input, the RNN goes back to 1 input, for the third input the RNN observes both previous inputs and so on. After calculating the gradients for the word ""dogs"", the values of the gradients are saved and the process is repeated for the word ""gods"". The gradients calculated for the second input ""gods"" are summed with the gradients calculated for ""dogs"" at the end of each epoch/iteration, and then the sum is used to updated all the weights. In each epoch, the inputs remain the same i.e ""gods"" and ""dogs"". In mini-batch training, in some epoch the RNN may encounter new inputs, hence, the loss may increase. However, I do not think that what I am doing qualifies as mini-batch training as there are only two inputs, both inputs are used in each epoch, and the sum of calculated gradients is used to update the weights.
","['neural-networks', 'machine-learning', 'recurrent-neural-networks', 'gradient-descent']",
Where is L2-regularization term applied,"
I have a confusion on where exactly is the L2 regularization (weight decay) is added.
In various resources I have come across, I find two equations where L2 regularization is applied.
Adding R(W) to loss function makes sense because it tries decrease large weights. Also, I have seen equations where we add R(W) to the weight update term, 2nd equation in 2nd line as shown in this image:

In the above image, using the weight update rule that
W(final) = W(initial) + (alpha) * (Gradient of W),

I obtain a different equation as compared to the other equation which is commonly written in various resources.
Where exactly is the regularization term added, I previously thought it was only added in Loss function but that gives me a different weight update equation from what is commonly presented in resources.( Or is my interpretation of the equation wrong? )
I presume it is also added in weight update equation because while constructing models, we add regularization term.
model.add(Conv2D(256, (5,5), padding=""same"", kernel_regularizer=l2(reg))) 

Would be grateful for any help.
",['regularization'],"The regularization terms are applied to the loss functions by default.
However, their gradients do appear in the update step as the gradient of loss appears in the update step."
Why are RNNs used in some computer vision problems?,"
I am learning computer vision. When I was going through implementations of various computer vision projects, some OCR problems used GRU or LSTM, while some did not. I understand that RNNs are used only in problems where input data is a sequence, like audio or text.
So, in kernels of MNIST on kaggle almost no kernel has used RNNs and almost every repository for OCR on IAM dataset on GitHub has used GRU or LSTMs. Intuitively, written text in an image is a sequence, so RNNs were used. But, so is the written text in MNIST data. So, when exactly is it that RNNs(or GRUs or LSTMs) need to be used in computer vision and when don't?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'recurrent-neural-networks', 'r-cnn']",
How to fine tune BERT for question answering?,"
I wish to train two domain-specific models:

Domain 1: Constitution and related Legal Documents
Domain 2: Technical and related documents.

For Domain 1, I've access to a text-corpus with texts from the constitution and no question-context-answer tuples. For Domain 2, I've access to Question-Answer pairs.
Is it possible to fine-tune a light-weight BERT model for Question-Answering using just the data mentioned above?
If yes, what are the resources to achieve this task?
Some examples, from the huggingface/models library would be mrm8488/bert-tiny-5-finetuned-squadv2, sshleifer/tiny-distilbert-base-cased-distilled-squad, /twmkn9/albert-base-v2-squad2.
","['natural-language-processing', 'bert', 'fine-tuning', 'question-answering']",
Number of LSTM layers needed to learn a certain number of sequences,"
Theoretically, number of units for a LSTM layer is the number of hidden states or the max length of sequences as per my practice.
For example, in Keras:
Lstm1 = LSTM(units=MAX_SEQ_LEN, return_sequences=False);

However, with lots of sequences to train, should I add more LSTM layers? because increasing MAX_SEQ_LEN is not the way as it doesn't help make the network better since the extra number of hidden states isn't useful any more.
I'm considering increasing number of LSTM layers, but how many are enough?
For example, 3 of them:
Lstm1 = LSTM(units=MAX_SEQ_LEN, return_sequences=True);
Lstm2 = LSTM(units=MAX_SEQ_LEN, return_sequences=True);
Lstm3 = LSTM(units=MAX_SEQ_LEN, return_sequences=False);

","['keras', 'long-short-term-memory', 'sequence-modeling', 'hidden-layers', 'network-design']","From my personal experience, the units hyperparam in LSTM is not necessary to be the same as max sequence length. Add more units to have the loss curve dive faster.And about the number of LSTM layers, trying out a single LSTM layer is a good start point, the model trains better with more LSTM layers.For example, MAX_SEQ_LEN=10, in Keras:"
Concrete example of how transposed convolutions are able to *add* features to an image,"
Say we have a simple gray scale image. If we use a filter which is just the 3x3 identity matrix (or more pointedly the identity matrix but with -1 instead of the 0 entries), it is fairly easy to see how applying this filter with stride length 1 and padding of 1 would produce an image of the same size that represents the presence of north-west diagonals in the input image.
As I am reading more about the generative networks in GAN paradigms, I am learning that 'transposed convolutions' are used to turn gaussian noise into meaningful images like a human face. However, when I try to look at sources for transposed convolutions, most articles address the upscaling use of these convolutions, rather than their 'generative' properties. Also it is not clear to me that upscaling is even necessary in these applications, since we could start with noise that has the same resolution as our desired output.
I am asking for an example, article, or paper that can provide me with more understanding as to the feature generation aspect of transposed convolutions. I have found this interesting article that relates the word 'transpose' to the transpose of a matrix. I have a good background in linear algegra, and I understand how the transpose would swap the dimensions of the input/output. This has obvious relation to upscaling/downscaling, but this effect would happen if we replaced the m x n matrix with any other n x m matrix, not specifically just the transpose. Essentially, I'm not sure how actual transpose functor can go from detecting a given feature associated to a convolutional filter, to producing that same feature
EDIT: I've done some thinking at it is clear to me now how the transpose matrix will produce an 'input image' that has the features specified by a given feature map. That is if $M$ is the matrix given by the convultion operation, and $F$ is a feature map then
$$M^T F$$
will produce an image with the corresponding features. Its obviously not a perfect inverse operation, but it works. However I still don't yet see how to interpret this transposed matrix as a convolution of its own
","['generative-adversarial-networks', 'generative-model', 'image-generation', 'transpose-convolution']",
How does training for DQN work if messing up in the environment in costly?,"
Suppose that we want to train a car to drive in the real world and decide to use Reinforcement Learning (specifically, DQN) for that. I am a bit confused about how training generally works.
Is it that we are exploring the environment at the same time that we are training the Q network? If so, is there not a way to train the Q network before actually going out into the real world? And then, aren't there millions of possible states in the real world? So, how does RL or I guess the neural network generalize so that it can function during rush hour, empty roads, etc.
","['reinforcement-learning', 'q-learning', 'dqn']","In general, you need to actively explore the environment to gather data to train your Q network. However, especially in your self-driving car example, you might be looking for Batch RL. In Batch RL you start with a given, fixed dataset of transitions (state, action, reward, next state) and you learn a policy (or Q function) based on the dataset without exploring. The Batch-Constrained Q-learning algorithm (BCQ) is a good example of this.With regards to acquiring the data necessary for Batch RL, that's another story. I suppose one could collect data through sensors in a car, and label datapoints with rewards according to some reward function."
Why do some DQN implementations not require random exploration but instead emulate all actions?,"
I've found online some DQN algorithms that (in a problem with a continuous state space and few actions, let's say 2 or 3), at each time step, compute and store (in the memory used for updating) all the possible actions (so all the possible rewards). For example, on page 5 of the paper Deep Q-trading, they say

This means that we don't need a random exploration to sample an action as in many reinforcement learning tasks; instead we can emulate all the three actions to update the Q-network.

How can this be compatible with the exploration-exploitation dilemma, which states that you have to balance the time steps of exploring with the ones of exploiting?
","['reinforcement-learning', 'dqn', 'deep-rl', 'exploration-exploitation-tradeoff']","The example that you linked is using a model (emulation) in order to look ahead at all possible actions from any state. It essentially explores off-policy and offline using that model. This is not an option that is available in all environments, but if possible it resolves the exploration/eploitation dilemma for a single time step nicely by investigating all options.Longer term the agent proposed by the link does not sufficiently explore for general use in my opinion. It appears to always choose a single action deterministically based on maximising action value. In other words it always attempts to exploit the training data so far, even though it augments the training data with short-term knowledge about exploration. However, this appears to be sufficient in the problem domain that it is used in. I suspect this is for a couple of reasons:The environment is non-stationary, making long-term state predictions unreliable in any case. An agent that learns to exploit in the short term (i.e. over only a few time steps into the future) is likely to be approximately optimal already.State transitions may be highly stochastic, meaning that state space will still be adequately explored even using a deterministic policy. This feature of the environment is also used by other well-known Q learning approaches with deterministic behaviour policies, such as TD GammonI think you have correctly identified a weakness of the approach used in the linked paper that means it may not make a strong general algorithm. The algorithm avoids addressing the exploration/exploitation balance in full, and instead relies on features of the environment to work well despite this. If you find yourself working in similar environments for your own projects, then it may well be worth trying the same approach. However, if you find yourself working in a more deterministic environment with more stationary behaviour and sparse rewards, the lack of state space exploration would be a serious limitation."
How exactly does nested cross-validation work?,"
I have trouble understanding how nested cross-validation works - I understand the need for two loops (one for selecting the model, and another for training the selected model), but why are they nested?
From what I understood, we need to select the model before training it, which points toward non-nested loops.
Could someone please explain what's wrong (or right?) with my line of reasoning, and also explain nested cross-validation in greater detail? A representative example would be great.
","['machine-learning', 'comparison', 'hyper-parameters', 'cross-validation']","""Selecting the model"" in this case refers to selecting the hyperparameters of the model. The reason to use a nested CV is simply to avoid overfitting training data.Consider the example in the link. First you like to select the best hyperparameters of your svm model by GridSearchCV(). This is done by 4-fold CV. Now the clf.best_score_ will be the mean cross-validated score of the best estimator (the model with the best hyperparameters). However now you used the same data for training and reporting the performance, although you used CV. Keep in mind that the folds are not independent. Therefore the hyperparameters might be too data specific, i.e. your generalization error estimates are too optimistic. Therefore we like to evaluate our final model performance outside / independent of the hyperparameter selection loop / process (the call cross_val_score()).In the provided plot, you can clearly see that the reported performance by GridSearchCV() is most of the time better than the performance reported by cross_val_score()."
Structure-preserving layer in a network with respect to a transformation,"
I'm reading this paper: https://arxiv.org/pdf/1602.07576.pdf. I'll quote the relevant bits:

Deep neural networks produce a sequence of progressively more abstract representations by mapping the input through a series of parameterized functions.  In the current generation of neural networks, the representation spaces are usually endowed with very minimal internal
structure, such as that of a linear space $\mathbb{R}$^n.
In this paper we construct representations that have the structure of a linear $G$-space, for some chosen group $G$. This means that each vector in the representation space has
a pose associated with it, which can be transformed by the elements of some group of transformations $G$. This additional structure allows us to model data more efficiently: A
filter in a $G$-CNN detects co-occurrences of features that have the preferred relative pose [...]
A representation space can obtain its structure from other representation spaces to which it is connected. For this to work, the network or layer $\phi$ that maps one representation
to another should be structure preserving. For $G$-spaces this means that $\phi$ has to be equivariant: $$\phi(T_gx)=T'_g\phi(x)$$That is, transforming an input $x$ by a transformation $g$ (forming $T_gx$) and then passing it through the learned map $\phi$ should give the same result as first mapping $x$ through $\phi$ and then transforming the representation.
Equivariance can be realized in many ways, and in particular the operators $T$ and $T'$ need not be the same. The only requirement for $T$ and $T'$ is that for any two transformations $g$ and $h$, we have $T(gh) = T (g)T (h)$ (i.e. $T$ is a linear representation of $G$).

I didn't understand the paragraph in bold. A structure preserving map is something that preserves some operation between elements in the underlying set. A simple example: if $f:\mathbb{R}^3\to\mathbb{R}$ such that $(x,y,z)^T\mapsto x+y+z$, then
$$f(r+s)=f((r_1,r_2,r_3)^T+(s_1,s_2,s_3)^T)=f((r_1+s_1,r_2+s_2,r_3+s_3))
\\=r_1+s_1+r_2+s_2+r_3+s_3=f(r)+f(s)$$
where the addition in the far left term is in $\mathbb{R}^3$ and addition in the far right is in $\mathbb{R}$. So the map $f$ preserves the additional structure of addition.
In the quoted paragraph, $\phi$ is the structure preserving map, but what's the structure being preserved exactly? And why is the operator on the right different from one on the left? i.e. $T'$ on RHS instead of $T$
",['convolutional-neural-networks'],
Is such a captcha AI-resistant?,"
Let's say we have a captcha system that consists of a greyscale picture (of a part of a street or something akin to re-captcha), divided into 9 blocks, with 2 missing pieces.
You need to choose the appropriate missing pieces from over 15 possibilities to complete the picture.
The puzzle pieces have their edges processed with glitch treatment as well as they have additional morphs such as heavy jpeg compression, random affine transform, and blurred edges.
Every challenge picture is unique - pulled from a dataset of over 3 million images.
Is it possible for the neural network to reliably (above 50%) predict the missing pieces? Sometimes these are taken out of context and require human logic to estimate the correct piece.
The chance of selecting two answers in correct order is 1/15*1/14.
","['convolutional-neural-networks', 'image-recognition', 'image-segmentation', 'feature-extraction', 'captcha']","Well to give you a short answer, I would say that YES, it would be MORE resistant than a more standard captcha approach...This being said, I would still go as far as to predict something like a 75-80% successful prediction rates, for a custom model which is designed specifically for defeating a mechanism such as what you describe. The reason why I am fairly confident in such an appraisal, is primarily because of the following:New techniques which researchers have begun to explore, which are intended to be ""Structure Preserving Convolutions"" which utilize a higher dimensional filter to store the extra correlation data.I think that the obfuscation efforts that you mention will definitely help to some degree, although they can be easily defeated by training the model on a dataset which you pull out some portion of the samples during pre-processing and inject the same sort of noise and glitch treatments, etc.TL;DR: If you cant beat 'em, then join 'em! Just train a model to defeat your captcha implementation, and then use the model to generate adversarial examples and then apply obfuscations to your data set accordingly!For more information on what I am talking about in my suggestion for further obfuscation efforts explore some of the papers you can find on Google Scholar - Ensemble Adversarial Training Examples"
How does best-first search differ from hill-climbing?,"
How does best-first search differ from hill-climbing?
","['comparison', 'search', 'hill-climbing', 'best-first-search']",
How to prevent image recognition of my dataset with neural networks and make it hard to train them?,"
Suppose I have a private set of images containing some objects.
How do i

Make it very hard for the neural networks such as ImageNet to recognize these objects, while allowing humans to do it at the same time?

Suppose I label these private images - a picture of a cat with a label ""cat"" - how do I make it hard for the attacker to train his neural network on my labels? Is it possible to somehow fool a neural network so that they couldn't easily train it to recognize it?


Like random transforms etc, so that they couldn't use a neural network to recognize these objects, or even train it on my dataset if they had labels.
","['convolutional-neural-networks', 'computer-vision', 'image-recognition', 'captcha']",
Is it possible to classify resistors using ResNet50?,"
I want to train ResNet50 model using resistor images like below:

I tried it by collecting data from google images and there were quite few. So accuracy was very low (around %10) but I wonder If it is due lack of images or is it really possible to classify these images? Because as it is seen the object to be classified is very small and its value as color coded. I thought maybe this is not a good idea. Searched it on google but could not find anybody tried to do it before. I have also tried data augmentation and changing to other models but still its accuracy was quite low.
P.S: I have also tried changin epoch numbers, optimizers and all other parameters. So I want to make sure If it is due low data or is it just very hard task to complete for a computer vision model.
And Is it rational to crop the image by using a mask before classifying it to make sure all color codes are bigger and easily valuable by model?
","['deep-learning', 'classification', 'computer-vision', 'datasets', 'categorical-data']","Yes it should be possible. You may have a bug in your code, or the wrong hyperparameters. Training ResNet-50 will take a long time. Try training on other sets of images and see what accuracy you get to check if your approach is correct. Or, try loading a pretrained model, and training from that."
"What does the notation ""for t=T to 1,−1 do"" in terms of time steps, in deep recurrent q network?","
In looking at an algorithm in the paper Learning to Communicate with Deep Multi-Agent Reinforcement Learning.
Here is the full algorithm:

What does the notation for t=T to 1,−1 do: refer to in terms of time steps?
The network structure is a deep recurrent q network.
Secondly, why do the gradients need to be reset to zero?
","['reinforcement-learning', 'recurrent-neural-networks', 'dqn', 'notation']",
Into which subcategories can reinforcement learning be divided?,"
In the course of a scientific work, I will discuss the different types of reinforcement learning. However, I have difficulties to find these different types.
So, into which subcategories can reinforcement learning be divided? For example, the following subdivisions seem to be useful

Model-free and Model-based
Dynamic Programming, Monte Carlo and Temporal Difference

Any others?
","['reinforcement-learning', 'monte-carlo-methods', 'temporal-difference-methods', 'model-based-methods', 'model-free-methods']","Your two suggestions are not mutually exclusive. If you go by this process, you'll have to do a ""Cartesian product"" of a bunch of different RL categorizations which would get out of hand. I recommend, if you can, to describe some sort of ""RL taxonomy"" instead. By this I mean describing different RL characterizations without assuming they're mutually exclusive.To add to your list :"
What is the role of embeddings in a deep recurrent Q network?,"
When describing the model architecture for a deep recurrent q network, the authors of the paper Learning to Communicate with Deep Multi-Agent Reinforcement Learning

each agent consists of a recurrent neural network (RNN), unrolled for $T$ time-steps, that maintains an internal state $h$, an input network for producing a task embedding $z$, and an output network for the Q-values and the messages $m$. The input for agent $a$ is defined as a tuple of $\left(o_{t}^{a}, m_{t-1}^{a^{\prime}}, u_{t-1}^{a}, a\right)$.

Can someone explain what the purpose of the embedding layer is in this specific context?
Implementation can be found here.
","['reinforcement-learning', 'recurrent-neural-networks', 'dqn', 'deep-rl', 'embeddings']","The purpose of the input network is to embed the input tuple into a state/task representation, that can then be fed into the RNN hidden state at each time step.$(o^a_t,m^a′_{t−1},u^a_{t−1},a)$ (input) $\rightarrow$ input network (embedding) $\rightarrow$ $z_t$ (task representation)According to to section 6.1 of the paper, the input is a tuple represented as $(o^a_t,m^a′_{t−1},u^a_{t−1},a)$. Each of these  terms is described in sec 3 as:$o_t$ - The observation. The authors assume a POMDPTwo types of actions:$a$ - The agent (This being a multi-agent DQN algorithm)They form the input to the Deep Recurrent Q-Network architecture.The purpose of the embedding network is to receive a tuple of these inputs and produce a state
embedding $z$. This state embedding is then fed  into a hidden state $h^a_{t-1}$ of the RNN.Though the authors refer to it as an input network that produces embedding, in practice, they use different embedding functions for each of these inputs. The final task/state embedding $z_t^a$ is expressed as a sum:$$
z_t^a = \text{MLP}(o_t^a) + \text{MLP}(m_{t-1}) + \text{LookupTable}(u_{t-1}) + \text{LookupTable}(a)
$$The summed embeddings are all of the same size.This answer makes an assumption that by ""embedding layer"" you meant the input embedding network. Since the paper makes no reference to a single embedding layer in the model architecture."
"What does it mean when a model ""statistically outperforms"" another?","
I was reading this paper where they are stating the following:

We also use the T-Test to test the significance of GMAN in 1 hour ahead prediction compared to Graph WaveNet. The p-value is less than 0.01, which demonstrates that GMAN statistically outperforms Graph WaveNet.

What does ""Model A statistically outperforms B"" mean in this context? And how should the p-value threshold be selected?
","['deep-learning', 'terminology', 'papers', 'statistics']","Most model-fitting is stochastic, so you get different parameters every time you train, and you usually can't say that one algorithm will always give you a better-performing model.However, since you can retrain many times to get a distribution of models, you can use a statistical test like the T-Test to say ""algorithm A usually produces a better model than algorithm B,"" which is what they mean by ""statistically outperforms.""p-value is usually set by consensus in the field. The higher the p-value, the less confidence you have that there's a statistical difference between the distribution of values being compared. 0.1 might be normal in a field where data is very expensive to collect (like risky, long-term studies of humans), but in machine learning, it's usually easy enough to retrain a model that 0.01 is common, and demonstrates very high confidence. To know more about selecting and interpreting the values, I recommend Wikipedia's page on statistical significance."
Suppress heatmap non-maxima in segmentation with UNet,"
I'm using U-Net for image segmentation.
The model was trained with images that could contain up to 4 different classes. The train classes are never overlapping.
The output of the UNet is a heatmap (with float values between 0 and 1) for each of these 4 classes.
Now, I have 2 problems:

for a certain class, how do I segment (draw contours) in the original image only for the points where the heatmap has significant values? (In the image below an example: the values in the centre are significant, while the values on the left aren't. If I draw the segmentation of the entire image without any additional operation, both are considered.)



downstream of the first point, how do I avoid that in the original image the contours of two superimposed classes are drawn? (maybe by drawing only the one that has higher values in the corresponding heatmap)

","['neural-networks', 'convolutional-neural-networks', 'image-segmentation', 'u-net']",
Are there any agents that are based on quantum computing?,"
Assuming the definition of an agent to be:

An entity that perceives its environment, processes the perceived information, and acts on the environment such that some goal is fulfilled.

Are there any agents that are based on quantum processing/computing (i.e. implemented by a network of quantum gates)?
Is there any work done towards this end? If so, could someone provide references?
","['intelligent-agent', 'quantum-computing']","I think you are looking for quantum machine learning (QML), which is a relatively new field that sits at the intersection of quantum computing and machine learning.If you are not familiar with quantum computing (QC) and you are interested in QML, I suggest that you follow this course by prof. Umesh Vazirani and read the book Quantum Computing for Computer Scientists (2008) by Yanofsky and Mannucci. If you have a solid knowledge of linear algebra, you should not encounter big problems while learning QC. Be prepared to deal with the weirdness and beauty of qubits, quantum entanglement, and so on.If you want to directly dive into QML (although I am not familiar with the details of QML, I suggest that you first get familiar with the basics of QC, which I am familiar with) there are already several courses, papers, overviews, and libraries (such as TensorFlow Quantum) on quantum machine learning.If you are interested in quantum reinforcement learning, maybe have a look at the paper Quantum Reinforcement Learning (2008) by Daoyi Dong et al."
96.91% accuracy on MNIST after 2 hours of training using custom made neural net library. Ways to improve?,"
I wanted to understand back-propagation so I made a basic neural network library. I used momentum, with learning rate = $0.1$, beta = $0.99$, epochs = $200$, batch size = $10$, loss function is cross entropy and model structure is $784$, $64$, $64$, $10$ and all layers use sigmoid. It performed terribly at first, so I initialized all the weights and biases in the range $[10^{-9}, 10^{-8}]$ and it worked. I am quite new to deep learning and I find TensorFlow doesn't seem as friendly to beginners who want to play around with hyper-parameters. How do you find the right hyper-parameters? I trained it on 100 digits (which took 10 minutes), tweaked hyper-parameters, chose the best set and trained the model using that set on the entire data set of $60,000$ images. I also found that halving the epochs and doubling the training set size gave better results. Are there fool proof heuristics to find good hyper-parameters? What is the best set of hyper-parameters (without regularization, dropout, etc) for MNIST digits? Here is the code for those who want to take a look.
","['deep-learning', 'hyperparameter-optimization']",
Why is 'scatter' used instead of variance in LDA?,"
I've been reading about Fisher's Linear Discriminant Analysis lately, and I noticed that the objective function (particularly for two-class classification) to be maximized contains scatter terms instead of variance, in the denominator. Why is that?
To clarify, the scatter of a sample is just the variance multiplied by the number of data points in the sample.
Thank you!
","['machine-learning', 'classification', 'supervised-learning']",
How to manually collect rectangular training data samples from images?,"
I want to collect training samples from images.
That can mean different things depending on the context. I think of the simplest case, which should be most commonly required. Because it is so common, there may be a standard tool for it.
An example would be to have a collection of images of random street scenes and manually collect images of nonoccluded cars from them into separate files.
What is a common way or tool to do this:
For a large number of images, select one or more rectangles (of arbitrary size and with edges parallel to the image edges) in the image and save them to separate image files.
Of course, it can be done with any general image editing program, but in this case, most of the work time would be used for opening new images, closing old images, saving sample images and the most time-consuming part of entering a non-conflicting file name for the individual sample image files.
For small numbers of samples per input file, this may need about an order of magnitude more time, and also more complex interaction.
I would prefer a tool running on Linux/Ubuntu.
If this does not exist, I'd be curious why.
","['machine-learning', 'training', 'image-processing']",
"Text classification of non-equal length texts, should I pad left or right?","
Text classification of equal length texts works without padding, but in reality, practically, texts never have the same length.
For example, spam filtering on blog article:
thanks for sharing    [3 tokens] --> 0 (Not spam)
this article is great [4 tokens] --> 0 (Not spam)
here's <URL>          [2 tokens] --> 1 (Spam)

Should I pad the texts on the right:
thanks for     sharing --
this   article is      great
here's URL     --      --

Or, pad on the left:
--   thanks  for    sharing
this article is     great
--   --      here's URL

What are the pros and cons of either pad left or right?
","['natural-language-processing', 'long-short-term-memory', 'sequence-modeling', 'text-classification', 'padding']","For any model that does not take a time series approach like an RNN does, the padding shouldn't make a difference.I prefer padding right simply because there also might be text you need to cut-off. Then padding is more intuitive as you either cut-off a text if it's too long or pad a text when it's too short.Either way, when a model is trained a certain way, it shouldn't make a difference so long the testing is also padded the way it was presented in training."
Do we need multiple parallel environments to train in batches an on-policy algorithm?,"
When using an on-policy method in reinforcement learning, like advantage actor-critic, you shouldn't use old data from an experience buffer, since a new policy requires new data. Does this mean that to apply batching to an on-policy method you have to have multiple parallel environments?
As an extension of this, if only one environment is available when using on-policy methods, does that mean batching isn't possible? Doesn't that limit the power of such algorithms in certain cases?
","['reinforcement-learning', 'environment', 'policies', 'experience-replay', 'on-policy-methods']",
Why does L1 regularization yield sparse features?,"

In contrast to L2 regularization, L1 regularization usually yields sparse feature vectors and most feature weights are zero.

What's the reason for the above statement - could someone explain it mathematically, and/or provide some intuition (maybe geometric)?
","['machine-learning', 'regularization', 'l2-regularization', 'l1-regularization']","In L1 regularization, the penalty term you compute for every parameter is a function of the absolute value of a given weight (times some regularization factor).
Thus, irrespective of whether a weight is positive or negative (due to the absolute value) and irrespective of how large the weight is, there will be a penalty incurred as long as weight is unequal 0. So, the only way how a training procedure can considerably reduce the L1 regularization penalty is by driving all (unnecessary) weights towards 0, which results in a sparse representation.Of course, the L2 regularization will also only be strictly 0 when all weights are 0. However, in L2, the contribution of a weight to the L2 penalty is proportional to the squared value of the weight. Therefore, a weight whose absolute value is smaller than 1, i.e. $abs(weight) < 1$, will be much less punished by L2 than it would be by L1, which means that L2 puts less emphasis on driving all weights towards exactly 0. This is because squaring a some value in (0,1) will result in a value of lower magnitude than taking the un-squared value itself: $x^2 < x\ for\ all\ x\ with\ abs(x) < 1$.So, while both regularization terms end up being 0 only when weights are 0, the L1 term penalizes small weights with $abs(x) < 1$ much more strongly than L2 does, thereby driving the weight more strongly towards 0 than L2 does."
Why are decision trees and random forests scale invariant?,"
Feature scaling, in general, is an important stage in the data preprocessing pipeline.
Decision Tree and Random Forest algorithms, though, are scale-invariant - i.e. they work fine without feature scaling. Why is that?
","['decision-trees', 'random-forests', 'feature-engineering']","Scaling only makes sense when there is something that reacts to that scale. Decision Trees though, just make a cut at a certain number.Imagine: For a feature that goes from 0 to 100 a cut at 50 may be improving performance. Scaling this down to 0 to 1 making the cut a 0.5 doesn't change a thing.Now on the other hand NN have some kind of activation function (leaving RELu aside) that react differently to input that is above 1. Here Normalization, putting every feature between 0 and 1 makes sense."
"What is the definition of the ""cost"" function in the SVM's objective function?","
In a course that I am attending, the cost function of a support vector machine is given by
$$J(\theta)=\sum_{i=1}^{m} y^{(i)} \operatorname{cost}_{1}\left(\theta^{T} x^{(i)}\right)+\left(1-y^{(i)}\right) \operatorname{cost}_{0}\left(\theta^{T} x^{(i)}\right)+\frac{\lambda}{2} \sum_{j=1}^{n} \Theta_{j}^{2}$$
where $\operatorname{cost}_{1}$ and $\operatorname{cost}_{0}$ look like this (in Magenta):


What are the values of the functions $\operatorname{cost}_{1}$ and $\operatorname{cost}_{0}$?
For example, if using logistic regression the values of $\operatorname{cost}_{1}$ and $\operatorname{cost}_{0}$ would be $-\log* \operatorname{sigmoid}(-z)$ and $-\log*(1-\operatorname{sigmoid}(-z))$.
","['objective-functions', 'math', 'support-vector-machine', 'hinge-loss']","That is the hinge loss, a type of loss most notably used for SVM classification. The hinge loss is typically defined as:$$
\ell(y)=\max (0,1-t \cdot y),
$$which, in your use case, is something like this:$$
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{array}{ll}
\max \left(0,1-\theta^{T} x\right) & \text { if } y=1 \\
\max \left(0,1+\theta^{T} x\right) & \text { if } y=0
\end{array}\right.
$$Check this article on it."
What reinforcement learning algorithm should I use in continuous states?,"
I want to use reinforcement learning in an environment I made. The exact environment doesn't really matter, but it comes down to this: The amount of different states in the environment is infinite e.g. amount of ways you can put 4 cars at an intersection, but the amount of different actions is only 3 e.g. go forward, right or left. The state exists out of five numbers. My question is: what algorithm should I use or at least what kind of algorithm?
","['reinforcement-learning', 'algorithm', 'dqn', 'ddpg']",I would recommend looking at Deep Q-Learning.
Tic-tac-toe: How would standard SARSA and Q-learning yield different results in the agent's behaviour?,"
I know this is deceptively simple. Tic tac toe is a well studied game for RL.
Assume your agent is playing aggainst a strong opponent.
I know you deal in after states. I know that in Q learning the optimal policy should be converged on faster as the Q(S,A) is becoming closer to the optimal each step. While in SARSA the Q function will not be updated towards the optimal sometimes as it is exploring. If epislon is fixed SARSA will convereg to the epislon greedy policy.
I came across the question above and I don't know the answer, is it that SARSA may play more conservatively? Opting for more draws rather than getting in board positions where one could is likely to either lose or win rather than draw.
","['machine-learning', 'reinforcement-learning', 'q-learning', 'sarsa']",
Should batch normalisation be applied before or after ReLU?,"
I know that there has been some discussion about this (e.g. here and here), but I can't seem to find consensus.
The crucial thing that I haven't seen mentioned in these discussions is that applying batch normalization before ReLU switches off half the activations, on average. This may not be desirable.
In other words, the effect of batch normalization before ReLU is more than just z-scaling activations.
On the other hand, applying batch normalization after ReLU may feel unnatural because the activations are necessarily non-negative, i.e. not normally distributed. Then again, there's also no guarantee that the activations are normally distributed before ReLU clipping.
I currently lean towards a preference to batch normalization after ReLU (which is also based on some empirical results).
What do you all think? Am I missing something here?
","['neural-networks', 'deep-learning', 'relu', 'batch-normalization']",
A neural network with 2 or more hidden layers is a DNN? [duplicate],"







This question already has answers here:
                                
                            




Is reinforcement learning using shallow neural networks still deep reinforcement learning?

                                (2 answers)
                            


How is a deep neural network different from other neural networks?

                                (2 answers)
                            

Closed 3 years ago.



I just learned the math behind neural networks so please bear with my ignorance. I wonder if there is a precise definition for DNN.
Is it true that any neural network with more than 2 hidden layers can be named as a DNN, and training a NN with 2 hidden layers using Q-learning we are technically doing a type of deep reinforcement learning?
PS: If it is conceptually that simple why do common people regard deep learning as something done by archmages in ivory towers.
","['neural-networks', 'deep-learning', 'deep-rl', 'terminology']","I don't think there is a fixed threshold that differentiates between Shallow and Deep Learning, but I would say that a 2 layer NN should not be considered deep. But now-a-days, almost all NN architectures are studied under the umbrella of Deep Learning.And yes, training a 2 hidden layers NN using Q-learning would technically mean doing deep RL.I guess it is conceptually simple but making NN perform optimally is an art. Tuning hyperparameters or debugging NN can be tough and one learns with experience. I guess others in the community would be much more suited to answer this question. But these were my 2 cents."
Why does shifting all the rewards have a different impact on the performance of the agent?,"
I am new to reinforcement learning. For my application, I have found out that if my reward function contains some negative and positive values, my model does not give the optimal solution, but the solution is not bad as it still gives positive reward at the end.
However, if I just shift all readings by subtracting a constant until my reward function is all negative, my model can reach the optimal solution easily.
Why is this happening?
I am using DQN for my application.
I feel that this is also the reason why the gym environment mountaincar-v0 uses $-1$ for each time step and $0.5$ at the goal, but correct me if I am wrong.
","['reinforcement-learning', 'dqn', 'rewards', 'reward-shaping', 'reward-functions']",
Do GANs also learn to map between the distribution from which the random noise is sampled and the true distribution of the data?,"
I am reading about GANs. I understand that GANs learn implicitly the probability distribution that generated the data. However, at the input we give a random noise vector. It seems that we can sample that random noise vector from whatever distribution we want.
My question is: given that there is ONLY ONE possible distribution that could have generated the data, and that our GAN is trying to approximate that distribution, can I think that the GAN also learns how to map between that distribution that it needs to learn and the distribution from which we sample the random noise vector?
I am thinking about this, as the random noise vector can be sampled from whatever distribution we want, so it can be different each time we start to train, so it can vary, but the GAN needs to be able to still imitate one unique distribution, so, in a way, it needs to be able to adapt to the distribution from which the noise comes.
",['generative-adversarial-networks'],
How are the coefficients of the Region of Interest being selected?,"
I was reading the following paper: Rl-Ncs: Reinforcement Learning Based Data-Driven Approach For Nonuniform Compressed Sensing, and my question is: how do they decide whether a signal is characterized as a region of interest coefficient or non-region of interest coefficient?
","['reinforcement-learning', 'papers']",
Is GAIL applicable if the expert's trajectories are for the same task but are in a different environment?,"
Is the GAIL applicable if the expert's trajectories (sample data) are for the same task but are in a different environment (modified but will not be completely different)?
My gut feeling is, yes, otherwise we can just simply adopt behavioural cloning. Furthermore, since the expert's trajectories are from a different environment, the dimension/length of state-action pairs will most likely be different. Will those trajectories still be useful for GAIL training?
","['reinforcement-learning', 'generative-adversarial-networks', 'generative-model', 'imitation-learning', 'gail']","The authors of the paper Learning Robust Rewards with Adversarial Inverse Reinforcement Learning (2018, published in ICRL), which introduced the inverse RL technique AIRL, argue that GAIL fails to generalize to different environment's dynamics. Specifically, in section 7.2 (p. 7), they describe an experiment where they disable and shrink the two front legs of the ant, then, based on the results, they concludeGAIL learns successfully in the training domain, but does not acquire a representation that is suitable for transfer to test domains.On the other hand, according to their experiments, AIRL is more robust to changes in the environment's dynamics."
How can I read any AI paper?,"
I have studied linear algebra, probability, and calculus twice. But I don't understand how can I reach the level that I can read any AI paper and understand mathematical notation in it.
What is your strategy when you see the mathematical expression that you can't understand?
For example, in Wasserstein GAN article, there are many advanced mathematical notations. Also, some papers are written by people who have a master's in mathematics, and those people use advanced mathematics in some papers, but I have a CS background.
When you come across this kind of problem, what do you do?
","['research', 'papers', 'academia']",
Is it possible to apply the associative property of the convolution operation when it is followed by a non-linearity?,"
The associative property of multidimensional discrete convolution says that:
$$Y=(x \circledast h_1) \circledast h_2=x\circledast(h_1\circledast h_2)$$
where $h_1$ and $h_2$ are the filters and $x$ is the input.
I was able to do exploit this property in Keras with Conv2D: first, I convolve $h_1$ and $h_2$, then I convolve the result with $x$ (i.e. the rightmost part of the equation above).
Up to this point, I don't have any problem, and I also understand that convolution is linear.
The problem is when two Conv2D layers have a non-linear activation function after the convolution. For example, consider the following two operations
$$Y_1=\text{ReLU}(x \circledast h_1)$$
$$Y_2=\text{ReLU}(Y_1\circledast h_2)$$
It is possible to apply the associative property if the first or both layers have a non-linear activation function (in the case above ReLU, but it could be any activation function)? I don't think so. Any idea or related paper or some kind of approach?
","['convolutional-neural-networks', 'activation-functions', 'convolution']","Yes, when you have non-linearity it is not possible to combine your convolution steps.However, you can approximate the two layers network with one layer net according to the Universal Approximation Theorem. You will probably need to use something like a knowledge distillation technique to do it. Note that, the theorem doesn't say anything about the number of neurons required or if the learning techniques that we usually use will work well.Also, $ReLU(x)$ is a linear mapping when $x \geq 0$, so if your input, weight, and biases are $\geq 0$, the net can be exactly modeled with a single layer network."
How to measure/estimate the energy consumption of CNN models during testing?,"
Does someone know a method to estimate / measure the total energy consumption during the test phase of the well-known CNN models? So with a tool or a power meter...
MIT has already a tool to estimate the energy consumption but it only works on AlexNet and GoogleNet, I need something for more Architectures (VGG, MobileNet, ResNet...). And I also need a metric to evaluate the well-known architects in terms of energy consumption. So at first estimate or measure the energy consumption and then evaluate the results with a good metric.
With a measuring device I would measure the power consumption before using the CNN and I will repeat this experiment a few times then I will average the results, and do the same thing while using the CNN and at the end I will compare the results. But I have three problems here:
1. how can I know that nothing else is running on the PC that also consumes energy while using the CNN?
2. how can I increase the accuracy of the measurements?
3. I don't find any power meter that measures the energy consumption in short periodes (1s).

Thats why I prefer a tool to estimate the energy consumption, the accuracy of the measurments will not be that good but I didn't find any another tool..
Does someone have an Idea, papers, sites that can help me?
Many thanks in advance for your reply!
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'comparison', 'pretrained-models']",
Studies on interest in results from ML purely due to use of ML,"
There is often interest in the results of machine learning algorithms, specifically because they came from machine learning algorithms -- as opposed to interest in the results in and of itself. It seems similar to the 'from the mouths of babes' trope, where comments made by children are sometimes regarded as containing some special wisdom, due to the honest and innocent nature of their source. Similarly, people seem to think that the impassionate learning of a machine might extract some special insight from a data set.
(Of course, anyone with such opinions has obviously never met either a machine learning algorithm or a child.)
Has this effect been discussed or studied anywhere? Does it have a name?
","['machine-learning', 'resource-request', 'ethics']",
Is it possible to create a named entity recognition system without using POS tagging in the corpus?,"
Is it possible to create a named entity recognition system without using POS tagging in the corpus?
","['natural-language-processing', 'named-entity-recognition', 'pos-tagging']",
NLP Identifying important key words in a corpus,"
I am intrigued with the idea of Zettelkasten but unsatisfied with the current implementations. It seems to me that a machine learning and NLP approach could be productive by helpfully identifying “important” keywords on which to links could be created, with learning to help narrow the selection of keywords over time.
My problem is that it’s been 30 years since AI classes in grad school and things have moved on. I’m sure I could become an nlp expert with study but I don’t wanna. So I’m looking for guidance: what are the right terms to describe identifying keywords in context, ideally with some semantic content; how would I apply ML with my training to improve the keyword identification.
I’d love references, ideas, and packages references. Python is preferred, but not strongly; I write most common (and many uncommon, SNOBOL and COBOL anyone?) languages so language isn’t all that much of an issue.
","['machine-learning', 'natural-language-processing', 'text-classification', 'text-summarization']",
Can fully connected layers be used for feature detection?,"
I need help in understanding something basic.
In this video, Andrew Ng says, essentially, that convolutional layers are better than fully connected (FC) layers because they use fewer parameters.
But I'm having trouble seeing when FC layers would/could ever be used for what convolutional layers are used for, specifically, feature detection.
I always read that FC layers are used in the final, classification stages of a CNN, but could they ever be used for the feature detection part?
In other words, is it even possible for a ""feature"" to be deciphered when the filter size is the same as the entire image?
If not, it's hard for me to understand Andrew Ng's comparison---there aren't any parameter reduction ""savings"" if we're not going to use an FC ""filter"" in place of a CNN layer in the first place.
A semi-related question: Can multi-layer perceptrons (which I understand to be fully connected neural networks) be used for feature detection? If so, how do image-sized ""filters"" make any sense?
","['convolutional-neural-networks', 'dense-layers', 'convolutional-layers', 'feature-detection']",
When to use convolutional layers as opposed to fully connected layers?,"
I am still new to CNNs, but I would like to check my understanding between when to use convolutional layers versus fully connected layers.
From what I have read, we can use convolutional layers with filters, rather than fully connected layers, with images, text, and audio. However, with regular data, for example, the iris dataset, a convolutional layer would not perform well because of the structure. As in the columns can be swapped, yet the record or sample itself does not change. For example we can swap the order of the Petal Length column with Petal Width and the record does not change. Where as in an image or audio, changing the column items would result in a different image or audio file.
These convolutional layers are ""better"" for images and audio because not all the features need to connect to the next layer. For example, we do not need the background of a car image to know it is a car, thus we do not need all the connections and we save computational costs.
Is this the right way to think about when to use convolutional layers versus fully connected layers?
","['convolutional-neural-networks', 'dense-layers', 'convolutional-layers']",
Should the agent play the game until the end or until the winner is found?,"
I'm using the DQN algorithm to train my agent to play a turn-based game. The winner of the game can be known before the game is over. Once the winning condition is satisfied, it cannot be reverted. For example, the game might last 100 turns, but it's possible to know that one of the players won at move 80, because some winning condition was satisfied. The last 20 moves don't change the outcome of the game. If people were playing this game, they, would play it to the very end, but the agent doesn't have to.
The agent will be using memory replay to learn from the experience. I wonder, is it helpful for the agent to have the experiences after the winning condition was satisfied for a more complete picture? Or is it better to terminate the game immediately, and why? How would this affect agent's learning?
","['reinforcement-learning', 'dqn']","You should probably grant reward at the point that the game is logically won. This will help the agent learn more efficiently, by reducing the number of timesteps over which return values need to be backed up.Stopping the episode at that point should also be fine, and may add some efficiency too, in that there will be more focused relevant data in the experience replay. It seems like on the surface that there is no benefit to exploring or discovering any policy after the game is won, and from the comments no expectation from you as agent developer that the agent has any kind of behaviour - random actions would be fine.It is still possible that the agent could learn more from play after a winning state. It would require certain things to be true about the environment and additional work from you as developer.For example, if the game has an end phase where a certain kind of action is more common and it gains something within the game (""victory points"", ""gold"" or some other numbered token that is part of the game mechanics and could be measured), then additional play where this happened could be of interest. Especially if the moves that gained this measure could also be part of winning moves in the earlier game. To allow the agent to learn this though, it would have to be something that it predicted in addition to winning or losing.One way to achieve this is to have a secondary learning system as part of the agent, that learns to predict gains (or totals) of this resource. Such a prediction could either be learned separately (but very similarly to the action value) and fed into the q function as an input, or it could be a neural network that shares early layers with the q function (or policy function) but with a different head. Adding this kind of secondary function to the neural network can also have a regularising effect on the network, because the interim features have to be good for two types of prediction.You definitley do not need to consider such an addition. It could be a lot more work. However, for some games it is possible that it helps. Knowing the game, and understanding whether there is any learning experience to be had as a human player to play on beyond winning or losing, might help you decide whether looking into trying to replicate this additional experience for a bot. Even if it works, the effect may be minimal and not worth the difference it makes. For instance running a more basic learning agent for more episodes may still result in a very good agent for the end game. That only costs you more run time for training, not coding effort."
How do I calculate the return given the discount factor and a sequence of rewards?,"
I know that $G_t = R_{t+1} + G_{t+1}$.
Suppose $\gamma = 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite sequence of $7$s. What is the value of $G_0$?
As it's infinite, how can we deduce the value of $G_0$? I don't see the solution. It's just $G_0 = 5 + 0.9*G_1$. And we don't know $G_1$ value, and we don't know $R_2, R_3, R_4, ...$
","['reinforcement-learning', 'markov-decision-process', 'rewards', 'return', 'discount-factor']",
Atari Games: Pretrained CNN to accelerate training?,"
DQN for Atari takes considerable training time. For example, the 2015 paper in Nature notes that algorithms are trained for 50 million frames or equivalently around 38 days of game experience in total. One reason is that DQN for image data typically uses a CNN, which is costly to train.
However, the main purpose of a CNN is to extract the image features. Note that the policy for DQN is represented by a CNN and an output layer equal to the number of discrete actions. Is it possible to use a pretrained DQN to accelerate the training process by fixing the weights of the underlying pretrained CNN, resetting the weights of the output layer, and then running another (possibly different) DQN algorithm to relearn the weights of the output layer? Both DQN algorithms would be run on the same underlying environment.
","['reinforcement-learning', 'convolutional-neural-networks', 'dqn']",
I need help understanding general back propagation algorithm,"
In section 6.5.6 of the book Deep Learning by Ian et. al. general backpropagation algorithm is described as:

The back-propagation algorithm is very simple. To compute the gradient of some
scalar z with respect to one of its ancestors x in the graph, we begin by observing
that the gradient with respect to z is given by dz = 1. We can then compute dz
the gradient with respect to each parent of z in the graph by multiplying the current gradient by the Jacobian of the operation that produced z. We continue multiplying by Jacobians traveling backwards through the graph in this way until we reach x. For any node that may be reached by going backwards from z through two or more paths, we simply sum the gradients arriving from different paths at that node.

To be specific I don't get this part:

We can then compute dz the gradient with respect to each parent of z in the graph by multiplying the current gradient by the Jacobian of the operation that produced z.

Can anyone help me understand this with some illustration? Thank you.
","['deep-learning', 'backpropagation']",
Why is 100% exploration bad during the learning stage in reinforcement learning?,"
Why can't we during the first 1000 episodes allow our agent to perform only exploration?
This will give a better chance of covering the entire space state. Then, after the number of episodes, we can then decide to exploit.
","['reinforcement-learning', 'exploration-exploitation-tradeoff']","Why can’t we during the first 1000 episodes allow our agent perform only explorationYou can do this. It is fine to do so either to learn the value function of a simple random policy, or when performing off-policy updates. It is quite normal when learning an environment from scratch in a safe way - e.g. in simulation - to collect an initial set of data from behaving completely randomly. The amount of this random data varies, and usually the agent will not switch from fully random to fully deterministic based on calculated values as a single step, but will do so gradually.this will give a better chance of covering the entire space stateThat will depend on the nature of the problem. For really simple problems, it may explore the space sufficiently to learn from. However, for many problems it is just a starting point, and not sufficient to cover parts of the space that are of interest in optimal control.When behaving completely randomly, the agent may take a very long time to complete an episode, and may never complete its first episode. So you could be waiting for a long time to collect data for the first 1000 such episodes. An example of this sort of environment would be a large maze - the agent will move back and forth in the maze, revisiting same parts again and again, where in theory it could already be learning not to repeat its mistakes.In some environments, behaving completely randomly will result in early failure, and never experiencing postive rewards that are available in the environment. An example of this might be a robot learning to balance on a tightrope and get from one end to the other. It would fall off after a few random actions, gaining very little knowledge for 1000 episodes.The state space coverage you are looking for ideally should include the optimal path through the space - at least at some point during learning (not necessarily the start). This does not have to appear in one single perfect episode, because the update rules for value functions in reinforcement learning (RL) will eventually allocate the correct values and find this optimal path in the data. However, the collected data does need to include the information about this optimal path amongst all the alternatives so that the methods in RL can evaluate and select it. In simple environments acting randomly may be enough to gain this data, but becomes highly unlikely when the environments are more complex.then after the number of episodes, we can then decide to exploitAgain this might work for very simple environments, where you have collected enough information through acting randomly to construct a useful value function. However, if acting randomly does not find enough of the optimal path, then the best that exploitation can do is find some local optimum based on the data that was collected.I suggest you experience this difference for yourself: Set up a toy example environment, and use it to compare different approaches for moving between pure exploration and pure exploitation. You will want to run many experiments (probably 100s for each combination, averaged) to smooth out the randomness, and you can plot the results to see how well each approach learns - e.g. how many time steps (count time steps, not episodes, if you are interested in sample efficiency) it takes for the agent to learn, and whether or not it actually finds the correct optimal behaviour. Bear in mind that the specific results will only apply in your selected environment - so you might also want to do this comparison on a small range of environments."
Does importance sampling for off-policy estimation also apply to the case of negative rewards?,"
Importance sampling is a common method for calculating off-policy estimates in RL. I have been reading through some of the original documentation (D.G. Horvitz and D.J. Thompson, Powell, M.J. and Swann, J) and cannot find any restrictions on the reward or value being estimated. However, it seems that there are constraints because the calculation is not what I would expect for RL environments that have negative rewards.
For example, consider for a given action-state pair ($a_i, s_i$), $\pi_e(a|s) = 0.4$ and $\pi_b(a|s) = 0.6,$ where $\pi_b$ and $\pi_e$ are the behavioral and evaluation policies respectively. Also, assume the reward range is $[-1,0]$, and this action has a reward of $r_{\pi_b}=-0.5$.
Under the IS definition, the expected reward under $\pi_b$ would be $r_{\pi_e} = \frac{\pi_b(a|s)}{\pi_e(a|s)} r_{\pi_b}$. In this example, $r_{\pi_e}=-0.75$ thus $r_{\pi_e} < r_{\pi_b}$. However, assuming a change of scale of the reward to be $[0,1]$ which result in $r_{\pi_b}=0.5$, results in $r_{\pi_e} > r_{\pi_b}$.
All examples of IS I have seen in reference focus on positive rewards. However, I find myself wondering if this formulation applies to negative rewards too. If this formulation does allow for negative reward structures, I'm not sure how to interpret this result. I'm wondering how changing the scale of the reward could change the order? Is there any documentation on the requirements of the value in IS? Any insight into this would be greatly appreciated!
","['reinforcement-learning', 'rewards', 'off-policy-methods', 'importance-sampling']",
What is the proof that the variance of the gradient estimate in Actor-Critic is smaller than in REINFORCE?,"
The intuition provided when introducing actor-critic algorithms is that the variance of its gradient estimates is smaller than in REINFORCE as, e.g., discussed here. This intuition makes sense for the reasons outlined in the linked lecture.
Is there a paper/lecture providing a formal proof of that claim for any type of actor-critic algorithm (e.g. the Q Actor-Critic)?
","['reinforcement-learning', 'reference-request', 'proofs', 'actor-critic-methods', 'reinforce']",
Should I build an environment from scratch myself or it is not always needed?,"
I am inspired by the paper Neural Architecture Search with Reinforcement Learning to use reinforcement learning for optimizing a child network (learner). My meta-learner (controller or parent network) is an MLP and will take as the reward function a silhouette score. Its output is a vector of real numbers between 0 and 1. These values are k different possibilities for the number of clusters (the goal is to cluster the result of the child network which is an auto-encoder, embedded images are the input to the meta-learner).
What I am confused about is the environment here and how to implement this network. I was reading this tutorial and the author has used gym library to set the environment.
Should I build an environment from scratch myself or it is not always needed?
I appreciate any help or hints or links to a source that helps me understand better RL concepts. I am new to it and easily gets confused.
","['reinforcement-learning', 'python', 'environment', 'gym']",
How is trajectory sampling different than normal (importance) sampling in reinforcement learning?,"
I am using Sutton and Barto's book for Reinforcement Learning.
In Chapter 8, I am having difficulty in understanding the Trajectory Sampling.
I have read the particular section on trajectory sampling (Sec 8.6) two times (plus 3rd time partially) but still, I do not get how it is different from the normal sampling update, and what are its benefits.
","['reinforcement-learning', 'comparison', 'importance-sampling', 'dyna']",
Implementing SARSA for a 2-stage Markov Decision Process,"
I am a bit confused as to how exactly I should be implementing SARSA (or Q-learning too) on what is a simple 2-stage Markov Decision Task. The structure of the task is as follows:

Basically, there are three states $\{S_1,S_2,S_3\}$ with $S_1$ is in the first stage for which the two possible actions are the two yellow airplanes. $S_2$ and $S_3$ are the possible states for the second stage and the feasible actions are the blue and red background pictures, respectively. There is only a reward at the end of the second stage choice. If I call the two first stage actions $\{a_{11},a_{12}\}$ and the four possible second stage actions $\{a_{21},a_{22},a_{23},a_{24}\}$, from left to right, then a sample trial/episode will look like:
$$S_1, a_{11}, S_2, a_{22},R \quad \text{ or }\quad S_1, a_{11}, S_3, a_{24}, R.$$
In the paper I am reading, where the figure is from, they used a complicated version of TD$(\lambda)$ in which they maintained two action-value functions $Q_1$ and $Q_2$ for each stages. On the other hand, I am trying to implement a simple SARSA update for each episode $t$:
$$Q_{t+1}(s,a)= Q_t(s,a) + \alpha\left(r + \gamma\cdot Q_t(s',a') - Q_t(s,a)\right).$$
In the first-stage, there is no reward so an actual realization will look like:
$$Q_{t+1}(S_1, a_{11}) = Q_t(S_1,a_{11})+\alpha\left( \gamma\cdot Q_t(S_3,a_{23}) - Q_t(S_1,a_{11})\right).$$
I guess my confusion is then how should it look like for the second stage of an episode? That is, if we continue the above realization of the task above, $S_1, a_{11}, S_3, a_{23}, R$, would should fill in the $?$:
$$Q_{t+1}(S_3,a_{23}) = Q_t(S_3,a_{23}) + \alpha\left(R +\gamma\cdot Q_t(\cdot,\cdot)-Q_t(s_3,a_{23}) \right)$$
One on hand, it seems to me that since this is the end of an episode, we assign $0$ to the $Q_t(\cdot,\cdot).$ On the other hand, the nature of this task is that it repeats the same episode over time for a total of $T$, a large number, times we need $Q_t(\cdot,\cdot) = Q_t(S_1,\cdot),$ with the additional action-selection in the first stage there.
I will greatly appreciate if someone can tell me what is the right way to go here.
The link to paper
","['reinforcement-learning', 'q-learning', 'markov-decision-process', 'sarsa']","In this game you can view end of an episode two ways:There is an implied, terminal, fourth state $s_4$ representing the end of the game.You could view the process as a continuous repeating one, where no matter what the choice is made in $s_2$ or $s_3$, the following state is $s_1$.The first, terminating, view is a simpler and entirely natural view since nothing that the agent does in one episode can influence the next. It will result in a Q table that predicts future rewards within a single episode for the current agent (as opposed to discounted view over multiple episodes).You are over-complicating things for yourself by ignoring that a zero reward is still a reward (of $0$). There is no need to remove $R$ from your initial update rule. In many environments there are rewards collected before the end of an episode.In addition, to complete the standard episodic view, you can note that $Q(s_4, \cdot) = 0$ always by definition, hence so does $\text{max}_{a'}[Q(s_4, a'] = 0$. It is common here though to have a branch based on detecting a terminal state, and use a different update rule:$$Q_{t+1}(S_3,a_{23}) = Q_t(S_3,a_{23}) + \alpha\left(R - Q_t(s_3,a_{23}) \right)$$In brief, most implementations of TD algorithms do this:Always assume a reward on each time step, which can be set to $0$Special case for end of episode with a simplified update rule, to avoid needing to store, look up or calculate the $0$ value associated with terminal statesWhen implementing the environment, it is common to have a step function that always returns reward, next state and whether or not it is terminal e.g.Details may vary around this. If you are working with an environment that does not have such a function (many will not have an inherent reward), then it is common to implement a similar function as a convenient wrapper to the environment so that the agent code does not have to include calculations of what the reward should be or whether the state is terminal."
Has reinforcement learning been used to prove mathematical theorems?,"
Coq exists, and there are other similar projects out there. Further, Reinforcement Learning has made splashes in the domain of playing games (a la Deepmind & OpenAI and other less well-known efforts).
It seems to me that these two domains deserve to be married such that machine learning agents try to solve mathematical theorems. Does anyone know of any efforts in this area?
I'm a relative novice in both of these domains, but I'm proficient enough at both to take a stab at building a basic theorem solver myself and trying to make a simple agent have a go at solving some basic number theory problems. When I went to look for prior art in the area I was very surprised to find none. I'm coming here as an attempt to broaden my search space.
","['reinforcement-learning', 'automated-theorem-proving', 'coq']","Artificial Intelligence for Theorem Proving is an active research area as witnessed by the existence of the AITP conference and of many publications on the topic. Some papers are mentioned in this thread: https://coq.discourse.group/t/machine-learning-and-hammers-for-coq/303. I haven't read these papers myself, so I cannot point you to a paper using reinforcement learning specifically, but given the important activity in this domain, I would be very surprised if it hadn't been attempted."
Should illegal moves be excluded from loss calculation in DQN algorithm?,"
I'm implementing DQN algorithm to train my agent to play a turn-based game. The action space for the game is small, but not all moves are available at all the states. Therefore, when deciding on which action to pick, agent sets Q-values to 0 for all the illegal moves while normalizing the values of the rest.
During training, when the agent is calculating the loss between policy and target networks, should the illegal actions be ignored (set to 0) so that they don't affect the calculations?
","['reinforcement-learning', 'dqn', 'deep-rl', 'objective-functions']","I've implemented this exact scenario before; your approach would most likely be successful, but I think it could be simplified.Therefore, when deciding on which action to pick, agent sets Q-values to 0 for all the illegal moves while normalizing the values of the rest.In DQN, the Q-values are used to find the best action. To determine the best action in a given state, it suffices to look at the Q-values of all valid actions and then take the valid action with highest Q-value. Setting Q-values of invalid actions to 0 is unnecessary once you have a list of valid actions. Note that you would need that set of valid actions to set invalid Q-values to 0 in the first place, so the approach I'm suggesting is more concise without worsening the performance.Since the relative order of the Q-values is all that is required to find the best action, there is no need for normalization. Also, the original DQN paper uses $\epsilon$-greedy exploration. Keep in mind to only sample from valid actions in a given state when exploring this way.During training, when the agent is calculating the loss between policy and target networks, should the illegal actions be ignored (set to 0) so that they don't affect the calculations?As noted in one of your previous questions, we train on tuples of experiences $(s, a, r, s')$. The definition of the Q-learning update is as follows (taken from line 6.8 of Sutton and Barto):$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[R_{t+1} + \gamma\max\limits_aQ(S_{t+1}, a) - Q(S_t, A_t)\right].$$The update requires taking a maximum over all valid actions in $s'$. Again, setting invalid Q-values to 0 is unnecessary extra work once you know the set of valid actions. Ignoring invalid actions is equivalent to leaving those actions out of the set of valid actions."
Why do bootstrapping methods produce nonstationary targets more than non-bootstrapping methods?,"
The following quote is taken from the beginning of the chapter on ""Approximate Solution Methods"" (p. 198) in ""Reinforcement Learning"" by Sutton & Barto (2018):

reinforcement learning generally requires function approximation methods able to handle nonstationary target functions (target functions that change over time). In control methods based on GPI (generalized policy iteration) we often seek to learn $q_\pi$ while $\pi$ changes. Even if the policy [pi] remains the same, the target values of training examples are nonstationary if they are generated by bootstrapping methods (DP and TD learning).

Could someone explain why the same is not the case if we use non-bootstrapping methods (such as Monte Carlo that is not allowed infinite rollouts)?
","['reinforcement-learning', 'monte-carlo-methods', 'temporal-difference-methods', 'stationary-policy', 'bootstrapping']",
How does Lateral Inhibition Provide Competition among Neurons?,"
I stumbled upon a paper from P.Diehl and M.Cook with the title ""Unsupervised learning of digit recognition using spike-timing-dependent plasticity"" and I'm trying to understand the logic behind the network connection they made.
The network is as follows. The inputs (size of an image 28x28) are connected to the usual all-to-all fashion with positive weights to an NxN layer of neurons. They are decoded using poisson random distribution in which the frequency of spikes of a pixel is set accordingly to the pixel value. The NxN layer is connected 1 on 1 to an NxN layer of inhibitory neurons. These neurons inhibit all other neurons except the one they are connected to. Thus they are connected all-to-all with an exception.
According to the paper this provides competition among neurons. I cannot understand how, in this particular connection, competition is provided. How can different neurons inherit different properties? To me it seems that all neurons will inherit the same properties, thus no differences in weights will be made in the training session among all neurons. For example, if the input 5 is passed to the network all weights of all neurons will try to adjust according to 5. Then if input 7 is passed next, all the weights will be updated according to the new number (7). It is expected, though, that some weights will keep the previous adjustment ie that some weights will have the properties of 5 and the others the properties of 7.
",['spiking-neural-networks'],
Why is it the case that off-policy evaluation using importance sampling suffers from high variance?,"
The average return for trajectories, $V^{\pi_e}$(s) is often computed via the importance sampling estimate $$V^{\pi_e}(s) = \frac{1}{n}\sum_{i=1}^n\prod_{t=0}^{H}\frac{\pi_e(a_t | s_t)}{\pi_b(a_t|s_t)}G_i$$ where $G_i$is the reward observed for the $i$th trajectory. Sutton and Barton gives an example whereby the variance could be infinite.
In general, however, why does this estimator suffer from high variance? Is it because $\pi_e(a_t|s_t)$ is mainly deterministic and, therefore, the importance weight is $0$ for most trajectories, rendering those sample trajectories useless?
","['reinforcement-learning', 'off-policy-methods', 'importance-sampling']",
q learning appears to converge but does not always win against random tic tac toe player,"
q learning is defined as:

Here is my implementation of q learning of the tic tac toe problem:
import timeit
from operator import attrgetter
import time
import matplotlib.pyplot
import pylab
from collections import Counter
import logging.handlers
import sys
import configparser
import logging.handlers
import unittest
import json, hmac, hashlib, time, requests, base64
from requests.auth import AuthBase
from pandas.io.json import json_normalize
from multiprocessing.dummy import Pool as ThreadPool
import threading
import time
from statistics import mean 
import statistics as st
import os   
from collections import Counter
import matplotlib.pyplot as plt
from sklearn import preprocessing
from datetime import datetime
import datetime
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import matplotlib
import numpy as np
import pandas as pd
from functools import reduce
from ast import literal_eval
import unittest
import math
from datetime import date, timedelta
import random

today = datetime.today()
model_execution_start_time = str(today.year)+""-""+str(today.month)+""-""+str(today.day)+"" ""+str(today.hour)+"":""+str(today.minute)+"":""+str(today.second)

epsilon = .1
discount = .1
step_size = .1
number_episodes = 30000

def epsilon_greedy(epsilon, state, q_table) : 
    
    def get_valid_index(state):
        i = 0
        valid_index = []
        for a in state :          
            if a == '-' :
                valid_index.append(i)
            i = i + 1
        return valid_index
    
    def get_arg_max_sub(values , indices) : 
        return max(list(zip(np.array(values)[indices],indices)),key=lambda item:item[0])[1]
    
    if np.random.rand() < epsilon:
        return random.choice(get_valid_index(state))
    else :
        if state not in q_table : 
            q_table[state] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])
        q_row = q_table[state]
        return get_arg_max_sub(q_row , get_valid_index(state))
    
def make_move(current_player, current_state , action):
    if current_player == 'X':
        return current_state[:action] + 'X' + current_state[action+1:]
    else : 
        return current_state[:action] + 'O' + current_state[action+1:]

q_table = {}
max_steps = 9

def get_other_player(p):
    if p == 'X':
        return 'O'
    else : 
        return 'X'
    
def win_by_diagonal(mark , board):
    return (board[0] == mark and board[4] == mark and board[8] == mark) or (board[2] == mark and board[4] == mark and board[6] == mark)
    
def win_by_vertical(mark , board):
    return (board[0] == mark and board[3] == mark and board[6] == mark) or (board[1] == mark and board[4] == mark and board[7] == mark) or (board[2] == mark and board[5] == mark and board[8]== mark)

def win_by_horizontal(mark , board):
    return (board[0] == mark and board[1] == mark and board[2] == mark) or (board[3] == mark and board[4] == mark and board[5] == mark) or (board[6] == mark and board[7] == mark and board[8] == mark)

def win(mark , board):
    return win_by_diagonal(mark, board) or win_by_vertical(mark, board) or win_by_horizontal(mark, board)

def draw(board):
    return win('X' , list(board)) == False and win('O' , list(board)) == False and (list(board).count('-') == 0)

s = []
rewards = []
def get_reward(state):
    reward = 0
    if win('X' ,list(state)):
        reward = 1
        rewards.append(reward)
    elif draw(state) :
        reward = -1
        rewards.append(reward)
    else :
        reward = 0
        rewards.append(reward)
        
    return reward

def get_done(state):
    return win('X' ,list(state)) or win('O' , list(state)) or draw(list(state)) or (state.count('-') == 0)
    
reward_per_episode = []
            
reward = []
def q_learning():
    for episode in range(0 , number_episodes) :
        t = 0
        state = '---------'

        player = 'X'
        random_player = 'O'


        if episode % 1000 == 0:
            print('in episode:',episode)

        done = False
        episode_reward = 0
            
        while t < max_steps:

            t = t + 1

            action = epsilon_greedy(epsilon , state , q_table)

            done = get_done(state)

            if done == True : 
                break

            if state not in q_table : 
                q_table[state] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])

            next_state = make_move(player , state , action)
            reward = get_reward(next_state)
            episode_reward = episode_reward + reward
            
            done = get_done(next_state)

            if done == True :
                q_table[state][action] = q_table[state][action] + (step_size * (reward - q_table[state][action]))
                break

            next_action = epsilon_greedy(epsilon , next_state , q_table)
            if next_state not in q_table : 
                q_table[next_state] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])

            q_table[state][action] = q_table[state][action] + (step_size * (reward + (discount * np.max(q_table[next_state]) - q_table[state][action])))

            state = next_state

            player = get_other_player(player)
            
        reward_per_episode.append(episode_reward)

q_learning()

The alogrithm player is assigned to 'X' while the other player is 'O':
    player = 'X'
    random_player = 'O'

The reward per episode:
plt.grid()
plt.plot([sum(i) for i in np.array_split(reward_per_episode, 15)])

renders:

Playing the model against an opponent making random moves:
## Computer opponent that makes random moves against trained RL computer opponent
# Random takes move for player marking O position
# RL agent takes move for player marking X position

def draw(board):
    return win('X' , list(board)) == False and win('O' , list(board)) == False and (list(board).count('-') == 0)

x_win = []
o_win = []
draw_games = []
number_games = 50000

c = []
o = []

for ii in range (0 , number_games):
    
    if ii % 10000 == 0 and ii > 0:
        print('In game ',ii)
        print('The number of X game wins' , sum(x_win))
        print('The number of O game wins' , sum(o_win))
        print('The number of drawn games' , sum(draw_games))

    available_moves = [0,1,2,3,4,5,6,7,8]
    current_game_state = '---------'
    
    computer = ''
    random_player = ''
    
    computer = 'X'
    random_player = 'O'

    def draw(board):
        return win('X' , list(board)) == False and win('O' , list(board)) == False and (list(board).count('-') == 0)
        
    number_moves = 0
    
    for i in range(0 , 5):

        randomer_move = random.choice(available_moves)
        number_moves = number_moves + 1
        current_game_state = current_game_state[:randomer_move] + random_player + current_game_state[randomer_move+1:]
        available_moves.remove(randomer_move)

        if number_moves == 9 : 
            draw_games.append(1)
            break
        if win('O' , list(current_game_state)) == True:
            o_win.append(1)
            break
        elif win('X' , list(current_game_state)) == True:
            x_win.append(1)
            break
        elif draw(current_game_state) == True:
            draw_games.append(1)
            break
            
        computer_move_pos = epsilon_greedy(-1, current_game_state, q_table)
        number_moves = number_moves + 1
        current_game_state = current_game_state[:computer_move_pos] + computer + current_game_state[computer_move_pos+1:]
        available_moves.remove(computer_move_pos)
     
        if number_moves == 9 : 
            draw_games.append(1)
#             print(current_game_state)
            break
            
        if win('O' , list(current_game_state)) == True:
            o_win.append(1)
            break
        elif win('X' , list(current_game_state)) == True:
            x_win.append(1)
            break
        elif draw(current_game_state) == True:
            draw_games.append(1)
            break

outputs:
In game  10000
The number of X game wins 4429
The number of O game wins 3006
The number of drawn games 2565
In game  20000
The number of X game wins 8862
The number of O game wins 5974
The number of drawn games 5164
In game  30000
The number of X game wins 13268
The number of O game wins 8984
The number of drawn games 7748
In game  40000
The number of X game wins 17681
The number of O game wins 12000
The number of drawn games 10319

The reward per episode graph suggests the algorithm has converged? If the model has converged shouldnt the number of O game wins be zero ?
","['reinforcement-learning', 'python', 'q-learning', 'game-ai', 'combinatorial-games']","The primary issue I see is that in the loop through time steps t in every training episode, you select actions for both players (who should have opposing goals to each other), but update a single q_table (which can only ever be correct for the ""perspective"" of one of your two players) on both of those actions, and updating both of them using a single, shared reward function.Intuitively, I guess this means that your learning algorithm assumes that your opponent will always be helping you win, rather than assuming that your opponent plays optimally towards its own goals. You can see that this is likely indeed the case from your plot; you use $30,000$ training episodes, split up into $15$ chunks of $2,000$ episodes per chunk for your plot. In your plot, you also very quickly reach a score of about $1,950$ per chunk, which is almost the maximum possible! Now, I'm not 100% sure what the winrate of an optimal player against random would be, but I think it's likely that that should be lower than 1950 out of 2000. Random players will occasionally achieve draws in Tic-Tac-Toe, especially taking into consideration that your learning agent itself is also not playing optimally (but $\epsilon$-greedily)!You should instead pick one of the following solutions (maybe there are more solutions, this is just what I come up with on the spot):After looking into the above suggestions, you'll probably also want to look into making sure that your agent experiences games in which it starts as Player 1, as well as games in which it starts as Player 2, and trains for both of those possible scenarios and learns how to handle both of them. In your evaluation code (after training), I believe that you always make the Random opponent play first, and the trained agent play second? If you don't cover this scenario in your training episodes, your agent may not learn how to properly handle it.Finally, a couple of small notes:"
Why isn't it wise for us to completely erase our old Q value and replace it with the calculated Q value?,"
Why isn't it wise for us to completely erase our old Q value and replace it with the calculated Q value? Why can't we forget the learning rate and temporal difference?
Here's the update formula.

","['reinforcement-learning', 'q-learning', 'temporal-difference-methods', 'value-functions']","Removing the learning rate will likely yield poor convergence to the optimal policy and optimal Q-values. Note that the current policy is completely dependent on the Q-values, as we take the action with highest Q-value in a given state (with a few other considerations such as exploration, etc.). If we were to remove the learning rate, then we are making a relatively large change to our Q-values and possibly to our policy as well after only a single update. For example, if the sample rewards have great variance (e.g. in stochastic environments), then drastic updates to a single Q-value may occur simply by chance when a learning rate is not used. Due to the recursive definition of Q-values, a few poor updates can undo the work of many previous updates. If this phenomenon were to occur frequently, then the policy may take a long time to converge to the optimal policy, if at all.Underlying the temporal-difference update and many other reinforcement learning updates is the notion of policy iteration in which the estimated value function is updated to match the true value function of the current policy and the current policy is updated to be greedy with respect to the estimated value function. This process proceeds iteratively and gradually until convergence to the optimal policy and optimal value function is achieved. Gradual changes such as setting a small learning rate (e.g. $\alpha = 0.1$) aim to speed up convergence by lessening the frequency of the phenomenon in the above paragraph. Sutton and Barto make comments on convergence throughout their book, with the remarks surrounding line 2.7 in Section 2.5 providing a summary."
Algorithm to train a neural network against differentiable and non-differentiable databases?,"
Let's say I have two databases, $(\mathbf{x_i}, \mathbf{\hat{p_i}})$ and $(\mathbf{x_j}, \mathbf{\hat{q_j}})$. A neural network with weights $\theta$ can receive an input $\mathbf{x}$ and produce an output $\mathbf{y}$. Mathematically, $\mathbf{y} = f_{NN}(\mathbf{x},\theta)$. To compare the output of my neural network and the database, I need two wrappers, $\mathbf{p}=g(\mathbf{y})$ and $\mathbf{q}=h(\mathbf{y})$.
The problem is: only $g(\cdot)$ is differentiable while writing $h(\cdot)$ in a differentiable manner would take a huge effort.
Is there any efficient way to train my neural network to minimize the following loss function?
$$
\mathcal{L}(\theta) = \sum_i \left\{g\left[f_{NN}(\mathbf{x_i}, \theta)\right] - \mathbf{\hat{p_i}}\right\}^2 + \sum_j \left\{h\left[f_{NN}(\mathbf{x_j}, \theta)\right] - \mathbf{\hat{q_j}}\right\}^2
$$
My thinking
If I am using gradient descent-type algorithm, I can only optimize the first part of the loss function while ignoring the second part.
If I am using evolutionary-type algorithm, I can optimize both parts, but it will take a long time and I don't make a full use of the differentiable property of $g(\cdot)$.
","['neural-networks', 'training', 'algorithm']",
Why is it not advisable to have a 100 percent exploration rate? [duplicate],"







This question already has an answer here:
                                
                            




Why is 100% exploration bad during the learning stage in reinforcement learning?

                                (1 answer)
                            

Closed 3 years ago.



During the learning phase, why don't we have a 100% exploration rate, to allow our agent to fully explore our environment and update the Q values, then during testing we bring in exploitation? Does that make more sense than decaying the exploration rate?
","['reinforcement-learning', 'q-learning', 'exploration-exploitation-tradeoff']",
Why do we update the weights of the target network in deep Q learning?,"
I know we keep the target network constant during training to improve stability, but why exactly are we updating the weights of our target network? In particular, if we've already reached convergence, why exactly are we updating the weights of our target network?
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']","If you are certain that you reached convergence then there is no point in continuing to train your agent, because of that there is also no point in discussing why is target network being updated after convergence is reached. You should simply stop training if you converged. During training we obviously need to keep updating target network to improve correctness of Q-value estimates."
What is the difference between distant supervision and self-supervision?,"
Weak supervision is supervised learning, with uncertainty in the labeling, e.g. due to automatic labeling or because non-experts labelled the data [1].
Distant supervision [2, 3] is a type of weak supervision that uses an auxiliary automatic mechanism to produce weak labels / reference output (in contrast to non-expert human labelers).
According to this answer

Self-supervised learning (or self-supervision) is a supervised learning technique where the training data is automatically labelled.

In the examples for self-supervised learning, I have seen so far, the labels were extracted from the input data.
What is the difference between distant supervision and self-supervision?

Is it that for self-supervision, the labels must come from the input data and for distant supervision it can come from anywhere (which would make self-supervision a type of distant supervision)?
Or must the labels from distant supervision come from somewhere else than the input data?
If ""In robotics, this can be done by finding and exploiting the relations or correlations between inputs coming from different sensor modalities."" then for self-supervised learning, the labels do not even have to originate from the input data. (Or did I misinterpret the quote?)


(Setup mentioned in discussion:

","['comparison', 'supervised-learning', 'self-supervised-learning']",
Does self-supervised learning require auxiliary tasks?,"
Self-supervised learning algorithms provide labels automatically. But, it is not clear what else is required for an algorithm to fall under the category ""self-supervised"":
Some say, self-supervised learning algorithms learn on a set of auxiliary tasks [1], also named pretext task [2, 3], instead of the task we are interested in. Further examples are word2vec or autoencoders [4] or word2vec [5]. Here it is sometimes mentioned that the goal is to ""expose the inner structure of the data"".
Others do not mention that, implying that some algorithms can be called to be ""self-supervised learning algorithms"" if they are directly learning the task we are interested in [6, 7].
Is the ""auxiliary tasks"" a requirement for a training setup to be called ""self-supervised learning"" or is it just optional?

Research articles mentioning the auxiliary / pretext task:

Revisiting Self-Supervised Visual Representation Learning, 2019, mentioned by [3]:


The self-supervised learning framework requires only unlabeled data in order to formulate a pretext learning task such as predicting context or image rotation, for which a target objective can be computed without supervision.


Unsupervised Representation Learning by Predicting Image Rotations,  ICLR, 2018, mentioned by
[2]:


a prominent paradigm is the so-called self-supervised learning that defines an annotation free pretext task, using only the visual information present on the images or videos, in order to provide a surrogatesupervision signal for feature learning.


Unsupervised Visual Representation Learning by Context Prediction, 2016, mentioned by
[2]:


This converts an apparently unsupervised problem (finding a good similarity metric between words) intoa “self-supervised” one:  learning a function from a givenword to the words surrounding it.  Here the context predic-tion  task  is  just  a  “pretext”  to  force  the  model  to  learn  agood word embedding, which, in turn, has been shown tobe useful in a number of real tasks, such as semantic wordsimilarity.


Scaling and Benchmarking Self-Supervised Visual Representation Learning, 2019:


In discriminative self-supervised learning, which is the main focus of this work, a model is trained on an auxiliary or ‘pretext’ task for which ground-truth is available for free. In most cases, the pretext task involves predicting some hidden portion of the data (for example, predicting color for gray-scale images

","['definitions', 'transfer-learning', 'representation-learning', 'self-supervised-learning', 'pretext-tasks']",
"Does ""transition model"" alone in an MDP imply it's non-deterministic?","
I am looking at a lecture on POMDP, and the context is that, when the quadcopter can't see the landmarks, it has to use reckoning. And then he mentions the transition model is not deterministic, hence the uncertainty grows.
Can transition models in MDP be deterministic?
","['reinforcement-learning', 'markov-decision-process', 'pomdp', 'transition-model']","Yes, in which case, it will be more like a search problem, if it is not POMDP and have finite number of states. Or you can use the same framework (used for POMDP) with constrained (deterministic) transition matrix modeling for model-based systems.If you think about it after you train any model/agent with an MDP modeling, during test time the optimal strategy is generally deterministic, i.e., given a feature/state you will take a particular action even if it has more than one non-zero element in each row of the transition matrix."
"How will the input be preserved as we go deeper in CNN, where dimensions decrease drastically?","
Our length of feature representation decreases as we go deeper into the CNN, I mean to say that horizontal and vertical lengths decrease while depth(channels) increase. So, how will the input be preserved, since there won't be any data left at the end of the network, where we connect, to say Multi Layer Perceptrons?
","['deep-learning', 'convolutional-neural-networks', 'convolution']","You can also think of a convolutional neural network (CNN) as an encoder, i.e. a neural network that learns a smaller representation of the input, which then acts as the feature vector (input) to a fully connected network (or another neural network). In fact, there are CNNs that can be thought of as auto-encoders (i.e. an encoder followed by a decoder): for example, the u-net can indeed be thought of as an auto-encoder.Although it is (almost) never the case that you transform the input to an extremely small feature vector (e.g. a number), even a single float-pointing number can encode a lot of information. For example, if you want to classify the object in the image into one of two classes (assuming there is only one main object in the image), then a floating-point is more than sufficient (in fact, you just need one bit to encode that information).This smaller representation (the feature vector) that is then fed to a fully connected network is learned based on the information in your given training data. In fact, CNNs are known as data-driven feature extractors.I am not aware of any theoretical guarantee that ensures that the learned representation is the best suited for your task (probably you need to look into learning theory to know more about this). In practice, the quality of the learned feature vector will mainly depend on your available data and the inductive bias (i.e. the assumptions that you make, which are also affected by the specific neural network architecture that you choose)."
How to deal with the addition of a new state to the environment during training?,"
Let's say we have a dynamic environment: a new state gets added after 2000 episodes have been done. So, we leave room for exploration, so that it can discover the new state.
When it gets to that new state, it has no idea of the Q values, and, since we're past 2000 episodes, our exploration rate is very low. What happens if try to exploit when all Q values are 0?
","['reinforcement-learning', 'environment', 'exploration-exploitation-tradeoff']",
How to design a graph neural network to predict the forces in truss elements of a space frame?,"
I am trying to create a Graph NN that will be able to predict the forces in truss elements of a space frame.
The input for the NN will be a graph, where the nodes represent the nodes of the spaceframe. And the output should be the forces in the edges of the supplied graph/frame.
The problem that I am facing is that for the NN to be beneficial I need to encode a lot of data per node:

3 floats for the position of the node,
3 floats for the vector of the force applied to the node,
a boolean/int to determine whether a node is a support.

I am not sure how to design my Graph NN to allow for so many parameters per node.
Maybe I should try a different approach?
Any help is greatly appreciated!
","['ai-design', 'geometric-deep-learning', 'graph-neural-networks']",
Why is non-linearity desirable in a neural network?,"
Why is non-linearity desirable in a neural network?
I couldn't find satisfactory answers to this question on the web. I typically get answers like ""real-world problems require non-linear solutions, which are not trivial. So, we use non-linear activation functions for non-linearity"".
","['neural-networks', 'deep-learning', 'activation-functions']","Consider what happens if you intend to train a linear classifier on replicating something trivial as the XOR function. If you program/train the classifier (of arbitrary size) such that it outputs XOR condition is met whenever feature a or feature b are present, then the linear classifier will also (incorrectly) output XOR condition is met whenever both features together are present. That is because linear classifiers simply sum up contributions of all features and work with the total weighted inputs they receive. For our example, that means that when the weighted contribution of either feature is sufficient already to trigger the classifier to output XOR condition is met, then obviously also the summed contributions of both features are sufficient to trigger the same response.To get a classifier that is capable of outputting XOR condition is met if and only if the summed contributions of all input features are above a lower threshold and below an upper threshold, commonly non-linearities are introduced. You could of course also try to employ a quadratic function to solve the two-feature problem, but as soon as the number of variables/features increases again, you run into the same problem again, only in higher dimensions. Therefore, the most general approach to solving this problem of learning non-linear functions, like the XOR, is by setting up large models with enough capacity to learn a given task and equipping them with non-linearities. That simplifies training since it allows for using stochastic gradient descent for training the system/classifier, preventing one from having to solve Higher-Degree polynomial equations analytically (which can get computationally quite expensive quite quickly) to solve some task.In case you are interested, here's one paper analyzing and dealing with the XOR problem (as one concrete instance of a problem where purely linear models fail to solve some task).EDIT:You can consider a layer in a network as a function $y = f(x)$, where $x$ is the input to some layer $f$ and $y$ is the output of the layer. As you propagate $x$, being the network's input, through the network, you get something like $y = p(t(h(g(f(x)))))$, where $f$ is the input layer and $p$ constitutes the output layer, i.e. a set of weights, by which the input to that respective layer gets multiplied.
If $h$, for example, is some non-linear activation function, like ReLU or sigmoid, then $y$, being the network's output, is a non-linear function of input $x$."
Do transformers have success in other domains different than NLP?,"
Everybody knows how successful transformers have been in NLP. Is there known work on other domains (e.g that also have a sequential natural way of occurring, such as stock price prediction or other problems)?
","['deep-learning', 'natural-language-processing', 'applications', 'sequence-modeling', 'transformer']","When it talks to other domains such as image or music, using transformer will always face a problem of sequence length limitation. To the best of my knowledge, the bottleneck of self-attention which uses a $n^2$ matrix quite limits transformer being applied to other domains. For example, a 32x32 pixel image, means a sequence of 1024 tokens.OpenAI did some related research, as the followings.Generative Modeling with Sparse Transformers:
In the paper, transformers with sparse attention are applied to image and waveform.ImageGPT:
A large transformer model trained on language can generate coherent text, the same exact model trained on pixel sequences can generate coherent image completions and samples. (Abstract from the blog)"
Artificial life simulator that is fully embodied and passes open endedness tests,"
Geb is an alife simulation that as far as I know passes all of the tests we have tried to come up with in defining open endedness. However, when you actually run the code, the behavioral complexity certainly increases, but the physical bodies of the creatures never changes (and cannot change), and Geb bodies are the only thing present in the world.
Tool development, or at least developing new physical capabilities and new “actions” seems to be a crucial part of what makes evolution open ended. I think the problem with Geb is that the evolution and progress all takes place in the networks and network production rules, which are systems outside the physical world. They are external systems that take in data from the world and output actions for the agents. So while this rich complexity and innovation is occurring, it’s not integrated with the agents actions and physical bodies.
This leads to a simple question: is there an alife system that passes all the same tests Geb does, but is “fully embedded” in its world? In the sense that any mechanism agents use to make actions must be part of the physical world of those agents, and subject to the same rules as the bodies of the agents themselves.
What I’m saying here is loose, you could come up with plenty of weird edge cases that meet what I ask exactly without meeting my intent. And perhaps being fully embodied isn’t necessary, just being more embodied would be enough. What my intent is is to ask if we have any systems that pass the open endedness tests Geb have passed, but have the innovation occurring in a way that leads to emergent growth of “actions” and emergent growth of “bodies” because to evolve and do better those aspects must be improved as well.
","['evolutionary-algorithms', 'artificial-life']",
What is the advantage of having a stochastic classification procedure?,"
What is the advantage of having a stochastic/probabilistic classification procedure?
The classifiers I have encountered so far are as follows. Suppose we have two outcomes $A = \{0,1\}$. Given a feature vector $x$, we have calculated a probability for each outcome and return the outcome $a \in A$ for which the probability is highest.
Now, I encountered a classification procedure as follows: first, map each $x$ to a probability distribution on $A$ by a mapping $H$. To classify $x$, choose an outcome $a$ according to the distribution $H(x)$.
Why not use the deterministic classification? Suppose $H(x)$ is 1 with probability $0.75$. Then, the obvious choice for an outcome would be $1$ and not $0$.
","['machine-learning', 'classification']","There are multiple potential reasons for having stochastic predictions (instead of categorical/binary).First, it often simplifies training and improves the training outcome when training a classifier on producing probabilities per class. For example, it allows for the usage of many nice loss functions like the Mean Squared Error (MSE), which is compatible with the famous back-prop algorithm. Moreover, improving classification based on the loss computed from stochastic probabilities per class allows for improving the classifier even further when the counting loss computed on predicted class labels has reached 0 already. So, if you only have binary decisions and your classifier gets all classes correct, training can stop immediately since the loss becomes 0. However, when predicting probabilities per class and iteratively driving the probability of the correct class towards 1, training can continue for much longer, even after all classes have been classified correctly already. So, the classifier trained on making probabilistic predictions can in the end perform much better since training can progress for much longer, allowing for increasingly pronounced discrimination between classes with every update. A nice introduction is given in this Stanford lecture recording.Second, it is often nice for a user to know how much certainty versus uncertainty there is in a classification. If one class has 80% probability, this is a pretty clear decision in favor of the 80%-probability-class. However, if you only get categorical class labels returned by your classifier, you cannot possibly know whether the evidence in favor of the winning class was only marginally higher than that of the other class (as measured by the classifier) or whether there was a clear difference."
Is there a good website where I can learn about Deep Deterministic Policy Gradient?,"
Is there a good website where I can learn about Deep Deterministic Policy Gradient?
","['reinforcement-learning', 'resource-request', 'ddpg']",
Why do we explore after we have an accurate estimate of the value function?,"
Suppose we have a small space state and that, after about 2000 episodes, we've accurately explored the environment and known the accurate $Q$ values. In that case, why do we still leave a small probability for exploration?
My guess is in the case of a dynamic environment where a bigger reward might pop up in another state. Is my assumption correct?
","['reinforcement-learning', 'q-learning', 'exploration-exploitation-tradeoff']","Suppose we have a small space state and that, after about 2000 episodes, we've accurately explored the environment and known the accurate $Q$ values. In that case, why do we still leave a small probability for exploration?It will depend on the goal of the work:If the learning algorithm is off-policy (e.g. Q learning), it is normal to continue to explore at a moderate-to-low rate because it can accurately estimate an optimal deterministic target policy from a close-to-optimal stochastic behaviour policy.Perhaps it is engineered with a low tolerance and will keep going even when you don't need it to.Perhaps the code is for education and run so long that convergence is easily visible. Or for comparison with other methods which really do take that long to converge, and you would like data on the same axis.For comparison with other methods for sample efficiency whilst learning and measuring regret (i.e. how much the exploration is costing you).When environment is dynamic and could change, then continuous exploration is potentially useful to discover the changes, as you suggest in the question.If you do really have an ideal agent, then of course you could just stop and say ""job done"". In practice for more interesting problems, you won't usually get small state spaces and perfect solutions inside 2000 episodes (or ever) - as a result if you are reading tutorials in reinforcement learning, they may just skip this point."
How much data do we need for making a successful de-noising auto-encoder?,"
Is there a guide how much data do you need for making successful denoising model using autoencoders?
Or the rule is, the more data, the better it is?
I tried with small dataset 350 samples,  to see what I will get as an output. And I failed. :D
","['convolutional-neural-networks', 'keras', 'autoencoders', 'sample-complexity', 'denoising-autoencoder']",
"Intuitively, why can the training of a neural network be formulated as a probability estimation problem?","
Neural network training problems are oftentimes formulated as probability estimation problems (such as autoregressive models).
How does one intuitively understand this idea?
","['neural-networks', 'comparison', 'training', 'probability-distribution']","Consider the case of binary classification, i.e. you want to classify each input $x$ into one of two classes: $y_1$ or $y_2$. For example, in the context of object classification, $y_1$ could be ""cat"" and $y_2$ could be ""dog"", and $x$ is an image that contains one main object.In certain cases, $x$ cannot be easily classified. For example, in object classification, if $x$ is a blurred image where there's some uncertainty about the object in the image, what should the output of the neural network be? Should it be $y_1$, $y_2$, or maybe it should be an uncertainty value (i.e. a probability) that lies between $y_1$ and $y_2$? The last option is probably the most reasonable, but also the most general one (in the sense that it can also be used in the case there's little or no uncertainty about what the object is).That's the reason why we can model or formulate this (or other) supervised learning problem(s) as the estimation of a probability value (or probability distribution).To be more concrete, you can formulate this binary classification problem as the estimation of the following probability\begin{align}
P(y_1 \mid x, \theta_i) \in [0, 1] \label{1}\tag{1}
\end{align}where $y_1$ is the first class (or label), $(x, y) \in \mathcal{D}$ is a labeled training example, where $y$ is the ground-truth label for the input $x$, $\theta_i$ are the parameters of the neural network at iteration $i$, so, intuitively, $P(y_1 \mid x, \theta_i) $ is a probability that represents how likely the neural network thinks that $x$ belongs to the class $y_1$ given the current estimate of the parameters. The probability that $x$ belongs to the other class is just $1 - P(y_1 \mid x, \theta_i) = P(y_2 \mid x, \theta_i)$.  In this specific case, I have added a subscript to $\theta$ to indicate that this probability depends on the $i$th estimate of the parameters of the neural network.Once you have $P(y_1 \mid x, \theta_i)$, if you want to perform classification, you will actually need to choose a threshold value $t$, such that, if $P(y_1 \mid x, \theta_i) > t$, then $x$ is classified as $y_1$, else it is classified as $y_2$. This threshold value $t$ can be $0.5$, but it can also not be.Note that, in the case above, $P(y_1 \mid x, \theta_i)$ is a number and not a probability distribution. However, in certain cases, you can also formulate your supervised learning problem so that the output is a probability distribution (rather just a probability). There are also other problems where you don't estimate a conditional probability but maybe a joint probability, but the case above is probably the simplest one that should give you the intuition behind the idea of formulating machine learning problems as the estimation of probabilities or probability distributions."
How is the performance of a CNN trained with monochrome images on image recognition tasks degraded?,"
For CNN image recognition tasks, like object recognition/face recognition/object segmentation/posture recognition, are there experiment results about how much will the performance be degraded with monochrome images?
The imaginary experiment is like:

Take the existing frameworks, reduce the channel number in the framework to fit monochrome images

Transform the existing training data and testing data to monochrome images

Train the model with monochrome training data.

Use the model to test the monochrome testing data.

Compare the result with the original result.


","['convolutional-neural-networks', 'datasets', 'reference-request', 'data-preprocessing', 'performance']",
How to handle the final state in experience replay?,"
I'm using the DQN algorithm to train my agent to play a turn-based game. The memory replay buffer stores tuples of experiences $(s, a, r, s')$, where $s$ and $s'$ are consecutive states. At the last turn, the game ends, and the non-zero reward is given to the agent. There are no more observations to be made and there is no next state $s'$ to store in the experience tuple. How should the final states be handled?
","['reinforcement-learning', 'dqn', 'experience-replay']","You do not store a terminal state as $s$ in the replay table because by definition its value is always $0$, and there is no action, reward or next state. There is literally nothing to learn.However, you may find it useful to store information that $s'$ is actually a terminal state, in case this is not obvious. That is typically achieved by storing an additional done boolean component. This is useful, because it allows you to branch when calculating the TD target g:"
Which heuristics guarantee the optimality of A*?,"
The following is a statement and I am trying to figure out if it's true or false and why.

Given a non-admissible heuristic function, A* will always give a solution if one exists, but there is no guarantee it will be optimal.

I know that a non-admissible function is $h(n) > h^*(n)$ (where $h^*(n)$ is the real cost to the goal), but I do not know if there is a guarantee.
Which heuristics guarantee the optimality of A*? Is the admissibility of the heuristic always a necessary condition for A* to produce an optimal solution?
","['search', 'heuristics', 'a-star', 'admissible-heuristic']","Given a non-admissible heuristic function, A* will always give a solution if one exists, but there is no guarantee it will be optimal.I won't duplicate the proof here, but it isn't too hard to prove that any best-first search will find a solution for any measure of best, given that a path to the solution exists and infinite memory. A* is a best-first search algorithm, so it will always find a solution if one exists.Which heuristics guarantee the optimality of A*? Is the admissibility of the heuristic always a necessary condition for A* to produce an optimal solution?Admissibility is not a necessary condition. Take any admissible heuristic $h_1$ and make a new function $h(n) = h_1(n)+5$. This heuristic is not admissible, but if you run A* on it, it will still find optimal solutions.But, we also have to ask what you mean by ""the optimality of A*"", because optimality can have two senses here. My point in the previous paragraph is in the sense of returning optimal paths. An alternate interpretation is that no algorithm performs fewer expansions that A* with the same information. This is probably not what was meant, and the answer in that context is far more complicated. But, with an inconsistent (but admissible) heuristic, A* can perform exponentially more expansions than other known algorithms, and thus is not the optimal algorithm to use."
Is some kind of dropout used in the human brain?,"
I've read that ANNs are based on how the human brain works. Now, I am reading about dropout.
Is some kind of dropout used in the human brain? Can we say that the ability to forget is some kind of dropout?
","['machine-learning', 'dropout', 'human-inspired', 'neuroscience']",
Is there an algorithm to calculate the weights of an ontology tree's inner nodes?,"
I have a tree that represents a hierarchical ontology of computer science topics (such as AI, data mining, IR, etc). Each node is a topic, and its child nodes are its sub-topics. Leaf nodes are weighted based on their occurrence in a given document.
Is there a well-known algorithm or function to calculate the weight of inner nodes based on the weights of leaf nodes? Or is it totally based on the application to decide mathematical calculation of the weights?
In my application, the node's weights should be some sort of accumulation of its child nodes weights. Is there a better mathematical formula or function to do that than just summing up weights of child nodes?
I am not asking about traversal, but rather about weighting function.
",['ontology'],
What happens when you select actions using softmax instead of epsilon greedy in DQN?,"
I understand the two major branches of RL are Q-Learning and Policy Gradient methods.
From my understanding (correct me if I'm wrong), policy gradient methods have an inherent exploration built-in as it selects actions using a probability distribution.
On the other hand, DQN explores using the $\epsilon$-greedy policy. Either selecting the best action or a random action.
What if we use a softmax function to select the next action in DQN? Does that provide better exploration and policy convergence?
","['reinforcement-learning', 'dqn', 'policy-gradients', 'epsilon-greedy-policy', 'softmax-policy']","DQN on the other hand, explores using epsilon greedy exploration. Either selecting the best action or a random action.This is a very common choice, because it is simple to implement and quite robust. However, it is not a requirement of DQN. You can use other action choice mechanisms, provided all choices are covered with a non-zero probability of being selected.What if we use a softmax function to select the next action in DQN? Does that provide better exploration and policy convergence?It might in some circumstances. A key benefit is that it will tend to focus on action choices that are close to its current best guess at optimal. One problem is that if there is a large enough error in Q value estimates, it can get stuck as the exploration could heavily favour a current best value estimate. For instance, if one estimate is accurate and relatively high, but another estimate is much lower but in reality would be a good action choice, then the softmax probabilities to resample the bad estimate will be very low and it could take a very long time to fix.A more major problem is that the Q values are not independent logits that define preferences (whilst they would be in a Policy Gradient approach). The Q values have an inherent meaning and scale based on summed rewards. Which means that differences between optimal and non-optimal Q value estimates could be at any scale, maybe just 0.1 difference in value, or maybe 100 or more. This makes plain softmax a poor choice - it might suggest a near random exploration policy in one problem, and a near determinitsic policy in another, irrespective of what exploration might be useful at the current stage of learning.A fix for this is to use Gibbs/Boltzmann action selection, which modifies softmax by adding a scaling factor - often called temperature and noted as $T$ - to adjust the relative scale between action choices:$$\pi(a|s) = \frac{e^{q(s,a)/T}}{\sum_{x \in \mathcal{A}} e^{q(s,x)/T}}$$This can work nicely to focus later exploration towards refining differences between actions that are likely to be good whilst only rarely making obvious mistakes. However it comes at a cost - you have to decide starting $T$, the rate to decay $T$ and an end value of $T$. A rough idea of min/max action value that the agent is likely to estimate can help."
What is the bias-variance trade-off in reinforcement learning?,"
I am watching DeepMind's video lecture series on reinforcement learning, and when I was watching the video of model-free RL, the instructor said the Monte Carlo methods have less bias than temporal-difference methods. I understood the reasoning behind that, but I wanted to know what one means when they refer to bias-variance tradeoff in RL.
Is bias-variance trade-off used in the same way as in machine learning or deep learning?
(I am just a beginner and have just started learning RL, so I apologize if it is a silly question.)
","['reinforcement-learning', 'deep-rl', 'monte-carlo-methods', 'temporal-difference-methods', 'bias-variance-tradeoff']","The bias-variance trade-off that you're referring to has to do with the return estimator. Any RL algorithm you choose needs some estimate of the cumulative return, which is a random variable with many sources of randomness, such as stochastic transitions or rewards.Monte Carlo RL algorithms estimate returns by running full trajectories and literally averaging the return achieved for each state. This imposes very few assumptions on the system (in fact, you don't even need the Markovian property for these methods), so bias is low. However, variance is high since each estimate depends on the literal trajectories that you observe. As such, you'll need many, many trajectories to get a good estimate of the value function.On the other hand, with TD methods, you estimate returns as $R_t + \gamma V(S_{t+1})$, where $V$ is your estimate of the value function. Using $V$ this imposes some bias (for instance, the initialization of the value function at the beginning of training affects your next value function estimates), with the benefit of reducing variance. In TD learning, you don't need full environment rollouts to make a return estimate, you just need one transition. This also lets you make much better use of what you've learned about the value function, because you're learning how to infer value ""piecewise"" rather than just via literal trajectories that you happened to witness."
How can I have the same input and output shape in an auto-encoder?,"
I'm building a denoising autoencoder. I want to have the same input and output shape image.
This is my architecture:
input_img = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1))  

x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
x = MaxPooling2D((2, 2), padding='same')(x)
x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
encoded = MaxPooling2D((2, 2), padding='same')(x)



x = Conv2D(32, (3, 3), activation='relu', padding='valid')(encoded)
x = UpSampling2D((2, 2))(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)
decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)


# decodedSize = K.int_shape(decoded)[1:]

# x_size = K.int_shape(input_img)
# decoded = Reshape(decodedSize, input_shape=decodedSize)(decoded)


autoencoder = Model(input_img, decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')

My input shape is: 1169x827
This is Keras output:
Model: ""model_6""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         [(None, 1169, 827, 1)]    0         
_________________________________________________________________
conv2d_30 (Conv2D)           (None, 1169, 827, 32)     320       
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 585, 414, 32)      0         
_________________________________________________________________
conv2d_31 (Conv2D)           (None, 585, 414, 64)      18496     
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 293, 207, 64)      0         
_________________________________________________________________
conv2d_32 (Conv2D)           (None, 291, 205, 32)      18464     
_________________________________________________________________
up_sampling2d_12 (UpSampling (None, 582, 410, 32)      0         
_________________________________________________________________
conv2d_33 (Conv2D)           (None, 582, 410, 32)      9248      
_________________________________________________________________
up_sampling2d_13 (UpSampling (None, 1164, 820, 32)     0         
_________________________________________________________________
conv2d_34 (Conv2D)           (None, 1162, 818, 1)      289       
===============================================================

How can I have the same input and output shape?
","['convolutional-neural-networks', 'tensorflow', 'keras', 'autoencoders']",
"What is meant by gene, chromosome, population in genetic algorithm in terms of feature selection?","
I am trying to understand the genetic algorithm in terms of feature selection and these features are extracted using a machine learning algorithm.
Let's suppose I have data of heart rate for 3 minutes collected from $50$ subjects. From these 3-minute heart rate, I extracted $5$ features, like the mean, standard deviation, variance, skewness and kurtosis. Now the shape of my feature set is (50, 5).
I want to know what are gene, chromosome and population in genetic algorithm related to the above scenario.
What I understand is each feature is a gene, and a set of all features for one subject (1, 5) is the chromosome, and the whole feature set (50, 5) is a population. But I think this concept is not correct. Because in the genetic algorithm, we take a random population, but according to my concept complete data is population, so how random data is selected.
Can anyone help me to understand it?

","['ai-design', 'genetic-algorithms', 'evolutionary-algorithms']",
What is meant by the number of channels of a network?,"
Currently, I am reading Rethinking Model Scaling for Convolutional Neural Networks. The authors are talking about a different way of scaling convolutional neural networks by scaling all dimensions simultaneously and relative to each dimension. I understand the scaling methods regarding the depth of a network (# layers) and the resolution (size of the input image).
What I was stumbling is the concept of the network's width (# channels). What is meant by the width or the number of channels of a network? I don't think it is the number of color channels, or is this the case? The number of color channels was the only link I found regarding the terms ""ConvNets"" and ""number of channels"".
",['convolutional-neural-networks'],It is exactly that - the number of color channels or any other analogue to color that you use.
How to handle changing goals in a DQN?,"
I created a virtual 2D environment where an agent aims to find a correct pose corresponding to a target image. I implemented a DQN to solve this task. When the goal is fixed, e.g. the aim is to find the pose for position (1,1), the agent is successful. I would now like to train an agent to find the correct pose while the goal pose changes after every episode. My research pointed me to the term ""Multi-Objective Deep Reinforcement Learning"". As far as I understood, the aim here is to train one or multiple agents to achieve a policy approximation that fits all goals.
Am I on the right track or how should I deal with different goal states?
","['reinforcement-learning', 'dqn']","The simplest thing you can do is to add data regarding the target pose to the state vector. This will allow any generalisations that the agent learns that apply to similar poses to be used directly.Clearly in normal use, where the target pose remains fixed during the episode, then that part of the state information will not change either during the episode. You will also need to train with a large variety of target poses - so training will take longer.Multi-Objective Deep Reinforcement Learning is slightly different in that it attempts to resolve prioritising between multiple sub-goals. It would also be a more complex solution, whilst augmenting the state vector should allow you to continue using a very similar DQN set up as you have already."
Why don't we use trigonometric functions for the output neurons?,"
Why don't we use a trigonometric function, such as $\tan(x)$, where $x$ is an element of the interval $[0,pi/2)$, instead of the sigmoid function for the output neurons (in the case of classification)?
","['neural-networks', 'classification', 'activation-functions']","Although it's true that if you use certain trigonometric functions, such as the tangent, you could have numerical problems (as suggested in this answer), that's not the only reason for not using trigonometric functions.Trigonometric functions are periodic. In general, we may not want to convert a non-periodic function to a periodic one. To be more concrete, let's suppose we use the sine function as the activation function of the output neurons of a neural network. Assuming only one input, if the input to any of those output neurons is $360k$, for any integer $k$, the result will always be $0$, but that may not be desirable."
How to choose an RL algorithm for a Gridworld that models a much more complex problem,"
I am considering using Reinforcement Learning to do optimal control of a complex process that is controlled by two parameters
$(n_O, n_I), \quad n_I = 1,2,3,\dots, M_I, n_O = 1,2,3,\dots, M_O$
In this sense, the state of the system is represented $S_t = (n_{O,t}, n_{I,t})$. It is represented, because there is a relatively complex system, a solution of coupled Partial Differential Equations (PDES), actually in the background.
Is this problem considered a partially observable Markov Decision Process (POMDP) because there is a whole mess of things behind $S_t = (n_{O,t}, n_{I,t})$?
The reward function has two parameters
$r(s) = (n_{lt}, \epsilon_\infty)$
that are results of the environment (solution of the PDEs).
In a sense, using $S_t = (n_{O,t}, n_{I,t})$ makes this problem similar to Gridworld, where the goal is to go from $S_0 = (M_O, M_I)$ to a state with smaller $(n_O, n_I)$, given reward $r$, where the reward changes from state to state and episode to episode.
Available action operations are
$inc(n) = n + 1$
$dec(n) = n - 1$
$id(n)  = n$
where $n$ can be $n_I$ or $n_O$. This means there are $9$ possible actions
$A=\{(inc(n_O), inc(n_I)),(inc(n_O), dec(n_I)),(inc(n_O), id(n_I)),(dec(n_O), inc(n_I)), \dots\}$
to be taken, but there is no model for the state transition, and the state transition is extremely costly.
Intuitively, as solving a kinematic equation for a point in space, solving coupled PDEs from fluid dynamics should have the Markov property (strongly if the flow is laminar, for turbulence, I have no idea). I've also found a handful of papers where a fluid dynamics problem is parameterized and a policy-gradient method is simply applied.
I was thinking to use REINFORCE as a start, but the fact that $(n_O, n_I)$ does not fully describe the state and questions like this one on POMDP and this one about simulations make me suspicious. Could REINFORCE be used for such a problem, or is there something that prevents this?
","['reinforcement-learning', 'pomdp']",
"How exactly is $Pr(s \rightarrow x, k, \pi)$ deduced by ""unrolling"", in the proof of the policy gradient theorem?","
In the proof of the policy gradient theorem in the RL book of Sutton and Barto (that I shamelessly paste here):

there is the ""unrolling"" step that is supposed to be immediately clear

With just elementary calculus and re-arranging of terms

Well, it's not. :) Can someone explain this step in more detail?
How exactly is $Pr(s \rightarrow x, k, \pi)$ deduced by ""unrolling""?
","['reinforcement-learning', 'policy-gradients', 'proofs', 'sutton-barto', 'policy-gradient-theorem']","The unrolling step is due to the fact you end up with an equation that you can keep expanding indefinitely.Note that we start with calculating $\nabla v_\pi(s)$ and arrive at
$$\nabla v_\pi(s) = \sum_a\left[ \nabla \pi(a|s) q_\pi(s,a) + \pi(a|s) \sum_{s'}p(s'|s,a) \nabla v_\pi (s') \right]\;,$$
which contains a term for $\nabla v_\pi(s')$. This is a recursive relationship, similar to the bellman equation, so we can substitute in a term for $\nabla v_\pi(s')$ which will be a term similar just with $\nabla v_\pi(s'')$. As I mentioned, we can do this indefinitely which leads us to$$\nabla v_\pi(s) = \sum_{x \in \mathcal{S}} \sum_{k=0}^\infty \mathbb{P}(s\rightarrow x, k, \pi) \sum_a \nabla \pi(a|x) q_\pi(x,a)\;.$$We need the term $\sum_{x \in \mathcal{S}} \sum_{k=0}^\infty \mathbb{P}(s\rightarrow x, k, \pi)$ because we want to take an average over the state space, however due to unrolling there are many different $s_t$'s that we need to average over (this comes from the $s',s'',s''',...$ in the unrolling) so we also need to add the probability of transitioning from state $s$ to state $x$ in $k$ time steps, where we sum over an infinite horizon due to the repeated unrolling.If you are wondering what happens to the terms $\pi(a|s)$ and $p(s'|s,a)$ terms and why they are not explicitly shown in this final form, it is because this is exactly what the $\mathbb{P}(s\rightarrow x, k, \pi)$ represents. The average over all possible states accounts for the $p(s'|s,a)$ and the fact that we follow policy $\pi$ in the probability statement accounts for the $\pi(a|s)$."
Are policy-based methods better than value-based methods only for large action spaces?,"
In different books on reinforcement learning, policy-based methods are motivated by their ability to handle large (continuous) action spaces. Is this the only motivation for the policy-based methods? What if the action space is tiny (say, only 9 possible actions), but each action costs a huge amount of resources and there is no model for the MDP, would this also be a good application of policy-based methods?
","['reinforcement-learning', 'policy-based-methods', 'value-based-methods']",
Are there any existing ontologies that model engineering data?,"
Are there any existing ontologies available for ""engineering"" data?
By ""engineering"" I mean pertaining to the fields of electrical, mechanical, thermal, etc., engineering.
","['reference-request', 'ontology']",
"Why does my ""entropy generation"" RNN do so badly?","
I'm new to relatively RNNs, and I'm trying to train generative and guessing neural networks to produce sequences of real numbers that look random. My architecture looks like this (each ""circle"" in the output is the adverserial network's guess for the generated circle vertically below it -- having seen only the terms before it):

Note that the adverserial network is rewarded for predicting outputs close to the true values, i.e. the loss function looks like tf.math.reduce_max((sequence - predictions) ** 2) (I have also tried reduce_mean).
I don't know if there's something obviously wrong with my architecture, but when I try to train this network (and I've added a reasonable number of layers), it doesn't really work very well.

Colab Notebook

If you look at the result of the last code block, you'll see that my generative neural network produces things like

[0.9907787, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827]

But it could easily improve itself by simply training to jump around more, since you'll observe that the adverserial network also predicts numbers very close to the given number (even when the sequence it is given to predict is one that jumps around a lot!).
What am I doing wrong?
","['neural-networks', 'machine-learning', 'deep-learning', 'recurrent-neural-networks', 'generative-adversarial-networks']",
What are the main differences between a language model and a machine translation model?,"
What are the main differences between a language model and a machine translation model?
","['natural-language-processing', 'comparison', 'models', 'machine-translation', 'language-model']",
Is my 57% sports betting accuracy correct?,"
I have been creating sports betting algorithms for many years using Microsoft access and I am transitioning to the ML world and trying to get a grasp on determining the success of my algorithms. I have exported my algorithms as CSV files dating back to the 2013-14 NBA season and imported them into python via pandas.
The purpose of importing these CSV files is to determine the future accuracy of these algorithms using ML. Here are the algorithm records based on the Microsoft access query:

A System: 471-344 (58%) +92.60

B System: 317-239 (57%) +54.10

C System: 347-262 (57%) +58.80


I have a total of 8,814 records in my database, however, the above systems are based on situational stats, e.g., Team A fits an algorithm if they have better Field Goal %, Played Last Game Home/Away, More Points Per Game, etc...
​
Here is some of the code that I wrote using Jupyter to determine the accuracy:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = LinearSVC(C=1.0, penalty=""l2"", dual=False)

clf.fit(X_train, y_train)

pred_clf = clf.predict(X_test)

scores = cross_val_score(clf, X, y, cv=10)

rfe_selector = RFE(clf, 10)

rfe_selector = rfe_selector.fit(X, y)

rfe_values = rfe_selector.get_support()

train = accuracy_score(y_train, clf.predict(X_train))

test = accuracy_score(y_test, pred_clf)

print(""Train Accuracy:"", accuracy_score(y_train, clf.predict(X_train)))

print(""Test Accuracy:"", accuracy_score(y_test, pred_clf))

print(classification_report(y_test, pred_clf, zero_division=1))

print(confusion_matrix(y_test, pred_clf))

print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

​
Here are the results from the code above by system:
A System:

Train Accuracy: 0.6211656441717791
Test Accuracy: 0.5153374233128835
F1 Score: 0.52
CONFUSION MATRIX: [[16 50] [29 68]]
Accuracy: 0.55 (+/- 0.10)

B System:

Train Accuracy: 0.6306306306306306
Test Accuracy: 0.5178571428571429
F1 Score: 0.52
CONFUSION MATRIX: [[49 23] [31 9]]
Accuracy: 0.55 (+/- 0.08)

C System:

Train Accuracy: 0.675564681724846
Test Accuracy: 0.5409836065573771
F1 Score: 0.54
CONFUSION MATRIX: [[15 29] [27 51]]
Accuracy: 0.57 (+/- 0.16)
​

In order to have a profitable system, the accuracy only needs to be 52.5%. If I base my systems off of the Test Accuracy, only the C System is profitable. However, all are profitable if based on Accuracy (mean & standard deviation).
My question is, can I rely on my Accuracy (mean & standard deviation) for future games even though my Testing Accuracy is lower than 52.5%?
If not, any suggestions are greatly appreciated on how I can gauge the future results on these systems.
","['machine-learning', 'algorithm', 'support-vector-machine', 'cross-validation']",
Is the self-attention matrix softmax output (layer 1) symmetric?,"
Let's assume that we embedded a vector of length 49 into a matrix using 512-d embeddings. If we then multiply the matrix by its transposed version, we receive a matrix of 49 by 49, which is symmetric. Let's also assume we do not add the positional encoding and we only have only one attention head in the first layer of the transformer architecture.
What would the result of the softmax on this 49 by 49 matrix look like? Is it still symmetric, or is the softmax correctly applied for each line of the matrix, resulting in a non-symmetric matrix? My guess would be that the matrix should not be symmetric anymore. But I'm unsure about that.
I ask this to verify if my implementation is wrong or not, and what the output should look like. I have seen so many sophisticated and different implementations of the transformer architecture with different frameworks, that I can't answer this question for myself right now (confusion). I still try to understand the basic building blocks of the transformer architecture.
","['transformer', 'attention', 'softmax']",
Continuous state and continuous action Markov decision process time complexity estimate: backward induction VS policy gradient method (RL),"
Model Description: Model based(assume known of the entire model) Markov decision process.
Time($t$): Finite horizon discrete time with discounting factor
State($x_t$): Continuous multi-dimensional state
Action($a_t$): Continuous multi-dimensional action (deterministic action)
Feasible(possible action) ($A_t$): possible action is a continuous multi-dimensional space, no discretization for the action space.
Transition kernel($P$): known and have some randomness associated with the current state and next state
Reward function: known in explicit form and terminal reward is know.
The method I tried to solve the model:

Discretize the state space and construct multi-dimensional grid for state space, starting from the terminal state, I used backward induction to reconstruct value function from the previous period. By using the Bellman equation, I need to solve an optimization problem, selecting the best action that gives me the best objective function.

$$V_t(x_t) = max_{a_t \in A_t}[R_t(x_t,a_t) + E[\tilde{V}_{t+1}(x_{t+1})|x_t, a_t]]$$
where $\tilde{V}_{t+1}$ here is an approximation using interpolation method, since the discrete values are calculated from the previous time episode. In other words: $\tilde{V}_{t+1}$ is approximated by some discrete value: $$V_{t+1}(x_0),V_{t+1}(x_1),V_{t+1}(x_2)\cdots V_{t+1}(x_N)$$ where $x_0,x_1,x_2\cdots x_N$ are grid points from discretizing the state space.
In this way, for every time steps t, I could have a value function for every grid point and my value function could be approximated by using some interpolation method(probably cubic spline). But here are some of the problems: 1. what kind of interpolation is suitable for high dimensional data. 2. Say we have five dimension for the state, then I discretize the state by giving 5 grid points to every dimension, then there are 5^5 = 3125 discrete state values I need to calculate through the optimization.(Curse of the dimensionality). 3. What kind of optimizer should I use? Since I do not know the shape of the objective function, I do not know if it is a smooth function and I do not know if the function is concave. So I may have to use a robust optimizer, probably some evolutionary optimizer. So eventually I end up with this computational complexity and it takes too long for the computation.
And recently I learned the techniques of policy gradient from OpenAI: https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html
This method avoid using this backward induction and approximating the value function by using interpolation. And obtained the approximated policy by firstly guessing a functional form of the policy and take approximated gradient of the policy function by using sampling(simulation) method. Since the model is known, every time it could sample new trajectories and use that to update the policy by using the stochastic gradient assent method. In this way updating the policy until it gets some sort of convergency.
I am wondering if this type of technique could potentially reduce the computational complexity significantly. Any advice helps, thanks a lot.
","['reinforcement-learning', 'optimization', 'time-complexity', 'finite-markov-decision-process', 'interpolation']",
Does the number of parameters in a convolutional neuronal network increase if the input dimension increases?,"
If I have a convolutional neuronal network, does the input dimension change the number of parameters? And if yes, why? If the sizes and lengths of the filters are still the same, how can the number of parameter in a network increase?
","['convolutional-neural-networks', 'filters', 'weights']","If I have a convolutional neuronal network, does the input dimension change the number of parameters? And if yes, why?If the convolutional neural network (CNN) only uses convolutional layers, then the number of parameters does not increase as a function of the spatial dimensions ($x$ and $y$) of the input. This is one of the advantages of CNNs!The reason is quite simple: the parameters of the convolutional layers are the kernels (aka filters), which typically have fixed dimensions and can, nevertheless, be applied to inputs of different spatial dimensions, provided that the necessary padding is used. However, note that padding can create bigger feature maps, but feature maps are not the parameters of the neural network: they are the output of the convolutional layers. That's probably what confuses you, when you see a diagram of a CNN, because you see bigger feature maps and you might think that the number of parameters increases.Here's a TensorFlow 2 (and Keras) program that shows that the number of parameters does not change as a function of the $x$ and $y$ dimension of the input.The parameters of a convolutional layer can increase if you increase the size of each kernel and the number of kernels, but this does not necessarily depend on the input.The parameters of the CNN can also increase if you increase the depth of the input, but that's typically fixed (either $3$ for RGB images or $1$ for grayscale images). The reason is quite simple too: the kernels in the first convolutional layer (connected to the input layer) will have the same depth as the depth of the input.If your CNN also has fully connected layers, then the number of parameters also depends on the dimensions of the inputs. This is because the parameters of the fully connected layers depend on the number and dimensions of the feature maps (remember the flatten layer before the fully connected layers?), which, as I said, can increase as a function of the input.If you don't want to use fully connected layers, you may want to try fully convolutional networks (FCNs), which do not make use of fully connected layers, but can, nonetheless, be used to solve classification (and other) tasks."
What would happen if we sampled only one tuple from the experience replay?,"
The concept of experience replay is saving our experiences in our replay buffer. We select at random to break the correlation between consecutive samples, right?
What would happen if we calculate our loss using just one experience instead of a mini-batch of experiences?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'experience-replay']","The concept of experience replay is saving our experiences in our replay buffer. We select at random to break the correlation between consecutive samples, right?Yes that is a major benefit of using a replay buffer.A secondary benefit is the ability to use the same sample more than once. This can lead to beter sample efficiency, although that is not guaranteed.What would happen if we calculate our loss using just one experience instead of a mini-batch of experiences?The algorithm is still valid, but the gradient estimate for the update step would be based on a single record of [state, action, reward, next state]. This would be a high variance update process, with many steps in wrong directions, but in expectation over many steps you should still see a correct gradient. You would probably need to compensate for the high variance per sample by reducing the learning rate.In addition, assuming the standard approach of collecting one time step then making one update to DQN neural network, each piece of experience would only be used once on average before being discarded.These two effects will likely combine such that the learning process would not be very sample efficient.The size of the minibatch is one of many hyperparameters you can change in DQN. It might be the case for some problems that choosing a low minibatch size is helpful, provided other adjustments (such as a lower learning rate) are made along with it. If you are not sure, you mostly have to try and see.In my experience on a small range of problems, a moderate size of minibatch - ranging from 10 to 100 - has worked the best in terms of end results of high scoring agents. However, I have not spent a long time trying to make low batch sizes work."
What are some good papers or resources for aspect extraction and opinion modelling from video or audio?,"
I am quite new to deep learning. I just finished the deep learning specialization by Professor Andrew NG and Deep Learning AI. Now, my professor (instructor) has advised me to look into some classic papers for aspect extraction and opinion mining from video. Could anyone suggest me some resources where I can get started? Can anyone suggest some papers I should read? Maybe a course or a book or some links to descriptive sessions. Your help would be appreciated.
","['deep-learning', 'computer-vision', 'feature-extraction', 'sentiment-analysis', 'data-mining']",
Can we give a command to an AI and wait for it to do the job without explicitly telling it how to do it?,"
I am a computer science student. I learned about programming languages recently, but I don't know much about artificial intelligence.
I want to know, why don't we program something in a way that we could tell the program

Hey! Do this for me!

And then just sit down and wait that the AI does the job?
Is this currently possible to do?
","['machine-learning', 'state-of-the-art']",
Is Reinforcement Learning what I need for this image to image translation problem?,"
I have a paired dataset of binary images A and B: A1 paired with B1, A2-B2, etc., with simple shapes (rectangles, squares).
The external software receives both images A and B and it returns a number that represents the error.
I need a model that, given images A and B, can modify A into A' by adding or removing squares, so that the error from the software is minimized. I don't have access to the source code of the software so I don't know how it works.
I tried to make a NN that copies the functionality of the software, and a generative NN to generate the modified image A' but I haven't got good results.
The software can only receive binary images, so I cannot use a loss function because my last layer of the generator being a softmax, if I apply a threshold, I will lose the track of the gradients, so I cannot apply gradient descent.
Someone told me that when you cannot calculate the gradient of the loss with respect to the weights, reinforcement learning with policy gradients is a good solution.
I'm new to this field, so I want to be sure I'm going in the right direction.
","['machine-learning', 'reinforcement-learning', 'image-processing', 'image-segmentation', 'image-generation']",
What's the best practice for Boltzmann Exploration temperature in RL?,"
I'm currently modeling DQN in Reinforcement Learning. My question is: what are the best practices related to Boltzmann Exploration? My current thoughts are: (1) Let the temperature decay through training and finally stop at 0.01, when the method will always select the best practice, with almost no randomness. (2) Standardize the predicted Q values before feeding into the softmax function.
Currently, I'm using (2), and the reward is suffering from high variance. I'm wondering whether it has something to do with the exploration method?
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'deep-learning', 'q-learning']","Do you have to use Boltzmann exploration, strictly? There is a modification for Boltzmann exploration called Mellow-max. It, basically, provides an adaptive temperature for Boltzmann exploration.Here is the link for the paper for tuning mellow-max with deep reinforcement learning (DQN is often mentioned): http://cs.brown.edu/people/gdk/pubs/tuning_mellowmax_drlw.pdfHere is the link for mellow-max implemented with SARSA (I recommend reading this first, to get an understanding of mellow-max): https://arxiv.org/pdf/1612.05628.pdf"
How to validate that my DQN hyperparameters are the optimal?,"
My DQN model outputs the best traffic light state in an intersection. I used different values of batch size and learning rate to find the best model. How would I know if I got the optimal hyperparameter values?
","['machine-learning', 'reinforcement-learning', 'q-learning', 'dqn']",
Is there a way to show convergence of DQN other than by eye observation?,"
I made a DQN model and plot its reward curve. You can see intuitively that the curve already converged since its reward value now just oscillates. How can I show confidence that my DQN already reached its optimal other than by just showing the curve? Are there any way to validate that it is already optimized?
","['machine-learning', 'reinforcement-learning', 'q-learning', 'dqn']",
Why care about the value of the action which I'm not gonna take in policy iteration?,"
In this article, there is an explanation (with an example) of how policy iteration works.
It seems that, if we replace all the probabilities of moves in the example by new probabilities where the best action is taken 100% of the time and the other moves are taken 0% of the time, then the final policy will end up to be (south, south, north) as in the example provided. However, if we are certain of our moves, we can go north and then south in the example to get most of the reward.
In other words, it seems to be incorrect to calculate the value of a state by summing up rewards for all possible actions out of the state, because, like in the case I described above, or a case where one action gives you a huge penalty, you are 100% gonna avoid it, therefore the value state is unfairly weighted.
Why care about the value of the action which I'm not gonna take?
","['machine-learning', 'reinforcement-learning', 'policy-iteration']",
How do I test an LSTM-based reinforcement learning model using any Atari games in OpenAI gym?,"
I am writing a couple of different reinforcement learning models based on Rainbow DQN or some PG models. All of them internally use an LSTM network because my project is using time series data.
I wanted to test my models using OpenAI Gym before I add too many domain specific code to the models.
The problem is that, all of the Atari games seem to fall into the CNN area which I don't use.
Is it possible to use OpenAI Gym to test any time series data driven RL models/networks?
If not, is there any good environment that I can use to examine the validity of my models?
","['machine-learning', 'reinforcement-learning', 'long-short-term-memory', 'open-ai', 'time-series']",
Is this figure a correct representation of off-policy actor-critic methods?,"

Does this figure correctly represent the overall general idea about actor-critic methods for on-policy (left) and off-policy (right) case?
I am a bit confused about the off-policy case (right figure). Why does the right figure represent the off-policy actor-critic methods?
","['reinforcement-learning', 'policy-gradients', 'actor-critic-methods', 'off-policy-methods']",
"In Deep Deterministic Policy Gradient, are all weights of the policy network updated with the same or different value?","
I'm trying to understand the DDPG algorithm shown at this page. I don't know what should the result of the gradient at step 14 be.

Is it a scalar that I have to use to update all the weights (so all weights are updated with the same value)? Or is it a list with a different values to use for updating for each weight? I'm used to working with loss functions and an $y$ target, but here I don't have them so I'm quite confused.
","['reinforcement-learning', 'policy-gradients', 'actor-critic-methods', 'ddpg']",
Updating action-value functions in Semi-Markov Decision Process and Reinforcement Learning,"
Suppose that the transition time between two states is a random variable (for example, unknown exponential distribution); and between two arrivals, there is no reward. If $\tau$ (real number not an integer number) shows the time between two arrivals, should I update Q-functions as follows:
$Q(s,a) = Q(s,a)+\alpha.(R+\gamma^{\tau} \max_{b \in A}Q(s^{\prime},b)-Q(s,a))$
And, to compare different algorithms, total rewards ($TR=R_{1}+ R_2+R_{3}+...+R_{T}$) is used.
What measure should be used in the SMDP setting? I would be thankful if someone can explain the Q-Learning algorithm for the SMDP problem with this setting.
Moreover, I am wondering when Q-functions are updated. For example, if a customer enters our website and purchases a product, we want to update the Q-functions. Suppose that the planning horizon (state $S_{0}$) starts at 10:00 am, and the first customer enters at 10:02 am, and we sell a product and gain $R_1$ and the state will be $S_1$. The next customer enters at 10:04 am, and buy a product, and gain reward $R_2$ (state $S_{2}$). In this situation, should we wait until 10:02 to update the Q-function for state $S_0$?
Is the following formula correct?
$$V(S_0)= R_1 \gamma^2+  \gamma^2V(S_1)$$
In this case, if I discretize the time horizon to 1-minute intervals, the problem will be a regular MDP problem. Should I update Q-functions when no customer enters in a time interval (reward =0)?
","['reinforcement-learning', 'q-learning', 'markov-decision-process', 'discount-factor', 'semi-mdp']",
Does the off-policy evaluation work for non-stationary policies?,"
As the title says, in reinforcement learning, does the off-policy evaluation work for non-stationary policies?
For example, IS (importance sampling)-based estimators, such as weighted IS or doubly robust, are still unbiased when they are used to evaluate UCB1, which is a non-stationary policy, as it chooses an action based on the history of rewards?
","['reinforcement-learning', 'policies', 'off-policy-methods', 'importance-sampling']",
Can StyleGAN be refined without a full training?,"
Can I refine StyleGAN or StyleGAN2 without retraining it for many days, such that its pretrained model is trained to generate only faces similar to a (rather small) set of reference images?
I would like to avoid creating a large dataset and training for many days to weeks, but use the existing model and just bias it towards a set of images.
","['generative-adversarial-networks', 'pretrained-models']",
"In layman's terms, what is stochastic computation graph?","
I'm going through the distributions package on PyTorch's documentation and came across the term stochastic computation graph. In layman's terms, what is it?
","['reinforcement-learning', 'pytorch']",
How to know if my DQN is optimized?,"
I made a DQN that controls a traffic light. The observation states are the number of vehicles of each lane in the intersection. I trained it for 500 episodes and saved the model every 50th episode. I plotted the reward curve of the model after the training and found out that around the 460th episode, the reward curve become unstable. Does it mean that the optimized DQN model is the 450th model? If not, how do I know if the my DQN is really optimized?
","['machine-learning', 'q-learning', 'dqn']","There is a good chance that your DQN is already optimized but you would have to take a look at its performance to really check and see whether its actions seem up to par.Reasons it may not be optimized: If you are tracking the reward after every episode, unstable rewards are very common just due to random chance, but if you are averaging the reward over the past 50 episodes or so, then it may also be your learning rate or epsilon.If your learning rate is too high or low you may either never be able to reach a fully optimized DQN or be stuck in a local minimum. An easy way to solve a problem like this would be to just add a simple learning rate decay, so that the learning rate would start off high as to not get stuck in local minima yet decay to a small enough number where you know that the agent has found the global minimum.The other problem could be that your epsilon may be too high or low. A high epsilon will never allow the agent to fully optimize while a low epsilon doesn't allow the agent to explore and discover better strategies, so it may be a good idea to mess around with this as well.The only way to really gauge the agent's performance would be to look at it making some decisions through a video or by analyzing some of its predictions. And, if it seems to be doing well, then it may very well be optimized, but, if the agent is not performing as well as it should, then it may be a good idea to try out some of the strategies above."
Two DQNs in two different time scales,"
I have the following situation. An agent plays a game and wants to maximize the accumulated reward as usual, but it can choose its adversary. There are $n$ adversaries.
In episode $e$, the agent must first select an adversary. Then for each step $t$ in the episode $e$, it plays the game against the chosen adversary. Every step $t$, it receives a reward following the chosen action in step $t$ (for the chosen adversary). How to maximize the expected rewards using DQN? It is clear that choosing the ""wrong"" (the strongest) adversary won't be a good choice for the agent. Thus, to maximize the accumulated rewards, the agent must take two actions at two different timescales.
I started solving it using two DQNs, one to decide the adversary to play against and one to play the game against the chosen adversary. I have two duplicate hyperparameters (batch_size, target_update_freq, etc), one for each DQN. Have you ever seen two DQNs like this? Should I train the DQNs simultaneously?
The results that I am getting is not that good. The accumulated reward is decreasing, the loss isn't always decreasing...
","['reinforcement-learning', 'deep-rl']","From comments, you say there is no ""outer"" goal for picking an adversary other than  scoring highly in an individual episode.You could potentially model the initial adversary choice as a partially separate Markov Decision Process (MDP), where choosing the opponent is a single-step episode with return equal to whatever reward the secondary MDP - which played the game - obtains. However, this ""outer"" MDP is not much of an MDP at all, it is more like a contextual bandit. In addition, the performance of the inner game-playing agent will vary both with the choice of opponent, and over time as it learns to play better against each opponent. This makes the outer MDP non-stationary. It also requires the inner MDP to know what opponent it is facing in order to correctly predict correct choices and/or future rewards.That last part - the need for any ""inner"" agent to be aware of the opponent it is playing against - is likely to be necessary whatever structure you chooose. That choice of opponent needs to be part of the state for this inner agent, because it will have an impact on likely future rewards. A characterisation of the opponents also needs to be part of whatever predictive models you could use for the outer agent.A more natural, and probably more useful, MDP model for your problem is to have a single MDP where the first action $a_0$ is to select the opponent. This matches the language you use to describe the problem, and resolves your issue about trying to run a hierarchy of agents. Hierarchical reinforcement learning is a real thing, and very interesting for solving problems which can be broken down into meaningful sub-goals that an agent could discover autonomously, but it does not appear to apply for your problem.This leaves you with a practical problem of creating a model that can switch between choosing between two sets of radically different actions. The select an opponent action only occurs at the first state of the game, and the two sets of actions do not overlap at all. However, in terms of the theoretical MDP model this is not an issue at all. It is only a practical issue of how you get to fit your Q function approximator to the two radically different action types. There a a few ways around that. Here are a couple that might work for you:One shared networkAlways predict for all kinds of action choice, so the agent still makes predictions for switching opponents all the way to the end of the game. Then filter the action choices down to only those available at any time step. When $t=0$ only use the predictions for actions for selecting an opponent, for $t \ge 1$ only use predictions relating to moves in the game.Two separate approximatorsHave two function approximators in your agent, use one for predicting reward at $t=0$ that covers different opponent choices, and use the other for the rest of the game. If $n$ is small and there is no generalisation between opponents (i.e. no opponent ""stats"" that give some kind of clue towards the end results), then for the first approximator, you could even use a Q table.For update steps you need to know whether any particular action value was modelled in one or other of the Q functions - and this will naturally lead you to bootstrap$$\hat{q}_{o}(s_0, a_0, \theta_0) \leftarrow r_1 + \gamma \text{max}_{a'}\hat{q}_{p}(s_1, a', \theta_1)$$where $\hat{q}_{o}$ is your approximate model for action values of selecting opponents (and $a_0$ must be an opponent choice) at the start of the game, and $\hat{q}_{p}$ is the nodel you use for the rest of it (and $a'$ must be a position play in the game). I've misused $\leftarrow$ here to stand in for whatever process used to update the action value towards the new estimate - a tabular method that would be a rolling average with current estimate, in neural networks of course that is gradient descent using backpropagation."
Creating 4k HDR video from 720p footage,"
So, my company recently bought a big 4k HDR TV for our reception, where we keep showing some videos that were originally shot/created at 720p resolution. Before this, we had a relatively small HD TV, so not a problem. Because the old videos now look dated, my boss wanted to upscale them and enhance their coloring to avoid shooting or procuring new animated videos.
This sounded like a fun project, but I know little about AI, and less so about video encoding/decoding. I've started researching and found some references, such as Video Super-Resolution via Bidirectional
Recurrent Convolutional Networks, so while it seems like I have homework to do, it's clearly ""been done before"". Would be great to find some code that works on standard formatted videos though.
What I'm struggling to find, but would need some good basis to answer in the negative, is: What about HDR? I'm not finding the research terms nor any mention on result for improving dynamic range on videos. Is there any research done on that? Though actual HDR is a format, most of the shots and pictures used for our videos were taken on cameras with small color gamut and latitude, thus everything looks ""washed"" and the new TV really makes this obvious by comparing against demo videos.
PS:
Unlike much of the literature I'm finding, I'm not aiming at real-time super resolution, it would be great if it took less than one night to process for a 10 minute video though.
","['machine-learning', 'reference-request', 'applications', 'research']",
"What do the notations $\sim$ and $\Delta (A) $ mean in the paper ""Fairness Through Awareness""?","
In this paper Fairness Through Awareness, the notation $\mathbb{E}_{x \sim V} \mathbb{E}_{a \sim \mu_x} L(x,a)$ is being used (page 5 top line), where $V$ denotes the set of individuals (so I guess set of feature vectors?) and the meaning of the other variables can be found in the paragraph above the mentioned notation. What does the $\sim$ in the expectation stand for?
Another notation that I do not know is $\Delta (A) $, where $A$ is the set of outcomes, for instance, $A = \{ 0,1\}$. What does it stand for?
","['machine-learning', 'papers', 'notation', 'explainable-ai']","The $\sim$ symbol means that a random variable is drawn from the given distribution, i.e. if I were to say $X$ has a Standard Normal distribution I would write $X \sim \text{Normal}(0,1)$. They write two explicit expectations here because $a$ is a random variable with distribution $\mu_x$ but $X$ is also a random variable with distribution $V$. I believe you are right that $V$ would be analogous to a set of features. So we are saying that $X$ is a random variable over the distribution over the features, or individuals in this context.As for the $\Delta(A)$, I have never seen this notation used before -- I am not sure if it is standard notation. I Googled to see if it was something that I just hadn't seen before, but there was no such answer. However, from the context of the paper they define $M: V \rightarrow \Delta (A)$ to be mappings from individuals to probability distributions over $A$, so I guess that $\Delta (A) $ is probability distributions over $A$."
Is the VC Dimension meaningful in the context of Reinforcement Learning?,"
Is the VC dimension meaningful for reinforcement learning (RL), as a machine learning (ML) method? How?
","['machine-learning', 'reinforcement-learning', 'computational-learning-theory', 'vc-dimension', 'vc-theory']","Yes, it is. This article (Approximate Planning in Large POMDPs via Reusable Trajectories) explain about it by means of the trajectory tree:A trajectory tree is a binary tree in which each node is labeled by a state and observation
pair, and has a child for each of the two actions. Additionally, each link to a child is
labeled by a reward, and the tree's depth will be $H_\epsilon$, so it will have about $2^{H_\epsilon}$ nodes. The root is labeled by $s_0$ and the observation there, $o_0$.Now a policy $\pi$ will be defined like the following base on the trajectory tree:For any deterministic strategy $\pi$ and any trajectory tree $T$, $\pi$ defines a path through
$T$: $\pi$ starts at the root, and inductively, if $\pi$ is at some internal node in $T$, then we feed to $\pi$ the observable history along the path from the root to that node, and $\pi$ selects and moves to a child of the current node. This continues until a leaf node is reached, and we define $R(\pi, T)$ to be the discounted sum of returns along the path taken. In the case that $\pi$ is stochastic, $\pi$ defines a distribution on paths in $T$, and $R(\pi, T)$ is the expected return according to this distribution.  Hence, given $m$ trajectory trees $T_1 , \ldots , T_m$, a natural estimate for $V^\pi(s_0)$ is $V^\pi(s_0) = \frac{1}{m}\sum_{i=1}^mR(\pi, T_i)$. *Note that each tree can be used to evaluate any
strategy, much the way a single labeled example $\langle x, f(x)\rangle$ can be used to evaluate any hypothesis $h(x)$ in supervised learning. Thus in this sense, trajectory trees are reusable.Now similar to definition of VC theory for classification methods:Our goal now is to establish uniform convergence results that bound the error of the estimates $V^\pi(s_0)$ as a function of the ""sample size"" (number of trees) $m$.And finally, we have the following theorem:Let $\Pi$ be any finite class of deterministic strategies for an arbitrary two-action POMDP $M$. Let $m$ trajectory trees be created using a generative model for $M$, and
$\widehat{V}^\pi(s_0)$ be the resulting estimates. If $m = O((V_{\max}/\epsilon)^2(\log(|\Pi|) + \log(1/\delta)))$, then with probability $1 - \delta$, $|V^\pi(s_0) - \widehat{V}^\pi(s_0)|\leqslant \epsilon$ holds simultaneously for all $\pi \in \Pi$.About the VC dimension of $\Pi$, if we suppose we have two actions $\{a_1, a_2\}$ (it can be generalized to more actions), we can say:If $\Pi$ is a (possibly infinite) set of deterministic strategies, then each strategy $\pi \in \Pi$ is simply a deterministic function mapping from the set of observable histories to the set $\{a_1, a_2\}$,
and is thus a boolean function on observable histories. We can, therefore, write $\mathcal{VC}(\Pi)$ to
denote the familiar VC dimension of the set of binary functions $\Pi$. For example, if $\Pi$ is
the set of all thresholded linear functions of the current vector of observations (a particular
type of memoryless strategy), then $\mathcal{VC}(\Pi)$ simply equals the number of parameters.and the following theorem:Let $\Pi$ be any class of deterministic strategies for an arbitrary two-action
POMDP $M$, and let $\mathcal{VC}(\Pi)$ denote its VC dimension. Let $m$ trajectory trees be created using a generative model for $M$, and $\widehat{V}^\pi(s_0)$ be the resulting estimates. If:
$$
m = O((V_{\max}/\epsilon)^2(H_\epsilon\mathcal{VC}(\Pi)\log(V_{\max}/\epsilon) + \log(1/\delta)))
$$
then with probability $1 - \delta$, $|V^\pi(s_0) - \widehat{V}^\pi(s_0)|\leqslant \epsilon$ holds simultaneously for all $\pi \in \Pi$."
Feature scaling strategy for many feature with very large variation between them?,"
I was running into a situation in which my input feature experience a very large variation in term of magnitude.
Particularly, consider feature 1 belong to group 1 and feature 2 3 4 belong to group 2,
Like this picture below

I was really worried that in this case feature 1 might dominate feature 2,3,4 (group 2) because its corresponding value is so large (I was trying to train this data set on a neural network).
In this situation, what would be the appropriate scaling strategy ?
Update: I know for sure that the value of feature 1 is an integer that is uniform on the interval [22,42]
But for feature 2 ,3 ,4 I do not have any insight
Thank you for your enthusiast !
","['neural-networks', 'features']",
"What introductory books to reinforcement learning do you know, and how do they approach this topic?","
Currently, I'm only going through these two books

Reinforcement Learning: An Introduction, by Sutton and Barto: RL explained on an engineering level (mathematical, but readable for a non-mathematician). Elementary notions from probability and statistics are required (conditional probability, total probability theorem, total expectation theorem, and similar. The MIT RES.6-012 ""Introduction to Probability"" course is a great source of information for these topics.).

Deep Reinforcement Learning, by Miguel Morales: this book introduces the main elements of reinforcement learning in a less formal way than Sutton and Barto (derivations for some equations are not given), using examples to describe the math.


What other introductory books to reinforcement learning do you know, and how do they approach this topic?
","['reinforcement-learning', 'reference-request', 'books']",
When to use AND and when to use Implies in first-order logic?,"
I am trying to learn the theory behind first-order logic (FOL) and do some practice runs of converting statements into the form of FOL.
One issue I keep running into is hesitating on whether to use an AND ($\land$) statement or an IMPLIES ($\rightarrow$) statement.
I have seen examples such as ""Some boys are intelligent"" turned into:
$$
\exists x \text{boys}(x) \land \text{intelligent}(x)
$$
Can I make a general assumption that when I see $x$ is/are $y$, I can use an AND?
With a statement such as ""All movies directed by M. Knight Shamalan have a supernatural character"", I feel that that statement can be translated to either:
$$
\forall x, \exists y \; \text{directed}(\text{Shamalan}, x) \rightarrow \text{super-natural character}(y)
$$
or
$$
\forall x, \exists y \; \text{directed}(\text{Shamalan}, x) \land \text{super-natural character}(y)
$$
Is there a better way to distinguish between when to use one or the other?
","['proofs', 'logic']","When considering the sentence ""Some boys are intelligent"", it makes sense to express it by ∃x boys(x) ∧ intelligent(x). This is because the existential quantifier makes sure to express that at least some, but not necessarily all boys are intelligent. More specifically, you say that there exists something which is a boy and which is intelligent. But if such a something exists, then there are some boys which are intelligent. Your statement is satisfied.Using a simple implication → here wouldn't work semantically in that context. For example, ∃x boys(x) → intelligent(x) expresses that: For some x, if x is a boy, then x is intelligent. By rewriting, we would get ∃x ¬boys(x) ∨ intelligent(x). This statement would even be true if all x were not boys, because ¬boys(x) would be true in these cases. So, ∃x boys(x) → intelligent(x) would hold as soon as something in your model wasn't a boy. In that case, ∃x boys(x) → intelligent(x) would be true, but ""Some boys are intelligent"" would not really be satisfied, since there are simply no boys in your model which could be intelligent. So, you will have to resort to something like ∃x boys(x) ∧ intelligent(x), since otherwise there is no guarantee that boys (even intelligent ones) are present in the resulting model."
Which kind of data does sigmoid kernel performance well?,"
While I was playing with some hyperparameters, I came to a wired situation. My dataset is IRIS dataset to be specific. SVM algorithm has some hyperparameters that we can tune, such as Kernels, and C value. 
(All accuracy calculations and SVM are from sklearn package to be specific)
I made a comparison between kernels and noticed sigmoid kernel was performing way worse in terms of accuracy. It is more than 3 times less accuracy than RBF, Linear, and Polynomial. I do know that kernels are quite data-sensitive and data-specific, but I would like to know ""Which types of data is sigmoid kernel good at any example? or is this my fault due to wrong C value for sigmoid kernel?""
","['algorithm', 'performance', 'support-vector-machine', 'accuracy', 'sigmoid']","The sigmoid kernel is better suited for binary classifications. As the IRIS dataset is for multi-class classification, its performance was not as good as other kernels. You can train with only 2 types of flowers to see if the sigmoid kernel can perform well."
Do convolutional neural networks perform convolution or cross-correlation?,"
Typically, people say that convolutional neural networks (CNN) perform the convolution operation, hence their name. However, some people have also said that a CNN actually performs the cross-correlation operation rather than the convolution. How is that? Does a CNN perform the convolution or cross-correlation operation? What is the difference between the convolution and cross-correlation operations?
","['neural-networks', 'convolutional-neural-networks', 'comparison', 'convolution', 'cross-correlation']","Theoretically, convolutional neural networks (CNNs) can either perform the cross-correlation or convolution: it does not really matter whether they perform the cross-correlation or convolution because the kernels are learnable, so they can adapt to the cross-correlation or convolution given the data, although, in the typical diagrams, CNNs are shown to perform the cross-correlation because (in libraries like TensorFlow) they are typically implemented with cross-correlations (and cross-correlations are conceptually simpler than convolutions). Moreover, in general, the kernels can or not be symmetric (although they typically won't be symmetric). In the case they are symmetric, the cross-correlation is equal to the convolution.To understand the answer to this question, I will provide two examples that show the similarities and differences between the convolution and cross-correlation operations. I will focus on the convolution and cross-correlation applied to 1-dimensional discrete and finite signals (which is the simplest case to which these operations can be applied) because, essentially, CNNs process finite and discrete signals (although typically higher-dimensional ones, but this answer applies to higher-dimensional signals too). Moreover, in this answer, I will assume that you are at least familiar with how the convolution (or cross-correlation) in a CNN is performed, so that I do not have to explain these operations in detail (otherwise this answer would be even longer).Both the convolution and the cross-correlation operations are defined as the dot product between a small matrix and different parts of another typically bigger matrix (in the case of CNNs, it is an image or a feature map). Here's the usual illustration (of the cross-correlation, but the idea of the convolution is the same!).To be more concrete, let's suppose that we have the output of a function (or signal) $f$ grouped in a matrix $$f = [2, 1, 3, 5, 4] \in \mathbb{R}^{1 \times 5},$$ and the output of a kernel function also grouped in another matrix $$h=[1, -1] \in \mathbb{R}^{1 \times 2}.$$  For simplicity, let's assume that we do not pad the input signal and we perform the convolution and cross-correlation with a stride of 1 (I assume that you are familiar with the concepts of padding and stride).Then the convolution of $f$ with $h$, denoted as $f \circledast h = g_1$, where $\circledast$ is the convolution operator, is computed as follows\begin{align}
f \circledast h = g_1
&=\\
[(-1)*2 + 1*1, \\
(-1)*1 + 1*3, \\
(-1)*3 + 1*5, \\
(-1)*5+1*4] 
&=\\ 
[-2 + 1, -1 + 3, -3 + 5, -5 + 4]
&=\\
[-1, 2, 2, -1] \in \mathbb{R}^{1 \times 4}
\end{align}So, the convolution of $f$ with $h$ is computed as a series of element-wise multiplications between the horizontally flipped kernel $h$, i.e. $[-1, 1]$, and each $1 \times 2$ window of $f$, each of which is followed by a summation (i.e. a dot product). This follows from the definition of convolution (which I will not report here).Similarly, the cross-correlation of $f$ with $h$, denoted as $f \otimes h = g_2$, where $\otimes$ is the cross-correlation operator, is also defined as a dot product between $h$ and different parts of $f$, but without flipping the elements of the kernel before applying the element-wise multiplications, that is\begin{align}
f \otimes h = g_2
&=\\
[1*2 + (-1)*1, \\ 
1*1 + (-1)*3, \\ 
1*3 + (-1)*5, \\ 
1*5 + (-1)*4] 
&=\\ 
[2 - 1, 1 - 3, 3 - 5, 5 - 4]
&=\\
[1, -2, -2, 1] \in \mathbb{R}^{1 \times 4}
\end{align}The only difference between the convolution and cross-correlation operations is that, in the first case, the kernel is flipped (along all spatial dimensions) before being applied.In both cases, the result is a $1 \times 4$ vector. If we had convolved $f$ with a $1 \times 1$ vector, the result would have been a $1 \times 5$ vector. Recall that we assumed no padding (i.e. we don't add dummy elements to the left or right borders of $f$) and stride 1 (i.e. we shift the kernel to the right one element at a time). Similarly, if we had convolved $f$ with a $1 \times 3$, the result would have been a $1 \times 3$ vector (as you will see from the next example).The results of the convolution and cross-correlation, $g_1$ and $g_2$, are different. Specifically, one is the negated version of the other. So, the result of the convolution is generally different than the result of the cross-correlation, given the same signals and kernels (as you might have suspected).Now, let's convolve $f$ with a $1 \times 3$ kernel that is symmetric around the middle element, $h_2 = [-1, 2, -1]$. Let's first compute the convolution.\begin{align}
f \circledast h_2 = g_3
&=\\
[(-1)*2 + 1*2 + (-1) * 3,\\ (-1)*1 + 2*3 + (-1) * 5,\\ (-1)*3 + 2*5 + (-1) * 4] 
&=\\ 
[-2 + 2 + -3, -1 + 6 + -5, -3 + 10 + -4] 
&=\\
[-3, 0, 3]
\in \mathbb{R}^{1 \times 3}
\end{align}Now, let's compute the cross-correlation\begin{align}
f \otimes h_2 = g_4
&=\\
[(-1)*2 + 1*2 + (-1) * 3, \\ (-1)*1 + 2*3 + (-1) * 5, \\ (-1)*3 + 2*5 + (-1) * 4] 
&=\\
[-3, 0, 3]
\in \mathbb{R}^{1 \times 3}
\end{align}Yes, that's right! In this case, the result of the convolution and the cross-correlation is the same. This is because the kernel is symmetric around the middle element. This result applies to any convolution or cross-correlation in any dimension. For example, the convolution of the 2d Gaussian kernel (a centric-symmetric kernel) and a 2d image is equal to the cross-correlation of the same signals.In the case of CNNs, the kernels are the learnable parameters, so we do not know beforehand whether the kernels will be symmetric or not around their middle element. They won't probably be. In any case, CNNs can perform either the cross-correlation (i.e. no flip of the filter) or convolution: it does not really matter if they perform cross-correlation or convolution because the filter is learnable and can adapt to the data and tasks that you want to solve, although, in the visualizations and diagrams, CNNs are typically shown to perform the cross-correlation (but this does not have to be the case in practice).In practice, certain libraries provide functions to compute both convolution and cross-correlation. For example, NumPy provides both the functions convolve and correlate to compute both the convolution and cross-correlation, respectively. If you execute the following piece of code (Python 3.7), you will get results that are consistent with my explanations above.However, NumPy is not really a library that provides out-of-the-box functionality to build CNNs.On the other hand, TensorFlow's and PyTorch's functions to build the convolutional layers actually perform cross-correlations. As I said above, although it does not really matter whether CNNs perform the convolution or cross-correlation, this naming is misleading. Here's a proof that TensorFlow's tf.nn.conv1d actually implements the cross-correlation.After having written this answer, I found the article Convolution vs. Cross-Correlation (2019) by Rachel Draelos, which essentially says the same thing that I am saying here, but provides more details and examples."
Looping over Sarsa algorithm for better Q values,"
Let's say an RL trading system places trades based on pricing data.
Each episode represents 1 hour of trading, and there are 24 hours of data available. The Q table represents for a given state, what is the most action with the highest utility.
The state is a sequence of prices, and the action is either buy, hold, sell.
Instead of ""Loop for each episode"" as per the Sarsa algorithm :

I add an additional outer loop. Now instead of just looping for each episode we have:
for 1 to N:     
   ""Loop for each episode""

Manually set N or exit out of the loop on convergence.
Is this the correct approach? Iterating multiple times over the episodes will produce more valuable state-action pairs in the Q table, because e greedy is not deterministic and for each iteration may exploit an action to greater reward than other episode epochs.
","['reinforcement-learning', 'algorithmic-trading']","Given that your state space is continuous, then I would recommend using Deep Q-Learning. As you say, running several episodes will definitely be beneficial so that the agent is able to explore the space more thoroughly."
How to understand mapping function of kernel?,"
For a kernel function, we have two conditions one is that it should be symmetric which is easy to understand intuitively because dot products are symmetric as well and our kernel should also follow this. The other condition is given below
There exists a map $φ:R^d→H$ called kernel feature map into some high dimensional feature space H such that $∀x,x'$ in $R^d:k(x,x') = <φ(x),φ(x')>$
I understand that this means that there should exist a feature map that will project the data from low dimension to any high dimension $D$ and kernel function will take the dot product in that space.
For example, the Euclidean distance is given as
$d(x,y)=∑_i(x_i−y_i)^2=<x,x>+<y,y>−2<x,y>$
If I look this in terms of second condition how do we know that doesn't exist any feature map for euclidean distance? What exactly are we looking in feature maps mathematically?
","['machine-learning', 'math', 'support-vector-machine', 'mapping-space']","A kernel function $f : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ is a valid support vector kernel if it is a Mercer kernel. Mercer's condition essentially ensures that the Gram matrix of the kernel is positive semi-definite. Interestingly, this ensures that the SVM objective is convex.The Euclidean distance function does not satisfy Mercer's condition since it's Gram matrix is not necessary positive semi-definite. Thus, it is not a valid kernel."
How to determine when the image is steady enough in a video sequence to take photos?,"
How do I calculate the points in a video sequence where the images are steady enough for a photo.  For example, I want to take maybe 20 photos for a facial recognition dataset.  Instead of asking the subject to hold still 20 times, I take a moving video of him and filter the images for 20 good photos.  The problem is that the subject face has to move into different poses and facial expressions and many times the movements blur the quality of the image.
Thanks in advance.
","['machine-learning', 'computer-vision', 'facial-recognition']",
How to measure sample efficiency of a reinforcement learning algorithm?,"
I want to know if there is any metric to use for measuring sample-efficiency of a reinforcement learning algorithm? From reading research papers, I see claims that proposed models are more sample efficient but how does one reach this conclusion when comparing reinforcement learning algorithms?
","['reinforcement-learning', 'sample-efficiency']","First of all, let's recall some definitions.A sample in the context of (deep) RL is a tuple $(s_t, a_t, r_t, s_{t+1})$ representing the information associated with a single interaction with the environment.As for sample efficiency, it is defined as follows [1]:Sample efficiency refers to the amount of data required for a learning system to attain any chosen target level of performance.So, how you measure it is closely related to the way it is defined.For example, one way to do it would be as shown in the figure below:On the y-axis you have the performance of your RL algorithm (e.g. in terms of average return over episodes as done in [2], or mean total episode reward across different environment runs as done in [3])On the x-axis you have the number of samples that you took.The dashed line corresponds to your performance baseline (e.g. the performance at which a certain game or any other RL environment is considered solved).So, you can measure sample efficiency at the intersection, where you get the number of samples needed to reach the performance baseline. So, an algorithm that requires less samples would be more sample-efficient.Another way to do it would be the other way around, i.e. the RL agent is provided with a limited budget for the number of samples it could take. As a result, you can measure sample efficiency my measuring the area under the curve, as illustrated below. So, that would be how much performance you got just by using those samples in the budget. An algorithm that achieves a higher performance than another with the same amount of samples would then be more sample efficient.I am not aware if there exist RL libraries that would provide you with this measure out-of-the-box. However, if you're using Python for example, I believe that using libraries like scipy or scikit-learn along with matplotlib could do the job.NB: Image credits go to the following presentation: DLRLSS 2019 - Sample Efficient RL - Harm Van Seijen"
Can imbalance data create overfitting?,"
I am doing human activity recognition project. I have total of 12 classes. The class distribution look like this:

$\color{red}{If \ you \ watch \ carefully, you \ can \ see \ that \ I \ have \ no \ data \ points \ for \ class \ 11 \ and \ class \ 8.}$ Also, the dataset is highly imbalanced. So, I took minimum data points (in this case 2028) for all of the classes. Now my balanced data look like this:

After doing this it looks like a balance data. $\color{red}{But \ still, \ I \ think \ it \ not, \ because \ I \ have \ zero \ datapoints \ for \ class \ 11 \ and \ class \ 8}$. In my opinion the classes are still imbalance.
I am using CNN model to solve this activity project. My model summary is following:

The main problem is, my model starts overfitting heavily when I train it.

Is it due to my imbalance data( class 8 and 11 has zero data points) or something else?
$\textbf{Hyperperameter:}$
$\textbf{features:}$ X, Y, Z of mobile accelerometer
$\textbf{frame size:}$ 80
$\textbf{optimizer:}$ Adam, $\textbf{Learning rate:}$ 0.001
$\textbf{Loss:}$ Sparse categorical cross-entropy
","['machine-learning', 'deep-learning', 'computer-vision']",
"In Deep Q-learning, are the target update frequency and the batch training frequency related?","
In a Deep Q-learning algorithm, we perform a batch training every train_freq and we update the parameters of the target network every target_update_freq. Are train_freq and target_update_freq necessary related, e.g., one should be always greater than the other, or must they be independently optimized depending on the problem?
EDIT Changed the name of batch_freq to train_freq.
","['reinforcement-learning', 'deep-rl']","It is fairly common in DQN to train a minibatch after every observation received after the replay memory has enough data (how much is enough is yet another parameter). This is not necessary, and it is fine to collect more data between training steps, the alogrithm is still DQN. The value higher than 1 for train_freq here might be related to use of prioritised replay memory sampling - I have no real experience with that.The update to target network generally needs to occur less frequently than training steps, it is intended to stabilise results numerically, so that over or under estimates of value functions do not result in runaway feedback.The parameters choices will interact each other, most hyperparameters in machine learning do so unfortunately. Which makes searching for ideal values fiddly and time-consuming.In this case it is safe to say that train_freq is expected to be much lower than target_update_freq, probably by at least an order of magnitude, and more usually 2 or 3 orders of magnitude. However, that's not quite the same as saying there is a strong relationship between choices for those two hyperparameters. The value of batch_size is also relevant here, as it shows the rate that memory is being used (and re-used) by the training process.The library you are using has these defaults:They seem like sane starting points. You are relatively free to change them as if they were independent, as there is no simple rule like ""target_update_freq should be 125 times train_freq"". As a very rough guide, you can expect that high values of train_freq, low values of batch_size and low values of target_update_freq are likely to cause instability in the learning process, whilst going too far in the opposite direction may slow learning down. You might be able to set train_freq to 1, but I am not completely certain about that either in combination with the prioritised replay memory sampling which seems to be the default in the library you are using."
"Why should the baseline's prediction be near zero, according to the Integrated Gradients paper?","
I am trying to understand Intagrated Gradients, but have difficulty in understanding the authors' claim (in section 3, page 3):

For most deep networks, it is possible to choose a baseline such that the prediction at the baseline is near zero ($F(x') \approx 0$). (For image models, the black image baseline indeed satisfies this property.)

They are talking about a function $F : R^n \rightarrow [0, 1]$ (in 2nd paragraph of section 3), and if you consider a deep learning classification model, the final layer would be a softmax layer. Then, I suspect for image models, the prediction at the baseline should be close to $1/k$, where $k$ is the number of categories. For CIFAR10 and MNIST, this would equal to $1/10$, which is not very close to $0$. I have a binary classification model on which I am interested in applying the Integrated Gradients algorithm. Can the baseline output of $0.5$ be a problem?
Another related question is, why did they choose a black image as the baseline in the first place? The parameters in image classification models (in a convolution layer) are typically initialized around $0$, and the input is also normalized. Therefore, image classification models do not really care about the sign of inputs. I mean we could multiply all the training and test inputs with $-1$, and the model would learn the task equivalently. I guess I can find other neutral images other than a black one. I suppose we could choose a white image as the baseline, or maybe the baseline should be all zero after normalization?
","['deep-learning', 'image-recognition', 'papers']","You are right that the baseline score is near zero only when there are a large number of label classes, i.e., when k is large. We should have qualified this line in the paper more carefully.In this sense, formally, the technique explains the *difference in prediction between the input score and the baseline score, as is made clear elsewhere in the paper (see Remark 1 and Proposition 1 for instance.)"
"What would be the good choice of algorithm to use for character action selection in an RPG, implemented in Python?","
I have developed an RPG in likeness to the features showcased in the Final Fantasy series; multiple character classes which utilise unique action sets, sequential turn-based combat, front/back row modifiers, item usage and a crisis mechanic which bears similarity to the limit break feature.
The problem is that the greater portion of my project depends on the use of some means of Machine Learning to, in some manner, act as an actor in the game environment, however, I do not know of my options under the bare-bones environment of a command line game; I am more familiar with the use of pixel data and a neural network for action selection on a frame-by-frame basis.
Could I use reinforcement learning to learn a policy for action selection under a custom environment or should I apply a machine learning algorithm to character data, find the example outlined below, determine the best action to use on a particular turn state?
+-------+--------+--------+---------+---------+------------+---------------------+------------+--------------+--------------------+--------------+--------------+
| Level | CurrHP | CurrMP | AtkStat | MagStat | StatusList | TargetsInEnemyParty | IsInCrisis | TurnNumber   | BattleParams       |ActionOutput  | DamageOutput |
+-------+--------+--------+---------+---------+------------+---------------------+------------+--------------+--------------------+--------------+--------------+
|    65 |   6500 |    320 |      47 |      56 |          0 | Behemoth            |0           | 7            | None               |ThiefAttack   |4254          |
|    92 |   8000 |    250 |      65 |      32 |          0 | Behemoth            |1           | 4            | None               |WarriorLimit  |6984          |
+-------+--------+--------+---------+---------+------------+---------------------+------------+--------------+--------------------+--------------+--------------+

I would like to prioritise the ease of implementation of an algorithm over how optimal the potential algorithm could be, I just need a baseline to work towards. Many thanks.
","['machine-learning', 'reinforcement-learning', 'game-ai', 'getting-started', 'gaming']",
Designing a reward function for my reinforcement learning problem,"
I'm working on a project lately and I'm trying to solve a problem with reinforcement learning and I have serious issues with shaping the reward function.
The problem is designing a device with maximum efficiency. So we simulated the problem as follows. There is a 4x4 grid (we defined a 4x4 matrix) and the elements of this matrix can be either 0 or 1 (value 0 means ""air"" and 1 means a certain material in reality), so there is a 2^16 possible configurations for this matrix. Our agent starts from the top left corner of this matrix and has 5 possible actions: move up, down, left, right and flip (which means flipping a 0 to 1 or vice versa). Based on flipping action, we get a new configuration and each configuration has an efficiency (which is calculated by maxwell equations in the background).
Our goal is to find the best configuration so that the efficiency of the device is maximum.
So far we have tried many reward functions and non of them seemed to work at all! I will mention some of them:

reward = current_efficiency - previous_efficiency (the efficiency is being calculated in each time step)


 if current_efficiency > previous_efficiency:
    reward = current_efficiency
    previous_efficiency = current_efficiency




 diff = current_efficiency - previous_efficiency
 if diff > 0:
    reward = 1
 else:
    reward = -2




and some other variations. Nothing is working for our problem and the agent doesn't learn at all! So far, we have used different approaches to DQN and also A2C method and so far no positive feedback. We tried different definitions of states as well, but we don't think that is the problem.
So, can somebody maybe help me with this? It would be a huge help!
","['machine-learning', 'reinforcement-learning', 'deep-learning', 'dqn', 'reward-design']",
How to combine several chatbots into one?,"
I'm in the middle of a project in which I want to generate a TV series script (characters answering to each other, scene by scene) using SOTA models, and I need some guidance to simplify my architecture.
My current intuition is as follows: for a given character C1, I have pairs of sentences from the original scripts where C1 answers other characters, for example, C2 (C2->C1). These are used to fine-tune a data-driven chatbot. At inference time, the different chatbots simply answer each other, and, hopefully, the conversation will have some sense.
This is however unpractical and will be kind of a mess with many characters, especially if I use heavy models.
Is there an architecture out there that could be used for conversational purposes, which could be trained only once with the whole dataset while separating the different characters?
I'm open to any ideas!
","['neural-networks', 'training', 'chat-bots', 'generative-model']","What you want to do is you have some character like hulk, thor, caption America, iron man, etc. and you want to Train a response generator for each character like for thor on his dataset, for hulf on his dataset etc... and then you wanted to make a conversation. If I Understood you well then.You can fine-tune GPT-2 small or GPT-2 Medium using your Dataset for each character. Reference for FinetuneYou can decode using simple nucleus sampling at each time step. Use greedy nucleus sampling multiple times in parallel to generate multiple responses. You can generate 30 such responses and also use the last 3-5 dialogue turn as a context or Short term memory. Decoder referenceTo Choose the best response out of all generated responses(30 responses as an example). you can use cosine similarity between generated responses and query + last 3-5 dialogue turn.Or you can train a reverse model for either all character or mix all character dialogue and use this model to calculate loss of all responses with your query. Reference reverse modelOr you can Combine result of cosine and reverse model to find response out of all the responses."
What is the relationship between gradient accumulation and batch size?,"
I am currently training some models using gradient accumulation since the model batches do not fit in GPU memory. Since I am using gradient accumulation, I had to tweak the training configuration a bit. There are two parameters that I tweaked: the batch size and the gradient accumulation steps. However, I am not sure about the effects of this modification, so I would like to fully understand what is the relationship between the gradient accumulation steps parameter and the batch size.
I know that when you accumulate the gradient you are just adding the gradient contributions for some steps before updating the weights. Normally, you would update the weights every time you compute the gradients (traditional approach):
$$w_{t+1} = w_t - \alpha \cdot \nabla_{w_t}loss$$
But when accumulating gradients you compute the gradients several times before updating the weights (being $N$ the number of gradient accumulation steps):
$$w_{t+1} = w_t - \alpha \cdot \sum_{0}^{N-1} \nabla_{w_t}loss$$
My question is: What is the relationship between the batch size $B$ and the gradient accumulation steps $N$?
By example: are the following configurations equivalent?

$B=8, N=1$: No gradient accumulation (accumulating every step), batch size of 8 since it fits in memory.
$B=2, N=4$: Gradient accumulation (accumulating every 4 steps), reduced batch size to 2 so it fits in memory.

My intuition is that they are but I am not sure. I am not sure either if I would have to modify the learning rate $\alpha$.
","['comparison', 'gradient-descent', 'stochastic-gradient-descent', 'gradient', 'batch-size']","There isn't any explicit relation between the batch size and the gradient accumulation steps, except for the fact that gradient accumulation helps one to fit models with relatively larger batch sizes (typically in single-GPU setups) by cleverly avoiding memory issues. The core idea of gradient accumulation is to perform multiple backward passes using the same model parameters before updating them all at once for multiple batches. This is unlike the conventional manner, where the model parameters are updated once every batch-size number of samples. Therefore, finding the correct batch-size and accumulation steps is a design trade-off that has to be made based on two things: (i) how much increase in the batch-size can the GPU handle, and (ii) whether the gradient accumulation steps result in at least as much better performance than without accumulation.As for your example configurations, there are the same in theory. But, there are a few important caveats that need to be addressed before proceeding with this intuition.Here are some interesting resources to find out more about it detail."
What are the main algorithms used in computer vision?,"
Nowadays, CV has really achieved great performance in many different areas. However, it is not clear what a CV algorithm is.
What are some examples of CV algorithms that are commonly used nowadays and have achieved state-of-the-art performance?
","['computer-vision', 'terminology', 'algorithm', 'definitions', 'image-processing']","There are many computer vision (CV) algorithms and models that are used for different purposes. So, of course, I cannot list all of them, but I can enumerate some of them based on my experience and knowledge. Of course, this answer will only give you a flavor of the type of algorithm or model that you will find while solving CV tasks.For example, there are algorithms that are used to extract keypoints and descriptors (which are often collectively called features, although the descriptor is the actual feature vector and the keypoint is the actual feature, and in deep learning this distinction between keypoints and descriptors does not even exist, AFAIK) from images, i.e. feature extraction algorithms, such as SIFT, BRISK, FREAK, SURF or ORB. There are also edge and corner detectors. For example, the Harris corner detector is a very famous corner detector.Nowadays, convolutional neural networks (CNNs) have basically supplanted all these algorithms in many cases, especially when enough data is available. Rather than extracting the typical features from an image (such as corners), CNNs extract features that are most useful to solve the task that you want to solve by taking into account the information in the training data (which probably includes corners too!). Hence CNNs are often called data-driven feature extractors. There are different types of CNNs. For example, CNNs that were designed for semantic segmentation (which is a CV task/problem), such as the u-net, or CNNs that were designed for instance segmentation, such as mask R-CNN.There are also algorithms that can be used to normalize features, such as the bag-of-features algorithm, which can be used to create fixed-size feature vectors. This can be particularly useful for tasks like content-based image retrieval.There are many other algorithms that could be considered CV algorithms or are used to solve CV tasks. For example, RanSaC, which is a very general algorithm to fit models to data in the presence of outliers, can be used to fit homographies (matrices that are generally used to transform planes to other planes) that transform pixels of one image to another coordinate system of another image. This can be useful for the purpose of template matching (which is another CV task), where you want to find a template image in another target image. This is very similar to object detection.There are also many image processing algorithms and techniques that are heavily used in computer vision. For example, all the filters (such as Gaussian, median, bilateral, non-local means, etc.) that can be used to smooth, blur or de-noise images. Nowadays, some deep learning techniques have also replaced some of these filters and image processing techniques, such as de-noising auto-encoders.All these algorithms and models have something in common: they are used to process images and/or get low- or high-level information from images. Most of them are typically used to extract features (i.e. regions of the images that are relevant in some way) from images, so that they can later be used to train a classifier or regressor to perform some kind of task (e.g. find and distinguish the objects, such people, cars, dogs, etc. in an image). The classifiers/regressors are typically machine learning (ML) models, such as SVMs or fully-connected neural networks, but there's a high degree of overlap between CV and ML because some ML tools are used to solve CV tasks (e.g. image classification)."
Doesn't the number of explored nodes with IDA* increase linearly?,"
I think I'm misunderstanding the description of IDA* and want to clarify.
IDA* works as follows (quoting from Wiki):

At each iteration, perform a depth-first search, cutting off a branch when its total cost exceeds a given threshold. This threshold starts at the estimate of the cost at the initial state, and increases for each iteration of the algorithm. At each iteration, the threshold used for the next iteration is the minimum cost of all values that exceeded the current threshold.

Suppose that we have the following tree:

branching factor = 5
all cost are different

Say we have expanded 1000 nodes. We pick the lowest cost of the nodes that we 'touched' but didn't expand. Since all costs are unique, there is now only one more node which satisfies this new cost bound, and so we expand 1001 nodes, and 'touch' 5 new ones. We now pick the smallest of these weights, and starting from the root expand 1002 nodes, and so on and so forth, 1003, 1004...
I must be doing something wrong here right? If not, the complexity is $n^2$, where n is the number of nodes with cost smaller than the optimum, compared to n for normal A*.
Someone pointing out my misunderstanding would be greatly appreciated.
","['search', 'a-star', 'ida-star']","No misunderstanding. IDA* is indeed n^2 vs n for A* in certain situations.See
https://pdfs.semanticscholar.org/388c/0a934934a9e60da1c22c050566dbcd995702.pdf
for a reference.IDA* only works in certain (not uncommon) situations. If there are many states with the same costs, and this number grows exponentially with the cost, then IDA* will be O(n). An example of such a problem is the 8 sliding piece puzzle.IDA* will however, as noted, become exponential when all costs differ. Examples include continuous costs, or discrete values spanning a large range.If you really wanted to apply IDA* to e.g. a map, a workable approach would likely be to round all values - or perform a log+floor operation."
"Connection between the Bellman equation for the action value function $q_\pi(s,a)$ and expressing $q_\pi(s,a) = q_\pi(s, a,v_\pi(s'))$","
When deriving the Bellman equation for $q_\pi(s,a)$, we have
$q_\pi(s,a) = E_\pi[G_t | S_t = s, A_t = a] = E_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$ (1)
This is what is confusing me, at this point, for the Bellman equation for $q_\pi(s,a)$, we write $G_{t+1}$ as an expected value, conditioned on $s'$ and $a'$ of the action value function at $s'$, otherwise, there is no recursion with respect to $q_\pi(s,a)$, and therefore no Bellman equation. Namely,
$ = \sum_{a \in A} \pi(a |s) \sum_{s' \in S} \sum_{r \in R} p(s',r|s,a)(r + \gamma E_\pi[G_{t+1}|S_{t+1} = s', A_{t+1} = a'])$ (2)
which introduces the recursion of $q$,
$ = \sum_{a \in A} \pi(a |s) \sum_{s' \in S} \sum_{r \in R} p(s',r|s,a)(r + \gamma q_\pi(s',a'))$ (3)
which should be the Bellman equation for $q_\pi(s,a)$, right?
On the other hand, when connecting $q_\pi(s,a)$ with $v_\pi(s')$, in this answer, I believe this is done
$q_\pi(s,a) = \sum_{a\in A} \pi(a |s) \sum_{s' \in S}\sum_{r \in R} p(s',r|s,a)(r + \gamma E_{\pi}[G_{t+1} | S_{t+1} = s'])$ (4)
$q_\pi(s,a) = \sum_{a\in A} \pi(a |s) \sum_{s' \in S}\sum_{r \in R} p(s',r|s,a)(r + \gamma v_\pi(s'))$ (5)
Is the difference between using the expectation $E_{\pi}[G_{t+1} | S_{t+1} = s', A_{t+1} = a']$ in (3) and the expectation $E_{\pi}[G_{t+1} | S_{t+1} = s']$ in $(4)$ simply the difference in how we choose to express the expected return $G_{t+1}$ at $s'$ in the definition of $q_\pi(s,a)$?
In $3$, we express the total return at $s'$ using the action value function

leading to the recursion and the Bellman equation, and in $4$, the total return is expressed at $s'$ using the value function

leading to $q_\pi(s,a) = q_\pi(s,a,v_\pi(s'))$?
","['reinforcement-learning', 'value-functions', 'bellman-equations']","Your understanding of the Bellman equation is not quite right. The state-action value function is defined as the expected (discounted) returns when taking action $a$ in state $s$. Now, in your equation (2) you have conditioned on taking action $a'$ in the inner expectation - this is not what happens in the state-action value function, you do not condition on knowing $A_{t+1}$, it is chosen according to the policy $\pi$ as per the definition of a bellman equation.If you want to see a 'recursion' between state action value functions, note that$$v_\pi(s) = \sum_a \pi(a|s)q_\pi(s,a)\;,$$Your equation (5) is incorrect -- you need to drop the outter sum over $a$ as we have conditioned on knowing $a$. I will drop the $\pi$ subscripts for ease on notation, and we can see a 'recursion' for state-action value functions as:$$q(s,a) = \sum_{s',r}p(s',r|s,a)\left(r + \gamma \left[\sum_{a'} \pi(a'|s')q(s',a')\right]\right)\;.$$"
"How to express $v_\pi(s)$ in terms of $q_\pi(s,a)$?","

This is exercise 3.18 in Sutton and Barto's book.
The task is to express $v_\pi(s)$ using $q_\pi(s,a)$.
Looking at the diagram above, the value of $q_\pi(s,a)$ at $s$ for each $a \in A$ we take gives us the value function at $s$ after taking the action $a$ and then following the policy $\pi$.
This is probably wrong, but if
$$v_\pi(s) = E_\pi[G_t | S_t = s]$$
and
$$q_\pi(s) = E_\pi[G_t | S_t = s, A_t = a]$$
isn't then $v_\pi(s)$ just the expected action value function at $s$ over all actions $a$ that are given by the policy $\pi$, namely
$$v_\pi(s) = E_{a \sim \pi}[q_\pi(s,a) | S_t = s, A_t = a] = \sum_{a \in A}\pi(a|s) q_\pi(s,a)$$?
","['reinforcement-learning', 'comparison', 'value-functions', 'sutton-barto']","isn't then $v_\pi(s)$ just the expected action value function at $s$ over all actions $a$ that are given by the policy $\pi$, namely$v_\pi(s) = E_{a \sim \pi}[q_\pi(s,a) | S_t = s, A_t = a] = \sum_{a \in A}\pi(a|s) q_\pi(s,a)$?Yes this is 100% correct.There is no ""trick"" to this or deeper thought needed. You have correctly isolated the key part of the MDP description that controls relationship between $v_{\pi}$ and $q_{\pi}$ in that direction.Note that for a deterministic policy, with $\pi(s): \mathcal{S} \rightarrow \mathcal{A}$ then the relationship is$$v_\pi(s) = q_\pi(s, \pi(s))$$The related exercise in the book - expressing $q_{\pi}$ in terms of $v_{\pi}$ and the MDP characteristics - is more complex because it involves a time step."
What is the equation of the learning rate decay in the Adam optimiser?,"
Adam is known as an algorithm that has an adaptive learning rate for each parameter. I believe this is due to the division by the term $$v_t = \beta_2 \cdot v_{t-1} + (1-\beta_2) \cdot g_t^2 $$ Hence, each weight will get updated differently based on the accumulated squared gradients in their respective dimensions, even though $\alpha$ might be constant. There are other StackOverflow posts that have said that Adam has a built-in learning rate decay. In the original paper also, the authors of adam paper says that the learning rate at time step $t$ decays based on the equation $$\alpha_t = \alpha \cdot \frac{\sqrt{1-\beta_2^t}}{{1-\beta_1^t}}$$
Is the second equation the learning rate decay that has been built into the Adam algorithm?
","['machine-learning', 'deep-learning', 'gradient-descent', 'learning-rate', 'adam']",
What is a convolutional neural network?,"
Given that this question has not yet been asked on this site, although similar questions have already been asked in the past (e.g. here or here), what is essentially a convolutional neural network (CNN)? Why are they heavily used in image recognition applications of machine learning?
","['neural-networks', 'convolutional-neural-networks', 'computer-vision', 'definitions', 'image-processing']","(Of course, similar questions have been asked in the past and there are many sites, papers, video lessons, online that explain how CNNs work, but I think it's still a good idea to have a reference answer that hopefully will give you the main ideas behind CNNs.)A convolutional neural network (CNN) is a neural network that performs the convolution (or cross-correlation) operation typically followed by some downsampling (aka pooling) operations.The convolution operation comes from the mathematical equivalent operation, which is an operation that takes as inputs two functions $f$ and $h$ and produces another function $g$ as output, which is often denoted as $f \circledast h = g$, where $\circledast$ is the convolution operation (or operator).In image processing, the convolution is used to process images in multiples ways. For example, it is used to remove noise (e.g. the convolution of a noisy image with a Gaussian kernel produces a smoother image) or to compute derivatives of the image, which can then be used e.g. to detect edges or corners, which are usually the main features of an image. For example, the Harris corner detector makes use of the partial derivatives (in the $x$ and $y$ direction) of the image to find corners (or interest points) in the image.In the context of CNNs, $f$ is the image (in the case of the first layer of the CNN) or a so-called feature map (in the case of hidden layers), $h$ is the kernel (aka filter) and $g$ is also a feature map. (In this answer, I explain these concepts, including how an image can be viewed as a function, more in detail, so I suggest that you read it).Here's an illustrative example of how the convolution works.where the $\color{blue}{\text{blue}}$ grid is the image, the $\color{gray}{\text{gray}}$ grid is the kernel and $\color{green}{\text{green}}$ grid is the feature map (i.e. the output of the convolution between the image and the kernel). The white squares are around the images represent the padding that is added around the image so that the convolution operation produces a feature map of a specific dimension.Essentially, the convolution is a series of dot products between the kernel and different patches (or parts) of the image. The convolution can even be represented as matrix multiplication, so the name convolution shouldn't scare you anymore, if you are familiar with dot products and matrix multiplications.As opposed to many operations in image processing where the kernels are typically fixed, in the context of CNNs, the kernels are learnable parameters, i.e. they change depending on the loss function, so they are supposed to represent functions that when convolved with their respective input functions are useful to extract meaningful information (i.e. features) to solve the task the CNN is being trained to solve. For this reason, the convolution is often thought of as an operation that extracts features from images. In fact, the output of the convolution, in the context of CNNs, is often called feature map. Moreover, a CNN is typically thought of as a data-driven feature extractor for the same reason.There are many different variations of the standard convolution operation. For example, there are transposed convolutions, dilated convolutions, etc., which are used to solve slightly different problems. For example, the dilated convolution can be used to increase the receptive field of an element of a feature map (which is typically a desirable property for several reasons). There are also upsampling operations which are particularly useful in the context of image segmentation. These different convolutions and their arithmetic are explained very well in the paper A guide to convolution arithmetic for deep learning by Vincent Dumoulin and Francesco Visin. Here's also the associated Github repository that contains all the images in this paper that show how the convolution operations work (from which I also took the gif above).To conclude, CNNs are very useful to process images and extract features from them because they use convolution operations (and downsampling and upsampling operations). They can be used for image (or object) classification, object detection (i.e. object localization with a bounding box + object classification), image segmentation (including semantic segmentation and instance segmentation), and possibly many other tasks where you need to learn a function that takes as input images and needs to extract information from those images to get someone high-level (but also low-level) output (e.g. the name of the object in the image), given some training data."
Additional (Potential) Action for Agent in MazeGrid Environment (Reinforcement Learning),"
In a classic GridWorld Environment where the possible actions of an agent are (Up, Down, Left, Right), can another potential output of Action be ""x amount of steps"" where the agent takes 2,3,.. steps in the direction (U,D,L,R) that it chooses? If so, how would one go about doing it?
",['reinforcement-learning'],"You can definitely define an environment that accepts more types of action, including actions that take multiple steps in a direction.The first thing you would need to do is implement support for that action in the environment. That is not really a reinforcement learning issue, but like implementing the rules of a board game. You will need to decide things such as what happens if the move would be blocked - does the move succeed up the point of being blocked, does it fail completely, is the reward lower depending on how much the agent tries to overshoot, etc.After you do that, you will want to write an agent that can choose the new actions. You have a few choices here:Simplest would be to enumerate all the choices separately and continue to use the same kind of agent as you already have. So instead of $\{U, D, L, R\}$ you might have $\{U1, U2, U3, D1, D2, D3, L1, L2, L3, R1, R2, R3\}$.If you want to take advantage of generalisation between similar actions (e.g. that action $U3$ is similar to $U2$ and also to $R3$), then you can use some form of coding for the action, such as the relative x,y movement that it is attempting. So you could express $U2$ as $(0,2)$ and $L3$ as $(-3,0)$. For that to then work with Q values, you cannot easily use a table. Instead, you would need to use function approximation, for example a neural network, so you can implement $q(s,a)$ as a parametric function - combining $s,a$ into the input vector, and learn the parameters to that the neural network outputs the correct action value. This is what the Q learning variation DQN can do, as well as other similar RL algorithms that use neural networks.Using a neural network, instead of tabular Q-learning, is not something you see often with grid world environments. It is a step up in complexity, but it is often required if state space or action space becomes large and might benefit from the generalisation possible from trainable function approximators."
Why can't we fully exploit the environment after the first episode in Q-learning?,"
During the first episode, it's 100% exploration, because all our Q values are 0. Suppose we have 1000 time steps, and it's terminated by meeting a reward. So, after the first episode, why can't we make it 100% exploitation? Why do we still need exploration?
","['reinforcement-learning', 'q-learning', 'exploration-exploitation-tradeoff']","You can't guarantee that you have taken every action from every state, even with  1000 time steps. There would be multiple outcomes:The episode terminates, either by success or failure before the 1000
time steps. The agent is trying to maximise reward, if this is achieved by taking less than 1000 steps then it will do. It won't just walk around until it hits an arbitrary number of time steps.If you have more states than time steps then you will
never be unable to visit all states and so you cannot guarantee that
the policy you followed was optimal (and hence would still want to explore). Even if you have #states = #time-steps then you will almost certaintly have more state-action pairs than timesteps. The only time this would be equal is if from every state there is only one action, which would be a trivial problem that wouldn't need RL to solve."
How to make spacy lemmatization process fast? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I am applying spacy lemmatization on my dataset, but already 20-30 mins passed and the code is still running. 
Is there anyway to make it faster? Is there any option to do this process using GPU?
My dataset size is 20k number of rows & 3 columns 
","['natural-language-processing', 'python', 'spacy']","https://spacy.io/api/lemmatizer just uses lookup tables and the only upstream task it relies on is POS tagging, so it should be relatively fast. For large amounts of text, SpaCy recommends using nlp.pipe, which can work in batches and has built in support for multiprocessing (with the n_process keyword), rather than than simply nlp.Also, make sure you disable any pipeline elements that you don't plan to use, as they'll just waste processing time. If you're only doing lemmatization, you'll pass disable=[""parser"", ""ner""] to the nlp.pipe call.Example code that takes all of the above into account is below."
How do GPUs faciliate the training of a Deep Learning Architecture?,"
I would love to know in detail, how exactly GPUs help, in technical terms, in training the deep learning models.
To my understanding, GPUs help in performing independent tasks simultaneously to improve the speed. For example, in calculation of the output through CNN, all the additions are done simultaneously, and hence improves the speed.
But, what exactly happens in a basic neural network or in a LSTM type complex models in regard with GPU.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'long-short-term-memory', 'gpu']","GPUs are able to execute a huge amount of similar and simple instructions (floating point operations like addition and multiplication) in parallel. In contrast to a CPU which is able to execute a few complex tasks sequentially very quick. Therefore GPUs are very good at doing vector & matrix operations.If you look at the operations performed inside a single basic NN layer you will see that most of the operations are matrix-vector multiplications:$$x_{i+1} = \sigma(W_ix_i + b_i)$$where $x_i$ is the input vector, $W_i$ the matrix of weights, $b_i$ the bias vector in the $i^{th}$ layer, $x_{i+1}$ the output vector and $\sigma(\cdot)$ the elemtwise non-linear activation. The computational complexity here is governed by the matrix-vector multiplication. If you look at the architecture of a LSTM cell you will notice that inside of it are multiple such operations.Being able to execute the matrix-vector operations quickly and efficiently in parallel will reduce execution time, this is where GPUs excel CPUs."
Can reinforcement learning algorithms be applied on problems involving a very large number of possible actions?,"
There is a question already about applying RL to ""large scale problems"", where large scale refers to the problem of a relatively small number of actions (that could be from a continous space) resulting in a very large number of states. 
A good exapmle of this kind of large-scale probems is modeling a motion of a boat as a point on a plane, with an action being a displacement vector $\mathbf{\delta}_b = (\delta_x, \delta_y)$ and there are infinitely many states, because the next state is given by the next position of the boat, in a circle surrounding the boat $\mathcal{B}(\mathbf{x}_b, \mathbf{\delta}_{b,max})$, where $\mathbf{x_b}$ is the boat's current position, and $\mathbf{\delta}_{b,max}$ the maximal possible displacement. So here, the displacement as an action (move the boat) is from an infinite space because it is a 2D vector ($\delta_b \in \subset \mathbb{R}^2$) and so is the state space $\mathcal{B}$. Still, I just have two actions to apply to the boat in the end: move in x-directions this much, and in y-direction that much.  
What I mean, is something even larger. Considering the example of a boat, is it possible to apply reinforcement learning on a system that has 100 000 of such boats, and what would be the methods to look into to accomplish this. I do not mean to have 100 000 agents. The agent in this scenario is observing 100 000 boats, they are its environment, and let's say the agent is distributing them in a current on the sea in such a way that they have the least amount of resistance in the water (the wake of one ship influences the resistance of its downstream neighbors). 
From this answer and from what I have read so far, I believe an approximation will be necessary for the displacements in $2D$ space $\mathbf{\delta}(x,y)$ as well as for the states and rewards, because there are so many of them. However, before digging into this, I would like to know if there are some references out there where something like this has already been tried, or if this is simply something where RL cannot be applied. 
","['reinforcement-learning', 'reference-request']",
What is Reinforcement Learning?,"
What is the cleanest, easiest way to explain someone who is a non-STEM work colleague the concept of Reinforcement Learning? What are the main ideas behind Reinforcement Learning?
","['reinforcement-learning', 'terminology', 'definitions', 'academia']","Humans are set loose in the world and go about their days doing stuff.Whenever they do specific things, their brain sends them good signals (endorphins, joy, etc.) or bad signals (pain, sadness, etc.). They learn through these signals which things they should be doing and which things they shouldn't be doing.Sometimes the signal is immediate and you know exactly what you're being ""rewarded"" or ""punished"" for (e.g. touch a hot stove and it hurts). Sometimes it takes a bit longer and there could be many possible reasons for the brain signal (even a combination of reasons), but you can hopefully figure out what caused it after it happens a few times (e.g. getting a stomach ache a few hours after eating a specific food).That's basically what Reinforcement Learning is."
How to use speaker's information as well as text for fine-tuning BERT?,"
I want to classify my corporate chat messages into a few categories such as question, answer, and report. I used a fine-tuned BERT model, and the result wasn't bad. Now, I started thinking about ways to improve it, and a rough idea came up, but I don't know what to do it exactly.
Currently, I simply put chat text into the model, but don't use the speaker's information (who said the text, the speaker's ID in our DB). The idea is if I can use the speaker's information, the model might better understand the text and classify it better.
The question is, are there any examples or prior researches similar to what I want to achieve? I googled for a few hours, but couldn't find anything useful. (Maybe the keywords weren't good.)
Any advice would be appreciated.
","['natural-language-processing', 'classification', 'bert']",
How to split data into training validation and test set when the number of data in classes varies greatly?,"
I have 5 classes of pictures to classify:
0 -> ~3 200 (~800 initial number before interference and duplication)
1 -> ~9 000 (I reduced from ~90 000)
2 -> ~8 000
3 -> ~3 000
4 -> ~7 200
How to divide the data? 
Now I have divided the data giving 2 000 to test and 2 000 to validation set 
by taking a fixed number of images (400) from each class. I don't have much knowledge so I don't know if this is a good division of data. 
The attached picture shows the results on the test data after about 60 epoch of CNN with 15 layers. 

The network continues to overfiting, and the results of validation and test set do not improve. 
I know that I could definitely improve my model but I would like to divide the data in some thoughtful and reasonable way. Pictures are spectrograms and are in RGB format.
","['deep-learning', 'classification', 'training', 'datasets']","I would make the distribution of the classes in test and validation sets the same as in the training set (and as in the whole original set). Anyway all your metrics are relative, not absolute and designed to provide reasonable results when classes are not ideally balanced."
How can I model this problem of delivering assets by choosing a route with reinforcement learning?,"
I would like to build a model based on reinforcement learning (RL) for the following scenario

Recommend the best route (of cities listed for a given country) that satisfies the required criteria (museum, beaches, food, etc) for a total budget of $2000.

Based on the recommendation, the user will provide its feedback (as a reward), so the recommendations can be fine-tuned (by reinforcement learning) the next time. I modeled the system this way:

States = (c,cr), where $c$ is the city and $cr$ is the criteria (history, beach, food, etc)

Actions = (p) is the price of visiting the city

Reward: acceptance of the cities selected by end user as a route (1 or 0)


The objective is to decide which list of cities together satisfy the
given budget.
Is this MDP model right and how can I implement this? May be the only option is using Monte Carlo methods and linear/dynamic programming.. Is there any other way?
","['reinforcement-learning', 'q-learning', 'optimization']",
Is it harmful to set the learning rate of training a model to be too high if there is some decay function for the learning rate?,"
It is known that if $\alpha$ is set to high, then the cost function of the model may not converge.  
However, would a decaying of the learning rate provide some ""tuning"" of the $\alpha$ value during training ? In the sense that if you set a high learning rate but you also have some form of learning rate decay, then eventually $\alpha$ value would fall within the ""just right"" and ""too low"" range eventually. Is it better to then set an initial learning rate that is more ""flexible"" in the higher ranges rather than a learning rate that is too low ?
","['machine-learning', 'deep-learning', 'learning-rate']",
Why am I getting better performance with Thompson sampling than with UCB or $\epsilon$-greedy in a multi-armed bandit problem? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 3 years ago.







                        Improve this question
                    



I ran a test using 3 strategies for multi-armed bandit: UCB, $\epsilon$-greedy, and Thompson sampling.
The results for the rewards I got are as follows:

Thompson sampling had the highest average reward
UCB was second
$\epsilon$-greedy was third, but relatively close to UCB

Can someone explain to me why the results are like this?
","['reinforcement-learning', 'multi-armed-bandits', 'thompson-sampling', 'upper-confidence-bound', 'epsilon-greedy-policy']",
What are bag-of-features in computer vision?,"
In computer vision, what are bag-of-features (also known as bag-of-visual-words)? How do they work? What can they be used for? How are they related to the bag-of-words model in NLP?
","['computer-vision', 'feature-extraction', 'features', 'bag-of-features', 'content-based-image-retrieval']","Bag-of-features (BoF) (also known as bag-of-visual-words) is a method to represent the features of images (i.e. a feature extraction/generation/representation algorithm). BoF is inspired by the bag-of-words model often used in the context of NLP, hence the name. In the context of computer vision, BoF can be used for different purposes, such as content-based image retrieval (CBIR), i.e. find an image in a database that is closest to a query image.The BoF can be divided into three different steps. To understand all the steps, consider a training dataset $D = \{x_1, \dots, x_N \}$ of $N$ training images. Then BoF proceeds as follows.In this first step, we extract all the raw features (i.e. keypoints and descriptors) from all images in the training dataset $D$. This can be done with SIFT, where each descriptor is a $128$-dimensional vector that represents the neighborhood of the pixels around a certain keypoint (e.g. a pixel that represents a corner of an object in the image). If you are not familiar with this extraction of computer vision (sometimes known as handcrafted) features, you should read the SIFT paper, which describes a feature (more precisely, keypoint and descriptor) extraction algorithm.Note that image $x_i \in D$ may contain a different number of features (keypoints and descriptors) than image $x_j \neq x_i \in D$. As we will see in the third step, BoF produces a feature vector of size $k$ for all images, so all images will be represented by a fixed-size vector. Let $F= \{f_1, \dots, f_M\}$ be the set of descriptors extracted from all training images in $D$, where $M \gg N$. So, $f_i$ may be a descriptor that belongs to any of the training examples (it does not matter which training image it belongs to).In this step, we cluster all descriptors $F= \{f_1, \dots, f_M\}$ into $k$ clusters using k-means (or another clustering algorithm). This is sometimes known as the vector quantization (VQ) step. In fact,  the idea behind VQ is very similar to clustering and sometimes VQ is used interchangeably with clustering. So, after this step, we will have $k$ clusters, each of them associated with a centroid $C = \{ c_1, \dots, c_k\}$, where $C$ is the set of centroids (and $c_i \in \mathbb{R}^{128}$ in the case that SIFT descriptors have been used). These centroids represent the main features that are present in the whole training dataset $D$. In this context, they are often known as the codewords (which derives from the vector quantization literature) or visual words (hence the name bag-of-visual-words). The set of codewords $C$ is often called codebook or, equivalently, the visual vocabulary.In this last step, given a new (test) image $u \not\in D$ (often called the query image in this context of CBIR), then we will represent $u$ as a $k$-dimensional vector (where $k$, if you remember, is the number of codewords) that will represent its feature vector. To do that, we need to follow the following steps.Extract the raw features from $u$ with e.g. SIFT (as we did for the training images). Let the descriptors of $u$ be $U = \{ u_1, \dots, u_{|U|} \}$.Create a vector $I \in \mathbb{R}^k$ of size $k$ filled with zeros, where the $i$th element of $I$ corresponds to the $i$th codeword (or cluster).For each $u_i \in U$, find the closest codeword (or centroid) in $C$. Once you found it, increment the value at the $j$th position of $I$ (i.e., initially, from zero to one), where $j$ is the found closest codeword to the descriptor $u_i$ of the query image.The distance between $u_i$ and any of the codewords can be computed e.g. with the Euclidean distance. Note that the descriptors of $u$ and the codewords have the same dimension because they have been computed with the same feature descriptor (e.g. SIFT).At the end of this process, we will have a vector $I \in \mathbb{R}^k$ that represents the frequency of the codewords in the query image $u$ (akin to the term frequency in the context of the bag-of-words model), i.e. $u$'s feature vector. Equivalently, $I$ can also be viewed as a histogram of features of the query image $u$. Here's an illustrative example of such a histogram.From this diagram, we can see that there are $11$ codewords (of course, this is an unrealistic scenario!). On the y-axis, we have the frequency of each of the codewords in a given image. We can see that the $7$th codeword is the most frequent in this particular query image.Alternatively, rather than the codeword frequency, we can use the tf-idf. In that case, each image will be represented not by a vector that contains the frequency of the codewords but it will contain the frequency of the codewords weighted by their presence in other images. See this paper for more details (where they show how to calculate tf-idf in this context; specifically, section 4.1, p. 8 of the paper).To conclude, BoF is a method to represent features of an image, which could then be used to train classifiers or generative models to solve different computer vision tasks (such as CBIR). More precisely, if you want to perform CBIR, you could compare your query's feature vector with the feature vector of every image in the database, e.g. using the cosine similarity.The first two steps above are concerned with the creation of a visual vocabulary (or codebook), which is then used to create the feature vector of a new test (or query) image.As a side note, the term bag is used because the (relative) order of the features in the image is lost during this feature extraction process, and this can actually be a disadvantage.For more info, I suggest that you read the following papers "
Proof of Maximization Bias in Q-learning?,"
In the textbook ""Reinforcement Learning: An Introduction"" by Richard Sutton and Andrew Barto, the concept of Maximization Bias is introduced in section 6.7, and how Q-learning ""over-estimates"" action-values is discussed using an example. However, a formal proof of the same is not presented in the textbook, and I couldn't get it anywhere on the internet as well. 
After reading the paper on Double Q-learning by Hado van Hasselt (link), I could understand to some extent why Q-learning ""over-estimates"" action values. Here is my (vague, informal) construction of a mathematical proof:
We know that Temporal Methods (just like Monte Carlo methods), use sample returns instead of real expected returns as estimates, to find the optimal policy. These sample returns converge to the true expected returns over infinite trials, provided all the state-action pairs are visited. Thus the following notation is used, 
$$\mathbb{E}[Q()] \rightarrow q_\pi()$$
where $Q()$ is calculated from the sample return $G_t$ observed at every time-step. Over infinite trials, this sample return when averaged converges to it's expected value which is the true $Q$-value under the policy $\pi$. Thus $Q()$ is really an estimate of the true $Q$-value $q_\pi$.
In section 3 on page 4 of the paper, Hasselt describes how the quantity $\max_a Q(s_{t+1}, a)$ approximates $\mathbb{E}[\max_a Q(s_{t+1}, a)]$ which in turn approximates the quantity $\max_a(\mathbb{E}[Q(s_{t+1},a)])$ in Q-learning. Now, we know that the $\max[]$ function is a convex function (proof). From Jensen's inequality, we have
$$\phi(\mathbb{E}[X]) \leq \mathbb{E}[\phi(X)]$$ where $X$ is a random variable, and the function $\phi()$ is a convex function. Thus,
$$\max_a(\mathbb{E}[Q(s_{t+1},a)]) \leq \mathbb{E}[\max_a(Q(s_{t+1}, a)]$$
$$\therefore \max_a Q(s_{t+1}, a) \approx \max_a(\mathbb{E}[Q(s_{t+1},a)]) \leq \mathbb{E}[\max_a(Q(s_{t+1}, a)]$$
The quantity on the LHS of the above equation appears (along with $R_{t+1}$) as an estimate of the next action-value in the Q-learning update equation:
$$Q(S_t,A_t) \leftarrow (1-\alpha)Q(S_t, A_t) + \alpha[R_{t+1} + \gamma\max_aQ(S_{t+1}, a)] $$
Lastly, we note that the bias of an estimate $T$ is given by:
$$b(T) = \mathbb{E}[T] - T$$
Thus the bias of the estimate $\max_a Q(s_{t+1},a)$ will always be positive:
$$b(\max_a Q(s_{t+1},a)) = \mathbb{E}[\max_a Q(s_{t+1},a)] - \max_a Q(s_{t+1},a) \geq 0$$
In statistics literature, any estimate whose bias is positive is said to be an ""over-estimate"". Thus the action values are over-estimated by the Q-learning algorithm due to the $\max[]$ operator, thus resulting in a $maximization$-$bias$.
Are the arguments made above valid? I am a student, with no rigorous knowledge of random processes. Thus, please forgive me if any of the steps above are totally unrelated, and doesn't make sense in a more mathematically rigorous fashion. Please let me know, if there is a much better proof than this failed attempt. 
Thank you so much for your precious time. Any help/suggestions/corrections are greatly appreciated!
","['reinforcement-learning', 'q-learning', 'proofs']","Your bias formula is a bit incorrect. You should subtract a true value, not an estimator.$\mathrm{b}(\hat{\theta}) \stackrel{\text { def }}{=} \mathbb{E}[\hat{\theta}]-\theta$$\mathrm{b}\left(\max _{a} Q\right)=\mathbb{E}\left[\max _{a} Q\right]-\max _{a} q=\mathbb{E}\left[\max _{a} Q\right]-\max _{a} \mathbb{E}[Q]$$\mathbb{E}\left[\max _{a} Q\right] \geq \max _{a} \mathbb{E}[Q] \Rightarrow \mathrm{b}\left(\max _{a} Q\right) \geq 0$"
What are the differences between SARSA and Q-learning? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



From Sutton and Barto's book Reinforcement Learning (Adaptive Computation and Machine Learning series), are the following definitions:


To aid my learning of RL and gain an intuition, I'm focusing on the differences between some algorithms. I've selected Sarsa (on-policy TD control) for estimating Q ≈ q * and Q-learning (off-policy TD control) for estimating π ≈ π *.
For conciseness I'll refer to Sarsa (on-policy TD control) for estimating Q ≈ q * and Q-learning (off-policy TD control) for estimating π ≈ π * as Sarsa and Q-learning respectively.
Are my following assertions correct?
The primary differences are how the Q values are updated.
Sarsa Q value update:
$ Q ( S, A ) ← Q ( S, A ) + α [ R + \gamma Q ( S ′ , A ′ ) − Q ( S, A ) ] $
Q-learning Q value update:
$ Q ( S, A ) ← Q ( S, A ) + α [ R + \gamma \max_a Q ( S ′ , a ) − Q ( S, A ) ] $
Sarsa, in performing the td update subtracts the discounted Q value of the next state and action, S', A' from the Q value of the current state and action S, A. 
Q-learning, on the other hand, takes the discounted difference between the max action value for the Q value of the next state and current action S', a.  Within the Q-learning episode loop the $a$ value is not updated, is an update made to $a$ during Q-learning? 
Sarsa, unlike Q-learning, the current action is assigned to the next action at the end of each episode step. Q-learning does not assign the current action to the next action at the end of each episode step
Sarsa, unlike Q-learning, does not include the arg max as part of the update to Q value.
Sarsa and Q learning in choosing the initial action for each episode both use a ""policy derived from Q"", as an example, the epsilon greedy policy is given in the algorithm definition. But any policy could be used here instead of epsilon greedy?
Q learning does not utilise the next state-action pair in performing the td update, it just utilises the next state and current action, this is given in the algorithm definition as  $ Q ( S ′ , a ) $ what is $a$ in this case ?
","['reinforcement-learning', 'comparison', 'q-learning', 'sarsa']","The main difference between the two is that Q-learning is an off policy algorithm. That is, we learn about an policy that is different to the one we choose to make actions. To see this, lets look at the update rule. $$Q(s,a) = Q(s,a) + \alpha (R_{t+1} + \gamma \max_aQ(s',a) - Q(s,a))$$$$Q(s,a) = Q(s,a) + \alpha (R_{t+1} + \gamma Q(s',a') - Q(s,a))$$In SARSA we chose our $a'$ according to what our policy tells us to do when we are in state $s'$, so the policy we are learning about is also the policy that we choose to make our actions. In Q-learning, we learn about the greedy policy whilst following some other policy, such as $\epsilon$-greedy. This is because when we transition into state $s'$ our TD-target becomes the maximum Q-value for whichever state we end up in, $s'$, where the max is taken over the actions. Once we have actually updated our Q-function and we need to choose an action to take in $s'$, we do so from the policy we are using to generate our actions from -- thus we are learning about the greedy policy whilst following some other policy, hence off policy. In SARSA when we move into $s'$ our TD-target is chosen by the Q-value for the state we transition into and then the action we would choose based on our policy. Within the Q-learning episode loop the $a$ value is not updated, is an update made to $a$ during Q-learning?It will be, because the policy we use to choose our actions is ensured to explore sufficiently around all of the state-action pairs and so it is guaranteed to be encountered at some point. Sarsa, unlike Q-learning, does not include the arg max as part of the update to Q value.It is not an $\arg \max$, it is a $\max$. This is defined as 
$$\max_x f(x) = \{f(x) | \forall y\; : f(y) \leq f(x) \}$$Sarsa, unlike Q-learning, the current action is assigned to the next action at the end of each episode step. Q-learning does not assign the current action to the next action at the end of each episode stepKind of - the action that you chose for your TD-target in SARSA becomes the next action that you consider in the next step of the episode. This is natural because essentially you are in state $s$, you take action $a$ and observe a new state $s'$, at which point you can use your policy to see which action you will take, call this $a'$, and then perform the SARSA update, and then execute that action in the environment. Sarsa and Q learning in choosing the initial action for each episode both use a ""policy derived from Q"", as an example, the epsilon greedy policy is given in the algorithm definition. But any policy could be used here instead of epsilon greedy? Yes, any policy can be used although you want to choose a policy that allows sufficient exploration of the state-space.Q learning does not utilise the next state-action pair in performing
  the td update, it just utilises the next state and current action,
  this is given in the algorithm definition as $Q(S',a)$ what is $a$ in
  this case ?in the algorithm it actually has $\max_a Q(S',a)$, which if you refer back to my earlier definition of what the $\max$ operator does, should answer this question. "
Overcome caveats on using Deep Learning for faster inference on limited performance availability,"
I am working in the field of Machine Vision, where accuracy and performance both play a major factor in deciding the approach towards a problem. Traditional rule based approaches work quite well in such cases.
I am gradually migrating towards deep learning, due to its umpteen advantages, where the results seem promising albeit with two huge caveats:

Lack of Training data in this field. To be precise, the lack of erroneous data.
Performance issues on inference. Accuracy and speed are required in equal proportion, and cannot be compromised.

In industrial settings, Point 1 plays a strong factor. I have been dabbling with Transfer learning techniques and using pre-trained models to overcome this situation. For simpler applications such as classification, this suits and gives good results. In other cases such as detection and localization, I have tried using MaskRCNN, which gives really good results but poor inference speed, means it is not production-ready.
The worrying factor in both cases is how slow detection and inference is, compared to traditional vision algorithms. A solution would be to buy Machine Vision software specifically from companies such as Cognex, HALCON, etc, who sell deep learning bundles. They are quite expensive and are to be used out- of- box with minimal modifications, which does not suit me currently.
Point 2, is highly necessary in production lines, where each iteration/image may take less than 500ms for execution.
Deep Learning gives a lot of opportunities in getting state of the art results with very less data in most of the situations, but in general without inference optimization in using apps such as TensorRT, the ""time"" metric does not give good results.
Is there an approach in using open source that can solve both point 1 and point 2? Creating a CNN from scratch is out of the question.
This post is to discuss ideas if possible, I know a concrete solution is not really possible in the scope of this question. I am the only person working on this problem at my company, thus any discussion ideas would be highly appreciated!
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'computer-vision']",
What are the main ideas behind NER?,"

Named entity recognition (NER), also known as entity chunking/extraction, is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes.

Briefly, how does NER work? What are the main ideas behind it? And which algorithms are used to perform NER? 
","['natural-language-processing', 'named-entity-recognition']","There are different algorithms, each with their advantages and disadvantages.Gazetteers: these have lists of the entities to be recognised, eg list of countries, cities, people, companies, whatever is required. They typically use a fuzzy matching algorithm to capture cases where the entity is not written in exactly the same way as in the list. For example, USA or U.S.A., United States, United States of America, US of A, etc. Advantage: generally good precision, ie can identify known entities. Disadvantage: can only find known entitiesContextual Clues: here you have patterns that you find in the text, eg [PERSON], chairman of [COMPANY]. In this case, sentences like Jeff Bezos, chairman of Amazon, will match, even if you have never come across either Bezos or Amazon. Advantage: can find entities you didn't know about. Disadvantages: could end up with false positives, might be quite labour-intensive to come up with patterns; patterns depend on the domain (newspapers vs textbooks vs novels etc_Structural description: this is basically a 'grammar' describing what your entities look like, eg (in some kind of pseudo-regex): title? [first_name|initial] [middle_name|initial]? surname would match ""Mr J. R. Ewing"" or ""Bob Smith"". Similar descriptions could match typical company names; you'd probably still need lists of possible surnames or firstnames. Advantages: some flexibility, and potentially good precision. Disadvantages: patterns need to be developed and maintained.Ideally you would want to combine all three approaches for a hybrid one to get the advantages of recognising unknown entities while keeping excess false positives in check.There might also be other machine-learning approaches, but I'm not too familiar with those. The main problem is that they are hard to fine-tune or work out why they do what they do.UPDATE: A good starting point would be to use a gazetteer-based approach to annotate some training data, and use that to identify contextual patterns. You can then use that data to train a machine learning approach (see OmG's answer on CRF) to broaden the approach; and then you add newly recognised entities to your list.Ideally you would want to have a gazette as your main database to avoid false positives, and use machine-learning or contextual patterns to just capture previously unseen entities."
"Why is the ""reward to go"" replaced by Q instead of V, when transitioning from PG to actor critic methods?","
While transitioning from simple policy gradient to the actor-critic algorithm, most sources begin by replacing the ""reward to go"" with the state-action value function (see this slide 5). 
I am not able to understand how this is mathematically justified. It seems intuitive to me that the ""reward to go"" when sampled through multiple trajectories should be estimated by the state-value function. 
I feel this way since nowhere in the objective function formulation or resulting gradient expression do we tie down the first action after reaching a state. Alternatively, when we sample a bunch of trajectories, these trajectories might include different actions being taken from the state reached in timestep $t$. 
So, why isn't the estimation/approximation for the ""reward to go"" the state value function, in which the expectation is also over all the actions that may be taken from that state as well?
","['reinforcement-learning', 'actor-critic-methods', 'reinforce', 'advantage-actor-critic', 'reward-to-go']","When you say simple policy gradient, I assume you mean something like REINFORCE. The main difference between actor-critic and REINFORCE-like algorithms is in how they estimate the reward to go: in REINFORCE, you wait until a trajectory terminates to make any updates, and your estimator of the reward to go is the actual reward to go that was observed in the trajectory. in actor critic algorithms, instead of using an entire trajectory to estimate a reward to go, you use the action value function (note that this way, you can compute updates at every time step). The reason why you use the action-value function is because at each transition, you have already committed to some action. The benefit of the actor critic estimator is that it exhibits much less variance. REINFORCE estimators are Monte Carlo estimators, which are known to exhibit extremely high variance. Now, the actor critic method in the slides you posted takes variance reduction one step further. Instead of estimating reward to go, it's estimating advantage -- that is, the difference between the current action value and your estimated state value. The state value function term doesn't depend on the action, so it doesn't affect the expected value of the policy gradient. However, it serves as a 'baseline', which helps reduce the variance of the reward to go estimator even further."
Solution to exercise 3.22 in the RL book by Sutton and Barto,"
The goal is to find an optimal deterministic policy for this MDP: 

There are two possible policies: left (L) and right (R). What is the optimal policy, when different discounts are used: 
A $\gamma = 0$
B $\gamma = 0.9$
C $\gamma = 0.5$
The optimal policy $\pi_* \ge \pi$ if $v_{\pi^*}(s) \ge v_{\pi}(s), \forall s \in S$, so to find the optimal policy, the goal is to check which one of those results in the largest state value function for all states in the system given discount factors (A,B,C). 
The Bellman equation for the state value function is 
$v(s) = E_\pi[G_t | S_t= s] = E_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$
The suffix $_n$ marks the current iteration, and $_{n+1}$ marks the next iteration. The following is valid if the value function is initialized to $0$ or some random $x \ge 0$.
A) $\gamma = 0$
$v_{L,n+1}(S_0) = 1 + 0 v_{L,n}(S_L) = 1$
$v_{R,n+1}(S_0) = 0 + 0 v_{R,n}(S_R) = 0$
$L$ is optimal in case A. 
B) $\gamma = 0.9$
$v_{L,n+1}(S_0) = 1 + 0.9 v_{L,n}(S_L) = 1 + 0.9(0 + 0.9 v_{L,n}(S_0)) = 1 + 0.81v_{L,n}(S_0)$
$v_{R,n+1}(S_0) = 0 + 0.9 v_{R,n}(S_R) = 0 + 0.9(2 + 0.9 v_{R,n}(S_0)) = 1.8 + 0.81v_{R,n}(S_0)$
$R$ is optimal in case B.
C) $\gamma = 0.5$
$v_{L,n+1}(S_0) = 1 + 0.5 v_{L,n}(S_L) = 1 + 0.5(0 + 0.9 v_{L,n}(S_0)) = 1 + 0.45v_{L,n}(S_0)$
$v_{R,n+1}(S_0) = 0 + 0.5 v_{R,n}(S_R) = 0 + 0.5(2 + 0.9 v_{R,n}(S_0)) = 1 + 0.45v_{R,n}(S_0)$
Both $R$ and $L$ are optimal in case C. 
Question: Is this correct? 
",['reinforcement-learning'],"Your answer is correct but I am not sure exactly on how you arrived at it, as e.g. in the last case you don't know that $v_{L,n}(S_0) = v_{R,n}(S_0)$. I will show for case B when $\gamma = 0.9$ as case A is trivial and hopefully you can apply what I've done in case B to case C so that you get exact answers. Now, as you stated $v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$. Assuming that $\gamma = 0.9$ we can calculate the values for each state under the policy of taking the left action. Note that because we are looking for deterministic policies and the environment is deterministic then a lot of the expectations can be disregarded as nothing random is happening. \begin{align}v(s_0) &= 1 + 0.9 \times v(s_L) \\ v(s_L) &= 0 + 0.9 \times v(s_0) \\ v(s_R) &= 2 + 0.9 \times v(s_0)
\end{align}
We can solve this set of linear equations to get $v(s_0) = \frac{100}{19}, v(s_L) = \frac{90}{19}, v(s_R) = \frac{128}{19}\;.$\begin{align}v(s_0) &= 0 + 0.9 \times v(s_R) \\ v(s_L) &= 0 + 0.9 \times v(s_0) \\ v(s_R) &= 2 + 0.9 \times v(s_0)
\end{align}
We can again solve these to obtain $v(s_0) = \frac{180}{19}, v(s_L) = \frac{162}{19}, v(s_R) = \frac{200}{19}\;.$As we can see, for each of the states the value function is larger for all of the states under the policy 'go right', thus this is the optimal policy for the case of $\gamma = 0.9$. It is important to note that if we take the 'left' action in state $s_0$ then our policy would never take us to state $s_R$, and the same for the right action and state $s_L$, however due to the definition of an optimal policy requiring $v_{\pi ^*}(s) \geq v_{\pi}(s)\; \forall s \in \mathcal{S}$ then we must evaluate the value function for all states, even ones that would not be visited under a policy you are evaluating. This means that the state for $s_R$ will change whether we go right or left, because the value of this state depends on the value of $s_0$, which clearly changes depending on whether we go right or left. "
Crossover method for gene value containing a set of values,"
I have a chromosome where each gene contain s set of values. Like the following:
chromosome = [[A,B,C],[C,B,A],[C,D,],[],[E,F]]

The order in each gene values matters. (A,B,C is different to A,C,B)
Each value should not appear more than once in a gene. ([A,B,B] is not desirable, B is repeated.)

In my current two-point crossover method. The genes values that are crossover is the whole set of values. (E.g the whole of [A,B,C] is crossed to another chromosome)
Soon, I realize my population lacks variations very quickly because the values within a gene always remain the same. Hence, my algorithm is evolving very slowly, and limited by the variation of gene values at initialization stage.  
What crossover can I implement to cross values within the set as well? 
I am pretty new to genetic algorithm. Any help will be much appreciated. Thank you. 
","['genetic-algorithms', 'evolutionary-algorithms', 'crossover-operators']",
"After a GAN is trained, which parts of it are used to generate new outputs from data?","
After a GAN is trained, which parts of it are used to generate new outputs from data?
Options are:

Neither
Discriminator
Generator
Both Generator and Discriminator

","['machine-learning', 'generative-adversarial-networks', 'homework']",
What is the difference between LSTM and fully connected LSTM?,"
I'm currently trying to understand the difference between a vanilla LSTM and a fully connected LSTM. In a paper I'm reading, the FC-LSTM gets introduced as

FC-LSTM may be seen as a multivariate version of LSTM where the input, cell output and states are all 1D vectors

But is not really expanded further upon. Google also didn't help me much in that regard as I can't seem to find anything under that keyword. 
What is the difference between the two? Also, I'm a bit confused by the quote - aren't inputs, outputs, etc. of a vanilla LSTM already 1D vectors?
","['deep-learning', 'comparison', 'recurrent-neural-networks', 'long-short-term-memory']","Based on the citations in the ConvLSTM paper I have come to the conclusion that they mean the Peephole LSTM when they say fully connected LSTM. In the paper that they have taken the encoder-decoder-predictor model from, where they refer to a fully connected LSTM, a Peephole LSTM is used. Also they take their fully connected LSTM definition from this paper, which again uses the Peephole LSTM.  With that the difference would be the added ""peephole connections"", that lets the gate layers look at the cell states and access the constant error carousel."
Generalising performance of Q-learning agent through self-play in a two-player game (MCTS?),"
I'm using Q-learning (off-policy TD-control as specified in Sutton's book on pg 131) to train an agent to play connect four. My goal is to create a strong player (superhuman performance?) purely by self-play, without training models against other agents obtained externally.
I'm using neural network architectures with some convolutional layers and several fully connected layers. These train surprisingly efficiently against their opponent, either a random player or another agent previously trained through Q-learning. Unfortunately the resulting models don't generalise well. 5000 episodes seems enough to obtain a high (> 90%) win rate against whichever opponent, but after > 20 000 episodes, they are still rather easy to beat by myself.
To solve this, I now train batches of models (~ 10 models per batch), which are then used in group as a new opponent, i.e.:

I train a batch of models against a completely random agent (let's call them the generation one)
Then I train a second generation of agents against this first generation
Then I train a third generation against generation two
...

So far this helped in creating a slightly stronger/more general connect four model, but the improvement is not as good as I was hoping for. Is it just a matter of training enough models/generations or are there better ways for using Q-learning in combination with self-play?
I know the most successful techniques (e.g. alpha zero) rely on MCTS, but I'm not sure how to integrate this with Q-learning? Neither how MCTS helps to solve the problem of generalisation?
Thanks for your help!
","['reinforcement-learning', 'q-learning', 'self-play']","To solve this, I now train batches of models (~ 10 models per batch), which are then used in group as a new opponent,This seems quite a reasonable approach on the surface, but possibly the agents will still lose generalisation if the solutions in each generation are too similar. It also looks like from your experiment that learning progress is too slow.One simple thing you could do is progress through the generations faster. You don't need to train until agents win 90% of games before upping the generation number. Yuo could set the target as low as 60% or even 55%.For generalisation, it may also help to train against a mix of previous generations. E.g. if you use ten opponents, have five from previous generation, two from each of two iterations before that, and one even older one.Although the setup you have created plays an agent you are training against another agent that you have created, it is not quite self-play. In self-play, an agent plays against itself, and learns as both players simultaneously. This requires a single neural network function that can switch its evaluation to score for each player - you can either make it learn to take the current player into account and make the change in viewpoint itself, or in zero-sum games (which Connect 4 is one) it can be more efficient to have it evaluate the end result for player 1 and simply take the negative of that as the score for player 2. This is also equivalent to using $\text{max}_a$ and $\text{argmax}_a$ for player 1's action choices and  $\text{min}_a$ and $\text{argmin}_a$ for player 2's action choices - applying the concept of minimax to Q learning.You can take minimax further to improve your algorithm's learning rate and performance during play. Essentially what Q learning and self-play does is learn a heuristic for each state (or state/action pair) that can guide search. You can add search algorithms to your training and play in multiple ways. One simple approach during training is to perform some n-step look ahead using negamax with alpha-beta pruning (an efficient variant of minimax in zero-sum games), and if it finds the end of the game:when training, use the result (win/draw/lose) as your ground truth value instead of the normal Q-learning TD target.when evaluating/playing vs human, prefer the action choice over anything the Q function returns. In practice, only bother with the Q function if look-ahead search cannot find a result.In the last few months, Kaggle have been running a ""Connect X"" challenge (which is effectively only Connect 4 at the moment). The forums and example scripts (called ""Kernels"") are a good source of information for writing your own agents, and if you choose to compete, then the leaderboard should give you a sense for how well your agent is performing. The top agents are perfect players, as Connect 4 is a solved game. I am taking part in that competition, and have trained my agent using self-play Q-learning plus negamax search as above - it is not perfect, but is close enough that it can often beat a perfect playing opponent when playing as player 1. It was trained on around 100,000 games of self-play as I described above, plus extra training games versus previous agents.I know the most successful techniques (e.g. alpha zero) rely on MCTS, but I'm not sure how to integrate this with Q-learning? Neither how MCTS helps to solve the problem of generalisation?MCTS is a variant of search algorithm, and could be combined with Q-learning similarly to negamax, although in Alpha Zero it is combined with something more like Actor-Critic. The combination would be similar - from each position in play use MCTS to look ahead, and instead of picking the direct action with the best Q value, pick the one with the best MCTS score. Unlike negamax, MCTS is stochastic, but you can still use its evaluations as ground truth for training.MCTS does not solve generalisation issues for neural networks, but like negamax it will improve the performance of a game-playing agent by looking ahead. Its main advantage over negamax in board games is a capability to scale to large branching factors. MCTS does work well for Connect 4. Some of the best agents in the Kaggle competition are using MCTS. Howver, it is not necessary for creating a ""superhuman"" Connect 4 agent, Q-learning plus negamax can do just as well."
How do deepfakes work and how they might be dangerous?,"

Deepfakes (a portmanteau of ""deep learning"" and ""fake"") are synthetic media in which a person in an existing image or video is replaced with someone else's likeness.

Nowadays most of the news circulating in the news and social media are fake/gossip/rumors which may false-positives or false-negatives except  WikiLeaks 
I know there has been a Deepfake Detection Challenge Kaggle competition for a whooping sum $1,000,000 prize money.
I would like to know how deepfakes work and how they might be dangerous?
","['deep-learning', 'deepfakes']","In general, deepfakes rely on advanced context-aware digital signal manipulations - usually image, video or audio - that allow for very natural looking modifications of content that previously have been costly or near impossible to produce in high quality. The AI models, often based on generative adversarial networks (GANs), style transfer, pose estimation and similar technologies, are capable of tasks such as transferring facial features from subject A to replace those of subject B in a still image or video, whilst copying subject B's pose, expression, and matching the scene's lighting. Similar technologies exist for voices.A good example of this might be these Star Wars edits, where actors faces have been changed. It is not perfect, you can in a few shots see a little instability if you study the frames - but the quality is still pretty good, and it was done with a relatively inexpensive setup. The work was achieved using freely-available software, such as DeepFaceLab on Github.The technology is not limited to simple replacements - other forms of puppet-like control over output are possible, where an actor can directly control the face of a target in real time using no more than a PC and webcam. Essentially, with the aid of deepfakes, it becomes possible to back up slanderous or libelous commentary with convincing media, at a low price point. Or the reverse, to re-word or re-enact an event that would otherwise be negative publicity for someone, in order to make it seem very different yet still naturally captured.The danger of this technology is it puts tools for misinformation into a lot of people's hands. This leads to potential problems including:Attacks on integrity of public figures, backed by realistic-looking ""evidence"". Even with the knowledge that this fakery is possible (and perhaps likely given a particular context), then damage can still be done especially towards feeding people with already-polarised opinions with manufactured events, relying on confirmation bias.Erosion of belief in any presented media as proof of anything. With deepfakes out in the wild, someone confronted with media evidence that went against any narrative can claim ""fake"" that much more easily.Neither of these issues are new in the domains of reporting, political bias, propaganda etc. However, it adds another powerful tool for people willing to spread misinformation to support any agenda, alongside things such as selective statistics, quoting out of context, lies in media that is text-only or crudely photoshopped etc. A search for papers studying impact of deep fakes should find academic research such as Deepfakes and Disinformation: Exploring the Impact of Synthetic Political Video on Deception, Uncertainty, and Trust in News.Opinion: Video content, presented feasibly as a live capture or report, is especially compelling, as unlike text and still image media, it directly interfaces to two key senses that humans use to understand and navigate the world in real time. In short, it is more believable by default at an unconscious and emotional level compared to a newspaper article or even a photo. And that applies despite any academic knowledge of how it is produced that you might possess as a viewer."
Why is it useful to track loss while model is being trained?,"
Why is it useful to track loss while the model is being trained?
Options are:

Loss is only useful as a final metric. It should not be evaluated while the model is being trained.
Loss dictates how effective the model is.
Loss can help understand how much the model is changing per iteration. When it converges, that's an indicator that further training will have little benefit.
None of the above

","['machine-learning', 'objective-functions', 'homework']",
Why is my Soft Actor-Critic's policy and value function losses not converging?,"
I'm trying to implement a soft actor-critic algorithm for financial data (stock prices), but I have trouble with losses: no matter what combination of hyper-parameters I enter, they are not converging, and basically it caused bad reward return as well. It sounds like the agent is not learning at all.
I already tried to tune some hyperparameters (learning rate for each network + number of hidden layers), but I always get similar results.
The two plots below represent the losses of my policy and one of the value functions during the last episode of training.


My question is, would it be related to the data itself (nature of data) or is it something related to the logic of the code?
","['reinforcement-learning', 'tensorflow', 'actor-critic-methods', 'hyperparameter-optimization', 'soft-actor-critic']","I would say it is the nature of data. Generally speaking, you are trying to predict a random sequence, especially if you use the history data as an input and try to get the future value as an output."
Can a machine learning approach solve this constrained optimisation problem?,"
I had done with different classification, regression and clustering approaches for predictions of values, etc. I was wondering if there is a machine learning approach for distribution of a whole based on some features (I do not know if there is an approach for that I just could not find one with my research).
An easy example might be lets consider we have height and weight data of many children and we have to distribute a given number of pizza slices amongst them so that skinny children get more pizza as compared to obese ones because pizza is more beneficial for skinny as compared to obese. So might have to find out the optimum number of slices for each child out of the total number of slices so that each child gets maximum possible nutrients. A more complex version could incorporate more features like age, overall health, blood sugar content, physical activity index, daily calorie consumption, and others.
A similar example might be to find out the optimal value of fuel to be allocated to each vehicle if we have a total of 100 gallons. Features might be distance they have to travel, mpg, driver competency, engine horsepower, etc., so that all of them might travel the maximum distance possible.
So, can we achieve a task like this with machine learning/deep learning approaches? If not what are the hurdles achieving this?
","['machine-learning', 'deep-learning', 'optimization']",
How do I know that the DQN has learnt an appropriate Q function?,"
Is there any sanity check to know whether the Q functions learnt are appropriate in deep Q networks? I know that the Q values for end states should approximate the terminal reward. However, is it normal that Q values for the non-terminal states have higher values than those of the terminal states?
The reason why I want to know whether Q values learnt are appropriate is because I want to apply the doubly robust estimator for off-policy value evaluation. Using doubly robust requires a good Q value estimate to be learnt for each state.
","['reinforcement-learning', 'q-learning', 'dqn', 'off-policy-methods', 'value-functions']",
Why do my rewards fall using tabular Q-learning as I perform more episodes?,"
Using the tutorial from: SentDex - Python Programming I added Q Learning to my script that was previously just picking random actions. His script uses the MountainCar Environment so I had to amend it to the CartPole env I am using. Initially, the rewards seem sporadic but, after a while, they just drop off and oscillate between 0-10. Does anyone know why this is?
Learning_rate = 0.1
Discount_rate = 0.95
episodes = 200

# Exploration settings
epsilon = 1  # not a constant, qoing to be decayed
START_EPSILON_DECAYING = 1
END_EPSILON_DECAYING = episodes//2
epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)

env = gym.make(""CartPole-v0"") #Create the environment. The name of the environments can be found @ https://gym.openai.com/envs/#classic_control
#Each environment has a number of possible actions. In this case there are two discrete actions, left or right

#Each environment has some integer characteristics of the state.
#In this case we have 4:

#env = gym.wrappers.Monitor(env, './', force=True)

DISCRETE_OS_SIZE = [20, 20, 20, 20]

discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/ DISCRETE_OS_SIZE 

def get_discrete_state(state):
    discrete_state = (state - env.observation_space.low)/discrete_os_win_size
    return tuple(discrete_state.astype(np.int))

q_table = np.random.uniform(low = -2, high = 0, size = (20, 20, 20, 20, env.action_space.n))

plt.figure() #Instantiate the plotting environment
rewards_list = [] #Create an empty list to add the rewards to which we will then plot
for i in range(episodes):
    discrete_state = get_discrete_state(env.reset())
    done = False
    rewards = 0
    frames = []

    while not done:
        #frames.append(env.render(mode = ""rgb_array""))

        if np.random.random() > epsilon:
            # Get action from Q table
            action = np.argmax(q_table[discrete_state])

        else:
            # Get random action
            action = np.random.randint(0, env.action_space.n)

        new_state, reward, done, info = env.step(action)

        new_discrete_state = get_discrete_state(new_state)

        # If simulation did not end yet after last step - update Q table
        if not done:

            # Maximum possible Q value in next step (for new state)
            max_future_q = np.max(q_table[new_discrete_state])

            # Current Q value (for current state and performed action)
            current_q = q_table[discrete_state, action]

            # And here's our equation for a new Q value for current state and action
            new_q = (1 - Learning_rate) * current_q + Learning_rate * (reward + Discount_rate * max_future_q)

            # Update Q table with new Q value
            q_table[discrete_state, action] = new_q

        else:
            q_table[discrete_state + (action,)] = 0

        discrete_state = new_discrete_state

        rewards += reward
        rewards_list.append(rewards)
    #print(""Episode:"", i, ""Rewards:"", rewards)
    #print(""Observations:"", obs)

    # Decaying is being done every episode if episode number is within decaying range
    if END_EPSILON_DECAYING >= i >= START_EPSILON_DECAYING:
        epsilon -= epsilon_decay_value

plt.plot(rewards_list)
plt.show()
env.close()


It becomes even more pronounced when I increase episodes to 20,000 so I don't think it's related to not giving the model enough training time.

If I set START_EPSILON_DECAYING to say 200 then it only drops to < 10 rewards after episode 200 which made me think it was the epsilon that was causing the problem. However, if I remove the epsilon/exploratory then the rewards at every episode are worse as it gets stuck in picking the argmax value for each state.
","['reinforcement-learning', 'q-learning']","The problem here is likely related to the state approximations you are using.Unfortunately, OpenAI's gym does not always give reasonable bounds when using env.observation_space, and that seems to be the case for CartPole:Processing this, similarly to your code:That means that all the velocities will get squashed down to $0$ in your approximation. Your agent cannot tell the difference between a nearly static balancing position (generally the goal) and transitioning through it really fast - it will think that both are equally good. It is also not able to tell difference between moving towards balance point, or moving away from it.I suggest you check for what reasonable bounds are on the space (a quick look suggests +/- 2.0 might be a reasonable starting point) and use that instead.The approximation approach of discrete grid is also very crude, although it does allow you do use tabular approaches. If you want to stick with a linear system (and avoid trying neural networks and DQN) then the next step up would be some form of tile coding, which uses multiple offset grids to obtain smoother interpolation between states."
How to autocorelate multiple variants of same text into one?,"
I want to improve quality of translations for open-source projects in Ukrainian language. We have multiple translations from different authors. We can also translate messages using machine translations. Sometimes machine translation is even better than human translation.
Given multiple variants of translation of the same original text, I want to create AI which will be able to ""translate"" from Ukrainian to Ukrainian, using these multiple variants in parallel as the source, to produce one variant of higher quality.
So, in general, given multiple similar input sequences, the neural network needs to ""understand"" them, and produce a single output sequence.
$$S_1, S_2, \dots \rightarrow S$$
For a simple example, we may want to train a NN to recognize a sequence of natural numbers: $1,2,3,4, \dots$. We give two sequences to NN: $23,4,24,6,8$ and $3,65,5,6,23$, then trained the NN is expected to produce $3,4,5,6,7$.
How to modify an existing neural network to achieve that? Is it possible at all?
","['natural-language-processing', 'ai-design', 'machine-translation']",
Which machine learning approach can be used to predict a univariate value?,"
I have a stream of data coming in like below (random numbers 0-9) 

7, 7, 0, 0, 8, 9, 2, 7, 3, 8, 2, 8, 5, 7, 0, 8, 7, 8, 5, 3, 2, 6, 1, 9, 5, 7, 5, 3, 4, 9, 1, 3, 5, 5, 0, 7, 7, 5, 2, 8, 8, 7, 5, 5, 5, 2, 9, 7, 2, 1, 0, 0, 5, 7, 1, 4, 2, 7, 8, 8, 5, 2, 7, 5, 7, 1, 7, 2, 0, 5, 7, 5, 2, 6, 3, 6, 3, 6, 1, 9, 1, 9, 7, 2, 3, 9, 8, 8, 4, 9, 8, 2, 5, 3, 4, 0, 3, 1, 0, 7, 2, 3, 8, 7, 5, 7, 3, 6, 0, 3, 3, 3, 6, 3, 1, 3, 0, 6, 9, 8, 0, 1, 4, 4, 9, 9, 3, 7, 4, 1, 0, 5, 0, 6, 8, 8, 8, 1, 7, 6

Ask: is to Predict the next numbers(at least 3-10). 
Which approach would be helpful in getting through this problem? 
",['prediction'],
Can the addition of unnoticeable noise to images be used to create subliminals?,"
I was reading this report: https://www.theverge.com/2017/4/12/15271874/ai-adversarial-images-fooling-attacks-artificial-intelligence
Researchers used noise to trick machine learning algorithms to misidentify or misclassify an image of a fish as a cat. I was wondering if something like that can be used to create subliminals.
What I mean by subliminals: United Nations has defined subliminal messages as perceiving messages without being aware of them, it is unconscious perception, or perception without awareness. Like you may be aware of a message but cannot consciously perceive that message in the form of text, etc. 
All the reports about the noise trick said the noise was so transparent that humans couldn't detect it. This can be changed to make it noticeable unconsciously but unnoticeable at a conscious level so a human can register the subliminal but not be aware of it.
Is it possible to take an output from a hidden layer to construct such subliminal for humans, with trial and error one can find right combination? Can it be possible to come up with a pixel pattern or noise with ML which allows one to impose subliminals?
","['machine-learning', 'social', 'adversarial-ml']",
Why we don't use importance sampling in tabular Q-Learning?,"
Why don't we use an importance sampling ratio in Q-Learning, even though Q-Learning is an off-policy method? 
Importance sampling is used to calculate expectation of a random variable by using data not drawn from the distribution. Consider taking a Monte Carlo average to calculate $\mathbb{E}[X]$. 
Mathematically an expectation is defined as 
$$\mathbb{E}_{x \sim p(x)}[X] = \sum_{x = \infty}^\infty x p(x)\;;$$
where $p(x)$ denotes our probability mass function, and we can approximate this by
$$\mathbb{E}_{x \sim p(x)}[X] \approx \frac{1}{n} \sum_{i=1}^nx_i\;;$$
where $x_i$ were simulated from $p(x)$. 
Now, we can re-write the expectation from earlier as 
$$\mathbb{E}_{x \sim p(x)}[X] = \sum_{x = \infty}^\infty x p(x) = \sum_{x = \infty}^\infty x \frac{p(x)}{q(x)} q(x) = \mathbb{E}_{x\sim q(x)}\left[ X\frac{p(X)}{q(X)}\right]\;;$$
and so we can calculate the expectation using Monte Carlo averaging 
$$\mathbb{E}_{x \sim p(x)}[X] \approx \frac{1}{n} \sum_{i=1}^nx_i \frac{p(x)}{q(x)}\;;$$
where the data $x_i$ are now simulated from $q(x)$. 
Typically importance sampling is used in RL when we use off-policy methods, i.e. the policy we use to calculate our actions is different from the policy we want to evaluate. Thus, I wonder why we don't use the importance sampling ratio in Q-learning, even though it is considered to be an off-policy method?
","['reinforcement-learning', 'q-learning', 'importance-sampling', 'bellman-equations']","In Tabular Q-learning the update is as follows $$Q(s,a) = Q(s,a) + \alpha \left[R_{t+1} + \gamma \max_aQ(s',a) - Q(s,a) \right]\;.$$Now, as we are interested in learning about the optimal policy, this would correspond to the $\max_aQ(s',a)$ term in the TD target because that is how the optimal policy chooses its actions - i.e. $\pi_*(a|s) = \arg\max_aQ_*(s,a)$, so eventually the greedy TD update would be greedy with respect to the optimal state-action value function due to the guaranteed convergence of Q-learning. The action $a$ in the update rule, i.e. the action we chose in state $s$ to receive the reward $R_{t+1}$, was chosen according to some non-optimal policy, e.g. $\epsilon$-greedy. However, as the $Q$ function is defined as the expected returns assuming we are in state $s$ and have taken action $a$ -- we thus don't need an importance sampling ratio for the $R_{t+1}$ term, even though it was generated from an action that the optimal policy might not have taken, because we are only updating the $Q$ function for state $s$ and action $a$, and by the definition of a $Q$ function it is assumed that we have taken action $a$ as we condition on this. "
What is the value of a state when there is a certain probability that agent will die after each step?,"
We assume infinite horizon and discount factor $\gamma = 1$. At each step, after the agent takes an action and gets its reward, there is a probability $\alpha = 0.2$, that agent will die. The assumed maze looks like this

Possible actions are go left, right, up, down or stay in a square. The reward has a value 1 for any action done in the square (1,1) and zero for actions done in all the other squares.
With this in mind, what is the value of a square (1,1)?
The correct answer is supposed to be 5, and is calculated as $1/(1\cdot 0.2) = 5$. But why is that? I didn't manage to find any explanation on the net, so I am asking here. 
","['reinforcement-learning', 'markov-decision-process', 'value-functions', 'value-iteration', 'discount-factor']","I will fill in some details in shaabhishek's answer for people who are interested.With this in mind, what is the value of a square (1,1)?First of all, the value function is dependent on a policy. The supposed correct answer you provided is the value of $(1, 1)$ under the optimal policy, so from now on, we will assume that we are finding the value function under the optimal policy. Also, we will assume that the environment dynamics are deterministic: choosing to take an action will guarantee that the agent moves in that direction.Possible actions are go left, right, up, down or stay in a square. Reward has a value 1 for any action done in square (1,1) and zero for actions done in all the other squares.Based on this information, the optimal policy at $(1, 1)$ should be to always stay in that square. The agent doesn't receive any reward for being in another square, and the probability of dying is the same for each square, so choosing the action to stay in square $(1, 1)$ is best.The correct answer is supposed to be 5, and is calculated as $\frac{1}{1 \cdot 0.2} = 5$. But why is that?By the Bellman Equation, the value function under the optimal policy $\pi_*$ at $(1,1)$ can be written as follows:$$v_{\pi_*}((1, 1))  = \mathbb{E}_{\pi_*}\left[R_t + \gamma v_{\pi_{*}}(s') | s = (1,1)\right],$$where $R_t$ denotes the immediate reward, $s$ denotes the current state, and $s'$ denotes the next state. By the problem statement, $\gamma = 1$. The next state is the $\texttt{dead}$ terminal state $\alpha = 20\%$ of the time. Terminal states have value $0$, as they do not accrue future rewards. The next state $s'$ is equal to $(1, 1)$ the remaining $(1-\alpha) = 80\%$ of the time because our policy dictates to remain in the same state and we assumed the dynamics were deterministic. Since expectation is linear, we can rewrite the expectation as follows (replacing $\gamma$ with $1$):\begin{align*}
v_{\pi_*}((1,1))  &= \mathbb{E}_{\pi_*}\left[R_t + v_{\pi_{*}}(s') | s = (1,1)\right]\\
&= \mathbb{E}_{\pi_*}\left[R_t |s=(1, 1)\right]+  \mathbb{E}_{\pi_*}\left[v_{\pi_{*}}(s') | s = (1,1)\right].\qquad (*)
\end{align*}We have $$\mathbb{E}_{\pi_*}\left[R_t |s=(1, 1)\right] = 1\qquad (**)$$because we are guaranteed an immediate reward of $1$ when taking an action in state $(1, 1)$. Also, from the comments above regarding the next state values and probabilities, we have the following:\begin{align*}\mathbb{E}_{\pi_*}\left[v_{\pi_{*}}(s') | s = (1,1)\right] &= (1-\alpha) \cdot v_{\pi_{*}}((1,1)) + \alpha \cdot v_{\pi_*}(\texttt{dead})\\
&= 0.8 \cdot v_{\pi_{*}}((1,1)) + 0.2 \cdot 0\\
&= 0.8 \cdot v_{\pi_{*}}((1,1)).\qquad (***)
\end{align*}Substituting $(**)$ and $(***)$ into $(*)$ yields the following:\begin{align*}
v_{\pi_*}((1,1)) &= 1 + 0.8 \cdot v_{\pi_{*}}((1,1))\\
v_{\pi_*}((1,1)) - 0.8 \cdot v_{\pi_{*}}((1,1)) &= 1\\
(1-0.8)v_{\pi_*}((1,1)) &= 1\\
v_{\pi_*}((1,1))  &= \frac{1}{1-0.8} = \frac{1}{0.2} = 5.
\end{align*}"
"How to compare SegNet, U-Net and EfficientNet?","
SegNet and U-Net are created for segmentation problem and EfficientNet is created for classification problem. I have a task and it is saying that train these models on the same dataset and compare results. Is it possible?
","['machine-learning', 'deep-learning', 'ai-design', 'tensorflow', 'image-segmentation']",
How is per-decision importance sampling derived in Sutton & Barto's book?,"
In per-decison importance sampling given in Sutton & Barto's book:

Eq 5.12 $\rho_{t:T-1}R_{t+k} = \frac{\pi(A_{t}|S_{t})}{b(A_{t}|S_{t})}\frac{\pi(A_{t+1}|S_{t+1})}{b(A_{t+1}|S_{t+1})}\frac{\pi(A_{t+2}|S_{t+2})}{b(A_{t+2}|S_{t+2})}......\frac{\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}R_{t+k}$
Eq 5.13 $\mathbb{E}\left[\frac{\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})}\right] = \displaystyle\sum_ab(a|S_k)\frac{\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})} = \displaystyle\sum_a\pi(a|S_k) = 1$
Eq.5.14 $\mathbb{E}[\rho_{t:T-1}R_{t+k}] = \mathbb{E}[\rho_{t:t+k-1}R_{t+k}]$

As full derivation is not given, how do we arrive at Eq 5.14 from 5.12?
From what i understand :
1) $R_{t+k}$ is only dependent on action taken at $t+k-1$ given state at that time i.e. only dependent on $\frac{\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}$ 
2) $\frac{\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})}$ is independent of $\frac{\pi(A_{k+1}|S_{k+1})}{b(A_{k+1}|S_{k+1})}$ , so $\mathbb{E}\left[\frac{\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})}\frac{\pi(A_{k+1}|S_{k+1})}{b(A_{k+1}|S_{k+1})}\right] = \mathbb{E}\left[\frac{\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})}\right]\mathbb{E}\left[\frac{\pi(A_{k+1}|S_{k+1})}{b(A_{k+1}|S_{k+1})}\right], \forall \, k\in [t,T-2]$
Hence, $\mathbb{E}[\rho_{t:T-1}R_{t+k}]= \mathbb{E}\left[\frac{\pi(A_{t}|S_{t})}{b(A_{t}|S_{t})}\frac{\pi(A_{t+1}|S_{t+1})}{b(A_{t+1}|S_{t+1})}\frac{\pi(A_{t+2}|S_{t+2})}{b(A_{t+2}|S_{t+2})}......\frac{\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}R_{t+k}\right] \\= \mathbb{E}\left[\frac{\pi(A_{t}|S_{t})}{b(A_{t}|S_{t})}\frac{\pi(A_{t+1}|S_{t+1})}{b(A_{t+1}|S_{t+1})}\frac{\pi(A_{t+2}|S_{t+2})}{b(A_{t+2}|S_{t+2})}....\frac{\pi(A_{t+k-2}|S_{t+k-2})}{b(A_{t+k-2}|S_{t+k-2})}\frac{\pi(A_{t+k}|S_{t+k})}{b(A_{t+k}|S_{t+k})}......\frac{\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}\right]\mathbb{E}\left[\frac{\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}R_{t+k}\right] \\= \mathbb{E}\left[\frac{\pi(A_{t}|S_{t})}{b(A_{t}|S_{t})}\right]\mathbb{E}\left[\frac{\pi(A_{t+1}|S_{t+1})}{b(A_{t+1}|S_{t+1})}\right]\mathbb{E}\left[\frac{\pi(A_{t+2}|S_{t+2})}{b(A_{t+2}|S_{t+2})}\right]....\mathbb{E}\left[\frac{\pi(A_{t+k-2}|S_{t+k-2})}{b(A_{t+k-2}|S_{t+k-2})}\right]\mathbb{E}\left[\frac{\pi(A_{t+k}|S_{t+k})}{b(A_{t+k}|S_{t+k})}\right]......\mathbb{E}\left[\frac{\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}\right]\mathbb{E}\left[\frac{\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}R_{t+k}\right] \\= \mathbb{E}[\frac{\pi_{t+k-1}}{b_{t+k-1}}R_{t+k}]\\=\mathbb{E}[\rho_{t+k-1}R_{t+k}]$ 
which is not equal to eq 5.14. What's the mistake in the above calculations? Are 1 and 2 correct?
","['reinforcement-learning', 'off-policy-methods', 'expectation', 'importance-sampling', 'conditional-probability']","As mentioned in the comments your assumption about independence is wrong. Here's why. To prove independence we need to show the following holds:$$P(X=x, Y=y) = P(X=x)P(Y=y)$$in the case of RL this becomes:$$P(X=a, X=a') = P(X=a)P(Y=a')$$The left hand side has the value:$$P(X=a, Y=a') = b(A_t = a| S_t = s) p(s'|a,s) b(A_{t+1} = a'|, S_{t+1} = s')$$while the right hand side has the value:$$P(X=a)P(Y=a') = b(A_t = a| S_t = s)b(A_{t+1} = a'| S_{t+1} = s')$$And hence not independent.Now let use look at why the following expression holds:Eq.5.14:   $\mathbb{E}[\rho_{t:T-1}R_{t+k}] = \mathbb{E}[\rho_{t:t+k-1}R_{t+k}]$I will not derive the exact expressions, but I hope you can form the reasoning I provide. By the rules of probability we know that sum of joint probability is equal to 1 i.e.:$$\sum_{X_1..X_n} P(X_1=a_1, X_2=a_2,...X_n = a_n) = 1$$I have alredy showed above, the trajectory is not independent. So $R_{t+k}$ will depend on the trajectory $S_{t:t+k-1}$ where $S_{t:t+k-1}$ is a particular trajectory. At the end of this trajectory we get a reward $R_{t+k}$ and thus $R_{t+k}$ is exclusively a function of $S_{t:t+k-1}$ i.e. $R_{t+k} = f(S_{t:t+k-1})$. The trajectory after this $S_{t+k:T-1}$ irrelevant since it will always sum up of to 1. i.e once you have reached a particular state at time step $t+k-1$ you are now conditioning based on that $P(S_{t+k:T-1}|S_{t:t+k-1})$ and taking the expected value over all trajectories possible from thereon i.e. $\sum_{S_{t+k:T-1}} P(S_{t+k:T-1}|S_{t:t+k-1})$ which is 1 by probability rules. Thus, what you are really doing is:$$P(S_{t:t+k-1})R_{t+k}(\sum_{S_{t+k:T-1}} P(S_{t+k:T-1}|S_{t:t+k-1}))$$and hence the remaining trajectory has no contribution.Another way of thinking this is you are taking weighted trajectories till time step $t+k-1$ weighted by rewards $R_{t+k}$ and hence you cannot sum up to 1. The rest of the trajectory after $t+k-1$ will sum up to 1.I hope this qualitative description suffices. You can do the maths, but you must be careful with the notations and the assumptions you make.Also all the equations are correct, I hope you can indirectly see it from my reasoning."
Deriving hyperparameter updates in Online Interactive Collaborative Filtering,"
I've been going through ""Online Interactive Collaborative Filtering Using Multi-Armed Bandit with Dependent Arms"" by Wang et al. and am unable to understand how the update equations for the hyperparameters (section 4.3, equation set (23)) were derived. I'd deeply appreciate it if anyone could provide a full or partial derivation of the updates. Any general suggestions regarding how to proceed with the derivation would also be appreciated.
ICTR Graphical Model

The variables are sampled as below
$$\mathbb{p}_m|\lambda \sim \text{Dirichlet}(\lambda)$$
$$\sigma^2_n|\alpha,\beta \sim \text{Inverse-Gamma}(\alpha,\beta)$$
$$\mathbb{q}_n |\mu_{\mathbb{q}}, \Sigma_{\mathbb{q}}, \sigma_n^2 \sim \mathcal{N}(\mu_{\mathbb{q}}, \sigma_n^2\Sigma_{\mathbb{q}})$$
$$\mathbb{\Phi}_k |\eta \sim \text{Dirichlet}(\eta)$$
$$z_{m,t} | \mathbb{p}_m \sim \text{Multinomial}(\mathbb{p}_m)$$
$$x_{m,t} | \mathbb{\Phi}_k \sim \text{Multinomial}(\mathbb{\Phi}_k) $$ 
$$y_{m,t} \sim \mathcal{N}(\mathbb{p}_m^T\mathbb{q}_n, \sigma_n^2)$$
And the update equations are below 

","['papers', 'hyperparameter-optimization', 'online-learning', 'bayesian-networks', 'probabilistic-graphical-models']",
Will adding memory to a supervised learning system makes it into a Bayesian learning system?,"
Seung et.al recently published GameGAN paper, GameGAN learned and stored the whole Pacman game and was able to reproduce it without a game engine. The uniqueness of GameGAN is that it had added memory to its discriminator/generator which helped it to store the game states.

In Bayesian interpretation, a supervised learning system learns by optimizing weights which maximizes a likelihood function.
$$\hat{\boldsymbol{\theta}} = \mathop{\mathrm{argmax}} _ {\boldsymbol{\theta}} P(X \mid \boldsymbol{\theta})$$
Will adding memory which can store prior information makes GameGAN a Bayesian learning system?
Can GameGAN or similar neural network with memory can be considered as a bayesian learning system. If yes, then which of these two equations(or something other) correctly explains this system (considering prior as memory)?

$$\mathop{\mathrm{argmax}} \frac{P(X \mid \boldsymbol{\theta})P(\boldsymbol{\theta})}{P(X)}$$

or

$$\mathop{\mathrm{argmax}} \frac{P(X_t \mid \boldsymbol{X^{t+1}})P(\boldsymbol{X^{t+1}})}{P(X_t)}$$

PS: I understand GAN's are unsupervised learning systems, but we can assume discriminator and generator models separately trying to find weights that maximize their individual likelihood function.
","['generative-adversarial-networks', 'bayesian-deep-learning', 'bayesian-statistics', 'game-gan']",
Why does the n-step return being zero result in high variance in off policy n-step TD?,"
In the paragraph given between eq 7.12 and 7.13 in Sutton & Barto's book:

$G_{t:h} = R_{t+1} + G_{t+1:h} , t < h < T$
where $G_{h:h} = V_{h-1}(S_h)$. (Recall that this return is used at time h, previously denoted t + n.) Now consider the effect of following a behavior policy $b$ that is not the same as the target policy $\pi$. All of the resulting experience, including the ﬁrst reward $R_{t+1}$ and the next state $S_{t+1}$ must be weighted by the importance sampling ratio for time $t, \rho_t = \frac{\pi(A_t|S_t)}{b(A_t|S_t)}$ . One might be tempted to simply weight the righthand side of the above equation, but one can do better. Suppose the action at time t would never be selected by $\pi$, so that $\rho_t$ is zero. Then a simple weighting would result in the n-step return being zero, which could result in high variance when it was used as a target.

Why does the n-step return being zero results in high variance?
Also, why is the experience weighted by $\rho_t$, it should be weighted by $\rho_{t:t+h}$?
","['reinforcement-learning', 'temporal-difference-methods', 'importance-sampling', 'return']",
Problem in understanding equation given for convergence of TD(n) algorithm,"
Given equation 7.3 of Sutton and Barto's book for convergence of TD(n): 

$\max_s|\mathbb{E}_\pi[G_{t:t+n}|S_t = s] - v_\pi(s)| \leqslant \gamma^n \max_s|V_{t+n-1}(s) - v_\pi(s)|$

$\textbf{PROBLEM 1}$ : Why is this error $|\mathbb{E}_\pi[G_{t:t+n}|S_t = s] - v_\pi(s)|$ compared with the error $|V_{t+n-1}(s) - v_\pi(s)|$.
There can be two other logical comparisons for the convergence of algorithm($TD(n)$):
1) If we compare and say that $V_{t+n-1}(s)$ is more close to $v_\pi(s)$ than $V_{t+n-2}(s)$ i.e we compare $|V_{t+n-1}(s) - v_\pi(s)|$ with $|V_{t+n-2}(s) - v_\pi(s)|$ 
2) We can also compare $|\mathbb{E}_\pi[G_{t:t+n}|S_t = s] - v_\pi(s)|$ with $|\mathbb{E}_\pi[V_{t+n-1}(S_t)|S_t = s] - v_\pi(s)|$ to show that  $\mathbb{E}_\pi[G_{t:t+n}|S_t = s]$ is better than $\mathbb{E}_\pi[V_{t+n-1}(S_t)|S_t = s]$ hence moving $V_{t+n-1}(S_t)$ towards $G_{t:t+n}$(as done in eq 7.2) can lead to convergence.
$\textbf{PROBLEM 2}$ : Are the above 2 methods of comparison for testing convergence correct.
Equations for reference:
Eq 7.1: $G_{t:t+n} = R_{t+1} + \gamma R_{t+2} +......+\gamma^{n-1}R_{t+n} + \gamma^{n}V_{t+n-1}(S_{t+n})$
Eq 7.2: $V_{t+n}(S_t) = V_{t+n-1}(S_t) + \alpha [G_{t:t+n} - V_{t+n-1}(S_t)]$
","['reinforcement-learning', 'temporal-difference-methods', 'value-iteration', 'expectation']",
Why isn't my decision tree classifier able to solve the XOR problem properly?,"
I was trying to solve an XOR problem, and the dataset seems like the one in the image.

I plotted the tree and got this result:

As I understand, the tree should have depth 2 and four leaves. The first comparison is annoying, because it is close to the right x border (0.887). I've tried other parameterizations, but the same result persists.
I used the code below:
from sklearn.tree import DecisionTreeClassifier    

clf = DecisionTreeClassifier(criterion='gini')
clf = clf.fit(X, y)

fn=['V1','V2']

fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (3,3), dpi=300)

tree.plot_tree(clf, feature_names = fn, class_names=['1', '2'], filled = True);

I would be grateful if anyone can help me to clarify this issue. 
","['machine-learning', 'python', 'decision-trees', 'scikit-learn']","I can reproduce this problem for an even more easily separable dataset:The ideal tree for it should be as follows:However, when I run DecisionTreeClassifier with the maximal depth = 2 in scikit-learn many times, it splits the dataset randomly and never gets it right.This is an example of 4 different runs:The problem is that scikit-learn has only two measures of the quality of a split: gini, and entropy. Both of them estimate mutual information between the target and only one predictor.However, in XOR problem, mutual information of each predictor with the target is zero. You can read more about it here: link from which you can see that this problem exists not only for XOR but for any task where interaction between features is important.In order to solve it, the tree should be built based neither on the Gini impurity, nor on the information gain but on measures that estimate how the target depends on multiple features, e.g. multivariate mutual information,  distance correlation, etc which might solve simple problems like XOR but might fail in the case of real tasks. It is easy to find simple cases when they fail (just try them for a regression for simple non-linear functions of a few variables). There is no such a measure that would estimate the dependence of a target on multiple interacting predictors very well and would work for all problems.EDIT to answer Asher's comment: I did several runs for max_depth=3. It is better than for max_depth=2 but still misses the correct classification from time to time. Taking max_depth=4 almost always gets XOR correctly with the occasional misses. Below are pictures of some runs for max_depth=3 and max_depth=4.However, the trees for max_depth=3 and max_depth=4 become ugly. They are ugly not only because they are bigger than the ideal tree shown above but they totally obscure the XOR function. For example, can you decipher an XOR from this tree?It is probably possible with some pruning technique but still, an extra work."
Studying the speech-generation model and have question about the confusing nature of model input and outputs,"
I am currently studying this model speech generation known as WaveNet model by Google using the linked original paper and this implementation.
I find the model very confusing in the input it takes and the output it generates, and some of the layer dimensions didn't seem to match based on what I understood from the WaveNet paper, or am I misunderstanding something?

What is the input to the WaveNet, isn't this a mel-spectrum input and not just 1  floating point value for raw audio? E.g. the input kernel layer shows as shaped 1x1x128. Isn't the input to the input_convolution layer the mel-spectrum frames, which are 80 float values * 10,000 max_decoder_steps, so the in_channels for this conv1d layer should be 80 instead of 1?


inference/input_convolution/kernel:0 (float32_ref 1x1x128) [128, bytes: 512]


Is there reason for upsampling stride values to be [11, 25], like are the specific numbers 11 and 25 special or relevant in affecting other shapes/dimensions?

inference/ConvTranspose1D_layer_0/kernel:0 (float32_ref 1x11x80x80) [70400, bytes: 281600]
inference/ConvTranspose1D_layer_1/kernel:0 (float32_ref 1x25x80x80) [160000, bytes: 640000]


Why is the input-channels in  residual_block_causal_conv 128 and residual_block_cin_conv 80? What exactly is their inputs? (e.g. is it mel-spectrum or just a raw floating point value?) Is the wavenet-vocoder generating just 1 float value per 1 input mel-spectrum frame of 80 floats?

inference/ResidualConv1DGLU_0/residual_block_causal_conv_ResidualConv1DGLU_0/kernel:0 (float32_ref 3x128x256) [98304, bytes: 393216]
inference/ResidualConv1DGLU_0/residual_block_cin_conv_ResidualConv1DGLU_0/kernel:0 (float32_ref 1x80x256) [20480, bytes: 81920]

I was able to print the whole WaveNet network using print(tf.trainable_variables()), but the model still seems very confusing.
","['deep-learning', 'tensorflow', 'speech-recognition', 'speech-synthesis', 'transpose-convolution']",
"When we use a neural network to approximate the Q values, is the Q target a single value?","
I have two questions 

When we use our network to approximate our Q values, is the Q target a single value?
During backpropagation, when the weights are updated, does it automatically update the Q values, shouldn’t the state be passed in the network again to update it?

","['neural-networks', 'reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']","When we use our network to approximate our Q values,is the Q target a single value?Yes, the target Q value is a single value if you are just updating a single training example. The loss function of a vanilla DQN for a single experience tuple $(s_t,a_t,r_t,s_{t+1})$ is calculated as $$L(\theta) = [r_t + \gamma \,max\,Q_{a_{t+1}}(s_{t+1},a_{t+1};\theta) - Q(s_t,a_t;\theta)]^2$$ where $r_t + \gamma \,max\,Q(s_{t+1},a_{t+1};\theta)$ is the target Q value. However, when using mini-batch gradient descent, you would have to compute multiple target Q values equivalent to the batch sizeDuring backpropagation, when the weights are updated, does it automatically update the Q values, shouldn’t the state be passed in the network again to update it?During backpropagation of the loss function, the weights $\theta$ are automatically updated. You do not need to pass in the state again. Because in the first place, you would have computed $Q(s_t,a_t;\theta)$ by passing in the state as input to the neural network. That is how backpropagation works for Deep Q networks.Training for the DQN is as follows:Also, since the weights have changed after backpropagation, the Q values for the same state would also be updated if you pass the same state in to the network again. Check out this paper as it explains how Deep Q Network works."
What is the reason for different learned features in upper and lower half in AlexNet?,"
I was reading AlexNet paper and the authors quoted 

the kernels on one GPU were ""largely color agnostic,"" whereas the kernels on the other GPU were largely ""color-specific.""

The upper GPU takes operates on filters on the top and lower GPU deals with the lower half. But what is the reason for each of them learning a different set of features, i.e. the top half of kernels learning the edges mostly and the bottom kernels learning color variation? Is there any reason behind it?

","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'alexnet']",
"Can we use a pre trained Encoder (BERT, XLM ) with a Decoder (GPT, Transformer-XL) to build a Chatbot instead of Language Translation?","
I was wondering if the BERT or T5 models can do the task of generating sentences in English. Most of the models I have mentioned are trained to translate from English to German or French. Is it possible that I can use the output of BERT as an input to my Decoder? My theory is that when I already have the trained Embeddings, I do not need to train the Encoder part. I can just add the outputs of sentences to the decoder to generate the sentences.
In place of finding the loss value from the translated version, Can I compute loss on the reply of a sentence?
Can someone point me toward a tutorial where I can use the BERT output for the decoder part? I have a data of conversation with me. I want to build a Chatbot from that data.
I have already implemented LSTM based Sequence2sequence model but it is not providing me satisfactory answer.
After some research, 2 such models are there as T5 and BART which are based on the same idea.
If possible, can someone tell me how can I use BART or T5 to make a conversational bot?
","['machine-learning', 'deep-learning', 'natural-language-processing', 'deep-neural-networks', 'bert']",
What is a fully convolution network?,"
I was surveying some literature related to Fully Convolutional Networks and came across the following phrase, 

A fully convolutional network is achieved by replacing the parameter-rich fully connected layers in standard CNN architectures by convolutional layers with $1 \times 1$ kernels.

I have two questions.

What is meant by parameter-rich? Is it called parameter rich because the fully connected layers pass on parameters without any kind of ""spatial"" reduction? 
Also, how do $1 \times 1$ kernels work? Doesn't $1 \times 1$ kernel simply mean that one is sliding a single pixel over the image? I am confused about this.

","['machine-learning', 'convolutional-neural-networks', 'computer-vision', 'image-segmentation', 'fully-convolutional-networks']","A fully convolution network (FCN) is a neural network that only performs convolution (and subsampling or upsampling) operations. Equivalently, an FCN is a CNN without fully connected layers.The typical convolution neural network (CNN) is not fully convolutional because it often contains fully connected layers too (which do not perform the convolution operation), which are parameter-rich, in the sense that they have many parameters (compared to their equivalent convolution layers), although the fully connected layers can also be viewed as convolutions with kernels that cover the entire input regions, which is the main idea behind converting a CNN to an FCN. See this video by Andrew Ng that explains how to convert a fully connected layer to a convolutional layer.An example of a fully convolutional network is the U-net (called in this way because of its U shape, which you can see from the illustration below), which is a famous network that is used for semantic segmentation, i.e. classify pixels of an image so that pixels that belong to the same class (e.g. a person) are associated with the same label (i.e. person), aka pixel-wise (or dense) classification.So, in semantic segmentation, you want to associate a label with each pixel (or small patch of pixels) of the input image. Here's a more suggestive illustration of a neural network that performs semantic segmentation.There's also instance segmentation, where you also want to differentiate different instances of the same class (e.g. you want to distinguish two people in the same image by labeling them differently). An example of a neural network that is used for instance segmentation is mask R-CNN. The blog post Segmentation: U-Net, Mask R-CNN, and Medical Applications (2020) by Rachel Draelos describes these two problems and networks very well.Here's an example of an image where instances of the same class (i.e. person) have been labeled differently (orange and blue).Both semantic and instance segmentations are dense classification tasks (specifically, they fall into the category of image segmentation), that is, you want to classify each pixel or many small patches of pixels of an image.In the U-net diagram above, you can see that there are only convolutions, copy and crop, max-pooling, and upsampling operations. There are no fully connected layers.So, how do we associate a label to each pixel (or a small patch of pixels) of the input? How do we perform the classification of each pixel (or patch) without a final fully connected layer?That's where the $1 \times 1$ convolution and upsampling operations are useful!In the case of the U-net diagram above (specifically, the top-right part of the diagram, which is illustrated below for clarity), two $1 \times 1 \times 64$ kernels are applied to the input volume (not the images!) to produce two feature maps of size $388 \times 388$. They used two $1 \times 1$ kernels because there were two classes in their experiments (cell and not-cell). The mentioned blog post also gives you the intuition behind this, so you should read it.If you have tried to analyze the U-net diagram carefully, you will notice that the output maps have different spatial (height and weight) dimensions than the input images, which have dimensions $572 \times 572 \times 1$.That's fine because our general goal is to perform dense classification (i.e. classify patches of the image, where the patches can contain only one pixel), although I said that we would have performed pixel-wise classification, so maybe you were expecting the outputs to have the same exact spatial dimensions of the inputs. However, note that, in practice, you could also have the output maps to have the same spatial dimension as the inputs: you would just need to perform a different upsampling (deconvolution) operation.A $1 \times 1$ convolution is just the typical 2d convolution but with a $1\times1$ kernel.As you probably already know (and if you didn't know this, now you know it), if you have a $g \times g$ kernel that is applied to an input of size $h \times w \times d$, where $d$ is the depth of the input volume (which, for example, in the case of grayscale images, it is $1$), the kernel actually has the shape $g \times g \times d$, i.e. the third dimension of the kernel is equal to the third dimension of the input that it is applied to. This is always the case, except for 3d convolutions, but we are now talking about the typical 2d convolutions! See this answer for more info.So, in the case we want to apply a $1\times 1$ convolution to an input of shape $388 \times 388 \times 64$, where $64$ is the depth of the input, then the actual $1\times 1$ kernels that we will need to use have shape  $1\times 1 \times 64$ (as I said above for the U-net). The way you reduce the depth of the input with $1\times 1$ is determined by the number of $1\times 1$ kernels that you want to use. This is exactly the same thing as for any 2d convolution operation with different kernels (e.g. $3 \times 3$).In the case of the U-net, the spatial dimensions of the input are reduced in the same way that the spatial dimensions of any input to a CNN are reduced (i.e. 2d convolution followed by downsampling operations). The main difference (apart from not using fully connected layers) between the U-net and other CNNs is that the U-net performs upsampling operations, so it can be viewed as an encoder (left part) followed by a decoder (right part)."
My Double DQN with Experience Replay produces a no-action decision most of the time. Why?,"
I've written a Double DQN-based stock trading bot using mainly time series stock data. The internal network of the Double DQN is a LSTM which handles the time series data. An Experience Replay buffer is also used. The objective function is cumulative stock return over the test period. My epsilon used for exploration is 0.1 (which I think is already very high).
My trading bot has a very simple action space, trade or no-trade. 
-- When it decides to trade, it sends a signal to buy and own a stock for a day. I'd get a positive return if stock price has gone up from today to tomorrow; equally would get a negative return if stock price has gone down.
-- When it decides to not trade, I own no stock and the daily return is 0 because there is no trading.
Strangely, my algorithm gives a daily 'no trade' signal most of the time when I run the algo through a number of different test periods. 
Very often, after giving a 'no trade' signal for many days, the algo would finally give a 'trade' signal but the next day reverse back to giving 'no trade' right away.
My questions:
Why am I getting this phenomenon? Most importantly, what can I do to make the algo not stuck in giving out 'no trade' signal most of the time?
","['machine-learning', 'reinforcement-learning', 'q-learning', 'dqn', 'experience-replay']",
Why can't neural networks be applied to preference learning problems?,"
In section 6.1 of the paper Neural Networks in Economics, the authors say

this leads to the problem, that no risk can be formulated which shall be minimized by a Neural Network learning algorithm.

So, why can't neural networks be applied to preference learning problems? 
See sections 6.0 of the same paper for a definition of preference learning.
","['neural-networks', 'machine-learning', 'papers']",
Can we achieve what a CNN can do with just a normal neural network?,"
When I was learning about neural networks, I saw that a complex neural network can understand the MNIST dataset and a simple convolution network can also understand the same. So I would like to know if we can achieve a CNN's functionality with just using a simple neural network without the convolution layer and if we can then how to convert a CNN into an ANN. 
","['neural-networks', 'convolutional-neural-networks', 'comparison', 'feedforward-neural-networks']",
Can Reinforcement Learning be used for UAV waypoint control?,"
I want to make a drone which can follow static and dynamic waypoints. I am a total beginner in the drone field so I can't figure out that should I use Reinforcement Learning or any other learning methods for the drone to make it follow both static and dynamic waypoints. If RL is the best choice for the task, then how would I go about training the model and upload it to the flight controller. And if RL is not required, then what should I use in order to achieve this task.
Please let me know how should I begin with this task
","['reinforcement-learning', 'algorithm', 'learning-algorithms']",
What are the differences between a knowledge base and a knowledge graph?,"
During my readings, I have seen many authors using the two terms interchangeably, i.e. as if they refer to the same thing. However, we all know about Google's first quotation of ""knowledge graph"" to refer to their new way of making use of their knowledge base. Afterward, other companies are claiming to use knowledge graphs.
What are the technical differences between the two? Concrete examples will be very useful to understand better the nuances.
","['comparison', 'terminology', 'knowledge-representation', 'knowledge-graph', 'knowledge-base']",
"When to use NLP, NLG and NLU in conversation agents?","
I had read some blogs (like 1, 2 or 3) about what the difference between all three of them is. I am trying to build an open domain conversation agent using natural language AI. That agent can do casual conversation, like a friend. So, for that, I want to know what is the importance of NLP, NLG, and NLU, so that I can learn that part first.
","['natural-language-processing', 'comparison', 'chat-bots', 'natural-language-understanding']","They're all important. NLP is an umbrella term that includes the other two; NLG is only concerned with generating language, ie transforming some internal data structure into human language. NLU is about processing information contained in language, and putting it into relation with a knowledge base etc.If you don't know anything about any of these fields, then I suggest your aim is far too optimistic. I work for a company that provides a conversational AI platform for businesses to develop their own agents, and it is a complex area.If you want to have a quick go and pick up some experience, I suggest you start with ELIZA. Even though this is 'ancient', many modern chatbots still work on the same principles. There are many implementations in a number of programming languages, so you should be able to find one that suits you and you can tinker around with it."
How GAN generator produce integer RGB colored picture?,"
For traditional neural networks, I know that we can't constraint the output to be strict integers. My question is what technique does GANs use to produce integer outputs, that can be then converted to RGB colored pictures?
","['neural-networks', 'machine-learning', 'deep-learning', 'tensorflow', 'generative-adversarial-networks']",
Do you have to add a dense layer onto the final layer of an LSTM?,"
If my understanding of an LSTM is correct then the output from each LSTM unit is the hidden state from that layer. For the final layer if I wanted to predict e.g. a scalar real number, would I want to add a dense layer with 1 neuron or is it recommended to have a final LSTM layer where the output has just one hidden unit (i.e. the output dimension of the final hidden state is 1)? If we didn't add the dense layer, then the output from the hidden layer I believe would be between (-1,1), if you use the traditional activations in the LSTM unit. 
Apologies if I've used wrong terminology, there seems to be some inconsistency with LSTM's when going between literature and definition in TensorFlow etc. 
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory']",
How to calibrate model's prediction given past images?,"
I want to predict how open is the mouth given a face image. It's a regression problem (0= mouth not open, 1=mouth completely open). And something between 0 and 1 is also allowed. ConvNet works fine for one person. But when I train with many people with hope that it will generalize to an unseen person, the model suffers from not knowing the limit of a person's mouth.
For example, if a new person uses the model to predict, the model doesn't have a clue whether this person has completely opened the mouth or not. Because it's hard to know how much a person can open the mouth from one image. People's mouth openness capability is not the same. Some guys cannot open their mouth that much, but some guys can open the mouth like they can swallow an apple. The only way you can know how much a person can open the mouth is to look at multiple images of their mouth movements, especially when they open the mouth completely.
I want to know how to make the model know the limit of a person's mouth by using the info from past images.
Is there a way for me to use a few unlabeled images of a new person in order to help the model calibrate its prediction? How do I do it?
This should help the model know the min/max of the person's mouth and also knows the intermediate values between 0 and 1. If you run the model continuously on a webcam, I expect the prediction to be smooth (not noisy).
My idea is to encode many images into an embedding that can be used as a calibration vector. The vector will be fed into the model along with the person's image. But I am not sure how to do it.
Any suggestions/tutorials would be welcomed.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'computer-vision', 'time-series']",
"What is the proof that ""reward-to-go"" reduces variance of policy gradient?","
I am following the OpenAI's spinning up tutorial Part 3: Intro to Policy Optimization. It is mentioned there that the reward-to-go reduces the variance of the policy gradient. While I understand the intuition behind it, I struggle to find a proof in the literature.
","['reinforcement-learning', 'policy-gradients', 'proofs', 'reward-to-go']",
Corner detection algorithm gives very high value for slanted edges?,"
I have tried implementing a basic version of shi-tomasi corner detection algorithm. The algorithm works fine for corners but I came across a strange issue that the algorithm also gives high values for slanted(titled) edges.
Here's what i did

Took gray scale image
computer dx, and dy of the image by convolving it with sobel_x and sobel_y
Took a 3 size window and moved it across the image to compute the sum of the elements in the window.
computed sum of the window elements from the dy image and sum of window elements from the dx image and saved it in sum_xx and sum_yy.
created a new image (call it result) where that pixel for which the window sum was computed was replaced with min(sum_xx, sum_yy) as shi-tomasi algorithm requires.

I expected it to give maximum value for corners where dx and dy both are high, but i found it giving high values even for titled edges. 
Here are the some outputs of the image i received: 

Result:

so far so good, corners have high values.
Another Image:

Result:

Here's where the problem lies. edges have high values which is not expected by the algorithm. I can't fathom how can edges have high values for both x and y gradients (sobel being close approximation of gradient). 
I would like to ask your help, if you can help me fix this issue for edges. I am open to any suggestions and ideas .
Here's my code (if it helps):
def shi_tomasi(image, w_size):
    ans = image.copy()
    dy, dx = sy(image), sx(image)

    ofset = int(w_size/2)
    for y in range(ofset, image.shape[0]-ofset):
        for x in range(ofset, image.shape[1]-ofset):

            s_y = y - ofset
            e_y = y + ofset + 1

            s_x = x - ofset
            e_x = x + ofset + 1

            w_Ixx = dx[s_y: e_y, s_x: e_x]
            w_Iyy = dy[s_y: e_y, s_x: e_x]

            sum_xx = w_Ixx.sum()
            sum_yy = w_Iyy.sum()

            ans[y][x] = min(sum_xx, sum_yy)
    return ans

def sy(img):
    t = cv2.Sobel(img,cv2.CV_8U,0,1,ksize=3)
    return t
def sx(img):
    t = cv2.Sobel(img,cv2.CV_8U,1,0,ksize=3)
    return t

","['python', 'computer-vision', 'feature-extraction']",
Why are the value functions sometimes written with capital letters and other times with lower-case letters?,"
Why are the state-value and action-value functions are sometimes written in small letters and other times in capitals? For instance, why in the Q-learning algorithm (page 131 of Barto and Sutton's book but not only), we the capitals are used $Q(S, A)$, while the Bellman equation it is $q(s,a)$? 
","['reinforcement-learning', 'value-functions', 'notation']",
Non-differentiable reward function to update a neural network,"
In Reinforcement Learning, when reward function is not differentiable, a policy gradient algorithm is used to update the weights of a network. In the paper Neural Architecture Search with Reinforcement Learning they use accuracy of one neural network as the reward signal then choose a policy gradient algorithm to update weights of another network. 
I cannot wrap my head around the concept of accuracy as a non-differentiable reward function. Do we need to find the function and then check if it is mathematically non-differentiable? 
I was wondering if I can use another value, for example silhouette score (in a different scenario) as the reward signal?
","['reinforcement-learning', 'policy-gradients', 'rewards']",
How to choose a suitable threshold value for the Shi-Tomasi corner detection algorithm?,"
While implementing the Shi-Tomasi corner detection algorithm, I got stuck in deciding a suitable threshold for corner detection.
In the Shi-Tomasi algorithm, all those points that qualify $\min( \lambda_1, \lambda_2) > \text{threshold}$ are considered as corner points. (where $\lambda_1, \lambda_2$ are eigenvalues).
My question is: what is a suitable criterion to decide that threshold?
","['computer-vision', 'hyperparameter-optimization', 'feature-extraction']",
Is the generator distribution in GAN's continuous or discrete?,"
I have some trouble with the probability densities described in the original paper.  My question is based on Goodfellow's paper and tutorial, respectively: Generative Adversarial Networks and NIPS 2016 Tutorial: Generative Adversarial Networks.
When Goodfellow et al. talk about probability distributions/densities in their paper, are they talking about discrete or continuous probability distributions? I don't think it's made clear. 
In the continuous case, it would imply, for instance, that both $p_{data}$ and $p_g$  must be differentiable since the optimal discriminator (see Prop. 1) is essentially a function of their ratio and is assumed to be differentiable. Also, the existence of a continuous $p_g$ is non-trivial. One sufficient condition would be that $G$ is a diffeomorphism (see normalising flows), but this is rarely the case. So it seems that much stronger assumptions are needed. 
In the case that the answer is discrete distributions: the differentiability of $G$ implies continuous outputs of the generator. How can this work together with a discrete distribution of its outputs? Does the answer have something to do with the fact that we can only represent a finite set of numbers with computers anyway?
","['machine-learning', 'generative-adversarial-networks', 'probability', 'probability-distribution']",
Is there a tutorial for beginners on capsule neural networks?,"
I am interested in capsule neural networks. I have already read the paper Dynamic Routing Between Capsules, but it is a little bit difficult to follow. Is there a tutorial for beginners on capsule neural networks?
","['neural-networks', 'reference-request', 'resource-request', 'capsule-neural-network']",
What is the difference between simulated annealing and deterministic annealing?,"
Not sure if this is the right place, but I was wondering if someone could briefly explain to me the differences & similarities between simulated annealing and deterministic annealing?
I know that both methods are used for optimization and both originate from statistical physics with the intuition of reaching a minimum energy (cost) configuration by cooling (i.e. slowly reducing the temperature in the Boltzmann distribution to calculate probabilities for configurations).
Unfortunately, Wikipedia has no article about deterministic annealing and the one about simulated annealing does not mention any comparison.
This resource has a brief comparison section between the two methods, however, I do not understand why the search strategy of DA is 

based on the steepest descent algorithm 

and how 

it searches the local minimum deterministically at each temperature.

Any clarification appreciated.
","['comparison', 'optimization', 'simulated-annealing', 'deterministic-annealing']","After diving deeper into the material I am able to answer my own question:Simulated Annealing tries to optimize a energy (cost) function by stochastically searching for minima at different temparatures via a Markov Chain Monte Carlo method. The stochasticity comes from the fact that we always accept a new state $c'$ with lower energy ($\Delta E < 0$), but a new state with higher energy ($\Delta E > 0$) only with a certain probability$$p(c \to c') = \text{min}\{1, \exp(-\frac{\Delta E}{T}) \},$$
$$\Delta E = E(c') - E(c).$$Where we used the Gibbs distribution $p(c) = \frac{1}{Z}\text{exp}(\frac{-E(c)}{T})$ to calculate probabilities for each state, with $Z$ being the partition sum. The temperature $T$ plays the role of a scaling factor for the probability distribution. If $T \to \infty $ we have a uniform distribution and all states are equally possible. If $T \to 0$ we have a Dirac delta function around the global optimum. By starting with a high $T$, sampling states and gradually decreasing it, we can make sure to sample enough states from the state space and accepting energetic higher states in order to escape local minima on the way to the global optimum. After sampling long enough while slowly decreasing the temperature, we theoretically arrive at the global optimum.Deterministic Annealing on the other hand directly minimizes the free energy $F(T)$ of the system deterministically at each temperature, e.g. by Expectation-Maximization (EM-algorithm). The intuition behind it is that we like to find an optimum at a high temperature (where it is easier to find one because there are fewer local minima), accept this as intermediate solution, lower the temperature, thus scaling the cost function such it is more peaked around it's optima (making optimization a bit more difficult) and start deterministically looking for an optimum again. This is repeated until the temperature is low enough and we (hopefully) found a global solution to our problem. Major drawback is that there is no guarantee to arrive at a global optimum in contrast to simulated annealing. The whole idea of scaling the energy function is based on the concept of homotopy: ""Two continuous functions [...] can be ""continuously deformed"" into each other."""
LSTM - MAPE Loss Function gives Better Results when Data is De-Scaled before Loss Calculation,"
I am building an LSTM for predicting a price chart. MAPE resulted in the best loss function compared to MSE and MAE. MAPE function looks like this
$$\frac{1}{N}\sum^N_{i=0}\frac{|P_i - A_i|}{A_i}$$
Where $P_i$ is the current predicted value and $A_i$ is the corresponding actual value. In neural network, it is always advised to scale the data between a small range close to zeros such as [0, 1]. In this case scaling range of [0.001, 1] is imperative to remove a possible division by zero.
Due to the MAPE denominator, the close the scaling range is to zero the larger the loss function becomes for a given $|P_i - A_i|$. If on the other hand, the data is de-scaled just before it is inserted in the MAPE function, the same $|P_i - A_i|$ would give a smaller MAPE
Consider a hypothetical example with a batch size of 1, $|P_i - A_i| = 2$ (this is scale indipendent) and $A_i = 200$. Therefore scaled $A_i = 0.04$. The MAPE error loss for the scaled version would be $\frac{2}{0.04} = 50$, and for the unscaled version $\frac{2}{200} = 0.01$
This will mean that the derivative w.r.t each weight of the scaled version will also be larger, therefore making the weights themselves even smaller. Is this correct?
I am concluding that scaling the data when using MAPE will effectively shrink the weights down more than necessary. Is that a good reason why I am seeing significantly better performance with de-scaled MAPE calculation?
Note: I am not keeping the same hyperparameters for scaled and de-scaled MAPE but a Bayesian optimization is performed with both runs, In the later a deeper network was preferred but in the scaled MAPE more regularisation was preferred.
Some expertise on this would be helpful.
","['long-short-term-memory', 'objective-functions', 'weights', 'loss', 'scalability']",
Action recognition using video stream data,"
Recently, I am working on an action recognition project where my input data is from the video stream. I read some of the concepts like ConvLstm, Convolutional Lstm, etc. I am looking for someone who already those kinds of staff already and can share his work with me that will be a really good help for me?  
","['machine-learning', 'deep-learning', 'computer-vision']",
Should the network weights converge when training Deep Q networks?,"
I have two sets of data, one training and one test set. I use the train set to train the deep q network model variant. I also continuously evaluate the agent Q values obtained on the test set every 5000 epochs I find that the agent Q values on the test set do not converge and neither do the policies.
iteration $x$: Q values for the first 5 test data are [15.271439, 13.013742, 14.137051, 13.96463, 11.490129] with policies: [15, 0, 0, 0, 15]
iteration $x+10000$:
Q values for the first 5 test data are [15.047309, 15.5233555, 16.786497, 16.100864, 13.066223] with policies: [0, 0, 0, 0, 15]
This means that the weights of the neural network are not converging. Although I can manually test each policy at each iteration and decide which of the policy performs best, I would like to know if correct training of the network would lead to weight convergence ?
Training loss plot:

You can see that the loss decreases over time however, there are occasional spikes in the loss which does not seem to go away.
","['deep-learning', 'reinforcement-learning', 'dqn']",
What are the differences between constraint satisfaction problems and linear programming?,"
I have taken an algorithms course where we talked about LP significantly, and also many reductions to LPs. As I recall, normal LP is not NP-Hard. Integer LP is NP-Hard. I am currently taking an introduction to AI course, and I was wondering if CSP is the same as LP. 
There seems an awful lot of overlap, and I haven't been able to find anything that confirms or denies my suspicions. 
If they are not the same (or one cannot be reduced to the other), what are the core differences in their concepts?
","['comparison', 'constraint-satisfaction-problems', 'linear-programming']",
How do I recognise a bandit problem?,"
I'm having difficulty understanding the distinction between a bandit problem and a non-bandit problem. 
An example of the bandit problem is an agent playing $n$ slot machines with the goal of discovering which slot machine is the most probable to return a reward. The agent learns to find the best strategy of playing and is allowed to pull the lever of one slot machine per time step. Each slot machine obeys a distinct probability of winning. 
In my interpretation of this problem, there is no notion of state. The agent potentially can utilise the slot results to determine a state-action value? For example, if a slot machine pays when three apples are displayed, this is a higher state value than a state value where three apples are not displayed. 
Why is there just one state in the formulation of this bandit problem? As there is only one action (""pulling the slot machine lever"" ), then there is one action. The slot machine action is to pull the lever, which starts the game.
I am taking this a step further now. An RL agent purchases $n$ shares of an asset and its not observable if the purchase will influence the price. The next state is the price of the asset after the purchase of the shares. If $n$ is sufficiently large, then the price will be affected otherwise there is a minuscule if any effect on the share price. Depending on the number of shares purchased at each time step, it's either a bandit problem or not. 
It is not a bandit problem if $n$ is large and the share price is affected? It is a bandit problem if $n$ is small and the share price is not affected?
Does it make sense to have a mix of a bandit and non-bandit states for a given RL problem? If so, then the approach to solving should be to consider the issue in its entirety as not being a bandit problem?
","['reinforcement-learning', 'terminology', 'definitions', 'multi-armed-bandits']","The bandit problem has one state, in which you are allowed to choose one lever among $n$ levers to pull.Why is there just one state in the formulation of this bandit problem?There is one state because the state does not change over time. Two notable consequences are that (i) pulling a lever does not change the internals of any slot machine (e.g. the distribution of rewards) and (ii) you are allowed to choose any lever without restrictions.  More generally, there is no sequential aspect of the state in this problem, as future states are unaffected by past states, actions, and rewards. It is not a bandit problem if $n$ is large and the share price is affected?Correct! If the share price is affected, then future states would be influenced by past actions. This is because the price per share is affected, which is one aspect of the state. Thus, you would need to plan a sequential strategy for your purchases. It is a bandit problem if $n$ is small and the share price is not affected?It all depends on the problem: as long as the state before buying shares remains completely the same after you purchase some shares, then yes. Share price being unaffected is only one of the requirements; another example requirement is that the maximum number of shares purchased is fixed at each time step, independent of the shares purchased previously. Does it make sense to have a mix of a bandit and non-bandit states for a given RL problem? If so, then the approach to solving should be to consider the issue in its entirety as not being a bandit problem?It makes sense to allow the share price to either be affected or unaffected based on $n$ in the same problem. Since some actions (large $n$) change the state, then there are multiple states, and actions sequentially affect the next state. Hence it is not a bandit problem as a whole, as you correctly stated.The agent potentially can utilise the slot results to determine a state-action value?Correct! I suggest reading Chapter 2 of Sutton and Barto to learn some fundamental algorithms of developing such strategies. Nice work on analyzing this problem! To help solidify your understanding and formalize the arguments above, I suggest that you rewrite the variants of this problem as MDPs and determine which variants have multiple states (non-bandit) and which variants have a single state (bandit)."
"How do we express $q_\pi(s,a)$ as a function of $p(s',r|s,a)$ and $v_\pi(s)$?","
The task (exercise 3.13 in the RL book by Sutton and Barto) is to express $q_\pi(s,a)$ as a function of $p(s',r|s,a)$ and $v_\pi(s)$. 
$q_\pi(s,a)$ is the action-value function, that states how good it is to be at some state $s$ in the Markov Decision Process (MDP), if at that state, we choose an action $a$, and after that action, the policy $\pi(s,a)$ determines future actions.
Say that we are at some state $s$, and we choose an action $a$. The probability of landing at some other state $s'$ is determined by $p(s',r|s,a)$. Each new state $s'$ then has a state-value function that determines how good is it to be at $s'$ if all future actions are given by the policy $\pi(s',a)$, therefore: 
$$q_\pi(s,a) = \sum_{s' \in S} p(s',r|s,a) v_\pi(s')$$
Is this correct?
","['reinforcement-learning', 'definitions', 'value-functions', 'sutton-barto']","Not quite. You are missing the reward at time step $t+1$.The definition you are looking for is (leaving out the $\pi$ subscripts for ease of notation)$$q(s,a) = \mathbb{E}[R_{t+1} + \gamma v(s') | S_t=s,A_t=a] = \sum_{r,s'}(r +\gamma v(s'))p(s',r|s,a)\;.$$Because $q(s,a)$ relates to expected returns at time $t$, and returns are defined as $G_t = \sum_{b = 0}^\infty \gamma ^b R_{t+b+1}$, thus $R_{t+1}$ is also a random variable at time $t$ that we need to take expectation with respect to, not just the state that we transition into."
"What is the difference between one-shot learning, transfer learning and fine tuning?","
Lately, there are lots of posts on one-shot learning. I tried to figure out what it is by reading some articles. To me, it looks like similar to transfer learning, in which we can use pre-trained model weights to create our own model. Fine-tuning also seems a similar concept to me. 
Can anyone help me and explain the differences between all three of them?
","['deep-learning', 'comparison', 'transfer-learning', 'one-shot-learning', 'fine-tuning']","They are all related terms.From top to bottom:One-shot learning aims to achieve results with one or very few examples. Imagine an image classification task. You may show an apple and a knife to a human and no further examples are needed to continue classifying. That would be the ideal outcome, but for algorithms.In order to achieve one-shot learning (or close) we can rely on knowledge transfer, just like the human in the example would do (we are trained to be amazing at image processing, but here we would also exploit other knowledge like abstract reasoning abilities, and so on).This brings us to transfer learning. Generally speaking, transfer learning is a machine learning paradigm where we train a model on one problem and then try to apply it to a different one (after some adjustments, as we'll see in a second).In the example above, classifying apples and knives is not at all trivial. However, if we are given a neural network that already excels at image classification, with super-human results in over 1000 categories... perhaps it is easy to adapt this model to our specific apples vs knives situation.This ""adapting"", those ""adjustments"", are essentially what we call fine-tuning. We could say that fine-tuning is the training required to adapt an already trained model to the new task. This is normally much less intensive than training from scratch, and many of the characteristics of the given model are retained.Fine-tuning usually covers more steps. A typical pipeline in deep learning for computer vision would be this:Note the head of our model does not match our needs (there's probably one output per category, and we only need two categories now!)Swap the very last layer(s) of the model, so that the output matches our needs, but keeping the rest of the architecture and already trained parameters intact.Train (fine-tune!) our model on images that are specific to our problem (only a few apples and knives in our silly example). We often only allow the last layers to learn at first, so they ""catch up"" with the rest of the model (in this case we talk about freezing and unfreezing and discriminative learning rates, but that's a bit beyond the question).Note that some people may sometimes use fine-tuning as a synonym for transfer learning, so be careful about that!"
Is subsection generation $O(n^4)$,"
When I say template matching, I'm referring to finding occurrences of a small image (the template) in a larger image. 
The OpenCV library provides the trivial solution, that slides the template over every possible location of the image. While this implementation provides translation invariance, it provides no stretching/scaling invariance. To get stretch and translation invariance, one would need to stretch the template (or shrink the main image) until iteratively, running the original template check over the image. This increases complexity to $O(S * n^2)$ where S is the number of different resolutions one checks - if one want's to check every possible resolution, the overall complexity is $O(n^4)$. Effectively, you generate $O(n^4)$ subsections and check if they're equal to the template.
From what I was taught, Image Segmentation Networks do just this - however, instead of using the basic template matching (i.e. checking the pixels match) the generated subsection is put through a classifier network - so this would be more expensive than standard templating.

My question is, are my calculations correct - for complete subsection generation, is the complexity $O(n^4)$ and is there really no better algorithm for generating these subsections - used by both image detection algorithms?
","['machine-learning', 'classification', 'algorithm', 'image-processing', 'time-complexity']",
"If the minimum Q value is decreasing and the maximum Q value increasing, is this a sign that dueling double DQN is diverging?","
I'm training a dueling double DQN agent with prioritized replay buffer and notice that the min Q values are decreasing, while the max Q values are increasing. 
Is this a sign that it is diverging?
Or should be just be looking at the mean Q value, which has a slight uptrend?

1 There are 2 different colored lines because the initial training (in orange) was stopped at around 1.3M time steps, and resumed (in blue) from a checkpoint at around 1.1M time steps
2 Plots are from Tensorboard, visualizing data generated by Ray/RLlib
3 Epsilon starts at 1.0 and anneals to 0.02 over 10000 time steps. The sudden increase in magnitude of Q-values appear to come after resuming from checkpoint, but might just be a coincidence.

After training for more steps...

","['reinforcement-learning', 'q-learning', 'deep-rl', 'dqn', 'convergence']",
"In Batch Normalisation, are $\hat{\mu}$, $\hat{\sigma}$ the mean and stdev of the original mini-batch or of the input into the current layer?","
In Batch Normalisation, are the sample mean and standard deviation we normalise by the mean/sd of the original data put into the network, or of the inputs in the layer we are currently BN'ing over? 
For instance, suppose I have a mini-batch size of 2 which contains $\textbf{x}_1, \textbf{x}_2$. Suppose now we are at the $k$th layer and the outputs from the previous layer are $\tilde{\textbf{x}}_1,\tilde{\textbf{x}}_2$. When we perform batch norm at this layer would be subtract the sample mean of $\textbf{x}_1, \textbf{x}_2$ or of $\tilde{\textbf{x}}_1,\tilde{\textbf{x}}_2$? 
My intuition tells me that it must be the mean,sd of $\tilde{\textbf{x}}_1,\tilde{\textbf{x}}_2$ otherwise I don't think it would be normalised to have 0 mean and sd of 1. 
","['neural-networks', 'deep-learning', 'batch-normalization']","Your intuition is correct. We will be normalizing the inputs of the layer under consideration (just right before applying the activation function). So, if this layer receives an input $\mathrm{x}=\left(x^{(1)} \ldots x^{(d)}\right)$, the formula for normalizing the $k^{th}$ dimension of $\mathrm{x}$ would look as follows:
$$\widehat{x}^{(k)}=\frac{x^{(k)}-\mathrm{E}\left[x^{(k)}\right]}{\sqrt{\operatorname{Var}\left[x^{(k)}\right]}}$$Note that in practice a constant $\epsilon$ is also added under the square root in the denominator to ensure stability. Source: The original Batch Normalization paper (Section 3). Andrew Ng's video on this topic might also be useful for illustration. "
How does $\mathbb{E}$ suddenly change to $\mathbb{E}_{\pi'}$ in this equation?,"
In Sutton-Barto's book on page 63 (81 of the pdf):
$$\mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t=s,A_t=\pi'(s)] = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_{t} = s]$$
How does $\mathbb{E}$ suddenly change to $\mathbb{E}_{\pi'}$ and the $A_t = \pi'(s)$ term disappears?
Also, in general, in the conditional expectation, which distribution do we compute the expectation with respect to? From what I have seen, in $\mathbb{E}[X \mid Y]$, we always calculate the expected value over distribution $X$.
","['reinforcement-learning', 'probability', 'probability-distribution', 'expectation', 'statistics']","Also, in general, in the conditional expectation, which distribution do we compute the expectation with respect to? From what I have seen, in $\mathbb{E}[X|Y]$, we always calculate the expected value over distribution $X$.No, for $\mathbb{E}[X|Y]$ we take expectation of $X$ with respect to the conditional distribution $X|Y$, i.e. $$\mathbb{E}[X|Y] = \int_\mathbb{R} x p(x|y)dx\;;$$where $p(x|y)$ is the density function of the conditional distribution. If your random variables are discrete then replace the integral with a summation. Also note that $\mathbb{E}[X|Y]$ is still a random variable in $Y$. How does $\mathbb{E}$ suddenly change to $\mathbb{E}_{\pi '}$ and the $A_t = \pi '(s)$ term disappears? This is because in this instance $\pi '(s)$ is a  deterministic policy, i.e. in state $s$ the policy will take action $b$ with probability 1 and all other actions with probability 0. NB: this is the convention used in Sutton and Barto to denote a deterministic policy.Without loss of generality, assume that $\pi'(s) = b$. The implication of this is that in the first expectation we have 
$$\mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = \pi'(s) = b] = \sum_{s',r}p(s',r|s,a=b)(r + \gamma v(s'))\;,$$
and in the second expectation we have 
$$\mathbb{E}_{\pi'}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] = \sum_a\pi'(a|s)\sum_{s',r}p(s',r|s,a)(r + \gamma v(s'))\;;$$
However, we know that $\pi'(a|s) = 0 \; \forall a \neq b$, so this sum over $a$ would equal 0 for all $a$ except when $a=b$, in which case we know that $\pi'(b|s) = 1$, and so the expectation becomes $$\mathbb{E}_{\pi'}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] = \sum_{s',r}p(s',r|s,a=b)(r + \gamma v(s'))\;;$$and so we have equality of the two expectations. "
Should I use exploration strategy in Policy Gradient algorithms?,"
In policy gradient algorithms the output is a stochastic policy - a probability for each action.
I believe that if I follow the policy (sample an action from the policy) I make use of exploration because each action has a certain probability so I will explore all actions for a given state.
Is it beneficial or is it common to use extra exploration strategies, like UCB, Thompson sampling, etc. with such algorithms?
","['reinforcement-learning', 'policy-gradients', 'exploration-exploitation-tradeoff', 'upper-confidence-bound', 'thompson-sampling']","I believe that if I follow the policy (sample an action from the policy) I make use of exploration because each action has a certain probability so I will explore all actions for a given state.Yes, having a stochastic policy function is the main way that a lot of policy gradient methods achieve exploration, including REINFORCE, A2C, A3C.Is it beneficial or is it common to use extra exploration strategy like UCB, Thompson sampling etc. n such algorithms?It can be, but needs to be done carefully, as the gradient sampling for the policy function is different. Many policy gradient methods are strictly on-policy and will not work if you simply add extra exploration. It is relatively straightforward to adjust the critic part of actor-critic methods by using e.g.  Q learning update rules for it. However, the gradient of the policy function is trickier.There are some policy gradient methods that do work with a separate, tunable, exploration function. Deep Deterministic Policy Gradient (DDPG) may be of interest to you - as per the title, it works with a deterministic policy function, and exploration is achieved by adding a separate noise function on top. The sampling for policy gradient is then corrected for being off-policy."
Can I apply DQN or policy gradient algorithms in the contextual bandit setting?,"
I have a problem which I believe can be described as a contextual bandit. 
More specifically, in each round, I observe a context from the environment consisting of five continuous features, and, based on the context, I have to choose one of the ten available actions. The actions do not influence the next context.
Based on the above I have the following questions:

Is this a contextual bandit or an MDP with a discount equal to zero (one step RL)? I have read that, in contextual bandits, we receive a different context for each action and I am a little bit confused.
Can I use the DQN algorithm with TD Target only the observed reward instead of the reward plus the predicted value of the next state?
Can I use a policy gradient algorithm, like REINFORCE or A2C? If yes, should I use a baseline and what this baseline should be?
I have seen in the literature that there are some algorithms for contextual bandits such as LinUCB, LinRel, NeuralBandit, etc. And I am wondering why the DQN, A2C and REINFORCE algorithms, which seem to work well in MDP setting, are not used in contextual bandits, given the fact that this problem can be described as an MDP with a discount equal to zero?

","['reinforcement-learning', 'dqn', 'actor-critic-methods', 'reinforce', 'contextual-bandits']","MDPs are strict generalisations of contextual bandits, adding time steps and state transitions, plus the concept of return as a measure of agent performance.Therefore, methods used in RL to solve MDPs will work to solve contextual bandits. You can either treat a contextual bandit as a series of 1-step episodes (with start state chosen randomly), or as a continuous problem with discount factor zero.Can I use the DQN algorithm with TD Target only the observed reward instead of the reward plus the predicted value of the next state?Yes. That's identical mathematically to having a discount of zero, or having 1-step episodes.Can I use a policy gradient algorithm, like REINFORCE or A2C? If yes, should I use a baseline and what this baseline should be?Yes. Once converted to an MDP, you can use the same baselines in these algorithms as normal (A2C's use of advantage instead of action value is already a baseline). Generally adding a baseline can help reduce variance, so it may still help when applying RL to contextual bandit problems.I have seen in the literature that there are some algorithms for contextual bandits such as LinUCB, LinRel, NeuralBandit, etc. And I am wondering why the DQN, A2C and REINFORCE algorithms, which seem to work well in MDP setting, are not used in contextual banditsThere are a couple of reasons that contextual bandit problems are not solved using RL techniques more often:The goal in contextual bandits is commonly focused on creating a highly efficient online learner that minimises regret. Regret is the long term difference in total reward between always exploiting the best action choice compared to the exploration necessary to find it. Some RL solvers - e.g. DQN - are poor by this metric.The lack of timesteps and state transitions can be used in algorithm design to be more efficient. Improvements to RL methods designed to help with sparse rewards and the assignment problem in MDPs are pointless for contextual bandits and may be wasteful or even counter-productive.Some RL algorithms do resolve to be nearly identical to their contextual bandit counterparts, and have the same performance characteristics e.g. REINFORCE with baseline for 1-step episodes is essentially the Contextual Gradient Bandit algorithm.It is also worth noting than many problem domains where contextual bandit algorithms do well - e.g. website recommendations and advertising - have research showing that that a more sophisticated MDP model and RL-like approach can do even better. Although that is not quite the same as your question, it typically means extending the model so that timesteps and state transitions are meaningful."
How to convert sequences of images into state in DQN?,"
I recently read the DQN paper titled: Playing Atari with Deep Reinforcement Learning. My basic and rough understanding of the paper is as follows:
You have two neural networks; one stays frozen for a duration of time steps and is used in the computation of the loss function with the neural network that is updating. The loss function is used to update the neural network using gradient descent.
Experience replay is used, which basically creates a buffer of experiences. This buffer of experiences is randomly sampled and these random samples are used to update the non-frozen neural network.
My question pertains to the DQN algorithm illustrated in the paper: Algorithm 1, more specifically lines 4 and 9 of this algorithm. My understanding, which is also mentioned early on in the paper, is that the states are actually sequences of the game-play frames. I want to know, since the input is given to a CNN, how would we encode these frames to serve as input to the CNN? 
I also want to know since $s_{1}$ is equal to a set, which can be seen in line 4 of the algorithm, then why is $s_{t+1}$ equal to $s_{t}$, $a_{t}$, $x_{t+1}$?
","['neural-networks', 'reinforcement-learning', 'convolutional-neural-networks', 'dqn']","I want to know, since the input is given to a CNN, how would we encode these frames to serve as input to the CNN? As nbro mentioned in a comment to your answer, this question has very recently been asked and answered here.I also want to know since $s_1$ is equal to a set, which can be seen in line 4 of the algorithm, then why is $s_{t+1}$ equal to $s_t$, $a_t$, $x_{t+1}$?The algorithm presented in the original DQN paper is relatively simple and written to express the main ideas of their approach (e.g. experience replay, preprocessing histories, gradient descent, etc.); in fact, it isn't even the exact algorithm used in the experiments! For example, the experiments use frame-skipping to reduce computation - this is not mentioned in Algorithm 1 in the paper. With that in mind, setting $s_{t+1}$ equal to $s_t, a_t, x_{t+1}$ in the algorithm signifies a general notion of constructing the next raw state $s_{t+1}$ from the previous preprocessed state $s_t$, previous action $a_t$, and current frame $x_{t+1}$. For example:The above examples should display how the encoding of the state cannot always simply be a stack of raw frames, or even a function of $s_t$, $a_t$ and $x_{t+1}$, and therefore a more general approach is often required."
How many training runs are needed to obtain a credible value for performance?,"
I'm trying to optimize a neural network. For that, I'm changing parameters like the batch size, learning rate, weight initialization, etc.
A neural network is not a deterministic algorithm, so, in each training set, I train the neural network from scratch and I stop it when it's full converged.
After training is complete, I calculate the performance of the neural network in a test dataset. The problem is, I trained the neural network from scratch 2 times with the same parameters, but the difference in performance was almost 5%, which is a BIG DIFFERENCE.
So, what's the reasonable number of training runs to obtain a credible performance number of a neural network?
","['neural-networks', 'training', 'optimization', 'hyperparameter-optimization', 'performance']",
"Why does the state-action value function, defined as an expected value of the reward and state value function, not need to follow a policy?","
I often see that the state-action value function is expressed as:
$$q_{\pi}(s,a)=\color{red}{\mathbb{E}_{\pi}}[R_{t+1}+\gamma G_{t+1} | S_t=s, A_t = a] = \color{blue}{\mathbb{E}}[R_{t+1}+\gamma v_{\pi}(s') |S_t = s, A_t =a]$$
Why does expressing the future return in the time $t+1$ as a state value function $v_{\pi}$ make the expected value under policy change to expected value in general?
","['reinforcement-learning', 'value-functions', 'bellman-equations', 'expectation']","Let's first write the state-value function as$$q_{\pi}(s,a) = \mathbb{E}_{p, \pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]\;,$$where $R_{t+1}$ is the random variable that represents the reward gained at time $t+1$, i.e. after we have taken action $A_t = a$ in state $S_t = s$, while $G_{t+1}$ is the random variable that represents the return, the sum of future rewards. This allows us to show that the expectation is taken under the conditional joint distribution $p(s', r \mid s, a)$, which is the environment dynamics, and future actions are taken from our policy distribution $\pi$.As $R_{t+1}$ depends on $S_t = s, A_t = a$ and $p(s', r \mid s, a)$,  the only random variable in the expectation that is dependent on our policy $\pi$ is $G_{t+1}$, because this is the sum of future reward signals and so will depend on future state-action values. Thus, we can rewrite again as
$$q_{\pi}(s,a) = \mathbb{E}_{p}[R_{t+1} + \gamma \mathbb{E}_{\pi}[ G_{t+1} |S_{t+1} = s'] | S_t = s, A_t = a]\;,$$
where the inner expectation (coupled with the fact its inside an expectation over the state and reward distributions) should look familiar to you as the state value function, i.e.
$$\mathbb{E}_{\pi}[ G_{t+1} |S_{t+1} = s'] = v_{\pi}(s')\;.$$
This leads us to get what you have
$$q_{\pi}(s,a) = \mathbb{E}_{p}[R_{t+1} + \gamma v_{\pi}(s') | S_t = s, A_t = a]\;,$$
where the only difference is that we have made clear what our expectation is taken with respect to.The expectation is taken with respect to the conditional joint distribution $p(s', r \mid s, a)$, and we usually include the $\pi$ subscript to denote that they are also taking the expectation with respect to the policy, but here this does not affect the first term as we have conditioned on knowing $A_t = a$ and only applies to the future reward signals."
What does it mean when the discriminator's loss gets a constant value while the generator's loss keeps on changing?,"
While training a GAN-based model, every time the discriminator's loss gets a constant value of nearly 0.63 while the generator's loss keeps on changing from 0.5 to 1.5, so I am not able to understand if this thing is happening either due to the generator being successful in fooling the discriminator or some instability in training. I have been stuck in this confusion for so many days.
","['neural-networks', 'machine-learning', 'generative-adversarial-networks']",
How to implement RAM versions of Atari games,"
I have coded the breakout RAM version, but, unfortunately, its highest reward was 5. I trained it for about 2 hours and never reached a higher score. The code is huge, so I can't paste here, but, in short, I used double deep Q-learning, and trained it like it was CartPole or lunar-lander environment. In CartPole, the observation was a vector of 4 components. In that case, my double deep Q-learning agent solved the environment, but in the breakout-ram version whose observation was a vector of 128 elements, it was not even close. 
Did I miss something?
","['reinforcement-learning', 'ai-design', 'q-learning', 'deep-rl']",
Why is there an inconsistency between my calculations of Policy Iteration and this Sutton & Barto's diagram?,"
In equation 4.9 of Sutton and Barto's book on page 79, we have (for the policy iteration algorithm):
$$\pi'(s) = arg \max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi}(s')]$$
where $\pi$ is the previous policy and  $\pi'$ is the new policy. Hence in iterations $k$ it must mean
$$\pi_{k+1}(s) = arg \max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v_{\pi_{k}}(s')]$$
But in the example given in the same book on page 77 we have:

Now, for the concerned state marked in red -

$v_{\pi_{1}} = -1$ for all four surrounding states
$r = -1$ for all four surrounding states
$p(s',r|s,a) = 1$ for all four surrounding states
$\pi_{2}(s) = arg \max_{a}[1*[-1+1*-1],1*[-1+1*-1],1*[-1+1*-1],1*[-1+1*-1]]$
$\pi _{2}(s) = arg \max_{a}(-2,-2,-2,-2)$

Hence this should give us a criss-cross symbol (4 directional arrow) in $\pi_{2}$(s) but here a left arrow symbol is given.
What's wrong with my calculations?
","['value-functions', 'policies', 'sutton-barto', 'policy-iteration']","Your calculations are correct, but you have misinterpreted the equations and the diagram. The index $k$ in $v_k$ for the diagram refers to the policy evaluation update iteration only, and is not related to the policy update step (which uses the notation $\pi'$ and does not mention $k$).Policy improvement consists of multiple sweeps through states to fully evaluate the current policy and estimate the value function for it. After that, it updates the policy in a separate policy improvement step. There are two loops - an inner loop indexed by $k$ in the equations and diagram, plus an outer loop which is not given an index notation.The diagram is not showing incremental $\pi'$ policies from outer loops over policy iteration. Instead it is showing ""Greedy Policy w.r.t. $v_k$"" steps in the inner loop - you can think of that as the policy $\pi'$ that you would get inside the first outer loop, if you terminated the policy evaluation stage after that iteration $k$ of the inner loop. The diagram only shows behaviour of policy iteration for a single outer loop. It demonstrates at least two interesting things:In the case of this very simple environment, if you ran a single outer loop with long enough policy evaluation stage ($k \ge 3$) you would find the optimal policy.Even before the value function estimate is close to convergence (with a high $k$), the policy that could be derived from the new estimates could be used to improve the policy. That leads to the idea of the value iteration method."
"Can we force the initial state of a neural network to produce an ""unknown"" class?","
Has anyone investigated ways to initialize a network so that everything is considered ""unknown"" at the start?
When you consider the ways humans learn, if something doesn't fit a class well enough, it falls into an ""unknown category"".
I would argue we all ultimately do some type of correlation matching internally with a certain threshold existing for recognition. 
Deep networks don't currently have this ability, everything falls into a class. What I'm curious about is how might we force a network to initially classify things as ""unknown"" as the default state. 
","['deep-learning', 'convolutional-neural-networks', 'computer-vision']",
Why does AlphaGo Zero select move based on exponentiated visit count?,"
From the AlphaGo Zero paper, AlphaGo Zero uses an exponentiated visit count from the tree search.
Why use visit count instead of the mean action value $Q(s, a)$?
","['reinforcement-learning', 'alphago-zero']","The answer is surprisingly hidden in the original AlphaGo paper:At the end of search AlphaGo selects the action with maximum visit count; this
  is less sensitive to outliers than maximizing action value.Unfortunately, there did not appear to be further details in the paper or in the related reference. The root child node (corresponding to an action) with maximum visit count is fittingly known as the robust child, as described here and referenced in a MCTS survey here."
How to take actions at each episode and within each step of the episode in deep Q learning?,"
In deep Q learning, we execute the algorithm for each episode, and for each step within an episode, we take an action and record a reward.
I have a situation where my action is 2-tuple $a=(a_1,a_2)$. Say, in episode $i$, I have to take the first half of an action $a_1$, then for each step of the episode, I have to take the second half of the action $a_2$.
More specifically, say we are in episode $i$ and this episode has $T$ timesteps. First, I have to take $a_1(i)$. (Where $i$ is used to reference episode $i$.) Then, for each $t_i\in\{1,2,\ldots,T\}$, I have to take action $a_2(t_i)$. Once I choose $a_2(t_i)$, I get an observation and a reward for the global action $(a_1(i), a_2(t_i))$.
Is it possible to apply deep Q learning? If so, how? Should I apply the $\epsilon$-greedy twice?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'dqn']",
How to train a reinforcement learning agent from raw pixels?,"
How would you train a reinforcement learning agent from raw pixels? 
For example, if you have 3 stacked images to sense motion, then how would you pass them to neural networks to output Q-learning values? 
If you pass that batch output, it would be a batch of values, so from here it is impossible to deduce which ones are the true Q-values for that state.
Currently, I am watching a YouTuber: Machine Learning with Phil, and he did it very differently. On the 13th minute, he defined a network that outputs a batch of values rather than Q-values for 6 states. In short, he outputs a matrix rather than a vector.
","['reinforcement-learning', 'ai-design', 'deep-rl']","How would you train a reinforcement learning agent from raw pixels? For example, if you have 3 stacked images to sense motion, then how would you pass them to neural networks to output Q-learning values?A Convolutional Neural Network (CNN) structure is a standard neural network architecture when working with 2D pixel input in reinforcement learning, and it is the technique used in the original DQN paper (see paragraphs 1 & 3 of Section 4.1 of https://arxiv.org/abs/1312.5602). CNNs typically take 3-dimensional input, where the first two dimensions are height and width of your images, and the third is rgb color. The technique in the paper was to convert each RGB frame (or image) to greyscale format (so it has only 1 color channel/dimension instead of 3) and instead use the rgb_color dimension as a frames dimension that is indexed by each stacked frame.Currently, I am watching a YouTuber: Machine Learning with Phil, and he did it very differently. On the 13th minute, he defined a network that outputs a batch of values rather than Q-values for 6 states. In short, he outputs a matrix rather than a vector.Later in the tutorial series, he most likely will discuss the training of the neural network. During training, you need to find the q-values of a batch of sets of stacked frames. Specifically, each element of the batch is a set of stacked frames. In other words, a set of stacked frames is treated as a single observation, so a batch of sets of stacked frames is a batch of observations.To find these q-values, you will perform a forward pass of the batch of observations through the neural network. A forward pass of a single observation (set of stacked frames) through the neural network yields a vector of q-values (one for each action). Thus, a forward pass of a batch of observations (batch of stacked frames) will yield a matrix of q-values (one vector of q-values for each observation (or set of stacked frames)). This technique is used because many standard neural network libraries are designed to perform a forward pass on a batch of inputs through the neural network much faster than performing a forward pass on each input separately."
What's the right way of building a deep Q-network?,"
I'm new to RL and to deep q-learning and I have a simple question about the architecture of the neural network to use in an environment with a continous state space a discrete action space. 
I tought that the action $a_t$ should have been included as an input of the neural network, togheter with the state. It also made sense to me as when you have to compute the argmax or the max w.r.t. $a_t$ it was like a ""standard"" function. Then I've seen some examples of networks that had as inputs only $s_t$ and that had as many outputs as the number of possible actions. I quite understand the logic behind this (replicate the q-values pairs of action-state) but is it  really the correct way? If so, how do you compute the $argmax$ or the $max$? Do I have to associate to each output an action?
","['deep-learning', 'reinforcement-learning', 'dqn', 'deep-neural-networks']","Do I have to associate to each output an action?You are absolutely correct! In DQN, each output node of the neural network will be associated with one of your possible actions (assuming a finite discrete action space). After passing an input through the network, the value of each output node is the estimated q-value of the corresponding action. One benefit of this architecture is that you only need to pass the input through the neural network once to compute the q-value of each action, which is constant in the number of actions. If you were to include an action as an input to the neural network along with an observation, then you would need to pass an input for each action, which scales linearly in the number of actions. This is mentioned in paragraph 2 of Section 4.1 in the original DQN paper (https://arxiv.org/abs/1312.5602)Is it really the correct way? If so, how do you compute the argmax or the max?It's one possible way that is used in many popular algorithms such as DQN. To find the argmax, you simply take the action corresponding to the output node with highest q-value after passing an input through the network."
What kind of problems cannot be solved using machine learning techniques?,"
For the problems that can be solved algorithmically.
We have very good formal literature for which problems can be solved in polynomial, exponential time and which cannot. P/NP/NP-hard
But do we know some problems in machine learning paradigm for which no model can be trained? (With/without infinite computation capacity)
","['neural-networks', 'machine-learning', 'training']",
"If the current state is $S_t$ and the actions are chosen according to $\pi$, what is the expectation of $R_{t+1}$ in terms of $\pi$ and $p$?","
I'm trying to solve exercise 3.11 from the book Sutton and Barto's book (2nd edition)

Exercise 3.11 If the current state is $S_t$ , and actions are selected according to a stochastic policy $\pi$, then what is the expectation of $R_{t+1}$ in terms of $\pi$ and the four-argument function $p$ (3.2)?

Here's my attempt.
For each state $s$, the expected immediate reward when taking action $a$ is given in terms of $p$ by eq 3.5 in the book:
$$r(s,a) = \sum_{r \in R} r \, \sum_{s'\in S} p(s',r \mid s,a) = E[R_t \mid S_{t-1} = s, A_{t-1} = a] \tag{1}\label{1}$$
The policy $\pi(a \mid s)$, on the other hand, gives the probability of taking action $a$ given the state $s$.
Is it possible to express the expectation of the immediate reward over all actions $A$ from the state $s$ using (1) as
$$E[R_t \mid S_{t-1} = s, A] = \sum_{a \in A} \pi(a \mid s) r(a,s)  \tag{2}\label{2}$$
?
If this is valid, is this also valid in the next time step
$$E[R_{t+1} \mid S_{t} = s, A] = \sum_{a \in A} \pi(a \mid s) r(a, s) \tag{3}\label{3}$$
?
If (2) and (3) are OK, then
$$E[R_{t+1} \mid S_{t} = s, A] = \sum_{a \in A} \pi(a \mid s) \sum_{r \in R} r \, \sum_{s'\in S} p(s',r \mid s,a)$$
","['reinforcement-learning', 'rewards', 'sutton-barto', 'expectation', 'transition-model']","First note that $\mathbb{E}[R_{t+1} |S_t=s] = \sum_{s',r}rm(s',r|s)$ where $m(\cdot)$ is the mass function for the joint distribution of $S_{t+1},R_{t+1}$. If you are currently in state $S_t$ and we condition on taking action $a$ then the expected reward at time $t+1$ is given as follows:\begin{align} \mathbb{E}[R_{t+1} | S_t = s, A_t=a] & = \sum_{s',r}rp(s'r|s,a)\;.
\end{align}However, action $A_t$ is taken according to some stochastic policy $\pi$ so we need to marginalise this out of our expectation by using the tower law - i.e. we take $$\mathbb{E}_{A_t \sim \pi}[\mathbb{E}[R_{t+1} | S_t = s, A_t=a]|S_t = s] = \sum_a \pi(a|s)\sum_{s',r}rp(s'r|s,a) = \mathbb{E}[R_{t+1} | S_t = s]\;.$$To see why this holds we can re-write using some arbitrary mass functions $f(\cdot),h(\cdot),g(\cdot),m(\cdot)$ as$$\pi(a|s)p(s'r|s,a) = \frac{f(a,s)}{g(s)} \times \frac{h(s',r,a,s)}{f(a,s)} = m(s',r,a|s)\;,$$
so when we would end up with (after re-arranging the summations)$$\sum_{s',r}r \sum_{a}m(s',r,a|s) = \sum_{s',r}r m(s',r|s) = \mathbb{E}[R_{t+1}|S_t = s]\;;$$
as required. NB: what you have done is mostly correct except be careful when going from (2) to (3). They are exactly the same equation except for the time stamps, which means you would have to change the time stamps in your $r(s,a)$. Note that when you are at time step $t$ you take your action $A_t$ from your current $S_t$ to transition into state $S_{t+1}$ and then receive reward $R_{t+1}$ (and the next state). "
How to obtain SHAP values,"
I want to obtain SHAP values with kernel SHAP without using python but I don't really understand the algorithm. If I have a kNN classifier, do I have to run the classifier for all the coalitions possible? For $n$ variables, $2^n$ predictions?
Also, how do I obtain the SHAP values after that? Linear regression?
",['feature-selection'],
What does it mean to train a model?,"
We hear this many time for different problems

Train a model to solve this problem!

What do we really mean by training a model?
","['neural-networks', 'machine-learning', 'deep-learning', 'training']",
How to predict Q-values based on stack of frames,"
I decided to train deep Q-learning agent based on getting raw pixels from environment.I have one particular problem:when I input stack of frames, suppose 4 consecutive frames, if action space is 6,then output is 4 by 6 matrix.So which one is real Q-value?I mean, I input batch of frames and it outputs batch of values and question is which is real Q-value out of those batch values?
","['q-learning', 'deep-rl']",
"In a DQN, can Prioritized Experience Replay actually perform worse than a regular Experience Replay?","
I've written a Double DQN-based stock trading bot using mainly time series stock data.
I've recently upgraded my Experience Replay(ER) code with a version of Prioritized Experience Replay (PER) similar to the one written by OpenAI. My DQN's reward function is the stock return over 30 days (the length of my test window).
The strange thing is, once the bot has been trained using the same set of time series data and let free to trade on unseen stock data, the version that uses PER actually comes up with worse stock returns than the version using a regular ER.
This is not quite what I'd expected but it's very hard to debug and see what might have gone wrong.
So my question is, will PER always perform better than a regular ER? If not, when/why not?
","['deep-learning', 'q-learning', 'dqn', 'experience-replay']",
When discounted MAB is useful?,"
Many of multi-armed bandit(MAB) algorithms are used when the total reward is the sum of all rewards. However, in RL, the discounted reward is mainly used. Why is the discounted reward not prevailing in MAB problem, and in what cases is this type of modeling valid and might be better?
","['reinforcement-learning', 'rewards', 'multi-armed-bandits', 'discount-factor']",
How do LSTM or GRU gates learn to specialize in their desired tasks?,"
While I was studying the equations for the computation inside GRU and LSTM units, I realized that although the different gates have different Weight matrices, their overall structure is the same. They are all dot products of a weight matrix and their inputs, plus bias, followed by a learned gating activation. Now, the difference between computation depends on the weight matrices being different from each other, that is, those weight matrices are specifically for specializing in the particular tasks like forgetting/keeping etc.
But these matrices are all initialized randomly, and it seems that there's no special tricks in the training scheme to make sure these weight matrices are learned in a manner that the associated gates specialize in their desired tasks. They are all random matrices that kept getting updated with gradient descent.
So how does, for example, a forget gate learn to function as a forgetting unit? Same question applies to others as well. Am I missing a part of the training for these networks? Can we ever say that these units learn truly disentangled functions from each other?
","['deep-learning', 'training', 'recurrent-neural-networks', 'long-short-term-memory', 'sequence-modeling']","It comes down to the order they're computed in, and what they're used in. I will be referring to the LSTM in this answer.Looking at the forget gate, you can see that it has the ability to manipulate the cell state. This gives it the ability to force a forget. Say (after training) it sees a super important input that means some previous data is irrelevant (say, like a full stop). This forget gate, while it might not force a forget, has the ability to force one, and will likely learn to.The input gate ultimately adds to the cell state. This gate doesn't have direct influence over the cell state (it can't make it 0, like the forget gate can), but it can add to it and influence it that way. So it is an input gate.The output gate is used to interpret the hidden state, and get it ready to be combined with the cell state for a final output at that time step.While these gates all use sigmoid functions, are all initialised randomly and have the same dimensionality, what their output is used in, and the order they're computed in gives them a certain role to play. Initially, they won't conform to this role, but logically as they learn, they likely will. "
Why is $G_{t+1}$ is replaced with $v_*(S_{t+1})$ in the Bellman optimality equation?,"
In equation 3.17 of Sutton and Barto's book: 
$$q_*(s, a)=\mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a]$$
$G_{t+1}$ here have been replaced with $v_*(S_{t+1})$, but no reason has been provided for why this step has been taken. 
Can someone provide the reasoning behind why $G_{t+1}$ is equal to $v_*(S_{t+1})$?
","['reinforcement-learning', 'value-functions', 'expectation', 'return', 'bellman-equations']","Can someone provide the reasoning behind why $G_{t+1}$ is equal to $v_*(S_{t+1})$?The two things are not usually exactly equal, because $G_{t+1}$ is a probability distribution over all possible future returns whilst $v_*(S_{t+1})$ is a probability distribution derived over all possible values of $S_{t+1}$. These will be different distributions much of the time, but their expectations are equal, provided the conditions of the expectation match.In other words, $$G_{t+1} \neq v_*(S_{t+1})$$But$$\mathbb{E}[G_{t+1}] = \mathbb{E}[v_*(S_{t+1})]$$. . . when the conditions that apply to the expectations on each side are compatible. The relevant conditions are Same initial state or state/action at given timestep $t$ (or you could pick any earlier timestep)Same state progression rules and reward structure (i.e. same MDP)Same policyMore detailsThe definition of $v(s)$ can be given as$$v(s) = \mathbb{E}_\pi[G_t \mid S_t = s]$$If you substitute step s' and index $t+1$ you get$$v(s') = \mathbb{E}_\pi[G_{t+1} \mid S_{t+1} = s']$$(This is the same equation, true by definition, the substitution just shows you how it fits).In order to put this into equation 3.17, you need to note that:It is OK to substitute terms inside an expectation if they are equal in separate expections, amd  the conditions $c$ and $Y$ apply to both (or are irrelevant to either one or both). So if for example $\mathbb{E}_c[Z] = \mathbb{E}_c[X \mid Y]$ where $X$ and $Z$ are random variables, and you know $Z$ is independent of $Y$ then you can say $\mathbb{E}_c[W + 2X \mid Y] = \mathbb{E}_c[W + 2Z \mid Y]$ even if $X$ and $Z$ are different distributions.$A_{t+1} = a'$ does not need to be specified because it is decided by the same $\pi$ in both $q(s,a)$ and $v(s')$, making the conditions on the expectation compatible already. So the condition of following $\pi$ is compatible with $\mathbb{E}_\pi[G_{t+1} \mid S_{t} = s, A_{t}=a] = \mathbb{E}_\pi[v_*(S_{t+1}) \mid S_{t} = s, A_{t}=a]$The expectation over possible $s'$ in $\mathbb{E}_\pi[v_*(S_{t+1})|S_t=s, A_t=a] = \sum p(s'|s,a)v_*(s')$ is already implied by conditions on the original expectation that the functions are evaluating the same environment - something that is not usually shown in the notation.Also worth noting, in 3.17 $\pi$ is the optimal policy $\pi^*$, but actually the equation holds for any fixed policy."
How is the formula for the Bayes error rate with an integral derived?,"
My questions concern a particular formulation of the Bayes error rate from Wikipedia, summarized below.

For a multiclass classifier, the Bayes error rate may be calculated as follows: $$p = 1 - \sum_{C_i \ne C_{\max,x}} \int_{x \in H_i} P(C_i|x)p(x)\,dx$$ where $x$ is an instance, $C_i$ is a class into which an instance is classified, $H_i$ is the area/region that a classifier function $h$ classifies as $C_i$.

We are interested in the probability of misclassifying an instance, so we wish to sum up the probability of each unlikely class label (hence we want to look at $C_i \ne C_{\max, x}$).
However, the integral is confusing me. We want to integrate an area corresponding to the probability that we choose label $C_i$ given $x$. But we drew $x$ from $H_i$, the region covered/classified by $C_i$, so wouldn't $P(C_i|x) = 1$?
I think most of my confusion will be resolved if someone can help clarify the intention of the integral.
Is it to draw random samples from the total space of $h$ (the classifier function), and then sum the probabilities from each classified $C_i \ne C_{\max, x}$? How does $x$ exist in the outer summation before it has been sampled from $H_i$ in the integral?
","['classification', 'probability-theory', 'bayes-error-rate']","Bayes Error RateFor the general case of K different classes, the probability of classifing x instance correctly is:\begin{equation} \label{eq1}
\begin{split}
P(correct) & = \sum_{i=1}^{K} p(x \in H_i, C_i) \\
 & = \sum_{i=1}^{K} \int_{x \in H_i} p(x,C_i) \, dx\\
 & = \sum_{i=1}^{K} \int_{x \in H_i} P(C_i|x)p(x)\,dx \\
\end{split}
\end{equation}where $H_i$ is the region where class $i$ has the highest posterior. So the Bayes Error Rate is:\begin{equation} \label{eq2}
\begin{split}
P(error) & = 1 - p(correct) \\
 & = 1 - \sum_{i=1}^{K} \int_{x \in H_i} P(C_i|x)p(x)\,dx
\end{split}
\end{equation}Be careful if we drew $x$ from $H_i$, the region covered/classified by $C_i$, $P(C_i|x) \ne 1$, because that would mean that we surely predict correctly all the time. If $x$ belongs to a decision area, this does not imply that it belongs to the corresponding class."
How can I predict the label given a partial feature vector?,"
Most of the traditional machine learning algorithms need a feature vector of a constant dimension to predict the label. 
Which algorithms can be used to predict a class label with a shorter or partial feature vector? 
For example, consider a search engine. In search engines, when the user types a few letters, the search engine predicts the context of the query and suggests more queries to user. 
Similarly, how can I predict a class label with an incomplete feature vector? I know one way is to pad the sequence, but I want a better solution.
","['machine-learning', 'classification', 'prediction', 'data-preprocessing']",
"What are the keys and values of the attention model for the encoder and decoder in the ""Attention Is All You Need"" paper?","
I have recently encountered the paper on NLP. It is very new to me and I am still unable to see how that works. I have used all the resources over there from the original paper to Youtube videos and the very famous ""Illustrated Transformer"".
Suppose I have a training example of ""I am a student"" and I have the respective French as ""Je suis etudient"". 
I want to know how these 3 words are converted to 4 words. What are the query, keys, values? 
This is my understanding of the topic so far.
The encoder part is:

Query: a single word embedded in a vector form. such as ""I"" expressed as a vector of length 5 as $[.2, 0.1, 0.4, 0.9, 0.44]$.
Keys: the matrix of all the vectors or in simple words, a matrix that has all the words from a sentence in the form of embeddings.
Values = Keys

For decoder:

Query: the input word in the form of a vector (which is output given by the decoder from the previous pass).
Keys = values = outputs from the encoder's layers.

BUT there are 2 different attention layers and one of which do not use the encoder's output at all. So, what are the keys and values now? (I think they are just like encoder, but just the generated until that pass)?
","['deep-learning', 'natural-language-processing', 'transformer', 'attention']",
Is there any difference between reward and return in reinforcement learning?,"
I am reading Sutton and Barto's book on reinforcement learning. I thought that reward and return were the same things. 
However, in Section 5.6 of the book, 3rd line, first paragraph, it is written:

Whereas in Chapter 2 we averaged rewards, in Monte Carlo methods we average returns.

What does it mean? Are rewards and returns different things?
","['reinforcement-learning', 'comparison', 'rewards', 'return']","Return refers to the total discounted reward, starting from the current timestep."
Confusion about the proof that optimizing InfoNCE equals to maximizing mutual information,"
In the appendix of Representation Learning with Contrastive Predictive Coding, van den Oord et al. prove that optimizing InfoNCE is equivalent to maximize the mutual information between input image $x_t$ and the context latent $c_t$ as follows:

where $x_{t+k}$ is the image at time step $t+k$, $X_{neg}$ is a set of negative samples that do not appear in the sequence $x_t$ belongs to, and $N-1$ is the negative samples used to compute InfoNCE.
I'm confused about Equation $(8)$. van den Oord et al. stressed that Equation $(8)$ becomes more accurate as $N$ increases, but I cannot see why. Here's my understanding, for $x_j\in X_{neg}$, we have $p(x_j|c_t)\le p(x_j)$ . Therefore, $\sum_{x_j\in X_{neg}}{p(x_j|c_t)\over p(x_j)}\le N-1$ and this does not become accurate as $N$ increases. In fact, I think the gap between the left and right of $\le$ increases as $N$ increases. Do I make any mistake?
","['machine-learning', 'deep-learning']",
Can we solve an $8 \times 8$ sliding puzzle using hill climbing?,"
Can we solve an $8 \times 8$ sliding puzzle using a random-restart hill climbing technique (steepest-ascent)? If yes, how much computing power will this need? And what is the maximum $n \times n$ that can be solved normally (e.g. with a Google's colab instance)?
","['algorithm', 'search', 'hill-climbing', 'local-search']",
Under what conditions can one find the optimal critic in WGAN?,"
The Kantorovich-Rubinstein duality for the optimal transport problem implies that the Wasserstein distance between two distributions $\mu_1$ and $\mu_2$ can be computed as (equation 2 in section 3 in the WGAN paper)
$$W(\mu_1,\mu_2)=\underset{f\in \text{1-Lip.}}{\sup}\left(\mathbb{E}_{x\sim \mu_1}\left[f\left(x\right)\right]-\mathbb{E}_{x \sim \mu_2}\left[f\left(x\right)\right]\right).$$
Under what conditions can one find the optimal $f$ that achieves the maximum? Is it possible to have an analytical expression for $f$ that achieves the maximum in such scenarios?
Any help is deeply appreciated.
","['deep-learning', 'generative-adversarial-networks', 'wasserstein-metric', 'wasserstein-gan']",
What are some good loss functions used to minimize extreme errors in regression and time series forecasting?,"
I'm working on a time series forecasting task, and, in some specific cases, I don't need perfect accuracy, but the network cannot by any means miss by a lot. So, in detriment of a smaller mean error, I want to have fewer big mistakes.
Any suggestions of loss functions or other methods to solve this issue? 
","['neural-networks', 'objective-functions', 'time-series']",
How is $\Delta$ updated in true online TD($\lambda$)?,"
In the RL textbook by Sutton & Barto section 7.4, the author talked about the ""True online TD($\lambda$)"". The figure (7.10 in the book) below shows the algorithm. 
At the end of each step, $V_{old} \leftarrow V(S')$ and also $S \leftarrow S'$. When we jump to next step, $\Delta \leftarrow V(S') - V(S')$, which is 0. It seems that $\Delta$ is always going to be 0 after step 1. If that is true, it does not make any sense to me. Can you please elaborate on how  $\Delta$ is updated? 

","['reinforcement-learning', 'temporal-difference-methods', 'td-lambda']","Let us denote the state we are in at time $t$ by $S_t$. Then at iteration $t$ we create a placeholder $V_{old} = V(S_{t+1})$ for the state we will transition into. We then update the value function $V(s) \; \forall s \in \mathcal{S}$ - i.e. we update the value function for all states in our state space. Let us denote this updated value function by $V'(S)$. At iteration $t+1$ we calculate $\Delta = V'(S_{t+1}) - V_{old} = V'(S_{t+1}) - V(S_{t+1})$, which does not necessarily equal 0 because the placeholder $V_{old}$ was created using the value function before the last update. "
How do I sample conditionally from deep belief networks?,"
Deep belief networks (DBNs) are generative models, where, usually, you sample by thermalising the deepest layer (as it's a restricted Boltzmann machine), and then forward propagating a sample towards the visible layer to get a sample from the learned distribution.
This is less flexible sampling than in a single layer DBN: a restricted Boltzmann machine. There, we can start our sampling chain at any state we want, and get samples ""around"" that state. In particular, we can clamp some visible nodes $\{v_i\}$ and get samples from the conditional probability $𝑝(v_j|\{v_i\})$
Is there a way to do something similar in DBNs? When we interpret the non-RBM layers as RBMs by removing directionality, can we treat it as a deep Boltzmann machine and start sampling at e.g. a training example again?
","['generative-model', 'conditional-probability', 'restricted-boltzmann-machine', 'deep-belief-network']",
Is better to spend parameters on weights or bias?,"
If a neural network has a limited number of neuron parameters to find, -let's say only 1000 parameters-, it is generally better to spend the parameters on weights or neuron bias?
For example, if each neuron has 2 weights and one bias, it uses 3 parameters per neuron, so only 333 neurons would be available.
But if each neuron uses no bias parameter, then 500 neurons are available with 1000 parameters.
I'm concerned with overfiting by using too many parameters, so I want to minimize the number of parameters meanwhile maximizing the quality of the result.
","['neural-networks', 'weights']","First of all, your estimates are a bit off. If you have 300 neurons, you won't have just 2 weights per neuron, but much more, assuming full connectivityBias isn't just an extra parameter to fit, it is an important adjustable parameter that sets the offset of the separating hyperplane represented by each neuron. Think of the simple equation $ax+b$, there's no way to shift the line unless you use the $b$ (bias) part.This would be especially important for small number of nodes and classification tasks (think perceptrons etc)"
Train a model using a multi-column text-filled excel sheet,"
I have an excel sheet filled with my own personal appreciations of movies I've watched, and I want to use it to train an AI model so that it can predict if I'll like a specific movie or not, based on the ones I've already seen.
My data is formatted as following (just a sample, the spreadsheet is filled with hundreds of movies):

And I would like to use all the columns to train my model. Because I am going to say if I liked the movie or not, I know it will be Supervised Learning. I already cleaned the data so there's no blank or missing data, but I do not know how to train my model using every column.
If required, I can be more specific on something, just ask and I'll edit the post.
","['machine-learning', 'python', 'supervised-learning', 'scikit-learn']",
Are there any good tutorials about training RL agent from raw pixels using PyTorch?,"
Is there any good tutorials about training reinforcement learning agent from raw pixels using PyTorch? 
I don't understand the official PyTorch tutorial. I want to train the agent on the atari breakout environment. Unfortunately, I failed to train the agent on the RAM version. Now, I am looking for a way to train the agent from raw pixels.
","['reinforcement-learning', 'reference-request', 'resource-request']",
Should I use minimax or alpha-beta pruning?,"
Should I use minimax or alpha-beta pruning (or both)? Apparently, alpha-beta pruning prunes some parts of the search tree. 
","['comparison', 'minimax', 'alpha-beta-pruning']","Both algorithms should give the same answer. However, their main difference is that alpha-beta does not explore all paths, like minimax does, but prunes those that are guaranteed not to be an optimal state for the current player, that is max or min. So, alpha-beta is a better implementation of minimax.Here are the time complexities of both algorithmswhere $b$ is an average branching factor with a search depth of $d$ plies."
"Video recognition (specifically video, not individual frames)","
There are libraries for recognizing individual video frames, but I need to recognize an object in motion. I can recognize a person in every single frame, but I need to know if the person is running or waving. I can recognize a tree in every single frame, but I need to find out if the tree is swaying in the wind. I can recognize a wind turbine in every frame, but I need to know if it's spinning right now. 
So the question is: do technologies, libraries, concepts, or algorithms exist for recognizing objects over a certain period of time? For example, I have a series of several pieces of frames, each of which exactly has a person, and need to find out if person are walking or waving their hands.
",['object-recognition'],
