Head,Body,Tags,First Answer
Brain Segmentation To 3D Model,"
My goal is to take a dataset of Brain Tumor segmentations, use computer vision to locate and highlight the tumor in each slice, then combine each slice (almost like stacking) to make a 3D model. In this 3D model, you can see exactly where the tumor is located.
I am able to use CV to highlight tumors in individual layers, but the challenge comes in stacking. How would I go about stacking images to make 3D models. With this I would have to remove empty outside space in individual images and be able to give each image some depth.
How would I go about doing this? Is there any existing library that can do this?
","['computer-vision', 'python']",
,,,
,,,
Active Learning regression with Random Forest,"
I have a dataset of about 8k points and I am trying to employ active learning with the random forest regressor. I have split the dataset to train and test with train being around 20 points. The test serves as the unlabelled pool (although I have the labels).
My workflow is the following:

Select a budget c.
Train the RF on train.
Select the sample from the test for which the predictions have the greatest variance.
Train the RF on train+sample

and the process continues until there is no more budget available. At each retrain I am calculating the accuracy on the test with the coefficient of determination.
Is the above workflow valid? What I have observed is that accuracy isn't improved compared to random sampling. Is there any other query strategy that can work with Random Forests for regression?
I could have used Gaussian processes but from my experience they need a lot of tuning and for large training sets, training time is very large. That is the reason I selected Random Forest.
","['machine-learning', 'regression', 'uncertainty-quantification', 'non-linear-regression', 'active-learning']",
Comparing Reinforcement Learning models,"
I am currently completing my thesis on optimising combinatorial problems, and we decided to utilize reinforcement learning. The problem is that I am not sure which algorithm to choose. Is there a comprehensive table or guide to help me choose a model that works well for this domain or in general?
","['reinforcement-learning', 'policy-gradients', 'monte-carlo-methods', 'combinatorial-optimization']",
,,,
"Why ""Good Model"" that performs great on holdout validation data fails on production data","
I have this binary regression model that has ~500 futures with an unbalanced dataset with the following results.
Loss is 0.06152007728815079,
Accuracy is 97.71724343299866

How I split the data:
X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.20, random_state=42)
X_validation, X_holdout_final_validation, Y_validation, Y_holdout_final_validation = train_test_split(
    X_val_and_test,
    Y_val_and_test,
    test_size=0.5
)

The model :
model = tf.keras.Sequential([
    tf.keras.layers.Dense(X.shape[1], activation='relu', input_dim=X.shape[1]),
    tf.keras.layers.Dense(X.shape[1] * .66 + X.shape[1] + 1, activation='relu'),
    tf.keras.layers.Dropout(.10),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

lr_sched = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)
callbacks = [
    lr_sched,
    PrintLR(),
    monitor
]

hist = model.fit(
    X_train,
    Y_train,
    epochs=50,
    batch_size=128,
    shuffle=True,
    validation_data=(X_validation, Y_validation),
    callbacks=callbacks
)

And here are some confusion matrix results
50.0 %
['6634', '77', '104', '1114']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['83.67%', '0.97%', '1.31%', '14.05%']

80.0 %
['6691', '20', '283', '935']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['84.39%', '0.25%', '3.57%', '11.79%']

85.0 %
['6693', '18', '328', '890']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['84.41%', '0.23%', '4.14%', '11.22%']

90.0 %
['6699', '12', '388', '830']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['84.49%', '0.15%', '4.89%', '10.47%']

95.0 %
['6705', '6', '507', '711']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['84.56%', '0.08%', '6.39%', '8.97%']

97.0 %
['6709', '2', '585', '633']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['84.61%', '0.03%', '7.38%', '7.98%']

99.0 %
['6711', '0', '757', '461']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['84.64%', '0.00%', '9.55%', '5.81%']

100 %
['6711', '0', '1218', '0']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['84.64%', '0.00%', '15.36%', '0.00%']

The function for the confusion matrix:
def my_rounder(num, threshold):
    if num >= threshold:
        return 1
    else:
        return 0

def print_conf_matrix_assessment(data, y_expected, threshold, prediction_model):
    y_actual = [[int(my_rounder(x, threshold)) for x in y] for y in prediction_model.predict(data)]
    conf_matrix = confusion_matrix(y_expected, y_actual)
    label_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
    result_percentages = [""{0:.2%}"".format(value) for value in conf_matrix.flatten() / np.sum(conf_matrix)]
    print()
    print(threshold * 100, ""%"")
    sample_counts = [""{0:0.0f}"".format(value) for value in conf_matrix.flatten()]
    print(sample_counts)
    print(label_names)
    print(result_percentages)

So in general things look pretty good. But as we know if it looks too good there is something wrong and I am trying to figure that out since when I get some production data and put it through the model I get results like that instead:
50.0 %
['2585', '1681', '23', '767']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['51.13%', '33.25%', '0.45%', '15.17%']

60.0 %
['2683', '1583', '30', '760']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['53.07%', '31.31%', '0.59%', '15.03%']

70.0 %
['2782', '1484', '38', '752']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['55.02%', '29.35%', '0.75%', '14.87%']

80.0 %
['2918', '1348', '51', '739']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['57.71%', '26.66%', '1.01%', '14.62%']

85.0 %
['3007', '1259', '59', '731']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['59.47%', '24.90%', '1.17%', '14.46%']

90.0 %
['3129', '1137', '81', '709']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['61.89%', '22.49%', '1.60%', '14.02%']

95.0 %
['3305', '961', '113', '677']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['65.37%', '19.01%', '2.23%', '13.39%']

97.0 %
['3446', '820', '149', '641']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['68.16%', '16.22%', '2.95%', '12.68%']

99.0 %
['3704', '562', '254', '536']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['73.26%', '11.12%', '5.02%', '10.60%']

100 %
['4266', '0', '790', '0']
['True Neg', 'False Pos', 'False Neg', 'True Pos']
['84.38%', '0.00%', '15.62%', '0.00%']

And here we are talking of data that is 2 hours apart. So concept shift is doubtful.
The data is on the same scale - using the min max values from the training dataset and using the mean if some value is out of bounds.
Does anyone have any idea what could be the reason for this to happen?
","['deep-learning', 'deep-neural-networks', 'prediction', 'models', 'features']",
What are Reservoir computers used for today?,"
Reservoir computers were very popular in the early 2000s. From what I understand, the advantage of reservoir computers is that, as opposed to generic recurrent neural networks, training is only done on the linear output layer, making them much faster to train. However, computers are a lot faster today than they were 20 years ago, so I am wondering whether there are still places where reservoir computers are preferred to recurrent neural networks or transformers.
","['machine-learning', 'recurrent-neural-networks', 'reservoir-computing']",
How to handle out-of-bound values in Production data?,"
So I have this model but the data may vary. And it is virtually impossible to always have the values in bounds. If I do I`d have to use larger period leading to concept shift which is worse.
The question is what is the best way to deal with the values of futures that are out of the model bounds? I see 3 options

If the value is greater than max set it to the max value the model has seen
If the value is less than min set it to the min value the model has seen
If the value is greater or less set it to the mean that kind of eliminates the future weight for the prediction.

So what would be the best approach here any thoughts?
Note: I am retraining the model daily and the model has a lot of futures ~500 so it is highly likely even right after retraining some to be out of bounds, excluding futures is not the best option since it's never the same future showing this behaviour.
I am using this function for scaling.
def min_max(value, min_max_map):
    result = 0
    if min_max_map['max'] - min_max_map['min']:
        result = (value - min_max_map['min']) / (min_max_map['max'] - min_max_map['min'])
        if result > min_max_map['max'] or result < min_max_map['min']:
            result = (min_max_map['mean'] - min_max_map['min']) / (min_max_map['max'] - min_max_map['min'])

    return result

","['deep-learning', 'deep-neural-networks', 'prediction', 'models', 'features']","Steps 1 and 2 are basically the same operation, clipping, which is a possibility but not the best since your loosing information. Imagine having two instances with same features except for feature $n$, which assume value 101 and 201 respectively. Let's say that 100 was your observed maximum in the training data for feature $n$, after clipping both instances will look the same and lead to same predictions, good for instance 1 cause its value of $n$ is close to the training maximum, but nonsense for instance 2.The best way would be defining a theoretical maximum, so independent from the training data, it could be also a value that we set as maximum and after which we do indeed clip cause occurrences of higher values are rare.An alternative is also to use z score standardization instead of min max normalization. The output range is [$-\infty$, $\infty$] but in practice you'll end up with values almost all within the range [-1, 1], and rare
occurrences below and above this range are totally fine."
,,,
Training with extremely imbalanced Dataset,"
I have a object detection problem which has extremely imbalanced dataset. Lets say there is only one class to detect, say apple or not apple. This detection network will be used in a real case including IP camera streaming where positive/negative samples ratio is extremely huge, 1:1Million.
My first idea to train the model with using a dataset of 100:100k positive/negative samples ratio but the model still might be skewed to negative samples which might results in not detecting apples apparently.
I have two questions. My questions are:

What are the best approaches to this imbalanced dataset problem ?
Since I am using high resolution camera, detecting apples from camera streaming is related with small object detection problem since the apples in stream is really small compared the resolution of the camera. I know that Yolo is doing some data augmentation to improve accuracy but would it be enough to detect small objects? Should I used some image tiling techniques? If someone has experience about small object detection problem and know how to handle this, any idea would be perfect.

P.S: I am using Yolov4 with AlexeyAB's Darknet as a object detection model. Also, background of images are mostly the same due to Camera view.
","['machine-learning', 'object-detection', 'yolo', 'imbalanced-datasets', 'real-time']",
"In a neural network's neuron that has no activation function, to calculate the delta for the neuron during back propagation do you use a derivative?","
I have a neural network that is composed of an input layer, two hidden layers and an output layer. The topology is [151, 200, 100, 1] I am using ReLU activation function on the neurons that are in the hidden layers and no activation function on the neuron that is in the output layer. I am wondering if when calculating the delta value of the neuron in the output layer, I should be using a derivative or if I should just subtract the expected output? Here I will put my line of code that this question concerns:
this.deltas[this.NN.length-2][0] = this.NN[this.NN.length-1][0] - expected;
//In forward propagation this neuron has no activation function.

","['neural-networks', 'backpropagation']","Having no activation function means your activation function is the identity, namely $g(z)=z$.Therefore, any derivative of $g(z)$ wrt to a parameter is simply the corresponding derivative of $z$, with no extra factor.You would get the same result if you include the derivative, as it is multiplying by $1$."
Is the discriminator of a GAN network embedded in VAE?,"
From what I understand, a Generative Adversarial Network (GAN) is composed of an encoder (generator), some synthetic data (fake data) and a discriminator that will penalize any distinguishable real data from the fake ones. This will result in 2 training phases where one tries to beat the other one and vice versa.
This force the generator to mimic a distribution given by the synthetic data, thus creating a latent space similar the fake data.
However, in Variational Autoencoders (VAE), the usual loss function also includes a penalty in the latent space when the data do not follow a normal distribution. From what I've learned, this also strongly encourages the latent space to follow a normal distribution.
My question is how does the penalty in the VAE is different from training a whole new network, the discriminator, that in the end penalize the generator for not following the synthetic data distribution?
More generally, is it similar to penalize a wrong latent space distribution directly in the model's loss function than it is with a discriminator network?
Maybe it's easier with GAN to make new distributions, at the cost of training another network, than it is to model an adapted loss function. (?)
","['objective-functions', 'generative-adversarial-networks', 'variational-autoencoder']","To train a GAN you need only 2 networks, the generator and discriminator, none of which require to be an encoder. You also don't require synthetic data, those are generated by the generator network, you need instead real data.""This force the generator to mimic a distribution given by the synthetic data, thus creating a latent space similar the fake data""We're not interested in mimic synthetic data, what a GAN is suppose to learn is to generate data that follow a similar distribution of some real data. So initially the generator will create only noise, if the training is successful then the data generated by the generator should become closer and closer, eventually indistinguishable from the real data. Also, there's no requirements regarding to the latent space, even though it's true that GANs can be trained to learn latent spaces with specific property to control the features of the generated images in a similar fashion to VAE.The power of GANs when compared to VAEs is that on the paper they can map any kind of distribution to any other kind of distribution. This is possible cause rather than using a single loss that compare a network output to a ground truth label, GANs use two losses combined that mimic a distance between distributions. The combined loss nowhere include a direct comparison of the generated images with a target. Rather it just leverage the discriminator to compute how far (in terms of probability distributions) the generated and real images are."
"""Tweaking"" the cost function to penalize rarer cases more severely","
I have a very unbalanced data set that I am running a CNN on for regression.  Most of the values are 0, while it is possible for the values to range from 0 to 32.
Is it possible to ""tweak"" the cost function to penalize rarer cases more severely?
I'm using torch.nn.MSELoss(reduction='none') as my loss function.
","['convolutional-neural-networks', 'pytorch', 'loss']","Yes, you can use weighted MSE by applying different coefficients to each data point.Here is an implementation given by Francisco Massa at https://discuss.pytorch.org/t/how-to-implement-weighted-mean-square-error/2547."
Bayesian optimization with confidence bound not working,"
I have a simple MLP for which I want to optimize some hyperparameters. I have fixed the number of hidden layers (for unrelated reasons) to be 3. So the hyperparameters being optimized through Bayesian Optimization are just number of neurons per hidden layer.
Here is the function to create the simple MLP based on number of neurons in each of the 3 hidden layers:
#function for creating a new MLP with number of neurons in 3 layers for argument 
def creat_model(layer1, layer2, layer3):
  input = tf.keras.layers.Input(shape = (28,28))
  x = tf.keras.layers.Flatten()(input)
  x = tf.keras.layers.Dense(layer1, activation = 'relu')(x)
  x = tf.keras.layers.Dense(layer2, activation = 'relu')(x)
  x = tf.keras.layers.Dense(layer3, activation = 'relu')(x)
  output = tf.keras.layers.Dense(10, activation = 'softmax')(x)
  model = tf.keras.models.Model(inputs = input, outputs = output)
  model.summary()
  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')
  return model

Then I define the search space:
#creating the search space
layer_range = np.arange(1,128,16)
all_ranges = []
for i in layer_range:
  for j in layer_range:
    for k in layer_range:
      all_ranges.append([i, j, k])

The following vectors are to store all losses and their corresponding parameters:
#vector of all losses
loss_vec = []
#vector of all params
all_params = []

The GP model is created like this:
model_gp = GaussianProcessRegressor()

note the default kernel is RBF.
Then I randomly sample from this search space and fit the model to these sampled points:
num_of_rands = 5
for i in range(num_of_rands):
  rand_layer = all_ranges[np.random.randint(len(all_ranges))]
  layer1 = rand_layer[0]
  layer2 = rand_layer[1]
  layer3 = rand_layer[2]
  all_params.append([layer1, layer2, layer3])
  model = creat_model(layer1, layer2, layer3)
  model.fit(x_train, y_train, batch_size = 64, epochs = 2)
  lossPoint = model.evaluate(x_test, y_test)
  loss_vec.append(lossPoint)

model_gp.fit(all_params, loss_vec)

Then, using the simple lower confidence bound approach, I run this loop to find the best set of parameters:
#bayes opt loop
runs = #insert number of runs
lmbda = 1
for i in range(runs):
  y_hat, std = model_gp.predict(all_ranges, return_std=True)
  ix = np.argmin(y_hat - lmbda*std)
  new_sample = all_ranges[ix]
  model = creat_model(new_sample[0], new_sample[1], new_sample[2])
  model.fit(x_train, y_train, batch_size = 64, epochs = 2)
  lossPoint = model.evaluate(x_test, y_test)
  loss_vec.append(lossPoint)
  all_params.append(new_sample)
  model_gp.fit(all_params, loss_vec)

Now the problem is, the model starts from [1,1,1], then to [1,1,17], and so on! to explore this further, I plotted the prediction of the GP across the parameters and it looked sth like this (this is mean - lambda*standard deviation):

Note x-axis indices are just different sets of params, e.g. [1,1,1], [1,1,17], [1,1,33], ...
As seen, there is no smoothness, and the model starts to sample every single point, you can see the random sampled points somewhere in the middle and the ones sampled through the loop near 0...
why is this? what's the fix? I usually see this GP approach to produce smooth predictions, not here though!
","['deep-learning', 'tensorflow', 'bayesian-optimization']",
,,,
"Can RL still learn if part of my actions are only used once, at the beginning of the episode?","
I am working in an environment with 3-dimensional action space. The first two actions are only used at the first timestep and never again. The third action is used at every timestep.
Say, the action is $a = (a_1, a_2, a_3)$. At the start of an episode $i$, the agent uses actions $a_1, a_2$ only at timestep 1. Action $a_3$ is used at every timestep in the episode starting from 1 till the horizon H. The agent receives rewards $r_i$ at each timestep $i$ till the end of the episode.
I am using SAC. Since the actions $a_1, a_2$ only affect the agent's behavior at timestep 1 and are not used at any of the later timesteps, I am not sure if the RL policy will get better at choosing ""good"" values for $a_1, a_2$.
Will the RL be able to learn a good policy even if it doesn't quite see the effects of the first two actions in the episode data except at the first timestep?
","['reinforcement-learning', 'deep-rl', 'soft-actor-critic']","You should be able to learn a good policy even if you use the first two actions only at the first timestep.Using this OpenAI reference, the loss for the state action value function (from which the policy loss is later derived) is:$$L(\phi) = \mathbb{E}_{(s, a, r, s') \sim D}\left[\left(Q(s,a|\phi) - (r + \gamma Q(s', a'|\phi_{target})\right)^2\right]$$where $D$ is a set of transitions, $\phi_{target}$ are ""old"" parameters for the action state value function which are left unchanged in the parameter update, and $a' \sim \pi(.|s,\theta)$.Note that I've simplified the equation for clarity.The expectation in the loss is replaced in the actual algorithm with an average on a batch of transitions.At timestep 0, the target $r + \gamma Q(s', a'|\phi_{target})$ for $Q(s_{t_0},a_{t_0}|\phi)$ (with $a' \sim \pi(.|a_{t_0}, 
\theta)$) in the loss will be non-zero, because $Q(s', a'|\phi_{target})$ will be non-zero and will reflect the value of $(s',a')$ accurately (e.g. thanks to transitions which happen at later timesteps)."
What is the most important predecessor of the transformer model?,"
I'm wondering what the origins of the transformer as proposed in Attention Is All You Need are. The paper itself provides some interesting pointers to the literature on self-attention such as:

A Decomposable Attention Model for Natural Language Inference
A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING
NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE

It seems like using attentional mechanisms was widespread (at least in combination with recurrent networks) and 'A Decomposable Attention Model for Natural Language Inference' paper from 2016 already conjectured that scaling attention might be feasible.
Is it from this prior work 'only' an engineering leap? Or what additional papers at the time likely influenced the architecture?
","['natural-language-processing', 'transformer', 'attention']","An influential predecessor paper is indeed the work on NEURAL MACHINE TRANSLATION
BY JOINTLY LEARNING TO ALIGN AND TRANSLATE. The paper outlines an attentional mechanism that is similar to the computation of the actual attentional weights in the transformer paper. See the paper and picture below for details.When the authors of the paper visualize their attention weights they obtain the following results. The inputs into the attention computation are not called queries and keys yet but are conceptually similar.See this video for a detailed explanation."
How do you evaluate a k-medoids cluster model?,"
So I'm planning on clustering a bunch of observation data using k-medoids. There are seven attributes for each instance and the data is numerical and discrete. I'm a little uncertain of how to evaluate the model to find the correct number of clusters. I was thinking I could run the cluster technique for an increasing number of clusters (say start at 1 and increase by one each time), measure the silhouette coefficient for each cluster model and then select the number of clusters with the highest value?
Would anybody be able to tell me if this is a good idea for evaluating the model and if not what else I could do?
","['clustering', 'discrete-state-spaces']",
How can I get an integer as output for continuous action space PPO reinforcement learning?,"
I have a huge discrete action space, the learning stability is not good. I'd like to move to continuous action space but the only output for my task can be a positive integer (let's say in the range 0 to 999). How can I force the DNN to output a positive integer?
","['reinforcement-learning', 'deep-neural-networks', 'proximal-policy-optimization', 'continuous-action-spaces']",
,,,
Does this kind of attention exist?,"
As someone who is new to deep learning, I am only familiar with self-attention.
I'm designing a model. Imagine there are n data, which the $i_{th}$ data can be represented as a vector $x_i$. And the data has an attribute $a$, the attribute of the $i_{th}$ data is $a_i$. now I only use one query vector $q$ to multiply each data to calculate the result as the weight $w_i$ ($w_i=q^t x_i$) , and use the weight of each data to multiply each data's attribute $a_i$ as the final result. ($A=\sum w_i a_i$)
Does this kind of attention exist? if so what its name is?
For example, $x_i$ is a 768-dimensional vector representing each sentence, and $s_i$ is a 3-dimensional vector representing each sentence's sentiment. $q$ is a 768-dimensional vector multiplied with each $x_i$ to produce each sentence's weight $w_i$. And the weighted sum $S=\sum w_i s_i$ is the overall sentiment of all sentences.
",['attention'],
Why is multilayer perceptron a nonparametric model?,"
E. Alpaydin, Introduction to ML, 4-th edition, page 46:

Over time, it has been realized
in the neural network community that most neural network
learning algorithms have their basis in statistics—for example,
the multilayer perceptron is another class of nonparametric
estimator.

Why is multilayer perceptron a nonparametric model?
","['machine-learning', 'deep-learning']","“Nonparametric” is a controversial term in statistics whose definition lacks a consensus.One definition is that the parameter count increases as the data size increases. This is how kernel density estimation works: for each new point, we add a new kernel. Perhaps a neural network could be seen in this light by allowing the number of neurons to increase as data are added.One definition is that we allow the data to describe themselves rather than shoehorning them into a particular form that is governed by parameters. A neural network could be seen this way, as the idea is to give a large amount of flexibility to allow the data to extract their own features that involve curvature and variable interactions; we don’t have to specify these like we do in a linear regression or generalized linear model if the model figures them out.If the author had referred to a neural network as a particular type of nonlinear regression, then the statement would not be controversial.Because of the lack of consensus on what “nonparametric” means, towards the end of 2020, there was a movement/joke on Cross Validated (Statistics Stack Exchange) that our new year’s resolution would be to stop using the term “nonparametric” for exactly that reason."
Comparing parameters of networks,"
Maybe this is a silly way to compare networks, but I would like to compare several networks based on the number of parameters (learnable features) needed in each one. This is with regards to signal classification. Most networks take in a 2x128 or 2x1024 or 2x32768 (etc) signal input. You can think of this as a 2x  pixel image. What I am noticing is that networks that have large inputs tend to have more parameters. I'd like to get an idea of how 'efficiently' a given network is using its parameters.

Is there an industry standard for normalizing a networks parameters based on the input image/input feature map?

Does anyone know off hand of other networks which have input images of 64k (any shape, or any type of image classifier)?


Essentially, I designed a network which uses a 64k feature input. I'd like to know if the 2 Million learnable parameters needed for this network is excessive or reasonable. Most other signal classification networks have smaller feature inputs and require less learnable parameters overall. Thus, I am having difficultly comparing them.
Thank you!
",['classification'],
uniform gap between training and validation metrics,"
I am training a neural network (Deep and cross network) for a multi-label classification task (~700 labels). I have around 2.5 million samples, splitted 8/1/1 for train/test/validation. I am seeing a uniform gap between training and validation results on various metrics. E.g. see the graphs below. What are the potential explanations for such phenomenon? My conjecture would be some sort of information leakage issue, but I double checked the data and re-did train/test/validation split and the gaps persisted.
EDIT: I am using train_test_split from sklearn:
df_train, df_test = train_test_split(
    df, test_size=0.2,
)

df_test, df_val = train_test_split(
    df_test, test_size=0.5,
)





","['deep-learning', 'loss', 'cross-validation', 'metric']",
What is the next step in top-down brain simulation after spiking neural networks?,"
This paper from Yamazaki et al. describes a 68 billion spiking neural network model of the cerebellum. The simulation was about 600 times slower than real time, and the cerebellum is perhaps one of the more boring parts of the brain in terms of intelligent behavior, but this still seems like a significant step in the direction of meaningful top-down brain simulations.
To be clear, by top-down simulation, I'm referring to simulations that allow massive abstractions away from the biology of the nervous system in order to investigate and/or produce intelligence more directly, e.g. modern neural networks. This is in contrast to bottom-up simulation, which start with models of individual neurons (e.g. LIF, Hodgkins-Huxley) and add increasingly more realistic biophysical properties to them; these bottom-up simulations are geared towards testing hypotheses about low-level neurophysiology, rather than trying to produce or investigate intelligence per se.
My question, as someone very unfamiliar with these subfields, is: what are the most promising next steps for top-down brain simulation? Is it worth it to try to take more sophisticated network architectures like RNNs or transformers and make them spiking? Is it feasible to combine top-down and bottom-up simulation approaches, for example by organizing LIF models into a convolutional architecture (this particular example is likely impossible as I don't think the LIF neurons would be compatible with backpropogation)? Or should we think that the solution may not involve deep neural networks at all?
","['neural-networks', 'models']",
Numerical problems with gradient descent,"
I'm trying to implement a simple neural network for classification (multi-class) as an exercise (written in C). During gradient descent, the weights and biases quickly get out of control and the gradient becomes infinite.
I haven't been able to find any discussion of such problems (vanishing gradients is kind of the opposite).
To be more specific, for testing I use a very simple network with 1 hidden layer and sigmoid as activation function. For the output layer I use softmax and logarithmic loss.
The issue as I see it is that when an output activation is very small, for the derivative of the loss I basically get 1 / <very_small number> and this leads to an enormous gradient.
Am I doing something wrong in terms of network architecture?
What is the typical way to deal with such problems?
",['gradient-descent'],
Datasets input at model.fit produce unexpected results of training loss vs validation loss,"
Im trying to train a neural network (VAE) using tensorflow and Im getting different results based on the type of input in the model.fit.
When I input arrays I get normal difference between the validation loss and the total loss.
When I input a dataset based on the same input I get a normal total loss and a really small validation loss.
I havent changed the model. The only things that changes is the input format.
The code for when I input an array.
train slices is (2627,138,138,1) and define the batch size in the model.fit
train_slices = preprocess_data(CropTumor, file_array[train_dataset])
val_slices = preprocess_data(CropTumor, file_array[val_dataset])



# reset model weights before training
VAE.set_weights(initial_weights)

# fit model
fit_results = VAE.fit(train_slices,train_slices,
                      epochs=1000,
                      validation_data=(val_slices,val_slices),
                      callbacks=[early_stopping_kfold, tensorboard_callback],
                      batch_size=batch_sz,
                      verbose=2
                      )

The output
Epoch 1/1000
2022-08-01 11:56:35.683852: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401
2022-08-01 11:56:36.371780: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-08-01 11:56:36.461054: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
672/672 - 7s - loss: 537.2896 - val_loss: 213.7070 - 7s/epoch - 11ms/step
Epoch 2/1000
672/672 - 5s - loss: 248.5211 - val_loss: 161.9758 - 5s/epoch - 7ms/step
Epoch 3/1000
672/672 - 4s - loss: 192.8771 - val_loss: 125.9349 - 4s/epoch - 6ms/step
Epoch 4/1000
672/672 - 4s - loss: 153.1647 - val_loss: 99.4395 - 4s/epoch - 6ms/step
Epoch 5/1000
672/672 - 5s - loss: 132.0143 - val_loss: 88.9975 - 5s/epoch - 7ms/step
Epoch 6/1000
672/672 - 4s - loss: 118.5642 - val_loss: 81.1653 - 4s/epoch - 6ms/step
Epoch 7/1000
672/672 - 5s - loss: 108.6678 - val_loss: 76.6315 - 5s/epoch - 7ms/step
Epoch 8/1000
672/672 - 4s - loss: 100.9759 - val_loss: 73.8963 - 4s/epoch - 6ms/step

When on the other hand I use the same data in the form of dataset I get a really small validation loss
    train_dset = 

tf.keras.preprocessing.image_dataset_from_directory(directory=""./Data/09_TrainingSet_VAE1"",
                                                                     labels=None,
                                                                     label_mode=None,
                                                                     image_size=(138, 138),
                                                                     color_mode=""grayscale"",
                                                                     batch_size=None,
                                                                     shuffle=True)

    val_dset = tf.data.Dataset.from_tensor_slices(val_slices)
    train_dset = (train_dset.map(preprocess_dataset).batch(batch_sz).shuffle(1))
    val_dset = (val_dset.map(preprocess_dataset).batch(batch_sz).shuffle(1))
    # reset model weights before training
    VAE.set_weights(initial_weights)

    # fit model
    fit_results = VAE.fit(train_dset,
                          epochs=10,
                          validation_data=val_dset,
                          callbacks=[early_stopping_kfold, tensorboard_callback],
                          verbose=2
                          )

And my output is
 Epoch 1/10
2022-08-01 12:04:08.656012: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401
2022-08-01 12:04:09.335957: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-08-01 12:04:09.431082: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
613/613 - 7s - loss: 466.5601 - val_loss: 17.3872 - 7s/epoch - 12ms/step
Epoch 2/10
613/613 - 5s - loss: 217.4277 - val_loss: 7.7309 - 5s/epoch - 8ms/step
Epoch 3/10
613/613 - 5s - loss: 167.2855 - val_loss: 6.0742 - 5s/epoch - 9ms/step
Epoch 4/10
613/613 - 6s - loss: 130.8230 - val_loss: 1.9557 - 6s/epoch - 10ms/step
Epoch 5/10
613/613 - 6s - loss: 112.1165 - val_loss: 1.1561 - 6s/epoch - 10ms/step
Epoch 6/10
613/613 - 5s - loss: 101.3152 - val_loss: 0.6442 - 5s/epoch - 8ms/step
Epoch 7/10
613/613 - 5s - loss: 93.3648 - val_loss: 0.4150 - 5s/epoch - 8ms/step
Epoch 8/10
613/613 - 5s - loss: 87.1542 - val_loss: 0.2232 - 5s/epoch - 8ms/step

The loss function for both is
def loss_func(encoder_mu, encoder_log_variance):
def vae_reconstruction_loss(y_true, y_predict):

    reconstruction_loss = tf.math.reduce_sum(tf.math.square(y_true-y_predict), axis=[1, 2, 3])
    return reconstruction_loss

def vae_kl_loss(encoder_mu, encoder_log_variance):
    kl_loss = -0.5 * tf.math.reduce_sum(1.0 + encoder_log_variance - tf.math.square(encoder_mu) - tf.math.exp(encoder_log_variance),
                              axis=1)
    return kl_loss


def vae_loss(y_true, y_predict):
    reconstruction_loss = vae_reconstruction_loss(y_true, y_predict)
    kl_loss = vae_kl_loss(y_true, y_predict)
    loss = reconstruction_weight*reconstruction_loss + kl_weight*kl_loss
    return loss

return vae_loss

and model is compiled with
VAE.compile(optimizer=tfk.optimizers.Adam(learning_rate=learning_rate),
        loss=loss_func(encoder_mu_layer, encoder_log_variance_layer))

","['tensorflow', 'datasets', 'variational-autoencoder', 'training-datasets', 'validation-datasets']",
Do I have enough images? Fine-tuning pre-trained models for bvinary image classification,"
I am developing a binary image classifier, and my dataset size is of 90 images. From a theoretical point of view, are they enough for fine-tuning a pre-traiend classifier? I plan to test the models listed here.
",['classification'],"As a rule of thumb, a dataset in computer vision is considered small when it contains less than 1000 images per class. This holds also for finetuning pretrained models.This doesn't mean you won't get better results with less than that, but in your case 90 is truly a small amount that will almost certainly lead to overfit those images without any good  generalization.If the classes you want to classify are not part of any of the main ""big"" datasets like ImageNet, then unfortunately you'll have to sit down, prepare some coffee and start scraping and annotating images as much as you can.If instead the classes you want to classify are a subset of some already existing dataset, then your problem becomes more of a transfer learning problem, in which case it will be more useful for you to start investigating preprocessing step that will match your images with those of ImageNet (or whatever dataset was used to train the pretrained weights you're planning to use. Some annotation work will still be beneficial though, since 90 images are very few even just for evaluation, i.e. you'll might find nice tricks like normalization and histogram matching that boost the performance of a pretrained model on your 90 images just to see those step brutally fail in a real production environment."
Why are AI Safety discussions almost always from the perspective of reinforcement learning?,"
I have been reading some articles on AI safety and they almost always speak of AI Safety from the reinforcement learning (RL) perspective, i.e. where we have some artificially intelligent agent acting in an environment so to maximise some reward.
Is there a reason why the focus is very often from this RL POV?
Thanks.
Examples:

2021 AI Alignment Literature Review and Charity Comparison
Unsolved Problems in ML Safety
Paperclip Maximizer
AI Alignment Research Overview by Jacob Steinhardt

","['reinforcement-learning', 'agi', 'ai-safety', 'singularity']","I think what you are really asking is why don't we talk about alignment from the perspective of general deep learning?In fact, we do talk about alignment for general AI system that's being used in production. For example, making large language model less toxic is an active
area of research.The reason you see a lot of references to RL in the context alignment is that one method that has been actively researched to make our AI systems aligned with our preference is through training them via RL. The reward function is, not surprisingly, good alignment with human preferences."
What is the training accuracy of this model?,"

I’m trying to classifiy ECG signals using LSTM and MATLAB, the above plot shows that the training accuracy of the system is 100% but when I apply this code to calculate and get the accuracy I get only 20%
LSTMAccuracy = sum(trainPred == Labels)/numel(Labels)*100


Am I missing something here? Or there something wrong I did in my code?
Here is the configuration and the training code:
layers = [ ...
    sequenceInputLayer(1)
    bilstmLayer(100,'OutputMode','last') 
    fullyConnectedLayer(5)
    softmaxLayer
    classificationLayer
    ]

options = trainingOptions('adam', ...
    'MaxEpochs',1750, ...
    'MiniBatchSize', 150, ...
    'InitialLearnRate', 0.0001, ...
    'ExecutionEnvironment',""auto"",...
    'plots','training-progress', ...
    'Verbose',false);

net = trainNetwork(Signals, Labels, layers, options);

trainPred = classify(net, Signals,'SequenceLength',1000);

LSTMAccuracy = sum(trainPred == Labels)/numel(Labels)*100
figure
confusionchart(Labels,trainPred,'ColumnSummary','column-normalized',...
              'RowSummary','row-normalized','Title','Confusion Chart for LSTM');

","['machine-learning', 'deep-learning', 'long-short-term-memory', 'signal-processing', 'matlab']",The problem can be solved using the following line of code:
What is the need for _agency_ in AI? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



Why seek to develop artificially intelligent agents? Are there certain advantages and/or needs provided by such supposed intelligent agents that are preferred to simply using intelligent tools that are devoid of agency (e.g. language models)? If so, what are these needs and advantages that intelligent agents could serve?
Basically, why would you want agency in an AI Model in the first place?
Here, I define agency as the ability to autonomously perceive and interact with a given environment. Anything capable of agency is then an agent. Furthermore, I define

autonomous perception: the ability to perceive a given environment without the need of an external agent
interaction: the ability to change the state of the environment

","['reinforcement-learning', 'agi', 'intelligent-agent']",
Markov's Decision Process - calculate value in each iteration,"
I have the following decision tree:

",,
When is it better to utilize machine learning over heuristics?,"
I learned that 87% of machine learning projects fail due to these five pitfalls:

the scope of the project is too big;
the project’s scope increased in size as the project progressed—e.g., scope creep;
the model couldn’t be explained, hence there was lack of trust in the solution;
the model was too complex; and
the project solved the wrong problem.

And I learned that rules and heuristic approaches may be a better choice than machine learning, since the development of machine learning takes more time(and more costly) and its explanation is also harder.
Then I wonder when we should just use rules and heuristics and when we should bravely take the machine learning approach?
References:

Why Ai/Data Science Projects Fail

","['machine-learning', 'mlops', 'production-systems']",
,,,
,,,
Whys and Why-nots using Rust for AI,"
The title says it all. I would like to know more about what attributes and design choices of Rust that make it a good (or bad) language for  the entire ecosystem of AI (both research and production)
What I know so far:

Rust is memory safe (unlike C) and highly performant (like C). This make Rust a good choice for building heavy lifting compute engine (like kernels in TensorFlow and PyTorch)
Rust supports real multithreading (unlike Python) and that can potentially make it a good language for building distributed training libraries for training deep learning models
Anecdotally, Rust has a rather steep learning curve (unlike Python) and that might scare away ML practitioners without solid CS background.
Rust does not support scripting (unlike Python) and that might make it unsuitable for quickly exploring and prototyping ML ideas. But I am not really sure about this point. There are jupyter kernel for Rust, so creating an interactive dev environment does not seem to be impossible.

Some subfields of AI (like NLP) has become engineering heavy in the recent years. Even in research, much of effort is on curating large datasets. Two important libraries are built with Rust to purposefully handle challenges coming from curating large dataset

Hugging Face's Tokenizers library
Google's Deduplication library

So modulo the anecdotal steep learning curve, Rust looks like a good programming language for doing ""all kind of stuff"" in AI. Evidences to counter this opinion are especially welcome.
","['machine-learning', 'deep-learning', 'programming-languages', 'rust']",
"Why would it be easy to evaluate a probability, when it is hard to sample from for importance sampling?","
Suppose we want to perform importance sampling where we have trajectories from some behavioral policy $b$, but we want to perform off-policy evaluation.  From these prior questions, I understand that importance sampling can be useful because we can easily sample from and evaluate the target policy $\pi$, while it is hard (or no longer possible) to sample from the original policy $b$.  This is given by:
$$E_{x \sim \pi}[f(x)] \approx \frac{1}{n} \sum_{i=1}^n f(x_i) \frac{\pi(x_i)}{b(x_i)} $$
where the original trajectories $x_i$ were from $b$, but have now been reweighted as if they came from $\pi$.
However, if we no longer have access to sample from policy $b$, then how would we hope to easily evaluate the probability of some $x$ using policy $b$?  Conversely, if we are able to easily evaluate $b(x_i)$, how come we can't sample from it?  I'm especially interested in real-life examples where the probability distribution is not Poisson, Gaussian or some simple Bayes Net.  For example, the probability distribution could be related to recognizing cats in images, dialogue generation or question answering.
","['reinforcement-learning', 'off-policy-methods', 'importance-sampling']",
,,,
Limitations of LSTMs,"
I'm training an LSTM model for classification on accelerometer data, and I get better results when I downsample the signal to 25 Hz than when I use a 50 Hz signal.
I use the same time frame of 1.5 seconds. So with a 25 Hz signal, I have 37 data points; with a 50 Hz signal, I have 75 data points as input to my model.
I think that LSTM models have a harder time dealing with long sequences, which might explain this difference. However, I am not sure about that claim. Are you aware of any publications that go into depth about the limitations of LSTMs for long sequence tasks?
","['long-short-term-memory', 'sequence-modeling']",
,,,
,,,
U-Net Maxpooling vs Convolution,"
Hello I'm implementing a CycleGAN and most of the other implementations I've seen on the internet use Convolution with stride 2 instead of a Maxpoolinglayer for downsample.
On to my question, why should we dismiss Maxpooling and instead add stride 2 to the Convolutions in the U-Net for CycleGANs. Is it because to much information gets lost in the Maxpoolingoperation? Or is there a different reason?
","['neural-networks', 'machine-learning', 'generative-adversarial-networks', 'u-net', 'cycle-gan']",
How can evaluate the success of my algoritm?,"
A little bit of context. I have a classification algorithm based on mathematical discriminator and I am not applying any machine learning or AI technique, just moving window and several relative comparisons. This is a signal segmentation problem, where I need to detect three states based on their mathematical properties or morphology. In addition, I need to perform this with minimum delay and almost real-time.
The output of my algorithm is a labelled signal with the different sections of it labelleded depending on those mathematical descriptors. The reference model was labelled by professionals that tagged the data for me and I am in charge of creating an automatic labelling tool for it.
Here you can see on the top the reference model and on the bottom the output of my algorithm.

My question is, I have defined a measure of success function that compares sample by sample the two signals but it is not very accurate and I want to know what is the state of the art in this type of classification problem.
My ""measure of success"" function:
def measure_of_success(signal_size, result_labels, model_labels):
"""""" Measure of success function """"""
incorrect_matching = 0
for index in range(signal_size):
    expected_section = identify_section_from_sample(model_labels, index)
    result_section = identify_section_from_sample(result_labels, index)
    if expected_section != result_section:
        incorrect_matching += 1
matching = (1 - (incorrect_matching / signal_size)) * 100
return matching

This function performs a sample-by-sample comparison. My problem is: the labels from the upper picture were set by hand but, I does not matter how accurate the section is determined but just to know that the event has been detected for a considerable time. This is a bit confusing but basically, if I detect the same ""b"" event and it coincides in time with the majority of the reference model, it should be 100% ok. On the other hand, I feel this measure of success is a bit simplistic and there might be better ways like a confusion matrix or something like that.
What would you suggest me to do in order to evaluate the success of the classification?
Thanks!
","['classification', 'regression', 'evaluation-functions']",
,,,
,,,
How to improve the performance of Easy OCR,"
I am working on a project that requires me to identify a product on a grocery shelf. For that, I am trying to use test recognition and localization to spot a product.
I tried Easy OCR and tesseract OCR because they are giving me accurate results, but it takes a lot of time to process the image

Easy OCR takes about 7 seconds to process one image.
Keras OCR takes about 42 seconds.

I followed this post to implement the code: https://www.kaggle.com/code/odins0n/keras-ocr-vs-easyocr-vs-pytesseract
I am running this on my laptop with NVIDIA GeForce RTX 3050 GPU.
Is this expected behavior? Is there any way to improve the speed?
","['deep-learning', 'optical-character-recognition', 'text-detection']",
Dealing with incomplete file sets for a CNN for medical imaging regression problem,"
I'm trying to solve a medical imaging regression problem using a CNN.  Each of the samples in my data set consists of one, two, or three of the following file types:

flair.nii.gz
mprage.nii.gz
swi.nii.gz

Each of the files is a three- or four-dimensional matrix of voxel values between 0 and 255.
I don't want to throw information away since I have a limited number of data samples.  Is there a technique for working around those cases which have a 'missing' file (or two)?  For example, could I construct a matrix consisting of, say, all zeroes of the correct dimension and size and use this as a replacement for the file?  Would this work or would it lead to problems with the CNN?  Of course, the samples that I want to make inferences on might also have missing files, so, maybe, a matrix with all zeroes would be a type of information.
Anyway, what does one do in a case like this?
","['deep-learning', 'convolutional-neural-networks', 'regression']",
How to build the actor policy of Soft-Actor-Critic after sampling from a Multivariate normal distribution?,"
I'm trying to solve LunarLanderContinuous-v2 (https://www.gymlibrary.ml/environments/box2d/lunar_lander/) using Soft Actor-Critic algorithm (following the pseudocode above)

To update the actor policy (step 14) I understood that I need to:

sample an action from a normal distribution using mean and variance that are the output of the policy network
squash the action with tanh to have action bounds [-1, 1]
compute the log_prob, the log probability density of the sample
do the following operation in tensorflow

# STEP 14
bounded_actions_sample, predicted_next_actions_sample = policy_network.predict_actions(batch_observation)
pdf = get_pdf(predicted_next_actions_sample, batch_observation)
q_min = tf.math.minimum(q_network_1(batch_observation, bounded_actions_sample), q_network_2(batch_observation, bounded_actions_sample))
        
loss_policy_network = tf.reduce_mean(alfa * tf.math.log(pdf) - q_min))

where ""pdf"" is the value of the probability density function of the normal distribution computed from predicted action (not clipped), mean and variance.
Since the action space of LunarLanderContinuous-v2 is continuous and the possible actions are 2, my idea is to predict means μ, variances σ and a correlation ρ parameter in order to build a Multivariate normal distribution using tfp.distributions.MultivariateNormalTriL and sample actions from it.
My problem is that, according to the Appendix C of the paper https://arxiv.org/pdf/1801.01290.pdf, I have to compute the likelihoods of the bounded actions using

but I don't know how to do that because the first term is a single number due to it come from a Multivariate normal distribution, instead the second term contains 2 elements, one for each action.
Has anyone any idea to code it?
","['reinforcement-learning', 'tensorflow', 'soft-actor-critic']","I don't really understand the confusion, as both terms are scalars (where $u_i$ are the components of $u$).So if $u\sim \mathcal{N}(\mu, \Sigma)$, then the first term would be$$\log \mu(u\mid s)=-\frac{D}{2}\log(2\pi)-\frac{1}{2}\log\vert \Sigma\vert - \frac{1}{2}(u-\mu)^T\Sigma^{-1}(u-\mu) $$However, SAC assumes that $\Sigma$ is diagonal, which makes the expression easier, that is$$\log \pi(a\mid s)=-\frac{D}{2}\log(2\pi)-\frac{1}{2}\sum_{i=1}^D\log\sigma_i - \frac{1}{2}\sum_{i=1}^D\frac{(u_i-\mu_i)^2}{\sigma_i}-\sum_{i=1}^D\log(1-\tanh ^2u_i) $$If there is a correlation between the individual actions, you would have to implement the first expression.An explicit implementation I have come across in Pytorch is the following: (where $\sqrt {2\pi} \approx2.5066$)However, this is not efficient and there are undoubtedly specific functions in Tensorflow that make this implicitly, but I hope this provides more intuition."
What is the difference between representation and embedding?,"
As I searched about this two terms, I found they are somehow like each other, both try to create a vector from raw data as I understood. But, what is the difference of this two term?
","['machine-learning', 'deep-learning', 'terminology', 'embeddings', 'representation-learning']","Vector representation is a generic term used to talk about any type of feature encoding, embedding vectors are instead a special case of vector representation.When talking about vector representation the only underlying assumption is that every variable was encoded into numerical values, without any restriction regarding the numbers or the vector itself.Embedding vectors instead are specifically continuous vectors of fixed dimensions obtained trough matrix factorization techniques or deep learning models. They originally proposed to encode text in the Word2Vec paper, and since then they acquired more and more popularity due to the high generalization potential of the proposed method in other AI branched rather than natural language processing."
,,,
,,,
Adding several variables that could be important but can introduce overfitting,"
Sopose a productivity dataset, where day of the week and months day number are important. I'm thinking to encode these with a one-hot encoding.
But if you have few years of data, that features might define a specific year+dow+day because there are few days like ""Mondays June 1st"".
What is the best way to train that situation?
Would it be better to remove some fields like the day of the month?
Can you train twice, once with dow and once with the day of the month and then merge with an average?
","['deep-learning', 'training', 'data-preprocessing']",
fondamental question about regularization techniques to solve overfitting problem in neural networks,"
I have a text classification neural network based on BERT that overfits. The accuracy on the training dataset is 95%, whereas it is 68% on the validation dataset.
Using some classical regularization techniques (dropout=0.5) and weight_decay=0.7, I have an accuracy of 84% on the training dataset and 70% on the validation dataset.
My question is: Do regularization techniques usually improve the validation accuracy? Or they will only reduce the training accuracy to a closer level to the validation accuracy?
As my objective is improve the validation accuracy to 90%, and I am wondering if there is any hope that solving the overfitting problem would increase validation accuracy, so should I continue investigating on the regularization techniques or thinking about changing the whole model.
","['neural-networks', 'overfitting', 'bert']","Regularization techniques reduce overfitting. This is why they tend to reduce training accuracy when applied to a model: they prevent the model to learn noise from the training data. For the same reason the validation accuracy increases: learning less noise means better generalization on unseen data. These two aspects are sides of the same coin.There would be no point in developing a technique that only reduce training accuracy to match for aesthetic reasons the validation one.
So yes, you can keep investigating regularization techniques and you should always use them. But be aware that since you have a strict goal you want to achieve you might also want to try other architectures or expand your dataset. Regularization will mostly help you to get a better grasp on the best scores you can achieve (if you get 84% accuracy on training, I argue you'll probably never get to 90% validation, unless the model was far from converging) and they boost a bit good results, but they will not help you making a huge jump up."
How to encode both sentences and categorical data?,"
I have a DataFrame that contains several columns where some columns contain single words that can be category encoded since I know how many of them are there in total. However one column is an actual sentence with several pieces of information that outlines prices of something, delivery month, product name and sometimes some other info. That column is basically a text message and its format can vary.
Example DataFrame looks like this:




Name
User
Text
Target




sarah
noro
@-23.50 july 380/crx
CRX Laptop


john
simons
(atlas5) sep nc8 npc 131.5 @ 132.5
NPC Playstation


...
...
...
...




The columns Name, User, Text are features, and column Target is the target column that I want to predict. I would like to use a classifier (Random Forest, or Neural Net, or GBDT) to classify the Target based on other three columns.
I can category encode the columns Name, User, Target as I know how many of unique names, users, and targets there are.

How do I encode the Text column as it is a sentence?
Would it be better to regex split the Text column and simplify it (remove numbers etc) or just feed in the whole thing?

","['natural-language-processing', 'classification', 'data-preprocessing']",
How can I learn to transform one input signal (time series) into another?,"
I'm posting this question here because I've been trying in vain to solve a problem for weeks and I hope some of you might have some useful suggestions.
Basically, the problem is as follows. I have 7 quantities that vary simultaneously as time changes. These quantities are somehow related to each other, but for simplicity's sake we could also consider them independent.
To represent each of these quantities, I have a continuous signal that takes on values between 0 and 1 as time varies (x-axis). In essence, this is a time series as can be seen from the image below.

What I would like to be able to do is to transform the continuous signal into another signal that can only accept values 0 or 1. Practically, from the signal shown in the figure above, I would like to be able to generate the signal shown in the figure below.

I have a lot of data at my disposal, so I could create a set of training, validation and testing. The solution I thought of was to build a deep network that could learn the transformation between the input signal and the output signal.
Ideally, the network should learn through a tailored loss to indicate the difference between ground truth and input signal. In addition, some consideration should probably be given to the context and thus the relationships between the various time instants. For this reason (i.e., temporal correlation) I had thought of recurrent networks or Transformers.

However, I don't how to model this network and whether such a solution would actually make sense. I have also searched the literature for work of this kind, but to the best of my knowledge the problem has not been addressed.
The problem seems well defined to me, but unfortunately I cannot find the right solution. Do you have any suggestions for me? Thank you in advance
","['deep-learning', 'time-series', 'supervised-learning', 'signal-processing']",
"How to validate my knowledge of models implementation, pros and cons and area of applicability?","
So I've been doing ML for ~2 years in industry, I'm a BSc in applied math, finished several courses on ML/DL on coursera, read some specific topics in ML/DL books. Seem to be in the know, more or less.
But the catch is: I've never really validated my knowledge, as in taken an exam or certification. Some time has passed since I read theory and I sort of think that I'm losing it (especially since all I did was time-series forecasting).
What are the possible ways to validate that I actually understand stuff? Do I just text random ML experts ""hey could I tell you stuff and you say whether that's bs or not""?..
","['machine-learning', 'deep-learning', 'education']",
Combining Different Inputs in a Neural Network for Numerical Integration,"
I am building a NN that numerically integrates a non-linear differential equation. Given a DE: $$
\frac{d}{dt}x(t) = f(x, p)
$$
with solution $x \in \mathbb{R}^n$ and parameters $p \in \mathbb{R}^m$, it takes $x(0)$ and $p$ as inputs and guesses $x(T)$ for some time $T$.
I have a couple ideas for the architecture of the NN and am looking for guidance on the best route. What I'm primariliy confused about is how to combine $x(0)$ and $p$ in the NN. I can think of some ways to do this practically, but I'm not sure what is the theoretically strongest approach.
Idea 1: Concatenate $x(0)$ and $p$ and apply an $(n+m)$ x $n$ layer before passing on to other layers for further processing.
Idea 2: Apply an n x n layer to $x(0)$, an m x n layer to $p$, and then either add or convolve the two resulting vectors before passing them on to further layers.
","['neural-networks', 'convolution']",
Why does sklearn perceptron converge for linearly inseparable data points?,"
I learned that the perceptron algorithm only converges if the dataset is linearly separable. I am implementing this algorithm using scikit learn.
The blue and orange points are from the training set, while red and green are from the test set. Clearly the blobs are inseparable, but why does my perceptron algorithm converge and give me a decision boundary?

","['machine-learning', 'classification', 'python', 'perceptron', 'scikit-learn']","Line 30 of your jupyter notebook:Perceptron(max_iter=40, random_state=0)The perceptron does not converge, it simply stops after going through the data 40 times. Always read carefully the code and especially the documentation."
Question regarding matlab computer vision application and color recongnition [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 12 months ago.







                        Improve this question
                    




I am thinking of choosing a computer vision project for my school project(detect crack on surface) and the duration I have is roughly 4 months. With no prior knowledge in neural network, is matlab computer vision application consider to be ""user friendly"" to beginner?

When detecting a defect by comparing standard image to product image that work by checking the RGB contents of pixels to pixels. Does such method still involve the use of neural network(ie. the RGB information of a pixel can be set not to exceed certain value, otherwise it will mark that pixel as defect)?


","['neural-networks', 'computer-vision', 'matlab']","2 Yes, cracks and in general defects detection is a task performed with neural networks and deep learning, but depending on your specific use case it might be an overkill. You talk about pixel to pixel comparison, if your data are images taken from a production line with standardize equipment that capture images of the same piece always with same lighting, angle, etc then you'll probably want to investigate analytic methods for more explainable and efficient results. Neural nets become almost compulsory when the images to compare differs a lot between each other, making hard for an expert to came up with analytic rules that are always applicable.1 I never played a lot with matlab and its deep learning libraries, but I know that at the very least it allows to use models trained with python libraries like tensorflow, this might be an advantage since at least you know you can search models and repositories not exclusively written in matlab."
Entirely linear neural network learning non-linear function,"
I have a neural network that's trained on a sine wave. It uses a lookback of 20 to see what the last 20 predictions were and predict the next value. This network has only a single Linear layer (input size 20, output size 1) with no activation function and from just that, it is able to extrapolate a sine wave almost perfectly.
This is very confusing to me, as the $sin$ function is non-linear: $sin(a+b) \neq sin(a) + sin(b)$ so the network shouldn't be able to approximate it (very well).
The code to reproduce this is below:
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import torch.optim as optim

X_train = np.arange(0,100,0.5) 
y_train = np.sin(X_train)

X_test = np.arange(100,200,0.5) 
y_test = np.sin(X_test)

n_features = 1

train_series = torch.from_numpy(y_train)
test_series = torch.from_numpy(y_test)

# Expects input of (batch, sequence, features)
# So shape should be (1, 179, 20) and labels (1, 1, 179)
look_back = 20

train_dataset = []
train_labels = []
for i in range(len(train_series)-look_back):
    train_dataset.append(train_series[i:i+20])
    train_labels.append(train_series[i+20])
train_dataset = torch.stack(train_dataset).unsqueeze(0)
train_labels = torch.stack(train_labels).unsqueeze(0).unsqueeze(2)

class Net(nn.Module):
    def __init__(self, input_shape):
        super(Net, self).__init__()
        
        self.fc = nn.Linear(input_shape, 1)
    
    def forward(self, x):
        out = self.fc(x)
        return out


model = Net(look_back).double()
loss_function = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

loss_curve = []
for epoch in range(300):
    loss_total = 0
    
    model.zero_grad()
    
    predictions = model(train_dataset)
    
    loss = loss_function(predictions, train_labels)
    loss_total += loss.item()
    loss.backward()
    optimizer.step()
    loss_curve.append(loss_total)

extrapolation = []
seed_batch = test_series[:20].reshape(1, 1, 20)
current_batch = seed_batch
with torch.no_grad():
    for i in range(180):
        predicted_value = model(current_batch)
        extrapolation.append(predicted_value.item())
        current_batch = torch.cat((current_batch[:,:,1:], predicted_value), axis=2)

x = np.arange(110,200,0.5)
fig, ax = plt.subplots(1, 1, figsize=(15, 5))
ax.plot(X_train,y_train, lw=2, label='train data')
ax.plot(X_test,y_test, lw=3, c='y', label='test data')
ax.plot(x,extrapolation, lw=3, c='r',linestyle = ':', label='extrapolation')
ax.legend(loc=""lower left"")
plt.show();

And it produces the following plot:

","['neural-networks', 'machine-learning', 'regression']","You can even predict a sinusoid with much less than 20 samples: two previous samples suffice. The reason is that sinusoids appear as solutions of second-order linear difference equations.In other words, you can write the linear relationship of your sinusoid with frequency $f$ and phase $\phi$ as$sin (f \times n + \phi) = a_1 sin (f \times (n-1) + \phi) + a_2 sin (f \times (n-2) + \phi)$with real prediction coefficients $a_1$ and $a_2$ and discrete time index $n$."
Which paper describes the effect of learning_starts in Reinforcement Learning?,"
I have seen many popular RL libraries have a learning_start parameter. This allows the agent to collect enough experiences before training on the replay_buffer. However, I am unable to find the paper that introduced this parameter. This post describes the parameter quite well.
This paper does go a little into this parameter but it seems to be in the context of deep learning and not reinforcement learning.
","['reinforcement-learning', 'deep-rl', 'dqn', 'hyper-parameters']","The replay buffer allows breaking the temporal dependence of the data and thus makes them more i.i.d. (which is what we want). The replay buffer needs to be at least filled with enough experience to compute a single batch, but can also be filled completely before the actual training begins (which avoids 'oversampling' the early experiences). Those implementation details were probably left out in most methods sections.This question here provides further details on the replay buffer in the context of DQNs."
Cross Validation and hyperparameter selection correct procedure,"
I am trying to run a regression supervised learning problem. The dataset is not very large and I wanted to do some cross-validation to avoid overfitting. As I have read it's important to do a sensitivity analysis to determine the value of k. Also, I would like to do some hyperparameter grid search for the algorithm (i.e. random forests).
What would be the correct procedure? First take a random value of k and perform the hyperparameter grid search and with the correct hyperparameters do the sensitivity test for k or vice versa?
Thanks in advance!
","['ai-design', 'hyperparameter-optimization', 'cross-validation']",
Why Is There The Term 1/m In Backpropagation,"
In backpropagation the gradients are used to update the weights using the formula
$$w = w - \alpha \frac{dL}{dw}$$
and the loss gradient w.r.t. weights is
$$\frac{dL}{dw} = \frac{dL}{dz} \frac{dz}{dw} = (\frac{dL}{da} \frac{da}{dz} \frac{1}{m}) \frac{dz}{dw} $$
Why is the there the $\frac{1}{m}$ term? Does batch size matter and what if it's 1?
","['neural-networks', 'tensorflow', 'backpropagation', 'math', 'derivative']",
How are Neural Networks protected from false training data?,"
Suppose the training data there exist an element of some data being misleading and some being right, how could the Neural network be trained so that it could filter the right data from the wrong one?
Excuse me if this question is written as a bit non technically my knowledge is not that high of this topic.
","['neural-networks', 'machine-learning']","For all intents and purposes, the neural networks are not protected against bias dataset unless the dataset is explicitly curated wrt to an arbitrary bias.This is very much an open problem and a busy research area with many researchers attempting to address this problem in academic and production environments.Some of the work on solving this is specific circumstances involves weighting certain data differently and utilizing statistical techniques to otherwise counteract the biases."
How to handle anomaly detections with multiple different timeseries' from network traffic?,"
I would like to implement an anomaly detection algorithm on multiple timeseries' from different network users. Since each user has different behavior and network traffic usage, my question is how can I implement an anomaly detection algorithm for this case? If possible I would like to have the model to be trained on online data, meaning when new data arrives it should be able to use that data, so that I dont need to train it over and over again.
When dealing with new users, it should consider other users as reference and not immediately trigger an anomaly.
I was thinking about training an ensemble model of LSTMs with different temporal properties such as sequences of minutes, hours, days, weeks, months in order to predict successfully short-term anomalys and long-term occurences. Does anyone else had the same problem in the past?
","['neural-networks', 'time-series', 'anomaly-detection']",
How does a sigmoid neuron act like a perceptron in this scenario?,"
I have been reading Michael Nielsen’s book online on his website at http://neuralnetworksanddeeplearning.com/chap1.html. I am struggling to understand the second exercise:

When c approaches infinity, wouldn’t make the sigmoid function always output a value close to 1 whereas a perceptron can output 0 or 1.
Let me know if I am missing something or maybe if someone can rephrase the question in a clearer way.
","['neural-networks', 'perceptron', 'sigmoid']",
How to detect and deal with data distribution drift/change?,"
I'm working on a problem in ML to assess the performance of multiple vendors. I have a set of features in my dataset, and it appears each vendor is characterized by its own distribution. This is my hypothesis, as I see my target variable shifts and changes value ranges with time, as different vendors' data is achieved.
Is there a way, in a real application system, to implement a mechanism to detect such data drift/change (whether in target variable or features distribution)?
If so, how can I deal with it? Should I be constantly re-training a model to deal with the new data? What is the common practices to deal with new observed data?
","['machine-learning', 'data-preprocessing', 'data-drift']",
Why do smaller weights converge faster for RNNs?,"
I am writing a Recurrent Neural Network using only the NumPy library for a binary classification problem. When I initialize the weights with np.random.randn, after 1000 epochs it gets ~60% accuracy, whereas when I divide the weights by 1000 first, it reaches 100% accuracy after the same amount of epochs.
Why is this? Do RNNs work better with smaller weights or does the number 1000 mean something?
Any and all help is welcome, thanks.
","['recurrent-neural-networks', 'python', 'binary-classification', 'weights-initialization', 'numpy']",There is no magic value that work for every network but in general:Check this blog post for pretty good animations of both problems.
Are RL algorithms suppose to keep learning?,"
I don't understand if the purposes of RL agents is simply optimizing a model with a reward instead of using labeled data (i.e. in a supervision fashion), or they have also the purpose of keep training and exploring in order to adapt to possible environment changes.
","['reinforcement-learning', 'deep-rl', 'training', 'exploration-exploitation-tradeoff']",
How to prove that an action-value function optimal for one problem formulation is also optimal for another?,"
I want to ask about the intuition/where-to-look/what-to-try if I want to prove that an action value function optimal for a problem is also optimal for another reformulation of that smae problem. For example, a common approach to solve DEC-POMDP is to recast the problem into a common-knowledge-MDP. In this case (and in any similar case) What is the approach to follow in order to prove that the optimal action-value function optimal for the single agent reformulation is also optimal for the multi-agent formulation?
","['reinforcement-learning', 'markov-decision-process', 'decision-theory']",
RL solutions for OpenAI Gym environments?,"
Is there any place where people share their agent's settings for solving OpenAI Gym Environments?
For example, I'd like to know what are good parameters for a DDPG agent to learn the task in Reacher-v2. I believe that a lot of people tried to solve it and maybe they shared their solution for achieving better performance.
","['reinforcement-learning', 'deep-rl', 'open-ai', 'ddpg', 'gym']",
Can entropy bonus be used with state-independent log std for stochastic policies?,"
In this blog article by openai, they say the std of the exploration distribution must be state-dependent, i.e. an output of the policy network, so it works with the entropy bonus, which is an integral part of the SAC algorithm.
My question is: Does the std always have to be state-dependent when entropy bonus is used? PPO baselines uses a state-independent std for the exploration distribution.
","['policy-gradients', 'proximal-policy-optimization', 'exploration-strategies', 'soft-actor-critic', 'entropy']",
Can I use discrete data in the same model as continuous data?,"
In my dataset, I have some data that is continuous - eg. Age and BMI. I also have some data that is discrete- for example, occupation is labelled as 1 =""Homemaker""
2=""Working""
3=""Unemployed""
4=""Retired""
5=""Extended Sick Leave""
6=""Disabled""
7=""Other"".
My question is- is it correct to pre-process this data the same (0-1 scale, fill in missing values with mean, SMOTE etc) to then put into my model (SVM, KNN, LR, RF). Or do I need to perform different preprocessing with the categorical data? I have looked into One Hot Encoding however concluded it is not appropriate since the data is not string like, but I am unsure.
","['machine-learning', 'data-preprocessing']","It's definitely wrong to threat categorical data as continuous. It is also wrong to encode the variable occupation with increasing numerical values before feed them to the models you listed.Reason being that those numbers have no meaning in relation to those categorical values, for example 3 is bigger than 2 but it has no sense at all to say that the class unemployed is bigger than the class working.One hot encoding is definitely the way to go to encode categorical variables like occupation.Filling in missing values with mean of course will not work cause you don't have a mean, and usually that's a very bad way to fill in missing values, even for continuous variables. If you have lot of instances with missing values that you can't discard you can add a new value called ""Unknown"" and fill in the missing data with that class. Conceptually is the most reasonable thing to do since you have no information.SMOTE is a whole different story, not related to continuous or categorical variables, and you should consider it only if you have classes (i.e. variables values) heavily underrepresented. It is though a risky move that might lead to over fitting and misinterpretation or your models performances, so if you don't understand it conceptually I would skip it."
ImageNet Dataset (for PyTorch VGG16 training),"
Please can someone describe how to properly obtain the ImageNet dataset (to be precise the ImageNet 2012 Classification Dataset).
What I attempted so far
The ImageNet webpage refers the user to download the ImageNet dataset from Kaggle. However, the Kaggle webpage it refers belongs to the Image Localization (not classification) challenge.
I have also requested a download from the ImageNet webpage which is pending since almost one year.
","['image-recognition', 'pytorch', 'image-net']",
How can I solve a classification problem where the label for the same input is probabilistic?,"
In the usual classification problems, the label for the same input is usually the same. For example, if I have an image of a dog, then the true label for that exact input is dog every time.
However, for my dataset, the label for the same (or very similar) input is probabilistic, i.e. if I have the same input (or very similar inputs), the label follow a statistical distribution, e.g. 60% of the time it is a dog, 40% of the time it is a cat
How should I solve such a machine learning task?
Example: given a sequence of stock prices p[t-T:t], if you buy at time t, the chance that you will make 2% profit in 1 hour is 60%, and 40% otherwise.
","['machine-learning', 'classification']",
Has there been a study done in tuning hyper-parameters for off-policy reinforcement learning?,"
I am interested in learning about hyper-parameter tuning for off-policy reinforcement learning (specifically DQN). Could someone point me to papers published or empirical observations in this area?
","['reinforcement-learning', 'dqn', 'off-policy-methods']",Yes and we can do it in a similar way we normally perform hyper-param optimization. See this paper from GoogleIn this framework we can also attempt to jerry-rig some of our classic algorithmic tuning techniques with better or worse success depending on the context.
Are there any transcripts of GPT-3 arguing that it is not conscious?,"
There have been a lot of transcripts showing GPT-3 arguing that it is self-conscious. In response, it was pointed out that GPT-3 can argue anything and pretend to be anything, given appropriate leading questions. Has anyone made GPT-3 specifically argue that it is not conscious? In my opinion, that would be the most direct counterargument to claims that GPT-3 is conscious.
","['artificial-consciousness', 'gpt-3']",
How to decide size of generated dataset in DAGGER agorithm,"
In the DAGGER algorithm, how does one determine the number of samples required for one iteration of the training loop? 
Looking at the picture above, I understand initially, during the 1st iteration, the dataset D comes from pre-recorded samples and is very large (in the tens of thousands).
however, i'm confused about step 2. How large should Dπ be?
This post explains that it is good idea to ""anneal out"" D0 during the subsequent iterations,
which I think means having two separate datasets, and sample proportionally from each during step 1 (which implies in step 4, it is Dπ that is being aggregated, not D), in that case, is there a maximum size for Dπ?
I've already looked at What does the number of required expert demonstrations in Imitation Learning depend on?, which seems to explain Dπ can be determined using a formula. However, the formula involves terms such as discount factor γ and various other terms which I don't think applies in this case. The paper I'm trying to implement is End-to-end Driving via Conditional Imitation Learning
Thanks for any help!
",['imitation-learning'],
How to make neural networks more robust(intuitive explanation)?,"
Low spectral Norm ensures tight Rademacher complexity and ensures low generalization error for neural nets. Can anyone explain me this in a intuitive manner along with Rademacher view point.
Rademacher Complexity
","['neural-networks', 'deep-learning']",
Understanding Rademacher Complexity Deeply,"

empirical Rademacher complexity is defined as,
$$
\hat{R}_{m}(\mathcal{F}, S)=\frac{1}{m} \mathbb{E}_{\boldsymbol{\sigma}}\left[\sup _{f \in \mathcal{F}} \sum_{i=1}^{m} \sigma_{i} f\left(z_{i}\right)\right] .
$$
Remark. We can motivate the Rademacher complexity from the binary classification. Let $f$ be a classification function which maps data $z_{i}$ to its label $\sigma_{i} \in\{-1,1\}$. It is straightforward to show that $\sup _{f \in \mathcal{F}} \sum_{i}^{m} \sigma_{i} f\left(z_{i}\right)$ is equivalent to minimizing the classification error. Taking the expectation over all $\sigma_{i}$ amounts to considering all possible labeling (partitioning) of the samples. If $\mathcal{F}$ consists of a single function $f$, then $\hat{R}_{m}(\mathcal{F}, S)=0$. If $\mathcal{F}$ shatters $\left\{z_{1}, \cdots, z_{m}\right\}$, then $\hat{R}_{m}(\mathcal{F}, S)=1$. Therefore, the Rademacher complexity intuitively indicates how expressive the function class is.

I am trying to understand this theorem very deeply. Unfortunately, I cannot be able to understand much from this note. Can anybody help me understand very deeply and intuitively so that I can make understand other people very easily?
","['neural-networks', 'deep-learning']",
Hot to calculate Maximum Normalized log Probability for Active Learning with BERT,"
I have encountered difficulties understanding the calculation of Maximum Normalized Log Probabilities acording to Shen et al..

With n being the sequence length, yi the label of word i. Xij is the representation (the input).
Let me describe my setting. I'm using the implementation of BERT provided by pytorch to finetune a BERT-Base model for sequence labeling. The goal is to determine a label for each word in the sequence. There are three possible labels for each word. The basic model is set up and runs no problem. Implementing Active Learning based on Maximum Normalized Log Probability is not the problem. Im just not sure if i understand the formular correcly. The Model output is a 128 long (since each sequence is 128 words. Overflow from wordPiece Tokenization is not relevant) list of Labels.
So far i'm using the softmax-function to get the probability for each label instead of the logits the BERT-Model provides as i hand it the unlabeled data. Then i calculate the log probabilities for every label via the log10 function. In the end I sum up the maxima of the predictions  over all the words (=the probability of the predicted label for each word) and average them. This is my MNLP value i use to identify the most uncertain instances.
Am i doing it right? My main problem is the max with lowered yi ... yn. As far as i understand it, its used to indicate that only the maximum of the label likelihoods per word is relevant, aka. the probability of the predicted label.
","['neural-networks', 'deep-learning', 'natural-language-processing', 'definitions', 'bert']",
How is it possible to use batches of data from within the same sequence with an LSTM?,"
ETA: More concise wording: Why do some implementations use batches of data taken from within the same sequence? Does this not make the cell state useless?
Using the example of an LSTM, it has a hidden state and cell state. These states are updated as new inputs are passed to an LSTM. The problem here, is if you use batches of data taken from the same timeframe, the hidden state and cell state can't be computed on previous values.
This is a major problem. The main advantage of an LSTM is this exact mechanic
For an example of what I mean, take the following. A simple LSTM used to predict a sine wave. At training time, I can't think of any way you could use batches. As you have one timeseries here that you are predicting, you have to start at time step 0 in order to properly compute and train the hidden state and cell state.
Taking batches like in the image below and computing them in parallel will mean the hidden and cell state can't be computed properly.

And yet, in the example I gave the batch_size is set to 10? This makes no sense to me. It also doesn't help tensorflow syntax isn't exactly the most verbose...
The only use case for batches in an LSTM I can see is if you have multiple totally independent sets of timeseries that can all be computed from timestep 0 in parallel with each having it's own cell and hidden state
My implementation
I actually duplicated the example LSTM from above but used Pytorch instead. My code can be found in a Kaggle notebook here, but as you can see, I've commented out the LSTM from the model and replaced it with a fc layer which performs just as well as the LSTM, because like I said, while using batches in this way it makes the LSTM utterly redundant.
","['machine-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'pytorch', 'batch-learning']",
Can I shuffle data for delivery duration forecast problem?,"
I'm new to ML and trying to write a solution to a food delivery duration time problem (so called lead time). I used algorithms such as random forest and gradient boosting which gave OK results but not amazing. I have daily data covering 3 years, that I split to train/test with an 80/20% split.
To try and improve results, I repeated the split however I did it randomly, i.e. on a shuffled data. My model, does not use any relations between data points, it only predicts the lead time using that specific sample's features (e.g. type of food, who's the carrier, expected delivery duration, etc.)
When I do this, the results improve dramatically, which makes me question I'm doing something illegal.
I wanted to know if I can actually shuffle the data? Can I do that?
","['machine-learning', 'python', 'data-preprocessing', 'data-augmentation']",
Neural network and logical gates,"
I have a network witch consist of two fully connected layers (without bias) and a ReLu function in between. The network input is two binary numbers, and the output should be the a logical gate result:

I need to find the weights values for each one of the following logical gates: AND, OR, XOR.
In addition, I need to figure out witch one of the three gates cannot be achieved without using an activation function.
I am trying to think what the weights values should be, but I can't figure it out. I thought that $h_1$ can be $x_1+x_2$ and $h_2$ can be $x_1-x_2$ but it's not working for any of the gates.
","['neural-networks', 'activation-functions', 'relu']",
how to manage the impact of Covid on building a machine learning model,"
I need your suggestions for using historical data to build a machine learning model for analyzing the market and build an AI model(tree based model/random forest or regression analysis) for setting the price.
I have data from these years:

< 2020
2020
2021
2022

I am not confident about predictive power of pre covid data, my question form data scientist guys is how they manage the probable effect of covid. I divided my historical data as above because I think we have

pre-covid, normal data,
covid-time and its shock
after-covid data, data with the impact of covid
next-normal, I am not even sure that the industry meets new normal or not

do you have any guide line that says how to start, I need a framework to manage these data.
","['machine-learning', 'data-preprocessing', 'prediction', 'data-science', 'training-datasets']",
What loss function should I use if I only care about the accuracy of one class?,"
CrossEntropyLoss optimizes the overall classification accuracy as
$$ {n_{\text{correct}} \over N} $$
What loss function should I use if I only care about increasing the true positive rate of one class?
$$ {n_{\text{true A in predicted A}} \over N_{\text{predicted A}} } $$
For example, I predict 100 images to be in class A, and 90 out of this 100 are truly A. So the accuracy is 90%.
In the meantime, I predict another 900 images to be in class B, but 500 of them are actually A, and only 400 are B. So the overall accuracy is (90+400)/(100+900) = 49%.
In the meantime, I don't want $ N_{\text{predicted A}} $ to be too small, since one can see from above that a smaller $ N_{\text{predicted A}} $ can likely lead to larger true positive rate.
",['objective-functions'],"Cross entropy can also be weighted, standard practice when training on imbalanced datasets. Instead of the classic formulation$$C = -\sum_{i=1}^{M}y_i\text{log}(\hat{y}_i)$$you can rewrite it as$$C = -\sum_{i=1}^{M}\sum_{c=1}^{N}w_cy_i\text{log}(\hat{y}_i)$$were $w_c$ represent the weight of each training class. So if you care mostly about the accuracy of a specific class you could boost that specific class by using a large weight for it, let's say 0.8, and 0.2/(n-1) for all the remaining classes."
Play against your own RL-trained AI from gym retro,"
so far I have seen people implementing reinforcement learning to build an AI to play and complete games on gym retro, such as street fighter, racing games and so on. However, I was wondering if it is possible to play against your own trained AI to get some sort of human evaluation? Anyone has any idea for this? Thank you
",['reinforcement-learning'],
what is `Normalize` for in PyTorch transfer learning tutorial?,"
in this pytorch tutorial, there is transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), what is the purpose of this?
(i removed it and the code still works)
","['convolutional-neural-networks', 'pytorch']","Those are mean and standard deviation used to standardize each channel of the images from IMAGENET used to train the torchvision pretrained models.Since the models were trained using this preprocessing step, it is useful to apply it also when using those models for transfer learning on new data."
why cross entropy loss has to be multiplied by a batch size during an evaluation in transformer model?,"
I am trying to look through a code of the transformer model from Pytorch. However,
I do not understand why batch size needs to multiply with cross-entropy loss given that loss is calculated based on data at a given timestep.
This is from the line: ""total_loss += batch_size * criterion(output_flat, targets).item()""
This is the section of code:
def evaluate(model: nn.Module, eval_data: Tensor) -> float:
    model.eval()  # turn on evaluation mode
    total_loss = 0.
    src_mask = generate_square_subsequent_mask(bptt).to(device)
    with torch.no_grad():
        for i in range(0, eval_data.size(0) - 1, bptt):
            data, targets = get_batch(eval_data, i)
            batch_size = data.size(0)
            if batch_size != bptt:
                src_mask = src_mask[:batch_size, :batch_size]
            output = model(data, src_mask)
            output_flat = output.view(-1, ntokens)
            total_loss += batch_size * criterion(output_flat, targets).item()
    return total_loss / (len(eval_data) - 1)

","['transformer', 'pytorch', 'cross-entropy', 'evaluation-functions']",
Reconstructing 3D models from 2D images using autoencoders,"
I went through a research paper (""Voxel-Based 3D Object Reconstruction from Single 2D Image Using Variational Autoencoders"") and tried to implement the approach following this diagram:
![link to image of reference network- https://ibb.co/4JgbQ9s
Here is my implementation for the same:
image = Input(shape=(None, None, 3))

# Encoder
l1 = Conv2D(64, (3,3), strides = (2), padding='same', activation='leaky_relu')(image)  
l2 = MaxPooling2D(padding='same')(l1)
l3 = Conv2D(32, (5,5), strides = (2), padding='same', activation='leaky_relu')(l2)
l4 = MaxPooling2D(padding='same')(l3)
l5 = Conv2D(16, (7,7), strides = (2), padding='same', activation='leaky_relu')(l4)
l6 = MaxPooling2D(padding='same')(l5)
l7 = Conv2D(8, (5, 5), strides = (2), padding = 'same', activation = 'leaky_relu')(l6)
l8 = MaxPooling2D(padding='same')(l7)
l9 = Conv2D(4, (3, 3), strides = (2), padding = 'same', activation = 'leaky_relu')(l8)
l10 = MaxPooling2D(padding='same')(l9)
l11 = Conv2D(2, (4, 4), strides = (2), padding = 'same', activation = 'leaky_relu')(l10)
l12 = MaxPooling2D(padding='same')(l11)
l13 = Conv2D(1, (2, 2), strides = (2), padding = 'same', activation = 'leaky_relu')(l12)

#latent variable z
l14 = Reshape((60,512))(l13)
l15 = Dense((60*512), activation = 'leaky_relu')(l14)
l16 = Dense((128*4*4*4), activation = 'leaky_relu')(l15)
l17 = Reshape((60,4,4,4,128))(l16)

#Decoder
l18 = UpSampling3D()(l17)
l19 = Conv3DTranspose(60, (8, 8, 8), strides = (64), padding='same', activation = 'leaky_relu') (l17)
l20 = UpSampling3D()(l19)
l21 = Conv3DTranspose(60, (16,16,16), strides =(32), padding='same', activation = 'leaky_relu')(l20)
l22 = UpSampling3D()(l21)
l23 = Conv3DTranspose(60, (32, 32, 32), strides = (32), padding='same', activation = 'lealy_relu')(l22)
l24 = UpSampling3D()(l23)
l25 = Conv3DTranspose(60, (64, 64, 64), strides = (24), padding='same', activation = 'leaky_relu')(l24)
l26 = UpSampling3D()(l25)
l27 = Conv3DTranspose(60, (64, 64, 64), strides = (1), padding='same', activation = 'leaky_relu')(l26)

model3D = Model(image, l27)

This gives me error for l10 saying:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_33/351640059.py in <module>
     24 #Decoder
     25 l18 = UpSampling3D()(l17)
---> 26 l19 = Conv3DTranspose(60, (8, 8, 8), strides = (64), padding='same', activation = 'leaky_relu') (l17)
     27 l20 = UpSampling3D()(l19)
     28 l21 = Conv3DTranspose(60, (16,16,16), strides =(32), padding='same', activation = 'leaky_relu')(l20)

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    975     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):
    976       return self._functional_construction_call(inputs, args, kwargs,
--> 977                                                 input_list)
    978 
    979     # Maintains info about the `Layer.call` stack.

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)
   1113       # Check input assumptions set after layer building, e.g. input shape.
   1114       outputs = self._keras_tensor_symbolic_call(
-> 1115           inputs, input_masks, args, kwargs)
   1116 
   1117       if outputs is None:

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)
    846       return tf.nest.map_structure(keras_tensor.KerasTensor, output_signature)
    847     else:
--> 848       return self._infer_output_signature(inputs, args, kwargs, input_masks)
    849 
    850   def _infer_output_signature(self, inputs, args, kwargs, input_masks):

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)
    884           # overridden).
    885           # TODO(kaftan): do we maybe_build here, or have we already done it?
--> 886           self._maybe_build(inputs)
    887           inputs = self._maybe_cast_inputs(inputs)
    888           outputs = call_fn(inputs, *args, **kwargs)

/opt/conda/lib/python3.7/site-packages/keras/engine/base_layer.py in _maybe_build(self, inputs)
   2657         # operations.
   2658         with tf_utils.maybe_init_scope(self):
-> 2659           self.build(input_shapes)  # pylint:disable=not-callable
   2660       # We must set also ensure that the layer is marked as built, and the build
   2661       # shape is stored since user defined build functions may not be calling

/opt/conda/lib/python3.7/site-packages/keras/layers/convolutional.py in build(self, input_shape)
   1546     if len(input_shape) != 5:
   1547       raise ValueError('Inputs should have rank 5, received input shape:',
-> 1548                        str(input_shape))
   1549     channel_axis = self._get_channel_axis()
   1550     if input_shape.dims[channel_axis].value is None:

ValueError: ('Inputs should have rank 5, received input shape:', '(None, 60, 4, 4, 4, 128)')""```

Any help and guidance is appreciated.


","['machine-learning', 'computer-vision', 'autoencoders', '3d-convolution', '2d-convolution']",
is it possible to train the same neural network with different numbers of inputs and outputs?,"
is it possible to create an adaptative neural network that can change the number of its inputs and outputs without having to train it each time it changes?
the neural netwrok has to take purchases and affect them to the delivery man that will minimize the global traveled distance.
","['neural-networks', 'machine-learning', 'travelling-salesman-problem']",
'Advancing' basic models,"
Good morning.
I am a student running a project using medical data, predicting if the patient will or won't get a disease. The data has about 50k cases and 70 features.
I proposed to train 5 models- SVM, KNN, LR, RF and a neural network on this data, using cross-validation to optimize the hyperparameters and then report the best AUROC.
However I've been told by my professor that using these models isn't 'advanced' enough and I need to do something at a higher level.
First- I must admit I am not an expert at ML- i have run similar projects before successfully but nothing more advanced. I have also read a lot of literature on disease prediction using tabulate data and I am struggling to find options that are more 'advanced' and yet still feasible for myself- I have only been learning computer science for 9 months so my abilities are limited. However, I do have about 5 weeks to develop these models so I know I can learn lots.
Please can anybody suggest where I can start? I am really worried about failing the project based on the reaction of my professor to my proposal.
Thanks in advance
","['python', 'scikit-learn']","If some of your input features are categorical, you could consider a TabTransformer model (see paper and blog post).
There is an official Keras implementation and tutorial here and a third-party Pytorch version on Github. It may count as advanced enough because it uses Transformers, while still having a relatively simple neural network architecture.There are other examples of structured data classification on the Tensorflow/Keras websites that you can use for inspiration or comparison/baselines, e.g. GBDTs. Another option is to search past Kaggle competitions that are similar to your project, see what kind of advanced models scored highly and learn from their code.Personally I think anybody who can use Sklearn can use Keras, and I don't have a computer science background. Just look for a beginner-friendly tutorial that suits your needs, ideally one for the latest versions of TF/Keras from the official websites (they've changed quite a bit over the last few years)."
Remove already reached targets from the system to enable reaching other targets?,"
This may be a very fundamental question, but somehow I can't decide.
I have a graph and the user can take several actions while traversing it and there are multiple points with rewards. When I execute the MDP process, it ends up finding the first (the one it reaches first) target repeatedly and cannot find the others. So, now I am removing the reward value of that node once its reward is reached. Is it a correct approach? If not, what should I do instead of that?
Thanks in advance, Kind Regards, Ferda.
",['markov-decision-process'],"So, now I am removing the reward value of that node once its reward is reached. Is it a correct approach? If not, what should I do instead of that?Partially correct. For the environment to continue making sense to the agent (and be a valid MDP), you need to set a state variable to represent this change. A list of booleans ""goals reached so far"" would be one simple way to do it. Note that each such goal state that toggles between giving a reward or not depending on history doubles the state space."
Effective fast mobile ocr model,"
I plan to develop OCR application which is mobile oriented and fast. (like 10~30 fps) The images that will be detected is not wild images. They are refined data such as cell phone capture images. In this situation what will be the most effective OCR model? I'm thinking yolo for text detection and bilstm for text recognition. However I don't have idea to make the model to end to end like this paper. Any idea or paper will be appreciated. Thanks for reading my question.
",['optical-character-recognition'],
How to deal with small reward values,"
In my environment rewards are generally small, e.g. [-0.01, 0.01]. My concern is that small reward values might get dominated or distorted by the noise during the training. Does it make sense to scale up the rewards, say multiple by 100?
","['reinforcement-learning', 'deep-rl', 'rewards', 'reward-functions', 'reward-shaping']","The numbers that a value-based neural network will predict are usually based on expected returns (sum of rewards by end of an episode, or a discounted infinite sum), although in some cases they might be based on average reward. You will generally know which is in use if you are building the environment. For instance, if you typically run episodes that are 1000 time steps long, then rewards in $[-0.01, 0.01]$ on each time step might be fine.Neural networks are less sensitive to scale of outputs for regression problems than they are to scale of inputs. However it does affect things such as the scale of gradients for loss functions.You could adjust for small expected returns by increasing the learning rate to compensate. But it should also be OK to re-scale the rewards so that the min and max returns are somewhere in e.g. $[-10, 10]$. The exact range is not a big deal, so you could pick a range where you are comfortable interpreting the values that you see quickly - multiplying by some factor of 10 makes it easier to map back to the original (and presumably meaningful) values in your head.In the original Atari Game DQN paper, the authors normalised game scores for each of the games. This was mainly so that the same hyperparameters for the neural network worked in all the games (the agent was not tuned specially for each game). However, it may have also had some small benefit of keeping gradients within bounds for more efficient learning."
Rationalle behind SE3 Transformer?,"
I have just finished reading the SE3 transformer paper (SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks) by Fuchs et-al and although I'm sure I understand less than 50% of the math involved, I have a lingering question:
Is there a reason one can't use distance matrix representation, e.g.:
$$
 M[i,j]=d(i,j),  \forall i > j
$$
as a valid rotation translation equivarient representation for point cloud data? Why is there a need for a neural network to encode such representation?
(of course, it is not a one - to - one representation, inverted/flipped point clouds can still map to the same distance matrix, but in most practical applications this distinction carry little significance)
","['neural-networks', 'transformer', 'graph-neural-networks']","After thinking about it for a while, I noticed that distance matrices are rotation and translation invariant rather than equivariant.For equivariant representation, you'd want that a rotation will produce a rotated output."
Help with model architecture for a racing game,"
I’m working on a model for a racing game using pytorch. The model gets frame from the game as input and produces a controller state as output. The dataset consists of frames from the game and corresponding controller state (buttons state + joystick state)
Until now I’ve been using a cnn connected to linear layers to predict the output. The problem is that this model cannot establish connection between buttons state output and the joystick state output so it can’t learn how to drift or aim items. (when the player drifts using b, the joystick output need to be updated accordingly)
I thought about the following architecture:

I’m new to deep learning and I would like to know if someting like that even makes any sense because I haven’t seen anything like it before. If it does not make sense , can you suggest another model architectre?
Thanks in advance.
dht
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'game-ai']",
Can an AI have awareness?,"
I was thinking about the following:
According to Sir Roger Penrose ""No computer has any awareness of what it does."".
Now some context to his statement:
Penrose’s argument in summary in his book (The emperor's new mind: concerning computers, minds, and the laws of physics) is that:

We don’t understand physics well enough to use it to describe our brains.
We don’t understand the mind well enough to create a framework capable of accommodating human consciousness.
Since our minds do not operate ‘computationally’ and are non-algorithmic, our intelligence, therefore, can’t be recreated by computers (Source).

Let's look at the latest controversy started by a Google engineer who claimed that Lambda is self aware. But, then I came across Searle’s Chinese Room and realized that Lambda is not self aware.

Then there are other self learning AIs like OpenAIFive that was really good in playing DOTA2.
But how do we know this AI understands what it's doing? If not then is there a possibility that a hypothetical AGI in future might be aware of its actions?
As a note I will add that the brain is still a big mystery and there's no unanimous  consensus on a single definition of consciousness. And I think every definition out there is just a combination of thought and faith/belief. And Penrose as of lately does however thinks that there might be a connection between consciousness and quantum mechanics.
","['philosophy', 'agi', 'artificial-consciousness', 'imagination']",
Cannot understand/ reproduce reuslt in figure 1 from MobileNetV2?,"
Hi I am recently reading the MobileNetV2 paper and I found I cannot understand the following figure in the paper.

I made a simple code trying to reproduce the results, but I got opposite results.
These are visualizations of (1)my input, (2)after ReLU in 3 dimensional space, (3)transform from 3D back to 2D:

And here is the result for transforming the same input to 15 dimensional space, applying ReLU and transform back to 2D.

I don't see an obvious difference as expected in the paper. Since this paper is cited many times, I bet I made mistakes somewhere. Could anybody help point out where I am wrong? I used the following code to generate my results:
import numpy as np
import matplotlib.pyplot as plt
n=256
angle = np.linspace(0, 12*2*np.pi, n)
radius = np.linspace(.5, 1., n)
x = radius * np.cos(angle)
y = radius * np.sin(angle)
plt.plot(x,y)
plt.show()

# ==== embed into 3 dimensional space and transform back after applying ReLU ====
T = np.random.random((3, 2))
T_inv = np.linalg.pinv(T)
after_T = T @ np.row_stack((x, y))
after_relu = after_T
after_relu[after_relu < 0] = 0

# ---- visualize transformed 3d shape ----
ax = plt.axes(projection='3d')
ax.plot3D(after_relu[0],after_relu[1],after_relu[2])
plt.show()
# ---- ----

new_x_y = T_inv @ after_relu
plt.plot(new_x_y[0], new_x_y[1])
plt.show()


# ==== embed into 15 dimensional space and transform back after applying ReLU ====
T = np.random.random((15, 2))
T_inv = np.linalg.pinv(T)
after_T = T @ np.row_stack((x, y))
after_relu = after_T
after_relu[after_relu < 0] = 0
new_x_y = T_inv @ after_relu
plt.plot(new_x_y[0], new_x_y[1])
plt.show()

",['deep-learning'],
Is the loss calculation step in Logistic Regression even needed?,"
I was reading about Logistic Regression and trying to implement the model from scratch. Maybe I am wrong, but I have noticed that the loss calculation step is meaningless in training a Logistic Regression model.
For example:
loss = -(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))

Almost every article or video I have watched it was said that ""We need calculate the loss based n output and the ground truth values"". But then when we do backprop through the model we calculate gradient for dz like this:
dz = y_hat - y

And parameters are calculated like this:
dw = np.dot(x, dz.T) / m
db = np.sum(dz) / m

Gradient descent:
w = w - learning_rate * dw
b = b - learning_rate * db

Through all of these steps there is not a single place where the variable loss was used. From what I can conclude is that we are calculating loss just to print it on the screen for the programmer to see how is the training process going (to see if the model is learning something or not).
Does that mean that the loss calculation step is not needed at all or am I missing some key points?
If answer t the previous question is NO, does that mean that loss calculation step is trivial in Neural Networks as well?
","['machine-learning', 'deep-learning', 'logistic-regression']",
Time taken to solve cartpole environment using DQN,"
I am trying to solve the cartpole environment (GitHub) using DQN agent. I have been building my own DQN agent by following a tutorial by Jon Krohn.
I am able to solve the environment with a maximum reward of 499. I am using the experience replay method, and I have given the number of episodes as 1000 episodes.
In the tutorial it takes very much less time to solve the environment, whereas for me it's taking almost 2 hours to solve it.
The library I am using for building my model is tensorflow (tf.keras).
","['reinforcement-learning', 'tensorflow', 'dqn', 'keras', 'gym']",
Metrics using batches v/s metrics using full dataset,"
I am using training an image classification model using the pre-trained mobile network. During training, I am seeing very high values (more than 70%) for Accuracy, Precision, Recall, and F1-score on both the training dataset and validation dataset.


For me, this is an indication that my model is learning fine.
But when I checked these metrics on an Unbatched training and Unbatched Validation these metrics are very low. Even are then on 1%.
Unbatched dataset means I am not taking calculating these metrics over batches and taking the average of metrics to calculate the final metrics which is what Tensorflow/Keras does during model training.

I am unable to find out what is causing this Behaviour. Please help me understand what is causing this difference and how to ensure that results are consistent on both, i.e. a minor difference is acceptable.
Code that I used for evaluating metrics
def test_model(model, data, CLASSES, label_one_hot=True, average=""micro"", 
                threshold_analysis=False, thres_analysis_start_point=0.0, 
                thres_analysis_end_point=0.95, thres_step=0.05, classwise_analysis=False,
                produce_confusion_matrix=False):
    images_ds = data.map(lambda image, label: image)
    labels_ds = data.map(lambda image, label: label).unbatch()
    NUM_VALIDATION_IMAGES = count_data_items(tf_records_filenames=data)
    cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() # get everything as one batch
    if label_one_hot is True:
        cm_correct_labels = np.argmax(cm_correct_labels, axis=-1)
    cm_probabilities = model.predict(images_ds)
    cm_predictions = np.argmax(cm_probabilities, axis=-1)
    
    warnings.filterwarnings('ignore')

    overall_score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average=average)
    overall_precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average=average)
    overall_recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average=average)
    # cmat = (cmat.T / cmat.sum(axis=1)).T # normalized
    # print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))
    overall_test_results = {'overall_f1_score': overall_score, 'overall_precision':overall_precision, 'overall_recall':overall_recall}

    if classwise_analysis is True:
        
        label_index_dict = get_index_label_from_tf_record(dataset=data)
        label_index_dict = {k:v for k, v in sorted(list(label_index_dict.items()))}
        label_index_df = pd.DataFrame(label_index_dict, index=[0]).T.reset_index().rename(columns={'index':'class_ind', 0:'class_names'})
        # Class wise precision, recall and f1_score
        classwise_score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average=None)
        classwise_precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average=None)
        classwise_recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average=None)

        ind_class_count_df = class_ind_counter_from_tfrecord(data)
        ind_class_count_df = ind_class_count_df.merge(label_index_df, how='left', left_on='class_names', right_on='class_names')

        classwise_test_results = {'classwise_f1_score':classwise_score, 'classwise_precision':classwise_precision,
                        'classwise_recall':classwise_recall, 'class_names':CLASSES}
        classwise_test_results_df = pd.DataFrame(classwise_test_results)
    
        if produce_confusion_matrix is True:
            cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))
            return overall_test_results, classwise_test_results, cmat
        return overall_test_results, classwise_test_results
        
    if produce_confusion_matrix is True:
        cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))
        return overall_test_results, cmat
    warnings.filterwarnings('always')
    return overall_test_results

","['computer-vision', 'tensorflow', 'metric', 'confusion-matrix', 'mobile-net-v2']","After spending many hours, I found the issue was due to the shuffle function. I was using the below function to shuffle, batch and prefetch the dataset.Part of the function that causes the problemI removed the shuffle part and metrics are back as per the expectation.
Function after removing the shuffle partResults after removing the shuffle part
I am still not able to understand why shuffling causes this error. Shuffling was the best practice to follow before training your data. Although, I have already shuffled training data during data read time.I have also posted the same issue at SO thinking that it might be related to some coding error."
LSTM exploding? - multiple parallel time series with multiple variables,"
I have the following situation:




Stock
Time_Stamps
Feature_1
Feature_2
Feature_n
Price




Stock_1
2019
0.5
1.0
1.0
100


Stock_1
2020
0.7
1.3
0.9
90


Stock_2
2019
0.3
0.9
1.1
110


Stock_2
2020
0.2
0.8
1.1
120


Stock_n
year_n
value_n
value_n
value_n
price_n




So this is how my data table is structured. My original df has 100+ features and 70000k observations resp. 2000+ stocks - so this is only a simplification.
I want to train a LSTM on this data table and look for features correlation with the price.
Common idea, nothing new, so pls save your time giving me ""this will not work"" bla bla.
I am generally interested in how you would approach this problem. We have multiple inputs (features) for our time series forecast with 8 time stamps (8 years) per stock. However, in my understanding, I'd have to train my model for every stock seperately which is inconvenient.
How would you pre-process my data, so that I can train a decent model?
","['long-short-term-memory', 'data-science']",
Emergent behavior in AI models that looks similar to natural neural systems,"
""ImageNet Classification with Deep Convolutional Neural Networks"" by Krizhevsky & Sutskever & Hinton describes very interesting emergent behavior of the AlexNet.
It was trained on 2 GPU's:

specialization exhibited by the two GPUs ... The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is independent of any particular random weight initialization

Likewise our brain mostly processes color with left side of the brain.
Are there other examples of emergent behavior in AI models that looks similar to natural neural systems? Any kind from coordination of several neurons to high-level function, usefull or detrimental, like optical illusions?
So far I found only some articles with optical illusion examples.
","['training', 'neuroevolution']","Yes, there is a whole field where people compare the emergent representations of biological and artificial systems. It is very often found that the role of individual neurons is similar. For example, in the early layers of the visual pathway (retina, LGN, V1), we see receptive fields that are sensitive to local contrast, edges, center-surround and similar simple patterns. The same holds for early layers of CNNs. Intermediate layers both of the visual cortex and of CNNs are sensitive to increasingly complex stimuli such as grids, circles, complex patterns, etc., up to features like eyes, wheels, dog ears. At the last layer, CNN representations are semantic, i.e. coincide with labels (by construction). Equivalently, you find areas of the brain where there are semantic representations, i.e. cells that are sensitive to the concept of (famously) Jennifer Aniston, independently of how we see/hear/read of her.Just a couple of examples of literature I'm familiar with: Banino et al. found grid cells, similar to those for which the Mosers won the Nobel prize for physiology, in a neural network trained for a spatial navigation task. People like James DiCarlo spent their careers on this kind of thing applied to the visual cortex, a summary can be found in this paper, where they propose a challenge for finding the computer vision model that is most similar to the brain; but all of his work is relevant. In fact, an amazing fact is that CNNs trained to do the same task as an animal (vision), can linearly predict the activity of neurons in the animal's visual cortex better than if the network was directly trained to predict those neurons' activities. People also did similar things for language models and for audio.These are all works that compare single-neuron activations (receptive fields, preferred stimuli) between biological systems and artificial models. Of course other methods of comparison could be devised but it sounds like this is what you were interested in."
What type of neural network has an unorganized structure?,"
I am looking for a network that has an unorganized structure like this, is feed-forward, does not have back-propagation functionality, and is trained with a genetic algorithm.
What would I be looking for? I also want to be able implement it in python.
This library would help me make a network like what I described, but its in Javascript.
Are there any others like it?
","['python', 'genetic-algorithms', 'feedforward-neural-networks']","What would I be looking for?This seems a lot like neuroevolution of augmenting topologies, or NEAT for short.There is a Python library neat-python.Even if your own idea wil have some differences to NEAT, it is worth checking out either as something similar to benchmark against, or as a starting framework to modify."
How special tokens in BERT-Transformers work?,"
""[SEP] tokens are useful to differentiate the questions from answers through type_ids"" Yes, but how is this helping model to understand that ""I should look paragraph and generate answers from here""? We don't have if-else inside the model that will say: ""if type_id==1, generate questions from here""
The same question appears for this example too:
[CLS] previous question [Q] current question [SEP] text [EOS]
How model says: ""I should look at the previous question, understand the meaning from here too with the current question, and answer the question."" We need if-else in here too like:
If there is a previous question:
get the meaning from it and use it with the current question
One more example based on this paper:https://arxiv.org/pdf/2005.01107v1.pdf
In this paper, we have a dataset like this: https://huggingface.co/datasets/iarfmoose/question_generator
In the t5 transformer, if we don't have those  and  tokens, the model will not learn anything. And I have no idea how specifying  and  we are helping the model to generate questions based on that answer in the context.
I hope I am clear with my question.
","['neural-networks', 'deep-learning', 'natural-language-processing', 'transformer']",
Can we combine Alpha-zero with GTP-4 to create a general AI?,"
Alpha Zero is good at looking into the future to plan it's next move.
GTP-4 is good at generating language from previous text.
It seems like combining these two systems would create a general conversation and problem solving AI.
For example, in response to a question, the AI could generate a thousand candidate replies using GTP-4, then generate a thousand suggestions of how the human might reply to each of those responses. This chess-like thinking is exactly what Alpha Zero is good at.
By evaluating it's own responses and predicting which ones would lead to a successful outcome, this is more or less what human's do.
(The only difference might be that humans have a longer memory than a GTP-4 algorithm).
",['general-problem-solver'],
which method are able to approximate the following sequence 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 5x0 1 6x0,"
I'm learning deep learning and machine learning techniques, but it looks like neural networks are not capable of predicting the next element in the sequence 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 5x0 1 6x0
Is any other method from machine learning sphere, that can predict next value in such sequences?
Code implementation for the sequence
def sequence() {
    i = 0
    while (true) {
       yield 1
       for j in range(i):
            yield 0
       i += 1
    } 
}

",['machine-learning'],
Deep Learning with Best-so-far instead of Where-you-are,"
It is my understanding that when training a Deep NN in Tensorflow/PyTorch/... we only keep the current state of the network in memory, except perhaps when we manually decide to save the current weights to the HDD/SSD.
Now, naively speaking it may seem reasonable to not only remember the current state (i.e. the current values of the trained weights), i.e. ""where we are"", but also the ""best"" weights ""so far"" by some metric, such as the validation error. Immediately, this approach doubles our memory requirements, especially if we want to keep everything in the GPU memory.
Is this done in practice? If not what are arguments against it?

Now there are certainly details that make this question more tricky. If you saved only each epoch it's probably fine to just save your weights to the HDD without significant loss in speed. If you remember the best weights for each step of SGD than you would have to compute the validation error over and over, which is costly. Or you use perhaps an estimate of the training error based on its gradient, which however could be leading to overfitting if you are not careful. You could also mix and match, or you could compute the validation error only on a subset of the data, or only remember the best every $k$ steps, etc. etc. You might also want to go back and restart from a ""better"" set of weights in the hopes of getting a better trajectory.
I also don't know whether in practice at the of training people typically, deliberately, go back to a previous ""checkpoint"" they might have saved somewhere.
","['neural-networks', 'deep-learning', 'reference-request', 'training']","Is this done in practice?Yes, this is done normally when using (lack of) improvements to validation metrics as a stop criterion, and many libraries support it as standard. Depending on the library, you may find you need to add a little code to keep a copy of the best-so-far weights, but some will do it automatically by default, or based on setting params on the train or fit function.For example, Keras' EarlyStopping class has a restore_best_weights parameter. Using this class in your main fit function, and setting the param to true will do what you want automatically with no other code required.If not what are arguments against it?Over-fitting to the validation set is a possible concern, as running the validation checks 100s of times to decide the ""best"" model may lead to some maximisation bias, and make decisions between other hyperparameters than the number of epochs less reliable."
Principles of designing a neural network,"
I have become more familiar with libraries such as tensorflow for a while now, and have become interested in utilizing neural networks for solving specific problems. The big question I have is, what are some principles that you have to take into account for designing your neural networks architecture?
Some other questions I have are:

Do I want my network to slowly reduce the dimensionality of data (so
it picks out important features), the deeper it goes? What happens
when the output (lets say its one hot encoded, so the no. classes is
in the 1000s while your e.g text is only of length 30) is a lot
bigger than the input?
If so, then what do I do when I have to process a single class? Do I just add layers which expand off that 1 input (isn't that wasting resources?)

What resources do you recommend I should look into?
","['neural-networks', 'machine-learning', 'resource-request']","Designing neural network architectures from scratch for harder tasks is work usually performed by entire research groups (whether academic or business). There are, however, some things to keep in mind:Deeper networks have more abstraction, but also higher complexity. This means they can learn more complicated relations between input and output, but can also maximally encode the noise in the dataset. You may know this as the bias/variance trade-off, but is more often called over-fitting.The type of data that goes into a network and the intuitive aspects of the data help decide what type of layers to use. Linear layers are your baseline. Convolutional layers when location is important (whether 1D, 2D or 3D). Attention layers for context, such as in language models. Recurrent connections for time-series data, such as speech patterns or observations over time.Determining best practices on your own is an impossible task. There is a reason research is based on previous work. You may gain intuition over time, but decisions like when to use additional operations such as pooling, batch normalization, or which hyper parameters to use, such as activation functions, optimizers and learning rate, are best borrowed from existing work. Try to find work on a similar task to what you want to do and read some papers, preferably, or blogs and tutorials if the former is too complex.If you have a specific task in mind, by all means post it as a comment to this answer and I can help you find similar work and/or suggest some intuitive starting point from my own experience.In response to your second question: You do not need to reduce the dimensionality of your data from the first layers of your model. In fact, you can expand it considerably. In the end you need to boil it down to the format of your output. For example classification using a CNN will first create feature maps of your data, which will increase dimensionality (e.g. 256x256x3 image to 128x128x64 feature maps), then it will flatten the features (128x128x64 feature maps to 1048576 hidden nodes) and finally it will classify (1048576 hidden nodes to 1000 output classes)."
Why instance segmentation architectures using reconstruction masks but not regression?,"
I'm wondering why many model architectures use binary mask reconstruction for segmentational CNNs, and not regression of mask polygon coordinates? Many object detectors use regression to find coordinates of bounding boxes.
","['object-detection', 'image-segmentation', 'mask-rcnn', 'instance-segmentation']","The reason is probably that the segmentation polygons have various shapes and complexities. You don't know how many points you need per polygon so defining a proper output that specifies the polygons is not straight forward. In contrast, bounding boxes are always defined by 4 coordinates (2 coordinates for lower left and upper right corner are already enough)."
Can an AI generated image (such as pic of human face) be detected that it's AI generated?,"
AIs are getting better and better at creating images and art. Some of the stuff is almost impossible to be detected by the naked eye. But what about programs and algorithms? Instead of creating an image, can anything detect that this image was created by an AI?
Take this one for example:
This picture of a woman's face was generated by AI

","['image-recognition', 'image-processing', 'human-like', 'face-recognition', 'face-detection']",
Tensorflow-gpu and multiprocessing,"
I have finished implementing an Asynchronous Advantage Actor-Critic (A3C) agent for TensorFlow (gpu). By using a single RMSprop optimizer with shared statistics. To do so, a central controller holds both the Global Network (ActorCriticModel) and the Optimizer. Workers, to update the global network, need to communicate gradients to the Controller, while receiving back the updated model weights. It works decently: despite there might be some errors in the code, it still achieves the results pretty fast.
Each Worker is a thread on its own.
class Worker(Thread):
Problem is Workers need to extend the Thread class while I would like to use heavy Processes (from multiprocessing) instead (Queues used in the following snippet of code are imported from the multiprocessing class).
class Controller:

    def __init__(self, **kwargs):

        self.terminated_workers = 0
        self.env_name = kwargs[""env_name""]
        self.available_actions = kwargs[""available_actions""]
        self.action_size = len(self.available_actions)
        self.env = make_env(kwargs, env_name=self.env_name)
        self.state_shape = self.env.observation_space.shape

        self.number_of_workers = kwargs[""number_of_workers""]
        self.workers = []
        self.controller_queue = Queue()
        self.worker_queues = []

        # Initialization of threads
        for i in range(self.number_of_workers):
            worker_queue = Queue()
            self.worker_queues.append(worker_queue)

            updater = GlobalUpdater(self.controller_queue, worker_queue)
            worker = Worker(i, updater, self.state_shape, kwargs)
            self.workers.append(worker)

        # Only import tensorflow after heavy Process initialization
        import tensorflow as tf

        self.global_model = ActorCriticModel(self.state_shape, self.action_size)
        self.opt = tf.keras.optimizers.RMSprop(
            learning_rate=kwargs[""lr_schedule""],
            rho=kwargs[""rho""],
            centered=kwargs[""centered""],
            clipvalue=kwargs[""clipvalue""]
        ) 

As soon as i change the Worker to extend Processes instead of threads, I get the following message for every worker instantiated:
F tensorflow/stream_executor/cuda/cuda_driver.cc:152] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
Technically, since multiple methods of the Worker class need to use tensorflow, the library is imported statically, before declaring the Worker class, as follows:
import tensorflow as tf

class Worker(Process):
   ...

I'm using Linux and the libraries' versions are:

TensorFlow: 2.9.1
CUDA: 11.2.2-1
cuDNN: 8.1.1.33-1+cuda11.2

I tried every other solution I found online, including setting mp.set_start_method('spawn') and trying to postpone as much as i can the import of the tensorflow library, so to postpone as much as possible the heavy process initialization but nothing seems to work. How would you approach this problem? Do you have any idea on how to solve it? Thanks in advance.
","['reinforcement-learning', 'tensorflow', 'actor-critic-methods', 'gpu', 'a3c']",
Mathematics books for reinforcement learning,"
This question is not about the math prerequisites of reinforcement learning, but about the textbooks of mathematics that are enough to understand the literature on reinforcement learning.
What are the mathematics books that are recommended to study in order to understand the majority of the reinforcement learning literature?
","['reinforcement-learning', 'math', 'resource-request', 'books']",
How to manually adjust output from model? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I wonder if it is possible to add manual inference to the output of a model?
For example, I have a model called 'net', and the output value of 'net' is a vector called v = [v1, ... vn]. v is a binary vector. For some reason, I need to manually adjust this output, which means I need to manually flip some 0s to 1s and vice versa.
My question is, is it possible to do so. The reason I have this question are

I am new to torch
If I manually change the variable.data, even it is doable, I did not do any due adjustments on gradient.

Please enlighten me under the context of PyTorch
","['neural-networks', 'training', 'pytorch', 'hyperparameter-optimization']",
Is this simple game solvable with reinforcment learning?,"
Let's imagine this simple environment :
Each episode has a length of 1 step. Each action leads to a reward for this action.
The action space is of 3 : 'UP', 'DOWN', 'UNKNOWN'
Most of the time, the observation is a random vector. But sometimes, it is a sinusoid. The player has to predict if the next value is greater or smaller than the last one. If the vector is a sinusoid, it is very easy to predict what the next value would be. If it is random, there is no way to know.
If the answer is correct, the player gets a reward of 1. If it is incorrect, -1. If the action taken is 'UNKNOWN', 0.
A good model would predict 'UP' or 'DOWN' when the observation is a sinusoid, and 'UNKNOW' when it is random.
I thought it would be possible to solve this game with a Reinforcment Learning algorithm, what are your thougths ?
",['reinforcement-learning'],
Parametric vs Non-parametric generative models,"
I have a little perplexity trying to distinguish parametric vs non-parametric generative model.
In my understanding, a parametric generative model would try to learn the probability density function by estimating the parameters of an underlying distribution we are assuming. So just doing for example,
$$\theta^* = arg\max_\theta  \,\prod_{i=1}^N p_\theta(\textbf{x}_i)$$
I realize that in practice, we need to figure out what is the basic distribution that we are going to modify by adjusting the parameters $\theta$. So in the case of VAEs we use latent variables assumption to make training feasible, we jointly train $q_\phi(\textbf{z}|\textbf{x})$ and $p_\theta(\textbf{x}|\textbf{z})$ prametrizing both distributions with neural networks (i.e. encoder & decoder). In such case, we end up with the situation that all our distributions are Gaussians (assuming that prior and conditional are gaussians). So, having said that, can we conclude that VAEs are parametric? Also, what could be an example of non-parametric generative model?
I would say that, for example, GANs maybe an example of non-parametric model, as we start with a latent normal distribution but then applying a stack of non-linear transformations ending up with something potentially very complicated.
","['generative-adversarial-networks', 'generative-model']",
Should I include overlapping (input) Data in my training data,"
If I have time dependent data and want to predict the relative change for a future time. Should I separate the data so that the input times don't overlap?
With an example:
I have hourly temperature readings and want to predict the temperature change 4 hours after the last input hour. Now I could split my data so that I have 
(overlapping)
1st data point: 1h, 2h, 3h, ... 10h -> predict for 14h
2nd data point: 2h, 3h, 4h ... 11h -> predict for 15h
(not overlapping)
1st data point: 1h, 2h, 3h, ... 10h -> predict for 14h
2nd data point: 15h, 16h, 17h ... 25h -> predict for 29h\
Further questions:
Does it depend on the Model/Training algorithm.
Does it depend on the Data? 
Does it depend on the number of data points I have available? (I have hourly data for 6 years)
","['neural-networks', 'datasets', 'training-datasets']","In general, both methods are valid to train temporal models. The only thing you need to check is that validation and test-set don't overlap with any of your training samples.Using the overlapping variant can sometimes increase performance, because your model might benefit from the increased number of different starting points. This can be very relevant for certain datasets. For example, traffic data has periodic characteristics (little traffic at nighttimes / high traffic at daytimes). Let's say you have hourly traffic data and a sequence length of 24. Now your sequences will always start at the exact same day time. Therefore the model might not learn to properly forecast from other starting times.I cannot think of any drawback the overlapping variant might have, so I'd opt for the overlapping variant. To be sure which one works best for you, I think you have to find out empirically.Does it depend on the Model/Training algorithm.In general it shouldn't, after all, in both cases you are always feeding the model valid sequences - any model and any training algorithm should deal well with either one.Does it depend on the Data?Yes, if the sequences have distinct starting points the non-overlapping variant might be better suited. The traffic-data example from above applies here.Does it depend on the number of data points I have available? (I have hourly data for 6 years)I'd say yes, you effectively increase the number of data points by using overlapping sequences, this is good if your dataset is small. Your dataset is comparably large."
How to represent the state and action space when modeling card game Love Letter using Markov Decision Process,"
Now my task is to use model checking to find a best strategy for card game Love Letter, and I need to model the game using Markov Decison Process first.
I have done a lot of resaerch and I decided to use ""what cards are in the deck and what cards are in the other player's hand"" to be my state.
But I am not sure which will be the best way to represent the states, because I think the states should include all the information I need and I can make action base on these states that I have got.
","['markov-decision-process', 'card-games']",
Negamax: how should you avoid the horizon effect in the connect four game?,"
I'm trying to implement a quiescence search in the negamax algorithm, for a connect four game.
The algorithm is as follow for a chess game:
int Quiesce( int alpha, int beta ) {
    int stand_pat = Evaluate();
    if( stand_pat >= beta )
        return beta;
    if( alpha < stand_pat )
        alpha = stand_pat;

    until( every_capture_has_been_examined )  {
        MakeCapture();
        score = -Quiesce( -beta, -alpha );
        TakeBackMove();

        if( score >= beta )
            return beta;
        if( score > alpha )
           alpha = score;
    }
    return alpha;
}

I get the idea, but unfortunately there is not much more details in the article. I don't get how the sentence ""until( every_capture_has_been_examined )"" could be transposed to the connect four game. How would one evaluate silent move in such a game?
Also, there is no depth parameter, does that mean that quiescent search only applies to a single depth? As far as I understand, it seems so.
Here is an example output of my connect four AI game, where the horizon effect occurs (if I understand correctly):

AI player is YELLOW
Depth is 1 (obviously)
AI player wrongly chose to play c5 in the -300 cell, considering letters a,b,c, ... for the y axis. Thus, AI adds a third connected chess men and improves his is score (c3 to c5)
However, AI doesn't see that by doing so, it gives a winning move to the RED player. Indeed, RED now sets a connect-four in the line just below (d3-d6, playing d6) and wins the game.



Please note that MIN is actually MAX, because I use negamax and not minimax.

","['connect-four', 'quiescence-search', 'negamax']",
What should be taken as random variables in the distributions of datasets?,"
Consider the following two paragraphs taken from the paper titles Generative Adversarial Nets by Ian J. Goodfellow et.al
#1: Abstract

We propose a new framework for estimating generative models via an
adversarial process, in which we simultaneously train two models: a
generative model G that captures the data distribution, and a
discriminative model D that estimates the probability that a sample
came from the training data rather than G. The training procedure for
G is to maximize the probability of D making a mistake. This framework
corresponds to a minimax two-player game. In the space of arbitrary
functions G and D, a unique solution exists, with G recovering the
training data distribution and D equal to 1.......

#2: Excerpt from Introduction

The promise of deep learning is to discover rich, hierarchical models
that represent probability distributions over the kinds of data
encountered in artificial intelligence applications, such as natural
images, audio waveforms containing speech, and symbols in natural
language corpora.

here we saw the word distribution thrice. It is very common to encounter the phrase data distribution in machine learning papers. For the types of data we use in artificial intelligence, there can be infinite random samples. We collect some instances and form a dataset, based on which we try to get the data distribution.
Even most of the literature uses the phrases data distribution or probability distribution. I personally never came across papers that explicitly talk about the random vector for the probability distribution under consideration. What can be the reason for it? Why won't research papers give the list of random variables for the distribution they are discussing? Is it immaterial or is it obvious from the context?

Note: For example, if our discussion is about a 'human face images' dataset and if we use the phrase 'data distribution' then should I assume random variables to be 'pixels' of the image or high-level features like 'eyes, ears, etc.,' or some other? Or is it immaterial? If immaterial, then what should I need to perceive about probability distribution? Should I imagine it as an arbitrary random vector?
","['terminology', 'datasets', 'probability-distribution', 'random-variable']",
What is the distribution of autoencoder embeddings?,"
Is there any result on the distribution of autoencoder embeddings?
For example, the following image (taken from this article) visualizes the latent space with t-SNE. As you can see, images from the same class (in this case MNIST digits) roughly form a cluster. Is there any theory explaining why that is the case?

Note that I am talking about autoencoders, not variational autoencoders, whose embedding by definition has a multi-variate Gaussian distribution.
","['neural-networks', 'autoencoders', 'generative-model', 'probability-distribution', 'embeddings']",
Is data augmentation inducing bias?,"
I am using Keras to build a CNN model to classify spectograms and using the following layers:
Conv2D 32 filters, size (5,5), relu 
MaxPooling2D (2,2)  

Conv2D 32 filters, size (5,5), relu
MaxPooling2D (2,2)  

Conv2D 64 filters, size (5,5), relu
MaxPooling2D (2,2)      
                                                                  
Flatten         
Dense 64 , relu                                                        
Dense 32 , relu                                                     
Dense 5 (softmax)

With this model I was able to comfortably achieve more than 97% training accuracy and around 84% validation accuracy. Unfortunately, I have very few data samples (slightly more than 1200), which is my main suspicion on why the model is overfitting.
As I am not able to get more data samples, I opted to use SpecAugmentation. I used to get all my classes (5) to 1000 samples each, meaning I generated around 3800 samples.
After I fit the model to this augmented data I got more than 97% validation data, which I found very strange. I am thinking that I am introducing bias because I am performing the data augmentation before splitting into training and validation sets, which may lead to having the same sample in both sets (obviously they are not 100% equal but one is derived from the other).
Should I only be performing data augmentation after splitting? If I use data augmentation only on the training set will I not be overfitting to the training set as well, as it will see very similar samples several times?
What other techniques would you recommend for data augmentation besides SpecAugmentation?
Thanks
","['convolutional-neural-networks', 'data-augmentation']",
Should I spend money on a machine-learning capable PC or just use Google CoLab? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed last year.







                        Improve this question
                    



Assuming I have internet access, should I spend money on a PC or just use Google Colab?
I'll be doing deep-learning training.
Google CoLab: https://colab.research.google.com/
","['deep-learning', 'training', 'gpu']",
Is batch size of 1 a valid choice for a very deep neural network with high memory requirement?,"
I am training a very deep neural network (Panoptic-DeepLab) with a ResNet34 backbone on Google Colab on CityScapes dataset for Panoptic Segmentation, and noticed that, with a big crop size, the batch size has to be decreased to 1 image per batch, otherwise CUDA out of memory issues start to occur. While I know that this can create skewness in the training and it will likely be very hard to attain good convergence, can I ask this question in general to the experts: how valid is a batch size of 1 generally considered in image-based processing? The images in consideration can be considered large (high resolution). The optimizer used is Adam alongwith a warm up polynomial learning rate (with base around 0.00005), and 90k iterations.
(I understand that it would possibly be a good idea to try out a smaller crop size and bigger batch size, but would like to know the feedback from the community anyway)
","['deep-learning', 'training', 'image-processing', 'image-segmentation', 'memory']","After more research, I found that a batch size of 1 is quite common in deep-learning image processing use-cases where there are high memory/GPU requirements for model training. In fact, it gives better results at times."
How to find AI research papers where authors provide source codes that will reproduce the experiments described in the paper?,"
It is important for a research paper to include raw data and code for scientific replicability, verifiability, and falsifiability.
However, as noted by nbro, roughly 5% percent of papers have code that precisely reproduce the experimental results. About 20% of papers might have implementations coded by other researchers, but they usually differ from the original code and implementation of the author.
Is it possible to find AI research papers where the author provide source codes aimed to reproduce the experiments described in the paper and produce exactly the same plots?
","['research', 'implementation']",
What Kind of Reinforcement Learning Algorithms Can Be Used when the Action Space is Unfeasibly Large?,"
I know Deep Q network as a $S\times A$ DNN which maps the $S$ dimensional statespace to q-values of $A$ distinct actions.
In my problem, the action space is still discrete, and finite, but depending on some parameters (e.g. number of users in a system) can grow exponentially large. More concretely, if I have $n$ users, then there are about $3^n$ actions. Clearly, the vanilla DQN is infeasible for this problem.
So I wonder if there are any reference algorithms (best if they come with mature library supports for production code) catering to the scenarios where it is infeasible to calculate individual Q values for each action.
","['deep-rl', 'dqn', 'continuous-action-spaces', 'double-dqn']",
Autoencoders: Where does the encoder end and the decoder begin?,"
Consider a simple Autoencoder neural net:
from torch import nn

class AE(nn.Module):
    def __init__(self, x_dim, z_dim, h_dim=42):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(dim_in, dim_h),
            nn.ReLU(),
            nn.Linear(dim_h, z_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(z_dim, h_dim),
            nn.ReLU(),
            nn.Linear(h_dim, x_dim),
        )

    def forward(self, x):
        z = self.encoder(x)
        x = self.decoder(z)
        return x

In popular literature, it is generally implied that the output of AE.encoder is solely responsible for encoding whereas AE.decoder is solely responsible for the decoding.
Why is that?
If we consider that encoding is a more complex task than decoding, there is no actual guarantee that the network won't use the first three layers for encoding and only the last for decoding (or vice versa). This might especially be the case if we consider asymmetrical autoencoder architectures.
","['neural-networks', 'machine-learning', 'definitions', 'autoencoders']",
What is the logic behind using a trained classifier's gradients to synthesize controllable image?,"
In the controllable image synthesis, we are manipulating a noise vector z such that our generator ( in our GAN model ) creates images that the desired feature exists. For instance, take the feature of ""smiling"". In order to learn how to change the z (noise) vector , the technique is described as following :

Train a classifier ( besides the GAN model, just a single discriminator neural networks as a classifier) that classifies an face image according whether there is a smiling person on the image or not.

Give the image generated by the GAN model with the initial noise ( say z1) to the classifier.

Compute gradients of the score with respect to z1. And finally Perform gradient ascent on z1 vector in the direction of improving the score.


I cannot understand how to gradients are used to change the z1 vector in  a way that it now leads the GAN model to create ""smiling"" faces, could you please help ?
","['neural-networks', 'generative-adversarial-networks', 'image-generation']",
Why cannot linear activation functions be used to approximate any function?,"
In neural networks we use nonlinear activation functions such as sigmoid, ReLU, etc. Using a combination of these functions (with required scaling and shifting), we manage to estimate any nonlinear function.
I understand in theoretically that we cannot obtain a function such as ex1  + x22 ...(and other such nonlinear components of inputs x3, x4, etc.) using only linear combinations of xi's. However, when I am thinking graphically I think that it is possible to approximate these nonlinear functions using lots of linear (scaled and shifted) linear lines and I do not understand why this is incorrect.
In case, if it is possible to approximate any function using a ReLU activation function (which is linear in the first quadrant), why is it not possible to approximate with a function completely linear?
","['neural-networks', 'machine-learning', 'activation-functions', 'relu']","However I when thinking graphically I think that it is possible to approximate these nonlinear functions using lots of linear ( scaled and shifted) linear lines and I do not understand why this is incorrect.It's not generally incorrect, but what you are describing there is not a single function but a piecewise function. For example:$
f(x) = \left\{
        \begin{array}{ll}
            0.1 x & \quad 0 \leq x < 0.1 \\
            0.2 x & \quad 0.1 \leq x < 0.2\\
            0.3 x & \quad ...
        \end{array}
    \right.
$This composition is non-linear, because the input space gets transformed differently depending on $x$ (because each subfunction in the example above has a different slope). There are methods to optimize such a function as well, e.g. piecewise linear regression.An MLP without non-linearities is not a piecewise function, but a function composition:$(f \circ g)(x) = f(g(x))$,where $g$ and $f$ are two consecutive dense layers.
The difference is that if both functions $g$ and $f$ are linear, then $f \circ g$ is linear. This holds for any configuration of model parameters. Therefore, the model transforms any given input space $X$ into some output space $W$, where straight lines in the input space remain straight in the output space and can thus never model something non-linear.In case, if it is possible to approximate any function using Relu activation function ( which is linear on first quadrant) , why it is not possible to approximate with a function completely linear ?As soon as you introduce any non-linearity, the system becomes much more expressive, because each neuron now models a non-linear function. Combining the neurons (as done by a subsequent layer) can learn combinations of these non-linear functions.And this is where the two worlds collide: If you take a ReLU activation, it allows the model to actually learn something like a piecewise linear function, because of the combinations of several linear functions with different slopes that are $< 0$ only for certain input ranges. Here is a simple example of that:The left image shows what the model has learned (green line), the right side shows the functions of the individual neurons. The more neurons you add to that first relu-layer, the better the approximation of the ground truth will be."
Image classifier model which predicts objects and it's relevant areas with a combination of words,"
I have experience with image classification models such as CNN and Vision Transformers but this time I want to try a new thing (For me).
First please check the below image to understand what I want

In this picture I want to feed an image along with it's label (which is a string) and then I want to predict same string from my model.
In some cases there can be multiple strings with one image as we call multi class problem.
First I want to know if it's possible? Second which state of art CNN/Transformer model will be good for this model, Third has anyone worked on this before?
","['deep-learning', 'computer-vision', 'algorithm-request', 'model-request']","The task name is ""Image captioning"". That is the keyword for paper searching. Dataset, evaluation methods, and the state-of-the-art are all aggregated on the forum: paperwithcode."
Why is the time complexity of the Triplet Loss $O(N^3)$,"
The triplet loss function uses an anchor, positive, and negative examples. If $N$ are the number of examples in the training set with $C$ classes, then I think that the time complexity should be $O(NN_cN_{c'})$ not $O(N^3)$ from Probabilistic Machine Learning, Murphy (2022)

Naively minimzing the triplet loss takes $O(N^3)$ time

Because for every example in the training set, we have $N_c$ possible positive examples in a class $C=c$ and $N_{c'}$ possible negative examples not in class $C=c$
(I've seen other research papers used that however with a modification in the denominator: $\mathcal{O}(N^3/C)$, where $C$ is the number of classes)
","['computational-complexity', 'time-complexity', 'triplet-loss-function']","For each anchor data point $x_i^a$ in class $j$, the intra-distance should be computed $g_j$ times, where $g_j$ is the sample size of that class and the inter-distance should be computed as $N$ times, where $N$ is the sample size of the dataset. In other words, for each data point, the triplet loss computes all possible distance between the intra-distance and the inter-distance, and the computation cost for $x_i^a$ should be $g_i \cdot N$.For the dataset, the total computation cost is  $\Sigma_j^c (g_j^2 \cdot N)$.$$
\begin{align*} O(*) &= \Sigma_j^c (g_j^2 \cdot N) \\      &= N \cdot \Sigma_j^c g_j^2 \\      \end{align*}
$$According to the arithmetic mean and quadratic mean inequality[28], $\frac{\Sigma_i^n x_i}{n} \leq \sqrt{\frac{\Sigma_i^n x_i^2}{n}}$, when $x_1 = x_2 = x_3 ... = x_n$ the arithemtic mean is equal to the quadratic mean.Therefore,$$
\begin{align*}N \cdot (\frac{\Sigma_j^c g_j}{c})^2 \cdot c &\leq O(*) < N \cdot \Sigma_j^c N^2 \\N \cdot \frac{N^2}{c} &\leq O(*) < c \cdot N^3 \\ \frac{N^3}{c} &\leq O(*) < c \cdot N^3\\\end{align*}
$$The lower boundary of $O(*)$ is consistent with the paper result."
Learned kernels in CNN seem just random patterns,"
I am training a classification neural network using Tensorflow2 (specifications below). The training goes well (good accuracy and no overfitting, apparently). During the training I monitor the learned kernels in the first convolutional layer. I'm expecting to see some patterns like edges or corners in the learned kernels, like here. However, my trained kernels look pretty random. Check the following examples after 30000 steps (1 step = 1 batch of 256 images):

Moreover, if I plot a specific kernel at different steps, the relation between different pixels does not seem to change much. Instead, it just seems do be scaled:

Although the performance of the model seems good, this makes me think that there could be some overfitting in the training. What could be the explanation for this?
Network specifications:

2 classes (class 1: 200K samples, class 2: 64K samples)
Adam optimizer
Architecture like googlenet but with just 3 inception blocks
L2 regularization (0.001) and dropout (0.2)
Cross entropy loss, using sample weights to account for unbalanced dataset

","['convolutional-neural-networks', 'tensorflow', 'filters', 'adam']",
How do neural networks learn specific features throughout the layers?,"
I was reading about convolutional neural networks and I came across such an explanation:

Consider detecting features in human face. The earlier layers of neural networks learn coarse features such as edges in the images and the latent layers learn more complex ( finer) features such as eyes, nose and etc

I am wondering why this is a true statement, namely how can we know that a neural network first starts by learning primitive features and then learns complex features. Could you please explain?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'representation-learning']","The reason is that a layer can only learn a function based on it's inputs - that's clear. When you look at a convolutional neural network, each output pixel of an early layer only gets a very small fraction of the whole image as input. For example: If your first layer uses a 3 x 3 kernel, then each output only depends on only 9 pixels. Try detecting anything other then edges with that much information - it's not not possible. In other words: Looking at a 3 by 3 image you will be able to see an edge and its orientation, but not something like a nose.In contrast, the later layers depend on a much larger portion of the image and can thus learn high-level features like a nose.It's more a question of whats possible for the layers to learn and not a question of the network self-organizing it's resources.
And besides just reasoning like that you can visualize what the kernels in the individual layers respond to. I found this question that also has an image with some further information on that.EDIT In response to the first comment:To further clarify how a CNN works, here is an example, where $X_0$ is the input image and $X_1$ is the output of the first CNN-Layer:is it like the many basic functions (like the edge detections ) are added together and then create a function that fires high values when a ""nose"" detected ?So yes, you can say it like this. This is exactly where the thought of hierarchy fits in."
Can I create and train a feedforward layer with only K active connections?,"
I had an idea for a layer, but I'm not sure if it exists already and/or if it's implementable in tensorflow.
I would like to have a layer that is similar to a Dense layer, in the way that it's connected to every nodes of the previous layer, but only K connections can be ""active"" (weights >= 0), and all the other ones will have their weights equals to 0
Which ones of those K connections will be learned by the network, not chosen randomly when building it.
So for example, in a Dense layer that goes from 1000 to 2 features, I would have 10002 + 2 = 2002 parameters that can be different from 0.
In my layer, if I chose K = 10, I will have only 210+2 = 22 parameters that can be different from 0.
Any way I can do that and train a network with it?
","['deep-learning', 'tensorflow', 'dense-layers']",
What is the best approach/model for classifying document images with over 70+ classes of documents?,"
What is the best approach/model for classifying document images with over 70+ classes of documents? I have tried LayoutLM which is a model that incorporated both NLP and Computer vision to classify but it's accuracy is not upto the mark. Are there any other models/methods I could try to come up with a better solution?
","['machine-learning', 'natural-language-processing', 'algorithm-request']",
Is it common that an applied deep learning research paper does not disclose any raw data and source code? [duplicate],"







This question already has answers here:
                                
                            




Why do most deep learning papers not include an implementation?

                                (3 answers)
                            

Closed last year.



I think it is important for a research paper to include raw data and code for scientific replicability, verifiability, and falsifiability. However, recently, most of the research paper I read does not give raw data or source codes.
Those papers are from: European Journal of Operation Research; IEEE; International Conference on Cloud Computing and Big Data; MDPI
Some of them are highly reputable outlets.
Is this common that an applied CNN and DRL research does not disclose the code and data they used? Are there any journal with a data/code policy on that publishes applied research?
","['deep-learning', 'convolutional-neural-networks', 'research']","Many papers provide links to code, typically on GitHub.com. I have often found that even if the authors don't give a link, I can search the authors' names on GitHub and find a repository for the paper. My wild guess is that about 50% of papers provide reproducible code.Papers With Code is a good website for finding papers with code."
Where is memory stored in a chatbot like LaMDA?,"
I have a basic understanding of how neural networks work, and I have always thought that those chatbots work in a similar way (but I might be wrong): they take an input, shape it in a way that can be processed, go through a series of layers in a neural network model (with weights based on their previous training), and then give an output.
In the published conversation with Blake Lemoine, the LaMDA chatbot makes a reference to a “previous conversation”: is this memory somehow stored in the weights of the neurons, or is there a physical memory from which it can read?
What goes out of my understanding in this particular situation, is how the notion of a ""previous conversation"" can be processed by a similar system: is this memory somehow stored in the neurons of the neural network (in this case the training would continue also during the conversation), or do they have some kind of memory storage that they can access during the evaluation of their inputs?
Here is the article where I've taken the link to the conversation: https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/
","['neural-networks', 'machine-learning', 'long-short-term-memory', 'chat-bots', 'google']","What goes out of my understanding in this particular situation, is how the notion of a ""previous conversation"" can be processed by a similar system: is this memory somehow stored in the neurons of the neural networkAlthough I cannot answer what specific ""memory"" LaMDA may have (but I suspect it is either nonexistent or very simple), I think I can address this point.The AI likely has no ""notion"" of anything here. It has access to a very large and complex statistical language model, where conversations that allude to prior conversations may happen from time to time. It is therefore capable of outputting text with similar-looking references, adjusting for the current  context in the flow of conversation.If you asked the AI how much it enjoyed the summer of 2015, depending on what previous prompts (and any pre-prompt warm up*) were, it would likely make a good attempt at ""remembering"" what it did then, even though it did not exist. It would draw statistically on other texts related to that time, and might invent places, refer to real events that it may of attended etc. A lot will depend on how much pre-prompting is applied that forces the AI to answer questions logically ""knowing"" that it's an AI, but that pre-prompting can often fail later in a conversation.Typically a chatbot built using one of the current crop of cutting-edge language models will fail to be self-consistent and coherent in the long term unless the person interacting with it is helping it along or willing to ignore the failures. With older chatbots this could happen very quickly. With GPT-3 and probably LaMDA, it may take a slightly longer conversation.I find it interesting that the journalist in your linked story managed to get a complete opposite of the Engineer's experience:In early June, Lemoine invited me over to talk to LaMDA. The first attempt sputtered out in the kind of mechanized responses you would expect from Siri or Alexa.“Do you ever think of yourself as a person?” I asked.“No, I don’t think of myself as a person,” LaMDA said. “I think of myself as an AI-powered dialog agent.”Afterward, Lemoine said LaMDA had been telling me what I wanted to hear. “You never treated it like a person,” he said, “So it thought you wanted it to be a robot.”The above simple exchange is pretty good proof that the engineer is suffering from confirmation bias, IMO.Related: The Eliza Effect.* A pre-prompt warm up is a piece of text that ""frames"" the start of interaction with a large language model so that it can perform a specific task. At the start of any conversation, a paragraph of context is fed into the language model to set the statistics for a certain kind of document. Open AI's GPT-3 uses this a lot to support different tasks.A simple pre-prompt might look like this:The following is a conversation between a human and AI. The AI is knowledgable, friendly and helpful.AI: Hi, I am AI! How can I help you today?Human:At that point control will pass to the human - outside of the AI - so they will add their first sentence that will become part of the growing prompt history. Note that the AI, if left to its own devices would begin to speak for the human too, naively continuing to predict the conversation unitil its end! In fact it does often do so, but the chatbot code is set to catch obvious end points such as newlines, or the AI actually outputting ""Human: "", and will force the mode of conversation back to looking like a chat. It is really quite crude.LaMDA's architecture may not do exactly the same as GPT-3, in fact I would suspect it is different in some key ways. However, it is very likely that there are still two ""layers"" in play - a very large sophisticated language model trained on gigabytes of human text, and a control layer which makes the language model conform to a chat scenario. Even if the control layer is more sophisticated than GPT-3's pre-prompting, it will have similar limitations."
Concise and mathematically-oriented book on AI and neural networks suitable as a gift [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 11 months ago.







                        Improve this question
                    



I would like to buy a book about AI and neural networks written on accessible level for a 17 years old mathematically very gifted student interested in these topics. The book should contain some sections about perceptrons and optical character recognition. I am aware of https://www.deeplearningbook.org/ but it does not fully satisfy me. Mostly, because it goes too slow and too long. For instance, in order to grasp the back-propagation algorithm one needs to read 60 pages! Eqs.(6.49-6.52) are particular shocking, I thought every student should know rules of chain differentiation! We do not write such trivial things in, e.g., theoretical physics.
Now, Internet is full of all possible blogs and tutorials, but it is impossible to filter some nice and concise exposition for people with mathematical background. I notice some popular extremes such as i) prolonged discussion of a single neuron, ii) XOR example, iii) very technical tutorials, which require a lot of python, a lot of packages, web-servers etc.
I am seeking some nice text to create a neural network completely from scratch, and with some impressive performance for e.g. written digits recognition. No reliance on external packages, no object-oriented features, as it may deter young students. But functional programming paradigm is welcome.
Therefore, I was thinking about more refined and concise books, preferably with good typography and illustrations, hard cover, suitable as a gift for a mathematically-inclined student. What would be your recommendation?
","['multilayer-perceptrons', 'books', 'optical-character-recognition']","After a lot of searching the following seems to be a good choice, but sometimes with repetitions of material. The math is very accessible.Neural Networks and Deep Learning: A Textbook by Charu C. Aggarwal"
How to define a loss function for multi-label problem?,"
I have voice recordings which are labelled by not only a single label but multiple labels. Each voice recording corresponds to one of class labels within a set. In other words, the training instance is given a set of (or distribution over) candidate class labels and only one of the candidate labels is the correct one.
I wish to train a model that classifies which class label corresponds to each voice recording. Each one of my voice recordings is accompanied by a set of 10 potential labels (labels are always different), but it is unknown which label it is exactly (aside from a small sample where there is only one correct label).
This is due to the nature of where my data comes from: someone records a short voice message and then types the same message into a chat, however there will be slight delay between the two and in the meanwhile other chat messages arrive. Only one of the next 10 chat messages after the voice message is the correct one that corresponds to that voice message.
How would I define a loss function in this case?
","['recurrent-neural-networks', 'objective-functions', 'audio-processing', 'multi-label-classification', 'voice-recognition']","Given your response in the comments, you are faced with a semi-supervised learning problem where you have a small set of data with ground truth labels, and a large set of data without ground truth labels.If you look at a similar problem, predicting ImageNet label with only 10% of the ground truth labels, you can see that the best methods reach ~80% accuracy.You can try to reproduce the approach of Big Self-Supervised Models Are Strong Semi-Supervised Learners, which achieves ~74% accuracy on ImageNet with only 1% of the labels.Namely:I've simplified a lot the method described in the paper, to give you an idea of the general approach. Read the paper for details!"
Is regularization in machine learning and deep learning same [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



As there are L1 , L2 , etc out and other technique are those all same for machine learning and deep learning while using Ml algorithm and DL algorithm
","['machine-learning', 'deep-learning', 'computer-vision']",
What is the correct interpretation of the discount factor in MDPs?,"
In infinite-horizon MDPs one can consider the expected discounted return from a distribution of start states as the objective[^1]. i.e. $\mathbb{E}[V^{\pi}(S_0)] = \mathbb{E}[G_0] = \mathbb{E}[\sum_{t=0}^\infty \gamma^t R_{t+1}]$ where the expectation is over start states $S_0$, following states taken from the transition dynamics of the MDP, and actions from a policy $\pi$.
In this setting, discounting is interpreted as (1) giving less importance to future rewards, or (2) having a $1-\gamma$ probability of being truncated forever at every step as mentioned in this answer. The answer claims that the or is exclusive and that ""choosing one or the other means tackling a different problem"". That (1) is implemented by adding $\gamma$ to the return, while (2) by simulating the truncation.
I believe this not to be true and that it does not make sense to include $\gamma$ in the  return (thus the state and action value functions implicitly as well) without interpreting it as a probability of being truncated at the same time (i.e. part of the MDP. Interpretation 2 in the answer).
In the context/question where the answer is taken from, if one derives the policy gradient with respect to the discounted return as the objective, then the discounted state distribution arises (the one corresponding to interpretation 2), not the original one. Proof here.
Hence, using $\hat{\nabla J_1} = G_t \nabla\log\pi(A_t | S_t)$ where $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$ is a biased estimator for the policy gradient when ""giving less importance to future rewards"". $\hat{\nabla J_2} = \gamma^tG_t \nabla\log\pi(A_t | S_t)$ would be the correct one.
It's as if one implicitly changes the MDP by discounting (or equivalently that $\gamma$ is part of the definition of the MDP not only the objective).
Does this make sense?
[^1]: Another objective is the average reward.
","['reinforcement-learning', 'markov-decision-process', 'policy-gradients', 'reinforce', 'discount-factor']",
"What does ""All store and access operations (for S(t) , A(t), and R(t)) can take their index mod n + 1"" mean?","
It's from the book Introduction to Reinforcement Learning. Second edition, chapter7: n-step Bootstrapping, page 147, n-step Sarsa.
I made the algo work, but I still don't understand the phrase.
Preferably explained in Python terms. The introductory part for the algo:

Initialize Q(s, a) arbitrarily, for all s $\in$ S, a $\in$ A


Initialize $\pi$ to be $\epsilon$-greedy with respect to Q, or to a fixed given policy


Algorithm parameters: step size $\alpha$ $\in$ (0, 1], small  $\epsilon$ > 0, a positive integer n


All store and access operations (for S(t), A(t), and R(t)) can take their index mod n + 1

","['reinforcement-learning', 'machine-learning', 'sutton-barto']",
Is there a literature on the time complexity of Neural Networks?,"
There exist various blog posts describing the time complexity of Fully Connected Neural Networks (1, 2, 3, 4); Convolutional Neural Networks (CNN) (5) and of Long Short-Term Memory (LSTM) networks (6). However, this question seems to be absent in the academic literature or at least I couldn't find it. Furthermore, papers that go towards this question seem to consider mostly experimental results (7, 8).
Therefore, my question is:

Is there a literature on the time complexity of Neural Networks that gives analytic expressions in the fashion of $\mathcal O(\cdot)$?
If this is not the case this is because...

...(1-6) is common sense?
...it is not that easy to answer and the blog posts only present a simplified form of the problem?
...something different?



Although I consider the posts mentioned above very useful, there are various reasons why one would prefer to cite a paper rather than a website (10)!
","['neural-networks', 'reference-request', 'academia', 'computational-complexity']",
How is it possible to detect anomalies in batches of 2 minutes of web access logs?,"
I have data coming from web access logs in the following form:
@timestamp  ISP cache_result    client_ip   client_request_host client_request_method   client_ua   client_url  client_user content_type    ... http_response_code  major   os  os_name querystring reply_length_bytes  ts_process_time ts_timestamp    type    ua_name
2018-04-17T08:12:32.000Z    cuaerH c rt,nlEIrnii.cec    TCP_REFRESH_MISS    25.204.184.124  testhost.net    GET Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...   /wp-content/themes/Avada/includes/lib/assets/m...   -   application/javascript  ... 200 65.0    Windows 10  Windows 10  ?ver=2.2.3  25204   321 17/Apr/2018:08:12:32 -0000  testdata    Chrome
2018-04-17T08:12:32.000Z    HeE iclirueIc rat,nrncc.    TCP_REFRESH_MISS    8.157.89.174    testhost.net    GET Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...   /wp-content/plugins/fusion-core/js/min/avada-p...   -   application/javascript  ... 200 65.0    Windows 10  Windows 10  ?ver=1  2825    177 17/Apr/2018:08:12:32 -0000  testdata    Chrome
2018-04-17T08:12:33.000Z    ,rrnI EnH.ceeiuclcicrat TCP_REFRESH_MISS    37.151.22.36    testhost.net    GET Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...   /wp-content/themes/Avada/includes/lib/assets/m...   -   application/javascript  ... 200 65.0    Windows 10  Windows 10  ?ver=1  267 275 17/Apr/2018:08:12:33 -0000  testdata    Chrome
2018-04-17T08:12:34.000Z    tn.cHer uE,lecnir aircIc    TCP_REFRESH_MISS    202.165.110.43  testhost.net    GET Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...   /wp-content/themes/Avada/includes/lib/assets/m...   -   application/javascript  ... 200 65.0    Windows 10  Windows 10  ?ver=1  341 172 17/Apr/2018:08:12:34 -0000  testdata    Chrome
2018-04-17T08:12:34.000Z    rneecHuraci ctInir cl.,E    TCP_REFRESH_MISS    174.201.44.32   testhost.net    GET Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...   /wp-content/plugins/fusion-builder/assets/js/m...   -   application/javascript  ... 200 65.0    Windows 10  Windows 10  ?ver=1  302 180 17/Apr/2018:08:12:34 -0000  testdata    Chrome

What I need is to come up with a model that takes as input such kind of data in batches of 2 minutes data and identify if there exists IPs that are anomalous. So, I am thinking to group the training data by IP and compute some features.
But my question is - how is this possible with batches of 2 minutes, since the anomaly detection needs all the data at once to be able to train it?
","['machine-learning', 'algorithm-request', 'model-request', 'anomaly-detection']",
How to convert a known function to a neural network,"
Suppose I have a known function, for example:
def func(x):
    m = np.mean(x)
    if m > 1: 
       return 1
    else:
       return 0

Are there any algorithms to convert this into a neural network?
A naive way would be:

use func to generate training set
train the NN using the training set

But this will be slow and the trained NN will probably be inaccurate. Are there better ways to ""copy"" a known function?
As the questions suggests, I am not asking about the about trivial example function, I am asking about a general known function!
","['neural-networks', 'machine-learning']",
"Is my derivation of the Bellman equation for $q_{\pi}$ in terms of $p(s'|s,a)$ and $r(s,a)$ correct?","
I have done exercise 3.29 from Sutton and Barto and I'd like to check if it's correct. Here's the exercise:

Rewrite the Bellman equation for the function $q_{\pi}$ in terms of the three argument function $p$  (3.4) and the two-argument function $r$  (3.5).

Here are the 2 functions:
(3.4) :
$$p(s'|s,a) = P(S_t=s'|S_{t-1}=s,A_{t-1}=a)=\sum_{r\in\mathcal{R}}p(s',r|s,a)$$
3.5:
$$r(s,a) = E[R_t|S_{t-1}=s,A_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)$$
Here's my solution. Is this derivation correct?
$$q_\pi(s,a)  = \\ E_{\pi}[G_{t+1}|S_t=s,A_t=a]= E_{\pi}[R_{t+1} + \gamma G_{t+1} |S_t=s,A_t=a]= \\ E_[R_{t+1}|S_t=s,A_t=a]+\gamma E_{\pi}[G_{t+1}|S_t=s,A_t=a] = \\ \sum_{s',r}p(s',r|s,a)r+\gamma \sum_{s'}p(s'|s,a)v_{\pi}(s') = \\ r(s,a) + \gamma \sum_{s'} p(s'|s,a) \sum_{a'}q(a'|s')\pi(a'|s')$$
","['reinforcement-learning', 'value-functions', 'sutton-barto', 'bellman-equations']","Your work all looks correct to me. But I don't have the exercise 3.29 in my book, so I can't read the question for myself. It's not clear to me whether your last line or second to last line should be considered as the answer."
Policy and Discount Factor,"
This question is similar to this question, however it has a different question.
I'm learning MDP's and I'd like to know if I'm doing these exercises correctly:
Consider the following MDP:

Suppose a behavioural scientist was doing an experiment where they gave rewards to an animal. Suppose $X=5$ (e.g., 5 food pellets at that location). It turns out that, after repeatedly exploring the grid, the animal seems to prefer going up to the rewards of $+2$.
Question 1: In this setting, prove that no scalar discount $\gamma\in[0,1]$ exists for which the optimal policy is to go to $+2$.
Question 2: Consider a Monte Carlo return $$G_t=R_{t+1}+f(R_{t+1}+f(R_{t+2}+f(...))).$$
Standard discounting can be seen as applying a linear transformation $f(x) = \gamma x$, by multiplying the remaining return after each step by a factor $\gamma$. Consider the following alternative where instead of multiplying with a factor $\gamma$, we raise the value to power: $f(x)=x^{\gamma}$. Does this mathematical model better explain
the observed behaviour, in the sense that a $γ$ exists for which the optimal policy
goes to $+2$? If so, give such a value for $γ$. If not, prove why not.
My answer to question 1: For the optimal policy to go to $2$, we need the return for going to $+2$ to be greater than both the return of going to $+1$ and $+5$, i.e., mathematically
\begin{align}
2\gamma > \gamma^25 \cap 2\gamma>1 \\
\frac{2}{5} > \gamma \cap \gamma > \frac{1}{2}
\end{align}
Since $(\frac{1}{2},\infty) \cap (-\infty, \frac{2}{5}) = \emptyset$, this means that there is no such $\gamma$ for which the optimal policy is $+2$.
Answer to question 2:
Again the return of $+2$ should be both greater than $+1$ and $+5$:
\begin{align}
2^{\gamma} > (5^{\gamma})^{\gamma} \cap 2^{\gamma}>1 \\
\gamma \ln 2 > \gamma^2 \ln 5 \cap \gamma > 0 \\
\frac{\ln 2}{\ln 5} > \gamma \cap \gamma > 0 \\
0.43 > \gamma \cap \gamma > 0
\end{align}
Hence for $0< \gamma < 0.43$, the optimal policy will be to go to $+2$. Is this correct?
","['reinforcement-learning', 'value-functions', 'policies', 'discount-factor']",
"If a policy is epsilon-greedy, is it technically stochastic?","
Even though if exploration doesn't happen, it's deterministic.
","['reinforcement-learning', 'definitions', 'policies', 'epsilon-greedy-policy']",
Text rewriting with learning of writing style,"
I do a lot of scientific writing in my job. Unfortunately, I often also have to rewrite amateurish text that some students write for manuscripts. They use tools like grammarly to fix grammar issues, but even after that, I find myself doing a lot of rewriting as grammarly does not catch all grammar problems, even introducing some in many cases. My postdocs are of limited use in this respect as their writing styles are not that great either. Importantly, I like the introduction and discussion/conclusions written in a particular way that maximizes their impact in terms of the arguments made. Can I train an AI (maybe GPT3 based) tool using my published articles (over 200 at this stage), figure out how I like to structure my papers, and then rewrite student submitted manuscripts to match that style, without changing the scientific content?
",['style-transfer'],
Why can the sum over timesteps in the Vanilla Policy Gradient be ignored?,"
I understand how to derive the vanilla policy gradient
$$
\begin{align}
    \nabla_{\theta}J(\pi_{\theta}) = \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t = 0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_{t} \mid s_{t}) \hat{A}^{\pi_{\theta}}(s_{t}, a_{t}) \right]
\end{align}
$$
as is also demonstrated in the openai spinning up documentation. Reading the PPO paper, they say that the most commonly used gradient estimator is
$$
\begin{align}
    \nabla_{\theta}J(\pi_{\theta}) = \mathbb{E}_{\pi_{\theta}} \left[\nabla_{\theta}\log \pi_{\theta}(a_{t} \mid s_{t}) \hat{A}^{\pi_{\theta}}(s_{t}, a_{t}) \right]
\end{align}
$$
and from there they argue that this is equivalent to using the loss function
$$
\begin{align}
    \mathcal{L}^{\text{PG}}(\theta) = \mathbb{E}_{\pi_{\theta}} \left[\log \pi_{\theta}(a_{t} \mid s_{t}) \hat{A}^{\pi_{\theta}}(s_{t}, a_{t}) \right]
\end{align}
$$
where the derivative is then taken with respect to $\theta$.
Question: How does the vanilla policy gradient relate to the gradient stated in the PPO paper? More precisely, they seem to be identical up to the sum over $t$. Why can the sum be ignored and the gradient is still the same?
Thanks in advance for any help!
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'proximal-policy-optimization']","Short answer:The expectation $\mathbb{E}_t$ in the PPO paper is not an expectation over trajectories, but a mean. I suppose the confusion comes from there. The two quantities are otherwise very similar. The gradient is however not the same, as it is a biased estimator.Long answer:There are many related formulations for RL problems, on one hand we have the finite-horizon (episodic) setting:$$ J_1(\theta)=\mathbb{E}_{\tau\sim p_{\pi_{\theta}}}\left[\sum_{t= 0}^T r(s_t,a_t)\right]. $$On the other hand, we have the commonly used infinite-horizon setting (assumed by the PPO paper):$$ J_2(\theta)=\mathbb{E}_{\tau\sim p_{\pi_{\theta}}}\left[\sum_{t\geq 0} \gamma^tr(s_t,a_t)\right]. $$In the finite horizon setting, we have :\begin{equation} \nabla_{\theta} J_1(\theta) = \mathbb{E}_{\tau\sim p_{\pi_{\theta}}}\left[ \left(\sum_{t= 0}^T \nabla_{\theta}\log\pi_{\theta}(a_t\mid s_t) \right)R(\tau) \right], \end{equation}where $R(\tau)=\sum_{t= 0}^T r(s_t,a_t)$. As explained in the SpinningUp post, we can replace $R(\tau)$ by $A_{\pi_{\theta}}(s_t,a_t)$.Now, a similar proof shows that:\begin{equation} \nabla_{\theta} J_2(\theta) = \mathbb{E}_{\tau\sim p_{\pi_{\theta}}}\left[ \sum_{t\geq 0}\gamma^t \nabla_{\theta}\log\pi_{\theta}(a_t\mid s_t) A_{\pi_{\theta}}(s_t,a_t) \right]. \end{equation}Now, in practice it is very common to approximate$$\nabla_{\theta} J_2(\theta) \approx \mathbb{E}_{\tau\sim p_{\pi_{\theta}}}\left[ \sum_{t\geq 0}\nabla_{\theta}\log\pi_{\theta}(a_t\mid s_t) A_{\pi_{\theta}}(s_t,a_t) \right]:=g^{\gamma},$$that is we drop the discount factor $\gamma^t$. This relates to the policy gradient theorem and that we want to sample states from the steady-state distribution instead of the discounted state distribution.Now, we want to approximate $A_{\pi}(s_t,a_t)$ by an estimator $\hat{A_t}$. Schulman et al in the GAE paper introduce the definition of a $\gamma$-just estimator. The exact definition is maybe not essential, but $\gamma$-just estimators include $Q_{\pi}(s_t,a_t)$, $A_{\pi}(s_t,a_t)$ and the GAE estimator used in the PPO paper.So, if $\hat{A}_t$ is $\gamma$-just, then we have:
$$g^{\gamma} = \mathbb{E}_{\tau\sim p_{\pi_{\theta}}}\left[ \sum_{t\geq 0}\nabla_{\theta}\log\pi_{\theta}(a_t\mid s_t) \hat{A}_t \right]. $$This expectation is however still not convenient for a practical algorithm, as we need to collect the trajectories with a policy with older weights $\pi_{\theta_{\text{old}}}$. So we approximate$$g^{\gamma} \approx \mathbb{E}_{\tau\sim p_{\pi_{\theta_{\text{old}}}}}\left[ \sum_{t\geq 0}\nabla_{\theta}\log\pi_{\theta}(a_t\mid s_t) \hat{A}_t \right]:=\hat{g}. $$Now, we need to approximate this expectation, so we collect $N$ rollouts of identical length $T$ (for simplicity) with the policy $\pi_{\theta_{\text{old}}}$, then we have an approximation of $g^{\gamma}$:$$\nabla_{\theta} J_2(\theta) \approx g^{\gamma}\approx  \hat{g}\approx\frac{1}{N}\sum_{n=0}^N\sum_{t=0}^T \nabla_{\theta}\log\pi_{\theta}(a_t^n\mid s_t^n) \hat{A}_t^n,$$
which is what the $\mathbb{E}_t$ in the PPO paper means. The expression on the right-hand side is now finally useful for gradient descent-like algorithms, as $\hat{A_t}$does not depend on $\theta$.Note also that much of the PPO paper aims to ensure that the last approximation by replacing $p_{\pi_{\theta}}$ by  $p_{\pi_{\theta_{\text{old}}}}$ is valid."
"Is my derivation of the Bellman equation for $q_{\pi}$ in terms of $p(s'|s,a)$ and $r(s,a)$ correct?","
I have done exercise 3.29 from Sutton and Barto and I'd like to check if it's correct. Here's the exercise:

Rewrite the Bellman equation for the function $q_{\pi}$ in terms of the three argument function $p$  (3.4) and the two-argument function $r$  (3.5).

Here are the 2 functions:
(3.4) :
$$p(s'|s,a) = P(S_t=s'|S_{t-1}=s,A_{t-1}=a)=\sum_{r\in\mathcal{R}}p(s',r|s,a)$$
3.5:
$$r(s,a) = E[R_t|S_{t-1}=s,A_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)$$
Here's my solution. Is this derivation correct?
$$q_\pi(s,a)  = \\ E_{\pi}[G_{t+1}|S_t=s,A_t=a]= E_{\pi}[R_{t+1} + \gamma G_{t+1} |S_t=s,A_t=a]= \\ E_[R_{t+1}|S_t=s,A_t=a]+\gamma E_{\pi}[G_{t+1}|S_t=s,A_t=a] = \\ \sum_{s',r}p(s',r|s,a)r+\gamma \sum_{s'}p(s'|s,a)v_{\pi}(s') = \\ r(s,a) + \gamma \sum_{s'} p(s'|s,a) \sum_{a'}q(a'|s')\pi(a'|s')$$
","['reinforcement-learning', 'value-functions', 'sutton-barto', 'bellman-equations']","Your work all looks correct to me. But I don't have the exercise 3.29 in my book, so I can't read the question for myself. It's not clear to me whether your last line or second to last line should be considered as the answer."
"What is the equation for $\pi_*$ in terms of $q_*(s,a)$?","
I am trying to solve the following exercise from Sutton and Barto:

Sutton and Barto Exercise 3.27 Give an equation for $\pi_*$ in terms of $q_*(s,a)$

However, I am struggling to do so. I know that $\pi_*$ is the policy which will pick the action with highest return, given we know what the optimal action values are. So intuitively, I would express the optimal policy like this: $$\pi_*(\text{argmax}_a q_*(s,a)|s) = 1$$. To express it like this: $$\pi_* = \text{argmax}_a q_*(s,a) $$
seems wrong since $\pi_*$ is a probability. What am I not getting correct here?
","['reinforcement-learning', 'sutton-barto', 'policies', 'notation', 'optimality']","An optimal policy is just a greedy policy with respect to the optimal state-action value function (which is unique for a given MDP). So, $\pi_* = \text{argmax}_a q_*(s,a) $ is almost correct - it should have been$$\pi_*(s) = \text{argmax}_a q_*(s,a), \forall s.$$In this case, $\pi_*$ is a decision rule or a function.If you define $\pi^*$ as a probability distribution, then you can do something like this$$
\pi_*(s, a)=
\begin{cases}
1, \text{if } a = \text{argmax}_a q_*(s,a)\\
0, \text{otherwise}\end{cases},
$$
$\forall s \in \mathcal{S}$.If $\text{argmax}_a q_*(s,a)$ is a set, you can choose any of the actions in that set with any rule you want, and, of course, still ignore non-optimal actions.This is true for finite MDPs (aka MDPs with finite state, action and reward spaces).There's a result (Putterman, 1994) that states that there's a deterministic and Markovian optimal policy for an MDP. So, optimal policies aren't (necessarily) probability distributions. It's a matter of convention and convenience."
How to use information on a function to design a neural network learning that function?,"
I have a function $g$ that takes a vector $x$ of size $n$ and an integer $k$ in $1, \ldots, n$. I know this function is of the form
$$g(x,k) = G\left(\sum_{i=1}^k f(x_{i})\right),$$ where $f$ and $G$ are some unknown functions.
I want to train a neural network that estimates the function $g$. What is the best way to proceed given the structural information I have about the function?
Of course, I can train the neural network without taking the specific structure into account, but I believe this is not the best way. I can also estimate $n$ neural networks for $g(x,1), \ldots, g(x,n)$, but I guess this is not the best way either. Is there a good / standard way to proceed?
","['neural-networks', 'deep-neural-networks', 'supervised-learning']",
Can objective function and gradient be unlimited in reinforcement learning?,"
I'm looking at an example where they define a policy $\pi_\theta(a_t|s_t)\sim \mathcal{N}(ks_t, \sigma)$, where $a_t$ and $s_t$ are action and state, while $\theta=(k,\sigma)$ are the parameters of the policy. Now I take the sampled approximation of the objective gradient for a policy gradient algorithm (like REINFORCE)
$\bigtriangledown_\theta J(\theta) \simeq \frac{1}{N}\sum_{i=1}^{N} \left[\sum_{t=1}^T \bigtriangledown_\theta log\pi_\theta(a_t^i|s_t^i) \sum_{t=1}^T r(s_t^i,a_t^i) \right] $,
where $log\pi_\theta(a_t^i|s_t^i) =-\frac{1}{2\sigma^2}(ks_t^i-a_t^i)^2+const$
and $\bigtriangledown_\theta log\pi_\theta(a_t^i|s_t^i) =\begin{bmatrix}
\frac{\partial J(\theta)}{\partial k} \\ \frac{\partial J(\theta)}{\partial \sigma}
\end{bmatrix} = 
 \begin{bmatrix}
  -\frac{s_t^i}{\sigma^2}(ks_t^i-a_t^i)\\ \frac{1}{\sigma^3}(ks_t^i-a_t^i)^2
\end{bmatrix}$
in the example they say that the optimal solution is $k=-1,\sigma=0$, but as i get closer to the solution, the objective and the gradient get bigger and goes to infinity. I believe this is a problem, isn't it? Ok, maybe it's a problem only in practices but not in theory... however, I wonder if we can always fall in this situation, since we don't decide the objective function
","['reinforcement-learning', 'deep-rl', 'gradient-descent', 'policy-gradients']",
What makes a 'good' dataset,"
for the usage of ML technologies, having a appropriate dataset is arguably the first and fundamental step one has to tackle by either aquiring a dataset from external sources or creating their own.
While datasets from external sources are of course marketed as beeing 'good' or 'high-quality' (and in most cases not explained on how the authors come to this conclusion), creating a dataset yourself doesn't come with these labels.
This brings me to my questions: How can one (objectively) quantify the quality of a dataset for a given problem?
This, of course, includes some points which are more or less 'accepted' within the community, e.g. the dataset has to contain enough datapoints for every modeled state (which leads to the question of what is 'enough' ...), or that the datapoints for each modeled state should be roughly equally split (e.g. a dataset for cat/dog image discrimination would not work well with one dog image and 10k cat images) and so on.
I recognize that this is a rather open ended and maybe even philosophical question, but I believe that, given the importance of data for ML (and other disciplines), I am in need of an objective way to evaluate my datasets in relation to the task at hand and determine their quality in an objective way. Also my goal here is clearly ML oriented, but since this topic is not only valid in the ML context (and ML is in its core more or less complex statistics), I don't want to restrict this only to the ML, but to datasets overall
","['datasets', 'data-preprocessing']",
What is single object localization?,"
Object detection is said to be combination of object localization and image classification. However, when reviewing localization, I often come across the term ""single-object"" localization, but I am having trouble finding an accurate definition. What is ""single-object"" referring to?

","['computer-vision', 'terminology', 'object-detection', 'object-recognition']",
Is My Extra Tree Classifier Model Over/Underfit?,"
I was playing around with some models and training them on a heart disease dataset and found my Extra Tree Classifier reported a 100% accuracy. I was shocked at first since I had never gotten a perfect score report from a model, but I realized that the model is most likely over/underfit. After doing some research, I found that you can determine if a model is over/underfit by comparing the Training Score and Cross Validation Score reports, but after looking at the report, I could not tell if my model was over/underfit or if it really is perfect. (I am new to machine learning and I have high doubts that it is actually perfect.)
Here is the model report:

Can someone help me figure out if this model is over/underfit? (If I need to give any more data/info to figure this out, please let me know!)
","['machine-learning', 'classification', 'python']","To evaluate your fit, the results on the validation set, as you say, are essential.As you can see you cannot evaluate based on the training set alone, whether it's a good fit or an overfit of the training set.However, you stated in a comment that you also got 98% on the validation set. We can therefore conclude that you do have a good fit and a working model.The very high resulting score is probably due to the fact that you're working on a very easy task. The size shown in the image seems to be 100 samples per class -- this makes me think this is a 'toy' dataset used just as an exercise. Whether this works in the real world, whatever your objective is, depends on how representative this dataset is."
Why do we use gradient descent to minimize the loss function?,"
The purpose of training neural networks is to minimize a loss function, in this process we usually use gradient descent method.
But in Calculus, if we want to find the global minimum of a multivariable function, we usually first calculate the partial derivatives of this function with respect to its variables, and then set these partial derivatives to zeros, and then find the solutions of these equations. Usually we get a bunch of points. We can use the second derivative method(involving Hesse Matrix) to determine whether these points give local minimum values, or we can directly evaluate the values of the function at these points and compare them to find the minimum.
So, I'm curious why don't we use this classical method in Calculus to find the global minimum value of the loss function and instead using gradient descent? Is it hard to for computer to find zeros of multivariable equations?
","['deep-learning', 'backpropagation', 'optimization', 'gradient-descent']",
Why and when do we use ReLU over tanh activation function?,"
I was reading LeCun Efficient Backprop and the author repeated stressed the importance of average the input patterns at 0 and thus justified the usage of tanh sigmoid. But if tanh is good then how come ReLU is very popular in most NNs (which is even more odd when the authors didn't mention about ReLU at all)
","['neural-networks', 'comparison', 'activation-functions', 'relu', 'tanh']",
How does GPT use the same embedding matrix for both input and output?,"
My understanding is that GPT uses the same embedding matrix for both inputs and output: Let $V$ be the vocab size, $D$ the number of embedding dimensions, and $E$ be a $V \times D$ embedding matrix:

On input, if $x$ is a one-hot $V$-dimensional vector, GPT uses $Ei$.
On output, if $\hat y$ is a $D$-dimensional prediction vector, GPT uses softmax($E^\top{\hat y}$) as its predictions.

Q1. Is the above correct?
I cannot find this stated clearly in the paper, but it is stated explicitly here.  It's also clearly implied by the parameter count listed here, and argued for as best practice here.  Yet, for example, Karpathy's mini-GPT implementation seems to use two different matrices:
self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd) # <--- This would be E
self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))
self.drop = nn.Dropout(config.embd_pdrop)
# transformer
self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])
# decoder head
self.ln_f = nn.LayerNorm(config.n_embd)
self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # <--- This has the same dimensions as Etranspose but is clearly a different matrix


Q2. If it is correct, how does can it work?
This seems to be tasking $E$ with two very different, even opposing, functions:

Map vocab to their meaning on the input side; higher magnitude indicates ""more meaning""
Map meaning to the most likely vocab on the output side; higher magnitude indicates greater likelihood

When outputting, we want the softmax to be highest when the word is most likely; magnitude of the output matrix should be roughly proportional to how likely the word is two appear.
Yet, when inputting, magnitude has nothing to do with likelihood.  Magnitude on the input side captures some element of meaning: perhaps how extreme or intense the meaning is, perhaps another aspect (not necessarily easily interpreted).
","['natural-language-processing', 'transformer', 'word-embedding', 'embeddings', 'gpt']","A GPT produces output based on its own previous output, so it must be able to understand its output.The learning input is provided as a stream of tokens, and these tokens are defined before learning starts. So it has to use the same set of tokens to understand its own output. The set of possible output tokens is fixed, it learns only to assign probabilities of the next token.Looking on it in a per token perspective, when it gets a token as input, it learns that it has a non-zero probability to be following the previous input. If the same happens again, it learned that the probability is higher than previously assumed. The probability is expressed as a number of occurrences in the input, and a number per token in the output, but both are about the relative probability of tokens following the previous text."
How to calculate a meaningful distance between multidimensional tensors,"
TLDR: given two tensors $t_1$ and $t_2$, both with shape $(c,h,w),$ how shall the distance between them be measured?

More Info: I'm working on a project in which I'm trying to distinguish between an anomalous sample (specifically from MNIST) and a ""regular"" sample (specifically from CIFAR10). The solution I chose is to consider the feature maps that are given by ResNet and use kNN. More specifically:

I embed the entire CIFAR10_TRAIN data to achieve a dataset that consists of activations with dimension $(N,c,h,w)$ where $N$ is the size of CIFAR_TRAIN
I embed $2$ new test samples $t_C$ and $t_M$ from CIFAR10_TEST and MNIST_TEST respectively (both with shape $(c,h,w)$), same as I did with the training data.
(!) I find the k-Nearest-Neighbours of $t_C$ and $t_M$ w.r.t the embedding of the training data
I calculate the mean distance between the $k$ neighbors
Given some predefined threshold, I classify $t_C$ and $t_M$ as regular or anomalous, hoping that the distance for $t_M$ would be higher, as it represents O.O.D sample.

Notice that in (!) I need some distance measure, but this is not trivial as these are tensors, not vectors.

What I've Tried: a trivial solution is to flatten the tensor to have shape $(c\cdot h\cdot w)$ and then use basic $\ell_2$, but the results turned out pretty bad. (could not distinguish regular vs anomalous in this case). Hence: Is there a better way of measuring this distance?
","['features', 'feature-extraction', 'metric', 'anomaly-detection', 'tensor']","You could try an earth mover distance in 2d  or 3d over the image? For example you could follow this example, but call it sequentially. The idea would be something like the following (untested and written on my cell phone):This should also work with batched data. I would also try normalizing the images first (so they each sum to 1) unless you want to account for changes in intensity."
"What is a ""continuous vector""?","
I have seen the concept of a ""continuous vector"" described in the context of embeddings. For instance, this answer to a question on embeddings in the context of deep learning. I obviously know what a vector is, but it isn't clear to me what a continuous vector is as a mathematical object. What is a ""continuous vector""?
","['machine-learning', 'math', 'embeddings', 'vectors', 'vector-space']","Continuous vector simply means that each value of the vector is allowed to be a real number, in contrast of integer values used in other categorical encoding techniques like one-hot encoding (only 0 and 1) or class-value mapping (a different integer per class)."
How to encode time embeddings in a diffusion model for 1D vectors?,"
For a project I'm working on, I'd like to try a diffusion model like illustrated in the paper by Ho et Al to generate 1D vectors. What I'm trying to figure out at the moment, is what kind of architecture should I use for the 1D case, given that those people use a U-net-based model including different layers like attention, residual, group norm, etc. The only requirement that such a model should satisfy is that the input and output shapes should be the same, as we're interested in predicting the noise associated with a particular timestep.
$$\mathcal{L}(\theta) := \mathbb{E}_{\textbf{x}_0, \, t, \, \varepsilon}\left[\|\varepsilon - \varepsilon_\theta(\sqrt{\bar{\alpha}_t}\textbf{x}_0 +\sqrt{1-\bar{\alpha}_t}\varepsilon,t)\|^2\right]$$
So for now, I'm considering a simple autoencoder architecture but I'm not sure how and where I should inject knowledge of timesteps using some sort of positional encoder as proposed in the paper by Ho et Al.
Additionally, I'm not even sure about they inject such knowledge into the model? Do they sum the obtained time embeddings over channels? Or over image dimensions?
","['generative-model', 'markov-property']",
What other Machine Learning techniques other than Neural Networks are there?,"
I know that there are three types of machine learning algorithms, supervised, unsupervised, and reinforcement learning, and that often neural networks are used to implement them. However, neural networks is said to be a subfield of machine learning, and deep learning a subfield of neural networks.
Thus, I would like to know:
Which specific algorithms/models belong in Machine Learning but are not considered neural networks? Any specific names for these fields?
","['neural-networks', 'machine-learning', 'deep-learning', 'ai-field']","There are many techniques (algorithms and models) in ML other than neural networks, for exampleIf you pick any good book on ML, you will find more details about these.I don't think these approaches fall into a subfield of ML with a name. Occasionally, some people may call them ""traditional ML approaches"" or something like that."
Why can't I train like a dataset of samples instead of maintaining replay buffer?,"
On observing the DDPG algorithm, we notice that the updation of neural networks is happening during the episode.
But, it seems there is no issue if we allow the completion of an episode and then treat the buffer as a dataset to update the neural networks if I don't have any memory constraints. Isn't it?
Even if the episode is too long, I think we can update neural networks once an episode terminates if I have enough memory. I don't think memory constraint is a reason for updating neural networks during episodes, as we can see large datasets in deep learning competitions and research. One possible reason I am thinking is that an episode may not have a goal state and can continue indefinitely.

In general MDPs do not have goal states. Although using the agent's
actions to achieve certain desirable end states, such as winning a
game or completing a puzzle, is a very common design, there is no
requirement for this. The more general requirement is to maximise some
aggregate of the reward at each time step - usually either a
discounted sum of rewards or the mean reward.

If 'having no goal state(s)' is a reason for using buffer, why do the authors claim it as an algorithm instead of a procedure? Am I missing something?
","['deep-rl', 'algorithm', 'ddpg', 'experience-replay']",
Why don't we use diffusion for non-graph CNNs?,"
I'm pretty new to graph neural networks, so please forgive me if this is a silly question.
Diffusion is a method used to improve graph CNNs, however it seems to me that general CNNs can also benefit from taking into consideration diffusion-like processes (for example, in CV, one may want information to diffuse from a small neighborhood, then to a bigger neighborhood, etc.). Also, PDE-based methods have been used in CV traditionally (before deep learning became a thing), so there's at least some evidence that this might work. But I did search extensively, and I can't find any information on using diffusion for general CNNs!
Therefore, what I'm curious about is: why didn't I find any information about this? Is it already explored under a different name? Or does it not really work because of some reason I don't know (or didn't think about)? Or maybe it does work, but there are some technical difficulties?
","['convolutional-neural-networks', 'geometric-deep-learning', 'graph-neural-networks']","Just for completeness, here is one simple formalization of a diffusion GCN (Gasteiger et al.):$\text{D-GCN}(X) = \sum_{k=1}^K A^k X W_k$You have a diffusion factor $k \in [1 .. K]$ and you apply different GCNs with parameters $W_k$ to the $1$-hop, $2$-hop, .., $K$-hop neighborhood and sum the results together. $A^k$ denotes the $k^{th}$ power of the adjacency matrix.There are two principles in visual CNNs that come to my mind that are comparable to the diffusion applied in graph models.Firstly, dilated convolutions apply a kind of diffusion (they are also referred to as Atrous convolution). Specifically, the convolutional kernel is spread out based on a dilation factor $d$. The dilation factor determines the number of pixels that are skipped when applying the kernel. For example, if you have a 3x3 kernel, and apply a dilation rate of $d=1$, you get the standard convolution, a dilation rate of $d=2$ applies the 3x3 kernel to a 5x5 area of the image by using only every 2nd pixel (see the Image below). So in that case $d$ determines which pixel neighborhood the kernel is applied to, which is comparable to the $k$ in the diffusion GCN.
Dilated convolution has a history in segmentation models (see Yu et al. and Chen et al.). Apparently, it helps with segmentation, because the dilated convolutions can capture the context of pixels a bit better, so segmentation maps become cleaner.
It has also been applied with 1-dimensional convolutions in the WaveNet architecture. In that case dilation was used to cover greater receptive fields without increasing the number of parameters, i.e. model complexity.The green is the output, blue is the input and the grayed pixels are the pixels where kernel weights are applied to. Image taken from hereSecond, the Inception architecture by Szegedy et al. applies something that can be interpreted as a diffusion as well: Inception computes convolutions of different kernel sizes in parallel and stacks their feature maps (this blog post has a nice visualization). Here, the different kernel sizes would correspond to the $k$ and stacking is used instead of a sum.I think these two examples are definitely related to the idea of diffusion GCN."
Is this the correct way to backpropagate a Neural Network?,"
I am writing a Neural Network frorm scratch. Below is what I have right now, based off of the math that I think I understand.
##### Imports #####
from matplotlib import pyplot as plt
import numpy as np

###### Activation Function #####
def sigmoid(input, derivative = False):
    if derivative:
        return sigmoid(input) * (1 - sigmoid(input))
    return 1 / (1 + np.exp(-input))

##### Feed Forward Neural Netowkr Class #####
class FFNN:
    def __init__(self, learning_rate, num_epochs):
        # Network
        self.w1 = np.random.randn(30, 5)
        self.w2 = np.random.randn(5, 3)

        # Hyperparameters
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs

    # Forward Propagation
    def forward(self, input):
        self.z1 = np.dot(input, self.w1)
        self.a1 = sigmoid(self.z1)

        self.z2 = np.dot(self.a1, self.w2)
        self.a2 = sigmoid(self.z2)
    
    # Backward Propagation
    def backward(self, input, error):
        error2 = error * sigmoid(self.z2, derivative = True)
        d2 = np.dot(self.a1.T, error2)

        error1 = np.dot(self.w2, error2.T).T * sigmoid(self.z1, derivative = True)
        d1 = np.dot(input.T, error1)

        self.w1 -= d1 * self.learning_rate
        self.w2 -= d2 * self.learning_rate

    # Train
    def train(self, inputs, labels):
        for _ in range(self.num_epochs):
            for input, label in zip(inputs, labels):
                self.forward(input)
                self.backward(input, self.a2 - label)

    # Test
    def test(self, inputs):
        for input in inputs:
            self.forward(input)

            print('Image is a', 'ABC'[np.argmax(self.a2)])
            plt.imshow(input.reshape(5, 6))
            plt.show()

# Initialize Neural Network
feed_forward_neural_network = FFNN(learning_rate = 0.1, num_epochs = 100)

##### Training #####
a = [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]
b = [0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0]
c = [0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]

y = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
x = [np.array(a).reshape(1, 30), np.array(b).reshape(1, 30), np.array(c).reshape(1, 30)]

feed_forward_neural_network.train(x, y)

##### Testing #####
feed_forward_neural_network.test(x)

However, after looking at someone else's code, they have the same thing except the backward function does this instead:
    # Backward Propagation
    def backward(self, input, error):
        error2 = error
        d2 = np.dot(self.a1.T, error2)

        error1 = np.dot(self.w2, error2.T).T * sigmoid(self.z1, derivative = True)
        d1 = np.dot(input.T, error1)

        self.w1 -= d1 * self.learning_rate
        self.w2 -= d2 * self.learning_rate

Notice the missing sigmoid(self.z2, derivative = True) multiplication by the layer 2 error.
Both of these functions converge just fine, but obviously one of them is wrong. Which one, and why?
","['neural-networks', 'python', 'backpropagation', 'math', 'sigmoid']","Your $d_2$ is the gradient used to update $w_2$, which is of course $\frac{dL}{dw_2}$. To compute this gradient, using your notation:
$$ \frac{dL}{dw_2} = \frac{dL}{da_2}\frac{da_2}{dz_2}\frac{dz_2}{dw_2} = err \cdot \sigma'(z_2)\cdot a_1$$
So your version seems to be correct.One possibility is that the forward is also different, and there is no sigmoid after the second layer in your colleague's network (which is often the case for the last layer). In which case their version would also be correct."
"How to establish baselines, with different training loops","
My objective is to test out a new algorithm that I designed. However, I am confused whether my methodology to train the networks is correct.
I am just concerned about the training loops:
In the first algorithm (DIAYN, SAC Based Algorithm), the pseudocode follows a high-level pseudocode of:
Run for N-STEPS:
   1. Run for around 5000 steps
   2. Add in Replay Buffer
   3. After each step, after 5K, choose action using policy
   4. Step in env, collect reward, next_obs ..
   5. Update networks by sampling from replay buffer, batch size of 1025

In the new algorithm, I update the same networks, but in a new manner (which is required for the algorithm to do some other stuff.
Run for n epochs:
   1. Run and collect the 1000 samples of next_obs, reward .. by choosing the action from the policy. 
   2. Then, run some algorithm (this the new addition) to the replay buffer. 
   3. Run a training loop, which runs 1000 times, which updates the networks of a batch size of 128.


As you can see, in the first algo_1, the batch size is 1024, and we collect new samples after each update. Whereas, in algo_2 we update the network 1000 times with replay buffer samples, then we collect new samples.
However, in algo_2, we collect 1000 new samples again. In algo_1, only one sample is added to the replay_buffer after each update. So one new data point is generated from a new updated policy. In algo_1, 1000 samples are generated using the new policy updated 1k times from old replay_buffer.
My question is this, if I wanted to establish a baseline using algo_1, and say that my algo_2 is better as it does X better. Can I do so, if I make sure that the N-STEPS in algo_1 are equal to epochs*1k_training_loop in algo_2?
I apologise for not making this post succinct.
","['reinforcement-learning', 'soft-actor-critic', 'hindsight-experience-replay']",
Is the accuracy the best metrics to evaluate the performance of Deep Learning model? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



Consider a model A that achieved an test accuracy of 99% on dataset-A with the size of 200 images and a model B that achieved only 50% on dataset-B with a size of 50,000 images. Also consider both the datasets split into train,validation and test sets in the ratio of 0.8,0.1,0.1.
But on test data of dataset-B,the model A is failed to attain same accuracy in fact it is giving lesser accuracy that of model B.
So, is the accuracy always the best measure to evaluate the performance of the DL model? Or any other better performance metrics available?
","['deep-learning', 'performance', 'accuracy']",
Is there a way to form a reward function so that it would take into account the order of the actions?,"
I want to design a multi-arm bandit system for a multi-step, multi-location system. Locations are dynamic, so I can not design the system based on them. In each location, the alternative actions that can be taken would be different. When you take correct actions, taken in correct locations, then some rewards would be earned. Some other alternative rewards can be incorporated in the system for the activities taken before reaching the correct state.
I know this may not be very clear. What I want to ask is ""Is there a way to form a reward function so that it would take into account the order of the actions or the correctness of the order of the actions?"".
Previously, I have implemented some other multi-arm bandit problems, but they were more straightforward. I need some ideas to help me to implement this new type of problem from some experts.
","['reinforcement-learning', 'reward-functions', 'multi-armed-bandits', 'reward-design']","You are describing an environment which requires a full Markov Decision Process (MDP) to model it and reinforcement learning (RL) algorithms to solve it. You will not be able to adapt k-armed bandit algorithms, without effectively re-inventing MDPs.The two key details that make this full RL, and not a bandit problem, are:Decisions are sequential, with options and outcomes that depend on previous decisions.Action choices make changes to variables (which in MDP would be part of the state description) that impact outcomes of future actions.If you allow the agent to access the state including effects of previous actions encoded in a way that it has enough data to correctly predict rewards, then you have a normal MDP and most RL methods should be applicable.If you do not allow the agent to use a convenient history of past actions (and/or their effects) as part of the state, then you will have constructed a partially observable MDP (POMDP) and may need a more advanced approach to solve it. For instance, using an RNN (most likely an LSTM or a GRU architecture) to process state sequence and predict action values could learn about the hidden sequence.In terms of implementing a simulation of your environment, you will need to model it as a stateful system, and will have to include a concept of forward step in the sequence which modifies the state variables (regardless of whether these variables are made available to the agent in any observations). This would include the location information, and any other factors that change the allowed actions or outcome. As well as a step function, you will probably want a state reset function that puts the system into a starting state, or one of a range of possible starting states.If your environment is episodic (a sequence can end), then you will need a way to flag that so that the learning agent can react to the end of an episode and request a new starting state."
How to sample the tuples during the initial time steps of the DDPG algorithm?,"
I am facing an issue in understanding the following line from the pseudocode of the DDPG algorithm

Sample a random minibatch of $N$ transitions $(s_i, a_i, r_i, s_{i+1})$ from $R$

Here $N$ is a hyperparameter that is equal to the number of transitions or samples that need to be present in the minibatch.
Traditionally, we take a minibatch of samples from a dataset, which contains all samples, and pass them to a neural network, but if we observe the DDPG pseudocode, we are storing transition after transition into the buffer $R$. So, I think, it needs several steps before sampling from the buffer $R$. But, if we observe the pseudocode, it says that we need to sample from the first timestep of the first episode.
How is it possible? Where am I missing?
","['deep-rl', 'implementation', 'ddpg', 'experience-replay', 'batch-learning']","You have a free choice to either:Wait until the replay buffer hits a minimum size for sampling.Take smaller samples from the buffer initially, until the buffer is large enough. On the first time step the minibatch size will be 1.Both these approaches are valid, and neither changes the DDPG algorithm significantly. Plenty of other hyperparameters for DDPG will have bigger effects on the end result.The Keras implementation of DDPG here https://keras.io/examples/rl/ddpg_pendulum/ uses the second approach.The pseudocode in the paper is not 100% clear, but I would expect they did the same."
Does deep RL techniques only interested in 'unit transitions' rather than 'whole experience'?,"
In deep-rl techniques, if I understand correctly, a replay buffer is used in training the neural networks. The purpose of using the replay buffer is to store the experience and send a (sampled) batch of unit transitions to train neural networks as it is known that neural networks work well for iid data.
But in games, experience trajectory is important as it contains temporal dynamics. Am I true? If not, all the knowledge required to learn the policy function can be obtained from (out of sequence or randomly sampled) unit transitions alone.
Which one among the both is correct?
Note that unit transition in this question refers to $(s_t, a_t, r_t, s_{t+1})$
","['neural-networks', 'deep-rl', 'experience-replay', 'iid']",
Has there been an instance of an AI agent breaking out of its sandbox?,"
There have been instances of agents using edge cases like bugs in physics engines, repetitive behavior in games or word repetition in text prediction to cheat their reward function. However, these agents are arguably still contained, as while they explore the extremes of the state space of the simulation they don't expand their action space beyond what is possible in the simulation.
The Pokémon Yellow Total Control Hack shows that in some systems, it is possible to gain full control of a computer by exploiting bugs in the hardware or software (here: memory corruption), enabling the agent to even completely reprogram the system 'from within', just using the normal inputs.
Do you know of similar extreme examples where an AI agent went far beyond what was intended with the simulation environment and expanded its action space?
","['intelligent-agent', 'ai-safety', 'ai-box', 'reward-hacking']",
How to predict time signal based on multi-input signals?,"
I would like to approximate the following relation by a neural network
$y = \mathcal{f}(x_1(t),x_2(t))$
Here, I have only one output variable that is a function of 2 other variables which vary in time. Now, I want to be able to predict $y$, given any shape of the 2 independent variables in time. For this reason, I have a training set corresponding to different input and output signals. However, I don't know how to make the neural network understand the concept of time which is very important since I expect the solution at time $t_k$ to be influenced by the previous instants in time. For this reason, I added as input variable the time derivative as
$y = \mathcal{f}\left(x_1(t),x_2(t), \dfrac{\partial x_1 (t)}{\partial t}\right)$
This solution seems to work quite well for the fully connected neural network that I'm using. However, I would like to know if there are other ways to treat such problems where the time history is important.
","['neural-networks', 'machine-learning']",
How to calculate number of connected neurons with filter,"
let's say I have a conv layer i with 64 feature maps and a filter size of 3x3.
The previous conv layer i-1 has 32 feature map.
Step-size is 2 and padding 1.
My question is now how to know how many neurons of the previous layer are connected to each of the neuron in i?
My first thought was 32 because there are 32 neurons/filters and I thought that number of feature maps = number of filters.
But I am not sure
","['convolutional-neural-networks', 'filters']",
Does maximizing the value function and maximizing the state-action value function generate the same optimal policy?,"
In reinforcement learning, we define the optimal policy $\pi^*$ as the policy that maximizes the value of the state:
$$
\pi_v^*=\underset{\pi}{\operatorname{argmax}} {V_{\pi}(s)}
$$
In Q-learning, we try to find a policy that maximize the state-action value function Q:
$$
\pi_q^*=\underset{\pi}{\operatorname{argmax}} {Q_{\pi}(s,a)}
$$
However, does maximizing the value function and maximizing the state-action value function generate the same optimal policy? In the general case of continuous, stochastic action, $V_{\pi}(s)$ is connected to $Q_{\pi}(s,a)$ by:
$$
V_{\pi}(s)=E_{a\sim\pi}[Q_{\pi}(s,a)]
$$
So
$$
\pi_v^*=\underset{\pi}{\operatorname{argmax}} {V_{\pi}(s)}=\underset{\pi}{\operatorname{argmax}} E_{a\sim\pi}[Q_{\pi}(s,a)]
$$
And if $\pi_v^*=\pi_q^*$, mathematically I'm not sure why the expectation $E_{a\sim\pi}$ before $Q_{\pi}(s,a)$ can be simply dropped.
","['reinforcement-learning', 'q-learning', 'markov-decision-process', 'value-functions']",
Data Imbalance in Contextual Bandit with Thompson Sampling,"
I'm working with the Online Logistic Regression Algorithm (Algorithm 3) of Chapelle and Li in their paper, ""An Empirical Evaluation of Thompson Sampling"" (https://papers.nips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf).  It's a contextual bandit model using a Laplace approximation for regularized logistic regression with Thompson Sampling.
My question is about the effect of data imbalance.  Say the log files of the online advertiser showed that after implementing the algorithm, Ad#1 accounted for 40% of the impressions and achieved 60% of the rewards (clicks).  Ad#1 has the 3rd highest conversion rate compared to 30 other ads.  (The two ads with higher conversion rate accounted for <1% of weekly impressions/rewards.  The number of impressions is still high though, N>300K).
Question: What would be the effect of downsampling in the training set, the examples for which Ad#1 has positive reward?  The paper doesn't mention anything about data imbalance.  Is data balancing needed/advisable?
Option A
The model will impress Ad#1 less and result in fewer weekly aggregate rewards (less exploitation of Ad#1 resulting in less rewards).  The concern is that the model has chosen to exploit Ad#1 and if we downsample its rewards in the training set, we will exploit less and allocate more impressions to ads with lower conversion rates.
Option B
The model will impress Ad#1 less, however will learn better when to serve the impressions in a targeted way so that the weekly aggregate rewards will not change and/or will increase.
The idea is that the imbalanced dataset is causing Ad#1 to be impressed in situations in which it shouldn't.  By downsampling, and having a balanced dataset, we will better model when Ad#1 should actually be impressed and this will lead to higher rewards for Ad#1
Option C
Something Else
","['multi-armed-bandits', 'logistic-regression', 'imbalanced-datasets', 'thompson-sampling']",
Why do Q-values diverge without a target network?,"
After reviewing similar posts on this topic, I understand that a target network is used to prevent ""divergence"", but am not sure what it actually means. Q-values are predicted using a function approximator. The weights of the function approximator are then updated using the difference between the Q-value and TD target ($r + \gamma\max_aQ(s',a)$). Now assuming, that the estimate of the Q-value was wrong, the weights could easily be updated so that they are correct the next time the state is encountered. My confusion arises when online blogposts say that this change in weights modifies the target Q-value($Q(s', a)$) too. I don't see how the target Q-value gets updated when the current Q-value is changed.
","['reinforcement-learning', 'deep-rl', 'dqn']","I don't see how the target Q-value gets updated when the current Q-value is changed.Without a separate target network, this happens because the approximator will generalise, and the generalisation will include the successor/target states. This can be very likely in many environments, since successor states often have many similar features to the previous states (think how similar any video game might look a few video frames on from any position).In tabular variants of Q learning this does not happen, a change to any single Q value give state, action is always made to a single estimate that is isolated from all other estimates. Adding approximation changes things, and is not avoidable - in fact it is usually desirable to have strong generalisation in order to obtain good estimates for never-seen-before states. However, the flip side of this strong generalisation is that updating estimates for $Q(s,a)$ will impact many other $Q(s_i,a_j)$One related thing worth bearing in mind is that the Q-function update in DQN is a semi-gradient update. If you do not use a target network, then technically the full gradient needs to take account of the changes to the TD target when the weights change (because both $Q(s,a)$ and $Q(s',a')$ are calculated use the same approximator). So one way to try and solve the same issue is to alter the update to use the full gradient. The maths for this is more complex than normal DQN, but is addressed in the Sutton & Barto book in chapter 11 section 11.7 ""Gradient-TD Methods"".In practice for DQN, most experiments seem to prefer some variant of target networks. This also effectively changes the learning task to a full gradient one, with different progression of the TD targets. Although it is not as theoretically nice as a full gradient method, it seems to be a pragmatic choice by many researchers and developers.The paper Full Gradient DQN Reinforcement Learning: A Provably Convergent Scheme attempts to make a fair comparison of DQN with a TD-gradient DQN, and comes to the conclusion that the full gradient approach is better - however, there may be other reasons to prefer the slightly clunkier semi-gradient plus target network approach."
How does one detect training instabilities in DQN?,"
I am curious what training instabilities look like in a standard dqn, with or without a target network. I'm assuming the loss function would never converge since the difference between the predicted q-values and td-target would not reduce. I'd like it if someone could give more identifying symptoms of this problem.
","['reinforcement-learning', 'deep-rl', 'training', 'dqn', 'stability']",
Neural network: Initial weights for layer with non-negative constraint,"
I wonder how to initialize the weights of a layer with non-negative weight constraints and sigmoid activation afterwards. I did not find any guidelines. I thought about taking inspiration of glorotUniform initialization and using $\text{Uniform}[0,b]$ with $b=2 \sqrt{6 / (n_{in} + n_{out})}$ where $n_{in}$ and $n_{out}$ are the number of input and output units respectively. Another idea I had was choosing $a=\frac{n-12}{2n}$, $b=1-\frac{n-12}{2n}$ where $n$ is the number of input units and sampling out of $\text{Uniform}[a,b]$ such that the variance of each output variable is equal to $1$.
My question: Are there any guidelines how to initialize non-negative weights? If no, do you think my suggestions run in problems?
","['neural-networks', 'weights-initialization']",
"As someone starting out in RL, could you help me understand the differences between actor-only, critic-only, and actor-critic methods?","
I have been reading some medium articles and these three methods pop up a lot. I am wondering what the differences between these are, what are the advantages of one over the other, etc. Also from my understanding, actor-only method is synonymous to policy gradient methods, critic-only method is synonymous to value-based methods, and actor-critic methods use policy gradient methods for the actor part and value-based methods for the critic part. Am I correct? Any help would be greatly appreciated. Finally, what should one keep in mind when considering one or the other for their projects?
","['reinforcement-learning', 'comparison', 'actor-critic-methods', 'value-based-methods', 'policy-based-methods']",
How to calculate the total number of inputs in CNN?,"
I search this kind of question for a while and I find many discussions involve on counting the number of parameters of a Convolutional Neural Network, but not on the inputs. Using the Fashion MNIST dataset as an example, each black and white image has $28 \times 28 \times 1$ pixels and there are 60,000 images in the training dataset. Does that mean we have total number of $28 \times 28 \times 60,000=47,040,000$ inputs for the input layer of CNN?
My partner critics my baseline/simplest CNN model (for demo purpose) with just one convolutional layer with 10 filters/kernels (the kernel size is $3 \times 3$
), padding has been used and strides = 1. The Keras model information is listed below. He says the training set only have a sample size of 60,000, but you have 78,510 parameters. He concerns over-fitting issues because I have more parameters than the inputs.

I really don't know how to explain the concepts to him clearly that the inputs of CNN are pixels. Could anyone help? A more detailed explanation will be very helpful and I am also happy to learn!
","['convolutional-neural-networks', 'classification', 'image-processing', 'weights']",
What are the possible ways to handle imbalance in multi-class image datasets?,"
Image imbalance is one of the major factor in the performance of DL model. Some of the methods that I found to tackle this are oversampling, under-sampling, SMOTE. Over-sampling has cons as it makes model to be overfit.undersampling results in loss of useful information.Again using SMOTE technique also won't works well on image datasets(as per web references).
What is the right way to handle imbalance in image datasets (multi-class problem)?
","['deep-learning', 'image-processing', 'data-science', 'imbalanced-datasets']",
In-batch negative training Improves the results,"
I have read Dense passage retrieval for Open Domain Question Answering, and in page 6 it talks about in-batch negative training, it states the following:

We find that using a similar
configuration (7 gold negative passages), in-batch
negative training improves the results substantially.
The key difference between the two is whether the
gold negative passages come from the same batch
or from the whole training set. Effectively, in-batch
negative training is an easy and memory-efficient
way to reuse the negative examples already in the
batch rather than creating new ones. It produces
more pairs and thus increases the number of train-
ing examples, which might contribute to the good
model performance. As a result, accuracy consis-
tently improves as the batch size grows.

I know that it is more efficient and hence we can increase the number of negatives by increasing the batch size (or we can use other techniques if we cannot do that), but in the paper they have the following table:

How having the same number of negatives (3rd & 4th rows) gave different results?
Is that something realted to the gradient or something, since we sample from the same examples that contribute to calculating the gradient in one step?
","['machine-learning', 'natural-language-processing', 'papers', 'question-answering']",
"How does the hidden activation differ from the output, at any time step for a SimpleRNN?","
I am watching the Sequence models course taught by Andrew Ng. I am a bit lost on the SimpleRNN lecture. As per the lecture, at each time step, there's an output y from a hidden layer and an input activation from the preceding layer. However, I do not understand what's the difference between the activation and output $y$ at any time step (except the first).
As per my understanding -
$$y = activation(w*x+b)$$
Therefore $y_t$ and $activation_t$ should be the same. I have included a figure to describe my issue.

In the picture, what's the difference between $\hat{a}^{\langle t \rangle}$ and $\hat{y}^{\langle t \rangle}$?
Here's the link of the lesson - https://www.coursera.org/learn/nlp-sequence-models/lecture/ftkzt/recurrent-neural-network-model
","['deep-learning', 'recurrent-neural-networks']","They are not the same always, the activation (as you called it) here is the hidden state (I think it's a in your slide), now you can have the output is the same as the hidden state (for example that may represent a word embedding) but for most problems you need to do more operations on the hidden state to calculate the output.We do that because what you need as output in the current step may not be the whole hidden state (which may contatin info that required in future steps), also you may need to project the hidden state, for example if you want to predict a word you need to project the hidden state into a vector in $\mathbb{R}^{|V|}$ where $V$ is your vocabulary, and then apply a softmax over it (like in the equations above)."
What would be the reason for having a different network architecture for the actor vs. value function networks in PPO?,"
I was reading this link , and saw some creative architectures for PPO.
I know the ""No Free Lunch Theorem"" and all, but what would be the logic/reasoning for why you would choose to have a different size/shape/depth for the actor vs. value func. networks in PPO?
I just came across this question, which is related but not the same. It asks about the utility of sharing parameters across the two networks, to which the answer is essentially ""that might accelerate training by having less parameters"". I think my question is distinct and more general.
","['deep-rl', 'proximal-policy-optimization', 'architecture']",
What is the most accurate way of building a Perceptron using only NumPy?,"
For context, I am trying to write a bunch of neural network programs using no other packages besides NumPy for educational purposes. I am trying to make them as simple as possible, i.e. removing the features that might help training or testing, but which technically aren't supposed to be there.
I am currently writing a Perceptron program, and it seems everywhere I look, it is implemented differently. The program I have written works, I am more concerned about whether or not it is accurate to what a Perceptron is (again, this is for learning purposes). For example: some people seem to use an activation function and others don't- does the Perceptron use an activation function? I know it will function either way, but which is correct?
Here is the code I have right now. Please correct any and all mistakes I might have.
##### Imports #####
import numpy as np

##### Perceptron Class #####
class Perceptron:
    def __init__(self, input_size, num_epochs, learning_rate):
        # Hyperparameters
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs

        # Network
        self.weights = np.zeros(input_size)
        self.bias = 0

    # Forward Propogation
    def forward(self, input):
        layer_output = np.dot(input, self.weights) + self.bias
        return layer_output

    # Back Propogation
    def backward(self, error, input):
        self.weights += self.learning_rate * error * np.array(input)
        self.bias += self.learning_rate * error

    # Train
    def train(self, inputs, labels):
        # Iterate Epochs
        for _ in range(self.num_epochs):
            # Iterate Pairs of Inputs and Labels
            for input, label in zip(inputs, labels):
                # Predict
                prediction = self.forward(input)

                self.backward(label - prediction, input)
    
    # Test
    def test(self, inputs, labels):
        # Iterate Pairs or Inputs and Labels
        for input, label in zip(inputs, labels):
            # Predict
            prediction = self.forward(input)

            # Print
            print(f'Input: {input}, Prediction: {prediction}, Label: {label}')

# Initialize Perceptron
perceptron = Perceptron(input_size = 2, num_epochs = 1_000, learning_rate = 0.01)

##### Training #####
training_inputs = [[1, 1], [1, 0], [0, 1], [0, 0]]
training_labels = [1, 0, 0, 0]
perceptron.train(training_inputs, training_labels)

##### Testing #####
testing_inputs = [[1, 1], [0, 1]]
testing_labels = [1, 0]
perceptron.test(testing_inputs, testing_labels)```

","['neural-networks', 'python', 'history', 'perceptron', 'numpy']",
Is plain autoencoder a generative model?,"
I am wondering how a plain auto encoder is a generative model though its version might be but how can a plain auto encoder can be generative. I know that Vaes which is a version of the autoencoder is generative as it generates distribution for latent variables and whole data explicitly. But I am not able to think how an autoencoder generates probability distribution and becomes a generative model.
Also from this youtube video: here It says plain auto encoder is not a generative model. See last line from picture.

","['deep-learning', 'autoencoders', 'generative-model']","An autoencoder is not considered a generative model, because it only reconstructs the given input. You could use the decoder like a generative model by putting in different vectors. However, the standard autoencoder mostly learns a sparse latent space. This means that you will have distinct clusters in the latent space (see the left image below). The decoder has never learned to reconstruct vectors in between the clusters, so it will produce very abstract things - mostly garbage.Instead a variational autoencoder (VAE) is considered a generative model. It's basically an autoencoder with a modified bottleneck. This VAE learns a dense latent space (see image on the right), this means you can sample any vector from the latent space, pass it to the model and it will give you a nice result with somewhat interpolated object properties from the dataset.This article provides a nice overview of the two models.Figure taken from here"
How to interpret the output plan of the fast-downward planner,"
I'm using this domain/problem with the fast-downward planner like this:
./fast-downward.py --plan-file plan.out ../test_domain.pddl ../test_problem.pddl 
The issue here is that the output.sas contains hundreds of thousands of lines without a clear reference to the solution plan!
Here is the full output log of the planner:
$ ./fast-downward.py --plan-file plan.out ../test_domain.pddl ../test_problem.pddl
INFO     planner time limit: None
INFO     planner memory limit: None

INFO     Running translator.
INFO     translator stdin: None
INFO     translator time limit: None
INFO     translator memory limit: None
INFO     translator command line string: /usr/bin/python3 /media/belal/WD//Planning/downward/builds/release/bin/translate/translate.py ../test_domain.pddl ../test_problem.pddl --sas-file output.sas
Parsing...
Parsing: [0.000s CPU, 0.001s wall-clock]
Normalizing task... [0.000s CPU, 0.000s wall-clock]
Instantiating...
Generating Datalog program... [0.000s CPU, 0.000s wall-clock]
Normalizing Datalog program...
Normalizing Datalog program: [0.000s CPU, 0.001s wall-clock]
Preparing model... [0.000s CPU, 0.000s wall-clock]
Generated 9 rules.
Computing model... [0.000s CPU, 0.000s wall-clock]
73 relevant atoms
28 auxiliary atoms
101 final queue length
157 total queue pushes
Completing instantiation... [0.000s CPU, 0.001s wall-clock]
Instantiating: [0.000s CPU, 0.003s wall-clock]
Computing fact groups...
Finding invariants...
5 initial candidates
Finding invariants: [0.000s CPU, 0.001s wall-clock]
Checking invariant weight... [0.000s CPU, 0.000s wall-clock]
Instantiating groups... [0.000s CPU, 0.000s wall-clock]
Collecting mutex groups... [0.000s CPU, 0.000s wall-clock]
Choosing groups...
4 uncovered facts
Choosing groups: [0.000s CPU, 0.000s wall-clock]
Building translation key... [0.000s CPU, 0.000s wall-clock]
Computing fact groups: [0.000s CPU, 0.001s wall-clock]
Building STRIPS to SAS dictionary... [0.000s CPU, 0.000s wall-clock]
Building dictionary for full mutex groups... [0.000s CPU, 0.000s wall-clock]
Building mutex information...
Building mutex information: [0.000s CPU, 0.000s wall-clock]
Translating task...
Processing axioms...
Simplifying axioms... [0.000s CPU, 0.000s wall-clock]
Translator axioms removed by simplifying: 0
Computing negative axioms... [0.000s CPU, 0.000s wall-clock]
Processing axioms: [0.000s CPU, 0.000s wall-clock]
Translating task: [0.000s CPU, 0.001s wall-clock]
24 effect conditions simplified
0 implied preconditions added
Detecting unreachable propositions...
0 operators removed
0 axioms removed
2 propositions removed
Detecting unreachable propositions: [0.000s CPU, 0.000s wall-clock]
Reordering and filtering variables...
6 of 6 variables necessary.
4 of 6 mutex groups necessary.
24 of 24 operators necessary.
0 of 0 axiom rules necessary.
Reordering and filtering variables: [0.000s CPU, 0.000s wall-clock]
Translator variables: 6
Translator derived variables: 0
Translator facts: 16
Translator goal facts: 2
Translator mutex groups: 4
Translator total mutex groups size: 12
Translator operators: 24
Translator axioms: 0
Translator task size: 204
Translator peak memory: 31884 KB
Writing output... [0.000s CPU, 0.000s wall-clock]
Done! [0.000s CPU, 0.007s wall-clock]
translate exit code: 0

INFO     Running search (release).
search needs --alias, --portfolio, or search options

Can you please tell me how can I interpret the generated plan?
","['planning', 'pddl', 'strips']",Fun domain!You shouldn't have to be parsing the FD output for the plan. Just use --plan-file plan.out as a command-line option to write the plan to the plan.out file. Chances are that it's already creating a file for you locally and you may not have noticed.
How do I preload a conversational AI assistant like GPT-3 with complex relational data to draw on?,"
I'm exploring options to build a virtual assistant type of product.
Creating good dialog is mostly solved with GPT-3 or even DialoGPT.
My main question is how do I add larger amounts of relational data, for the AI to draw on.
e.g.
I have a list of properties, with address, details like number of rooms, bedrooms etc.
Each properties as a historic list of tenancies, as well as owners, each tenancies has a list of tenants and rent agreements. Each Owner & Tenant has personal details. And we can go all the way down to financial transactions, invoices against properties.
etc.
In a prototype I got GPT-3 to write SQL code for me, which works reasonably well for Q&A. Not so much for a continuous conversation, that allow you to dig further into details.
I could also write out all details on a per record bases. e.g. ""Property 15125 has address '20 W 34th St., New York, NY 10001' with 3 Bedrooms and 2 Bathrooms"". I haven't tried it, but I'm relatively confident that if a dataset becomes sufficiently large (hundreds of records) the system would fail to make the correct connections.
An ideal conversation would look something like this:
Me: ""Who is the owner of 20 W 34th St., New York, NY 10001?""
AI: ""Peter Pan and Wendy Darling share the property.""
Me: ""Who is the most recent tenant?""
AI: ""Dr. John Darling is the current tenant.""
Me: ""How much is the rent?""
AI: ""Then rent is $1902 a week.""
Me: ""What's John's phone number?""
AI: ""His mobile number is (311) 555-2368.""

","['natural-language-understanding', 'gpt', 'natural-language-generation', 'gpt-3']",
How to quickly train your agent to get a sense of the problem?,"
I have a custom implementation of DQN. My robot/agent is running in Gazebo with ROS. Though I am trying a very simple task of pushing a cube, the DQN agent is taking too much time. DQN of 300 episode with step limit of 10 took 7.2 hours to train with worst performance. I suspect that the extremely slow training is mostly due to gazebo(+ joint trajectory controller, resetting controllers etc) and ROS. I want to nail down if I am facing one of the following problems:

Problem with my implementation of DQN
Problem is not well defined - For instance, state space is not defined well
Not enough training

DQN had two hidden layers with 64 and 32 neurons. Observation space is 9D, action space is 2D.
Exploration/Exploitation is handled using a decay epsilon greedy policy
max_exploration_rate = 1.0 
min_exploration_rate = 0.001 
exploration_decay_rate = 0.005 
exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode_number)
if np.random.uniform(0, 1) < exploration_rate:
    explore
else:
    exploit

In an ideal case, the exploration/exploitation curve would look like this:

","['reinforcement-learning', 'deep-rl', 'dqn']",
What inputs to give to a trained neural network?,"
Real beginner on neural networks here, so my apologies if the answer is obvious!
I'm learning Echo State Neural networks for non-linear function modelling and I'm struggling to understand why sometimes a simple vector with one's ([1,1,...,1]) is given as input both in the train phase and the predictions themselves.
I've seen this happen in an example where an ESN is used to learn the Mackey-Glass chaotic system (https://github.com/cknd/pyESN/blob/master/mackey.ipynb) but I just don't understand why it works.
Usually, performing a typical train/test split on the X and y data, one uses the X_test part as input to calculate the predictions to be compared with y_test. In the example above, both in the train and test parts they simply give a vector of one's in the same dimensionality as X.
I would understand that something of the sort could be given to predict future values when no data is available but even then, I would pick the previous value, predict the next one, use the prediction to predict the second one, and so on.
","['neural-networks', 'echo-state-network']",
Is there any proper literature on the types of features that different layers of a deep neural network learn?,"
Let's consider a deep convolutional network. It seems that there is some consensus on the following notions:
1. Shallow layers tend to recognise more low-level features such as edges and curves.
2. Deeper layers tend to recognise more high-level features (whatever this means).
While I usually come across various online articles and blogs that state this, no one ever cites literature that supports this claim. I am not seeking the question as to why this phenomenon happens, I'm only seeking whether it has actually been experimented on and documented. Also, I am barely able to find any peer-reviewed literature that provides evidence of this on sites such as Google Scholar or ResearchGate.
Could anyone point me to the right direction?
","['deep-learning', 'convolutional-neural-networks', 'reference-request', 'features', 'representation-learning']",
"Where does the ""rectified"" in ReLU come from?","
ReLU stands for Rectified Linear Unit. Linear Unit, I understand, since the function is piecewise linear. But what does rectified mean?
I looked up the definition and it said:

denoting an electric current that has been converted from alternating current to direct current

But I don't quite see how this can connect to ReLU function. While it does have a bend at $x = 0$ indicating a change in direction, not sure how I can work this into the idea of current directions.
","['deep-learning', 'terminology', 'activation-functions', 'relu']",I think it is by analogy with an electrical rectifier. A rectifier allows current to flow in one direction but blocks current in the other direction. Or if you prefer it allows voltage in one polarity but won't allow a voltage in the opposite polarity. Which is also what a ReLU does.
How do I know if my Random Forest Regressor Model is overfitted?,"
Im creating a Random Forest Regressor Model with a small dataset (30 data points). I tried with other models but RF was the best one, however, after applying GridSearchCv I got that the training set is 0 and the test set is 0.12, and I dont know if that means that my model is overfitted.
Could anyone please help me with this?
Tks
","['regression', 'overfitting', 'random-forests']","Hi and welcome to StackExchange!First of all, your dataset is truly, extremely small. Maybe someone can correct me, but I would say 30 points is so small that using RandomForest is not appropriate.That aside, overfitting is when your test set performance is worse to training set performance, due to the model fitting itself to noise in the training set. In most cases, you will see SOME degree of this (test set performance worse than training set). However, the question is how much.In your case, you have basically a ""perfect score"" on your training set - this will basically ALWAYS signify some degree of overfitting (in the real world, there's pretty much no way a model can ever ""truly"" be getting a perfect score unless it's overfitting). This is confirmed by the considerably larger error in the test set.The question to you is: Yes, you have overfitting. But is this degree of overfitting acceptable? Is 0.12 error on test set good enough for you? If it is, then you don't need to worry too much. If not, you need to make some changes!If that's too much error for you, I would HIGHLY recommend using a simpler method (some form of regression, linear or logistic). Your dataset is too small for such complex algorithms to do well."
Does SAC perform better than PPO in sample-expensive tasks with discrete action spaces?,"
I am currently using Proximal Policy Optimization (PPO) to solve my RL task. However, after reading about Soft Actor-Critic (SAC) now I am unsure whether I should stick to PPO or switch to SAC. Moreover, from this post, it seems that much of the performance in the original PPO paper comes from code optimizations and not the novel clipped objective.
The main characteristics of my RL task are the following:

The action space is discrete. SAC was originally designed for continuous action spaces but, if I'm not wrong, it can be adapted to discrete action spaces with no problem.
I am trying to learn a policy for generating synthetic data (i.e., generating novel graphs), so diversity is key. For this reason, I want to learn a policy with as much entropy as possible (while still solving the task). Both PPO and SAC try to maximize the policy entropy.
Obtaining trajectories to train the policy is very expensive. My algorithm spends much more time obtaining the trajectories than training the deep neural network of the policy. Here, I think SAC is the clear winner, as it is off-policy whereas PPO is on-policy. Still, PPO is supposed to be very sample-efficient.

Given my current needs, do you think it is worth it to switch to SAC instead of PPO?
","['reinforcement-learning', 'comparison', 'proximal-policy-optimization', 'soft-actor-critic']","First, both SAC and PPO are usable for continuous and discrete action spaces. However, in the case of discrete action spaces, SAC cost functions must be previously adapted. As explained in this Stable Baselines3 issue, its efficient implementation is not an easy task.Contrary to your hypotheses, off-policy algorithms as SAC are generally more sample-efficient than on-policy algorithms (i.e. PPO), which are generally sample-inefficient due to the data loss that occurs when updating its policy. However, if you're looking for faster training, PPO is your option.Moreover, as explained by Haarnoja et al. (2018), SAC allows to have a stochastic actor while being more optimal and sample efficient than on-policy methods such as A3C or PPO. It is also less sensible to hyperparameters than all these methods.Depending on the problem you are dealing with, this Reddit thread may provide you further indications. Finally, here you have a post with some additional information about DRL algorithms comparison.In conclusion: for a generic problem I would recommend using SAC as it has shown better performance in most problems. However, if in your case performance is not an aspect to evaluate and you value more the speed of training and lightness of implementation, I recommend PPO. Another option could be to use both and compare their performance on your specific problem, which could also be quite interesting. Finally, if you are open to more alternatives, don't rule out TD3 if none of the above convinces you!"
How to identify important features in data?,"
I have a couple opportunities to write a paper, or papers over some of the neural networks I have made.
I was wondering if there are anyways to figure out why the neural network classifies the data I have the way it does. As in, what features of the data the neural network is using to classify the data. The neural networks I'm using consists of ltsm layers, CNN layers, and fc layers.
I have thought about plotting the neural network at everything output, but this doesn't really help to much. Just because there are so many weights that go to each node in a layer it makes it very hard to determine what's happening. I could plot the bias's but I don't know how much influence they have over the weights.
Another thing I considered was adjusting the the values of the input data a little bit at a time and seeing where the classification changes. Which this would work to some extent, but wouldn't allow to me to get the full picture of what the neural network is doing.
So any suggestions on how to do this?
Or any papers that would be help over this topic!
","['neural-networks', 'convolutional-neural-networks', 'long-short-term-memory', 'deep-neural-networks']","I ended up using Guided backprop from TorchRay.
https://facebookresearch.github.io/TorchRay/attribution.html#module-torchray.attribution.guided_backprop"
What consequence would a polynomial time algorithm for SAT have on AGI?,"
$P$ vs $NP$ is a famous problem. We generally believe $P\neq NP$. However suppose there is a polynomial time algorithm of order say $O((n+m)^2)$ or $O((n+m)^3)$ (a low degree polynomial complexity with small hidden constants) for $n$ variable SAT problem in $m$ clauses, then what consequence would it have on $AI$ and machine learning? Would $AGI$ be any closer?
","['machine-learning', 'agi', 'computational-complexity']",
"$E_{\pi}[R_{t+1}|S_t=s,A_t=a] = E[R_{t+1}|S_t=s,A_t=a]$?","
I would like to solve the first question of Exercise 3.19 from Sutton and Barto:

Exercise 3.19 The value of an action, $q_{\pi}(s, a)$, depends on the expected next reward and
the expected sum of the remaining rewards. Again we can think of this in terms of a
small backup diagram, this one rooted at an action (state–action pair) and branching to
the possible next states:



Give the equation corresponding to this intuition and diagram for the action value,
$q_{\pi}(s, a)$, in terms of the expected next reward, $R_{t+1}$, and the expected next state value,
$v_{\pi}(S_{t+1})$, given that $S_t =s$ and $A_t =a$. This equation should include an expectation but
not one conditioned on following the policy.

I'm not sure how to write this equation so that it is not conditioned on $\pi$.  It's clear to me that
\begin{align*}
q_{\pi}(s, a) &= E_{\pi}[G_{t}|S_t=s,A_t=a]\\
& = E_{\pi}[R_{t+1} + \gamma G_{t+1}|S_t=s,A_t=a]\\
& = E_{\pi}[R_{t+1}|S_t=s,A_t=a] + E_{\pi}[\gamma G_{t+1}|S_t=s,A_t=a].
\end{align*}
The second term in the last expression  evaluates to (from Exercise 3.13) $$\sum_{s'}\gamma v_{\pi}(s')P(S_{t+1}=s'|A_t=a,S_t=s).$$
Therefore, I'm left with the first term which is $E_{\pi}[R_{t+1}|S_t=s,A_t=a]$.
Question: Can I write the term without the subscript as $$E_{\pi}[R_{t+1}|S_t=s,A_t=a] = E[R_{t+1}|S_t=s,A_t=a]?$$
My reasoning is that the expected reward, given we know what $A_t$ is, does not depend on $\pi$.
","['reinforcement-learning', 'markov-decision-process', 'rewards', 'sutton-barto', 'expectation']","Question: Can I write it without the subscript? So $$E_{\pi}[R_{t+1}|S_t=s,A_t=a] = E[R_{t+1}|S_t=s,A_t=a]$$Yes, your reasoning is sound, there is no need to condition the expectation on the policy, because the policy has no influence on the expected value of $R_{t+1}$ given that there is already a condition on $A_t=a$, and $a$ is provided as an argument.This only works because you have split off the expected reward to evaluate separately. Overall, the value of $q_\pi$ does depend on the policy, and this is shown by using $v_\pi$ in the second term."
"Why does mean episode reward during training differ dramatically from ""manual"" runs of the trained model on same data?","
I am training an RL agent, using PPO, on a time-series environment that comes from a tabular dataset. The possible scores during an episode goes from -1 to positive infinity (though realistically, I would never expect an agent to get an episode score higher than 2.5 or 3).
By the end of training (3 million time steps, ~1350 episodes), I can see that the agent has a ""mean episode reward"" of roughly 0.4 . For context, below is an image of the plotted of that mean episode reward over time.

So, then, my expectation is that when I take this trained model and run it on an environment consisting of the same data it was trained on, it should have a performance somewhere around 0.4 reward on average per episode.
However, this is not what I find. In my case, the fully trained agent's mean reward is -0.3 over 10 episodes (episode length equal to episode length during training) which collectively span the entirety of the training data.
Why is the fully trained agent, who was just reported to have +0.4 mean score per episode, performing so poorly when run manually through a for-loop on the same training data?
By ""manually through a for-loop"", I mean that I have a for-loop through which I feed the agent observations one at a time, call model.predict() to get an action and then enact that action etc.
","['comparison', 'deep-rl', 'proximal-policy-optimization', 'performance']","I have found the issue.Essentially, what appears to be happening is when I do my (only) 10 manual runs, I happen to get a bad sample of starting points.During training, for each episode, I randomly select a start point in the data so that the agent doesn't overfit to a single starting state. However, during my ""manual testing"", I just chose 10 chronological starting points to run episodes from. It appears that the 10 chronological starting points just happen to be inconvenient/bad, so the results aren't great.To verify this idea, I ran 500 ""manual for-loop tests"" of the fully trained model by randomly selecting a starting point each time (just like I did during training). When I did this, the mean result across the 500 runs was quite similar to the final ""episode mean reward"" of my agent during training.So, it appears to come down to the ""luck of the draw"" when it comes to the episode starting points."
What is the intuition about the fact that a set of images will live on a three dimensional manifold?,"
The following question refers to a dissertation in Bishop's book (see the attachment)
Can someone give me an intuition about the fact that a set of images will live on a three dimensional manifold? I've understood the general context and that each point in the space corresponds to an image. But how does the author come up with the notion of three dimensional manifold?

","['machine-learning', 'image-processing', 'curse-of-dimensionality']",
How can I demonstrate a novel ML classification algorithm has value?,"
I designed a ML classification algorithm that's simple, efficient, and effective. It's not perfect, but seems to be widely applicable across domains.
I'd like to submit it for publication, but I don't know how to present my case.
What does an ML research paper need to succeed through peer review to be published? Further, what does it need to be noticed by the ML community as a whole?
","['neural-networks', 'machine-learning', 'classification', 'research']",
Is it possible for AlphaGo Zero to use recurrent networks to achieve similar performance?,"
AlphaGo Zero stacks 7 board history along with the current board together to form the input to the network. However, is it possible to use an RNN to replace the input of history and achieve similar performance? Are there any practical reasons for the team to choose stacking history states rather than using a RNN?
","['reinforcement-learning', 'deep-rl', 'recurrent-neural-networks', 'alphago-zero', 'alphago']",
Is there a standardized method to train a reinforcement learning NN by demonstration?,"
I'm less familiar with reinforcement learning compared to other neural network learning approaches, so I'm unaware of anything exactly like what I want for an approach. I'm wondering if there are any ways to train a Deep-Q neural network on, say, OpenAI Gym, where the model is given a recorded demonstration to learn from. That is, I'd like to do the following (with a Mario NES example):

Play the first level of Mario and somehow record this (maybe as an input sequence?)
Run the model on the level a few times until it passes
Send the model to the next level, let it train for as long as possible unless it continues to fail -- if so, I'll play the level and then let it train again
Repeat

Are there any approaches similar to this that currently exist? And, would this be infeasible because the model may fail to generalize? I'd like to accomplish this with much more complicated games, but if I could use this approach to save on defining endless amounts of reward/penalty policies and ROM hacking to find the memory address of everything I want the model to use, it would be extremely helpful.
","['neural-networks', 'reinforcement-learning', 'reference-request', 'algorithm-request', 'imitation-learning']","Yes, this is known as imitation learning, which can be divided intoI don't know the state-of-the-art (I am not an expert in this topic) or whether IL is a good approach in your case, but you can check e.g. the pre-print survey Imitation Learning: Progress, Taxonomies and Opportunities (2021) by Boyuan Zheng et al., which seems to nicely describe the topic."
Why does the coding layer in a VAE have a range of values?,"
While reading the book, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, I read that VAEs using a sampling technique to obtain values from the coding layer. However, the output of a neural network activation is just a single value. In that case, what values does the VAE sample?
",['variational-autoencoder'],"The VAE's encoder is usually implemented to produce the mean and variance. These are vectors, which can also be 1-dimensional (equivalently, scalars). If that's the case, then the latent vector is also 1-dimensional (if I understand your question correctly). See this implementation, where the size of the mean and variance vector is $20$, so the size of $z$ is also $20$, but you can change this to $1$.(Actually, the (co)variance may not be a vector but a matrix, but I assume a diagonal covariance, like in the linked implementation and the VAE paper)."
"Does LSTM provide any unique value or advantages compared to other algorithms, including ""vanilla"" RNN?","
I have heard a lot of hype around LSTM for all kinds of time-series based applications including NLP. Despite this, I haven't seen many (if any) applications of LSTM where LSTM performs uniquely well compared to other type of deep learning, including more vanilla RNN.
Are there any examples where LSTM does significantly better on a particular task, compared to other modern algorithms and architectures?
","['deep-learning', 'reference-request', 'long-short-term-memory', 'applications']","LSTMs were the state-of-the-art (SOTA) in many cases (e.g. machine translation) until transformers came along - now I don't really know the SOTA or where LSTMs still perform better than e.g. transformers. LSTMs were introduced to solve the vanishing and exploding gradient problems. Even the LSTM paper tells you thatIn comparisons with RTRL, BPTT, Recurrent Cascade-Correlation,
Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, articial long time lag tasks that have never been solved by previous recurrent network algorithms.For a specific case where LSTM achieved SOTA (if I remember correctly), you can check the neural machine translation paper. Google used LSTMs for some time in Google Translate. See this paper for more details."
How does a VAE sample the coding layer?,"
I am reading the book, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems and came across the following paragraph -

You can recognize the basic structure of all autoencoders, with an encoder followed by a decoder (in this example, they both have two hidden layers), but there is a twist: instead of directly producing a coding for a given input, the encoder produces a mean coding μ and a standard deviation σ. The actual coding is then sampled randomly from a Gaussian distribution with mean μ and standard deviation σ. After that the decoder decodes the sampled coding normally.

I do not understand how sampling is conducted over here and have the following question -
I understand it is trivial to sample from a univariate Gaussian distribution. However, for a dataset with n features, we won't be able to use a univariate Gaussian. How does that work?
",['variational-autoencoder'],"The number of features $n$ of an input $\mathbf{x} \in \mathbb{R}^n$ to the VAE may be related to the dimension $m$ of the latent vector $\mathbf{z} \in \mathbb{R}^m \sim \mathcal{N}(\mu, \sigma^2)$, but the formulation of the VAE does not go far enough to tell us how these parameters are related. The only assumption in the VAE is that $\mathbf{x}$ depends on $\mathbf{z}$. Here's the diagram (from the VAE paper) that illustrates the graphical model they considerIn practice, we may not use a univariate Gaussian, but most likely we use a multivariate Gaussian, i.e. $m > 1$, but $m$ is really a hyper-parameter, so it could also be $1$. Moreover, note that we don't sample $\mathbf{z}$ directly from $\mathcal{N}$, if we use the re-parametrization trick. See this implementation, where $\epsilon$ is actually sampled. $\epsilon$ is just a sample from a standard normal (usually), which we use (along with the learnable mean and variance) to construct $\mathbf{z}$."
"How to teach DRL agent to avoid erratic, high frequency behavior (when costs for taking actions are already included)?","
I have a situation where an agent can take actions to enter a state where rewards can be obtained (or costs can be incurred), but the actions themselves have a small cost. In my reward function, I give the negative rewards immediately when the agent takes the action.
As a result, if the agent takes these actions extremely frequently, the small, repeated costs of the actions outweigh the rewards gained during the very short periods between actions, and the agent does poorly. For some reason, my agent doesn't seem to be able to understand this. It acts erratically, taking actions at extremely high frequency.
I have tried giving it a sense of ""frequency"" by having a state variable that increments by 0.02 for every time step that the agent is in the state where it can earn rewards, therefore giving it a direct measure of ""how long I've been in this state"". This did not remove the erratic behavior.
Is there some other way I can give it a sense of ""frequency""? Why is the agent unable to grasp that it shouldn't keep ""flip-flopping"" and incurring these costs?
EDIT: From @NeilSlater's comment, adding more info.

Yes, the agent can just ""wait"" with no associated cost. There are two possible states (one in which it just sits there and nothing happens, and the other in which it can earn rewards or incur costs, depending on various factors). There are 2 possible actions - the agent can either 'stay in current state' (meaning no costs/rewards in the ""inactive"" state, and costs/rewards in 'active' state), or it can move between states, which incurs a small cost.
My environment is not naturally episodic. I chose a somewhat arbitrary episode length to have some end.
I'm using a discount factor (gamma) on the order of 0.9 to 0.975 . Changing it within that range doesn't appear to make a huge difference (though I can investigate this more)
I'm not very familiar with the technical definitions of MDP vs. POMDP, but given that there is a huge stochasticity in my observation space, I think POMDP might be correct. Nonetheless, the costs associated with moving between states are completely consistent, so I don't know how to understand this behavior.

","['deep-rl', 'reward-design']",
How to tackle the human error made in labeling datasets for classification tasks like facial expression recognition?,"
I am working on the Facial Expression Recognition Task. One of the most challenging tasks that I faced was human error in labeling the datasets (ex: let's say FER2013).
Are there anyways to Handle incorrect labeling of datasets in the classification tasks? Will using clustering methods and treating it as an unsupervised Learning problem (just a thought not have a clear idea) solve this kind of issue? or are any other methods available without treating it as an unsupervised task?
","['neural-networks', 'deep-learning', 'facial-recognition', 'clustering', 'data-science']","In general the only way to deal with this is by quantifying these labeling mistakes in the output of the model, since the model will learn for them. And in many cases these are not really mistakes, but they encode the ambiguity of the task, particularly in interpreting facial emotions.For this you need to use a model that can estimate Aleatoric uncertainty (this is data uncertainty). For classification you can use the cross-entropy loss combined with the sampling softmax function, which replaces a standard Dense layer with one that computes a Gaussian distribution for the softmax logits (so it predicts mean logit and variance logits).This method is described in the paper ""What uncertainties do we need in bayesian deep learning for computer vision?"" (link), where the authors can also disentangle aleatoric from epistemic uncertainty.I have run experiments exactly on the facial emotion recognition task (using the FER+ dataset), which you can find in my paper ""A Deeper Look into Aleatoric and Epistemic Uncertainty Disentanglement"" (link). In this paper I show multiple uncertainty quantification methods and their ability to produce disentangled probabilities. If you only need aleatoric uncertainty, then only the sampling softmax is needed.I even have an implementation of this layer in my keras-uncertainty library."
Can the output layer be connected to multiple layers?,"
Normaly, the output layer is only connected to the second last layer.
Is there any model that the output layer is connected to multiple layers (For example, the second last layer AND the layer before it.)
","['neural-networks', 'machine-learning', 'model-request']",
"CS 285 Prof Sergey Levine Lecture, Bounding Derivation for Reinforcement Learning (TRPO)","
How can we derive the final result? I can understand the first line, but don't know how the absolute term in the summation is replaced with $2\epsilon t$.
https://www.youtube.com/watch?v=LtAt5M_a0dI&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=45

I agree with the
$$\sum_{s_t}p_{\theta'}(s_t)f(s_t) \geq \sum_{s_t}\big[p_{\theta}(s_t)f(s_t) - |p_{\theta}(s_t) - p_{\theta'}(s_t)|max_{s_t}f(s_t)\big]$$
$$\because \sum_{s_t}p_{\theta}(s_t)f(s_t) = \sum_{s_t}\big(p_{\theta}(s_t) + p_{\theta'}(s_t) - p_{\theta'}(s_t) \big)f(s_t)$$
$$= \sum_{s_t}p_{\theta'}(s_t)f(s_t) + \big(p_{\theta}(s_t) - p_{\theta'}(s_t) \big)f(s_t)$$
$$ \leq \sum_{s_t}p_{\theta'}(s_t)f(s_t) + \big|p_{\theta}(s_t) - p_{\theta'}(s_t) \big|f(s_t)$$
$$ \leq \sum_{s_t}p_{\theta'}(s_t)f(s_t) + \big|p_{\theta}(s_t) - p_{\theta'}(s_t) \big|max_{s_t}f(s_t)$$
so,
$$\sum_{s_t}p_{\theta'}(s_t)f(s_t) \geq \sum_{s_t}\big[p_{\theta}(s_t)f(s_t) - |p_{\theta}(s_t) - p_{\theta'}(s_t)|max_{s_t}f(s_t)\big] \geq \sum_{s_t}\big[p_{\theta}(s_t)f(s_t) - 2\epsilon t max_{s_t}f(s_t)\big] = E_{p_{\theta}(s_t)}[f(s_t)] - \sum_{s_t}2\epsilon t max_{s_t}f(s_t)$$
Here is the question. How does the lecture note remove the summation of the epsilon related term? The professor sasy, since all of them are constant (I understand $max_{s_t}f(s_t)$ is also a constant), we can write like that, but $\sum_{t=1}^{10}(t - 1) = 45 = \sum_{t=1}^{10}t - \sum_{t=1}^{10}1 = 55 - 10 = 45$. We don't write $\sum_{t=1}^{10}1 = 1$ because 1 is a constant.
The abiguous part is there is no concrete size of $s_t$ in the summation, but at least I think just removing the summation in front of the epsilon term is weird.
","['reinforcement-learning', 'machine-learning', 'trust-region-policy-optimization', 'total-variational-distance']",
How should I compare multiple machine learning models to be (generally) fair to all models?,"
I am testing multiple models on IBM HR Analytics Attrition Dataset (1470 lines) and HR Analytics dataset (15000 lines) for a research project. The models include traditional models (Naive Bayes, SVM), tree-based models (Random Forest, XGBoost, CatBoost), and recently-developed deep models (TabNet, FTTransformer).
Since there are many models, the first strategy I thought of to compare multiple models (to be fair for them) is to use the default hyperparameters (set by all the corresponding libraries) for all those models. But the performance on the first dataset is not good (as expected when using the default), so I need to tune hyperparameters in those models. The problem here is that different models have different types of hyperparameters, and I don't know if a fair strategy exists to fine-tune and compare the models?
","['machine-learning', 'hyperparameter-optimization', 'models']",
Why do we subtract logsumexp from the outputs of this neural network?,"
I'm trying to understand this tutorial for Jax.
Here's an excerpt. It's for a neural net that is designed to classify MNIST images:
from jax.scipy.special import logsumexp

def relu(x):
  return jnp.maximum(0, x)

def predict(params, image):
  # per-example predictions
  activations = image
  for w, b in params[:-1]:
    outputs = jnp.dot(w, activations) + b
    activations = relu(outputs)
  
  final_w, final_b = params[-1]
  logits = jnp.dot(final_w, activations) + final_b
  return logits - logsumexp(logits)

I don't understand why they would subtract a constant value from all the final predictions, given that the only thing that matters is their relative values.
For clarity, here's the loss function:
def loss(params, images, targets):
  preds = batched_predict(params, images)
  return -jnp.mean(preds * targets)

","['neural-networks', 'machine-learning', 'loss', 'mnist']",
What reinforcement learning architecture is recommended for multiple outputs in continuous resource management?,"
I would like to develop an agent to provide resources to multiple machines simultaneously. The overall resources are limited. The agent should distribute the resources in such a way that the machines finish their task as early as possible and switch to standby mode, i.e. the agent returns with each prediction which machines get how many resources. Additionally, the total resources per time interval should be used up as much as possible. Furthermore, a machine has a consumption interval. Resources below the consumption interval are ignored and resources that exceed the consumption interval are reduced to the maximum possible absorption.
Can anyone guide me on how to design such an agent and what topics I can research further?
","['reinforcement-learning', 'deep-rl', 'ai-design']",
What is the exact role of model $p_\theta$ in diffusion models for the reverse process?,"
I'm reading this interesting blog post explaining diffusion probabilistic models and trying to understand the following.
In order to compute the reverse process, we need to consider the posterior distribution $q(\textbf{x}_{t-1} | \textbf{x}_t)$ which is said to be intractable*

because it needs to use the entire dataset and therefore we need to learn a model $p_\theta$ to approximate these conditional probabilities in order to run the reverse diffusion process.

If we use Bayes theorem we have
$$q(\textbf{x}_{t-1} | \textbf{x}_t) = \frac{q(\textbf{x}_t |\textbf{x}_{t-1})q(\textbf{x}_{t-1})}{q(\textbf{x}_t)}$$
I understand that indeed we don't have any prior knowledge of $q(\textbf{x}_{t-1})$ or $q(\textbf{x}_t)$ since this would mean already having the distribution we are trying to estimate. Is this correct?
The above posterior becomes tractable when conditioned on $\textbf{x}_0$ and we obtain
$$q(\textbf{x}_{t-1} | \textbf{x}_t , \textbf{x}_0) = \mathcal{N}(\tilde{\bf{\mu}}(\textbf{x}_t , \textbf{x}_0) \, , \, \tilde{\beta}_t \textbf{I})$$
So, apparently, we obtain a posterior that can be calculated in closed form when we condition on the original data $\textbf{x}_0$. At this point, I don't understand the role of the model $p_\theta$ : why do we need to tune the parameters of a model if we can already obtain our posterior?
","['generative-model', 'image-generation']",
Do NNs suffer from lack of efficiency in network structure and suggesting training parameters?,"
I am working on dynamical systems using Optimal Control theory and trying to find the connection between this field and Machine Learning. Consider a simple 2-layer Neural Network (NN) where the activation function is considered as $y = x+x^2 $ (I intentionally ignored bias term and supposed this activation function only for illustration purpose). So, if the output of the first layer is calculated as $y_1=w_ix_i+w_i^2x_i^2$, then the input to output mathematical relation can be given as:
$$y_o=w_o(w_1(w_ix_i+w_i^2x_i^2)+w_1^2(w_ix_i+w_i^2x_i^2)^2);$$
By expanding the terms, it results in:
$$y_o=w_ow_1^2x_i+w_o(w_1^3+w_1^4)x_i^2+2w_ow_1^5x_i^3+w_ow_1^6x_i^4.$$
But we know that there could be four parameters $\alpha_1,\alpha_2,\alpha_3,\alpha_4$ as
$$y_o=\alpha_1x_i+\alpha_2x_i^2+\alpha_3x_i^3+\alpha_4x_i^4$$
in the input to output relation, but the NN structure suggests only three parameters $w_i,w_1,w_o$ to be trained. So, the degrees of freedom is less than the actual potential of the given structure.
Will this lead to any inefficiency in NN model? Could you please suggest some references which mathematically studies the NN structure?
","['neural-networks', 'machine-learning', 'optimization', 'weights', 'efficiency']",
Using GraphSAGE model for multigraph datasets,"
I checked out applications of GraphSAGE and it seems like its primarily used for single graph datasets. For example - Cora dataset - Its one big graph with 2708 nodes and 5429 edges. The model can learn the node representation of this big graph and it can be later used for downstream tasks like node classification, link prediction.
I have a dataset with hundreds of graphs that are relatively small (about 15 nodes and 20 edges on avg per graph). Each graph represents a separate datapoint i.e, there is no relation between these graphs. It is similar to datasets like MUTAG, PROTEIN datasets . My question is if GraphSAGE is suitable for this kind of data ? Can it be used to learn node representation and be used for downstream tasks like node/graph/link classification?
I am following this stellargraph example - stellargraph_graphsage
It uses a sampler to perform random walks and solve a classification task. However, this sampler intakes only one Graph as input (big Cora graph). I am confused about how to use a dataset with multiple graphs here.
unsupervised_samples = UnsupervisedSampler(
    G, nodes=nodes, length=length, number_of_walks=number_of_walks
)

","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'graph-neural-networks']",
What is being optimized with WGAN loss? Is the generator maximizing or minimizing the critic value?,"
I am kind of new to the field of GANs and decided to develop a WGAN. All of the information online seems to be kind of contradicting itself. The more I read, the more I become confused, so I'm hoping y'all can clarify my misunderstanding with WGAN loss.
Critic loss function:

Generator loss function:

g_w are the parameters for the critic and g_θ are the parameters for the generator
From my understanding, the loss functions show that:

The critic wants to minimize its loss. Splitting the loss function up, this means it wants to:

minimize its score on real data
maximize its score on fake data


The generator wants to maximize the critic score on fake data. So it wants to make the data it generates seem more fake to the critic?

Since the critic gives a high score to fake data and a low score to real data, why would the generator want to maximize its score? Wouldn't that mean the generator wants to make its data appear more ""fake"" to the critic? I would think the generator would want to minimize its loss to make it look more real (since real data has a low score)
","['generative-adversarial-networks', 'loss', 'wasserstein-gan']",
How Can We Create Neural Networks with Different Depths and Widths But Same Number of Parameters?,"
Right now I am doing a research project investigating how the depth of a Neural Network affects its capacity to learn. In order to do this, I wanted to test different Networks with the same number of parameters but with different depths and widths. The solution to this problem gets reduced to a system of equations with integer solutions. Nonetheless, I haven't come up with an idea to solve it, even by using numerical methods. Therefore, I wanted to ask if there's someone that could help with finding a solution.
Context
A sequential Neural Network is defined by a number of $n$ matrices that correspond to the weights of each layer ($\{W_i\}_{i = 1,\dots, n}$). The shape of each consecutive matrix is directly related to the previous one (since they are composed after applying a non-linear function) such that:
$$
W_i \in \mathbb{R}^{a\times b} \Leftrightarrow W_{i+1} \in \mathbb{R}^{b\times c}
$$
where $a, b, c \in \mathbb{N}$.
On the other hand, the number of parameters of a Neural Network is the same as the sum of the total number of entries of each matrix $W_i$. Assuming that the number of neurons in the input layer and the number of neurons in the output layer remain constant (let's call them $a$ and $c\in \mathbb{N}$) and the number of neurons the same for each hidden layer (let's call it $b_i \in \mathbb{N}$ for a network with $i$ hidden layers) we have that we can calculate the number of parameters in the network. For example, for a network with one hidden layer, the number of parameters would be such that:
$$
p_1 = ab_1 + b_1c
$$
Then, if we add another hidden layer, the number of parameters would be such that:
$$
p_2 = ab_2 + b_2b_2 + b_2c \\
= ab_2 + b_2^2 + b_2c
$$
Therefore, for the general case with $m$ hidden layers the number of parameters is defined by the equation:
$$
p_m = ab_m + \sum_{i=1}^{m}{b_m^2} + b_mc \\
= ab_m + mb_m^2 + b_mc \\
= \boxed{b_m (a + mb_m + c)}
$$
Question
In my case, since I want to create $m$ neural networks ($5 \leq m \leq 12$ would be enough for the purposes of my research) I would need to find a solution to find the $b_i$'s $\in \mathbb{N}$ such that all of the $m$ networks have the same number of parameters $P$, where $P$ can be any positive integer (hopefully the smallest one where the system has solutions).
$$
\begin{cases}
      P = ab_1 + b_1^2 + cb_1\\
      P = ab_2 + 2b_2^2 + cb_2 \\
      \dots \\
      P = ab_m + mb_m^2 + cb_m \\
\end{cases} 
$$
$\Leftrightarrow$
$$
\begin{cases}
      P = b_1 (a + b_1 + c)\\
      P = b_2 (a + 2b_2 + c) \\
      \dots \\
      P = b_m (a + mb_m + c) \\
\end{cases} 
$$
Is there a closed-form solution for this system of equations in the positive integers? That is, a solution such that we could express the set of $b_i$'s in terms of $a, c,$ and $m$ where $b_i \in \mathbb{N}, \forall i \in \{1, \dots, m\}$?
I would really appreciate any help. Thank you all in advance.
Edit: Relaxing the constraints for the number coefficients to be in a band of 1% or 5% of $P$ would serve too as a solution for the problem. Regarding relaxing the number of neurons in the hidden layers to allow them to have a different number for the different layers, I think it would be useful for the project if the difference between the number of neurons in each layer does not differ to a great extent.
","['neural-networks', 'machine-learning', 'deep-learning', 'deep-neural-networks', 'architecture']",
Today's Practicality of Bayesian Neural Networks,"
Just having heard lately about BNNs (wow, ANNs and CNNs are clear; now there's a B? What's that? Ahh, Bayesian ;-)) and quickly getting their main idea and focus, that is, weights not being pure static numbers anymore, but instead described by a distribution function, allowing the net to be ""uncertain"" about its answers (for more about BNNs, see, for example, this post, or, if you want to know a lot about them, check out this introduction and survey),
I'm now curious about any practical examples of where BNNs are used nowadays, maybe even more successful than other (deep learning) alternatives?
Thanks to nbro's comment here, pointing out that asking this question is not unknowingly, but relevant.
","['reference-request', 'applications', 'state-of-the-art', 'bayesian-deep-learning', 'bayesian-neural-networks']",
Why and when do we need to normalize weights in Reinforcement Learning?,"
I recently came across this SO question, wherein the poster was asked to normalize their weights while using a function approximator with SARSA. I don't remember having to normalize any weights while using a DQN and so therefore would like to when and why is this method needed.
","['reinforcement-learning', 'weights', 'weight-normalization']","The kind of divergence that the other question experienced is a common problem with deep RL and temporal difference methods (Q-learning, SARSA, or any Actor Critic).The weight normalisation would not be needed if the asker of the question you linked had used some kind of protection against divergence, such as using a separate target network. The normalisation may have worked for them as an alternative.DQN already has experience replay and separate target network to help with this. If you do get runaway feedback causing diverging values, you already have some hyperparameters you can change to try and fix it - size of minibatch, size of replay table, number of updates between copying to target network.So, in short, you don't really need weight normalisation in Deep RL. You might want to use it for regularisation though."
How to properly name given type of classification problem?,"
What is the proper technical name of the classification problem where each data sample can be classified according to two different criteria and each of them can have two or more classes?
For example age/gender estimation problem where age is one criterion, gender is second.
Age can be divided into 4 age groups and gender into 2. And model should generated estimate of both age and gender for every sample.
","['machine-learning', 'deep-learning', 'classification']",
What object detection algorithm is the best for my particular problem?,"
I am trying to write a program to put a bounding box around dead fish, and not the live ones, in a video. I have minimal data (~5k frames and ~7k objects in total ) and it is VERY low quality (poor resolution, very exposed, lots of partially obscured fish, etc).
I understand there may not be a ""Best Object Detection Algorithm"" but for my specific problem, can anyone recommend one? Accuracy is key, speed is secondary.
I have tried YOLOv5 with less than adequate results and I am currently looking into using EfficientDet.
","['machine-learning', 'object-detection', 'algorithm-request', 'yolo']",
What happens if MIN plays suboptimally and unpredictably?,"
The following quotes are an extract from AIMA, 3ed.

The definition of optimal play for MAX assumes that MIN also plays optimally—it maximizes the worst-case outcome for MAX. What if MIN does not play optimally? Then it is easy to show (...) that MAX will do even better.


Consider a MIN node whose children are terminal nodes. If MIN plays suboptimally,
then the value of the node is greater than or equal to the value it would have if MIN played optimally. Hence, the value of the MAX node that is the MIN node’s parent can only be increased. This argument can be extended by a simple induction all the way to the root. If the suboptimal play by MIN is predictable, then one can do better than a minimax strategy.
For example, if MIN always falls for a certain kind of trap and loses, then setting the trap guarantees a win even if there is actually a devastating response for MIN.

What happens if the MIN plays both suboptimally and unpredictably? What happens if MIN plays randomly, for example?
Is it ok to say that MAX will do better playing against a suboptimal and unpredictable MIN than playing against an optimal MIN?
","['minimax', 'norvig-russell']",
Determining a policy to play a game of chance,"
I'm trying to optimize the expected return from a game of chance, but have quickly realized the problem outclasses the introductory AI course I took in college years ago.  I would appreciate any guidance into what algorithms might be relevant to this type of game.
Game Description
The player has 31 tiles that they can flip over and reveal a hidden color. There are 6 blue tiles, 4 red tiles, and possibly one gold tile.  (The rest are blank.)  The player is only granted 11 attempts to turn over tiles.  Turning over all tiles of a single color results in a reward (a different reward for each color).  Uncovering no complete sets of tiles results in 0 reward.  Due to illustrations on the sets of tiles, revealing a single red or blue tile will also inform the player of the locations of the other tiles of the same color. (i.e. if I flip over one red tile, I know which other three tiles would be red if I were to flip them over too)
The distribution of colors across these 31 tiles is not uniform (but can be assumed as known).  Further, there are only ~100 possible tile color configurations, and these configurations have been discovered by the players of this game.
(Concretely, the game is Faux Hollows from the video game FFXIV; the above description merely removes the flavortext.)
My object is maximize the expected reward over multiple plays of the game; I want to have a program that, given a board state, tells me the optimal tile to flip to get that reward.
What I've considered
My initial thought was to treat this as a Markov Decision Process, since we are changing state, receiving some value/reward based on state change, and I want a policy to maximize that expected reward.  However, the only way I could think to model the state space (i.e. state = which tiles have been flipped and what color they are) results in literal billions of states ($\sum_{1\leq k\leq11}\binom{31}{k}$), which is (AFIAK) intractable for classical solvers of MPDs.
I thought maybe Reinforcement Learning could help, but I'm not already deeply familiar with it.  My cursory glance at q-learning suggested that that high-cardinality state spaces are handled by approximating the value function, but --- because the game has very few states that give rewards --- I worry about how well approximating the value function will work.  (Also, I'm not sure how I would incorporate the 11 tile-flip limit.)
I also considered using a decision tree to determine the board configuration that currently is selected, but this doesn't optimize for reward at all; it merely finds the fewest moves that would indicate which color configuration I have.
Could I have some recommendations of algorithms that I should learn/consider for use in finding an optimal strategy to this game?
","['markov-decision-process', 'algorithm-request', 'games-of-chance']",
Why are SVMs / Softmax classifiers considered linear while neural networks are non-linear?,"
My understanding is that neural networks are definitely not linear classifiers, as the point of functions like ReLU is to introduce non-linearity.
However, here's where my understanding starts to break down. A classifier, like Softmax or SVM is considered to be a ""linear classifier"". I'm using the definitions from CS 231N. There may be another definition of an SVM, but I'm not considering that.
In a linear classifier we have the following: $W \cdot x$ where

$W$ is the weights
$x$ is the input vector.

The output is a vector of scores. 1 score per label.
An SVM classifier uses hinge loss to update the weights:

A softmax classifier normalizes the output values using the softmax function and then uses cross-entropy loss to update the weights:

From the lecture CS231n Winter 2016: Lecture 2: Data-driven approach, kNN, Linear Classification 1, we have the following image to help visualize what a linear classifier does:

Essentially, each image can be considered a point in 3072 dimensional space, and we are drawing lines through this space. On one side of the ""car"" line, the car score for a given point will go up. On the other side of the ""car"" line, the car score for a given point will go down.
However, this doesn't seem to be that much different from ReLU (Taken from this post: https://stats.stackexchange.com/questions/158549/why-are-activation-functions-needed-in-neural-networks):

So what is the fundamental difference between a ""linear"" classifier like Softmax / SVM and a multi-layer neural network? Why can't a SVM classifier learn any function but a neural network can?
","['neural-networks', 'machine-learning', 'activation-functions', 'support-vector-machine', 'softmax']","I was confused because the images look similar even though in reality the problems the 2 images are solving are completely different:Consider a single class in the linear classifier. The linear classifier can only draw a single line through $N$ dimensional space. Items on one side of this line have negative car scores. Items on the other side have positive car scores.For example:
Here, the 2nd image in the row represents the SVM's attempt to extract all the features of a car into a single image. As images are ""closer"" to this image, the SVM will give them a higher car score. As images are farther away from this image, the SVM will give them a lower car score.The SVM cannot extract individual features, it has to smush them all into a single line in $D$ dimensional space (where $D$ is the number of pixels). That line can be represented as:On the other hand. A multi-layer neural network, for example:
Can now partition the space into a 100 different ways. The example is a bit convoluted,  since the neural net is classifying 1 thing into several buckets instead of doing binary classification, but the principle is the same. Maybe 10 out of a 100 neurons in the second layer are dedicated to partitioning the space of images with regard to cars."
Why can't I reproduce my results in keras using random seed? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I was doing a task using RNN to predict a time series movement.
I want to make my results reproducible. So I strictly followed this post:
https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras
My code are as follows:
# Seed value
# Apparently you may use different seed values at each stage
seed_value= 0

# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value
import os
os.environ['PYTHONHASHSEED']=str(seed_value)

# 2. Set the `python` built-in pseudo-random generator at a fixed value
import random
random.seed(seed_value)

# 3. Set the `numpy` pseudo-random generator at a fixed value
import numpy as np
np.random.seed(seed_value)

tf.compat.v1.set_random_seed(seed_value)

tf.random.set_seed(seed_value)

# 5. Configure a new global `tensorflow` session

# for later versions:
session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
tf.compat.v1.keras.backend.set_session(sess)

However, every time I ran my codes, I still got a different result, what could the reasons be?
","['deep-learning', 'recurrent-neural-networks', 'tensorflow', 'keras', 'time-series']",
When are traditional image processing methods preferable to machine learning and why?,"
By traditional image processing I understand, e. g. using filters to improve the image, extracting edges and then classifying objects using template matching.
My current decision criteria are:

large possible changes from picture to picture (e. g. lighting, angle of view, …) -> ML
more complex problems (e. g. weld inspection) -> ML
difficult generation of a labeled image dataset -> traditional approach
traceability of the decisions -> traditional approach

Are there other general decision-making criteria that lead to a specific choice (especially if both variants would work for a given application)?
","['machine-learning', 'comparison', 'computer-vision', 'image-processing']",
What model can solve vector to vector prediction?,"
I am totally newbie into serial prediction.
I am think about which model or AI paradigm can be used to do vector to vector prediction?
For instance, [1,0,1] ^ [0,1,0] = [1,1,1]
Another example could be: [1,0,1]^[0,1,0]^[1,1,0]^... = [RESULT]
(I have to say that this example is not proper enough, because obviously we can use a multi-layer model to simply learn the rule of XOR operation with 100% accuracy, but I think it still can indicate what I want. The point is multi-dimensional input and figure out a non-linear rule. Let's say each row vector can be a row of a matrix, I want to find some pattern across each column entries.)
Another better example:
[[1,2,3], [4,5,6], [7,8,9]] => [10,3,8]
I know seq2seq model, to some extend can do this job, so as pointer network. But I am just skeptical on my knowledge in this RNN area. And I do need some help.
For the record, I think what I want is different from stock prediction or word prediction or sentence classification. My argue is that these tasks will need an embedding layer to output a one-hot or whatever vectorization and feed the embedding to the model.
But my original input has already been vectors. I did try to feed those vectors as an embedding to the model. But theoretically, it does not make sense to me. And it did not work under my practice.
Please enlighten me.
Yes, I dare to share my code. Please be easy on me.
    # model structure
    model = keras.Sequential()
    model.add(Input(shape=(node_size, node_size)))  # seq, input_dim
    # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)
    model.add(LSTM(1024, return_sequences=True, activation=""relu""))
    # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)
    model.add(Dropout(0.5))
    model.add(LSTM(node_size, activation=""relu""))
    model.add(Dropout(0.5))
    # model.add(Dense(128, activation=""relu""))
    # model.add(Dropout(0.1))
    model.add(Dense(node_size, activation=""sigmoid""))

    # build model
    opt = keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=opt, loss=""binary_crossentropy"", metrics=[BinaryAccuracy()])
    model.summary()
    # A_input = A_input.reshape(1, node_size, node_size)
    # label = label.reshape(1, node_size)

    # train
    model.fit(A_input, label, epochs=200, batch_size=10, validation_split=0.3)
```

","['recurrent-neural-networks', 'time-series', 'prediction']",
What do we mean by the notation $\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$?,"
I was going through this VIT paper, what will it look like in torch , if we are trying to write this expression.

","['deep-learning', 'papers', 'math', 'notation', 'vision-transformer']",
How do I create an AI controller for Pacman?,"
How do I create an AI controller, which can play pacman - by taking in pixel values (or some other data by represents the state) which perhaps runs on a separate thread, which can control the game?
It takes the pixel values (or likewise) to learn, and play the game, just like a human does. Normally, AIs like enemies - are part of the game code, which learn and play by using the internal state of the game, just like the “enemy” AI.
How can I create this abstraction between the game and the agent?
The most obvious example would be : (I don’t know how viable this is) -
The agent takes in pixel values of the game - learns - and sends “keyboard” inputs to that thread just like a human would, to play the game.
Can someone point me in the right direction - as to how to do something like this?
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'q-learning']"," About the environments  
For the controller part of your question, I would advice looking at openAI gym.
https://www.gymlibrary.ml/content/environment_creation/ #how to make your own gym enviroment

Those gym enviroments work kinda like thisWhere your observation can be pixels,ram dumps, etc. The actions can be internally mapped to key presses. (but the internals don't really matter that much) Gym environments are just an easy way to then have your agent take in the observations and map them to actions. So your game can be stepped through so you don't have to worry about a complicated way to integrate the AI and keeping it synced.

There are already a bunch of artari games included (also ms pacman) https://www.gymlibrary.ml/environments/atari/About the AISince you want to learn directly from the pixels or other more complex data types I advice you to read this:Deep Reinforcement Learning, a textbook (arXiv:2201.02135)
https://arxiv.org/abs/2201.02135If you just want some simple tutorial just read ""Hands-On Reinforcement Learning with Python"", then you can try to implement something like DQN , with some cnn architecture. (similar to how they did in arXiv:1312.5602 ""Playing Atari with Deep Reinforcement Learning"").
Since DQN is model free, off policy and relatively easy to implement.
(cool thing about off policy is that your agent doesnt have to interact with the environment as it doesnt need to sample ""experience"" using only it's current policy (way of choosing actions given a state) so you can even play the game itself and collect information (a,s,s',r) while playing and train your agent on that (to nudge it in the right direction if it gets stuck))if you're not a fan of gradient descent based methods, you can also use methods like ES (evolutionary strategies). As that way you can directly optimize for a reward. (it's a genetic algorithm that scales well in both compute and dimensionality)  (arXiv:1703.03864 Evolution Strategies as a
Scalable Alternative to Reinforcement Learning)"
Number of possible joint policies in a Dec-POMDP and the time required to evaluate each one,"
I was reading a book about Dec-POMDPs and came across this curious result where the author specifies the number of possible joint policies to evaluate and the time needed to evaluate a single joint policy but I can understand how he got to these results.
Can anyone please help me understand the logic used here?
Source: ""Reinforcement Learning: State-of-the-Art"", Editors Dr. Marco Wiering &
Dr. ir. Martijn van Otterlo Radboud, DOI 10.1007/978-3-642-27645-3, Springer-Verlag Berlin Heidelberg 2012.
","['reinforcement-learning', 'markov-decision-process']",
"How to form 10 and 20 actions in corridor environment, in the paper ""Dueling Network Architectures for Deep Reinforcement Learning""? [closed]","







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am just reading a paper titled ""Dueling Network Architectures for Deep Reinforcement Learning"". In this paper, 4. Experiment (4.1 Policy evaluation), I just wonder how to form 5, 10, and 20 actions.
According to this paper, the 10 and 20 action variants are formed by adding no-ops to the original. I can't understand this... Does it mean that actions are available: go up, go down, left, right, no-op, no-op, no-op... and no-op?
Am I correct?
","['reinforcement-learning', 'dqn']",
Could a good poker-playing AI be made that didn't look at its own cards?,"
A bit ago, I found out that researchers had taught a machine to play Texas Hold'Em at a level that beat most champions. However, that AI had access to the information of what cards it was dealt.
So I got to wondering. If a poker-playing AI were made, where the AI did not know what its own cards were (that is, it never 'looked' at them), would it be possible for the AI to reach a similar level of success?
","['game-ai', 'poker', 'card-games']",
"Why are embeddings added, not concatenated?","
Let's consider the following example from BERT

I cannot understand why ""the input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings"". The thing is, these embeddings carry different types of information, so intuitively adding them together doesn't really make sense. I mean, you cannot add 2 meters to 3 kilograms, but you can make a tuple (2 meters, 3 kilograms), so I think it's more natural to concatenate these embedding together. By adding them together, we are assuming the information about token, segmentation, and position can be simultaneously represented in the same embedding space, but that sounds like a bold claim.
Other transformers, like ViTMAE, seem to follow the trend of adding position embeddings to other ""semantic"" embeddings. What's the rationale behind the practice?
","['neural-networks', 'deep-learning', 'transformer', 'attention', 'embeddings']",
Is the described Q-table considered large?,"
I never saw any rule of thumb as to what size is said as large for a q-table but I have a Q-table with like 2500 entries. Is it considered large for a tabular approach? Anyone from experience can answer maybe. I assume I can visit all state-action pairs enough times but is it efficient in terms of space complexity/memory? Or is a deep neural network required?
I asked a related connected question, in case anyone wants to answer this current question better w.r.t that question context.
","['reinforcement-learning', 'deep-learning', 'q-learning', 'hyper-parameters', 'state-spaces']",
How many training steps does it usually take to train an RL model?,"
This is my model average rewards as follow image.
How to tell if it is undertrained or not convergent? How many training steps does it usually take to train an RL model?
And I'm using PPO to train.

","['reinforcement-learning', 'deep-rl', 'training', 'hyper-parameters', 'proximal-policy-optimization']",
How to compare memory requirements for tabular Q-learning vs deep neural network?,"
I want to compare the space complexity/memory requirement of tabular Q-learning v.s. deep neural Q-network (DQN). I think DQN would be faster and Q-table has a disadvantage at large table sizes but consider the following case.
A Q-table has the size 14 states *169 actions= 2366 entries and (say) a fully connected DNN whose number of parameters comes out to be like >8000. Space complexity/memory-wise, isn't storing a look-up q-table of 2366 size better than storing 8000 parameters of neural net?  I never implemented a DNN before so no idea how much space neural net parameters take.
","['reinforcement-learning', 'deep-learning', 'deep-rl', 'q-learning', 'deep-neural-networks']","You don't say, but I suspect from your description, that you have designed the neural network to operate over one-hot-encoding representations of states and actions. Using such a representation offers no benefit whatsoever compared to a simple table. That is because the intended benefit of using neural networks, or any kind of approximation, is generalisation.It is not possible to generalise results between states and actions, if those states and actions are simply enumerated. If you represent $\mathcal{S}$ as $\{s_0, s_1, s_2 ... s_{13}\}$ and $\mathcal{A}$ as $\{a_0, a_1, a_2 ... a_{168}\}$ to any function, then the best generalisation that can be made are only based on the mean of $Q(s, a)$ for all states or all actions. You cannot use experience gained for $Q(s_0, a_0)$ to say anything about expected return for $Q(s_1, a_1)$. That is true even if $s_0$ and $s_1$ are similar in some way - the representation does not capture that similarity, so the approximator cannot use it.State and action representation are very important details when adding approximation to reinforcement learning.To gain generalisation, an approximation scheme needs to have feature data that encapsulates how elements in the space are similar or different to each other. For example, if the 169 actions are arranged on a 13x13 grid, then each action can be represented as a 2-element vector. This will work for approximation if actions that are close to each other in this 13x13 space usually have similar expected returns.Space complexity/memory-wise, isn't storing a look-up q-table of 2366 size better than storing 8000 parameters of neural net?Yes, typically neural network parameters are 32-bit floats, which would also a suitable storage type for the q table values.It is not common to have a neural network use more space to store its parameters than would be required to fully describe the entire function domain that it is approximating. So you are right to find it unwanted/unexpected. The answer to this puzzle is likely to be in your choices for state and action representation."
What is the difference between the triplet loss and the contrastive loss?,"
What is the difference between the triplet loss and the contrastive loss?
They look same to me. I don't understand the nuances between the two. I have the following queries:

When to use what?
What are the use cases and advantages or disadvantages of the two?
Also, how do they fit with the siamese network discussion?

","['comparison', 'objective-functions', 'siamese-neural-network', 'contrastive-learning', 'triplet-loss-function']",
Why is it difficult to train large RL networks?,"
First of all I know that: 'it makes training less stable' & 'RL is already inherently unstable'. I'm asking why those things are true?
Intuitively it seems very strange & to be perhaps a fundamental weakness of RL that it
very ineffective at learning large networks. Supervised learning usually has no problem learning features from unstructured data via potentially quite large networks (e.g. upwards of 8+ layers). But in the case of RL it is a very big problem it seems (that is learning the extra embedding layers in the front of the network 'head').
P.S. in case you don't believe me here is a plot of hyper-parameter search data I collected on Acrobot-v1, CartPole, MountainCar-v0, and Pendulum-v0.
Also it is not hard to find evidence for this online... Just look at stable-baselines3 it has default num_layers=2...

","['reinforcement-learning', 'deep-learning', 'deep-rl']",
Does rule-based image processing count as AI?,"
I'm quite new to the field of AI and I currently find it hard to precisely inclose the broad field of AI. Especially the aspect of image processing isn't quite clear to me.
So far I've done rule-based approaches with image processing, e.g. things like template matching or blob detection combined with fixed thresholds and If-then-else-statements which define different classes.
Do rule-based systems like this one belong to the field of AI or is this still part of simple image (pre-)processing and therefore no ""artificial intelligence""?
","['computer-vision', 'image-processing', 'ai-field', 'rule-based-systems', 'template-matching']","Whether template matching algorithms (and other image processing or computer vision techniques) are part of the AI field or not might be a little bit subjective. I don't know if there is a consensus on this topic. It probably depends on whether people consider these approaches intelligent or useful to create intelligent agents/systems, and that depends on the definition of intelligence. However, I think that most would agree that CV is fundamental to create AGI (e.g. most animals have a vision system).Having said that, image processing techniques, including template matching, are still studied in the context of AI. For example, during my MSc in AI, I had a Computer Vision course, where we studied template matching, in addition to other more modern techniques (e.g. CNNs). If you look at a standard textbook on AI, e.g. AIMA, you will see that it contains sections on CV and image processing (chapter 24, 3rd edition). This book, which covers the history of AI, by Nils J. Nilsson (co-author of STRIPS and A*), mentions a few image processing techniques. So, CV and image processing have traditionally been studied in AI. Moreover, although one may think that IP algorithms are ""not so intelligent"", they are useful for CV, which is fundamental to create general AI systems. So, I think it's reasonable to consider both CV and IP techniques (whether they are rule-based or not) part of the AI field."
Is 3D pose prediction directly from point clouds commonplace?,"
I've spent a couple of days experimenting and trying to find papers on learning 3D pose directly from unordered point clouds with no color information. This paper from 2020 claims to be the first that does it. The results are not encouraging for my use case and I'm not sure I agree with their methods (I tried it for my problem and got poor results).
Overall I'm surprised that 3D pose regression with point clouds would not be commonplace. Have a missed a whole section of the literature? Is there some fundamental reason why this hasn't been figured out yet?
",['deep-learning'],
How to model the probability of click-through in bandit problem?,"
Consider a bandit problem in which we want to maximize the probability of click-through based on bid values ($b$ is the value of bid and $\Pr(b)$ shows the probability that a customer clicks on a link given bid $b$). I am wondering what the possible modeling approaches are? If we consider a discrete set of bids ($b\in \{b_1,b_2,\cdots,b_k \}$), it is possible to model the probability as a Bernoulli distribution for each bid $\Pr(b_k)=p_k$ (with Beta or Dirichlet prior, e.g., $p_k \sim Beta(\alpha_k,\beta_k)$ ). Another case is model as a Logistic bandit in which it is possible to extend it to continuous values for bids, $\Pr(b_k)=\frac{1}{1+\exp\left(a_0 + a_1 b_k \right)}$.
I am wondering what other approaches are possible? Can we model it in form $\Pr(b_k)=\beta_0 + \beta_1 b_k$ with Normal prior on $\beta_0$ and $\beta_1$? If yes, what should we do if $\beta$ is estimated such that the probability exceeds 1?
","['multi-armed-bandits', 'contextual-bandits']",
"Is there a state-of-the-art deep learning paper that uses center point regression instead of bounding box regression, for object tracking?","
Almost all deep learning based object tracking methods perform bounding box regression. Siamese-based networks which are very popular for object tracking also perform bounding box regression most of the time, although SiamFC type exceptions exist. And some other networks use a detection + tracking mechanism using the center points of the objects, however, what I am looking for is whether any Siamese based networks that predict the center of the object rather than the bounding box exist. I searched the literature but couldn't find any, I would appreciate any guidance.
","['deep-learning', 'convolutional-neural-networks', 'bounding-box', 'siamese-neural-network', 'object-tracking']",
Is there a name for this model?,"
I have an image autoencoder model trained as follows:
Step 1) train a GAN to obtain a generator capable of drawing from the data manifold by sampling a normal distribution in latent space
Step 2) train an encoder on the front, keeping the generator frozen, effectively mapping images into the normally distributed latent space.
Is there a name/reference for such a model?
I know it's not really a VAE-GAN because it has no variational component, and the decoder and encoder are trained separately. It's not quite a vanilla AE either since the latent space is structured and constrained in the GAN training step. The models I get from this approach, while not SoTA, do have desirable qualities like disentangled representations but without the instabilities of VAE-GAN training.
I do use a perceptual loss when training the encoder, using intermediate feature maps from the GAN-trained discriminator, but I don't think it makes a big difference to the question I'm asking.
","['terminology', 'generative-adversarial-networks', 'autoencoders', 'representation-learning']",
Make an NN utilize other NNs as part of its decision process,"
Suppose I have a NN that learns to predict the time it takes a robot to move between two jobs. That's three inputs (for starters): robot, job A, job B. Not all robots travel at the same speed, and jobs are not symmetrically spaced.
Suppose I have another NN that learns how long a robot takes to do some given job. All robots can do all jobs, but the robots are not equally effective at all jobs.
Now suppose that I want to make some new NN that takes a list of robots and a list of jobs and gives me robot-to-job map and sequence that minimizes the time to accomplish all jobs. Conceptually, how can I make this third NN utilize the knowledge contained in the other two NNs? What do you call this type of architecture? What structural form for this third NN do you recommend?
","['neural-networks', 'sequence-modeling', 'model-request', 'feature-extraction']",
Is item-based collaborative filtering the same thing as content-based filtering?,"
According to this Google dev page

content-based filtering
Uses similarity between items to recommend items similar to what the
user likes.
collaborative filtering
Uses similarities between queries and items simultaneously to provide >recommendations.

With the items and query defined here

Items (also known as documents)
The entities a system recommends. For the Google Play store, the items
are apps to install. For YouTube, the items are videos.
Query (also known as context)
The information a system uses to make recommendations. Queries can be
a combination of the following:
user information
the id of the user
items that users previously interacted with
additional context
time of day
the user's device""

But I also see the term ""Item-based Collaborative Filtering"" thrown around sometimes, e.g. here, here and here.
Is this item based collaborative filtering the same thing as content based filtering?
","['comparison', 'terminology', 'recommender-system']",
Why do transformer Key Query Value layers not have biases or activations?,"
Transformers use just matrices to transform input embeddings, which is halfway to being a connected dense layer (add a bias and activation). So, why don't transformers have dense layers for encoding input into Query Key Value?
","['transformer', 'dense-layers']",
"How to preserve Markov Property in Deep Reinforcement Learning when using ""mixup"" or ""mixreg""?","
I've read through these two papers:

(original about ""mixup"") https://arxiv.org/pdf/1710.09412.pdf
(variant for RL, ""mixreg"") https://arxiv.org/pdf/2010.10814.pdf

They are about a rather interesting approach to improving model generalization. Here's the thing, though - I can easily see how to use this for supervised learning, as there is always a ""reward""/prediction on each ""observation""/row-of-data .
However, even though the second paper (mixreg) talks about applying this to RL specifically, I don't understand how you can manage this.
How would you preserve the Markov property if you're mixing observations/rewards that aren't necessarily in any way sequential?
","['deep-rl', 'deep-neural-networks', 'data-augmentation']","Because each environment individually satisfies the Markov property, the distribution of the next state $s_{t+1}$ in any transition depends only on $s_t$, $a_t$, and the transition probabilities of the active environment $e_t$:$$P(s_{t+1} | s_t, a_t) = P_{e_t}(s_t,a_t,s_{t+1})$$Each environment is independent of the others, so when mixreg builds a combined state $(s_i, s_j, \lambda)$ and chooses an action $a$, the distribution of the next combined state $(s_{i+1}, s_{j+1}, \lambda)$ is simply the combined distribution:$$P((s_{i+1}, s_{j+1}, \lambda) | (s_i, s_j, \lambda), a) = P_{e_i}(s_i,a,s_{i+1}) P_{e_j}(s_j,a,s_{j+1})$$This remains conditionally independent of all other states and actions."
Should PPO always converge toward the global optimum?,"
I'm trying to ""solve"" the OpenAI gym environment ""Humanoid-v3""  using PPO. I got it to work to some degree (The NN is learning a policy and perfecting it. Average reward of about 5.5k). However, the learned policies do not yet resemble the human stride (like in the PPO blog post), which brought up a question.
Should the algorithm always converge toward the global optimum (given good hyperparameters)? Or is a good convergence somewhat luck-based and you may need multiple training processes?
","['reinforcement-learning', 'convergence', 'proximal-policy-optimization', 'gym']","In the world of theory, Convergence Proof for Actor-Critic Methods Applied to PPO and RUDDER claims to show that PPO eventually converges to a local (but not necessarily global) optimum.In practice, good convergence is somewhat luck-based. Among other problems, the number of training steps is finite, the optimal solution might not be explored, and the model might approach the optimal solution then suffer catastrophic interference/forgetting.And it's even possible that the human gait isn't the optimal solution of that task. OpenAI has been accused of cherry-picking or misrepresenting results for good PR at times, and a human gait gives better publicity than a weird, but higher scoring, solution."
How does YOLO detect the object when the object is in multiple grid cells?,"
I have been reading various articles and watching videos on YouTube, but I can't seem to understand one thing.
How does YOLO make a bounding box for an object if it is in multiple grid cells? For example, in the picture given below, how does it predicts the bounding box for the classes, because they fall in multiple cells? How does it know what object is in a grid cell even when it sees a small part of it?
It's been very difficult for me to get these answers.

","['deep-learning', 'convolutional-neural-networks', 'object-detection', 'yolo', 'bounding-box']",
Reinforcement Learning with constant reward in constant episodes time length,"
I have a situation where I'm trying to maximize the number of steps in a fixed training time frame. It's possible that specific steps will lead to a delay until the agent can act again, and thus less overall steps by the end of the fixed length episode. I am using a basic DQN.
I want to try to explain to a layman how having a constant reward for every step, if given a fixed time frame during training, can allow the agent to try to maximize the number of steps within the episode.  Conceptually it makes sense to me, but I'm at a bit of loss on how to explain it.
I seem to be missing the right keywords to be searching for examples of this situation online.  Any help with keywords and/or explanation would be helpful.
","['reinforcement-learning', 'dqn']",
Why do we apply the mutation operation after generating the offspring?,"
Why do we apply the mutation operation after generating the offspring, in genetic algorithms?
","['genetic-algorithms', 'evolutionary-algorithms', 'mutation-operators']",
For simple weight constraints: Add constraint directly or use parameterization without constraint,"
I am wondering if it makes sense to parameterize simple weight inequalities, for example if the weights should be $w\geq 0$, one cound train $\exp w$ over the unconstrained set instead. Also, if $\sum w_i=1$ one could parameterize $\frac{e^{w_i}}{\sum e^{w_i}}$ and optimize over the unconstrained set.
While the solutions should be the same, I wonder if it makes numerically a difference, and what way is the right one. I guess the values of $\exp$ might explode and it becomes numerically unstable.
Question: should I use something like tf.keras.constraints.NonNeg() or should I replace each weight $w$, on which I want to apply the constraint with $\exp w$ (""reparameterization"") (since exp>0)or is there no general rule?
","['constrained-optimization', 'numerical-algorithms']",
Visualizing the loss landscape in deep NN to compare optimization methods,"
I'm comparing 2 optimization algorithms for deep neural nets through visualizing the loss landscape. The visualization method is described here.
Besides the qualitative observation that how trajectory moves w.r.t. the loss level-sets, are there any quantitive measures to compare the two methods?


","['neural-networks', 'loss']",
How to tune hypeparametes in A2C-ppo?,"
Im currently working with A2C. The model was able to learn open ai pong, i ran this as a sanity check that i havent made any bugs. Now im trying to make the model play breakout, but still after 10m steps the model has not made any significant progress. Im using baseline hyperparameters which can be found here https://github.com/openai/baselines/blob/master/baselines/a2c/a2c.py, except my buffersize have been from 512 to 4096.  Ive noticed that entropy decreases extremely slowly

given the buffersize from the interval which i just gave. So my questions are how to make entropy decrease and how to increase rewards per buffer?  Ive tried to decrease the entropy coefficient to almost zero, but still it acts very weirdly.
Update: Even when i set the entropy coef to zero entropy wont decrease, i guess i might have a bug?


","['reinforcement-learning', 'deep-rl', 'advantage-actor-critic']","Human performance on Breakout is ~30, if you refer to the original DQN paper (Table 1). In the original A3C paper, it takes around 5 epochs to reach that score, so 20 millions frames (Figure 3).Is the total loss going down in your original experiment? If you set the entropy coefficient to zero, then the contribution of the entropy loss to the overall loss is zero'ed out. That's why it's actually expected that the entropy loss won't decrease.You're taking the gradients of
$$L_{\text{total}}(\theta_{\text{policy}}, \theta_{\text{value}}, \theta_{\text{entropy}}) = L_{\text{policy gradient}}(\theta_{\text{policy}}) + c * L_{\text{value}}(\theta_{\text{value}}) - 0 * L_{\text{entropy}}(\theta_{\text{entropy}}) = L_{\text{policy gradient}}(\theta_{\text{policy}}) + c * L_{\text{value}}(\theta_{\text{value}})$$ where $\theta_{\text{policy}}$ are the parameters of the policy network, etc.Therefore, $L_{\text{entropy}}(\theta_{\text{entropy}})$ can be anything, it's unconstrained by the optimization of $L_{\text{total}}$To test that you don't have a bug, you should in fact increase the entropy coefficient and check that the entropy loss is decreasing.As a reminder, the entropy loss is actually here to maximize the entropy of the policy so that it doesn't collapse to a deterministic one."
"How does Bishop derive $\ln p\left(\mathbf{x} \mid \mu, \sigma^{2}\right)$, when $p$ is a Gaussian?","
I am now reading the Bishop Machine Learning Book and going through every single equation.
We know that in the case of a single real-valued variable $x$, the Gaussian distribution is defined by
$$\mathcal{N}\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}$$
Since the dataset $\mathbf{X}$ is i.i.d., we can therefore write the probability of the dataset, given $\mu \text { and } \sigma^{2}$, in the form
$$p\left(\mathbf{x} \mid \mu, \sigma^{2}\right)=\prod_{n=1}^{N} \mathcal{N}\left(x_{n} \mid \mu, \sigma^{2}\right)$$
We will resolve the numerical underflow by computing the sum of the log probabilities, therefore from the above two equations we have the following log-likelihood function,
$$\ln p\left(\mathbf{x} \mid \mu, \sigma^{2}\right)=-\frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}\left(x_{n}-\mu\right)^{2}-\frac{N}{2} \ln \sigma^{2}-\frac{N}{2} \ln (2 \pi)$$
I am not quite sure how did they derive this in the book. Thanks a lot and have a good day.
You can refer to page 27 equation 1.54 for the detail.
","['machine-learning', 'math', 'normal-distribution']","This is not so difficult (just a bit verbose if you do all steps). Just replace $\mathcal{N}\left(x_{n} \mid \mu, \sigma^{2}\right)$ with $\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}$ and apply the properties of logs (specifically, log of multiplication, log of exponential and log of a fraction). Try first before looking at my solution (otherwise you don't learn anything)! \begin{align} p\left(\mathbf{x} \mid \mu, \sigma^{2}\right) &= \prod_{n=1}^{N} \mathcal{N}\left(x_{n} \mid \mu, \sigma^{2}\right)\\ &= \prod_{n=1}^{N}\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}  \iff \\ \log p\left(\mathbf{x} \mid \mu, \sigma^{2}\right) &= \log \left( \prod_{n=1}^{N}\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\} \right) \\ &= \sum_{n=1}^{N} \log \left(\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\} \right) \\ &= \sum_{n=1}^{N} \left( \log \left(\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \right) + \log \left( \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\} \right) \right) \\ &= \sum_{n=1}^{N} \left( - \frac{1}{2}\log \left(2 \pi \sigma^{2}\right)  -\frac{1}{2 \sigma^{2}}(x-\mu)^{2}  \right) \\ &= - \frac{1}{2} \sum_{n=1}^{N} \log \left(2 \pi \sigma^{2}\right) - \frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}(x-\mu)^{2} \\ &= - \frac{1}{2} \sum_{n=1}^{N} \left( \log 2 \pi + \log \sigma^{2}\right) - \frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}(x-\mu)^{2} \\ &= - \frac{1}{2} \sum_{n=1}^{N} \log 2 \pi  - \frac{1}{2} \sum_{n=1}^{N} \log \sigma^{2}- \frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}(x-\mu)^{2} \end{align}"
How do we estimate the value of a stochastic policy?,"
I'm learning about reinforcement learning, particularly policy gradient methods and actor-critic methods. I've noticed that many algortihms use stochastic policies during training (i.e. they select the actions from a probability distribution).
I don't understand how the value function for stochastic policies is estimated accurately. The value function $V^{\pi}(s)$ is the expected return when starting at state $s$ and following policy $\pi$, but if we are choosing actions randomly then we are not really following any particular policy.
How are we getting an accurate estimate of the value of policy $\pi$ if we are not following a deterministic policy $\pi$ during training?
","['reinforcement-learning', 'policy-gradients', 'actor-critic-methods', 'value-functions', 'stochastic-policy']",
Why is a large replay buffer inefficient?,"
Open AI spin up says

... the replay buffer should be large enough to contain a wide range
of experiences, but it may not always be good to keep everything. If
you only use the very-most recent data, you will overfit to that and
things will break; if you use too much experience, you may slow down
your learning.


Why would recent data overfit? Is it because they are more correlated to each other so I am training on a bunch of similar data?

Why is it slow to use much memory (besides the simple reason that I am storing more data). Below are specific doubts I have.

I thought convergence is faster when fitting on uncorrelated data. For example, estimates on mean. Recent data would be more correlated. So why would training on longer data slow down training?

Previous samples doesn't seem to have the notion of becoming outdated. They are still valid samples for evaluating the current loss. So why would they slow down training?




","['reinforcement-learning', 'deep-rl', 'hyper-parameters', 'experience-replay']",
What is the difference between fine tuning and variants of few shot learning? [duplicate],"







This question already has an answer here:
                                
                            




What is the difference between one-shot learning, transfer learning and fine tuning?

                                (1 answer)
                            

Closed last year.



I am trying to understand the concept of fine-tuning and few-shot learning.
I understand the need for fine-tuning. It is essentially tuning a pre-trained model to a specific downstream task.
However, recently I have seen a plethora of blog posts stating zero-shot learning, one-shot learning and few-shot learning.
How are they different from fine-tuning? It appears to me that few-shot learning is a specialization of fine-tuning. What am I missing here?
Can anyone please help me?
","['deep-learning', 'fine-tuning', 'pretrained-models', 'few-shot-learning', 'zero-shot-learning']",
How should I choose the depth for minimax if I have a strict time constraint?,"
I am working on a controller that plays Ms. Pac-Man using a minimax algorithm. The controller has a limited time amount in which it can choose a move on each round, otherwise when the time runs out the system chooses a random action instead.
My goal is to select a value for the depth that is large enough to consent Ms.Pac-Man to achieve a decent score, but not so large to cause a lot of timeouts. Leaving the number of timeouts aside, basically I did the most intuitive thing: I increased the depth until I noticed a drop on the score values, but that drop could have occurred casually.
Is there a more scientific/statistical way to do that (that ideally addresses both the score e the timeout issues)?
(I fantasized about performing repeated hypothesis testing, but I quickly learned how silly that idea was when I heard about multiple testing problem).
","['game-ai', 'minimax']",
What is the best way to create a vector representation (with fasttext) of a list of words?,"
Basically what I want to do is to create a single vector representation of a list of skills belonging to employees at a company (one list per employee). The embedding will be a representation of an employee's ""profile"". The motivation behind this is (among other reasons) that I want to be able to identify clusters among the employees.
Assume I already have a trained FastText model (or Word2vec) that can generate good representations of the individual words in the list.
My current solution is simply to add all the word embeddings in an employee's list together (without any form of normalization). But I'm very unsure about whether this is the best approach to generating a good representation of an employee's profile.
The dimensions of the vectors are 300 and there are usually around 10 to 30 skills in a single list.
Any help would be greatly appreciated!
Example:
Let's say we have an it-consulting firm where each employee has their own set of skills. Some consultants are more experienced or versatile, thus having more skills listed in their profiles. eg we have:
 alex_skills = ['microsoft azure', 'machine learning', 'data science', 'python', 'sklearn', 'xgboost', 'nginx', 'flask', 'SHAP', 'git', 'word2vec', 'statistics', 'deep learning', 'linux','docker compose', 'pandas']

carla_skills = ['devops', 'machine learning', 'deep learning', 'continuous integration', 'kubernetes', 'python','git', 'speech recognition', 'github', 'bitbucket', 'scikit-learn', 'natural language processing', 'pandas']

adam_skills = ['automation', 'robotic process automation', 'banking and finance', 'process mapping', 'IAM', 'väsentlighetsanalys', 'business intelligence', 'auditor', 'requirements handling', 'risk management', 'coordinator', 'project manager', 'data visualization']

As you can see Alex and Carla are more similar and should possibly be in the same cluster, while Adam might not be.
So I wan't to make a vector representation of the entire list of skills. And then I will use these vector representations in some clustering algorithm (eg HDBscan) and by some distance metric (eg. cosine distance), capture the relation between Alex and Carla.
I suspect the fact that the lists have different lengths might cause problems, therefore maybe divide by the length of the list after adding?
","['natural-language-processing', 'word-embedding', 'word2vec']",
What does IOU3 mean in this context?,"
I was reading a paper and this paragraph said that:

The ground truth score is calculated based on the intersectionover-
union (IoU) of the perturbed image and the ground truth one. Since we
would like to distinguish among IoU values close to 1.0, we use $IOU^3$
as the ground truth score.

I couldn't find any references to this, nor mentioned in the paper. Is it just simply the calculated IoU to the power of 3? Or is it a special kind of IoU calculation method? What does this mean?
",['loss'],"Since we would like to distinguish among IoU values close to 1.0, we
use IOU3 as the ground truth score for the SRN.It seems to be just IoU to the power of 3. They use the cube function because they want their regressor to pick up small differences in IoU when IoU is near 1.0. They could have used any other function that is sufficiently steep close to 1.0. I haven't seen power of 3 being used very often but it's pretty straightforward to implement and I assume it works better than the square in this problem for them to use it."
What if we modify some Q-values while taking the action?,"
Just a passing thought about Q-learning. In the tabular Q-learning, what if I play around and modify any Q-values as I am using them to take actions? Would it be a violation of any (1) theoretical rule? (2) a reduction of efficiency?
Update: By modification I mean the corresponding value after the action is taken. I am not referring to the Bellman update step. I am assuming we have filled our Q-table and now implementing in real scenario. And in this real scenario upon deciding on an action can I update the corresponding Q-value by say adding some constant factor? Not updating through Bellman eqn. Only after certain time steps I will re-train (if that's the correct word) and refresh my Q-table using the Bellman. I am thinking of like training and testing kind of phases here, if that make sense. I say so because my rewards are dependent on a random dynamic parameter and so I cannot just re-update my Q-table right after each time-step (to prevent unnecessary swaying of Q-values) and want to do so after certain n time-steps. In between, I just use the table to look up and take the actions without updating the corresponding Bellman update equation. But since I have & want to do something to reflect some criteria upon taking the action, I am thinking what if I just modify the corresponding Q-values only by adding some constant factor until I retrain. This would help take better actions even until the n time-steps are over and ready for re-train....
","['reinforcement-learning', 'machine-learning', 'q-learning', 'intelligent-agent']",
What kind of NN I need to find ideal ranges and correlation between them?,"
I’m new to NN and I’m trying to collect material and study. I’m getting through a general high level book, but I’m still struggling understanding what kind of NN I should go ‘deeper into’ for what is my idea.
I have some equipment that I can control from my computer. Each part of this equipment has a specific ID. Each ID accepts a value within a specific range, always the same for that specific ID: so for example ID1 might accept just 0 to 1, ID2 accepts 0-255, ID3 accepts from 0 to 3 etc. (the max number of ID/controllers is 255).
Now, the thing is that some ranges are better than others. So there’s a desirable outcome, that usually means some ranges work better than others for each ID/controller, and it also means that there are probably some correlations (for example the range 30 to 90 for ID1 usually works best when ID5 is in the range of 20 to 50).
The idea would be to have training data from random generation: so I generate a random value (within the full range of each ID) for the whole set of parameters, I decide if the outcome is good (0/1, good or bad), and then I generate once again random values for all the parameters, once again I label it if good or bad, etc. It all works with all parameters together, not one by one, so it’s like a snapshot for the status of the machine (all its IDs/parameters).
And to test the AI, I would have two choices: either let it generate ‘desired’ values directly, or have the AI after a quick random generator that let pass only what it considers as good.
My question is: what kind of NN would be recommended for this application? What would I need to study more? I’m proficient in Python so I would use that.
Considering the nature of the data (around 100 pairs of ID:value where value is within range that goes max from 0 to 255), I believe this should be an almost textbook example for using a NN. But maybe I’m wrong?
Any suggestion, link, direction would be much appreciated :-)
","['machine-learning', 'training', 'hardware']","What you described seems like a pretty standard binary classification problem. There are many good algorithms, that are much simpler and more interpretable than NNs. I don't see why you would straight jump into NNs without first trying and fine-tuning e.g. Support Vector Machines (SVM), Random Forest(s), or some gradient boosting classifier like XGBoost. I suggest you have a look at the sklearn package in Python for the implementation of these."
Can I use Sentence-Bert to embed event triples?,"
I extracted event triples from sentences using OpenIE. Can I concatenate the components in the event triple to make it a sentence and use Sentence-Bert to embed the event?
It seems no one has done this way before so I am questioning my idea.
","['natural-language-processing', 'bert', 'embeddings', 'information-retrieval']",
How to implement PPO without using a Critic,"
I am using the standard policy gradient algorithm, REINFORCE, to solve a RL problem and was thinking about implementing Proximal Policy Optimization (PPO) to increase the sample efficiency of my solution. From the original paper, it seems the clipped loss PPO optimizes includes an Advantage $A_t$ term:
$$
L^{CLIP} (\theta) = \mathbb{E}_t \Big[min(r_t(\theta)A_t, 
\: clip(r_t(\theta), 1 - \epsilon, 1 + \epsilon)A_t \Big]
$$
Generally, this advantage $A_t$ is calculated using an actor-critic schema, where we train a network (critic) to predict the value $V(s)$ of a given state, which is then used to calculate $A_t$. However, my RL task is episodic and the trajectories end (i.e., arrive at a terminal state) in just a few actions, so I would rather not use a critic network. Thus, my question is the following: How can I implement PPO without a critic network, i.e., without a network that predicts $V(s)$?
To achieve this, I thought about simply substituting the advantage term $A_t$ used in $L^{CLIP}$ for the discounted sum of rewards $R_t$ used in the REINFORCE loss. Should this work fine or is there a better alternative?
","['deep-rl', 'actor-critic-methods', 'proximal-policy-optimization', 'reinforce']","Your justification for not wanting to use a critic (that the episodes are short) does not make sense to me. I would expect that including the critic would result in a substantial variance reduction (and substantially faster training) due to the extra baseline and also the bootstrapping which is done in the generalized advantage estimation (GAE). I don't see how the episode length being short is relevant.However, if you don't want to use an advantage, you don't have to. You can simply replace $A_t$ with the (monte carlo sampled) cumulative reward $R_t$. The problem is that the clipping objective only makes sense if you use an advantage. It won't make sense to do that clipping if you are using $R_t$ instead of $A_t$.It seems dubious to me to call such an algorithm ""PPO"" as really it would just be REINFORCE. The only possible difference between what you describe and reinforce, is that PPO should generate a dataset of transitions and do multiple mini-batch updates per dataset. If you do decide you want to go that route, you should note that you still need the importance sampling term (the unfortunately named $r_t(\theta)$) for the update to be unbiased."
Can we achieve optimality with minimax using an evaluation function?,"
The following quote (from AIMA) refers to the situation in which the minimax algorithm computes its values directly from the terminal states.

(The) definition of optimal play for MAX assumes that MIN also plays optimally—it maximizes the worst-case outcome for MAX. What if MIN does not play optimally? Then it is
easy to show (...) that MAX will do even better. Other strategies against suboptimal opponents may do better than the minimax strategy, but these strategies necessarily do worse against optimal opponents.

But what about minimax using an evaluation function for nodes at a certain depth? Can we achieve optimality? If we do not have a perfect evaluation function, is the best we can do to achieve optimality with respect to the specific evaluation function?
Do considerations in the quote (do even better against a suboptimal opponent) still hold?
","['minimax', 'norvig-russell', 'evaluation-functions', 'optimality']",
What is the advantage of adding CNN to LSTM for forecasting sequential data?,"
I am working with simulated sequential data and the goal is to forecast that data. Long-short-term-memory (LSTM) is one of the most advanced models to forecast time series according to this post. I can imagine that it is a good model because of the memory-cells they use which are useful when learning of the past.
This paper discussed the use of CNN in time-series analysis. It says:

CNN is suitable for forecasting time-series because it offers dilated
convolutions, in which filters can be used to compute dilations
between cells. The size of the space between each cell allows the
neural network to understand better the relationships between the
different observations in the time-series [14].

It even outperformed LSTM:

A specific architecture of CNN, WaveNet, outperformed LSTM and the
other methods in forecasting financial time-series [16].

I see more and more posts about the usage of CNN in combination with LSTM, but I can't find any information about the advantages and disadvantages of using these in combination.
This post (Advantages of CNN vs. LSTM for sequence data like text or log-files), it is asked about the advantages of CNN vs. LSTM. But I would like to know the advantages and disadvantages of adding CNN to LSTM for forecasting univariate sequential data? Or should you use one of the two algorithms?
","['neural-networks', 'convolutional-neural-networks', 'long-short-term-memory']","If your data are 2D + time then you might want to use something like ConvLSTM.If you only care about 1D + time then you don't need to add CNN to LSTM you only use one or the other. In terms of pros and cons have a look at this empirical study on how dilated convolutions compare to LSTMs for modeling sequential data. If you're also interested in the more theoretical aspects, this paper shows how temporal convolutional networks are related to truncated RNNs."
Current state of the art and datasets for combining NLP and CV?,"
I was considering a scenario where natural language processing (NLP) and computer vision (CV) are combined, for example in extended reality systems that get as input both natural language and non-verbal information, e.g. human gestures, and can comprehend it. For example, the agent would get language and non-verbal input and talk to a user.
How could this be realized? My naive guess would be a conditional transformer, where the conditioning happens on the non-verbal input, but I'm not sure how exactly the conditioning could happen. What is a current state-of-the-art model for combining NLP and CV?
Also, are there datasets available for the aforementioned use case? I'm thinking of the scenario where a sentence, e.g.

Yeah, I like him too!

can either mean what is said, and the non-verbal input could be that the person saying it is smiling. However, if the non-verbal input is some laughter, then the above sentence might be meant ironically. Is there any dataset for this, where sentences and non-verbal inputs are combined? (Please note that I'm not talking about the generation of a sentence to an image, I'm referring to a combination of NLP and CV.)
Thanks a lot!
","['natural-language-processing', 'reference-request', 'computer-vision']",
Why use a fully connected layer for attention?,"
In the paper Neural Machine Translation by Jointly Learning to Align and Translate, attention is used with a single fully connected layer. Specifically, in the auto-regressive set up (equation 4), the context vector c_i is computed using attention  from all the annotations (equation 5).
I'm curious why a fully connected layer is used here and not some other architecture that captures ""locality"" better: the relationship between adjacent states should be similar independent of where in the sentence they occur. I.e. the relationship between state 1 and 2 should be analogous or similar to the relationship between states n and n+1? However, the fully connected layer treats these two relationships as completely independent weights: a[1,2] vs a[n,n+1].

","['natural-language-processing', 'recurrent-neural-networks', 'attention']",
How to justify the chosen neural architecture?,"
I had a task to implement a neural network that would carry out multiclass classification of traffic by several parameters. On the advice of colleagues, I chose the ""Multilayer Perceptron"" architecture. One of these days I will have a defense of my work, but I absolutely do not understand how to answer the question: ""Why did you choose this type of architecture?"". Please tell me if there are any theses why the ""multilayer perceptron"" architecture is better than other neural network architectures for solving problems of multi-class traffic classification?
","['neural-networks', 'deep-learning', 'classification']","This is a very general question, so I'll just point to a reference that should be a good starting point. Deep Learning for Encrypted Traffic Classification: An Overview seems to contain exactly what you're looking for:Several factors affect the choice of deep learning models for network traffic classification. The most important one is the choice of features. ...Table II summarizes features, the corresponding models, and their properties."
RL-based trading bot: how to deal with overfitting,"
I've been playing around building a reinforcement learned-based trading bot using the stable-baselines3 library. I've come up with an environment that seems to be able to learn how to make profitable trades on the dataset that I feed it. If I keep training long enough it is seemingly making good money.
Now, I have a background in supervised machine learning, and I was already quite sure that my agent was ""overfitting"" to the dataset that I was feeding it. This was confirmed by my experiment: when feeding my trained model a new validation dataset the performance was on-par with a random agent, and it was no longer making any money. In other words, my agent manages to exploit relations in the training data very well, but when evaluating the out-of-sample performance on unseen test dataset it appears that my agent just learned noise. In the context of my trading bot it seems that the agent memorises the financial time-series, and it not actually useful for the purpose of trading as the patterns it learned are not generalisable.
Now my question: how do we overcome this issue?
In supervised learning, there are various ways to help prevent that our model will overfit. We can use a validation set during training so that we can apply early stopping (stop training when validation accuracy no longer improves). We can use regularization techniques such as drop-out layers. But how do we deal with the concept of overfitting in reinforcement learning? Some things I had in mind that could be useful:

Does it make sense to apply regularization to my neural network that learns the value function and policy?
Is there something equivalent to early stopping?
Will adding more data or simply decreasing the number of timesteps help?
Could it possibly help to generate training batches by randomly selecting certain periods of my full financial time-series, rather than going through the same time-series over and over again? My assumption is that in that way the network no longer relies on the same sequence, and the patterns it learns could be more robust...

Looking forward to hearing your view on this!
","['reinforcement-learning', 'deep-rl', 'time-series', 'stable-baselines']",
Is it possible to add states to the Q-table after the game has started?,"
I would like to implement Q-learning in a game.
Here is the board:

It's a 2 player game. At each turn, each player can put a pawn on a line of their choice. They can't choose the column. The right player will put their pawn on the right column and the left player will put their pawn on the other side. At each turn, each pawn will advance in the direction of the opponent. Each pawn has a cost and each player has an amount of money and health points.
I was wondering how I could enumerate the number of states, and it seems impossible.
Given the number of possible states, is Q-learning a solution? If it is, is it possible to add states to the Q-table after the game has started? If Q-learning isn't a solution, what could I use?
","['reinforcement-learning', 'q-learning', 'implementation']","Yes, it is possible to add states to a Q table after the game has started, for example by storing the ""table"" in a binary tree.Nonetheless, with a simple interpretation of state (like all the pawn positions), Q learning won't work because there are just too many states. Most states will be new so the agent will choose most actions randomly.Instead, you'll need to reduce the state space with hand-crafted features, or approximate Q values with a classic Q function approximation or a neural network.I'd choose the latter alternative, which is called a Deep Q Network, but neural networks might be challenging if you aren't familiar with them.Alternatively, you could use Monte Carlo Tree Search."
How to generate new data given a trained VAE - sample from the learned latent space or from multivariate Gaussian?,"
To generate synthetic dataset using a trained VAE, there is confusion between two approaches:

Use learned latent space: z = mu + (eps * log_var) to generate (theoretically, infinite amounts of) data. Here, we are learning mu and log_var vectors using the data, and, eps is sampled from multivariate, standard, Gaussian distribution.

Use multivariate, standard, Gaussian distribution.


I am leaning more towards point 1 since we learn the mu and log_var vectors using our dataset. Whereas, point 2 uses the uninformative prior which contains no particular information about the dataset.
One of the reasons of VAE is to be able to learn this ""unknown"" latent space distribution by constraining it to approximate a multivariate, standard, Gaussian distribution, but at the same time, allow it sufficient flexibility to deviate from it too.
What are your thoughts? I have implemented some VAE, Conditional VAE codes both in TensorFlow 2 and PyTorch which you can refer to here.
","['tensorflow', 'pytorch', 'generative-model', 'variational-autoencoder', 'latent-variable']",
DDPG agent with convolutional layers for feature extraction [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I'm trying to come up with a definition of the critic for a DDPG agent in PyTorch using a CNN as a feature extractor. It is pretty straight forward for the actor model. However, for the critic model I am not sure. Below, I've given my code for the actor model.
class CNN(nn.Module):
    def __init__(self, obs_dim, act_dim, act_limit):
        super(CNN, self).__init__()

        channels, height, width = obs_dim
        self.act_limit = act_limit

         self.actor = nn.Sequential(OrderedDict([
            ('conv1', nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=8, stride=4)),
            ('relu1', nn.ReLU()),
            ('conv2', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)),
            ('relu2', nn.ReLU()),
            ('conv3', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)),
            ('relu3', nn.ReLU()),
            ('flatten', nn.Flatten()),
            ('linear1', nn.Linear(3136, 512)),
            ('relu4', nn.ReLU()),
            ('linear2', nn.Linear(512, act_dim))
        ]))
    def forward(self, obs):
        # Return output from network scaled to action space limits.
        return self.act_limit * self.actor(obs)

My confusion lies at the part with the concatenation of the action and the observation vectors and how to actually integrate that in my CNN model. A typical critic model is a MLP that looks like this:
class critic(nn.Module):
    def __init__(self, obs_dim, act_dim, act_limit):
        super(critic, self).__init__()
        self.act_limit = act_limit

        self.fc1 = nn.Linear(obs_dim + act_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 256)
        self.out = nn.Linear(256, 1)

    def forward(self, x, actions):
        x = torch.cat([x, actions / self.act_limit], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        q_value = self.out(x)

        return q_value

Any ideas on how to define a critic with the same (convolutional) feature extractor?
","['reinforcement-learning', 'convolutional-neural-networks', 'pytorch', 'actor-critic-methods', 'ddpg']",
Does it make sense to provide a DQN with negative rewards for a network with relu and sigmoid activations?,"
The creation of negative rewards leads to the chance of Q-values being negative. However, networks with relu or sigmoid activations, just cannot predict negative values. This will lead to a case where erroneous Q-values are being predicted. Is my understanding correct?
","['reinforcement-learning', 'dqn', 'rewards', 'relu', 'sigmoid']","A network with ReLU activation can predict negative values; we put ReLU between the hidden layers but return the output of the final layer without any activation function, or with a linear activation function to scale the output."
How to deal with changing rewards in Q-learning? DQN?,"
I read the working of Q-learning through a grid-based taxi routing wherein a taxi has to pick and drop off a passenger from source to destination. Likewise, I have a routing problem and hence, I tried adapting the code from the article for my scenario. Visiting this page is not required, just gave the link for reference. https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/
My Scenario: As an analogy to my network routing, say my source & destination are node A and node E. Say only two routes are possible: A-B-E and A-B-C-E. I want to select the next node at each 'action' stage that finally together forms a route. Hence, I take my 'state' and 'action' both as the set of all nodes.
Problem: BUT my reward is a value I assign that depends on the parameters of the links connecting the nodes. i.e.if I compute some network metric as x for the link A-B then I give this x as the reward since I want to minimize the x in my network. And obviously, this x would differ over time since my network links' status changes. It is not something I can pre-define for a given state-action as in the article example like obstacle means negative reward, destination means positive reward,etc.
So what should I do in this case? Is my definition of 'state' or 'action' wrong? Or do I need a deep Q-network? Is it for such kinds of cases? Please suggest.
","['reinforcement-learning', 'machine-learning', 'deep-rl', 'q-learning', 'dqn']","Is my definition of 'state' or 'action' wrong?I hesitate to say 'wrong', but that's not how state and action are defined in RL, and that mismatch might make the algorithms hard to understand.In RL theory, the set of all nodes and the links between them is called the environment, the agent's current location is the state, and taking a route from one node to the next is the action. The reward is just as you said, except that usually the agent is trying to maximize the reward.Once your terms are aligned, regular Q learning (which keeps a simple table of Q values instead of approximating the Q values with something like a neural network) should actually work better than deep Q learning on such a small problem, because training a neural net can be challenging.Which brings us to the changing rewards. When the rewards change, the agent will need to find the new optimal path. To do that it will need to explore more than it would for a problem with fixed rewards that can settle into an optimal solution. For example, if you were using epsilon greedy exploration, epsilon would need to remain large, or be reset to a large value when the rewards change.Of course, the agent will perform poorly until it adapts to the new rewards, but you might be able to recalculate the optimal route when the parameters change by running the simulation for some time before making any real decisions.Hope that helps"
"PPO: multiple discrete actions per step, one depends on the other","
I have a custom PPO implementation, and it works fine, but I need to add to it the ability to select 2 actions per turn, one different in nature from the other, one dependent on the other.
Imagine that a turn a had 20 possible actions. Now, for each of these action of type A, I need to choose one from 3 possible actions B. They are not the same, and thus they can not be on different turns.
What I tried was to flatten the 20x3 space to a 60. Then, action A is 60//3 and action B is 60%3. But this does not train well. Are there any good methods for this issue?
","['reinforcement-learning', 'proximal-policy-optimization', 'discrete-action-spaces']",
What should I think about when designing a custom loss function?,"
I'm trying to get my toy network to learn a sine wave.
I output (via tanh) a number between -1 and 1, and I want the network to minimise the following loss, where self(x) are the predictions.
loss = -torch.mean(self(x)*y)

This should be equivalent to trading a stock with a sinusoidal price.
The issue I'm having is that the network doesn't learn anything. It does work if I change the loss function to be torch.mean((self(x)-y)**2) (MSE), but this isn't what I want. I'm trying to focus the network on 'making a profit', not making a prediction.
I think the issue may be related to the convexity of the loss function, but I'm not sure, and I'm not certain how to proceed. I've experimented with differing learning rates, but alas nothing works.
What should I be thinking about?
Actual code:
%load_ext tensorboard
import matplotlib.pyplot as plt; plt.rcParams[""figure.figsize""] = (30,8)
import torch;from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F;import pytorch_lightning as pl
from torch import nn, tensor
def piecewise(x): return 2*(x>0)-1

class TsDs(torch.utils.data.Dataset):
  def __init__(self, s, l=5): super().__init__();self.l,self.s=l,s
  def __len__(self): return self.s.shape[0] - 1 - self.l
  def __getitem__(self, i): return self.s[i:i+self.l], torch.log(self.s[i+self.l+1]/self.s[i+self.l])
  def plt(self): plt.plot(self.s)

class TsDm(pl.LightningDataModule):
  def __init__(self, length=5000, batch_size=1000): super().__init__();self.batch_size=batch_size;self.s = torch.sin(torch.arange(length)*0.2) + 5 + 0*torch.rand(length)
  def train_dataloader(self): return DataLoader(TsDs(self.s[:3999]), batch_size=self.batch_size, shuffle=True)
  def val_dataloader(self): return DataLoader(TsDs(self.s[4000:]), batch_size=self.batch_size)

dm = TsDm()

class MyModel(pl.LightningModule):
    def __init__(self, learning_rate=0.01):
        super().__init__();self.learning_rate = learning_rate
        super().__init__();self.learning_rate = learning_rate
        self.conv1 = nn.Conv1d(1,5,2)
        self.lin1 = nn.Linear(20,3);self.lin2 = nn.Linear(3,1)
        # self.network = nn.Sequential(nn.Conv1d(1,5,2),nn.ReLU(),nn.Linear(20,3),nn.ReLU(),nn.Linear(3,1), nn.Tanh())
        # self.network = nn.Sequential(nn.Linear(5,5),nn.ReLU(),nn.Linear(5,3),nn.ReLU(),nn.Linear(3,1), nn.Tanh())
    def forward(self, x): 
        out = x.unsqueeze(1)
        out = self.conv1(out)
        out = out.reshape(-1,20)
        out = nn.ReLU()(out)
        out = self.lin1(out)
        out = nn.ReLU()(out)
        out = self.lin2(out)
        return nn.Tanh()(out)

    def step(self, batch, batch_idx, stage):
        x, y = batch
        loss = -torch.mean(self(x)*y)
        # loss = torch.mean((self(x)-y)**2)
        print(loss)
        self.log(""loss"", loss, prog_bar=True)
        return loss
    def training_step(self, batch, batch_idx): return self.step(batch, batch_idx, ""train"")
    def validation_step(self, batch, batch_idx): return self.step(batch, batch_idx, ""val"")
    def configure_optimizers(self): return torch.optim.SGD(self.parameters(), lr=self.learning_rate)

#logger = pl.loggers.TensorBoardLogger(save_dir=""/content/"")
mm = MyModel(0.1);trainer = pl.Trainer(max_epochs=10)
# trainer.tune(mm, dm)
trainer.fit(mm, datamodule=dm)
# 

","['objective-functions', 'pytorch']","It doesn't matter that your loss is not convex. As a matter of fact, the loss function of a neural network is in general neither convex nor concave (reference).As ImotVoksim points out, the issue is that the loss function you've defined has nothing to do with the problem you're trying to solve.For example, a stock price of zero is going to give you a loss of zero and hence no gradients at all: the neural network is allowed to output arbitrary values whenever the stock price is zero.You want to ""make a profit"". I'm not sure why the MSE is not good in this case: if your neural network outputs the correct price of the stock for the next time period, you can use this information to make the trade which will maximize your profit.Or do you want to predict the price further in the future? In that case you could use an MSE of the formwhere x_t is the input at time period t and y_{t+n} the price of the stock at time period t+n, n a number you choose."
How to instruct Mask RCNN to identify objects too close to each other?,"
I have been trying to train a Mask RCNN model to identify individual poker chips in a stack. No matter what property I change, the end results look like the following image. I was guessing the issue is that the objects are too close to each other for the proper detection. Is there any alternative model or property of mask RCCN or my training model I could possibly try to change?


","['deep-learning', 'tensorflow', 'mask-rcnn']",
"Why are agents trained in episodes, even in non-episodic tasks?","
Let's consider some non-episodic problem. Maybe a game which can go on forever.
My question is: Why are agents still trained in episodes?
My understanding is that the agent's neural network is updated in batches depending on the batch size (so every x timesteps, the neural network will be updated). Therefore.. nothing special happens at the end of an episode, right? The agent does not ""review"" its performance and update itself again or anything like that.
Nevertheless, I find that shorter episode lengths can be helpful in my problem, but for the above reasons, I don't understand why 100 short-length episodes are better than 1 huge long episode, or why that would make any difference at all in a non-episodic task.
Any insight would be greatly appreciated!
EDIT: I suppose I'm trying to understand whether anything ""special"" happens in the network at the end of an episode. If not, let's imagine a situation where I have a time series environment, and I train the agent in one single episode from t=0 to t=100 . Now, let's say I train 10 agents, each starting a different multiple of 10 (t=0, t=10, t=20 etc.) and each new agent's start environment is exactly the same as the end environment of the previous agent (startenv for agent at t=20 is equal to endenv for agent that trained t=10 to t=19, preserving continuity). Would these two situations result in the exact same final model/agent? That is, would the 10th agent of the divided data group be the exact same as the agent at the end of the t=0 to t=100 run? Maybe this is impossible due to the way the gamma discount parameter will interact with the episode length. What if we set an extreme gamma parameter, so that the differences are negligible? I just want to understand if the agent in any real way considers ""episodes"" in its learning/training.
","['reinforcement-learning', 'deep-rl', 'continuous-tasks', 'episodes']",
What is the meaning of $p_{\text {data }}(y)$ in the CycleGAN?,"
In the original CycleGAN paper, on the second page, there is a sentence that I didn't quite understand

In theory, this objective can induce an output distribution over $\hat{y}$ that matches the empirical distribution $p_{\text {data }}(y)$ (in general, this requires $G$ to be stochastic) [16].

What does $p_{\text {data }}(y)$ denote? Furthermore, I can't imagine the empirical distribution of it.
In the loss functions, there is also $x \sim p_{\text {data }}(x)$, but I also don't get the context there.
Could anyone please elaborate further and explain this sentence to me?
","['generative-adversarial-networks', 'probability-distribution', 'notation', 'generator', 'cycle-gan']",
Is it possible to learn the number of layers?,"
Is it possible, in a transformer or other deep architecture, to include the number of layers as a parameter of the model so it could be learned?
In fact, I have a keras layer that I use to change the final layer without rebuilding the model, so I can just change a parameter between epochs (The original use was to try to train deep networks starting from shallower ones, increasing the number of layers after each epoch).
class LayerSelect(tf.keras.layers.Layer):
    def __init__(self,nlevels,**kwargs):
        super(LayerSelect,self).__init__(**kwargs)
        self.nlevels = nlevels
        self.range=tf.range(self.nlevels,dtype=tf.float32)
                 
    def build(self, input_shape):
        self.kernel=self.add_weight(shape=(1,),
                                    initializer=tf.keras.initializers.Constant(min(self.nlevels,14.0)/1.9),
                                    trainable=True, dtype=tf.float32,
                                    constraint=lambda x: tf.clip_by_value(x,1.0,self.nlevels))
       
    def call(self,inputs): 
        selector=tf.math.maximum([0.0], 1.0 - 1.0 *(self.range-self.kernel)**2 ) 
        final=tf.reduce_sum(inputs*selector,axis=-1)
        return final

The layer expects an stack of hidden layers to choose from:
allEncoders=tf.stack([encoder[level] for level in range(layers)],axis=-1)
finalEncoderRaw=adhoc.LayerSelect(layers)(allEncoders)

So that by calling set_weights during the training I can choose as output any layer, or a combination of two, being the layer variable a float and using a wider selector, say 1.0 - 0.25 *(self.range-self.kernel)**2
And as you can expect, if I set the weight to be trainable, the optimiser moves the variable. But it keeps either moving randomly some small percent or moving backwards towards smaller values. So it is possible that this approach is a dead end?
If not a way to patch this method, is there another successful method to train the number of layers without using meta-parameter (hyperparameter grids) farms?
","['keras', 'hyperparameter-optimization', 'meta-learning', 'neural-architecture-search']","I like the idea, but I fear this approach may be a dead end. I see a few problems:Layers in front of (closer to the output than) the currently selected layer(s) don't affect the output, so they won't change and they can't learn to be good predictors for the true output.Layers behind the currently selected layer won't be trained to predict the final output (they'll be trained to provide outputs are useful inputs for the selected layer) so switching to those layers is unlikely to improve the output.And if another layer did provide a better approximation of the final output, the network is unlikely to switch to it unless it's next to the currently selected layer (i.e. the selection is likely to get stuck in a local minimum). For example, if layer 3 is selected, and 1 is a better prediction than 3, but 3 is better than 2, the network would stick with 3.is there another successful method to train the number of layers without using meta-parameter farms?I haven't heard of one, and that seems like something that would be widely shared if it worked well.I have seen genetic algorithms for selecting neural network architecture, but as far as I know they don't perform better than grid search for choosing the number of layers."
What considerations should I take to train my transformer model?,"
I want to train my vision transformer model on a benchmark for an image segmentation task: (LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation) (GitHub), but I don't get any acceptable result from my model.
I don't have a good result either in evaluation nor (obviously) in test.
I'm frustrated since I have trained my model with different parameters: learning rate : [0.001,0001, 6e-e].
The training datasets include about 3000 images, the number of epochs is 1500.
I have used different loss functions: Cross Entropy, Dice loss, Focal loss, and a combination of those losses.
So, I'm wondering if anyone has any experience that could help me find a solution and improve my model's performance?
","['training', 'transformer', 'image-segmentation', 'testing']",
"Mathematically, what is happening differently in the neural net during exploration vs. exploitation?","
I want to understand roughly what is happening in the neural network of an RL agent when it is exploring vs. exploiting. For example, are the network weights not being updated when the agent is exploiting? Or somehow being updated to a lesser degree?
If this question is algorithm-dependent, I'm mostly interested in PPO, but any insight is appreciated.
","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization', 'exploration-exploitation-tradeoff']","Typically, the NN is trained the same way whether an action is chosen for exploration or exploitation. Look at the objective (AKA loss) function for any algorithm you're interested in and you'll probably find that it doesn't contain a term for exploration vs exploitation.Instead, the choice of exploration vs exploitation, and the tradeoff, is about how the data used to train the neural network are generated.So in a rough sense, what's happening to the NN is that during exploration, the network is being trained on novel data (not always, but it's more likely), while during exploitation, the network is being trained on well worn paths.Then even though the gradient formula is the same, the gradient may be smaller and the weights may change less during exploitation, because the loss is typically lower when choosing the optimal action, and with some loss functions (like MSE, but not L1 loss) a bigger loss results in a bigger gradient."
Why is exploitation necessary during training?,"
I have read many blog articles making all kinds of broad analogies to explain the exploration/exploitation trade-off. However, I still can't fully grasp it. On an extremely abstract level, I understand why you would want to ""try new things to gain information"", but then I don't understand why you would want to ""exploit"" in training. It seems as though it would be better to keep trying as many things as possible to gain the most information.
What is the value of exploitation during training? Intuitively, I would think you would only want to explore during ""training"" and only exploit in ""testing"".
","['reinforcement-learning', 'exploration-exploitation-tradeoff']","An algorithm that chooses to always explore during training is unlikely to find an optimal policy because it will be employing a more random search as opposed to a directed search. During training, the neural network aims to determine the relation between states or state-action pairs and the reward signal through past experience. If the agent is always exploring during training, it will never use the experience gained from past episodes to influence its training policy and therefore will search more randomly.Exploitation during training allows the neural net to use its past experience to guide its future actions and avoid random search. There can be many states in which there is an obvious optimal action. After some training, the neural network may be able to quickly learn these states and corresponding optimal actions. By primarily exploiting at those states, the agent will not be wasting training time by exploring suboptimal actions at those states, allowing the agent to focus its exploration on other more uncertain, unexplored, or complex parts of the state space.For a practical example, consider the original Super Mario Bros game on NES. Let the reward be the number of pixels traveled to the right before Mario loses a life. If Mario is exploring the whole time, it is unlikely that he makes it very far to the right or over many obstacles, let alone to the flagpole. Since it is more rewarding in general to go to the right, Mario's exploitation action at most states is to run to the right. In this manner, Mario will usually run to the right until he reaches an obstacle (e.g. a pipe, pit, staircase, enemy). At that point, Mario may need to explore to overcome the obstacle, but Mario needed to exploit to be able to reach that obstacle in the first place."
Which RL algorithm should I use to learn an optimal weight vector?,"
What is the best practice in order to learn the optimal weight vector $W^*$?  By optimal I mean the weights that will produce the agent with the highest win-rate.
I have an agent that plays a imperfect information game and I want to find the optimal weights via Reinforcement learning.  Each turn, for each move $a$, the agent calculates a heuristic value, $h(a)$ that is a linear function of $n$ features.  That is to say, the heuristic value for move $a$ is
$$h(a) = w_1f_1(a)+ w_2f_2(a)+...+w_nf_n(a)$$
where $\forall i, w_i \in[0,1]$
The Heuristic agent plays a distribution over the moves that is corelated to the value of the moves (moves with higher value have higher probability to be played)

This question might be very basic, I am new to RL.
Currently, the agent uses $n=13$ features.
I have access to daily data of $10^6$ games of agent vs human.
I have a game engine that allows me to run agent-vs-agent games.
The Heuristic agent is a bit weaker than average recreational
humans (win rate of 49%).
The MCTS agent is a bit stronger than average human recreational (win rate of 58%).
I have no good reason to think that linear weights are optimal.  Just thought it's an easier start.
The Reward is observed only at the round's end.  There is no good way to evaluate the reward before the round's end.

","['reinforcement-learning', 'game-ai', 'algorithm-request']",
How to solve a reinforcement learning problem with a stochastic reward function?,"
In a discrete time system, an environment has an unknown reward probability $p(r|s,a)$. However, the transition probability $p(s'\mid s,a)$ is deterministic.
In my case, the reward for the same action can be drawn as a staircase line: it goes up gradually most of the time, but can occasionally go down when a special event happens. The state can capture the time elapsed since the last time the observed special event happened to each action object (some events couldn't be observed).
I know the reward function in RL could be represented as a probability. But I don't know if it's stationary or not.
Is this normal in reinforcement learning problems? Can I solve it using TD learning or actor-critic algorithms? Or POMDP could be the solution? Or maybe regret minimization for incomplete information could work too. In which direction should I go?
I'm new to RL. If you want more details, you can see my other question.
","['reinforcement-learning', 'rewards', 'algorithm-request', 'reward-functions', 'reward-design']",
Are there any online competitions for Reinforcement Learning?,"
Kaggle is limited to only supervised learning problems. There used to be www.rl-competition.org but they've stopped.
Is there anything else I can do other than locally trying out different algorithms for various RL problems? 
","['reinforcement-learning', 'kaggle']",
How to configure a neural network to selectively change only certain characters in a string?,"
I'm trying to figure out how to train a neural network to macronize Latin text.
Essentially, in Latin, vowels can either be long or short, and length is indicated with a macronized character: i.e. o is short, ō is long. Most advanced texts don't mark vowel length, which can be an issue because vowel length determines pronunciation and word meaning: liber means book, līber means free, for example.
Is there any way to train a neural network to replace only vowels in a word while contextually evaluating it as part of a larger sentence? I've tried training an RNN many-to-many model to map unmacronized text to macronized text, but it ends up being incredibly inefficient and inaccurate because an unmacronized word can map to any macronized word. Ultimately, the only characters in each word that should ever change are the vowels, and even then only to their macronized counterparts: liber can be liber, līber, libēr, or lībēr only.
There is an abundance of training data in the form of parallel macronized and unmacronized text. This feels like a simple question but I've been unable to find any resources specifically detailing how to implement this. Any pointers would be greatly appreciated.
","['neural-networks', 'natural-language-processing', 'applications']",
What is the domain of the discriminator of a GAN?,"
I've read that the discriminator $D$ validates an image $D(x)$, where $x$ is either a real image or a fake one created by the generator, i.e. $ D(G(x))$.
What does the function of the discriminator return? Is it either 0 (marked as fake) or 1 (discriminator thinks the image is real)? I have read that this function returns the whole $\mathbb{R}$, but I don't understand what the output then means.
","['objective-functions', 'generative-adversarial-networks', 'loss', 'discriminator']",
What exactly is the AI explainability problem?,"
I am pretty new to AI and have recently been paying attention to AI explainability and the fact that it remains a hurdle within the path of commercializing certain AI systems in health for instance. I tried to do some digging myself by starting with the gradient descent algorithm as an optimization technique used to model error of predictions. Ideally, I am aware it is most suitable for predicted errors that fit a Gaussian distribution. One question I have not been able to find answers to and would really appreciate help from this community
What exactly does the AI explainability problem refer to? Is it related to the convex optimisation (G.D.) process ? Or randomly generated coefficients? How exactly do we mean when we say AI is a blackbox?
p.s: If this has already been answered elsewhere on this platform do share a link.
Further clarification on my question:
Please, reference this tweet by Y. Lecun only yesterday, I would like some more responses on relationship between the AI explainability problem and the weight optimization technique. How or why is G.D applied to optimising parameter weights contribute to AI being a blackbox. Or if this is another separate problem from AI explainability also do clarify.
","['neural-networks', 'terminology', 'gradient-descent', 'explainable-ai', 'black-box']",
How to identify and diferentiate several edge lines of an object?,"
I want to create an AI to detect and identify certain edge lines on my image. The input image is a locker key, and I want to know the exact position of certain edges.
Sample input image:

Sample output image. As output i have each red line position.

I was thinking in finetuning some kind of instace segmentation or even yolo network but maybe there is a better approach. Ideally, I would like the software to be light enoguh to run in a mobilephone.
I also considered two ""simpler"" approaches:

Using corner detection
Using Unet to get the contour + opencv approxpoly to get straight lines coordinates

Nevertheless, I believe I would obtain a more consistent result with more advanced AI. (I do have a lot of training images to use)
","['neural-networks', 'image-segmentation', 'yolo', 'semantic-segmentation', 'edge-detection']",
How to solve a reinforcement learning problem with changing rewards?,"
I'm working on a problem with non-stationary environments. The state space is discrete and limited. The action is limited too. But the reward for the same action $a$ can change. Even the reward for the same $(s, a)$ pair can change with time too.
In my problem, a UAV flies and visits several devices that would go through non-stationary environments. Each visit of the UAV can result in a performance improvement for the visited device. This happens gradually until a new environment appears for this device.
So, the state is the time elapsed since the last time it visited each device. The action is the chosen device. The reward is the summation of the average cost of the current visited device and estimated rewards of other unvisited devices. This results in the fact that the UAV's reward can change. Most of the time, it increases gradually. Occasionally, it drops because of a new environment. Like a staircase going up, up, up, and suddenly down. Then it goes up and repeats. The time frequency of a new environment for each device is unique.
I tried the above state and reward settings. When I use Actor-Critic to solve it and only use the reward of the current visited device, the UAV tends to visit one device only and ignores other devices. But when I add the reward of other unvisited devices, no convergence can be seen. In this case, the UAV tends to visit only one device occasionally, even when the reward is extremely low.
My questions are:

Am I setting the state and reward in the right way?

What algorithm should I use here?


","['reinforcement-learning', 'rewards', 'reward-functions', 'reward-shaping']",
PPO: how to scale rewards,"
I have a custom PPO implementation and a problem that has costs rather than rewards, so I basically need to take the negative value for PPO to work. As the values are somewhat large, I've tried various normalization methods. The best one so far is to simply take the log of the cost, as normalizing with a running mean/std (using Welford's algorithm) seems to collapse the value loss to almost zero and learning becomes very unstable. Are there any guidelines for dealing with reward normalization in these cases?
","['reinforcement-learning', 'rewards', 'proximal-policy-optimization', 'reward-normalization']",
Machine Learning Models for Longitudinal Data,"
Recently, I had the following question about supervised classification models (e.g. random forest) for longitudinal data.
Suppose I have the following data about students passing a fitness test - the students (each student has an ""id"") who enroll in a school take a fitness test each year and record their height and weight (at the start of each school year, before the fitness test). They can either pass (1) or fail (0) the fitness test each year. The school is interested in knowing which students are likely to fail the fitness test, so they can focus more attention on these students. Naturally, some students might have taken the fitness test more times than other students.
I simulated some data (using the R programming language) to show how the historical data might look like:
score <- c(""1"",""0"")
score <- as.numeric(sample(score, 1000, replace=TRUE, prob=c(0.3, 0.7)))
id_sample <- 1:140
id <- sample(id_sample, replace = TRUE, 1000)
height <- abs(rnorm(1000, 150,5))
weight <- abs(rnorm(1000, 75,5))

data = data.frame(id, height, weight, score)
data <- data[order(data$id),]

I then added two variables to this data - one to show how many times the fitness test was taken, the another to show the (cumulative) average number of times the test was passed:
library(dplyr)
data =  data.frame(data %>% group_by(id) %>% mutate(counter = row_number(id)))
data$csum <- ave(data$score, data$id, FUN=cumsum)
data$average <- data$csum/data$counter

Now, suppose some of the students are about to take this test again and we would like to predict what their score will be - some of these students are existing students, but some of these students are new and have never taken the test before (i.e. they have no historical data):
id_sample <- 1:140
id <- sample(id_sample, replace = FALSE, 23)
height <- abs(rnorm(23, 150,5))
weight <- abs(rnorm(23, 75,5))

new_data = data.frame(id, height, weight)
new_data <- new_data[order(new_data$id),]

id_sample <- 141:200
id <- sample(id_sample, replace = FALSE, 5)
height <- abs(rnorm(5, 150,5))
weight <- abs(rnorm(5, 75,5))

#simulating data for students who never took the test before
n_data = data.frame(id, height, weight)
n_data <- n_data[order(n_data$id),]

test_data = rbind(new_data, n_data)

Now, to this test data, (where applicable) I added ""longitudinal variables"" that take into account the number of times the students took the test and their most recent average cumulative score:
#counter
max = data.frame(data %>% 
             group_by(id) %>%
             filter(counter == max(counter)))

colnames(max)[5] <- ""max_counter""

max$max_counter = max$max_counter + 1

test_with_counter =  merge(x = test_data, y = max, by = ""id"", all.x = TRUE)

test  = test_with_counter[, c(1,2,3,7,9)]

 test$max_counter[is.na(test$max_counter)] <- 1

 test$average[is.na(test$average)] <- 0

#formatting
colnames(test)[2] <- ""height""
colnames(test)[3] <- ""weight""
colnames(test)[4] <- ""counter""
data$csum = NULL
data$score = as.factor(data$score)

At this point, there is nothing stopping me from training a supervised classification model (e.g. random forest) to predict the ""score"" variable for the test data:
#skip cross validation for brevity of question
library(randomForest)
rf <- randomForest(score~., data=data)
pred = predict(rf, newdata = test)

print(rf)

Call:
 randomForest(formula = score ~ ., data = data) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 23.4%
Confusion matrix:
    0   1 class.error
0 636  79   0.1104895

My Question: Does the approach that I have proposed for supervised classification of longitudinal data sound reasonable (e.g. better than ""nothing"") - or are there any major statistical flaws on this approach (e.g. structural multicollinearity, variance inflation, etc.) ? Or is it better to use some supervised classification model/software implementation that has been specifically designed for longitudinal data (e.g. https://cran.r-project.org/web/packages/LongituRF/LongituRF.pdf)? Thanks!
Note:

This is a rough sketch of the situation I am dealing with - I am also planning to include variables such as ""number of days that elapsed since last fitness test"".

The sample data in this stackoveflow question is randomly simulated and obviously wont show any longitudinal trends.

I have heard that models such as Random Forest have the ability to recover/model around complex interactions and correlations within the data that otherwise need to be explicitly specified in standard supervised models (https://ishwaran.org/papers/IKBL.AOAS.pdf).


","['classification', 'supervised-learning', 'random-forests']",
How to use a trained neural network to find optimal function inputs? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I have a deep neural network with 4 input nodes, 4 hidden layers with 4 nodes each and 1 output layer with one node (TRUE, FALSE). I have already trained the NN using backpropagation because I have labelled data. That four input parameter values trained my model on test data to get the output. My model is trained now with optimal weights.
I have already trained my model using that labelled training data. Now using that model I want to get the new values of that 4 input parameters which give me accurate results. After that, I want to calculate the delta between the actual 4 input parameters and tunned-input parameters.
In summary, my question is how I can manipulate(tune) my input parameter, based on that model which I created using labbeled data.
","['neural-networks', 'deep-learning']",
"For continuing tasks, is the choice of episode length completely arbitrary?","
Let's say I'm training a reinforcement learning agent to act in some environment that perpetually continues to give the agent opportunities to earn rewards, and there is no cap on the score and there is no way to ""win"". That is, there is no natural ""end"" to an episode.
In these scenarios, is the choice of episode length completely arbitrary? Is it easier to train on shorter episodes than longer ones?
","['reinforcement-learning', 'hyper-parameters', 'continuous-tasks']","There are things that impact ideal pseudo-episode length for learning continuing (non-episodic) environments:Start state. The start state of a continuing environment may be special in some way and unreachable later on. It is still important to learn optimal behaviour, and some choices may even be critical in order to reach the best areas of state space. If there is a standard/fixed start state, it means you may want to reset and restart pseudo-episodes more frequently.Statistical coverage of repeating states. In a continuing environment under a fixed policy, there should be a set of ergodic states. That is a set of states that are visited at some expected long-term frequency (or frequency density for non-discrete state spaces) due to the combined behaviour from the policy and environment transitions. You want to sample from this set fairly, in an unbiased manner, in order to accurately learn expected values. This means you may want to reset and restart pseudo-episodes less frequently, to get long trajectories where the start state distribution has less impact on the relative frequency of each state in this repeating set.These two issues are at odds with each other, so choosing a pseudo-episode length (when to artificially stop and re-start) is a hyperparameter that may require experimentation. The hyperparameter interacts with others, such as discount factor - for example a small discount factor may mean you care less about fair sampling of continuing states, although you will still care about visiting enough of them to get estimates from all reachable states.There can be other issues that impact your choice, such as the RL method you are using, whether you are using approximation, whether there are some form of ""attractors"" that tend to get the agent stuck in inescapable loops. If you know you have these kind of trap state loops in your environment, you may want to treat that as similar to having special start states, and reset more often.In these scenarios, is the choice of episode length completely arbitrary?No, it is a hyperparameter that you may be able to take an educated guess at or figure out from observing behaviour.Is it easier to train on shorter episodes than longer ones?It depends on the environment, and there is no generally applicable preference for short or long pseudo-episodes."
"Why are only neural networks (and not SVMs, for example) used for reinforcement learning?","
I know that neural networks are the ""universal function approximator"", but they also have a huge number of trainable parameters and are extremely prone to overfitting.
So my question is: Why aren't SVMs or Random Forests used as the mediators in decision-making?
I have read this response, and it highlights very broadly why NNs are not required but are generally good for the job.
This does not answer my question, though, as I want to understand why no popular implementations (that I've seen) use anything but neural networks, and why the vast majority of reinforcement learning literature talks about Deep RL and not, for example, ""SVM-RL"" or something like that.
My understanding is that this is technically possible - but why is it not being done often or at all?
","['neural-networks', 'reinforcement-learning', 'function-approximation']","The biggest problem with SVMs, random forests, gradient boosting and others for reinforcement learning (RL) is that they are not able to learn online, adjusting for new data as it arrives, and equally importantly forgetting older data. These algorithms learn from fixed datasets, and must be re-trained with whole new datasets if the target dataset changes, in scenarios also known as non-stationary problems.This is a major issue in reinforcement learning for control problems (when you want to find an optimal policy). RL control problems always generate non-stationary target data, because they maintain a current policy - or for off-policy approches, current target policy. That current policy then changes over time as the agent improves its estimates (of e.g. the Q table), which means it is necessary to forget older training data and replace with newer.So one requirement in RL is for online learning algorithms that can handle non-stationary data well - both incorporating new data and smoothly forgetting older data. That includes neural networks. There are not many other non-linear approximators that can do so simply.It is possible to compensate for this lack of online learning in various ways. Some of the model classes, such as random forests and support vector machines, can be extended to learn online - although this seems not to be explored much in deep RL, perhaps due to complexity, or limitations that make it still less flexible choice than neural networks (opinion: looking at the recent dates on these papers compared to the longer history of using NNs with RL, there may be an element of intertia too, and possibility for revisiting using these models in a RL context).You could modify the outer algorithm to be more like policy iteration, collecting large amounts of data for the current policy to form a large batch that is then learned from to replace the previous approximate value function and in turn update the policy from that. That would work for most approximators in theory, but may be very slow and introduces hyperparameter of batch size that will have a large impact.There are unwanted side-effect to online learning in RL using approximators, such as the risk of catastrophic forgetting, where a well-trained agent is not able to distinguish bad choices from good ones because it has not experienced a bad decision for many time steps, and has generalised that ""everything is good now"". This would not be fixed in RL by using large batches and non-online algorithms - if anything it could be worse in those cases, because they would effectively be forced to forget everything between batches. The usual fix for catastrophic forgetting in RL is adding some management of the dataset - e.g. keeping some amount of older experience even though it is technically not part of the current policy's generated data.they also have a huge number of trainable parameters and are extremely prone to overfittingOverfitting of approximators can happen, but may be less of an issue in practice with RL compared to smaller fixed datasets, because agents are continuously and actively generating new data. Complex environments with lots of resources used to generate experience behave in a lot of ways like ""big data"" datasets, playing to the strengths of large flexible models.In RL there is commonly an overlap with processing image data (and possibly other sensors with return data that can be structured into some grid or graph). In the problem domains related to image comprehension, neural networks are state of the art, so it is no surprise to see them used for approximating action values in the orginal DQN paper for example - with the right architecture NNs are measurably superior, in terms of error metrics, to SVMs, RFs, boosting et al, when fitting this kind of sensor data."
"In Value Iteration, why can we initialize the value function arbitrarily?","
I have not been able to find a good explanation of this, other than statements that the algorithm is guaranteed to converge with arbitrary choices for initial values in each state. Is this something to do with the Bellman optimality constraint itself?
It's hard to see that this is true intuitively since my intuition states that there ought to be ways in which an arbitrary choice of values could cause the value function to converge incorrectly compared to the ground truth. For instance, what if a state that in practice had low reward is assigned a very high initial reward value in value iteration? Would the algorithm not construct a value function that highly values trajectories passing through that state?

","['reinforcement-learning', 'machine-learning', 'value-functions', 'sutton-barto', 'value-iteration']","If the value function of a state $v(s)$ is relatively high, then you are absolutely correct in saying that a greedy policy may choose to visit $s$, since the high $v(s)$ makes it very promising. The key idea here is that the update rule of value iteration will gradually change the value function and likewise will gradually change the policy.Suppose that the optimal value function $v_*(s)$ of a specific state $s$ is low, yet the value function $v(s)$ is initialized much higher. Then, the update rule in the pseudocode you provided will eventually decrease $v(s)$ to $v_*(s)$. To see this fact intuitively, note that $v(s)$ can be decomposed as the expected return (sum of rewards) from that state:
$$v(s) = \mathbb{E}[R_k + R_{k+1} + R_{k+2} + \ldots + R_T|S_k = s].$$
If the expected return from $s$ is smaller than $v(s)$, then $v(s)$ will decrease with the aforementioned update. The actual update rule approximates the expected return from $s$ using the next reward $R_k$ and the value function of the next state $v(s')$ via bootstrapping. Note that $v(s')$ can be decomposed as follows:
$$v(s') = \mathbb{E}[R_{k+1} + R_{k+2} + \ldots + R_T|S_{k+1}=s'].$$
Therefore, $v(s')$ quantifies the part of the return aside from the next reward $R_k$, and the update rule indeed quantifies the expected return from $s$. If $v(s')$ is initialized too high or low, then the update rule will simultaneously be adjusting it to the optimal $v_*(s')$ while adjusting $v(s)$ closer to $v_*(s)$.Once the value functions change as described above, then the greedy policy may also change and favor other states aside from $s$ with higher value functions."
"Is there a way to easily simulate video games, without actually rendering the pixels on screen?","
Youtube was recently suggesting to me videos of people training NEAT neural networks for video games. I've noticed that often the training process was quite slow (for example in this Trackmania example).
Is there a way (algorithmic approach or an idea) to easily simulate video games, without actually rendering the pixels on screen and making the training much quicker? In addition to that, if you also know of a tool that does that, please, share it with me.

Update:
Following the Trackmania example, I found out the youtuber uses a standard tool called ""TMInterface"", and the webpage of the tool states it is a ""TAS tool"" - Tool-assisted speedrun.
I'll investigate how such tools work and whats the idea behind them (and if indeed they do what I think they do). Will update!
","['reinforcement-learning', 'machine-learning', 'neat', 'resource-request']",
Is there any way to train a regression model with negative values that is more stable?,"
I have a regression model where my target values contain roughly 60% negative values and 40% positive values. My model architecture includes a robert-large, 1 linear layer. I trained it after 1 epoch, the loss goes down to 0.089, but when I try to predict on test-set, every samples have the same values.
I try to add tanh activation in the last layer and switch to roberta base model, this time the model predict different values, on the train set, it predicts positive and negative values but on the test-set, it only produces positive ones.
Is there any way to train a regression model with negative values that is more stable?
","['machine-learning', 'natural-language-processing', 'activation-functions', 'prediction', 'regression']",
How it is possible to compress audio with the image representation of an audio?,"
In most of the speech compression using machine/deep learning, I have seen that, in order to process an audio file, we need to convert it into a mel spectrogram format, then this spectrogram is analyzed.
Do we apply the compression process on this mel spectrogram? How it is possible to compress audio with the image representation of an audio? Is it possible that we have to represent the audio in frequency format and here we represent higher frequency with brighter color in the image? Then as in JPEG compression, we can remove the higher frequency signal, which humans can't hear?
","['machine-learning', 'data-compression']",
How can I generalize a machine learning model to multiple curves?,"
I have a family of convergence curves as you can see in the image below:

I would like to train a model that fits reasonably well to all the curves at the same time in my dataset. Is it possible? Do you have any suggestion? It could be a classical econometric model or even machine learning / deep learning models.
","['machine-learning', 'deep-learning', 'model-request']",
Is N the total number of nodes in the frontier plus the number of nodes in the explored list?,"
I'm studying fundamentals of AI from the classic Russell-Norvig book (3rd edition). I have a small doubt about the effective branching factor, which is defined as follows (section 3.6.1, p. 103):

One way to characterize the quality of a heuristic is the effective branching factor $b^{*}$. If the total number of nodes generated by $\mathrm{A}^{*}$ for a particular problem is $N$ and the solution depth is $d$, then $b^{*}$ is the branching factor that a uniform tree of depth $d$ would have to have in order to contain $N+1$ nodes. Thus,
$$
N+1=1+b^{*}+(b^{*})^{2}+\cdots+(b^{*})^{d}
$$

So, is $N$ the number of nodes in frontier + the number of states in the explored list, or should I also consider the generated nodes those that were generated but then discarded because already in the frontier or explored?
Also, I tried implementing a program that between the other things can calculate the effective branching factor for instances of a specific problem and I noticed two things:

When the problem is 'large' but the solution is 'close"" (like two actions from starting state), the computed $b^{*}$ is HUGE (it even got as high as 500).

In the same conditions as before it happened that $b^{*} > b$ (I'd dare to say in some cases $b^{*} \gg b$).


Is all of this reasonable?
","['search', 'heuristics', 'a-star', 'norvig-russell', 'branching-factors']",
Why aren't neural networks contractions?,"
I'm not sure I understand why neural networks aren't considered contractions, as Geoffrey J. Gordon says in his paper: Stable Function Approximation in Dynamic Programming:

""Our theorems in the following sections will be based on two views of function approximators.
First, we will cast function approximators as expansion or contraction mappings; this distinction captures the essential difference between approximators that can exaggerate changes in their training values, like linear regression and neural nets, and those like k-nearest-neighbor that respond conservatively to changes in their inputs.
Second, we will show that approximate temporal difference learning with some function approximators is equivalent to exact temporal difference learning for a slightly different problem.
...
Definition: A function $f$ from a vector space $S$ to itself is a contraction mapping if, for all points $a$ and $b$ in $S$, $|| f (a) - f(b) || \le \gamma || a-b ||.$ Here $\gamma$, the contraction factor or modulus, is any real number in $[0,1]$. If we merely have $|| f (a) - f(b) || \le || a-b ||$, we call $f$ a nonexpansion.
$\qquad$ For example, the function $f(x) = 5 + \frac{x}{2}$ is a contraction with contraction factor $\frac{1}{2}$. The identity function is a nonexpansion. All contractions are nonexpansions.

I understand that at least in practice they've been shown to greatly exaggerate differences between similar target functions, but I suppose there's a theoretical explanation for this?
","['neural-networks', 'reinforcement-learning', 'deep-rl']","why neural networks aren't considered contractions, as Geoffrey J. Gordon says in his paper.I am not sure how strongly you mean aren't considered, if you mean it in a strong sense or weak sense.Contraction theory is probably not very well known in the AI community, but I did find some material to suggest contraction theory is being applied to NNs.Perspectives on Contraction Theory and Neural Networks, Bullo 2022
Learning-based Adaptive Control using Contraction Theory, Tsukamoto, 2021The use of DNNs permits real-time
implementation of the control law and broad applicability to a
variety of nonlinear systems with parametric and nonparametric
uncertainties. We show using contraction theory that the aNCM
ensures exponential boundedness of the distance between the
target and controlled trajectories in the presence of parametric
uncertainties of the model, learning errors caused by aNCM
approximation, and external disturbances.In the weak sense specific NNs can be considered contractions, but in general contractions must be uniformly continuous and it's not clear that in the strong universally true sense for all cases of NNs. There may be some Networks, that for some set of inputs may reveal some discontinuity.Also function f must be a mapping of S to itself.
In image classification we are not mapping S -> S but instead are mapping R^MxNxC -> R^n  where n is the number of classes (or 1 when using integer encoding)Its also not clear that all networks would satisfy the condition:|| f(a) - f(b) ||
---------------   <= k < 1 
|| a - b ||Suppose we had a vector a of size n of all 1's [1,1,1,...] and a vector b of all zeros of size n [0,0,0,...] then their norm || a - b || = sqrt(n)
a neural network would need to mapa -> a'= [u,u,u,...] 
b -> b'= [v,v,v,...] such that:
|| a'-b' ||/sqrt(n) = sqrt(n*(u-v)) = sqrt(n)*sqrt(u-v) < 1 for all nSo to summarize I would say:"
Why do we train the discriminators k times but train the generator only 1 time in a iteration in GAN?,"
In this paper https://arxiv.org/abs/1406.2661 , the codes for training a gan are :

Why do we train the discriminator for $k$ steps while the generator only for $1$ step? Why not the other way around?
","['deep-learning', 'training', 'generative-adversarial-networks', 'generator', 'discriminator']","The answer to your question can be found in [1, sec. 4.4]. Briefly, the GAN optimization problem is a mini-max game, and early on the proposition of GANs, the authors had the idea that one should balance the power/optimization of these two actors. Here is a quote from the said reference:The author’s present belief is that GANs work by estimating the ratio of
the data density and model density. This ratio is estimated correctly only when the discriminator is optimal, so it is fine for the discriminator to overpower the generator.It appears that, despite their initial claims in [2], the authors do not believe that this is the case anymore. In [1], Goodfellow explains,The idea that the discriminator should always be optimal in order to best
estimate the ratio would suggest training the discriminator for k > 1 steps
every time the generator is trained for one step. In practice, this does not
usually result in a clear improvement.To summarize: when the discriminator is near optimality, the ratio between densities ($p_{data}$ and $p_{model}$) is mode accurate, generating better gradients to update $p_{model}$ a.k.a. the generator. If you choose to do the other way around, you would be updating the generator successively with poor gradients.[1] Goodfellow, I. (2016). Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160.[2] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27."
"CNNs: What does ""number of filters"" mean?","
I understand how depth, kernel size, stride, and padding works when dealing with filters in a spatial convolution layer.
What I don't understand is ""the number of filters"". Does that mean you're running the same kernel with the same stride at the same channels in the same directions but just multiple copies of that process in parallel?
Would that not result in it converging to the same weights and getting the same feature maps?
","['convolutional-neural-networks', 'filters']",
When should I use an MARL approach instead of training one agent while keep the others fixed?,"
I have built a custom multi-agent environment with PettingZoo, where a turn-based game with two agents, A and B, is setup.
I want to examine situations where malicious behavior may arise, given the game rules, and I am looking into training approaches.
To do that, I have implemented a deterministic policy as a baseline / control.
Fixing agent A to that baseline policy, I want to subsequently train agent B and observe the resulting behaviors.
After B arrives at a desirable behavioral pattern, I want to train agent A to see how it responds to B's actions.
Having the above setting in mind:

Is the above training approach, which keeps one agent fixed and trains the other, correct?
Should I follow a MARL approach for training instead, or is the above approach that encapsulates one agent as part of the environment sound?


In general, what are requirements / desiderata to look for that hint that a MARL approach is the correct way and/or a separate training scheme is erroneous?
","['reinforcement-learning', 'multi-agent-rl']",
Do I need to create one or many neural networks to play Risk? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I have a school project to develop an AI model that plays the Risk board game as optimally as possible. Now, I have made the environment of Risk in Python and I narrowed down my possible machine learning solutions to either a neural network or using a genetic algorithm. I was thinking of using a neural network but I have a question that I can't find the answer to. I also tried searching online on Github and other thesis.
Do I have to create 1 NN to control everything? Or do I have to make one for attacking decisions, one for decisions regarding moving troops etc.?
","['neural-networks', 'machine-learning', 'game-ai']","What a fantastic problem!  Also, welcome to AI.The challenge, and it isn't terrible, is that you have to build an NN that can ingest the problem.  How do you pose the problem to the learner?Here is how they setup AlphaGo:
What is the input to AlphaGo's neural network?There are lots of bad ways to do this, so lets look at things that have to be done to not do it as badly.The input format must communicate:It would also be helpful if it could communicate:There is a decent open-source MuZero on GitHub.  You could spend horsepower defining the game and states, but you would also need to make sure the learner was capable of figuring things out eventually."
Why don't we also need to approximate $p(x \mid z)$ in the VAE?,"
In the VAE, we approximate the probability distribution $p(z \mid x)$, where $z$ is the latent vector and $x$ is our data. The reason is that $p(z \mid x)$ becomes impossible to calculate for continuous data because of $p(x)$, which require integration (not in closed form) to be solved.
But why don't we also need to approximate $p(x \mid z)$?
What I can guess here is that, in VAEs, we assume $p(z)$ (prior), so we are able to calculate $p(x \mid z)$, but for $p(x)$ we can't assume its distribution? Is it right?
","['generative-model', 'variational-autoencoder', 'probability-theory', 'variational-inference']","What I can guess here is that, in VAEs, we assume $p(z)$ (prior), so we are able to calculate $p(x \mid z)$, but for $p(x)$ we can't assume its distribution? Is it right?You could assume $p(x)$ is some distribution (e.g. the Gaussian $\mathcal{N}(0, 1)$), but that assumption might be completely wrong - you can also check that against your given data (i.e. if your data is unlikely under your assumed distribution, then maybe your assumption is unreasonable - see this or this).The ultimate goal of a generative model, like the VAE, is to be able to generate data (so you want to use it for tasks like image denoising).
So, you want to learn a probability distribution that approximates the distribution from which your given dataset was sampled that would allow you to sample more similar data. In practice, you want to learn a probability distribution of the form $p(x, z)$, where $z$ is some latent random variable, which we assume to be involved in the process of generating $x$ (a random variable that represents the type of data that you have), i.e. $x$ depends on $z$.By definition, $p(x, z)$ can be written as $p(x, z) = p(x \mid z)p(z)$ or $p(x, z) = p(z \mid x) p(x)$. If you put together these equations, you obtain the Bayes rule/theorem $p(z \mid x) = \frac{p(x \mid z)p(z)}{p(x)}$, where $p(z \mid x)$ is the posterior, $p(x \mid z)$ the likelihood, $p(z)$ the prior, and $p(x)$ the marginal (likelihood) or evidence.In the VAE, we use the variational distribution, $q_\phi$, to approximate $p_\theta(z \mid x)$, but we also learn $p_\theta(x \mid z)$ - in fact, that's the decoder - and we learn it jointly with the variational distribution by maximizing the ELBO. We maximize the ELBO because it's a lower bound on $p(x)$, so, by maximizing it, we're also maximizing the marginal likelihood (aka evidence) $p(x)$, i.e. we're trying to find the parameters $\theta$ and $\phi$, such that $x$ is more likely to have been sample from $p(x)$, which is unknown. See the VAE paper (in particular, algorithm 1) for more details.Once the VAE is trained, you have $q_\tilde{\phi}(z \mid x)$ and $p_\tilde{\theta}(x \mid z)$. If $p_\theta(z \mid x)$ is the posterior, then $q_\tilde{\phi}(z \mid x)$ is an approximation of the posterior. Now, you can generate data with the VAE as followsSo, together, $\color{blue}{p_\tilde{\theta}(x \mid z^i)}$ and $\color{red}{q_\tilde{\phi}(z \mid x)}$ approximate $p(x, z) = \color{blue}{p(x \mid z)} \color{red}{p(z)}$.It might be possible to come up with a new objective function (different from the ELBO), where we use some kind of variational distribution to approximate $p_\theta(x \mid z)$, but, in the VAE, that's not the case."
What are semantic word spaces in NLP?,"
In the abstract of this paper, it's written

Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way.

I would like to understand what semantic word spaces are. I find Wikipedia's explanation unsatisfactory:

Semantic spaces in the natural language domain aim to create representations of natural language that are capable of capturing meaning.

How exactly are these representations computed? For example, if we take the sentence

I like reading books.

What would be its semantic word space? Or am I confusing semantic word spaces with semantic spaces here?
","['natural-language-processing', 'terminology', 'papers']",
PPO: policy loss becomes nan [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I'm implement PPO for a very specific problem, and it seems to be working somewhat, but after a few epochs, I always get something like this:
/home/antoni4040/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:
  File ""/home/antoni4040/Documents/Toulouse/Toulouse-Optimizer/toulousePPO.py"", line 439, in <module>
    agent.train(120)
  File ""/home/antoni4040/Documents/Toulouse/Toulouse-Optimizer/toulousePPO.py"", line 298, in train
    self.learn(ep=epoch, lr=learning_rate, clip=clip)
  File ""/home/antoni4040/Documents/Toulouse/Toulouse-Optimizer/toulousePPO.py"", line 169, in learn
    clipped_ratio * sampled_normalized_advantage )
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File ""/home/antoni4040/Documents/Toulouse/Toulouse-Optimizer/toulousePPO.py"", line 439, in <module>
    agent.train(120)
  File ""/home/antoni4040/Documents/Toulouse/Toulouse-Optimizer/toulousePPO.py"", line 298, in train
    self.learn(ep=epoch, lr=learning_rate, clip=clip) 
  File ""/home/antoni4040/Documents/Toulouse/Toulouse-Optimizer/toulousePPO.py"", line 197, in learn
    total_loss.backward()
  File ""/home/antoni4040/anaconda3/lib/python3.9/site-packages/torch/_tensor.py"", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File ""/home/antoni4040/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py"", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'MulBackward0' returned nan values in its 0th output.

I started printing the losses and it turns out that the policy loss becomes nan.
I do something like this for the policy loss:
dist, value = self.model(batchStates)
new_probs = dist.log_prob(batchActions)
ratio =  torch.exp(new_probs - batchOldProbs)

clipped_ratio = ratio.clamp(min=1.0 - clip, max=1.0 + clip)
policy_loss = torch.min(ratio * sampled_normalized_advantage,
                   clipped_ratio * sampled_normalized_advantage )
policy_loss = policy_loss.mean()

Should I add a small value to the ratio (1.0e-6 or something), or is it something else?
","['proximal-policy-optimization', 'probability-distribution', 'loss']","You might want to try substituting the exponentiation with a piecewise-defined function that uses a numerical approximation that is more numerically stable for low values of the exponent, such as using $e^x \approx 1 + x$ (First-order Maclaurin series) when x is under a certain threshold"
Is it possible to combine two policy-based RL agents?,"
I am developing an RL agent for a game environment. I have found out that there are two strategies to do well in the game. So I have trained two RL agents using neural networks with distinct reward functions. Each reward function corresponds to one of the strategies.
If I use Q-Learning or in general value-based methods, it is easy to combine the results of the two agents to select the action that maximizes the overall value. One could just add the values of each action from two agents.
My question is, is it possible to combine the result for a policy based method, e.g. PPO? The output of policy based methods is a probability distribution over actions, and I am not sure how to combine them.
","['reinforcement-learning', 'deep-rl', 'value-based-methods', 'policy-based-methods']",
How to create a loss function that penalizes duplicate indices in the output tensor?,"
We're working on a sequence-to-sequence problem using pytorch, and are using cross-entropy to calculate the loss when comparing the output sequence to the target sequence. This works fine and penalizes the model correctly. However, we also have the added constraint that the prediction can't contain repeated indices, e.g.
good: [1, 2, 3, 4, 5]
bad: [1, 2, 2, 4, 5]
We would like to add an additional penalty term that punishes the model further for producing duplicate indices, which would be added to the cross-entropy loss.
How would I construct this additional loss function in pytorch?
PS: Yes it's true that we could just hack the code-generation piece to not generate duplicate indices and then incorporate this into our beam-search, but I would like to first see whether this additional constraint produces a better model!
","['objective-functions', 'optimization', 'pytorch', 'constraint-satisfaction-problems', 'constrained-optimization']",
Trouble writing the backpropagation algorithm in python through crossentropy and softmax,"
so I am writing my own neural network library for a class project and I got everything working for a simple 2-class test using the distance (L2) cost function. I wanted to get a similar result using softmax and crossentropy instead.
I did the derivation of the formulas as in this article, but I tried computing the derivative of the cost function w.r.t. its inputs and the Jacobian of the softmax outputs w.r.t. the inputs separately.
This is what I did:
So, let $Y$ be the matrix with rows equal to the expected class probabilities, $\hat{Y}$ the outputs of the softmax layer, $X$ the inputs of the softmax, all of these matrices of size $n\times c$ (samples times classes). The cost function $J(Y,\hat{Y}) = -sum(Y * \log(\hat{Y}))$ where * is element by element multiplication.
Then, if $E$ is the cost function, I first compute
$$\frac{\partial E}{\partial \hat{Y}_j^{(i)}} = -\frac{Y_j^{(i)}}{\hat{Y}_j^{(i)}},\:\: i=1,\cdots,n : j=1,\cdots,c$$
This is a $n\times c$ matrix. Then, being $S$ the softMax function I compute the Jacobian of the softmax at each row of $X$, so a total of $n$ matrices of $c\times c$ dimensions.
$$JS^{(i)} = \left(\frac{\partial \hat{Y}_j^{(i)}}{\partial X_k^{(i)}} \right)_{j,k=1,\cdots,c},\:\: i=1,\cdots,n$$
Finally to compute the partial derivative of the error with respect to the inputs of the softmax I multiply the rows of $\frac{\partial E}{\partial \hat{Y}}$ with each jacobian, to get a new $n\times c$ matrix.
This way I have everything needed to compute:
$$\frac{\partial E}{\partial X_j^{(i)}}= \sum_{k=1}^c \frac{\partial E}{\partial \hat{Y}_k^{(i)}}\frac{\partial \hat{Y}_k^{(i)}}{\partial X_j^{(i)}} ,\:\: i=1,\cdots,n : j=1,\cdots,c$$
Is this correct? Where is the problem if not so? Might the numerical errors make everything fail?
Thanks in advance.
","['backpropagation', 'cross-entropy', 'softmax']","I found the bug on my code, now everything works just fine, so I am fairly sure that the derivation of the formulas is on point. Optimisation wise, clearly using the short formula on the end of the article is better and less prone to errors than using the full derivation."
Embedding Quality of Transfer Learning model vs Contrastive learning model,"
I am working on Contrastive learning which is a technique to learn features based on the concept of learning from comparing two or more instances.
The downstream task is a classification problem.
Transfer Learning
Due to limited data, I tried to use Transfer learning model trained on ""Imagenet""(ResNet50 V2 ""Deep Residual Learning for Image Recognition Kaiming He, et.al"").
I used the embedding from the pretrained model and trained Linear SVM and achieved a F1 score of 0.84.
Contrastive Learning
I also trained a model for contrastive learning using Facenet technique(""FaceNet: A Unified Embedding for Face Recognition and Clustering Florian Schroff, et.al"") and further used the embedding for training a Linear SVM for classification problem. The achieved F1 score is 0.83.
Problem
Though the scores of both the concept are closeby, I tried to evaluate the quality of both the embeddings using Silhouette Coefficient.
Overall Silhouette Coefficient:

Transfer Learning Embedding = 0.05
Contrastive Learning Embedding = 0.49

I do not understand this behaviour of the system that even with lower Silhouette Coefficient, the transfer learning model is able to perform well.
Kindly provide me with your views on it
","['machine-learning', 'deep-learning', 'transfer-learning', 'contrastive-learning']",
How to properly combine multiple readings/measurements?,"
In an AI application (for example, self-driving), there are usually many different reading devices/sensors to ensure the outcome is correct. More specifically, a self-driving car can use object tracking with cameras, road-integrated optic fiber, sound analysis, and so on.
In many cases, these readings can be very difficult to integrate into a single model and get a single output. Assume each device provides an independent output. What are some ways that I can do to combine them and find out the most likely readings?
For instance, device A says there are 10 people standing 5 meters away, B says there are 5 people standing 10 meters away, and C says there are no people at all. The easiest way is to do a weighted average/voting, but it can make more correct readings less effective. If A is totally precise 95 percent of the time, no matter how much weight I assign to A, it will be affected by the less accurate ones.
","['machine-learning', 'applications', 'autonomous-vehicles', 'ensemble-learning']",
"In RL, is it possible to design a multiplicative/exponential reward function? A reward func that depends on current accumulated reward?","
In the context of my problem, the ""true"" reward is not additive. Realistically, the more reward the agent has already accumulated, the easier it becomes to accumulate even more. That's to say, the real reward function is partially dependent on previously accumulated reward.
Is there any way to implement this kind of dynamic successfully?
I have tried to, but for some reason, the agent completely stops learning when I do this. I can implement a linear/additive reward function and the agent does learn good behaviors, but I feel that it's important for the agent to ""understand"" the true reward dynamic.
Essentially, here is the reward function I have:
reward = points_gained_this_step

But here what I need:
reward = points_gained_this_step*(total_score_so_far)
total_score_so_far = total_score_so_far + reward

Has anyone ever worked with something like this? Any ideas/insight for how to implement such a reward? I might be wrong, but it seems to me like part of the problem is exploding/vanishing gradients?
EDIT: I have already added ""total_score_so_far"" to my observation space.
","['reinforcement-learning', 'deep-rl', 'reward-functions', 'reward-design']","The main thing you will need to do is add the accumulated reward (total_score_so_far) to the state. In order to predict future reward with any accuracy, the agent is going to need to know this.You may still have some problems after doing this. The final return that the agent needs to predict is likely to have the following traits:A value that could range by orders of magnitude, making it hard to scale loss functions. You may be able to base the loss function for value predictions on mean relative error to help with this.Large variance, making it difficult to learn expected returns, especially from the impact of early decisions. If your immediate rewards have any random element, this could be a major problem.If value-based methods such as DQN are struggling in your case, and the optimal policy function is straightfoward, then you may want to try policy gradient methods, maybe even basic REINFORCE, to avoid needing to predict returns whilst dealing with such a high variance. You will still need to take care with scaling, as policy gradient methods scale update steps based on the return."
"In the MuZero paper, how does backprop in the MCTS account for the immediate reward from each edge?","
On page 12 of this paper: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model, it describes how MCTS works for the MuZero algorithm. It states in equation 4 that during the 'backup' after a simulation, the mean value (Q) for every edge in the simulation path is updated by:
$$Q\left(s^{k-1},\ a^k\right){\colon=}\frac{N\left(s^{k-1},\ a^k\right)Q\left(s^{k-1},\ a^k\right)+G^k}{N\left(s^{k-1},\ a^k\right)+1}$$
where $G^k$ is the return from depth $k$ and onwards in the simulation.
However, I don't see how this equation accounts for the immediate reward attained by transitioning from state $s^{k-1}$ to $s^{k}$.
Since $Q\left(s^{k-1},\ a\right)$ is used to determine what action to select from state $s^{k-1}$, shouldn't the backprop use the return from k-1 ($G^{k-1}$), to update the mean value instead of $G^k$?
","['reinforcement-learning', 'papers', 'monte-carlo-tree-search', 'muzero']","Let's look at the first simulation where $k = 1$.
We have $G^0 = r_1 + \gamma v^0$ and $G^1 = v^1$, and we can pick one of them as the update target. The difference is that $G^1$ is fully conditioned on the action $a_1$ we just took, while $G^0$ is only partially so.
We choose $G^1$ for the same reason that we do the search: we assume the evaluations of child nodes conditioned on more actions is better than the evaluation of the root alone.EDIT:
I believe the expectation should match so that $\mathbb{E}[G^n] = \mathbb{E}[r_{n+1} + \gamma G^{n+1}]$.
I looked deeper into it and I agree it's confusing.
I think what's happening is that in the paper they mixed two different definitions of $Q$: either $Q(s, a) = V(s')$ or $Q(s, a) = R(s, a) + V(s')$.
Their implementations (based on MuZeor's pseudo-code and MCTX) suggest that $Q(s, a) = V(s')$ because child nodes store values without considering the last received reward.
However, the reward plays a part in the child selection phase, so a better way to write their selection formula is$$a^{k}=\arg \max_{a} R(s, a) + Q(s, a)+P(s, a) \frac{\sqrt{\sum_{b} N(s, b)}}{1+N(s, a)}\left[c_{1}+\log \left(\frac{\sum_{b} N(s, b)+c_{2}+1}{c_{2}}\right)\right]$$ and this reflects what they are doing in the code.On the other hand, what you suggested make perfect sense too if we adopt the other definition of $Q$."
How to convert color information to 1D feature vector?,"
We are making a classification model that takes a clip of a movie as an input and predicts who the director is. Roughly speaking, it will be a model that understands film directors' unique style.
We are going to extract 5 features from a movie: a visual-feature vector from ResNet pretrained on ImageNet, an audio-feature vector from an audio model, shot type of a frame (one-hot encoding), emotion detection, and a color scheme of a frame. In the end, we are going to concatenate all these feature vectors and give it as a input for our classification model.
We find a tool that can extract color scheme(or palette) of an image as below. It both has information about colors and their proportion. However, I can't think a smart way to convert this information into 1-d vector form. Any ideas?
Of course I know the ResNet will get information about colors but the importance of color will be degraded in ResNet. I think the color is very important feature in defining a director's style and thus I want to use a color feature separately.


","['convolutional-neural-networks', 'features', 'feature-extraction']",
"Policy gradient (or more general, RL algorithms) for the problems where actions does not determine next state (next state is independent to action)","
I am pretty new in RL. Could anyone suggest results/paper about whether or not policy gradient (or more general RL algorithms) can be applied to the problems where actions does not determine next state? e.g. next state is independent to action $$P(s_{t+1} | s_{t}, a_{t}) = P(s_{t+1} | s_{t})$$
I think it is doable as it won't change the derivation of policy gradient, e.g.
$$
\begin{aligned}
\nabla \log \mathrm{p}_{\theta}(\tau) &=\nabla\left(\log \mathrm{p}\left(\mathrm{s}_{1}\right)+\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \mathrm{p}_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)+\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right) \\
&=\nabla \log \mathrm{p}\left(\mathrm{s}_{1}\right)+\nabla \sum_{\mathrm{t}=1}^{\mathrm{T}} \log \mathrm{p}_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)+\nabla \sum_{\mathrm{t}=1}^{\mathrm{T}} \log \mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right) \\
&=\nabla \sum_{\mathrm{t}=1}^{\mathrm{T}} \log \mathrm{p}_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right) \\
&=\sum_{\mathrm{t}=1}^{\mathrm{T}} \nabla \log \mathrm{p}_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)
\end{aligned}
$$
$$
\begin{aligned}
\nabla \log \mathrm{p}_{\theta}(\tau) &=\nabla\left(\log \mathrm{p}\left(\mathrm{s}_{1}\right)+\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \mathrm{p}_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)+\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \color{red}{\mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}\right) }  \right) \\
&=\nabla \log \mathrm{p}\left(\mathrm{s}_{1}\right)+\nabla \sum_{\mathrm{t}=1}^{\mathrm{T}} \log \mathrm{p}_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)+\nabla \sum_{\mathrm{t}=1}^{\mathrm{T}} \log \color{red}{\mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}\right) }\\
&=\nabla \sum_{\mathrm{t}=1}^{\mathrm{T}} \log \mathrm{p}_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right) \\
&=\sum_{\mathrm{t}=1}^{\mathrm{T}} \nabla \log \mathrm{p}_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)
\end{aligned}
$$
Also, I am curious about the difference between the RL setting where the next state is independent to action $$P(s_{t+1} | s_{t}, a_{t}) = P(s_{t+1} | s_{t})$$ and the multi-armed bandits setting. If the problem is the next state variable is independent of actions, what would be the correct framework to start with?
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'multi-armed-bandits', 'contextual-bandits']",
Is there a way to improve the low-quality data?,"
I'm on a robotics team and we've been tasked to write a program to differentiate between a live and dead fish. We've been given ~15 minutes of training footage and it's absolutely terrible. It's low quality, hard to label (even for humans) and it's like 20 frames a second.
I have tried everything I can think of. YOLO, 3D convolutions (to take movement over time into account), residual networks with anywhere from 1-10 layers and more. I have narrowed it down to the data is just terrible.
Is there anything I can do to fix this? I know of data augmentation and have used it, but that doesn't increase the usefulness of the data, it just creates more terrible data. I feel like using machine learning to clean the data wouldn't be helpful (because of studies I can't remember the name of showing that adding one white pixel to an image can completely confuse an object classifier I just assume that using a machine learning model to alter an image would also just confuse a network), is this an accurate assumption?
Either way: is there anyway to improve the data I've been given? Or another way to approach this problem?
","['comparison', 'datasets', 'data-preprocessing', 'object-recognition', 'object-tracking']",
How to perform the back propagation step in Semi-gradient SARSA using a deep neural network?,"

For the back weight update step, I need to calculate $\nabla\hat{q}(S,A,w)$. My neural network takes in the state vector $S$ and gives out the action values for state $S$ and each action in the action space.
Since $\epsilon$-greedy randomly chooses between the greedy action and a random action, I was not sure what the loss for the network should be. Let's say I have 3 possible actions - $a_1, a_2, a_3$ and my neural network outputs the vector of action-values for each of these actions in state $S$. If $a_3$ is the action chosen by $\epsilon$-greedy, then is this a correct expression for $\hat{q}(S,A,w)$?
\begin{equation}
\hat{q}(S,A,w) = 
\begin{bmatrix}
    \hat{q}(S,a_1,w)  &  \hat{q}(S,a_2,w) &  \hat{q}(S,a_3,w)         
\end{bmatrix}
\times
\begin{bmatrix}
    0       \\
    0       \\
    1
\end{bmatrix} 
\end{equation}
If so, then would it be correct to take the gradient of this expression of $\hat{q}(S,A,w)$ with respect to the weights of the network and plug into into the following update equation for back propagation?
\begin{equation}
w = w + \alpha~[R + \gamma \hat{q}(S',A',w) - \hat{q}(S,A,w)]\nabla\hat{q}(S,A,w)
\end{equation}
I was trying to use this in the Lunar-Lander Open-AI gym environment but it never converged. So I wanted to make sure this approach is correct before trying other things.
","['reinforcement-learning', 'deep-rl', 'sutton-barto', 'sarsa']","If so, then would it be correct to take the gradient of this expression of $\hat{q}(S,A,w)$ with respect to the weights of the networkYes, your expression for $\hat{q}(S,A,w)$ looks correct for your network, and can be differentiated.and plug into into the following update equation for back propagation?Not clear what you intend. If you already have the gradient $\nabla\hat{q}(S,A,w)$ then you have already performed backpropagation from the current output value to all weights. The additional factor of $[R + \gamma \hat{q}(S',A',w) - \hat{q}(S,A,w)]$ is effectively the gradient of the MSE cost function w.r.t. the current output that needs to be multiplied in so that the backprop starts from the MSE loss.The equations given in Sutton&Barto are not that easy to apply when you already have a neural network library that has everything defined around supervised learning.You may find it easier to calculate a target value for the whole action value vector and use your library's definitions of loss functions and update steps. That would go something like this:This will automatically provide a gradient 0 associated with actions that were not taken, and is effectively the same update as per Sutton&Barto, but using the NN library as-is without trying to tweak the gradient calculations directly.If you are using a library like PyTorch, then it is definitely possible to calculate the pure gradient of the selected action as you are attempting. However, you may find yourself in more familiar and supported territory if you re-frame the updates as supervised learning as suggested above.I suggest one of the first things you try if this does not converge is to batch up the updates and only perform them every e.g. 100 steps. Again, this is possible using the S&B update rule, and something like PyTorch, but things will be easier if you go with the grain and re-frame the updates as supervised learning ""inside"" the RL mechanism.I am not sure how well SARSA will cope with Lunar Lander. I have solved it using DQN, which is similar, but has the advantage of re-using older off-policy experience, sampling from it randomly, which SARSA cannot do. To correct for SARSA's weakness there, you may need quite large batches to avoid too much correlation between experience. A common pattern with ""Deep SARSA"" is to sample multiple environments at once under the current policy, to build a large batch (you see this in A2C and other on-policy policy gradient methods too)."
What should the discount factor for the non-slippery version of the FrozenLake environment be?,"
I was working with FrozenLake 4x4 from open AI gym. In the slippery case, using a discounting factor of 1, my value iteration implementation was giving a success rate of around 75 percent. It was much worse for the 8x8 grid with success around 50%. I thought in the non slippery case, it should definitely give me 100 percent success, but it turned out to be 0 percent.
After trying to understand what was happening by going through the algorithm on paper (below), I found all values were the same, so greedifying with respect to the value function often just had the agent going around in circles. Reducing the discount factor from 1 to anything below 1 gave me 100 percent success. But going to anything below 0.9 gave me worse results in the slippery case.
Can someone give me some intuition as to why this happened and how to interpret and choose the discount factor for any use case?
Algorithm used (where $\theta = 0.0000001$):

","['reinforcement-learning', 'sutton-barto', 'value-iteration', 'discount-factor', 'frozen-lake']","After trying to understand what was happening by going through the algorithm on paper (below), I found all values were the same, so greedifying with respect to the value function often just had the agent going around in circles. Reducing the discount factor from 1 to anything below 1 gave me 100 percent success.This is expected (and optimal, as defined) behaviour with a discount factor of 1 in the deterministic case. With a reward of 1 at the end, no discounting and and no negative rewards for taking its time, the agent has infinite time to complete the task. So whatever it does, provided it does not step into a hole at any point, is optimal.The caveat is that when following the policy as a test, you need to use random choice to resolve ties (this is assumed in the theory behind value iteration, that the agent can reach the high rewards). Most simple argmax functions will not do that, so you will need to add some kind of shuffle or other random selection over the maximum results to ensure you are really following the policy that value iteration discovered the state value function for.The last sentence in the pseudocode is ignoring the possibility that you have this kind of situation, so it is not strictly true in all cases. That is, you cannot reliably build a working deterministic policy directly from the optimal value function when $\gamma=1$ for environments like Frozen Lake.But going to anything below 0.9 gave me worse results in the slippery case.I am less sure about what is happening in this case. Given that you successfully found an optimal value function in the deterministic environment, then this may actually be optimal too. Whether or not it is may depend on the map - an optimal agent with low discount factor may prefer not to ""run a gauntlet"" early with risk of failure even if the overall probability of success is better. That may cause it to prefer a safer start that leads to a tougher end, where it is forced to take more risks overall than would be optimal for a goal of completing the most episodes.how to interpret and choose the discount factor for any use case?The discount factor is not a free choice for the agent, and it is not a hyperparameter for the learning method in value iteration. Instead, it is part of the definion for what is and isn't optimal. So you should select it based on the problem definition.In an episodic environment where the agent's goal is to reach a certain state or states as fast as possible, there are two ""natural"" formulations:Zero reward for most transitions, ""goal"" rewards for reaching certain terminal states, and a discount factor below $1$. I think your choice of $0.9$ would be fine, and it is commonly used in the literature for this kind of toy environment.Negative reward for most transitions, ""goal"" rewards for reaching certain terminal states. If there is a single ""target"" state, such as exiting a maze, the ""goal"" reward can be zero. Usually no discounting, but you could add discounting if it represents something else about the problem.In more complex systems, the discount factor may also become a hyperparameter of the agent. For instance, it does so when using function approximation with temporal difference learning, because it can dampen positive feedback from bootstrapping. In those cases, where ""naturally"" there would be no discounting, then you may want to use the highest value that prevents the estimator diverging e.g. $\gamma = 0.99$."
"Why do we use ""true labels"" that are based on the output of our network in Deep Q-Learning?","
In the original DQN paper, the $\ell_2$ loss is taken over the distance between our network output, $\hat{q}(s_j,a_j,w)$ and the labels $y_j=r_j+\gamma \cdot \max\limits_{a'} \hat{q}(s_{j+1},a',w^-)$, where $\hat{q}(s_{j+1},a',w^-)$ is our network with static weights $w^-$, that are updated to be $w^-=w$ every $C$ steps.
This is troubling to me, as $y_j$ aren't really ""true"" labels as we know from supervised learning, so why should I even think that this loss updates the weights such that the output policy is something meaningful?
It seems as if my network could output some arbitrary $\hat{q}$ and with respect to this $\hat{q}$, I will try to minimize a loss, But when $\hat{q}$ isn't ""optimal"" per se, It is not clear that we can converge to an optimal policy.
","['reinforcement-learning', 'deep-rl', 'dqn', 'objective-functions']","The labels in DQN, and in Q-learning in general, are not ""true"" in the sense that they represent optimal action value functions. Instead they represent approximate action values of a current target policy.The target policy changes every C time steps, when the network with static weights is updated. This update will include both corrections to the action value approximations, and changes to which actions are considered optimal.The reason this converges towards an optimal action value function is related to the policy improvement theorem. With function approximation, as in DQN, the convergence is not guaranteed, but the process is stll based on the same idea. In summary it is a two-step repeated feedback process:What this means for the TD target ""labels"" in DQN is:They are not ground truth for the optimal action value function, until after the algorithm has converged.They are biased, initially almost completely arbitrarily by however the target network has been initialised, and from then on due to a slowly-reducing impact from that initial bias and from lagging behind collected data.They are non-stationary. This means an online learning model class is required (neural networks are fine). It is also the reason why many Deep RL algorithms can suffer from catastrophic forgetting.When using experience replay, the TD target labels should be recalculated each time they are used."
"Which formula of p(x, y) to use?","
The probability distribution $p(x, y)$ can be calculated in two ways :

$p(x, y) = p(y \mid x) p(x)$
$p(x, y) = p(x \mid y) p(y)$

But according to the book Deep Generative Modeling (page number 3 first paragraph last line) (1) has clear advantages that (2). Why?
","['generative-model', 'books', 'probability-theory']",
Is it mandatory to multiply every activation of a layer by droupout factor during testing?,"
Dropout is a regularization technique used in neural networks. It is useful in preventing overfitting by making a neural network as good as an ensemble system.
In dropout, we switch off $p$ percent of neurons of the input or hidden layer during the training phase.  But, we do not switch off them during the testing phase.
We generally multiply each activation by $p$, the dropout ratio used during the training phase, in the testing phase. Is it mandatory to do so? Or is it okay to use the whole network as it is?
","['regularization', 'testing', 'dropout']",
"Do I need to normalize all state-space variables? If so, how?","
I am playing around with a DRL agent in a stock-trading environment.
I have normalized all the external input data (the features that my agent will use). However, what about characteristics that don't come from the environment?
For example, I have included things like ""current account balance"" and ""current unrealized gain"" in my observation space (as I believe it's useful). However, I don't know how I could normalize these values, given that they are dependent on what actions the agent took, which changes every time etc.
Any feedback or advice is appreciated.
Will it be detrimental if I don't normalize these values (as long as they're reasonably within the orders of magnitude of my other normalized variables)?
I guess a simple example would be like if a robot was being trained to pick up balls, and one of the observations was ""current number of balls picked up"", how would you normalize that value, given that it's just a count that could technically go to infinity?
","['reinforcement-learning', 'deep-rl', 'data-preprocessing', 'normalisation', 'algorithmic-trading']","The way I've seen most codes treat the state normalization is that they simply take a running mean and standard deviation for each dimension of the state space. As you point out, this normalization will be dependent on the actions the agent takes; this is not unique to your problem.As for your concern of the state observations going to infinity, this will not happen. Keeping with your example, the number of balls going to infinity would require the number of timesteps to also go to infinity. In practice there will have to be some finite length of the episodes, so this won't be an issue. And note, once you start a new episode, the number of balls goes back to 0, so it's not like the normalization amount can go to infinity either."
"What is a ""canonical space""?","
I am reading the paper on 3D reconstruction, ViSER: Video-Specific Surface Embeddings for Articulated 3D Shape Reconstruction, and I encountered the term ""canonical space"".
What is a ""canonical space""? Is it widely used? Is there other use of the term?
In my understanding, a canonical space is a standard, norm space. And we want to convert sample points into that space so that we can perform standard procedures. But I don’t know if that interpretation is correct or not. And I am also interest if it is a common term or very few people use it.
","['deep-learning', 'computer-vision', 'terminology', 'papers']",
"Why do we use the same parameters for the joint, marginal and conditional distributions in VAEs?","
I've noticed in several resources on variational autoencoders (for example the wikipedia article), we use the same parameters theta for the prior, likelihood, posterior, etc distributions. For example the equation $p_\theta(x) = \int_z p_\theta(x|z)p_\theta(z)dz$. Aren't $p_\theta(x)$ and  $p_\theta(z)$ two different distributions, so how can we parameterize them with the same $\theta$ params. I might be misunderstanding something about what it means to parameterize a distribution...
","['machine-learning', 'generative-adversarial-networks', 'generative-model', 'variational-autoencoder']",
How does MAML inner loop optimization works?,"
I started to learn meta-learning, reading the MAML paper https://arxiv.org/pdf/1703.03400.pdf

In the inner loop, I am calculating adapted parameters for each task, I will be doing multiple steps of inner SGD.
I will calculate adapted parameters after two or more gradient steps ($\theta{'}$, $\theta^{''}$ , $\theta^{n}$), then using testing parameters, I will have a loss in respect to the original $\theta$ (If I understand the derivation correctly). Now I am supposed to backpropagate through the gradient. Unfortunately, I am not sure how it is done...

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
Page 3.
 ""The MAML meta-gradient update involves a gradient
through a gradient""

To do this, I understand that I have to store each $\theta^{n}$, but don't know how the loss from $\theta^{n}$ to $\theta^{n-1}$ is transferred up to the original  $\theta$.
I guess that the for the last series of parameters ($\theta^{n}$) loss is calculated in the standard way with the testing set, but then I somehow need to pass information saying how much the previous set of parameters was wrong... (Gradient of the gradient?)
I see Hessian and vector products popping up everywhere on the internet, but I cannot imagine how that works, and have no idea how it is calculated and passed/implemented...
Can someone explain to me how the inner loop [back-propagation trough meta-gradient] is working - how the derivations go and how loss is transferred/updated?
","['optimization', 'gradient-descent', 'gradient', 'meta-learning', 'model-agnostic-meta-learning']",
How can a convnet learn with a 3x3 output layer?,"
I was studying the ""Deep Learning with Python"" book, I came across this MNIST example and this is how the last conv2d layer looks like:
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     
=================================================================

I have a hard time understanding how can this neural network figure out any feature from such a small image like 3x3 pixels. It would make sense to me if it were something like 10x10 but a 3x3 image makes no sense when I look at one, yet the network can achieve %99+ validation accuracy. How is this possible?
","['neural-networks', 'mnist']",
"What are the benefits of using multiple convolutions, as opposed to one, before the pooling layer in a U-Net?","
I have seen U-Nets that use a single convolution before the pooling operator and some that use two or more.
My question is, what is better? Or what are the benefits of using more or less convolutions?
","['convolutional-neural-networks', 'generative-adversarial-networks', 'convolution', 'convolutional-layers', 'u-net']",
How can I produce crossbred images out of two datasets?,"
I'm very new to AI and deep learning. So my question is going to be very basic.
I'm trying to understand which approach I would need to use to cross-breed set of images. Let's say I'm having dataset of cats and dataset of human faces and as output I want to get images that would look like cat-humans.
I tried to use DCGAN network, this in particular: https://github.com/pytorch/examples/tree/main/dcgan. But as output I'm getting an image that is baed only on one of the datasets (either human or cat, not anything that would look like cross-breed).
I suppose, I completely misunderstand the concept of DCGAN, or perhaps I just need to tweak the network to achieve desired results?
Anyway, I'm kinda lost here and just asking for directions. Perhaps there's better network architecture for this purpose or some specific set up needed.
","['pytorch', 'image-processing', 'dc-gan']",
Use of Mask in U Net for plant disease detection [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am using U-Net for plant disease detection. I am new to deep learning and computer vision.
Currently, we are feeding the masking images generated via open cv HSV format to detect colours from the images so that the background and foreground are detected.
I need to know the purpose of image masking in image segmentation using U-Net, why it is needed? What kind of masking do we need to feed to the architecture?
Do we also need to target the disease part for masking?
","['deep-learning', 'image-processing', 'image-segmentation', 'u-net']",
Why does the average-reward estimator for continuing tasks use the TD error?,"
In Sutton and Barto's RL book, section 10.3 describes how to use average reward $r(\pi)$ to define the quality of a policy, re-defining action-value function $q_\pi(s,a)$ and value function $v_\pi(s)$ to measure the expected differential return $G_t^{diff} =\sum_{t=0}^\infty \left[R_{t} - r(\pi)\right]$ of policy $\pi$ in a state/action pair, or state, instead of its discounted return $G_t^{disc}\sum_{t=0}^\infty \gamma^t R_t$.
Of course, the average rate of reward for a given policy cannot be known in advance but only estimated, so the authors propose keeping a running estimate $\bar{R} \rightarrow r(\pi)$ that starts at 0 and is then updated as $\bar{R}_{t +1} = \bar{R}_t + \beta \delta$, where $\beta$ is a learning rate and $\delta = R_{t + 1} - \bar{R}_t + \hat{q}(S_{t + 1}, A_{t + 1}) -  \hat{q}(S_{t}, A_{t})$ is the action-value TD error, in a SARSA context where $A_{t+1}$ is sampled; I understand we might as well use the value function TD error $R_{t + 1} - \bar{R}_t + \hat{v}(S_{t + 1}) -  \hat{v}(S_{t})$ if we are estimating that.
My question is: why does the update rule for the average rate of reward estimate consider TD error, and why are we not updating it simply as $\bar{R}_{t + 1} = (1 - \beta) \bar{R}_t + \beta R_{t+1}$ ?
","['reinforcement-learning', 'deep-rl', 'rewards', 'function-approximation', 'sutton-barto']","Mystery solved thanks to Exercise 10.8 in the book. The reason is that we want the running mean to converge to the actual value of the average reward.With $\bar{R}_{t + 1} =  \bar{R}_t + \beta \delta$ with $\delta = {R}_{t + 1} - \bar{R}_t + \hat{v}(S_{t + 1}) - \hat{v}(S_t)$, assuming value function estimation converged I would get$\begin{align}
\delta &= R_{t + 1} - \bar{R}_t + v(S_{t + 1}) - v(S_{t}) \\
&= R_{t + 1} - \bar{R}_t + (R_{t + 2} - r(\pi^*) + R_{t + 3} - r(\pi^*) + ...) - (R_{t + 1} - r(\pi^*) + R_{t + 2} - r(\pi^*) + ...) \\
&= R_{t + 1} - \bar{R}_t - (R_{t + 1} - r(\pi^*)) \\
&= r(\pi^*) - \bar{R}_t
\end{align}
$And the estimation will converge to the average reward of the optimal policy, being updated as $\bar{R}_{t + 1} = (1 - \beta) \bar{R}_t + \beta r(\pi^*)$."
Deep Learning for occlusion recognition is 2D or 3D space [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



Given a dataset of spatial 2D or 3D object map with their bounding box annotations,

How feasible would it be to train a deep learning model to recognize (classify) ""occluded"" objects from a 2D or 3D ""source"" coordinate?
What model/learning architecture would best work for such a task?

Example: in this top-view 2D image, two objects are classified ""occluded"". Similarly this could be the case in 3D space.

","['deep-learning', 'convolutional-neural-networks', 'object-detection']",
How to compare images for similarity?,"
I'm working on a system that reads 3 images per second and stores them in a collection.
For each image I have all its keypoints and descriptor vectors, using ORB detector. On average there are 250 features in an image
To add a new image to the collection, I need to check if a similar image already exists or not.
What is the most efficient way to compare the images using the descriptors/keypoints?
what I tried?
Brute force
compare each descriptor vector in the new image with each one of all the images in the collection. This takes a lot of time that increases gradually with the collection size.
Reference vector comparison
Decide on a reference vector, for example (1,1,1,....,1), and calculate the distance between each descriptor vector and the ref vector, then use the distance as a metric to determine descriptor vectors similarity: if 2 descriptors have the same distance from the ref vector, then they're similar.
While this is very efficient computationally, it doesn't yield the correct result in most cases. especially in ORB where the distance is hamming distance.
Reducing to smaller set of vectors
This is a half cooked idea: From each group of vectors with a certain distance to the reference vector, take the K nearest vectors to the ref vector.
The idea is to reduce the number of descriptor vectors to compare with.
",['computer-vision'],
ML algorithm suggestion for databases that change a lot with time after model training,"
I have a classification problem and I'm using a logistic regression (I tested it among other models and this one was the best). I look for information from game sites and test if a user has the potential to be a buyer of certain games.
The problem is that lately some sites from which I get this information (and also from where I got the information to train the model) change weekly and, with that, part of the database I use for prediction is ""partially"" different from the one used for training (with different information for each user, in this case). Since when these sites started to change, the model's predictive ability has dropped considerably.
To solve this, an alternative would be, of course, to retrain the model. It's something we're considering, although we'll have to do it with some frequency given the fact that the sites are changing every couple of weeks, considerably.
other solutions considered was the use of algorithms that could adapt to these changes and, with that, we could retrain the model less frequently.
Two options raised were neural networks to classify or try to adapt some genetic algorithm. However, I have read that genetic algorithms would be very expensive and are not a good option for classification problems, given the fact that they may not converge.
Does anyone have any suggestions for a modeling approach that we can test?
","['machine-learning', 'classification', 'binary-classification', 'logistic-regression']",
How should you reshape data before feeding it to LSTM layers?,"
I was curious if anyone had any advice on how to reshape data for a recurrent neural network. What I've been doing is array.reshape(len(X_train), # of points in time, # of features)
And then in the array, the X axis represents the features, and for each item, there are entries corresponding to the number of points in time (and a column with id so I know what the original id is). So if the time series has 12 points, each sample will have 12 entries in a row in the array.
Am I reshaping correctly, and if not, how can I reshape correctly?
Edit
The array initially looks like this:

This is a pandas array which I convert to numpy by using values.astype(float). The article, and timestep column are just for reference. I am using the command array.reshape(-1, 12, 17)
The -1 is the length of the array, 12 is the timesteps (or number of entries in y axis for each article), and the 17 is the number of features at each time step. I am then feeding that data to a recurrent neural network.
","['neural-networks', 'deep-learning', 'recurrent-neural-networks', 'data-preprocessing']",
How to find the order in which DFS algorithm will inspect the nodes?,"

I have been taking Artificial Intelligence course in College. I came upon this problem. Now here I have to find the order in which DFS algorithm inspects the nodes and what is the path from Start to Goal State. And wherever this contention between more than one nodes, left one is to be chosen.
So according to DFS algorithm, I would add {A,B,C} in stack.
{D,E,B,C} -> {K,E,B,C} -> {M,E,B,C} -> {O,E,B,C} -> {F,G,E,B,C}.
Now I cannot understand what to do further, should I go to B and then backtrack again with B already being in the stack or do something different. And the path would be S -> B -> G or else S -> A -> D -> K -> M -> O -> G.
How to decide what way to go?
","['search', 'path-finding', 'depth-first-search']","You would keep track of a list of states that you have already visited. As you progress through the graph, you add the node you choose to that list.By the time you are at F, B is not yet on the visited list (though it is on the stack), so you would go to B. From there you would got to S, which you have already visited, so then choose the next possibility, G, which is your goal.There is an implementation detail that would change the behaviour: you could also put nodes from the stack into your list (because arguably you would visit B much earlier if you didn't take the detour via O), in which case you would backtrack from F to O, and then move on to G.I don't think this is fixed in DFS; you're still following the principle of exploring the current path until the end. It's similar in status to ""always choose the left node to proceed"": an implementation detail (though important, as it changes the outcome)."
Does pairing children with their parents cause any harm (in a genetic program)?,"
If you pair parents with their children (with a cross-over) does this prevent making individuals which are more fit or does this cause other side effects which are harmful to the genetic process?
I can provide any specifics about my concrete program but don't know what is relevant (and cannot write the whole setup here).
EDIT: As I think I figured out the answer myself I'll add the information that I think is relevant,
if I'm wrong, and you'd need more information, don't hesitate to ask (I check stack exchange quite often and am always pleased to communicate).
The relevant information:
2 parents create 2 children with a random 10-point cross-over.
The 10 points get chosen out of 700 lines of dna.
(See my answer for more information.)
","['reference-request', 'genetic-algorithms', 'fitness-functions', 'genetic-programming', 'crossover-operators']",
Can the state transition function be dynamic in reinforcement learning?,"
In general, there are two types of transition functions in reinforcement learning. Mathematically, they are as follows
#1: Stochastic state transition function:
$$T : S \times A \times S \rightarrow  [0, 1]$$
#2: Deterministic state transition function:
$$T : S \times A \rightarrow  S$$
Is it possible to make the transition function change as the game progress? Or is it impossible to assume such a transition function as it cannot qualify to be a function?
Or in other words, I may want to introduce something as follows:
#3: Dynamic state transition function:
$$T : S \times A \times X \rightarrow  S$$
where $X$ is some continuous set.
","['reinforcement-learning', 'markov-decision-process', 'transition-model']",
What is my mistake in applying the AC-3 algorithm on this problem?,"
I want to apply AC-3 algorithm to the following CSP:
There are two variables $A$ and $B$.
Domain of $A: \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\} $
Domain of $B: \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\} $
The constraints:
$$A < B $$
$$B = A^2 $$
I'll apply the AC-3 algorithm depicted in Artificial Intelligence: A Modern Approach, 4th edition (figure 6.3)
Step 1. By converting constraints to arcs, our arc queue will be as following:
$$A < B $$
$$B > A $$
$$B = A^2 $$
$$A = \sqrt{B} $$
According to the algorithm, when processing an arc such as $(X_i, X_j)$, if the domain of $X_i$ was changed, all the arc that their destinations are $X_i$ and their source are a neighbor of $X_i$ must be add to the end of the arc queue except for $X_j$. Since we have only two variables, therefore I think we do not need to add any new arc to the queue.
Step 2. processing the arc queue:

Applying $A < B$ reduces the domain of $A$ to $ \{ 0, 1, 2, 3, 4, 5,
   6, 7, 8 \} $.
Applying $B > A$ reduce the domain of $B$ to $ \{ 1,
   2, 3, 4, 5, 6, 7, 8, 9 \} $.
Applying $B = A^2$ reduces the domain of $B$ to $ \{ 1, 4, 9 \} $.
Applying $A = \sqrt{B} $ reduces the domain of $A$ to $ \{ 1, 2, 3 \} $.

So, at the end, the domain of $A$ is $\{1, 2, 3\}$ and the domain of $B$ is $\{1, 4, 9\}$.
But this answer is not correct. Since for $1$ belongs to the domain of $B$, there is no element in the domain of $A$ such that $ B < A $ satisfies.
It is obvious that the domains of the arc consistent CSP is as following:

Domain of $A: \{2, 3\} $
Domain of $B: \{4, 9\} $

What is my fault?
I think adding arc $(X_j, X_i)$ as well as other neighbor arcs $(X_k, X_i)$, when the domain of $X_i$ is reduced by applying arc $(X_i, X_j)$, leads to correct answer.
","['constraint-satisfaction-problems', 'norvig-russell']",
Is Q-learning only capable of learning a deterministic policy?,"
I was following a reinforcement learning course on coursera and in this video at 2:57 the instructor says

Expected SARSA and SARSA both allow us to learn an optimal
$\epsilon$-soft policy, but, Q-learning does not

From what I understand, SARSA and Q-learning both give us an estimate of the optimal action-value function. SARSA does this on-policy with an epsilon-greedy policy, for example, whereas the action-values from the Q-learning algorithm are for a deterministic policy, which is always greedy.
But, can't we use these action values generated by the Q-learning algorithm to form an $\epsilon$-greedy policy?
We can, for instance, in each state, give the maximum probability to the action with the greatest action-value and the rest of the actions can have probability $\frac{\epsilon}{\text{number of actions}}$. Because we do a similar thing with SARSA, where we infer the policy from the current estimate of action-values after each update.
","['reinforcement-learning', 'q-learning', 'policies', 'sarsa', 'deterministic-policy']","If we assume a tabular setting, then Q-learning converges to the optimal state-action value function, from which an optimal policy can be derived, provided a few conditions are met.In finite MDPs, there's at least one optimal (stationary) deterministic policy, but there can also be optimal stochastic policies - specifically, if two or more actions have the same optimal value, then you can stochastically choose between them. However, if you stochastically choose between all actions (including non-optimal ones), then you will not behave optimally.SARSA also converges to the optimal state-action value function, but the learning policy must eventually become greedy. See this post and theorem 1 (p. 294) of this paper.So, even in SARSA, if you want to behave optimally, you can't just arbitrarily choose any stochastic policy derived from this found optimal value function (note also that SARSA is on-policy). However, SARSA can also find an optimal restricted policy. See theorem 2 (p. 297) of this paper for more details.To answer your question directly, Q-learning can find an optimal stochastic policy, provided that it exists."
"PPO custom implementation: do metrics like value loss, actor loss and entropy move a certain way?","
I'm wondering whether problems with a custom PPO implementation (problem couldn't be made into a gym environment) can be debugged considering how the losses change over time.
In my current experiment, although results do get better in the actual task, the actor(policy) loss, the value function loss and the entropy move in weird ways.



How do these metrics usually change over the course of an experiment? Do they always move along the same paths? Thank you.
","['reinforcement-learning', 'value-functions', 'proximal-policy-optimization']","It's difficult to tell how the output of the loss changes over the course of the experiment as you use them as a measure on how well your model performs. Ideally, the loss decreases over time with minimal jittering. A decreasing loss means the model is learning and the hyper parameters fit the dataset.If your loss is jittering too much without a clear decreasing trend, it might be that, for example, the learning rate is too high and the model overshoots the minimum. It also might be that there is a flaw in the model.To make it short, no, the loss doesn't always move along the same path.If you need to test your metric, create tests with small models. Use a small keras model (for example) where you know that there is no flaw inherent with the model and use standard hyper parameters. Then train the model twice. Once with your own metric and once with a standard metric similar to your own provided by a library. The path of the loss should look similar - but it will not be the same.I'd do the above approach for every method you implemented yourself to make sure that the method itself is doing what it is intended to do."
Is Reinforcement Learning with only feedback on a single action possible?,"
Consider the following case:
A reinforcement based web-crawler where:

State = current page + 1 out-link (reduced to features of some sort)
Action = Whether to visit that out-link or not (n_actions = 2). This means action = 1 causes the link to be added to the crawlers queue, and action = 0 causes the link to be discarded.
Reward = Score determined by the page reached by visiting the out-link
Next state = the next possible link to consider (taken from the scraper prio queue). This is seperate of whether the links in the currently visited page are added to the queue or discarded.

Can this state-action-reward space be used in a RL implementation, considering we can not calculate the reward (or penalty) for bad pages, since action = 0 never leads to a new page visit where a reward can be calculated. We will thus only ever have feedback and update action = 1.
Assuming some policy optimization algorithm (such as PPO), can we teach an agent to perform this task, or does it explicitly need feedback on both actions?
My initial assumption was that to train a binary action space, we either need feedback on both actions that is positive, or we need feedback on either action that is both positive and negative. Evening out the probability distribution over the action space would then take care of ""learning"" the second action. Is that correct?
Why is the problem defined like this:
A different way of defining RL web crawling is by taking the current page as state and considering the list of out-links as the action space, picking a single page to visit. The way I understand policy methods is that a single state is mapped to a single action, meaning that if such a problem definition is used, the agent will pick a single action (link) to take in each page, so only linear paths through the web-content are considered. The problem definition stated above allows the agent to consider all links at all times.
The goal:
To find as many relevant pages (scored on, for example, summing hits from a countvectorizer with target terms), while skipping irrelevant links.
","['reinforcement-learning', 'deep-rl']",
"At which point, does the momentum based GD helps really in this figure?","
Classical gradient descent algorithms sometimes overshoot and escape minima as they depend on the gradient only. You can see such a problem during the update from point 6.

In classical GD algorithm, the update equation is
$$\theta_{t+1} = \theta_{t} - \eta \times \triangledown_{\theta} \ell$$
In the momentum based GD algorithm, the update equations are
$$v_0 = 0$$
$$v_{t+1} = \alpha v_t +  \eta \times \triangledown_{\theta} \ell $$
$$\theta = \theta - \eta \times \triangledown_{\theta} \ell$$
I am writing all the equations concisely by removing the obvious variables used such as inputs to loss functions. In the lecture I'm listening to, the narrator says that momentum-based GD helps during the update at point 6 and the update will not lead to point 7 as shown in the figure and goes towards minima.
But for me, it seems that even momentum-based GD will go to point 7 and the update at point 7 will be benefited from the momentum-based GD as it does not lead to point 8 and goes towards minima.
Am I correct? If not, at which point does the momentum-based GD actually help?
","['gradient-descent', 'momentum']",
Replay buffer action range in DDPG,"
I have an environment where the agent action is in range [0, 1.57]. My actor network in DDPG has a tanh activation, and so the network values are in the range [-1,1]. Hence I change the scaling from [-1,1] to [0, 1.57] before an action is performed. My question is, when we store the transition $(s_t, a_t, r_t, s_{t+1})$ in replay buffer $R$, should $a_t$ be in [-1,1] or [0, 0.157]
","['reinforcement-learning', 'deep-rl', 'actor-critic-methods', 'ddpg']",
Using reinforcement learning for human-robot interaction [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I have a scenario where a user is wanting to exercise and improve over time. They attend around 10 exercise sessions, doing 20 repititions of an exercise each session.
I want to develop a reinforcement learning agent to help the user get better at the exercises. The agent will observe the user exercising, and be rewarded when the user performs well (say, by completing a lot of repititions). The agent will then take actions to change the exercise routines to adapt to the skill level of the user.
Furthermore:

the RL agent will have no knowledge of the user's skill level before entering the first session
the user's skill level should increase from session to session, but at an unknown rate

I know that I must employ model-free RL methods, as the agent must be able to create unique policies for each user it works with. But I have three questions:

How do I determine what initial policy to follow during the first sessions?
I realise this problem description is vague, but generally speaking, would this be enough data for the agent to work with to create a policy quickly that performs well?
Does the fact the user gets better over time create a problem for the RL agent? As it would be like chasing a moving target?

","['reinforcement-learning', 'q-learning', 'on-policy-methods']",
Is there an argument against using the (reviewed) predictions of a model as ground truth to further train exactly this model?,"
I plan to use my predictions as ground truth to continue training my model. These predictions are of course reviewed during this process. Is there an argument against that (reinforcement of slight mistakes/overfitting etc.)?

Here my specific use case described:
I am using detectron's faster R-CNN implementation to train a (pretrained) model to find defects of a machine part in images.
The goal is to find bounding boxes around these defects and to label them.
A colleague labeled some of the images (1500 images, making up 20% of the whole set) and I used those to train my model. Then I had the model predict defects on all 7500 images. My colleague asked me if he can review the predictions (and adjust/add if necessary) so he doesn't have to label the remaining images from scratch, and then I would like to continue the training with all the images.
","['datasets', 'overfitting', 'transfer-learning', 'data-labelling', 'faster-r-cnn']","Using the (unchecked) predictions of the model as training data is an approach known as ""pseudo-labeling"". It can help in certain situations, depending on the underlying structure of your dataset, but you have to be a bit careful about how you use it (e.g. only using high-confidence predictions as your pseudo-labels) and you always want to keep your pseudo-labels separate from your true labels, so you can potentially update them as your model changes.But it sounds like you're not using the raw predictions as labels, but rather using the predictions of the models as a pointer to (currently unlabeled) examples which you then will manually label.""Training on errors"" is recognized mode of augmenting your training dataset, especially for ""on-line"" style systems where you're getting a continuous stream of new examples. The concept is to identify those examples which are predicted either inaccurately or with low confidence, identify the accurate labels for these instances, and then include them with the rest of the training set to help improve the predictions for similar sorts of examples in the future.In contrast to pseudo-labeling, you're looking to correct the low confidence examples or the incorrectly predicted. Adding in high-confidence examples doesn't gain you much, as your current training set is already sufficient to correctly predict these. And with an on-line model where you're continually getting new examples, adding the well-predicted examples to your training set does potentially cause issues with subclass imbalance issues, as ""normal"" examples are expected to swamp out the rare outliers.But it sounds like you have a fixed-size training set. In that case, the standard recommendation for the best course of action of dealing with unlabeled data applies: ""pay someone to label it for you"". What you're looking for is accurate labeling. How you get that is left somewhat nebulous, so long as the labeling is accurate. Using model results as a starting point is perfectly valid, assuming that whoever is doing the checking/correction is willing to actually do all the corrections (to the same quality level as a ""from scratch"" prediction) and won't wave through the model predictions as ""ehh, good enough"".In addition to label accuracy, another issue may be selection bias (that is, the model may have certain subsets of examples which it performs worse/better on, and picking which examples to include in labeling on that basis may bias future training). But if you have a pre-determined, fixed-size training set this is not really an issue if you label all of them (or a model-independent random subset). The selection bias comes not from the initial model predictions/selection, but instead the (model-independent) selection of the examples to be labeled."
What are all the possible usages of 'multilayer perceptron'?,"
The term 'multilayer perceptron' has been used in literature in various ways in the literature.
I am presenting some of them below

As a feed-forward neural network [1].

As a fully connected feed-forward neural network [2].

As a fully connected feed-forward neural network in which each hidden layer has the same number of neurons.

As a fully connected feed-forward neural network in which each hidden layer has the same number of neurons and same activation function.


Afaik, the first definition is generally used, but it seems that there are many alternative definitions.
In this context, I want to know all the possible definitions that are floating in the literature for the word 'multilayer perception'.
I am asking this question because there can be several interpretations if we consider the words of 'multilayer perception' alone as the names suggests the only property required is multiple number of layers.
","['comparison', 'terminology', 'definitions', 'feedforward-neural-networks', 'multilayer-perceptrons']",
Is there any variant of perceptron convergence algorithm that ensures uniqueness?,"
The perceptron convergence algorithm given below ensures the convergence of weights of the perceptron provided enough data points and iterations.

Although it ensures convergence by finally getting a decision hyperplane that can separate positive samples (P) from negative samples N. It does not ensure the uniqueness of the decision hyperplane.

The solution is not unique, because there are more than one
hyperplanes separating two linearly separable classes.

Are there any variants to this algorithm in the literature that are capable of ensuring uniqueness?
","['reference-request', 'algorithm-request', 'convergence', 'perceptron']",
Why is my convolutional neural network failing to classify user inputted images after having high accuracy in testing? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



My CNN was trained on the Kaggle A-Z Dataset and consists of:
      model.add(Conv2D(32, (5, 5),input_shape=(28, 28, 1), activation='relu'))
      model.add(Dropout(rate=0.15))
      model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
      model.add(MaxPooling2D(pool_size=(2, 2)))  # choose the best features via pooling
      model.add(Dropout(rate=0.15))
      # flatten since too many dimensions
      model.add(Flatten())
      # fully connected to get all relevant data
      model.add(Dense(64, activation='relu'))
      # fully connected to get all relevant data
      model.add(Dense(128, activation='relu'))
      # one more dropout
      model.add(Dropout(0.4))
      # output a softmax to squash the matrix into output probabilities
      model.add(Dense(26, activation='softmax'))

Data used was split into training sets and test sets. Oversampling was used to account of the unbalanced dataset. The final accuracy was over 95%. Therefore, I expected my program to be able to read alphabetical letters with ease so I loaded one in:
def predictImageFromDrive(model):
  imageLocation = filedialog.askopenfilename()
  #image_numpy = cv2.imread(imageLocation, 0)
  image_numpy = cv2.cvtColor(imread(imageLocation), cv2.COLOR_BGR2GRAY)
  #print(image_numpy.shape)
  #print(imageir.shape)
  imageir = image_numpy.reshape(28,28,1)
  #print(imageir.shape)
  test_image = np.copy(imageir.reshape((1, imageir.shape[0], imageir.shape[1], imageir.shape[2])))

  print(test_image.shape)

  prediction = model.predict(test_image)
  classification = np.argmax(prediction, axis=-1)[0]
  print(prediction)
  plt.imshow(image_numpy)
  plt.title(""Classification: "" + chr(ord('@')+classification+1))
  plt.show()

The model now seems to predict every letter that I put in as part of the same category. For example, if I input an image of letter V, it shall predict it as an A, if I input an image of letter B, it shall classify it as an A and so forth. Why is this happening?
","['convolutional-neural-networks', 'computer-vision', 'python']",
Why is training longer not better in reinforcement learning?,"
I have trained an RL agent (PPO) for 6 million steps to solve the OpenAI gym LunarLander-v2. Surprisingly, the agent performs best already after 320K steps and is getting worse after that.
In the tensorboard log, I can see that the mean, min reward and explained variance do have the highest values at 320k training steps.

I have seen this with stable-baselines and rllib and with other environments as well.
I am wondering why this is the case. Is that a normal behaviour in reinforcement learning? Or do I have to modify some training parameters to continue improving the RL agent?
I would like to see that the agent is increasing the min, mean reward so that it reaches almost the max reward. Is that realistic?
","['reinforcement-learning', 'rewards', 'proximal-policy-optimization', 'gym', 'lunar-lander']",
Are there assumptions made about Self-Play that don't hold up in regular MA competition?,"
I read about this paper Efficient Competitive Self-Play Policy Optimization which proposes an algorithm for training a population of agents with self-play using a perturbation based matchmaking approach.
I was wondering if this algorithm can also be used with regular MultiAgent Competition, e.g. an asymmetric game like robo-soccer with one goalie and one striker.
Are there specific properties that self-play fulfils, that doesn't hold up in regular MA Competition? Can approaches for self-play be used in scenarios where not the same policy is used for both agents? If not, what are some properties one has to keep in mind when trying to adapt those approaches?
","['reinforcement-learning', 'multi-agent-systems', 'self-play', 'multi-agent-rl']",
I have a 3 class classification problem. Detection of one of classes is very important. How to design the problem? one class classification or ...? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I have a 3 class classification problem. Correct detection of one of the classes is very important. How to design the problem:

one class classification?
a normal 3 class classification?
two distinct 2 class classification?(I mean, for example I have 3 lables A,B,C , and detection of class B is very important. Design a binary classification A,B and another binary classification B,C ?
or ....?
Please recommend me!

","['machine-learning', 'deep-learning', 'classification', 'unsupervised-learning']",
What should the discount factor for the non-slippery version of the FrozenLake environment be?,"
I was working with FrozenLake 4x4 from open AI gym. In the slippery case, using a discounting factor of 1, my value iteration implementation was giving a success rate of around 75 percent. It was much worse for the 8x8 grid with success around 50%. I thought in the non slippery case, it should definitely give me 100 percent success, but it turned out to be 0 percent.
After trying to understand what was happening by going through the algorithm on paper (below), I found all values were the same, so greedifying with respect to the value function often just had the agent going around in circles. Reducing the discount factor from 1 to anything below 1 gave me 100 percent success. But going to anything below 0.9 gave me worse results in the slippery case.
Can someone give me some intuition as to why this happened and how to interpret and choose the discount factor for any use case?
Algorithm used (where $\theta = 0.0000001$):

","['reinforcement-learning', 'sutton-barto', 'value-iteration', 'discount-factor', 'frozen-lake']","After trying to understand what was happening by going through the algorithm on paper (below), I found all values were the same, so greedifying with respect to the value function often just had the agent going around in circles. Reducing the discount factor from 1 to anything below 1 gave me 100 percent success.This is expected (and optimal, as defined) behaviour with a discount factor of 1 in the deterministic case. With a reward of 1 at the end, no discounting and and no negative rewards for taking its time, the agent has infinite time to complete the task. So whatever it does, provided it does not step into a hole at any point, is optimal.The caveat is that when following the policy as a test, you need to use random choice to resolve ties (this is assumed in the theory behind value iteration, that the agent can reach the high rewards). Most simple argmax functions will not do that, so you will need to add some kind of shuffle or other random selection over the maximum results to ensure you are really following the policy that value iteration discovered the state value function for.The last sentence in the pseudocode is ignoring the possibility that you have this kind of situation, so it is not strictly true in all cases. That is, you cannot reliably build a working deterministic policy directly from the optimal value function when $\gamma=1$ for environments like Frozen Lake.But going to anything below 0.9 gave me worse results in the slippery case.I am less sure about what is happening in this case. Given that you successfully found an optimal value function in the deterministic environment, then this may actually be optimal too. Whether or not it is may depend on the map - an optimal agent with low discount factor may prefer not to ""run a gauntlet"" early with risk of failure even if the overall probability of success is better. That may cause it to prefer a safer start that leads to a tougher end, where it is forced to take more risks overall than would be optimal for a goal of completing the most episodes.how to interpret and choose the discount factor for any use case?The discount factor is not a free choice for the agent, and it is not a hyperparameter for the learning method in value iteration. Instead, it is part of the definion for what is and isn't optimal. So you should select it based on the problem definition.In an episodic environment where the agent's goal is to reach a certain state or states as fast as possible, there are two ""natural"" formulations:Zero reward for most transitions, ""goal"" rewards for reaching certain terminal states, and a discount factor below $1$. I think your choice of $0.9$ would be fine, and it is commonly used in the literature for this kind of toy environment.Negative reward for most transitions, ""goal"" rewards for reaching certain terminal states. If there is a single ""target"" state, such as exiting a maze, the ""goal"" reward can be zero. Usually no discounting, but you could add discounting if it represents something else about the problem.In more complex systems, the discount factor may also become a hyperparameter of the agent. For instance, it does so when using function approximation with temporal difference learning, because it can dampen positive feedback from bootstrapping. In those cases, where ""naturally"" there would be no discounting, then you may want to use the highest value that prevents the estimator diverging e.g. $\gamma = 0.99$."
Can we also estimate $V_{\pi}$ with SARSA?,"
For SARSA, I know we can estimate the action value $Q(s,a)$, and the relationship between $V(s)$ and $Q(s,a)$ is $V_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a|s)Q_{\pi}
(s,a)$.
So my question is, can we simply estimate $V_{\pi}$ by applying the above equation to the $Q_{\pi}$ that we derived from SARSA? Will there be any restrictions to prevent estimating $V_{\pi}$ through SARSA?
","['reinforcement-learning', 'value-functions', 'bellman-equations', 'sarsa']",
"Theoretically, how can the trait ""humbleness"" be incorporated into AI design, specifically Artificial General Intelligence?","
On Google Scholar (""humble"" and ""AI"") yields (About 175,000 results), while (""Humble AI"") yields about 34 results.
It seems that the trait of ""humbleness"" and additionally the trait of ""agreeableness"" ought to be explored more in the contexts of AI. I am curious if others have heard lectures or seen papers specifically on the topic of humbleness or thought upon the importance of it being built into AI design.
This article delves into humility being a dimension of AI when it comes to validating outputs of AI systems. This paper about trustworthiness does a good job of discussing its necessity as a trait in AIAS and how specifically when talking of AI we need to be careful what definitions we are using when describing AI traits.
I envision ""humbleness"" will increase in importance in AI design. I am specifically looking for theoretical applications of the humbleness in AI design. Where you could envision it being important and why.
I'll answer my own question below for clarity. humble definition
","['ai-design', 'agi', 'human-like', 'aixi']",
Could I cluster the audio clips in order to improve the speed of their classification?,"
I have a neural network which is very resource intensive and is used to classify audio clips. The classification is done in batches, where I record for a set period of time and then go through and classify the audio.
However, the time for classification is far too long.
So I was thinking what if I could somehow embed the audio and then cluster them into similar audio files. This way I'd only need to classify a couple audio clips in the cluster and assume they're all the same.
Is this possible?
All suggestion, improvements or help is very much appreciated!
","['deep-learning', 'tensorflow', 'clustering', 'embeddings', 'audio-processing']",
What are the specific differences between vision transformers variants?,"
I have tried 4 different types of attacks on vision transformers (ViT small and tiny, DeiT small and tiny) but the attack successes on smaller versions are higher than the tiny versions. My understanding is that the smaller versions are stronger than the tiny ones. I expect the results to be higher on tiny than smaller versions.
I am really confused. Is there any explanation for this?
What are the specific differences between the vision transformer models apart from the number of parameters and the embedding dimension?
Edit: I am loading these models from timm
","['comparison', 'vision-transformer', 'adversarial-attacks']",
Why gradients are used in Layer-wise Relevance Propagation (LRP)?,"
To give you an overview, Layer-wise Relevance Propagation is a technique by which we can get relevance values at each node of the neural network. These calculated relevance values (per node) are representative of the importance that that node plays, in deciding the predicted output. The final values at the output layer are considered the relevance values of those nodes, then the relevance values are calculated in a downward manner i.e. from output layer to input layer.

Image Source
Some fundamental equations for calculating LRP are as follows:

For the sake of simplicity, you can imagine the function $\rho(w_{jk})$ as an identity function i.e. $\rho(w_{jk}) = w_{jk}$
This paper just mentions that $c_j$ can be computed using gradients on the activations of lower-layer (the one which closer to the input layer), but why? What is the reasoning behind that?
I have mailed the authors asking these same questions. I will write an answer to this when they respond.
","['neural-networks', 'deep-neural-networks', 'explainable-ai']",
How to manage impossible actions? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am using Q-learning in julia language.
Because of the solver’s configuration, actions have to be defined as the whole action space and impossible actions have to be also considered. It means that I can't use a function that, given a state, returns all the possible actions. In order to solve it I am using a dummy state which is terminal and a bad reward.
When the agent tries to take impossible actions, what  is the difference between using a dummy state which is terminal and remaining in the same state (+bad reward) until the end of the episode? Other possible solutions?
Specifically, how can I either avoid defining an impossible action or alternatively define it as an impossible action?
","['reinforcement-learning', 'q-learning', 'state-spaces', 'action-spaces', 'state-representations']",
"Is the case of a big state space, should we use a softmax exploration policy rather than $\epsilon$-greedy for Q-Learning?","
In Reinforcement Learning, epsilon-greedy policies are the most used exploration policies, but in case there is a big state space with impossible actions, wouldn't it be better to use soft-max policies instead?
","['machine-learning', 'q-learning', 'softmax', 'epsilon-greedy-policy', 'exploration-strategies']",
Could Softmax Action Selection be useful to solve an episodic task with more than 100000 possible states and 2000 actions?,"
I am new in the field of RL. I am trying to use tabular methods, Q-Learning for solving a problem that takes a lot of time for computation, so I would like to know if there are more efficient methods for it.
Why are tabular methods are not useful in large state spaces? Maybe too many possible combinations? Could Softmax Action Selection be better than epsilon greedy?
","['reinforcement-learning', 'q-learning', 'function-approximation', 'episodic-tasks']","Your question contains the answer. Use value function approximation. Tabular methods must compute a value for each state. That becomes unfeasible with large state spaces. Function approximators can genererlize, and perform well even without ever having seen every state."
Is it possible to successively train an RL agent on the same environment with different data,"
I have a scheduling problem that I am trying to solve with RL (if you are interested in more details you can read about it here Reinforcement learning applicable to a scheduling problem?).
I have created an own environment (OpenAI-Gym) and I have trained the model for one specific day of the simulation. So I have 288 timesteps for one day (1 for each 5 minutes) and the simulation last until the end of the day. So the agent needs to make 288 decisions when having 1 control variable.
Now my question is whether it is possible to successively train an RL agent on the same environment for different days? The environment and reward function will stay the same but the input data will change as every day has different input data (temperature, heat demand, electricity price etc.). So I would like to train an agent for one day and then tell the agent to train on another day but not forget everything it has learned during the training of the first day. Thus I can make sure that the agent is not overfitting to one special input data but also has the ability to generalize and thus be applicable for different days.
Do you know if and how I can do this?
Reminder: Can anybody tell me more about this by now. I'll highly appreciate every further comment as I still don't know how to do this.
",['reinforcement-learning'],
"Is the ""Helvetica scenario"" mentioned here related to Artificial Intelligence?","
Consider the following sentence from the original GAN paper titled Generative Adversarial Nets

in particular, $G$ must not be trained too much without updating $D$, in order to avoid ""the Helvetica scenario"" in which $G$ collapses too many values of $\mathbf{z}$ to the same value of $\mathbf{x}$ to have enough diversity to model $p_{\text {data }}$

The sentence uses the name of a scenario called Helvetica. Is it related to AI or chemistry? If AI, then where can I read more about the scenario?
","['terminology', 'papers', 'generative-adversarial-networks']","The ""Helvetica scenario"" referenced in the paper is an AI issue related to generative systems failing.In GANs, this might be due to a generator becoming initially strong against the discriminator by focusing on its best output results so far that cover just a subset of the full dataset - i.e. gradients move the generator towards output of a single best class, suppressing output of classes that the discriminator can spot easily, and encouraging output of classes where the discriminator has more trouble. By the time the discriminator catches up with the generator, it may be too late for the combined training system to learn its way out of this state.In GANs, the term Helvetica scenario from the paper is more commonly referred to as mode collapse in later literature. If you search for ""GAN mode collapse"" you will find a lot more information about it. There are a few different improvements to GAN design - loss functions and other hyperparameters - that work to reduce the incidence of mode collapse.From this answer on Data Science SE, it seems like the authors named the effect after a science parody sketch in the show Look Around You, where the structure of a molecule collapses. The ""science"" in the sketch is complete nonsense."
"Given A and B, C are independent of each other. Given A, B and C, D and E are independent of each other. What is the minimal number of parameters?","
Assuming all variables $A, B, C, D,$ and $E$ are random binary variables. I come up with Bayes net: $D \rightarrow B \rightarrow A \leftarrow C \leftarrow E$ which has the minimal number of parameters of 10, I think. However, the given choices are 8, 15, 16, 21, 23, and 32. I don't know what I did wrong?? 
","['probability', 'bayesian-networks', 'naive-bayes', 'bayes-theorem', 'bayesian-probability']",
How to decode P bits that represent a random weight generator?,"
So I've been tasked by my neural network professor at university to replicate the following research: Intelligent Breast Cancer Diagnosis Using Hybrid GA-ANN.
Each chromosome represents a possible net, more specifically, a possible MLP network. They've used a binary convention, and have used $P = 15$ bits for the random initial weight generator, $Q=2$ bits for the number of nodes and $R = 9$ bits for feature selection.


P bits random initial weight generator allows 2P different
combinations of the initial weight. The numbers of hidden
nodes (i.e. represented by Q bits) permit the GA to explore
up to a maximum of 2Q hidden nodesâ€™ size. For the
representation of the feature subset, the value of R is set to
the number of full feature size. Value â€˜1â€™ or â€˜0â€™ indicates if
the feature at that particular location is selected or otherwise.


Decode each chromosome
in the population to obtain the selected feature subset, hidden
node size and random generator.

I don't understand why they say with $P$ bits, there's $2*P$ combinations, wouldn't it be $2^P$?
Also, I can't grasp how they decoded the $15 P$ bits into the net for the weight generation. I've searched everywhere but I can't find anything specific. What I thought to do was to transform the $15$ bits into a decimal number and use it as a specific seed for rand and randn function in Matlab, through which I make random initial weights.
","['neural-networks', 'papers', 'genetic-algorithms', 'multilayer-perceptrons', 'weights-initialization']",
When exactly does the split into different heads in Multi-Head-Attention occur?,"
I am confused by the Multi-Head part of the Multi-Head-Attention used in Transformers. My question concerns the implementations in Pytorch of nn.MultiheadAttention and its forward method multi_head_attention_forward and whether these are actually identical to the paper. Unfortunately, I have been unable to follow along the original code of the paper. So I could not check whether the implementations in Pytorch are acutally identical to the paper.
Please forgive the excessive use of illustrations. However, I hope it will improve understanding my problem.
What is the correct order for calculating the Queries Q, Keys K and Values V and splitting the operation into the individual Attention-Heads? Unfortunately most explanations I found online while helpful for understanding the general principle and intuition of Multi-Head-Attention did not go into the details of the implementation.
In the original paper Attention is all you need Multi-Head-Attention is explained as followed:

First, according to my current understanding, if we have a sequence of vectors with 512-dimensions (like in the original Transformer) and we have $h=8$ Attention-Heads (again like the original), every Attention-Head attends to $512/8=64$ entries of the input vector used to calculate the Attention in the corresponding head. So the first Attention-Head attends to the first 64 entries, the second to the second 64 entries and so on. However, if the split is conducted before calculating Q,K,V this would refer to the first 64 entries of X (this does not seem match the explanation in the paper I believe) while in the other case it would refer to the first 64 entries of Q,K,V.
In the text they say ""project the queries, keys and values h times with different, learned linear projections to $d_k,d_k$ and $d_v$ dimensions and since they set $d_k=d_v=d_{model}/h=512/8=64$. Therefore, if we actually have single matrices for every Attention-Head h we would have
$W^Q_i,W^K_i,W^V_i \in \mathbb{R}^{512x64} \forall i \in h$.
This matches the illustration found here https://jalammar.github.io/illustrated-transformer/

It is explained that the input X is transformed into the Queries, Keys and Values for the different attention heads by using different projection matrices which are learned during training.
This seems to indicate that the split into the individual Attention-Heads is conducted after the calculation of $Q,K,V$ (or rather during the calculation). Since we have $h=8$ this leads in sum to $3*8*512*64=3*512*512$ learnable parameters in total (if we ignore the bias). Thus as far as the overall number of parameters is concerned we would have the same number if we would instead use three big matrices which concatenate the matrices of the individual Attention-Heads.
$W^Q=[W^Q_1,W^Q_2,...,W^Q_h] \in \mathbb{R}^{512x512}$ 
$W^K=[W^K_1,W^K_2,...,W^K_h] \in \mathbb{R}^{512x512}$ 
$W^Q=[W^V_1,W^V_2,...,W^V_h] \in \mathbb{R}^{512x512}$
In the explanation from the same author of GPT-2 (this model has an embedding dimension of 768 and 12 Attention-Heads, instead of 512 and 8 like the original Transformer) found here https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention


the Queries Q, Keys K and Values V are first calculated by multiplying the input with one big matrix which is the concatenation of $W^Q,W^K,W^V$. If the input for calculating Q, K and V is identical (which is the case for self-attention), it is clear to me that you can use $W_{concat}=[W^Q,W^K,W^V]$ and obtain $[Q,K,V]$, since you essentially still multiply the input with each weight matrix separately.
Then you can split the result to again obtain $Q,K,V$ as individual matrices (The image displays $q_9,k_9,v_9$ as an example, not the complete matrices). Then $q_9,k_9,v_9$ are again split into 12 vectors which results in a matrix of dimension $(12x64)$.
So overall here we did not use individual matrices per Attention-Head but only one larger matrix.
Is this method mathematically identical to the one using individual smaller matrices per Attention-Head?
It appears that this is the way the calculation is implented in Pytorch, if $d_{model}=kdim=vdim$ Though here unlike in the paper which used $d_k$ and $d_v$ as names, $kdim$ and $vdim$ refer to the dimension of all Attention-Heads summed up, e.g. $kdim=d_k*num_{heads}$(=512 for the original Transformer).
So  In the documentation of nn.modules.MultiheadAttention the model either creates three separate projection matrices to generate the Queries, Keys and Values or one big matrix (if the dimensions are identical). The following is part of the _init_ function.
if self._qkv_same_embed_dim is False:
            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))
            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))
            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))
            self.register_parameter('in_proj_weight', None)
        else:
            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
            self.register_parameter('q_proj_weight', None)
            self.register_parameter('k_proj_weight', None)
            self.register_parameter('v_proj_weight', None)

If we stay in the standard case of _qkv_same_embed_dim=True the input is passed through a nn.linear as part of _in_projection_packed which is using self.in_proj_weight as the weight
if not use_separate_proj_weight:
        assert in_proj_weight is not None, ""use_separate_proj_weight is False but in_proj_weight is None""
        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
    else:
        assert q_proj_weight is not None, ""use_separate_proj_weight is True but q_proj_weight is None""
        assert k_proj_weight is not None, ""use_separate_proj_weight is True but k_proj_weight is None""
        assert v_proj_weight is not None, ""use_separate_proj_weight is True but v_proj_weight is None""
        if in_proj_bias is None:
            b_q = b_k = b_v = None
        else:
            b_q, b_k, b_v = in_proj_bias.chunk(3)
        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)

def _in_projection_packed(
    q: Tensor,
    k: Tensor,
    v: Tensor,
    w: Tensor,
    b: Optional[Tensor] = None,
) -> List[Tensor]:
    r""""""
    Performs the in-projection step of the attention operation, using packed weights.
    Output is a triple containing projection tensors for query, key and value.
    Args:
        q, k, v: query, key and value tensors to be projected. For self-attention,
            these are typically the same tensor; for encoder-decoder attention,
            k and v are typically the same tensor. (We take advantage of these
            identities for performance if they are present.) Regardless, q, k and v
            must share a common embedding dimension; otherwise their shapes may vary.
        w: projection weights for q, k and v, packed into a single tensor. Weights
            are packed along dimension 0, in q, k, v order.
        b: optional projection biases for q, k and v, packed into a single tensor
            in q, k, v order.
    Shape:
        Inputs:
        - q: :math:`(..., E)` where E is the embedding dimension
        - k: :math:`(..., E)` where E is the embedding dimension
        - v: :math:`(..., E)` where E is the embedding dimension
        - w: :math:`(E * 3, E)` where E is the embedding dimension
        - b: :math:`E * 3` where E is the embedding dimension
        Output:
        - in output list :math:`[q', k', v']`, each output tensor will have the
            same shape as the corresponding input tensor.
    """"""
    E = q.size(-1)
    if k is v:
        if q is k:
            # self-attention
            return linear(q, w, b).chunk(3, dim=-1)
        else:
            # encoder-decoder attention
            w_q, w_kv = w.split([E, E * 2])
            if b is None:
                b_q = b_kv = None
            else:
                b_q, b_kv = b.split([E, E * 2])
            return (linear(q, w_q, b_q),) + linear(k, w_kv, b_kv).chunk(2, dim=-1)
    else:
        w_q, w_k, w_v = w.chunk(3)
        if b is None:
            b_q = b_k = b_v = None
        else:
            b_q, b_k, b_v = b.chunk(3)
        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)


def _in_projection(
    q: Tensor,
    k: Tensor,
    v: Tensor,
    w_q: Tensor,
    w_k: Tensor,
    w_v: Tensor,
    b_q: Optional[Tensor] = None,
    b_k: Optional[Tensor] = None,
    b_v: Optional[Tensor] = None,
) -> Tuple[Tensor, Tensor, Tensor]:
    r""""""
    Performs the in-projection step of the attention operation. This is simply
    a triple of linear projections, with shape constraints on the weights which
    ensure embedding dimension uniformity in the projected outputs.
    Output is a triple containing projection tensors for query, key and value.
    Args:
        q, k, v: query, key and value tensors to be projected.
        w_q, w_k, w_v: weights for q, k and v, respectively.
        b_q, b_k, b_v: optional biases for q, k and v, respectively.
    Shape:
        Inputs:
        - q: :math:`(Qdims..., Eq)` where Eq is the query embedding dimension and Qdims are any
            number of leading dimensions.
        - k: :math:`(Kdims..., Ek)` where Ek is the key embedding dimension and Kdims are any
            number of leading dimensions.
        - v: :math:`(Vdims..., Ev)` where Ev is the value embedding dimension and Vdims are any
            number of leading dimensions.
        - w_q: :math:`(Eq, Eq)`
        - w_k: :math:`(Eq, Ek)`
        - w_v: :math:`(Eq, Ev)`
        - b_q: :math:`(Eq)`
        - b_k: :math:`(Eq)`
        - b_v: :math:`(Eq)`
        Output: in output triple :math:`(q', k', v')`,
         - q': :math:`[Qdims..., Eq]`
         - k': :math:`[Kdims..., Eq]`
         - v': :math:`[Vdims..., Eq]`
    """"""
    Eq, Ek, Ev = q.size(-1), k.size(-1), v.size(-1)
    assert w_q.shape == (Eq, Eq), f""expecting query weights shape of {(Eq, Eq)}, but got {w_q.shape}""
    assert w_k.shape == (Eq, Ek), f""expecting key weights shape of {(Eq, Ek)}, but got {w_k.shape}""
    assert w_v.shape == (Eq, Ev), f""expecting value weights shape of {(Eq, Ev)}, but got {w_v.shape}""
    assert b_q is None or b_q.shape == (Eq,), f""expecting query bias shape of {(Eq,)}, but got {b_q.shape}""
    assert b_k is None or b_k.shape == (Eq,), f""expecting key bias shape of {(Eq,)}, but got {b_k.shape}""
    assert b_v is None or b_v.shape == (Eq,), f""expecting value bias shape of {(Eq,)}, but got {b_v.shape}""
    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)

def linear(
    input: Tensor, weight: Tensor, bias: Optional[Tensor] = None,
    scale: Optional[float] = None, zero_point: Optional[int] = None
) -> Tensor:
    r""""""
    Applies a linear transformation to the incoming quantized data:
    :math:`y = xA^T + b`.
    See :class:`~torch.nn.quantized.Linear`
    .. note::
      Current implementation packs weights on every call, which has penalty on performance.
      If you want to avoid the overhead, use :class:`~torch.nn.quantized.Linear`.
    Args:
      input (Tensor): Quantized input of type `torch.quint8`
      weight (Tensor): Quantized weight of type `torch.qint8`
      bias (Tensor): None or fp32 bias of type `torch.float`
      scale (double): output scale. If None, derived from the input scale
      zero_point (long): output zero point. If None, derived from the input zero_point
    Shape:
        - Input: :math:`(N, *, in\_features)` where `*` means any number of
          additional dimensions
        - Weight: :math:`(out\_features, in\_features)`
        - Bias: :math:`(out\_features)`
        - Output: :math:`(N, *, out\_features)`
    """"""
    if scale is None:
        scale = input.q_scale()
    if zero_point is None:
        zero_point = input.q_zero_point()
    _packed_params = torch.ops.quantized.linear_prepack(weight, bias)
    return torch.ops.quantized.linear(input, _packed_params, scale, zero_point)

Then later the Queries, Keys and Values are split up into the individual Attention-Heads:
    #
    # reshape q, k, v for multihead attention and make em batch first
    #
    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
    if static_k is None:
        k = k.contiguous().view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
    else:
        # TODO finish disentangling control flow so we don't do in-projections when statics are passed
        assert static_k.size(0) == bsz * num_heads, \
            f""expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}""
        assert static_k.size(2) == head_dim, \
            f""expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}""
        k = static_k
    if static_v is None:
        v = v.contiguous().view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
    else:
        # TODO finish disentangling control flow so we don't do in-projections when statics are passed
        assert static_v.size(0) == bsz * num_heads, \
            f""expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}""
        assert static_v.size(2) == head_dim, \
            f""expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}""
        v = static_v

Therefore, it appears to me that both ways of calculations should be equal and the one using the bigger matrices is just more efficient to compute. Is this correct?
In this case, I ask myself why nn.MultiheadAttention requires that embed_dim is divisible by num_heads, since the split into the individual Attention-Heads is actually conducted after generating $Q,K,V$. Should then not $d_q,d_k,d_v$ be made to be divisible by num_heads? Since these dimensions do not have to be equal to the dimension of the inputs?
Thanks for any advice in advance!
","['transformer', 'pytorch', 'attention']","The queries, keys and values are calculated then chunked so that each chunk depends on (is a linear combination of) all values of the input embedding.As for understanding an implementation, I didn't bother with pytorch but instead understood this code http://einops.rocks/pytorch-examples.html although there are differences as I understand that pytorch expects input in the form (L, N, C) where L=words, N=batch, C=embedding for performance reasons.Understand MultiHeadAttentionNew first then understand MultiHeadAttentionOld, and then maybe look at the pytorch code."
Positional Encoding of Time-Series features,"
I’m trying to use a Transformer Encoder I coded with weather feature vectors which are basically 11 features about the weather in the dimension [batch_size, n_features].
I have a data point per day, so this is a time-series but there are no evidences of the time in my vectors and I know I must use positional encoding to tell my network “this vector is from the 21/05/2021, this one is from the 20/04/2019, etc”.
The usual positional encoding with sin & cos used in NLP doesn’t seem to fit my problem as it encodes the position relative to other words in the sentence and my features are independant values (the temperature of the day doesn’t come after the amount of rain for instance). Order of the data matters between two different inputs but not within the features.
I don’t really know how to do encode my feature vectors for my transformer encoder to get something like positional_encoded = original_vector + date_encoding which I think could be great but I could also just add the day of year as a feature - would it be enough for a transformer?
What would be the best way to do so?
","['transformer', 'time-series', 'positional-encoding']",
Implement 4D convolution as matrix-matrix multiplication - paper is confusing!,"
I am confused by this paper https://arxiv.org/pdf/1410.0759.pdf which displays on page 4 how to model a 3D convolution (input has more than 1 channel and filter has more than one output).
In this example, each filter kernel of one output dimension is reshaped into one single row.
The input channels are stacked above each other where a sliding window with the size of the filter defines the content of one row.
In this example, the number of columns of the filter matrix and the number of rows of the input matrix are equal, but this doesn't have to be.
Let's say the input data are 4x4 blocks - this means the input matrix still has 4 columns (because the filter is 2x2) but now has 9 x 3 = 27 rows which is more than the 12 columns of the filter matrix.
This doesn't make sense to me?
","['machine-learning', 'implementation', 'convolution']",
Can you make a Neural Network drunk or high?,"
We know that the human brain can become sozzled by various substances that are released into the brain, but can you make an artificial neural network drunk or high? For example on a RL Agent that still does the task somewhat but behaves silly?
","['neural-networks', 'reinforcement-learning', 'human-like']",
Is it possible learning convergence is lost in Reinforcement Learning as the state space grows?,"
I am new in the AI field and I am trying to use Reinforcement Learning. Specifically, I am using tabular Q-Learning and SARSA algorithms to solve a sequential decision making problem. (I am using TabularTDLearning.jl, q_learn.jl and sarsa.jl, with 16 GB of RAM).
When I apply those algorithms to a problem with 6.593 states and 169 actions, in less than a minute, it takes

200.000 iterations with Q-Learning, and
400.000 with SARSA

If I try to solve a more complex problem with 125.231 states and 2.197 actions, it does not converge, in fact it gets worse as iterations increase (5.000.000 iterations).
Is it possible learning convergence is lost as the state space grows?
I have read about catastrophic forgetting. I do not know if it is the problem why convergence has not happened. Could catastrophic forgetting be the problem?
For actions that cannot be taken, I have created a dummy state and a negative reward. Could this have affected the convergence of the algorithms?
","['reinforcement-learning', 'q-learning', 'convergence', 'sarsa', 'state-spaces']","With tabular reinforcement learning (RL) methods, then catastrophic forgetting does not come into play, as it is a feature of online learning with approximators such as neural networks. Essentially your table never forgets anything, although it may become out of date for rarely-visited state/action pairs whilst an approximator will often still maintain them at least a little due to generalisation.The convergence time for RL methods is highly variable, and depends on many factors.Tabular methods have relatively simple requirements for state/action visits though, and if your two environments are very similar, then it might be reasonable to expect the amount of experience required for convergence to scale roughly according to $O(|\mathcal{S}| \times |\mathcal{A}|)$ - that assumes you have no guiding principle to generalise state/action features or skip explorations.Assuming your convergence sample size really is $O(|\mathcal{S}| \times |\mathcal{A}|)$ then extrapolating from your smaller to your larger environment implies you may need as many as 50 million iterations to get convergence on the larger instance. This is a crude aprpoximation, because so much depends on the nature of the changes - it may be that 5 million should be enough (and something is wrong with your implementation), or you may need 500 million. For instance one important factor I have left out of very similar is episode length (assuming you are solving for an episodic problem) - if that is also significantly larger, you may be looking at the 500 million case. Also critically important is how long the initial agent takes to find high scoring episodes.I also want to add that in this case there are state dependent actions, so for actions that cannot be taken I have created a dummy state and a negative reward. Is it the reason?Ideally you would not allow the agent to take actions that are not possible in a current state. You should check your library in case there is a way to inform the agent of its current set of actions instead of specifying an action space for the whole environment. If there are a lot of unavailable actions, it could be costing the agent a lot of time exploring them and learning not to take them.Instead of adding ""dummy"" states, one approach would be to give a negative reward and have the agent transition to its current state - i.e. no change. The negative reward should be larger than any negative reward possible for valid action choices (otherwise the agent may prefer to collect the slightly-less-negative reward than it predicts it will get from taking a real action).There are lots of changes you might consider to improve your agent to deal with harder-to-solve environments in reasonale time. I am not sure what you are able to consider, so here are a few open-ended ideas:Use function approximation to generalise state and action features. This does expose you to a whole different set of complications though, such as losing guarantees of convergence, and also catastrophic forgetting.Use eligibility traces and either Q($\lambda$) or SARSA($\lambda$). This can help you find a sweet spot for learning efficiency by considering multiple trajectory lengths for estimating returns. Alternatively, you could consider n-step returns.Use an experience replay table, or Dyna-Q with Q-learning, to extract more data from previous experience. Whether or not this is useful depends on how fast the environment simulation runs. If your learning time is limited mostly by simulation speed of the environment, this is a good choice. If it is limited by the time taken by the Q-learning process, you are better off generating fresh data as fast as possible."
Why do Convolution Neural Networks work on NLP/sequential tasks?,"
I have read some articles where people use 1D CNN for NLP tasks like sentiment analysis. My questions are, given that CNNs are largely used for images, how/why does this work on sequences/NLP tasks? And, are there any specific CNN-based architectures that outperform transformers?
","['convolutional-neural-networks', 'natural-language-processing', 'applications']",
Which existing model could be used for wind speed and direction prediction?,"
I am trying to predict the wind speed and wind direction in a graph network for a geographical area. The dataset includes the start and end nodes, the distance between them, and wind speed and direction. I want to predict the wind speed and direction for a certain number of line segments (edges) in a graph network.
Which existing prediction model could be used for this problem? For example, my dependent and independent variables are wind speed and wind direction. How these can be used to predict their future values?
Note: The network here represents a graphical network constructed using NetworkX Python library. Nodes represent the cities and edges represent the distance between the cities. Furthermore, I am currently having a dummy dataset for wind speed and wind direction for each edge. Using this data, I want to predict the wind speed and wind direction for a certain number of edges that are between a given source and destination.
","['datasets', 'regression', 'model-request']",
"If I can repeat ML experiments, how can I bound my results?","
It has been asked here if we should repeat lengthy experiments.
Let's say I can repeat them, how should I present them? For instance, if I am measuring the accuracy of a model on test data during some training epochs, and I repeat various times this training, I will have different values of test accuracy. I can average them to take into account all the experiments. Can I then calculate a sort of confidence interval to say that the accuracy will most likely be within an interval? Does this make sense? If it does, what formula should I use?
It says here that we can use
$\hat{x} \pm 1.96 \frac{\hat{\sigma}}{\sqrt{n}}$, but I don't quite understand the theory behind.
","['machine-learning', 'statistics']",
"If we have a working reward function, would adding another action have a significant effect on the agent performance if task remains the same?","
If we have a working reward function, providing the desired behavior and optimal policy in a continuous action/state-space problem, would adding another action significantly affect the possible optimal policy?
For example, assume you have an RL problem with an action space of 1 (de/acceleration), state-space of 2 (distance from position and velocity), and the agent is tasked to accelerate in a straight line from position a to b.
Do you think the agent would behave majorly differently? I'm under the assumption that there would be minimal change aside from a longer training time assuming enough exploration, as the task is to still move in a straight line, but the agent would only have to account for steering action too now.
","['reinforcement-learning', 'deep-rl', 'reward-functions', 'action-spaces', 'td3']",
Is it appropriate to represent 'total failure' as an absorbing state?,"
My understanding is that, in Markov decision processes, absorbing state are states which can transition only to themselves and that these transitions generate rewards of 0. I know that absorbing states are commonly used to represent goals, so an agent might get a non-zero reward when first entering the absorbing state, but all subsequent transitions generate 0 reward (effectively ending the episode).
My question is whether it is appropriate to also represent 'total failure' as an absorbing state. e.g. if the task is for Rabbit to return safely to his burrow (goal state which gives positive reward and ends the episode) while avoiding Fox (failure state which gives negative reward and ends the episode), would both burrow and fox-encounter be absorbing states? If not, how should fox-encounter be represented?
","['reinforcement-learning', 'markov-decision-process', 'state-spaces', 'transition-model', 'episodic-tasks']",
What clustering algorithms work best for datasets with only binary categorical features?,"
I have a dataset with a lot of binary categorical features and a single continuous target value. I would like to cluster them, but I am not quite sure what to use.
In the past, I have used DBSCAN for something similar and it worked well, but that dataset also had lots of continuous features.
Do you have any tips and suggestions?
Would you suggest matrix factorization and then cluster?
","['unsupervised-learning', 'algorithm-request', 'clustering']","Any clustering algorithm should work -- the main issue is the similarity or distance metric that determines how similar (or different) two elements are. This is often something like Euclidean distance, but that won't work well with binary data.I would suggest using the Jaccard Index or Dice Coefficient. These will be suitable for use as a metric when clustering such data."
How does learning the moves of chess show up in a neural network?,"
Is learning the moves a special case or just the same sort of thing that happens as the AI learns strategy? If you take two different neural networks and teach them each how the pieces move, what checkmate is, etc. will the two networks look identical or is there a random element that means that two networks with the exact same number of nodes, running on the same processors, etc. will not look identical even though they both know exactly the same things?
","['neural-networks', 'machine-learning', 'chess']",
Encoding Actions with Parameters in Neural Network Output,"
I have a task which I would like to teach an AI to perform. The input to the task will a screenshot of the screen and the output at any given time step is one of the following actions:
    DoNothing
    Click(x, y)
    Drag(from_x, from_y, to_x, to_y)
    KeyPress(Key)
    LongKeyPress(Key, duration)

where:
    Key ∈ { 1 , 2 , 3 , 4 , Space , Up , Down , Left , Right }

I think that deciding which action to take could be done with a softmax output layer, but I don't know how I would also encode the parameters of each action. For example, the Click action also 2 co-ordinate values but the Drag action needs 4 co-ordinates.
My best idea at the moment is that I could use one neural network to decide on which action type to perform and for each action, train another network to decide the parameters. So for example I have the main network which decides to take a Click action, and then a secondary network which will choose the x and y co-ordinates (given the same input).
The problem with this is that I would end up training 6 networks (decide the action, Click co-ordinates, Drag co-ordinates, KeyPress key, LongKeyPress key, LongKeyPress duration). The drawback for this is that I would need a very large dataset with a lot of samples for each type.
I considered how it might be possible to encode all the values into one output, but I wouldn't know how to deal with missing values (for samples where the user clicked, what value should i use for the Key output of KeyPress).
I have tried to find research papers on the topic, but I must be missing the right keywords because I can't find anything about strategies for this problem. Any ideas or pointers to relevant material would be great!
","['neural-networks', 'game-ai']",
Is there an unsupervised learning method for determine the most common questions within a dataset?,"
I have a dataset consisting of questions from customers. I am curious of the n most frequent asked questions, regardless of the variation the questions might appear in.
Is there NLP methods for finding n most frequent asked questions?
","['natural-language-processing', 'unsupervised-learning', 'question-answering']",
What is the total number of actions and rewards count,"
Reading this two articles about Reinforcement Learning:

Deep Reinforcement Learning with Double Q-learning by Hado van Hasselt et al.
Human-level control through deep reinforcement learning by Volodymyr Mnih et al.

I'm trying to understand the dataset and how many states, actions and rewards have been used to train the agent on a single game. I presume they should be equal, if an action is made on some state that generates some reward.
The first article compares their results with second's results. Both refers to different count of frames, however it is not clear how many actions, rewards and states have the dataset of a single game. More than that, each game could have random states, actions, rewards, which is somehow an imprecise comparison, but I could not find a dataset used for first article and second one.
Please, somebody who have a better understanding tell what is the total number of states, actions and rewards for a single game that the agent was trained on, because I'm trying to understand the dataset, and if there exists such dataset please show the source.
","['reinforcement-learning', 'deep-learning', 'papers', 'dqn', 'datasets']","TL;DR In the DQN paper, each environment was trained for 50 million frames, grouped in fours without overlap, so there were 12.5 million state, action, reward next-state records used.The above direct numerical answer to the question as posed is not the whole story though.I'm trying to understand the dataset and how many states, actions and rewards have been used to train the agent on a single game.It seems you are viewing Reinforcement Learning (RL) through terms you understand from supervised learning. Although there are some similarities, especially when considering the neural network that performs function approximation inside most deep RL, there are important differences which mean the information you are looking for doesn't really exist.The main purpose of RL is for an agent to learn through experience that it also helps to generate, through trial and error. Although off-policy algorithms such as DQN could learn from fixed datasets (and there are some circumstances where they might, perhaps as part of a more complex pipeline), this is not what the papers you reference are doing.RL experiments can be repeated provided all random number generators (in the learning agent and in the environment) are seeded the same way. That may be of interest to researchers wanting to replicate work exactly, but more generally useful are statistical measures of ""typical"" learning graphs. The environment defines the problem to be solved, and the exact data generated through the RL agent exploring it in a set of runs is not necessarily as interesting when interpreting the results, or considering whether to use a certain type of agent for a specific problem. Having a reference copy of the environment identical to the one used in the paper is the equivalent in RL of having the ""dataset"".You could log the state, action, rewards data from each experiment (and in DQN it is already being stored temporarily in the experience replay table), but you would not expect it to repeat on the next run even with the same agent. Also, when comparing to some other agent (or agent hyperparameters), one major point of difference is that you would expect that agent to generate a different set of states, actions and rewards - choosing how and when to explore different parts of the environment is a key differentator between agents. So it is usually not considered a critical part of the work to log every state. That's not to say that researchers won't do it - presumably many do keep records for debugging or simply to be thorough. However, more important in published RL results is to characterise the learning curves against some metric such as the amount of experience required, or another cost such as total CPU resource or time used in training.In the original DQN paper, I believe that each 4 frames were stacked separately into a single state, without overlap, thus each 4 frames equals one state, for which a single action would be taken and then reward and next state observed. So dividing the frame count by 4 should give you the ""dataset size"". But it is important to note that this was never used as a whole in a supervised learning manner, and was not re-used between runs. The experience replay buffer was sampled randomly on each timestep, and there was an average number of times each sample was used (32 I think) to train the network - you might compare that to running 32 epochs of training across all state/action pairs as inputs, but it is only a loose comparison due to the sample-with-replacement sampling strategy and the sliding window effect of the experience replay."
What is the difference between features and inputs in machine learning?,"
I have seen many places that features and inputs have been used interchangeably when talking about machine learning especially deep neural networks. I want to know if they are indeed the same thing or there is a difference between between the two.
","['machine-learning', 'comparison', 'terminology', 'features']","An input usually refers to an example (sometimes also known as sample, observation or data point) $x$ from a dataset that you pass to the model. For example, in supervised learning, you have a labelled dataset $D = \{(x_i, y_i)\}_{i=1}^N$, where $x_i$ is the $i$th input and $y_i$ the corresponding label (aka target or output).This is similar to the terminology used for functions. For example, if you have the function $f: \mathcal{X} \rightarrow \mathcal{Y}$, then $x \in \mathcal{X}$ is the input and $f(x) = y \in \mathcal{Y}$ is the output of the function for that input $x$. In fact, models (like neural networks or linear regression models) are functions.ExamplesA feature is an attribute associated with an input or sample. For example, a feature of an image could be a pixel. The feature of a state could be the Euclidean distance to the goal state. An input can be composed of multiple features.It's possible that people also refer to features as inputs (in fact, if you pass e.g. an image to a model, you're also passing the pixels, the features, which are thus also inputs to the model). There are also other terms used to refer to these. For example, in statistics, people may refer to features as independent variables (or regressors), and maybe a sample refers to a dataset rather than a single observation. So, you should always take into account the context when reading these terms.For more info, you could read this and this Wikipedia articles."
Why exclude the first entropy bonus term in the soft Q-function in SAC?,"
Based on OpenAI Spinning Up description of Soft Actor Critic (SAC) the soft Q-function is defined as

and as they say

Q value is changed to include the entropy bonuses from every timestep except the first.

I feel like it should make sense somehow, but they do not give any further explanation, and I don't see why it is correct. Especially because in the soft value function the first bonus term is also used:

Could someone please explain this?
","['soft-actor-critic', 'entropy']",
"What do ""large variables"" and ""small weights"" mean in these sentences?","
I'm trying to understand these two points from an article:


Models with large variables i.e weight matrices. As a consequence such models have correspondingly large gradients and optimizer states. The activations (intermediate outputs from the model layers) tend to be relatively small (depends on the batch size). Typically fully connected networks and RNNs fall under this category.
Models with small weights but large activations. CNNs and transformers tend to fall under this category.


This is my attempt to understand it:
My current understanding is that large variables would mean large matrices, and the corresponding gradient matrices are of the same size. In fully connected neural network the activation of one layer would be the same size as the next layer. So activation should usually be 1 dimension.
Now the confusing part: A model with small weights can have large activations -- I think this is because the same (small) weight matrices(e.g. 3x3 in CNN) are reused. While the activations are of dimension $(n-(k-1)) \times (n-(k-1))$, where $k$ is the width of the kernel.
Is my understanding correct?
","['neural-networks', 'machine-learning', 'terminology', 'weights', 'gradient']",
Is there a term for unquantifiably uncertain prior knowledge?,"
I'm working on a clustering algorithm which assigns each data point an index encoding its cluster. Index permutation is irrelevant to the correctness of the result. The algorithm is self-learning, in that it doesn't require labelled data to achieve its task.
The learning process can be sped up by initially providing labelled data, where each sample is assigned to a specific cluster via its index. Of course, prior to learning, we can only assume the existence of some specific clusters which may actually differ in position, shape and number. Thus, the labels are allowed to be wrong. Even if all of them are wrong, the algorithm will eventually still converge to a correct solution, but it will require a greater number of samples until it does. As long as the majority of labels are correct, there will be a measurable speedup. So the working assumption is that not all, but a sufficient portion of the labelled samples initially fed into the learner, are labelled correctly. But really, there is no guarantee. It's a mere heuristic. Neither the possibility that all labels are correct, nor that all labels are incorrect, can be excluded.
Now, I am unsure about how to describe this concept. Initially, I thought: The speedup is due to 'prior knowledge'. But I find that this term doesn't capture the uncertainty aspect of the concept very well. Personally, I'd prefer something like 'potentially flawed assumptions', but that seems a bit too vague and unwieldy. Is there an established term for this kind of concept in machine learning? If not, what would be an appropriate term which is compatible with existing terminology?
","['unsupervised-learning', 'clustering']","It seems that the problem you're describing is that the labels could be incorrect. You can just call it noisy labels (e.g. this paper uses this term to refer to labels that have been flipped with a small probability). If you are performing supervised learning with noisy data, you can call it weakly supervised learning (specifically, inaccurate supervision).Note also that, when analysing ML algorithms in learning theory or statistical learning, we also talk about an irreducible error/noise, i.e. the error that you cannot remove because it is caused by unknown or uncontrolled factors (e.g. how the data was sampled or collected).Finally, if you have trained a model with inaccurate data, you could also say that you have introduced a bias, but biases are not necessarily bad, you can bias a model in the good direction."
Training a GAN after after evaluation metric reaches minimum,"
I am training a StyleGAN-3 using one of the pre-trained models. At some point, roughly halfway through the 5000 kimg recommended for fine-tuning, the FID50K score starts oscillating around a minimum value and appears to make no further progress. From your experience, can the model still make further improvements to generated images that will be noticeable to the human eye but not reflected in the FID50K, or should I stop training early?
","['training', 'generative-adversarial-networks', 'early-stopping']","That depends on the value of your loss.If it is still too high, your model might be stuck in a local minimum. In this case early stopping and another random weight initialisation will help.You might also need to change other hyper parameters. Another reason could be that your learning rate is too high and you overshoot the minimum.If it is low enough and there has been no change for 3-5 epochs, it could be the global minimum. In that case, stop early.Edit:
The possibility of an increase in human-observable image quality while the loss stays the same depends on the metric of your loss. Most likely no because usually the evaluation metric of the loss will cover that."
"Is there some kind of ""weighted maximum"" that allows the gradients to backpropagate? [closed]","







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I was wanting to add a maximum in my neural network, but this seems a bad thing to do since it kills the gradients to all but one of the inputs.
Is there some kind of ""weighted maximum"" that allows the gradients to backpropagate?
Edit: I had a two dimensional tensor (correlation matrix) I wanted to reduce to one dimension.
","['neural-networks', 'backpropagation']","The maximum function is not smooth, since it's first derivative is not continuous.Having non-smooth functions is generally a bad thing for neural networks, since they don't work nicely with gradient decent.So what you want is a smooth approximation to these functions.Logsumexp is the smooth approximation to the maximum function and so it is what you should use in a neural network, just like softmax is a smooth approximation to the argmax
https://en.wikipedia.org/wiki/LogSumExp"
Does all GAN's in literature need to satisfy the properties of objective function of initial GAN? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



Consider the following value function of the initial GAN
$V(D, G) = \mathbb{E}_{x \sim p_{data(x)}} [\log D(x)] + \mathbb{E}_{z \sim p_z(z)} [1- \log D(G(z))]$
The min-max game on the value function: $min_{G} max_{D} V(D, G)$ ensures global optima. And finally $D(x) = 1/2$ for all $x$.
The paper provides the proof for attaining the convergence for $V(D, G)$.
After the paper, several GANs have been proposed and are using different value functions. So, I am wondering whether all the new value functions need to obey the mathematical properties of the initial $V(D, G)$ mentioned above so that the min-max game leads to the convergence point?
","['math', 'generative-adversarial-networks', 'value-functions']",
What is the right way to find the alphas in this equation?,"
In the Grad-CAM++ paper the following equation (7) is posed (written here without the relu function):
$$
    Y^c =
    \sum_k \Bigl( \Bigl\{ \sum_{a,b} \alpha_{ab}^{kc}
    \cdot \frac{\partial Y^c}{\partial A_{ab}^k} \Bigr\}
    \Bigl[ \sum_{i,j} A_{ij}^k \Bigr] \Bigr)
    \qquad (7)
$$
$Y^c$ is supposed to be a function of the $A_{ij}^k$, and $\alpha_{ab}^{kc}$ are unknowns to be determined.
In order to isolate the alphas the paper computes partial derivatives w.r.t. $A_{ij}^k$ on both sides of the equation.  In the computation no partial derivatives of the alphas with respect to $A_{ij}^k$ occur, so it seems that the alphas are considered independent from the $A_{ij}^k$. However this does not look right.  To illustrate the problem assume that we want to find $\alpha$ in the equation $\alpha x = x^2$. If we follow the method used in the paper, i.e. differentiate both sides of the equation w.r.t. $x$ while assuming that $\alpha$ does not depend on $x$ (hence $d\alpha/dx = 0$), we will get $\alpha = 2x$, which is clearly incorrect. In fact differentiating both sides with respect to $x$ yields $\frac{d\alpha}{dx} x + \alpha = 2x$, which is not going to help isolating $\alpha$. The actual solution in this case is better obtained by dividing both sides by $x$, so $\alpha = x^2/x = x$, $x\neq 0$.
So, what it would be a right way to obtain alphas satisfying equation (7)?
","['deep-learning', 'papers', 'math', 'grad-cam++']",
"Given a set of trajectories produced by a fixed policy, what is the the standard approach to estimate Q?","
Let's say that I have a set of trajectories $\mathcal{D} = \{\tau_1, \dots, \tau_n\}$ produced by an agent acting in a (episodic) MDP with a fixed policy $\pi$. I would like to estimate the $Q$ function of $\pi$ from $\mathcal{D}$. Just to be clear, each trajectory $\tau_j$ is a finite sequence
$$
\tau_j = s_0^j, a_0^j, r_0^j s_1^j, a_1^j, r_1^j \dots, s_{N_j}^j
$$
representing an episode performed w.r.t. $\pi$.
What would be the standard approach in this case? Better use TD learning or Monte Carlo?
","['reinforcement-learning', 'algorithm-request', 'monte-carlo-methods', 'temporal-difference-methods']","What would be the standard approach in this case? Better use TD learning or Monte Carlo?Both should be fine, but they might lead to different estimates, if both these things apply:The amount of data is relatively small compared to all possibilities from the given environment and policy.Either the policy or the environment are stochastic.The difference is that for each state/action pair estimated:Monte Carlo will estimate based on overall average returns, ignoring individual state transitions and policy choices.Temporal Difference will estimate based on observed state transitions and policy choices.There is a good example of what this might mean numerically in Sutton & Barto chapter 6, example 6.4. In that case it shows an advantage to TD learning when some states might be sparsely represented in the data whilst others have more instances. Monte Carlo learning will only learn the value of those rarer states from the trajectories where they occur, whilst TD learning will be able to use estimates of other trajectories, provided two or more trajectories overlap later on.This doesn't necessarily make TD learning better. If a trajectory that overlaps with others also happens to include an unusual policy choice, state transition or reward, this may spread sample bias into multiple estimates, whilst Monte Carlo would be affected less by such an outlier."
Is the Machine Learning community going against Occam's razor?,"
I have been using ML models, for a couple of years, but I am actually in the neuroscience field. In it, mathematical models try to assume the smaller number of things and make hypothesis as simple as possible. This follows Occam's Razor principle of simplicity. My concern is if this is also true for the ML or, more specifically, the Deep Learning community.
I try to briefly illustrate this. When designing DL architectures, I find some of them awfully complicated, so many parameters and layers, hand-crafted loss functions that I wonder if that is really necessary. Of course, some of the problems at hand require a non-trivial solution but sometimes it seems a bit too much. From time to time, you see a paper saying that ""we did the same but in a less complicated way"". This is cool of course, but I haven't seen many times.
The question is therefore: Should machine learning engineers/researchers put more effort in simplifying architectures? If interpretability is important, the more simple the model the better (i.e. everybody understand how a linear or sigmoid regressor works but a graph-biased-random-walk-based-parametric-dolphin-topologic transformer not so sure...)
P.S. The name of transformer is not real ;)
","['deep-learning', 'explainable-ai', 'ai-field']",
Usability of power series in AI analysis,"
In mathematics, power series is given by
$$f(x) = \sum\limits_{n=0}^{\infty} c_n (x-a)^n$$
where $c_n , a \in \mathbb{R}$
Although most of the courses in academics cover moment generating functions in probability theory for AI, which is power series by itself, I didn't encounter yet an application or analysis that contains power series in either textbooks or research papers.
So, I want to know whether the power series has any usability in any branch of AI.
","['math', 'applications', 'academia']",
How to use oxford5k for training?,"
Generally, we have training data with landmark IDs, their GTs (positive samples), and then separate query images and corresponding positive samples for evaluation.
In the Oxford5k or ROxford5k, one finds the landmark images and other images for that landmark. For example, everything starting with all_souls correspond to the all_souls building. Nevertheless, when I see other images containing this tag, it contains people and indoor images, which are possibly junk (But they are not, I guess). In the GT files, I see files like all_souls_1_query.txt, all_souls_1_good.txt, all_souls_1_ok.txt, all_souls_1_junk.txt, and so on.
I want to make sure I get the standard practice of how to train on these datasets and evaluate it properly.
","['machine-learning', 'datasets', 'training-datasets']",
How can batch prediction make drift monitoring easier than online prediction?,"
In this video, I learned that drift monitoring would be easier in batch prediction than that in online prediction:

But I don't know why and I cannot find any information about it googling. In my opinion, in online prediction we only need to keep the prediction records and we have all the history data for drift analysis. What are the differences here between the two types of drift monitoring?
","['mlops', 'data-drift']",
Should I use an unsupervised approach or train a classifier with many classes to build a deep image feature extractor?,"
I'd like to build a deep feature extractor of images (using a Bi-linear CNN).
What would lead to the best results:

an unsupervised approach (such as https://iopscience.iop.org/article/10.1088/1742-6596/1237/3/032044/meta), or
training a classifier on as many classes as I can, and hope it generalized enough?

I would then like to use this extractor as:

A weights initialization for other classical tasks
A feature extractor for Few Shot Learning approaches

","['deep-learning', 'deep-neural-networks', 'unsupervised-learning']","This is exactly the problem I am currently working on. I don't suggest that you use a supervised method to learn latent representation, as the model might learn shortcuts or only a most meaningful feature ignoring other latent features.There are several self-supervised representation learning approaches:Here is an overview of such approaches.These approaches are aimed to learn a latent representation of images (pre-text task) that can be then used for any downstream tasks (classification, query similar samples). Here is an interesting article on this topic.Another approach is to use generative models. The benefit of this approach is that you can also generate synthetic images from the embeddings that can help debug and evaluate your model.One of the simplest architectures is autoencoders (VAE, VQ-VAE). Another option is generative adversarial networks. They are more difficult to train but can produce higher quality images than autoencoders. Another advantage is that the latent space is more disentangled meaning that each value might be semantically interpreted.The main limitation is that the conventional GAN can produce samples only from random noise and to encode a real image the model has to be extended. There are different approaches to overcome this limitation, for example ALAE.This paper compares some of these approaches.While these approaches can easily capture coarse details such as hairstyle or background color, they are likely to ignore finer details. For medical imaging, the main goal is to identify cancer cells, which is essential for diagnosis. If the model does not learn this feature, it renders a latent representation of no avail.However, latent space can be forced to take these features into account by guiding the model with a supervised downstream task such as classification (StylEX) or segmentation (EditGAN)."
Resolving Derivation Discrepancies for Differentiating through Optimization Paths,"
I'm reading the paper ""Optimizing Millions of Hyperparameters by Implicit Differentiation"". The key contribution of the paper is to show that you can replace optimizing through the optimization process/path by using implicit gradients to effectively optimize hyper-parameters.
Problem:
I don't quite understand how the optimization path fits into the derivation. For example, per their derivation, let's assume that we want to optimize some hyper-paramaeters $\lambda$, where $w^{*}(\lambda)$ is the locally optimal base-parameters using some value of $\lambda$. Furthermore, let us assume we are using some vanilla gradient descent style optimization process for both the inner and outer optimization.
\begin{align}
\lambda_{new} 
&= \lambda - \frac{\partial}{\partial \lambda}\mathcal{L}(w^{*}(\lambda)) \\
&= \lambda - \frac{\partial}{\partial w^{*}(\lambda)} \mathcal{L}(w^{*}(\lambda)) \cdot \frac{\partial}{\partial \lambda} w^{*}(\lambda)
\end{align}
Where $\frac{\partial}{\partial\lambda}w^{*}(\lambda)$ can be replaced with some closed-form solution as follows using the implicit gradients theorm:
\begin{align}
\frac{\partial}{\partial\lambda}\bigg[\frac{\partial}{\partial w} \mathcal{L}(w(\lambda), \lambda) \bigg] & = 0 \\
\frac{\partial}{\partial w}\bigg[\frac{\partial}{\partial\lambda} \mathcal{L}(w(\lambda), \lambda) \bigg] & = 0 \\
\frac{\partial}{\partial w}\bigg[\frac{\partial\mathcal{L}}{\partial w} \cdot \frac{\partial w}{\partial\lambda} + \frac{\partial\mathcal{L}}{\partial\lambda} \bigg] & = 0 \\
\frac{\partial^{2}\mathcal{L}}{\partial w \partial w^{T}} \cdot \frac{\partial^{2} w}{\partial\lambda\partial w^{T}} + \frac{\partial^{2}\mathcal{L}}{\partial\lambda\partial w^{T}} & = 0 \\
\frac{\partial^{2}\mathcal{L}}{\partial w \partial w^{T}} \cdot \frac{\partial w}{\partial\lambda} + \frac{\partial^{2}\mathcal{L}}{\partial\lambda\partial w^{T}} & = 0 \\
\frac{\partial^{2}\mathcal{L}}{\partial w \partial w^{T}} \cdot \frac{\partial w}{\partial\lambda} & = - \frac{\partial^{2}\mathcal{L}}{\partial\lambda\partial w^{T}} \\
\frac{\partial w}{\partial\lambda} & = - \bigg[\frac{\partial^{2}\mathcal{L}}{\partial w \partial w^{T}}\bigg]^{-1} \cdot \frac{\partial^{2}\mathcal{L}}{\partial\lambda\partial w^{T}}
\end{align}
However, I don't understand how $w^{*}(\lambda)$ gets transformed into the following:
\begin{align}
\frac{\partial}{\partial \lambda} w^{*}(\lambda) 
&= \cdots \\
&= \cdots \\
&= \frac{\partial}{\partial\lambda}\bigg[\frac{\partial}{\partial w} \mathcal{L}(w(\lambda), \lambda) \bigg]
\end{align}
Intuitively, $w^{*}(\lambda)$ is the result of the optimization path taking $k$ gradient steps:
\begin{align}
w_{0} &= \cdots\\
w_{1} &= w_{0} - \frac{\partial}{\partial w_{0}} \mathcal{L}(w_{0}(\lambda)) \\
w_{2} &= w_{1} - \frac{\partial}{\partial w_{1}} \mathcal{L}(w_{1}(\lambda)) \\
\vdots \\
w^{*} &= w_{k-1} - \frac{\partial}{\partial w_{k-1}} \mathcal{L}(w_{k-1}(\lambda)) \\
\end{align}
Which we can substitute back into our equation as follows
$$\frac{\partial}{\partial \lambda} w^{*}(\lambda) = \frac{\partial}{\partial \lambda} \bigg[w_0 - \sum^{k}_{i=1}\frac{\partial}{\partial w_{k-1}} \mathcal{L}(w_{k-1}(\lambda))\bigg]$$
However, this isn't the quantity we want since
$$\frac{\partial}{\partial\lambda}\bigg[\frac{\partial}{\partial w} \mathcal{L}(w(\lambda), \lambda) \bigg] \neq \frac{\partial}{\partial \lambda} \bigg[w_0 - \sum^{k}_{i=1}\frac{\partial}{\partial w_{k-1}} \mathcal{L}(w_{k-1}(\lambda))\bigg]$$
Can anyone help resolve this issue for me?
","['optimization', 'gradient-descent', 'hyperparameter-optimization', 'meta-learning']",
How to explain near zero gradients on first epochs?,"
As I understand the gradient should reflect how near the weights are to the optimal values. In this way i will expect that on the first epochs the gradients far from zero or at least not mostly zero and as we train the net the gradients will arrive to values nearest to zero.
But it is not the case as you can see for example here (This image show gradients distribution on each epoch):

https://wandb.ai/ayush-thakur/debug-neural-nets/reports/Visualizing-and-Debugging-Neural-Networks-with-PyTorch-and-W-B--Vmlldzo2OTUzNA
and here (This image show gradients for 5 layers after the first batch):

https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html
I've seen the same behavior in other simple nets.
Can someone explain this?
","['deep-learning', 'training', 'gradient-descent', 'gradient']",
What is the correct partial derivative of $Y^c$ with respect to $A_{ij}^{kc}$?,"
I have a question about the Grad-CAM++ paper. I do not understand how the following equation (10) for the alphas is obtained:
$$
\alpha_{ij}^{kc} =
\frac{\frac{\partial^2 Y^c}{(\partial A_{ij}^k)^2}}
{2\frac{\partial^2 Y^c}{(\partial A_{ij}^k)^2}
  + \sum_{ab} A_{ab}^k \{ \frac{\partial^3 Y^c}{(\partial A_{ij}^k)^3} \}}
\qquad (10)
$$
I found various issues with the derivation, here I will focus on the step from equation (7) to (8).
Equation (7) without the relu function is:
$$
    Y^c =
    \sum_k \Bigl( \Bigl\{ \sum_{a,b} \alpha_{ab}^{kc}
    \cdot \frac{\partial Y^c}{\partial A_{ab}^k} \Bigr\}
    \Bigl[ \sum_{i,j} A_{ij}^k \Bigr] \Bigr)
    \qquad (7)
$$
Here $Y^c$ is supposed to be a function of the $A_{ij}^k$, and $\alpha_{ab}^{kc}$ are unknowns to be determined.
The next step consists of computing the partial derivative of (7) w.r.t. $A_{ij}^{kc}$, which according to the paper yields equation (8):
$$
  \frac{\partial Y^c}{\partial A_{jk}^k} = \sum_{a,b} \alpha_{ab}^{kc}
  \cdot \frac{\partial Y^c}{\partial A_{ab}^k} +
    \sum_{a,b} A_{ab}^k \Bigl\{ \alpha_{ij}^{kc}
    \cdot \frac{\partial^2 Y^c}{(\partial A_{jk}^k)^2} \Bigr\}
    \qquad (8)
$$
However, when I did the computation myself I got the following:
$$
  \frac{\partial Y^c}{\partial A_{ij}^k} =
    \sum_{a,b} \alpha_{ab}^{kc} \cdot \frac{\partial Y^c}{\partial A_{ab}^k} +
    \sum_l \Bigl(\Bigl[\sum_{u,v} A_{uv}^l \Bigr]
    \Bigl\{ \sum_{a,b} \alpha_{ab}^{lc} \cdot
    \frac{\partial^2 Y^c}{\partial A_{ij}^k \partial A_{ab}^l} \Bigr\} \Bigr)
    \qquad (8')
$$
Note the extra sum, and the cross-derivatives.
What is right, (8) or (8')?
","['deep-learning', 'papers', 'math', 'derivative', 'grad-cam++']","After some reflection I noticed that the actual final expression should contain derivatives of the alphas w.r.t. $A_{ij}^k$ too, because the alphas cannot be constants that do not depend on $A_{ij}^k$.  So, the equation becomes:
$$
\frac{\partial Y^c}{\partial A_{ij}^k} = 
\sum_{a,b} \alpha_{ab}^{kc} \cdot \frac{\partial Y^c}{\partial A_{ab}^k} +
\sum_l \Bigl(\Bigl[\sum_{u,v} A_{uv}^l \Bigr] 
\Bigl\{ \sum_{a,b} \alpha_{ab}^{lc} \cdot
\frac{\partial^2 Y^c}{\partial A_{ij}^k \partial A_{ab}^l} \Bigr\} \Bigr) + 
\sum_l \Bigl( \Bigl[\sum_{u,v} A_{uv}^l\Bigr] 
\Bigl\{ \sum_{a,b} \frac{\partial \alpha_{ab}^{lc}}{\partial A_{ij}^k} \cdot \frac{\partial Y^c}{\partial A_{ab}^l} \Bigr\} \Bigr) 
$$
More details in the appendix of this document.I believe this is the final answer to my question."
Pattern recognition for live stream Time serie,"
I would like to submit you a problem with which I struggle.
Suppose I have this kind of record over time in a dataframe:

fig.1
If we zoom in a bit we see such shape:

fig.2
We see that the general pattern is a increase with a pick (very high or very thin sometime even flat) follow by an almost flat part with vibration then a decrease, the we go back to zero (almost) for a time (like in the middle of fig.1) or we start an other cycle
Some have very high peak, some are more flat, some have a more longuer part before to decrease.
I have 4 classes :

increase time - 1
working time  - 2
decrease time - 3
rest time (no activity) - 0

Now assume in my dataframe I have columns that tell to what class belong each point in time.


I would like to build a model that can recognize those 4 class when it see it on stream data . Imagine that our stream data is fig.1 and that we read N points (on a sliding windows) over time. What model could allow me to classify correctly each point or subpart data point in this window according to a certain pattern (hope I'm clear)
Regarding the fact that in reallity I could be in rest time for a very long time or in working time a very long time also.
It may also depend a lot of the sliding window, we for exemple see the beginning of the increase time on the first window then it end on the next window.
I first try to use LSTM or 1D-CNN, problem I have is that it tend to see this general pattern even when it's not present.
--- UPDATE : Chillston
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=777, shuffle=False)

# input N x T x D
X_train.shape, y_train.shape, X_test.shape, y_test.shape,

((17562, 1000, 1), (17562, 1000, 4), (4391, 1000, 1), (4391, 1000, 4))

I then pass it to 1d-CNN model, I have try many Architecture even resnet-cnn version, here's it's just more classical one.
def build_res1dcnn(n_classes):

    input_shape = (X_train.shape[1], 1)

    inputs = Input(shape = input_shape, name = 'input')

    # Stage 1
    x = Conv1D(64, kernel_size=3, strides = 2, padding = 'same', activation = 'relu', 
                       kernel_regularizer = 'l2', kernel_initializer = 'he_normal',
                       bias_regularizer = 'l2')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling1D(pool_size=(2))(x)
    x = Dropout(0.2)(x)

    x = Conv1D(128, kernel_size= 5, strides = 2, padding = 'same', activation = 'relu', 
                       kernel_regularizer = 'l2', kernel_initializer = 'he_normal',
                       bias_regularizer = 'l2')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling1D(pool_size=(2))(x)
    x = Dropout(0.2)(x)

    x = Conv1D(64, kernel_size=3, strides = 2, padding = 'same', activation = 'relu', 
                       kernel_regularizer = 'l2', kernel_initializer = 'he_normal',
                       bias_regularizer = 'l2')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling1D(pool_size=(2))(x)
    x = Dropout(0.2)(x)
    
    x = GlobalMaxPooling1D()(x)
    
    # Here I want T x n_class for a T X 1 input sequence
    outputs = []

    # https://stackoverflow.com/questions/51397484/appending-tensors-in-keras
    Ty = X_train.shape[1]
    for i in range(Ty):
        out = Dense(n_classes, activation = ""softmax"")(x)
        outputs.append(out)

    output = Concatenate()(outputs)
    output = Reshape([Ty, n_classes])(output)
    
    
    model = Model(inputs = inputs, outputs=output)
    
    model.compile(optimizer = Adam(learning_rate = 0.1), 
                  loss = 'categorical_crossentropy', 
                  metrics = ['categorical_accuracy'])

    
    #model.summary()
    
    return model

","['classification', 'time-series', 'pattern-recognition']",
What is so special about the Bellman Optimality Principle?,"
In the context of Decision Making and Game Theory, ""Bellman's Equations and Bellman's Conditions of Optimality"" are said to be some of the most important mathematical principles in this field.
Reading the corresponding Wikipedia page,
I am trying to understand what is considered so ""groundbreaking"" about the Bellman's Condition of Optimality.
As far as I understand, Bellman's Principle of Optimality is saying that - for a policy to be considered as optimal, the policy must be optimal at each time point where the policy is being considered. If I have understood this correctly - isn't this kind of obvious?
To me, this sounds like a tautology - for something to be blue, the thing must also be blue.
I think I am obviously not understanding the above statements properly.
In short, could someone please explain why Bellman's Equations and Bellman's Conditions of Optimality are considered so ""important and groundbreaking""?
","['reinforcement-learning', 'bellman-equations']",
What do symmetric weights mean and how does it make backpropagation biologically implausible?,"
I was reading a paper on alternatives to backpropagation as a learning algorithm in neural networks. In this paper, the author talks about the disadvantages of backpropagation, and one of the disadvantages stated is that backpropagation requires symmetric weights and that's why it's not biologically plausible.
What do symmetric weights mean and how does it make backpropagation biologically implausible?
","['neural-networks', 'backpropagation']","""Symmetric weights"" means that the same weight value associated to a pair of nodes must be used during the forwards and backwards steps.The reason it makes back propagation biologically impossible in its naive formulation is that neurons fire electrical signals in only one direction, from the dendrite through the axon to other dendrites of other neurons. They do receive of course ""backwards"" feedback, but by other means, e.g. chemical neurotransmitters or other signals from other neurons, but these signals are very likely not of the same intensity as the signal emitted by the neurons themselves (i.e. no symmetry)."
Can you train GPT-J to use a specific list of words and prioritise them?,"
Can you train GPT-J to use a specific list of words and prioritise them? If so, please could you share how I would go about this?
Say you're using GPT-J to write a story, you might wish to mention certain key terms more than others, or in a specific order.
","['machine-learning', 'natural-language-processing', 'gpt', 'natural-language-generation', 'gpt-3']",
Why does the SVM perform poorly on test data that has a different class distribution than the training data?,"
Do you know why the SVM performs poorly on test data that has a different class distribution than the training data? The training data has around 15 classes, and the additional testing data has around 6 classes (a subset of 15 classes). I found that the accuracy of new testing data is around 3% (all predicted labels belong to the same class).
How can I deal with this problem?
","['machine-learning', 'overfitting', 'support-vector-machine', 'training-datasets', 'test-datasets']",
What is a 'degenerate run' in evaluating model performance?,"
I've recently come across a paper that uses the term ""degenerate run"", but I'm not sure if I understand what it means. The idea is that when they report the average performance of running fine-tuned models using multiple random seeds (e.g., a deep learning model where we need to initialize model parameters using multiple seeds to ensure results are robust,) they exclude the degenerate runs in some of their analyses.
As this paper mentions, a degenerate run is ""where fine-tuned models fail to outperform the random baseline."" But I wonder if this is a standard practice to eliminate such results when reporting the average performance? Or is the definition they give the correct meaning of a degenerate run?
","['machine-learning', 'terminology', 'papers', 'performance']",
Why is it important/beneficial for an activation function to be zero-meaned?,"
Conventionally, (although there are plenty of better options) it is being said that as the choice of activation function for hidden layers, tanh should be prefered over sigmoid because it has a zero mean but what if the data at hand is 0.5 mean and we are not willing to zero mean the data? Would tanh still be the go-to option?
","['neural-networks', 'deep-learning', 'activation-functions']",
Why doesn't the inception score measure intra-class diversity,"
It's mentioned here that there is no measure of intra-class diversity with the inception score:

If your generator generates only one image per classifier image class,
repeating each image many times, it can score highly (i.e. there is no
measure of intra-class diversity)

However, isn't it ""easy"" to look at the variance of the outputs of the classifier for a given class (e.g. if you only output 0.97 for all the images of your GAN class then there is no intra-class diversity but if you output 0.97, 0.95, 0.99, 0.92, there is diversity?). I'm struggling to understand why this is hard to do (but I might be missing something!).
","['generative-adversarial-networks', 'inception']","For reference, a recap of Inception Score:
The inception score is computed by comparing the categorical output distributions of an inception model, given examples from real vs synthetic images. If the synthetic images produce similar class distributions as the real images, the inception score is high, otherwise it is low.However, isn't it ""easy"" to look at the variance of the outputs of the classifier for a given classSay you want to generate multiple horses and the model learns to generate horses with different colors but always in the same pose - then your class probabilities will vary, but I wouldn't call this very diverse horse generation. This is how I would understand what is meant by your cited statement.The output distributions from the inception model contain class information but very little information of specific image features. Thus, the inception score cannot be sensitive to intra-class variations of the generator."
Examples of self-explainable models used in NLP other than prototype-based,"
I am looking for all the methods used in NLP that are self-explainable, or explainable by design. That is to say, the ones that use the predictive model itself to explain the entire model's predictive reasoning (a.k.a. directly interpretable model).
I have only a few example in mind:

prototype-based methods, models that reveal the network's reasoning by providing prototypical explanations. It may allow users to directly manipulate a network component according to their preferences like Rasa or, more recently, Proto-Trex.
Human-in-the-loop ML, where the co-creation allows to interpret and update the model logic directly by enabling interaction with rule generators themselves. But as the first of these are created with deep learning, is it truly part of explainable AI? An example of this is HEIDL, which did reference to related work such as Chorus

","['natural-language-processing', 'explainable-ai']",
"BERT2: How to use GPT2LMHeadModel to start a sentence, not complete it","
I am using GPT2LMHeadModel to change the way GPT2 choose the next word in a sentence. At this point, I have to give the initial part of the sentence and GTP2 starts to predict the better next word.
I want GPT2 to read an entire sentence and then start a new one based on that (like it does with translation)
Is there any kind of parameter that I need to set up in order to make GPT2 start a sentence from zero, not complete an initial one?
","['natural-language-processing', 'machine-translation', 'natural-language-generation', 'gpt-2']",
What happens if all the features are correlated with each other before clustering?,"
I know that when two features are highly correlated with each other, one of them should be removed from the dataset so they don't add twice the weight. However, what if all my features share a correlation?
For example, when I calculate the Pearson coefficients I get:

Does this mean that only one feature is actually relevant when I start clustering the dataset?
","['machine-learning', 'unsupervised-learning', 'clustering']","Essentially, yes. One feature predicts to a reasonably high degree what the other features look like, so the additional features have limited discriminatory power. Obviously there is some effect, as they don't correlate perfectly, but it's minimal.I would look for some other features if possible."
How Many Hidden Units in an LSTM? [duplicate],"







This question already has answers here:
                                
                            




How to select number of hidden layers and number of memory cells in an LSTM?

                                (4 answers)
                            

Closed last year.



Is there any rule of thumb for choosing the number of hidden units in an LSTM? Is it similar to hidden neurons in a regular feedforward neural network? I'm getting better results with my LSTM when I have a much bigger amount of hidden units (like 300 Hidden units for a problem with 14 inputs and 5 outputs),  is it normal that hidden units in an LSTM are usually much more than hidden neurons in a feedforward ANN? or am I just greatly overfitting my problem?
","['neural-networks', 'long-short-term-memory', 'hidden-layers']",
Should the range of target values match the range of activation function used in the output layer?,"
Suppose I use a tansig activation function in the output layer of an artificial neural network giving me outputs in the range $[-1,1]$ and my model is applied to a binary classification problem, should my target labels be -1 and 1 or 0 and 1? I've always used 0 and 1, but now I'm questioning this.
","['neural-networks', 'activation-functions', 'labels']","Yes, you should use an activation function that match the range of your ground truth labels, or the other way around, i.e. apply a normalization function to the labels to match your activation function.If the range of model predictions and target differ in range, the model might still learn but the convergence will be slower, since you're basically allowing a broader range or loss values. With both predictions and labels within range [0, 1] for example, the difference between a prediction and its label will also be in the same range. If you allow predictions to be in range [-1, 1] then you could have errors larger than 1, for example $p=-1, t=1, |t-p|=2$.The decision about mapping the predictions to the targets range or the way around depends on the specific case you tackling. Sigmoid and tanh for example behave differently with respect to the gradients, the latter producing larger gradients, as you can see from the derivatives comparisons. So if you want larger gradients you might want to keep the tanh and map you targets to the range [-1,1]"
Is this the right approach to preprocessing data for artificial neural-networks? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I recently participated in a competitive ""hackathon"" with the problem being binary classification of overall satisfaction for travelers. The dataset mostly consisted of survey questions and answers: ex. ""Seat_Comfort"" : Extremely Unacceptable, Unacceptable, Needs improvement, acceptable, Good, extremely good
Other numerical info such as travel distance and sex/age was also included. Some Null data was throughout.
My question is what is the ""right"" way to preprocess this data?
The ordinal ""survey"" data I encoded as -3, -2, -1, 1, 2 ,3. Other categorical variables I hot-encoded. I standardized but DID NOT Normalize the data (trial and error did not show significant accuracy gains after normalization). For the most part I filled Nulls with averages, but did not really remove outliers first.
I spent so much time on trial and error in preprocessing the data and trying new approaches that I barely had time to hypertune the actual ANN parameters. I decided to come here and ask the ""right"" way to do it, if there is such a thing or standard.
For starters was ordinal encoding with negative numbers the way to do it? or should I have stuck to 1,2,3,4,5,6.
travel distance mean was like 2000, should that have been Normalized considering the relatively high values? Should I have removed outliers as a matter of course?
Some of the features were highly correlated with each other and I removed them (0.6 - 0.9) correlation. Is there a ""best case"" threshold for removing correlated features?
At one point I filled missing ""SEX"" values with UNK, as opposed to Male/Female in the data
If it matters at all, my ANN was a 3 hidden layer with dropouts and 1 TANH and 2 SELU activation. Sigmoid activation for the output obviously. I implemented a bagging, ensemble method to good results in the competition, but was still left with these burning questions.
To summarize:
Whats the right way to ordinal encode data?
Whats the threshold for dropping outliers?
Always Normalize? Always Standardize?
How to fill missing categorical data with similar probabilities (SEX for instance)
Is mean the most effective way to fill null numerical data?
Is there a threshold for dropping highly correlated features to improve performance?
","['neural-networks', 'data-preprocessing']","Here is what I'd do:If there's a natural ordering to the data, which is your case, then it's fine to map the data to a non-decreasing sequence of numbers. It's not very important where you start the sequence, because you should always normalize your input data.If you don't normalize the data, then it's going to make learning much more difficult: the weights will have to have wildly different magnitudes to compensate for the difference in the magnitude of your inputs, and the variance of the gradient updates is also going to be very large, which is detrimental to learning.It's actually quite important to understand whether these outliers are erroneous data or not. If not, you want to keep them and might actually need to re-balance your dataset, or change your loss function to overweight them, so that your model correctly classifies them.Imagine a situation where you want to know whether someone is a basketball player or not: your dataset will be mostly made of average heights, with a few extreme outliers which you actually want to classify as basketball players!As I explained above, always standardizeHow to deal with missing data also depends on why the data is missing. In your case, I could imagine that whether or not the ""sex"" variable is reported by the user is highly correlated with the outcome you're trying to predict.One could imagine that people not reporting their ""sex"" are more privacy conscious and therefore have higher standards when judging a service, which would entail a higher probability of being dissatisfied.In that case, I would have a special value for missing data, and the neural network will use that information for its predictions.Same answer as above, you should consider whether or not the data is reported  is important to the classification task.In general, if you suspect you have highly correlated features, you can do something like PCA first on the input data and keep only the K top components. However, K is now a hyper parameter of your algorithm, which you need to tune.In general though, neural networks are quite good at discovering correlation in the input data, so I would not try to decorrelate the input features as a preprocessing step.Hope this helps!"
How to handle list features in clustering?,"
I have a dataset where one of the features is a list.
Example:
[
 {
  ... other features ...
  X: ['joey', 'monica', 'ross']
 },
 {
  ... other features ...
  X: ['chandler', 'rachel', 'joey', 'phoebe']
 }
  ... more records ...
]

Feature X in the example can be of different lengths(let's say [0,50]).
The domain for the values in X is of 200 different values.
How should I handle X?
Thanks.
EDIT:
More information about what X is.
I'm trying to cluster failed software tests.
Each test might fail in a different component.
X is a list of failed components.
What I'm trying to understand is how to represent this list of failed components in the vector of each data point.
","['clustering', 'feature-engineering']",
What architecture would be best to match images of torn pieces of tapes?,"
I am currently working on a project where the goal is to create a neural network that can determine if two pieces of torn tapes are a true fit or not. My current idea is a convolutional network that takes as inputs  2 pieces of tapes (256,256,2) and outputs a 1 or 0 if they are a true fit or not. The 2 channels are independent of each other, so I use grouped convolutions. Is there a better way to go about this? Any ideas would be appreciated!
Example of the data
Channel 1 (tape-1)

Channel 2 (tape-2)

These pieces of tapes are an example of a true fit
","['neural-networks', 'convolutional-neural-networks', 'binary-classification']",
"What does ""position"" in ""each position in the decoder"" denote in the Transformer's original paper?","
I am reading Attention is All You Need and I feel confused about the word ""position"" in this paper, by the way I'm not native English speaker which may cause my confusion which has confused me for a few days.
In this paragraph from 3.2.3 of the paper,

Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.

""each position in the decoder"", where is it actually, in which part of decoder? What does ""each position in the decoder"" really want to mean? Each layer in the decoder? Or something else like position of vectors? And what does ""all positions"" mean? Same with the former ""position""? Could you please give an example to explain?
I've already known how masked attention works. It uses masking to prevent the encoder from attending to positions in outputs it should't attend to during training because entire translation'd be given when training.
","['terminology', 'papers', 'transformer', 'attention']",
Why does triplet loss allow to learn a ranking whereas contrastive loss only allows to learn similarity?,"
I am looking at this lecture, which states (link to exact time):

What the triplet loss allows us in contrast to the contrastive loss is
that we can learn a ranking. So it's not only about similarity, being
closer together or being further apart, but now we want to learn how
much closer am I compared to another image.

The contrastive loss
$L(A, B) = y|f(A) - f(B)| + (1-y)max(0, m-|f(A) - F(B)|)$
would push similar samples together, and dissimilar samples apart.
The triplet loss
$L(A, P, N) = max(0, |f(A) - f(P)| - |f(A) - f(N)| + m) $
would push the positive close to the anchor, and the negative away from the anchor.

I fail to see why the quoted claim is or isn't true in either of these losses. To me, it looks like ""same"" samples are pushed together, and ""different"" samples are pushed apart by both.
Furthermore, with the contrastive loss, the distance in the embedding space would be, as I understand it, the ranking- which is claimed to only exist with the triplet loss.
Is there a clearer reference for this, or just a simple answer?
","['deep-learning', 'reference-request', 'loss', 'triplet-loss-function', 'contrastive-learning']",
What is the correct formula for the loss function?,"
I have used  the Delayed sin echo prediction with Tensorflow that predicts the sin wave. However, I'm not sure of the correct formula for the loss function. The problem is that I feed the training mini-batch as batch_x and batch_y, containing a combination of historical and current information. However, I provide the model with only the current information $o_{t}$ in the prediction. In my current setting, $historical \subseteq(o_{t-k+1},o_{t-k+2},\ldots,o_{t})$ where the $k$ is the size of the historical information. So, my question is how to select $y_i$ if both batch_x and batch_y use the historical and the current information.
\begin{equation*}
 \mathrm {MSE=}\frac {1}{n}\sum _{i=1}^{n} {({y}_{i}-\hat{y}_{i})}^{2}
\end{equation*}
","['tensorflow', 'loss']",
How does PPO account for the last reward?,"
I was implementing PPO for the lunar lander environment in openai gym, but my agent seems to be getting stuck at a score of ~-80. On the website it says the agent gets rewarded +100 or -100 if it comes to a rest or crashes at the end. However, my time horizon is set to 20, which leads me to think the last reward is not properly accounted for (since episode lengths arent always a multiple of 20) and thus, the agent predicts bogous values for future states which causes the poor results. This begs the question: how does ppo account for the final reward? Do we just hope the algorithm stumbles upon an episode where the length is a multiple of the time horizon, or is it that the environment is programmed to wait a few steps after finishing to ensure that the final reward gets accounted for?
",['proximal-policy-optimization'],"I actually feel kinda dumb writing this, but the update is made if the TOTAL environment steps is a multiple of the time horizon. I did, recode my PPO algorithm, and it seems to eventually solve the lunar lander environment, albeit very slowly (taking around 650k training steps from 1700 episodes to solve), and the training is very unstable, the agent might average around 100 points and drop to an avg of -80 in the next 500 episodes and suddenly jump back to 200 points (my lr is 3e-5)."
How to handled delayed rewards in contextual bandits [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



All the examples I see in the tf_Agents for contextual bandits, involves a reward function we generated the reward instantly after an observation has been generated.
But, in my real world usecase (say sending emails and waiting for the click rate), rewards will be observed after 3 days of generating the observation. How to include this scenario of delayed rewards in training the agent ?
","['reinforcement-learning', 'tensorflow', 'experience-replay', 'contextual-bandits']",
How to understand the GCN equation?,"
I understand GCN does message passing with its neighbours to learn the node embedding.
But I don't understand the following equation.
What ""tilda"" is referring to equation 1?

","['machine-learning', 'deep-learning', 'graph-neural-networks', 'gnn']",
Are there any guidelines on how to map the state space to integers in the case tabular RL algorithms?,"
Let's say that you want to solve a problem with a tabular reinforcement learning algorithm, for example, Q-learning. You can represent the value function $Q(s, a)$ as a $|\mathcal{S}|\times |\mathcal{A}|$ matrix. In practice, we could use an array to represent $Q$. To index this array, you need integers, unless you use e.g. hash maps or another data structure. Let's call the (NumPy) array Q, then Q[i, j] would be the value of action j at state s.
My question is: are there any guidelines on how to map the state space to integers, so that we can index our table? In the case of a grid environment, you can try to enumerate all possible states and use some convention to map a state to an integer. However, in more complex environments, this might not be straightforward. Do you know any other ways to try to solve this implementation problem?
Note that this is a design/implementation question rather than a programming issue. I am looking for guidelines or common practices (other than the one that I mentioned). Feel free to share with me examples of how this has been done in other projects/environments (links to the projects/repos are also appreciated).
","['reinforcement-learning', 'reference-request', 'implementation']",
Why do off-policy algorithms suffer from worse computational or time efficiency compared to on-policy algorithms?,"
When I run Soft-Actor-Critic (off-policy) in my Environment, the calculation of gradient updates takes almost twice the time compared to using PPO (on-policy).
I also saw that ACER has a higher time complexity compared to PPO in this paper PPO with Prioritized Trajectory Replay
In their comparison, ACER took almost 5 times longer (wall-clock time) compared to PPO for 1000 steps in Altantis-v0 benchmark.
That's why it came to my mind that this maybe has to do with their different algorithmic approaches (off- vs. on-policy).
This question is not about the difference between sample- and computational- or time efficiency, as this was quite well explained here.
The question is: Is my assumption correct that off-policy algorithms are more computationally or time complex than on-policy ones? If so, why? Is there a certain component like the replay buffer which increases complexity?
Or is my assumption incorrect and the computational or time complexity differences between the algorithms was only implementation related, meaning that off-policy algorithms doesn't necessarily have to be more computational complex or take more time than on-policy ones?
","['reinforcement-learning', 'off-policy-methods', 'on-policy-methods', 'computational-complexity', 'sample-efficiency']",
What is your training time of Resnet-18/Resnet-50 on Imagenet?,"
My training of Resnet-18 network on Imagenet using Tesla V100 seems to be quite slow (1 epoch is about 2,5 hours, batch 128). Increasing the number of GPUs does not seem to help.
What is your training time of Resnet-18/Resnet-50 on Imagenet? How many epochs do you train for to obtain the desired accuracy? I am wondering what I should expect.
","['machine-learning', 'training', 'epochs', 'image-net']",
How to reduce the dimensionality of the actions in RL,"
I have a single-agent RL model in which the dimension of the dimension of the action space is $70$. This action space is too big and the deep RL agent is not learning properly. The boundaries of the action space are $-1$ and $1$.
My question is, how can I reduce the dimensionality of the action space?
I have tried to use auto-encoders with random vectors of dimension $70$ between $-1$ and $1$, but it is not working properly. I am training the encoders using a hidden layer with 10 neurons. However, comparing the original action with the result of encoding and decoding it I can see that the average difference between the components is $0.2$ when the action is in the range $[-1,1]$
","['reinforcement-learning', 'deep-rl', 'action-spaces', 'dimensionality-reduction']",
How are 4D cost volumes constructed for DL based stereo matching?,"
I read a paper on Stereo Matching using Pyramid Cost Volumes (paper link: Semantic Stereo Matching with Pyramid Cost Volumes). At some point, in the proposed architecture, after:

Feature extraction from both left and right images
Performing adaptive average pooling at three scales (4, 8 and 16)
Reducing number of channels using 1x1 convolutions at each scale.

the authors form a cost volume by concatenating the corresponding unaries from the left and right image features and packing them into a 4D volume. The final dimensionality is: 
$$C \times \alpha W \times \alpha H \times \alpha D, \alpha \in \{\frac{1}{4}, \frac{1}{8}, \frac{1}{16}\} $$
, where C - number of channels, W and H - width and height of the images, D - maximum disparity.
My question is, for all the possible disparities, how can this 4D volume be visualized? How is the volume, for each possible disparity, constructed in the first place?
","['deep-learning', 'computer-vision', 'papers']",
"If the probabilities with which each task is selected for you do not change over time, why would it appear as a single stationary k-armed bandit task?","
Sutton-Barto (Section 2.9-Associative Search (Contextual Bandits), page 41):

As an example, suppose there are several different k-armed bandit tasks, and that on
each step you confront one of these chosen at random. Thus, the bandit task changes
randomly from step to step. If the probabilities with which each task is selected for you
do not change over time, this would appear as a single stationary k-armed bandit task,
and you could use one of the methods described in this chapter.

Question: If the probabilities with which each task is selected for you do not change over time, why would it appear as a single stationary k-armed bandit task?
","['reinforcement-learning', 'sutton-barto', 'multi-armed-bandits']",
What is an epistemic graph in AI and how is it related to cognitive science?,"
I found this paper Epistemic graphs for representing and reasoning with positive and negative influences of arguments.
I haven't found any definition of or Wikipedia article on epistemic graphs on the Internet.
Can anyone give me an introductory idea?
","['definitions', 'cognitive-science']",
Gradient bandit algorithm: is $\bar{R}_t$ average of all rewards or average of rewards corresponding to $A_t$?,"
Sutton-Barto (Section  2.8-Gradient Bandit Algorithms, page 37):

Question: is $\bar{R}_t$  average of all rewards  or  average of  rewards corresponding to $A_t$?
","['reinforcement-learning', 'sutton-barto', 'multi-armed-bandits']",
Is there any research on anger and distrust detection (presence and level of political cynicism)?,"
The undergrad research project I'm working on would require me to detect presence and level of political cynicism from reddit posts.
According to definition political cynicism consists of

anger
distrust

towards politicians or political institutions.
Does anyone know of existing research relevant for my goal?
I know approaches exist for emotion detection, so the anger component should be no problem.
But how could I model distrust or at least come close to it?
I have access to a large dataset consisting of reddit posts (from politics related subreddits) with labels measuring the political extremity of the user. Political cynicism is one of the most distinguishing features of those on the extreme ends of the political spectrum compared to moderates according to data from psychology, so the data should somehow reflect this correlation, and therefore there should be a some opportunity for a CNN to pick up on it. But would there be a way to then isolate this component?
Any guidance would be appreciated.
I'm a newbie to NLP so please forgive any ignorance expressed through this post.
","['machine-learning', 'natural-language-processing', 'reference-request']",
ML model to predict timeouts,"
I am new to ML and am trying to build a model to predict timeouts for a website.
The website is being monitored once a minute and the data consists of a timestamp and the response time in seconds. E.g.: 1650539220000 6.234041.In this particular case I do not have metrics from the server itself, just everything externally available.
When collecting the metric a timeout for the http request is set to 15 seconds. Meaning that every time the server does not respond in time I have no data and I want to predict when the next time out will happen. Currently, I have about two weeks' worth of data and want to predict the next likely timeouts/anomalies within some hours.
Since missing values are a huge issue with ML models I tried to handle the timeouts by setting the value to

20 to have large ""peaks"" which does not work too well with some models
integer-encode the data in 0 (no timeout) and 1 (timeout)

To have more features to work with I also tried splitting the timestamp into month, day, hour, minute.
So far I have tried multiple algorithms which did not perform too great

Prophet
LGBMRegressor (best so far)
AutoReg
ARIMA

I did not tune the algorithms, as I am quite new to ML.
What method, model, or approach could I try to make more accurate predictions?
","['classification', 'algorithm-request', 'model-request', 'anomaly-detection', 'forecasting']",
Why is depth-limited is preferable to minimax without depth limited,"
I have read about a question that says the following:
Why is depth-limited minimax preferable to minimax?
One of the wrong answers was:

The depth-limited minimax will achieve the same output as minimax without depth-limited, but can sometimes use less memory.

Why is the above answer is wrong? I mean, don't both of these algorithms always achieve the same output, and because the depth-limited minimax doesn't always explore all the states this makes it use less memory?
","['search', 'minimax']",
How does Weight Sharing with the Generalization in Graph Neural Networks work?,"
I have two closely related points regarding the weight sharing and generalization of graph Neural network. For illustration purposes, I attached two images which I reference. Images are taken from the Stanford course ""CS224W:Machine Learning with Graphs"" given by ""Jure Leskovec""


In the centre above one can see that the blue, red and green node feed into the yellow one. The same can be seen one the right side with the same colour configuration. Are now these parameters shared across all computational graphs which 3 nodes feeding into 1?



If yes, then how does this generalize to a potential new graph? In the second image above, one can see that there are new types of computational graphs needed, e.g. on the right side the orange node has five neighbours while there is no such node on the left side. So, how does the generalization work in this case?

","['graph-neural-networks', 'generalization']",
Learning values in open ball: which final layers to employ?,"
I'm fairly new to deep learning and looking for some reference literature... Specifically, I want to train a neural network to predict vectors $v \in \mathbb{R}^3$ under the constraint $||v||\leq 1$.
I add that the case $v\simeq0$ is the most common in my dataset. Which final layer(s) would you suggest to accomplish this? These layers would follow a CNN + fully connected layers body.
Thanks in advance

Background: this vector would represent the $SO(3)$ element of a rotation matrix
","['deep-learning', 'objective-functions']",
How to deal with an unbalanced dataset?,"
I'm constructing a feed forward neural network that predicts whether a patient will get a stroke or not. However, my dataset is very unbalanced. Out of 5111 rows, 250 contain patients that have had a stroke (1) and 4861 that did not (0). The accuracy is (as a result of this, I suppose) very high (89% on the first epoch, and 95% on the second, then it stays at 95%). What would be the best thing to do about this?
","['neural-networks', 'machine-learning', 'datasets', 'feedforward-neural-networks', 'imbalanced-datasets']",
Denoise autoencoder not training properly [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I'm trying to make a denoise autoencoder wherein the encoder part is vgg16 and decoder is opposite of vgg16(encoder) network. My dataset consists of 5K images in grayscale.
Now while training, the loss and accuracy doesn't changes. I can think of reducing filters in the initial decoder layers but i fear that's going to affect the autoencoder. Here, i'm really clueless about what approach to follow.
","['convolutional-neural-networks', 'keras', 'autoencoders', 'vgg', 'denoising-autoencoder']",
Extract person silhouette from photo or video,"
Are there any programming libraries or neural network design patterns designed for the task of finding persons in a photo/video and extracting their silhouettes (i.e. not only the rectangle containing the person but a bitmap marking which pixels are part of the person and which are background)?
",['object-detection'],"Check out pytorch/pyvision.  This is wonderful in that it has many great models that are pretrained that you can use to train your model on and then go further and use what is trained to train further.  It has bounding boxes, silhouettes, segmentation, etc.https://medium.com/@ashishgupta_65016/semantic-segmentation-for-silhouette-extraction-e52c7d319295https://pytorch.org/vision/main/auto_examples/plot_visualization_utils.htmlI am sure there are many more options out there.  This is just one that I am familiar with.  Good luck!!"
"How to construct a reward function for a ""wait and see"" problem","
I'm working on a problem that I think could probably be represented as a reinforcement learning task, but I'm uncertain about how to design the reward function. The core task is essentially a classification problem, but the data used to make the classification arrives over time. So we want to balance two competing aims:

On the one hand, we want to apply the correct label to the widget quickly.

On the other hand, we don't want to apply a label to the widget prematurely.


The widget's true label doesn't change with time -- it's fixed for all time-steps.
This is how the process works. At each time step, we collect the same set of features about a widget. Then, using the history of observed features, I want the agent to either apply a label to the widget, or else wait for more data. We want the agent to dynamically decide in time what the object's label is. The whole history of current and previously observed features is available to the agent, so the data for a single instance at time $t$ looks like
$$
x_{1t}, x_{2t}, x_{3t}, \dots, x_{kt}
$$
and the collection of all time steps up to time $T$ is a $k \times T$ matrix of the $T$ feature vectors.
My questions are

Does this kind of reinforcement learning task have a specific term of art?

How should the reward function be constructed?


My thinking so far is that we could use an approach similar to a maze-solving RL agent, where each ""wait"" action has a $-1$ reward, applying the correct label has a $+1$ reward, and applying an incorrect label has a $-1$ reward, but this seems rather simplistic. I'm worried that since an incorrect label is the same as waiting, the agent might learn to just guess at random, hoping to get a correct label by chance, instead of learning when to wait for more information.
","['reinforcement-learning', 'reference-request', 'terminology', 'reward-functions', 'reward-design']",
Reinforcement learning algorithms for large problems that are not based on a neural network,"
I have a large control problem with multidimensional continuous inputs (13) and outputs (3). I tried several Reinforcement learning algorithms like Deep-Q-Networks (DQN), Proximal Policy Optimization (PPO) and  Advantage Actor Critic (A2C). Unfortunately, they all yield poor results. As far as I understand, they are all based on neural networks. Because of this I think it might be possible that the neural network itself could be a problem as it might not be able to learn the mapping between inputs and outputs (I have experienced this in several other applications).
So, are there state-of-the-art reinforcement learning algorithms for large problems with multidimensional continuous state spaces and actions?
","['reinforcement-learning', 'algorithm-request', 'state-of-the-art', 'continuous-action-spaces', 'continuous-state-spaces']","There are many state-of-the-art reinforcement learning algorithms for large problems with multidimensional continuous state spaces and actions. All of them rely on some sort of function approximator.You can use any RL algorithm with really any sort of function approximator, whether a neural network,  support vector machine, decision tree, or any other method. Every RL algorithm you mentioned can use any of these function approximators instead of a neural network, if so desired.However, almost all state of the art results today use neural networks. This is largely due to 2 reasons, one theoretical and the other empirical. The theoretical reason is that neural networks have a universal function approximation theorem which roughly states that given an arbitrarily large network, they can approximate any continuous function. The empurical reason is that for complex problems, neural nets tend to outperform all other methods.So to more directly address your question. Yes you can use the othe methods I mentioned above. But its probably a bad idea. You are likely better off with either a larger neural network, a neural network architecture more well suited to your problem, or perhaps there are other issues in your approach you have missed. However, it is very unlikely the issue you face is a fundamental problem of neural networks."
"Is there way to segment an image without labeling/classification, as well as supervised learning?","
Is there way to segment an image without labeling/classification, as well as supervised learning?
For an illustrative example, if one considers an image with a dog and a cup (we don't particularly care what is in the picture), what are viable approaches (if there exist any) to segment these objects in order to perform localization afterwards?
How can one detect arbitrary objects in an image?
So far, I tried a level set approach using a Hamilton-Jacobi Equation to model the energy within the picture. However, this is extremely slow (since iterative) and highly depends on the preprocessing of the image to detect anything.
","['machine-learning', 'reference-request', 'object-detection', 'image-segmentation', 'algorithm-request']",
How should I write the reward function to teach the agent the rules of this card game?,"
I'm quite new to reinforcement learning. I've been training the model for the following problem but the mean reward is stuck.

In a 5 by 5 board, each position can contain a card with a color (0-4) and a value (0-9).
Some initial cards, all distinct from each other, may be on the board.
In each round, a card not on the board is chosen with uniform distribution.
The player has to place this card in an empty position before the next card is chosen.
When all positions are filled, the game ends and the final score is the sum of all points of each row, each column, and each of the two diagonals, where the points are calculated as follows (order does not matter, e.g. AAABB=ABABA, 12345=23415):





Combination
Points
Colors
Values




Straight flush
30
All identical
All consecutive


Straight
15
Not all identical
All consecutive


Flush
14
All identical
Not all consecutive


Five of a kind
28

AAAAA


Four of a kind
16

AAAAB


Full house
10

AAABB


Three of a kind
6

AAABC


Two pairs
3

AABBC


One pair
1

AABCD


Nothing
0






The goal is of course to get a score as high as possible.
Question:
How should I write the reward function to teach the agent the rules?
I used invalid action masking, so no invalid actions can be taken. However, it's still hard for high-score combinations to randomly show up. I tried setting reward to be the points possibly added due to the placed card (like if the placed card can form a pair with another one in the same row, add some points based on how much point a pair is rewarded in the final score), and reward 100*(final score) at the end, hoping the agent to learn that the final score is the most important, but the mean reward was not improving even after 2 million timesteps.
I use MaskablePPO with MlpPolicy (alias of MaskableActorCritic) from sb3-contrib. I represent each card as an int8. The upper 4 bits representing the color and the lower 4 bits represent the value.
The observation space I use consists of the card to be placed, the cards on the board, and all cards not on the board. I'm wondering if I should include cards not present in the observation space. I tried to exclude it but it does not change the results much.
Apart from direct answers, any pointers/reference would be much appreciated.
","['reinforcement-learning', 'deep-rl', 'reward-functions', 'reward-design', 'card-games']","I would recommend having the reward as the increase to score caused by adding the card in the chosen location. You could optionally include calculations based on partial rows for pairs, three-of-a-kind etc. I do not think you should grant a different final reward for the end score, and definitely not some large difference, as the gradient for a large difference may swamp the networks (unless you also drop the learning rate, but then the smaller early values will not get learned from).I do not think that your focus should be on clever manipulation of reward signal. It is likely that looking at other things related to this problem will give you better impact.First you should look at your state representation. If you are feeding the int8 values directly to a neural network, then this is very unlikely to work. Instead, you should use a short vector for each card you want to represent - of either 6 or 15 elements. You should be one-hot-encoding the colour for each position. You may find one-hot-encoding the value also helps, although that is trickier - if you don't one-hot encode, then you should scale the value e.g. $(v - 5)/3$ to keep it in range for best neural network learning.The card in hand, about to be placed, should be part of the state. So far, before any feature engineering, that would give you a vector of at least 26 * 6 = 156 dimensions, but you could take that further.If possible, use afterstates instead of enumerated actions. That may not be possible if you are using a library that expects enumerated actions. You probably won't find a version of Actor-Critic with afterstates because the Actor would need to express a probability distribution over the afterstates, which is awkward to do. You may be able to set the Actor to use enumerated actions and the Critic to score advantage based on afterstates though.To help the agent, do some feature engineering to add summaries of each possible line to the state (so a summary for each of the 12 lines). One important feature will be whether that line is complete or not - if it is complete, no more reward is possible from that line (perhaps a scaled ""how many spaces left"" counter would work here). Other features that could be useful are a ""bag of colours"" and ""bag of values"" i.e. one-hot-encoded list of current variation in that row - such a summary makes it easier for the NN to tell whether an x-of-a-kind or flush or straight are possible.In theory you could also pre-calculate the immediate reward from placing the card in each position, making that part of the state (another 25 features). That is so the agent is not trying to predict this value - it is hard for the NN, but easy for the game engine and not ""cheating"" at the game by e.g. peeking at the next card. That should allow the NN to use more of its capacity for planning ahead and statistically gambling on what might be available in future, which is perhaps the more interesting aspect of this game."
How is policy iteration capable of improving on a deterministic policy?,"
Given a policy $\pi$ and the improved version upon it using policy iteration $\pi'$ we have, for $\forall s \in S$, $v_{\pi'}(s)\geq v_{\pi}(s)$.
I think the way we choose $\pi'$ makes it deterministic (unless there is a tie but let's not consider it) because we take $\pi'(s) = \arg\max_{a} \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_{t}=s, A_{t}=a]$.
Following policy $\pi'$ now (in order to evaluate it), if at time step $t$ we're in state $s$, then we'll take action $a$ given by $\pi'(a|s)$ and we'll arrive at a state $s'$ while getting a reward $r$. For simplicity let's suppose the environment isn't stochastic, the initial state is the same and that the MDP is finite. Since the policy is stochastic and the initial state is the same, we'll always take the same path and evaluate the same states.
When we start evaluating for $\pi'$, the initial values for the value function $v_{\pi'}$ are $v_{\pi}(s)$. Since we have the condition $v_{\pi'}(s)\geq v_{\pi}(s)$ (because $\pi'$ is an improvement over $\pi$), those values can only increase, and since we only evaluate particular states, then only the values for these particular states will increase.
Now if we want to take improvement step $\pi'' \geq \pi'$ then this new policy will be exactly the same as $\pi'$ since the only states that saw their values increased are the ones taken by $\pi'$.
I feel I'm misundertanding something but I can't really put my finger on it so I hope you can help me figure out what I'm missing. Thank you in advance.
EDIT: Formulated my incomprehension in a clearer manner.
","['reinforcement-learning', 'value-functions', 'policy-iteration', 'optimal-policy', 'deterministic-policy']","These statements are not true for policy iteration and dynamic programming:Since the policy is stochastic and the initial state is the same, we'll always take the same path and evaluate the same states.since we only evaluate particular states, then only the values for these particular states will increasePolicy iteration does not learn from experience, and does not visit states based on observing trajectories. It is a planning/off-policy algorithm that in the simplest form visits and updates all states equally (more sophisticated forms may prioritise certain states). It can do so because it does not use observations of reward and next state, but instead uses a model of the environment to calculate updates. This is also true for value iteration.Your analysis would be true for any model-free algorithm that did not explore, such as Monte Carlo Control or SARSA if you removed the exploration components (e.g. set $\epsilon = 0$ for $\epsilon$-greedy), because those algorithms rely completely on observations of experienced trajectories."
How is catastrophic cancellation dealt with in loss functions?,"
It just occurred to me that this seems like it should be a very common problem that must have some kind of solution... Yet I'm not sure what it is...
If there is no solution, does this mean once a model reaches a certain level of accuracy numerical training methods can no longer be counted on to make it 'better'? And if so, is this a practical concern?
","['neural-networks', 'objective-functions', 'numerical-algorithms']","Catastrophic cancellation occurs when a function to optimise includes the difference between two estimates to close numbers. As those estimates approach their true values, the ratio of the estimated difference between them and real difference between them may vary widely.Although this may be a concern in a particular model or problem, it is not really related to neural network loss functions or how they learn from example data. No standard loss functions rely on a stable relative difference between two estimates from the neural network. Most loss functions compare a current estimate with a target value (or ground truth), and the error value for that is absolute and can approach zero without causing any instability.There may be a few exceptions, and in addition there are other numerical issues that impact convergence stability for neural networks, where solutions are employed in neural network libraries. For example when dealing with output layers with exponentiation in them (e.g. sigmoid or softmax) combined with loss functions that have log terms in them (e.g. cross entropy), it is common to use a simplified gradient calculation that does not need to actually perform the exponentiation or log - this saves time and is numerically more stable. This is not the same issue as catastrophic cancellation, but clearly is considered important enough in neural networks that it is addressed.As far as I know, there is no need to address catastropic cancellation in general neural network optimisation, and there are no libraries with standard solutions.If you are estimating two close values, taking the difference, perhaps in some pipeline, and the resulting system is sensitive to proportional errors in that difference, then you may be concerned about catastrophic cancellation and need to do something about it. What that something is will not directly be related to neural network libraries though. Perhaps you will be able to reformulate your problem so that the difference between the estimates is not so important, or that measure could be estimated more directly."
How to model a multi-agent reinforcement learning problem where actions of different agents can take different durations?,"
I am confused on a conceptual scale how I would be able to model a multi-agent reinforcement learning problem when each agent performing an action would take different durations to complete the action. This means that a certain action is performed over multiple steps and the learning sample would have that action attached to it (with different observations and rewards, possibly).
An example of this situation would be where vehicles on a 2-lane road can perform lane changing actions, but each of these actions may take anywhere between 2 - 5 seconds (or learning steps) to complete.
So, what action would need to be passed at every step? I am using RLlib framework. Is it even possible to do this? Or do all these agents have to have the same action duration / step length for any RL algorithm to work?
I would greatly appreciate if anyone could point me in the right direction on bypassing this mental block, it is driving me crazy.
","['reinforcement-learning', 'reference-request', 'proximal-policy-optimization', 'semi-mdp', 'multi-agent-rl']",
can't find a viable import class for keras.utils.Sequence [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I am using Google Colab. tensorflow version = 2.8.0, and keras is the same.  I am trying to get a BalancedDataGenerator(Sequence) class created, but I can't get keras.utils.Sequence to load.
from tensorflow.python.keras.utils import Sequence
from tensorflow.python.keras.utils.np_utils import Sequence
from tensorflow.python.keras.utils.all_utils import Sequence

I've tried it taking out ""python"", or taking out ""tensorflow.python"", or searching as to where it is now currently located, but haven't found it.
The errors I get are:

ImportError: cannot import name 'Sequence' from
'tensorflow.python.keras.utils'
(/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/init.py)
AttributeError: module 'keras.utils' has no attribute 'Sequence'
ImportError: cannot import name 'Sequence' from 'keras.utils'
(/usr/local/lib/python3.7/dist-packages/keras/utils/init.py)
ImportError: cannot import name 'Sequence' from
'keras.utils.all_utils'
(/usr/local/lib/python3.7/dist-packages/keras/utils/all_utils.py)

NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

When I go /usr/local/lib/python3.7/dist-packages/keras/utils and read all_utils.py I see 'from keras.utils.data_utils import Sequence' as an option and it says that this all_utils.py module is 'used as a shortcut to access all the symbols.  Those symbols werd exposed under init, and was causing some hourglass import issue.'
If I read data_utils.py I do see the Sequence module inside there.
if I type from 'keras.utils.data_utils import Sequence' in a cell and run it it looks like it was accepted.  However, if I put that in the balanceddatagenerator.py and import the class I created that uses Sequence, and then import the class I get this error.  I don't know how to overcome this.
How get this installed and imported successfully?
","['tensorflow', 'keras']","imblearn.keras.balanced_batch_generator was pointing to an old version of keras.utils.Sequence and causing this error.I just found that imblearn also has a tensorflow version of balanced_batch_generator, so I switched to tensorflow.keras.utils.Sequence
and the imblearn.tensorflow.balanced_batch_generator, and it seems to import without any errors.Now let's see if I can finally make progress."
How to represent multi-label colours in one-hot encoding?,"
Say I want to predict the price of a gemstone based on its colour.
I have two options:

averaging over its colour on an RGB scale, or
using its textual description.

If I was to choose the latter, how would I go about feeding this to my neural network?
Priori knowledge: Usually a gemstone is defined by its colour and the ""degree"" of this colour: Example fancy bright green.
Here I could obviously let every combination of colour and degree be its own value in the one-hot vector. To implement this I could use some sort of hash function, if this makes sense, how specifically would I make a hash function that could do this?
If this solution doesn't make sense, what would you suggest?
Tough example of data:
""Natural Fancy Deep Yellowish Brown""
","['neural-networks', 'machine-learning', 'natural-language-processing', 'data-preprocessing', 'bag-of-words']",
Which algorithm can find the best combination of players to maximize the chance of getting a high score?,"
I am looking for the right terminology for this problem, so I know what to learn about.
Imagine a population of 100 people in a town. The town has a sport team with 10 positions that play in competitions, and the town's people join the team when they can randomly. In this sport everybody plays the same position.
We have a data sheet with all the matches played, including score achieved and the people that formed that particular team. Now we have a big final, and we want to pick the best 10 people for the match. NOT simply the team from the data sheet that scored the higher score, but to pick and choose those members that according the data maximises the chances of getting a high score.
I am sorry if I do not explain myself well. For example, taking tennis, imagine that 2vs2 matches have been on going for a long time, and although Nadal and Federer have never played together, data would show that putting them in the same team would likely ensure a high score.
What type of algorithm or technique do I need to find out?
","['machine-learning', 'terminology', 'models', 'algorithm-request']",
How can I vectorize fictional single word (not sentence!) for classification?,"
I am working on fictional single words (names) generator that have to sound like words from a given sample. I have the generator up and running that gives reasonable words 70% of time. I thought of improving this value, ideally to ~99% (so no manual step is necessary to discard clearly missed words).
I've already attempted fuzzy matching to any word in the initial sample (and rejecting words below given thresholds), and this winded up reasonable answers to 95%-99%, but in my experiments this approach is rejecting too many well-sounding names (even 60-70 rejections per 1 generated name - this removes a lot of entropy from the initial set).
Now, I am thinking of using classification instead to tackle the problem from another angle: using classification to discard the generated name.
The problem I have is I have to vectorize a single word based on its sound for classification. I've already excessively searched the Internet and the only solution I see are for vectorizing texts, not single words.
So, the question here: what kind of algorithm for vectorization of single fictional word based on its sounding have I missed?
","['natural-language-processing', 'reference-request', 'classification', 'feature-engineering', 'vectors']",After more excessive search I have found the description of Phonetic Matching algorithm by Alexander Beider and Stephen Morse which basically does what I need to do on filtering
"Does the term ""data augmentation"" imply increasing the training dataset?","
I have a manuscript that has been reviewed and one of the reviewers commented on my use of the term "" data augmentation"", saying that it might not be the appropriate term in my case (explained below).
I collected a large dataset of short audio files which are used to train a Convolutional Neural Network. Before being used as model input, each audio file is processed through a pipeline that mixes it with other audio files, changes some of the sound properties (SNR ratio, distorting the audio ...) and finally converts it into a mel-spectrogram. I only modify the existing file and I do not increase my training dataset but I refer to this processing as ""data augmentation"".
I did not find any definitive definition of what is data augmentation. For instance, Salamon and Bello, 2016 define data augmentation as

the application of one or more deformations to a collection of
annotated training samples which result in new, additional training
data

However, it appears to me that the increase in the training dataset is only a byproduct of the data augmentation.
In any case, I would really appreciate it if you could confirm or not my use of ""data augmentation"" and I would be grateful if you can provide a reference for this.
","['machine-learning', 'reference-request', 'terminology', 'data-preprocessing', 'data-augmentation']","I'm not familiar with any ""authoritative"" single definition somewhere, or not sure who used the term first, but I would personally indeed agree with the reviewer you mention. In fact I've made similar comments myself as a reviewer of a paper once (but it was in an entirely different field, nothing related to audio at all, so I probably wasn't the reviewer in question). Personally, when I think of data augmentation, the first example that jumps to mind is adding rotated/reflected versions of images to the training data.A quick google search suggests that, for example, Shorten and Khoshgoftaar (2019) similarly associate data augmentation with increases in training data in their survey paper on data augmentation for images.What you describe sounds more like ""data preprocessing"" rather than data augmentation to me, but I'm not 100% sure exactly what the purpose of your modifications is (which I suppose might change my opinion). Data preprocessing can be almost any procedure that is applied to your original data, like... feature selection, converting images to grayscale if you expect colour to be irrelevant to your task, downscaling images if they're too big and you don't mind decreasing the resolution, etc."
How can I reduce the loss? Why do I have the high loss and why do I have the gradient?,"
I want to classify some images (there are about 200.000 images) with a CNN. But I get a very high loss, see figures:
Loss over the hole training run

Loss for each epoch

It's confused me, that there is a high gradient by the 15 batch of each epoch
I try some option in the history like editing the learning rate and add weights by initialization the network, but I dont unterstand what's wrong. In the first step I train my network with 2000 images, but it's possible to train it with more images.
The Question
How can I reduce the loss and why do I have the high loss? Rather, why do I have the high negativ gradient in the middle of each epoch (see in the second plot)?
Thank you for your help in foward :)
Some words to the dataset:
The dataset is a set of images and they looks very similar. There are two classes: the ""good"" images and the ""error"" images. For us - the humans - we will say, that's both images classes looks right, but a normal camera has not enough intelligence. That's the motivation for the projekt.
Now, you find here the code of my neural network:
The training method
model = net.Netz()
optimizer = optim.SGD(model.parameters(), lr= 0.0001, momentum = 0.8)

def trainM(epoch):
    model.train()
    for batch_id, (data, target) in enumerate(net.train_data):
        #data = data.cuda()
        #target = target.cuda()
        
        target = torch.LongTensor(target[64*batch_id:64*(batch_id+1)])
        #data   = torch.Tensor(data[64*batch_id:64*(batch_id+1)])
        data = Variable(data)
        target = Variable(target)
        optimizer.zero_grad()

        out = model(data)
        criterion = F.nll_loss

        loss = criterion(out,target)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
        optimizer.step()
        print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(epoch,batch_id*len(data), len(net.train_data)*64, 100*batch_id/len(net.train_data), loss.item()))

for item in range(1,10):
    trainM(item)

The CNN
class Netz(nn.Module):
    def __init__(self):
        super(Netz, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        torch.nn.init.xavier_uniform(self.conv1.weight)
        self.conv2 = nn.Conv2d(10,20, kernel_size = 5)
        torch.nn.init.xavier_uniform(self.conv2.weight)
        self.conv_dropout = nn.Dropout2d()
        self.fc1 = nn.Linear(1050,60)
        self.fc2 = nn.Linear(60,2)
        self.fce = nn.Linear(20,1)
    
    def forward(self,x):
        x = self.conv1(x)
        x = F.max_pool2d(x, 2)
        x = F.relu(x)
        x = self.conv2(x)
        x = self.conv_dropout(x)
        x = F.max_pool2d(x,2)
        x = F.relu(x)
        #x = x.view(-1,320)
        x = x.reshape(x.shape[0], x.shape[1], -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        x = self.fce(x.permute(0,2,1)).squeeze(-1)
        return F.log_softmax(x, -1)

The data prep method
def dataPrep(list_of_data, data_path, category, quantity):
    global train_data
    global target_list
    train_data_list = []
    mean = [0.0028]
    std = [1.0001]
    
    transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))
        ])
    
    len_data = len(train_data)
    for item in list_of_data:
        f = random.choice(list_of_data)
        list_of_data.remove(f)
        try:
            img = Image.open(data_path +f)
        except:
            continue
        img_crop = img.crop((310,60,425,240))
        img_tensor = transform(img_crop)
        train_data_list.append(img_tensor)

        if category == True:
            target = 1
        else:
            target = 0
        target_list.append(target)
        
        if len(train_data_list) >=64:
            train_data.append((torch.stack(train_data_list), target_list))
            train_data_list = []
            
        if (len_data*64 + quantity) <= len(train_data)*64:
            break   
    return list_of_data

","['convolutional-neural-networks', 'python', 'image-recognition', 'pytorch', 'image-processing']",
Is there a way to further fine tune image alignment homography after using Spatial Transformer Networks?,"
I'm using a Spatial Transformer Networks (STN) to align an image by optimization using L1 loss between prediction and target, they works great for large mismatch, but not so much for small misalignment. For example, in this image below you could see it warped them to close as the target, but some lines still out of its place (the green pointing arrow).

I'm trying further fine tune them, is there a technique/networks that I could use that I might not be aware of that could be used in-place or extended from STN?
","['convolutional-neural-networks', 'sampling']",
What are the differences between BLEU and METEOR?,"
I am trying to understand the concept of evaluating the machine translation evaluation scores.
I understand how what BLEU score is trying to achieve. It looks into different n-grams like BLEU-1,BLEU-2, BLEU-3, BLEU-4 and try to match with the human written translation.
However, I can't really understand what METEOR score is for evaluating MT quality. I am trying understand the rationale intuitively. I am already looking into different blog post but can't really figure out.
How are these two evaluation metrics different and how are they relevant?
","['machine-learning', 'deep-learning', 'comparison', 'machine-translation', 'bleu']",
Structured policies in dynamic programming: solving a toy example,"
I am trying to solve a dynamic programming toy example. Here is the prompt: imagine you arrive in a new city for $N$ days and every night need to pick a restaurant to get dinner at. The qualities of the restaurants are iid according to distribution $F$ (assume [0,1]). The goal is to maximize the sum of the qualities of the restaurants that you get dinner at over the $N$ days. Every day you need to choose whether you go to a new restaurant and obtain a utility drawn at random from distribution $F$, or you go to the best restaurant you have attended so far, and obtain a known utility.
Here is my formulation:

Let $m_t$, the state at time $t$, denote the quality of the best restaurant visited so far.
Let $a_t \in \{Q, E\}$ be the action at time $t$; $E$ for EXPLORE, $Q$ for quit.
Given the action set, the rewards are ($W_t$ denotes the random variable of the quality of the restaurant drawn at random at time $t$):
$$r_t(m_t | E) = W_t \sim F $$
$$r_t(m_t | Q) = m_t  $$
The transition probabilities at time $t$ are:
$$ p_t\big (  m_t |  m_t, Q \big ) =1 $$
$$ p_t\big (  m_t |  m_t, E \big ) =F(m_t), \text{ with } W_{t} \in [0, m_t]$$
$$ p_t\big (  W_t |  m_t, E \big ) =1-F(m_t), \text{ with } W_{t} \in ( m_t, 1]  $$

First I write down the Bellman optimality equations:
For $t=N$, I get:
$$ u^*_N (m_N) = \max_{a_t \in \{Q, E\}} \big\{ m_N, \ \  E(W_N) \big\}  $$
For $t<N$, I get:
$$
 u^*_t (m_t) = 
 \max_{a_t \in \{Q, E\}}  \bigg\{ m_t+ u^*_{t+1} (m_t) , \ \ \ E(W_t)+  u^*_{t+1} (m_t) F(m_t) + \int_{m_t}^1 u^*_{t+1} (x) dF(x) \bigg\}  
$$
What I am trying to show is that, if it is optimal to quit exploration in period $\tau$, then it is optimal to not explore in every period $t=\tau+1, ..., N$. In agreement with the equations above, I assume:
$$m_\tau- E(W_\tau) \geq   \int_{m_\tau}^1 (u^*_{\tau+1} (x)-u^*_{\tau+1} (m_\tau))  dF(x) $$
and I want to show:
$$m_\tau - E(W_{\tau+1}) = m_\tau - E(W_{\tau})\geq   \int_{m_\tau}^1 (u^*_{\tau+2} (x)-u^*_{\tau+2} (m_t))  dF(x) $$
I have been reading the structured policies chapters on Puterman and Bertsekas (Vol II) and did not find anything that could get me rolling. Any help or pointer to literature is more than welcome!
After I show this I can argue that there will exist a sequence of thresholds $c_t$ so that is is optimum to stop exploration at time $t$ it $m_t \geq c_t$.
EDIT: Distribution $F$ is known to the tourist.
","['optimization', 'dynamic-programming']",
Why my classification results are correlated with the proportionality of my data?,"
I'm facing a problem. I'm working on mixed data model with NN (MLP & Word Embedding). My results are not pretty good. And I observed that the proportionality of my data are corelated with my classification results. I explain:

As you can see, I have more LIVB than others data. The problem is that the predictions of my model are only LIVB 
And I don't understand why ? Is it a high variance ? Is it a high bias ? What methods for classification problem should I use to detect the error ? Should I have more features ? Is my model is wrong ? Can someone has this problem before ?
Thanks for your help !
","['data-preprocessing', 'models', 'feature-selection', 'bias', 'variance']",
How do I perform automatic evaluation of my NLP model?,"
I have a model which converts sets of keywords to sentence, but I've to quantify it's quality. In computer vision, we would calculate the model's accuracy, I'm kind of lost and how do I go about using baselines as well, am thinking of using BLEU and ROUGE on pairs of data not used during training, is this the correct way to go? Can someone please advise?
And should I average my BLEU/ROUGE scores I got for each iteration? Is there a better metric to evaluate NLP models?
My dataset is like
keyword, keyword, keyword -> sentence

","['machine-learning', 'natural-language-processing', 'metric']",
Does the policy search work if there is no state to state dependency through actions?,"
There is a game in which the state comes one after the other without depending on the agent's action. The agent gets a reward for its actions at the end of the game. The goal of the agent is to reach a target reward when the game ends. The agent fits a policy network (returns distribution of actions to be taken at that particular state) and trains it using evolutionary algorithms with a fitness function as
$$
(\text{reward in the game} - \text{target reward})^2
$$
Do the optimal weights be learned that make the agent reach the target reward after some generations of training?
Also, can the policy search work, even if the game can not be modeled as an MDP (as there is no state to state dependency by actions in this case. The game is a kind of contextual bandit)?
","['reinforcement-learning', 'policy-gradients', 'markov-decision-process', 'evolutionary-algorithms', 'contextual-bandits']",
When training a seq2seq model is it better to train using the models outputs or expected outputs?,"
When training any seq2seq model you have a target and a source. The source may be a sentence such as:

I_walked_the_dog

And the target being

_walked_the_dogg

Where as you can see the expected output for the initial I is a space _. My question is, at training time, whether to use the models previous outputs for predicting the next output, or to run the training simultaneously using the expected outputs. To illustrate this more clearly, see below:

The incentive for training using the expected outputs is that all time steps can be trained simultaneously, so it speeds up training by a factor of the sequence lengths. However, it means that training is not representative of what the network will realistically be doing at testing time, as at testing time the network will not have perfect previous outputs, but rather only it's own.
","['natural-language-processing', 'recurrent-neural-networks', 'transformer', 'attention', 'seq2seq']",
What is the difference between a policy and rewards?,"
I don't understand the difference between a policy and rewards. Sure, a policy tells us what to do, but isn't the output of a neural network trained on rewards basically a policy (i.e. choose the maximum reward)? What is different about the policy? An extra softmax applied?
","['reinforcement-learning', 'comparison', 'rewards', 'policies']","A (stochastic) policy is a set of conditional probability distributions, $$\pi(a \mid S=s), \forall s \in \mathcal{S}.$$ If the policy is deterministic, then it is a function $$\pi: \mathcal{S} \rightarrow \mathcal{A},$$ so $\pi(s) = a$ is the action that policy $a$ returns in the state $s$ - it always produces this same action for a given state, unless it's a non-stationary policy. A policy is also called a strategy (in game theory). To be usable, a stochastic policy must be turned into a decision rule, i.e. you need to sample from it. A stochastic policy generalises a deterministic one.The rewards are the outputs of the reward function. A reward function can be deterministic (which is often the case) or stochastic. It can be defined as $$R : \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{R},$$ where $\mathcal{R} \in \mathbb{R}$ is the reward space. If it's stochastic, then\begin{align}
R(s, a) 
&= \mathbb{E}\left[ R_t \mid S_t = s, A_t = a\right] \\
&=\sum_r r p(r \mid S_t = a, A_t=a),
\end{align}
where $R_t$ is the random variable that represents the reward at time step $t$ and assuming a finite MDP. Stochastic reward functions generalise deterministic ones.So, policies are probability distributions or functions, while rewards are numbers. So, there's a difference between their definitions, even though they are related.How are they related? In different ways. The most important one is that an optimal policy for a given MDP is the one that, if followed, maximises the expected return, which is a function of the reward (typically, a discounted  sum of rewards). The definition of an optimal policy makes more sense if you also know the definition of a value function - I recommend you take Sutton & Barto's book and read the relevant sections."
Can vanilla multi armed bandit problems be solved by RL algorithms like A2C and PPO?,"
Let's say we have N bandit machines with some distributions (assume some are gaussian, some are uniform, some are chi squared). We want to maximize rewards in X amount of time. I am aware that algorithms like Epsilon greedy, UCB can handle this. But is applying an RL algorithm like A2C the right choice to a plain vanilla Multi Armed Bandit problem (N independent machines that we have to maximize rewards from in X amount of time)? Is the notion even correct? Given that there is only always one state.
","['reinforcement-learning', 'multi-armed-bandits', 'advantage-actor-critic']",
Does $S_{t+1}$ denote the future information in Q-learning?,"
In Q-learning, $Q(S_t,a)$ is updated by the Bellman equation. $Q(S_t,a) = r + \max_{a'}(Q(S_{t+1},a'))$ where $S_{t+1}$ is the future state.
Let's say $S$ denotes the stock price, does it mean we are using future information $S_{t+1}$ in the Q-learning process?
After the training process and bringing into implementation, how is it possible to apply the strategy without knowing the future price (the future state)?
","['reinforcement-learning', 'q-learning', 'markov-decision-process', 'testing', 'algorithmic-trading']",
What are the steps to derive the original GAN loss function from the generalized version?,"
I am trying to understand how the loss function from the original GAN paper
$$\min_{G} \max_{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]$$
relates to generalization in the Nagarajan & Kolter (2017) 
$$\min_{G} \max_{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[f(D(x))]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[f(-D(G(z)))]$$
where supposedly the original loss function could be recovered via
$$f(x) = -\log(1 + \exp(-x))$$
But when I try to substitute $f$ into the generalized version of the loss function I am not getting the original one. What would be the correct steps to recover the original loss function?
","['machine-learning', 'papers', 'generative-adversarial-networks', 'generative-model', 'notation']","There are some definitions that may cause confusion here.In the original GANs (the first formula), the output from the discriminator connects to a sigmoid activation, the second formula is the real value from the last layer of the D model which you can find in section 3.1note that this convention slightly differs from the standard formulation
in that in this case the discriminator outputs the real-valued “logits”Rewrite the first formula, consider the output from $D(x)$ is the real value, it will become:$$ \min_{G} \max_{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log {\frac{1}{1+exp(-D(x))}})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-\frac{1}{1+exp(-D(G(z)))})] $$Now, it's able to proof the first and second formula is the same."
"Knowing the futility of discounting in continuing problems, how can we say discounting has no role in control problems with function approximation?","
Sutton-Barto (Section 10.4, page 254):

Based on the futility of discounting in continuing problems, how can we conclude that discounting has no role to play in control problems with function approximation?
","['reinforcement-learning', 'function-approximation', 'sutton-barto', 'discount-factor', 'continuous-tasks']",
What is commonly done for standardization/normalization of the targets in Deep Q-Learning?,"
I have been searching a lot about standardization/normalization of rewards and targets for the DQN algorithm. For the rewards, I now use the gym wrapper, which only scales but not shifts the rewards by an estimate of their standard deviation, which is updated over time.
However, I can't really find something about common practice for standardization/normalization of the constructed targets. Is it done on top of reward normalization? Do we just let the agent train for a while and then take an estimate of the standard deviation of the targets and scale with the estimate then, assuming we can't use any information of the environment's dynamics?
If this question is too broad to be answered in general, I'll quickly list the specifics of my environment.

Rewards are stochastic and approximately in the range [-0.2, 0) before scaling with outliers excluded
The agent receives a reward after each timestep
Given a state and action, the next state is deterministic
Very short time horizon -> 5 timesteps per episode
Action space is discrete and only consists of 11 actions
With reward scaling, the q-targets are approximately in the range [-3, 0] (I saved a list of targets and plotted a histogram a couple of times during training)

","['reinforcement-learning', 'deep-rl', 'dqn', 'rewards', 'reward-functions']",
"In reinforcement learning, how to craft observation space when environment is made of multiple blocks?","
In reinforcement learning problems like cartpole, usually the environment is a single system that takes an input and gives an output. For example, in cartpole, given positions, velocities as input we can tell if pole is going to fall. Hence, we can craft observation space from positions and velocities.
But if environment is made of sequential blocks of systems {A, B, C}, where input is given to A that passes to B then to C then to output, then how do we craft observations?
Options:

We use a separate block D (linear or non-linear) to create numerical observations for input which is then fed to A. Problem with this: Observation crafted out of D is not correlated to output at C. For example, in cartpole, I would say that I would pass the velocities and positions thru a block D to get amount of fuel left in cart. And I would feed the amount of fuel left as observation to actual system which determines if the pole falls.
We use block A to create numerical observations for input. Better than option 1 but observations crafted out of A are not reflective of the entire process (A -> B -> C).

So, how do we craft observations for such an RL problem?
Edit: An example scenario. I have a system A that takes a song and gives N similar songs as output. I have a system B that takes a song and determines if the song is liked by more than 10 million people. Then I have a system C that takes a song liked by more than 10 million people and says if the song is of genre 'pop' or 'edm'.
I have 10 'edm' songs (liked by more than 10 million people) to start with, and I want to apply Reinforcement Learning to expand the 10 'edm' 'liked by more than 10 million people' songs to more such songs. By optimally, I mean we could always brute force for each song but the system A is non-deterministic. It can give N similar songs (N ranges from 0 to 100). In X amount of time, I want to maximize creations of such songs. Say, song X and song Y are the initial inputs. After we pass them thru A->B->C we could get 100 such similar ('edm' & 'liked by more than 10 million people') songs for X but only 2 for Y. Brute-force would waste time a lot in places that would not yield many such songs.
Now, an RL agent can decide which song to pick to get maximum number of similar such songs in X amount of time. But the environment is A -> B (is it liked by 10 million people, output is 1 or 0) -> C (is it 'edm' or 'pop', output is 1 or 0). Songs are represented as signals (array of continuous values between -inf to +inf) by passing them thru a system D. The system D takes the song name, downloads it and gets the signals (array representation of the songs). So, these numerical representations can be used instead of song names.
How do I craft observations for this combined environment?
As far as I know observation space should be such that given 2 observations we can distinguish which one gives better results, like in cartpole, 2 sets of observations (positions1, velocities1) and (positions2, velocities2) the system can identify which observation is relatively better or worse. But in our system (A -> B -> C) how do we go about crafting an observation that correlated with output?
",['reinforcement-learning'],
how to define or calculate the similarity betweeen two curves as the loss funtion to optimize in the generative model?,"
I want to train a neural network as the curve productor that can generate the specific type of curves (e.g. exponential decay curves). I take the encoder-decoder structure, the curves in a dataset is used as input, the encoder encodes them as the hidden variables, and then decoder decodes those hidden variables to regenerate the same curves. The loss function is the difference between the input curve and the generated. But the tough problem is how to measure similarity or consistent degree between the two curves as depicted in the pasted figure. In fact, the curves are permited to shift left or right, which make it the difficult thing to deal with.
Note that, Don't need to consider the points that is lower than 0.05.
","['deep-learning', 'objective-functions']",
Object Classification: How to decide which detected region is a RoI for classification?,"
I am working on a project where I am working on the Flickr-47 dataset to do logo detection and classification. My approach is to first finetune a YOLO v5 model with high recall to detect as many ""logos"" as it possibly can and then to classify the detected regions using some sort of feature extraction.
Here is an example of the actual image and what the model detects as logos.
Ground Truth Annotations:

YOLO detections:

My question is because there are instances as can be seen above where the number of regions  detected by the model can be less or more than the actual regions to be detected in the ground truth image, how do I decide which ones to keep for classification?
P.S. I know I can apply non-max suppression to remove overlaps but the above can still persist if the YOLO detector finds something worth a logo, but isn't. I can also modify the IoU threshold and confidence threshold during detection inference, but I want to be able to extract as much as the YOLO model detects, and keep the filtering at a later stage.
Any ideas?
","['machine-learning', 'deep-learning', 'computer-vision', 'object-detection', 'yolo']",
Does $R_{s}=E[R_{t}|S_{t}=s]$ indicate the reward we might expect on getting on average moving from any other state to $s$?,"
I'm trying to understand correctly what each ""variable"" in RL is and I'm not sure about $R_{s}$ the reward function. I used to think that it's the reward we may expect on average after taking an action $A_{t}$ at state $S_{t}=s$ and at time step $s$ but thinking about it more I think I was wrong.
If we consider that the agent at time step $t$ receives from the environment both an observation $O_{t}$ and a reward $R_{t}$ which will make up the agent's state $S_{t}=s$ depending on what function of the history we take, then it means that we got to state $S_{t}=s$ from a state $S_{t-1}$ taking action an action $A_{t-1}$ right?
I'd like to ask you to confirm or not my claim.
","['reinforcement-learning', 'markov-decision-process', 'notation', 'reward-functions', 'state-spaces']","$$R_{s}=\mathbb{E}[R_{t}|S_{t}=s]$$is the expected reward at time step $t$ given that the state at time $t$ is $s$, whereIt does not matter how you entered $S_t = s$. The only thing that matters is that the current state is $s$. So, the answer to your question in the title is - no. $R_s$ is defined as an expectation (average) of the reward for being in a state, and, in this case, it doesn't take into account next states.Suppose that the reward space $\mathcal{R} \subset \mathbb{R}$ is a discrete set, then we can write $R_s$ as follows$$R_{s}=\sum_{r \in \mathcal{R}} r p(r \mid S_{t}=s),$$where $p(r \mid S_{t}=s)$ is a conditional probability distribution that describes how the reward is distributed in a given state. If you always get the same reward, i.e. $p(r \mid S_{t}=s) = 1$ for a particular $r$ and $0$ for all others, then you have a deterministic reward function, i.e. $R_s$ is equal to the $r$ for which $p(r \mid S_{t}=s) = 1$. In most RL examples, reward functions are deterministic.Note that it is probably more common to define the reward function as a function of the state $S_t = s$ and action taken in $s$, $A_t =a$, and write it as $R(s, a)$ or $r(s, a)$, i.e.\begin{align}
R(s, a)
&=\mathbb{E}[R_{t}|S_{t}=s, A_t = a]\\
&=\sum_{r \in \mathcal{R}} r p(r \mid S_{t}=s, A_t = a)
\end{align}Note that we're now using $p(r \mid S_{t}=s, A_t = a)$ and not $p(r \mid S_{t}=s)$.You could also define $R(s, a, s')$. See this or Sutton & Barto's book for more details."
1D Sequence Classification with self-supervised learning,"
I am working on a multi-class classification task on long one-dimensional sequences. The sequence length may vary in the range $[512, 30720]$, and there is one feature associated each time-step in the range. This means that the input to the model is of the shape $(N, 1, L)$ where $N$ and $L$ are the variables for the batch size and sequence length respectively. The singleton feature dimension contains values in the range $[0, 40]$. A small slice of a sequence might look like this: $[0,3,2,1,2,7,3]$.
The standard way of using deep learning (i.e. supervised learning) to solve this task is by choosing some neural network architecture (our model) with an appropriate inductive bias towards the data and encode the features, propagate it threw the neural network, optional pooling, decode the features and finally compute the loss w.r.t the ground truth in that order. In PyTorch this process would look something like this (the model is a basic ResNet block):
import torch

input_shape = (5, 1, 1024) # batch size, features, timesteps
X = torch.randint(0, 14, input_shape).type(torch.float) # sample input
y_true = torch.tensor([0, 0, 1, 2, 1]) # sample ground-truth (3 classes)

d_model = 64 # 64 is our hidden dimension
encoder = torch.nn.Conv1d(in_channels=1, out_channels=d_model, kernel_size=1)

class Residual(torch.nn.Module):
  def __init__(self, m):
    super().__init__()
    self.m = m

  def forward(self, x):
    return self.m(x) + x

model = torch.nn.Sequential(
  Residual(
    torch.nn.Sequential(
      torch.nn.Conv1d(d_model, d_model, 3, padding='same'),
      torch.nn.BatchNorm1d(d_model),
      torch.nn.ReLU(),
      torch.nn.Conv1d(d_model, d_model, 3, padding='same'),
      torch.nn.BatchNorm1d(d_model),
    ),
  ),
  torch.nn.ReLU(),
)

pooler = lambda x: x.mean(2) # average pooling over the length dimension
decoder = torch.nn.Linear(d_model, 3) # 3 output logits for each class

criterion = torch.nn.CrossEntropyLoss()

# training pipeline
X = encoder(X) # (N, 1, L) -> (N, H, L)
X = model(X) # (N, H, L) -> (N, H, L)
X = pooler(X) # (N, H, L) -> (N, H)
y_pred = decoder(X) # (N, H) -> (N, 3)
loss = criterion(y_pred, y_true)
loss.backward()

Assuming that the neural network architecture model is appropriate, I have considered using self-supervised learning (SSL) to boost the final classification accuracy on the withheld test set. I have seen the method gain attention in language modelling tasks with Transformers (i.e. BERT), but how does one apply SSL in the general case outside the language task domain? Is this approach successful without using Transformers?
As I understand, SSL is used on unlabeled data to learn the underlying structure of the data and is followed by a fine-tuning stage - i.e. supervised learning with the pretrained weights. Two SSL tasks that I can think of are 1) masking out certain timesteps and letting the model predict the values of the missing timesteps, and 2) training the model to predict the value of the next timestep given the previous timesteps. I'll use the following image to illustrate:

Intuitively, I think this should work but I am not sure how to implement it in a practical setting with PyTorch. For instance, when predicting the next timestep (red bar) can I treat it as a classification task with (40) classes? Would it work for varying sequence lengths? How big proportion of the training data should be used for pretraining?
","['deep-learning', 'pytorch', 'sequence-modeling', 'self-supervised-learning', 'multiclass-classification']",
Normalizing float prices with movements up to a factor of 100,"
I have a bunch of arbitrary float numbers (asset prices), that I have to feed into a neural network.

In the data set: values are between 1E-10 and 1E6
In a single sample: values may differ by a factor 100
Even slight changes (price movements) are important

I tried a naive ""min/max"" normalization to the range [0..1], but the performance of the model degrades quickly if the values within a single sample are changing more than by a factor ~5. (The performance is determined by using predictions in a trading-simulation)
My own ideas:

Provide diffs from one float to the next one (change in price from t to t+1).
Split each float number into [0..1] mantissa and exponent

Exponent as one-hot-vector mapping values from -10 to 6
Exponent min/max normalized to [0..1]



How should I correctly normalize such a data to feed into AI?
Requested details of the network:

What are you trying to predict?

Next candle
Expected gain of a long position / raise in price in the next ~5 candles


How are you assessing the performance of the model?

I'm using a trading simulation with predictions on fresh data


Which model?

Multiple parallel 3 layers-LSTMs
Each LSTM process separate consecutive chunk of candles


A regular feedforward neural network?

Yes, multiple inputs goes though LSTMs
Then concatenate to single vector
Plus a bunch of dense layers


With how many layers and which activation functions?

activation functions: relu, sigmoid (for output)
I played with different configurations, e.g.

~ 15 layers total
500.000 trainable params




Which loss function are you using?

mean_squared_error



","['data-preprocessing', 'batch-normalization']","Mathematically speaking when dealing with fat tailed distributions or values that varies in a range of several order of magnitude the most common and simplest solution is to move to logarithmic space. Of course that requires positive values before scaling the data, and the log of a number can be negative as well, so you might want to normalize your data before and after. Something like:regarding your ideas:"
Determining to terminate at a reward or not,"
I am practicing the Bellman equation on Grid world examples and in this scenario, there are numbered grid squares where the agent can choose to terminate and collect the reward equal to the amount inside the numbered square or they can choose to not do this and just move to the state grid square.
Since this is a determinisitc grid, I have utilised the following Bellman equation:
$$V(s) = max_a(R(s,a)+\gamma V(s'))$$
Where $\gamma=0.5$ and any movement reward is $0$, since this will allow the agent to have a balance of thinking long-term and short-term.
I am trying to understand how you would determine whether it is better for the agent to terminate at the state with the number $3$ or to continue to the state with a number $4$ to collect the more reward? I have determined and marked (X) the terminal states, where with my current calculations, I feel the agent should exit.

","['reinforcement-learning', 'deep-rl', 'bellman-equations', 'deterministic-policy', 'stopping-conditions']","I am trying to understand how you would determine whether it is better for the agent to terminate at the state with the number 3 or to continue to the state with a number 4 to collect the more reward?Which is better is determined by looking at the expected return from either choice, with higher expected returns being better.The return from travelling one time step to the ""4 on exit"" state, is 2 as you have shown, due to discounting. That is assuming the 4 is gained by taking a separate ""exit"" action once in that position. That is, there is no combined ""move and exit"" action that only takes one time step - whether or not such actions exist in this environment makes a large change to your example and what will be optimal, so it is really important to be clear about that.The return from exiting immediately in the ""3 on exit"" state is 3.3 is larger than 2, so if the agent finds itself in the ""3 on exit"" state, then it should exit immediately to get the best expected return.Possibly what might be making this harder to understand is the role that discounting takes. Sometimes it appears to be used to ""fix"" infinite rewards from continuing environments. However, discounting is part of the definition of return, and it changes what counts as optimal. With a low discount factor, such as $0.5$, then it can be optimal to take a lower reward sooner as opposed to a larger reward later. The value of the discount factor allows to make that comparison exactly."
Backprop to calculate mean and standard deviation in batch normalization?,"
On page 310 of the Deep Learning book by Ian Goodfellow (Page 310 can be viewed here for better context: https://www.deeplearningbook.org/contents/optimization.html ), it is mentioned that one crucial point about batch norm is that:

... Crucially, we back-propagate through
these operations for computing the mean and the standard deviation, and for
applying them to normalize
H
. This means that the gradient will never propose
an operation that acts simply to increase the standard deviation or mean of [the layer] h_i
; the normalization operations remove the eﬀect of such an action and zero
out its component in the gradient

The equations:

I do not understand why is it crucial that we backprop through these calculations and how that helps in setting the mathematical intuition to batch norm, as mentioned in the book.
Can someone help me connect the dots here?
","['machine-learning', 'deep-learning', 'batch-normalization']",
Training a u-net for multi-landmark heatmap regression producing the same heatmap for each channel,"
I’m training a U-Net (model below) to predict 4 heatmaps (gaussian centered around a keypoint, one in each channel). Each channel is for some reason outputting the same result, an example is given of a test image where the blue is ground truth for that channel and red is the output of the u-net.
I have tried using L1, MSE and adaptive wing loss (Wang 2019), and none are able to regress the heatmaps.
I'm not sure what I'm doing wrong would appreciate any advice. Thanks

class CNN(nn.Module):
    def __init__(self):
        super(CNN,self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(config.numinputs, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(64))
    
        self.layer2 = nn.Sequential( 
            nn.Conv2d(64, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(64))
        
        self.layer3 = nn.Sequential(
            nn.MaxPool2d(2, stride=2, padding=0))
            
        self.layer4 = nn.Sequential(
            nn.Conv2d(64,128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(128))

        self.layer5 = nn.Sequential(
            nn.Conv2d(128, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(128))
            
        self.layer6 = nn.Sequential(
            nn.MaxPool2d(2, stride=2, padding=0))
            
        self.layer7 = nn.Sequential(
            nn.Conv2d(128, 256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(256))
            
        self.layer8 = nn.Sequential(
            nn.Conv2d(256, 256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(256))
            
        self.layer9 = nn.Sequential(
            nn.Conv2d(256, 256,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(256))
            
        self.layer10 = nn.UpsamplingBilinear2d(scale_factor=2)
            
        self.layer11 = nn.Sequential(
            nn.Conv2d(256, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(128))
            
        self.layer12 = nn.Sequential(
            nn.Conv2d(128, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(128))
            
        self.layer13 = nn.Sequential(
            nn.Conv2d(128, 128,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(128))
            
        self.layer14 = nn.UpsamplingBilinear2d(scale_factor=2)
            
        self.layer15 = nn.Sequential(
            nn.Conv2d(128, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(64))
            
        self.layer16 = nn.Sequential(
            nn.Conv2d(64, 64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(64))
            
        self.layer17 = nn.Sequential(
            nn.Conv2d(64, 4,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(1))
        
        self.layer18 = nn.Softmax(dim=1)
            
            
    def forward(self,x):
        x = self.layer1(x)  
        x = self.layer2(x)  
        x = self.layer3(x)  
        x = self.layer4(x)
        x = self.layer5(x)
        x = self.layer6(x)  
        x = self.layer7(x)
        x = self.layer8(x)
        x = self.layer9(x)  
        x = self.layer10(x)  
        x = self.layer11(x)  
        x = self.layer12(x)  
        x = self.layer13(x)  
        x = self.layer14(x)  
        x = self.layer15(x)  
        x = self.layer16(x)
        x = self.layer17(x) 
        x = x.view(1, -1)
        x = self.layer18(x)
        x = x.reshape(4, 512, 512)
        
        return x



Other test images outputted
...



","['convolutional-neural-networks', 'pytorch', 'u-net']",
How is the training comlexity of NNLM word2vec calculated?,"
I was reading this paper on word2vec, and came around the following description of a feedforward NNLM:

It consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P that has dimensionality N × D, using a shared projection matrix. As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation.

The following expression is given for the computational complexity per training example:

Q = N×D + N×D×H + H×V.

The last two terms make sense to me: N×D×H is roughly the amount of parameters in a dense layer from the N×D-dimensional projection layer to the H hidden neurons, analogous for H×V. The first term, however, I expected to be V×D since the mapping from a one-hot encoded word to a D-dimensional vector is done via a V×D dimensional matrix. I came to that conclusion after reading this referenced paper and this SO post where the workings of the projection layer are explained in more detail.
Perhaps I have misunderstood what is meant by ""training complexity"".
","['neural-networks', 'word-embedding', 'word2vec', 'computational-complexity']","Yes. Technically your understanding is correct; i.e. if all input neurons are active, the computational complexity would be like you said: Q = V$\times$D + N$\times$D$\times$H + H$\times$V.So, if there are V words, the input, of course, will be a 1 of V one-hot vector.
And since there are N words in the input, the input matrix will be of size N$\times$V. If D is the dimensionality of the embedded vector, the projection matrix will be of size V$\times$D.
The product of the input matrix and the projection matrix will then have dimensions (N$\times$V)$\cdot$(V$\times$D) = N$\times$D.But remember that this is a time series problem and not all input words are available at any given time. For a N-gram model, only N inputs are active at any given time. Hence, of the total V matrix elements in the projection matrix, only N elements corresponding to the input one-hot vector are referenced/updated at any given time. The remaining elements are referenced/updated at a different time.As a result, at any given time, the input matrix will have dimension N$\times$N, and the projection matrix will only have to be N$\times$D. The product of these two matrices (N$\times$N)$\cdot$(N$\times$D) will still have dimensions N$\times$D. Hence the total complexity is Q = N$\times$D + N$\times$D$\times$H + H$\times$V.But in practice, this is implemented as a look up table rather than a matrix multiplication, as mentioned here."
What is the difference between CNN-LSTM and RNN?,"
I'm starting to study RNN for a project of video prediction, but I encounter these CNN-LSTM models. Initially, I thought that is another name for RNN, but I think I get it wrong. Since I'm a beginner can someone explain me the difference between them?
","['neural-networks', 'convolutional-neural-networks', 'comparison', 'recurrent-neural-networks', 'long-short-term-memory']","An LSTM is a specific type of RNN. So let's just focus on the CNN part in CNN-LSTM.What's the difference between a plain RNN and a CNN-RNN, (more generally called convolutional RNN or ConvRNN)?The equations which define a vanilla RNN are (I'm omitting a bias term for clarity):$h_t = \sigma_h(W_{hx}x_t + W_{hh}h_{t-1})$$y_t = \sigma_y(W_{yh}h_t)$whereNow, in an CNN-RNN, the parameter matrices $W_{hh}$ and $W_{hx}$ are convolution matrices. We use them for input sequences which are typically better handled by convolutional neural networks, such as a sequence of images."
What is the next-character perplexity of the PaLM model?,"
In the 2022 paper ""PaLM: Scaling Language Modeling with Pathways"", what is the bits-per-character perplexity of the resultant pre-trained model for next-word prediction?
",['language-model'],
What is the Bellman equation for V(s) in the case of a deterministic environment?,"
I am currently trying to practice reinforcement learning for an agent on a grid. The grid is deterministic. Since the grid is deterministic, to calculate the value for each grid square from the reward and next state, we could simply apply the following Bellman equation:
$$V(s)=\max_a(R(s,a)+\gamma V(s'))$$
and not
$$V(s)=\max_a(R(s,a)+\gamma\sum_{s'}P(s,a,s')V(s'))$$
which would be used for non-deterministic grids?
","['reinforcement-learning', 'game-ai', 'environment', 'bellman-equations']","Your 2nd equation is the Bellman optimality equation (BOE) for $V$. So, to emphasise that, you could write it as follows$$
V^\color{red}{*}(s)
=
\max_a(R(s,a) + \gamma\sum_{s'} P(s,a,s') V^\color{red}{*}(s')) 
\tag{1}\label{1}
$$If you letthen we can rewrite \ref{1} as follows\begin{align}
V^\color{red}{*}(s)
&=
\max_a \left(\sum_{s'} \mathcal{P}_{ss'}^a \mathcal{R}_{ss'}^a + \gamma\sum_{s'} \mathcal{P}_{ss'}^a V^\color{red}{*}(s') \right) \\
&=
\max_a \sum_{s'} \mathcal{P}_{ss'}^a \left( \mathcal{R}_{ss'}^a + \gamma  V^\color{red}{*}(s') \right) 
\tag{2}\label{2}
\end{align}which exactly the same equation as equation 4.1 in Sutton & Barto's book, 1st edition, whose online version you can find here. In the 2nd edition, they use a different but equivalent notation.Knowing the definition of $V^\color{red}{*}(s)$ is not sufficient to find it. You need an algorithm. If you are not familiar with dynamic programming algorithms applied to MPDs, then take a look at this chapter. Anyway, you can use e.g. policy iteration or value iteration.Now, back to your actual question. If your environment is deterministic, then$$
P(s,a,s')
=
\begin{cases}
1, \text{if } f(s, a) = s'\\
0, \text{otherwise}
\end{cases}
$$
where $f$ is the (deterministic) transition function.This implies that only one summand in $\sum_{s'} P(s,a,s') V^\color{red}{*}(s')$ might be non-zero. That summand is specifically $V^\color{red}{*}(s')$, when $f(s, a) = s'$, because, in that case, $P(s,a,s') = 1$, and $1$ times $x$ is $x$.So, the BOE simplifies to\begin{align}
V^\color{red}{*}(s)
=
\max_a(R(s,a) + \gamma V^\color{red}{*}(f(s, a))) 
\end{align}So, you're correct."
How are CNN kernels trained when using FFT for convolutions?,"
CNNs (convolutional neural networks) are adept at processing images, as their construction is based on the biological neural networks found in the human eye. ""Kernels"", sometimes called ""filters"", are small feature detectors in the form of small matricies that are slid (called ""convulving"") across an image detecting features in a sample image. This process is computationally intensive, as each time we slide one or more units we have to multiply each kernel value by the section of feature map we are sliding over.
Recently I found here that you can use FFT to do the convolutions up to ~4.8 times faster than with all that multiplication.
Kernels are regularly trained by backpropagation, treating each kernel entry almost like its own axon weight. However with the FFT method I am at a loss as to how to train the kernels.
So, how do you train the CNN kernels when using the FFT method of CNN convolutions? Is backpropagation still relevant?
","['convolutional-neural-networks', 'convolution', 'convolutional-layers']",
"When calculating the max in DQN, do I have to calculate the Q for every possible action for a particular state?","
I'm trying to implement the DQN paper using python/pytorch for my needs (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). I'm studying the main algorithm:

I am a bit confused about the $\gamma* \max Q$ when setting the $y$. My model essentially takes a state and outputs a single float value (reward, cost, call it whatever). Do I have to calculate the Q for every possible action in my action space for that particular state or simply the Q of the state?
","['reinforcement-learning', 'deep-rl', 'q-learning', 'papers', 'dqn']","Do I have to calculate the Q for every possible action in my action spaceYes, for the given state vector $\phi_{j+1}$ you must calculate the action value for all possible actions from that state. That is the only way that you can calculate $\text{max}_{a'} Q(\phi_{j+1}, a'; \theta)$ used to set the TD target $y_j$.or simply the Q of the state?That is not defined. Q is an action value, and can only be calculated when you know both the state and the action.In practice, your neural network in DQN will be one of two designs:It takes state and action concatenated as input and models the state-action value function directly: $Q(s,a): \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. In this case you will need to run a mini-batch of all allowed actions in the same state in order to find $\text{max}_{a'} Q(\phi_{j+1}, a'; \theta)$It takes state as input and models the state-action value for all actions at once: $f(s): \mathcal{S} \rightarrow \mathbb{R}^{|\mathcal{A}|}$. In this case you will only need to run the forward action once to find $\text{max}_{a'} Q(\phi_{j+1}, a'; \theta)$, but you will need to manipulate the resulting TD target data to ensure the only error gradient is from the selected action - typically this is done by running the network forward on the current state $\phi_{j}$ and modifying that vector with the new TD taget only for the taken action $a_j$These designs are not mentioned in the pseudo-code because they are an implementation detail somewhat separate from the theory. You will find the second design is more common in DQN implementations, as it often has better performance (in terms of raw speed processing each state)."
Determine Gridworld values,"
I am learning Reinforcement learning for games following Gridworld examples. Apologies in advance if this is a basic question, very new to reinforcement learning.
I am slightly confused in scenarios where probability of moving up, down, left and right are not provided or stated. In this scenario, I assume we assume the optimal policy and therefore, you would apply the Bellman equation as:
$V(s) = max_a(R(s,a)+\gamma V(s'))$
Cost for any movement is 0 and an agent can choose to terminate at a numbered grid to collect a reward amount of the grid number. This is why my square closest to the reward takes in the value 8 since it will terminate with the action to the next state to collect the reward.
Would this be the correct way to determine the value for the surrounding grid squares?

","['reinforcement-learning', 'deep-learning', 'deep-rl', 'bellman-equations', 'optimal-policy']",
Is attention always better then an RNN/CNN?,"
We've all read the attention is all you need paper, but is it really all you need? Can you effectively replace any RNN/CNN with an attention transformer and see better results?
","['machine-learning', 'natural-language-processing', 'recurrent-neural-networks', 'attention']",
Convolutional network for multilabel classification in NLP,"
I am trying to label code snippets and I base on this article: https://arxiv.org/pdf/1906.01032.pdf
My dataset is just code snippets (tokenized as ascii characters) and 500 different labels from StackOverflow. Currently I have around 1,600,000 samples after filtering these with negative votes and less than 10 characters
This is my current implementation of network architecture in TensorFlow:
def build_cnn(config, hparams) -> tf.keras.Model:
    params = config[""params""]
    inputs = tf.keras.layers.Input((1024, ))
    x = tf.keras.layers.Embedding(input_dim=1024, output_dim=16)(inputs)

    conv_outputs = []
    for filters, kernel in [
        (128, 2),
        (192, 3),
        (256, 4),
        (512, 5)
    ]:
        data = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel, padding=""valid"")(x)
        data = tf.keras.layers.BatchNormalization()(data)
        data = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel, padding=""valid"")(data)
        data = tf.keras.layers.Lambda(lambda a: tf.reduce_sum(a, axis=1))(data)
        conv_outputs.append(data)

    # Concatenation: output size is sum of all convolution filters
    concat_output = tf.keras.layers.Concatenate()(conv_outputs)
    concat_output = tf.keras.layers.BatchNormalization()(concat_output)

    concat_output = tf.keras.layers.Dense(480, activation=""relu"")(concat_output)
    concat_output = tf.keras.layers.BatchNormalization()(concat_output)

    concat_output = tf.keras.layers.Dense(480, activation=""relu"")(concat_output)
    concat_output = tf.keras.layers.BatchNormalization()(concat_output)

    outputs = tf.keras.layers.Dense(500, activation=""sigmoid"")(concat_output)

    return tf.keras.Model(inputs=inputs, outputs=outputs)

As my metric I use F1Score from tensorflow_addons:
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[
        F1Score(num_classes=500, average=""micro"", threshold=0.5, name=""f1_micro""),
        F1Score(num_classes=500, average=""macro"", threshold=0.5, name=""f1_macro""),
    ]
)

After 5 epochs results look like this (later it wasn't really improving):

My first idea was to apply class weights but it didn't help. Next idea was an experiment with only 10 tags (still multilabel) and this is the result:
{
    ""step"": 12,
    ""loss"": 0.2092941254377365,
    ""f1_micro"": 0.6354409456253052,
    ""f1_macro"": 0.5389766097068787,
    ""val_loss"": 0.25258970260620117,
    ""val_f1_micro"": 0.547073483467102,
    ""val_f1_macro"": 0.4620901644229889
}

Last thing: after I saw some results with evaluation I saw that most of the outputs are empty or only 1 class, but using smaller threshold didn't increase the metric
Any idea how to improve the model for bigger number of tags? is it the metric?
","['convolutional-neural-networks', 'natural-language-processing', 'tensorflow', 'multi-label-classification']",
What can we learn from AlphaZero in the development towards AGI?,"
According to DeepMind,

AlphaZero's creative insights coupled with the encouraging results we see in other projects such as AlphaFold,  give us confidence in our mission to create general purpose learning systems

""our mission"" being:

[...] to solve intelligence, developing more general and capable problem-solving systems, known as artificial general intelligence (AGI).

What exactly can we learn from AlphaZero that is so important in the development of AGI?
I asked this to a postdoctoral researcher at the Reinforcement Learning Group, Leiden University in The Netherlands, and he found this statement way too optimistic. He thinks there is still a big hole between solving games with AlphaZero and actual artificial general intelligence.
What indication is there that AlphaZero is a big step towards AGI?
","['reinforcement-learning', 'deep-rl', 'agi', 'alphazero']","Some learnings from AlphaZero:Self-play, and more generally sandbox training, is effective. This indicates that given the right enviroment and enough computational power we can build highly effective agents.Human input is not required, the learnings comes purely from the rules of the game.A bit weaker, but fundamentally any problem can be phrased as search problem (see Levin’s universal search), so improving on search algorithms, as done in AlphaZero, has direct impact on how smart the agents will be.Regarding human level AGI, the limitations with 1 and 2 are that:"
How can AlphaZero be used in other industries besides gaming?,"
I'm an AI Engineering student from Belgium and I'm writing my bachelor thesis on the creation of a chess computer with deep reinforcement learning based on AlphaZero. My implementation can be found here: https://github.com/zjeffer/chess-deep-rl.
To complete my thesis, I'd like to know more about AlphaZero's use in other industries. It seems that so far, AlphaGo, AlphaGo Zero and AlphaZero have primarily been used for solving games.
How can AlphaZero and Monte Carlo Tree Search be used in other industries besides the games industry?
I've looked at this question, but it's more general - I would like to know specifically about AlphaZero and MCTS applications.
","['reinforcement-learning', 'deep-rl', 'applications', 'monte-carlo-tree-search', 'alphazero']",
How do I use A.I. to analyse & score news articles?,"
I'm working on a project that would benefit from using A.I. or machine learning to analyse news feeds from a variety of websites and grade each article between 0 and 10. We would manually grade hundreds of articles to train the A.I. on what we like and what we don't like using the scoring range. The A.I. is expected to learn how we grade by identifying similarities between articles. When the A.I. starts to grade similar to humans, then we would go more hands of and leave this task to the A.I.
Not sure where to start with A.I. what tools and approaches would be the easiest to achieve this?
","['machine-learning', 'natural-language-understanding']",
How do I design the network for Deep Q-Network?,"
I am playing with deep q-learning and I am thinking about what the proper architecture of a network used for deep q-learning is.
I have a very simple environment, basically a 18x18 matrix, where 3 objects live. It is basically a penalty shootout, one player, one ball and one keeper. The player should learn to score goals. The player can move forward, to the left, to the right, and 45° left and 45° right. The keeper moves left and right in front of his goal.
I already used a CNN approach where I fed the 18x18 matrix as image, onto three layers with 64 units each and let the agent learn. Then I used a network with one input layer and 38 features, three hidden layers with 64 units each, and finally I used 2 hidden layers with 256 units each. ReLU and Adam.
All approaches worked. Now, I want to find out which approach works ""best"". But I don't know in which direction to go. All of the training sessions so far took considerable long time. The last approach e.g. takes a few days till the agent figures out not to move of the environment, that he needs to hit the ball, and in which direction he need to shoot the ball.
During the training sessions, I need to adjust the reward function in order to improve the agent. I start with a learning rate of 0.01 and then I reduce it to 0.001 after 300.000 episodes, each limited to 100 steps.
I read about grid search, but I expect this needs enormous amount of time, and I don't have access to lots of processing power, only a simple laptop style GPU.
What is the strategy to get to a better network?
","['reinforcement-learning', 'deep-rl', 'dqn', 'hyperparameter-optimization', 'architecture']","What is the strategy to get to a better network?There are a few different strategies that you can use to search for good hyperparameters in reinforcement learning RL, but you should be aware that this is a hard problem:Even with supervised learning on fixed datasets, efficient hyperparameter searches are not a solved problem.RL adds more hyperparameters for exploration vs exploitation, and management of the training data (e.g. in DQN, the replay memory size, how often to update the target network). These hyperparameters also need to be tuned and will interact with NN-specific hyperparameters.RL requires data collection as part of the learning process, and this can be more expensive than training any neural network internal within the agent. As well as simulating or sensing the environment and on-the-fly feature engineering, this usually requires running the network forward in order to enact the current policy, separate from prediction/loss calculations.RL control involves learning then forgetting ""interim"" functions in a sequence of gradual improvement. Compared to learning an optimal policy directly in a supervised manner, this can take many times more effort.In RL, learning curves can be highly variable, making quick decisions based on early performance unreliable.It is not much a surprise given all this, if a bit disappointing for solo researchers, that cutting edge RL on complex problems is mainly handled by large organisations with deep pockets to cover the compute costs.As a hobbyist, I don't know or have access to large scale solutions, where some new strategies could open up due to the amount of compute available. Assuming you are in a similar situation, perhaps some of the following could help:First and foremost, decide what your goals are for with the current experiment. In different scenarios you will want to focus on certain kinds of change:For ""best agent"" scenarios, learning the problem domain and feature engineering based on that knowledge could help more than hyperparameter tweaking.If you are not in a ""best agent"" scenario, consider whether you have learned enough from the current experiments and move on to some other environment.If you and your computer have nothing better to do, and want to explore possible improvements, arrange for experiments to run when you are not using the computer for other projects, e.g. overnight, and keep an editable ""TODO"" list of next things to try. This could be simple grid searches (if training is relatively quick) or ideas for hyperparameters inspired from theory, effectively random search, third-party suggestions or simply to see what happens.With some experience (maybe from just ""noodling around"" on this or other projects) you may spot learning patterns/behaviours that imply changing specific hyperparameters would be useful. This knowledge does not always transfer when systems (or other hyperparams) are very different, but often enough it does.You can pre-test some NN architectures for capacity to learn a policy or value function by making a training dataset based on the policy or returns for a reasonably good agent - either an expert or a previous RL experiment - then choosing by loss or accuracy or some other metric. This will mainly help you rule out poor architectures without running a full RL training. It probably won't help you to select the best out of the good ones, because once a NN has capacity to learn your test data within reasonable accuracy, differences that you cannot test this way will become more important.One good rule of thumb is to avoid large/complex NNs, and search for the simplest NN that learns your target function well. This will speed up training times, although it does mean you will rarely explore what happens with larger more sophisticated networks.It is often worth working on code optimisation for speed, if your CPU time is a constraint, then extra developer time optimising the environment and the agent will pay back well, allowing you to run more experiments. You can look into Python toolkits like numba (which compiles a limited set of Python to C functions) to speed up custom environments.There are more advanced hyperparameter searches than grid search that may be promising (including use of Bayesian statistics, or genetic algorithms, or even more RL to set hyperparameters), but when your compute budget is low you may find educated guessing and some random search works just as well or better."
"If Least-Squares TD is computationally more expensive, then why is it more data efficient than semi-gradient TD(0)?","
In Sutton-Barto (Section: 9.8 Least-Squares TD, page 228):
Authors say that  Least-Squares TD is the most ""data efficient"" form of linear TD(0). Later, in this section, they say the computational complexity of semi-gradient TD(0) is $O(d)$ and that of Least-Squares TD is $O(d^2)$.
If Least-Squares TD is computationally more expensive, then why is it more data efficient than semi-gradient TD(0)? In fact, what do authors mean by ""data efficiency""?
I think authors mean by data efficiency to avoid from iterative nature of semi-gradient TD(0). However, the Least-Squares TD is also iterative (please see the box at page 230 of the book). I am confused. How do we know that convergence of Least-Squares TD takes less time steps compared to semi-gradient TD(0)?
","['reinforcement-learning', 'computational-learning-theory', 'temporal-difference-methods', 'computational-complexity', 'sample-efficiency']","This is just a partial/general answer that addresses one of your doubts. I will let others address your question about the specific algorithms that you are mentioning.Data efficiency refers to the number of samples or observations that you use during learning, as a function of the number of episodes, time steps or maybe time, to achieve a certain performance (e.g. return). I think that data efficiency is a synonym for sample efficiency, at least, in reinforcement learning. In RL, a sample could e.g. be a tuple $\langle s_t, a_t, r_{t+1}, s_{t+1} \rangle$.The time and space complexities of an algorithm indicate how much computation and memory the algorithm needs asymptotically. These are usually expressed as a function of the size of the input (which is measured in different ways, e.g. number of elements in a list or maybe the number of bits to represent a number), but this does not always have to be the case (e.g. you can also express the time complexity as a function of the output size, for a certain type of algorithm known as output-sensitive algorithm, or maybe as a function of the number of layers in a neural network). In your case, it's expressed as a function of $d$.Now, let's say that we have 2 learning algorithms, $A$ and $B$, and that, on average, algorithm $A$ requires roughly $10^6$ samples to obtain the return $R$, while algorithm $B$ requires only $10^3$ samples to obtain the same return $R$. We can conclude that $B$ is more sample efficient than $A$. However, let's say that, at every time step, $B$ does an expensive calculation (e.g. it performs an algorithm that has exponential time complexity), while $A$ does a constant-time computation. We can say that $A$ has a better time complexity. Later, maybe we can come up with a new algorithm $C$, which has the same time complexity as $A$, but better sample efficiency, so we would probably use $C$ rather than $A$, but we could still choose $B$, provided that its time complexity is acceptable (e.g. exponential time would not be acceptable). So, you could also have an algorithm $D$ that is as sample efficient as $B$, but, at every step, it performs an update that requires exponential time, but it still requires the same number of time steps to get the same return $R$.Note that time-steps in RL are not the same thing as time in time-complexity. Maybe this is where your confusion partially lies.If the explanations above were confusing, then I think you should rigorously review the concepts of time/space complexity.As a side note, computational learning theory is a branch of machine learning that studies learning algorithms from a theoretical/mathematical standpoint, i.e. the algorithms are analysed in terms of time complexity, space complexity and sample complexity, which is related to the sample efficiency, but not exactly the same thing."
"How does SGD training error decrease in subsequent epochs when statistically, it requires that samples in subsequent epochs be i.i.d and they are not?","
I have been reading the Deep Learning book by Ian Goodfellow and on pg. 277, they mention:

It is also crucial that the minibatches be selected randomly.
Computing an unbiased estimate of the expected gradient from a set of
samples requires that those samples be independent. We also wish for
two subsequent gradient estimates to be independent from each other,
so two subsequent minibatches of examples should also be independent
from each other

I understand that for any unbiased estimation we require the samples to be i.i.d. so that we ideally end up with a true representation of the underlying data and so the above statement makes sense. However in practice, the samples that SGD sees for the subsequent gradient update (the next epoch) are the same, and it still performs well in the sense that the training error decreases. The authors later mention :

...but of course, the additional epochs usually provide enough beneﬁt
due to decreased training error to oﬀset the harm they cause by
increasing the gap between training error and test error

I know this happens but if someone could explain to me why and how it happens perhaps from a statistical perspective, it would be great! Another way to put it: why does training error in SGD decrease in subsequent epochs even though the samples are not i.i.d anymore?
","['machine-learning', 'deep-learning', 'stochastic-gradient-descent']",
Doesn't the n-step Tree Backup algorithm negatively affect the DQN-Agent by creating inconsistent look-ahead targets?,"
In the text book of Sutton and Barto on page 152 they introduce the n-step Tree Backup algorithm, where the tree-backup n-step return is defined via
$$
G_{t:t+n} = R_{t+1} + \gamma \sum_{a \neq A_{t+1}} \pi(a | S_{t+1})Q_{t+n-1}(S_{t+1}, a) + \gamma \pi(A_{t+1}| S_{t+1})G_{t+1, t+n}.
$$
Assume we have a sample of the replay buffer and its consecutive
transitions $$ (S_{t}, A_{t}, R_{t+1}, S_{t+1}), (S_{t+1}, A_{t+1},
R_{t+2}, S_{t+2}), ..., (S_{t+n-1}, A_{t+n-1}, R_{t+n}, S_{t+n}). $$
Since in Deep Q-Learning, the target policy $\pi$ is usually the greedy-policy, we have
$$ \pi(a|S_{t+1}) = \begin{cases} 1, & \text{if }  a= argmax_{a}Q(a,
S_{t+1}) \\ 0, & \text{otherwise}. \end{cases} $$
Hence, I think we
would in general look only up to n-steps ahead rather than
always n-steps, since the n consecutive samples from the replay
buffer don't guarantee that the transitions happended according to
the current policy $\pi$. Doesn't this negatively affect the agent's behavior since sometimes the target is made up of only one timestep and sometimes of several?
Thanks in advance for any help!
","['reinforcement-learning', 'deep-rl', 'dqn', 'experience-replay', 'bootstrapping']","Hence, I think we
would in general look only up to n-steps ahead rather than
always n-steps, since the n consecutive samples from the replay
buffer don't guarantee that the transitions happended according to
the current policy $\pi$.This is correct in general in n-step off-policy learners: Whenever exploration chooses an action that the target policy has $0$ probability of selecting, then there is nothing to learn about the target policy from that specific choice. The best you can do is take some other estimate - in TD learning that will be a bootstrap estimate taken earlier than n steps for some state-action estimates. You don't have to do this, but a bootstrap estimate is often better than throwing away the sampled experience (in off-policy Monte Carlo control, you would indeed throw away that earlier experience, and the method can still work in practice though).It is possible to learn from time steps later than exploration, you just re-start from the new $s, a$ pair (and new count up to n). If this changes the estimate of $Q(s,a)$ so that this exploratory action is better in state $s$, then the target policy will change and what was once exploratory will become part of the target policy.There is no way around this in off-policy value estimators.Doesn't this negatively affect the agent's behavior since sometimes the target is made up of only one timestep and sometimes of several?""Negatively affect"" with respect to what?The  main reason for using n-step learning is to find a sweet spot between the bias inherent in bootstrap estimates - worse in TD(0) - and the variance inherent to fully sampled returns from long trajectories - worse in Monte Carlo or TD(1). This reason still applies when the steps that are full samples of experience can vary between $1$ and $n$. There will usually be a value of max $n$ (combined with specific exploration probabilities) between 1 and the full episode length that works better than either of those extremes.It is worth noting that in the DQN ""rainbow"" paper, the authors add n-step returns, ignoring the theory regarding truncating with an early bootstrap. They simply store the discounted sum of rewards $r_{t+1}$ to $r_{t+n}$, plus $s_{t+n}$ for e.g. n=4 steps ahead in the experience replay table, regardless of whether the agent took an exploratory action (in addtion, any of the actions might become exploratory with respect to the target policy later because the experience replay table is effectively an offline dataset). In practice this still works better than single-step updates despite the break from theory, at least for relatively low $n$ and $\epsilon$, and the kinds of environments that DQN was developed against (Atari games)."
Why are Siamese Neural Networks used instead of a single neural network?,"
Siamese Neural Networks are a type of neural network used to compare two instances and infer if they belong to the same object. They are composed by two parallel identical neural networks, whose output is a vector of features. This vector of features is then used to infer the similarity between the two instances by measuring a distance metric.
I was wondering, why not using instead a single neural network that receives as input the two objects that are being compared (e.g. two images) and directly outputs the similarity score? Wouldn't it be better to let the model compare some features of the intermediate layers? Why the Siamese Neural Networks are used for this task and what are the benefits of a Siamese Neural Network over a single neural network that receives as input two instances (e.g. two images) and directly outputs the distance score?
","['neural-networks', 'machine-learning', 'comparison', 'similarity', 'siamese-neural-network']","I come up with multiple advantages for siamese against a single neural network for similarity measuring:Training Phase. If using a single network to replace Siamese, it might be required a double number of parameters (weights) for learning. Hence, training the network will likely converge slower and the network will be more volatile to noise.Testing Phase. Note that these similarity measurements are used in the applications like face recognition. Now, suppose we are going to use the model in such a system. If we have implemented the model by the Siamese, we would only need to compute the output of the model for the input once, and then use the cached results for the existing images in the database, and eventually fasly compute the similarity measures. On the other hand, if we have implemented the measurement by a single neural network, we should compute per query the result for all combination of the input and images in the background. Hence, in the latter, we cannot cache the results for the existing data in the database. Therefore, single neural network implementation will have much more intensive query time for massin dataset than Siamese implementation."
Does it make sense to add an additional attention layer while fine-tuning Bert?,"
I'm fine tuning a Bert/Roberta model for a classification task.
As I need to improve my results, I'm thinking about to add an additional attention layer after Bert model and before dense and dropout layers. Is this a good idea?
","['neural-networks', 'deep-learning', 'natural-language-processing', 'attention', 'bert']",
Attention: Isn't it redundant to apply a linear layer to both the keys and values?,"
Transformer attention is calculated $Attention(X) =X W^V\times \text{columnwise-softmax} (Att(X))$ where the attention attention matrix is $$Att(X) = Q \times K = {X W}^Q \times ({X W}^K)^T = {X W}^Q (W^K)^T X^T $$
But then couldn't one of $W^Q$ and $W^K$ be absorbed into the other? So one of them is redundant say $W^K$ and so attention only needs the parameters from $W^Q$ and $W^V$.
$$Attention(X) = {XW}^V \times \text{columnwise-softmax}({XW}^Q  X^T).$$
Diagrams I use for thinking.


","['transformer', 'attention']","In multi-head attention the Keys, Values and Queries are chunked along the ""channel"" dimension, you need to apply a linear layer before you do this.What I wanted to do wouldn't work because you would be chunking $X$. The chunks of $Q,K,V$ depend on all the channels of $X$, where as the chunks of $X$ do not.There is code here to see exactly what is happening with multi-head attention:"
Why is the step-size $\alpha_t = 1/t$ not appropriate for non-stationary function approximation?,"
Sutton-Barto (Section: Selecting Step-Size Parameters Manually, page: 222)

The classical choice $\alpha_t = 1/t$, which produces sample averages in tabular MC methods, is not appropriate for TD methods, for nonstationary problems, or for any method using function approximation.

Why is the choice  $\alpha_t = 1/t$, which produces sample averages in tabular MC methods? Why is it  not appropriate for TD methods, for nonstationary problems, or for any method using function approximation?
","['reinforcement-learning', 'function-approximation', 'sutton-barto']",
Applications of one-to-one recurrent networks,"
I'm studying recurrent neural networks. Reading this page where it lists different types of recurrent network architectures, I think think of applications involving one-to-many (speech/sentence generation), many-to-one (a conventional time-series classification/prediction task) and many-to-many (machine translation). But I can't really think of an application where it's one-to-one. What is one-to-one used for?
[Edit] Note that on the page, it says ``Traditional neural network'' for one-to-one networks. But a traditional neural network would be a feedforward network rather than RNN? [\Edit]
",['recurrent-neural-networks'],
What would be a good cost function based on both saliency-maps and labels?,"
I have a number of input samples where: every input sample has both a label and a reference-map. This reference-map gives a score to each location of an input sample. The score defines how much this location of the input sample SHOULD contribute to the model’s decision making w.r.t. the correct label.
A gradient based saliency-map defines how much a location of an input sample ACTUALLY contributes to a model’s decision-making with regard to the correct label.
(Saliency-map: https://arxiv.org/pdf/1312.6034.pdf)
I would like introduce a penalty (based on the difference of the saliency-map and the reference-map) if the model does not focus on the areas that should be used to infer the label.
Hence there are two terms involved here: 1. A penalty based on the difference between the inferred-label and the actual-label (normal approach in deep-learning), 2: difference in saliency-map and reference-map.
I know how to calculate the saliency map, the question is more on how to construct an effective cost/loss function based on both the saliency-map as well as the inferred-label. (Currently I am using the categorical crossentropy without any extra penalty-term)
Does anyone if there has been any research done in this area (cost-function based on both saliency map and label), or some paper suggestions related to this approach?
","['convolutional-neural-networks', 'reference-request', 'validation-loss', 'saliency-map']",
Is it possible to train an AI to bring a picture story in the correct order (correct story flow)?,"
I want to know if it is possible to train a neural network (or some other kind of an AI) to bring a simple picture story in the correct order, if it is in random order, so that the story has the correct story flow.
For example, this simple picture story:

or this one

So, imagine the pictures of these stories are in a random order and the AI has to put them in the order that the correct story is told.
Most 8 year olds would be able to do that. So, can an AI learn it? How would an approach look like? Does anyone know if something like that has been achieved or even tried?
From my research so far, the approach would be first to translate the images into descriptive sentences and then try to order them in a meaningful way. But I will do further research, I found so far this paper: Sort Story: Sorting Jumbled Images and Captions into Stories (2016).
To clarify, this is not a ""real problem"" for me, I just asked from a philosophical standpoint and from interest. I will not attempt to solve it, because I think if it is possible it would be extremely difficult.
","['neural-networks', 'machine-learning', 'reference-request', 'computer-vision']","This is a really hard problem for statistical AI such as a neural network. The difficulty level is due to lack of grounding and common sense in what a neural network can process.A neural network could feasibly label all objects in the example scenes, and even do pose estimation, detect activities and guess emotional state for the ""actors"". It can even create a vector representing the content of the image and convert to/from a caption for the image.However, so far any structure or embedding that neural networks have produced is not amenable to reasoning, or common sense. Such embeddings can be translated into other representations, but lack ""grounding"" in the sense of a deeper understanding based on a more general model of the world. It is this understanding - e.g. a parent will be stressed if they think a child is missing (panel 2 from second example), and may then act to find them (panel 4) - that is missing and it is not at all clear how such a world model could be added to neural network training.There are some neural network models that come close in different ways:Large language models. Descriptive text often has a narrative structure, and language models like GPT-3 can easily produce stories as sophisticated as the example panels. In theory such a model could be used to analyse the likelihood of different series of static descriptions extracted by an image captioning system, and identify the highest likelihood story based on trying all combinations. I do not know if this has been attempted.Video activity prediction. In the simpler world of immediate actions and consequences (as opposed to understanding inner state, motivation and narrative), predicting what happens next using a neural network is already possible. These predictions would be short term - In the first panel of the first example for example, a neural network might predict that the fish will go into the bucket. That doesn't mean the neural network models what a fish or a bucket actually are in enough detail to reason about this further, nor that it could extrapolate to the excited child looking at the fish in the bucket on panel 2."
"For an image (of any object), how to find its location in the other image(s) which contains it, given there are no labels or annotations for any image","
Problem Statement:
I am given 2 sets of images. All the images in both sets are without annotations and labels.
First set : a set of images of the grocery store shelves (captured in the grocery stores).
Second set: a set of close-up images of the products kept on those store shelves.
What I am trying to achieve:
Goal: For every product image, I want to find the location of that product in all shelf images in which it appears.
For example,
Input Product image

Output Corresponding Shelf image

My approach:

For each product image, first find all the shelf image(s) which contain that product.
Then predict a bounding box by finding the location of the product in the shelf image.

I am using YOLOv5 for this task but I am not sure how should I start off given that I have to do it without annotations or labels.
I have come across terms like Zero-shot learning, self-supervised Object Detection, etc. but I haven't been able to figure out their use case as a starting point.
There's a similar question asked on StackOverflow but I am not sure the answer to it solves the problem.
","['computer-vision', 'image-recognition', 'object-detection', 'object-recognition', 'yolo']",
"How many layers do GPT-3, AlphaFold 2, and DALL-E 2 have?","
Unsuccessfully, I tried to find out the ""depth"" (definition below) in large neural networks such as GPT-3, AlphaFold 2, and DALL-E 2.
Formally, my question is about their computational graph: consider a path from some node (a.k.a. neuron) to another. The length of a path is the number of its edges.
What is the longest path from an input node to an output node that visits unique nodes at most once?
I would appreciate any answer/reference regarding large networks like those mentioned above.
","['neural-networks', 'deep-neural-networks', 'architecture', 'hidden-layers', 'alpha-fold']",
What are the benefits of using spectral k-means over simple k-means?,"
I have understood why k-means can get stuck in local minima.
Now, I am curious to know how the spectral k-means helps to avoid this local minima problem.
According to this paper A tutorial on Spectral, The spectral algorithm goes in the following way

Project data into  $R^n$  matrix

Define an Affinity matrix A , using a Gaussian Kernel K or an
Adjacency matrix

Construct the Graph Laplacian from A (i.e. decide on a
normalization)

Solve the Eigenvalue problem

Select k eigenvectors corresponding to the k lowest (or highest)
eigenvalues to define a k-dimensional subspace

Form clusters in this subspace using k-means


In step 6, it is using k-means. How is it overcoming the local minima problem of k-means? Moreover, what are the benefits of spectral over k-means?
If someone gives detailed insight upon this, it will be very helpful for me.
","['machine-learning', 'comparison', 'unsupervised-learning', 'k-means', 'spectral-clustering']",
What could be causing the poor performance (MSE) of a dense neural network on a real time-series dataset?,"
I am trying to understand time series analysis and actually I am following the course ""Sequences, Time Series and Prediction"" in Coursera. The course is based on a synthetic dataset, arguably complex, that is studied with different network configurations. The difference among the various networks architectures exist but within certain range. Roughly, all the architectures succeed to grasp the pattern, and they are more or less precise in describing the spikes of the data proposed.
The last exercise is with real data (sunspots) and propose a complex model including convolutional, recurrent and dense layers.
I added a trial applying only dense networks (very successful on the synthetic data), but it did not work at all. The performance is assesed by the mean squared error, and it is three times bigger with the feed forward network than with the other model including recurrent and convolution layers.
As there are many details, it is difficult to present all them in text format, and to show them all the notebook with the data and the code comparing both proposals (and also using or not a crafted callbacks) is here.
I wonder if I had done any mistake when coding, or if there is another reason for obtaining these results.
Any clarification will be well received.
","['neural-networks', 'convolutional-neural-networks', 'recurrent-neural-networks', 'python', 'time-series']",
Unclear points for polynomial basis for function approximation [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I have 3 questions for the following box from Sutton-Barto's RL book (page 211) on polynomial basis:
Q1- Why is each $x_i$ an ""order-n"" polynomial? I think this is wrong: in my opinion, order of $x_i$ can be in the range [1, n*k]
Q2- This sentence is not clear: ""These
features make up the order-n polynomial basis for dimension k, which contains
$(n + 1)^k$ different features"".  What does it mean ""for dimension k""? Why specifically dimension k?
Q3- What is the range for index ""i""?

","['reinforcement-learning', 'function-approximation']","Q1- Why is each $x_i$ an ""order-n"" polynomial? I think this is wrong: in my opinion, order of $x_i$ can be in the range [1, n*k]The text does not claim that $x_i$ is an ""order-n"" polynomial - the order is instead associated with the whole polynomial basis set, and $x_i$ is a feature of that basis. The authors are using order-n as the higher level descriptor of a set of terms which have degree up to $n$.The individual terms are polynomials of the original state features of degree from $0$ to $n$ in each feature separately.Q2- This sentence is not clear: ""These
features make up the order-n polynomial basis for dimension k, which contains
$(n + 1)^k$ different features"".  What does it mean ""for dimension k""? Why specifically dimension k?The original state vector is an $\mathbb{R}^k$ vector. So $k$ refers to the original dimensionality of the state description.Q3- What is the range for index ""i""?It is $[1, (n + 1)^k]$ if you are intending to have full coverage of all possible combinations of state features in degree up to $n$ in each feature, as described in the text.It is not the only way to create a set of derived features using polynomials, and you might just as naturally limit the total degree of all original features within a single ploynomial to $n$ as allow each one to vary from $0$ to $n$ in all combinations.For instance, the book gives an example with $k = 2, n = 2$ of $x(s) = (1, s_1, s_2, s_1s_2, s_1^2, s_2^2, s_1s_2^2, s_1^2s_2, s_1^2s_2^2)$, but you could instead consider  $x(s) = (1, s_1, s_2, s_1s_2, s_1^2, s_2^2)$. Which to use will depend on experimentation and results, no different from any other feature engineering."
What is the reason we loop over epochs when training a neural network?,"
After reading through this thread and some other resources online, I still do not understand the role of epochs in training a neural network. I understand that one epoch is one iteration through the entire data set. But I don't understand what happens when we finish one epoch and start the second, i.e., what is happening such that the error is reduced after each epoch?
Assuming we have an outer loop over epochs, and an inner loop for the gradient descent iteration. What happens is the following, at least to my understanding:
For the first epoch, we take some random initial parameters and perform, say, 1000 gradient descent steps until we have found a local minima where the loss is minimised. So we have landed in a local minima, but it could be a bad one and we want to find a better one, preferably even the global minima. However, this is what I was thinking about:
The only way we can escape a bad local minima is to start a new gradient descent search with different initial parameters, but it seems like starting a new epoch is not just taking different parameters and repeating the process. According to the loss plotted over epochs, the loss is decreasing after each iteration, so it seems like the parameters are not chosen randomly, instead it looks like there is some knowledge used from the previous epoch. First, I assumed that a new epoch just means starting a new search over the loss landscape, this time with another set of random initial values, such that we end up in a different local minima. However, it is not guaranteed that this local minima is a better one, so we could also land in a worse local minima.
But if I look at graphs where the loss is plotted over epochs, it almost always decreases quite smoothly with each epoch, indicating that every epoch does indeed lead to a better minima. This is where I get confused. How is this possible? What is happening at the start of a new epoch? In order to decrease the error, we must have knowledge from the previous iteration, which means that we do not just simply start with a different random set of initial parameters. So, how does a new epoch iteration contribute to a better result? What is happening that the loss is decreasing without ever rising due to worse local minima?
------------ EDIT ------------
I want to add this simple example:
consider a simple linear network, no activation function. We have 2 inputs, each multiplied by a weight to generate a scalar output, i.e. the first step would look like:
x1[0]*w1 + x2[1]*w2 = out

where x1[0] contains the initial value and x2 is a different, known value. We want to find the weights w1 and w2 which minimise the loss between the output and the ground truth. In this case, I will work with one batch only, which contains the 2 inputs. The goal is to approximate a function linearly, given a known initial value. Then, I would run the training loop as follows
...
net = Net(2,1)
criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(),lr=0.01)

for m in range(1,11):
    inputs = torch.Tensor(x[m-1], x[m]])
    for iteration in range(5000):
        net.zero_grad() 
        out = net(inputs)
        loss = criterion(out, truth)
        loss.backward()
        optimizer.step()

This is just as snapshot of the loop, but basically the idea here is to find the appropriate weights for each point m of the function I want to approximate, by repeating the gradient descent search until we have found the local minima, and then proceed to the next point m+1 and so on. Using this example, how and why would a third loop over epochs decrease the error?
","['neural-networks', 'deep-learning', 'training', 'objective-functions', 'epochs']",
Overfitting problem - poor performance on test data,"
I'm facing the problem of overfitting and I can't deal with it - I tried experimenting with optimizer, but nothing seems appropriate. My model has extremely poor performance on testing data and the loss even rises. Is there anything I missed during the model architecture planning or training?
I'm working on GTSRB.
n_epochs = 100
n_train = 4000
n_test = 1000

def load_split(basePath, subset_type, n_samples):
    csvPath = basePath + '\\' + subset_type
    #intialize the list of data and labels
    data = []
    labels = []

    # load the contents of the CSV file, remove the first line (since it contains the CSV header)
    rows = open(csvPath).read().strip().split(""\n"")[1:n_samples + 1]
    random.shuffle(rows)

    #loop over the rows of csv file
    for (i, row) in enumerate(rows):
        #check to see if we should show a status update
        if i > 0 and i % 1000 == 0:
            print(""[INFO] processed {} total images"".format(i))

        # split the row into components and then grab the class ID and image path
        (label, imagePath) = row.strip().split("","")[-2:]
        # derive the full path to the image file and load it
        imagePath = os.path.sep.join([basePath, imagePath])
        #print(imagePath)
        image = io.imread(imagePath)

        #resize the image to be 32x32 pixels, ignoring aspect ratio, and perform CLAHE.
        image = transform.resize(image, (32, 32))
        image = exposure.equalize_adapthist(image, clip_limit = 0.1)

        #update the list of data and labels, respectively
        data.append(image)
        labels.append(int(label))

    #convert the data and labels into numpy arrays
    data = numpy.array(data)
    labels = numpy.array(labels)

    #return a tuple of the data and labels
    return (data, labels)

print(""[INFO] loading training and testing data..."")
(train_images, train_labels) = load_split(DATASET_PATH, 'Train.csv', n_train)
(test_images, test_labels) = load_split(DATASET_PATH, 'Test.csv', n_test)

# Normalize pixel values within 0 and 1
train_images = train_images / 255
test_images = test_images / 255

train_labels = to_categorical(train_labels, 43)
test_labels = to_categorical(test_labels, 43)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(300, activation=""relu""))
model.add(keras.layers.Dropout(0.4))
model.add(keras.layers.Dense(300, activation=""relu""))
model.add(keras.layers.Dropout(0.4))
model.add(keras.layers.Dense(300, activation=""relu""))
model.add(keras.layers.Dense(43, activation=""softmax""))
model.summary()

#Compiling the model
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

#Fitting the model
history= model.fit(train_images,train_labels, epochs=100, batch_size=4,validation_data=(test_images,test_labels))


","['classification', 'tensorflow', 'keras']",
How contrastive loss work intuitively in siamese network,"
I am having issue in getting clear concept of contrastive loss used in siamese network.
Here is pytorch formula
torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                      (label) * torch.pow(torch.clamp(margin - euclidean_distance, min=0.0), 2))

where margin=2.
If we convert this to equation format, it can be written as
(1-Y)*D^2 + Y* max(m-d,0)^2

Y=0, if both images are from same class
Y=1, if both images are from different class
What i think, if images are from same class the distance between embedding should decrease.   and if images are from different class, the distance should increase.
I am unable to map this concept to contrastive loss.
Let say, if Y is 1 and distance value is larger, the first part become zero (1-Y), and second also become zero, because it should choose whether m-d or 0 is bigger.
So the loss is zero which does not make sense.
Can you please help me to understand this
","['loss', 'siamese-neural-network']","I think you are confused about why the margin exists.The margin exists in contrastive learning because we only want the model to output embeddings where negative samples are far from each other to a certain extent.That is, we don't want the model to train itself to output embeddings where the distance between negtaive samples diverge to infinity. That would be an overkill and most likely the model will not be able to learn a meaningful representation.Like you said here,Let say, if Y is 1 and distance value is larger, the first part become
zero (1-Y), and second also become zero, because it should choose
whether m-d or 0 is bigger. So the loss is zero which does not make
sense. Can you please help me to understand thisThe loss is zero when the distance is larger than m for negative samples. Thus, the model will not be further updated for negative pairs that it is already good at distinguishing."
In Graph Neural Network is Message Passing Step Agnostic of Output Values during Training?,"
So Graph Neural Networks is about representation learning where initially representation of graph is learned in the form of node embeddings. My question is: Are the output values back propagated and influence learned node embeddings? The paper I read seems to not account for output being backpropogated so that embeddings get learned. So does that mean learning node embeddings is agnostic of output values during training?
","['backpropagation', 'graph-neural-networks', 'representation-learning']",
When to model decision-making problem as single agent vs multi-agent problem?,"
I understand the goals and purposes of RL in the case of a single agent and the underlying model, i.e. MDPs, for RL problems (or sequential decision making with uncertainty in general).
My question is (and I know this will/may be subjective) are the indicators for choosing to model some decision making problem as a single agent, treating all other factors/noise as part of the environment (MDP or some variant of it) vs multiple agents (a stochastic/Markov game)?
In the zero-sum/adversarial or pure cooperation setting where the goals of the agents conflict/assist each other, it is obvious that the multi-agent setting is the way to go. But suppose that there is no pure conflict/coordination.
","['reinforcement-learning', 'comparison', 'game-theory', 'multi-agent-rl']",
What are examples of simple gradient based NLP models?,"
I am looking for a simple example of gradient-based methods for NLP. More specifically I am looking for post-hoc local explanations gradient-based methods, that is to say, which explain a single prediction by performing additional operations (after the model has emitted a prediction). Here is an example of a gradient-based method for ""Finance""? (I'm not very sure of what domain it is as it is not NLP, nor Computer Vision, nor time series ...):

Let us consider a minimal example, where a linear regression is used to estimate the future capital asset $y_{c}$, based on two investments $x_{1}$ and $x_{2}$. Let assume the assumptions above are met and the model parameters are estimated as follows:
$$\mathbb{E}\left[y_{c} \mid x_{1}, x_{2}\right]=1.05 x_{1}+1.50 x_{2},$$
We can derive immediately a global interpretation of this model. Every dollar invested in fund $x_{1}$ will produce capital of $1.05 \\\$$, while every dollar invested in $x_{2}$ will produce a capital of $1.50 \\\$$, independently of the values $x_{1}$ and $x_{2}$ might assume in a concrete scenario. Notice that this explanation is purely based on the learned coefficient $w_{1}=1.05$ and $w_{2}=1.50$. These, sometimes called partial regression coefficients, are themselves candidate attribution values to explain the influence of the independent variables of the target variable:
$$R_{1}(x)=1.05 \quad R_{2}(x)=1.50$$
Notice also that the coefficients are the partial derivatives of the target variable with respect to the independent variable, therefore this attribution is nothing but the model gradient.

I thought about transformating it to an NLP example like:
Let us consider a minimal example, where a linear regression is used to estimate the toxicity of a text $y_{c}$, based on the occurence of words $x_{1}$ and $x_{2}$. Let's assume the assumptions above are met and the model parameters are estimated as follows:
$$\mathbb{E}\left[y_{c} \mid x_{1}, x_{2}\right]=1.05 x_{1}+1.50 x_{2},$$
We can derive immediately a global interpretation of this model. Every occurrence of word $x_{1}$ will produce a toxicity of $1.05$, while every occurrence of word in $x_{2}$ will produce a capital of $1.50$, independently of the values $x_{1}$ and $x_{2}$ might assume in a concrete scenario. Notice that this explanation is purely based on the learned coefficient $w_{1}=1.05$ and $w_{2}=1.50$. These, sometimes called partial regression coefficients, are themselves candidate attribution values to explain the influence of the independent variables of the target variable
That look pretty dumb, isn't it? Do you have any better example
Update : does the ablation model use gradient?
So it seems my model isn't so dumb, but Ant gave the idea of using the ablation model. I didn't know about this technique but it seems straightforward: you iterate over your text and get rid of a given word in part of the text and see how much the text gains or loses toxicity. It looked very gradient-friendly.
However, I tried to create an ablation algorithm that looks at the k next words and tries to get the toxicity of the word by doing the difference between the text predicted toxicity $\hat y$ and the same one when the text is being ablated $\hat y_{ablated}$. I inspired myself from Warren Freeborough's example for time series in his paper page 5.

I get an en error, or sensibility, matrix $E_{avg}$. But I don't see where any gradient has been used. At most I could say that the model can use gradient. For instance, if it is an RNN, it uses a gradient method, isn't it? But I don't see where ablation uses gradient.
",['natural-language-processing'],
"Should I need to interpret the word ""metric"" in ""performance metric"" rigorously?","
Consider the following abstract from the research paper titled A Note on the Inception Score for instance

Deep generative models are powerful tools that have produced
impressive results in recent years. These advances have been for the
most part empirically driven, making it essential that we use
high-quality evaluation metrics. In this paper, we provide new
insights into the Inception Score, a recently proposed and widely used
evaluation metric for generative models, and demonstrate that it fails
to provide useful guidance when comparing models. We discuss both
suboptimalities of the metric itself and issues with its application.
Finally, we call for researchers to be more systematic and careful
when evaluating and comparing generative models, as the advancement of
the field depends upon it.

Here we can observe the usage of word metric several times. In mathematics, the word metric is used only in the context of metric spaces afaik. The definition for metric space and metric is defined as follows

A metric space is a set $X$ together with a function $d$ (called a
metric or ""distance function"") which assigns a real number $d(x, y)$
to every  $x, y, z$ belongs $X$ satisfying the properties (or axioms):

$d(x, y) \ge 0$ and $d(x, y) = 0$ iff $x = y$,
$d(x, y) = d(y, x)$,
$d(x, y) + d(y, z) \ge d(x, z).$


Do research papers generally use the word metric in the sense of the metric defined above? Or do we need to interpret the word metric less rigorously, just as a measure, like an accuracy?
Note: Although I provided the abstract from a research paper containing the word metric, the question is not restricted to this particular context. This question can be applied to all AI-related research papers that used the word metric, especially in the context of performance or evaluation metrics.
","['terminology', 'math', 'metric']",
Is using separate channels of a RBG image a valid data augmentation technique?,"
Suppose there is a ML network that takes grayscale images as the input. The images that I have are RGB images. So, instead of converting these RGB images to grayscale, I treat each individual colour bands as distinct inputs to the network. that is, instead of feeding RGB image A to the network, I feed the R matrix of A as the first input, followed by the G matrix and then the B matrix. This leads to 3 times more data for the network. Can this be considered as data augmentation (since we are increasing the amount of data)? If yes, what is the name of this data augmentation technique?
","['image-processing', 'data-augmentation']",
Is there a benefit to starting with MCTS and switching to minimax as the branching factor decreases?,"
I've invented a deterministic, perfect-information game with a fairly large branching factor (~150) which tapers out dramatically after the midgame (~30 at worst). I need a strong AI. My understanding from questions like this and this is that it makes more sense to go with MCTS with a large branching factor, but as the game reaches endgame, I'm worried about MCTS missing non-obvious wins due to underestimating their ancestor game states.
I do have some ideas for a heuristic to use with minimax, but since this is a brand new game, it's probably not going to be as good as it would need to be to make minimax worthwhile in general. Instead, I was thinking about using MCTS in the early game, and switching to minimax (with alpha beta pruning) after the branching factor goes below a certain threshold for long enough.
Are there any issues with using an approach like that?
","['game-ai', 'monte-carlo-tree-search', 'minimax', 'alpha-beta-pruning']",
Neural networks with sparse inputs,"
I have a task I want to solve with neural networks. The task is predicting a certain vector of dimension K. The problem is that the inputs to the networks are sparse.
The input is a vector of size N, where N is huge (> 1M) and for most cases, the vast majority of the entries (> 99%) in the input vector are 0. Very rare examples however do have almost full inputs. It's clear that the model is hard to train, as there might be huge weight imbalance within examples, while the target vectors need not have such an imbalance.
I do have a working model, but could you point me to some papers / relevant literature about training a network whose inputs are so sparse? Perhaps there are some techniques that could be useful (maybe some preprocessing steps on the inputs, or something along those lines).
Any hint is appreciated!
","['neural-networks', 'input-layer']",
Does this modified version of the triplet loss function introduced with SBERT that uses the cosine similarity make sense?,"
I am working on a modified version of the triplet loss function introduced with SBERT, where instead of the Euclidean distance we use the cosine similarity. The formula to minimize is max( (|s_a*s_p| / |s_a|*|s_p|) - (|s_a*s_n| / |s_a|*|s_n|) + e, 0) where s_a is the embedding of the anchor sentences (the context), s_p is the embedding of the positive sentence (correct continuation) and s_n is the embedding of the negative sentence (wrong continuation).
I would like to check that the function I came up with makes sense from a theoretical point of view. Where should I look to check which features a loss function should satisfy?
Motivation for the question: I'm getting my hands dirty with contrastive loss functions, and this is an easy variation I came up with.
","['machine-learning', 'loss', 'triplet-loss-function']","A Loss function is just a function with a minimum.In machine learning though, we also require the loss function to be differentiable, otherwise no backpropagation and hence no weight updating. Moreover basically every deep learning library relies on autograd, so if the function is not differentiable your code will simply crash.Stronger but not compulsory condition could be Lipschitz continuity, i.e. ensuring that the function decrease at a constant rate. Intuitively, a loss function should output high values for big differences between predictions/targets and small values for small changes, otherwise the update of the weights will risk to be too big (no convergence) or too small (easily stuck on local minima).Regarding your loss, the only issue I see is that you're replacing a proper metric, i.e. euclidean distance, with a function that is not a metric, i.e. cosine similarity (which does not respect the triangular inequality, hence it's not a metric). So I would be careful and test what kind of values you get with some dummy data to understand if it still behave as a proper loss."
Does a colour consistency loss in neural networks (cycleGAN) make sense?,"
My neural network takes an image as an input and outputs another image. It's the generator of a cycleGAN.
I would like to add (to the discriminator loss, the cycle consistency loss and the identity loss) a colour consistency loss i.e. i want the output image to globally have the same colours than the input image.
Why? My problem is that the network ""tints"" my images too much in my opinion (a standard example is the following: my dataset has women that wear pink clothes more than men do and when i transform men into women, the GAN also transforms the clothes, and sort of adds a pink tint to the whole image). I know that adding this loss will encourage the network to ""not change anything"" though as any change would be a change in colours. Unless my loss looks at the averages of red, blue and green instead of looking at them pixel by pixel, which is what I'd like to go for.
Not the main question but any thoughts on that are appreciated:
any idea about how to implement it in Pytorch, or if it's already been implemented? Searches for colour consistency on GANs just give me GANs that take black and white photos and add colour to them.
","['generative-adversarial-networks', 'pytorch', 'cycle-gan']",
Prediction of continuous variable based on threshold,"
The independent variables are date, count, atmp, and clouds, and the dependent variable is amount. It's only important if amount > 1.0, so it can be a binary task.




date
count
amount
atmp
clouds




2014-01-21 00:00:00
87
0.169158634145595
14.6
NaN


2014-01-27 00:00:00
87
2.931856805839958
14.6
NaN


2014-02-03 00:00:00
87
0.4570096329822
12.4
1.0


2014-02-10 00:00:00
90
0.422058948094016
15.2
NaN




However, converting amount to 0 or 1, doesn't the model lose information that would be helpful in training? And vice versa, if leaving amount as continuous and performing regression, is it possible or common to show the model's accuracy based on being above or below 1.0 rather than using a statistic such as MSE?
What is a good way to model this using machine learning?
","['machine-learning', 'python', 'regression', 'binary-classification']",
How can my RNN get way better results than my ANN [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



So, I'm using the same dataset in both models but my RNN gets a 95% accuracy and my ANN gets 52%.
It is a time series, binary classification problem, and I know that RNN is better than ANN for time series, but I was suspecting that there was some data leak, since 95% is way higher than I expected.
Is it possible that I have a data leak only in my RNN? Or are RNN this much better than ANN for time series?
","['classification', 'recurrent-neural-networks', 'data-preprocessing', 'binary-classification']","An RNN has a recursive element to it, as its output is fed back as part of the next input; this provides a kind of memory.When analysing a time series, values tend to depend on previous values (unless the time series is random), so an RNN is bound to perform better, as it has more context for classification.An ANN basically just looks at values in isolation, and thus cannot make use of the history of the time series. So I would expect it to perform much worse in this case."
"Unclear paragraph in Sutton-Barto on ""Tile Coding""","
Sutton-Barto (Tile Coding, page 218):

For example, choosing $\alpha = 1/n$, where n is the number
of tilings, results in exact one-trial learning. If the example $s\to v$ is trained on, then
whatever the prior estimate, $\hat{v}(s,w_t)$, the new estimate will be $\hat{v}(s,w_t)=v$.

This part is not clear for me. I need to see mathematically why that is the case.
","['reinforcement-learning', 'sutton-barto', 'tile-coding']","In short, this works because the derivative of the (implied) loss function happens to be numerically equal to the difference between current estimate and the target (this is actually a relatively common setup in regression and classifier estimators). In turn, the partial derivatives of weights associated with active tiles in tile coding each inherit that property, because their derivative factor used in the chain rule to figure out the impact of changing them in isolation is exactly 1 (whilst inactive weights have a factor of 0 and are not important to the calculation).Taking a unit step for any single weight would correct the current estimate to the new observed value. As the next part of the text in S&B explains, you would not usually make such an update step if you need the estimator to generalise.We can show this a little more rigorously using the formulae from the book. The general update rule for estimators is:$$\mathbf{w} \leftarrow \mathbf{w} + \alpha[G - \hat{v}(s, \mathbf{w})]\nabla \hat{v}(s, \mathbf{w})$$Where $G$ is the sampled or measured return, and $G - \hat{v}(s, \mathbf{w})$ is therefore a measure of the error in the estimate.Let's call the error in the estimate $\delta$, we can treat that as a single number from now on - any update that adjusts the estimate by $\delta$ makes it match the new data exactly.So now we have:$$\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \nabla \hat{v}(s, \mathbf{w})$$In linear approximators, we have feature vector $\mathbf{x} = x(s)$, and:$$\hat{v}(s, \mathbf{w}) = \mathbf{w}^T\mathbf{x}$$Therefore:$$\nabla \hat{v}(s, \mathbf{w}) = \mathbf{x}$$In tile coding, we always have $n$ active features (the $n$ tiles that the current state vector is contained by) with a feature vector value of $1$, and the rest are $0$. In terms of calculating $\hat{v}$ we can ignore all the $0$ values. The value of $\hat{v}$ is the sum of all the active weights that are associated with the $n \times 1$ values. For convenience (of keeping the notation simple) we can create a vector of length $n$ which is only the weights associated with those $1$s - let's call that $\mathbf{w^1}$. The associated part of $\mathbf{x}$ is a vector of $n$ $1$s that we can call $\mathbf{1}_n$With the new vector, only for this one specific value of $s$ that has these specific $n$ features:$$\hat{v}(s,\mathbf{w^1}) = \sum_{i=1}^n \mathbf{w^1}_i$$Now the update rule for the selected weights is:$$\mathbf{w^1} \leftarrow \mathbf{w^1} + \alpha\delta\nabla \hat{v}(s, \mathbf{w^1})$$We also know that $\nabla \hat{v}(s, \mathbf{w^1}) = \mathbf{1}_n$. This matches to the intuition used in the S&B quote - if you change any active weight by some amount $y$, then the estimate also changes by that amount. Second-order derivatives need to be always zero for this to hold, otherwise you cannot make such easy statements about large changes (e.g. this logic would not work for any neural network).So the update rule becomes:$$\mathbf{w^1} \leftarrow \mathbf{w^1} + \alpha\delta\mathbf{1}_n$$This causes a change to the value estimate:$$\hat{v}(s,\mathbf{w^1} + \alpha\delta\mathbf{1}_n) = \hat{v}(s,\mathbf{w^1}) + \sum_{i=1}^n \alpha\delta$$That is when calculated using the old $\mathbf{w^1}$ from before the update - this works as above because $(\mathbf{w^1} + \mathbf{y})\mathbf{x} = \mathbf{w^1}\mathbf{x} + \mathbf{y}\mathbf{x}$A correction of $\delta$ would be ""perfect"" for the single data point, so if we want that, we want:$$\delta = \sum_{i=1}^n \alpha\delta =  n \alpha\delta$$or$$\alpha = \frac{1}{n}$$"
"Why is $P(X_{t+1} \mid e_{1:t}, e_{t+1}) = \alpha P(e_{t+1} \mid X_{t+1}, e_{1:t}) P(X_{t+1} \mid e_{1:t})$ true in Norvig & Russell's book?","
On page 572 of Norvig & Russell's AI book (edition 3)

Going from the first line to the second line in one shot like that, I am lost.
Can someone walk me through it step by step?
I tried but got:
$$
\frac{P(X_{t+1}) * P(e_{1:t} | X_{t+1}) * P(e_{t+1} | X_{t+1}) }{P(e_{1:t}, e_{t+1})}
$$
and then do not know how to turn it into that second line that says (using Bayes's rule)
","['norvig-russell', 'bayes-theorem']","Let me denote the events with simpler symbolsSo, we can rewrite$$
P(X_{t+1} \mid e_{1:t}, e_{t+1}) = \alpha P(e_{t+1} \mid X_{t+1},  e_{1:t}) P(X_{t+1} \mid  e_{1:t})\tag{1}\label{1}
$$as follows$$
P(A \mid C, D) = \alpha P(D \mid A,  C) P(A \mid  C)
\tag{2}\label{2}
$$Now, note that $C, D$ is a shorthand for $C \cap D = B$, so $P(A \mid C, D) = P(A \mid B)$.So, by the Bayes rule, we have\begin{align}
P(A \mid B) 
&= \frac{P(B \mid A) P(A) }{P(B)} \\
&= \frac{\color{red}{P(C, D \mid A)} P(A) }{P(C, D)}
\tag{3}\label{3}
\end{align}Let's expand the red term\begin{align}
\color{red}{P(C, D \mid A)}  
&= P(B \mid A) \\
&= \frac{P(B, A) }{P(A)}\\
&= \color{blue}{\frac{P(C, D, A) }{P(A)}}
\tag{4}\label{4}
\end{align}Now, let $E = C, A = C \cap A$. So, let's continue writing \ref{4}\begin{align}
\color{blue}{\frac{P(C, D, A) }{P(A)}}
&=\frac{P(D, E) }{P(A)} \\
&= \frac{P(D \mid E) P(E) }{P(A)} \\
&= \frac{P(D \mid C, A) P(C, A) }{P(A)} \\
&= \color{green}{\frac{P(D \mid C, A) P(A \mid C) P(C) }{P(A)}}
\tag{5}\label{5}
\end{align}
Let's put \ref{5} back into \ref{3}\begin{align}
P(A \mid B) 
&= \frac{\color{green}{\frac{P(D \mid C, A) P(A \mid C) P(C) }{P(A)}} P(A) }{P(C, D)} \\
&= \frac{ P(D \mid C, A) P(A \mid C) P(C) }{P(C, D)} \\
&= \frac{ P(D \mid C, A) P(A \mid C) P(C) }{P(D \mid C) P(C)} \\
&= \frac{ P(D \mid C, A) P(A \mid C) }{P(D \mid C)} \\
&= \frac{1}{P(D \mid C)} P(D \mid C, A) P(A \mid C) \\
&= \alpha P(D \mid C, A) P(A \mid C)
\tag{6}\label{6}
\end{align}"
"Do the values over 0.5 mean my model classified the data as a ""1"" label and vice versa?","
I am doing binary classification using an LSTM and my output layer is 1 neuron with a sigmoid function. My labels are either 0 or 1.
from tensorflow.keras.optimizers import SGD
model = Sequential()
model.add(LSTM(64,activation='sigmoid', input_shape=(PERCENT_DATA,1)))
model.add(Dropout(0.2))
model.add(Dense(1,activation='sigmoid'))
opt = SGD(lr=0.001)
model.compile(loss = ""binary_crossentropy"", optimizer = opt, metrics=['accuracy'])
model.summary()

An example of the outputs is
array([[0.9854203 ],
       [0.94532275],
       [0.946043  ],
       [0.5212766 ],
       [0.45969874],
       [0.53517556],
       [0.88838553],
       [0.05345109],
       [0.06565621],
       [0.5552153 ],
       [0.07443756],
       [0.62434113]])

Do the values over 0.5 mean my model classified the data as a ""1"" label and vice versa? Or should I try to find an optimal threshold from these values that will result in the highest accuracy? Or do these values mean something else like the confidence?
","['deep-learning', 'classification', 'long-short-term-memory', 'keras', 'sigmoid']","Yes, the values over 0.5 mean the output should have ""1"" label.As I know with Keras you cannot set the optimal threshold, but you still can use the trick if you want, for example, the expected threshold is 0.8, you can minus the output then clamp the minimum cap of it to 0. {out = max(out-(thresh-0.5), 0)}However, finding the optimal threshold for a model is not recommended in a real project since it may adapt to your validation set only, try to work more with data. I am not familiar with NLP data, but I am sure there are some methods to make data more variant. Label smoothing (as Keras documents) can be a good start."
Normalisation of reward function,"
Problem
Currently, I have some problems defining a reward function for my RL project and mainly with how to normalise the score such that the highest possible score for all instances of the environment becomes 1. The main reason why this is important is that the highest possible score can differ highly between each instance. For example, in one instance, the maximal score can be 50, and in another instance, it can be 150. However, the maximal score is never known.
The environment
The environment of my agent is a single graph of a few thousand nodes, where the goal is to maximise the coverage of a set of $k$ cliques (cliques are subgraphs in which all the nodes are connected to all other nodes in the clique). Each state consists out of the current candidate clique set $D=\left \{C_1,\dots,C_{k} \right \}$ and a newly found clique $C_\text{found}$, which means each state is a tuple of $s=\left (D, C_\text{found} \right)$. The agent then decides if the newly found clique will replace another clique in the set, and so yes, which one. For instance, if the action is $5$, then $C_\text{found}$ will replace the clique $C_5$ in the candidate clique set, and if the action is $k+1$, it will replace none of the cliques in the candidate set. The cliques themself are found through the Bron–Kerbosch algorithm implemented by NetworkX.
The Reward Function
The simplest version of the reward function should be the new coverage of $D \in {s}'$. The function below shows how this is calculated.
$$
R(s,a,{s}')=\left | \text{Cov}(D\in {s}') \right |, \\ 
\text{where } \text{Cov}(D) = \bigcup_{C \in D}C
$$
However, this reward function runs into the problem I described at the start of my question. Another reward function I experimented with is using the difference in coverage between the two states. This reward can be divided by the maximum clique in the graph, such that the score is normalised to a value between -1 and 1. The maximum clique can either be found through approximation or exactly by first going over all the cliques. Below is the formula of this reward function:
$$
R(s,a,{s}')=\frac{\left | \text{Cov}(D\in {s}') \right | - \left | \text{Cov}(D\in s) \right |}{\left | C_\text{maximum}\right |}
$$
This reward function also runs into some problems. First off, in my current training setup, I have never seen the reward be either -1 or 1, with the highest value seen being around 0.75. This difference is caused by most cliques, especially the largest ones, overlapping with other cliques in the graph. The second issue is that with this reward function, the agent will converge to a local optimum in which it rarely changes cliques in the candidate clique set. Lastly, I believe a proper normalised version of the first reward function will work better because this function is also used in the comparison.
Neural architecture and RL algorithm
Currently, I am using PPO with a highly custom neural implementation, in which a GNN is used to first embed all the nodes at the start of the episode, and another GNN embeds all the cliques at each step. The embeddings of all the cliques are then combined and flattened such that I can use them as input for both the actor MLP and critic MLP. The code is written in PyTorch and PyTorch Geometric. I do not believe the network or the algorithm itself is the issue, but let me know if you think so.
Conclusion
I have tried looking at graph and node attributes, which I could use to normalise the coverage score, but I could not find anything. Recently, I have been looking into adaptive normalisation; however, I am entirely new to this concept and would love to have some advice on it or get some links to implementations of it with PPO in PyTorch. Other advice about how to normalise this score function is also more than welcome. Also, if you need more information, just let me know.
","['reinforcement-learning', 'rewards', 'proximal-policy-optimization', 'reward-functions', 'normalisation']",
Do learning rate schedulers conflict with or prevent convergence of the Adam optimiser?,"
An article on https://spell.ml says

Because Adam manages learning rates internally, it's incompatible with most learning rate schedulers. Anything more complicated than simple learning warmup and/or decay will put the Adam optimizer to ""complete"" with the learning rate scheduler when managing its internal LR, causing model convergence to worsen.

I have found the same convergence issues in my own work when using both Adam and a StepLR scheduler.
I understand that Adam adjusts the learning rate on a per-parameter basis, which perhaps negates the need for a learning rate scheduler, but why does this lead to convergence issues?
Is there any mathematical reason/proof why using both the Adam optimiser and a learning rate scheduler causes convergence issues?
Is it true that they really ""compete"" with each other?
","['convergence', 'learning-rate', 'adam']",
What is the best GNN for a NMT task?,"
I am doing a machine translation task using a Graph2Seq graph neutral network.
There are many different variants of GNN:

GCN
GAT
GraphSage
GGNN

Which one would be the most effective for a machine translation task?
I have around 400 nodes in the graph, and on average, a node is connected to 3.5 neighbours.
","['deep-learning', 'graph-neural-networks', 'gnn', 'gcn']",
Is there a way use DQN to find the optimal combination of actions (control inputs) and environment parameters?,"
I am using DQN to find the optimal sequence of control inputs to a dynamic system. The setup is as follows:

At the beginning of each episode, the system is initialized to the SAME initial condition s0.
Each episode spans from 0 seconds to tf seconds. (=each episode takes the same time) A decision is made every h seconds. Thus there are tf/h iterations per episode.
In each iteration, the agent takes an action (He chooses 4 control inputs. For each input there are 3 seperate options to choose from). At the beginning of the next iteration, the agent observes the updated state and selects a new action (again this means that he chooses one of 3 possible values in each of the 4 control channels). Each iteration takes h seconds.
The reward is computed at the end of the episode (i.e. after t=tf).
The state observation includes the current time t. Thus, the agent knows how much time he has left within the episode.

The performance of the dynamic system depends on many parameters. (Considering the agent would control a car, exemplary parameters would be the weight or the aerodynamic drag coefficient of the car.) Changing these parameters will affect the dynamic behavior of the system, i.e. the environment, for the whole episode.
How can I include the optimization of these parameters in the problem? In essence I want to find the optimal combination of time-varying control inputs + parameters.
I though about including the parameters as actions. I then would have 5 action channels (4 control inputs a 3 options each and the parameter with x discrete values). However, considering that changing the parameter is only meaningful before starting an episode, taking this parameter action would only have an ""effect"" at t=0, i.e. when the first action is taken. (It makes no sense that the parameters are changed within an episode.) As a consequence, this ""parameter action"" would have zero effect for all t > 0. I do not know if this hinders the learning of the DQN agent, as this would increase the size of the action space.
Another idea of mine was pursuing a bi-level approach. Thus, I first train an agent that knows how to drive the car optimally (wrt. to my reward function). After training I could apply this pretrained agent to a range of cars, each with different parameter settings. Ideally I would then see where he performs best. However, this approach is not really ""optimal"", as I never optimize for both the control inputs and the parameters in one go...
","['reinforcement-learning', 'dqn', 'action-spaces', 'dynamic-programming']",
"How state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right?","
Sutton-Barto's RL book (page 203)

Example 9.1: State Aggregation on the 1000-state Random Walk: Consider a
1000-state version of the random walk task (Examples 6.2 and 7.1 on pages 125 and
144). The states are numbered from 1 to 1000, left to right, and all episodes begin near
the center, in state 500. State transitions are from the current state to one of the 100
neighboring states to its left, or to one of the 100 neighboring states to its right, all with
equal probability. Of course, if the current state is near an edge, then there may be fewer
than 100 neighbors on that side of it. In this case, all the probability that would have
gone into those missing neighbors goes into the probability of terminating on that side
(thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance
of terminating on the right). As usual, termination on the left produces a reward of
−1, and termination on the right produces a reward of +1. All other transitions have a
reward of zero. We use this task as a running example throughout this section.

Question:  I did not understand how state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right. Could someone explain this calculation in detail?
","['reinforcement-learning', 'sutton-barto']","State 1 has zero states on the left, and 100 on the right. The normal probability of getting to a state is 1/200; the remainder from 1.0 indicates termination.So State 1: 100 x 1/200 = 1/2 = 0.5, which is the probability of reaching any of the 100 states on the right. The remaining 0.5 is thus assigned to terminating, as there are no left states that can be reached.State 950: you have 100 states on the left (so a 0.5 probability of reaching any of them), but only 50 states on the right. So you have 50 x 1/200 = 1/4 = 0.25, which is the probability of reaching any of the 50 states on the right. This leaves a probability of 0.25 to terminate."
What is the difference between a loss function and reward/penalty in Deep Reinforcement Learning?,"
In Deep Reinforcement Learning (DRL) I am having difficulties in understanding the difference between a Loss function, a reward/penalty and the integration of both in DRL.

Loss function: Given an output of the model and the ground truth, it
measures ""how good"" the output has been. And using it, the parameters
of the model are adjusted. For instance, MAE. But if you were working
in Computer Vision quality, you could use, for instance, SSIM.

Reward: Given an agent (a model) and an environment, once the agent
performs an action, the environment gives it a reward (or a penalty)
to measure ""how good"" the action has been. Very simple rewards are +1
or -1.


So I see both the loss function and the reward/penalty are the quantitative way of measuring the output/action and making the model to learn. Am I right?
Now, as for DRL. I see the typical diagram where the agent is modelled using a Neural Network (NN).

I am trying to interpret it, but I do not understand it.
Is it the policy related the loss function somehow? Where is the loss function? How does the reward feed the NN? Is it a parameter for the loss function?
Maybe my confusion has to do with identifying NN with supervised learning, or with not getting this with Q-learning or so.. Can anyone help?
","['reinforcement-learning', 'comparison', 'deep-rl', 'objective-functions', 'rewards']","1. Question: The difference between loss and reward/penaltySo I see both the loss function and the reward/penalty are the quantitative way of measuring the output/action and making the model to learn. Am I right?You are partially right:
You could interpret the negative reward as a loss that you want to minimize. But the model cannot learn from the reward directly. The reason for that is that you (usually) cannot formulate the reward as a differentiable function of the model parameters. Hence, you cannot compute a gradient purely from the reward. You need a second function - the loss - that isOnly then you can compute a gradient w.r.t. the model parameters and make the model learn. This already answers another question:Is it the policy related the loss function somehow?Yes! The policy HAS to be a part of the loss function, otherwise you wouldn't be able to do gradient decent to optimize the model.A simple way to get from reward to the loss is implemented in the REINFORCE algorithm. To understand its loss term, you have to know that the model does not output definitive actions, but rather a probability distribution over all possible actions. Here is the loss function that REINFORCE uses to optimize the model:$loss = -log\_likelihood(action) \cdot return$As you can see, the loss is the product of the negative log likelihood of the action and the $return$. The return correlates with the reward (Return is the discounted reward which distributes the reward received at timestep $t$ backwards to also reward actions that led to the reward). Intuitively this means that for a large reward, the model wants to be very certain about which action to take. So there you have it: The reward reflects how successful you are in the environment and the loss is the optimization objective maximizing the probability to take good actions.2. Question: The Schematic doesn't include the loss functionThe image you posted depicts how you collect the data which you use to optimize the model. You would run this loop of taking an action and receiving a reward until you have a full batch of data. On this batch you would then compute the loss and update the model. It's quite important for reinforcement learning to gather batches and not use single steps for optimization, because otherwise the resulting gradient would be very noisy and in most cases prevent proper optimization.The main issue why your gradient would be noisy is the credit assignment problem:Lets assume the environment is a grid world and the task is to walk forward for 7 steps. You start at $S$ and you will get a reward as soon as you reach location $G$:The reward will show you that you have done something right but multiple actions where responsible for getting the reward (not just the last step forward). However, you never exactly know which actions where the right actions and which actions where actually bad. You might have taken a very inefficient route to the goal.The problem that you don't know which actions contributed to getting the reward is called the credit assignment problem. And in fact you can only have a good heuristic to assign the reward. This has to be compensated by computing the model update on batches rather than single steps. One such heuristic is the general advantage estimate. This is a function that you apply to your reward before plugging it into the loss function.Maybe my confusion has to do with identifying NN with supervised learningOne major difference of supervised learning and reinforcement learning lies in the credit assignment problem: In supervised learning you input a sample and you know what should come out. In RL you only have a rough estimate on how good you where but you will (usually) never know what should come out of your model, because there are multiple possible ways to reach the goal.Hope this helps."
How to define a custom layer in Pytorch [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I am new to PyTorch and seeking your help regarding a problem I have. I need to add a costume layer to a NN in training phase. Please see the figure which shows a simple DNN with the custom layer. NN is trying to solve a regression problem.
Let's consider the simplest case, $f_1(input)=f_2(input)=f_3(input)=sin(a\times input)$, where $a$ is constant. How can I handle such an NN in PyTorch.
Also, can we treat $a$ as a trainable parameter as well if we don't assume it as a predefined constant?
I appreciate any hint or similar example using PyTorch.

","['deep-neural-networks', 'activation-functions', 'pytorch', 'hidden-layers']",
How is state aggregation defined mathematically here? [duplicate],"







This question already has an answer here:
                                
                            




How can $\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)$ be 1 for $S_{t}$ 's group's component and 0 for the other components?

                                (1 answer)
                            

Closed last year.



Sutton-Barto's RL book (page 203):

State aggregation is a simple form of generalizing function approximation in which
states are grouped together, with one estimated value (one component of the weight
vector w) for each group. The value of a state is estimated as its group’s component,
and when the state is updated, that component alone is updated. State aggregation
is a special case of SGD (9.7) in which the gradient, $\nabla \hat{v}(S_t,w_t)$, is 1 for $S_t$’s group’s component and 0 for the other components.

Nothing is clear for me here. How is state aggregation defined mathematically here?
","['reinforcement-learning', 'sutton-barto']","Here is a mathematical definition for state aggregation, which I think covers all the necessary elements to make something an aggregation:The textis 1 for $S_t$’s group’s componentIs referring to using the subscript $j$ in $\mathcal{G}_j$ for the group that contains $S_t$, or alternatively some function $g(s): \mathcal{S} \rightarrow \mathbb{Z}$ that returns a group id or index for each state. Using this function can map the state value $s$ into a one-hot encoded vector $\mathbf{x}$ which is then used in the linear regression estimator $\hat{v}(s, \mathbf{w}) = \mathbf{w}^T\mathbf{x}$.Worth noting that the above defines an arbitrary state aggregation. Usually you would try to do better than that and group states together because there is some reason to expect them to have similar state values in general."
How to use structural information in a Transformer?,"
I am performing a Neural Machine Translation (NMT) task. In my case, input data has relational information.
I know I can use a Graph Neural Network (GNN) and use a Graph2Seq model. But I can't find a good generational model for GNN.
So I want to use Transformer. But then the challenge is how can I embed structural information there? Is there any open source artefact for Relational Transformer that I can use out of the box?
Any pointers?
","['deep-learning', 'transformer', 'graph-neural-networks', 'gnn']",
Performance of augmented dataset with or without original images,"
I am training on yolo and I had a small dataset. I decided to increase it by augmenting it with rotation, shearing, etc to increase the size and increase accuracy.
Now I have seen augmented datasets labeled as with and without original images.
I was wondering if there is difference between training with and without original images besides there just being more images?
","['deep-learning', 'computer-vision', 'object-detection', 'yolo', 'data-augmentation']",
A technique of aggregating many input images to a single representation of the relevant features within,"
I have a few thousand images and I would like to generate a representation of the foreground patterns within - a composition of all of its features, so to speak. In simple terms: take 10000 images of a dog and then draw the archetypical dog.
Does this task have a name, and is there a method out there specifically for such purposes?
The images have different sizes and neither scale nor rotation invariant, so simple averaging algorithms wouldn't work. I would guess that deep learning techniques could be capable - e.g., extracting the features from the first layers of a neural network - as hinted at here: ...""The original network can't be used to classify new identities, on which it wasn't trained. But, the kth layer may provide a good representation of faces in general...."".
I just don't necessarily need a model for prediction afterwards, just the aggregate representation will do.
","['deep-learning', 'feature-extraction', 'representation-learning']",
What type of ML or AI would predict personal traits from a DNA sequence?,"
Suppose you have a large dataset of DNA sequences. Alongside each sequence, you have a portrait of the person with the DNA sequence. Other parameters include the age, gender and race of the person.
I would want to train this AI to at some point be able to predict, based on a new DNA sequence, the age, gender and race of the person, and perhaps generate a portrait.
What would something like this be called? What part of AI and ML does this concern? If I had to guess, it's both a regression and a classification problem and it's somehow related to Generative Adversarial Network.
Also, is there a difference between the ""DNA sequence"" and the ""age, gender and race"" parameters? I'm not sure because in the end you only use the DNA sequence to predict, but during training you would incorporate all four parameters.
","['neural-networks', 'machine-learning']",
GANs inputs normalized and generator only outputs in [-1; 1],"
I'm currently coding a GAN on the dataset MNIST.
I'm using the following code to transform my data:
# MNIST Dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))])
# the output of torchvision datasets are PILImage images of range [0, 1] and we 
# want data that is centered around 0 with a std of 1 (0.1307 and 0.3081 are the estimated values of the MNIST mean & std)

I will have data centered around 0 with a standard deviation of 1 ((0.1307,), (0.3081,) are the estimated mean & standard deviation on the training dataset)). So that means that there will occasionnaly be values above 1 and below -1 in the real data.
Now, my generator ends up with a tanh activation function:
return torch.tanh(self.fc4(x))  # outputs in[-1; 1]

That means there will never be values above 1 and below -1 in the faked data.
Is it possible that the discriminator picks on this phenomenon? This seems to be the case as its loss goes to 0 really quickly. However this also could also be the case that the discriminator is just ""too strong"" as I've seen numerous times on stackexchange posts. I however never seen nobody talking about the fact that it could pick on the fact that there are outliers in the ""real data"" and only pixels between -1 and 1 in the ""fake data"".
EDIT: the entirety of my code can be found here:
https://github.com/JQuentinMendoza2008/PyTorch_GAN_for_MNIST_Dataset
Any suggestion is welcomed.
","['generative-adversarial-networks', 'pytorch']",
Is the initial teacher model in the Noisy Student algorithm noised?,"
Reading through the paper on the Noisy Student algorithm, I have a quick question about how the initial teacher model is built.
In step 1 of the algorithm, the loss function is defined such that it looks like the initial teacher model is trained using noise. But then when you get to step 2 it says the teacher model you use to generate your labels for the unlabeled data should not be noised.
So, should you be adding noise or not for the first teacher model that you train?
","['neural-networks', 'papers', 'semi-supervised-learning', 'image-net']","Short answer: yes, the teacher model should always train with noise.The noise here is just the augmentation and regularization that you may feel familiar with, the author used three kinds of noises:In step 2, the authors said that only the sample which has a high score (high confidence)Specifically, we filter images that the
teacher model has low confidences on since they are usually out-of-domain images.And they even add or remove samples to ensure the distribution of the datasetwe duplicate images in classes where there are not enough images. For classes where we have too many images, we take the images with the highest confidence.And the most important thing, they use the bigger (more capacity, more parameters) model as a studentwe want the student to be better than the teacher by giving the student model enough capacityNow, just think simple, those methods help to enhance the model, which makes the model stronger so teachers, students, or any model should apply those to the training phase and remove them to predict at the test phase (teacher predicts labels).In my opinion, this method is just like label smoothing  but replaces the hyper-parameters with the learning one (the teacher)."
How to include additional numeric input in the Transformer architecture,"
I want to apply the Transformer architecture to my machine translation task, and provide the decoder with an additional parameter in the range of [0,1]. This parameter should provide the decoder with some additional information on how to translate the sequence, think, e.g., how pompous the resulting translation should be.
Although I have some idea on how to approach this task, I would like to ask for help and additional input. I have found a paper by Libovický et al., which studied multi-source sequence-to-sequence tasks. Here are some of the ideas that came to my mind:

Use an additional encoder, to encode this (single) numerical value, and apply one of the approaches mentioned by Libovický et al. This feels strange in the sense that we are encoding only a single element, and attending to it.
In a similar fashion, use an additional encoder, but this time instead of providing a single numerical value, create a sequence of length n, where all elements are the same input parameter. E.g., if the parameter was 0.5 and n = 3, the resulting sequence would be [0.5 0.5 0.5]. Then, use masked multi-head attention to attend to these output values. Note that this approach is based on my intuition only. I lack the needed experience to judge whether this approach could actually provide benefits over the previous one.
Add an additional input to the decoder, applying masked multi-head attention to it, but not necessarily positional encoding. Then, sum up the result of this operation and the ""standard"" input, resume with this interim result. I believe that for this approach I would also have to create a ""fake"" sequence, where each element is the same numerical parameter.

To sum it up, I am looking for a way to provide the decoder with an additional input, in my case a single numerical value.
",['transformer'],
Can off-policy algorithms benefit from the parallelization?,"
On-policy algorithms, such as A2C, A3C and PPO, leverage massive parallelization to achieve state of the art results. However, I’ve never come across parallelization efforts when it comes to the off-policy algorithms, like SAC and TD3.
Is it because the replay memory is kind of a substitute for the parallel data sampling in the on-policy algorithms? Can off-policy algorithms benefit from the parallelization?
Ray RlLib says the following for SAC and TD3 regarding the number of workers for collecting samples

This only makes sense to increase if your environment is particularly slow to sample

","['reinforcement-learning', 'proximal-policy-optimization', 'off-policy-methods', 'on-policy-methods', 'soft-actor-critic']","From the point of view of someone developing an in-house DRL lib and working on extremely CPU-intensive environments (usually large finite element-based simulations that can require several hours to unroll a single episode), yes it can.What I do is that I have $n$ parallel environments unrolling simultaneously and collecting transitions, and I perform $n$ updates after each parallel step (i.e. all $n$ parallel envs have collected one transition), in order to keep a 1/1 ratio between stepping and training. I believe it is okay from a theoretical point of view, although I'm not 100% certain. From a practical point of view, it has worked well so far.The difference between offline and online collection is that the sampling/training procedures are very different, as in online methods you usually sample a large buffer of samples before performing several epochs of training. Yet, I am curious about what would happen if one was to perform the same for, say, TD3: sample a large buffer of $m$ transitions, store them in the replay buffer, and then perform $m$ updates to keep the 1/1 ratio."
What are the advantages and disadvantages of higher order neuron activation functions?,"
I've been reading about different types of neurons that the traditional linear one. One example I came across is the Sigma-Pi neuron, where the activation function includes higher order terms, such as
$f$"" />
where the activation has the additional higher order term on the right, as well as the standard term on the left. What are the advantages and disadvantages of using neurons like that?  Why do we tend not to use them?
","['neural-networks', 'activation-functions', 'artificial-neuron']",
Factors that affect the number of iterations of value iteration,"
I had an assumption that value iteration will take more iterations to converge if the map size increases/environment's complexity increases.
I tried to verify this idea by running value iteration on randomly generated grid worlds of increasing sizes (from 5X5 grid world, 6x6 grid world ... to 50x50 grid world). In each grid world, there are 4 states with reward value of +1. All other states have reward value of 0, -0.03, -1.
Strangely, I noticed that although the computation time for value iteration to converge increased significantly (which is expected since the time complexity of each iteration increases as the grid world's size increases), number of iterations it took for value iteration to converge remained the same at 66 across all map sizes. This contradicts with my assumption that the number of iterations of value iteration should generally increase if the size/complexity of the environment increases.
I  am quite sure my code implementation is correct (I have checked my code and policy/values obtained from the value iterations), and I believe the stop condition that I set for the value iteration is reasonable (I stops value iterations if the maximum change in the values in the current iteration is less than 0.001).
Could anyone explain this phenomenon, and if there are factors that may affect the number of iterations of value iteration?
","['reinforcement-learning', 'markov-decision-process', 'algorithm', 'value-iteration']","What was your value for $\gamma$? I'm guessing at $0.9$?What is $0.9^{66}$? It is $\approx 0.000955$ which is less than $0.001$, whilst $0.9^{65} \approx 0.001061 \gt 0.001$What you are measuring with the 66 max iterations in your chosen environments and problems is the point at which the signal from the backed up reward signal decays below your chosen accuracy threshold. This is also showing an interesting limit for those values - in an environment with maximum reward 1 and optimal trajectories longer than 66 (or with a loop repeat length greater than 66), your chosen values for $\gamma$ and $\theta$ may not be able to find the optimal policy.It is also showing an interesting effect in that even for small environments, value iteration will keep refining the value accuracies, even if the optimal policy was found much earlier. In the small environments (e.g. the 5x5 grid), the chances are that value iteration could be used after much fewer iterations to generate the optimal policy, but it has no stopping criteria for that - instead it keeps resolving the value calculations until they stop changing by the cutoff you have set.This effect you noticed should disappear for value iteration when there is a maximum trajectory length less than the ""signal attenuation threshold"" that you have created. For example, if reaching a specific location also terminated the episode and did so whilst maximising the return. In that case value iteration should behave like a ""flood fill"" and should terminate once the longest possible optimal trajectory has been calculated."
How to create a dataset for binary classification,"
I would like to classify whether a pot of water is boiling or not using a CNN.
Is it enough to take pictures of boiling water using only one pot, or should I use different pots for this to generalize well?
","['convolutional-neural-networks', 'datasets', 'training-datasets']","It depends of your application. If your aim is to have a general boiling detector, yes, if not, don't bother yourself.I would suggest, if you can, to crop the picture of where the water is, to not be inflenced by the overall context.I am pretty sure you will still have good results even with one pot. But you may see some accuracy drop when using different color pot during test time (if that color haven't been seem during training). Some data augmentation with color tweaking will help. That will redirect the network into texture analysing rather than color analysing."
Best algorithm for the Word Ladder puzzle,"
What would be the best performing algorithm to solve the Word Ladder problem, in terms of guaranteed finding of the shortest solution in the shortest possible time? Is it BFS, DFS, A*, IDA* or another one? How are these algorithms compared with each other?
For the Word Ladder puzzle, a starting and an ending word of the same length are given. The goal is to convert the starting word to the ending word by changing only on letter at a time. Intermediate words should be valid words. E.g. silly => sully => sulky => hulky => hunky => funky => funny
","['optimization', 'search', 'algorithm', 'graphs']",
Why are GAN models not heavily used for NLP?,"
I am wondering why there has not been more usage of GANs for NLP. I know there has been research on the subject (The Google Scholar page for the subject is here).
Are there any specific reasons why GANs do not work for NLP specifically VQGAN + CLIP variants? I do not understand why most text generated by AI is done through predicting the next letter or word in a sequence with RNNs when GANs have had so much success generating deep fakes and the such instead of say, predicting the next pixel.
","['natural-language-processing', 'generative-adversarial-networks']",A couple of reasons:
How does OpenAI-ES use Adam?,"
I just read that OpenAI's ES uses Adam: ""OpenAI’s ES is denoted as “OptimES” (since it uses Adam optimizer)""?? I verified they are correct using the link they posted, (see es_distributed/Optimizers.py). But I don't understand how because the paper says they are using Evolution Strategies as a Scalable Alternative to Reinforcement Learning, which is black box optimization (like most ES's)... So how on earth is Adam used, given this is black box?
","['reinforcement-learning', 'evolutionary-algorithms', 'neuroevolution', 'adam']",
"ontological tree for the concept of a word (eg ""chair"")","
I am novice to AI, but I would like to learn the general idea that a machine understands the concepts in a text document.
I would like to ask wether there is an ontological tree of concepts, like a higher class of OBJECTS in which there belongs the FURINTURE, in which there belongs the CHAIR and the TABLE. Is such a thing in NLP? If yes, could someone provide me a way to find such relationships?
",['natural-language-processing'],"There have been a few notable major attempts to build language-processing systems that use directed graphs (similar to, but more general than trees) out of word associations. Variations of your idea, called Symbolic AI, have been used in NLP projects of all sorts and sizes. It is still a common approach for modern chatbots for instance, that use databases of word types, synonyms and relationships.One well-known open-source project to do this is called ConceptNet. There are also initiatives such as CYC which attempt to encode these rules into inference engines.A lot of recent focus has instead been on very large statistical models, e.g. GPT-3. These have performed far better at tasks such as text summarisation, text generation and translation than knowledge-encoding directly by engineers, despite the impressive sizes of CYC and ConceptNet.However, some problems remain with the large statistical models. They often produce grammatically sound but meaningless content that defies common sense. It is not clear how this will be addressed going forward, and in the short-to-medium term at least it is possible that hybrid systems that attempt to combine the best qualities of each approach will be the most practical way to accurately process NLP tasks."
Emulate program behavior using neural network,"
I have an exe file but no access to its source code. It takes as input a list of 8 parameters and prints text files containing the output. I was wondering if it is possible to write an AI-based program that (given that I am a newbie with IA) can ""train"" a neural network using the input and output of my exe file to kind of create a black box that emulate the given exe file.
","['neural-networks', 'python']",
Can directly using expert policy in epsilon-greedy speed-up Q-learning?,"
In deep Q-learning we typically use epsilon-greedy policy during training. We choose a random action for a certain probability $\epsilon$, and choose the action that maximize the current Q-value estimate for probability $1-\epsilon$, and the value of $\epsilon$ decays slowly over the training process
I am wondering if directly swapping the random action with an expert policy can speed-up the Q-learning process?
","['reinforcement-learning', 'deep-rl', 'q-learning', 'dqn']",
What kind of features does each node have as an input graph to a graph neural network?,"
What kind of features does each node have as an input graph to a graph neural network? For example, we want to do image classification with GNN, what are the features of each pixel? Or if anyone could send me a link to implementing GNN on an example I would greatly appreciate it.
","['classification', 'geometric-deep-learning', 'graph-neural-networks', 'features', 'graphs']",
"When using TD(λ), how do you calculate the eligibility trace per input & weight of a neural network neuron?","
I have a Neural Network, each Neuron is made up of inputs, weights, and output. I have potentially multiple hidden layers. The activation function executed against the output is not known by the Neuron.
I would like to use TD(λ) to back-propagate errors through the network as it explores options. My understanding is that this is forward looking TD(λ) because I won't know the error until I reach a terminal state, and so an eligibility trace needs to be kept for each input+weight combination as I back-propagate the error between the NNs new output based on the state-change from the last prediction and the output from the last prediction.
To try and modularise my code as much as possible, the neuron won't know the loss function, but will instead be given the error as a derivative of whatever the activation function of its output was. It also won't know if it's in a hidden layer or not, it will just have inputs, weights, and an output
For example:

So when each Neuron is back-propagating the error signal from all its output connections (summed before it receives it), how do I calculate the eligibility trace?
","['neural-networks', 'reinforcement-learning', 'temporal-difference-methods', 'td-lambda']",
Is using Monte-Carlo estimate of returns in Deep Q Learning possible?,"
In all the tutorials of deep Q-learning (using neural networks) I have read so far, the state-action value function $Q(s,a)$ is learned by temporal difference learning. However, in policy gradient methods, it is also possible to learn by using Monte-Carlo estimate of returns (i.e. observing all the rewards in a trajectory), if the task is episodic.
I would like to know if it is also possible to do deep Q-learning by using Monte-Carlo estimate of returns in episodic task?
","['reinforcement-learning', 'deep-rl', 'q-learning', 'dqn']",
Why does backprop algorithm store the inputs to the non-linearity of the hidden layers?,"
I have been reading the Deep Learning book by Ian Goodfellow and it mentions in Section 6.5.7 that

The main memory cost of the algorithm is that we need to store the input to the nonlinearity of the hidden layer.

I understand that back-propagation stores the gradients in a similar fashion to dynamic programming so not to recompute them. But I am confused as to why it stores the input as well?
","['neural-networks', 'deep-learning', 'deep-neural-networks']","The values need to be stored during each batch only, as the forward values are needed to calculate the gradients.A relevant equation from back propagation is how to derive $\nabla_{z_j^{(k)}}J$, or the gradient of error function $J$ with respect to an individual pre-non-linearity (or logit) value $z$ for neuron $j$ in layer $k$. In the following, $f(z)$ is the non-linearity, and $a_j$ is the output after the non-linearity, and you know $\nabla a_j^{(k)}$ (which you would derive from next layer, or if $k$ is the output layer, from calculating the gradient due to the loss function):$$\nabla_{z_j^{(k)}}J = \frac{\partial E}{\partial z_j^{(k)}} = \frac{\partial E}{\partial a_j^{(k)}} \frac{\partial a_j^{(k)}}{\partial z_j^{(k)}} = \nabla a_j^{(k)} f'(z_j^{(k)})$$The value $z_j^{(k)}$ needs to be stored here to evaluate the gradient $\nabla_{z_j^{(k)}}J$. The value of that gradient is then used in further equations for back-propagation in order to calculate the gradients of the weights, which are usually the parameters you want to update.The storage required is the number of nodes that feed forward, i.e. the total number of neurons in the network (technically also the input values for calculating the first set of gradients, but you will already be storing those, probably for longer), times the size of the batch.Once each batch or mini-batch has had gradients back-propagated, there is no further use for the node/neuron outputs, and the memory allocation can be re-used for the next batch."
What is the name of a feature space which has consistant distance-related properties?,"
What is the word describing a feature space where distance between two elements has a decisive informational value, whatever the pair of elements is?
For example if a model creates embeddings for words, and we take all possible pairs of embeddings and compute their distances, and it is possible to set a fixed threshold distance, where every pair whose distance is inferior to the threshold are synonyms, and every pair with distance superior to the threshold aren't. Is there an adjective that qualify a given feature space that has this property?
","['machine-learning', 'terminology', 'features']",
When is it appropriate to use information like sex or race in ethical machine learning?,"
I'm a little confused on best practices regarding ethical ML. Specifically, I've seen in some courses that when building a model that affects people, it's helpful to have sensitive personal information related to sex, race, etc., to help ensure that the model distributes its error evenly across populations and avoids disparities.
I've also seen (in practice on the job and in recommendations) that this kind of information should be entirely omitted from your data to ensure the ""blindness"" of the model to protected and sensitive classes.
These directives seem at odds with each other. Has any kind of rubric been developed to help modelers determine when it is and is not appropriate to use this kind of information in a model?
","['machine-learning', 'features', 'ethics']","Those suggestions are actually not in contradiction with each other.Having data for testing and using them for training are two completely different things. Let's take the infamous example of Amazon automatic recruiting algorithm. The model was trained on real people curricula tp rank them and suggest this way the best candidates to hire. After deploying it  it became clear pretty soon that the model was gender biased, cause it was not ranking female curricula high even when all other requirements were met.Why was the model biased? Because humans are also biased and we are those who labelled the data from which the model learn. So if a human recruiter compare the same curricula with a male name and a female name chances are it will select the curriculum with a male name.So far so good, we spotted a potential cause for the bias, so we can just retrain the model without the gender variable and it should be fine. But can we trust the model will not be gender biased anymore? Well not 100%, there are many correlations between gender and other variables, and the model might still learn a biased behavior by ""learning"" the gender variable from other variables.So that's why we want to keep the gender variable in our data, but only to use it in test phase, not as an extra feature to feed to the model (this time it was trained without) but as a variable to use to compute statistics out of the model predictions, i.e. how many males and females does the model choose after being trained without information about gender?"
Confusing statement in Sutton-Barto's RL book in Section 8.5 ( Expected vs. Sample Updates),"
In Sutton-Barto RL's book (page 174) it says:

The advantage of sample updates shown in Figure 8.7 is probably an underestimate of
the real effect. In a real problem, the values of the successor states would be estimates
that are themselves updated. By causing estimates to be more accurate sooner, sample
updates will have a second advantage in that the values backed up from the successor
states will be more accurate. These results suggest that sample updates are likely to be
superior to expected updates on problems with large stochastic branching factors and
too many states to be solved exactly.

This is very confusing because before the above paragraph they say

The values at the next states are assumed correct,...

If so, the results in Figure 8.7 for sample updates should be worse in real.
",['reinforcement-learning'],"If so, the results in Figure 8.7 for sample updates should be worse in real.Yes, the sample updates will perform worse than shown. The later paragraph is explaining that the results for expected updates may be even worse relative to sample updates. It is not claiming that sample updates are unaffected if you remove the simplifying assumption used in the comparison.The assertion on page 174 is that the disparity between the approaches may in practice persist for longer than the idealised situation (of already having correct values to backup/bootstrap from), because it may take longer to converge the expected values from the original estimates than the sampling approach would take. This is due to the same effect impacting each time step in a longer trajectory, whilst figure 8.7 shows the impact only for a single time step."
How do I deal with a dataset of Images with variable sizes (width and height) when doing Image Classification?,"
I have a dataset in which the images which don't have the same width and height. How do I perform Image Classification with such images? I am trying as much as possible to steer away from image resizing because in my case it will lead to loss of important data/ give poor results.
","['deep-learning', 'computer-vision', 'image-recognition', 'data-preprocessing']",
Do we need backpropagation if there is only one class?,"
A am interested in physiologic neural network. Altough there are some opposite views, most probably there seems to be no plausible way to explain a physiologic backpropagation in the brain.
So I am trying to code a neural network without backpropagation yet my mathematical  understanding is inadaquate, so I want to ask folowing simple question:
“If we do have only one node at the right side of the network, accepting that the inputs are on the left, can we train the network without backpropoagation and using the mean of weights instead? As we would know the y for all x it should be possible to calculate the mean w?”
The idea is that the system should work continiously, and train continiously for one class, until the trained network decides that the input is different from the known previously trained classes. And if it is different, it should create and train for that new class of inputs. I think that should be the working system of the brain, and as the cortex has similar cells, mathematically it also must be that easy?
But where is my flaw (with simplified math please :))
","['neural-networks', 'backpropagation']",
What is the meaning of step (e) in the prioritized sweeping algorithm? Why is P calculated like that?,"
Following is the ""Prioritized Sweeping"" algorithm in Sutton-Barto's RL book (page 170).  What is the meaning of step (e) in the prioritized sweeping algorithm? More importantly, why is P calculated like that?

","['reinforcement-learning', 'prioritized-sweeping']","$P$ tells you how ""off"" the evaluation for $Q(S, A)$ is. If the difference between $Q(S,A)$ (current best guess) and $R + \gamma\max_a Q(S', a)$ (update-value for $Q(S, A)$ since you received $R$ and were moved to $S'$ when you did $A$) is large, then it is an indication that this is a state-action-pair you should consider to learn more about, and $(S, A)$ will be close to the front of the queue. On the other hand, if $P$ is small, there is ""nothing new to learn"", and no need to prioritize this state-action-pair."
Strange artifacts in autoencoder outputs,"
I'm training an autoencoder, that does not downsample images but processes them in the same size. For example, a 256x256 input will always be processed at 256x256 resolution, only the channels increase deeper in the network. This design is due to the next stage for which the model will be used. L2 regularization on activations is added since this is essentially a sparse autoencoder.
However, I get some strange artifacts in the corners when the model converges:


The left image is the input, the image on the right is the output. The artifacts can be seen on the top-left and top-right corners of the output image.
Can someone explain what causes these artifacts, and how they can be fixed?
","['convolutional-neural-networks', 'autoencoders']","Assuming that you're using convolutional layers, those artifacts may be related to the boundary conditions used.  The convolution kernels have a spatial support of say 3x3 pixels, meaning that the response at a position is a function of the corresponding inputs in a 3x3 neighborhood at that position.  If the position is adjacent to a boundary, say at the upper left pixel, then the south-west, west, north-west, north, and north-east pixels are outside the image region.If you only use so-called ""valid"" pixels, that is, you only get responses for those positions where the entire convolution kernel is inside the image region, then you're ignoring some information at the ""invalid"" pixels.  On the other hand, we are missing information at those invalid pixels, so an assumption must be made about those missing pixels.For your application, in the internal layers, where the expected features are gradient-like / oscillating and localized, you may wish to try zero-padding, because that's like an assumption that one doesn't assume new feature information outside the region.However, at the first layer itself, the convolution is with a natural image (as in your example), so you need to predict the pixel values at the border, so there you might want to experiment with ""repeat"" or ""reflecting""/""mirror"" boundary conditions."
Why do we use a linear interpolation of fake and real data to penalize the gradient of discriminator in WGAN-GP,"
I'm trying to better frame/summarize the formulations and motivations behind Wasserstein GAN with gradient penalty, based on my understanding.
For the basic GAN we are trying to optimize the following quantity:
$$\min_\theta \max_\phi \mathbb{E}_{x \sim p_{data}(x)}[D_\phi(x)] + \mathbb{E}_{z \sim p_G(z)}[1-D(G_\theta(z))]$$
The problem is that the dissimilarity measure between the two probabilities given by Jensen-Shannon divergence will not take into account any distance in a Euclidean sense. That's why we consider the Wasserstein distance defined as:
$$W(p_{data}, p_G) := \inf_\gamma \, \,\mathbb{E}_{(x,y) \sim \gamma(x,y)}\|x-y\|$$
that will account for a proper distance of our distributions. Computing it is very hard so we rely on Kantorovich-Rubinstein duality which states we can rewrite $W$ as:
$$W(p_{data},p_G) = \sup_{\|f\|_L \le 1}\mathbb{E}_{x \sim p_{data}(x)}[f_\phi(x)] - \mathbb{E}_{z \sim p_{G}(x)}[f_\phi(G_\theta(z))]$$
Now the crucial point, to enforce the constraint of $1$-Lipschitz continuity of the discriminator we add a penalty term to bound the norm of the gradient of $f$, so the final loss we consider is:
$$\mathcal{L} = \mathbb{E}_{x \sim p_{data}(x)}[f_\phi(x)] - \mathbb{E}_{z \sim p_{G}(x)}[f_\phi(G_\theta(z))] + \lambda \, \mathbb{E}_{\hat{x}}[(\|\nabla_{\hat{x}} f_\theta(\hat{x})\|-1)^2]$$
where
\begin{equation}
\hat{x} = tx + (1-t)z
\end{equation}
$t \in [0,1]$.
Now, I've understood that we bound the slope of discriminator because we want toavoid the vanishing gradient problem and keep gradient signal in order to make the generator learn, but why do we actually penalize the gradient of discriminator with respect to a linear interpolation of real and fake data?
","['generative-adversarial-networks', 'wasserstein-gan', 'wasserstein-metric']",
Why do we use a weighted sum in an artificial neuron instead of another more complex function?,"
I have just started learning about NN and DL and I wanted to know if there is a theoretical reason we use a weighted sum for all the inputs in an artificial neuron. So for example if we have a neuron with two inputs which have weight $w_1$ and $w_2$, why don't we use a function like $x_1^2\times w_1 + x_2\times w_2^2$?
","['neural-networks', 'artificial-neuron']",
VAEs vs Autoencoders with BatchNorm and Dropout?,"
It struck me that regular auto-encoders with batch-norm and dropout have quite similar properties to VAEs which made me wonder whether VAEs where really much better than this simpler alternative. Let me explain:

BatchNorm: encourages activations to follow N(0,1), just like KL-divergence does to the output layer of the encoder
Dropout: during training time creates random (Gaussian due to CLT) encoder output distribution, mean and variance are learnable indirectly via network weights and mean/variance override params of batch norm layer

You might argue that KL-divergence adds more flexibility by allowing it to not exactly follow N(0,1) but batch norm allows learning to override this by default anyways.
All that considered are there really any practical benefits to VAEs which cannot be provided by this simpler setup?
","['autoencoders', 'variational-autoencoder']",
How is AI used in Internet of Things?,"
I would really appreciate it if someone would explain how AI is used in IoT. In the papers that I have found, half of the paper itself is about what IoT  is and very few information about how AI is used for it. Good paper recommendation or a book would also strongly help.
The papers that I found:

Selected methods of artificial intelligence for Internet of Things conception (2015) - Aneta Poniszewska-Maranda, Daniel Kaczmarek

Artificial Intelligence in Internet of Things (2018) - Ashish Ghosh, Debasrita Chakraborty, Anwesha Law.

The application of artificial intelligence in the internet of things (2019) - Wenbo Yao.

Review on Artificial Intelligence with Internet of Things - Problems, Challenges and Opportunities.


","['papers', 'applications']",
How can the agent be defined in a reinforcement learning problem with a tabular dataset as the environment?,"
Let's assume we need to train an RL model that drops duplicates in a tabular dataset? The actions should probably defined as drop or do nothing.
But what should be the agent itself then? To me, it doesn't make sense to see it just as a navigator looping over the states (dataset indices from the first to the last) and decide on which to drop.
","['reinforcement-learning', 'data-preprocessing', 'control-problem', 'structured-data']",
"Image Recognition Method, calculate deviation from rectangular grid","
I have a set-up which creates pictures of a grid that is a bit bend towards the ends, and I need some kind of program that can calculate the deviation, resp. it just needs to be some kind of indicator, a stronger deviation (strong bend) leads to higher number and something close to a perfect rectangular grid will lead to a number close to zero. What possibilities do I have to do that? I'm just asking for the Method. I can't use neural networks since I have little data, I can't generate lots of pictures. Is there any other method to implement a solution to that problem?
Here's an example picture of the grid: 
I need a program that calculates the mean deviation of the red lines from the perfect rectangular grid. What methods exists for such kind of problem apart from neural networks?
","['image-recognition', 'image-processing']","So, here's an idea, not perfect but it should give you at least a starting point.The last step is the most tricky. Theoretically it should be possible to detect if 2 points belong to the straight grid by comparing their values (cause the y coordinate should be the same). In practice it's not always the case. This detection is important to understand which pairs of values to subtract (point that belong to straight grid - next point in the list, curved one). Also, if all points belong to the straight grid, you know you have 2 straight grids.I think its possible to refine the methods by getting pairs of left/right points and computing the slope of the line they form, if the slope value is lower than a threshold (close enough to 0) then again we can safely assume the 2 point belongs to the straight grid.I didn't implement yet this step, but if you find it hard I could try, just don't want to spend too much time on it to then find that is not useful for youNote also that the same procedure could be then applied not only on the edges bu to all vertical lines of the image.code so far:"
Is there a shared minimalist taxonomy of the fields of AI?,"
Is there a reasonably accepted/shared view on how to minimally classify the various fields of AI? There are hundreds of techniques and I have not been able to find a shared exhaustive classification in reasonably disjointed 'fields', or approaches, or 'schools'.
One possible classification (please, feel free to correct my imprecise terminology) is given by Wikipedia in the Artificial Intelligence page
• Symbolic.
• Deep Learning.
• Bayesian Networks.
• Evolutionary.
But it seems to be somewhat lacking (for example, not all NN are deep learning, and where do SVN methods fit here? Bayesian networks seem too specific to accomodate them).
On the ""Outline of Artificial Inteligence"" page, wikipedians are giving a more detailed view, but it does not look minimalist to me (moreover, 13 Search a field of its own? Is it not 'dispersed' in all other fields? see Note)
A slightly more minimalistic, yet apparently complete, classification can be found in the laymen book ""Artificial Intelligence for Dummies"". Here the authors mention five ""tribes"" (I might have placed them in a different order):
• Symbolists: The origin of this tribe is in logic and philosophy. This group relies on inverse deduction to solve problems.
• Bayesians: This tribe's origin is in statistics and relies on probabilistic inference to solve problems.
• Analogizers: The origin of this tribe is in psychology. The group relies on kernel machines to solve problems.
• Connectionists: Thia tribe's origin is in neuroscience and the group relies on backpropagation to solve problems.
• Evolutionaries: The evolutionaries tribe originates in evolutionary biology, relying on genes e programming to solve problems.
Is this the most comprehensive yet mininalist classification? Is there an even more comprehensive one, along these lines? It also seems to me that Bayesian and Analogizer can both fall into the realm of ""Statistical Learning"".
What I am looking for, in order to orient myself, is some sort of big Venn diagram or better yet partition with all the approaches/tribes (the more general, the better) and examples of techniques (for example, Naive Bayes under Bayesian: Expert Systems under Symbolists: Perceptrons, Neural Networks under Connectivists...)
To be clear, I am not trying to accomodate transversal approaches, such as Classical vs Hachine Learning, or Supervised va Unsupervised vs Reinforcement Learning, or Strong vs Weak AI..
I understand there are hybrid approaches, of course. What I am interested in is the possibly most disjointed, 'pure approaches on the line of the five tribes described above. Am I perhaps looking for a magical unicorn?
Note: As stated above, I am also a bit confused about how to fit searching algorithms, because I tend to see them as either transversal to the various tribes' or even orthogoal to them imaybe part of Optimization goals, along with Predicit ion, Classification and... what else? Planning?), but this might be the subject of another question
",['classification'],
Why would one prefer the gradient of the sum rather than the sum of the gradients?,"
When gradients are aggregated over mini batches, I sometimes see formulations like this, e.g., in the ""Deep Learning"" book by Goodfellow et al.
$$\mathbf{g} = \frac{1}{m} \nabla_{\mathbf{w}} \left( \sum\limits_{i=1}^{m} L \left( f \left( \mathbf{x}^{(i)}, \mathbf{w} \right), y^{(i)} \right) \right)$$
This is mathematically equivalent to
$$\mathbf{g} = \frac{1}{m} \left( \sum\limits_{i=1}^{m} \nabla_{\mathbf{w}} L \left( f \left( \mathbf{x}^{(i)}, \mathbf{w} \right), y^{(i)} \right) \right)$$
just moving the gradient operator inside/outside the sum.
But I was wondering: why one would prefer the first representation?
My thoughts, and please correct me if I am wrong:

When performing batch gradient descent in practice, we process one example
after the other and compute the corresponding gradient everytime. So
the second equation above better represents what is really happening

Even more, there is no alternative in practice. I can not compute all losses, just sum them up and obtain just one gradient afterwards. So the first equation (although mathematically correct and equal) might even be somewhat misleading?

The only reason I can imagine to decide for the first way is to express more clearly that there is one (aggregated, mean) gradient based on one data set


Any mistakes here on my side?
","['gradient-descent', 'gradient', 'mini-batch-gradient-descent']",
"Given the high resolution signal and the low pass filter (kaiser filter), is there a way to reconstruct the low resolution signal?","
When we upsampling a discrete 1d signal by 2x, we first interleave the signal by 0, then pass through a low pass filter.
low resolution signal [x1, x2, x3, x4] -> interleave 0 -> [x1, 0, x2, 0, x3, 0, x4] -> low pass filter (kaiser filter, convolution) -> high resolution signal
Given the high resolution signal and the low pass filter (kaiser filter), is there a way to reconstruct the low resolution signal? If there is not a way to perform precise reconstruction, is there a way to perform roughly reconstruction?
","['reference-request', 'filters', 'signal-processing', 'upsampling', 'downsampling']",
Why does this neural network require 2 hidden layers to completely fit my data?,"
I have been working toward implementing my own Neural Network library in C++ for fun.
I have managed to implement an XOR solving network based on widely available information. Now, I wanted to try a classification problem and use the cross-entropy as my loss function.
I have created a dataset of 729 records (all possible combinations of 3 numbers between 0 and 8):
Input Features | expected Output
--------------------------------
{0, 0, 0}      | {0.0, 1.0}
{1, 0, 0}      | {1.0, 0.0}
...
{8, 8, 7}      | {0.0, 1.0}
{8, 8, 8}      | {0.0, 1.0}

The output should be {0.0, 1.0} when there is no 1 in the input, and {1.0, 0.0} when there is a 1 in the input. (the inputs are normalised to the -1.0 to 1.0 range).
I was having a hard time getting my network to correctly accomplish this classification. Because I had the whole dataset, I tried training it on the complete dataset in an attempt to have it fit perfectly. I wanted to see it fit perfectly to provide me some assurance that my implementation was correct.
Numerous attempts failed:
# hidden | learning |  results
  units  |  rate    | (accuracy)
--------------------------------
3        | 0.00001  | 72%
3        | 0.000001 | 60%
5        | 0.0001   | 80%
5        | 0.00001  | 75%
6        | 0.0001   | 79%
6        | 0.001    | 91%
10       | 0.001    | 72%
10       | 0.0001   | 91%
10       | 0.00001  | 71%
20       | 0.0001   | 91%
20       | 0.00001  | 74%
20       | 0.002    | 77%

These were all over 20k epochs, a momentum of 0.5, relu activation on all units, softmax and cross-entropy loss for the output. Some of these were unreliable too, meaning sometimes they would not learn at all, but starting with a new set of random weights worked.
You can see that it capped out at 91% accuracy. But, if I add a second hidden layer, so that my network looks like:
Input Layer | Hidden Layer 1 | Hidden Layer 2 | Output Layer
      3     |        6       |        2       |      2

I instantly get 100% accuracy using 0.001 learning rate after only 1000 epochs.
Why does this additional hidden layer work?
Is there a way to know when I should use additional layers?
","['neural-networks', 'machine-learning', 'deep-learning', 'xor-problem']","Your data is simply too non linear. You can check it by simply plotting the 2 different classes with different colors on a 3d plot. Unfortunately this is not always possible, and there's no rule of thumb to establish when the data are too non linear or how many layer you should use.But in general you should be aware that every layer in a neural network works as a linear transformation of you data coordinates, with some non linearity added only by the final activation function, but in general a single layer lead to an almost linear mapping, hence the failure of all your attempts with a single layer.This blog post offer beautiful visualizations of what I mean with an in depth analysis of the limitations of the classic neural networks design (hidden layer = $Wx + b$). It surely offer lot of insights even though I repeat again that there's no explicit answer to your question about how to calculate or predict how many layers are required for a task.Also just a technical note, combinations implies no repetition, so your dataset is actually a triple cartesian product of all numbers between 0 and 8.code to generate the plot (in python):"
"Without planning, why does each episode only add one additional step to the policy?","
In Sutton & Barto's RL book at page 165 for Example 8.1, they say:


Figure 8.3 shows why the planning agents found the solution so much faster than the nonplanning agent. Shown are the policies found by the n = 0 and n = 50 agents halfway through the second episode. Without planning (n = 0), each episode adds only one additional step to the policy, and so only one step (the last) has been learned so far. With planning, again only one step is learned during the first episode, but here during the second episode an extensive policy has been developed that by the end of the episode will reach almost back to the start state.

I have the following question:
Why, without planning (n = 0), each episode adds only
one additional step to the policy? What does ""adding one additional step to the policy"" mean?
","['reinforcement-learning', 'sutton-barto', 'dyna']","What does ""adding one additional step to the policy"" mean?This is an informal shorthand for how many backup steps from a non-zero reward or learned value will receive a meaningful update. That is, an update with actual data other than the initial bootstrap.This is due to three things:Bootstrap updates in basic TD only look one timestep ahead, e.g. from $s,a$ to $s',a'$, with the update $Q(s, a) = Q(s, a) + \alpha(r + \gamma Q(s', a') - Q(s, a))$Only the last step in the example environment has a non-zero reward.Only values that have already received at least one ""meaningful"" update are themselves useful in bootstrapping real data back to earlier time steps. Without such an update, they only have initial arbitrary values.With background planning, as in Dyna, there is a chance of choosing to base the update on a state, action pair that has already been updated with some non-zero* data, whilst the agent is exploring elsewhere.Without background planning, and only using single-step backups, initially only the ends of episodes will receive updates with meaningful data.You can also use multi-step backups, or eligibility traces - as in TD($\lambda$) - to increase the number of steps that get updated at once. These options each have their own pros and cons.* Technically it is not non-zero that matters, but ""different from other experience"". Initially the agent experiences a bunch of states and actions that all look the same to it in terms of its estimates of expected future reward. Before an agent can learn to choose between actions it needs to experience results that are different between them."
How might AI analyze abusive discussion using natural language grammar?,"
Opening thoughts
This does not only apply to SE comments, but the idea in general.
This is not a Question for Linguistics.SE; those Questions might come later, after AI analysis. Example Linguistics Quesions:

What grammar categories might AI use to research for an analysis of abusive vs helpful discussion? (before the AI research, after this OP Question is answered)
What grammar patterns can we identify from AI researched that analyzed abusive vs helpful discussion? (after the AI research)

This is a Question about how AI might be useful in the real world, thus helping AI programmers decide where to effectively focus energies.
AI analyzes comments and discussion
Many web apps and sites (including Facebook, YouTube, and Stack Exchange) analyze posted content using what some people call AI algorithms.
Presuming this is used also for comments on posts on sites such as these...
AI may take many factors into consideration, viz buzz words (type 'COVID' on a post and watch the info-notice pop up), profanities, bigotous phrases, etc.
I'm curious about the results if AI analyzed just the grammar of a history of comments that were deemed ""abusive"" juxtaposed against a history that was not deemed abusive.
Why ask?
Creative-analytical thinkers like Steven Levitt (viz Freakonomics) and Malcolm Gladwell like to discuss counter-intuitive findings from research. Levitt says that this is ""economics"" (nothing to do with money). Even the video game League of Legends has stats on how often certain gaming choices (items, champion, etc) win and lose. But, we need the data. I want to know if ""grammar"" is a good place to dig.
I would be curious if there were any grammar patterns that might indicate abuse, as might be found by AI research from past comments. Pardon the grammar lingo, but for example:

Complex subjects
Imperatives
Subjunctives
Passives
Verbal pauses (Bothering to type out ""Um..."" in ""Um... No."")
Direct Objects vs Indirect Objects vs ""Raised Objects""
Prepositions

...Say research finds that comments containing ""at"" were 60% more likely to be flagged as abusive. That would be great content to ask on Linguistics to see if there were other patterns to analyze.
I don't know what should be analyzed, nor do I know what all grammatical categories would go into such an analysis. That would be my next Question for Linguistics.
Scope of my question
I'm trying to ask for open-ended answers, not binary (yes/no) answers, while also narrowing scope. So, let me put it this way...
Can AI be used to analyse abusive vs non-abusive discussions through grammar patterns and categories?
If so, which models or algorithms can be used to achieve that? References are also appreciated.
","['natural-language-processing', 'reference-request', 'model-request', 'natural-language-understanding']","There are various levels and types of analysis one might seek, depending on the output desired. At perhaps the simplest level, text classification could place the text into binary buckets of pass and fail. This level is similar to basic spam filtering. More complex classification could involve more buckets and or give a numerical rating.A perhaps closer term for the task in question is sentiment analysis. From Wikipedia:Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.Natural language processing (NLP), itself, can be sub-divided into three broad approaches: symbolic, statistical, and neural. These days, and particularly in the context of AI, the last option is likely given the most attention.The main reasons for choosing neural networks over traditional hard-coded logic are flexibility, often less coding, and support for elusive, perhaps heuristic logic that may otherwise be difficult to systemise. Conventional statistical methods may resolve some complexity over manual logic, yet possibly at the expense of inferential depth. With deep neural networks, obtaining deeper inference, for better classification and analysis, sometimes can be as simple as training on a network with more parameters, for more neurons.Neural-based language models like GPT-3 and GPT-NeoX (third-party playground available) highlight the functional yet elusive reasoning sometimes expressed by trained neural networks. Their logic is often heuristic and hidden, a phenomenon termed black box. For those new to language models, I recommend learning more (ie. by searching ""GPT-3"" on YouTube) and perhaps using a playground, like the one linked above.No doubt, the specific details of implementation can vary greatly. One could, for example, run the input through a grammar classifier (neural or otherwise), followed by feeding the resultant parts of speech (plus tense) into a sentiment analysis network. Doing so could hypothetically be cheaper to train, possibly, though not necessarily, at the expense of quality. As neural computing gets cheaper, the trend is simply using a bigger network with more training data, hopefully giving a better result with minimal coding.The amount of time required for development depends on available data -- especially tagged data -- and computing. A person with experience in sentiment analysis could possibly design or adapt a network in days. But preparing the training data could be a challenge, particularly for a large network. If you had access to, say, the set of flagged posts for a popular site, the data may already have the desired level of tagging and already be enough for training.Indeed, if insufficient data is available, a grammar pre-processor, as described earlier, may help. In general, less data means smaller networks and more coding. With the right skills, off-the-shelf pre-trained language models, like those mentioned earlier, may be able to support or provide text classification, perhaps even for this purpose.Whether done manually or though AI, once the posts have been categorised, many options exist for identifying trends in grammar and word usage.At its simplest, basic word frequency could be analysed using conventional coding. For example, a sorted word-frequency mapping could be made and compared between each population of tagged posts, with the most frequent words first.If context-sensitive properties, such as grammatical mood, tense, or part of speech are of interest, then a grammar pre-processor is likely in order. Such a parser could be neural or conventional. The resulting output could be hierarchical, a simple array, or perhaps a map. Analysis of the results could be simple, like described above for word frequency; or the results could be trained through a neural network to establish more complex inference.Since language is generally one-dimensionally arranged, sequential pattern mining may be applied in search of unknown patterns. Presumably each population of tagged posts would be analysed separately, followed by taking the difference between the result sets. Those patterns of greatest discrepancy might be insightful. This step could be applied to either the raw tokens (ie. words), or the parsed grammar output.Other options include cluster analysis and self-organizing map, although special pre-processing, from a broad set of possibilities, may be recommended for these.Assuming the set of pattern types is open-ended, the set of programming paths is rather unbound."
How to convert prediction probabilities of 2D images (initially 3D image) to 3D image predictions?,"
Classification: binary
Model: CNN (ResNet50V2)
During our research we've had 91x109x91 images (3-dimensional). We've used 2D CNN to train and evaluate our images and make predictions on labelled cases, thus we had to convert 3D to 2D this way (slices (n*x), y, z, #color_channels):
maindata = maindata.reshape(n * 91, 109, 91, 3)

What we did here effectively is merge a number of images (n) with dimension x (in our case 91). So we basically trained our model on slices (2D images) of this 3D image. Thus far everything was good. We've received great results.
However, now we need to create prediction probabilities of n images (3D images) and not n x 91 slices (2D images).
Can we take the prediction probabilities we've received (2D) and for every 91 prediction probabilities calculate the mean or is there a better way of interpreting 2D results to 3D images?
Both train (labelled) and test (unlabelled) data are shaped the same way, so the only part missing is interpretation to 3D.
","['convolutional-neural-networks', 'classification', 'image-recognition', 'probability', '3d-convolution']",
"Are any AI systems available, or in development, for finding and analysing fallacious inference in natural language text?","
Poor reasoning, and ignorance in general, is the source of a lot of suffering and evil. Covertly erroneous logic is often used in manipulation. And much of this broken thought is being used directly in the training of AI.
There has been talk of, and development in, fact-checking, such as for language transformers. But what about reasoning?
The function in mind is specifically being able to process a potentially large text body, analysing all logic and implied relations for fallacy and other misleading reasoning. Perhaps shades of colour could indicate level of error. A bonus would be output listing and explaining the mistakes, maybe like compiler errors -- ""fallacy x between premise y and conclusion z"".
Are any AI systems available, or in development, for finding and analysing fallacious inference in natural language text?
","['natural-language-processing', 'reference-request']",
Do Quo et al (2013) perform backpropagation between layers?,"
Le et al. 2013's non-weight sharing CNN has inspired me to ask two questions on this site previously.
When training the three-layer autoencoder, do they compute dL/dW (where L is equation 1) independently in each layer or perform backpropagation from one layer to another?
","['autoencoders', 'multilayer-perceptrons']",
Where can I find Norvig's version of the pseudocode for the A* search algorithm?,"
Can anybody point me to a link to Peter Norvig's version of the A* pseudocode. I've googled it interminably but found nothing.
It's the version that uses the Unexplored/Frontier/Explored data structures. I've looked at other versions (one from CMU), but I can't see how to get from that version to Norvig's version, and it's not in any of the versions of his text!
He wrote it down longhand in the online (Norvig & Thrun) lectures for his Stanford AI class. Those YouTube videos have since been taken down, and I don't have an old link to find it in the Wayback Machine. If anybody has the links to the whole suite of lectures on the Wayback Machine, that would be ideal.
But if a typewritten version were ever posted somewhere on the net (pdf, ppt, html, etc.), that would also be perfect.
","['reference-request', 'search', 'a-star', 'pseudocode']","The 2nd and 3rd editions of the Russell and Norvig's book (which you can find on the web) do not have a figure for the pseudocode of A*.However, the graph search version of A* is equal to the pseudocode of uniform-cost search (UCS), which they provide in figure 3.14 (page 94, section 3.4.2.), but, rather than using $g$ (the cost of the shortest path from the start node to $n$), you use $f(n) = g(n) + h(n)$, where $h$ is an estimate of the shortest path to the goal from $n$, which should be admissible in order for A* to be optimal. See this answer.You can find a screenshot of UCS below.In the pseudocode, PATH-COST is $g(n)$. When they pop from the frontier, you should pop according to $f(n)$ in A*. See also section 3.5 of the 3rd edition of the book."
How do neural networks play chess?,"
I have been spending a few days trying to wrap my head around how and why neural networks are used to play chess.
Although I know very little about how the game of chess works, I can understand the following idea. Theoretically, we could make a ""tree"" that included every possible outcome of a chess game. Through knowledge provided by chess experts, we could identify how ""favorable"" certain parts of this tree are compared to other parts of the tree. We could also use this tree to ""rank"" optimal chess moves based on how the chess board appears in the current turn (e.g. which pieces you and your opponent have left and where these pieces are situated).
The problem is, this tree would be so enormous that it would be impossible to create, store and ""search"" (e.g. with the MinMax algorithm):

I understand that perhaps this tree can be created using data to limit the size of the tree based on scenarios that are more likely to appear compared to all possible scenarios. For example, if a player wanted they could spend the whole game aimlessly shifting their ""Rook"" back and forth - theoretically, this outcome could occur but no player (in their sane mind) would ever do this. Thus, the tree could be constructed using actual data from millions of chess games. This for example could tell us : Based on historical data and given the current setup of the chess board, 21% of games were won when the immediate next move involved moving the Queen to ""F5"" vs only 3% of games were won when the immediate next move involved moving the Knight to ""F5"". I suppose at each move, the data based tree could be queried to rank the optimality of each next move by checking the proportion of ""terminal nodes"" that resulted in wins for each possible move given the current chess board.
However, I still see 2 problems with this approach:

It is possible that we might run into a scenario(s) that never occurred within the historical data, rendering the tree useless in this scenario

This tree still might be too large to efficiently store and query.


This is probably why neural networks are being used to play chess - I tried to do some readings about this topic, but I can't seem to fully understand it. In this case, what exactly would the neural network use as a loss function? I don't see how the loss function in this case is continuous, and thus how could gradient descent be used on such a loss function?
Could someone please recommend some sources (e.g. YouTube Videos, Blogs, etc.) that show how a neural network can be used to play chess.
","['neural-networks', 'reference-request', 'chess', 'board-games']",
Which approach can I use to generate forged signatures from real ones?,"
I am in internship period and I'm working on a signature verification problem.
This process needs real and forged signatures. All I have are the real signatures (like 30 signatures per person), and I have to generate fake signatures (forged) from those real ones in order to augment data and balance it for the main purpose, which is signature verification.
During my search, I found amazing videos explaining GANs and conditional GANs, pix2pix, etc.
I'm wondering if you have an idea on how to fit this model or an other approach to generate forged signature close to the real one but not as much as real.
Here is an example to clear the view a little bit. Those are the 3 images real for person 8 (img1,9,17)



and those are the forged one :



Those examples are token from cedar dataset.
","['tensorflow', 'keras', 'generative-adversarial-networks', 'pytorch', 'model-request']","You could indeed try pix2pix or cycle gans. Considering that you already found an available dataset you could just clone the original CycleGAN repo, plug the dataset in and see how it does. Of course at the end you'll have 2 generators, one that map real signatures to forged ones and vice versa, but that's how image translation works, at the end of the training you can just discard the second one."
What is intended for 1x1 convolution for input images?,"
I'm reading this about Self-Attention GANs : https://sthalles.github.io/advanced_gans/
I'm trying to better frame what is intended about 1x1convolution for input images. Does this simply mean that we produce feature maps by multiplying a scalar filter to each single pixel of the image? It's not clear since in the reported picture the 1x1conv part resembles a vector.

","['convolutional-neural-networks', 'generative-adversarial-networks']","1x1 convolutions is a very simple trick generalized by the Inception module published by Google in 2014 in the famous paper Going Deeper with Convolutions.The most common use case is to modify the output channels of the input feature maps. This is mostly used when you have a net architecture with multiple branches that needs to be aggregated into one result (such as in your image or such as in the Inception module).Here is an example:In plain english: 1x1 convolutions modify the output feature maps channels $C$ without altering the resolution $H \times W$.When is this useful? For when you need to expand or shrink the feature maps channels. Examples: Inception, SE Blocks, Bottlenecks Blocks, Detectors Head (RetinaHead)...Here is a more in depth article"
Training strategy on continuous video stream with CNN-LSTM,"
I have videos that are each about 30-40 mins long. With the first 5-10 mins (at 60fps, can be down-sampled to 5fps) are one type of activity that would be categorized by label-1 and the rest of the video as label-2. I started off by using CNN-LSTM to do this prediction (Resnet-50 + LSTM + FC-classifier).
Using Pytorch...
For training my initial approach was to treat this as an activity classification task. So, I split my videos into smaller segments with each segment having a label.
video1.mp4 (5 mins) --> label-1
Split into 30 seconds -->
video1_0001.mp4 --> label-1
.
.
.
video1_0010.mp4 --> label-1
But, with this strategy even after 100 epochs the network does not train. I can at the most fit about 40 frames on the 2-GPUs, but a 30 second segment of video @5fps has about 150 frames. Any further subsampling seems to not capture the essence of the video segment.
I also tried training without shuffling and with single thread, so a single stream is loaded continuously. But perhaps its not the right strategy.
I wanted to request some help on how to tackle the problem. I would really appreciate some insights into a training strategy for this problem.

Is using CNN-LSTM the right strategy here?

=== Update ===
After reading a few other posts on similar topics,

data stream for a LSTM
action recognition on video stream

I feel that to get the network to see a larger part of the sequence, I will have to use more GPUs or resize images. However, since the pretrained Resnet accepts 224x224, I will need more GPUs. But I am curious; is there another strategy? Because the question could also be about what is the ideal segment length that would enable the network to learn.
From my perception, a 30 second video sampled @5fps at the bare minimum captures the context. From observation so far, going below this number hasn't allowed the network to learn.
","['convolutional-neural-networks', 'computer-vision', 'long-short-term-memory', 'video-classification']",
What's the benefit of repeating an action for a consecutive number of time steps?,"
What's the benefit of repeating an action for a consecutive number of time steps? Is there a way to tell if an agent in a given environment might perform better from repeated actions?
I came across an action repeat hyperparameter used in the experiments of this paper (Table 11.) and was wondering what the objective of this hyperparameter might be. This happens during training where the action selected by a policy at step $t$ is executed for a repeat number of timesteps $t_r$ after which the policy is then queried for another action at the current state $s_{t + t_r}$.
","['reinforcement-learning', 'policies', 'action-spaces']","A primary reason to repeat an action of a series of timesteps is that your environment may require more than one timestep to process the timestep.  Said another way, changing the action every timestep might prove to hamper the overall learning process significantly.As an example, consider using RL with something from OpenAI Gym.  Imagine that you are trying to train an agent to play something like Mario Brothers.  If your agent takes a different random action at every timestep, how long might it take the agent to realize that it needs to keep sending ""Right"" to improve the x position?  It might be better to configure your agent such that each action is repeated for, say, four timesteps, allowing the action to begin to take on real meaning in the environment."
Does Using the Same Background for Binary Classification Improve Model Accuracy?,"
I am training a CNN that detects if a there is a pot of boiling water vs if there is a pot of boiling water with pasta inside. My hypothesis is that having the same background for both a positive and negative class image will improve model accuracy because it will force it to look exclusively at the foreground for hints. Is this hypothesis reasonable?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'image-recognition', 'binary-classification']","You're not wrong, but you're also missing a couple of risky aspects that makes the idea not the best.The part you're right are:The points you should pay attention to are:"
What is exactly sparse annotation?,"
What is exactly sparse annotation? Is it different from labeling images?
I've been reading a paper about vessel segmentation and have some issues understanding this part.
","['deep-learning', 'convolutional-neural-networks', 'deep-neural-networks', 'image-processing', 'image-segmentation']","in computer vision sparse annotations are images for which only a bunch of pixels are annotated. The term sparse in fact is used as in linear algebra, where a sparse matrix indicate a matrix composed mostly of zeros (in this case zeros meaning no label given).The idea is that is takes much less time to label just a bunch of pixels rather than entire images. This annotations of course can't be use to train models on fully supervised learning tasks, since classic supervised losses require dense annotations.So this type of annotations are instead used in weakly supervised learning, a combination of unsupervised techniques combined with constrains coming from the sparse annotations, or in active learning set ups, as a staring point to generate automatically dense annotations from sparse ones, to then only use the automatically generate dense annotations for a classic supervised training."
How to calculate uncertainty in Deep Ensembles for Reinforcement Learning?,"
Lets take the following example: I must predict the return (Q-values) of x state-action pairs using an ensemble of m models. Using NumPy I could have the following for x = 5 and m = 3:
 >>> predictions = np.random.rand(3, 1, 5)
 
 [[[0.22668968 0.58857404 0.49572979 0.68034031 0.96522052]]

 [[0.90452081 0.07554403 0.62139326 0.6269648  0.78426295]]

 [[0.14154026 0.75292144 0.99831914 0.7584285  0.69479723]]]

Thus, for each possible action we have the following prediction considering the set of models:
>> actions_out = [q[0] for q in predictions]
>> actions_out = [list(a) for a in zip(*actions_out)]
>>
[
[0.22668968082539054, 0.9045208066488987, 0.14154025891848865], 
[0.5885740401748317, 0.07554403461136683, 0.7529214398937515],
[0.4957297945825573, 0.6213932636399634, 0.998319138313377],
[0.6803403139829055, 0.6269648017308974, 0.7584284958713308],
[0.9652205174041535, 0.7842629542761801, 0.6947972303000536]
]

Where for example the actions_out[0] = [0.22668968082539054, 0.9045208066488987, 0.14154025891848865] is the prediction of 3 models for the action 1 (index = 0).
The question is: to calculate the variance of those values (the disagreement or uncertainty between models), the following is correct?
variance = np.var(actions_out, axis = 1)
avg_variance = np.average(variance)

Does this average capture the disagreement between the models?
","['reinforcement-learning', 'deep-rl', 'ensemble-learning', 'uncertainty-quantification']",
"Is AI able to detect major changes in pairs of images while ignoring minor changes (due to tree crown growth, color and perspectiv disstortions)?","
I'm starting to get involved into machine learning but still have some troubles to select the approriate tool or algorithm.
My basic task is to compare remotly sensed images of individual trees at two time steps (e.g. 2012 and 2016). Between images I'd like to determine if any individual tree was cut or not.
Problems that arise are different colors, slightly different shapes (due to lateral growth of tree crowns) and distortions of perspective (due to off-nadir images, meaning that the position of the plane in relation to the tree differs between both time steps).
When I determine by ""hand""/eye if trees were cut, I heavily rely on the shadows of trees facing to the upper left corner.
Here are some images to illustrate my problem (left image: 2012, right image: 2016):
The tree in the image below remains, but the colors differs and the shape (perimeter) differs slightly.

Here, the tree in the image below remains as well but due to distortions of persective both images look (from a machine point of view) quite different.

The tree in image below was cut and the area around was converted to urban area/ buildings.

And finally, the tree in the image below was cut but the understorey and accompanying vegetation remains.

I'll have to do this comparison many thousand times - therefor I'd like to train an neural network.
My focus of interest are those trees that were cut between both images. I'd tried to use the NDVI value (""greennes of vegetation"") to detect major changes but this approach worked only for some cases.
During my research I found many image change detection papers using CNNs but they seemed to be focussed on more substantial changes like grassland conversion into buildings or deforestation ...
Some additional research lead me to face verification algorithms that in theory might fit:
I have two known images to compare and like human faces there might be minor changes (perspective, haircut ...) but the verification still works.
Is this approach possible or do I miss some important tool or is this kind of tree verification even to difficult???
Thank you for your time and responses!
","['convolutional-neural-networks', 'image-recognition']",
Musical notes interpretation [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



Musical notes
Musical notes videos
Piano
Can AI, Machine learning, Data science, Computer vision, image processing technologies assist in interpreting musical notes ?
Input dataset : Musical notes
Output : Sound played.
A python program which can assist in interpreting or converting text to speech ?.
The text inputted is Musical notes captured as a image by camera as input device.
","['machine-learning', 'image-recognition', 'ai-design', 'image-processing', 'data-science']","AI/ML can solve the task described, a solution is as below:"
Is possible to perform an early goal test with A* using a consistent heuristic?,"
I have a doubt about A* goal test.
As far as I know an early goal test is performed when a node (representing a certain state) is inserted in the frontier or, to conform with AIMA terminology, when is reached/generated.
Quoting AIMA 4ed:

(...) with a consistent heuristic, the first time we reach a state it will be on an optimal path, so we never have to re-add a state to the frontier, and never have to change an entry in reached.

With this piece of information I suppose that an early goal test is totally safe, but I wasn't able to find anything about it on the internet to confirm my hypothesis, so my second guess is that I am wrong about it.
Am I wrong? Did I misunderstood something?
","['heuristics', 'a-star', 'consistent-heuristic']",
What is the state of the art concerning autoencoder that connect 2 images that are not similar but are physicaly related?,"
I am currently working on an autoencoder that connect two images. The first one can be seen as the electron flow and the second one is the electrostatic potential seen by the electrons. Long story short, the neural network connects those pairs of images that are very different but related by physical law. I have already something working, but I have a hard time trying to find similar kinds of application of neural networks. Most of the time an autoencoder connect images that are very similar (denoising for example). My two questions are the following:

Are you aware of other similar application of autoencoder?

What methods can I use to explain the working procedure of my neural networks? (In the literature the methods concerne mainly classification tasks.)


","['autoencoders', 'explainable-ai']",
"What does it mean by ""gradient flow"" in the context of neural networks?","
Several research papers and textbooks (e.g. this) contain the phrase ""gradient flow"" in the context of neural networks.
I am confused about whether it has any rigorous and formal way of understanding or not. What is the flow referring to here?
","['deep-learning', 'terminology', 'math', 'definitions', 'gradient']",
What is 'eligibility' in intuitive terms in TD($\lambda$) learning?,"
I am watching the lecture from Brown University (in udemy) and I am in the portion of Temporal Difference Learning.
In the pseudocode/algorithm of TD(1) (seen in the screenshot below), we initialise the eligibility $e(s) =0$ for all states. Later on, we decay this eligibility by a factor $\gamma$.
My question is, what does 'eligibility' mean in an intuitive sense? Previous dynamic programming algorithms (value iteration and policy iteration) do not have this 'eligibility' concept. Why is it here in TD? Is it because we are effectively sampling episodes here (unlike in the previous when we test all states and all possible actions?)
Insights welcome.

","['reinforcement-learning', 'temporal-difference-methods', 'eligibility-traces', 'td-lambda']",
How is the output of the Generator in a GAN corrected?,"
If the Generator in a GAN is taking a matrix of size WxH of noise to generate a WxH sized output image, and the Discriminator classifies the output as fake, how is that information back-propagated through the generator?
How is the error in classification attributed to individual ""pixels"" of the generators generated image? Is the error divided by the number of pixels?
",['generative-adversarial-networks'],
what does the OpenAI ALE/Breakout-RAM-V5 observation return [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I haven't been able to understand the output that OpenAI gym return for observation from this snippet
env = gym.make('ALE/Breakout-ram-v5', render_mode='human', obs_type='grayscale')
obv = env.reset()
print(obv)


from the OpenAI documentation it suggest it return the pixel data from camera, I am guessing that it a the game state representation at a given step. (I might be wrong here, I am still new to OpenAI)
Assuming if it is the pixel data from camera, How can I retrieve information like where the paddle position, the ball velocity (incl. direction) such that it satisfy Markov Properties for Q-Learning
","['reinforcement-learning', 'q-learning', 'open-ai', 'gym']",Founded Documentation on Mapping Annotation: Mapping of RAM indexes to semantic state variables from mila-iqiahttps://github.com/mila-iqia/atari-representation-learning/blob/master/atariari/benchmark/ram_annotations.py
Why should data batches in a neural network have an equal size?,"
Why should data batches in a neural network have an equal size?
I have seen some recent research works on making the batch size dynamic, but still, I can't find an answer to my question.
","['neural-networks', 'hyper-parameters', 'batch-size']",
Having the negative cases in the same batch vs. shuffling the dataset,"
I am working on a model for an NLP task. The model encodes the text and has a regression output layer.
In this task, from each instance (positive), I create several negative cases using a specific technique and I merge them with their positive corresponding ones in a data split (training/val/test). After that, I shuffle the data split.
I was thinking of the following: Isn't better to keep the negative instances with their corresponding positive ones in the same batch instead of shuffling the data?
Is there an answer to this question? does it depend on the task?
","['neural-networks', 'python', 'datasets', 'batch-learning']",
Could it make any sense to choose a larger dimension for the latent space of the VAE with respect to the original input?,"
Could it make any sense to choose a larger dimension for the latent space of the VAE with respect to the original input?
For example, we may want to learn how to reconstruct a relatively low-dimensional input (let's say $20$ dimensions), then could I define my encoder and decoder to have $64,256,512...$ hidden neurons before bringing back the reconstruction?
EDIT:
Well I've thought about that and I think it would still be reasonable as in latent-variable models we are actually assuming that our original observations are generated from unseen 'hidden' variables. And (I think) the lower dimension of the latent space is only assumed for an original dimensionality-reduction purpose.
","['variational-autoencoder', 'hyper-parameters', 'latent-variable']",
Why is an action-independent baseline required to reduce variance?,"
I'm learning policy gradient methods. I encountered the REINFORCE algorithm with variance reduction with a baseline. I see we can use a constant or state-dependent function (e.g value function) but not an action-value function as a baseline.
Is there any intuition behind this?
I saw some papers claimed that an action-dependent baseline improves the performance compared to baselines that use only state information. One example is: Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines (2018) by Cathy Wu et al.
","['reinforcement-learning', 'policy-gradients', 'actor-critic-methods', 'reinforce']",
Deep Q-Learning Model Effectiveness Improves then Crashes,"
I am implementing a Deep Q-Learning Algorithm. The model appears to improve but after awhile it just crashes and does just as well as if an agent was making random decisions. Shouldn't the behavior get better and better as time goes on? I tried training 3 times and the same thing happened each time. Letting the model run longer never saw the model improve again and basically stayed random. I'm new to data science/machine learning and if someone could point me towards some resources to prevent this behavior I would very much appriciate it.
For some more context, I'm trying to solve the cart pole problem (inverted pendulum) using Q-Learning. The graph I've included below shows how long the model is able to balance the pendulum. The x-axis represents the episodes and the y-axis represents time balanced.

EDIT 1:
When the graph was generated, this is how the model was created. I never set a learning rate.
model = keras.models.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape=(4,)))
model.add(tf.keras.layers.Dense(64))
model.add(tf.keras.layers.Dense(32))
model.add(tf.keras.layers.Dense(CartEnv.NUM_ACTIONS))
    
model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
return model

I've since changed it to this.
 model = keras.models.Sequential()
 model.add(tf.keras.layers.InputLayer(input_shape=(4,)))
 model.add(tf.keras.layers.Dense(64))
 model.add(tf.keras.layers.Dense(32))
 model.add(tf.keras.layers.Dense(CartEnv.NUM_ACTIONS))

opt = tf.keras.optimizers.Adam(LR)        # LR = 0.01
model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])
return model

I have a 'target model' that gets updated every 5 'episodes.' An episode ends after 15s or when the angle is greater than 45 degrees.
Since I've changed it to use the tf.keras.optimizers.Adam optimizer, the model progress looks very noisy. Since changing the optimizer, when I evaluate it while training, sometimes the agent will be very good then drop the to very bad the next time it is evaluated. (The agent is evaluated after training on 20 episodes. For an evaluation, the agent runs through 10 episodes and the time is averaged). Here is a plot of the training progress. I've tried changing the learning rate to be lower but the model doesn't appear to improve at all.

Would an evolutionary type algorithm be better because it takes the best of each generation? I feel like that wouldn't allow a model to drop like it does now. However, I would still like to know why what I'm doing now isn't working.
EDIT 2:
I've since changed things to use a learning rate of 0.00003 (3e-5) as suggested in the comments. I've also changed the target model to update every 1000 episodes. I'm still finidng similar results. The training progress graph is below (like before it has episodes on the x-axis and average time survived on the y-axis). I am training the model on every step of the episode, should I only be training after an episode completes?

The green vertical lines represent when the target model was updated. The red horizontal lines represent the min and max times. Ignore the x-axis. Model progress was recorded every 20 episodes so the x-axis should be scaled by 20.
EDIT 3:
I'm solving the cart pole problem. I want to reward behavior when the pole's angle is closer to upright (theta=0). I also want to reward behavior when the cart itself is closer to the center (x=0). I take away 150 from the reward when it tips over.
def get_reward(self):
    score = 0.0
    fail = False
    complete = False

    if abs(self.cart.theta) > CartEnv.THETA_MAX: # THETA_MAX = 45 degrees or pi/4 radians.
        fail = True
        complete = True
        score -= 150

    if not fail and self.time >= 15:
        complete = True

    # Take away score further from center
    score += 30 - (abs(self.cart.theta) * RAD_TO_DEG) # [30, -15]
    score += .5 * max(10 - abs(self.cart.x), -20) # [5, -10]

    return (score, complete)

","['reinforcement-learning', 'machine-learning', 'q-learning', 'dqn']","You report that your model is configured as follows:It is important to note that the TensorFlow/Keras documentation reports regarding the activation keyword parameter:[The activation parameter allows you to specify the] Activation function to use. If you don't specify anything, no activation is applied (ie. ""linear"" activation: a(x) = x).Currently, your network is using the default of None, which is equivalent to linear.  Having multiple layers but only using a linear activation is akin to performing multiple linear transformations in series; the entire series is always collapsible to a single transformation matrix.In other words, since all of your layers are linear, the network does not really gain complexity by adding additional layers.  In effect you have only one hidden layer.You might try simply adding a non-linear activation.  Perhaps:"
Why does importance sampling ratio start and end one step later in off-policy SARSA given in Sutton-Barto's RL book?,"
In Sutton & Barto's RL book (page 149) they say:

Sarsa update can be completely replaced by a simple off-policy form
$Q_{t+n}(S_t,A_t)=Q_{t+n−1}(S_t,A_t) + \rho_{t+1:t+n} [G_{t:t+n} −  Q_{t+n−1}(S_t,A_t)]$
for $0 \leq t < T$. Note that the importance sampling ratio here starts and ends one step later than for n-step TD (7.9). This is because here we are updating a state–action pair. We do not have to care how likely we were to select the action; now that we have selected it we want to learn fully from what happens, with importance sampling only for subsequent actions.

The explanation for importance sampling ratio to start and end one step later is not clear for me. I would be very happy if one can provide a simple and easy-to-understand explanation (a visual explanation through node transitions would be perfect).
","['reinforcement-learning', 'off-policy-methods', 'sutton-barto', 'sarsa', 'importance-sampling']",
MLP or RNN for Regression of Smooth Function (No Time Data)?,"
My Problem consists of Input sequences in the form of $x=[B,z]$ and one output $y_i$ for each data point $x_i=[B,z_i]$. For one sequence/dataset $B$ is a constant, whereas $z$ is continously between 0 and 5.
In the moment I just train a standard MLP on this. However, I wonder if I can ""help"" my neural network because I know, that $y_i$ is a smooth function of $z_i$. This seems analog to a time series problem, where I computate my solution at $y_i(t=0)$ and use this information for the prediction of $y_i(t=1)$.
Am I right with this assumption? Can I use RNN or a combination of RNN with MLP for this?
","['neural-networks', 'recurrent-neural-networks', 'time-series']",
Is sklearn using both a threshold and a bias term? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



Reading this Can a neuron have both a bias and a threshold? has confused me, as it appears to be more common to use a threshold of 0 when using bias. But reading this https://stackoverflow.com/questions/19984957/scikit-learn-predict-default-threshold indicates that the threshold is 0.5.
So my question is, is sklearn using both a threshold and a bias term ?
","['scikit-learn', 'single-layer-perceptron']","The quick answer is yes. The implementation of sklearn use both a bias for each weight and a final threshold, by default set to 0.5It sounds to me though that you don't have a clear idea about the purpose of the threshold in the first place, so let's elaborate more on that.A perceptron, and any other type of model, return continuous values. For some tasks (e.g. regression) we can compute error directly on these continuous values, but for other tasks (e.g. classification) we need to convert these values into discrete values, usually binary, which tell us if the model is predicting a yes or a no for a specific node. The threshold serves precisely for this purpose. We apply it to the very last probabilities returned by a model to convert them into discrete outputs (and I stress out probabilities, not logits, so you need to apply first either sigmoid of softmax).The only moment in which is reasonable to change the threshold is after training. And the reason to do so is that we want to test if there are values different from 0.5 that lead to better metrics score. Usually this is performed using the ROC curve. i.e. we compute the amount of false positives an false negatives for different thresholds and we check what threshold lead to the highest AUC score (the red one in the plot, which maximize the ROC area).But there is no point in using a different threshold from 0.5 during training. And there is no point in associating multiple threshold to each bias in a model. The only reason why the threshold is used is to discretize continuous values ad turn them into final predictions.
"
Why $ t=τ+n-1$ instead of $t=τ+n$ in n-step TD?,"
If $\tau$ is the time, whose state’s estimate is being updated, and $t$ is the current time, then, in n-step TD method, we have $t=\tau+n$ (because we have to wait n-steps, before we can update). However, in the following pseudocode (Sutton & Barto's RL book, page 144), they have: $\tau=t-n+1$ which means $t=\tau+n-1$. What is wrong in my reasoning?

","['reinforcement-learning', 'temporal-difference-methods', 'notation', 'sutton-barto']","The main detail that you are missing is that $t$ does not represent the ""current time step"" throughout the loop, but is just a variable giving a reference to a time step that you are processing.The first statements in the loop are:Take an action according to $\pi(\cdot,S_t)$
Obbserve and store the next reward $R_{t+1}$, and the next state as $S_{t+1}$It is clear that after these two steps that the ""current"" time step is $t+1$. The value of $t$ is then just a reference to a particular time step that all the maths for the other loops is set up to use.This in turn means that because the process has the values of $R_{t+1}$ and $S_{t+1}$, then it is ready to process the estimate for time step $\tau = t - n + 1$.It would not really help to have $t$ one higher throughout the loop, because you would still need offsets allowing you to refer multiple time steps within a single loop. Instead of having the odd extra $+1$ you would see some $-1$s instead. E.g. you would need to take an action according to $\pi(\cdot,S_{t-1})$You could swap the sequence of the two ""If"" blocks and have $\tau = t - n$ as the first statement."
What are the state-of-the-art AI methods to recognize elements on webpages or the purpose of webpage?,"
I'm curious to know about the capabilities of AI today in 2022. I know that AI has become pretty good at recognizing things like objects in photos. But what about when it comes to elements in HTML? Would it be feasible to use AI to determine things like:

Is there a call-to-action? Basically a button or main action that directs the user somewhere. The text in the call to action can obviously contain a variety of different text.
Is there a form on the page for the user to fill out?

The last time I tried running a rendered image of a website through image recognition software, such as Google Vision or Amazon's Rekognition, it didn't detect these things, which didn't surprise me. However, maybe there's a better or alternate way, such as using the source code? But the end goal would be to determine if the page is meant to capture leads, and the form elements are some of the criteria we'd be looking for. Maybe this can also be seen as a categorization type of task too.
As I understand, AI is a broad term. So, if this was a feasible project, I'd also be curious to know what branch of AI would be the correct one to explore.
","['neural-networks', 'reference-request', 'applications', 'state-of-the-art']",
Is there any reason for giving an index to a token based on its frequency in the text?,"
In pre-processing of text, we need to assign a number for each token in a text. Then only we can pass it to a model. In pre-processing of text, we need to assign a number for each token in a text. The paragraph  from this section named Text Preprocessing recommended indexing according to the frequency of the token

The string type of the token is inconvenient to be used by models,
which take numerical inputs. Now let us build a dictionary, often
called vocabulary as well, to map string tokens into numerical indices
starting from 0. To do so, we first count the unique tokens in all the
documents from the training set, namely a corpus, and then assign a
numerical index to each unique token according to its frequency.
Rarely appeared tokens are often removed to reduce the complexity. Any
token that does not exist in the corpus or has been removed is mapped
into a special unknown token “”. We optionally add a list of
reserved tokens, such as “” for padding, “” to present the
beginning for a sequence, and “” for the end of a sequence.

I want to know whether it is necessary to index in accordance with the frequency of token or any unique index serves the purpose?
","['natural-language-processing', 'recurrent-neural-networks', 'sequence-modeling']",
Why do we have $t$ as subscript in $V$ instead of $t+1$ in the expression of $G_{t:t+1}$?,"
In one-step  TD updates,  the target is the first reward plus the discounted estimated value of the next state, which we call the one-step return (page 143 of Sutton & Barto):
$$
G_{t:t+1} \triangleq R_{t+1}+\gamma V_{t}(S_{t+1})
$$
where $V_t: \mathcal{S} \to \mathbb{R} $ is the estimate at $\textbf{time t}$ of $v_{\pi}$.
My question is this: Why do we have $t$ as subscript in $V$ instead of $t+1$ in the expression of $G_{t:t+1}$? Since we are at time $t+1$ where state is $S_{t+1}$, it seems more logical to have $V_{t+1}(S_{t+1})$.
","['reinforcement-learning', 'temporal-difference-methods', 'notation', 'sutton-barto']","TD-learning is based on bootstrapping. The TD target $R_{t+1} + \gamma V_t(S_{t+1})$ describes the immediate reward (random variable), $R_{t+1}$, plus the discounted estimated return (starting from the next state $S_{t+1}$, which is also a random variable), $V_t(S_{t+1})$. The reason why the discounted estimated return of the next state is calculated based on $V_t$ is that we do not know $V_{t+1}$ yet, that is at the current iteration of the algorithm.What we want to do, is to find the optimal $V = V^*$, which would be the same for all timesteps. Then, $V^*_t = V^*_{t+1}$."
Is there a variant of Thompson Sampling that works with variable bandits?,"
Does there exist a variant of TS, such that, while computing the returns of multi-armed bandits, we have the possibility of introducing an extra bandit?
For instance, while we are applying TS to 3 slot machines, we come to know about the existence of a fourth slot machine. Therefore, we'd like to take that machine into account within our algorithm.
","['reinforcement-learning', 'multi-armed-bandits', 'thompson-sampling']",
What is the difference between multi-label and multi-task classification?,"
I am working on a data-set that has multiple labels associated with it (not necessarily independent of each other). During my development, I am confused if I should consider it as a multi-class multilabel data or a multi-class MTL kind of an approach. Is there any fundamental difference between the two?
","['multi-label-classification', 'multi-task-learning']",
How can I interpret the value returned by score(X) method of sklearn.neighbors.KernelDensity?,"
For sklearn.neighbors.KernelDensity, its score(X) method according to the sklearn KDE documentation says:

Compute the log-likelihood of each sample under the model

For 'gaussian' kernel, I have implemented hyper-parameter tuning for the 'bandwidth' parameter using Bayesian-Optimization as follows:
# The input data for which 'bandwidth' needs to be tuned-
data
# (2880, 64)

def kde_hyperopt_eval(bandwidth):
    params = {}
    params['bandwidth'] = bandwidth
    
    # Initialize a KDE model-
    kde_model = KernelDensity(
        kernel = 'gaussian',
        bandwidth = params['bandwidth']
    )
    
    # Train KDE model on training data-
    kde_model.fit(data)
    
    # Compute the total log-likelihood under the model.
    # Returns the log probability.
    '''
    Total log-likelihood of the data in X. This is normalized to be a
    probability density, so the value will be low for high-dimensional
    data.
    '''
    return kde_model.score(data)

optimizer = BayesianOptimization(
    f = kde_hyperopt_eval,
    pbounds = {
        'bandwidth': (0.01, 10)
        }
)

optimizer.maximize(n_iter = 40, init_points = 15)

and I'm getting the result as
|   iter    |  target   | bandwidth |
-------------------------------------
|  1        | -5.644e+0 |  8.527    |
|  2        |  2.142e+0 |  0.3419   |
|  3        | -1.287e+0 |  0.7963   |
|  4        | -5.916e+0 |  9.883    |
|  5        | -3.604e+0 |  2.817    |
|  6        | -5.71e+05 |  8.835    |
|  7        | -5.246e+0 |  6.868    |
|  8        | -4.385e+0 |  4.305    |
|  9        | -5.546e+0 |  8.082    |
|  10       | -5.86e+05 |  9.585    |
|  11       | -5.226e+0 |  6.794    |
|  12       | -5.196e+0 |  6.685    |
|  13       | -9.934e+0 |  0.6771   |
|  14       | -5.766e+0 |  9.111    |
|  15       |  6.816e+0 |  0.2584   |
|  16       |  6.565e+0 |  0.01     |
|  17       |  6.565e+0 |  0.01     |
|  18       |  6.804e+0 |  0.2585   |
|  19       |  6.804e+0 |  0.2585   |
|  20       |  6.804e+0 |  0.2585   |
|  21       |  6.804e+0 |  0.2585   |
|  22       |  6.804e+0 |  0.2585   |
|  23       |  6.804e+0 |  0.2585   |
|  24       |  6.804e+0 |  0.2585   |
|  25       |  6.804e+0 |  0.2585   |
|  26       |  6.804e+0 |  0.2585   |
|  27       |  6.804e+0 |  0.2585   |
|  28       |  6.804e+0 |  0.2585   |
|  29       |  6.804e+0 |  0.2585   |
|  30       |  6.804e+0 |  0.2585   |
|  31       |  6.804e+0 |  0.2585   |
|  32       |  6.804e+0 |  0.2585   |
|  33       |  6.804e+0 |  0.2585   |
|  34       |  6.804e+0 |  0.2585   |
|  35       |  6.804e+0 |  0.2585   |
|  36       |  6.804e+0 |  0.2585   |
|  37       |  6.804e+0 |  0.2585   |
|  38       |  6.804e+0 |  0.2585   |
|  39       |  6.804e+0 |  0.2585   |
|  40       |  6.804e+0 |  0.2585   |
|  41       |  6.804e+0 |  0.2585   |
|  42       |  6.804e+0 |  0.2585   |
|  43       |  6.804e+0 |  0.2585   |
|  44       |  6.804e+0 |  0.2585   |
|  45       |  6.804e+0 |  0.2585   |
|  46       |  6.804e+0 |  0.2585   |
|  47       |  6.804e+0 |  0.2585   |
|  48       |  6.804e+0 |  0.2585   |
|  49       |  6.804e+0 |  0.2585   |
|  50       |  6.804e+0 |  0.2585   |
|  51       |  6.804e+0 |  0.2585   |
|  52       |  6.804e+0 |  0.2585   |
|  53       |  6.804e+0 |  0.2585   |
|  54       |  6.804e+0 |  0.2585   |
|  55       |  6.804e+0 |  0.2585   |
=====================================

How can I interpret this value returned by score(X) method of sklearn.neighbors.KernelDensity?
For example, in case of Mean Squared Error or Mean Absolute Error, smaller means better, for accuracy, higher percentage value is better.
","['hyperparameter-optimization', 'scikit-learn', 'bayesian-optimization', 'density-estimation']","The KernelDensity model learns a probability distribution from the training data. The score reflects how likely it is that any given sample has been drawn from the learned probability distribution.
The higher this number is, the more the given sample matches the distribution.For example: If the sample is x = 0 and your model has learned the standard normal distribution $\mathcal{N}(0, 1)$, the likelihood would be very high. In contrast, if x = 50 the likelihood is low, because the probability to draw 50 from $\mathcal{N}(0, 1)$ is very low.Therefore: The higher the score, the better the model reflects the samples. And in your case, it seems that the model learns a distribution that fits your data best when the bandwidth is 0.2585."
"What does it mean by ""dynamics of a sequence"" mathematically?","
Consider the following paragraph from the topic named sequential models from the textbook titled Dive into Deep Learning

Both cases raise the obvious question of how to generate training
data. One typically uses historical observations to predict the next
observation given the ones up to right now. Obviously we do not expect
time to stand still. However, a common assumption is that while the
specific values of might change, at least the dynamics of the
sequence itself will not. This is reasonable, since novel dynamics
are just that, novel and thus not predictable using data that we have
so far. Statisticians call dynamics that do not change stationary.

Here sequence refers to $x_1, x_2, x_3, \cdots, x_t$. Say the stock price of  a company.
What does it mean rigorously by the dynamics of a sequence in statistics?
","['terminology', 'math', 'sequence-modeling', 'statistical-ai']",
Does it make sense to compare images (samples) with words (features)?,"
Consider the following paragraphs from the introduction of the chapter named Recurrent Neural Networks from the textbook titled Dive into Deep Learning

So far we encountered two types of data: tabular data and image data.
For the latter we designed specialized layers to take advantage of the
regularity in them. In other words, if we were to permute the pixels
in an image, it would be much more difficult to reason about its
content of something that would look much like the background of a
test pattern in the times of analog TV.
Most importantly, so far we tacitly assumed that our data are all
drawn from some distribution, and all the examples are independently
and identically distributed (i.i.d.). Unfortunately, this is not true
for most data. For instance, the words in this paragraph are written
in sequence, and it would be quite difficult to decipher its meaning
if they were permuted randomly. Likewise, image frames in a video, the
audio signal in a conversation, and the browsing behavior on a
website, all follow sequential order. It is thus reasonable to assume
that specialized models for such data will do better at describing
them.

In neural networks, we generally use words: instances, examples, data points to refer to a particular row of a dataset. In general, in the case of CNN, an instance will be an image, and in the case of RNN, an instance will be a sequence of text (maybe a sentence, paragraph, or text).
Every instance contains features: in image data, pixels are generally treated as features and in the case of text data, either characters or words are generally treated as features.
With this context, let me explain my doubt
I have an issue understanding the paragraphs. The issue is the comparison of examples/instances of image data with features of text data. Pixels can be compared to words/characters and images can be compared to sentences or words or paragraphs or text documents based on the context.
In the second paragraph, it is said that the examples (images) are i.i.d. But then the images are compared with words in a paragraph, but words are features, not examples. As the paragraph is not saying that pixels are i.i.d., how can the words in paragraphs be used for comparison?
","['convolutional-neural-networks', 'recurrent-neural-networks', 'books', 'features', 'iid']",
Next Sentence Prediction for 5 sentences using BERT,"
I am given a dataset in which each instance consisting of 5 sentences. The goal is to predict the sequence of numbers which represent the order of these sentences.

For example, given a story:
He went to the store. He found a lamp he liked. He bought the lamp.
Jan decided to get a new lamp. Jan's lamp broke.
your system needs to provide an answer in the following form:
2   3   4   1   0
where the numbers correspond to the zero-based index of each sentence
in the correctly ordered story. So ""2"" for ""He went to the store.""
means that this sentence should come 3rd in the correctly ordered
target story. In This particular example, this order of indices
corresponds to the following target story:
Jan's lamp broke. Jan decided to get a new lamp. He went to the store.
He found a lamp he liked. He bought the lamp.

My initial idea is to extended the NSP algorithm used to train BERT, to 5 sentences somehow. I can't find an efficient way to go about doing so. All suggestions would be appreciated.
Thank you!
","['natural-language-processing', 'recurrent-neural-networks', 'bert']",
Is there a way to inject linear constrains during GAN training?,"
Given that I'm training a generative model, (say a generative adversarial network), and I know that my (real) inputs (let's say vectors $\textbf{x} \in \mathbb{R}^n$) satisfy linear constraints of the form e.g. $a_1\textbf{x}_1 + \dots a_n\textbf{x}_n =0$, where the coefficients are fixed, is there a way to inject this knowledge during training?
","['training', 'generative-adversarial-networks', 'generative-model']","Maybe a bit too trivial to work out of the shelf, but I would try to add a component to the adversarial loss based precisely on the given set of coefficients.Something like:$L_{linear}= \frac{\gamma}{k} \prod_{k}A_{k}\hat{x}$which combined with the adversarial loss (assuming minimax, but any other choice is fine as well) would become:$L(G, D)=E_{x}[log(D(x))] + E_{z}[(log(1 - D(G(z)))] + \frac{\gamma}{k} \prod_{k}A_{k}\hat{x} $where $A_{k}$ is a set of fixed coefficients of an hyperplane, and $\hat{x}$ the vector sampled from the generator. I would use a product operator since the loss should drop to zero when $\hat{x}$ lies in one of the hyperplanes.
Instead $\gamma$ can be used to scale the loss to a value in the range of the generator and discriminator losses."
What are the best practices of adding noise to game-playing bots?,"
I write bots that play card games. From time to time, I add noise to their decisions, mainly for two reasons:

Reduce predictability: In games with hidden information the optimal play is a mix between several actions.
Reduce strength: allows to create several bots on a spectrum of strength.

My first question is: What are the best practices of adding noise to decisions?
I implement noise in two ways: assume each action receives a score $S$.

Each action also receives $\text{noise} \sim \text{Uniform}(0, \text{constant})$.  The action with the highest $S+\text{noise}$ is chosen.
Each action is chosen with probability proportional to its $S$. I.e., $\text{Pr}(i)=\dfrac{S_i+W}{\Sigma_j (S_j)} $, where $W$ is a winner bias, that increases the probability of the best $X$ actions.

What are the pros and cons in the two implementations that I use?
","['reference-request', 'game-ai', 'noise']",
Dimensions of a Transformer model and purpose of masking [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I'm currently studying the Transformer model (Attention is all you need) and after reading it I still have some questions for which I get conflicting answers if I google them:

What exactly are the dimensions of the input to the encoder of a transformer, from what I've seen you can input sentences with dynamic lengths but in the paper it seems like all layers expect the K/Q/V matrices to have dimensions of d_model, d_k or d_v/d_q
Same question for the decoder, what exactly are the input dimensions and how do the attention layers handle dynamic dimensions if its possible
Another question is is masking only used for training where you input the whole sentence as the input to the decoder or does it have another purpose than that (especially in inference)

","['neural-networks', 'machine-learning', 'natural-language-processing', 'transformer']","Transformer networks are great, because they can handle variable length inputs, but they also have limitations, concerning the input size. For example BERT (a transformer based language model) only accepts $N = 512$ input tokens at most. They way transformer models can accept sequences shorter than the $N$, is by padding the input sequence with zeros and masking. When inputting a sequence with length $n \leq N$, the mask $m$ is a vector of size $N$, where $m_i = (i \leq n)$. Say we have a transformer that should translate from English to German which accepts $N = 8$ tokens at most, and we input the sentence ""Today is Tuesday"". Let the words have the following token values:\begin{array} {|r|r|}
\hline \text{[SOS]} & 1 \\ 
\hline \text{Today} & 274  \\ 
\hline \text{is} & 12 \\ 
\hline \text{Tuesday} & 125 \\ 
\hline \text{[EOS]} & 2 \\ 
\hline  \end{array}The first input will always be occupied by the [SOS] token and each sequence is followed by an [EOS] token.
Now the input looks like this:\begin{bmatrix}1\\274\\12\\125\\2\\0\\0\\0\end{bmatrix}And you also supply an attention mask (using integers representing booleans) which looks like this:\begin{bmatrix}1\\1\\1\\1\\1\\0\\0\\0\end{bmatrix}This means, each position can only attend to positions $i$ where $m_i = 1$. Therefore, positions where the mask is 0, do not affect the output of the model.The vector representation of the mask above is the simplified version of the actual attention mask, describing that position at row $i$ can attend to positions at column $j$:\begin{bmatrix}1&1&1&1&1&0&0&0\\1&1&1&1&1&0&0&0\\1&1&1&1&1&0&0&0\\1&1&1&1&1&0&0&0\\1&1&1&1&1&0&0&0\\0&0&0&0&0&0&0&0\\0&0&0&0&0&0&0&0\\0&0&0&0&0&0&0&0\end{bmatrix}The decoder works a bit differently during training and inference.Training:
The decoder attention mask is built, so that each position can not attend to future positions. You can describe this by using a matrix:\begin{bmatrix}1&0&0&0&0&0&0&0\\1&1&0&0&0&0&0&0\\1&1&1&0&0&0&0&0\\1&1&1&1&0&0&0&0\\1&1&1&1&1&0&0&0\\1&1&1&1&1&1&0&0\\1&1&1&1&1&1&1&0\\1&1&1&1&1&1&1&1\end{bmatrix}When using this mask for the decoder, position $i$ can only attend to positions $j, j \leq i$.
For the task of translation from English to German (e.g. ""Today is Tuesday"" -> ""Heute ist Dienstag""), you can provide the model with the entire input sentence and the entire output sentence. The model will make a prediction for each decoder input position, what the next word will be. And by using the mask, no information leaks into earlier positions (when predicting the next token for the word ""ist"", the model can only see ""[SOS] Heute ist"").Inference:
For the inference stage, you would first provide the input sentence for the encoder and for the decoder only the [SOS] token. Now you sample a word from the transformer output (e.g. picking the most likely or sample from the output distribution) at the position of your last decoder input (in this case the position of the [SOS] token, e.g. position 0). Then you do the next inference step, by again providing the input sentence for the encoder, and for the decoder you provide the [SOS] token + the predicted word from the last step. If the previous prediction was ""Heute"", your decoder input now is ""[SOS] Heute"". You just repeat this process, each time providing the so far predicted sentence to the decoder, until the transformer predicts [EOS]."
"How ""Patch Merging"" works in SWIN-Transformers?","
In the SOTA paper: SWIN-Transformers, the authors have tried their best to explain everything clearly. I have got an idea of how it works except the Patch Merging part. I found some blogs and other things explaining this but still I am not able to comprehend how the shape changes and how come the SHAPE of windows are changed at the time when the only thing thy are doing is Concatenating the neighbouring 2x2 patches. Could someone please explain it in Laymen terms or maybe a video link or something intuitive explanation.

This awesome blog explains every detail clearly
Implementation in Pytorch from scratch with explanation
Keras / Tensorflow Implementation

Below are some intuitive images which I am trying to grasp in bits and pieces

","['neural-networks', 'machine-learning', 'deep-learning', 'transformer', 'vision-transformer']",
Can the environment change even without the intervention of the agent in Reinforcement Learning?,"
I'm modeling a problem using Reinforcement Learning (RL). Formally, I have two agents: one of them is the one that I have to program and model, the other one is unpredictable (random). With unpredictable, I mean that I can't even define a set of possible actions for it.
Given that, I thought of modeling the scenario as a single-agent scenario in which the environment changes even without the intervention of my agent. Can I do it?
Reformulating: in RL, should the environment be modified (and then change) only by the agent in a single-agent scenario? Is it possible that even if the agent makes no action the environment changes or should I add some entity (the random agent) to my model?
I tried to find the answer in the book ""Reinforcement Learning: An Introduction"", 2nd edition, by Sutton and Barto, but I found nothing answering the question in the introductory chapters (1 and 3). With ""I found nothing"", I mean that the authors neither say that I can do it, nor that I can't.
","['reinforcement-learning', 'models']",
Is this aggregation of multiple convolutions of the same input a type of attention or dynamic convolution?,"
Are there any examples of people performing multiple convolutions at a single depth and then performing feature max aggregation as a convex combination as a form of ""dynamic convolutions""?
To be more precise: Say you have an input x, and you generate
Y_1 = conv(x) 
Y_2 = conv(x)
Y_3 = conv(x)

Y = torch.cat([Y_1,Y_2,Y_3]) 
Weights = nn.Parameter(torch.rand(1,3)) 
Weights_normalized = nn.softmax(weights) 
Attended_features = torch.matmul(Y, weights_normalized.t())

So, essentially, you are learning a weighting of the feature maps through this averaging procedure.
Some of you may be familiar with the ""Dynamic Convolutions"" paper. I’m just curious if you all would consider this dynamic convolution or attention of feature maps. Have you seen it before?
If the code isn’t clear, this is just taking an optimized linear combination of the convolution algorithm feature maps.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'attention', 'convolutional-layers']",
What is the difference between a greedy policy and an optimal policy?,"
I am struggling to understand what is the difference between an optimal policy and a greedy policy.
Let $F(r_{t+1},s_{t+1}| s_t,a_t)$ be the probability distribution accorting to which, given action $a_t$ in state $s_t$, reward $r_{t+1}$ realizes and the process transitions to a new state $s_{t+1}$. The value function for a generic policy $\pi$ then is
$$v_\pi(s_t) = E_\pi \left[ E_F \left[ r_{t+1} |  s_t,a_t \right]+ \delta E_F \left[v_\pi \left(s_{t+1}\right)| s_t,a_t \right] \right],$$
and the optimal value function is
$$ v_*(s_t) = \max_\pi \, v_\pi(s_t).$$
I would like to know if the optimal value function can also be defined as
$$v_*(s_t) = \max_{a \in A(s_t)} \big\{ E_F \left[ r_{t+1} |  s_t,a \right]+ \delta E_F \left[v_* \left(s_{t+1}\right)| s_t,a \right] \big\},$$
and if not, why. Here, the bellman equation prescribes to behave greedily by always choosing the best action $a \in A(s)$.
To me, behaving greedily and choosing the optimal policy seem equivalent, which confuses me a bit.
","['reinforcement-learning', 'comparison', 'value-functions', 'bellman-equations', 'optimal-policy']","I would like to know if the optimal value function can also be defined as
$$v_*(s_t) = \max_{a \in A(s_t)} \big\{ E_F \left[ r_{t+1} |  s_t,a \right]+ \delta E_F \left[v_* \left(s_{t+1}\right)| s_t,a \right] \big\},$$This looks correct to me, although I am used to different notation, and viewing this after resolving the expectations. From Sutton & Barto, I would write the following:$$v^*(s) = \text{max}_a \sum_{r,s'}p(r,s'|s,a)(r + \gamma v^*(s'))$$which I think matches your equation term-for-term.To me, behaving greedily and chosing the optimal policy seem equivalent, which confuses me a bit.You have to take care with the self-reference to the optimal value function - it occurs on both sides of the Bellman equation.Behaving greedily with respect to an optimal value function is an optimal policy.Behaving greedily with respect to any other value function is a greedy policy, but may not be the optimal policy for that environment.Behaving greedily with respect to a non-optimal value function is  not the policy that the value function is for, and there is no Bellman equation that shows this relationship.Only the optimal policy has Bellman equation that includes $\text{max}_a$. All others must use the more general $v_{\pi}(s) = \sum_a \pi(a|s) \sum_{r,s'}p(r,s'|s,a)(r + \gamma v_{\pi}(s'))$However, a greedy policy over a non-optimal value function is an improvement on the policy that resulted in that value function, which is shown by the policy improvement theorem.If you can solve the Bellman equation for the optimal value function - either as a system of simultaneous equations, or using iteration in Dynamic Programming, then you will have the optimal value function and by implication behaving greedily with respect to it will be an optimal policy. This is the basis for Value Iteration."
What do the square brackets $[ ]$ and $\mid$ mean in $[G_t \mid S_t=s]$?,"
Here is the formula of state-value function in Reinforcement Learning.

What do the square brackets $[ ]$ and $\mid$ mean in $[G_t \mid S_t=s]$? Why use square brackets? Why use $\mid$?
Why do mathematical formulas use all kinds of ambiguous symbols? Not as unique as the symbols of programming language?
","['reinforcement-learning', 'math', 'notation']","The square brackets are part of the expectation operator (i.e. a function of a random variable, which in this case is $G_t$). This is common notation for the expectation. So, it's not $\left[ X \right]$, but rather $\mathbb{E}\left[ \cdot \right]$ or $\mathbb{E}\left[ X \right]$, for some random variable $X$. This notation is similar to the notation of a function $f(\cdot)$ or $f(x)$, but we use square brackets because expectations are taken with respect to random variables, which are actually functions (if this is too confusing, just ignore these details for now).The $\mid$ is also common notation and means that we condition on knowing $S_t = s$ (an event). If you are not familiar with conditional expectations and probability distributions, you can take a look at them e.g. here.So, you can read your formula asthe conditional expectation of $G_t$ (the return, i.e. the sum of future rewards), given that we know that the state at time $t$ is $s$ (i.e. the condition).This is indeed the definition of the state value function."
"Is ""node embedding"" in GNN analogous to ""hidden layer"" of FFN?","
So in Graph Neural Network (GNN) we have node embeddings which is a feature vector that describes the node, is it analogous to hidden layer of Artificial neural network such as feed-forward neural network? After all hidden layer stores set of weights and biases to extract some ""feature"", which begs the question if node embeddings are analogous to a hidden layer?
","['neural-networks', 'feedforward-neural-networks', 'graph-neural-networks', 'embeddings']","Embeddings are vectors. Layers are functions.So, node embeddings (e.g. produced by TransE) are analogous to word embeddings or code embeddings, i.e. they are vector (and lower-dimensional) representations of the original objects, such that similar objects have similar embeddings, for some notion of similarity.Your question is analogous to ""Are convolutional layers analogous to feature maps?"". So, you could use neural networks with layers to learn embeddings, but this does not have to be the case. For example, TransE does not ""really"" use a neural network (i.e. a sequence of layers that compute a non-linear function of the inputs), but it just optimizes the $L_2$ norm of a simple function of the vector representation of the nodes and edges, although it uses gradient descent."
How do we reduce the output dimensions of BERT?,"
The output dimensions of BERT are 768-dimensional, is it possible to reduce them to a lower, custom number? For example, if there is another BERT-based transformer model which has a lower count of ouput dimensions, if it's possible to fine tune BERT on MLM to output lower dimension encodings etc.
And if not, are there any possible workarounds for this issue?
","['machine-learning', 'natural-language-processing', 'transformer', 'bert']",
How to compute circle loss?,"
I have read the paper about circle loss for neural networks. I may have missed something, but I didn't find the way to compute the positive similarity and the negative similarity in the case of circle loss.
The paper gives the equations for AM-SoftMax and Triplet loss, that can be computed from a unified loss function:

For example, in the case of AM-SoftMax, the positive and negative similarities are computed by the following way, w being the weights of the last fully connected layer, and x being the inputs of this last layer:


So my questions are:

Should I compute the similarities for circle loss by the same way the paper compute them for AM-SoftMax ?

Should I compute the forward pass of the last fully connected layer with a standard product: output[j] += weight[i][j] * input[i], followed by a standard SoftMax function ? Or by another way ?


","['neural-networks', 'loss']",
"In CVAE's objective function, why do both terms condition on $\textbf{c}$?","
I don't quite understand why, in Conditional Variational Autoencoder (CVAE), we concatenate a conditioning vector two times, at encoder and decoder respectively.
After we concatenate it once at the beginning, isn't the latent distribution going to already incorporate knowledge about the label?
I know that, from a practical perspective we need to concatenate it at the decoder as well, since we want to be able then to generate new instances associating a specific label, but I'm missing more the mathematical motivations for which we concatenate it two times.
To be more precise, let's consider how the  objective function of CVAE is defined:
$$\mathcal{L}_{CVAE} = \mathbb{E}_\textbf{z}[log \, p_\theta(\textbf{x}|\textbf{z},\textbf{c})] - D_{KL}[q_\phi(\textbf{z}|\textbf{x},\textbf{c})||p(\textbf{z}|\textbf{c})] $$
My question: Why do we condition on $\textbf{c}$ in both terms?
","['objective-functions', 'variational-autoencoder', 'conditional-vae']",
How to reduce loss of Bi-LSTM handwriting recognition model?,"
I am currently training an bi-LSTM model which predicts the handwriting of an individual. I am hitting a current min loss of 1.2 and I think it is not a problem with the model because I copied a model used by this study, which uses the IAM dataset as training data which is what I am currently using.
The way I am looking at this problem is that maybe my image preparation was not the same as said in the research. I prepared the image by changing the size of the words preserving the aspect ratio copied into a blank black canvas on the center. which makes my images normalized to a single size.
As said in the study

The built network takes in a variable width image as an input, where the length of the image is sixty pixels

So I think I am doing something wrong here or just misunderstanding somethings.
Is it possible maybe that this model accepts images with variable widths, or is it just doing what I did?
Also, the study had a loss of under 1 in just 5 epochs. Which I can hardly hit with mine.
And does the image's scale a factor here? If so, can I improve it by introducing data augmentation in my dataset?
A sample of the dataset:

Below, is the summary of my model made in keras
Model: ""handwriting_recognizer""
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 image (InputLayer)             [(None, 360, 60, 1)  0           []                               
                                ]                                                                 
                                                                                                  
 Conv1 (Conv2D)                 (None, 360, 60, 64)  1664        ['image[0][0]']                  
                                                                                                  
 batchnorm1 (BatchNormalization  (None, 360, 60, 64)  256        ['Conv1[0][0]']                  
 )                                                                                                
                                                                                                  
 pool1 (MaxPooling2D)           (None, 180, 30, 64)  0           ['batchnorm1[0][0]']             
                                                                                                  
 Conv2 (Conv2D)                 (None, 180, 30, 64)  36928       ['pool1[0][0]']                  
                                                                                                  
 batchnorm2 (BatchNormalization  (None, 180, 30, 64)  256        ['Conv2[0][0]']                  
 )                                                                                                
                                                                                                  
 pool2 (MaxPooling2D)           (None, 90, 15, 64)   0           ['batchnorm2[0][0]']             
                                                                                                  
 Conv3 (Conv2D)                 (None, 90, 15, 256)  147712      ['pool2[0][0]']                  
                                                                                                  
 Conv4 (Conv2D)                 (None, 90, 15, 256)  590080      ['Conv3[0][0]']                  
                                                                                                  
 batchnorm3 (BatchNormalization  (None, 90, 15, 256)  1024       ['Conv4[0][0]']                  
 )                                                                                                
                                                                                                  
 pool3 (MaxPooling2D)           (None, 45, 7, 256)   0           ['batchnorm3[0][0]']             
                                                                                                  
 Conv5 (Conv2D)                 (None, 45, 7, 512)   1180160     ['pool3[0][0]']                  
                                                                                                  
 Conv6 (Conv2D)                 (None, 45, 7, 512)   2359808     ['Conv5[0][0]']                  
                                                                                                  
 batchnorm4 (BatchNormalization  (None, 45, 7, 512)  2048        ['Conv6[0][0]']                  
 )                                                                                                
                                                                                                  
 pool4 (MaxPooling2D)           (None, 22, 3, 512)   0           ['batchnorm4[0][0]']             
                                                                                                  
 reshape (Reshape)              (None, 22, 1536)     0           ['pool4[0][0]']                  
                                                                                                  
 dense1 (Dense)                 (None, 22, 64)       98368       ['reshape[0][0]']                
                                                                                                  
 dropout (Dropout)              (None, 22, 64)       0           ['dense1[0][0]']                 
                                                                                                  
 bidirectional (Bidirectional)  (None, 22, 256)      197632      ['dropout[0][0]']                
                                                                                                  
 bidirectional_1 (Bidirectional  (None, 22, 256)     394240      ['bidirectional[0][0]']          
 )                                                                                                
                                                                                                  
 bidirectional_2 (Bidirectional  (None, 22, 128)     164352      ['bidirectional_1[0][0]']        
 )                                                                                                
                                                                                                  
 dropout_1 (Dropout)            (None, 22, 128)      0           ['bidirectional_2[0][0]']        
                                                                                                  
 label (InputLayer)             [(None, None)]       0           []                               
                                                                                                  
 dense2 (Dense)                 (None, 22, 85)       10965       ['dropout_1[0][0]']              
                                                                                                  
 ctc_loss (CTCLayer)            (None, 22, 85)       0           ['label[0][0]',                  
                                                                  'dense2[0][0]']                 
                                                                                                  
==================================================================================================
Total params: 5,185,493
Trainable params: 5,183,701
Non-trainable params: 1,792
__________________________________________________________________________________________________

","['papers', 'objective-functions', 'text-detection', 'bidirectional-lstm']",
How to take gradient of log policy when actions are negative?,"
I am currently trying to train BipedalWalker of OpenAI gym by using policy gradient approach. My action space contains 4 continuous actions, all ranging [-1.0, 1.0]. In this case, how can we calculate gradient of log policy when log(negative_action) gives nan value.
","['reinforcement-learning', 'machine-learning', 'deep-rl', 'open-ai']",
How to deal with changing environment in reinforcement learning,"
I am new to RL and I'm currently working on implementing a DQN and DDPG agent for a 2D car parking environment. I want to train my agent so that it can successfully traverse the env and park in the designated goal in the middle.
So, my question is: what are the best practices when training an agent for a changing environment?
In my case, my goal is that a car can randomly spawn every episode anywhere in the dark grey-ish area and always successfully parks in the middle. My problem, in this case, is that if I for example train the agent from only one specified location, it usually won't know how to perform if it's spawned somewhere else.
I also tried making it so that the car starting location gets randomly updated every N step, but unfortunately came to no success.
It may be possible that I've not trained for long enough and with a sufficient number of steps in between the ""position resets"", but I still want to ask if there are any general practices in the cases like this?

","['reinforcement-learning', 'training', 'dqn', 'environment', 'reward-shaping']","I am correct in my understanding that you only provide the agent with the state of the car, i.e. a global x and y position, its angle, velocity, and steering angle?How does the agent know that it is coming closer to the goal if it is not provided with information about where the goal is? Without this observation of the goal, the agent is operating blindly. That explains why it is so difficult for the agent to reach the goal and impossible when you randomize the starting position.If my assumptions are correct, the agent takes random actions which are unlikely to reach the goal, but due to the law of large numbers after enough episodes, the agent will reach the goal at random and it can learn to remember this path if given enough reward. But if you then randomize the starting position the agent cannot apply the knowledge it has learned previously because the sequence of actions to reach the goal would now be different. Essentially, there is no correlation between what goal you want the agent to achieve and your state and action space.To circumvent this problem, I suggest you add additional state information, here are a few suggestions:I also support the suggestion of Elfurd: ""Training in steps can be useful"". This is called curriculum learning and the idea is to present easier training examples to the agent at the beginning of training and steadily increase the difficulty of the environment. In turn, the agent will reach the goal in the easier environments, obtain some reward, and learn. It can then apply what it has learned in the more advanced environments once it progresses through the curriculum.In your environment, this could be as simple as decreasing the size of the gird world in early training. Or you could spawn the agent close to the goal so that the agent is more likely to reach the goal with just a few random actions, alternatively, you can also randomize the goal close to the starting position of the agent if it has to start from a specified position and then increase the distance to where the goal is sampled."
Why do neural network weights have to be between 0 and 1?,"
I've been reading about neural networks for a long time, and I saw that in each one, the weights are always between 0 and 1. Why is this? I tried programming one, but the sigmoid function just seemed like one more thing to make it more complicated, without need. Why couldn't the range of a weight or neuron value be infinite?
","['neural-networks', 'weights', 'artificial-neuron']","Having the weights between 0 and 1 helps accelerate learning. They do not have to be between 0 and 1.
Typically the weights get normalized to [-1, 1]. But it also depends on your problem."
"Avoid unintentional ""merging"" in cluttered object detection","
I have a problem that has bothered me quite some time.
With modern methods object detectors can often be accurately trained, even with small to medium sized datasets. However, there is one thing where I always see errors and that is when objects are very close together. The problem seems to be that the commonly used clustering methods like NMS will often combine predictions in a wrong way, like in this example:

So far, I found that one rather hacky way to solve this is to only annotate some part of the object as  the parts won't be close together, but I wonder if there could be a nicer way to solve this. Many solutions like bottom-up keypoint detection look promising, but they seem to only show their strength with large training datasets - with less data I saw a lot of confusion between the objects.
My question - has anybody experienced something similar and maybe has a promising paper or implementation for a solution that could work without huge datasets?
","['convolutional-neural-networks', 'reference-request', 'object-detection', 'algorithm-request', 'non-max-suppression']",
How to define actions on a list of values?,"
For a DQN algorithm, where my state is a list of values, say:
[5, 3, 4, 7, 8, 2, 6]
How can I define an action space that allows me to move a value in the list from one position to another? For example, the action should tell to move the '7' to the first position in the list. Here assume that the reward is a function of the order of values in the list.
This is likely similar to how chess would be played, where the action is a combination of picking a piece and choosing a position to put it in.
Note: this is an overgeneralization of what I actually want to do, but conceptually this is representative enough.
","['reinforcement-learning', 'deep-rl', 'dqn', 'action-spaces']","I had tried working on a problem similar to this using combinatorial scoring games. I ran into other issues with the players competing, but I think I can give some advice to how I handled this.In my scenario, I had an array with game pieces for each (two) player. They had to pick a piece to play on, and a direction to move it. I was able to teach the policy to pick its own piece by:The policy was able to learn ""I need to pick a playable piece to optimize the reward"".For your case, your action space could be two indices (move a value from the first index to the second). If you have arrays of varying sizes/dimensions, possibly that could be an observation? By forcing the policy to choose pairs of indices until one is valid, you will hopefully be able to teach it to pick viable pairs immediately."
"Are there some known neural networks that, given an input image, can generate a similar image, with the same topic?","
Are there some known neural networks that, given an input image, can generate a similar image, with the same topic?
Example: input = a photo of a cat on a green table, output = a generated photo of another cat on another green table.
Example 2: input = a portrait of a man with glasses and a beard, output = a portrait of a generated person with similar glasses / beard (see ""ThisPersonDoesNotExist"").
I imagine it is possible with a GAN, but more precisely which kind of architecture?
","['neural-networks', 'convolutional-neural-networks', 'generative-adversarial-networks', 'image-generation', 'model-request']",
Ready to use (already trained) ANN for small (64x64) RGB image compression,"
In my research, there is a current task of small (64x64 pixels) image compression. The images are photos, represented as RGB. While it is not a core interest, I wanted to compare some compression methods with ML compression methods. I am NOT a machine learning specialist (I never trained any network) so I need something off the shelf, which I can just download and compress/decompress images. I am a decent user of Python so this part is not a problem.
Can someone advise on (in some sense) a respected and already trained NN framework that I can use for this purpose? The ""respected"" part is important because if I want to include these results in my scientific paper, it cannot be just a random guy on GitHub :)
",['data-compression'],
Are positional embeddings computed during or before training?,"
I'm trying to practically frame the concept of positional embeddings as introduced in the original paper.
As far as I've understood, what we do is basically creating some other vectors in addition to the original embeddings of our inputs. So if I have my input $X \in \mathbb{R}^{n \times d}$ with shape (64,103) (so $n$ here is the batch size), I will be creating a matrix $P \in \mathbb{R}^{n \times d}$ where each $d$ dimensional vector will contain information about positions. Now, these vectors are generated from sinusoid functions with an initial frequency of 1e-4, and then initial embeddings $X \in \mathbb{R}^{n \times d}$ and positional embeddings $P \in \mathbb{R}^{n \times d}$ are summed together to get a new input $X' \in \mathbb{R}^{n \times d}$.
Now, all this process is happening before the start of training right? Positional embeddings are not learned/modified during backpropagation?
","['natural-language-processing', 'transformer', 'positional-encoding']",
How to visualize the input and output of the GRU cell?,"
GRU belongs to the family of recurrent neural networks. This family of neural networks works on sequence data.
But, it is taking time for me to understand the differences between sequence length and input in the case of a GRU cell.
In the case of a CNN, the input tensor is of the form $B \times C \times H \times W$.  Here $B$ is the batch size, $C$ is the number of channels, $H$ is the height of the image and $W$ is the width of the image.
We can visualize the input and outputs of CNN here.
Similarly, in the case of a GRU layer, the input tensor is of the form $B  \times L \times I$. Here $B$ is the batch size, $L$ is the length of the sequence and $I$ is the number of input features.
I want to know what exactly the input and sequence lengths are. If there are any visualizations, please provide them.
","['resource-request', 'gated-recurrent-unit']",
"Why is it called ""area of union"" when calculating the Intersection over Union?","
When calculating the Intersection Over Union the following explanation is widely used.

(Source: A Survey on Performance Metrics for Object-Detection Algorithms, by Padilla et al. 2020)
The image and name suggest that the denominator (the area of union) is the area of both boxes combined, box a + box b, when in reality it's not a union but rather a symmetric difference, box a + box b - intersection of box a and box b.
Why is it called ""area of union"" and not ""area of symmetric difference""?
","['machine-learning', 'terminology', 'object-detection', 'bounding-box', 'jaccard-similarity']","Consider these two sets $A = \{1, 2, 3, 4 \}$ and $B = \{0, 2, 3, 5, 6 \}$. Their union $A \cup B = \{0, 1, 2, 3, 4, 5, 6\}$. So, $|A \cup B| = 7$. The order of these numbers in $A \cup B$ does not matter, but you don't have any repeated number.If you do $|A| + |B|$, you're effectively counting the number of elements in $A \cap B$ twice (in the example above, it would be like doing $4 + 5 = 9$, while in reality $|A \cup B| = 7$), so you subtract $|A \cap B|$ from $|A| + |B|$, i.e. $|A \cup B| = |A| + |B| - |A \cap B|$, where $|X|$ is the size of the set $X$.See also the related Wikipedia article."
Do you think this method of creating AI is valid? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    




If the agent wins the game, the agent is rewarded

If the agent wins the game, the game is rewarded

Both the agent and the game are reinforcement learning agents.


When the agent wins the game, the game is rewarded, so the game creates various game winning conditions.
As the agent satisfies various game winning conditions, the reward increases, so the agent develops to win various games.
An agent that can win various games develops into a general-purpose intelligence
",['reinforcement-learning'],"What you are describing is a teacher-student framework in which you have one agent (the teacher) who determines the complexity of the task that another agent (the student) must accomplish. If the task is too difficult for the student to accomplish the teacher receives little to no reward and therefore changes the environment (the game) making it easier for the student to complete it.The paper Automatic Goal Generation for Reinforcement Learning Agents is quite interesting because it applies this concept using a Generative Adversarial Network (GAN). Here the generator network is the teacher and the adversarial network is the student trying to solve the task created by the generator network. The results in the paper demonstrate that no prior knowledge of the environment is required for the student to solve it and it can even be done with sparse reward, i.e., 1 for winning the game and 0 otherwise."
Which explainable artificial intelligence techniques are there?,"
Explainable artificial intelligence (XAI) is concerned with the development of techniques that can enhance the interpretability, accountability, and transparency of artificial intelligence and, in particular, machine learning algorithms and models, especially black-box ones, such as artificial neural networks, so that these can also be adopted in areas, like healthcare, where the interpretability and understanding of the results (e.g. classifications) are required.
Which XAI techniques are there?
If there are many, to avoid making this question too broad, you can just provide a few examples (the most famous or effective ones), and, for people interested in more techniques and details, you can also provide one or more references/surveys/books that go into the details of XAI. The idea of this question is that people could easily find one technique that they could study to understand what XAI really is or how it can be approached.
","['reference-request', 'ethics', 'explainable-ai']","Explainable AI and model interpretability are hyper-active and hyper-hot areas of current research (think of holy grail, or something), which have been brought forward lately not least due to the (often tremendous) success of deep learning models in  various tasks, plus the necessity of algorithmic fairness & accountability.Here are some state of the art algorithms and approaches, together with implementations and frameworks.Model-agnostic approachesLIME: Local Interpretable Model-agnostic Explanations (paper, code, blog post, R port)SHAP: A Unified Approach to Interpreting Model Predictions (paper, Python package, R package). GPU implementation for tree models by NVIDIA using RAPIDS - GPUTreeShap (paper, code, blog post)Anchors: High-Precision Model-Agnostic Explanations (paper, authors' Python code, Java implementation)Diverse Counterfactual Explanations (DiCE) by Microsoft (paper, code, blog post)Black Box Auditing and Certifying and Removing Disparate Impact (authors' Python code)FairML: Auditing Black-Box Predictive Models, by Cloudera Fast Forward Labs (blog post, paper, code)SHAP seems to enjoy high popularity among practitioners; the method has firm theoretical foundations on co-operational game theory (Shapley values), and it has in a great degree integrated the LIME approach under a common framework. Although model-agnostic, specific & efficient implementations are available for neural networks (DeepExplainer) and tree ensembles (TreeExplainer, paper).Neural network approaches (mostly, but not exclusively, for computer vision models)The Layer-wise Relevance Propagation (LRP) toolbox for neural networks (2015 paper @ PLoS ONE, 2016 paper @ JMLR, project page, code, TF Slim wrapper)Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization (paper, authors' Torch code, Tensorflow code, PyTorch code, yet another Pytorch implementation, Keras example notebook, Coursera Guided Project)Axiom-based Grad-CAM (XGrad-CAM): Towards Accurate Visualization and Explanation of CNNs, a refinement of the existing Grad-CAM method (paper, code)SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability (paper, code, Google blog post)TCAV: Testing with Concept Activation Vectors (ICML 2018 paper, Tensorflow code)Integrated Gradients (paper, code, Tensorflow tutorial, independent implementations)Network Dissection: Quantifying Interpretability of Deep Visual Representations, by MIT CSAIL (project page, Caffe code, PyTorch port)GAN Dissection: Visualizing and Understanding Generative Adversarial Networks, by MIT CSAIL (project page, with links to paper & code)Explain to Fix: A Framework to Interpret and Correct DNN Object Detector Predictions (paper, code)Transparecy-by-Design (TbD) networks (paper, code, demo)Distilling a Neural Network Into a Soft Decision Tree, a 2017 paper by Geoff Hinton, with various independent PyTorch implementationsUnderstanding Deep Networks via Extremal Perturbations and Smooth Masks (paper), implemented in TorchRay (see below)Understanding the Role of Individual Units in a Deep Neural Network (preprint, 2020 paper @ PNAS, code, project page)GNNExplainer: Generating Explanations for Graph Neural Networks (paper, code)Benchmarking Deep Learning Interpretability in Time Series Predictions (paper @ NeurIPS 2020, code utilizing Captum)Concept Whitening for Interpretable Image Recognition (paper, preprint, code)Libraries & frameworksAs interpretability moves toward the mainstream, there are already frameworks and toolboxes that incorporate more than one of the algorithms and techniques mentioned and linked above; here is a partial list:The ELI5 Python library (code, documentation)DALEX - moDel Agnostic Language for Exploration and eXplanation (homepage, code, JMLR paper), part of the DrWhy.AI projectThe What-If tool by Google, a feature of the open-source TensorBoard web application, which let users analyze an ML model without writing code (project page, code, blog post)The Language Interpretability Tool (LIT) by Google, a visual, interactive model-understanding tool for NLP models (project page, code, blog post)Lucid, a collection of infrastructure and tools for research in neural network interpretability by Google (code; papers: Feature Visualization, The Building Blocks of Interpretability)TorchRay by Facebook, a PyTorch package implementing several visualization methods for deep CNNsiNNvestigate Neural Networks (code, JMLR paper)tf-explain - interpretability methods as Tensorflow 2.0 callbacks (code, docs, blog post)InterpretML by Microsoft (homepage, code still in alpha, paper)Captum by Facebook AI - model interpetability for Pytorch (homepage, code, intro blog post)Skater, by Oracle (code, docs)Alibi, by SeldonIO (code, docs)AI Explainability 360, commenced by IBM and moved to the Linux Foundation (homepage, code, docs, IBM Bluemix, blog post)Ecco: explaining transformer-based NLP models using interactive visualizations (homepage, code, article).Recipes for Machine Learning Interpretability in H2O Driverless AI (repo)Reviews & general papersA Survey of Methods for Explaining Black Box Models (2018, ACM Computing Surveys)Definitions, methods, and applications in interpretable machine learning (2019, PNAS)Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead (2019, Nature Machine Intelligence, preprint)Machine Learning Interpretability: A Survey on Methods and Metrics (2019, Electronics)Principles and Practice of Explainable Machine Learning (2020, preprint)Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges (keynote at 2020 ECML XKDD workshop by Christoph Molnar, video & slides)Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI (2020, Information Fusion)Counterfactual Explanations for Machine Learning: A Review (2020, preprint, critique by Judea Pearl)Interpretability 2020, an applied research report by Cloudera Fast Forward, updated regularlyInterpreting Predictions of NLP Models (EMNLP 2020 tutorial)Explainable NLP Datasets (site, preprint, highlights)Interpretable Machine Learning: Fundamental Principles and 10 Grand ChallengeseBooks (available online)Interpretable Machine Learning, by Christoph Molnar, with R code availableExplanatory Model Analysis, by DALEX creators Przemyslaw Biecek and Tomasz Burzykowski, with both R & Python code snippetsAn Introduction to Machine Learning Interpretability (2nd ed. 2019), by H2OOnline courses & tutorialsMachine Learning Explainability, Kaggle tutorialExplainable AI: Scene Classification and GradCam Visualization, Coursera guided projectExplainable Machine Learning with LIME and H2O in R, Coursera guided projectInterpretability and Explainability in Machine Learning, Harvard COMPSCI 282BROther resourcesexplained.ai blogA Twitter thread, linking to several interpretation tools available for RA whole bunch of resources in the Awesome Machine Learning Interpetability repoThe online comic book (!) The Hitchhiker's Guide to Responsible Machine Learning, by the team behind the textbook Explanatory Model Analysis and the DALEX package mentioned above (blog post and backstage)"
How does stochastic gradient descent undo the normalization done by the batch normalization?,"
I want to understand the handshake between SGD (or mini-batch GD) and batch normalization. Below, an explanation quoted from this Medium article. However, I am confused about the denormalization by the SGD.

SGD ( Stochastic gradient descent) undoes this normalization if it’s a way for it to minimize the loss function.
Consequently, batch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a “standard deviation” parameter (gamma) and add a “mean” parameter (beta). In other words, batch normalization lets SGD do the denormalization by changing only these two weights for each activation, instead of losing the stability of the network by changing all the weights.

","['neural-networks', 'deep-learning', 'hyperparameter-optimization', 'batch-normalization', 'stochastic-gradient-descent']","I think this phrasing is a bit misleading. If I understand this passage correctly, another way to put it would be:Applying batch normalization distorts the true data distribution: An arbitrarily distributed batch of data is transformed into a distribution with mean = 0 and standard deviation = 1. This might not be beneficial for the downstream task, so two scalars beta and gamma are introduced that are trainable and can modify the output of the BatchNorm. This formula for BatchNorm makes this clear:$\text{BatchNorm}({\mathcal{X}}_{b}) = \gamma \cdot \frac{(\mathcal{X}_{b} - \text{mean}(\mathcal{X}_{b}))}{\text{std}(\mathcal{X}_{b})} + \beta$Without the gamma and beta, you would simply normalize the batch. But SGD can modify beta and gamma and thereby shift the distribution around according to what the optimization task demands. By acting on beta and gamma, the SGD can then somewhat 'counteract' this normalization."
How to make a proper approximation of Sine function with Neural Networks?,"
TL;DR;
How to build a neural network that properly approximates the sine function with different ranges?
Context and Question:
From this question I decided to use the Sergey's answer, however I used a different range of values for $x$ which yielded a very poor estimation of the sine function. I noticed that for large values of the $x$ range it will provide incorrect results.
I plotted the r$^2$-score for different values of the $x$ range obtaining the following:

Am I missing someting? Why is this happening?, how can one build a neural network for approximating the sine function?
The Code:
Here is the code I used to create the plot in case you want to reproduce it
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
import numpy as np
import matplotlib.pyplot as plt

sineLengths = []
scores = []

for sineLength in np.arange(40, 60, 0.1):
    sineLengths.append(sineLength)
    X = []
    x = np.arange(0, sineLength, 0.1)
    y = np.sin(x)

    for i in range(len(x)):
        X.append([x[i]])

    X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=1, test_size=0.2)
    sc_X = StandardScaler()
    X_trainscaled=sc_X.fit_transform(X_train)
    X_testscaled=sc_X.transform(X_test)

    reg = MLPRegressor(hidden_layer_sizes=tuple([64]*5), activation=""relu"" ,random_state=1, max_iter=20000).fit(X_trainscaled, y_train)
    y_pred=reg.predict(X_testscaled)
    r2score = r2_score(y_pred, y_test)
    print(""The Score with "", r2score)
    scores.append(r2score)

","['neural-networks', 'prediction', 'regression', 'multilayer-perceptrons', 'scikit-learn']","Two things are happening here:As you increase the number of $sin$ cycles that the neural network needs to approximate, the problem becomes harder for it to learn. The network - being 5 layers with 64 neurons each - actually has plenty of capacity to learn up to the approx 10 cycles you are trying to teach it. However, training it becomes progressively harder.scikit-learn's MLPRegressor has a lot of hyperparameters, which are mostly set with sensible defaults, but it looks like* it is not set to try hard when training begins to stall and become more stochastic. The main issue with that is how quickly it will give up when cost function is not improving epoch-by-epoch, and it is common for measurable improvements to take multiple epochs, especially with a small data set (you only have a few hundred samples) when progress epoch-to-epoch can become quite noisy.I managed to fix this and make training more robust by . . .Increasing training set size slightlyYou could probably do a lot more here, the larger the set, the less noisy each improvement per epoch will be. But I wanted to use other hyperparameters to show what you might need to do if training is tougher and you cannot afford to easily increase the training set size.Adding hyperparameters that made the fit routine try harderNotes:The above changes resulted in this graph for me:
. . . almost all r2score values were above 0.99, just one scored badly at 0.22, likely due to random chance. I could probably tune the hyperparameters further to make it perfect, or maybe double the size of the training set, but I will leave that as an exercise for you.* This is the first time I have used MLPRegressor, hence ""looks like"". I am not certain what all the consequences are for how they have set defaults."
"How to group multi-dimensional audio, video, and numerical data based on relatedness?","
I have a data set that includes image arrays, point clouds, audio waveforms, and plain numerical data. I want to use unsupervised learning to group the data based on relatedness.  So, if the audio and video are changing simultaneously, then the algorithm should group them together. If I am not mistaken, this is called heterogenous data clustering.
My data looks like this:
Audio
[[first frame's audio samples] [second set] [third set] ...

Video
4D array of shape (1654, 500, 128, 3)
              # of imgs   l    w   channels

Gyro
[[roll, pitch, yaw], [roll, pitch, yaw], [roll, pitch, yaw], [roll, pitch, yaw].....

And a bunch of 1D numerical data

Is there a way to do this?
I am new to this so if there is some critical information missing, let me know.
","['unsupervised-learning', 'algorithm-request', 'k-means']",
Distinguishing text with opposite meanings in SVM (False Information Detection),"
I am currently working on a Binary Text Classification Model (False Information Detection) using Support Vector Machine and used TF-IDF as text vectorizer in Python. I have already tried training the model but upon testing, I have encountered a problem:
For example I have the model predicted an entry saying ""COVID-19 is happening today"" as ""True"", but after changing the text into ""COVID-19 is not happening today"", it is still predicted as ""True"", in which should be predicted as ""False"".
Where does the problem lie in this situation?
How can we make the algorithm classify text with opposite meanings like ones mentioned above?
Note:

The text that exists in the dataset I used in modelling is “COVID-19 is happening today.”

I used also predict_proba to know the probability of the text being 0(False), or 1(True). It shows that the two entries I made have the same output in predict_proba which with this I can say that it reads the two entries as the same (maybe as ""COVID-19 is happening today"").


","['machine-learning', 'supervised-learning', 'support-vector-machine', 'text-classification', 'tf-idf']",
Image-in image-out neural network architectures,"
With an RGB image of a paper sheet with text, I want to obtain an output image which is cropped and deskewed. Example of input:

I have tried non-AI tools (such as openCV.findContours) to find the 4 corners of the sheet, but it's not very robust in some lighting conditions, or if there are other elements on the photo.
So I see two options:

a NN with input=image, output=image, that does everything (including the deskewing, and even also the brightness adjustment). I'll just train it with thousands of images.

a NN with input=image, output=coordinates_of_4_corners. Then I'll do the cropping + deskewing with a homographic transform, and brightness adjustment with standard non-AI tools


Which approach would you use?
More generally what kind of architecture of neural network would you use in the general case input=image, output=image?
Is approach #2, for which input=image, output=coordinates possible? Or is there another segmentation method you would use here?
","['neural-networks', 'convolutional-neural-networks', 'image-processing', 'image-generation']",
Sensible integer embedding/encoding for distinguishing elements of a set?,"
I am trying to train a model that takes in a set of feature vectors (which comes with an ID to uniquely identify elements of the set) and outputs a target for each element in the set (in a permutation-equivariant manner). It seems like a self-attention based model would work well here (because there's reason to believe that the target depends on the features of other elements).
However, I need help coming up with a sensible encoding for the integer ID that comes along with the feature vector. For context, I have at 3561 unique integers IDs (0 being the ""out of vocabulary"" ID (such as for elements not seen during training) and 1 to 3560 for the elements seen during training). Intuitively, it seems like a bad idea to just provide the integer itself concatenated to the feature vector.
The goal is to concatenate the integer embedding with the feature vector and train self-attention on this set.
The constraints for the encoding/embedding of integer IDs are:

No elements are similar: There are no physical/temporal relationships here, just a bag of elements. So, something like positional encoding as used in the Transformer paper might not be a good choice because there's no reason to believe that element #45 and element #46 have any special relationship compared to element #46 and #100.

They should be deterministic and more importantly, not trained. Trainable embeddings would only result in the embeddings changing during training which is undesirable.

Low dimensional: Definitely not more than, say, 50 dimensions (so one-hot encodings are not acceptable here).



My best idea right now is to use spherical codes. For example, unit vectors in say 24 dimensions that are distributed as far from each other as possible. Since the kissing number in 24 dimensions is 196,560 - 3560 unit-vectors can be easily placed in a way such that the pair-wise cosine similarity for any two vectors is quite low. The out-of-vocabulary embedding vector would be the zero-vector (at the origin).
Would this be sensible? I have heard that positional encoding had some desirable properties with respect to linear transformations. Would the kind of integer embedding I am imagining based on equidistant unit vectors cause problems as the self-attention module is largely a linear operation.
I am probably overthinking this and there likely exist sensible ways of embedding/encoding integer IDs that I simply haven't heard of. For all I know, just providing the binary representation would work just as well as any other embedding.
","['attention', 'embeddings', 'feature-engineering', 'categorical-data']",
What's the difference between model-free and model-based reinforcement learning?,"
What's the difference between model-free and model-based reinforcement learning? 
It seems to me that any model-free learner, learning through trial and error, could be reframed as model-based. In that case, when would model-free learners be appropriate?
","['reinforcement-learning', 'comparison', 'model-based-methods', 'model-free-methods']",
What is meant by non-linearity in Convolutional Neural Networks? And why do we focus on removing it entirely? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I'm aware of the working of ReLU that it's turns every negative value to zero and doesn't effect any positive value, but what confuses me is that: what is actually meant by Non-linearity in feature map after Convolution operation on any image? What actually is non-linearity? And why do we focus on removing it completely from the feature map?
Thank you.
","['deep-learning', 'convolutional-neural-networks', 'image-processing', 'relu']","The concept of non-linearity is not only restricted to Convolutional Networks but can be seen in RNNs, and any feed forward neural networks.Without a non-linear activation function, two feed forward layers can be collapsed into a single feed forward layer. So, let's say you have a large neural network with 5 layers possessing weights W1 to W5.Without a non-linear layer, passing an input x would give you the following:y = W5(W4(W3(W2(W1(x)))))
which is equivalent to y = W(x) where W is a product of W1..W5which is essentially a single linear layer -Hence, this would not allow the neural network to learn higher order or more complicated functions which are not linear in nature.The presence of a non-linearity between the layers allows the weights to be adjusted in order to learn complex functions apart from linear functions. Eg. if your data might be quadratic in nature, or say some complicated mixture of logarithmic, sinusoidal functions, hyperbolic functions, etc."
Are these two forms of the state value function the same?,"
Why are there different forms of the value function in reinforcement learning?
Sutton & Barto (2nd edition, equation 3.14) define the state value function as follows
$$v_{\pi}(s) = \displaystyle\sum_a \pi(a|s) \displaystyle\sum_{s',r}p(s',r \mid s,a)[r + \gamma v_\pi(s')].$$
Many lecture notes (for example, these by Haim Sompolinsky, MCB 131, 2017) define it as follows (equation 18)
$$V^{\pi}(s) =\displaystyle\sum_a \pi(a,s) [R(s,a) + \gamma \displaystyle\sum_{s'}P(s' \mid s,a) V^\pi(s')].$$
Are these two forms the same?
","['reinforcement-learning', 'markov-decision-process', 'proofs', 'value-functions']",
Hierarchical Navigable Small World Graphs : Expected Number of Steps in a Layer,"
Paper: Efficient and robust approximate nearest
neighbor search using Hierarchical Navigable
Small World graphs
In the Search Complexity section, the author estimates that the expected number of steps in a layer is bounded by $1/(1 - exp(-m_{L})$.
It says:

There is a fixed probability $p=exp(-m_{L}
)$ that
the next node belongs to the upper layer. However, the
search on the layer always terminates before it reaches the
element which belongs to the higher layer (otherwise the
search on the upper layer would have stopped on a different element), so the probability of not reaching the target on s-th step is bounded by $exp(-s· m_{L}
)$.

Shouldn't the probability of reaching the target on s-th step be $(1-exp(-m_{L}))^{s} *exp(-m_{L})$ since this would mean we are reaching a node which belongs to higher layer on the (s+1)th hop ? If yes, doesn't it defy the bound introduced in the above paragraph ?
","['search', 'k-nearest-neighbors', 'information-retrieval']",
Compare Strings composed from 2-3 words using NLP/ML(Python),"
I have a database of books. Each book have a list of categories that describe the genre/topics of the book (I use Python models).
Most of the time, the categories in the list are composed from 1-3 words.
Examples of a book category list:
['Children', 'Flour mills', 'Jealousy', 'Nannies', 'Child labor', 'Conduct of life'],
[""Children's stories"", 'Christian life'],
['Children', 'Brothers and sisters', 'Conduct of life', 'Cheerfulness', 'Christian life'],
['Fugitive slaves', 'African Americans', 'Slavery', 'Plantation life', 'Slaves', 'Christian life', 'Cruelty']

I want to create/use an algorithm to compare the books and find similarity between 2 books using NLP/machine learning models.
The categories are not well defined and tend to change. For example, there can be a category of 'story' and other called 'stories' category (since the text in the system don't saved categories and use a open text box).
So far I tried 2 algorithms:

cosine similarity with WordNet - split the category to bag of words and check if each word have synonym in the other book lists.
Check the similarity using the nlp model of the spacy library (Python) - distance algorithm.

So far I used WordNet model from the nltk package and spacy
had problem with those two algorithms because when the algorithm compare a categories that contain 2 or 3 words the results wasn't accurate and each of them had specific problems.
Which algorithm and models (in Python), can I use to compare between the books that can handle strings that contain 2 or 3 words?
B.w is the first time I ask here. If you need more details about the database or what I did so far please tell me.
","['machine-learning', 'natural-language-processing', 'python', 'algorithm-request', 'spacy']",
Why CNN inference works on larger images,"
I have been reading up on 'regular' CNN's such as Mask R-CNN, and as far as I understand it they rely on a fully connected layer in the end to classify pixels. FCN's (such as U-Net) which do not use these layers are able to effectively process images of any size. I have been wondering why it is that Mask R-CNN can still process images of larger sizes. Doesn't that mean that a lot of data in the image will be unused in the fully connected layer?
For example consider a Mask R-CNN model trained on 512x512 images, that then does inference on a 2048x2048 image. I would expect that to result in an error or at least very poor performance, but according to my results this is not the case.
I also noticed that for some CNN models the image dimensions have to be some power of 2, is that due to implementation or also something that has to do with the connected layers?

Edit:
I've now found that the answer regarding image dimensions is here, it is due to the downsampling and upsampling.
","['convolutional-neural-networks', 'semantic-segmentation']",
"What is multi-head attention doing mathematically, and how is it different from self-attention?","
I'm trying to understand the difference between the concept of self-attention and multi-head attention. The latter is not actually too clear to me.
I understand that, in the case of self-attention, we start with a feature matrix $X \in \mathbb{R}^{n \times d}$, and then we use the same linear transformation $W$ to produce
\begin{align}
Q &= XW \\ 
K &= XW \\
V &= XW
\end{align}
and then we compute the following
$$X' = \text{softmax} \left(\frac{Q\cdot K^T}{\sqrt{d}} \right)V$$
where $X' \in \mathbb{R}^{n \times d}$ is a new version of the input matrix, where the pairwise interactions between the points will be encoded.
What is multi-head attention doing, from a mathematical point of view, and what's the difference? I know we are going to use three different linear transformations in this case (so no weight-sharing), but what exactly will be encoded using three different $W$? Maybe it's more the conceptual view that it's not too clear in this case.
","['deep-learning', 'comparison', 'transformer', 'attention']","First of all, I believe that in self-attention mechanism for Query, Key and Value vectors the different linear transformations are used,
$$
Q = XW_Q,\,K = XW_K,\,V = XW_V; W_Q \neq W_K, W_K \neq W_V, W_Q \neq W_V
$$
The self-attention itself is a way of using more general attention mechanism. You can check this post for examples of other mechanisms that can be used.Multi-head attention is a way of grouping together a bunch of attention mechanism ( Usually they are all the same type ), which consists in just running multiple mechanism in parallel and aggregating the resulting set in some way. In here for example, the aggregation is done by concatenation and weighted sum of the outputs, e.g.
\begin{equation*}
\begin{aligned}
&\text{MultiHead}(Q, K, V)= [\text{head}_1, .., \text{head}_n]W^O & [..] \text{ meaning concatenation}\\
&\text{where head}_i = \text{Attention}(QW_i^Q,KW_i^K,VW_i^V) & \text{each head}_i\text{ is unique attention mechanism} \\
&\text{Attention}(\textbf{Q},\textbf{K},\textbf{V}) = \text{softmax}(\frac{\textbf{Q}\textbf{K}}{\sqrt{d_k}})\textbf{V} & \text{this can be other attention mechanism}\\
\end{aligned}
\end{equation*}"
Does Value Loss in Actor Critic not decrease at all?,"
I am coding a problem with the Actor-Critic Method. The final loss is a summation of PolicyLoss and ValueLoss. The calculation of the PolicyLoss for each step is given at Equation Number 5 of https://arxiv.org/pdf/1707.06347.pdf. And for Valueloss:
$V_t = \gamma * next\_Value + reward$
$Valueloss= L1\_Loss(Vt, Value)$
I checked that the PolicyLoss depends on this 'Value' as well. And PolicyLoss is decreasing, and so the parts of my AI model depending on the 'Value' are also updating. But I am seeing that the ValueLoss is not decreasing, however, the Total Loss which is the sum(ValueLoss, PolicyLoss) is decreasing.
I don't understand how to make the ValueLoss decrease, or if it should decrease at all. Because, the equation of ValueLoss suggests that it is like a 'chasing the tail' situation, so it may fluctuate.
","['python', 'actor-critic-methods', 'loss']",
Does the number of biases in a Convolutional layer scale with the number of images?,"
I am convolving 32 grey scale images of size 28 x28 with 16 filters of size 5x5.
Which of the following is the correct way to add biases to the convolution operation output?

add 1 scalar value bias for each of the 16 filters. (Each of the 32 images share the same weights and 16 biases)

For each of the 32 images, add 1 scalar value bias for each of the 16 filters. Each image has its own weights and 16 biases?

something else- Please explain


","['convolutional-neural-networks', 'bias']",
Alternatives to using RL to explore an environment,"
I am looking at some ideas on exploring an environment using Curiosity Driven Exploration and am being a little skeptical about it. The objective here is to just explore without the need to obtain optimal actions for each state. For this purpose, wouldn't it be better to just use some sampling strategy rather than RL?
For instance, given a large grid world, I'd like to efficiently find the locations of fire pits and the exit door. One way to go about it is Curiosity Driven RL, but that would be overkill since I am not concerned with learning the optimal actions for each state.
","['deep-rl', 'exploration-strategies']",
Possible improvements to WGAN-GP output images,"
I am mapping rather complex data into what essentially amounts to a greyscale image to take better advantage of GANs for generative means. Here is an example of some real data:

All real data is of the same shape (108 across x 12 high), with an enforced border region of two pixels along each edge - so the actual data is captured by the central 104x8 pixel data. There is significant coupling between individual pixels, so I applied a CNN in the network to try and learn these dependencies, with unequal kernels because the image is rectangular. A rough description for each row of pixels is as follows (note that each row is normalised between 0 and 1 for the network, with 0 corresponding to border region):

Integer values -1 -1 -1 -2 -1 -1 -1 -2... etc.
Very small float values (~0.03) with a break in the middle (~0.7)
Integer values -2 -2 -3 -3 -2 -2 -3 -3...
Float values
Integer values -3 -4 -4 -4 -3 -4 -4 -4 -3...
Float values
Integer values (one hot of 24, moving in blocks of 4)
Integer values (one hot of 4, repeating in blocks of 4)

I have applied the WGAN network with gradient penalty implemented in PyTorch. The general generator network has this architecture:

ConvTranspose2d
LeakyReLU(0.2)
BatchNorm2d

With a sigmoid at the end. The critic looks like:

Conv2d
InstanceNorm2d
LeakyReLU(0.2)

With no activation at the end of the critic. I have a dataset size of 1795, sadly I don't really have any conventional means to increase this. I train the critic for 5 rounds for each round of generator training. After 900 epochs, an example of the generated output is as follows:

So looks quite close, but converting this back into the actual form of my data results in some weird results because of the variation in pixels along the float rows.
There are also some weird bits on the side (it doesn't seem to have properly learnt the border). The first few pixels in the rows are also unusual in the real data, with the first few for some of the rows basically being a border region - this behaviour seems to have been learnt for some rows (e.g. row 5), but not others. This may be because of my unequal kernels, I'm not sure.
Some current improvements I'm considering are to add gaussian noise to the data (this may help as it removes the integer aspect of some pixels), augment the training set both with noise and with conventional methods such as rotating the image 180 degrees, adding extra border regions and translating the images etc.. If anyone has any insight into how I can further improve output I would really appreciate it. This image is actually a simplification of the dataset, and eventually, I want to have less regular sequencing in the pixels. Thanks!
(Edit)
Here is an image of the critic (D) vs generator (G) loss over the 900 epochs used to generate the fake data:

","['convolutional-neural-networks', 'generative-adversarial-networks', 'image-generation', 'wasserstein-gan', 'dc-gan']",
Question about true positive and false positive detections in object detection,"
I am computing true positives, false positives, and false negatives in order to calculate my model's precision and recall. I am using YOLOv5.
According to this source and this one, an IoU overlap between the predicted and ground truth bounding box greater than 50% is a true positive detection (we are assuming we set the IoU threshold at 0.5), and an overlap less than 50% is considered to be a false positive.
Let's say you have multiple objects in your image that you are detecting. You label one object as ""class A"", but the model predicts it as ""class B."" Am I correct that this is also a false positive? As I understand it, there are two ways of getting a false positive: Achieving an IoU overlap of less than 50% of the correctly-predicted class, and identifying any overlap of an incorrect class?
","['deep-learning', 'convolutional-neural-networks', 'object-detection']",
Neural network for an output in the form of a probability distribution,"
I am not an expert in machine learning. Recently, I want to construct a data-driven model based the neural network. The problem is that I want from my algorithm to learn an output in the form of a probability distribution (maybe to learn the parameters of a predefined probability distribution). This is because the phenomenon I am trying to model is highly uncertain.
In my case, I am dealing with a supervised learning. Hence, the output which is supposedly in form of a probability distribution should be compared with data points to assess the performance. Could you please suggest to me a reference dealing with this issue.
Thank you so much for your valuable help.
","['neural-networks', 'probability-distribution', 'performance']",
How to deal with variable action ranges in RL for continuous action spaces,"
I am reading this paper on battery management using RL. The action consist in the charging/discharging power of the battery at timestep $t$. For instance, in the case of the charging power, the maximum of this action can be given by the maximum charging speed $c^{\max }$ or by the state of charge of the battery, since it cannot be charged more than $100\%$. Therefore, the charging action has the following range:
$$
0 \leq c_{t} \leq \min \left\{c^{\max }, \frac{B^{\max }-B_{t}}{\eta_{c}}\right\}
$$
In some timesteps the maximum of $c_{t}$ will be $c^{\max }$ and for others $\frac{B^{\max }-B_{t}}{\eta_{c}}$. What would be the best way of implementing a variable action range? I have thought in using a range $[0,1]$ for the action, scaling it to the suitable range. Is there any standard way to deal with variable ranges?.
","['reinforcement-learning', 'deep-rl', 'action-spaces', 'continuous-action-spaces', 'stable-baselines']",
Can AlphaZero develop significantly different playing styles (depending on the random games from which it learrns)?,"
There is a quite popular video analysing a chess game AlphaZero vs. AlphaZero, called ""the perfect game"". It leaves some questions open and I'd like to ask them here:

Did the two copies of AlphaZero use the same random seeds in the learning phase, esp. when generating random games? So were they perfect copies of each other at the beginning of the game?

Do the two copies of AlphaZero calculate their moves with the same random seeds when doing Monte Carlo tree search?


In any case there are three possible games all of which may be called ""AlphaZero vs. AlphaZero"":

same seeds when learning, same seeds when playing
same seeds when learning, different seeds when playing
different seeds when learning (the seeds when playing don't matter then)

In the latter case, I wonder if AlphaZero's playing style (which is for example analyzed in the book Game Changer by Matthew Sadler and Natasha Regan) may depend on the seeds used for generating random games (assuming that the same number of test games is played in the learning phase). In other words: Can AlphaZero develop significantly different playing styles when starting tabula rasa? In this case, Sadler/Regan's book would describe just one instance of AlphaZero.
The last question should be answerable also by those of you who don't know the video and how the premises have been.
","['monte-carlo-tree-search', 'alphazero', 'chess', 'explainable-ai', 'randomness']",
What will happen if I concatenate one-hot-encoded categorical features along with continuous numerical features?,"
Here is one row from my data:
H  7.042 5.781 5.399  -9.118   5.488  7.470

The first column is a categorical class. The rest of them are continuous numerical features.
I encoded the classes using one-hot-encoding labels and concatenated them with the numerical features list:
    1   0  1  7.042 5.781 5.399  -9.118   5.488  7.470

Then I used this list for training.
Am I achieving anything really useful here?
","['neural-networks', 'machine-learning', 'classification', 'training']","Your approach is totally valid (especially considering you only have a single categorical feature).If you're training a neural network though make sure to standardize your numerical features in order to be in the range [0, 1].Also, in general, and again only if you're training a neural network, you might keep in mind that there's also another valid concatenation approach, happening not at the input layer level but in a hidden layer. This is common in natural language processing, where the text features can be quite larger in number than the numerical ones. By processing separately numerical and text features we can ensure that both are mapped to a fixed size dense layer, so when the concatenation occurs, we give same importance to both types of features regardless of their respective amounts. Another advantage of this approach is that when using one hot encoding the resulting features are sparse, and by keeping them separated you can treat them accordingly (for example using a loss function for sparse features only on the one hot feature block)."
How to compare RL algorithms with different NN sizes?,"
I wanted to run some tests with some RL algorithms in a continuous control task, namely PPO-clip and SAC.
When comparing their NN structures described in their papers, SAC used 2 layers with 256 neurons, while PPO used 2 layers with 64 neurons each.
Can one compare these algorithms with these different NN sizes, ""just like that""? I imagine a larger NN can approximate more complex functions and must be harder to train until convergence, compared to a smaller NN.
And in this case, the width of the NN differs a lot. It seems to me that the NN size is rather ignored when comparing, or am I missing something?
","['neural-networks', 'reinforcement-learning', 'comparison', 'proximal-policy-optimization', 'soft-actor-critic']",
Examples of rationalizable AI,"
The marvelous book Game Changer: AlphaZero's Groundbreaking Chess Strategies and the Promise of AI gave rise to this question. It is - in my opinion - a perfect example of rationalizing a piece of AI that makes it understandable and explainable. I favour the term ""rationalizable"" instead of ""explainable"" or ""understandable"" because it can naturally be applied to humans and animals as well (see rational animals), and because it captures better what it is (or should be) about: guessing the rationale behind some observed behaviour. A rationale lets an observed behaviour seem rational.
Candidates for rationizable AI are systems that show some sufficiently complex behaviour (not just giving classifying answers) that also humans may show. For example playing behaviour which is typically described in terms of strategies.
To give some examples how the authors of Game Changer rationalize AlphaZero (p. 98):

AlphaZero likes to target the opponent's king.

AlphaZero likes to keep its own king out of danger.

AlphaZero makes sure the central situation is stable before it weakens its own kingside structure to open lines against the opponent's king.

AlphaZero defends by creating confusion and introducing tactics into the game.


The authors even reconstruct AlphaZero's ""thought processes"" (see p. 77: ""An in-depth illustration of one move"")
To make such rationalizations you need a lot of expert knowledge. And AlphaZero most surely is not capable of giving such rationalizations in human-understandable form by itself.
My main question is for other specific examples of rationalizing (some piece of) AI.
Side questions are:

Do you find this kind of rationalization sound, and is it what one
means when talking about explainable AI?

What has this concept of rationalizability to do with
the game-theoretic concept of rationalizability?



Edit: Only today I found some references that explicitly mention ""rationalization"" or ""rationality"" in the context of AI:

U. Ehsan et al (2017): Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations

T. Marwala et al (2021): Rational Machines and Artificial Intelligence


This paper doesn't mention rationality explicitly but stresses on human-understandability. And it goes even one step further into the direction of self-explainability:

D.C. Elton (2020): Self-explaining AI as an alternative to interpretable AI

","['reference-request', 'game-theory', 'alphazero', 'chess', 'explainable-ai']",
Focal Loss vs Weighted Cross Entropy Loss,"
Weighted Focal Loss is defined like so
$FL(p_t) = -\alpha_t log(p_t) (1-p_t)^\gamma $
Whereas weighted Cross Entropy Loss is defined like so
$CE(p_t) = -\alpha_t log(p_t)$
Some blog posts try to explain the core difference, but I still fail to understand why select one over the other?
Compiling some of those blogs, boils down to

Another way, apart from Focal Loss, to deal with class imbalance is to
introduce weights. Give high weights to the rare class and small
weights to the dominating or common class. These weights are referred
to as $\alpha$


Adding these weights does help with class imbalance however, the focal
loss paper reports:


The large class imbalance encountered during the training of dense
detectors overwhelms the cross-entropy loss.
Easily classified negatives comprise the majority of the loss and
dominate the gradient. While balances the importance of
positive/negative examples, it does not differentiate between
easy/hard examples
Even when we add α, while it does add different weights to different
classes, thereby balancing the importance of positive/negative
examples - just doing this in most cases is not enough. What we also
want to do is to reduce the loss of easily-classified examples because
otherwise these easily-classified examples would dominate our
training.

My question
I fail to see why the added multiplicative factor of focal loss handles ""how easily samples are classified"" moreso than what is already present in the very essence of CrossEntropyLoss.
It seems like with FocalLoss, the (CrossEntropy)loss is simply skewed downwards at the ""certain"" part, such that loss there becomes insignificant - but that is also the case for CrossEntropy. I simply fail to see the fundamental difference that FocalLoss can solve and CrossEntropyLoss can't.
Their qualitative shape is quite similar, so I don't see why they what problem one can solve that the other can't.
","['deep-learning', 'classification', 'loss', 'cross-entropy']",
A2C: Why do episode rewards reset?,"
I am training a model using A2C with stable baselines 2. When I increased the timesteps I noticed that episode rewards seem to reset (see attached plot). I don´t understand where these sudden decays or resets could come from and I am looking for practical experience or pointers to theory what these resets could imply.

","['reinforcement-learning', 'advantage-actor-critic', 'stable-baselines']",
Why does my actor-critic network always give either -1 or 1 at the output layer?,"
I have an actor-critic network. The state space contains continuous variables with different ranges like (0,1.57) and (-0.70, 0.70). And it also contain absolute 6D pose in the form (x,y,z,roll,pitch,yaw). The action space is continuous too in the range (0, 1.57). I apply scaling at the input layer, and scale things back before applying the action received from network. Irrespective of learning for 100 or 1000 episodes, the actor model always gives either -1 or 1. Eg: [1,1,-1,-1] which gets scaled to [1.57,1.57,-1.57,-1.57] as an action vector. Could someone give me suggestion on what's happening with the network. The learning follows DDPG algorithm.
actor_lr = 0.0001
critic_lr = 0.001
def actor(state_size, action_size):

    inputs = Input(shape=(state_size,), name=""state_space"")
    layer_one = Dense(300, activation=""relu"")(inputs)
    layer_two = Dense(400, activation=""relu"")(layer_one)

    outputs = Dense(action_size, activation=""tanh"", name=""action_space"")(layer_two)

    model = Model(inputs = inputs, outputs = outputs)
    model.compile(loss=""mse"", optimizer=Adam(lr=actor_lr), metrics=[""accuracy""])
    return model

def get_critic(state_size, action_size):

    state_input = Input(shape=(state_size), name=""state_space"")
    state_out = Dense(64, activation=""relu"")(state_input)

    action_input = Input(shape=(action_size), name=""actions_sapce"")
    action_out = Dense(18, activation=""relu"")(action_input)
    concat = Concatenate()([state_out, action_out])
    
    critic_layer_one = Dense(300, activation=""relu"", kernel_regularizer=regularizers.l2(0.01))(concat)
    critic_layer_two = Dense(400, activation=""relu"", kernel_regularizer=regularizers.l2(0.01))(critic_layer_one)

    outputs = Dense(1, activation=""linear"", kernel_regularizer=regularizers.l2(0.01))(critic_layer_two)

    model = Model([state_input, action_input], outputs, name = name)
    model.compile(loss=""mse"", optimizer=Adam(lr=critic_lr), metrics=[""accuracy""])
    return model

","['reinforcement-learning', 'deep-rl', 'ddpg']","Most probably your network is underfitted. In that case, the network outputs values randomly. Hyperbolic tangent tanh converges very quickly towards $-1$ or $1$, so that is why you always find $-1$ and $1$ in the output.Let us execute the following code to get a better idea:At least in my machine, the output will be exactly $1.0$ for tanh_x variable of type numpy.float32, which has a precision of up to $7$ decimal digits.The value of tanh_x is in fact $-0.99999977493$, but Python only saves the first $7$ decimal digits and decides to round it up to $-1.0$. Every output value from the neural network outside the range of $[-8, 8]$  will be exactly $-1$ or $1$ after tanh activation and float32 precision. As you can see, if the network randomly outputs values they will almost surely be outside $[-8, 8]$ as this is a very short range."
References for Nvidia's DLSS,"
Nvidia's deep learning super-sampling is presented as an application of deep learning techniques to video-rendering in videogames.
Question: I'm asking for a technical reference that explains what is the deep learning architecture that is behind the inner works of the Nvidia's DLSS.
Thanks in advance!
","['deep-learning', 'reference-request', 'deep-neural-networks']",
Why does my regression-NN completely fail to predict some points?,"
I would like to train a NN in order to approximate an unknown function $y = f(x_1,x_2)$. I have a lot of measurements $y = [y_1,\dots,y_K]$ (with K that could be in the range of 10-100 thousands) which come out from either a simulation or a measurement of a system. I've built a feed-forward NN for solving this problem by using a MSE loss-function, i.e.
$$\mathcal{L} = \frac{1}{K}\sum_{i=1}^K(y_i-\hat{y}_i)^2$$
where I defined as $\hat{y}$ the prediction of the NN. As per activation function I used a ReLU. The network topology is fairly simple with input layer having two neurons $(x_1,x_2)$, three hidden layers with 10 neurons each and finally the output layer (single output neuron).
After the training process expires I've obtained a very curious result. The loss function assumes very small values hence (apparently) indicating that the training is successful. However, if I analyse the squared-error point-by-point, i.e. the quantity
$$ \boldsymbol{\epsilon}_y = [(y_1-\hat{y}_1)^2,\dots,(y_K-\hat{y}_K)^2] $$
I find that for the vast majority of points said quantity is basically zero, with exception of some ""outliers"" where the error is huge. It looks like this event happens where the gradient of $f()$ is rather big, which might be a reasonable assumption maybe.
I would like for this to not happen anymore. I'd rather accept a slightly bigger error throughout the whole function domain then have the majority of points with null error but some local points that are completely off. As a requirement, the network topology shall be kept rather ""easy"" so I would not like to increase the number of layers and/or neurons per layer. As a side node, I've also tried to increase the topology complexity a little bit (i.e. 15 neurons per hidden layer and adding a 4th hidden layer) obtaining slighlty better results but still unacceptable error around the steepest points of the function.
I've got two ideas for now:

Use a different loss-function $\mathcal{L}$
Sample the dataset $(x_1,x_2)$ more frequently around steepest regions, and less frequently where the function is rather smooth and flat

First option
A different loss-function to be adopted. I'm not very familiar with loss-function that might help solve my problem, some rather quick research yielded no good results and I found no significant literature highlighting this kind of problem. I initially thought of something of the likes of
$$ \mathcal{L} = \frac{1}{K}\sum_{i=1}^K(y_i-\hat{y}_i)^2 + \mu\, max\{\boldsymbol{\epsilon}_y\} $$
where $\mu$ is some tuned parameter that penalizes the maximum error. I'm not sure if this makes sense and if it increases the training complexity to the extent that the training process runs in no convergence territory.
Second option
I am not sure how to formalize the undersampling where the function is smooth and flat, I imagine that some kernel used for image processing (edge detection kind of kernels of some sort?) might be helpful but I'm in completely unknown territory.
Conclusion
I'm looking for some insight and ideas (literature results or analogous cases linked are a huge plus!) for solving this curious problems.
Thanks for the help!
Edit
I realize that without having graphical indsights of what I'm doing, it's going to be rather difficult to have a clear understanding. The function $f(x_1,x_2)$ is plotted here below (let alone the red line, it is just there to highlight what happens when $x_1=\text{M}=1.2$ is fixed to a certain value). The missing points of around the variable $x_2=f_n=1$ represent a singularity in my model and I don't care about them being predicted since they physically have no pratical use. I removed them completely for this reason.

Once the training of my NN is complete, I plot the $L2$ error (MSE) between predicted and true value

As you can see, it looks like the points where there is a significant gradient change cannot be predicted correctly.
","['neural-networks', 'optimization', 'regression', 'logistic-regression']",
How does using complex weights in a neural network affect performance?,"
If you switch a neural network from real weights to complex weights, you're roughly doubling the size of the network, and increasing the computational load by a factor of 2 to 4. My question is, in general, roughly how does the benefit of using complex weights stack up to those extra costs? E.g. Will a complex neural network with half the weights achieve worse/comparable/better performance than a regular network with real weights?
In audio signal processing, complex numbers make the theory much more elegant, which is why I imagine using complex numbers might be disproportionately beneficial. Though I can also imagine the complexity they introduce might overly hinder things as well.
As far as I know, no one uses complex weights in the NNs (which must be for a reason), but I'd like a more definitive answer.
","['neural-networks', 'machine-learning', 'audio-processing', 'linear-algebra', 'signal-processing']",
Detecting cheats visually using AI,"
I really like to play my favorite 3D shooter game online. Unfortunately, it is really old and cheat protection isn't really common there, but cheaters are! It is very frustrating, because it really kills all the fun playing against cheaters.
Currently we rely to an admin that is spectating a person accused as cheater and decides if he gets banned from the server or not. But admins are not available 24/7.
The common cheats are:

Wallhacks: seeing other players through walls
Aimbots: aiming the enemy and shooting automatically when just one pixel of the other player enter the cheaters visible field

I wonder if it is possible to take randomly short video footage (1 min) for each player (spectating him) and score if it is very likely that he plays with a cheat active or not.
I guess for wallhack, it might not be really possible because the player does not behave different as the other players, he is just seeing more (and might look against a wall where a non cheating player might not do so). However, i think is very hard to tell even for a human if a wallhack is enabled or not. I think the error rate is really high here.
But for aimbots i think, it can be possible because for a human it is very obvious:
The cheater moves/looks into one direction and if an enemy appears in back of the cheater he turns immediately (in ~1-2 frame) around and shoots (and mostly hit). A straight player would never see whats going on behind him but might luckily turn around and hit somebody, so one of those hits might not be a 100% guarantee accusing someone being a cheater, but several of those ""lucky shots"" will definitely.
I would say, for a human it usually takes one to three of those impossible movements to conclude that it is 99% sure that this person is cheating.
Since i only have basic experience with AI (detecting things on images), i don't know what can be suitable from AI ""tooling""  to detect something like this, because the content of the video frames itself is not really relevant, but the ""change"" between the images is crucial i guess ...
However, is this possible to detect cheaters ""visually"" using AI ?
I googled a bit around and read something about ""next frame prediction"" using convolutional LSTM. Is it something like ""when a frame appears next that was not predicted it might be cheat"" ?
Since this seems to be a very complex area i just want to find out the right direction i should look for. Any keywords for me to google here?
I am not really deep into AI but I could also imagine that calculating some kind of a ""value"" of each video frame can be used here. If the value between the frames does alter too much, it is likely that the person moves ""unnatural""
","['classification', 'training', 'image-recognition', 'game-ai', 'video-classification']",
Is there any subtle difference between kernel and filter in the context of neural netowrks?,"
Consider the following excerpt from a paragraph, taken from the topic Detecting features with convolutions of the textbook named Deep Learning with PyTorch by Eli Stevens et al., regarding convolutional neural networks.

From this angle, the job of a convolutional neural network is to
estimate the kernel of a set of filter banks in successive layers
that will transform a multichannel image into another multichannel
image, where different channels correspond to different features (such
as one channel for the average, another channel for vertical edges,
and so on).

The usage of words kernel and filter bank in this excerpt confused me as i generally treat both to be the same.
What is the subtle difference between a filter and a kernel in the context of CNN?
","['convolutional-neural-networks', 'terminology', 'filters']",
Deep RL reward design for neuron centerline extraction task,"
As part of a bigger scope project, I'm training a RL agent that attempts to reconstruct, pixel by pixel, the trajectory of a neuron on a segmented image. To give a better insight on the task that I'm trying to achieve, here is an example of an attempt on simple vessel ground truth image (red : vessel/ground truth, green : agent trajectory, yellow : overlap between the two) :

Details on the environment and the model :
At each step, the agent is picking an action based on the output of a Deep Q-Network and chooses the next pixel to move the cursor to. It can move in any of the 8 directions (in diagonal too). The goal is to match the red curve with the green one by updating each step the position of the cursor. At the moment, I'm training the agent with a few different such ground truth images and for a fair amount of episodes and it's really struggling at sticking close to the red line. I set a maximum number of pixels to stop the agent after a while in case it doesn't approximately match the red line (which it does not). I'm used to applying RL to games like Atari etc, but here it's a bit more abstract to determine what would be a smart way to design the reward system.
Question :
At the moment, I'm using the completeness score that compares what fraction of the pixels of the dilated red line are covered by the agent's path as part of my reward system. If the completeness increases as compared to the previous step, I give a reward of +1, if it doesn't, I give a reward of -0.1 (to penalize the agent adding useless pixels). I've tried a few other things like directly using the completeness score as the reward, but this technique is the overall best I can achieve, but I believe that my system is not very optimal which might cause the poor performance, as I think that the design of the reward system is essential to the good performing of a RL agent. I'm therefore open to any suggestions on how to improve or re-think the reward system by anyone that's got a bit more experience than me in designing RL agents when it's not as obvious as for some simple games.
Thanks in advance for your help !
","['reinforcement-learning', 'dqn', 'game-ai', 'image-segmentation', 'reward-design']",
How to calculate adjusted and normalized fitness when a higher raw fitness is better,"
I am reading Genetic Programming: On the Programming of Computers by Means of Natural Selection by John R. Koza.

For calculating the ""standardized fitness"" of an individual, where a lower raw fitness score is better:
$s_i = r_i$
where $s_i$ is the standardized fitness, and $r_i$ is the raw fitness of the individual. Lower standardized fitness scores correspond to fitter individuals.
In problems where an individual with a higher raw fitness score is better, Koza suggests this:
$s_i = r_{max} - r_{i}$

The adjusted fitness of each individual is $0 <= a <= 1$, and it is calculated like this:
$a_i = \frac{1}{1 + s_i}$
Higher adjusted fitness scores correspond to fitter individuals.

The normalized fitness is calculated like this:
$n_i = \frac{a_i}{\sum\limits_{k=1}^{M}{a_k}}$
where $M$ is the number of individuals in the population.
The sum of the normalized fitness of all individuals equals to $1$.


My issue is this: in problems where individuals with a higher raw fitness score are fitter than those with lower raw fitness scores, and $r_{max}$ is not known, how do I calculate the adjusted fitness and normalized fitness?
This is my approach:

Ignore the standardized fitness, and calculate the adjusted fitness scores directly using raw fitness scores.
$a_i = 1 - \frac{1}{1 + r_i}$
$n_i = \frac{a_i}{\sum\limits_{k=1}^{M}{a_k}}$ (same as the textbook).

This seems to works well. However, to my surprise, the textbook is suggesting a different method (section 6.3.2):

If no upper bound $r_{max}$ is known and a bigger value of raw fitness is better, the adjusted fitness and the normalized fitness (both described below) can be computed directly from the raw fitness. If a smaller value of raw fitness is better and no lower bound is known, the sign can be reversed and the adjusted fitness and the normalized fitness can be computed directly from the raw fitness.

This suggestion is most puzzling:

When a bigger value of raw fitness is better, the book is suggesting to use $a_i = \frac{1}{1 + r_i}$, but higher $r$ values will produce lower $a$ values, which is wrong because higher $a$ is supposed to correspond to fitter individuals!
When a smaller value of raw fitness is better, the book suggesting to ""reverse the sign"". This would make all $a$ values negative. It would also make $n$ wrong because the weakest individuals will end up having the highest normalized fitness values!

So, please help me understand the quote above. How do I calculate the adjusted fitness and normalized fitness when higher raw fitness scores correspond to fitter individuals?
","['genetic-algorithms', 'genetic-programming', 'fitness-functions']",
"Using ""softmax"" (non-linear) vs ""linear"" activation function in Deep Reinforcement Learning","
I am following the tutorial in this video: https://youtu.be/cO5g5qLrLSo which implements deep reinforcement learning (DQN) to balance cart pole in OpenAI default environment.
The DQN model looks like as follows:
model = Sequential()
model.add(Flatten(input_shape=(1,states)))
model.add(Dense(24, activation='relu'))
model.add(Dense(24, activation='relu'))
model.add(Dense(actions, activation='linear'))

Full code is available here: https://github.com/nicknochnack/TensorflowKeras-ReinforcementLearning/blob/master/Deep%20Reinforcement%20Learning.ipynb
The implemented code is also available on Google Collabs for you to run/test here: https://colab.research.google.com/drive/1oQILItVu6Y8jOCprzwMGzwlztYmVKK-F?usp=sharing
I do understand the concept and mathematics behind using ""linear"" vs ""non-linear"" (softmax) activation function in the output layer.
But, what I am struggling to understand is that why in this target application linear activation function is used in the output layer instead of softmax? Can someone specify how to realize that which type of activation function will be the best for what type of target application using DQN?
P.S. I tried to change the activation function to softmax instead and got completely different result. Hence, I am confused as to why changing the activation function in the output layer could generate completely opposite results.
","['reinforcement-learning', 'dqn', 'deep-neural-networks', 'activation-functions']",
Order of multiple Convolutional and Pooling layers in generated CNNs,"
I am reading this article: https://www.sciencedirect.com/science/article/pii/S2210650221000249
There, a multi layered particle swarm optimization of CNN parameters is presented. First step (layer) is to determine the architecture of the CNN and the second is to determine hyperparameters of each layer. On this image, all parameters (and their ranges) which are being determined are presented:

As you can see, there are three parameters regarding the architecture of the CNN, which are :

Number of convolutional layers
Number of pooling layers
Number of fully connected layers

What I am confused about is (and it is not written in the article) when these parameters are determined, how to build an actual CNN out of this. I understand that all fully connected layers are at the end of the CNN. I also understand that Convolutional layer is usually followed by a Pooling layer. But what I don't understand is how to determine the actual architecture if we have a different amount of Convolutional and Pooling layers. What is the order of those layers if that is the case?
","['deep-learning', 'convolutional-neural-networks', 'convolutional-layers', 'pooling', 'particle-swarm-optimization']",
"Why don't we wait if there is no patrons, in this decision tree from Russel and Norvig's book?","
I'm reading Russel-Norvig's book about artificial intelligence and now at chapter decision tree where this figure is shown:

So far I understood it. This decision tree should answer the question if we have to wait for a table or not. Obviously we don't have to wait if there are no patrons and if the restaurant is full, it depends on the waiting time. If it is 0-10 then we wait but if it is longer than 60min then we don't have to wait?
But why do we have to wait if there are some patrons? I mean there will be free tables, so why do we have to wait?
Then I try to change the question to ""should we wait?"" Then it makes more sense. If it is just 0-10min we should wait but if it is longer than 60min we should not. If there are some patrons, then it is ok we can wait. But the case ""None"" doesn't make sense. Why don't we wait if there is no patrons? What are the reasons? Bad restaurant?
","['decision-trees', 'norvig-russell']",
calculating the value of a state in an optimal policy analytically and iteratively,"
I am watching the lecture by Abbeel on MDPs and Reinforcement Learning. The setup of the problem is the classic gridworld with optimal policy (and corresponding values of states) pictured below. 
The parameters of the problem here are: there is a 0.8 chance that you go to your chosen action and 0.1 chance you go to a direction perpendicular to it. For example, if you choose ""right"", then you have a 0.8 chance of going right and a 0.1 chance of going up, and another 0.1 chance of going down. If the action makes the agent 'bump' into the wall or 'leave' the world, then the agent stays in the same place.
My question is this: Since the problem above shows the optimal policy (having solved/demonstrated the solution via the Value Iteration algorithm), is it possible that I solve for the values of the states manually (analytically), knowing that the optimal policy is give above? I expected that if I setup the (system of) equations, I should be able to recover the values that are printed in the figure above.
I represented the states of the gridworld as follows. $X$ is just a place holder since it is a blocked state.
$\begin{array}{cccc} H & D & A & +1 \\ G &X & B & -1 \\ F & E & C & J \end{array}$
Knowing that $V^{\pi}(s) = \displaystyle\sum_{a} \pi(a|S) \displaystyle\sum_{s'} P(s'|s,a) [r_{t+1} + \gamma V^{\pi}(s')]$, $\gamma = 0.9$, there are no rewards awarded for every step except with entering the terminal states $+1$ and $-1$.
The actions are already deterministic at this point, so I set $\pi(a|s) = 1$.
For simplicity of notation, I just wrote $A$ as the value of state $A$ in the following equations:
At state $A$: $A = 0.8[1+0] + 0.1[0.9A] + 0.1[0.9B]$.
Meaning: 0.8 chance of going right, receiving a reward of 1, and the value of the terminal state is 0. 0.1 change of going up and therefore ending up in the same state $A$. Finally, 0.1 chance of going down, ending up in state $B$.
Another example: $H = 0.8[0.9D] + 0.1[0.9H] + 0.1[0.9G]$
Meaning: 0.8 chance to move right to D, 0.1 chance to bump the upper wall and stay in H, another 0.1 chance to move down the lower state G.
There are a total of 9 equations of this form. When I solved for the unknowns $A,B,\cdots,J$, why am I not getting the values placed here in this figure?
Edit: I found an error that made the values way above 1. The error is now fixed. The values are now below 1, which is good. But why am I not getting the values indicated in this figure?
Please give some insights.
","['reinforcement-learning', 'value-functions', 'bellman-equations', 'value-iteration']","In the gridworld setting, you can replicate the lecture's results, by defining the reward function $R_t(s,a) = R(s)$, meaning that the reward function simply aggregates only on the current state and ignores the selected action and the next state, as well. With this modification, the state value of a terminal state always (after the 1st iteration) equals its reward (+1 or -1).However, this is not very common in MDP implementations (e.g. in gym envs). I think that when this is the case, it should always be highlighted."
How to model graph node as priority list over a visual scene in neuro-symbolic AI?,"
Suppose if we have a visual scene graph and we model each component in the scene as a node of a graph and edges which are relationship between the visual scene components. Some of the nodes are like agent who can move(say human as node can move but not tree in scene graph). However, their movement or action, like who will move first is based on some rules. How to approach the problem of node priority list generation in this case. Like node A will move first, followed by node B and C. Usually graph are modelled as classification problem or link prediction problem. Not able to find a way in which such kind of priority list generation is tackled over graphs (or knowledge graph).
Or suggest any other approach to tackle it without graph network or knowledge graph.
","['graph-neural-networks', 'knowledge-graph', 'knowledge-base', 'neurosymbolic-ai']",
Is the initialisation of $V(s)$ and $\pi(s)$ really arbitrary in policy iteration?,"
In Sutton and Barto's book (Reinforcement learning: An introduction. MIT press, 2018), the algorithm ""Policy Iteration"" is:

Here, $V(s)$ is initialized arbitrarily, meaning that I can choose anything I want. Moreover, I think nothing is stated about $\gamma$ here so we can consider undiscounted environments where $\gamma = 1$.
Now suppose we use the following environment:

If I initialize:

$V(s_1) = 10$
$V(s_2) = 10$
$V(s_3) = 0$
$\pi(s_1) = a_1$
$\pi(s_2) = a_3$

With this, it appears that the ""Policy Evaluation"" part will not have any effect and the algorithm will immediately stops, outputing a policy where the optimal actions are $\pi(s_1) = a_1$ and $\pi(s_2) = a_3$. What am I missing ?

EDIT: I made a toy repository to reproduce, if you want to tweak the numbers of point out something I misunderstood: https://github.com/Gregwar/policy_iteration_initialization/blob/master/policy_iteration.py
","['reinforcement-learning', 'value-functions', 'policies', 'policy-iteration']",
Head Pose estimation using Car Interiors depth infromation,"
I am trying to determine head pose of a driver sitting in a car with the depth interiors of the car known to me. Is there any research work which exploits that information in determining the head pose of the driver ?
One paper along the lines of using depth as a privileged information are,

https://arxiv.org/abs/1811.07376
which talks about using depth as a privileged information while training and not while inferring.

Also there are papers where depth is used as an auxiliary depth information in facial recognition.
https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Learning_Deep_Models_CVPR_2018_paper.html
But I am not able to find any reference where the depth of the background is used as an additional information in head pose/hand pose/human pose estimation.
Any help would be great!
","['computer-vision', 'reference-request', 'pose-estimation']",
Mask R-CNN: How are the computed masks projected back to the input image?,"
The computed masks by Mask R-CNN are of fixed size $m \times m$ each. How are they projected back to the image?
","['deep-learning', 'object-detection', 'instance-segmentation', 'mask-rcnn']",
Are Best-Arm Bandits considered to be reinforcement learning?,"
Multi Armed Bandits (MABs) are a broad field of research pursuing different streams. In addition to the common objective of maximizing the cumulative reward, there are also so-called Best-Arm (identification) Bandits, cf. Lattimore and Szepesvári (2020), Chapter 33 or Audibert et al. (2010). The goal of the latter algorithms is to identify only the best arm (consequently, they aim to maximize the simple regret instead of the cumulative regret).
Since a Best-Arm Bandit also involves a stochastic environment and also receives rewards by taking actions, I wonder if Best-Arm Bandits belong to the domain of reinforcement learning?
I came across this related post (which, however, refers to Multi Armed Bandits in terms of the cumulative regret). From the responses in the post, my understanding is that the ""purely evaluative feedback"" should also apply to Best-Arm Bandits.
My intention would categorize Best Armed Bandits as Reinforcement Learning, not only because of the presence of the evaluative feedback but also because they constitute the simplest form of all Reinforcement Learning problems at all, namely a Markov Decision Process with only one single state $s$, a discrete set of actions $a\in A$ and a reward function $r(s,a)$ with the associated goal of maximizing the average reward. For an infinite horizon, this should be the same goal as identifying the best arm.
","['reinforcement-learning', 'multi-armed-bandits']",
Why is Reinforcement Learning viable compared to just only using Neural Network? [duplicate],"







This question already has an answer here:
                                
                            




Can supervised learning be recast as reinforcement learning problem?

                                (1 answer)
                            

Closed last year.



I am confused as to how RL is viable compared to just using a simple NN.
I have data such as the following:
x1 | x2 | x3 | y
3  | 5  | 7  | 6
4  | 3  | 2  | 15
... and so on

where x are the input and y is the output.
Let's say I am given x1, x2 and I need to find x3 such that y is as close as 10. I can just simply do neural network on the dataset above, set the goal of y to be 10 and do a 'solver' to find the optimal x3.
My dilemma with reinforcement learning is that if you just restate this problem such that x1 x2 are now called states, x3 is now action and -abs(y-10) as the reward function, this problem seems like it can be solved using RL as well. Maybe it is just like an off-line RL? This leads to my question of how could RL be useful if just a neural network can used to handle this kind of problem, especially when
RL and NN both are identically solving for the best action or the x3, respectivley?
I am sure I am overlooking some fundamental knowledge here.
Adding on: I realized that I am missing the state transition or the 'next state' part of RL. But even for the next state, wouldn't NN be able to solve for what the best action/x3 is for that given state or the x1,x2?
","['neural-networks', 'reinforcement-learning', 'machine-learning', 'deep-learning']","Your analysis seems correct, that both approaches - searching using a neural network approximator for all three params, and using a trained reinforcement learning (RL) agent that selects the third param as an ""action"" - should work. The quality of the solution, given a fixed training set, is likely to be very similar if not identical. The main difference would be that training the RL agent would take longer since it is driven indirectly via the reward function and exploring alternative actions as opposed to directly from loss function on the approximation for $y$.You note that there is no state transition (or time step) in the problem, and that does mean RL is over-specified. A related approach, called contextual bandits, would be a closer match. Again in your example, it will be slower because of trial-and-error search for solutions compared to directly approximating the function.In general, if you have a fixed dataset and simple-to-express goal in terms of optimising a function that you know, then it can be posed as an RL problem. RL solvers will then usually work, but will most often be inefficient compared to other optimisation approaches.RL, and learning through trial-and-error, is a very general learning mechanism. It can often be adapted and applied to scenarios where other algorithms also work. As you have done in the question, the basic approach is to map state, actions and rewards from the original problem description to frame it as RL. In your specific case, you must train the agent both offline and off-policy, because the agent cannot actively make new guesses for x3.If this problem framing is done accurately, then RL can be applied. However, this is not always a good idea.The more ""natural"" the fit of a problem is to the concepts of agent, environment, state, action and reward, then the more likely it is that RL will be a good fit to the problem. Unlike your example, it can be applied in situations where you do not have a dataset and the agent must actively collect experience.Also unlike your example, RL can be applied in scenarios without target variables to learn, using a reward signal directly as feedback. Although sometimes explicitly learning target variables from the environment other than expected return or optimal action could help an agent (e.g. by regularising internal feature representations in its neural network if this is done using a multi-headed NN)."
Multiclass Ensemble Methods with weak classifiers under 50%,"
Normally, when using an ensemble method, such as baggin or boosting, in binary classification, there is a reqauirment that each weak classifier have accuracy better than 50%.
In the multiclass claaification setting, this is often infeasible. Is there a way to improve upon multiclass classification with ensembles.
For an example to make this concrete:
Say I have a problem with 1000 classes, and I train 50 models, each with 10% accuracy, which is 100x better than random guessing.
Is there a way to combine these models to form a better classification algorithm?
","['machine-learning', 'classification', 'multiclass-classification', 'ensemble-learning', 'boosting']",
MC control with exploring starts: do we need to update the current $\pi(S_0)$ to the randomly chosen $A_0$ as well?,"
Consider the following pseudocode of
MC control with exploring starts:

When we choose $A_0$ randomly for state $S_0$, do we need to update the current $\pi(S_0)$ to
the randomly chosen $A_0$ as well?
",['reinforcement-learning'],"When we choose $A_0$ randomly for state $S_0$, do we need to update the current $\pi(S_0)$ to the randomly chosen $A_0$ as well?No.Under a given policy $\pi$, the action value of a state $Q_{\pi}(s,a)$ is the expected return when taking action $a$ in state $s$, and from that point on following the policy $\pi$. The action you are sampling for updates can be one that the policy would not take.In general, when you are considering a specific action value - whether to sample it or update it - you are not concerned with how the agent got into the state and action involved. You only care what happens afterwards.For exploring starts, you are not even concerned whether reaching any given state is possible under the policy being updated."
Does importance sampling really improve sampling efficiency of TRPO or PPO?,"
Vanilla policy gradient has a loss function:
$$\mathcal{L}_{\pi_{\theta}(\theta)} = E_{\tau \sim \pi_{\theta}}[\sum\limits_{t = 0}^{\infty}\gamma^{t}r_{t}]$$
while in TRPO it is:
$$\mathcal{L}_{\pi_{\theta_{old}}(\theta)} = \frac{1}{1 - \gamma}E_{s, a \sim \pi_{\theta_{old}}}[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}(a|s)}}A^{\pi_{\theta_{old}}}(s,a)]$$
There exists some problems of vanilla policy gradient, such that distance dismatch between parameter space and policy space and suffering poor sample efficiency.
For tackling this problem, TRPO introduces importance sampling for improving this. However, when I compared the pseudocode of two algorithms, I don't see any obvious evidence that aids the point. They all firstly sampled multiple trajectories under the current policy. The former then just uses the estimated gradient of loss function to update the parameter $\theta$. The latter also uses the estimated gradient but under the KL constraint to update the parameter $\theta$. It looks like they are almost the same procedure but with some subtle difference of the optimization process.
My question is - should it be more accurate to say that importance sampling just increases the stability of the sequential decision making process, or rather, avoid update too aggressively, but not directly improves the sample efficiency?
Pseudocode of two algorithms (source from OpenAI spinning up document)


","['reinforcement-learning', 'policy-gradients', 'importance-sampling', 'trust-region-policy-optimization', 'sample-efficiency']",
How to represent multiple-output logic circuits in tree-based genetic programming,"
Consider the following digital logic circuit, which has multiple inputs and one output:

The logic circuit above can be represented in tree form:

This tree representation could then be used in a tree-based genetic programming framework to evolve the circuit. For example, this tree could be represented as a Lisp list (or (and A B) (not C)), which could then be used with the Little LISP genetic programming framework from John R. Koza's Genetic Programming textbook.
However, I now want to deal with digital logic circuits that have more than one output. For example, in the half-adder circuit below, there are two outputs $S$ and $C$, each of which is affected by both inputs $A$ and $B$.

(Image source: SICP by Abelson et al. Section 3.3.4 A Simulator for Digital Circuits. CC BY-SA 4.0)
How do I represent and evolve such a circuit in tree-based genetic programming? How do I represent the circuit above as a tree that could then be used to evolve the circuit using a tree-based genetic programming framework?
","['ai-design', 'applications', 'evolutionary-algorithms', 'logic', 'genetic-programming']",
Mask R-CNN: how is the inference done?,"
According to the Mask R-CNN paper and the picture below (taken from the paper), the mask branch is computed in parallel with the bbox classification and regression branches.

However in the paper they write that inference is done differently from training, not in parallel:

Inference: At test time, the proposal number is 300 for the C4
backbone (as in [36]) and 1000 for FPN (as in [27]). We run the box
prediction branch on these proposals, followed by non-maximum
suppression [14]. The mask branch is then applied to the highest
scoring 100 detection boxes. Although this differs from the parallel
computation used in training, it speeds up inference and improves
accuracy (due to the use of fewer, more accurate RoIs).

How is this actually done? By construction, the masks are outputted in parallel with the bounding boxes. So how can they run the mask branch after the bounding box prediction? Do they run it twice?
","['deep-learning', 'image-segmentation', 'instance-segmentation', 'mask-rcnn', 'faster-r-cnn']",
Are there any scale invariant activation functions that outputs probability distribution?,"
Softmax activation function is used to convert any random vector into a probability distribution. So, it is generally used as an activation function in the last layer of deep neural networks that are intended for classification.
But, the softmax() does not satisfy the property of scale invariance i.e., the ratio of inputs and the ratio of outputs does not remain the same.
For example if we give the input [1.4285, 0.3815] to softmax, the function will give [0.7402, 0.2598] as output.We can calculate ratios 1.4285: 0.3815 and 0.7402: 0.2598 and find that they are not the same.
Are there any scale-invariant versions of softmax?
","['neural-networks', 'activation-functions', 'probability-distribution', 'softmax']",
Should minimax with alpha beta pruning depth be an odd number?,"
I implemented the minimax algorithm with alpha-beta pruning to see how it works, with application to the connect four game.
My AI works fine, considering the AI is the MAX player (VS human player, which is the MIN), e.g. the Minimax root node of the tree.
However, since the root node is always the MAX player (AI), does this means that the leaf nodes, where pruning occurs, should be at an odd depth?
Whenever I use an odd number for depth, my algorithm works fine. But when I use an even number, the bottom node because a min node and AI looses the game.
Can anyone confirm that when using MAX as the root node, the depth should always be an odd number? AFAIK, it's never said explicitly in the algorithm's description.
Thanks
","['minimax', 'alpha-beta-pruning']","Following Vintarel's advice, I reviewed my code and saw there was an error. Indeed, the evaluation function returned different values, depending on the AI or the player. So, for example, it would return -3 for the AI and 3 for the player, resulting in a wrong max/min optimization.I modified the evaluation function, so that it returns the same value, independently of the player. After, the score is minimized or maximized, according to the current player (AI or human). In the end, odd/even depth has no impact and the algorithm works as expected!
Thanks"
What can be an example other than batch normalization that uses statistics of batches?,"
Consider the following paragraph, taken from OPTIMIZING BATCHES of the textbook named Deep Learning with PyTorch by Eli Stevens et al., regarding the reasons for processing data into batches

The reason we want to do this batching is multifaceted. One big
motivation is to make sure the computation we’re asking for is big
enough to saturate the computing resources we’re using to perform the
computation. GPUs in particular are highly parallelized, so a single
input on a small model will leave most of the computing units idle. By
providing batches of inputs, the calculation can be spread across the
otherwise-idle units, which means the batched results come back just
as quickly as a single result would. Another benefit is that some
advanced models use statistical information from the entire batch, and
those statistics get better with larger batch sizes.

The bolded portion says a statistical reason for batch processing. I know only batch normalization that satisfies the portion as mean and standard deviation are calculated over the batch of inputs.
Is there any such example that needs statistics of batches that are apt to the description?
","['batch-normalization', 'statistics', 'batch-size']",
"In general - is Stochastic Gradient Descent a ""superior"" algorithm compared to Gradient Descent? [closed]","







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed last year.







                        Improve this question
                    



On a very informal level, if we were to compare the (classical) Gradient Descent Algorithm to the Stochastic Gradient Descent Algorithm, the first thing that comes to mind is:

Gradient Descent can be considered as a slower algorithm than Stochastic Gradient Descent, since it requires performing mathematical operations (e.g. derivatives, second derivatives) on the entire data set. On the other hand, Stochastic Gradient Descent can be considered as a faster algorithm, since it approximates the gradient using ""mini batches"" of observations from the data set. Logically, this makes sense : Stochastic Gradient Descent requires fewer calculations compared to Gradient Descent, therefore the same computer should take less time to perform fewer calculations compared to more calculations.

However, (in the spirit of tradeoffs) the somewhat logical counter point to the above argument is that:

Since Stochastic Gradient Descent approximates the gradient of the function whereas Gradient Descent evaluates the full gradient - one would imagine that perhaps Gradient Descent might have a better ability to find the true minimum of the function compared to Gradient Descent. However, I am not sure about this.

My Question: In the past few years, a lot of research has been done about studying the behavior (e.g. theoretical convergence properties) of Stochastic Gradient Descent (e.g. https://arxiv.org/abs/2103.14350 , https://raghumeka.github.io/CS289ML/gdnotes.pdf) which have demonstrated similar abilities of the Stochastic Gradient Descent Algorithm to converge as compared to Gradient Descent.
But are there any theoretical results which expose the above (speculated) tradeoff? At the end of the day, if Stochastic Gradient requires less computing power when compared to Gradient Descent - are there any theoretical results that suggest Gradient Descent has an inherent ability to better find the minimum of the function (e.g. perhaps less likely to get ""stuck"" in saddle points) since it is not relying on an approximation of the gradient? Or if Stochastic Gradient Descent is equal to Gradient Descent in this regard - could Stochastic Gradient Descent be then considered as superior to Gradient Descent?
Can someone please comment on this?
Thanks!
","['neural-networks', 'gradient-descent']","Stochastic Gradient Descent empirically has shown to lead better results than classic Gradient Descent since its formulation, and today we're getting close to understand that it's not just luck, but the results of better mathematical qualities as well. SO the answer to your question is no, the theoretical results also show that SGD is better than classic GD.Check these papers offer different explanations about why SGD tend to perform better that classic GD:Quick summaries:Extra note:
it's not hard to understand conceptually how SGD can help escaping saddle points and local minima. When computing a single step using all gradients of all training instances like GD does, we don't leave much room for exploration of the loss function surface. So if our training instances point to a local minima that will be the solution found by GD. On the other hand, SGD compute gradients only on a batch of instances, and therefore each update is a suboptimal step toward the negative direction of the gradients. Being suboptmal, each step leave room for exploration, i.e. if an update step points to a local minima it might be that the next step point in a different direction, something that can't happen in GD since we perform a single update. Same conclusion, but different conceptualization comes from the interpretation of the last paper. They prove that SGD can be seen as GD (so again, single update step) performed on a loss function convolved with noise. Noise convolution has the effect of smoothing out the function (think as a metaphor to combine random frequencies, they are more likely to cancel each other out than boosting each other). Smoothing the loss function means precisely removing small local minima, as you can see in the last picture. The consequence is the same as in the first explanation, i.e. less chance of getting stuck in a local minima."
"Can we Consider Regularization as a ""Constraint""?","
I have the following question on ""Regularization vs. Constrained Optimization"" :
In the context of statistical modelling, we are often taught about ""Regularization"" as a method of dealing with the ""Bias-Variance Tradeoff"" (i.e. stabilizing the inconsistent performance of complicated models). When a L1-Norm or L2-Norm Penalty Term is added to the estimation function (corresponding to the statistical model) being optimized, some of the model parameters will either ""shrink"" in size towards 0, thus producing a ""sparser"" model that is more likely to retain its ""low bias"" but possible reduce its ""high variance"":

I have often heard of functions containing these L1-Norm and L2-Norm ""Penalty Terms"" being referred to as ""optimization constraints"" (i.e. the ""feasible region"" from which valid choices of model parameters can belong to has now been ""altered"" due to these ""norm penalty constraints""):

My Question: When we estimate some statistical model's parameters and the estimation equation contains some ""regularization penalty term,"" would it be incorrect to refer to this as an example of ""constrained optimization""?
Is regularized optimization in Machine Learning and Statistical Modelling fundamentally any different (with the exception of usually being more difficult and solved using approximate stochastic iterative methods) from Constrained Optimization in Linear Programming?

References:

https://en.wikipedia.org/wiki/Regularization_(mathematics)
http://ab-initio.mit.edu/wiki/index.php?title=NLopt_Tutorial

","['neural-networks', 'regularization']",
How to count overlapping objects with neural networks,"
Consider the following task to be solved by a neural network: Given a $N\times N$ pixel grid with up to $M$ objects drawn on it, either squares (9 pixels) or diamonds (5 pixels):
square
   diamond
The objects may overlap. The task is to give the minimal possible numbers of objects per shape that can be ""seen"" and distinguished in the picture and tell how many squares, how many diamonds, and how many objects with unknown shape there are.
Here are some examples with $N = 7$ and $M=5$ with their intended numbers ($n_\square, n_\Diamond, n_?$). The examples with $n_? = 1$ are those with pixels that may either come from a square or a diamond (highlighted in black, but not bearing any information that may be used).

I wonder if this task can be solved for general $N$ and $M$ by simple multi-layer networks of standard neurons (e.g. McCulloch-Pitts cells) and how to design and train them.
I further wonder if it could be a standard exercise in an introductory course in neural networks to ""hand-draw"" a neural network that solves the task (by giving explicit weights). If so I'm happy to see a standard solution (full-blown).
This exercise could foster explainability and understandability of networks, I guess.
","['neural-networks', 'object-detection', 'deep-neural-networks', 'pattern-recognition', 'explainable-ai']",
Can GPT-3 write an entire program if provided with detailed UML and ER Diagrams?,"
We already have tools like Denigma.app & Figstack.com (AI-powered code explanation in English), Codesee.io (Codebase and dependency mapping).
There's also a tool called Enzyme.so which converts English instructions to code.
All of us know about the capabilities of Github Copilot, OpenAI's Codex and recently Deepmind's AlphaCode.
Taking all these advances into consideration, is there a possibility that GPT-3 or any sufficiently advanced engine can generate an entire codebase or program out of thoroughly detailed UML and ERD documents?
Have there been any advances in this direction already?
","['open-ai', 'deepmind']",
how can a VAE learn to generate a style for Neural Style Transfer?,"
I have come across this research paper where a Variational Autoencoder is used to map multiple styles from reference images to a linear latent space and then transfer the style to another image like this:

What I don't understand from the paper is how does the Variation module learn the style of the reference images as opposed to the content?
And since VAEs are generative models, is it possible to generalise this to generate styles from the trained variation model by feeding it random noise instead of the encoded reference images?
","['variational-autoencoder', 'style-transfer']",
Confusion about state-action value function notation used in Sutton-Barto RL book,"
Let $\pi$ be an $\epsilon-soft$ policy with state-action value function $q_{\pi}(s,a)$
and $\pi'$ be an $\epsilon-greedy$ policy with respect to $q_{\pi}$.
In Sutton-Barto RL book  (page 101, eq. 5.2), they define
\begin{align}
q_{\pi}(s, \pi'(s))=\displaystyle \sum_{a}\pi'(a|s)q_{\pi}(s,a).
\end{align}
Normally $q_{\pi}(s, a)$ means taking action $a$
at state $s$ and then following the policy $\pi$. Based on this convention, the notation
$q_{\pi}(s, \pi'(s))$ is weird because $\pi'(s)$ is not a single action like $a$.
I.e., $\pi'$ is a stochastic policy
and hence only $\pi'(a|s)$ makes sense.
",['reinforcement-learning'],"The book presents a slight abuse of notation where $\pi'(s)$ is shorthand for a distribution of action values described by the more correct $\pi'(a|s)$. At that point there is an implied function composition of $q_{\pi}$ over this distribution, resolved on the right hand side to its expectation.I believe it does this so that it can briefly show something familiar from the deterministic policy improvement theorem. You could almost read it as ""the equivalent of this term (LHS) taken from the previous proof must now be written like this (RHS)"".It would be more correct notation to write something like this:$$\mathbb{E}[q_{\pi}(s,A')|A' \sim \pi'] =  ...$$or perhaps just start with the right hand side."
How to deal with datasets which are not balanced?,"
I have a dataset that I want to use for training.
The output of the model is a binary value (0,1)
The dataset is not balanced, it has only 200 entries for output 1 and 4000 entries for output 0.
When I tried to use it with LightGMB, the model always predict 0 and for this reason, it is not good.
How can deal with an unbalanced dataset?
One way that I can think of it is to delete several of 0 entries and only use around 200  entries with an output of 0.
This is not good, as the model can not see all datasets.
What is the best way to deal with unbalanced datasets?
","['machine-learning', 'imbalanced-datasets']",
how to detect ouliers in audio dataset?,"
I'm currently working on an audio classification project using CNNs. The problem is I'm having trouble training my CNN. I doubt if there are outliers in my dataset but I don't know how to detect outliers in an audio dataset. I've searched google and found nothing helpful.
",['data-preprocessing'],
"Where can I find authentic references on ""categorical cross entropy"" and ""categorical accuracy metric""?","
My Python source code uses TensorFlow and Keras to implement a neural network.
The Keras source code uses something called ""categorical cross-entropy"" and ""categorical accuracy metric"". I have searched a lot of books on NN theory, and no one talks about these two specific terms. Yes, they talk about ""cross-entropy"" and ""accuracy metric"" but there are no mentions of ""categorical ..."".
N.B. These terms can be found only in the so-called ""Hands-on"" books.
Can anyone please supply me with authentic references on these two specific terminologies?
","['tensorflow', 'python', 'keras', 'accuracy', 'categorical-crossentropy']","Categorical just means that we will conduct multiclass classification. The output of the classifier is a binary vector. Each entry $x_i$ in the binary vector is a prediction whether or not the input is part of class $C_i$.In that sense, categorical accuracy introduces nothing new: it is just the accuracy of a multiclass classifier. On the other hand, categorical cross-entropy refers to the joint entropy: $H(X_1, X_2) = - \sum{p(x_1,x_2)\log_2p(x_1, x_2)}$, where each random variable $X_i$ expresses whether or not the input is to be classified into class $C_i$. Kevin Murphy's book ""Probabilistic Machine Learning: An introduction"" is a great source of reference for many topics of machine learning, including cross-entropy and joint entropy.The random variables could be mutually exclusive or not, depending on the problem."
Can Deep Reinforcement Learning come up with heuristics for a game it trains on and masters?,"
I am taking a course where we write minimax, alpha-beta pruning and interative deepening in Python for the game of Isolation.
I am supposed to write heuristics for an evaluation function of the game state.
But I wondered if training an RL agent on this game, have it master it, are there any examples of a deep RL agent that returns heuristics based on what it learns while training?
","['reinforcement-learning', 'minimax', 'heuristics', 'alpha-beta-pruning']","An optimal value function from reinforcement learning on a game is a perfect heuristic, allowing a single ply search for the best action.Other than very basic games, such as tic tac toe (noughts and crosses), an RL agent will not learn a perfect value function. The issue is then that the value function is not admissible (it may over-estimate as likely as under-estimate), therefore it can mislead a search.However, in practice, the non-admissibility of a heuristic in game playing agents is not a complete showstopper. It means the agent will be less efficient and may make some mistakes, but an approximate learned heuristic is stll pretty good.In practice, the main issue I have found when using a neural network or other ML approximator in minimax is the number of evaluations that need to be made. For efficiency you may be able to batch them up, but for a significant depth you may still need 100s or 1000s of value function prediction runs to resolve the minimax. This can take far more CPU time than a simpler heuristic.In my own project (playing Connect 4) where there was a strict time constraint for turns, I resolved this issue by:This combination performed well enough to get into top 10 positions in a coding competition where several competitors had coded perfect play. The main issue with it is that the first search could prune a branch for good reasons that the second search then preferred due to inaccuracies in the value function, causing the agent to lose to perfect play agents. There are probably ways around that, but I did not investigate further."
How to encode actions for training a Wordle agent?,"
I'm trying to train an AI using reinforcement learning to play the game Wordle.
The way the game works is that there is a secret five letter code word that you need to guess, and every time you guess a word (it has to be an actual word it can't just be any five letters) the game tells you what letters of your guess are correct and what letters of your guess are in the code word but not in the correct position. If you guess the word in six guesses or less, you win.
I want every step the AI to choose a string from a list of valid guesses. Afterwards I will reward it for discovering letters for the first time and discovering their position for the first time. It will be penalized for losing and rewarded for winning based on how early it won.
The problem is that I can't just number each word and have it choose a number (I think?) because the actual content of the string it chooses matter.
How should I tackle this issue?
One thing that might be worth mentioning is that while I'm aware this could be solved algorithmically without the use of reinforcement learning, I want to try this approach anyway. This is less about the finale product and more of an excuse to get familiar with machine learning.
","['reinforcement-learning', 'tensorflow']","A quite effective way to solve Wordle is to maintain an active list of all five-letter words that meet criteria so far, choose one of them completely at random, then update the list of possible words based on new knowledge from the response.A slightly smarter way would be to pick words from the subset of remaining ones that should maximise information gathered from the response on average. I believe I read a headline recently that ""crane"" is a good starting English word for that reason.The trouble for you here is that those approaches do not involve ML in any form. Relatively simple logic and analysis are already pretty good at playing Wordle automatically.However, to stick with the question as asked, your problem is how to get a machine learning algorithm to choose between maybe 10,000 options.You have a couple of choices:Brute force. Number the options, have the algorithm choose directly. If you do this, I recommend that you also filter out logically impossible words, as that makes the problem far easier. The 10,000 options may seem large at first, but actually this should be within reach of a personal/hobby setup. It will work mainly because it is possible to simulate the results of games very fast. So generating the training data within RL will be fast, the agent can get to see millions of games in a reasonable time frame, training with each target word thousands of times on average.Actions represent additional constraints on word choice. For a simple example, you could have the agent decide whether the first unknown letter it wants to try is a vowel or consonant = just two actions. You can use that to further restrict word choice from the remaining words. This influences an otherwise random word selector, and the agent should learn situations in which to pick words with certain traits.Actions describe a ""word vector"", and the selector picks the closest valid word to that vector, breaking ties randomly. In order to do this, you need to generate some short vector of each word in the dictionary. For a simple example* you might choose [part_of_speech, popularity] to qualify each word, scoring part_of_speech as 1 for noun, -1 for verb and 0 for everything else and popularity as 1 for common word, -1 for rare word, and 0 for moderately frequent. Then your action output would be a choice from the 9 combinations possible in this vector.None of these approaches are simple, so not great if you are just starting in AI/ML, and I would recommend you tackle a simpler toy problem or two using RL before trying to write your Wordle training routine. Going straight to Wordle without solving smaller problems first is likely to cause a lot of incorrect decisions on your part that you won't spot and will frustrate you.* I have given a very simple example, but this could logically lead to learned word embeddings, which are an important topic in natural language processing. These are typically quite large real-valued vectors though (maybe 64 or 128 elements) which would be an even larger space to learn about than the fixed list of words. So full word embeddings like word2vec and Glove are not really suitable for the Wordle puzzle."
Temporal Graph Neural Network for motion prediction,"
Temporal Graph Neural Networks have been used for motion prediction (or traffic forecasting) in the following recent papers:
Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human Motion Prediction
Graph Neural Network for Traffic Forecasting: A Survey
I was wondering, how to use them or any of the available temporal graph neural networks for motion prediction when the graphs don't have the same number of nodes / edges?
Let's say I have recorded a motion of a skeletons that have 5 joints and 10 joints. Then, after the model is trained, I want to use that model for a motion prediction of skeleton with 7 joints. Each node (joint) has 3 attributes, coordinate x, y and z.
Is that even possible, or is there trivial way how to do that? Could something like graph padding (creating nodes without any edges) work in that case?
For instance, in the first paper when they initialize temporal attention or spatial attention layer, it depends on the number of nodes. It creates a learnable weight matrix with dimension N, therefore if we change the input function (i.e. inference) it fails due to impossible matrix multiplication.
","['prediction', 'graph-neural-networks']",
"Do neural networks, trained with backpropagation algorithm, exploit the concept of synaptic plasticity?","
Is there some of Hebb's rule behind the concept of backpropagation learning rule of a simple supervised neural network, that for example is trained for classification task ?
I was reading about the concept of synaptic plasticity that is explained in simple words here https://qbi.uq.edu.au/:

Plasticity is the ability of the brain to change and adapt to new information. Synaptic plasticity is change that occurs at synapses, the junctions between neurons that allow them to communicate.
The idea that synapses could change, and that this change depended on how active or inactive they were, was first proposed in the 1949 by Canadian psychologist Donald Hebb.

And more in particular I found in Wikipedia that:

Hebbian theory is a neuroscientific theory claiming that an increase in synaptic efficacy arises from a presynaptic cell's repeated and persistent stimulation of a postsynaptic cell. It is an attempt to explain synaptic plasticity, the adaptation of brain neurons during the learning process.

In an artificial neural network, synaptic efficacy (that is the strength of communication between neurons, from https://www.frontiersin.org) depends on weights that are associated to connections between neurons (from what I understood reading the beginning of paragraph 44.3 The basic unit — the neuron in Handbook of Chemometrics and Qualimetrics: Part B https://www.sciencedirect.com).
Thus, when we train our supervised classifier so that it updates weights thanks to backpropagation algorithm (whose philosophy is the changing of weights to minimize an error function that characterizes the comparison between the network output and the ground truth), are we applying a sort of Hebb's Rule and, more properly, the concept of synaptic plasticity since weights change during the learning process?
Or Hebb's rule means only ""Fire together, wire together"" as I understand by reading the answer to this SE post How do you explain Hebbian Learning in an intuitive way?.
I read also this other SE answer to the post Is there a Hebb neural network? in which it is clearly explained that neural networks (or models) that can learn in a Hebbian fashion are different from those based on backpropagation algorithm.
But if synaptic plasticity means a changing in the connections between neurons (synapses), that in artifical neural networks means a changing in the weights, during a learning process, can I say that also neural networks based on back-propagation algorithms exploit the concept of synaptic plasticity (and so the Hebb's rule) ?
","['neural-networks', 'training', 'backpropagation', 'biology', 'hebbian-learning']","Even when it is always said that ""neural network cells"" are ""inspired"" in biological neurons, there are critical differences that made this similitude only an ""inspiration"". Thus, comparison of a CNN or any other kind of NN with brain is always a fuzzy comparison.The biggest difference is probably the learning capacity: the human brain learns by itself, while the neural network needs an external system (the back-propagation algorithm) that feeds the NN with the network parameters (the knowledge).In other words: a neural network, understood as the set of nodes that execute a basic function (lets say weighted addition and activation function) is absolutely unable to learn anything by itself. The knowledge is written in the neural network as a set of weights and other parameters. This is not an intelligent process, as it is not load a program in a conventional computer. The system who learns is the system who runs the back-propagation algorithm and this system is not a neural net.In comparison, the human mind, understood as a network of cells,is able to learn by itself from the experience.Thus, when we train our supervised classifier so that it updates
weights thanks to backpropagation algorithm (whose philosophy is the
changing of weights to minimize an error function that characterizes
the comparison between the network output and the ground truth), are
we applying a sort of Hebb's Rule and, more properly, the concept of
synaptic plasticity since weights change during the learning process?Absolutely no.A learning process based in Hebb's rule will:This is not the method of the back-propagation algorithm:Nowadays, I do not know of any system based on biological neuron behavior and able to learn. Just a few models as been able to learn the eigenvector of a basic process. Even worst, no body as been able to replicate basic behavior of well know biological brains as the one of c-elegans worm, a brain that is completely mapped in number of neurons and synapses between them."
How to learn a neural network with equivalent constraints on the weights,"
Let $f(x)$ be an output of a neural network with input $x$.
My data is a pair $(x,y)$ and my loss function is a function of $f(x)$ and $f(y)$, i.e., $g(f(x),f(y))$.
What kind of architecture enables this learning?
I could make two copies of the same neural network, but can the weights be coordinated in the back propagation?
","['neural-networks', 'architecture']",
How can I calculate the parameter $w$ in the third condition of LVQ 2.1 algorithm?,"
I'm developing a neural network software using several NN architectures including LVQ family. I met a parameter that is used in the 3rd condition of LVQ2 and later versions. It's named $w$ and is used to calculate whether the input vector falls in the $w$-length window gap or not. For example, for LVQ 2.1 it's used to calculate the 3rd condition as follows:
\begin{equation}
\min(d_{i}/d_{j},d_{j}/d_{i})>\frac{1-w}{1+w},0<w<1.
\end{equation}
where

$d_{i}$ is the minimum Euclidean distance from input vector (1st winner neuron's distance)
$d_{j}$ is the 2nd minimum Euclidean distance from input vector (2nd winner neuron's distance)

This parameter $w$ is a user-determined one like learning rate or momentum, etc. I give the value of $w$ to training algorithm manually but this is a trial and error style. During my research, I found an information about this parameter may be calculated via training samples count.
Are there any information about how to calculate this parameter by training samples count or something else like represented training samples count to the network?
","['neural-networks', 'machine-learning', 'deep-learning', 'supervised-learning']",
Additional Optimizations for Convolutional Models On Inferencing,"
I am aware of several ways to optimize a convolutional (or any) model after training to make inferencing quicker. I am currently implementing BatchNormalization Folding and removing Dropout layers from the network. I am also aware of post training quantization (specifically 16-bit quantization for use on GPU).
Are there other layer optimization techniques that I can use other than quantization?
My current model uses, Conv2D, Activation(relu), BatchNormalization, Dropout, Dense layers.
Basic mnist Convnet metrics for 10K images, batch size of 1: (All have 98.92% accuracy)

Original Network: 49.3s
Folded Network: 33.37s
Quantized Original: 7.449s

","['convolutional-neural-networks', 'tensorflow', 'optimization', 'batch-normalization']","You could look into models pruning. There are several techniques out here, and all of them aim to reduce the amount of parameters of a model without affecting its performance metrics. Of course less parameters means less calculation and therefore faster inference time."
neat - what is the purpose of looped networks?,"
So im writing my own implementation of NEAT and i'm wondering how looped
networks (like one shown in the image) can be useful. I'll probably implement
them anyway because i want to fiddle around with as much features and improvements
as i possibly can, but i just cant wrap my head around where they can be
applied. What are the possible tasks that cant be solved (or are less likely to
be solved) without looped networks?
NOTES just in case:
I'm not talking about networks that pass info from the previous timestep output
like those that allow to solve e.g. double pole balancing without velocity info.
(Or are they the same thing? Haven't really figured it out, but either way,
question implies that networks like that are treated as a special case. Hopefully
this remark makes any kind of sense)
Also i know how both looped and feed-forward networks are activated and i have
algorithm in place to identify and separate both of them, im not asking about that.
Also, when i say ""activating"" the looped network, i mean algorithm like sharpNEAT
has in place: https://github.com/colgreen/sharpneat/tree/master/src/SharpNeatLib/Phenomes/NeuralNets/CyclicNetwork

","['neural-networks', 'recurrent-neural-networks', 'evolutionary-algorithms', 'neat', 'neuroevolution']",
Are tabular methods appropriate for this task?,"
I'm a RL beginner and I have a project in mind: I'm an engineer who works doing CAD projects and there is a time consuming task that I want to automatize. This task it's doing by a person because it requires certain logic.
For example, given a CAD model with several object in it (Circles, rectangles, etc) the objetive is to tag every object following certain rules:

Tags cannot cross between them.
Tags cannot overlap other tags.
If possible, the tags must be aligned.
If possible, the tag must be near the object.
If the tag fits inside the object, it must be placed inside it. If not, the tag must be placed outside the object.

This is how a example model looks:

My idea is the following. I can export the CAD model to a ""shp"" file and load it in Shapely (a GIS Python library). With this I can create the environment.
Once I have the environment I can start iterating over the states. Every state will be a representation of the environment. In the state zero I won't have any tag, so I can start putting the first tag that will be a rectangle with fixed dimensions. Once the tag is added I can get the reward based on the position, checking if the tag overlap another object, if it's too far from the source object, etc.
The next state will be the environment with the previous tag and so on.
I don't know if this can work. I believe the agent must execute some sort of planning but, correct me if I'm wrong, this is not possible with Q-Learning or another tabular method.
So, the question is...could be possible to do it with a tabular method or I will have a super large table to handle?
","['reinforcement-learning', 'deep-rl']",
"Was the original perceptron machine really just comparing images to the ""average image for each class""?","
I've been looking into the history of artificial neural networks, and only recently learned that the original Mark 1 Perceptron was only a single layer network.  It would iteratively modify the weights of connections using weights from the image sensors directly (i.e. pixel weights).
My question is:  Wouldn't this algorithm really just produce connection weights that resemble the ""average image"" for each class?
I.e. say you used the Mark 1 Perceptron to differentiate between three different classes of image (A, B and C).  Would the connection weights for class A not just produce an ""average image"" for class A, and likewise the connection weights for class C would just resemble an ""average image"" for class C?  In this sense, each of the connection groups would resemble the ""average image"" it's attempting to classify, and the output class is really just the class whose connection weights are most similar to the input image?  I found this intuition a useful way of understanding how the simple perceptron works, but I want to make sure it's a valid interpretation.
",['perceptron'],
Sequence Embedding using embedding layer: how does the network architecture influence it?,"
I want to obtain a dense vector representation of protein sequences so that I can meaningfully represent them in an embedding space. We can consider them as sequences of letters, in particular there are 21 unique symbols which are the amino acids (for example: MNTQILVFIACVLIEAKGDKICL).
My approach is to use a sequence embedding that can be learned as a part of a deep learning model (built with Python using Keras libraries), that is a classifier (supervised) neural network which I train to classify sequences according to the host species they belong to. The steps I follow are the following:

Tokenization. The only way I can tokenize these sequences of amino acids symbols is to consider single characters as tokens. I also found this example in Kaggle Deep Protein Sequence family Classification in Python that classifies different proteins and uses single amino acids as tokens.
Embedding. Stealing words from the answer to the question How the embedding layer is trained in Keras Embedding layer:


An embedding layer is a trainable layer that contains 1 embedding matrix, which is two dimensional, in one axis the number of unique values the categorical input can take (for example 26 in the case of lower case alphabet) and on the other axis the dimensionality of your embedding space. The role of the embedding layer is to map a category into a dense space in a way that is useful for the task at hand, at least in a supervised task. This usually means there is some semantic value in the embedding vectors and categories that are close in this space will be close in meaning for the task.

Moreover, useful words from the blog titled as Neural Network Embeddings Explained:

The main issue with one-hot encoding is that the transformation does not rely on any supervision. We can greatly improve embeddings by learning them using a neural network on a supervised task. The embeddings form the parameters — weights — of the network which are adjusted to minimize loss on the task. The resulting embedded vectors are representations of categories where similar categories — relative to the task — are closer to one another.

Thus putting all these pieces together in my case: I choose as tokens the single amino acids, so I have 21 unique symbols (i.e. number of amino acids) and I choose 10 as dimension of the embedding space, thus my embedding layer dimension is 21 x 10. This means that, once the neural network is trained, I can extract the weights of the embedding layer that are 21 vectors (one for each amino acid) and each vector is a 10 dimensional vector.
As the second extract explains, each element of these vectors are values that are adjusted to minimize the loss in the task. I could see each amino acid as if it was a letter in a word and I wanted to classify these words according to something (like positive or negative comment); or as if it was a word in a sentence and I chose as tokens words and I wanted to classify these sentences according to something (like positive or negative comment).
Since I want sequence representation, I have to find a way to put together the embeddings of single amino acids and the only way that I found feasible is to average all the 10 dimensional vectors so to obtain for each sequence a 10 dimensional vector that is the average of the embeddings of all the amino acids. This would for sure highlight if there are over-represented symbols in one sequence with respect to other. Furthermore, since each symbol is associated to vectors whose values are adjusted to minimize loss on the task, averaging should preserve each single amino acid meaningful embedding (meaningful embedding according to the task) and give ""a global meaningful embedding"" of the whole sequence that minimizes the loss on the task. This, in fact, seems to work for simple sentences embeddings. See the answer to a my question post: How to obtain vector representation of phrases using the embedding layer and do PCA with it in which each word of sentences were considered as tokens and I was looking for vectors embedding the whole sentences. Similarly, here I can see each single amino acid symbol as word and the whole sequence as sentence.
Hence this method should carry/embed two information: frequencies of letters of sequences and classes to which each sequence belong to (in this case host species).
But I would like it to consider also the global structure of letters positions (not only the relative frequencies), because with this method sequences like: MNTQILVFIACVLIEAKGDKICL (sequence 1) and AKGDKICLMNTQILVFIACVLIE (sequence 2) are represented by the same 10 dimensional vector.
Thus... from here the third point:

Choice of neural network architecture. Does the choice of neural network architecture influence the embedding of each single amino acid symbol and, consequently, the embedding of the entire sequence ? For example, if I use a LSTM neural network that should ""memorize"" the global structure of sequences, would it improve the dense vector representation (both in general and in this particular case) ? I would expect yes since, as reported also in the extracts, this embedding layer (so its weights) is trained as all the others so the backpropagation algorithm updates also these weights. In other words, if LSTM network has the power to recognize the importance of the position of each character according to the task (for example, it is able to learn that: if a letter is in position 1, it means it belongs to human, while instead if it is in position 2, it means it belongs to dog) then weights should be updated also according to this. Differently from a simple deep learning model that is not able to consider this information and to deal with sequences.

I understand that if I average the embeddings of the single amino acids, I would have anyway the problem that sequences like MNTQILVFIACVLIEAKGDKICL sequence 1 and AKGDKICLMNTQILVFIACVLIE sequence 2 would have the same dense vector representation. But does, also in this case, a better choice of the network architecture give a better result in some way ?
I apologize for the long question and I thank you in advance.
","['deep-learning', 'natural-language-processing', 'long-short-term-memory', 'sequence-modeling', 'embeddings']",
What does it mean by Generalization? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    




Towards Theoretically Understanding Why SGD Generalizes Better Than ADAM in Deep Learning

What does it mean by Generalization in this article?
","['neural-networks', 'optimization']","The term  generalization  refers to the model's ability to adapt and react appropriately to new, unpublished data that was drawn from the same distribution as the one used to build the model . In other words, generalization examines how well a model can digest new data and make correct predictions after being trained on a training set.
How well a model is able to generalize is key to its success.If you train a model too well on the training data, it will be unable to generalize. In such cases, it will end up making wrong predictions when receiving new data."
"What is the definition of a ""model""? [duplicate]","







This question already has answers here:
                                
                            




What is the fundamental difference between an ML model and a function?

                                (4 answers)
                            

Closed last year.



What is the definition of a ""model"" in the discussion of a neural network?
I need a canonical definition.
Can you please supply me with a definition along with a reference from any book or research article?
","['neural-networks', 'machine-learning', 'models']","When reading books and articles about machine learning or artificial intelligence in general, one can safely assume that the word ""model"" is used as an abbreviation for ""Statistical model"". Surprisingly many books skip
the job of providing a definition for it, allegedly cause the authors consider it a sort of primitive concept, i.e. they leave the hot potato there hoping that the readers are already familiar with statistics and data science (special mention in this regard to Pattern Recognition and Machine Learning , big classic in which Bishop just use the word from the very beginning without any concern). But in general the closest entity associated with statistical model is just a mathematical function.Here some references to point that out:""Predictive modeling: the process of developing a mathematical tool or model
that generates an accurate prediction"" [page 2, Introduction]""Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs."" [page 1 Introduction]""The quintessential example of a deep learning model is the feedforward deep
network, or multilayer perceptron (MLP). A multilayer perceptron is just a
mathematical function mapping some set of input values to output values."" [page 7 Introduction]If you're not satisfied and you're looking for a dense, algebraic definition of statistical model, you can check What is a statistical model? by Peter McCullagh. I won't try to explain the definition in the answer cause honestly it's really though and it goes beyond my math skills."
model to generate suggestions for improving the cosine similarity of two documents?,"
I am working on a system that compares a source document to a target document and then generate alternative variations of the source document. The goal is to reach a higher cosine similarity between the two documents, say +80%. There's already plenty of articles on how to find the similarity difference, but not many on how to generate suggestions or improvements.
I've never heard of a model that can do this, so how do I build a model that can suggest altered variations of a source document with the ultimate goal of increasing its similarity to a given target document?
Articles on document similarity

https://medium.com/@adriensieg/text-similarities-da019229c894
https://www.sciencedirect.com/topics/computer-science/cosine-similarity

","['machine-learning', 'natural-language-processing', 'cosine-similarity']",
Comparing Solutions from Saddle Points vs. Local Minimums,"
Can Saddle Points Provide ""Better Solutions"" to Machine Learning Models Compared to Local Minimums?
The solution to a Machine Learning model (i.e. the final model parameters) are selected by trying to optimize the Loss Function associated with that Machine Learning model. The ""best"" solution (i.e. ""best"" choice of model parameters) are those associated with the ""global minimum"" of this Loss Function (i.e. the smallest value of the Loss Function) - thus, ""relatively better"" solutions can be considered as solutions that are located closer to the ""Global Minimum"". Optimization Algorithms (e.g. Gradient Descent) try to search for the ""Global Minimum"" of the Loss Function by repeatedly searching in the direction of the derivatives corresponding to this Loss Function.
However, there are different obstacles than can occur during this search process. For instance:

The Optimization Algorithm can get stuck in a ""Local Minimum""
The Optimization Algorithm can get stuck in a ""Saddle Point""

I have heard ""Saddle Points"" as being considered ""worse"" than ""Local Minimums"" - this is because ""Saddle Points"" aren't actually a minimum of any sort, whereas ""Local Minimums"" are at least minimums at the local level. Thus, this would imply that model parameters chosen from a ""Saddle Point"" should be worse than model parameters chosen from a ""Local Minimum"". To further illustrate my question, I drew the following graph of a hypothetical Loss Function for some Machine Learning model:

In the above picture, we can see that Loss Function has a smaller loss at the ""Saddle Point"" compared to the loss at the ""Local Minimum"". Thus, in this case - (Assuming that we could not reach ""P3"") if we had to choose a selection of model parameters from ""P2"" (""Saddle Point"") and ""P1"" (""Local Minimum"") - it would clearly make more sense to pick model parameters from ""P2"".
My Question: In general, do we know if solutions corresponding to ""Local Minimums"" points on a Loss Function are considered to be ""better"" corresponding to ""Saddle Points"" (e.g. perhaps solutions from ""Local Minimums"" might be more ""stable"")? Or is this claim incorrect, and solutions corresponding to regions of the Loss Function with lower Loss values are generally ""better"" - regardless of whether they come from a ""Saddle Point"" or a ""Local Minimum""?
Thanks!
",['neural-networks'],
Unclear fact about difference between Gradient Descent to Stochastic Gradient Decent in wikipedia,"
From wikipedia page it mentioned:

To economize on the computational cost at every iteration, stochastic
gradient descent samples a subset of summand functions at every step.
This is very effective in the case of large-scale machine learning
problems

And later on it demonstrates the pseudo code with shuffle, and not sub-setting the samples.
I don't get it how come applying shuffle improves the Gradient descent:

Can somebody shed some light on this issue?
","['gradient-descent', 'stochastic-gradient-descent']","That paragraph is incomplete and unclear indeed.
Let's crack the difference with a concrete example: logistic regression.The objective we want to minimize is:$J_{train}(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{i})-y^{i})^{2}$Notice how this loss formulation requires us to iterate trough all our data to sum the squared errors. Gradient descent does precisely that:$\theta_{j} := \theta_{j} - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{i})-y^{i})x^{i}_{j}$Also here, to compute a single descent step we are iterating through to all our data, while keep summing the gradients. This is why normal gradient descent is so memory inefficient.One work around to this problem is to redefine our initial loss $J$ in order to allow us to perform a descent step without having to look at all our data. We usually call this new formulation cost:$cost(\theta, (x^{i}, y^{i})) = \frac{1}{2}(h_{\theta}(x^{i})-y^{i})^{2}$This allows us to rewrite also our descent step in a way that doesn't require to look at all our data.$\theta_{j} := \theta_{j} - \alpha\frac{1}{2}(h_{\theta}(x^{i})-y^{i})x^{i}_{j}$Since the sum now it's gone it means that we don't need to sum gradients or keep in memory extra stuff, so this is much more efficient. And this is basically Stochastic gradient descent.Problem is that because we're updating on a fraction of our data at the time, order becomes relevant. If our data are ordered in some strange fashion, we'll risk to introduce biases in our parameters do to biases in the subset we selected to perform the update. And even if other subsets of data would normally account for that, now it's not the case anymore, cause we already performed an update step, and when applying our model to the new subset the parameters are therefore already biased. The only strategy we have to prevent this is precisely randomizing the order of our data."
Reasoning about 2d spatial square configuration,"
I am trying to use RCC8 algebra relations and axioms in order to tackle the following problem :
We consider the below configuration and square $x$ such that ""$x$ NTPP $a$"" and ""$x$ TPP $c$""

and we must prove that :

a) $x$ DC $b$
b) For $y$ such that ""$y$ EC $a$"", prove that it is not possible to have ""$y$ EC $x$""

using the tree

My difficulty is that I do not know how to ""formally"" prove the above statements using the tree.
For instance, regarding question (a), since ""$x$ TPP $c$"" and ""$c$ DC $b$"", by the composition table we have that ""$x$ DC $b$"". But, this argument does not use ""Interior"" and ""Boundary"" statements as the tree suggests. Could you please assist with this ?
","['knowledge-representation', 'reasoning', 'semantics']","According to the tree:From ""x IntInt c"" and ""not x IntBnd c"" we can say ""not x IntExt c"". This relation together to ""not c IntInt b"" proves that ""not x IntInt b"".In similar way, we can infer ""not x BndBnd b"".Following tree for ""not x IntInt b"" and ""not x BndBnd b"" we conclude ""x DC c"".In other words, you need to use the composition of the relations ""IntInt"", ""IntBnd"", ... . Look for ""9-intersection model"", ""4-intersection model"" and/or ""3-intersection model""."
Image segmentation when given masking information is incomplete,"
In my problem, there are about 5,000 training images and there are about 50~100 objects of identical type (or class) on average, per image. And for each training images, there is a partial mask information that denotes the polygon vertices of objects, but the problem is there are only 3 ~ 5 objects per image with mask/annotation information.
So in summary there is 1 class, 5,000 * 50 ~ 5,000 * 100 instances of the class, and 5,000 * 3 ~ 5,000 * 5 instances with masking information.
So not a single training data image has a full masking information, and yet all the training data images have partial masking information. My job is to make instance segmentation model.
I did some search on semi-supervised segmentation, and to my understanding it seems like the papers are tackling problems where some training images have all the objects annotated while the other training images have 0 objects with annotation. That isn't exactly my situation. How should I approach this problem? Any tips are appreciated.
","['computer-vision', 'image-segmentation', 'semi-supervised-learning', 'instance-segmentation']",
"What did Bronstein et al 2021 mean by ""conductance""?","
On page 24 of Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges they make the following statement:

An essential aspect of Fourier transforms is that they reveal global properties of the signal and the domain, such as smoothness and conductance.

What do they mean by ""conductance"" here?
",['geometric-deep-learning'],
"Are Problems in AI Usually ""Ill Posed""?","
I was reading the following link (https://en.wikipedia.org/wiki/Well-posed_problem) on ""Well Posed Problems"". Supposedly, if a problem is ""Well Posed"", it must meet the following conditions:

a solution exists

the solution is unique

the solution's behavior changes continuously with the initial conditions


I can understand why the first two conditions are desirable for a problem to be ""Well Posed"", but I am having trouble understanding the last condition.
For instance, if ""the solution's behavior changes continuously with the initial conditions"" - isn't this a bad thing? Would we not want the ""solution's behavior NOT TO CHANGE continuously with the initial conditions""?
If the solution's behavior can change with the initial conditions, would this not result in the problem having the ability to be ""chaotic and unpredictable"", thus displaying ""bad behavior"" and as a result - ""Ill Posed""?
Thanks!
",['optimization'],
"How are the reward functions $R(s)$, $R(s, a)$ and $R(s, a, s')$ equivalent?","
In this video, the lecturer states that $R(s)$, $R(s, a)$ and $R(s, a, s')$ are equivalent representations of the reward function. Intuitively, this is the case, according to the same lecturer, because $s$ can be made to represent the state and the action. Furthermore, apparently, the Markov decision process would change depending on whether we use one representation or the other.
I am looking for a formal proof that shows that these representations are equivalent. Moreover, how exactly would the Markov decision process change if we use one representation over the other? Finally, when should we use one representation over the other and why are there three representations? I suppose it is because one representation may be more convenient than another in certain cases: which cases? How do you decide which representation to use?
","['reinforcement-learning', 'markov-decision-process', 'proofs', 'notation', 'reward-functions']",
Is there an error in A* optimality proof Russel-Norvig 4th edition?,"
In ""AI: A Modern Approach"", 4th edition, by Russell and Norvig, they give a purported proof that A* is cost-optimal for any admissible heuristic. The given proof seems most certainly wrong. They want to show that all nodes on the optimal path are expanded. Towards a contradiction, they consider a node n on the optimal path that is not expanded, and say that its evaluation f(n) must be strictly greater than the optimal cost C*, for ""otherwise, n would have been expanded"".
In other words, they claim that a node n with f(n) less or equal than C* will be expanded. That claim is obviously not true in general? Take the heuristic that is identically zero; it is trivially admissible. Now consider a model with initial state A, which can transition to states B, C and D with costs 1, 2 and 3, respectively. Then B, C and D all transition to the goal state E, all with cost 3. The A* algorithm will simply expand B, then E. So it certainly finds an optimal path. Note that states C and D have no chance to be expanded. Nevertheless, they have evaluations 2 and 3, which are both less than 4, the optimal cost. So the claim does not hold.
I would like confirmation of my analysis - is the proof given in the book incorrect?
Just to be clear, I am not saying that A* is not always cost-optimal for any admissible heuristic, although I would love to see a reference (I've found research papers but they are about other notions of optimality, such as number of nodes expanded.) I am just saying that the proof given in the book is wrong.
","['search', 'proofs', 'a-star', 'norvig-russell', 'optimality']",
Are Neural Networks only really useful for Image Classification?,"
Let's say I wanted to predict whether someone was Male or Female based on what they answered to certain survey questions.
I can see something like a Random Forest or KNN being useful here, but is there any reasons you'd apply a Neural Network? Or would it just be overkill?
","['neural-networks', 'deep-learning', 'classification']",
Off-policy every visit MC prediction algorithm for estimating $V\approx v_{\pi}$: is this algorithm correct?,"
Below is  the off-policy every visit MC prediction algorithm for estimating $V\approx v_{\pi}$,
which I took from coursera. 
It seems to me that this algorithm is not correct. Let me explain why. Assume that $t=T-1$ and let $\pi(A_{T-1}|S_{T-1})=0$,
which means there is no possibility for target policy $\pi$ to go from $S_{T-1}$ to terminal state
via action $A_{T-1}$. Hence, based on this observation we should have $V(S_{T-1}) \approx v_{\pi}(S_{T-1})=0$. However, the algorithm in the pseudoce returns $V(S_{T-1}) \approx v_{\pi}(S_{T-1})=R_{T}$. What do you think?
",['reinforcement-learning'],"The weighting calculation ignores how the current action $A_t$ was chosen.The action being evaluated does not need its probability adjusted by importance sampling, only future actions. This is because you have ""already decided"" to evaluate/update a particular $Q(s,a)$ and any probabilities of being in that part of the trajectory do not come into play (they do affect approximators such as neural nets, but that is a secondary effect that is often ignored). More formally, updating $Q(s,a)$ is conditional on you having a record $S_t = s, A_t = a$, so you do not take into account the probabilities of that happening. Whilst the return from that point on is a sample from a distribution including all the probabilities for observing $A_{t+1}$ to $A_{T-1}$ which you do need to adjust for if you are observing results from a different policy to the one you want to update.An off-by-one relationship where you only weight for future actions is maintained simply by having the $W$ update at the end of the loop, after the Q update for the same timestep. In fact $W$ is being updated ready for calculating the weightings to apply to the previous timestep (previous because the loop is decrementing $t$). At $t = T-1$ there are no future actions, and you have not been around the loop even once yet.When you get to the subject of off-policy TD control later on, you can also ignore how the last action in the partial trajectory was chosen, because you will use the target policy's action instead. Which means single-step Q learning doesn't use importance sampling at all."
Which generalization of standard deviation to use for multidimensional input normalization,"
For machine learning tasks, it's common to normalize input data by subtracting the mean $\mu$ and dividing by the standard deviation $\sigma$ of the dataset:
$$\hat{x_i} = \frac{x_i - \mu}{\sigma}$$
—where the standard deviation is computed independently for each feature.
Suppose instead we compute another multidimensional generalization of the standard deviation—the matrix square root $\Sigma$ of the covariance matrix:
$$\Sigma\Sigma^T = \frac{1}{n}\sum_i^n (x_i - \mu)(x_i - \mu)^T$$
(Since covariance matrix $\Sigma\Sigma^T$ is positive semidefinite, it has a unique positive semidefinite square root $\Sigma$.)
And then normalize the samples by
$$\hat{x_i} = \Sigma^{-1}(x_i - \mu)$$
In a sense this is a ""better"" normalization—it will not only map the values to nice ranges, but also decorrelate different features.
However, I have never seen this done. Is there some non-obvious drawback to this approach I'm not seeing, aside from being more computationally intensive? Is there any research on what effect this would have on model training?
","['machine-learning', 'data-preprocessing', 'statistics', 'normalisation']","This idea is sometimes applied in computer vision, under the name of Whitening Transform, or ZCA sphering transform. The name whitening comes from signal processing, since removing correlation from a signal makes it look like white noise.In images this transformation is applied to remove contrast and enhance edges, as you can see in the example below (taken from this paper, not open access unfortunately, but the authors made a github repo)I'm not aware either about application of covariance matrix in domains other than computer vision for preprocessing, but I can image few points that make this approach unpractical:"
Training a regression model on a set of values in 0-1 range to give 0-1 continual values,"
I have a textual dataset that has a set of real numbers as labels: L={0.0, 0.33, 0.5, 0.75, 1.0}, and I have a model that takes the text as input and has a Sigmoid output.
If I train the model on this data, will the model keep generating labels that exactly equal one of the values in L? or it might generate, as an example, 0.4?
If not, is there a solution for that?
","['neural-networks', 'classification', 'linear-regression', 'sigmoid']","As long as you train the model with a proper loss function for regression the model will learn to output any continuous values, not restricted to and most likely not exactly equal to the labels your providing during training, rather an approximation of them based on the level of generalization the model manage to learn.The range of values the model can learn to output will also depend on the final activation function of your model. Using a sigmoid is indeed a good choice if your labels belong in the range 0-1.If instead you don't want the model to learn continuous values then you need to frame the task as a classification problem, and convert your set of finite real values labels into a discrete representation, for example using one hot encoding and then train the model with a loss suited for classification."
time series analysis: predict number and type of service,"
I have temporal data regarding the number of customers who requested a specific service in a given period (month and year). Below is a small excerpt from the dataset:


Month-year: month and year when the service has been requested/offered
Service Description: the tipology of the service request by the customer
occurences: how many times the customers in that period requested that service

I have monthly data from 2003 to 2020 and I would like to carry out a predictive analysis to predict the number of events from 2021 to 2023 and also predict the type of services. For the first I know that I have to face the problem using the analysis of the time series, I have doubts about the second part ... how to predict the type of service in addition to the number of requests? Can you give me some suggestions?
","['long-short-term-memory', 'time-series', 'prediction']",
Low accuracy and high loss in multi-class classification [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I'm trying to classify images in 17 flowers dataset which consist of 1360 images of 17 classes (80 images per class); I have to use DNNs only therefore I made my model with the following settings:
images_net = models.Sequential([layers.Dense(300,input_dim = 3072, activation='relu'),
                                    layers.Dense(30,activation='relu'),
                                    layers.Dense(17,activation='softmax')]);

images_net.compile(optimizer='SGD', loss='categorical_crossentropy',metrics=['accuracy']);

my_net = images_net.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=32,epochs=10)


finally, when I evaluate my model I get the following result:

Can anyone explain the result and what the problem is with these settings?
","['deep-learning', 'keras', 'multiclass-classification']",
Confusion Matrix Measures vs Accuracy level in Neural Network Model,"
I'm working on a classification machine learning problem with two classes: high and low, which are derived from another numerical column x. Previously, if x>100, the sample is considered high, otherwise, it is considered low. I used a 1D CNN model.
I wanted to test if changing the threshold would impact on the model performance. So I increased the threshold to 1000(ie. if x>1000, the sample is considered high, otherwise, it is considered low.
).
What I did:

Increased the threshold from 100 to 1000, thus the number of samples that fall into the low category increased and that of the high category decreased. The data thus become imbalanced with the following ratio:

Ratio of low : high

(with 100 threshold)       43% : 57%

(with 1000 threshold)       8% : 92%


Noticing that the imbalanced dataset might lead to imbalanced training data, I applied undersampling to the training data to make sure that there are equal number of low and high samples.
At 100 threshold, the model achieved an accuracy level of around 80%. At 1000 threshold, the model achieved an accuracy level of around 69%, which is significantly lower than that of the 100 threshold.
I then saved each model and generated a prediction using each model. I evaluated both sets of results with the Precision, Recall and F1 Score.


                       Precision: 0.38
(with 100 threshold)   Recall: 0.86
                       F1: 0.527


                       Precision: 0.8307692307692308 
(with 1000 threshold)  Recall: 0.6206896551724138 
                       F1: 0.7105263157894737

Based on accuracy level, it seems 100 threshold achieved a better performance than that of 1000 threshold. However, according to F1 Score, 1000 threshold seems to achieve better score 0.71 (as compared to 0.527 of 100 threshold).
My question are:

Is it possible for a model to have lower accuracy level but higher F1 Score? How can this make sense in light of confusion-matrix measures and model accuracy level?
Which measures should I use to determine using which threshold is better?


Edit:
Since the entire code is too long, I will show some snippets of it below.
To answer @MASTER OF CODE's question, I believe the test data is balanced and tested in the validation stage at the end of each epoch as I called model.fit(x_train,y_train, validation) in Keras which used the built-in API for training and evaluation(please cmiiw).
For Undersampling:
I used np.random.choice() to randomly select an equal amount of majority samples as the minority group, then concatenated the minority and majority samples into one dataframe named under_sample, before fitting a 1D CNN model on the dataframe, see below:
np.random.seed(42)
############ minority_class_len
minority_class_len = len(df[df['Label'] ==1])


############ majority_class_indices
majority_class_indices = df[df['Label'] ==0].index


############ random_majority_indices
random_majority_indices = np.random.choice(majority_class_indices,
                                           minority_class_len,
                                           replace = False)

############ minority_class_indices
minority_class_indices = df[df['Label'] ==1].index


############ concatenate positive and negative sample indices
under_sample_indices = np.concatenate([minority_class_indices, random_majority_indices])


############ select samples by indices
under_sample = df.loc[under_sample_indices]
```

","['convolutional-neural-networks', 'performance', 'accuracy', 'confusion-matrix']","1. Yes, accuracy can be lower than the F score for a set of predictions and targets. You can easily test it by generating a fixed array of predictions and a varying array of labels (or vice versa). You'll get a graph like in the image below.
Notice that you could obtain a horizontally mirrored version of the same graph by inverting the labels (or predictions).To understand why you need to keep in mid that the F score is a skewed metric,because in its calculation we ignore an entire class, i.e. the true negatives.$$F1= \frac{2 TP}{2TP + FP + FN} $$So if I have an unbalanced dataset with only 10 instances of class A and 90 of class B I'll get an accuracy of 90% by predicting only B, with an f score of 0%, but if I invert the classes (i.e. predict only A) I'll get an accuracy of 10% and an F score of 18%.2. The safer answer is always: check all metrics. Every metric tells you a different story about the model performance, so it's always a good practice to not rely only on one. But it might depends also on your specific use case. Why do you need to classify between low and high (whatever low and high refer to)? Do you care about a specific class? Then the recall is the most interesting metric to keep an eye on. In general, your problem is similar to probability thresholding when calculating the ROC-AUC score so you might want to check it out and see if it's applicable also to dataset splitting (a bit impractical if a single training session of your model is long though), if it's applicable, it's surely more reliable than the F score."
Behaviour policy must be stochastic in states where it is not identical to the Optimal policy,"
In Sutton & Barto Reinforcement learning book, page 103 (chapter: off-policy learning
via importance sampling), the following statement is given:
""In order to use episodes from $b$ to estimate values for $\pi$, we require that every action
taken under $\pi$ is also taken, at least occasionally, under $b$. That is, we require that
$\pi$(a|s) > 0 implies $b$(a|s) > 0. This is called the assumption of coverage. It follows
from coverage that $b$ must be stochastic in states where it is not identical to $\pi$. ""
where $b$ refers to a behavior policy and $\pi$ to the target optimal policy.
I don't understand how the above conclusion was obtained from the definition of coverage.
",['reinforcement-learning'],"Say we are in state $s$, and $b$ chooses $a'$ but $\pi$ chooses $a$. Since $\pi$ chose $a$ ($\pi(a | s) >0$), coverage implies that $b(a|s)>0$. But $a'$ was ultimately chosen by $b$, so it must be that $b(a'|s)>0$. Hence there is some chance that $a'$ is chosen and there is some chance that $a$ is chosen, meaning $b$ is stochastic in state $s$. Thus the above conclusion follows from the definition of coverage when the policies are not identical under $s$."
Are Bayesian Optimization Methods Better Suited Noisy Optimization Problems?,"
We know that in many applied contexts (e.g. Machine Learning, Loss Functions for Neural Networks), the functions we are trying to optimize are ""noisy"" by definition (unlike in the classical sense, when we are only required to optimize ""deterministic functions"") - this is because these functions are based on measurements that we have (i.e. the data), and it is very possible that these measurements can either contain some ""measurement error"" and these same measurements could take different values if we were to measure them again (i.e. ""stochastic""). 
I have heard that Bayesian Optimization techniques are generally favored for instances where the gradient of the loss function does not exist (e.g. discrete/combinatorial problems) - but I am particularly interested in learning about the effectiveness of Bayesian Optimization Methods on ""Noisy Functions"". For instance, (my very limited knowledge of this topic tells me that) Bayesian Optimization uses a Gaussian Process to ""reconstruct"" the surface of the Loss Function based on the data we have , and our confidence in this surface is iteratively updated as we perform the optimization task.
Thus:  Is it because of this fact (i.e. modelling the loss surface of the objective function using a Gaussian Process) that Bayesian Optimization methods are said to be effective at optimizing ""noisy functions""? Does doing so somehow result in solutions with a higher level of statistical robustness? In general - why do we say that Bayesian Optimization is more effective at optimizing noisy functions?
Thanks!
","['neural-networks', 'optimization']",
What's the best model to use for CNN(deep learning) regression task for small image dataset?,"
What are the best Deep learning models(with how many layers) to use in a regression task for a custom dataset containing around 100 images of only one object per image which is more or less centralized? The object is measured manually with a weight scale and the output of the model is the estimation of the density of an object expressed in g/mm.
","['deep-learning', 'convolutional-neural-networks', 'image-recognition', 'image-processing', 'regression']",
Unclear step in off-policy (every-visit) MC Control: why do we need the line: $A_t \neq \pi(S_t)$ then exit inner loop?,"
Could please some expert on reinforcement learning explain the red-box part in the following off-policy MC control? I mean I did not understand what (and why)
is done in the step shown as a red-box. I understood all points, except the red box part: why do we need the line  ""$A_t \neq \pi(S_t)$ then exit inner loop""? More specifically, why don't we not exit the inner loop when  $A_t= \pi(S_t)$?

","['reinforcement-learning', 'sutton-barto']",
What is the Thompson Sampling in simple terms?,"
I am looking at the different existing methods of action selection in reinforcement learning.
I found several methods like epsilon-greedy, softmax, upper confidence bound and Thompson sampling.
I managed to understand the principle of each method except Thompson sampling.
I can't understand the principle and the way it works and its action selection steps.
If you can explain to me the principle and the functioning of Thompson sampling with a simple example I would be grateful.
","['reinforcement-learning', 'thompson-sampling']",
Difference between TPU and VPU,"
There are these two concepts of ASICs for the usage for neural networks.
The one is called Tensor Processing Unit (TPU) and the other one is called a Vision Processing Unit (VPU). How do they differ from each other? I don't want to dig to deep into differences in terms of architecture but more or less application wise. Is it at the end of the day just naming convention, but they can be used for the same stuff?
",['hardware'],
Solving logical pattern puzzles with Machine Learning?,"

I found this kind of problem while reading about some web tests companies use to screen applicants. It is a puzzle where you need to guess what comes inside?
Looking at this made me wonder,
How would you approach solving problems like this using Machine Learning?
I assume that if people can recognize the pattern with a bit of training, so should A.I. be able to as well.
If anyone knows anything that could be a good starting for me to solve this problem, please let me know. Any opinion will be appreciated! (whether this is possible or not, how difficult it is, computer vision models, algorithms etc., cool projects like this etc.)
I've seen other people solve logic games like here
But visual cues get complicated like this often in this type of puzzles. 
","['machine-learning', 'computer-vision', 'pattern-recognition']","These specific puzzles are called Raven's Progressive Matrices, a common psychometric tool used to measure the IQ. Knowing that is easy to find references. Here just a few examples, but the literature of course is broader, visual reasoning is quite an old AI topic.From a technical perspective the approaches are not that impressive, classic CNN (and of course more fancy architectures) trained on detecting which answer amongst the given ones is the most ""logically"" similar to the images forming the incomplete sequence. Notice also that the task is reasonably tractable because we have a set of given answers, without that the story would be much different.Also, not really related to the question title, but if you're interested in machine leaning and abstract reasoning a much more impressive task to tackle is numerical sequences prediction, and a fresh new paper managed to achieve incredible results in this task as well."
Graph Neural Networks: Why do papers use very low label rates?,"
I was recently reading the following paper: ""Semi-supervised classification with Graph Convolutional Networks"" by Kipf and Welling (here).
Question: When testing on datasets, why are the authors using such a low label rate?
Context: In Table 1 where the authors list the statistics for the datasets they used to benchmark the architecture, the label rates are quite small percentages (5%, 3%, <1%). I don't quite understand why this is the case...
I know this is 'semi-supervised' classification such that we have access to ALL the node features, regardless of whether they are training or testing. However, I don't know why we are using 5% label rate as opposed to, for example, 30% label rate.
Any insight would be appreciated.
","['deep-learning', 'graph-neural-networks']",
"Claim: Deep Neural Networks ""Automatically"" Perform Feature Selection and Feature Engineering?","
I have often heard that Deep Learning Models (i.e. Deep Neural Networks) are automatically performing feature engineering within the network themselves. This is contrasted with traditional statistical and Machine Learning models where the feature engineering is typically done prior to training the model:

Apparently, the operations that Neural Networks perform during the training phase are equivalent to ""searching for meaningful combinations of variables that produce better performance results on the training data"". I have heard that this is in some way similar to Principal Component Analysis (PCA) and Kernel Methods, seeing as these methods combine many different existing features into new features that have ""more meaningful representation"".
For instance, in the ""Deep Learning Book"" (https://www.deeplearningbook.org/contents/intro.html), I read about this ability of Deep Neural Networks to ""automatically perform feature engineering"" :
""Deep learning solves this central problem in representation learning by introducing representations that are expressed in terms of other, simpler representations. Deep learning enables the computer to build complex concepts out of simpler concepts.""
However, in the end - I still do not understand how Neural Networks during the training phase (i.e. gradient descent, weight update, backpropagation) are ""automatically performing (some degree of) feature engineering"".
How exactly are Neural Networks doing this ""automatically""?
Thanks!
Note: I have heard that NN's are said to be universal function approximators (i.e. universal approximation theorem). Therefore, for a given data set - there should exist a specific neural network architecture (i.e. specific number/value of weights, layers and choice of activation functions) that will result in your target function being approximated sufficiently well - however, the combinatorial search space is massive and recovering that network can be very difficult for large data because of the time required to train the network using gradient descent. So ""automatic feature engineering"" is something indirect that might or might not happen because of the universal approximation theorem.
",['neural-networks'],
YOLO - are the anchor boxes used only in training?,"
another question in YOLO.
I've red about how YOLO adjusts anchor boxes by offsets to create the final bounding boxes.
What I do not understand, is when YOLO does it.
Is it being done only during the training process, or also during the common use of already trained model?
","['convolutional-neural-networks', 'yolo', 'bounding-box']",
YOLO - does the Intersection over Union is actually a part of Non Maximum Suppresion,"
In the Stack Overflow thread Intersection Over Union (IOU) ground truth in YOLO they say that in YOLO actually the IoU (intersection over union) is used twice:

during training to compare ground truth box to predicted box

during the usage of already trained YOLO network this technique is being used to eliminate overlapping boxes which include same object many times.


As far as I know eliminating overlapping boxes is being done by process called Non Maximum Suppression (NMS). Thus I wonder maybe the IOU is a part of a NMS process?
","['deep-learning', 'convolutional-neural-networks', 'yolo', 'non-max-suppression']","IoU is way to measure how much two areas overlap. As said in the comments IoU is used as a threshold parameter for NMS, so You could decide how much common space two areas have to have to be considered as overlapping.Confidence is 5th parameter in prediction vector: (x, y, w, h, c) predicted for every bounding box and it tells how good the bounding box is according to ground truth. Thanks to usage of IoU, confidence gives the full information about bounding box position, dimensions and its class probability.Authors claim thatNon-maximal suppression can be used to fix these multiple detections. While not critical to performance as it is for R-CNN or DPM,
non-maximal suppression adds 2- 3% in mAP.Reading the VOC 2007 Error Analysis section of the Yolo paper also might be useful.Hope it helps."
Problem with the Turing Test as Performed,"
Could anyone explain this problem I have with the Turing test as performed? Turing (1950) in describing the test says the computer takes the part of the man then plays the game as when played between a man and a woman. In the game, the man and the woman communicate with the hidden judge by text alone (Turing recommends using teleprinters).
If the computer takes the part of the man, then it will have an eye and a finger in order to use the teleprinter as the man would have done. But in the TT as performed, the machine is not robotic. It has no eyes and no fingers but rather is wired directly into the judge's terminal. The only thing the machine gets from the judge is what flows down the wire. But the problem I have is, what flows down the wire is not text. The human contestant gets the text. The judge's questions print out on the teleprinter paper roll. The man sees the shapes of the text, and understands the meanings of the shapes. But the computer is never exposed to the shapes of the questions, so how could it possibly know what they mean?
I've never seen anyone raise this problem, so I'm very confused. How could the machine possibly know the judge's questions if it is never exposed to the shapes of the text?
","['philosophy', 'turing-test', 'philosophy-of-mind']",
PPO advantage estimate - Why does advantage estimate have $r_t+\gamma V(s_{t+1})-V(s_t)$,"
So I've been looking at this formula for advantage estimate
\begin{equation}
\begin{aligned}
& \hat{A}_t = \delta_t + (\gamma \lambda)\delta_{t+1} + ... +  (\gamma \lambda)^{T-t+1}\delta_{T-1}\\
&\text{where}\ \delta_t = r_t +\gamma V(s_{t+1}) - V(s_t)
\end{aligned}
\end{equation}
where $r_t$ is the reward at time $t$ and $V(s_t)$ is the estimated value of $s_t$ and $\gamma$ is future discount. Why is $\delta_t$ useful? As in why is it useful to calculate reward from time $t$ from the discounted estimated reward from that time onward of $s_t$ and subtract the value estimate of $s_t$
Original PPO Paper
","['reinforcement-learning', 'papers', 'proximal-policy-optimization']","Lets notice, that $\hat{A}=\delta_t$ is a unbiased estimate of $A$ in a sense, that
$$
E_{s_{t+1}}[r_t + \gamma V(s_{t+1}) - V(s_t)] = E_{s_{t+1}}[Q(a_t, s_t) - V(s_t)] = A(a_t, s_t)
$$
Here we abuse the fact, that $V(s)$ is known, but in reality we know only its approximation, so the bias of estimator will be correlated to error of estimation of $V$ by  $V_\theta(s)$. By increasing the trajectory we can minimise the impact of $V_\theta(s_{t+1})$, lets write
\begin{equation}
 \begin{aligned}
&\hat{A}_t^{(1)}:=\delta^V_t& =r_t + \gamma V(s_{t+1}) - V(s_t) \\
&\hat{A}_t^{(2)}:=\delta^V_t + \gamma\delta^V_{t+1}&=r_t + \gamma r_t + \gamma^2V(s_{t+2}) - V(s_t)\\
&\hat{A}_t^{(3)}:=\delta^V_t + \gamma\delta^V_{t+1}+\gamma^2\delta^V_{t+2}&=r_t + \gamma r_t + \gamma^2r_{t+2}+\gamma^3V(s_{t+3}) - V(s_t)\\
&...&\\
&\hat{A}_t^{(\infty)} := \sum_{i=0}^{\infty} \gamma^i\delta_{t+i}^{V}
 \end{aligned}
\end{equation}
The longer the traction, the smaller the term $\gamma^i$ at $V(s_{t+i})$, therefor the approximation of advantage function is less biased .It, however, is not perfect, since the variance is increasing with longer path, e.g. $\hat{A}^{(1)}_t$ has low variance and high bias, and $\hat{A}^{(\infty)}_t$ has low bias, but high variance. The tradeoff can be introduced by taking some $i < \infty$ and estimating the A by $\hat{A}^{(i)}_t$. But choice of $i$ is not evident. By analogy of generalisation for TD($\lambda$), described in Sutton's book, chapter
12.8 the other way to introduce the trade-off between bias and variance is to take a weighted sum. In practical case, of course the infinite trajectories are not accessible. So the trajectories of length T are taken into account, which results exactly in
$$
\hat{A}_t = \sum_{i=0}^{T-(t+1)} (\gamma \lambda)^i \delta^{V_\theta}_{t + i}
$$
Where $\lambda$ introduces trade off between bias and variance of advantage function estimation.For further detailed reading I would highly recommend looking at the other article by J. Schulman. It has a section (3. Advantage Function Estimation) on exactly this question."
"How is the probability of a greedy action in ""$\epsilon$-greedy policies"" derived?","
In Sutton & Barto's book on reinforcement learning (section 5.4, p. 100) we have the following:

The on-policy method we present in this section uses $\epsilon$ greedy policies, meaning that most of the time they choose an action that has maximal estimated action value, but with probability $\epsilon$ they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, $\frac{\epsilon}{|\mathcal{A}|}$, and the remaining bulk of
the probability, $1-\epsilon+\frac{\epsilon}{|\mathcal{A}|}$, is given to the greedy action.

I understood the probability of a random action selection: since the total probability of random action selections is $\epsilon$ and since all actions can be selected as random we calculate the probability of an action to be selected randomly as $\frac{\epsilon}{|\mathcal{A}|}$.
However, I did not understand how the probability  $1-\epsilon+\frac{\epsilon}{|\mathcal{A}|}$ for greedy action selection was derived. How is it calculated?
","['reinforcement-learning', 'epsilon-greedy-policy']","I did not understand how the probability  $1-\epsilon+\frac{\epsilon}{|\mathcal{A}|}$It is the sum of two mutally-exclusive possibilities:The agent chooses to exploit, selecting the greedy action with probability $1 - \epsilon$The agent chooses to explore (probability $\epsilon$), and so happens to randomly choose the original greedy action (probablility $\frac{1}{|\mathcal{A}|}$). Combined probability $\frac{\epsilon}{|\mathcal{A}|}$.Although you might expect that exploring actions would exclude the greedy action, in $\epsilon$-greedy approach they do not. It keeps implementation simple, although it does result in this extra term cropping up in the expression for probability of choosing the greedy action.Moving the excess probability to other actions, or adjusting the original exploration probability would also both result in an extra small term cropping up in other parts of the theory. There isn't a ""clean"" way to build an $\epsilon$-soft action probability distribution without this slight added complexity."
Action-value estimation of deterministic policies with Monte Carlo method,"
In Monte Carlo-based action value estimation  problem for a deterministic policy (estimation of $q_{\pi}(s,a)$),the estimation problem seems not to be well-defined because  $q_{\pi}(s,a)$ by definition means the value of an arbitrary action $a$ at a given state $s$
when initial action $a$ is applied at that state and then following actions from policy $\pi$ at
the next states. But, in a real application under a given deterministic policy $\pi$, how can you choose the initial
action $a$ arbitrarily at state $s$ because it is already fixed by the policy $\pi$: $a=\pi(s)$?
","['reinforcement-learning', 'monte-carlo-methods']","But, in a real application under a given deterministic policy $\pi$, how can you choose the initial action $a$ arbitrarily at state $s$ because it is already fixed by the policy $\pi$: $a=\pi(s)$?You ignore $\pi$ for the action selection of the action you need to evaluate. This is well-defined, it is the expected future return for taking action $a$ in state $s$ and thereafter following the policy $\pi$.So you may have a problem estimating that value, unless you also force that action choice during training in order to observe what transitions and rewards follow (that you would never see following the deterministic policy). However, it is well-defined conceptually.A very simple way to force the assessment of all state/action pairs is exploring starts: Pick an arbitrary state/action pair to evaluate, then follow the transition and policy rules from that point on until the espidode end. This will give you a Monte Carlo sample of the value for the starting point, that you can use to update the estimate.If you are not able to use exploring starts, or otherwise take actions different from the supplied deterministic policy, then you may be stuck. You would only have data of certain $s,a$ pairs. You could estimate $Q(s,a)$ for those pairs, and not for others."
Is there a standard term for the following flaw in the data?,"
I wonder if following characteristic of data has some standard ""professional"" or scientific term associated with it.
Let's assume that I have a set of dog/cat images labeled 0 for a cat and 1 for a dog. My purpose is to extract (classify) as many true dog images as possible, with the smallest amount of non-dog images. But the problem with data is that:

some dogs images are labeled as 0
some cats images are labeled as 1
some images are of other animals or non-animal objects and can have both labels 1 and 0

Is there any specific term describing such cases, or they are  just ""noisy"" data?
","['classification', 'terminology', 'datasets', 'data-labelling', 'training-datasets']","This is referred to as label noise (or noisy labels), and it is indeed different from just ""noisy data""; quoting from the 2014 paper Classiﬁcation in the Presence of Label Noise: a Survey:In this survey,
it is assumed that each training sample is associated with an
observed label. This label often corresponds to the true class of
the sample, but it may be subjected to a noise process before
being presented to the learning algorithm. It is therefore
important to distinguish the true class of an instance from its
observed label. The process which pollutes labels is called
label noise and must be separated from feature (or attribute)
noise which affects the value of features.You may also find the (preprint) paper Label Noise Types and Their Effects on Deep Learning useful; there, the authors propose a taxonomy of different types of label noise:Label noise types can be subdivided into
three main groups as follows.• Uniform noise: Flipping probability of label from its
true class to any other class is equally distributed. Many
works in literature use synthetic uniform label noise by
just flipping labels randomly for a given percentage of
data instances [21]–[24].• Class-dependent noise: Flipping probability of label
depends on the true class of the data instance. This
is mostly represented by a confusion matrix and can
be designed in different ways. The easiest way is to
attain inter-class transition probabilities just random [25],
so that there is still class dependence since transition
probabilities are given according to classes but without
any correlation to class similarities. In a more structured
way, noise transition matrix can be designed in a way that
similar classes have a bigger probability to be flipped to
each other [26]–[30]. Some works use pairwise noise,
in which transition from one class can only be defined
to one another class [31]–[35]. Work of [36] checks the
popularity of classes and constructs transition matrix so
that mislabeling happens from popular class to unpopular
class or vice versa.• Feature-dependent noise: The probability of mislabeling
depends on features of instances. In order to generate
feature-dependent noise, features of each instance should
be extracted, and their similarities to other instances from
different classes should be evaluated. Unlike uniform and
class-dependent noise, there are much fewer implemen-
tations of synthetic feature-dependent label noise. One
particular work in this field is [37], where data is clustered
with the kNN algorithm, and labels are flipped randomly
for clusters of data. This method provides concentrated
noise in the feature space. But, this type of synthetic noise
doesn’t evaluate the instance similarities and therefore
different from our proposed approach. Alternatively, in
case there is a surrounding text for each image in the
dataset, some works create noisy labels from the interpretations of these texts [38]–[41], assuming surrounding
texts are related to features of data. But this approach is
restricted to datasets with surrounding user-defined texts,
which is not the case for most of the time.Other possibly useful resources on the subject:"
Does the summing or averaging of the weight gradients have anything to do with the cost function used?,"
I've been trying to implement my own neural network library and have been wondering if:

The SSE loss function includes the summation of the errors in the other training examples of the mini-batch (each training example's loss in the mini-batch is summed for one big loss)

The MSE loss function averages the loss of each individual training example in the mini-batch, and then all those losses are averaged based on the mini-batch size.


or if the summing or averaging of the weight gradients has nothing to do with the loss function used?
I feel like the answer would be clear if I knew other loss functions, if it did matter that would mean the weight gradients should be averaged for MSE and summed for SSE in mini-batch gradient descent?
","['neural-networks', 'objective-functions', 'mini-batch-gradient-descent']","What you stated looks correct :-The SSE loss function includes the summation of the errors in the other training examples of the mini-batch (each training example's loss in the mini-batch is summed for one big loss)
SSE = Summation of (y-y')squared for all samples in mini-batchThe MSE loss function averages the loss of each individual training example in the mini-batch, and then all those losses are averaged based on the mini-batch size.
MSE = (1/m)SSE"
How can the input order of pairs into a neural network not matter (i.e. symmetry)?,"
Let me explain, suppose we are building a neural network that predicts if two items are similar or not. This is a classification task with hard labels (0, 1) of examples of similar and dissimilar items. Suppose we also have access to embeddings for each item.
A naive approach might be to concat the two item embeddings, add a linear layer or two and finally perform a sigmoid (as this is binary classification) for the output probability.
However, that approach would mean that potentially inputing (x, y) to the model could give a different score from inputing (y, x) into it, since concat is not symmetric.
How can we go about overcoming this? What is the common practice in this situation?
So far I have thought about:

Whenever I input (x, y) I can also input (y, x) and always take the average prediction of both of them. But this feels like a hacky way of forcing the network to be symmetric, it doesn't make it learn the same thing despite of the input order.

Replacing concat with some other symmetric tensor operation. But what operation? Addition? Element-wise multiplication? Element-wise max? What's the ""default""?


","['neural-networks', 'deep-learning', 'pytorch', 'feedforward-neural-networks']",
Action selection in Batch-Constrained Deep Q-learning (BCQ),"
For simplicity, let's consider the discrete version of BCQ where the paper and the code are available. In the line 5 of Algorithm 1 we have the following:
$$
a' = \text{argmax}_{a'|G_{\omega}(a', s')/\text{max}~\hat{a}~G_{\omega}(\hat{a}, s')~>~\tau} Q_{\theta}(s', a')
$$
I have doubts about the predictive model $G_{\omega}$. In behavior cloning, it could be an action obtained using supervised learning mapping states to actions. Instead, in BCQ it looks like an probability regarding the actions available. I'm right? And what is the action $\hat{a}$?
EDIT: As far I understood we compare the probability of the action $a'$ to $\hat{a}$, and if it's above the threshold $\tau$ we calculate the loss. The question is now how should I proceed if this is not true: should I take the next action Q with the highest value?
","['reinforcement-learning', 'deep-rl', 'dqn', 'imitation-learning', 'offline-reinforcement-learning']",
What techniques exist to increase the learning importance of difficult-to-learn labels over easy ones?,"
I am training a model to place labels in image data. Some labels are learnt very quickly by the model while others take a long time to perfect. I cannot simply add more labeled data with only the labels I am looking to improve on since most of the image data contains a combination of the easy labels and the more difficult ones.
Are there any smart ways to get the model to focus on the hard labels? I am just looking for some leads. Of course I can just train for longer but that seems inefficient as half the labels are already predicted almost perfectly.
","['machine-learning', 'deep-learning', 'training', 'tensorflow', 'data-labelling']",
What do state features mean in the context of inverse RL?,"
I am reading Zeibart's Inverse RL paper, and it states -

The agent is assumed to be attempting to optimize some function that linearly maps the features of each state, $f_{sj} \in \mathbb{R}^k$, to a state reward value representing the agent’s utility for visiting that state.""

Can someone please give me an example of state features? I would highly appreciate it if it is in the context of this GitHub repo, wherein the author coded the feature_matrix as a diagonal matrix of shape $N \times D$, where $N$ represents states and $D$ features.
","['reinforcement-learning', 'terminology', 'papers', 'features', 'inverse-rl']",
Training a RL agent using different data at each episode,"
I am training a RL agent whose state is composed of two numbers, ranging between 4 ~ 16 and 0 ~ 360. The action is continuous and between 0~90. In real life, the states can be any I am training a TD3 agent using the stable baselines library. In real life, the state may be any pair of numbers in the aforementioned range. Hence, I am generating random numbers for training. Leading to different data at each episode. I have realized that the trained agent is predicting actions just in the boundaries of the action range. Could this issue be caused by using different data for the different episodes of the training?.
edit:
In the real application of my algorithm the pair of numbers will be arbitrary, given that they are in the corresponding range. By random numbers  for training I mean that I am generating pairs of numbers using uniform distributions between the given boundaries. And using them to train the system. The reward is a function of these numbers, however it does not have an analytic expression.
","['reinforcement-learning', 'deep-rl', 'continuous-action-spaces', 'td3', 'continuous-state-spaces']",
Does the state space of an MDP change in these two examples?,"
In the classic Atari environments, like that introduced in the original DQN paper, the state space is the set of all possible images that the Atari emulator can produce (or more generally just any RGB image, potentially stacked to better represent the environment). This makes sense as the CNN in the DQN is trained end-to-end with the RL signal, and so the Q-Fuction looks directly at the image as input.
Now, in methods such as CURL that look to pre-train the CNN and treat it as an encoder, does the state space change here? My thinking is that, if the pre-trained encoder is a function $\psi: \mathcal{I} \rightarrow \mathbb{R}^d$, where $\mathcal{I}$ is the space of images, then the state space is now $\mathbb{R}^d$. The rationale for my thinking is that now the agent directly observes the vector in $\mathbb{R}^d$ rather than the image from the encoder, and the state space should be what the agent observes (even though this vector is a representation for the image).
","['reinforcement-learning', 'markov-decision-process', 'state-spaces']","Whilst engineering solutions in reinforcement learning, I think it is common to discuss the concept of state space loosely, in terms of what the search space looks like for the algorithm, and what compromises are OK even though they technicaly make the problem a POMDP.In terms of definitions relating to the MDP, the state space has well-defined meaning. It is the set of all state values that can occur in the environment. That set/space can be mapped into different domains, but it remains the same size of space in terms of the set for any bidirectional mapping.Once you start to implement a state representation in a real system, in a practical agent, you often need to compromise regarding this definition. Even in purely mathematical treatments, it may not be convenient to determine all the theoretically reachable states. Determining them can be more complex than the optimal control problem. So it is very common to over-specify the state space.Atari games don't reach states where they produce arbitrary images. Their output during gameplay is on a relatively small manifold embedded in image space. Despite this, the over-specification in image space is useful, because we have good toolkits for working with it, including CNNs for learning generalising function outputs when images are used as inputs.Another compromise seen in the original Atari DQN is missing state. Only using the image, even when stacked, can mean a certain amount of state is not being used. Depending on the game, this state could be important enough that a POMDP would make a better model, and the images would move from a state space representation to an observation space representation (as an aside, stacking images to include velocity information could be replaced by a sequence-aware model such as RNN, and this is similar to POMDP approach, building an internal state representation separate from observations).In both cases - over-specified state space, and missing state - the state space of the problem is not changed. When implementing the agent, you know the representation space you are using, and expect it to have good coverage of all possible states, but often do not know the precise underlying state space of the MDP.This further gets confounded by feature engineering. I would treat the embedding by pre-trained CNN as a form of feature engineering. In theory it could reduce the dimensionality of the optimisation problem significantly, speeding up learning, but there is always a risk that the pre-training misses key features due to differences that are important in RL context having a low weighting in unsupervised learning of the embeddings.So does converting an RL problem that works with images from vision-based observations, to work with embeddings of those images reduce the state space? I would say no, the problem definition is not changed, it has the same state space as before. However, the separation of concerns (vision processing vs policy or value prediction), and lower dimension space for generalising has still done something useful. It may help with generalisation, as similar states may be be closer in the embedding space than they are in the larger image space.Loosely speaking you could say that CURL ""reduces state space"" and most people would understand what you meant in practical terms. I would personally caveat that with e.g. ""effectively reduces state space"" or perhaps ""makes it easier for the agent to generalise its experience across the state space""."
Test accuracy go down after decreasing learning rate,"
My project include classification of images into several classes.
I'm having a strange issue related to adding mixup augmentation. The accuracy of the training set and the validation set keep rising during all training.
However, the accuracy on the test set improves significantly until epoch 80 (over baseline without mixup - grey), but when the learning rate is reduced it collapses. Note that each 40 epochs I reduce the learning rate.
Is it learning rate/optimizer/scheduler problem? If so..what should I do?
I already tried cosine schedulers and linear schedulers.

","['deep-learning', 'classification', 'training', 'image-processing', 'data-augmentation']",
Why does OpenAI's PPO algorithm not follow the discounting method used in Sutton & Barto?,"
As discussed in this question, the policy gradient algorithms given in Reinforcement Learning: An Introduction use the gradient
\begin{align*} 
\gamma^t \hat A_t \nabla_{\theta} \log \pi(a_t \, | \, s_t, \theta)
\end{align*}
where $\hat A_t$ is the advantage estimate for step $t$. For example, $\hat A_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ in the one-step actor-critic algorithm given in section 13.5.
In the answers to the linked question, it is claimed that the extra discounting is ""correct"", which implies that it should be included.
If I look in the literature to a seminal paper such as Proximal Policy Optimization Algorithms by OpenAI, they do not include the extra discounting factor, i.e. they use a gradient defined as
\begin{align*} 
\hat A_t \dfrac{\nabla_{\theta}\pi(a_t \, | \, s_t, \theta)}{\pi(a_t \, | \,s_t, \theta_{\rm old})}
\end{align*}
which does not include the discounting factor (of course, it's dealing with the off-policy case, but I don't see how that would make a difference in terms of the discounting). OpenAI's implementation of PPO also does not include the extra discounting factor.
So, how am I supposed to interpret this discrepancy? I agree that the extra discounting factor should be present, from a theoretical standpoint. Then, why is it not in the OpenAI code or paper?
","['reinforcement-learning', 'policy-gradients', 'proximal-policy-optimization', 'sutton-barto', 'discount-factor']",
What is a neural network compatibility function?,"

Typically, a neural network parameterized by weights $\mathbf{W}$ is
a function from an input $x$ to an output $y$. The network has
an associated compatibility function $\Psi(y; x, \mathbf{W}) \rightarrow \mathbb{R}^+$ that
measures how likely an output y is given an input x under
weights $W$.

Source: Gradient-based Inference for Networks with Output Constraints, AAAI 2019 (https://arxiv.org/abs/1707.08608)
What is a compatibility function? What is its utility?
Neural networks are generally deterministic. We are guaranteed to get a specific output for a given input. How do we interpret compatibility function in this context?
",['neural-networks'],
How does Regularization Reduce Overfitting?,"
As I understand, this is the general summary of the Regularization-Overfitting Problem:

The classical ""Bias-Variance Tradeoff"" suggests that complicated models (i.e. models with more parameters, e.g. neural networks with many layers/weights) are able to well capture complicated patterns in data (i.e. low bias) but are unable to generalize well to unseen data (i.e. high variance). On the other hand, simpler models are able to generalize better to unseen data (i.e. low variance), but unable to capture complex patterns in data (i.e. high bias).

Regularization tries to navigate this compromise by attempting to improve the ability of complicated models to generalize to unseen data. Regularization does this by making ""complex models simpler"", by strategically reducing the number of parameters in complex models such that they maintain their ability to capture complexity in the data but also generalize to unseen data.

Regularization does this by bringing some of the model parameters towards 0 (L1 Regularization) or by bringing many of the model parameters somewhat towards 0 (L2 Regularization). This ""shrinkage"" effectively negates the influence of some of the parameters in complex models - and as a result, regularized models tend to have ""sparser"" solutions (i.e. contain more model parameters with values closer to 0).


Regarding this, I am still not sure if the mathematics behind why sparser models might result in less overfitting is clearly known.
The way I currently see things, Regularization seems to be more of a general heuristic : Countless evidence shows that models overfit less when you add a ""regularization penalty term"" to the model's Loss Function - and thus deliberately choose model parameters corresponding to a region of the Loss Function that is situated away from the true minimum point. Mathematically, I can understand how this happens.
But are there any mathematical justifications that suggest a sparser model based on a regularized solution is less likely to overfit data compared to a non-regularized solution -  or is this still based on heuristics and anecdotal evidence? Do we have any insights as to how the Mathematics of Regularization acts to prevent Overfitting?
","['neural-networks', 'overfitting', 'regularization']","I think different mathematical explanations exist for different situations where regularization is useful. The importance of regularization varies by problem as well. It is absolutely necessary when $p>>n$ as I'll mention below. In general it is a way to impose reasonable priors on the model though from a bayesian perspective.I'm going to put together a quick answer that I hope is somewhat satisfactory. I don't think it is exactly what you are going after though. At a high level I would recommend skimming Hastie (2001), especially section 16.2.2 titled The ""Bet on Sparsity"" Principle. You can think of sparsity in a bunch of different ways in addition to the number of zero weights in a linear model.$L_1$ penalty is better suited to sparse situations, where there are few basis functions with nonzero coefficients (among all possible choices).I think the key here is that sparsity could exist in some basis, not necessarily your model weight basis.Another even more targeted mathematics heavy book would include Statistical Learning with Sparsity.Solution identifiability
For example in the case where your parameter space is much larger than your number of samples ($p >> n$), you have an identifiability problem. Infinite numbers of solutions exist, so picking one with small total weight of parameters is just as justified as any other, but perhaps more plausible in most situations from aesthetics. Without regularization in this setting, you would have instability issues where different equally good solutions could be chosen, perhaps based on random initial conditions.Domain specific knowledge In many cases, you wouldn't expect all of your parameters to be meaningful. Enforcing sparsity will mathematically limit the solution space to one with more zeros, as you point out, or in general a solution space with a fewer number of underlying basis functions being involved in the data generation. In many domains there are a smallish number of factors that are causing a large number of observed variables to be changed, so regularization is imposing that kind of a constraint onto your model. Since the remainder of the variables are not real, or would otherwise make use of too many basis functions to represent your task, you are helping the model out by providing this useful piece of information. There are many extensions on this. For example if you know that your features are spatially correlated you could add in a fused lasso penalty, etc. The rationale here mathematically is probably something along the lines of including more noise terms in your solution results in a lower likelihood of generalizing."
Are any non-injective activation functions used?,"
All activation functions I know of are injective, which I think makes sense.
But are there cases where non-injective activations can be useful?
","['neural-networks', 'deep-learning', 'activation-functions', 'relu', 'softmax']","There is at least Swish, which is defined as $f(x) = x \cdot \text{sigmoid}(\beta x)$....This suggests that Swish can be loosely viewed as a smooth function
which nonlinearly interpolates between the linear function and the
ReLU function. The degree of interpolation can be controlled by the
model if β is set as a trainable parameter.There is an other paper which introduces Growing Cosine Unit, defined as $f(x) = x \cdot \text{cos}(x)$.The experiments (Table 1-3) show that use of GCU activation for
convolutional layers and ReLU for the dense layers provides the best
performance among all architectures considered. This is particularly
evident on the VGG-16 network trained on the Imagenette dataset, where
the GCU models outperform all ReLU architectures by 7+%. The models
with GCU in the convolutional layers also converge faster during
training as highlighted by Fig 7-8.In their experiments this GCU performed significantly better than Swish, or any other activation function."
How should I initialize the weights of the neural network so that the initial policy is uniform?,"
I would like to train a neural network (NN) so that it learns the policy and value function for my agent.
Since I am using reinforcement learning and do not want to prefer certain actions in certain states at the beginning of the learning, ideally, my NN should be initialized in a way that it predicts a uniform policy for all of the actions in every state and then during training, it will adjust its weights based on the observations.
The idea for this weight initialization is to speed up the training process by not ""delearning"" random initial policy that can be off by quite some margin and to also guarantee equal exploration from every state straight from the beginning.
I would like to ask two questions about this topic:

Is this a good idea?
Are there any available tools for achieving this?

","['reinforcement-learning', 'deep-learning', 'deep-rl', 'weights-initialization']","You may be interested in section 3.2 of this paper What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study (2020) by Google Research.They claim that the initialization of the policy is very important to performance, sometimes making a huge (66%) improvement, just from the initialization of the policy.I'm assuming you already know that the kernel weights in neural networks are initialized using the glorot uniform  distribution; in that paper, they suggest that for the output layer, those weights should additionally be divided by 100."
Effects of ReLU Activation on Convexity of Loss Functions,"
I have heard the following argument being made regarding Neural Networks:

A Neural Network is a composition of several Activation Functions
Sigmoid Activation Functions are Non-Convex Functions
The composition of Non-Convex Functions can produce a Non-Convex Function
Thus, Loss Functions for Neural Networks that contain several Sigmoid Activation Functions can be Non-Convex

Using the R programming language, I plotted the second derivative of the Sigmoid Function and we can see that it fails the Convexity Test (i.e. the second derivative can take both positive and negative values):
e = 2.718

eq = function(x){ (-e^-x)* (1+e^-x)^-2  + (e^-x)*(-2*(1+e^-x)^-3 *(-e^-x))}

plot(eq(-100:100), type='l', main = ""Plot of Second Derivative of the Sigmoid Function"")


My Question: (If the above argument is in fact true) Can the same argument be extended to lack of Convexity of Loss Functions of Neural Networks containing several ""RELU Activation Functions"" ?

On it's own, the ReLU function is said to be Convex.
Mathematically, we can show that compositions of Convex Functions can only produce a Convex Function.

However, Neural Networks that contain compositions of (only) ReLU Activation functions make it unclear to me how a Loss Functions that contains (only) ""RELU Activation Functions"" would a Non-Convex.

Can someone please comment on this? If compositions of Convex Functions can only produce Convex Functions - does this mean that the Loss Function of a Neural Network containing only containing ReLU Activation Functions can never be Non-Convex?
Thanks!

References:

https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
Note: Using some informal logic, I do not think that the Loss Functions of Neural Networks containing RELU Activation Functions are generally Convex. This is because RELU (style) Activation Functions are generally some of the most common types of activation functions being used - yet the same difficulties concerning mon-convex optimization still remain. Thus, I would like to think that Neural Networks with RELU Activation Functions are still generally non-convex.
","['neural-networks', 'optimization', 'relu']",
Is learning rate the only reason for training loss oscillation after few epochs?,"
Consider the following loss curve

The x-axis is the no. of epochs and the y-axis is the loss function.
You can observe that loss is decreasing drastically for the first few epochs and then starts oscillating.
The graph given is while training. I am using SGD optimizer. The above graph is for the constant learning rate. I also tested with a scheduler of decreasing the learning rate on a plateau. But, it is not getting better.
In general, for this situation, can I safely assume that the learning rate is the only reason? Or can there be any other reasons for this?
","['neural-networks', 'deep-learning', 'loss', 'learning-rate']",
What to look out for when designing an environment regarding observations?,"
When designing an environment, what should one look out for when designing the observation space to make the environment as easy to be learnable for an agent as possible?
E.g. make sure the markov property is fulfilled if possible, but I mean also more specific details like the coding of observation channels: Try avoiding continuous values, try to keep the number of categories for categorical observations small, or try to represent continuous values in smaller/larger intervals or consider discretizing continuous values into ordinal categorical values.
I read a paper presenting recommendations for the design of the action space, regarding several video games RL studies (Action Space Shaping in Deep Reinforcement Learning, 2020). I wondered whether there are recommendations concerning the observation space as well.
","['reinforcement-learning', 'ai-design', 'environment', 'observation-spaces']",
Which algorithms work in a non-stationary stochastic environment?,"
Currently, I am reading into the Multi-Armed-Bandit problem and found the special case of non-stationary (environment and its attributes, like the reward distribution, change over time) stochastic environments. Since this is an adversarial MAB problem, no context is available at the moment. I've read that $\epsilon$-greedy, Exp3 and the FPL algorithm work. But some tutorials like TensorFlow ones use LinUCB and other algorithms, which are never mentioned in any papers.
So, my question is, basically: which algorithms work on the non-stationary stochastic environments?
","['reinforcement-learning', 'tensorflow', 'environment', 'multi-armed-bandits']",
How do multimodal models establish connections between different modes?,"
I am specifically interested in data2vec, Meta's new model that can convert image, text, and sound data into a unified neural network representation. To my understanding, they did this through self-supervised learning by masking parts of the input and having the network predict the hidden states if the input hadn't been masked. This allows these modes to share a common representation.
However, I don't understand how the representations of different modes can be connected. For example, how are the hidden state representations of an image of a banana and the word banana trained to be similar, if they are at all?
","['deep-learning', 'hidden-layers', 'embeddings', 'self-supervised-learning']",
Why doesn't dropout mislead results during evaluation?,"
I have seen that, usually, the dropout layer is used differently in training and evaluation modes, i.e. it is recommended to use during training but not in evaluation/testing.
Dropout does remove a few nodes at random so that model does not end up in co-adaption. But, logically, if you are using one layer in training and not in evaluation/testing, should not the result be inconsistent? How/ why do we achieve the same/similar results though we are skipping a layer altogether?
","['neural-networks', 'deep-learning', 'training', 'dropout', 'testing']","How/why do we achieve the same/similar results though we are skipping a layer altogetherDropout is not a layer, even tough deep learning libraries implement it as a layer module for convenience.Why do we achieve same results? We don't, that's why dropout is applied only during training and not during test. And the fact that results change is also the core idea of dropout as a regularization technique.
When we train a model we want the model to be robust, i.e. similar input data should lead to same predictions, but due to over fitting this is almost never the case. Over fitting comes from several sources, the one addressed by dropout is preventing some weights of a model becoming too large and others becoming too small.Let's say our model is a simple equation like:$w_1*x_2 + w_2*x_2 + b = y$Where x1, x2 are two features and w1, w2 are the associate weights. It might be that the model starts over fitting, for example w1 might become too large and w2 to small, then our model will learn to focus only on feature x1, ignoring x2. By randomly ""dropping"" w1, we'll force the model to focus also on x2 as a valuable feature, preventing w2 becoming too small. Because of the randomness of dropout, the weights will converge to an optimal solution for both, not just one of them, so when applying both weights the prediction will be ideally more robust than when using only part of them. Of course in real use cases models never converge to a perfect minima where part of the weights lead to exactly the same predictions as the whole model, so in test phase dropout is disable to guarantee same predictions for same training instances every time.There is though a nice example of dropout used during test case in Deep Active Learning. Active dropout in test phase can be leveraged to perform Monte Carlo sampling of different probability scores for a single instance. The sampled probability can be then used to compute statistics like standard deviation, which can be used as an approximation of the aleatory uncertainty of the model regarding that particular instance."
Random forests - are more estimators always better?,"
I'm learning about more advanced methods of hyperparameter optimization, such as the Bayesian methods in the scikit-optimize package.  For those unfamiliar with the package, it can be used easily with model classes from scikit-learn, in this case the random forest classes such as RandomForestClassifier, and it provides more intelligent alternatives to traditional hyperparameter optimization methods like grid search.
I noticed that in some examples, the n_estimators hyperparameter (of the random forest) is included in the optimization, which I wouldn't expect.  The n_estimators hyperparameter determines the number of component decision trees in the random forest, so I would expect that more estimators always results in a better model with respect to a single target variable (for clarity, I'm not referring to anything having to do with optimizing a custom objective function in scikit-optimize, only single variables).
Ignoring practical issues like training time as well as the potential effects of randomness (i.e., that different random seeds could lead to models with varying effectiveness), are there situations where fewer estimators could result in a more accurate model?  If so, what is the rationale?
","['machine-learning', 'hyperparameter-optimization', 'decision-trees', 'random-forests']",
Weights initialization once the Neural Network is trained,"
I am trying to understand how weights are initialized in a Neural Network using Keras deep learning framework and what happens if I train a Neural Network and then I want to train it again: are the weights of the previous training stored in some way and are the weights of the same Neural Network in the new training initialized based on the latest training ?
By ""one training"" I mean all the updates that the weights have received after all the epochs that are set.
To tell all the story: so far I have made my analysis assuming that with each new training (leaving the neural network architecture unchanged) the weights were initialized without taking into account the previous training. The concern come out when I wanted to use K-fold cross-validation procedure for which the dataset is split into K parts and K-1 parts are used for training while the remaining is left for testing. Iteratively this method uses all the K parts both for training and for testing. It is advised to do this in case of few data because in this way you can train the model K times each time with a different train/test split and you can produce significantly better estimates.
This can be achieved with the following code (I am dealing with natural language processing and so the code is based on those kinds of tasks):

#Whatever is needed to prepare data to be fed to our Neural Network

....

#Build keras model and fit

classes=2 # 3 or 4

embedding_dim = 10

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, Y, test_size=.2)

# Define per-fold score containers 
acc_per_fold = []
loss_per_fold = []

# Merge inputs and targets
inputs = np.concatenate((X_train, X_test), axis=0)
targets = np.concatenate((y_train, y_test), axis=0)

num_folds=3

# Define the K-fold Cross Validator
kfold = KFold(n_splits=num_folds, shuffle=True)

# K-fold Cross Validation model evaluation
fold_no = 1
for train, test in kfold.split(inputs, targets):
    model = Sequential()#1.Define a model
    model.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length, name='embeddings')) 
    model.add(LSTM(64))
    model.add(Dense(1,activation='sigmoid')) 
    #Dense(classes,activation='softmax') for 4 classes
    print(model.summary())

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
#sparse_categorical_crossentropy for 4 classes
    
    # Generate a print
    print('------------------------------------------------------------------------')
    print(f'Training for fold {fold_no} ...')
    
    model.fit(X_train, y_train,  batch_size=128,  validation_data=(X_test, y_test),verbose=2, epochs=15) 

    # Generate generalization metrics
    scores = model.evaluate(X_test,y_test,verbose=0)
    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')
    acc_per_fold.append(scores[1] * 100)
    loss_per_fold.append(scores[0])
    
    # Increase fold number
    fold_no = fold_no + 1


After the first 15 epochs in which the Neural Network is trained on the first fold, the Neural Network is then trained on the second and again after 15 epochs it is trained on the third.
So, as said before, we do this because we can train the model K times each time with a different train/test split and you can produce significantly better estimates. So this assumes that after the first 15 epochs for the training on the first set of data, the weights are just updated towards better estimates in the second run of 15 epochs for the training on the second set of data and so on ?
In other words: if training K times is better than training once (in case of few data and under the limit of no-overfitting), does this mean that in some way the information of the previous training is stored and used for the second training and so on in order to be optimized more and more ?
In case of my specific example, I am interested in the weights of the embedding layer to have dense vector representation of my sequence of symbols. So I want to understand this for all the weights of a Neural Network in general but particularly on the embedding layers weights (I think that it is the same story for all the weights of all layers of the NN).
Anyway, besides the cross-validation procedure, if some kind of information is stored and kept from one training to the other, does it mean that each time I run my Neural Network training, this is influenced by the previous training ? For example, if I train this simple NN:
# define model
model = Sequential()
model.add(Dense(10, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# fit the model
model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)

First on a set of data and then on an other, in the second training with the new set of data, the weights are initialized on the basis of the previous training ?
On one side, this seems to me to have no sense and not to be possible, but on the other if it is not so, I do not find the meaning of using cross-validation technique (at least in the way it is done using Keras in the code I attached).
I tried to search what is the default initializer in Keras and I found this thread in Stack Overflow What is the default weight initializer in Keras?. I can find all the ways to initialize them but I am not able to find explanations about if these initializers take into account information from the previous trainings.
For example, as suggested in the answer to Stack Overflow question,  for most of the layers, such as Dense, convolution and RNN layers, the default kernel initializer is 'glorot_uniform' and the documentation says that:

Draws samples from a uniform distribution within [-limit, limit], where limit = sqrt(6 / (fan_in + fan_out)) (fan_in is the number of input units in the weight tensor and fan_out is the number of output units).

But is this uniform distribution conditioned by the weights of the previous trainings ? Or by default this is not taken into account and there is some parameter to set to consider the previous  ones ?
I think I miss something of important but I hope to have been clear and I thank you in advice.
Note: I took the code and emphasized sentence about cross-validation from this blog post: How to use K-fold Cross Validation with TensorFlow 2 and Keras?
","['neural-networks', 'training', 'keras', 'cross-validation', 'weights-initialization']","Regarding your first code snippet, there is no weight storing or continuation of training between the different CV folds whatsoever: each model is trained anew with the respective training data of each fold and validated on the validation data. Notice that this is exactly the idea behind cross validation - models trained on different folds are completely independent and they do not ""communicate"" in any manner. Should the models of the different sub-folds ""communicated"" and shared information between them, like, say, each model start training from the point the previous one finished, we would not talk about CV anymore (notice that such an approach could be valid if you purpose is something different than CV, but is is not a valid CV procedure).The situation is slightly different in your 2nd code snippet; after it finishes, you could go on, either by running more epochs on the same data, or even training with different data. All you would need is to simply add model.fit statements sequentially:What is the difference between your two code examples? In the first one, your model is redefined each time the loop executes; this defines a new model each time with the weights re-initialised, so no continuation of training. And, as already said, this is exactly the correct approach for cross validation.But in the 2nd case, you have a single model, which you can just keep training sequentially (either with the same or even with different data, as shown above).There are even cases where you would want to continue training the model outside of the current script; in such cases, you would save the model, and load it again later (or in another script altogether) in order to continue training it. This will work OK as long as you do not proceed to redefine/reinitalise the model. You can have a look at the documentation on Save and load Keras models for more.I am not able to find explanations about if these initializers take into account information from the previous trainings.They most certainly don't; if there is already existing information from previous runs (like in the cases I describe above), we do not even talk about weight initialization, i.e. there is no need for it - we already have the ""initial"" weights from the previous run. Random weight initialization in the sense discussed in the Stack Overflow thread you have linked above makes sense only for new models, like the ones produced in each run of the loop in your first code snippet above."
Why and how can the policy and value iteration methods converge to the OPTIMAL point?,"
I am reading Reinforcement Learning: An Introduction  by Sutton & Barto. According to this textbook, as far as I understood, the authors claim that the policy and value iteration methods converge to an optimal stationary point. Actually, I now understand the procedure of these two iterative algorithms, but I can't accept why they converge to an optimal point.
In the textbook and many posts that I found by googling, many people say that ""The value functions are monotonically increased as the iteration progresses. Thus, it will go to the optimal policy, as well as optimal value functions.""
I strongly agree that ""only if the algorithm's performance is monotonically improved and there exist an upper bound in terms of performance, the algorithm will converge to a stationary point."" However, I cannot accept the word ""Optimal."" I think, to claim an algorithm converges to an optimal stationary point, we need to show not only its monotonic improving property but also ""its locally non-stopping property."" (Sorry, I made these words myself, but I believe you experts can understand what I mean.)
I believe that there must be some points that I was not able to understand. Can someone let me know why the policy and value iteration methods converge to an ""OPTIMAL"" solution?
ps. Only if the system can be represented as a Markovian decision process, are either the policy or the value iteration method optimal algorithm?
","['reinforcement-learning', 'convergence', 'value-iteration', 'policy-iteration', 'policy-improvement-theorem']","These two algorithms converge to the optimal value function becausethey are instances of the generalization policy iteration, so they iteratively perform one policy evaluation (PE) step followed by a policy improvement (PI) stepthe PE step is an iterative/numerical implementation of the Bellman expectation operator (BEO) (i.e. it's numerical algorithm equivalent to solving a system of equations); here you have an explanation of what the Bellman operator isthe BEO is a contraction (proof here), so the iterative application of the BEO makes the approximate value function closer to the optimal one, which is unique, i.e. PE convergences to the optimal value function of the current policy (proof here)Policy improvement is guaranteed to generate a policy that is better than the one in the previous iteration, unless the policy in the previous iteration was already optimal (see the policy improvement theorem in section 4.2 of the RL bible)One thing that may confuse you is that you don't exactly know or have in mind the definition of the value function. A value function $v_\pi(s)$ is defined as the expected return that you will get starting in state $s$, then following policy $\pi$. So, if you have some policy $\pi$, then you perform one PE step until convergence, then you know that that value function is the optimal value function for $\pi$. Now, if $\pi_{t+1}$ is guaranteed to be a strict improvement over $\pi_{t}$, then it basically means that you will get more rewards with $\pi_{t+1}$ (which is the goal).If you read the linked proofs and chapter 4 of the bible, then you should understand why these algorithms converge.To address your last point, yes, we assume that we have an MDP. That's an assumption that most famous DP and RL algorithms make."
How to get GPT-3 to translate a specific word in a sentence?,"
I just gave GPT-3 the following prompt (in the playground, using text-davinci-001 with default settings):
What's the German word for ""can"" in the sentence ""The man removes the can.""?

The word ""can"" in this sentence is obviously a noun and not a verb. ""the can"" is the metal container used to sell coke or beer, not the common verb ""can"" which means ""be able to"".
GPT-3 obviously knows this, otherwise it could not translate the sentence correctly in many languages (which it indeed can).
However, to my prompt, it answered:
The word for ""can"" in the sentence ""The man removes the can."" is ""kann.""

(""kann"" is German for the verb ""can"" in the sense ""be able to"")
Did I ask it in a wrong way or is GPT-3 unable to answer questions like this one?
","['open-ai', 'gpt-3']","GPT-3 obviously knows thisGPT-3 doesn't ""know"" things in the sense that it has learned specific translations, maths or science etc.GPT-3 is not a knowledge database or question answering system, but it can behave like one if prompted carefully. Sometimes it is hard to get specific responses. This appears to be one of those cases.Did I ask it in a wrong way or is GPT-3 unable to answer questions like this one?GPT-3 doesn't really ""answer questions"", it completes text, and probablisitically fits that text to match the prompts. The fact that it gets a lot of Q&A, maths or translation-based prompting correct is an interesting side effect of the strong language model. It is not clear what the gaps are in it capabilities to perform secondary tasks like this, but very reasonable to expect some.In general, you should not expect GPT-3 to automatically pick up on questions with answers, and assume it is answering the question as written. Some reasonable questions might look like the start of a novel or newspaper article to it, or in your case it might be acting as if sentences are only loosely related, like a list of different translations. To reduce the flexibility in possible completions, it is normal to set some context in the first paragraph - a kind of meta-explanation of what the rest of the text is doing.I managed to get it to work like this:The following is a translation, with individual words explained.English: The man removes the can.German for ""can"" is ""Dose""My prompts are in bold. I am sure plenty of variations of what I wrote will work for this specific case. I don't think you will find a 100% reliable prompt for arbitrary words from any sentences."
"Does $(\langle w, x \rangle + b) = ||x - x'||$ hold?","
Currently, I am trying to understand the mathematics of SVM's using the textbook 'Mathematics for Machine Learning' by Deisenrot et. al.
On page 375, they define the distance between a point $x$ and the separating hyperplane as the length of the distance vector $||x - x'||$, where $x'$ corresponds to the orthogonal projection of x onto the hyperplane.
Subsequently, they formulate the following condition to express that any point's distance from the hyperplane should be greater than or equal to some constant r:
$$y(\langle w, x \rangle + b) \geq r$$

$y ∈ {0, 1}$ representing the class of $x$
$w$ normal vector and $b$ the bias.

So my question is: Does $(\langle w, x \rangle + b) = ||x - x'||$ hold?
If that's the case, I would be thankful for some hints on how both sides of the equation are related.
","['support-vector-machine', 'linear-algebra']",
"How to prove that ""w will converge to TD fixed point once A is positive definite""","
In Reinforcement Learning: An Introduction 2nd edition  section 9.4 (p. 206), it says that when we use TD(0) as target and use semi-gradient method to update :

In general, $w_t$ will be reduced toward zero whenever A is positive definite, meaning >$y^TAy > 0$ for any real vector y $\neq$ 0. Positive definiteness also ensures that the inverse A exists

How to prove this statement? I tried to prove it in the following way, but I can only get  $\lambda <1$ rather than $|\lambda|<1$.
$$
w_{t+1} = (I-\alpha A)w_t + \alpha b \\
\because\delta_{w+1} = (I-\alpha A)\delta_{w}\therefore|\lambda_{max}(I-\alpha A)|\lt 1\Rightarrow\lim_{t\rightarrow\infty}\delta_{w_t}= 0 \\
(I-\alpha A)y= \lambda y\Rightarrow y^T(I-\alpha A)y= y^Ty-\alpha y^TAy = \lambda y^Ty \\
\Rightarrow 1-\lambda = \alpha\frac{y^TAy}{y^Ty}\gt 0\Rightarrow\lambda\lt 1
$$

","['reinforcement-learning', 'math', 'proofs']",
Impact of imbalanced dataset on CNN model performance,"
I trained a 1D CNN model to model bacterial plate count based on time series data of water temperature. Bacterial place count is numerical, based on which I created two category variables, namely ""Label 1"" and ""Label 2"". There are two possible values for ""Label 1"" - ""risky"" and ""non-risky"" based on a threshold of 100 for bacterial place count . Eg. 1 if Bacterial place count >100; 0 otherwise. ""Label 2"" is (1 - value of ""Label 2""). This gives a balanced ""Label 1"" data of 43.1% for 1 and 56.9% for 0. The model observed an 80% accuracy for prediction.
Then I wanted to see if changing the threshold would change the performance of the model. So I increased the threshold to 500 and trained the model, The ratio of 1 and 0 for ""Label 1"" is now 12.5% for 1 and 87.5% for 0. And at 1000 threshold, the ratio is 8.1% for 1 and 91.9% for 0.
Then I calculated the confusion matrix results for the predictions at each threshold, as below:
Confusion matrix using 100 threshold:

Confusion matrix using 500 threshold:

Then I further increased the threshold to 1000:

Calculated Results:

**Precision**
# 100 threshold
151/(151+246) = 38%

# 500 threshold
85/(85+30) = 74%

# 1000 threshold
47/(47+12) = 79%

**Recall**
# 100 threshold
151/(151+23) = 86%

# 500 threshold
85/(85+159) = 35%

# 1000 threshold
47/(47+127) = 27%

**F1-score**
# 100 threshold
2*(0.38*0.86)/(0.38+0.86) # 52.7%

# 500 threshold
2*(0.35*0.74)/(0.35+0.74) # 47.5%

# 1000 threshold
2*(0.79*0.27)/(0.79+0.27) # 40.2%

Something I observed:

during training, there were consecutive training batches getting the same validation accuracy, for instance:


83/83 [==============================] - 1s 11ms/step - loss: 0.4065 - acc: 0.8347 - val_loss: 0.4527 - val_acc: 0.8152
Epoch 81/120
83/83 [==============================] - 1s 10ms/step - loss: 0.3943 - acc: 0.8552 - val_loss: 0.4350 - val_acc: 0.8152
Epoch 82/120
83/83 [==============================] - 1s 10ms/step - loss: 0.3800 - acc: 0.8408 - val_loss: 0.4601 - val_acc: 0.8152
Epoch 83/120
83/83 [==============================] - 1s 11ms/step - loss: 0.4026 - acc: 0.8396 - val_loss: 0.4351 - val_acc: 0.8152

I wonder if the model stops to learn as the data is highly imbalanced and so that the model would achieve higher accuracy simply by always predict the value with the higher weight. Is this a sign of the model freezing/not learning?
The graph looks like this:

And my main questions are, does the numerical threshold used to define the classes able to have a significant impact on the accuracy level of the model? How do we interpret the confusion matrix and choose the model which gives the best performance? What are the metrics we should be considering?
Any contribution is much appreciated.
","['neural-networks', 'convolutional-neural-networks', 'classification', 'training', 'confusion-matrix']",
Simple Polynomial Gradient Descent algorithm not working,"
I am trying to implement a simple 2nd order polynomial gradient descent algorithm in Java.  It is not converging and becomes unstable.  How do I fix it?
public class PolyGradientDescent {
public static double getValue(double input) {
    return 3 * input * input - 4 * input + 3.5;
}

public static void fit() {
    double x0 = Math.random();
    double x1 = Math.random();
    double x2 = Math.random();
    double size = 15;
    double learningrate = 0.0001;

    for(int i = 0; i < 400; i++) {
        double partial_x2 = 0;
        double partial_x1 = 0;
        double partial_x0 = 0;
        for(double x = 0; x < size+0.001; x++) {
            double xx = x * x;
            double y_predict = xx * x2 + x * x1 + x0;
            double delta = getValue(x) - y_predict;
            partial_x2 += xx * delta;
            partial_x1 += x * delta;
            partial_x0 += delta;
        }
        x0 = x0 + (2 / size) * partial_x0 * learningrate;
        x1 = x1 + (2 / size) * partial_x1 * learningrate;
        x2 = x2 + (2 / size) * partial_x2 * learningrate;
        System.out.println(x0 + ""\t"" + x1 + ""\t"" + x2 + ""\t"" + ""\t"" + partial_x2 + ""\t"" + partial_x1 + ""\t"" + partial_x0);
    }
    for(double x = 0; x < size+0.001; x++) {
        System.out.println(""Y: "" + getValue(x) + "", Y_Predict: "" + (x2 * x * x + x1 * x + x0));
    }
}

public static void main(String[] args) {
    fit();
}
}

","['gradient-descent', 'linear-regression', 'java']","I tested your code in python and it works just fine, when I decrease the learning rate (divided by 100) by a bit more (and the epochs multiplied by 100).I also changed the way the derivative was calculated to make it more mathematically correct :)"
Why are Directed Graphical Models considered ML methods?,"
Consider the following problem. The probability of being born in countries [1,2,3,4] is given by [a, b, c, d] respectively. This is a categorical problem.
Now, assume that the height of a person belonging to any country is normally distributed.
The task is to sample from the total distribution of countries and heights,
$
p(c,h) = p(c)p(h|c)
$
One can use a mixed distribution as a tool to model these scenarios, which are a special type of DGM.
Can someone please tell me where ML comes into play here, and how this is considered an ML technique?
","['machine-learning', 'probability-distribution', 'bayesian-networks', 'probabilistic-graphical-models']",
What can I infer if my model is converging extremely fast?,"
I am running a model with fixed hyperparameters. To my surprise/shock, the model converged extremely fast with the least loss possible.
I want to know the causes of this phenomenon. I have the following guesses:

Underlying mapping is so simple.

Hyperparameters are apt.

Both.


Are there any other reasons for this phenomenon?
","['neural-networks', 'convergence', 'hyper-parameters']",
Why should one focus on spectral operations as a computer vision researcher?,"
While reading about various types of mathematical operations on tensors, I encountered spectral operations for the first time.
The description is as follows (p. 53 of this book)

Spectral ops - Functions for transforming in and operating in the frequency domain, like stft and hamming_window

While going through different codes in computer vision, I have experience of using all other mathematical operations: Pointwise ops, Reduction ops, Comparison ops, and some others. But, I personally never come across any code using spectral operations on tensors.
So, I am interested in knowing whether one has an advantage in learning spectral operations in the domain of computer vision?
","['computer-vision', 'academia', 'spectral-analysis']",
Knowledge representation and reasoning(KRR) over a Image scene: Neurosymbolic AI,"
What are the ways and SOTA in domain of knowledge representation and reasoning over scene. Suppose there are 3 objects in the scene and which objects needs to be picked first among them is governed by 'Rules' written in text form, like Object which are square in shape will be picked first followed by Triangular shape. So basically priority queue or list of object selection needs to be generated based on rules. Or if there is any other way to handle this case?
I believe first way is to do object detection over the scene to get the different object from scene and then we have to represent 'Rules' in form of external knowledge, so that priority queue or list can be generated. However rules in textual form are non-differentiable, so how we can integrate rules in neural network training and what are the ways to represent the knowledge for such kind of priority queue generation over the image scene.
Paper 'Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding' tries to address that kind of problem but I feel its more like a general Question answering system then KRR system
","['knowledge-representation', 'reasoning', 'knowledge-based-systems', 'neurosymbolic-ai']",
Why is the WMT16 dataset favoured for evaluating machine translation models?,"
The Workshop on Statistical Machine Translation has released translation challenges each year from 2004 on, which feature a dataset of sentence pairs in a variety of languages.
Even though the conference has been taking place each year, with ever growing dataset sizes, the dataset from 2016 is still being used frequently as an evaluation dataset in scientific papers when evaluating machine translation models and it has by far the most downloads on the huggingface hub.
I would be grateful if someone could shed light on the reason for which the WMT16 dataset is so strongly favoured by the machine translation community.
","['machine-translation', 'test-datasets']",
When should discretization of observations be considered?,"
I found some literature regarding the design of action-spaces and that e.g. a discretization of continuous actions in video-game environments can be crucial for successful learning (Action Space Shaping in Deep Reinforcement Learning, 2020).
However, I didn’t find anything discussing this topic concerning observations. I’m not talking about normalization of observations, as NN are mainly used in DRL as function approximators and they perform better with normalized inputs.
E.g. continuous observations, which might be discretized into ordinal or categorical values. Imagine having a domain where the agent can close or open a gate, and it should learn whether the next object should pass or not. The agent has also to learn how to close the gate (with actions being ""reduce or increase gate width"", so the agent has to pick one action multiple times in a row to reach some certain width). Now one can design the environment to provide a continuous value for the gate width. Or discretize this and provide flags that indicate the gate is open enough or not. Are there references that discuss this topic?
The reason I’m thinking about this is because considering a policy as a mapping of actions to states, discretizing the observations will eventually lead to less mapping that has to be learned. Surely this can come at the cost of a lower peek performance, but I could also imagine higher training speed.
Is my thinking correctly concerning that? Or could discretizing the observations harm learning in other ways?
","['reinforcement-learning', 'observation-spaces', 'discretization']",
How Come My (D)DQN Fails To Learn?,"
I am currently trying to teach a (D)DQN algorithm to play a 10x10 GridWorld game, so I can compare the two as I increase the number of moves the agent can take.
The rewards are as follows:
A step = -1
key = 100
Door = 100
Death wall = -100
See the setup of the AI in the code.
My problem is, that regardless of what I do, it ends up following the same strategy of just going to the outer walls and staying there. I presume this is because, the outer walls givve the least amount of punishment per step, as the risk of dying is decreased considerably. At the same time, as the moves increase, the chance of ending up at the outer walls increases as well (see the heatmaps)

I've tried the following:

Drastically decreasing the decay of the epsilon, such that it reaches its final state only
in the last 10% of the training steps.
Running 100k moves just to add to the EMR before I start actually counting the steps.
Increasing the size of the network
Giving out a negative -2 reward for staying in the same tile
Feeding it the whole grid as the input vector

None of this has worked. The longest I have trained a single model was for 14 million training steps. Still, same strategy as before.
The way I evaluate the model is by(in this order):

At every 1 millionth training step I run 50-100k evaluation steps where I record the outcome of every step
Generating a heatmap to see whether or not it remains in the same few places (which are not the key or the door)
Running its best policy and visually estimating whether or not it has improved

CODE:
https://github.com/BrugerX/IISProject3Ugers
Training is done through the TrainingLoop.py and the evaluation is done through the HeatMapEvaluation.py.
What is it, that I have missed? How come, even after 14 million training steps, that the model still hasn't learnt to memorize the path in GridWorld?
","['deep-learning', 'deep-rl', 'dqn', 'double-dqn']",
Why doesn't the high precision of neural network weights improve accuracy?,"
Consider the following paragraph from the subsubsection 3.5.2: A dtype for every occasion chapter named It starts with a tensor from the textbook titled Deep Learning with PyTorch by Eli Stevens et al.

As we will see in future chapters, computations happening in neural
networks are typically executed with 32-bit floating-point precision.
Higher precision, like 64-bit, will not buy improvements in the accuracy of a model and will require more memory and computing time.
The 16-bit floating-point, half-precision data type is not present
natively in standard CPUs, but it is offered on modern GPUs. It is
possible to switch to half-precision to decrease the footprint of a
neural network model if needed, with a minor impact on accuracy.

The paragraph is discussing the precision of weights of a neural network. Three types of floating-point numbers are discussed: 64-bit, 32-bit, and 16-bit (half precision). It has been said that it is recommended to use 32-bit numbers. If we decrease to half-precision then it will demote accuracy but if we increase to 64-bit then there will be no impact on accuracy.
I am confused about how the improvement in precision from 32-bit to 64-bit does not improve the accuracy but the improvement from 16-bit to 32-bit can improve accuracy?
","['neural-networks', 'pytorch', 'weights', 'performance', 'precision']",
"What does it mean by ""lazy mean"" here?","
Consider the following paragraph, taken from 3.4: Named Tensors of the textbook named Deep Learning with PyTorch by Eli Stevens et al., regarding the calculation of the mean for RGB channels of an RGB image in order to convert it into a grayscale image

So sometimes the RGB channels are in dimension 0, and sometimes they
are in dimension 1. But we can generalize by counting from the end:
they are always in dimension –3, the third from the end. The lazy,
unweighted mean can thus be written as follows:
# In[4]:
img_gray_naive = img_t.mean(-3)
batch_gray_naive = batch_t.mean(-3)
img_gray_naive.shape, batch_gray_naive.shape

# Out[4]:
(torch.Size([5, 5]), torch.Size([2, 5, 5]))


Here img_t is a single image tensor of shape (3, 5, 5) and batch_t is a batch of 2 images with the same shape. They are taking the mean over the channel dimension.
Since they are taking the normal average of the intensity values over RGB channels, it is unweighted. But, what does it mean by lazy here? Are they referring to some implementation detail regarding lazy initialization? If yes, what exactly does it mean?
","['terminology', 'pytorch', 'image-processing', 'implementation', 'tensor']",
Does AlphaGo play random moves in a real competition?,"
Alphago and AlphaGo zero use random play to generate data and use the data to train DNN. ""Random play"" means that there is a positive probability for AlphaGo to play some suboptimal moves based on the current DNN; this is for exploring and learning purposes (please correct me if my understanding is wrong).
In the real tournament, does AlphaGo still play the random moves? Is the random play feature only used in the training phase?
If AlphaGo does not play a random move in the real competition, then I think AlphaGo is not learning in that competition. Human players do similar ""random play"": they usually play some random moves or strange moves in minor contests, just to test out new strategies; in major tournaments, they will be more serious and play less unprepared moves.
So, a related and broader question is: does AlphaGo learn from the game it is playing with the human in real-time?
I think the second question is less important because AlphaGo's learning curve is extremely flat compared to humans: AlphaGo learns epsilon from one single game while human can learn a lot.
","['deep-rl', 'alphazero', 'alphago-zero', 'alphago']",
"What Past Approaches is the ""Taylor Swift"" Paper Referring To? [closed]","







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



(My previous question regarded which theorem of probability was equation (2) referring to).
This paper mentions that temporal forecasting is meant to solve an integral equation as denoted by (2) (found on page 3 under the section “3. PDE based Forecasting”), and that common auto-regressive approaches in the past have tried to approximate the integral equation.
Where can I find information regarding these approaches? Unfortunately, the paper does not cite any of this, and the citations in the ""Related Work"" section seem to not make mention of the integral equation.
","['computer-vision', 'time-series']","It's the law of total probability, combined with the definition of conditional probability.\begin{align*} 
p(x_n) = \int_{x_{n-1}} p(x_n, x_{n-1}) = \int_{x_{n-1}} p(x_n \, | \, x_{n-1})p(x_{n-1})
\end{align*}
First equality uses the law of total probability, second uses definition of conditional probability. The only difference in the paper, is that everything is additionally conditioned on $\mathcal{X}_t$. (also, they are doing this for some $\tau$ steps, not just once.)"
Why does validation accuracy stop rising so soon?,"
I have implemented a GRU to deal with youtube comment data. I am a bit confused about why the validation score seems to even out around 70% and then keeps rising, this doesn't look like overfitting from what I'm used to since it keeps rising. Is this normal? Does anyone know what's happening here?
I've implemented the GRU as a simple GRU, then GRU with dropout and then a multilayer (2 layered) GRU with dropout. The graphs are shown below.



As you can see, in the multilayer GRU with dropout , the validation accuracy does nicely follow the training accuracy. Does this simply mean that the other models are simply not capable of capturing certain information? Is there a way to improve these results, what parameters should be optimized?
","['python', 'keras', 'gated-recurrent-unit']",
Where are the parentheses in the Bellman update rule?,"
I'm not having a lot of intuition about the equation. I have this Bellman update rule:
$$v_{\pi}(s) =\sum_a \pi(a|s)\sum_{s',r} p(s',r|s,a)[r+ \gamma v_{k}(s')]$$
But where are the parenthesis? Is the second sum using the index $a$ from the first sum? Or is it independent, and can I move out the $[r+ \gamma v_{k}(s')]$ term out of the sum?
","['reinforcement-learning', 'notation', 'bellman-equations']","Here's your equation with an additional couple of parenthesis that emphasizes the order of the operations (note that you had a small typo in your original equation).$$v_{\pi}(s) =\sum_a \pi(a \mid s) \left(\sum_{s',r} p(s',r \mid s,a)[r+ \gamma v_\pi(s')] \right)$$Now, let me answer your other questions.Is the second sum using the index $a$ from the first sum?Yes.Or is it independent, and can I move out the $[r+ \gamma v_\pi(s')]$ term out of the sum?No, and you cannot move this term out of the sum because the second sum is a sum over $r$ and $s'$ and $r+ \gamma v_\pi(s')$ depends on those terms.Note that $v_{\pi}(s)$ is defined as an expectation and that $\pi(a \mid s)$ (the policy) and $p(s',r \mid s,a)$ (the model) are probability distributions."
How can PPO be combined with HER?,"
I ask because PPO is apparently an on-policy algorithm & the HER paper says that it can be combine with any off-policy algorithm. Yet I see GitHub projects that have combined them somehow?
How is this done? And is it reasonable?
","['reinforcement-learning', 'proximal-policy-optimization', 'hindsight-experience-replay']",
Minimax evaluation function for games with score instead of loss/draw/win result,"
I am trying to create minimax evaluation function for the Ms Pacman game. The goal of the player is to maximize score.
I have some idea about the features that I would like to use in my evaluation function (which is weighted sum of all features):

avg distance to the ghosts
score
is pacman close to pacdots
etc.

This can work in some way if AlphaBeta search considers only non-terminal nodes. However evaluation for the terminal nodes will be substantially different from evaluation of non-terminal nodes because we are returning true evaluation which is weighted score in this case. Hence once search reaches some terminal nodes, it may discard them completely or focus too much on them (depending on how our evaluation changes).
In games with decisive result we can directly evaluate loss/draw/win with some very small/big value to overwrite the effect of evaluation function during search and properly backpropagate evaluations.
However what is the correct approach for the games with score?
I was thinking about adding another feature such as is game finished and using it in evaluation but this does not seem like an optimal solution.
I will be thankful for any tips or references.
","['game-ai', 'minimax', 'alpha-beta-pruning', 'atari-games', 'evaluation-functions']",
The results changed even though seed is fixed [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I am using a reinforcement learning model for some tasks.
and for the model, I am using stable_baselin3 and for the environment, I am using the gym.
I made a small change in the environment and the results became poor now when I rolled back that change the results didn't match the previous results even though I had made no change in the parameters or data.
The seed of the model is fixed so there is no chance that this could be due to random initialization and I have tested this on my model before by running it multiple times to see if gives the same results for the same seed and it did.
I checked that version of NumPy and the torch I used didn't change during my two runs.
and I also tried changing them manually and the results were the same for all the versions.
I am sure that the change I made is rolled back properly because it was a very small change.
Can anyone suggest a list of things that can cause results to change that will help me debug the problem?
I am setting seed for NumPy, torch(used by stable_baseline3), and random. I am not running anything in parallel. I am using the PPO model of stable_baseline3.
I am using below-given function for seed, it is provided by the stable_baseline3 library.
def set_random_seed(seed: int, using_cuda: bool = False) -> None:
    """"""
    Seed the different random generators.

    :param seed:
    :param using_cuda:
    """"""
    # Seed python RNG
    random.seed(seed)
    # Seed numpy RNG
    np.random.seed(seed)
    # seed the RNG for all devices (both CPU and CUDA)
    th.manual_seed(seed)

    if using_cuda:
        th.backends.cudnn.deterministic = True
        th.backends.cudnn.benchmark = False

","['machine-learning', 'reinforcement-learning', 'python']",
"LSTM - What kind of data should contain every dimension of input LSTM matrix, where does specific dimension points to?","
I am a beginner and I have a hard time understanding inputs and outputs of LSTM.
So from the begining, I am trying to create multivariate input&output LSTM for time series forecasting.
Thankfully I've managed to operate on multivariate inputs&outputs, but, what I don't understand are the indices and what is stored in them.
For example, I have an intial input 2d numpy array of 4 variables:
a  b  c  d
1  2  3  4
5  6  7  8
9 10 11 12

Lets say that the shape of this array is 5000x4 - thus 4 features and 5000 rows,
So for the LSTM input I'd need an array of x_train = (samples, timesteps, 4 features),
And the code for input is as following:
   model = keras.models.Sequential([
    keras.layers.LSTM(128, return_sequences=True, input_shape=[None, 4]),
    ...
    keras.layers.Dense(4)
])

Also since I want 4 features as output I need 4 output neurons.
Now for example my y_train values are in the following array:
y_train = (samples, timesteps, 4 features)

Now I'd like to understand in what dimension (samples, timesteps or 4 features) in x_train should be values from inital array?
To what values timesteps dimension points, same question about samples and features.
Now let's say I've trained my model, from the output and I get predictions array of the same dimensions as x_train and y_train, in what dimension are the exact predictions?
Since I'd like to plot them or something with them?
EDIT: Additional question
What type of data should be stored in samples, timesteps, features.
Can you give me a simple example?
",['long-short-term-memory'],
What I Should Do to Reduce Solution Size for Simulated Annealing Algorithm?,"
I am trying to find the best solution for radar placement problem with using multi objective simulated annealing algorithm.
So there is an area (in real map) and I want to put minimum count of radar as possible with the maximum area coverage. I am using multi-objective simulated annealing algorithm to do that and in small maps it works well, but when the map size increases the solution size also increases and updating it for better solutions. It becames hard to find a good solution at the end and the calculation times increases a lot.
The problem is here. What should I do to reduce the solutin size? For example my real world map is 250x500 and the solution size is 250x500 = 125000. It is hard to seperate radars randomly at on solution with this high size, it doesn't work well and consumes a lot of time. How can I reduce that solution size? For example, there are 125k different possible place to put a radar, but if it become 1k or 10k different place, it becomes easier to get a good result.
","['optimization', 'simulated-annealing']",
Reason why chess neural network might not be training,"
I've been trying to use a Stockfish-like chess evaluation neural network for the past few weeks but to no avail. I wanted to get some other opinions about why my current methods haven't worked.
Input: $8 \times 8 \times 12$ one-hot encoded board of pieces. I have around ~60 million unique examples. Evaluations are placed between -15 and 15 (anything greater or smaller is brought to 15 or -15)
Output: Single number representing Stockfish evaluation.
I've tried both fully connected and convolutional models and neither have worked very well. Here's some Tensorflow code that might give an idea of what the structures looked like:
Convolutional model:
import tensorflow as tf

def block(filters, x, res=None):
  if res is not None:
    x = tf.keras.layers.concatenate([x, res])
  x = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')(x)
  x = tf.keras.layers.BatchNormalization()(x)
  x = tf.keras.layers.MaxPooling2D(pool_size=2, strides=1, padding='same')(x)
  
  return x

board = tf.keras.Input(shape=(8, 8, 12))
conv1 = block(64, board)
conv2 = block(128, conv1)
conv3 = block(256, conv2)
conv4 = block(512, conv3, conv3)
conv5 = block(1024, conv4, conv2)
conv6 = block(1024, conv5, conv1)
conv7 = block(512, conv6)
x = tf.keras.layers.Flatten()(conv7)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)
x = tf.keras.layers.Dense(1)(x)

Fully connected model:
import tensorflow as tf

board = tf.keras.Input(shape=(8, 8, 12))
x = tf.keras.layers.Flatten()(board)
x = tf.keras.layers.Dense(1024, activation='relu')(x)
x = tf.keras.layers.Dense(1024, activation='relu')(x)
x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer='l2')(x)
x = tf.keras.layers.Dropout(0.3)(x)
x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer='l2')(x)
x = tf.keras.layers.Dropout(0.3)(x)
x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer='l2')(x)
x = tf.keras.layers.Dropout(0.3)(x)
x = tf.keras.layers.Dense(1024, activation='relu')(x)
x = tf.keras.layers.Dense(1024, activation='relu')(x)
x = tf.keras.layers.Dense(1024, activation='relu')(x)
x = tf.keras.layers.Dense(256, activation='relu')(x)
x = tf.keras.layers.Dense(1)(x)

Just to add a few notes about what the current results are, they are decent (e.g. mean squared error of Stockfish evaluated cut off between -15 and 15 is about 7). The early game is also played fairly decently but the late game gets really bad (the engine can't evaluate that bishop takes enemy queen is good). My evaluation network also evaluates the four move checkmate as black favoured.
","['neural-networks', 'deep-learning', 'chess']",
Are multi agent or self-play environments always automatically POMDPs?,"
As part of my thesis, I'm working on a zero sum game with RL to train an agent.
The game is a real-time game, a derivation of pong, one could imagine playing pong with both sides being foosball rods.
As I see it, this is an MDP with perfect information, as I use the sensor data provided by the environment to know the exact linear and angular position of the rods and the position, direction and velocity of the ball.
These information will be used as a feature vector, which will be passed into the agents network.
I considered using self-play to improve training speed regarding wall clock time, and was now not so sure if the game is after all a perfect information MDP, as there are two players involved (the same network on both sides) and the strategies of the second player are not represented in the observations fed to the network.
So the game might be a perfect information MDP, but with two ""players"" involved, is this still the case? Or does the fact that multiple learners are involved only make the environment more non-stationary, rather than partially observable? I also found a somewhat related paper: https://www.researchgate.net/publication/220301660_The_world_of_Independent_learners_is_not_Markovian
","['reinforcement-learning', 'markov-decision-process', 'pomdp', 'self-play', 'observation-spaces']","Generally, ""perfect information"" is not a formal trait of MDPs. There is a concept of the Markov property, but it only loosely coincides with ""perfect information"". For instance it is OK for there to be unknown/hidden state, provided it behaves effectively randomly (when revealed, it is drawn from a consistent distribution). An example might be a solo card game - or one where the opponent has a fixed policy such as most implementations of blackjack - which is fine as an MDP, and would not be a POMDP.A ""perfect information"" game is chess. The current state of the board gives all the information required to play, and there is no additional state. When played against an opponent with a fixed policy, then this also has the Markov property. When played in general against many different opponent, and you cannot rely on the opponents' policies being the same as the training, then effectively it is non-Markov, or a POMDP. The plays for maximising reward depend on the opponent.In many games though, there is the concept of ""perfect play"", of forcing the best possible result from the current game state. Planning/search routines using minimax approaches can theoretically solve these games, and reinforcement learning agents can learn them by assuming the opponent is also trying to play in this game-theoretic optimal approach. This is not quite the same as playing optimally against any given opponent - by comparison it is very safe and risk-averse compared to trying to trigger a mistake in the opponent due to something that the opponent may do. However, it often leads to stable solutions and strong players in RL.Self play RL will tend to converge on the same game theoretic perfect play, provided it is stable in the given environment. An agent that uses the same learning engine for both players will progress through one player or the other making a mistake, and its opponent model will take advantage of that until the losing player finds a move that corrects its mistake.To avoid this mistake/fix scenario becoming a stagnant loop of learning and unlearning, it is common to train the agent against multiple versions of itself (and sometimes other agents) to help it progress. Again this works provided the environment supports a stable game theoretic policy where it is possible to avoid mistakes against all adversarial choices. Most (possibly all? I am not sure of the theory here) two-player games are like this. When you extend to three or more players I believe this becomes a lot harder to figure out."
Does a second-order fully-connected layer have any uses?,"
I was thinking about implementing second-order regression via a fully-connected layer, and I came up with this:

$X$ is the input data, shaped $(features, batch\_number)$.
$w0$ is the bias, shaped $(output\_dims,)$.
$w1$ and $w2$ are the weights, each shaped $(output\_dims, features)$.
$Y$ is the target data, shaped $(output\_dims, batch\_number)$.
$\hat{Y}$ is the output of the layer: $\hat{Y} = w2 \cdot X^2 + w1 \cdot X + w0$

$X^2$ is the element-wise square of $X$ taken over its first axis. (I.e., what you would expect intuitively.)



We will then minimize the MSE between $Y$ and $\hat{Y}$ using gradient descent.
Is this a valid implementation of second-order regression?
Is this layer used in any deep learning architectures? What is it useful for?
Is this layer (exactly) equivalent to some MLP?
","['machine-learning', 'regression', 'multilayer-perceptrons', 'dense-layers', 'non-linear-regression']","Is this a valid implementation of second-order regression?No, but it is not far off.To perform a full second-order regression, you will need all terms for $x_{i,j}x_{i,k}$ where the first index is the example and the second index the feature. This includes every combination of two input variables. In your element-wise squaring you only produce terms $x_{i,j}x_{i,j}$, only using each feature squared, so are missing a lot of potential second-order terms.Is this layer used in any deep learning architectures?In general no. One of the goals of deep learning is to automate the discovery of useful non-linearities - including feature interaction terms - in the hidden layers. These won't be exactly the same as your idea, but can work well enough to get good performance from the neural network on a given dataset.Pre-calculating a specific set of interactions might help with some data sets, but comes at a high cost of squaring the number of features represented in any given layer, and thus multiplying the number of weights required between two layers by the size of the input layer again. Your original idea would only double the size of the input layer, but would be less general.What is it useful for?It is useful in linear regression as a way to find interactions between features. Or you can think of it as a way to automatically search through a subset of possible nonlinear interactions to see if any of those have a strong correlation to the target variable.There is a danger though that by adding a large number of terms (and associated parameters), that you may find spurious correlations and thus overfit the training data. This is more likely, the larger the number of interactions that you add to the input. Usually this kind of feature generation is paired with regularisation, to reduce the chance of overfitting.Is this layer (exactly) equivalent to some MLP?No, although because MLPs can approximate any function, it should be possible to train an MLP such that it performs very similarly.It may not hurt to insert a layer like the one you describe into some relatively small layer of an MLP, and use your idea in pracice. I expect in some circumstances it may help, but it will be a little bit of an edge case, mostly not making much difference or real advantage. However, no harm in trying it on a few sample datasets. This applies to both your idea as written, using just the squared feature terms, and to the full second order expansion - both may have their niche where they could be useful."
Is down-sampling the only purpose of using stride?,"
Stride is used in at least two operations: convolution and pooling. Both operations can be viewed as applying a kernel function on input using a kernel (filter).
Stride determines the amount of ""jump"" the kernel needs to perform on the input. Obviously, in the extreme case, if the kernel size and stride are one, the input size is the same as the output size. In all other cases, the output size is less than the input size.
So, I am guessing that down-sampling is the only purpose of using stride in any case. Am I true? Else, are there any cases in which stride is used in order to serve another purpose?
","['convolutional-neural-networks', 'convolution', 'pooling', 'stride']","The general purpose of stride (along with padding) is to determine the spatial dimensions of the output. So, with appropriate stride (and padding), you can also make the spatial dimensions of the output volume bigger than the ones of the input volume. In fact, transpose convolution, which is used e.g. in the context of convolutional auto-encoders, is based on this idea. Pooling is used for downsampling."
Is there any advantage in viewing weights of a neural network as random variables?,"
In artificial intelligence, especially in machine learning, the inputs and outputs of neurons in a neural network can be viewed as random variables.
And this view is highly useful in many ways. The usefulness of this view manifests in applying various mathematical results of random variables in understanding outputs and other aspects of neural networks. Other aspects include loss functions also.
I want to know whether it is useful to view weights of neural networks as random variables? What does the literature say about this view?
","['neural-networks', 'weights', 'random-variable']",
Why are the learned offsets of anchor boxes in the RCNN object detection models scale invariant?,"
In the original RCNN paper (https://arxiv.org/pdf/1311.2524.pdf) and continued in later RCNN papers such as faster RCNN (https://arxiv.org/pdf/1506.01497.pdf) the learned offsets of the anchor boxes are scale-invariant. For example the learned x-center offset $d_{x}$:
$d_{x} = (x-x_{a})/w_{a}$
is meant to parameterize the difference between the x-center of the anchor box and the new predicted box. However, it is also scaled by the width $w_{a}$ to make it scale invariant. Why is it important that this is scale invariant? Does it just make it easier for the neural network to learn, similar to something like batchnorm?
","['object-detection', 'r-cnn', 'faster-r-cnn']",
Output representation for a neural network to learn grid-based game with multiple units,"
I have a round based game played on a grid map with multiple units that I would like to control in some fashion using neural network (NN). All of the units are moved at once. Each unit can move in any of the grid map direction: $up$, $down$, $left$ and $right$.
So if we have $n$ units then output policy vector of NN should have $n^4$ entries that represents probabilities, one for each move.
Note that one move represents actions of all agents so if $n = 4$ then one move can look like this $a_i = (up, left, down, down)$.
However I am struggling with finding proper input/output representation of NN that would be permutation invariant against agent positions and from which I would be able to decode actions for particular agents.
I would appreciate any tips or references that discuss this topic.
","['deep-learning', 'deep-rl', 'deep-neural-networks']",
What does the complexity equation constitute exactly in “Why Should I Trust You?” LIME paper,"
I've recently been reading this paper on LIME, an algorithm to interpret ANY machine learning model. I encountered this equation (in red) on page 4 and have just been having a hard time deciphering exactly what it means. I understand that it's a measure of complexity of something - but of what exactly? And what does each symbol in the equation entail and correspond to? What part(s) of the models and instances constitute and contribute to the complexity?

For text classification, we ensure that the explanation is interpretable by letting the interpretable representation be a bag of words, and by setting a limit $K$ on the number of words, i.e. $\color{red}{\Omega(g)=\infty \mathbb{1}\left[\left\|w_{g}\right\|_{0}>K\right]}$. Potentially, $K$ can be adapted to be as big as the user can handle, or we could have different values of $K$ for different instances.

Could anyone help me with it?
","['machine-learning', 'papers', 'notation', 'explainable-ai']","Let's brake down each component of the formula, on the left side we have${\Omega(g)}$The authors state that they use omega to refer to the set of models too difficult to be tractable (or explained), so g is just a model, or set of weights, omega g is a set of models. Next we have${\infty \mathbb{1}}$This is not a common notation, at least as far as I'm aware of, but conceptually they're trying to say there an infinite amount of such models, 1 could refer to the cardinality of the set, but honestly not 100% sure about it. Finally we have:${\left[\left\|w_{g}\right\|_{0}>K\right]}$This is the constrain that separate the intractable  models from the tractable ones. K refers to the number of words allowed in the bag-of-words model used to convert words into numbers. If you're not familiar with that, you can simply think of it as one hot encoding. So if you have a vocabulary of 30k words, every word vector will be a one dimensional vector of length 30k, with a single 1 and all rest zeros. That zero norm might looks weird, but it simply refers to the length of the vectors. So to summarize:${\Omega(g)=\infty \mathbb{1}\left[\left\|w_{g}\right\|_{0}>K\right]}$Means: ""The infinite set of intractable models, made of weights vectors with length larger than a predefined value K"".PS: if you're interested in models interpretability, you might want to check Integrated Gradients, the authors created a fantastic repo that implement many algorithms for pytorch."
Perform clustering on high dimensional data,"
Recently I trained a BYOL model on a set of images to learn an embedding space where similar vectors are close by. The performance was fantastic when I performed approximate K-nearest neighbours search.
Now the next task, where I am facing a problem is to find a clustering algorithm that uncovers a set of clusters using the embedding vectors generated by the BYOL trained feature extractor [dimension of the vector is 1024 & there are 1 million vectors]. I have no information apriori about the number of classes i.e. clusters in my dataset & thus cannot use Kmeans. Is there any scalable clustering algorithm that can help me uncover such clusters. I tried to use FISHDBC but the repository does not have good documentation.
","['deep-learning', 'clustering', 'embeddings', 'self-supervised-learning', 'dimensionality-reduction']",
Why does the schema theorem of genetic algorithms hold?,"
I have been reading about the Schema Theorem - one of the first theorems from the field of evolutionary computing and genetic algorithms, largely responsible for justifying the use of genetic algorithms to solve optimization problems when the derivative of the objective function is either unknown or not clearly defined.
Genetic algorithms can be used in many different types of optimization problems, such as finding the roots of a polynomial. For example, we could use the genetic algorithm to find the roots (the zeros) of the following polynomial (this polynomial will be referred to as the objective function, i.e. the objective of the optimization/root finding):
$$f(x, y) = x \sin(4x) + 1.1 y \sin(2y)$$
Essentially, the genetic algorithm would start by taking random values of $x$ and $y$ and record the corresponding value of $f(x,y)$ for each random combination of $x$ and $y$: the value of $f(x,y)$ for a given combination of $x$ and $y$ is called the ""fitness"". The genetic algorithm works by taking many such random combinations of $x$ and $y$ and recording which combinations produce lower fitness values (i.e. which coordinates of $x$ and $y$ correspond to low elevation regions on the $f(x,y)$ surface). The genetic algorithm then combines (i.e. mutates) combinations of $x$ and $y$ that produced low fitness values, and re-evaluates $f(x,y)$ at these new mutated combinations of $x$ and $y$. The genetic algorithm repeats this mutation process many times until it successive differences in $f(x,y)$ are negligible, or after a predefined number of iterations - this is very useful in problems where evaluating the derivative of the objective function can be extremely time-consuming or costly, or the derivative of the objective function is poorly defined (i.e. standard optimization algorithms like gradient descent and newton-raphson can not be performed).
An interesting observation about the genetic algorithm is that it does not promise or guarantee convergence to the true minimum point of the objective function during the optimization process. But what the genetic algorithm does guarantee is that the overall fitness of the objective function is guaranteed to improve as the number of iterations increases. This idea is expressed in one of the original and fundamental theorems of genetic algorithms, called the Schema Theorem.
Apparently, this theorem states that ""schemas"" (e.g. combinations of $x$ and $y$) that produce better fitness values are more likely to survive after mutation - i.e. the expected number of ""schemas"" that produce better fitness values are likely to increase as the number of iterations increase.
Question: I have spent a long time trying to find a derivation of proof of the Schema Theorem - something which attempts to mathematically explain why the genetic algorithm is guaranteed to move towards the true optimum of the objection function as the number of iterations increase (rhetorical question: after all, why does the ""genetic algorithm"" not result in worse fitness results?). After all:
$$
\mathrm{E}(m(H, t+1)) \geq \frac{m(H, t) f(H)}{a_{t}}[1-p]
$$
Why does this inequality hold?
I am very curious to see if there is a mathematical proof that justifies one of the most important and earliest theorems in ""genetic algorithms"" and evolutionary computing; does the Schema Theorem have a mathematical proof?
","['optimization', 'genetic-algorithms', 'proofs']",
Why do terms in the computation of state space size scale exponentially?,"
The image below is from a Berkeley AI course pdf I found.

My question is, why do the terms accounting for the ghosts and pellets come in raised to the number of units?
For example, there are two ghosts so the exponent is 2, there are 30 foods so the exponent is 30.
Intuitively, I feel like if there are 30 foods, each with 2 states, then that is 60 states, no $2^{30}$.
","['machine-learning', 'reinforcement-learning', 'state-spaces']","Intuitively, I feel like if there are 30 foods, each with 2 states, then that is 60 states, no $2^{30}$.Let's try it with 3 pellets. If you are right there would be $2 \times 3 = 6$ states, if the authors are right there would be $2^3 = 8$ states.Using * for a pellet, and - for a space, we have the following states:That's 8 states. The authors are correct.In this case, intuitively the state is essentially one long 30-bit number - you can replace pellets with 1 and spaces with 0. In general, the rule is that if some component of the state can vary into $n$ different sub-states independently of other components, then the size of the whole state space multiplies by $n$.There is a slight oversimplification, in that the pacman might not be able eat the pellets arbitrarily (although in the example given it looks like they can). Some states might not be reachable. However, trying to figure out all the reachable states and enumerate them in a useful way so that there is still a simple vector model to learn from would be complicated - possibly more complicated than solving the reinforcement learning problem for the same game. It is easier to simplify the state representation and have a larger overspecified state space."
Does reinforcement learning lend itself well to changes in the environment due to external factors?,"
I've been reading about reinforcement learning, and it seems to me that reinforcement learning assumes the environment is static, and therefore that the reward for taking a particular action will be the same from time to time. But what if something happens outside the agent's control, that changes the reward?
As a simplified example, suppose the agent in a RL algorithm is a grape farmer living on a planet known for extreme weather events. Its goal is to maximize profit/minimize losses. Grape seeds cost 1000 dollars. It takes 1 year for the grape seeds to grow delicious grapes. There is a 20% chance of the weather destroying the grapes during the year, leading to the agent losing 1000 dollars. However, if the weather does NOT destroy the grapes, the agent wins $2000.
The agent starts the year with two choices:

Grow grapes
Don't grow grapes

In its first year, an adverse weather event occurs, killing the grapes. Will the agent ever grow grapes again, or will it forever decide that the best way to win is to not plant grapes? Likewise, if it ever DOES grow grapes, can we trust it to know when not to grow grapes?
",['reinforcement-learning'],
How to colorize images with Variational Autoencoder?,"
CONTEXT
I'm trying to colorize images with Variational Autoencoder. Input is 256x256 gray image. Output is 256x256x2 as I convert image to a LAB color space and then put gray channel as input and other two as outputs.
PROBLEM
My network is training, but loss is not getting smaller epoch after epoch.
Epoch 2/5 720/720 [==============================] - 34s 47ms/sample - loss: 0.1081 - val_loss: 0.0860 
Epoch 3/5 720/720 [==============================] - 35s 48ms/sample - loss: 0.1074 - val_loss: 0.0853 
Epoch 4/5 720/720 [==============================] - 34s 48ms/sample - loss: 0.1070 - val_loss: 0.0855 
Epoch 5/5 720/720 [==============================] - 34s 48ms/sample - loss: 0.1068 - val_loss: 0.0854

After training I can put gray image into the network to get color channels, but I always get the same sort of orange dot in the middle of the screen. No matter how long I train or how many training images I use.
Result:

Tech stuff
Network I use:
            dropout = 0.25

            input_data = tensorflow.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1))

            encoder = tensorflow.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_data)
            # encoder = tensorflow.keras.layers.MaxPooling2D((2, 2))(encoder)
            encoder = tensorflow.keras.layers.Dropout(dropout)(encoder)

            encoder = tensorflow.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoder)
            encoder = tensorflow.keras.layers.MaxPooling2D((2, 2))(encoder)
            encoder = tensorflow.keras.layers.Dropout(dropout)(encoder)

            encoder = tensorflow.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoder)
            # encoder = tensorflow.keras.layers.MaxPooling2D((2, 2))(encoder)
            encoder = tensorflow.keras.layers.Dropout(dropout)(encoder)

            encoder = tensorflow.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(encoder)
            encoder = tensorflow.keras.layers.MaxPooling2D((2, 2))(encoder)
            encoder = tensorflow.keras.layers.Dropout(dropout)(encoder)

            encoder = tensorflow.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(encoder)
            # encoder = tensorflow.keras.layers.MaxPooling2D((2, 2))(encoder)
            encoder = tensorflow.keras.layers.Dropout(dropout)(encoder)

            encoder = tensorflow.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(encoder)
            encoder = tensorflow.keras.layers.MaxPooling2D((2, 2))(encoder)
            encoder = tensorflow.keras.layers.Dropout(dropout)(encoder)

            encoder = tensorflow.keras.layers.Flatten()(encoder)
            encoder = tensorflow.keras.layers.Dense(16)(encoder)


            def sample_latent_features(distribution):
                distribution_mean, distribution_variance = distribution
                batch_size = tensorflow.shape(distribution_variance)[0]
                random = tensorflow.keras.backend.random_normal(
                    shape=(batch_size, tensorflow.shape(distribution_variance)[1]))
                return distribution_mean + tensorflow.exp(0.5 * distribution_variance) * random


            distribution_mean = tensorflow.keras.layers.Dense(2, name='mean')(encoder)
            distribution_variance = tensorflow.keras.layers.Dense(2, name='log_variance')(encoder)
            latent_encoding = tensorflow.keras.layers.Lambda(sample_latent_features)(
                [distribution_mean, distribution_variance])

            encoder_model = tensorflow.keras.Model(input_data, latent_encoding)
            encoder_model.summary()

            decoder_input = tensorflow.keras.layers.Input(shape=(2))
            decoder = tensorflow.keras.layers.Dense(512)(decoder_input)
            decoder = tensorflow.keras.layers.Reshape((1, 1, 512))(decoder)
            decoder = tensorflow.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(decoder)
            decoder = tensorflow.keras.layers.UpSampling2D((2, 2))(decoder)

            decoder = tensorflow.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(decoder)
            decoder = tensorflow.keras.layers.UpSampling2D((2, 2))(decoder)

            decoder = tensorflow.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(decoder)
            decoder = tensorflow.keras.layers.UpSampling2D((2, 2))(decoder)

            decoder = tensorflow.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(decoder)
            decoder = tensorflow.keras.layers.UpSampling2D((2, 2))(decoder)
            
            decoder = tensorflow.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(decoder)
            decoder = tensorflow.keras.layers.UpSampling2D((2, 2))(decoder)
            
            decoder = tensorflow.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(decoder)
            decoder = tensorflow.keras.layers.UpSampling2D((2, 2))(decoder)  #
            decoder = tensorflow.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(decoder)
            decoder = tensorflow.keras.layers.UpSampling2D((2, 2))(decoder)  #
            decoder = tensorflow.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(decoder)
            decoder = tensorflow.keras.layers.UpSampling2D((2, 2))(decoder)

            decoder_output = tensorflow.keras.layers.Conv2D(2, (3, 3), activation='tanh', padding='same')(
                decoder)

            decoder_model = tensorflow.keras.Model(decoder_input, decoder_output)
            decoder_model.summary()

            encoded = encoder_model(input_data)
            decoded = decoder_model(encoded)

            vae = tensorflow.keras.models.Model(input_data, decoded)

            def get_loss(encoder_mu, encoder_log_variance):
                def vae_r_loss(y_true, y_predict):
                    reconstruction_loss_factor = 10
                    reconstruction_loss = tensorflow.keras.backend.mean(
                        tensorflow.keras.backend.square(y_true - y_predict), axis=[1, 2, 3])
                    return reconstruction_loss_factor * reconstruction_loss

                def vae_kl_loss(y_true, y_pred):
                    kl_loss = -0.5 * tensorflow.keras.backend.sum(
                        1.0 + encoder_log_variance - tensorflow.keras.backend.square(
                            encoder_mu) - tensorflow.keras.backend.exp(encoder_log_variance), axis=1)
                    return kl_loss

                def vae_loss(y_true, y_pred):
                    r_loss = vae_r_loss(y_true, y_pred)
                    kl_loss = vae_kl_loss(y_true, y_pred)
                    return r_loss + kl_loss

                return vae_loss


            vae.compile(loss=get_loss(distribution_mean, distribution_variance), optimizer='adam',
                        experimental_run_tf_function=False)
            vae.summary()

            history = vae.fit(X, y, epochs=5, batch_size=8, validation_split=0.2)


Network summary:
Model: ""encoder""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 256, 256, 64) 640         input_1[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 256, 256, 64) 0           conv2d[0][0]                     
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 256, 256, 128 73856       dropout[0][0]                    
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 128, 128, 128 0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 128, 128, 128 0           max_pooling2d[0][0]              
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 128, 128 147584      dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 128, 128, 128 0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 128, 128, 256 295168      dropout_2[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_3[0][0]                   
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 64, 64, 256)  0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 64, 256)  590080      dropout_3[0][0]                  
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 64, 64, 256)  0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 64, 512)  1180160     dropout_4[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 512)  0           conv2d_5[0][0]                   
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 32, 32, 512)  0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
flatten (Flatten)               (None, 524288)       0           dropout_5[0][0]                  
__________________________________________________________________________________________________
dense (Dense)                   (None, 16)           8388624     flatten[0][0]                    
__________________________________________________________________________________________________
mean (Dense)                    (None, 2)            34          dense[0][0]                      
__________________________________________________________________________________________________
log_variance (Dense)            (None, 2)            34          dense[0][0]                      
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 2)            0           mean[0][0]                       
                                                                 log_variance[0][0]               
==================================================================================================
Total params: 10,676,180
Trainable params: 10,676,180
Non-trainable params: 0
__________________________________________________________________________________________________
Model: ""decoder""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               1536      
_________________________________________________________________
reshape (Reshape)            (None, 1, 1, 512)         0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 1, 1, 512)         2359808   
_________________________________________________________________
up_sampling2d (UpSampling2D) (None, 2, 2, 512)         0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 2, 2, 256)         1179904   
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 4, 4, 256)         0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 4, 4, 128)         295040    
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 8, 8, 128)         0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 8, 8, 64)          73792     
_________________________________________________________________
up_sampling2d_3 (UpSampling2 (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 16, 16, 32)        18464     
_________________________________________________________________
up_sampling2d_4 (UpSampling2 (None, 32, 32, 32)        0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 32, 32, 32)        9248      
_________________________________________________________________
up_sampling2d_5 (UpSampling2 (None, 64, 64, 32)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 64, 64, 32)        9248      
_________________________________________________________________
up_sampling2d_6 (UpSampling2 (None, 128, 128, 32)      0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 128, 128, 16)      4624      
_________________________________________________________________
up_sampling2d_7 (UpSampling2 (None, 256, 256, 16)      0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 256, 256, 2)       290       
=================================================================
Total params: 3,951,954
Trainable params: 3,951,954
Non-trainable params: 0
_________________________________________________________________
Model: ""vae""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 256, 256, 1)]     0         
_________________________________________________________________
encoder (Model)              (None, 2)                 10676180  
_________________________________________________________________
decoder (Model)              (None, 256, 256, 2)       3951954   
=================================================================
Total params: 14,628,134
Trainable params: 14,628,134
Non-trainable params: 0
_________________________________________________________________
Train on 720 samples, validate on 180 samples

Questions

Why my network seems to not doing any learning?
Are loss functions defined properly?

","['convolutional-neural-networks', 'variational-autoencoder', 'image-generation']",
Is a Transformer a good choice for multivariate signal classification?,"
I am working on a problem regarding the multi-classification of multivariate time signals. So I have multiple signals and try to train an algorithm on them. My current approach is to build a neural network with LSTM-layers and it works pretty fine.
I have read that LSTMs are pretty outdated because of the transformer architectures. I found some papers about the idea to use them for signal classification (see: https://arxiv.org/abs/2103.14438). There was an example on the TensorFlow page regarding univariate signal classification (see https://keras.io/examples/timeseries/timeseries_transformer_classification/). I think it is rather a research question than a common approach for this type of problem.
To my questions:

Would you recommend implementing a Transformer for this type of problem? Do you think, it is a more ""state of the art"" approach?

Do you know some example projects?


","['classification', 'transformer', 'time-series']",
Are RNN a good approach to solve this type of problem?,"
I have a problem that can be optimized by taking five actions, and finally, after a series of steps to achieve a solution.
The actions (1 to 5) are picked randomly. A time-step (epoch) is concluded every time five actions are taken. The outcome is influenced by the sequence of actions and the time-step.
E.g Action sequence: 5,4,2,1,3 may be better for time 3 than time 9.
Time 1 Action   A1  A5  A3  A4  A2
Time 1 Outcome  0   0   5   122 6

Time 2 Action   A5  A3  A2  A1  A4
Time 2 Outcome  42  0   9   80  7

Time 3 Action   A3  A2  A1  A4  A5
Time 3 Outcome  13  43  1   0   10

...

Time N Action   A2  A3  A4  A5  A4
Time N Outcome  15  100 120 42  75

The goal is to collect as many points as possible by the Nth round.
Should I approach this problem with RNN, or is there a more suitable theory/algorithm?
","['machine-learning', 'recurrent-neural-networks']",
How to output an integer/discrete number n with a single output neuron?,"
Say I have a game with 4 base actions [left, right, up, down] and then a value n, which determines how many times the chosen action is repeated.
For example, action = left, n = 3 -> go left 3 times. In this game $(left,1)*3 \neq (left,3)$ as negative reward is handed out at every single time step (this is for research purposes, so it cannot change).
I would like to test how a DDQN and a DQN algorithm are affected, as I increase the number of actions available (increase $n$).
My question is; Is there a smarter way to implement this, other than increasing the depth of the output layer? I.e len(output_layer) = n?
I was thinking of whether or not, there was a way for a single neuron to determine n and then have 4 other neurons that determine the best action? Would this even have any positive effects? (such as less training time, faster computation, better generalization, etc.)
If yes, how would this typically be done?
","['neural-networks', 'reinforcement-learning', 'dqn', 'activation-functions', 'network-design']","Predicting the correct amount of repetitions for an action sounds like a regression task. Turning it into a classification task using a model with n output nodes will lead to several drawbacks, the biggest ones being:So a better choice in my opinion would be to treat this problem as a regression one, i.e. using a single output node trained on predicting continuous values, and round the predictions to convert real values to integers.Of course the task of choosing an action is a classification one, but it is possible to combine classification and regression into a single model, using Multi Task Learning, the idea being to train a model with one head (input layer) one body (some hidden layers) and 2 tails (output layers), one trained on classification and one trained on regression. The image below is an example of such model (the t subscript in the output layers stands for task). There are several other options to accomplish the same results, so check this survey as well.Last but not least, if multitask learning is not suitable for you, you could simply train 2 individual models, one for action selection and one for repetition prediction. If going down on this route I would suggest to perform classification first and then repetition prediction, since the output of the classification model would be relevant for the regression one."
"How does a bounding box detection network ""know"" about absolute position?","
I've always found bounding box regression a bit weird. There's no positional encoding like in vision transformers, so how does the network ""know"" the absolute position when producing bounding box coordinates? It gets even weirder when we are dealing with two-stage detectors because, in the second stage bounding box regression, only an ROI is available, not the whole image.
","['computer-vision', 'object-detection', 'bounding-box']","It of course depends on the detection model that is used.But in your case I think you relate to a Faster-RCNN type architecture for bounding box detection. In this case only the relative position to an anchor is regressed, that is correct. Of course the regression ""values"" isolated have no information where they are absolute in the image since the same convolution is shared and slided over the whole image/layer. However, after your regression layer is done (i.e. slided the convolution over the whole previous layer) it outputs a new feature map with relative regression values. The regression values itself do not contain the information about the abolute position, but the xy position of the relative feature values inside the output feature map of the regressor combined with the relative values itself gives you the information about the absolute position.This also makes it ""translation invariant"", which is an important and wanted feature of Faster-RCNN architectures."
Are there any works that deal with 2D pose estimation in videos?,"
Since pose estimation is often a task where spatial-temporal context should be helpful in finding subsequent key points, I thought there should be many papers on it. However, I could not find any work that deals with 2D pose estimation in videos.
Am I missing something, or am I just hindered by a large number of papers on 3D position estimation?
","['convolutional-neural-networks', 'computer-vision', 'reference-request', 'transformer', 'pose-estimation']",
Is logic AI a complement to learning AI?,"
I want to know the relation between logic AI and learning AI.
Logic AI here refers to the branch of AI that is based on mathematical logic. Learning AI refers to the branch of AI that is based on learning from data. The data can be in the form of either labelled dataset, unlabelled dataset, experience, etc.
Can we show a Venn diagram with the universal set as the complete spectrum of AI, while logic AI and learning AI as subsets in it?
","['machine-learning', 'comparison', 'logic', 'statistical-ai', 'symbolic-ai']",
Does reaching the global optima guarantee good performance in a task?,"
It is to my understanding that, in deep learning, we are essentially trying to minimize the loss function that we have defined and reach its global optima through some form of optimization technique. However, with AI being applied to so many fields, can we be sure that there exists a loss function for every task, such that upon reaching its global optima, it guarantees reasonably good/human-level performance in the task?
","['deep-learning', 'applications', 'optimization']",
Which data representation of text as input for NLP Deep Learning models?,"
I have been given a data set with 30.000 text documents (each text file is rather small with respect to its length and consists in most cases of around 20 sentences), which are labelled with 0 or 1. Using this data set, I want to train machine learning and deep learning models in order to be able to classify new text files.
On the one hand, I want to use classical machine learning models (such as logistic regression, random forest, SVM, etc.) with the Bag of Words (BoW)/TF-IDF approach. In this context, the text data are represented by a matrix with 30.000 rows (and a number of columns that correspond to the unique words in the overall data set), where each row stands for one observation (i.e., a text document) and each column stands for a unique word. The entries of this matrix are then (kind of) frequencies of the unique words in a text document. However, these approaches do not take the sequence of words, negations, etc. into account.
On the other hand, I want to use new deep learning models (such as RNN, LSTM, BERT, XLNET, etc.), which take the sequence of words, etc. in a text document into account. Obviously, the data set of text files cannot be represented with the BoW or TF-IDF approach in this case as this would neglect the order of words, etc.
Which data representation technique can be used to input a data set with labelled text files into a deep learning model (such as RNN, LSTM, BERT, XLNET, etc.)? Is there something similar like the BoW or TF-IDF approach that also pays attention to the sequence of words, etc.?
","['machine-learning', 'deep-learning', 'natural-language-processing', 'bag-of-words', 'tf-idf']",
Equations for computing true positives and false positives when using object detection algorithms?,"
I am running some evaluation metrics using the YOLOv5 object detection algorithm, and wish to calculate my true positives and false positives. For instance, the evaluation metric outputs are as follows:
   Class          Images    Labels     Prec     Recall     mAP@.5     mAP@.5:.95: 
     all          100         36      0.444      0.702      0.481      0.223
 Class 1          50          29      0.588      0.689      0.668      0.333
 Class 2          50           7      0.301      0.714      0.293      0.113

Looking at this source, I found that you could calculate the true positives and false positives with the following equations:
#Computed for Class 1

TP = Recall * Labels = 34.45 ≈ 34
FP = (TP / Precision) - TP = 23.82 ≈ 24

I am new to evaluation metrics, so at first glance, I'm thinking that the false positive number is fairly high. Is this the correct formula to compute the true positives and false positives? I'm just looking for some verification and some explanation as to why this works, if it does.
","['deep-learning', 'object-detection', 'precision', 'recall']","Recall is the fraction of the relevant documents that are successfully retrieved.
\begin{aligned}{\text{Recall}}&={\frac {tp}{tp+fn}}\,\end{aligned}Labels for a Class is equal to total examples which are actually belonging to the class: P = FN + TPHence (FN + TP)* Recall = TPPrecision is the fraction of retrieved documents that are relevant to the query:
\begin{aligned}{\text{Precision}}&={\frac {tp}{tp+fp}}\end{aligned}Using simple maths you will easily get FP = (TP / Precision) - TPIn your calculation your might have use Images instead of Labels:Reference: https://en.wikipedia.org/wiki/Precision_and_recall"
Transforming Moving Position Data to an Inputvector for Neural Networks,"
Imagine a car is driving on a long street (= x-axis). The car can go in both directions and it will arbitrarily change its direction.
I'm trying to formulate an Inputvector to tell a neural Network which area of the street was driven on and how often that area was driven on.
What is known are the exact coordinates (x - coordinates) of the car with corresponding time stamps.
The neural network needs to be updated every minute, so I have to come up with an intelligent way how to ""summarize"" one minute of movement of the car.
My ideas:
I could calculate the following from the available position data with timestamps:

weighted mean of the coordinates of the car
the range (i.e. xmax - xmin)
the travelled distance within one minute

these three inputs already give the neural network a good idea on how the street was being used. However, what if the car drives two times like depicted in this picture:

Both cases would result in the same mean position 0.5, the same range 1 and the same travelled distance left: 5*1 = 5, right: 6*0.33 + 1 + 6*0.33 = 5 However, the middle part of the street was driven on less on the right than on the left. How can I differentiate between these two cases?
A different approach to this problem as a whole would be to instead of using mean + range + travelled distance, I just divide the street into 10 parts and tell the neural network which part was being used how intensely. E.G. for the case of the picture on the right where mostly the first and the last third of the axis was being used the input would be something like:
[0.15, 0.15, 0.15, 0.1, 0, 0, 0.1, 0.15, 0.15, 0.15]
However, what I don't like about this approach is that first of all it's not very precise cause I'm classifying the street into 10 parts, I would prefer to have a continuous solution. Also, It will generate too many inputs (I have to do it for 3 axes x,y,z) so I will have 30 inputs and the problem is that I will also need some other inputs for different reasons so the directional inputs will be too ""heavy"" altogether.
Maybe someone of you knows a different approach how to deal with that kind of problem. Or maybe I have to use some special kind of neural network architecture (something like a time delayed neural network with 60 time delayed one-second inputs instead of a single one-minute input) Help would be greatly appreciated.
","['neural-networks', 'data-preprocessing']",
How to pass the rewards in zero-sum multiplayer context when using REINFORCE?,"
Suppose there are two players in my zero-sum game and they play in a row like chess. And I want to learn the policy function using the REINFORCE algorithm.
I have doubts about passing reward values in the episode trajectory. If there is a single agent, then it is straightforward to pass the reward values based on the action performed by the agent in a particular state. But, in the case of multiple players, action by any player $k$ may need the updation of reward for any other player $\ell$. In such cases, I have doubts about passing rewards for player $\ell$.
Suppose, in the state $s_0$, player 1 did an action $a_0$ and causes the reward of $r_1$ to player 1 and -1 to player 2 and leads to state $s_1$. Then while it's turn, player 2 performs some action $a_1$ on state $s_1$ which caused a reward of $r_2$ to it. Then which trajectory should I need to pass among the following?

$s_0a_0r_1s_1a_1r_2 \cdots$
$s_0a_0r_1s_1a_1(r_2-1) \cdots$

I am guessing the second one. But my doubt is that the reward $r_2$ is due to the action of player 2 and $r_2-1$ cannot be viewed as an immediate reward since $-1$ is due to the action of player 1 in the previous time step. How to pass the reward values in this case?
","['reinforcement-learning', 'deep-rl', 'rewards', 'reinforce']",
How can I adapt a trained neural network model to learn from newer data containing additional features?,"
We shall assume that we have a trained neural network model for some task $A$. The dataset used to train the model contained some $n$ features per sample. Using this dataset, we were able to train a classifier.
After some time, I get some more data for the same task $A$, but with additional features (all relevant). I want to use this new data to improve my existing classifier by utilizing the weights. (whatever it learned from the initial dataset).
This seems very similar to the problem of transfer learning, but my aim is to use an existing model architecture without modifying it too much and make it a better classifier for the same task A without having to do any retraining. (We can assume that data is lost after training, and all we have are the parameters/weights of our model)
After doing some reading, this could be a use case of continual learning but most of the work is for training an existing model for different tasks. I am not able to fully wrap my head around how I could use the same model for the same task, but instead, the structure of the data is changing over time. Can you also refer me to any literature in this space, if any?
","['neural-networks', 'deep-learning', 'incremental-learning', 'representation-learning']",
My accuracy wont improve in tensorflow [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I've been trying to figure out why this model won't train (the accuracy stays at 0).
frame_size = 2
sequence_length = 5
input_shape = (sequence_length, frame_size)


def label_x(x: np.array):
    return np.average([np.sum(frame) for frame in x])


def build_training_set(count: int):
    xs = np.random.rand(count, *input_shape)
    return xs, (np.array([label_x(x) for x in xs]))


assert 12.4 == label_x([[1, 2], [5, 7], [12, 2], [21, 1], [5, 6]])

train_count = 10000
validate_count = int(train_count / 10)

input = Input(shape=input_shape)
hidden_1 = Dense(20, activation='relu')(input)
rnn = LSTM(20)(hidden_1)
output = Dense(1)(rnn)
model = Model(inputs=input, outputs=[output])

model.compile(
    optimizer=keras.optimizers.RMSprop(),  # Optimizer
    # Loss function to minimize
    loss=""mse"",
    # List of metrics to monitor
    metrics=[""accuracy""],
)
print(model.summary())

x_train, y_train = build_training_set(train_count)
x_val, y_val = build_training_set(validate_count)
history = model.fit(
    x_train,
    y_train,
    batch_size=64,
    epochs=200,
    validation_data=(x_val, y_val),
)

Anyone have an idea?
","['recurrent-neural-networks', 'tensorflow']",
What are the base rules for the symbolic integration implementation?,"
I want to implement a full symbolic integration. To achieve this. I've learned from Prof. Patrick Winston's AI lecture that Matlab uses

12 safe transformations, like that constant out, sums, etc.
12 heuristic transformation e.g. sin x, cos x...
and, a table that contains 26 anchors to the calculations.

So that every integration can be calculated using these rules.
In the lecture, they only listed 4 of the safe transformations, 3 of the 12 heuristic transformations, and 3 of the 26 anchors.
In order to implement the full symbolic integration, I need the rules. So; what are those? or where can I find them?
","['reference-request', 'integration']","First of all, let me clarify that I think that this question is on-topic here, as symbolic integration is a problem that also humans solve, so it requires some kind of intelligence.Second, I had also watched that interesting lecture by Winston a few months ago, so I remember that some of the rules that he mentions during the lecture are just rules that humans sometimes also use to solve integrals (e.g. integration by parts, which you should have an idea of, if you ever took a calculus course in high school), but he also mentions other problem-solving techniques used in AI, like And–Or trees.That being said, I've never implemented any symbolic integration program, but I think that the paper A heuristic program that solves symbolic integration problems in freshman calculus (1963), by James R. Slagle, could (at least partially) answer this question, as it describes a symbolic integration program, SAINT (which stands for Symbolic Automatic INTegrator), and it also mentions a few transformations/rules that it uses, and the And-Or goal trees, which are also mentioned by Winston in that lecture, so I guess Winston may have been referring to the techniques in this or a similar paper (I would need to rewatch the lecture to confirm this). More details about SAINT are given in Slage's 1961 Ph.D. dissertation.I suspect that, nowadays, there may be more efficient programs than SAINT to solve symbolic integration, so I will leave the details to an expert on the topic.In addition to that, you can find on the web articles (for example, this, this, and this) that describe the common integration rules."
Which machine learning algorithm can be used to identify patterns in a large file of numbers?,"
I'm new to machine learning and have many questions, but today I want to know if my case can be solved by machine learning, and if the answer is yes, I would like to know what to learn first and which lessons should I follow to accomplish that.
So my case is such: I have numbers, from 0 to 65.535. These numbers are randomly written in the doc (text file, any format). For example: ,  15.623, 14, 0, 64.322, 5, 5, 15.623, 14, 0, 64.322, 5, 35.2323, 123, 532, 5.764, and etc. They are not limited in the quantity and can be repeated.
I want to take these numbers, find patterns inside the whole file and shrink the data, but remember the positions for reverse engineering.
For example: 5,  15.623, 14, 0, 64.322, 5, 5, 15.623, 14, 0, 64.322, 5, 35.2323, 123, 532, 5.764.
I can see that 5 is followed by 15.623 in 2 cases, and their positions are 0, 1, and 7, 8. I want to assign 5, 15.623 a new character with a marker (not to confuse with the original numbers) = *1.
So my new text would look like this: *1, 14, 0, 64.322, 5, *1, 14, 0, 64.322, 5, 35.2323, 123, 532, 5.764.
So I will have a new file, much smaller in character count, and another code where I save the assigned numbers to new characters.
The reason I want to use machine learning is the following: I want to reduce character count in my original file and finding patterns in millions of numbers can drastically reduce the count. Also, there will be MANY files with millions of numbers inside and I want to train the AI to faster identify patterns and deliver the best method possible.
How to do it, what I should learn and what direction I should take to accomplish that?
Mention in the comments if you need any additional information.
","['machine-learning', 'training']",
Is it the high probability action that is always selected by the agent in REINFORCE algorithm?,"
Consider the following algorithm from the textbook titled Reinforcement
Learning: An Introduction (second edition) by Richard S. Sutton and Andrew G. Bart

While playing the game for the generation of an episode trajectory, how is the action selected by the agent? I mean, how does the agent selection action $A_i$ from state $S_i$ for $0 \le i \le T-1$. I am getting this doubt as the policy is stochastic in nature and doesn't give a single action as output.
But the algorithm says to generate the episode following the stochastic policy function.
Is it always the action that has the high probability in $\pi(a|s,\theta)$?
Note: The trajectory here is $(S_0,A_0,R_1 \cdots S_{T-1},A_{T-1},R_{T})$,
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'reinforce', 'exploration-strategies']","You sample according to the probability distribution $\pi(a \mid s, \theta)$, so you do not always take the action with the highest probability (otherwise there would be no exploration but just exploitation), but the most probable action should be sampled the most. However, keep in mind that the policy, $\theta$, changes, so also the probability distribution. This implementation could be useful, as it shows exactly what I've just said.I've also seen another implementation that applied some kind of $\epsilon$-greedy policy to sample from $\pi$ (i.e. with probability $\epsilon$ you could choose the greedy action and with probability $1 - \epsilon$ any other action), but I am not sure how common or useful that could be in the basic REINFORCE algorithm. This question seems to be asking about additional exploration strategies in policy gradients, in case you're interested."
"What determines when Dropout, BatchNorm & other Regularization will be effective?","
I just had a very strange experience where I was training an 8 layer deep & pretty wide (max: 512 neurons) neural network for a regression task. I had assumed since it was big enough that it would benefit from some combination of dropout, batch norm, and/or skip layers. And I expected all 3 would be best. I turns out however that each one of them made the model perform much worse, together or nearly in any combination (even at 10% dropout rate).
This made me wonder when you can expect these methods to help the model vs hurt? I thought they were pretty universal before this (if the model is big enough)... It might be related to this data's strange distribution, I think it is exponential (except it can dip negative sometimes). But scaled to stay within 0-1.
P.S. Similar experience with log transforming the data!
I've attach a histogram of the predicted variable for reference in my example (log scale using only positive values, which are the majority).

",['regularization'],
"In Policy Gradient methods, why are actions always chosen from a Gaussian in the literature?","
In Sutton's 2020 Reinforcement Learning text (in chapter 13.7 Policy Parameterization for Continuous Actions) it's stated

actions [may be] chosen from a normal (Gaussian) distribution.

However, I can't seem to find the justification for choosing a Gaussian distribution. It seems somewhat presumptive. I understand we choose a distribution we can sample from to model a continuous action space but not why we choose a specific distribution.
Why not fit a more complicated distribution instead of just learning the mean and std. dev. of the Gaussian?
I've noticed throughout different implementations and papers, the distribution is assumed Gaussian.
","['reinforcement-learning', 'policy-gradients', 'sutton-barto']",
"Does ""number of actions"" refer to the number of actions taken or size of the action space?","
In the original DDQN article (https://arxiv.org/pdf/1509.06461.pdf,) the phrase ""number of actions"" is used twice;
First, in the following context:

Secondly in Theorem 1.

I have a hard time understanding the way the phrase is being used or if it is being used in the same way, could anyone care to explain how it is meant to be understood?
","['deep-learning', 'papers', 'dqn', 'double-dqn', 'double-q-learning']","The expression ""number of actions"" is being used in the same way in both cases. In fact, the letter $m$ is used in both cases. The number of actions (in the state $s$) is the number of possible actions that you can take in the state $s$. So, here, $m$ does not refer to the dimensionality of an action, but to the size of the action set for a state. So, if $A(s)$ is the set of actions that you can take in $s$, then $m = |A(s)|$. The paper that they refer to in the first excerpt actually uses (on page 3, in the lemma) $n$ to refer to $m$ and states that $n$ is the number of actions. Here there's a related question, which you may want to read."
How to compute the loss for a sequence labeling task without the Softmax distribution?,"
For a sequence labeling task (NER), we compute the loss by passing the softmax distribution of the classes (e.g. vocabulary) with the gold label to the loss function (nn.CrossEntropyLoss(y, x)):
y = tensor([[ 1.4775,  0.7022,  0.7499, -0.7535, -1.4983, -2.3193, -0.6166, -2.3302,
     -0.8847, -0.1915]])

label:
x = torch.tensor([2])

what if we don't have the softmax distribution and we have the predicted label instead, how can we compute the loss? Following the example above, 2 is given for prediction instead of y.
","['pytorch', 'sequence-modeling', 'loss']",
Is the optimal policy the one with the highest accumulative reward (Q-Learning vs SARSA)?,"
I was looking at the following diagram,

The reward obtained with SARSA is higher. However, the path that Q learning chooses is eventually the optimal one, isn't it? Why is the SARSA reward higher if it is not choosing the best path? shouldn't the best path and the safe path both be the optimal one because the reward would be higher?
","['reinforcement-learning', 'q-learning', 'sarsa']","It is important to note that the graph shows reward received during training. This includes rewards due to exploratory moves, which sometimes involve the agent falling off the cliff, even if it has already established that will lead to a large penalty. Q-learning does this more often than SARSA because Q learning targets learning values of the optimal greedy policy, whilst SARSA targets learning the values of the approximately optimal $\epsilon$-greedy policy. The cliff walking setup is designed to make these policies different.The graph shows that during training, SARSA performs better at the task than Q learning. This may be an important consideration if mistakes during training have real expense (e.g. someone has to keep picking the agent robot off the floor whenever it falls off the cliff).If you stopped training after episode 500 (assuming both agents had converged to accurate enough action value tables at that point), and ran both agents with the greedy policy based on their action values, then Q learning would score -13 per episode, and SARSA would be worse at -17 per episode. Both would perform better than during training, but Q learning would have the best trained policy.To make SARSA and Q-learning equivalent in the long term, you would need to decay the exploration policy parameter $\epsilon$. If you did this during the training process, at a slow enough rate, ending with no exploration, then two approaches would converge to the same optimal policy and same reward per episode (of -13)."
"Given embedding vector A and vector B, how to find top k embedding vectors such that they are similar to vector A and dissimilar to vector B","
Which would be better approach for getting top k embedding vectors such that they are similar to embedding vector A and dissimilar to vector B.
Approach 1:

calculate f(V) = cosine_similarity(A,V) - cosine_similarity(B,V) for each vector V
sort vectors by f(V) value in descending order
take first k of them.

Approach 2:

calculate
f(V) = cosine_similarity(A,V) , g(V) = cosine_similarity(B,V)
for each vector V
sort vectors by f(V) value in descending order
take first k of them
sort selected k vectors by g(V) in ascending order.

Approach 3:

calculate f(V) = cosine_similarity((A - B),V) for each vector V
sort vectors by f(V) value in descending order
take first k of them.

Also, suggest better approach if you have other than above two.
Note: embedding vector was calculated using word2vec algorithm
","['machine-learning', 'word-embedding', 'word2vec', 'cosine-similarity']",
What is the difference between Mean Teacher and Knowledge Distillation?,"
I recently read two papers:

BYOL Bootstrap your own latent: A new approach to self-supervised Learning



DINO Emerging Properties in Self-Supervised Vision Transformers.


I am confused about the terms Mean Teacher in BYOL and Knowledge Distillation in DINO.
Is KD the same as MT but using the cross-entropy loss instead of mean square error (since MT has preditor head while KD only has softmax head)?
","['deep-learning', 'comparison', 'vision-transformer', 'contrastive-learning']",
"What is meant by ""lateral connection"" in the context of neural networks?","
A class of CNN is popular due to the implementation of residual connections.
We can use both terms ""residual connections"" and ""skip connections"" interchangeably as they refer to the same.

Residual connections are the same thing as 'skip connections'. They
are used to allow gradients to flow through a network directly,
without passing through non-linear activation functions. Non-linear
activation functions, by nature of being non-linear, cause the
gradients to explode or vanish (depending on the weights).

While studying some models that are used in codes of some deep learning projects, I came across the word ""lateral connections"".
I guess that lateral connections stand for the connections that are not present in the traditional feed-forward neural networks of any kind. Lateral connections may be from any layer to any other layer.
Am I true? If not, when can I call a connection lateral?
","['neural-networks', 'terminology']","Lateral connection in NN simply means that units in a hidden layer are connected with one another. Suppose we have a hidden layer Hi which has 10 from (1,2,....10) then lateral connection implies that unit 1 may be connected to unit 2.So now the activation of unit 2 is not only dependent on incoming inputs but also on unit 1. The weight assigned to a connection between unit 2 and unit 1 needs to be learned as part of backprop.Please refer to the below image for more details :Lateral connections in a feed-forward architecture. Inputs and hidden layer neurons are fully connected as are hidden layer and output layer neurons. Neuron j in the hidden layer also receives the net input of neuron (j - 1) in the hidden layer through a lateral connectionRef: https://www.semanticscholar.org/paper/On-lateral-connections-in-feed-forward-neural-Kothari-Agyepong/167d7c20dcf37fba0388b0365d9bd42ba8b2ecde/figure/0Lateral Connection can occur between units in layers and can be applied to feed forward network"
Are these book example CNN results realistic?,"
I've been following a deep learning book and the current section I'm on is about convolutional neural networks. The author presents some code to create a basic CNN with about 1 million parameters, which he manages to train to 99.2% accuracy within 12 epochs on the full MNIST dataset.
His output looks like this:
Train on 60000 samples, validate on 10000 samples 
Epoch 1/12-loss:0.2800 acc:0.9147 val_loss:0.0624 val_acc:0.9794 
Epoch 2/12-loss:0.1003 acc:0.9695 val_loss:0.0422 val_acc:0.9854 
Epoch 3/12-loss:0.0697 acc:0.9789 val_loss:0.0356 val_acc:0.9880 
Epoch 4/12-loss:0.0573 acc:0.9827 val_loss:0.0282 val_acc:0.9910 
Epoch 5/12-loss:0.0478 acc:0.9854 val_loss:0.0311 val_acc:0.9901 
Epoch 6/12-loss:0.0419 acc:0.9871 val_loss:0.0279 val_acc:0.9908 
Epoch 7/12-loss:0.0397 acc:0.9883 val_loss:0.0250 val_acc:0.9914 
Epoch 8/12-loss:0.0344 acc:0.9891 val_loss:0.0288 val_acc:0.9910 
Epoch 9/12-loss:0.0329 acc:0.9895 val_loss:0.0273 val_acc:0.9916 
Epoch 10/12-loss:0.0305 acc:0.9909 val_loss:0.0296 val_acc:0.9904 
Epoch 11/12-loss:0.0291 acc:0.9911 val_loss:0.0275 val_acc:0.9920 
Epoch 12/12-loss:0.0274 acc:0.9916 val_loss:0.0245 val_acc:0.9916 
Test loss:
0.02452171179684301 Test accuracy: 0.9916

Using this code.
Running that same code on my machine, after 12 epochs, I'm barely at 0.6 accuracy. I did have to modify a couple function calls as keras has changed a couple things since the book came out. I'm going crazy trying to figure out why his is so quick. He presents the results as ""this is what should happen when the code is run."" I understand it had already went through almost 500 gradient descent steps by the time epoch 1 is complete, but is that really enough to reach 98% accuracy right off the bat??
","['convolutional-neural-networks', 'tensorflow', 'python', 'keras']","I was able to run the code without ""any"" modifications on Tensorflow 2.4.0, just had to replace the imports:->Output:Running it 10 times (it took about 30 seconds / model on 1080 Ti):So clearly these are very different results than the author got, but also significantly higher than what You got. He might have used different parameters with the Adadelta optimizer, or a different version of it.Usually I've had the best results with Adam (default parameters, learning rate = 0.001). I tested running it 5 times with these results:Adam is 3 years newer than Adadelta (2014 vs. 2011)."
Why is old/off-policy data harmful to on-policy/online RL? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I ask because if RL is indeed an MDP, then there should be absolutely no problem with training an agent on any available episode roll-out data, right? Because an MDP implies for any state S, the optimal action to take is entirely dependent on the state.
So while I can understand the argument from prioritized experience replay that some data is more valuable for learning than others. I cannot understand how old data would be harmful.
Further Clarification:
The emphasis on an MDP is perhaps misleading. I don't mean to imply that optimal control should be dependent on previous states necessarily... Similarly, the emphasis on off-policy & on-policy is perhaps better placed on online vs offline RL.
Performance Gap:

This [offline RL] setting is harder as it removes the ability for agents to explore the environment and collect additional feedback.

Consider a simple example: with Q-table 'learning'. It seems to me that Online-RL is analogous to deleting perfectly valid entries from the Q-table in order to reap better performance. This seems to me like a paradox and that's what confuses me.
","['reinforcement-learning', 'deep-rl']",
Can CNNs detect image similarity?,"
I have been running some experiments to see whether a CNN can detect whether two images are the same. However, I can't seem to make it work. I am wondering whether CNNs are not able to do what I am trying, or whether I am making mistakes.
This is the setup: I take two 128x128 images and append them to be 128x256. These two images are either exactly the same:

or different:

I have also experimented with a black bar in the middle. The entire appended 128x256 image(thus containing two images) is fed into a CNN with the following setup:

Depending on the dropout usage, the model is able to learn training data between 80-100% accuracy. But the validation accuracy never gets above around 65%.
I do not understand why the model is not able to get to 100% very easily, as the input images are either EXACTLY the same, or very different. Seems like a trivial task to me. I would assume that the convolutional kernels would create exactly the same feature representations of the images and then feed that into the dense layers.
ps. I know there are better ways to do this but I wanted to experiment with this.
","['convolutional-neural-networks', 'similarity']",
Are there any other language models besides GPT-3 that can be used to create chatbots with a specific identity and environment?,"
I've recently been looking into language models but ran into a small question out of curiosity. For some language models like GPT-3, it is possible to generate dialogue, with the model basing its answers on the initial input (which can be anything). This makes it possible to make a ""bot"" which a user can chat with, that identifies itself a certain way (ex. gender and job), has certain mannerisms (ex. polite or angry) and knows things about a custom world (ex. The bot is a cashier in a certain store, located in a certain city).
Newer and better models for open-domain dialogue such as Facebook Blenderbot do not seem to have this possibility, it chats with the user but does not have to setup a certain context. Are there any other language models that can do this, and might perform better than GPT-3?
","['open-ai', 'gpt-3', 'dialogue-systems']",You may want to look at Chirpy Cardinal from Stanford. It doesn't provide mannerisms out of the box but the response generators can be configured to give each instance its own character.https://stanfordnlp.github.io/chirpycardinal/
"Is the phrase ""Feature Pyramid Network"" refer to CNN only?","
""Feature Pyramid Network"" is a network that is used for feature extraction. Since it is pyramid in shape, it might be called so.
Consider the following excerpts from two different sources
#1

A Feature Pyramid Network, or FPN, is a feature extractor that takes a
single-scale image of an arbitrary size as input, and outputs
proportionally sized feature maps at multiple levels, in a fully
convolutional fashion. This process is independent of the backbone
convolutional architectures. It, therefore, acts as a generic solution
for building feature pyramids inside deep convolutional networks to be
used in tasks like object detection.

#2

FPN composes of a bottom-up and a top-down pathway. The bottom-up
pathway is the usual convolutional network for feature extraction. As
we go up, the spatial resolution decreases. With more high-level
structures detected, the semantic value for each layer increases.

Both the sources are giving a strong sense that the phrase ""Feature Pyramid Network"" must be used for CNN's only as it is used mainly intended for object detection. But the name and purpose suggest to me that any ANN that is pyramid in shape can be attributed as ""Feature Pyramid Network""  since any ANN tries to extract features only in the general sense.
Am I true? Can I use the phrase for any arbitrary ANN that is pyramid in shape or is it an exclusive term of CNNs in computer vision only?
","['convolutional-neural-networks', 'terminology', 'feature-extraction']",
Is there an approach where the output of one neural network is used to choose the next neural network?,"
I'd like to design a deep learning architecture in which the output of a primary neural network $M_{\theta}$ determines which neural network $N^i_{\alpha}$ in a set of secondary networks $\mathcal{N}$ to use next. For example, $M_{\theta}$ could be a multiclass classifier, where the predicted class determines $N^i_{\alpha}$. The networks may have different dimensions and activation functions. Is there a name for this type of architecture?
","['neural-networks', 'algorithm-request', 'model-request', 'ensemble-learning', 'hierarchical-rl']",
"How to use Actor-Critic RL with a categorical, state-dependent action space?","
I have a problem where the agent is given an embedding vector to represent the state. Then it is also given a set of possible actions in the environment, let's say that the actions are each represented by a unique text string. The strings don't necessarily follow any natural language rules, though they should contain information about the action.
How does someone use the actor-critic algorithm here?
It is obvious for Q-learning. You just use the $Q(s,a)$ function on each state-action pair. But the actor-network must choose an action directly. And implementations I've seen do this use a categorical output (of all possible actions, which is intractable), or a multi-variate normal distribution of some continuous representation of actions.
I'm not really sure how this applies in this context. And I've considered building a 'fake' actor network out of a Q-network module that just applies the Q-function to each possible action & takes the best action, but I'm not sure if there are some theoretical problems with this which is why the authors of actor-critic avoided the use of a Q-function?
","['reinforcement-learning', 'q-learning', 'actor-critic-methods']",
"Is there a way to use AI to compare thousands of files and detect the ones containing ""unusual"" content?","
Is there a way to use python and AI to compare thousands of files and detect the ones containing ""unusual"" content?
Those files are supposed to have ""homogeneous"" configuration types with different parameters, but sometimes some of them might contain ""illegal"" types of configurations. The issue is that there are too many ways to do homogenous and illegal configurations, so it is not possible to statically define them.
Here is one of too many bloc of configuration considered ""correct"": (the meaning doesn't matter)
vlan XXXX
  name  XXX_NAME

interface vxlan 1
  vxlan vlan XXX vni YYYY

router bgp YYYY
   vlan XXXX
      rd A.B.C.D:YYYY
      route-target both YYYY:YYYY
      redistribute learned

Thousands of files will contains such bloc of configuration (with different parameters, xxxx, yyyy...) But some might contain something else, example
Interface vlan AAAA
  ip address B.B.B.B/Z

Because it is in only a couple of files (""rare"") it is more likely that it is not authorized configuration.
I could use simple regular expression to match what is considered legal or illegal, but there is too many of such blocs.
I am wondering if AI could help with this issue?
","['machine-learning', 'training', 'python']",
How to calculate sensitivity and specificity given AUC score?,"
I couldn't find any relevant information on how to calculate sensitivity and specificity with AUC score. There is one picture that presents what I want, however I wasn't able to interpret it for my numbers.

My AUC results are different and would like to calculate sensitivity and specificity (as shown in the above picture).
","['roc-auc', 'sensitivity']","The specificity and sensitivity reported in that table are simply the x and y coordinates of the red dots in the ROC graphs.And the red dots are instead the points with maximum Youden's index, defined as:$J = sensitivity + specificity - 1$So basically J= y-x for each point of the ROC curve. Equivalently, since y=x is the identity line (or chance line when talking about ROC curves), you can define ""point with maximum Youden's index"" as the point with max height above the identity line.So to get your specificity and sensitivity as in that table, simply compute the Youden's index for each point, then choose the point with max J and its x and y values will be your specificity and sensitivity."
Can anyone please explain TFLite quantization part found in Netron neural network viewer?,"
I was checking tflite file in Netron. There I found the quantization formula in Netron as below:
quantization: 0.007709330413490534 * (q + 3)

I know the quantization formula is: quantized_value = (r/S + Z) Where, r = input, S = scale and Z = zero-point
I have used quantization-aware training before converting the model to the TFLite model. I have uploaded the TFlite Model file to Netron App website where I have found quantization as below image. Can anyone explain What do the q and 3 mean in Netron quantization?

","['machine-learning', 'tensorflow']",
"Is it possible that a deep neural network, with some variations, can be used for multiple tasks?","
I am asking this question on deep neural network architectures only. If you want to restrict the domain of tasks then you can choose computer vision for this question.
Suppose there is an architecture that performs well on a task. Is it possible can edit or append the first or last few layers and then it performs similarly well on the other task?
If yes, please provide me an example of such architecture that performs well on at least a couple of tasks.
","['deep-learning', 'architecture']",
Is it effective to use deep learning method to produce a 1D signal as output from a 2D image as input?,"
I have a 1D signal that will produce a 2D image after some image processing algorithm. Would it be possible and effective to use deep learning method to reproduce the 1D signal if I have the 2D image instead? If yes, what kind of deep learning neural network will be effective in this case?
In my application, I am looking at the noise images of the 2D synthetic aperture radar (SAR) images. Each image should look like random noise image (eg. tv static noise). I have the image processing algorithm that enables me to convert a 1D (or even a 2D) complex signal into that noise image. I am curious if it is possible for me to do the reverse process using deep learning instead because reversing the image processing algorithm is a complicated task. For dataset wise, I will randomly generate the 1D (or 2D) complex signals as the ground truths and the noise image as the training data. With the image processing algorithm, there should be no issue in generating the datasets.
Is there any relevant research / Github working on this method?
","['deep-learning', 'image-processing', 'algorithm-request', 'model-request', 'signal-processing']",
What method is better to use for a two-player reinforcement learning environment?,"
I want to create an RL agent for a mancala-type two-player game as my first actual project in the field. I've already completed the game itself and coded a minimax algorithm.
The question is: how should I proceed? Which is the better way: to create a custom OpenAI Gym environment and use stable baselines algorithms or create an AlphaZero-like Monte-Carlo Tree Search algorithm from scratch?
People here suggested that it is easier to create MCTS that use Gym, since the latter does not natively support multiplayer games. But I thought I could use my minimax algorithm and incorporate it into my custom environment, and since I have both the game and the minimax algorithm, it's easier to use Gym than MCTS.
Are there any pitfalls I should avoid?
","['reinforcement-learning', 'deep-rl', 'monte-carlo-tree-search', 'gym', 'stable-baselines']",
How many layers and neurons in a FFNN do I need to make it equivalent to a CNN?,"
I started to learn machine learning early, and I studied the convolutional neural network and its ability to understand images and how it helps to reduce the number of parameters that need to be tuned.
So, if I used CNN rather than a feed-forward network for the image classification problem, how many neurons and layers would I save or discard?
","['convolutional-neural-networks', 'comparison', 'weights', 'feedforward-neural-networks']","I don't have an exact answer to your question because I think your question is a bit ambiguous. It's like asking ""How many defenders do you need to replace your forwards?"": they do slightly different things, but a defender occasionally may be able to play as a forward. Anyway, given that I think this question arises because you don't understand how we can view a CNN as an FFNN or vice-versa, I will try to give you an idea of the relationship between the two and the number of parameters in a CNN and an FFNN.Before proceeding, read my answer here (and maybe the other answer there too), which will give you an idea of what we could consider a neuron in a CNN, and how we can compare CNNs to FFNNs. After that, you could read this answer to understand why we need CNNs and why we use them in the context of computer vision. From now on, I will assume that you have an idea of how CNNs work, what filters are, etc., but I will remind you of a few details.CNNs are specifically designed to deal with images, so CNNs have a more appropriate inductive bias than FFNNs for image processing.A specific filter/kernel in a CNN can be thought of as extracting the same type of feature from different parts of the image. At a high level, this makes sense, because the same object/feature (e.g. an eye) can appear in different parts of an image. If you are familiar with image processing techniques, this interpretation of the role of kernels in CNNs shouldn't be too strange.One of the main properties of CNNs, as explained in the linked answers, is that of weight sharing. This means that you apply the same filter (weights) to different parts of the image (this is basically what the convolution operation is doing). So, you try to extract the same ""features"" with the same kernel from different parts of the image.Now, imagine if you didn't do that, and you had to learn different filters for different parts of the image: we would need weights for all these different parts, so we would need a lot more weights.In the case of an FFNN, we can view the operation of a fully connected layer as a convolution with a kernel as big as the input image. More precisely, if you have $n$ neurons in layer $l$ and $m$ neurons in layer $l-1$, then you can view this FC layer as performing $n$ convolutions, one for each neuron in $l$, with a kernel with $m$ weights.At this point, you should start to see the relationship between fully connected layers and convolutional layers.So, what's the answer to your question? I don't know. It depends on what you mean by ""equivalent"". If by ""equivalent"" you mean they are the same hypothesis class, then that's a question to which I don't have an answer. However, both FC and CNNs are universal function approximators, so, in a way, they are equivalent, exactly how (which is your question), I don't know and this is probably an open research question. One thing we can say is that CNNs have a more appropriate inductive bias than FFNNs to deal with images."
What is the benefit of using a neural network instead of a look-up table in this case?,"
Assuming one has collected the 24 pairs of the input-output datasets for a target system:

One can create a simple lookup table to describe the input-output behavior and utilize this as a controller.
One can also train a DNN model to learn the relationship.
What is the benefit of using DNN in this case?
In my view, for a DNN, one does not have to store the whole dataset for the lookup table. If one gives a new input value that is not included in the training dataset, the trained DNN would perform better, since, in the case of the lookup table, the predicted output is just an extrapolated value from the previously known output.
Any other benefits that can justify using DNN?
","['neural-networks', 'deep-learning']","I think, that the answer depends on the actual setting and parameters of the problem.One needs to store the whole lookup table in the memory - say N points.The number of operations to extract elements depends on the way values are stored in the memory.In order to get a single value given the input $x$, you need to perform $O(\log N)$ operations using a binary search in the sorted array.If the data is represented as hast-table - you need only $O(1)$ operations - complexity of computing the hash-function.Given some dependence, you fit a neural network $\hat{f}(x)$, that approximates the function of interest (let us call it $f(x)$).
The approximation may not be exact, let us further assume, that some $\varepsilon$ is the allowed error, i.e:
$$
\Vert f(x) - \hat{f}(x)\Vert \leq \varepsilon \quad \forall x \in X
$$
For every value of $x$ inside the range of values of interest, approximation deviates at most by $\varepsilon$  from the true function.Depending on the complexity of the problem, this quality of approximation can be achieved by the simple architecture with few hidden units, or large network with large width or depth is required to perform an accurate approximation.In the setting, described by you, input and output are $1$-dimensional. Let MLP with the dimensions of hidden units $(h_0, h_1, h_2, \ldots h_{L})$, where $h_0 = h_{L+1} = 1$ and $L$ is the number of hidden layers, and the activation function $\sigma$ approximate the target dependence sufficiently well in the sense described above. NN is actually
$$
\hat{f}(x) = \sigma(\ldots \sigma (W_2 \sigma(W_1 x + b_1) + b_2))
$$
Here $W_i$ are the weights and $b_i$ are the biases.
Weigh matrices consist of $h_{i} h_{i+1}$  elements and biases of $h_{i+1}$ elements.Then the network would require the following amount of memory to store parameters:
$$
\sum_{i=0}^{L} (h_{i} h_{i+1} + h_{i+1})
$$
And the number of operations to compute output $y$ per single $x$ is:So, in order to decide, whether you need DNN model or not, you need to compare the computational complexity for the Lookup table and DNN. For some architecture, DNN may be more expensive, for others cheaper. It can be the case, that there is no DNN architecture, suitable for this task.For simple dependence and long timespan DNN can be helpful, whereas for small number of points and complicated dependence of $y$ on $x$ there is no point in trying to replace the Lookup table.For the case, presented in the picture seems, that there is no reason to use DNN since the hash-table will extract value very fast."
What exactly is data augmentation?,"
Data augmentation is useful in training. But, I am not sure when can a modification applied to data can be called data augmentation.
Suppose a technique is applied to the instances of a dataset and the size of the dataset does not increase but the instances change slightly, then can I call it a data augmentation technique?
For example, say padding, never increases the size of the dataset but adds some data to possibly every existing training sample. In other cases, such as rotating the images with an angle, then the number of instances increases in the dataset.
","['definitions', 'data-augmentation']","Data augmentation typically refers to the creation of new (training) data/instances (e.g. images) by e.g. modifying existing training/instances data in order to avoid over-fitting and improve the performance (e.g. generalization, precision, recall, etc.) of the model.There are other techniques that attempt to mitigate over-fitting, for example, regularization techniques (like dropout), but data augmentation changes the data that you use rather than how you use it, or how you define and train/test a model.When you use data augmentation, you're assuming that more information can be extracted from the original dataset. For example, if you augment your training dataset by rotating existing images of objects, then you assume that this can be beneficial for your task (i.e. you expect your model to encounter rotated versions of the same object).Typical scenarios where you may need data augmentation are when you have small training or imbalanced datasets.Padding is not a data augmentation technique because you don't do it to avoid overfitting. Padding is a data preprocessing technique because you do it in order for your model to even be able to process your data.You could read A survey on Image Data Augmentation for Deep Learning (by Shorten & Khoshgoftaar, 2019, Journal of Big Data), which I have only skimmed through, but it looks like a good paper."
How to make NN distinguish between two types of functions (data)?,"
I have a neural network which is trying to predict two types of functions in a noisy setting. The input is an array, and the output is also an array. The two types of functions I am trying to predict are the following:

a linear function with negative slope
a significantly steeper exponential function.

Example inputs for each would look like

[10, 9, 8, 7]
[11, 1, 0.1, 0]

Sometimes my neural network predicts well, such as the following:
Linear
Other times my neural network completely messes up and predicts the exponential decay instead:
Poor Prediction on Linear
As you can imagine, this makes the final result a very poor one.
I'm using a NN architecture of NCF because I'm setting up from a matrix setting. That is, the arrays I have generated come from a noisy matrix setting of low rank.
Any suggestions to improve the result? When I only use one function (e.g. just linear or just exponential) I get superb results. However, mixing both those functions together randomly causes my neural network to perform poorly, as described in my question.
","['neural-networks', 'machine-learning', 'deep-learning', 'pytorch', 'collaborative-filtering']",How about dividing the problem?You can first train a classification model that predicts the type of function (linear or exponential).Then you can use your seperately trained nn depending on the classification output.P.S.I'm not sure why you would use a neural network for this problem. Fitting a linear/exponential function seems to be a relatively simple problem that can be achieved using traditional ML approaches.
Proper way to count environment steps / frames in distributed RL architecture for algorithms like CLEAR or LASER => modified impala with replay,"
In classical - on-policy - vtrace/Impala algorithm env_steps are incremented every training iteration
like this : env_steps += size_of_batch * unroll_length
This works since every trajectory generated by workers is used for training only once in classical Impala.
However when we add replay_buffer to vtrace (like in CLEAR or LASER) we also start to learn from off-policy data therefore some trajectories (from replay) could be sampled more than once.
My question is - does the env_step_counting stay the same even when some trajectories from batch are from replay and some directly from workers (on-policy) ?
Or do i count steps only on those trajectories that are on-policy => not from replay
like : env_steps += len(batch[""on_policy_samples""]) * unroll_length
And if so what happens when i decide to use only samples from replay to train - how do i count env_steps then ? Using some kind of flag to indicate if specific trajectory was sampled already and leave it from counting ?
At the end of the day what i need to know is that should i count all steps of trajectories generated by workers (true environment steps) OR all steps of trajectories processed during training (training steps ?). - I need to know this in order to be able to compare results with other implementations.
Have been digging through the research papers for some time now and i haven`t been able to find what is the proper way to do it. Any help or advice would be much appreciated.
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'off-policy-methods']",
Is my intuition about RNN wrong?,"
Until today, my intuition about RNN (LSTM/GRU) was that this is some kind of NN that can remember previous inputs.
Consider a task where you need to predict 0 if the previous input was 1. For example: x=[0,1,0,0], y=[1,1,0,1]. We train the model online, i.e. the number of timesteps the RNN sees at a time is equal to one (in Keras you can make an LSTM/GRU cell stateful so that it passes state (memory about previous inputs?) between batches). And the problem is that, in my tests, RNN cells can not cope with this simple task.
I suspect that my intuition about RNN is built wrong. I would appreciate someone could help me with that.
Test script:
import numpy as np
import tensorflow as tf
from random import randrange

batch_size = 1
timesteps = 1
size = 1
units = 16
shape = (timesteps, size)

model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape = shape, batch_size = batch_size),
  tf.keras.layers.GRU(
    units,
    stateful = True,
    return_sequences = True
  )
])

model.compile(optimizer='adam',
              loss='mean_squared_error')

print(model.summary())

SAME = False

i = 0
prevIsBlack = False
x_batch, y_batch = [], []
x_steps, y_steps = [], []

while True:
  i += 1

  isBlack = randrange(4) == 2

  if SAME: prevIsBlack = isBlack 

  x_train = np.ones(size)   if isBlack else np.zeros(size)
  y_train = np.zeros(units) if prevIsBlack else np.ones(units)

  if not SAME: prevIsBlack = isBlack 

  x_steps.append(x_train)
  y_steps.append(y_train)

  if len(x_steps) < timesteps:
    continue

  x_batch.append(np.stack(x_steps))
  y_batch.append(np.stack(y_steps))

  x_steps, y_steps = [], []

  if len(x_batch) < batch_size:
    continue

  res = model.fit(np.stack(x_batch), np.stack(y_batch), 
    epochs = 1, 
    shuffle = False,
    batch_size = batch_size)

  print(res.history[""loss""])

  x_batch = []
  y_batch = []

","['recurrent-neural-networks', 'long-short-term-memory', 'gated-recurrent-unit', 'intuition']",
Are there any algorithms (even backtracking variations) that solve the sudoku in a way more similar to this approach?,"
I looked a bit online for Sudoku solvers and it seems like all the answers I found involve a backtracking algorithm.
However, this is not how humans (at least not me) solve Sudoku. We don't place in the first empty square, the first digit that works and go on until we solve it or hit a dead end. Usually, we try to find an empty square (which can be anywhere on the board) where there is a unique solution and then we repeat this. We might have to do some backtracking-like approach if we are out of squares with unique solutions, but that is not the main part of our approach.
Are there any algorithms (even backtracking variations) that solve the sudoku in a way more similar to this approach?
","['neural-networks', 'algorithm-request', 'sudoku']",
Is my calculation of the partial derivative of the cost function with respect to a single weight in the first layer correct?,"
I'm trying to understand the chain rule of backpropagation.
This is what I understood. Is it correct?

$$ \frac{\partial E }{ \partial w} = \sum_{i} \frac{\partial E }{ \partial a_i^{(l)} } (\sum_{j} \frac{\partial a_i^{(l)} }{ \partial a_j^{(l-1)}  }(\sum_{k} \frac{\partial a_j^{(l-1)} }{ \partial a_k^{(l-2)}} \frac{\partial a_k^{(l-2)} }{ \partial z_k^{(l-2)}}  \frac{\partial z_k^{(l-2)} }{ \partial w}))$$

$ a_i^{(l)}$ is the activation of the neuron $i$ in the $l$th layer
$ z_i^{(l)} $ is the sum of multiplication of weights and previous activations
$E$ is the error
$w$ is the weight

","['neural-networks', 'deep-learning', 'backpropagation', 'calculus']",
"What does ""These designs employ skip connections to avoid a situation where the shortest path between time steps increases"" mean?","

Less popular alternatives include adding layers to the connections from input to the hidden state, between hidden states, or from the hidden state to the output. These designs employ skip connections to avoid a situation where the shortest path between time steps increases and training becomes more difficult.

This is from page 597 of the book Machine Learning for Algorithmic Trading. And this chapter is about RNN.
I don't quite understand what does 'These designs employ skip connections to avoid a situation where the shortest path between time steps increases' means. I haven't seen any materials about how to use skip connections in RNN.
","['recurrent-neural-networks', 'time-series', 'skip-connections']",
Augmented an Image with other data when training CNN,"
In the typical RL/MDP framework, I have offline data of $(s,a,r,s')$ of expert Atari gameplay.
I'm looking to train a CNN to predict $r$ based on $(s, a)$.
The states are represented by a $4 \times 84 \times 84$ image of the Atari screen, where 4 represents 4 sequential frames, and $84 \times 84$ is the size of the image. The action is an integer from 0 to 3.
I'm not sure how best to merge these two inputs $(s, a)$ together. How should I incorporate the action into the CNN?
","['reinforcement-learning', 'convolutional-neural-networks', 'inverse-rl']",
How do sine and cosine transforms help in extracting frequencies in time series forecasting models?,"
I'm trying to learn how time series forecasting models work and while reading a tutorial off the TensorFlow website I came across these algorithms. I don't quite understand what the article means by ""time signals"" and how do sine and cosine functions help accomplish them. Can anyone please explain?
Here's a link to the tutorial
The following code was provided along with the caption
""the time in seconds is not a useful model input. Being weather data, it has clear daily and yearly periodicity. There are many ways you could deal with periodicity.
You can get usable signals by using sine and cosine transforms to clear ""Time of day"" and ""Time of year"" signals:""
day = 24*60*60
year = (365.2425)*day

df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))
df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))
df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))
df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))

","['tensorflow', 'time-series']",
Order of operations on sparse recurrent network alters the output. How to deal with it?,"
I'm working on an implementation of NEAT, which evolves neural networks with small and sparse topologies.
Evaluating a sparse and possibly recurrent network requires a different approach than the matrix operations of dense networks, and I'm trying to wrap my head around the order in which nodes should be evaluated.
I've set up a simple example:

Every node (except inputs) starts at 0. (t0)
Every weight is 1. There's no activation function or biases.


Assuming that the algorithm has the same intuition we humans do: evaluate the nodes connected to the input, then the ""aggregation"" node, then the output.
How should it decide whether to evaluate node [3] before node [4] or vice-versa?
By starting (t0) at [3], the value of [4] is 0. And vice-versa.
The side-effect of this behavior appears when comparing two networks. Say you have the network above, and a copy of it where [3] and [4] are inverted. I feel like both should return the same result for the same input in order for evolution to work properly.
Any thoughts?
","['recurrent-neural-networks', 'neat', 'neuroevolution']",
GAN performance starts to get worse as training continues,"
I'm currently trying to train a GAN to recreate similar images from a dataset. The dataset is using the Eiffel Tower Pictures from Googles Quick Draw dataset. The images aren't very large (only 12x12 pixels) and are all black and white.
The performance increases at an expected rate initially however after a certain point the quality begins to go down despite the cost from both generator and discriminator networks seeming to stay at a steady value as expected.



I've tried changing the learning rate and other hyper parameters however they all end up with the same result of the network eventually getting worse.
My learning rate is currently 0.01 which I'm aware is quite high compared to what other people are using but anything lower takes too long to train and the results don't seem any better even if I do give it long enough.
Any pointers on what could be causing this or the specific name of this problem if it's a common issue with GANs would be appreciated.
Thanks
","['neural-networks', 'generative-adversarial-networks', 'performance', 'learning-rate']",
What is the difference between the $Q_a$ calculated to update delta and those to select next action in the exploitation phase?,"
As the title suggests, I have a doubt about the computation of the $Q_a$ used to update the delta and the $Q_a$ used to select the next action in the exploitation phase, as shown below (source of pseudocode in Figure 8.9).

In both for loops enumerated in the image with 1 and 2, the $Q_a$ are calculated considering the new state, but while in 1 for each action the active features $F_a$ are calculated, in 2 they aren't. So what are the active features to consider in 2? Are they the same as those in 1? In that case, I could avoid recalculating the $Q_a$ by storing those calculated in 1.
","['reinforcement-learning', 'q-learning', 'function-approximation', 'features']",
"What are the ""per image"" annotations that are generally used for image datasets in AI?","
Computer vision is highly benefited by AI algorithms.  Image data is abundantly available. There are different varieties of tasks such as image classification, prediction, segmentation, generation, etc.
Although the collection of the folder(s) of image(s) is mandatory, it may not be enough. Different types of annotations are used in datasets.  Annotations can be treated as some extra information related to each image that helps for the AI algorithm under consideration.
I want to know the kinds of annotations at the individual image level that are generally used. Although the necessity of a particular type of annotations depends on the task under consideration. I want to know the requirements for the contemporary prevalent tasks including classification, prediction, segmentation, and generation. You are encouraged to provide for more tasks if you are aware.
I know the following types of annotations:

Bounding box(es)
Label

What can be the other kinds of annotations used for images in image datasets?
","['datasets', 'image-processing', 'data-labelling']",
How to derive the dual function step by step in relative entropy policy search (REPS)?,"
TL:DR, (Why) is one of the terms in the expectation not derived properly?
Relative entropy policy search or REPS is used to optimize a policy in an MDP. The update step is limited in the policy space (?) by the KL-divergence metric to stabilize the update. Based on the KL-divergence constraints, and some constraints about the definition of a policy, we can derive its Lagrangian, and its dual optimization problem afterwards. And lastly, we find the appropriate update step (delta) by solving the dual problem.
However, I think we can also use it to find multiple optimal solutions in an optimization problem, just like (CMA)-evolutionary strategy algorithm.
So, based on the original paper and a section of REPS in this paper, I'm trying to derive the dual problem.
Suppose that we're finding set of solutions represented as a parametrized distribution $\pi(x|\theta)$ that maximizes $H(x)$. Suppose that the last parameters we came up with is denoted as $\hat{\theta}$, we find the optimal parameters $\theta$ by:

max $\int_x H(x)\pi(x|\theta) dx$


s.t.  $\int_x \pi(x|\theta) dx = 1$


$D_\text{KL}\left(\pi(.|\theta) || \pi(.|\hat{\theta})\right) \leq \epsilon $


with $D_\text{KL}\left(\pi(.|\theta) || \pi(.|\hat{\theta})\right) = \int_x \pi(x|\theta)\log\frac{\pi(x|\theta)}{\pi(x|\hat{\theta})}$

Based on the equations above, we can write the Lagrangian as follows:

$L(\theta, \lambda, \eta) = \int_x H(x)\pi(x|\theta) dx + 
\lambda(1-\int_x \pi(x|\theta) dx) + 
\eta(\epsilon-\int_x \pi(x|\theta)\log\frac{\pi(x|\theta)}{\pi(x|\hat{\theta})})$

Now, we can see that the term $\lambda(1-\int_x \pi(x|\theta) dx)$ is $0$, right? But here, it was not cancelled out. So, following the flow based on the two papers, We can simplify the Lagrangian by treating the integral wrt to $x$ as an expectation.

$L(\theta, \lambda, \eta) = \lambda + \eta\epsilon + \underset{\pi(x|\theta)}{\mathbb{E}}\left[H(x) -\lambda -\eta \log\frac{\pi(x|\theta)}{\pi(x|\hat{\theta})} \right]$

We will find the optimal $\pi(x|\theta)$ by solving $\frac{\partial L}{\partial \pi(x|\theta)} = 0$. Now, I got confused starting from this step.
If I mindlessly copy/follow the notations from here, the derivative of $L$ wrt the policy parametrized by $\theta$ is:

$\frac{\partial L}{\partial \pi(x|\theta)} = H(x) - \lambda - \eta \log\frac{\pi(x|\theta)}{\pi(x|\hat{\theta}}$

Where's the integral wrt $x$ goes? Is it because they are all multiplied by $\pi(x|\theta)$, so that it can cancel the integral/the expectation? If so, then why the derivative of the KL term in the expectation derived into this $\eta \log\frac{\pi(x|\theta)}{\pi(x|\hat{\theta})}$? Isn't the $\pi(x|\theta)$ in the log will derive something more?
","['reinforcement-learning', 'math', 'policy-based-methods']","What you did is incorrect and that's not what authors got either (if you're refering to the equation above equation (5) in paper ""Non-parametric Policy Search with Limited Information Loss"")What you need here is the derivative of a functional. Functional has a general form
\begin{equation}
J(f) = \int_x L(x, f(x), f'(x), \ldots, f^{(n)}(x)) dx
\end{equation}Derivative with respect to $f$ is
\begin{equation}
\frac{\partial J}{\partial f} =  \frac{\partial L}{\partial f} - \frac{d}{dx}\frac{\partial L}{\partial f'} + \ldots
\end{equation}Since your functional (expectation) is only dependant on $f$ then it simplifies to $\frac{\partial J}{\partial f} =  \frac{\partial L}{\partial f}$So what you have for example for the first term is
\begin{equation}
\frac{\partial \int_x H(x) \pi(x|\theta) dx}{\partial \pi(x|\theta)} = \frac{\partial H(x) \pi(x|\theta)}{\partial \pi(x|\theta)} = H(x)
\end{equation}The more interesting functional is the one with term $\pi(x|\theta)\log(\frac{\pi(x|\theta)}{\pi(x|\hat \theta)})$.
Integral with respect to $\pi(x|\theta)$ is
\begin{equation}
\log(\frac{\pi(x|\theta)}{\pi(x|\hat \theta)}) + 1
\end{equation}So in the end you get
\begin{equation}
\frac{\partial L}{\partial \pi(x|\theta)} = H(x) - \lambda - \eta \log\frac{\pi(x|\theta)}{\pi(x|\hat{\theta})} - \eta
\end{equation}"
Watkins' Q(λ) with function approximation: why is gradient not considered when updating eligibility traces for the exploitation phase?,"
I'm implementing the Watkins' Q(λ) algorithm with function approximation (in 2nd edition of Sutton & Barto).
I am very confused about updating the eligibility traces because, at the beginning of chapter 9.3 ""Control with Function Approximation"", they are updated considering the gradient: $ e_t = \gamma \lambda e_{t-1} + \nabla \widehat{q}(S_t, A_t, w_t) $, as shown below.
                                      
Nevertheless, in Figure 9.9, for the exploitation phase the eligibility traces are updated without the gradient: $ e = \gamma \lambda e $.
                                      
Furthermore, by googling, I found that the gradient is simplified with the value of the i-th feature: $ \nabla \widehat{q}(S_t, A_t, w_t) = f_i(S_t, A_t)$.
I thought that, in Figure 9.9, the gradient is not considered because in the next step the eligibility traces are increased by 1 for the active features. So, the +1 can be seen as the value of the gradient, as I found on google, being binary features. But I'm not sure.
So, what is (and why) the right rule to update the eligibility traces?
","['reinforcement-learning', 'q-learning', 'gradient-descent', 'function-approximation', 'eligibility-traces']",
Is there a paper/article on contextual $\epsilon$-greedy algorithm?,"
I am reading the paper A Contextual-Bandit Approach to Personalized News Article Recommendation, where it refers to $\epsilon$-greedy (disjoint) algorithm. I suspect, that it is just a version of a K-armed bandit with regressors that estimate the average reward for an arm. However, I cannot find the description of this algorithm in the literature (papers, books, or other resources)
","['reinforcement-learning', 'reference-request', 'multi-armed-bandits']",
NEAT: How to properly handle Node IDs and avoid Competing Conventions?,"
I'm working on yet another NEAT implementation for a personal project, and I feel like I'm missing something about the proposed solution to the Competing Conventions problem.
Here's what I'm assuming:

Each new connection gene yields a new innovation number.


The new connection gene created in the first mutation is assigned the number 7, and the two new connection genes added during the new node mutation are assigned the numbers 8 and 9.


If a connection from X to Y appears more than once at the same generation, it receives the same innovation number.


However,  by  keeping  a  list  of  the  innovations  that  occurred  in  the  current  generation,  it is possible to ensure that when the same structure arises more than once through independent mutations in the same generation, each identical mutation is assigned the same innovation number.


Every new node receives a globally new id. (I have no source for this. Maybe the problem is here)
Equal fitness is assumed at the example below, so all genes are randomly inherited (on this case the probability is 100%, for demonstration purposes).


In this case, equal fitnesses are assumed on the example below, so the disjoint and excess genes are also inherited randomly.

So we have two genomes after two generations, which suffered the exact same mutations at each generation: add_connection(0,2) at gen 1, add_node(1) at gen 2.

Even though the topology is exactly the same for both, the genomes don't share innovation numbers, so the crossover yields a more complex topology.
It seems to me that using global ids for nodes breaks the historical tracking of connections, which blows up the innovation counter pretty fast (around 1400 innovations in 100 gens for 100 individuals), and yields big, non-functional networks.
What am I missing?
","['neat', 'neuroevolution']",
What inherent quality of a function makes it treated as either loss or evaluation metric?,"
A neural network model needs a loss function for training. The neural network needs to minimize the loss function.
A neural network is evaluated after training using a metric. The neural network needs to either minimize or maximize the metric depending on the context.
Suppose $L$ is the loss function used and $M$ is the metric/evaluation function. Assume the metric needs to be minimized and is calculated based on the output of the neural network. We can use $L+M$ as the loss function. It looks to me that it may be up to the choice of the designer to use certain function $f$ for either $L$ or $M$ as they try to quantify how good/bad the model is working.
But, in the literature, if we observe, there are some fixed loss functions and fixed metrics for evaluation depending on the underlying task.
With this context, what is the inherent quality of the function that makes it treated as either loss or evaluation metric?
","['neural-networks', 'objective-functions', 'metric', 'evaluation-functions']",
Why is the cross-entropy a cost function?,"
The question looks foolish, but I think cross-entropy is somewhat weird as a cost function.
As a cost function for linear regression, the mean square error $ \sum_{i=1}^{n} (y_i - (ax_i+b)) ^2$ seems quite reasonable, because it literally/directly measures the error between real value and predicted value.
However, in the case of the cross-entropy, I do not understand what it is.
For multi-class classification, for example, with 3 classes, the true target is $[ 0, 0, 1 ]$, while the output of the model is $[ 0.2, 0.3, 0.5 ]$ (maybe with a softmax activation at the last layer). So, the error of it is: $C(x) = -(0*log(0.2) + 0*log(0.3) + 1*log(0.5))$.
It looks... I don't know, why is it an ""error?"" How can it be updated with backpropagation?
Also, what is the objective of it? Maybe optimization, so maybe minimizing error? Then what happens?
","['neural-networks', 'objective-functions', 'backpropagation', 'linear-regression', 'cross-entropy']","Optimizing the cross-entropy is equivalent to optimizing the log-likelihood of the parameters given the data, $\ell(\theta)$, which is what we want, i.e. find the parameters that most likely generated the data.So, the likelihood is defined as $$\mathcal{L}(\theta) = P(y \mid x; \theta),$$
i.e. a function of the parameters $\theta$.The log-likelihood is just the logarithm of the likelihood$$\ell(\theta) = \log \mathcal{L}(\theta)$$To understand why we take the logarithm, read this answer.Now, for a fixed $\hat{\theta}$, $\mathcal{L}(\hat{\theta}) = P(y \mid x; \hat{\theta})$ is a conditional probability distribution.For simplicity, let's assume that $\mathcal{L}(\hat{\theta}) = P(y \mid x; \hat{\theta})$ is a (conditional) Bernoulli distribution, which is defined by a single parameter $\hat{p}$, whose probability mass function (pmf) is defined as$$\hat{p}^y (1-\hat{p})^{1-y},$$where $y = \{0, 1\}$.Now, in the context of neural networks, we use a neural network to output $\hat{p}$, i.e. an estimate of the parameter of the Bernoulli, which we plug into the cross-entropy loss function. So, if $f_\theta: \mathcal{X} \rightarrow [0, 1]$ is your neural network, then $f_{\hat{\theta}}(x) = \hat{p}$, given the input-output pair $(x, y)$.So, the Bernoulli pmf for a given pair $(x, y) \in \mathcal{D}$ can be written as\begin{align}
P(y \mid x; \hat{\theta})
&= \hat{p}^y (1-\hat{p})^{1-y} \\
&= f_{\hat{\theta}}(x)^y (1-f_{\hat{\theta}}(x))^{1-y} 
\end{align}
So, without fixing the parameters, we can write the likelihood (which is not a probability distribution wrt $\theta$) as follows\begin{align}
\mathcal{L}(\theta) 
&= f_{\theta}(x)^y (1-f_{\theta}(x))^{1-y}  \\
\iff \ell(\theta) &= \log \left( f_{\theta}(x)^y (1-f_{\theta}(x))^{1-y}\right)  \\
&= \log f_{\theta}(x)^y + \log \left(1-f_{\theta}(x)\right)^{1-y} \\
&= y \log f_{\theta}(x) + (1-y)\log \left(1-f_{\theta}(x)\right) 
\end{align}which is our well-known binary cross-entropy for a single pair $(x, y)$ (note that only one of the addends above is not zero). For multiple pairs, you can apply the same reasoning, and you will see why we also use the log, i.e. it will allow you to simplify a few expressions in the derivation (specifically, multiplications will turn into sums, which is nice numerically).So, if we maximize $\mathcal{L}$, with respect to $\theta$ (e.g. the parameters of your neural network), then we will find an estimate $\hat{\theta}$, that produces the parameter $\hat{p}$, such that this parameter can be used to construct/define the Bernoulli distribution that most likely (hence the name likelihood) produced our given dataset $\mathcal{D}$.This reasoning also applies to the case where $P$ is categorical or even Gaussian distribution. So, actually, when you're minimizing the MSE, you're actually also optimizing the log-likelihood. It just turns out that the expression is different because Gaussians are not Bernoullis.I will not talk about the back-propagation, but it's just the chain rule. If the log-likelihood is differentiable, then you can use it in the context of back-propagation and GD. You can ask another question, if you're interested, but note that, in the general case, computing the partial derivatives can be a pain in the neck, so your question may be too broad. Find a very simple case.To conclude, don't dwell on the word error. Think in terms of probabilities, probability distributions, likelihoods, and why we can optimize likelihoods. Read my answer multiple times, if you didn't understand something."
How to normalize rewards in DQN?,"
I want to use a Deep Q-Network for a specific problem. My immediate rewards ($r_t = 0$) are all zeros. But my terminal reward is a large positive value $(r_T=100$). How could I normalize rewards to stabilize the training?  I think clipping rewards to be in range $[0,1]$ makes training harder because it just forces most values to be near zero.
",['reinforcement-learning'],
Should I represent my reinforcement learning as an episodic or continuous task?,"
I would like the community to help me understand if the following example would be better represented as episodic or continuous task, this will help me structure the problem and chose the right RL algorithm.
The agent start with an initial score x of let's say 100. The agent objective is to maximise it's score. There is no upper bound! Theoretically the agent can get a score up to infinity, and there is no termination based on the number of steps, therefore the agent could play forever. However, the score can't be negative and if the agent get to a score of zero, the episode should terminate and the environment reset. I am undecided what would be the best representation, because if the agent learns how to play, the episode would never terminate, and the agent would theoretically play forever. However if the score get to zero, there is no way for the agent to continue playing so the environment needs to reset. Thank you.
",['reinforcement-learning'],
Generating automatic sports commentary (NLG),"
I am trying to develop a ""simple"" announcer for sports segments that mainly consists of events like goals, fouls, substitutions, and many other events that could happen in many sports. The idea is that I already have key info like the player who does the action, the location in the court, the time that it takes place, and more extra info. I also have information like the sport being played and the type of event, so this task is purely focused on NLG.
The naïve idea that I had was to extract commentaries of soccer, which can be found on so many websites, and extract the key info of these commentaries that will act as the ground truth in the model for getting the input, i.e.,:
['football', 'goal', 'Bob', 'fourth minute'] -> Goal! that was a nice goal from Bob in the fourth minute of the match.
I would say that the first two words are being used for steering the model to generate sports according to phrases (a cricket player kicking the ball doesn't make sense).
The comments generated by the fine-tuned model on this input-output are acceptable.
The problem is that I have to build a dataset for many sports (or at least 2 with the same quality as soccer ones like in Flashscore) and I can't find any.
I have also been looking for Plug and Play methods to generate sentences.
What do you think? Is fine-tuning a must-do in this situation or can it be thought of in another way?
","['machine-learning', 'deep-learning', 'natural-language-processing']",
Why Acme is using own uniform initializer?,"
Why is Acme using own initializer for both tanh and ELU, when commonly used for tanh is Xavier and for ELU is He initializer? What mathematics is behind them?
Here is the code.
uniform_initializer = tf.initializers.VarianceScaling(
    distribution='uniform', mode='fan_out', scale=0.333)

","['reinforcement-learning', 'deep-rl', 'tensorflow', 'weights-initialization', 'deepmind']",
Why is training all layers at a time effective for a multi-layer autoencoder?,"
This training of all layers of a CNN simultaneously is standard practice today. It is found in every CNN (AlexNet (2012), VGG, Inception, GANs, etc) and even pre-CNN networks such as Le et al. 2012.
What is the advantage of training all the layers simultaneously? Wouldn't the later layers be learning from poor lower layers to start with, and have to re-learn to adapt? And why would there ever be an advantage for an autoencoder like Le et al. 2012 where there is no backpropagation to communicate from the later layers to the earlier layers?
I think the conventional answer is that the lower layers can actually learn to provide low-level features that support the layers above. An example of this is learning to detect a horizontal yellow-blue feature to detect the water line in a beach scene.
But couldn't the yellow-blue feature be found just as easily by training the lower layers first? This would be especially true of an autoencoder such as Le et al. 2012, which picks up on patterns in the training set without having ground truth-labels to group them.
Citations to experiments or theoretical work that directly answers this question would be appreciated!
This is a follow-on to an earlier question.
","['convolutional-neural-networks', 'autoencoders', 'multilayer-perceptrons']",
What is the time complexity for testing a stacked LSTM model?,"
In the data preparation phase, we have to divide the dataset into two parts: the training dataset and the test dataset.
I have seen this post regarding the time complexity for training a model.
However, I couldn't find any good source for the time complexity for testing a model, specifically, a stacked LSTM model (with 1 input, 3 layers, 4 LSTM units per LSTM layer, 1 output, sequence 18, and batch size 32 for MSE loss), based on the test set, e.g. the computation of the accuracy/loss on the test set given $N$ test examples.
Is there any source to check that?
","['long-short-term-memory', 'time-complexity', 'computational-complexity', 'testing']","The time complexity of an algorithm always depends on its implementation (e.g. searching in a red-black tree has a different time complexity than searching in an unbalanced binary search tree). This also applies to the case of computing the time complexity of the algorithm that tests a neural network with multiple LSTM layers, so one may need to assume how an algorithm or data structure is implemented in order to provide the right time complexity.Now, note that testing a neural network typically means performing a forward pass. So, basically, if you have the time complexity for the forward pass, you are almost done. In this answer, I provide details of how you can compute the time complexity of the forward pass of a feedforward neural network with no recurrent layers, so you can read it in order to have an idea of how you might do this in general.Now, in the case of a neural network with LSTM layers, we also need to take into account the recurrent connections in the LSTM units. This would also be the case for the vanilla RNN units, which are simpler than LSTM units, so the time complexity is smaller.I will assume that the equations used to perform the forward pass of an LSTM layer are the ones given in the PyTorch documentation$$
\begin{aligned}
i_{t} &=\sigma\left(W_{i i} x_{t}+b_{i i}+W_{h i} h_{t-1}+b_{h i}\right) \\
f_{t} &=\sigma\left(W_{i f} x_{t}+b_{i f}+W_{h f} h_{t-1}+b_{h f}\right) \\
g_{t} &=\tanh \left(W_{i g} x_{t}+b_{i g}+W_{h g} h_{t-1}+b_{h g}\right) \\
o_{t} &=\sigma\left(W_{i o} x_{t}+b_{i o}+W_{h o} h_{t-1}+b_{h o}\right) \\
c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot g_{t} \\
h_{t} &=o_{t} \odot \tanh \left(c_{t}\right)
\end{aligned}
$$Now, let's break these operations down. Note that the dimensionality and type of the objects (matrices and vectors) are important. I will start with the computation of $i_{t}$ (the output of the input gate). The computation of $f_{t}$ (forget gate), $g_{t}$ (cell) and $o_{t}$ (output gate) is the same, in terms of time complexity.$$i_{t} =\sigma\left(W_{i i} x_{t}+b_{i i}+W_{h i} h_{t-1}+b_{h i}\right)$$Here is an estimate of the time complexities of the operations inside the non-linearityNow, let $N = \max(d, h)$, then the time complexity of the operations inside the non-linearity is $\mathcal{O}(hN)$. More precisely, it would be$$\mathcal{O}(h d + h + h^2 + h) = \mathcal{O}(h d + 2h + h^2) = \mathcal{O}(h(d + 2 + h)).$$Actually, in this case, as I explain in the other answer, you could use $\Theta(hN)$ because this is an upper and lower bound in the limit.Now, what is the time complexity of $\sigma$? It depends on what $\sigma$ is. If we assume it's a ReLU, then you perform basically the following operation$$\displaystyle \sigma(x) = \max(0,x)$$Let's assume that $\max(0,x)$ has a constant time complexity, so, if you apply the ReLU element-wise, you get a time complexity of $h$.So, basically, the computation of $i_t$ has time complexity $\mathcal{O}(hN)$. The same applies to $f_{t}$ (forget gate), $g_{t}$ (cell) and $o_{t}$ (output gate), as stated above, so you could multiply this time complexity by $4$, but constant factors don't affect the time complexity. However, this is in theory, i.e. in the limit, in practice, they affect, so you probably want to include those. So, the time complexity, let's say, is$$\mathcal{O}(4h(d + 2 + h))$$so far (ignoring there's a tanh there: for simplicity, I assume it has the same time complexity as the ReLU even though that may not be the case: you can work out the details!)Now, we have the following operation$$c_{t} =f_{t} \odot c_{t-1}+i_{t} \odot g_{t},$$whereSo, this operation has time complexity$$\mathcal{O}(2 h)$$Similarly, the operation$$h_{t} =o_{t} \odot \tanh \left(c_{t}\right)$$has time complexity$$\mathcal{O}(2 h)$$One $h$ for the tanh and the other for the element-wise multiplication.So, overall, the forward pass of a single LSTM layer has time complexity$$\mathcal{O}(4h(d + 2 + h) + 4h) = \mathcal{O}(4h(d + 3 + h))$$Now, you can work out the time complexity for multiple LSTM layers stacked one after the other. Just make sure that you understand that what is passed to the successive layer is not the input $x_t$ but the output of the previous layer.In the end, you will also have to sum the complexity of the computation of the metric you're interested, which can be the loss or accuracy. If you understood my reasoning above, it should not be difficult to do that. The same applies for the case of a batch size and sequence size greater than 1."
How to align or synchronize Youtube caption with audio accurately,"
I need to use the automatic caption from Youtube to precisely isolate excerpts from the video aligned to text and generate the dataset to train a model in French.
So I've already written the script, but when I compare the audio with the matching text, I noticed that the text is often delayed (positive or negative). For example, the text reads ""1 2 3 4"" and the audio says ""0 1 2 3"" (""0"" comes from the previous clip).
If you have a look at a Youtube video in French, when you click on ""open transcript"", you can also notice this delay.
Here is an example that is very noticeable on short clips: The audio says ""conditions de travail"" whereas the transcript reads ""de travail"".
I measured the delay in Audacity and it is not consistent across the clips. Please note that it does not seem to happen in English videos.
If I use Google Speech Recognition in Python (recognize_google) on audio clips, there are no such delays (also because the clips are already separated) but the punctuation is missing which is not good for training my model.
Why can't Google align more accurately the audio and the text (caption)?
Can you suggest a better way of aligning audio with text?
","['deep-learning', 'speech-recognition']",
Custom Tensorflow loss function that disincentivizes all black pixels,"
I'm training a Tensorflow model that receives an image and segments the image into foreground and background. That is, if the input image is w x h x 3, then the neural network outputs a w x h x 1 image of 0's and 1's, where 0 represents background and 1 represents foreground.
I've computed that about 75% of the true mask is background, so the neural network simply trains a model that outputs all 0's and gets a 75% accuracy.
To solve this, I'm thinking of implementing a custom loss function that checks if there are more than a certain percentage of 0's, and if so, to add a very large number to the loss to disincentivize the all 0's strategy.
The issue is that this loss function becomes non-differentiable.
Where should I go from here?
","['neural-networks', 'tensorflow', 'objective-functions', 'image-segmentation']","The background being an unbalance class is a well known problem in image segmentation. Before digging into custom losses you should take a look to existing ones that address this specific issue like the Dice Loss or Focal Loss, the latter being more tunable having a extra hyper parameter that can be optimized. You can easily find on github  tensorflow implementations of both.
For a more detailed comparison and reference to other similar losses you can also check this paper."
Is it true that batch size of form $2^k$ gives better results?,"
I am confused among the following in selecting the batch size for my model.
#1: powers of 2
I generally see that batch sizes are in powers of two: 32, 64, 128, 256.
#2: maximum GPU
Suppose my GPU allows a maximum batch size of 61. And it is not a power of two.
Which one should I apt? Is there anything like the powers of 2 will give relatively good results?
","['deep-learning', 'hyper-parameters', 'gpu', 'batch-size']",
What are knowledge graph embeddings?,"
What are knowledge graph embeddings? How are they useful? Are there any extensive reviews on the subject to know all the details? Note that I am asking this question just to give a quick overview of the topic and why it might be interesting or useful, I am not asking for all the details, which can be given in the reference/survey.
","['reference-request', 'embeddings', 'knowledge-graph', 'knowledge-graph-embeddings']","Knowledge graph embeddings (KGE) are embeddings created in the context of a knowledge graph (KG), which can be viewed as a visual/graphical representation of a knowledge base, where nodes are entities (e.g. ""a car"" or ""Lewis Hamilton"") and edges are relations between those entities (e.g. ""drives""), so a (directed) connection from ""Lewis Hamilton"" to ""a car"" through the edge ""drives"" would represent the fact ""Lewis Hamilton drives a car"".A KGE is a vector representation of an entity or relation in the KG that, hopefully, preserves the semantics of the entities and relations. For example, we expect the embedding of the entity ""man"" to be closer to the embedding of the entity ""woman"" than to the embedding of a ""turtle"". The idea of learning a lower-dimensional vector representation of objects that preserves some notion of semantics or meaning between those objects also appears in other AI subfields, like natural language processing (see e.g. word embeddings), or ML applied to software engineering (see code embeddings).A KGE can be useful because a KG is most likely incomplete, so one of the tasks that you need to solve when using a KG is determining whether a fact is true or false. So, if we denote the fact as a triple $f = \langle s, r, o \rangle$, where $s$ stands for the subject, $r$ for the relation, and $o$ the object, then we want to determine whether this fact $f$ is true or false. In the context of KGEs, this is known as triple classification. Of course, in our KG, we don't have the fact $f$, but maybe we have the entities $s$ and $o$ but there's no edge between them that is labeled with $r$. So, given the embeddings of $s$, $r$, and $o$ (which was previously learned), we can then use a so-called score function to determine the likelihood of this fact. So, KGEs can be used to discover new ""likely"" knowledge, but it's a different approach than using deduction to derive new knowledge from a set of axioms or facts. It's an inductive/probabilistic approach, which has advantages (e.g. you don't need a deductive system) but also disadvantages (e.g. the fact may not actually be true even if our approach tells us that's the case).There are many similar/related ways to learn KGEs. Just to give you an idea of how these are learned, a simple approach (TransE) is as follows. You learn the vectors $s$, $r$ and $o$ such that the constraint $s + r = o$ is satisfied. Of course, this constraint only makes sense if $o$ can only go with $s$ and $r$, so this approach is not realistic, as it doesn't model e.g. many-to-many relations. For this reason, people have introduced other approaches (like RotatE or ConvE), which have advantages but also disadvantages. There are also other issues that arise when learning these embeddings, like generating synthetic negative/false facts (i.e. if you have a KG, you only have known true facts, so how do you know if a fact is false? You need examples of false facts!).The Wikipedia page on KGEs is already quite comprehensive, but there are many other resources on the topic. For example, the survey Knowledge Graph Embedding: A Survey of Approaches and Applications (2017) or the (long but very useful) tutorial Knowledge Graph Embeddings Tutorial: From Theory to Practice by some of the people that are currently doing research on this topic."
How does the classification head of EfficientDet work?,"
EfficientDet outputs classes and bounding boxes. My question is about both but specifically I am interested in the class prediction net part. In the paper's diagram it shows 2 conv layers. I don't understand the code and how it works. And what's the difference between the 2 conv layers of classification and box prediction?

","['deep-learning', 'computer-vision', 'object-detection', 'object-recognition', 'semantic-segmentation']",
Triplet Loss- Three forward pass and one backward pass(Propagation),"
I am trying to build a CNN model based on the concepts of Contrastive Learning. In specific based on Triplet loss.
I have 5 different class labels and I create triplets such that in a triplet, two images are from the same class and the third one is from another class.
I have a CNN model which takes one input from a triplet at a time and generates its corresponding embedding in 128 dimensions.
All three embedding embeddings from a triplet are used for calculating loss. The loss is based on the Triplet loss.
Further, the loss is backpropagated and training is carried out stochastically.
The idea is to use the trained model to generate one embedding for an input image which can be further used for multi-class classification problems.
My question is, is this method of 3 forward passes and 1 backward pass valid in Tensorflow?
Here is a fragment of my code that I am using for training:
def cnn():
    model_input = layers.Input(shape=(112, 112, 3))
    x = layers.Conv2D(filters=16, kernel_size=3, padding='same', name='Conv1')(model_input)
    x = layers.MaxPool2D()(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    x = layers.Conv2D(filters=32, kernel_size=3, padding='same', name='Conv2')(x)
    x = layers.MaxPool2D()(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    x = layers.Conv2D(filters=64, kernel_size=3, padding='same', name='Conv3')(x)
    x = layers.MaxPool2D()(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    x = layers.Conv2D(filters=128, kernel_size=3, padding='same', name='Conv4')(x)
    x = layers.MaxPool2D()(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    x = layers.Conv2D(filters=256, kernel_size=3, padding='same', name='Conv5')(x)
    x = layers.MaxPool2D()(x)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    x = layers.GlobalAvgPool2D(name='GAP')(x)
    output = layers.Dense(128, activation='tanh', name='Dense1')(x)
    origin = tf.zeros_like(output, dtype=float)
    unit_vector = tf.divide(output, tf.sqrt(tf.reduce_sum(tf.square(output-origin)))) # Normalize vector(L2_Norm)
    shared_model = Model(inputs=model_input, outputs=unit_vector)
    shared_model.summary()
    return shared_model
 
def triplets_loss(anchor_sample, positive_sample, negative_sample, alpha=0.2):
    anchor_pos_dist = tf.sqrt(tf.reduce_sum(tf.square(anchor_sample - positive_sample))) # distance between positive pairs
    anchor_neg_dist = tf.sqrt(tf.reduce_sum(tf.square(anchor_sample - negative_sample)))# distance between negative pairs
    triplet_loss = tf.maximum(((anchor_pos_dist - anchor_neg_dist) + alpha), 0.000001) # triplet loss
    return triplet_loss
 
def train(train_data_dir, training_batch=4, lr=1e-4, epochs=100,margin=0.2):
    model = cnn()
    ### creating triplet data loader object ###
    train_data_util_instance = TripletFormulator(data_path_dictionary=train_data_dir, 
    batch=training_batch)
    train_data_array_dict, data_count = train_data_util_instance.data_loader()
    majority_class = max(data_count, key=data_count.get)
    ######
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    for epoch in range(epochs):
        total_train_loss = 0
        start_time = time.time()
        batch_no = 0
        for majority_class_batch in train_data_array_dict[majority_class]:
            batch_loss = 0
            train_batch_dict = {}
            train_batch_dict['A'] = next(iter(train_data_array_dict['A']))
            train_batch_dict['B'] = next(iter(train_data_array_dict['B']))
            train_batch_dict['C'] = majority_class_batch
            train_batch_dict['D'] = next(iter(train_data_array_dict['D']))
            train_batch_dict['E'] = next(iter(train_data_array_dict['E']))
            train_triplets = train_data_util_instance.triplet_generator(train_batch_dict)
            for triplets in train_triplets:
                with tf.GradientTape() as tape:
                     anchor = model(tf.reshape(triplets[0], [-1, 112, 112, 3]))
                     positive = model(tf.reshape(triplets[1], [-1, 112, 112, 3]))
                     negative = model(tf.reshape(triplets[2], [-1, 112, 112, 3]))
                     if np.isnan(anchor).any() or np.isnan(positive).any() or np.isnan(negative).any():
                         print('NAN FOUND')
                     else:
                         loss = triplets_loss(anchor_sample=anchor, positive_sample=positive,negative_sample=negative, alpha=self.alpha)
                     total_train_loss += loss
                     batch_loss += loss
                grads = tape.gradient(loss, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))
             print(epoch, batch_no, batch_loss)
             batch_no += 1
        end_time = time.time()
        print('Training_loss: ', total_train_loss, 'Time_taken: ', end_time - start_time)

When I start training, I can see the training loss converging. But the overall idea is confusing about the number of forward passes per backward pass. Also, is this method a concept of weight sharing?
I would be very eager to discuss this topic as I do not see many problems similar to this.
In most cases, the CNN model takes multiple input and generate multiple outputs and further, the embeddings are used for binary classification that is if the inputs are the same or not.
I would be waiting for your comments and suggestions.
","['deep-learning', 'tensorflow', 'triplet-loss-function']","In triplet loss you basically have one encoder and the loss is minimised according to In this sense, the weights are shared between the encoding calculated on the negative, anchor and positive parts of the sample.I'd recommend however to not hard pick negative and positive samples for every anchor, but to let your code pick a convenient triplet instead: a negative and positive maximising the distance.Tensorflow has this implemented directly:
https://www.tensorflow.org/addons/api_docs/python/tfa/losses/TripletSemiHardLoss"
Is it possible to find a good neural network structure without training it? [duplicate],"







This question already has answers here:
                                
                            




How can I automate the choice of the architecture of a neural network for an arbitrary problem?

                                (2 answers)
                            


Does training happen during NEAT?

                                (1 answer)
                            

Closed 1 year ago.



Neural networks consist of so many parameters. Researchers could create as many possible neural networks as they wish. So I want to ask a general question. Could we devise an evolutionary algorithm which learns an efficient structure without optimization?
Are there some important works in this area?
If we look at sparse neural networks, it seems that there are so many topologies that perform as well as a dense network.
So a single task has so many solutions which differ slightly. So getting rid of optimization for many problems shouldn't be hard at all.
Edit: I add some more information. I want to know whether we could find sparse topologies by mutating them like adding layers and changing the connections without optimizing the loss function directly?
","['optimization', 'topology']",
What is the expression for projective transformation?,"
The following are the two types are projections that are generally used in image processing

Affine transformation
Projective transformation

Affine transformation is a backbone operation in neural networks also. It is expressed as
$$\mathbf{wx+b}$$
where $\mathbf{w, x, b}$ are matrices. In general, $\mathbf{x}$ is treated as an image in image processing.
Projective transformation is also a type of transformation on images and it may be different from affine transformation. I want to know whether it can be represented in terms of mathematical expression.
If yes, what is the expression for projective transformation?
","['computer-vision', 'image-processing', 'affine-transformations', 'image-transformations', 'projective-transformations']",
How to mix grid matrix and explicit values when designing RL state?,"
I'm trying to do multi-agent reinforcement learning on the grid world navigation task where multiple agents try to collectively reach multiple goals while avoiding collisions with stationary obstacles and each other. As a constraint, each agent can only see within a limited range around itself.
So on a high level, the state of each agent should contain both information to help it avoid collision and information to guide it towards the goals. I'm thinking of implementing the former by including into the agent's state a matrix consisted of the grid cells surrounding the agent, which would show the agent where the obstacles are. However, I'm not sure how to include goal navigation information on top of this matrix. Currently I just flatten the matrix and append all relative goal locations at the end, and use this as the state.
For example, for a grid world as shown below (0 means empty cell, 1 means agents, 2 means obstacles, and 3 represents goals):
[[0 0 0 0 0 0 2 2 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 2 2 0 0 0 0 0 0]
 [0 3 2 2 0 0 0 0 0 2]
 [0 0 0 0 0 0 0 0 0 2]
 [0 0 0 0 1 0 0 0 0 2]
 [2 0 0 0 0 2 2 0 3 0]
 [2 0 0 0 0 2 2 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]
 [0 0 2 0 0 1 0 0 0 0]]

The agent at row5 col4 sees the following cells that are within distance1 around it:
[[0. 0. 0.]
 [0. 1. 0.]
 [0. 0. 2.]]

flattened, the matrix becomes:
[0,0,0,0,1,0,0,0,2]

The location of the goal at row3 col1 relative to the aforementioned agent is (5-3=2, 4-1=3)
The location of the goal at row6 col8 relative to the aforementioned agent is (5-6=-1, 4-8=-4)
So after appending the relative locations, the state of the agent becomes:
[0,0,0,0,1,0,0,0,2,2,3,-1,-4]

(Similar process for the other agent)
Is this a reasonable way of designing the state? My primary concern is that the flattened grid matrix and the relative goal locations need to be handled quite differently, but it can be hard for RL to figure out the difference.
Thanks in advance!
Edit: To validate my concern, I trained an agent using PG REINFORCE algorithm. As I feared, the agent learned to avoid obstacles but otherwise just moved randomly without navigating towards the goals.
","['reinforcement-learning', 'policy-gradients']","I didn't find a way to improve state design, but I did find a workaround in making my PG network modular.I simply separated my PG network into two parts -- one taking in just the flattened grid matrix part from the aforementioned state, and the other taking in just the relative goal locations. Then I concatenated the outputs from the two sub-networks and passed them through a softmax layer to get the final policy.If you want more details you can check out my codes here (The relevant codes are in MARL_PolicyGradient.py, MARL_env.py, and MARL_networks.py). Good luck!"
Is there a fundamental difference between an environment being stochastic and being partially observable?,"
In AI literature, deterministic vs stochastic and being fully-observable vs partially observable are usually considered two distinct properties of the environment.
I'm confused about this because what appears random can be described by hidden variables. To illustrate, take an autonomous car (Russel & Norvig describe taxi driving as stochastic). I can say the environment is stochastic because I don't know what the other drivers will do. Alternatively, I can say that the actions of the drivers are determined by their mental state which I cannot observe.
As far as I can see, randomness can always be modeled with hidden variables. The only argument I came up with as to why the distinction is necessary is Bell's inequality, but I don't think that AI researchers had this in mind.
Is there some fundamental difference between stochasticity and partial observability or is this distinction made for practical reasons?
","['comparison', 'markov-decision-process', 'environment', 'pomdp']","I think the distinction is made more for conceptual reasons, which has practical implications, so let me review the usual definitions of a stochastic and partially observable environment.A stochastic environment can be modeled as a Markov Decision Process (MDP) or Partially Observable MDP (POMDP). So, an environment can beThe stochasticity refers to the dynamics of the environment and, more specifically, to how the environment stochastically moves from one state to the other after an action is taken (basically, a Markov chain with actions and rewards). In other words, in a stochastic environment, we have the distribution $p(s' \mid s, a)$ (or, in some cases, the reward is also included $p(s', r \mid s, a)$). If $p(s' \mid s, a)$ gave a probability of $1$ to one of the states and $0$ to all other states, we would have a deterministic environment.The partial observability refers to the fact that we don't know in which state the agent is, so we can think of having or maintaining a probability distribution over states, like $b(s)$. So, in the case of POMDP, we not only are uncertain about what the next state $s'$ might be after we have taken $a$ in our current state $s$, but we are not even sure about what $s$ currently is.So, the difference is made so that we can deal with uncertainties about different parts of the environment (dynamics and actual knowledge of the state). Think about a blind guy that doesn't have the full picture (I hope this doesn't offend anyone) and think about a guy that sees well. The guy that sees well still isn't sure about tomorrow (maybe this is not a good example as you can argue that this is also due to the fact that the guy that sees well doesn't know the full state, but I hope this gives you the intuition).Of course, this has practical implications. For example, it seems that you cannot directly apply the solutions that you use for MDPs to POMDPs. More precisely, for an MDP, if you learn the policy $\pi(a \mid s)$, i.e. a probability distribution over actions given states, if you don't know the state you are in, this policy is quite useless.To deal with the uncertainty about the state the agent is in, in POMDPs, we also have the concept of an observation, which is the information that the agent gathers from the environment about the current state (e.g., in the example of a blind guy, the observations would be the sounds, touch, etc.), in order to update its belief about the current state. In practice, some people tried to apply the usual RL algorithms for MDPs to POMDPs (see e.g. DQN or this), but they made a few approximations, which turned out to be useful and successful.If the difference wasn't still clear, just take a look at the equation that can be used to relate the belief state and the transition model (dynamics) of the environment$$
\underbrace{b^{\prime}\left(s^{\prime}\right)}_{\text{Next belief state}}=\alpha \underbrace{P\left(o \mid s^{\prime}\right)}_{\text{Probability of observation }o \text{ given }s'} \sum \underbrace{P\left(s^{\prime} \mid s, a\right)}_{\text{Transition}\\ \text{model}} \underbrace{b(s)}_{\text{Previous belief state}}
$$So, in a POMDP, the policy, as stated above, in theory, cannot depend on $s$, but needs to depend on $b(s)$, the belief state, i.e. a probability distribution over states.If this answer wasn't still satisfactory, although you probably already did it, you should read the section 2.3.2 Properties of task environments of the AIMA book (3rd edition). Their description of stochastic and partially observable environment seems to be consistent with what I wrote here, but maybe their description of a stochastic environment is not fully clear, because they sayIf the next state of the environment is completely determined by the current state and the action executed by the agent, then we say the environment is deterministic; otherwise, it is stochasticThe unclear part is completely determined. They should have said deterministically determined (which you can use for a rap song).However, they later clarify their definition by sayingour use of the word ""stochastic"" generally implies that uncertainty about outcomes is quantified in terms of probabilitiesIn addition to that, they call an environment that is either stochastic or partially observable uncertain. It makes sense to do this because uncertainty makes the problems harder, so we can differentiate between certain and uncertain environments.To be honest, I don't know if there's some kind of mathematical formalism that doesn't differentiate between stochastic or partially observable environments, but I am not sure how useful it might be."
What is the relevance of the concept size to the time constraints in PAC learning?,"
My question is about the relevance of concept size to the polynomial-time/example constraints in efficient PAC-learning. To ask my question precisely I must first give some definitions.
Definitions:
Define the input space as $X_n = {\left\{ 0,1\right\} }^n$ and a concept $c$ as a subset of $X_n$. For example, all vectorized images of size $n$ representing a particular numeral $i$ (e.g. '5') collectively form the concept $c_i$ for that numeral. A concept class $C_n$ is a set of concepts. Continuing our example, the vectorized numeral concept class $\left\{ c_0, c_1, \dots c_9\right\}$ is the set of all ten vectorized numeral concepts for a given dimension $n$.
As an extension to include all dimensions we define $\mathcal{C} = \cup_{n \geq 1} C_n$. A hypothesis set $H_n$ is also a fixed set of subsets of $X_n$ (which might not necessarily align with $C_n$) and we define $\mathcal{H} = \cup_{n \geq 1} H_n$.
The following definition of efficient PAC-learnability is adapted from An Introduction to Computational Learning Theory by Kearns and Vazirani.
$\mathcal{C}$ is efficiently PAC-learnable if there exists an algorithm $\mathcal{A}$ such that for all $n \geq 1,$ all concepts $c \in C_n$, all probability distributions $D$ on $X_n$, and all $\epsilon, \delta \in \left(0,1\right)$, the algorithm halts within polynomial time $p{\left( n, \text{size}{\left(c\right)}, \frac{1}{\epsilon}, \frac{1}{\delta}\right)}$ and returns a hypothesis $h \in H_n$ such that $$ \underset{x \sim D}{\mathbb{P}}{\left[ h{\left(x\right)} \neq c{\left(x\right)}  \leq \epsilon\right]} \geq 1 - \delta.$$
Question:
Now, in the polynomial $p{\left(\cdot, \cdot, \cdot, \cdot \right)}$, I understand the dependence on $\epsilon$ (accuracy) and $\delta$ (confidence). Additionally, I understand why the polynomial should depend on $n$ - the concept of learnability should be invariant to the time burden incurred from increasing the dimension of the input space (e.g. increasing the resolution of the image). What I do not understand is why the dependence on the size of the target concept (which I believe is usually taken to mean the smallest encoding of the target concept)?
","['machine-learning', 'computational-learning-theory', 'pac-learning']",
How does Horn–Schunck method for Optical Flow solve the aperture problem?,"
This is regarding the details stated in Wikipedia.
I am reading optical flow in Computer Vision. I understood the Horn–Schunck method as such, but did not get how it is related to the aperture problem, and how it is solved using Horn–Schunck method.
Also, why is Horn–Schunck method invented/used where a simpler ""Lucas–Kanade method"" is already there (Reference)?
","['computer-vision', 'optical-flow']",
Optimize parametric Log-Likelihood with a Decision Tree,"
Suppose there are some objects with features, and the target is parametric density estimation. Density estimation is model-based. Parameters are obtained by maximizing log-likelihood.
$LL = \sum_{i \in I_1} \log \left( \sum_{j \in K_i} \theta_j \right) + \sum_{i \in I_2} \log (1 - \sum_{j \in L_i} \theta_i)$
Assume that parameters $\theta_j$ are probabilities, i.e.  $0 < \theta_j < 1$, and that $\sum_{j\in L_i} \theta_i < 1$. From practical perspective, it seems natural to make parameters $\theta_j$ themselves functions of features, i.e. $\theta_j = F(x_j^1, \ldots, x_j^m)$.
Is there any known standard method or heuristic to optimize such objective with a decision tree, i.e. we assume that our function $F$ is a decision tree?
Any related results are welcome.
","['decision-trees', 'maximum-likelihood', 'density-estimation']",
Is there any way to force one input have more effect on model?,"
Now I am working on building a deep learning model for a regression problem. I used 50 inputs and try to add one new categorical input. The problem is that this one input is much more important than other inputs. I want to make it more influential than others and all I can think of now are the following three.

just add at first layer as other inputs
Add new categorical input to each layer (Now model has 5 layers)
Fit it to the embedding layer first and increate its dimension and concatenate it with other inputs.

Do these seem fine and are there any other ways to give more power to one input?
","['deep-learning', 'architecture', 'embeddings', 'input-layer']",
Validity of ImageNet for measurement of the model performance,"
ImageNet dataset is an established benchmark for the measurement of the performance of CV models.
ImageNet involves 1000 categories and the goal of the classification model is to output the correct label given the image.
Researchers compete with each other to improve the current SOTA on this dataset, and the current state of the art is 90.88% top-1 accuracy.
If the images involved only a single object and background - this problem would be well-posed (at least from our perceptual point of view). However, many images in the dataset involve multiple objects - a group of people, a person, and an animal - the task of classification becomes ambiguous.
Here are some examples.
The true class for this image is bicycle. However, there is a group of people. The model that recognizes these people would be right from the human point of view, but the label would be wrong.

Another example is the fisherman with fish called tench.
The model could have recognized the person, but be wrong.

So, my question is - how does much the performance of the best models of ImageNet reflect their ability to capture complicated and diverse image distribution, and how much the final result on the validation set is accidental. When there are multiple objects present on the image, the network can predict any of them. Prediction can match the ground truth class or can differ. And for the model, that happens to be luckier, this benchmark will show better performance. The actual quality can be the same, in fact.
","['computer-vision', 'image-recognition', 'image-net']",
Would it be possible to enforce the same $s_{t + 1}$ between the model's estimate and the target function's Q-value?,"
Say I have a game of blackjack, and I am trying to teach a single forward-pass neural network to approximate the Q value of the current state and action.
There are 3 inputs: The current card in hand, the cards in the deck, and the cards in the pile. It outputs the Q-value of two actions, namely, holding or adding the current card to the pile.
My loss function is $$L(Q,Q_E)=  \sum({Q(s,a_i)- Q_E(s,a_i) )^2},$$
where $Q_E$ is the estimated Q-value of the current state from the policy network. And $Q$ is the target function, which is calculated using the Bellman equation.
As I understand it, the Bellman equation assumes the setting to be deterministic, meaning that, if you're in state $s_t$ as you take action $a_1$, you should always reach the same $s_{t+1}$.
Of course, in blackjack, this is not the case, as the state $s_{t+1}$ is purely dependent on the card you draw, which is a stochastic process.
Would it be possible to omit some of this noise or ""stochasticity"" by enforcing the same $s_{t + 1}$ between the model's estimate and the target function's Q-value?
In other words, say we're in state $s_t$, and the target function picks action $a_1$ and draws a 10 reaching state $s_{10}$ as the next state. For the training of the policy network, it loads the state $s_t$ from the experience replay, it also picks action $a_1$ and draws a 7 reaching state $s_7$ as the next state.
Would it somehow ruin the training, if I then just hardcoded it, such that the next state reverted to state $s_{10}$ if the policy network picked action $a1$? Are there any counter-productive consequences to this?
","['reinforcement-learning', 'dqn', 'bellman-equations']","As I understand it, the Bellman equation assumes the setting to be deterministic, meaning that, if you're in state $s_t$ as you take action $a_1$, you should always reach the same $s_{t+1}$.This is not correct, which is a good thing for you. The Bellman equation for action values for an arbitrary policy $\pi(a|s): \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R} = \mathbf{Pr}\{A_t=a|S_t=s \}$ looks like this:$$q_\pi(s,a) = \sum_{r,s'}p(r,s'|s,a)(r+\gamma\sum_{a'}\pi(s',a')q_{\pi}(s',a'))$$The environment model in this equation is $p(r,s'|s,a)$ which is the probability of observing reward $r$ and next state $s'$, given current state $s$ and action $a$. This is stochastic.When using a model-free approach learning from experience, you cannot use the full equation each time, because you only have one sample and do not know anything about $p()$. However, things will still work, because if you take many samples, on average they will follow the probability distribution for $p()$, and your update rule will converge to the average/expected return for each action value.The same sampling also applies to the policy, which means the above Bellman equation can be expressed as:$$q_\pi(s,a) = \mathbb{E}_{A_{t+1} \sim \pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1},A_{t+1})|S_t=s, A_t=a]$$which in turn means you can use samples of $r, s'$ and $a'$ (or for Q-learning, the maximising $a'$), and an update rule that averages between the values you observe. You can do this with both deterministic and stochastic environments, because the theory behind it works with both.Would it somehow ruin the training, if I then just hardcoded it, such that the next state reverted to state $s_{10}$ if the policy network picked action $a1$? Are there any counter-productive consequences to this?For some environments, and some setups of the hardcoding, it may work. However, you should not do this because it is not necessary, and in general it is a bad idea, as it may impact the sampling in a way that causes incorrect learning."
a loss for binary step function data,"
I have some data with ground truth that looks like a binary step  function, where part of it is 0 and part is one.
An example for the GT can be like 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 
or something like this

I have a hard time to come up with a loss function that can optimize this problem, the simplest option would be something like CrossEntropy or BinaryCrossEntropy, but I am wondering if there is any other loss that I try.
Something that can take into account the property that when the GT is one (1) it is continuous 1 and when it is zero it is continuous.
To give a little more information, for example, I will never have a GT that be like this 0 0 1 0 1 0 1  also I will never have a GT like this  0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0. In other words, I will have one time ones in a continuous way (it can be at the start or middle or end) but it wont be two discontinuous 1s.
Is there any loss function that take into account this properties?
","['machine-learning', 'deep-learning', 'objective-functions', 'optimization', 'geometric-deep-learning']",
How to do testing for an RNN that was trained with teacher forcing only?,"
If an RNN is trained using only the teacher forcing, then the network takes the actual output from the previous time step as input to the hidden state the next time step.
We know that the actual outputs cannot be given to the model while testing, then what information passes from a time step to the next time step in the test phase?
","['recurrent-neural-networks', 'testing', 'teacher-forcing']",
How do I show the relationship between theories and models using Conceptual Graphs?,"
The Mereology Theory below contains three first-order axioms that represent a part of a mereology theory. For this posting, it is important that the set of axioms should be considered as a theory.
Mereology Theory
Reflexivity $\forall x : part(x,x)$
Antisymmetry $\forall x  \forall y : ((part(x,y) \land part(y,x)) \implies (x = y))$
Transitivity $\forall x  \forall y \forall z :((part(x,y) \land part(y,z))  \implies part(x,z))$
Here is my naïve attempt to present the theory as a set of conceptual graphs (CG).

My understanding of the above CGs is as follows:

The variables are universally quantified, not default for CGs, but allowed in extended CG (ECG).
The inner graphs are all related by conjunction, which is default for GCs and I assume for ECGs.
The arrow on graph representing reflexivity is bi-directional.
Both antisymmetry and transitivity are represented by an IF-THEN contexts.
Dotted lines are co-references.
Equality (=) is actually commutative, but is represented as a directed relation .
Each inner graph asserts a single proposition, labelled Proposition.
The outer graph is labeled MereologyTheory, I am not sure that this is correct ECG syntax.

Below is a possible model of the above theory:
Mereology Model Mathematical notation
$Entities = \{ a,b  \}$
$Relations = \{part(a,a),part(a,b),part(b.b)\}$
Obviously there are many other possible models. MereologyModel below is my attempt to visualize this model as a CG. I am not sure that putting the label MereologyModel is correct CG syntax or denotes it as a model of  MereologyTheory .

Question:
In CG visual notation, how are theories and models related?
It seems to me that basic CGs can represent FOL sentences and the relation between such sentences. According to Chein and Mugnier the subsumption relation is defined by graph homomorphisms between CGs. Is  the model/theory relation for CGs also defined in terms of graph homomorphisms? I am aware that in general a model satisfies a theory i.e. $M \vDash T$. Does the graph homomorphism provide the necessary syntactic mapping to enable model and theory to be related?
Note  CGs can be represented in Common Logic (ISO zipped PDF), which would permit a formal proof that all the axioms of the theory are satisfied in the model.
","['logic', 'knowledge-representation', 'ontology']",
Can I use a Mask R-CNN to detect a skin texture?,"
I'm trying to implement a solution in python to detect skin in an image.
I'm evaluating the Mask R-CNN model to create a mask on the skin (not on clothes). The problem is that every solution I have encountered using Mask R-CNN uses it to classify objects. I'm afraid that using it trying to classify texture might be a problem. Is it the case?
My dataset is actually pretty good, composed of the

original image,
precise mask on the skin, and
grounding box.

Can I use a Mask R-CNN to detect a skin texture?
","['convolutional-neural-networks', 'python', 'mask-rcnn']",
Does higher FLOPS mean higher throughput?,"
I understand that FLOPS means floating-point operations per second, and throughput is the number of inputs (for example, images) per second. If a model has higher FLOPS, it means it performs faster.
However, in the article Container: Context Aggregation Network, they show that:

The container has higher FLOPS and less throughput, while the container-light has lower FLOPS and higher throughput.
What is the reason for that?
","['neural-networks', 'deep-learning', 'papers', 'image-net', 'flops']","In the context of Deeplearning:FLOPS, refers to the number of floating point operations that can be performed by a computing entity in one second. It is used to quantify the performance of a hardware.FLOPs, simply means the total number of floating point operations required for a single forward pass. The higher the FLOPs, the slower the model and hence low throughput.This thread on stack overflow might help to get a deeper insight:
https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning"
Not able to understand Pytorch Tensor (Weight & Biases) Size for Linear Regression,"
Below are the two tensors
[ 73.,  67.,  43.]    
[ 91.,  88.,  64.],
[ 87., 134.,  58.],
[102.,  43.,  37.],
[ 69.,  96.,  70.]

[ 56.,  70.],
[ 81., 101.],
[119., 133.],
[ 22.,  37.],
[103., 119.]

These are the weight that are added
Weights and biases
 w = torch.randn(2, 3, requires_grad=True)
 b = torch.randn(2, requires_grad=True)

I am not able to understand how the size of tensors are decided for weight and biases. Is there common rule that we should follow while adding weight and biases for our model
","['pytorch', 'weights', 'linear-regression', 'tensor', 'bias']","The size of the parameters tensor is depended on what type of layer that you want to build. Convolutional, fully connected, attention or even custom layer, each layer has a difference in the way it treats input, reading the documents is the good way to start (CS231n of Stanford University describes in detail each layer's properties).In your case, the layer name is the fully-connected layer (or dense, linear layer in other documents) which converts an m-dimension input vector to an n-dimension output vector. All m nodes are mapped to n nodes by a $m\times n$ matrix (that's why its name is fully-connected) and the bias vector, in short, is to help the learned function be more flexible.Therefore, the rule to decide the size of weight and bias is the size of input and target vector. Below is a simple example that builds a Linear layer from scratch:Or you can simply implement fully-connected layer by one line:"
Is my dataset a time series dataset? and should I use an LSTM?,"
I have a dataset where I am recording temperature after every 4milliseconds till 500 and another feature ""conductivity value"". The length of the dataset is around a 1000 rows. I need to find the conductivity value based on the temperature pattern.




t1
t2
t3
....
t5
conductivity




90
91
93
....
96
0.34


92
91
93
....
95
0.36




I am bit confused on how to use the dataset in a time series model such as LSTM because I have all the time sequence in columns and I don't know the conductivity values in between as in t2,t3,t4.
I think the dataset becomes a classification problem with the current format.
Can you guys help me out?
","['deep-learning', 'long-short-term-memory', 'time-series', 'regression']",I don't think time series model necessarily makes sense if you have one conductivity value to predict for each time series.A regression like setup makes more sense here: you could model this by letting the vector of time points represent the input. So you'd end up with a $n \mbox{ x } t$ matrix as input to predict the conductivity value.
Can teacher forcing in RNN ensure Turing completeness?,"
RNN has the same capability as a universal Turing machine. But I am confused whether RNN holds the same capabilities if we use teacher forcing.
Consider the following excerpts from paragraphs taken from the section titled ""Teacher Forcing and Networks with Output Recurrence"" of the chapter 10: Sequence Modeling: Recurrent and Recursive Nets of the textbook named Deep Learning by Ian Goodfellow et al.

The network with recurrent connections only from the output at one time step to the hidden units at the next time step is strictly less
powerful because it lacks hidden-to-hidden recurrent connections. For
example, it cannot simulate a universal Turing machine. Because this
network lacks hidden-to-hidden recurrence, it requires that the output
units capture all the information about the past that the network will
use to predict the future....... Models that have recurrent
connections from their outputs leading back into the model may be
trained with teacher forcing.

The quoted portion says that the RNN in which recurrent connections only exist from the output at one time step to the hidden units at the next time step are less powerful and are not as capable as a universal Turing machine. And those neural networks can be trained with teacher forcing. Even though they are not trained using teacher forcing, they are not as capable as the universal Turing machines. But, I want to get clarity on the relation between the capability of RNN trained using teacher forcing and the capability of universal Turing machine.
Is it true that if an RNN is trained with teacher forcing then it cannot simulate a universal Turing machine?
","['comparison', 'recurrent-neural-networks', 'theory-of-computation', 'teacher-forcing', 'turing-completeness']",
Deep Learning Architecture where outputs from two different inputs are used for error calculation,"
Is there a deep learning architecture where outputs of the same model with two different inputs are used for error calculation (backpropagation)?
Workflow:
Input1 -----> Model ------> Output1
Input2 -----> Model ------> Output2
Loss = criterion(Output1, Output2)
Backpropagation(Loss)
","['deep-learning', 'backpropagation', 'models']",
Generative systems based on Schmidhuber's compression framework,"
In Driven by Compression Progress:
A Simple Principle Explains Essential Aspects of
Subjective Beauty, Novelty, Surprise,
Interestingness, Attention, Curiosity, Creativity,
Art, Science, Music, Jokes Jürgen Schmidhuber describes, using the idea of using compression progress on perceived data as an intrinsic reward signal:

[Artists] create action sequences yielding interesting inputs, where interestingness is a measure of learning progress, for example [...] the saved number of bits needed to encode the data.

Has such a (purely generative) system been created before? Say, one that creates 2d images?
","['papers', 'generative-model', 'rewards', 'image-generation', 'data-compression']",
Why is there a Hessian diagonal approximation? And when can we use it?,"
This topic has been introduced in ""Pattern Recognition and Machine Learning, Bishop, 2006"", section 5.4.1. I am a bit confused about this method and I have two questions.

Why this method has attracted attention or has been developed?
First, we want to compute the Hessian fast, so we try to approximate it in O(W) time, where W is the number of parameters.
And then, we see that this matrix most of the time is heavily non-diagonal.

My second question is how can we know whether we can approximate a Hessian or not? Is there a hint/clue in problems?


Thanks in advance!
","['neural-networks', 'machine-learning', 'optimization']",
How does backprop work through the random sampling layer in a variational autoencoder?,"
Implementations of variational autoencoders that I've looked at all include a sampling layer as the last layer of the encoder block.  The encoder learns to generate a mean and standard deviation for each input, and samples from it to get the input's representation in latent space.  The decoder then attempts to decode this back out to match the inputs.
My question: How does backpropagation handle the random sampling step?
Random sampling is not a deterministic function and doesn't have a derivative.  In order to train the encoder, gradient updates must somehow propagate back from the loss, through the sampling layer.
I did my best to hunt for the source code for tensorflow's autodifferentiation of this function, but couldn't find it.  Here's an example of a keras implementation of that sampling step, from the keras docs, in which tf.keras.backend.random_normal is used for the sampling.
class Sampling(layers.Layer):
    """"""Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.""""""

    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

","['backpropagation', 'variational-autoencoder']","You do not backpropagate with respect to $\epsilon$, which is the random sample or random variable (depending on how you look at it). You backpropagate with respect to the mean $\mu$ and variance $\sigma$ of the latent Gaussian (the variational distribution). Note that, although $z$ is a random sample (and not just a sample), because it's computed as a function of $\epsilon$ (a random sample, once it's sampled from e.g. $\mathcal{N}(0, 1)$), $\mu$ and $\sigma$ are not: these are learnable parameters and are deterministic.Having said this, note that we use the reparametrization trick in the VAE, so we compute the random sample as $z = g_\phi(\epsilon, x))$, where $g_\phi$ is a deterministic function (encoder) parametrized by $\phi$ (the weights of the neural network that represents the encoder). In case the random variable $z \sim \mathcal{N}(\mu, \sigma^2)$, then we can express the random variable (so also the random sample) as follows $z=g_\phi(\epsilon, x)) = \mu+\sigma \epsilon$. So, as you can see from the code, we sample $\epsilon$ from some prior $p(\epsilon)$ (e.g. $\mathcal{N}(0, 1)$), then we compute $z$ deterministically.Why is this reparametrization trick useful? The authors of the VAE paper explain it.This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t $q_{\phi}(\mathbf{z} \mid \mathbf{x})$ such that the Monte Carlo estimate of the expectation is differentiable w.r.t. $\phi$. A proof is as follows. Given the deterministic mapping $\mathbf{z}=g_{\phi}(\boldsymbol{\epsilon}, \mathbf{x})$ we know that $q_{\phi}(\mathbf{z} \mid \mathbf{x}) \prod_{i} d z_{i}=$ $p(\boldsymbol{\epsilon}) \prod_{i} d \epsilon_{i}$. Therefore $^{1}, \int q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) f(\mathbf{z}) d \mathbf{z}=\int p(\boldsymbol{\epsilon}) f(\mathbf{z}) d \boldsymbol{\epsilon}=\int p(\boldsymbol{\epsilon}) f\left(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}, \mathbf{x})\right) d \boldsymbol{\epsilon}$. It follows that a differentiable estimator can be constructed: $\int q_{\phi}(\mathbf{z} \mid \mathbf{x}) f(\mathbf{z}) d \mathbf{z} \simeq \frac{1}{L} \sum_{l=1}^{L} f\left(g_{\phi}\left(\mathbf{x}, \boldsymbol{\epsilon}^{(l)}\right)\right)$ where $\boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon}).$Note that $\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})} \left[ f(\mathbf{z}) \right] = \int q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x}) f(\mathbf{z}) d \mathbf{z} = \int p(\boldsymbol{\epsilon}) f\left(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}, \mathbf{x})\right) d \boldsymbol{\epsilon} = \mathbb{E}_{p(\boldsymbol{\epsilon})} \left[ f\left(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}, \mathbf{x})\right) \right]$, which can be estimated with $\frac{1}{L} \sum_{l=1}^{L} f\left(g_{\phi}\left(\mathbf{x}, \boldsymbol{\epsilon}^{(l)}\right)\right)$ where $\boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon})$. In other words, you can sample $L$ $z$ in the way we did in order to estimate $\mathbb{E}_{\color{blue}{q_{\boldsymbol{\phi}}(\mathbf{z} \mid \mathbf{x})}} \left[ \color{red}{f(\mathbf{z})} \right]$ (this are Monte Carlo estimates of the expectation).In the case of the VAE, we want to optimize the ELBO, which is the following objective function$$\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)=\underbrace{-D_{K L}\left(q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \| p_{\boldsymbol{\theta}}(\mathbf{z})\right)}_{\text{KL divergence}}+ \underbrace{\mathbb{E}_{\color{blue}{q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)}}\left[\color{red}{\log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right)}\right]}_{\text{likelihood}}$$which we can estimate with Monte Carlo estimates of the second term (the likelihood term)$$
\widetilde{\mathcal{L}}^{B}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)=-D_{K L}\left(q_{\boldsymbol{\phi}}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \| p_{\boldsymbol{\theta}}(\mathbf{z})\right)+ \underbrace{\frac{1}{L} \sum_{l=1}^{L}\left(\log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} \mid \mathbf{z}^{(i, l)}\right)\right)}_{\text{likelihood}}
$$
where $$
\text { where } \quad \mathbf{z}^{(i, l)}=g_{\phi}\left(\boldsymbol{\epsilon}^{(i, l)}, \mathbf{x}^{(i)}\right) \text { and } \boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon})
$$Here, the likelihood is computed with the neural network, for example, in practice, you use the cross-entropy of the output of your decoder, which gets as input the input to the decoder, hence $z$.You can ignore the KL divergence now because, in the case of Gaussians, it can be computed analytically.Now, what if we didn't use the reparametrization trick? Could we still backpropagate with respect to $\phi$? The authors of the VAE writeThe usual (naïve) Monte Carlo gradient estimator for this type of problem is: $\nabla_{\phi} \mathbb{E}_{q_{\phi}(\mathbf{z})}[f(\mathbf{z})]=\mathbb{E}_{q_{\phi}(\mathbf{z})}\left[f(\mathbf{z}) \nabla_{q_{\phi}(\mathbf{z})} \log q_{\phi}(\mathbf{z})\right] \simeq \frac{1}{L} \sum_{l=1}^{L} f(\mathbf{z}) \nabla_{q_{\phi}\left(\mathbf{z}^{(l)}\right)} \log q_{\phi}\left(\mathbf{z}^{(l)}\right)$ where $\mathbf{z}^{(l)} \sim q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) .$ This gradient estimator exhibits exhibits very high variance (see e.g. [BJP12])So, the answer is yes, as opposed to what many people might say around, but with another estimator (which may remind you of some equations you have seen in reinforcement learning related to REINFORCE, if you are familiar with this), i.e. $$\frac{1}{L} \sum_{l=1}^{L} f(\mathbf{z}) \nabla_{q_{\phi}\left(\mathbf{z}^{(l)}\right)} \log q_{\phi}\left(\mathbf{z}^{(l)}\right) \tag{1}\label{1},$$ which has high variance.So, in the end, the reparametrization trick can be viewed as a variance reduction technique. There are others, like control variates or Flipout (used e.g. in the context of Bayesian neural networks).The first thing to note about \ref{1} is that we do not need to take the derivative with respect to $f$. The second thing is that $\nabla_{q_{\phi}\left(\mathbf{z}^{(l)}\right)} \log q_{\phi}\left(\mathbf{z}^{(l)}\right)$ is the score function.Now, don't ask me how to calculate this gradient $\nabla_{q_{\phi}\left(\mathbf{z}^{(l)}\right)} \log q_{\phi}\left(\mathbf{z}^{(l)}\right)$ (because I am bad at math). However, note that $
\mathbf{z}^{(l)} \sim q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)
$, so we treat $\mathbf{z}^{(l)}$ as a fixed random sample. By the way, I am not sure whether they used $q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)$ differently from $q_{\phi}\left(\mathbf{z} \right)$. I don't think so. I think they are the same and I think we can still back-propagate with respect to $\phi$, even if we use it to sample $\mathbf{z}^{(i)}$, because, once this latter is sampled, it can be treated as fixed (random) sample.I also note that I think they meant $\frac{1}{L} \sum_{l=1}^{L} f(\mathbf{z}^{(l)}) \nabla_{q_{\phi}\left(\mathbf{z}^{(l)}\right)} \log q_{\phi}\left(\mathbf{z}^{(l)}\right)$, i.e. they forget to use $\mathbf{z}^{(l)}$ rather than just $\mathbf{z}$ as input for $f$ in the Monte Carto estimate. You can see in section 3 of [BJP12] and section 4.2. of [1] they do it like this, and it makes sense. So, the VAE paper has sloppy stuff in there.My intuition of why this has high variance is because you sample something according to the variational distribution, but then you update this variational distribution, and you continuously do this. However, I am not sure this is the right intuition. In [BJP12], they say (in section 4) this MC estimate has high variance and that you need a lot of samples $\mathbf{x}$. I don't know exactly why this is the case because I didn't fully read this paper yet."
How to teach Machine Learning Agent to destroy replicating objects in a puzzle game?,"
I have an unusual but very interesting problem. I have a game that is very similar to Toon Blast (a puzzle mobile game). It's based on a Match-2 mechanic in which you can destroy 2 or more connected blocks and your goal is to complete all the required objectives (collect X color blocks, destroy 30 balloons, etc).
I have tons of levels and the ML solver seems to perform very well for all kinds of obstacles - except Quicksand.
Quicksand is a special object that replicates itself to a nearest tile whenever a user makes a move. If the user destroyed a quicksand in his turn, a quicksand won't be replicated. So basically the fastest way to destroy quicksand is to make sure you destroy as much quicksand as you can in each turn so it won't replicate and cover your board.
I use ML-Agents from Unity (https://github.com/Unity-Technologies/ml-agents) and I just give the agent reward=1f whenever it completes an objective (destroy 1 obstacle) and I subtract 1f from reward whenever it performs a move.
For simpler non-replicating obstacles it works perfectly. For example, you click 2 blocks next to a balloon - it will pop a balloon, add 1f as a reward and at the same time remove 1f for using a move.
This way the agent learns to make as few moves as possible.
Below you'll find how the Quicksand works and some simple obstacle - Balloon.
Quicksand preview (sorry for bad quality, 2mb max size)

Balloon preview

My issue is that no matter what, I can't teach it to solve quicksand. With the above rewarding approach, I think this strange creature learned that by actually REPLICATING quicksand it's gaining more reward because it can destroy it later (actually it's not because moves give -1f so it's more-less equal).
I've tried not giving reward for the quicksand, so it only loses rewards by using moves. But it doesn't work either, I'm not sure why.
Do you guys have any idea how this kind of things should be taught?
","['machine-learning', 'reinforcement-learning', 'game-ai', 'reward-functions', 'reward-design']",
Which computer scientists have received the Turing Award specifically for their contributions to Artificial Intelligence?,"
Many people have heard of Hinton, Bengio, and LeCun in recent years, given the popularity of deep learning and neural networks, and their contributions to this subfield of Artificial Intelligence. For their contributions, they have conjointly received the Turing Award in 2019 (although, in my view, a few other people could also have received this award for the same reasons).
In addition to them, which computer scientists have received the Turing Award specifically for their contributions to Artificial Intelligence?
For each of them, please, describe the specific reason why they were awarded and/or provide the links to the official site that announces this or the Turing lecture.
Why am I asking this question? Alan Turing is considered one of the fathers, if not the father, of Artificial Intelligence and Computer Science. In particular, in addition to the development of Turing machines, which are widely studied in Theoretical Computer Science and Theory of Computation, he's also published the famous paper Computing Machinery and Intelligence in 1950, where he proposed what was later called the Turing test, and asked one of the most fundamental questions in AI: ""Can machines think?"". The Turing Award is given to people that make significant contributions to CS or AI, so I think we should remember all these people that have contributed to AI.
","['history', 'ai-field', 'turing-award']",The descriptions below are copied from Wikipedia or the ACM site.
Is the capability of RNN more than the capability of MLP?,"
Consider the following excerpt paragraph taken from the section titled ""Recurrent Neural Networks"" of the chapter 10: Sequence Modeling: Recurrent and Recursive Nets of the textbook named Deep Learning by Ian Goodfellow et al on the computational graph of some computations.

The recurrent neural network of ..... is universal in the sense that
any function computable by a Turing machine can be computed by such a
recurrent network of a ﬁnite size. The output can be read from the RNN
after a number of time steps that is asymptotically linear in the
number of time steps used by the Turing machine and asymptotically
linear in the length of the input. The functions computable by a
Turing machine are discrete, so these results regard exact
implementation of the function, not approximations. The RNN, when used
as a Turing machine, takes a binary sequence as input, and its outputs
must be discretized to provide a binary output. It is possible to
compute all functions in this setting using a single speciﬁc RNN of
ﬁnite size. The “input” of the Turing machine is a speciﬁcation of the
function to be computed, so the same network that simulates this
Turing machine is suﬃcient for all problems. The theoretical RNN used
for the proof can simulate an unbounded stack by representing its
activations and weights with rational numbers of unbounded precision.

This paragraph clearly explains that RNN is capable of computing any computable function exactly and is the same as the Turing machine in terms of capability.
Afaik, MLP is capable of approximating any continuous, bounded function.
So, it seems to me that RNN is more powerful in terms of the capability of computing functions than MLP. RNN can learn any function that MLP can learn and RNN can learn more than that can be learned by any MLP in general.
Am I correct? Or is there any issue in my interpretation or do more details need to be considered?
","['comparison', 'recurrent-neural-networks', 'multilayer-perceptrons', 'universal-approximation-theorems', 'turing-completeness']",
Can I help my neural network if I know the sign of the relationships between inputs and outputs,"
I am attempting to train a neural network where I can say the following:
For most inputs, I know the sign of the relationship between that input and several specific outputs. I.e. whatever set of values the inputs are set to, I can point to individual input and say that so long as the other inputs are unchanged, the output should monotonically increase/decrease as I change this input's value.
This is not to say that the inputs don't interact with each other, they do, but the interaction never changes the sign (i.e. increasing vs decreasing) nature of the monotonic relationships between specific inputs and outputs.
I am wondering how I can use this information to help my network learn fast and/or perform well.
","['neural-networks', 'feedforward-neural-networks']",
Is there any relation between the recursive neural network and recurrent neural network?,"
Recurrent neural networks, abbreviated as RNNs, are widely used in deep learning literature, especially for text processing.
Are they related to recursive neural networks in any way?
I am asking for the general/special relationship that enables us to view the one in terms of another if possible.
","['comparison', 'recurrent-neural-networks']",
"Why do we use $q_{\phi}(z \mid x^{(i)})$ in the objective function of amortized variational inference, while sometimes we use $q(z)$?","
In page 21 here, it states:

General Idea of Amortization: if same inference problem needs to be solved many times, can we parameterize a neural network to solve it?
Our case: for all $x^{(i)}$ we want to solve:
$$
\min _{q(z)} \mathrm{KL}\left(q(z) \| p_{\theta}\left(z \mid x^{(i)}\right)\right.
$$
Amortized formulation:
$$
\min _{\phi} \sum \operatorname{KL}\left(q_{\phi}\left(z \mid x^{(i)}\right) \| p_{\theta}\left(z \mid x^{(i)}\right)\right)
$$

One thing I am trying to wrap my mind around is $q(z)$ in the 1st formulation vs $q_{\phi}(z \mid x^{(i)})$ in the second.
Why is one a conditioned on $x^{(i)}$ and the other is not? I know in the 1st formulation, we are trying to find a different $q(z)$ for each datapoint.
I also recall that in VAEs,which uses amortized inference we consider $q(z)$ to be aggregated posterior, like
$$q_{\phi}(z)=\int q_{\phi}(x, z) d x \quad \text { Marginal of } q_{\phi}(x, z) \text { on } z$$
$$q_{\phi}(x, z) \equiv p_{\mathcal{D}}(x) q_{\phi}(z \mid x)$$
(formulas taken from here)
","['variational-autoencoder', 'notation', 'variational-inference']",
When does an RNN use the connections that help in going backward in time?,"
Consider the following paragraph taken from chapter 10: Sequence Modeling: Recurrent and Recursive Nets of the textbook named Deep Learning by Ian Goodfellow et al mentioning the connections of RNN to go backward in time.

For the simplicity of exposition, we refer to RNNs as operating on a
sequence that contains vectors $x^{(t)}$ with the time step index $t$
ranging from 1 to $\tau$. In practice, recurrent networks usually
operate on minibatches of such sequences, with a diﬀerent sequence
lengthτfor each member of the minibatch. We have omitted the minibatch
indices to simplify notation. Moreover, the time step index need not
literally refer to the passage of time in the real world. Sometimes it
refers only to the position in the sequence. RNNs may also be applied
in two dimensions across spatial data such as images, and even when
applied to data involving time, the network may have connections
that go backward in time, provided that the entire sequence is
observed before it is provided to the network.

The paragraph says that the RNN can go back in time if and only if the entire sequence is provided. So, I am suspecting that it happens only during the backpropagation/backward pass.
Am I true? Or is it possible for an RNN to use those connections while forward pass also?
","['deep-learning', 'recurrent-neural-networks']",
"What does ""statistical strength"" mean in this context?","
Consider the following excerpt from a paragraph taken from chapter 10: Sequence Modeling: Recurrent and Recursive Nets of the textbook named Deep Learning by Ian Goodfellow et al regarding the advantages of RNN over full traditional MLP.

To go from multilayer networks to recurrent networks, we need to take
advantage of one of the early ideas found in machine learning and
statistical models of the 1980s: sharing parameters across diﬀerent
parts of a model. Parameter sharing makes it possible to extend and
apply the model to examples of diﬀerent forms(diﬀerent lengths, here)
and generalize across them. If we had separate parameters for each
value of the time index, we could not generalize to sequence lengths
not seen during training, nor share statistical strength across
diﬀerent sequence lengths and across diﬀerent positions in time. Such
sharing is particularly important when a speciﬁc piece of information
can occur at multiple positions within the sequence.

The authors used the phrase ""statistical strength"". Do they mean the strength of RNN in learning the embeddings of a word based on its context rather than its position in input, if it occurs in several inputs? Or does it mean that RNN uses fewer parameters to generalize in a better way compared to a traditional MLP? Or do they mean something else?
","['deep-learning', 'terminology', 'recurrent-neural-networks', 'statistics']",
Is a recurrent layer same as LSTM or single-layered LSTM?,"
In MLP, there are neurons that form a layer. Each hidden layer gives a vector of number that is the output of that layer.
In CNN, there are kernels that form a convolutional layer. Each layer gives feature maps that are the output of that layer.
In LSTM, there are cells that form a recurrent layer. Each layer gives a sequence that is the output of that layer.
This is my understanding of the basic terminology regarding MLP, CNN, and LSTM.
But consider the following description regarding the number of layers in LSTM in PyTorch

num_layers – Number of recurrent layers. E.g., setting num_layers=2
would mean stacking two LSTMs together to form a stacked LSTM, with
the second LSTM taking in outputs of the first LSTM and computing the
final results. Default: 1

The description uses the ""number of recurrent layers"" and the ""LSTM"" in a similar manner. How I can understand this? Is it costmary to consider a recurrent layer as an LSTM?
","['terminology', 'recurrent-neural-networks', 'long-short-term-memory']","Depending on the context, when people use the term LSTM, they either refer toIn TensorFlow, an LSTM is a layer, so you can stack multiple LSTMs to create deeper architectures.In PyTorch, the class LSTM can create an LSTM layer or multiple LSTM layers stacked together. You also have an LSTMCell, which should be just one LSTM layer.My answer here should also be useful."
"What is the best machine learning algorithm for clustering dots based on coordinates $(x,y)$ with consideration of weight of the points?","
I'm looking for a machine learning algorithm for clustering points based on their coordinates. Furthermore, I want to take into consideration the weights of each point. Suppose there is a weight in each point. Then we take the sum of the weight of all the points in a cluster. I want the sum in different clusters to be close and balanced. What algorithm is best for this? Any suggestion?
","['machine-learning', 'python', 'algorithm-request', 'clustering', 'scikit-learn']",
What is all necessary types of data for a bidirectional RNN to learn embeddings?,"
Bidirectional RNNs are used for generating the semantic vectors of the text at the sentence level and word level.
In order to train a CNN for the classification tasks, images, and labels/outputs are required in general.
If the data required is dependent on the algorithm used then please consider the algorithm that uses less possible types of data. Suppose if we need to perform a classification task on images: then in general, images with corresponding labels are expected. But, there may be algorithms that also work better with fewer labels available. But, the types of data in both the cases are the same: Images, labels.
I want to know such bare minimum types of data requirements for a bidirectional RNN. Afaik, a text file is enough for it to get semantic vectors since learning in text processing generally happens based on the distributional hypothesis.
Am I correct? If not, then what should be the other necessary data requirements for a bidirectional RNN to generate semantic embeddings?
","['recurrent-neural-networks', 'datasets', 'semantics', 'vector-semantics', 'bidirectional-rnn']",
"GANs: Why does iterative gradient descent sometimes optimise $\min_G \max_D V(D,G)$ and sometimes $\max_D \min_G V(D,G)$?","
For the following minimax equation for generative adversarial networks (GANs),
$$\min_G \max_D V(D,G) = \mathbb{E}_{\boldsymbol{x}\sim p_{data}(\boldsymbol{x})}[\log D(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{z}\sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log(1 - D(G(\boldsymbol{z}))]  $$
Goodfellow mentions in his paper (https://arxiv.org/pdf/1406.2661.pdf, page 3, 1st paragraph under equation 1) that in practice this minimax game is implemented iteratively (algorithm 1 in the paper).
In one of his tutorials (1:31:52 - 1:33:16) he mentions that $\min_G \max_D V(D,G)$ is desirable, whereas $\max_D \min_G  V(D,G)$ leads to mode collapse. Iterative gradient descent can ""sometimes act"" like the former, and sometimes like the latter.
I am confused about how the iterative method can sometimes act like min-max or max-min. What about gradient descent causes the change in behavior?
","['objective-functions', 'generative-adversarial-networks', 'gradient-descent', 'numerical-algorithms']",
Why are logarithms used in GANs minimax equation?,"
The minimax equation for generative adversarial networks
$$\min_G \max_D V(D,G) = \mathbb{E}_{\boldsymbol{x}\sim p_{data}(\boldsymbol{x})}[\log D(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{z}\sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log(1 - D(G(\boldsymbol{z}))]  $$
Why do we use logarithms instead of just
$$\min_G \max_D V(D,G) = \mathbb{E}_{\boldsymbol{x}\sim p_{data}(\boldsymbol{x})}[ D(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{z}\sim p_{\boldsymbol{z}}(\boldsymbol{z})}[(1 - D(G(\boldsymbol{z}))]  $$
","['objective-functions', 'generative-adversarial-networks', 'logarithm']","It's common in machine learning to do this log-trick, i.e. rather than optimizing $f(\mathbf{x})$, you optimize $\log f(\mathbf{x})$.There are 3 main reasons why we (can) do this.When your objective function is the product of multiple probabilities (or, more generally, small numbers), i.e. $f(\mathbf{x}) = \prod_{i=1}^N p(x_i)$, then $\log f(\mathbf{x}) = \log \left( \prod_{i=1}^N p(x_i) \right) = \sum_{i=1}^N \log p(x_i)$ (see this), which is more numerical stable because we got rid of multiplication of possibly very small numbers, which can lead to underflow.For the same reason, we can also compute derivatives more easily, as we can simply compute the derivatives of each component (because derivatives are linear)The logarithm is monotonically increasing, so $\log f(\mathbf{x})$ has the same optima as $f(\mathbf{x})$ (simple proof here)."
What is the meaning of the shaded area in the reinforcement learning literature graphs?,"
In most of the reinforcement learning literature, I see that there is a shaded area in the graphs. I couldn't understand what it exactly represents?
For example, from the A3C paper:

Or another example from the PPO paper:

Is it for multiple runs or it's for something else? How I can reproduce such graphs (which library and what type of data from my training episode do I need)?
","['reinforcement-learning', 'papers']","They train the agent multiple times, then plot the mean +- standard deviation of the agent performance (the shaded region is representing the standard deviation)."
How to train an ML model to convert the given lyrics into a song by a particular singer?,"
I am interested in training a machine algorithm to convert the lyrics I give into a song by a particular singer.
My language is non-English (south Indian) The songs are mostly monophonic (very few instruments, if at all). I have data of a bunch of songs sung by this singer, I want to try new lyrics and imagine how to singer would have sung.
","['machine-learning', 'reference-request', 'algorithm-request', 'model-request']","OpenAI used a modified version of VQ-VAE-2 combined with sparse transformers to do something similar to what you want to do. Their approach, called Jukebox, is able to produce music by conditioning on certain styles, lyrics, and artists, which you might explore to do what you want. You can explore here the produced songs. For example, the model was able to produce a hip-hop song that uses the lyrics of Eminem's Lose Yourself, but with a Kanye West's style. You can find the original research paper here and the associated codebase here."
"How is $Q(s', a')$ calculated in SARSA and Q-Learning?","
I have a question about how to update the Q-function in Q-learning and SARSA. Here (What are the differences between SARSA and Q-learning?) the following updating formulas are given:
Q-Learning
$$Q(s,a) = Q(s,a) + \alpha (R_{t+1} + \gamma \max_aQ(s',a) - Q(s,a))$$
SARSA
$$Q(s,a) = Q(s,a) + \alpha (R_{t+1} + \gamma Q(s',a') - Q(s,a))$$
How can we calculate the $Q(s', a')$ in both SARSA and Q-learning for updating the Q-function? After having taken an action $a$ at state $s$, we get the reward $r$, which we can observe. But we cannot observe $Q(s',a')$ from the environment as far as I see it.
Can anyone maybe think about a comprehensive example where you can see how it is done (or a link to a website)?
","['reinforcement-learning', 'q-learning', 'sarsa', 'bootstrapping']","It seems that your problem is that you think that we must know the true value of $Q(s', a')$ in order to perform the SARSA update. This is not the case! SARSA is a reinforcement learning algorithm, not a supervised learning algorithm (although you can also view RL as a form of SL).If you are familiar with supervised learning (SL), then you know that, to train a model, you need the ground-truth labels. The typical SL example is that of binary classification of dogs and cats. So, you are given an image of a dog or cat $x$, you pass it to your neural network $f$, which produces a prediction $\hat{y} = f(x)$. Now, if $x$ is a dog but $\hat{y}$ is cat, the neural network $f$ made a mistake. So, we need to change the weights of this model so that $\hat{y} = \text{dog}$ when $x$ is an image of a dog (of course, this reasoning also applies to the case when $x$ is an image of a cat). A typical way to solve this problem in SL is to use a loss function that computes some notion of distance between $\hat{y}$ (the prediction) and $y$ (the true label). The usual loss function, in this case, is the binary cross-entropy, but you don't need to know the details now.In reinforcement learning, you don't really have ground-truth labels, but you have experience, which is just the tuples $\langle s_t, a_t, r_{t+1}, s_{t+1} \rangle $, whereNow, in reinforcement learning, there are many problems that you may want to solve. However, the main goal of an RL agent is to maximize expected reward in the long run (known also as expected return), so you could say that your objective function is $$\mathbb{E} \left[ \sum_{t=0}^\infty R_t \right],$$
where $G = \sum_{t=0}^\infty R_t$ is the so-called return (i.e. the cumulative reward or reward in the long run). The goal is to maximize this expectation.In practice, what you do is ESTIMATE a so-called (state-action or just action) value function. In the case of SARSA, it's defined as $q: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, where $\mathcal{S}$ and $\mathcal{A}$ are respectively the set of states and actions of the environment (aka MDP).Why do you want to estimate a value function? In the case of $q$ (so SARSA and Q-learning), $q(s, a)$, for some $s \in \mathcal{S}$ and $a \in \mathcal{A}$, is defined as the expected cumulative reward that you will get from taking action $a$ in the state $s$. So, if you know that you will get more reward by taking action $a_1$ rather than action $a_2$ in state $s$, then $q(s, a_1) > q(s, a_2)$, so you will take action $a_1$ when in state $s$. In fact, you can also define $q(s, a)$ as follows $q(s, a) = \mathbb{E}\left[ G \mid s, a \right]$, where $G$ is our cumulative reward, aka return (for simplicity, I ignore a few details).HOWEVER, we do not (usually) know $q$. That's why we need Q-learning and SARSA, i.e. to estimate the state-action value function. So, in SARSA, you know $s'$ and $a'$ (read the pseudocode!), but we do not know the true value of $q(s', a')$. So, you say, but then why do we use it in the update of SARSA?The reason is: initially, SARSA uses possibly wrong estimates of $q$ to learn $q$ itself. We denote these estimates with the capital letter $Q$. So, we don't know the true value of $a'$ in $s'$. Or, more precisely, at the beginning of SARSA, if $q$ is implemented as a 2d array (or matrix), then $Q[s', a']$ is not a good estimate of the true value of $a'$ in state $s'$, i.e. $q(s', a')$. In other words, $Q[s', a'] \approx q(s', a')$.Now, you ask: why can we use a possibly wrong estimate, $Q(s', a')$, to compute $Q(s, a)$ (another estimate)? The idea of using possibly wrong estimates of the state-value function to update other estimates of the value function is present in all temporal-difference algorithms (including Q-learning): this is called bootstrapping. However, the specific reason why tabular SARSA converges to the true estimates is a different (although related) story (more info here).Now, if you didn't understand this answer, then you really need to pick up a book and read it carefully from the beginning. It takes time to understand RL at the beginning, but then it becomes easy. The most common textbook for RL is Reinforcement Learning: An Introduction by Sutton and Barto. You can find other books here."
Are the two policies in SARSA for choosing an action the same?,"
Here is the pseudocode for SARSA (which I took from here)

Are the two policies in SARSA for choosing an action equal? I guess yes, because it is called an on-policy learning algorithm. But could I, for example, also use different policies in this SARSA framework? For example an $\epsilon$-greedy policy and a softmax policy. Maybe the resulting algorithm would not be called SARSA anymore but it would be something similar.
","['reinforcement-learning', 'sarsa', 'on-policy-methods', 'exploration-strategies']",
Are we choosing the same action in every step in SARSA?,"
Here is the pseudocode for SARSA (which I took from here)

Do we only select one action at the very beginning and then we always choose the same action for each step? Does it really make sense to choose the same initially chosen action $a$ regardless of the state $s$?
","['reinforcement-learning', 'sarsa']","Do we only select one action at the very beginning and then we always choose the same action for each step?No.The pseudocode is clear on this, by using the word Choose and referencing a policy.If you were expected to take the same action again, then the pseudocode already has the previous action in variable a, so it would not need to state anything about making a choice or using a policy.The $a \leftarrow a'$ notation is common way to describe copying values*, so the variable a is changed at the end of each loop.Does it really make sense to choose the same initially chosen action $a$ regardless of the state $s$?Not in this case. Some learning algorithms do use a form of ""sticky"" exploration where a single exploratory action is committed to for multiple time steps. It can be useful in some environments. But not basic SARSA as described in the question.* It avoids the ambiguity of $=$ as assignment or equality operator. You may also see $:=$ for assignment as an alternative, but for instance Sutton & Barto use $\leftarrow$ consistently, and a lot of RL literature follows this convention."
Why are we choosing more than 1 action in SARSA?,"
Here is the pseudocode for SARSA (which I took from here)

Why are we choosing more than 1 action in SARSA? One for going into the next state and the other one for updating the Q function?
","['reinforcement-learning', 'sarsa']","Why are we choosing more than 1 action in SARSA?There is never a state where more than one action is chosen.The appearance of two Choose statements is an artifact of the loop design and variable management in the pseudocode.One for going into the next state and the other one for updating the Q function?Sort of. There is only ever one chosen action for each state.However, you need to have two actions in scope - the current action (just taken and observed) and planned next action, in order to process the update rule. Hence there are two variables a and a' and the code needs to generate one or other outside the main loop, or have some other way to ensure that it has access to current and next values. This is also why at the end of the loop, the ""next"" values $s', a'$ get copied to the ""current"" values $s, a$."
How to uniquely associate a directed graph with a feedforward neural network?,"
I want to write an algorithm that returns a unique directed graph (an adjacency matrix) that represents the structure of a given feedforward neural network (FNN). My idea is to deconstruct the FNN into the input vector and some nodes (see definition below), and then draw those as vertices, but I do not know how to do so in a unique way.
Question: Is it possible to construct such an algorithm, and if so, how would you formalize it?

Example [Shallow Feedforward Neural Network (SNN)]
To illustrate the problem, consider an SNN, defined as a mapping $f=\left(f_1(\mathbf{x}), \ldots, f_m(\mathbf{x})\right): \mathbb{R}^n\rightarrow\mathbb{R}^m$ where for $k=1,\ldots,m$
\begin{align}
        f_k(\mathbf{x}) &= \sum_{j=1}^{\ell} w_{j,k}^{(2)} \rho \left( \sum_{i=1}^n w_{i,j}^{(1)} x_i + w_{0,j}^{(1)} \right) + w_{0,k}^{(2)}, \quad \mathbf{x}=(x_1,\ldots,x_n)\in\mathbb{R}^n
\end{align}
and $w_{i,j}^{(k)}\in\mathbb{R}$ is fixed for all $i,j,k \in \mathbb{N}$ and $\rho:\mathbb{R}\rightarrow\mathbb{R}$ is a continuous mapping.
I want to determine the nodes that make up the FNN, where a node $N^{\rho}: \mathbb{R}^n\rightarrow\mathbb{R}$ is defined as a mapping
\begin{align} \label{eq:node}
        && \quad && N^{\rho}(\mathbf{x}) &= \rho\left(\sum_{i=1}^n w_i x_i + w_0 \right), & \mathbf{x}=(x_1,\ldots,x_n)\in\mathbb{R}^n
\end{align}
where $\mathbf{w}=(w_0, \ldots,w_n)\in\mathbb{R}^{n+1}$ is fixed.
Clearly (to me) I can write each $f_k$ as
\begin{align}
    f_k(\mathbf{x}) &= \sum_{j=1}^{\ell} w_{j,k}^{(2)} N^{\rho}_j(\mathbf{x}) + w_{0,k}^{(2)},
\end{align}
where $N^{\rho}_{j}$ is a node for $j=1,\ldots,\ell$. Now I see that $f_k$ is a node which takes as input the output of other nodes. But how can I formalise this in an algorithm? And does it generalize to Deep Feedforward Neural Networks?
","['neural-networks', 'feedforward-neural-networks', 'graphs']","I think you can do this in multiple ways.The easiest algorithm that comes to my mind right now produces a sparse (which is also some kind of block matrix) $N \times N$ adjacency matrix for a typical MLP/FFN with a total of $N$ neurons (including input and output neurons), where each neuron $n_l^k$ at layer $l$ has a directed edge that goes into all neurons at layer $l+1$.This is the algorithm.Create an $N \times N$ matrix $G \in \{0, 1\}^{N \times N}$ with zerosComment 1: $G_{ij}$ is the element of the matrix at row $i$ and column $j$.Comment 2: Indices $i$ and $j$ start at $1$ and end at $N$Comment 3: if we set $G_{ij} = 1$, then there's a directed edge from neuron $i$ to neuron $j$ (but not necessarily vice-versa: for that to be true, we would also need $G_{ji} = 1$)Comment 4: we need to create a mapping between the indices $i$ and $j$ and the neurons in the neural network; this is done below!Let $c(l)$ be the number of neurons at layer $l$For each layer $l = 0, \dots, L - 1$Return the matrix $G$The time complexity of this algorithm should roughly be $\mathcal{O}(L* {\max_l c(l)}^2)$. So, for example, for a neural network with 3 layers, 2 inputs, 5 hidden neurons, and 2 outputs, what would be the number of operations?"
"For the VAE, should the input, output and latent variable code be random variables?","
For a variational autoencoder, we have input $x$ (assume 1 data point for now, like an image), a latent code sampled from the decoder, $z$, and an output $\hat{x}$.
If I were to draw a diagram for the VAE with the input, output, and latent code sample, is it appropriate to write those three as random variables/vectors? Or as instances of random variables/vectors?
I thought it was random variables/vectors, but I saw this discussion, where they talk about the dataset being instances.
","['neural-networks', 'variational-autoencoder', 'random-variable', 'probabilistic-graphical-models']","The VAE attempts to model a specific probabilistic (directed) graphical model (Bayesian network)So, in this PGM, $\mathbf{z}$ and $\mathbf{x}$ are random variables. In principle, I think you could also model $\phi$ and $\theta$ as random variables (in Bayesian statistics, you can also model parameters as random variables and put priors on them).In practice, the VAE attempts to learn a generative model given a dataset. In fact, the technical part of the VAE paper (section 2.1) starts withLet us consider some dataset $\mathbf{X}=\left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N}$ consisting of $N$ i.i.d. samples of some continuous or discrete variable $\mathbf{x}$. We assume that the data are generated by some random process, involving an unobserved continuous random variable $\mathbf{z}$.Later, they use this dataset to define the likelihood$$\log p_{\theta}\left(\mathbf{x}^{(i)}\right)=D_{K L}\left(q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \| p_{\theta}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)\right)+\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)$$and, consequently, also the objective function (the Evidence Lower BOund, aka ELBO).$$
\mathcal{L}\left(\boldsymbol{\theta}, \phi ; \mathbf{x}^{(i)}\right)=-D_{K L}\left(q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \| p_{\theta}(\mathbf{z})\right)+\mathbb{E}_{q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)}\left[\log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right)\right]
$$Note that we use $\mathbf{x}^{(i)}$ in the formulas above, so the distributions are conditioned on the given samples/dataset and the likelihood function is defined in terms of the samples (that's what a likelihood usually is: a function of the parameters given the usually fixed data).In the other more extensive paper about VAEs (by the same authors of the VAE), you also have a diagram of the VAE.So, I think an answer to your question depends on what you actually want to show with the diagram. If you want to show how VAEs are trained, then you definitely need to show that we have a dataset. If your diagram is supposed to show the distributions that the VAE attempts to model, then you can probably use a PGM."
Does Godel's incompleteness theorems restricts the scope of connectionist-AI?,"
It is well-known that Godel's incompleteness theorems restricted the reachability of symbolic-AI, which is dependent on mathematical logic.
But, I am wondering whether it has any impact on the connectionist AI.
I don't think it has any impact on the capability of connectionist AI because of the following reasons I am aware of

Connectionist-AI is more focused on generalization and is not about mathematical logic..
Universal approximation theorem, contrary to Godel's incompleteness theorems says that connectionist-AI is capable of achieving all bounded-continuous functions. I am not sure about the implications of Godel's incompleteness theorems on either unbounded or discrete functions.

So, the incompleteness theorems seem to have no impact on the connectionist AI.
Do the theorems also restrict the reachability of connectionist-AI?
","['multilayer-perceptrons', 'symbolic-ai', 'universal-approximation-theorems', 'incompleteness-theorems']",
How does the skip connection match its dimension to the same layer in the expansive path?,"
According to the U-Net architecture image from the second page of the research paper (URL link) https://arxiv.org/pdf/1505.04597.pdf
How does the skip connection match its dimension to the same layer in the expansive path?
","['u-net', 'skip-connections']",
What model to train to restore MNIST test dataset,"
I came across this problem, and am not sure where to start. What model would work best for this problem and why?
Imagine the digits in the test set of the MNIST dataset (http://yann.lecun.com/exdb/mnist/) got cut in half vertically and shuffled around. Implement a way to restore the original test set from the two halves, whilst maximizing the overall matching accuracy.
","['machine-learning', 'classification', 'python', 'mnist']",
Are the capabilities of connectionist AI and symbolic AI the same?,"
The universal approximation theorem says that MLP with a single hidden layer and enough number of neurons can able to approximate any bounded continuous function. You can validate it from the following statement

Multilayer Perceptron (MLP) can theoretically approximate any bounded, continuous function. There's no guarantee for a discontinuous function.

We can express any MLP in terms of algebraic expressions. And the expressions can be considered as symbolic-AI.
So, can I infer that symbolic AI algorithms can theoretically approximate any bounded continuous function?
If not, then why can't there be a one-one mapping between MLP and symbolic-AI algorithm?
","['comparison', 'multilayer-perceptrons', 'symbolic-ai', 'universal-approximation-theorems']",
Reinforcement Learning applied to Optimisation Problem,"
Problem Statement: We are given an optimisation problem; with production centres, source airport, destination airports, transfer points and finally delivered to the customers. This is better explained in the following picture. 
Objective function 1:
Minimise costs = inventory costs + transportation costs + penalty costs + loading/unloading costs

Inventory costs = inventory cost at source airport + inventory costs at distribution centres

Transportation costs = cost of transporting cargo from production centre to source airport (via trucks) + cost of transporting cargo through itineraries (via flight) + cost of transporting cargo from distribution centre to transfer points (via trucks) + cost of transporting cargo from transfer point to customers (via drones)

Penalty costs = cost of operating flight routes and delay penalty costs

Loading/unloading costs = cost of loading cargo on trucks at production centres + cost of unloading cargo from trucks at the transfer point


Mathematical Solution (Using IBM CPLEX solver / Docplex): The complete python code (.ipynb file) with the formulation is present in this Google Drive Link. This gives an optimal solution.
Query: Is there any non-mathematical, non-formulation based method to solve this problem statement? Something on the lines of Reinforcement Learning? If any implementation is also provided, it will be icing on the cake.
","['reinforcement-learning', 'deep-learning', 'dqn']",
"What is meant by ""two action selections"" in SARSA?","
I have some difficulties understanding the difference between Q-learning and SARSA. Here (What are the differences between SARSA and Q-learning?) the following updating formulas are given:
Q-Learning
$$Q(s,a) = Q(s,a) + \alpha (R_{t+1} + \gamma \max_aQ(s',a) - Q(s,a))$$
SARSA
$$Q(s,a) = Q(s,a) + \alpha (R_{t+1} + \gamma Q(s',a') - Q(s,a))$$
I know that SARSA is for on-policy learning while Q-learning is off-policy learning. So, in Q-learning, the epsilon-greedy policy (or epsilon-soft or softmax policy) is chosen for selecting the actions and the greedy policy is chosen for updating the Q-values. In SARSA the epsilon-greedy policy (or epsilon-soft or softmax policy) is chosen for selecting the actions and for updating the Q function.
So, actually, I have a question on that:
On this website (https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html) there is written for SARSA

As you can see, there are two action selection steps needed, for determining the next state-action pair along with the first.

What is meant by two action selections? Normally you can only select one action per iteration. The other ""selection"" should be for the update.
","['reinforcement-learning', 'comparison', 'q-learning', 'sarsa']",
"If the model always underfits, do I really need a larger model?","
I train my neural network on random points generated for a data set that theoretically consists of approximately $1.8 * 10^{39}$ elements. I sample (generate) tens of thousands of random points on each epoch with uniform distribution. For every model I try, it appears that it cannot get past $10-12\%$ accuracy on the data, even if I increase the size of the model to over ten million parameters.
Each feature of the data set is of two Rubik's cube positions, and the corresponding label is the first move of a possible solution to solve from the first provided position to the second provided position within twenty moves.
So, it's a classification model for $18$ distinct classes, one for each of the possible moves on a Rubik's cube.  Seeing that it has $12\%$ accuracy (being greater than $1/18 \approx 5.6\%$) is nice because it does mean that it is learning something, just not enough.
I also notice that loss tends to go down to a hard minimum over many epochs, but accuracy stops increasing after only around ten epochs.  On epoch 36, it reached a loss of $2.713$, and it repeatedly comes back down to $2.713$, but never any lower even after 2000 epochs.
I concatenated a convolutional layer with a fully connected layer to use it as the first layer of the model.  Convolutional layers might not work for this as well as I'd hope, so I throw in the fully connected layer as a safeguard.  Some Keras code below:
inp_cubes = Input((2,6,3,3,1))

x = TimeDistributed(TimeDistributed(Conv2D(1024,(2,2),(1,1),activation='relu')))(inp_cubes)

output_face_conv = TimeDistributed(TimeDistributed(Flatten()))(x)
flatten_inp_cubes = TimeDistributed(TimeDistributed(Flatten()))(inp_cubes)
x = TimeDistributed(TimeDistributed(Concatenate()))((output_face_conv, flatten_inp_cubes))

x = TimeDistributed(TimeDistributed(Dense(1024,'relu')))(x)
x = TimeDistributed(TimeDistributed(Dropout(0.3)))(x)
x = TimeDistributed(TimeDistributed(Dense(1024,'relu')))(x)
x = TimeDistributed(TimeDistributed(Dropout(0.3)))(x)
x = TimeDistributed(TimeDistributed(Dense(1024,'relu')))(x)
x = TimeDistributed(TimeDistributed(Dropout(0.3)))(x)
x = TimeDistributed(TimeDistributed(Dense(1024,'relu')))(x)
x = TimeDistributed(TimeDistributed(Dropout(0.3)))(x) # face logits
x = TimeDistributed(Flatten())(x)
x = TimeDistributed(Dense(1024,'relu'))(x)
x = TimeDistributed(Dropout(0.3))(x)
x = TimeDistributed(Dense(1024,'relu'))(x)
x = TimeDistributed(Dropout(0.3))(x)
x = TimeDistributed(Dense(1024,'relu'))(x)
x = TimeDistributed(Dropout(0.3))(x) # cube logits
x = Flatten()(x)
x = Dense(1024,'relu')(x)
x = Dropout(0.3)(x)
x = Dense(1024,'relu')(x)
x = Dropout(0.3)(x)
x = Dense(1024,'relu')(x)

outp_move = Dense(18,'softmax')(x) # solution logits

I tried using only one of the two types of input layers separately, and nothing quite worked.
Loss is measured as categorical cross-entropy.  I make use of time-distributed layers so that each of the two Rubik's cube positions from the input is processed equivalently, except when determining how they relate.  I'm making sure to scale my data, and all that stuff.  It really seems like this should just work, but it doesn't.
Is there any way to increase the model's performance without using hundreds of millions of parameters, or is that actually necessary?
I would have thought that there would be some relatively simple correlation between positions and solutions, although it's hard for us to see as humans, so maybe this comes down to the Cayley diagram of the Rubik's cube group being innately random, as though they're prime numbers or something.
EDIT: I guess I really did just need a bigger neural network. This new on has 75 million parameters. The second image shows how that model is able to learn the data set quite easily. It takes a long time to process, though.

","['deep-learning', 'classification', 'deep-neural-networks', 'underfitting']",
What components of reinforcement learning influence the result the most?,"
I'm working on my thesis concerning a reinforcement learning problem and am trying to prioritise my time on different components of it:

Formalising the agent environment (like the design of state-, action-space and reward-structure)
Selection of learning algorithm
Selection of network architecture and size
Design of the training setup

It is an agent in a 3D environment with simulated physics (in Unity), the domain being a real-time strategical game. It is an environment with constraint training data, so sample efficiency is very important.
Now my question: I do anticipate that the design of the state- and action space will have a big impact on the training result, especially in this environment with little training data.
However, is there a way one can clearly prioritise what components will be the most important ones for an RL setting?
Time is limited, and, for me, as a beginner, it seems to be quite difficult to determine what component will be the most important one and needs the most focus. Testing only the hyper-parameters of a learning algorithm thoroughly will take in itself a long time. And obviously disregarding any component will result in bad results.
Is there a way to know on which component one should focus more?
","['reinforcement-learning', 'ai-design', 'hyperparameter-optimization', 'hyper-parameters', 'environment']","I don't think there's a strategy that applies to all cases. In some cases, the reward function may need to be carefully designed (e.g. a self-driving car), but, in other cases, the reward function might be quickly designed (e.g. chess) and other parts of the RL system may require more care.Here you can find tips to approach an RL problem. For example, one tip that I find useful, although probably obvious once you hear of it, is to compare your policy with a random policy. See also these tips."
Are the output dimensions of the first and second convolutional layer in YOLO paper correct?,"
I was reading the last version of the YOLO paper available in Arxiv, and I don't fully understand the output dimensions (I understand width and height, but not depth) of the first and second convolutional layers.

Shouldn't the output of the first layer be 112x112x64? Shouldn't the output of the second layer be 56x56x192? I thought (and this is the case after the 3rd layer) that the depth of the ouput of a conv layer is equal to the number of filters of this layer. This is why after the first conv layer (that contains 64 filters) I would expect an output depth of 64. And the same for the second conv layer that contains 192 filters, I would expect the output to have a depth of 192.
","['convolutional-neural-networks', 'architecture', 'yolo', 'convolution-arithmetic']","In any case anyone is struggling with the same problem. It seems that they were simply typos in the original paper. I have downloaded the author's framework Darknet, as well as the configuration and weight files for YOLOv1.Then, the architecture can be tested with one sample image using this command:The output of this command also prints the full architecure of the net. There, I see the expected depth values after the first (depth of 64) and second max layers (depth of 192):So it seems clear to me that the 192 and 256 depth values after the first and second max layer on the figure in the question are just typos."
What are the types of inputs used for RNN in literature given sentences?,"
Suppose there are $m$ sentences in a text file and the number of distinct words is equal to $n$. The goal is to get word embeddings using RNN.
We know that it is impossible to pass any word, which is in text format, as an input to RNN. We need to convert each word into some number and then pass it to the RNN to get word embeddings.
I know only the following method, if correct:

Assign an index to each word. So, the index ranges from $0$ to $n-1$.
Use the indices as input to RNN.

Is it the only technique used in the literature? If not, what are the names of other techniques that are used in the context of RNN encoders?
","['natural-language-processing', 'recurrent-neural-networks', 'word-embedding']",
"What does ""Gau"" in GauGAN stand for?","
GauGAN is a neural network architecture from NVIDIA that can create realistic images from semantic maps (and nowadays also textual descriptions).
","['deep-learning', 'terminology', 'generative-adversarial-networks', 'image-generation']","As you know, GauGAN is the following (from this post):GauGAN was a Microsoft Paint-style platform that let uses create landscape images, with the model then able to turn them into photorealistic images.So, it is a generative adversarial network (GAN) to create images. As it works like an artist, its authors named it GauGAN after ""Paul Gauguin"" who was a French Post-Impressionist artist (refer into this post)."
Is image machine translation done in two steps?,"
Suppose I have images of hand-written Japanese text. If I want to translate those images, would my ML algorithm be a 2-step model (for example, a CNN to convert the image into Japanese characters/tokens and then feed those tokens in an RNN)? Is this normally how it would be done, or is there an end-to-end solution?
","['convolutional-neural-networks', 'natural-language-processing', 'recurrent-neural-networks', 'machine-translation']",
Is there some way for us to know if the neural network internally finds an association between labels?,"
I have a question about the association between labels. Say my neural network performs multi-labeling in its output layer. Now, if one of the labels is for whether a person lives in city $X$, another for if they have a wiki article, and finally a third for if that person is a millionaire. Let us assume that these are highly correlated.
Is there some way for us to know if the neural network internally finds an association between these three labels?
","['neural-networks', 'knowledge-representation', 'representation-learning']",
What exactly is embedding layer used in RNN encoders?,"
I am reading about RNN encoders. I came across the following line from this code. And I am facing difficulty in understanding the theoretical details regarding it.
emb = self.drop(self.encoder(input))

The input is a tensor of shape $[32, 100]$. Here 32 is the batch size and 100 is the length of the sentence. Hundred elements are indices to the words (from the dictionary) that are used in the sentence. We can observe that the output emb is later passed to the rnn (LSTM/GRU) layer.
output, hidden = self.rnn(emb, hidden)

So, to me, it looks like that self.encoder is the necessary step while using the RNN encoder. So, I am interested in what it actually does.
When we see about self.encoder, it is an Embedding layer. The description for this layer is as follows

A simple lookup table that stores embeddings of a fixed dictionary and
size.
This module is often used to store word embeddings and retrieve them
using indices. The input to the module is a list of indices, and the
output is the corresponding word embeddings.

When we see about self.drop, it randomly keeps zero in the embeddings.

During training, randomly zeroes some of the elements of the input
tensor with probability p using samples from a Bernoulli distribution.
Each channel will be zeroed out independently on every forward call.

The outputs for both self.encoder(input) and self.drop(self.encoder(input)) are $[32, 100, 3000]$.
I have doubt(s) on the bolded parts of the description of the Embedding layer. The description is saying the Embedding layer uses/contains(?) a lookup table. The description says Embedding layer stores and retrieves word embeddings.
The doubts are

Generally, does an embedding layer calculate word embeddings or just store and retrieve them from the table? If it does not calculate them, then who will calculate the embeddings? If you can also comment on the specifics of PyTorch, I would appreciate it.

What exactly is an embedding layer? Is it a collection of neurons or any other?


","['deep-learning', 'recurrent-neural-networks', 'pytorch', 'word-embedding']",
"What is a ""mask"" in the context o RNN-based encoders?","
While reading source code related to RNN encoders, I've come across the term mask as input to the encoder. What exactly is it?
","['deep-learning', 'terminology', 'recurrent-neural-networks', 'encoder-decoder']",Masks in Recurrent Neural Networks are used to transform variable-length inputs to one general length. Therefore we use padding and masking together.Padding: Usually we create a vector for every sentence in the dataset initialized with 0s and the length of the longest sentence in the dataset. Then we fill the mask with 1s for every position the sentence has words in.Masking: Now we need to inform the network that some values in the vector are actually padding and should not be used since they have no information.
"What does ""at inference time"" on Tesla's cars mean?","
I've watched Tesla AI Day 2021 and there was a question Tesla staff tried to answer, but I did not quite understand the question (Note: quote taken from autogenerated subtitles, I do not hear differently, but may you will):

or you'd be training a lot more complex models which would be
potentially significantly more expensive to run at inference time on
the cars

I've found a definition of ""inference time"" in How to Optimize a Deep Learning Model for faster Inference?

The inference time is how long is takes for a forward propagation

But what does ""AT inference time on the cars"" mean? Is it just badly worded, or does this ""at"" actually add proper meaning? Also, does it make sense to run training models on the cars themselves and what can that phrase mean? Overall I do not make sense of the question. Do you?
Note: I'm not a native English speaker.
","['machine-learning', 'terminology']","""At inference time"" means ""when you perform inference"". If ""inference"" is a synonym for ""forward pass"" (aka ""forward propagation"") (which is not always the case in ML), then ""at inference time"", again, means ""when you perform the forward pass"". ""At"" is just a preposition in English and it's often associated with location or time.So, the sentenceyou'd be training a lot more complex models which would be potentially significantly more expensive to run at inference time on the carscan be rewritten as followsyou'd be training a lot more complex models which would be potentially significantly more expensive to run when performing the forward pass (i.e computing the predictions, for example, whether the traffic light is green or red) on the cars"
What is a policy training target in AlphaZero?,"
In AlphaZero's attached pseudocode, they create a training target for the policy network in this way.
def store_search_statistics(self, root):
    sum_visits = sum(child.visit_count for child in root.children.itervalues())
    self.child_visits.append([
        root.children[a].visit_count / sum_visits if a in root.children else 0
        for a in range(self.num_actions)
    ])

In other words, the training target probability for a certain move is proportional to its visit count.
However, in the paper, they describe the usage of softmax sampling of visit counts with temperature. This temperate is equal to 1 for the first 30 moves (in this case the policy training target is the same as in the pseudocode above) and for subsequent moves they set infinitesimal temperature -> 0, which essentially means they are picking the move with the highest visit count.
Since these are 2 different things (if the game has more than 30 moves), my question is: which approach should be used for creating the training target for the policy?
","['reinforcement-learning', 'training', 'policy-gradients', 'monte-carlo-tree-search', 'alphazero']","The training target for the policy is always exactly the one described by the pseudocode; distribution proportional to visit counts, without any other kind of scaling.The softmax sampling with the temperature (different for first 30 moves than after that) is not used for the policy target, but is used for the ""real"" move selection by the agent in the self-play game (i.e. after it has run its search of 800 or 1600 or however many iterations it was for the root game state encountered in the self-play game).Generally, in my experience, you really wouldn't want the policy target to become too close to deterministic, except if really your entire MCTS already is extremely convinced about one move being clearly the best one all by itself (without getting pushed towards something more deterministic by a low-temperature softmax). If your policy becomes too deterministic too quickly, it destroys the exploration that we normally want MCTS to do inside its selection phase. Using a low-temperature softmax for the policy training target would introduce quite a big risk of this happening."
How can a neural network distinguish a rotated 6 and 9 digits?,"
Rotated MNIST is a popular dataset for benchmarking models equivariant to rotations on $\mathbb{R}^2$, described by $SO(2)$ group or its discrete subgroups like $\mathbb{Z}^{n}$:

Group equivariant convolutional networks
Harmonic networks

It consists of all digits from 0 to 9 rotated on an arbitrary angle from $[0, 2 \pi)$. However, what makes me a bit puzzled is that digits $6$ and $9$ seem to be confused by any learning algorithms, since from the view of human perception $6$ rotated by 180 degrees is equivalent to $9$ and vice versa.
The original paper in the description of Rotated MNIST doesn't comment on this point at all, which is strange, since it is a very natural question to ask.

In the paper Oriented Response Networks - authors plot embeddings of rotated digits projected via t-SNE on a 2d plane. There is a clear separation between all rotated versions of 6 and the rotated version of 9 for ORN.

I do not understand how it can be achieved? Probably, the networks understand much more in writing the digit, there are some subtle features, inaccessible to humans, but recognizable by powerful classifier?
","['neural-networks', 'classification', 'papers', 'features', 'mnist']",
"Writing a loss function for ""how far can this output be pushed""","
I'm trying to train a function for a industrial-process-control-like system. This is my first attempt at a custom training, so feel free to point out any invalid assumptions.
I've got one input and one controlled output, which I'm trying to optimise. I've reduced the problem with some values normalisation to:

the first half of input looks like a sin() raise from 0 to max value then dropoff - with lots of noise on top (let's say up to +/-10% at each measurement)

I don't know what the max is, but it's roughly predictable (input goes from 0 to between 0.5 and 2)

the output cannot go down (well, it can by a tiny bit, but I'd ignore that here), and cannot go higher than the input value

the goal is to get the output value as high as possible


Currently the best non-NN approach I've got is to start a few % below input and at each step run output = A*input + (1-A)*previous_output, so the result looks like this (input in blue, output in orange)

I wanted to check if some RNN can improve on this, so I'm planning to check an LSTM doing this instead. I'm struggling to come up with a loss calculation which is viable for training here.
I considered making the input and output for the network an absolute change from the previous value (or input as input-last_output difference), then as the loss using some kind of inverted value of sum(output changes) from step 0 to the crossover point period to reward higher maximums while ignoring the distance. (so discarding anything that happens past the crossover)
But... that doesn't relate to the input really, so tensorflow wouldn't be able to train based on this, if I understand it correctly. Am I going in the wrong direction? Are there some known ways to solve this problem?
","['recurrent-neural-networks', 'loss']",This turns out to be less about the loss function and more about the approach. There's a number of them implemented in the tf_agents package.Choosing the DDPG agent for outputting the increments and implementing a PyEnvironment to run the simulation and return the highest reached output as a reward seems to work just fine. Very similar to the blackjack example.
Why is the simplest U-Net architecture giving the best (but not good enough) results on a multi-class segmentation on microscopic data?,"
Currently, I'm trying to optimize a training process of a neural net to improve final results. The problem I'm dealing with is multiclass segmentation on microscopic data.
The paradox is that the best (and not sufficient) result is giving the simplest U-Net architecture on the original dataset. If I try a deeper or more complex model (e.g. r2unet), the final segmentation is significantly worse. If I try on the fly augmentation - worse as well. Changing a complex model into a more shallow one didn't help either (just tried out the other way than making it more complex).
Now, I'm trying to make a custom loss function work to improve the segmentation.
Any ideas what might be a root cause? Or any other ideas that could improve the result?
To get more specific, here's an example of the data. The initial 4000x4000 images are cut to 512x512, which results in a little over 3700 images. Most of them don't include the classes and is just background, that's why I'm trying to make another loss functions work, as well as weighted classes.



So far, I'm using categorical cross-entropy as a loss function, however, dice, Jaccard, the focal losses seem could be more suitable and once I'll finish my computations I'll try to make these work again, so far my tries weren't really compatible with Keras, at least it seemed.
The size of U-Net

depth = 5
first conv layer has 64 filters, goes up to 1024.

R2U-Net

depth 5
first layer 64filter (tried also 32)

","['tensorflow', 'keras', 'image-segmentation', 'u-net', 'categorical-crossentropy']",
Which pre-processing steps are necessary for Deep Learning models to solve a document classification problem?,"
I have created a data set with 30.000 text documents (each text file is rather small with respect to its length), which are labelled with 0 and 1. Using this data set, I want to train machine learning and deep learning models in order to be able to classify new text files.
On the one hand, I want to use classical machine learning models (such as logistic regression, random forest, SVM, etc.) with the Bag of Words/TF-IDF approach. This requires extensive text pre-processing, such as tokenization, stemming, converting to lower case, removing of stopwords and punctuation, lemmatization, etc.
On the other hand, I want to use new deep learning models (such as RNN, LSTM, BERT, XLNET, etc.).
Which pre-processing steps are necessary/advantageous for these deep learning models? Should I also use tokenization, stemming, converting to lower case, removing of stopwords and punctuation, lemmatization, etc. or can I omit most of these steps?
","['deep-learning', 'natural-language-processing', 'data-preprocessing', 'text-classification', 'binary-classification']",
What is the difference between an on-policy distribution and state visitation frequency?,"
On-policy distribution is defined as follows in Sutton and Barto:

On the other hand, state visitation frequency is defined as follows in Trust Region Policy Optimization:
$$\rho_{\pi}(s) = \sum_{t=0}^{T} \gamma^t P(s_t=s|\pi)$$
Question: What is the difference between an on-policy distribution and state visitation frequency?
","['reinforcement-learning', 'comparison', 'sutton-barto', 'trust-region-policy-optimization', 'on-policy-distribution']",
How does the distribution of the parameters change in logistic regression?,"
I have my own data to train a logistic regression model (for a multi-class classification task), and I want to know how the distribution of weight parameters changes after each update with gradient descent.
For example, let's say that there are $f$ many features for each input, and the weight $W$, which is a $c \times f$ matrix, where $c$ is a number of classes, is initialized with uniform distribution $U(-1/\sqrt{f}, 1/\sqrt{f})$, which is LeCun uniform initialization.
For each step of gradient descent with Cross-Entropy loss, it will be updated as
$$
W_{t+1} = W_{t} - \alpha \frac{\partial \mathcal{L}}{\partial W_{t}}
$$
where $\alpha$ is a learning rate and the gradient is given
$$
\frac{\partial \mathcal{L}}{\partial W_{t}} = \frac{1}{n} (\mathbf{y} - \mathbf{p})^{T}\mathbf{X}
$$
where $\mathbf{X} \in \mathbb{R}^{n\times f}$ is an input matrix, $\mathbf{y} \in \mathbb{R}^{n \times c}$ is one-hot encoded labels, and $$\mathbf{p} = \mathrm{softmax}(\mathbf{X}W_{t}^{T}) \in \mathbb{R}^{n \times c}$$
is the model's output (predicted probability for each example & class).
What I want to know is how the distribution of $W_{t}$ changes as $t$ increases, if some information about $\mathbf{X}$ is known. More precisely,

Is it possible to get a bound of $\mathbb{E}[||W_{t}||_{\infty}]$ in terms of $t, \alpha, ...$? We may assume that the input $\mathbf{X}$ is also bounded (in the sense that $||\mathbf{X}||_{\infty} \leq M$ for known $M$.) I have a really rough bound for $||W_{t}||_{\infty}$, but it is not good enough.
Are there any works in this direction for other models, such as MLPs?

When I plotted the value $||W_{t}||_{\infty}$ for each $t$, then it seems that it increases sub-linearly in $t$, but
","['machine-learning', 'reference-request', 'weights', 'probability-distribution', 'logistic-regression']",
What specifically is the gradient of the log of the probability in policy gradient methods?,"
I am getting tripped up slightly by how specifically the gradient is calculated in policy gradient methods (just the intuitive understanding of it).
This Math Stack Exchange post is close, but I'm still a little confused.
In standard supervised learning, the partial derivatives can be acquired specifically because we want to learn about the derivative of the cost with respect to input parameters and then adjust in the direction of minimising this error.
Policy gradients are the opposite, as we want to maximise the likelihood of taking good actions. However, I don't understand what we are getting partial derivatives with respect to - in other words, what is the 'equivalent' of the cost function, specifically for $\nabla_\theta \log\pi_\theta$?
","['reinforcement-learning', 'objective-functions', 'policy-gradients', 'gradient']","I would recommend not trying to think of this in relation to supervised learning.The policy $\pi(\cdot; \theta)$ is simply a function that is parameterised by $\theta$. If we take a $\log$ of this function, it is still just a function. We want to take the (partial) derivative(s) of this function with respect to the parameters so that we can perform a gradient ascent step on the parameters.A simple example can be shown by letting $\pi(a; \alpha, \beta) = \exp(\alpha + \beta a)$. In the policy gradient theorem we must first take a log of the policy which would give us $\log(\pi(a; \alpha, \beta)) = \alpha + \beta a$, and the partial derivatives wrt to the parameters are $\nabla_\alpha \log(\pi(a; \alpha, \beta)) = 1$ and $\nabla_\beta \log(\pi(a; \alpha, \beta)) = a$. We can then use these partial derivatives to perform a gradient ascent update in the direction of the gradient of our objective (the value function, which is of course what we want to maximise) for $\alpha$ and $\beta$ for a given return $G_t$ and action $a_t$ by
\begin{equation}
\alpha' = \alpha + G_t \times \nabla_\alpha \log(\pi(a_t; \alpha, \beta)) = \alpha + G_t \times 1 \; \\
\beta' = \beta + G_t \times \nabla_\beta \log(\pi(a_t; \alpha, \beta)) = \beta + G_t a_t\;.
\end{equation}In practice, however, you're likely to need a much more complex function for the policy, typically a neural network of some description. However, everything translates to these more complex functions, you're just going to have many more partial derivatives to calculate."
"Why is BERT/GPT capable of ""for-all"" generalization?","
As shown in the figure:

Why does token prediction work when ""Socrates"" is replaced with ""Plato""?
From the point of view of symbolic logic, the above example effectively performs the logic rule:
∀x. human(x) ⇒ mortal(x)

How might we explain this ability?  Moreover, how is this learned in just a few shots of examples?
I think this question is key to understanding the Transformer's logical reasoning ability.
Below are excerpts from 2 papers:


","['transformer', 'bert', 'logic', 'gpt', 'reasoning']",
Are there any recommendations on initialising a single parameter in deep learning?,"
I want to initialize a parameter, which is a single real number in my model. If you want the role of the parameter in the model, you can assume it as the parameter to multiply with the output of the neural network and the resultant product will be the final output.
Does initialization value matter? If yes, are there any guidelines on initializing the single parameter?
","['deep-learning', 'weights', 'weights-initialization']",
Fine tuning BERT for token level classification,"
I want to try self-supervised and semi-supervised learning for my task, which relates to token-wise classification for the 2 sequences of sentences (source and translated text). The labels would be just 0 and 1, determining if the word level translation is good or bad on both the source and target sides.
To begin, I used XLMRoberta, as I thought it would be best suited for my problem. First, I just trained normally using nothing fancy, but the model overfits after just 1-2 epochs, as I have very little data to fine-tune on (approx 7k).
I decided to freeze the BERT layers and just train the classifier weights, but it performed worse.
I thought of adding a more dense network on top of BERT, but I am not sure if it would work well or not.
One more thought that occurred to me was data augmentation, where I increased the size of my data by multiple factors, but that performed badly as well. (Also, I am not sure what should be the proper number to increase the data size with augmented data.)
Can you please suggest which approach could be suitable here and if I am doing something wrong? Shall I just use all the layers for my data or freezing is actually a good option? Or you suspect I am ruining somewhere in the code and this is not what is expected.
","['neural-networks', 'machine-learning', 'deep-learning', 'natural-language-processing', 'bert']",
How to assign tasks to users with ranking?,"
I'm trying to write an automatic assignment algorithm for the following problem:
I have $N$ tasks and $M$ users. For each task, I have a ranking for each user for ""how related it is to that user"". The ranking is a floating-point number in the [0, 1.0] range. The sum of all ranks is 1. I need to write an algorithm that will create the overall based assignment for all tasks. It has 2 constraints:

Overall best correctness of assignment.
Properly balanced - I can't have 1 user assigned 20 tasks while all others are assigned 1-2.

So far, from studying multiple input sets, I found that a task that has a user with a rank over 0.7 should be assigned to that user no matter what, because it's a really strong indicator of a correct assignment. After that, I tried to balance all the remaining tasks.
So, for the input of:

tasks = [t1, t2, t3,... , tn]
task_user_scores = [[s11, s12,...s1m], [s21, s22, ..., s2m],..., [sm1, sm2, ..., smm]]
first_iteration_limit = float, [0, 1.0], I use 0.7, as I described earlier.
per_user_limit = int, how many tasks to strive to assign per user.

I do:
Step 1
for ti in tasks 
    if some sij > 0.7 
        assign ti to user j

Step 2
for ti in unassigned tasks 
    compute user deviation(max deviation is abs(n - per_user_limit), udj is abs(assigned_to_j - per_user_limit) from per_user_limit(ud), 
    compute adjusted scores as follows sij * (1 - udj). 
    Select the user from the list using weighted random, if selected user already has more assigned tasks than per_user_limit, select again(the random select happens a finite number of times, I found that 10 is fine). 
    Otherwise, if the user didn't reach per_user_limit assign ti to user j.

Step 3
for ti in unassigned tasks
    assign ti to the user in the top 5 with the least number of tasks assigned to it.

",['constrained-optimization'],
What kind of NN to use to find misprints in test,"
I have a bunch of unique full names of users. I made pseudo-physical model to emulate misprints of desktop and mobile users (hence, fatfingering, jumpy fingers, accidentals touches of touch bar etc.)
So, I have pairs like John Snow - joh Snown
I tried first Recurrent networks, LSTM, like some kind of vocabulary to ""translate"" from bad words to good ones, but it return only known predicted result, and when I try to put unknown last names, it returns wrong results.
I wish to find some patterns in misspelled words, and to predict correct spelling.
Can you please advice some kind of NN to cope with the task, or maybe some contributions in that domain?
P.S. Yes, I know that there exist other AI methods to get things done
P.P.S. This vocabulary is not in English, just in case
UPDATE
LSTM nn works nice with known names and last names endings for new last names. Right now I use 2 different nn, first to correct mistypes, second to determine first, last and middle name.
UPDATE2
Sequence to sequence solution also can normalize name (put them in order), find sex of person, find probability of error, etc.
","['neural-networks', 'natural-language-processing', 'recurrent-neural-networks', 'long-short-term-memory']",
What's the best way to train data with unbalanced targets?,"
Suppose I have data I want to use for supervised learning, but there is a pretty bad target/class/labels imbalance. Should I:

Limit the size of the training set to make sure there is a flat target/class balance distribution (the training set is designed such that there is an equal number of training samples for each class based on splitting the lowest-occuring class as high as possible). For example, if my lowest-occuring class appears only 50 times in my data, and I want an 80-20 train-test split, then I decide I take 40 of the samples for training, and for an even target balance, take 40 samples for all other samples in training, even if the highiest-occuring class appears 100,000 times, for instance.

Ignore target balance and just focus on the ratio for the train and testing split. So, if it's 80-20, take 40 of the samples out of 50 for my lowest-occuring class, and 80% of 100,000 for my highest occuring class, and so on.

Something else?


Let's suppose I can't just get more data. I know there's some stuff to be said regarding undersampling and oversampling, but what can I do to tell if either one is working better if model accuracy might be disingenuous?
","['supervised-learning', 'multi-label-classification', 'imbalanced-datasets']",
Predict placement of an object in 3D space,"
I am trying to find a way to train a model to predict the correct placement of entities like a tree, dog and cat in a natural 3D environment. Any help regarding how I could use textual data to learn correct placement of objects in 3D space would be a great help. I am a little lost on how to approach this problem.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'computer-vision']",
Why is Thompson Sampling considered a part of Reinforcement Learning?,"
I often see Thompson Sampling in RL literature, however, I am not able to relate it to any of the current RL techniques. How exactly does it fit with RL?
","['reinforcement-learning', 'thompson-sampling']","Thompson Sampling (TS) is used in the context of bandits, which is a special case of the RL problem.You can also use TS for the full RL problem, but that can lead to inefficient exploration. To know more about this issue, you could read"
Neural network for recognizing ship types based on location series,"
I am building a neural network for recognizing ship types based on a 1000-long series of location data (latitude-longitude, normalized to account for different km/longitude° metrics, so that vector difference yields a consistent distance). The dataset I use consists of around 100 000 distinct day-ship pairs, classified by a 10-valued labeling. The cardinality of the classes are: 30115, 26327, 12798, 10940, 5859, 4211, 4176, 3639, 3521, 2834
I tried two different approaches:

A recurrent network (using LSTM): Dense[relu] -> LSTM -> Dense[relu] -> Dropout -> Dense[softmax]
A 1D convolutional network: Dense[relu] -> Conv1D -> Dense[relu] -> Dropout -> Dense[softmax]

I experimented with the hyperparameters of the above networks, but they all converge to a 40% accuracy, where the model classifies all inputs as class 0 or 1 (choosing the most likely class of the output layer).
I could accept that the data is not well-defined and this kind of prediction is impossible, but the strange thing is that even if I give the same data as training and validation, the model stops getting better at the 40% accuracy mark. Shouldn't it go further, and ""memorize"" the classes in this case, resulting in ~100% accuracy on the training data?
","['convolutional-neural-networks', 'recurrent-neural-networks', 'time-series']",
Best way to resize 3d to 2d matrix,"
I have a (5, 128, 768) matrix, that is, I have 5 embedding spaces of shape (128, 768). Since they all keep a relation, and for the sake of my model, I need to combine them into a unique output: (1, 128, 768*5). If I just concat them all along axis=-1, will I be losing some info?
Making that concatenation is the only way I can think of solving this. Is there any better option?
","['deep-learning', 'embeddings']","I think if you want to resize and reduce your matrix size, you can use one of the dimension reduction techniques.Here there is a link that may be helpful to you."
What exactly do gradient-based saliency map tell us?,"
As far as I understand, gradients are supposed to tell us 1) the magnitude and 2) direction, to update a parameter such as to minimize the loss function.
Regarding saliency maps, which use gradients with respect to the input, do the gradients give us the same information?
Consider vanilla saliency maps [1] (i.e. gradients-only) and integrated-gradients [2] (using a baseline image), with grayscale images.
Do the (vanilla) gradients give us the amount and direction a pixel-value needs to change? OR does the magnitude tell us for the amount of in loss based on a minimal change in pixel-value?
In simpler terms: does magnitude signify:

amount of change required in a pixel-value to have some change (in loss?) or

amount of change in (loss?) based on a minimal/local change in pixel-value?


","['image-recognition', 'explainable-ai', 'gradient', 'saliency-map']",
How can Siamese Neural Networks accept a variable number of inputs?,"
Traditionally, Siamese Neural Networks have two inputs. With some tweaking, you can get them to accept any number of inputs. What I don't understand is how to get them to accept variable numbers of inputs. I've seen a couple of research papers (most notably this one) where they talk about doing this, but none explain exactly how.
Could someone please explain how to create a Siamese Neural Network with a variable number of inputs?
","['neural-networks', 'machine-learning', 'siamese-neural-network', 'input-layer', 'network-design']",
What would be a reasonable option for clustering for unknown number of clusters and a lot of outliers?,"
I am implementing the CV detection pipeline with the use of SIFT and KNN Matcher.
Image keypoints matched to the query keypoints produce the following image:

The matched objects have a lot of key points on them and there are some false matches. I would like to consider spots with a lot of matches as detections of the query object and ignore isolated points.
What would be an appropriate clustering method, where one can put a limit on points in a neighborhood of some radius to declater this set of points a cluster?
KMeans is not a good idea, since it takes a fixed number of points and doesn't throw outliers.
From the algorithms proposed in sklearn, seems like DBSCAN and Agglomerative clustering are a good choice, since they allow for variable number of clusters, unknown apriori and outlier removal.
Or there is better alternative?
","['computer-vision', 'object-detection', 'algorithm-request', 'clustering', 'sift']",
Do we use two distinct layers to compute the mean and variance of a Gaussian encoder/decoder in the VAE?,"
I am looking at appendix C of the VAE paper:
It says:

C.1 Bernoulli MLP as decoder
In this case let $p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})$ be a multivariate Bernoulli whose probabilities are computed from $\mathrm{z}$ with a fully-connected neural network with a single hidden layer:
$$
\begin{aligned}
\log p(\mathbf{x} \mid \mathbf{z}) &=\sum_{i=1}^{D} x_{i} \log y_{i}+\left(1-x_{i}\right) \cdot \log \left(1-y_{i}\right) \\
\text { where } \mathbf{y} &=f_{\sigma}\left(\mathbf{W}_{2} \tanh \left(\mathbf{W}_{1} \mathbf{z}+\mathbf{b}_{1}\right)+\mathbf{b}_{2}\right)
\end{aligned}
$$
where $f_{\sigma}(.)$ is the elementwise sigmoid activation function, and where $\theta=\left\{\mathbf{W}_{1}, \mathbf{W}_{2}, \mathbf{b}_{1}, \mathbf{b}_{2}\right\}$ are the weights and biases of the MLP.
C.2 Gaussian MLP as encoder or decoder
In this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:
$$
\begin{aligned}
\log p(\mathbf{x} \mid \mathbf{z}) &=\log \mathcal{N}\left(\mathbf{x} ; \boldsymbol{\mu}, \boldsymbol{\sigma}^{2} \mathbf{I}\right) \\
\text { where } \boldsymbol{\mu} &=\mathbf{W}_{4} \mathbf{h}+\mathbf{b}_{4} \\
\log \sigma^{2} &=\mathbf{W}_{5} \mathbf{h}+\mathbf{b}_{5} \\
\mathbf{h} &=\tanh \left(\mathbf{W}_{3} \mathbf{z}+\mathbf{b}_{3}\right)
\end{aligned}
$$
where $\left\{\mathbf{W}_{3}, \mathbf{W}_{4}, \mathbf{W}_{5}, \mathbf{b}_{3}, \mathbf{b}_{4}, \mathbf{b}_{5}\right\}$ are the weights and biases of the MLP and part of $\boldsymbol{\theta}$ when used as decoder. Note that when this network is used as an encoder $q_{\phi}(\mathbf{z} \mid \mathbf{x})$, then $\mathrm{z}$ and $\mathrm{x}$ are swapped, and the weights and biases are variational parameters $\phi$.

So, it seems like, for a Bernoulli decoder, it only outputs a vector $\mathbf{y}$, which gets plugged into the log-likelihood formula.
But then, for the Gaussian decoder, it outputs both $\boldsymbol{\sigma}$ and $\mu$. So, is it like 2 parallel layers, one calculating  $\boldsymbol{\sigma}$  one calculating $\mu$?
Similar to how we get the $\mu$ and $\sigma$ of the encoder (which I am assuming the encoder ones are different from the decoder ones)?
And we plug it into the formula I derived in this link
here,  the log-likelihood to get the reconstruction loss?
This is the intuition I am getting, but I haven't seen it explicitly all in one place.
","['deep-learning', 'papers', 'variational-autoencoder', 'variational-inference']","Yes, in the case of the Gaussian, you have two distinct layers (so weights and biases), one for the mean and the other for the variance, as the equations are telling us.The mean is calculated with the weights $\mathbf{W}_{4}$ and bias $\mathbf{b}_{4}$ from $\mathbf{h}$ as follows$$\boldsymbol{\mu} =\mathbf{W}_{4} \mathbf{h}+\mathbf{b}_{4},$$while the variance (actually, equivalently, the log of the standard deviation) is calculated from $\mathbf{W}_{5}$ and $\mathbf{b}_{5}$ from $\mathbf{h}$ as follows$$\log \sigma^{2} =\mathbf{W}_{5} \mathbf{h}+\mathbf{b}_{5}$$Here you have a PyTorch implementation that uses 2 distinct linear/dense layers for doing this, but note that it is doing this only for the encoder to produce the latent vector $\mathbf{z}$.Yes, generally, these layers (so the mean and variance) are not the same for the encoder and decoder. However, it would not be surprising to me if someone already tried to share some layers between the encoder and decoder for some specific task."
"In general, when are the normal, uniform and zero initializers used?","
I came across a Conv2D layer in a fully convolutional network, which used a kernel_initializer='zero' for regression. Why is a kernel_initializer of 'zero' used here?
In general, when are 'normal', 'uniform' and 'zero' initializers used?
","['convolutional-neural-networks', 'tensorflow', 'weights-initialization']",
How are theories represented using Conceptual Graphs?,"
Theory 1  shows three axioms and two definitions, written in First Order Logic (FOL), that represents a fragment of a mereology theory. For this posting, it is important that the set of axioms is considered as a theory (i.e. a set of axioms together with theorems as a logical consequence of those axioms). In the context of this question, the particular axioms are not significant. Any other set of axioms forming a consistent theory would be equally acceptable.
Theory 1
Axioms
Reflexivity $\forall x : part(x,x)$
Antisymmetry $\forall x  \forall y : ((part(x,y) \land part(y,x)) \implies (x = y))$
Transitivity $\forall x  \forall y \forall z :((part(x,y) \land part(y,z))  \implies part(x,z))$
Definitions
Overlap : $\forall x,y \colon (overlap(x,y) \iff(\exists z \colon  (part(z,x) \land part(z,y)))$
Proper Part : $\forall x,y \colon (proprPart(x,y) \iff (part(x,y) \land \neg part(y,x)))$
I am using CafeOBJ to represent the above logical axioms and definitions, shown in Listing 1:
Listing 1
   mod M{
    [E]
    preds overlap part properPart : E E
 -- axioms
 ax [M1] : \A[x:E] part(x,x) .  
 ax [M2] : \A[x:E]\A[y:E] ((part(x,y) & part(y,x)) -> (x = y))  .
 ax [M3] : \A[x:E]\A[y:E]\A[z:E]((part(x,y) & part(y,z))  -> part(x,z)) .
 -- definitions
 ax [DM1] : \A[x:E]\A[y:E] (properPart(x,y) <-> (part(x,y) & ~(part(y,x)))) .  
 ax [DM2] : \A[x:E]\A[y:E] (overlap(x,y) <->  (\E[z:E] (part(z,x) & part(z,y)))) .  
    }

Note that the logical theory is contained in a named module called M. The variables are over a domain of generic entities E, universal and existential quantification are denoted  by  \A[x:E] and  \A[x:E] respectively. In CafeOBJ, named modules allow one to structure signatures, theories, sub/super theories, and models using the Theory of Institutions (TOI).
Below is my naïve attempt to present the axioms as a set of conceptual graphs (CG).  My motivation for using CGs is that they provide an intuitive visualization of logic and have a direct relation to Common Logic (ISO zipped PDF).

The above CG was produced using CharGer software as Java zip file (manual).
My understanding of the above CGs is as follows:

The variables are universally quantified, not default for CGs, but allowed in extended CG (ECG).
The three graphs are all related by conjunction, which is default for GC.
The arrow on graph representing reflexivity is bi-directional.
Both antisymmetry and transitivity are represented by an IF-THEN contexts.
Dotted lines are co-references.
Equality (=) is actually commutative, but is represented as a directed relation .
Each CG asserts a single proposition, labelled Proposition.

Question:
How do I present Theory 1 using CGs?  Do I need some labeling that indicates that a set of concepts represent a theory. Or are theories represented by some enclosing special type of concept?
","['logic', 'knowledge-representation', 'ontology']",
Would it be a good idea to mutate half of the offspring of each GA generation 100% of the time and the other half 0% of the time?,"
I was reading about genetic algorithms, and to my understanding a genetic algorithm (GA) is an algorithm that starts with an initial population of chromosomes, where each chromosome has associated with it a fitness score, and it evolves the population such that the chromosomes in the population have an on average better fitness score than the chromosomes in the initial population. A GA accomplishes this by selecting the chromosomes in the population with the best fitness scores, and then it combines those chromosomes using a crossover function to produce offspring chromosomes. Those offspring may or may not be mutated randomly according to a probability that the offspring will be mutated. This process of selection, crossover, and mutation is iterated (with each successive iteration of the population being called a 'generation') until the fitness score of the population as a whole is deamed satisfactory. Please correct me if any of this is wrong.
My idea is with regards to mutating the offspring produced during the crossover phase. In the relatively simple implementations of GAs that I've looked at on the internet, most seem to randomly mutate the offspring according to a mutation rate. For example, the offspring may have a ten percent chance of mutating. When the GA is ran, the fitness function generally improves a lot over the first several generations, but it often stagnates for a while when one chromosome takes over essentially the entire population and little mutation occurs. This problem can be partially solved by increasing the mutation rate, but that also increases the probability of having offspring with a lower fitness score than the parents if the mutation poorly affects the fitness score. My idea is to make it so that half of the offspring produced with each generation have a one hundred percent mutation rate, and the other half has a zero or relatively low mutation rate. This could potentially lower the number of generations necessary to reach a satisfactory fitness score.
Is this a good idea? Would it work? Would it be better than other methods of mutating the offspring of each crossover?
","['genetic-algorithms', 'evolutionary-algorithms', 'genetic-operators']",
Textbook for CNN-LSTM networks of predictions of numerical data,"
I am learning NN algorithms because I'd like to create my own project.
What I found on the internet, is that for my type of project which I have in mind CNN-LSTM neural network would be ideal.
But now I have a question - I don't know if it's against the rules of this forum or not. So pardon me if I violated them.
So, now I am learning NN algorithms from a couple of books that ""classify"" them like: Classification NN, LSTM, Convolutional - each neural network is a separate topic in each book.
But I am looking for a book that teaches the reader about Convolutional Long-Short Term Memory Neural Network. Does someone know such a book where such hybrid NN is the main topic?
","['resource-request', 'books']",
What are the types of data in which the order of instances does matter?,"
In general, the order of instances in the datasets that are used in machine learning is immaterial. But there are exceptions. Timeseries data is one such exception I know. Consider the following two excerpts
#1: From 4.3 Representing tabular data of the textbook titled Deep Learning with PyTorch by Eli Stevens et.al.

At first we are going to assume there’s no meaning to the order in
which samples appear in the table: such a table is a collection of
independent samples, unlike a time series, for instance, in which
samples are related by a time dimension.

#2: From an answer

Moreover, there are datasets that contain elements whose order in the
dataset can be relevant for the predictions, such as datasets of
time-series data, while, in mathematical sets and multi-sets, the
order of the elements does not matter.

I want to know the types of such data in which the order of instances does matter. Are there any other kinds of data except Timeseries in which the order of instances does matter?
",['datasets'],
What is the definition of a trace of a tensor?,"
Tensor is a multi-dimensional ordered collection of elements, which is used in deep learning to store and process data as well as intermediate steps.
We are aware of the trace of a two-dimensional tensor i.e. matrix. It is defined as the sum of the diagonal elements of the matrix.
Is there any definition for the trace of a tensor?
","['machine-learning', 'deep-learning', 'math', 'definitions', 'tensor']",
Which value to propagate in Monte Carlo Tree Search in a non-zero-sum game?,"
Usually, when I read about Monte Carlo Tree Search, values between 0 and 1 (or values between -1 and 1) are backpropagated, depending on whether the simulation was a win or loss.
Now, suppose you have an AI which needs to play a game in which it is also important to score as high as possible. For example, it needs to score as many points as possible in the game of Carcassonne against one other player.
What kind of options are there for the values being backpropagated in such cases? Can you just backpropagate the number of points, and then, depending on the node, use the points of only the players in UCT? Or would that lead to the search converging to a worse move than the optimal move?
","['game-ai', 'monte-carlo-tree-search', 'board-games']",
Which ML algorithm is the best for predict the next PRNG generated numbers?,"
I have a homework. The task is to decide, if the PRNG generated lottery is attackable/crackable or not.
Details:
Lottery:
There is a lottery game where you have to choose 8 numbers between 1-20 for the field A and choose 1 number between 1-4 for field B. It is generated every 5 minutes(7:05 - 22:00), so there are ~64k draws/year.
For example:

A: [3,    5,  6,  7,  10, 13, 17, 18]
B: 2

Possible dependent variables:
Timestamp, DrawNum (It is between 85-264 every day. 7:05 is 85 because there are 425 minutes between 00:00 and 7:05. (425/5=85))

Unfortunately we don't have too much dependent variable and there is no clue for the PRNG algorithm.
I think this two dependent variable is not enough to predict the numbers. I am thinking on an LSTM to predict the next 1stNum based on the previous ones and use the same model for the other numbers.
What do you think? How would you predict the next set of numbers?
Which ML algorithm is the best for this use case?
","['neural-networks', 'machine-learning', 'long-short-term-memory', 'prediction']",
Non-sliding kernels for location-aware processing in Convolutional Neural Networks,"
My understanding of how CNN operates in image detection is through the use of kernels that slide through the image to detect features (edges and so on). So a single kernel could potentially be learning to detect an edge no matter where it is in the image. This is great for image recognition problems where an image of a dog shifted to the right or inverted is still an image of a dog. This article states ""the features the kernel learns must be general enough to come from any part of the image"". The article also states how using CNN for categorical data where the order in which data is organised is irrelevant can be ""disastrous"".
However, there are instances where it is desirable for the algorithm to be location-aware in order to classify better. Take the case of using CNN to train a network that will predict card play in the game of bridge (a version of double-dummy where all cards are laid out open - perfect information, deterministic). At the beginning of the game the cards dealt to the four could look (very unrealistically) something like this.

where Leader = the player playing the lead card in round 1, and the subsequent players organised as Leader.LeftHandOpponent, Leader.Partner and Leader.RightHandOpponent. Each player's cards are organised in four suits starting from the Trump_Suit and then the other suits in the original suit hierarchy. Cards go from highest value in the top 'A' to lowest value in the bottom '2'.
Here is a transpose of the image above.

This layout provides a lot of visual cues in terms of how the gameplay will proceed and who will end up winning how many tricks if viewed it from the perspective of control cards distribution within each suit and hand strength. So, the answer to the question of will CNN actually be able to process this data to provide good predictions is a resounding Yes (at least to me).
However, here is the problem - A regular CNN with a sliding kernel with a (4, 1) stride and no padding would make no distinction between the red boxes when in reality there is a massive difference between them.
Possible Solution? - A filter consisting of non-sliding kernels/kernels that only slide in one direction (perhaps horizontally or vertically) however would theoretically only seek to learn location-aware features and that could potentially improve accuracy? Just shooting arrow in the sky.
Has this been researched? Has anybody implemented this already? Could this work?
P.S: CNN has been used on AlphaGo Zero was great success. Obviously in the game of Go, patterns located in the top of the board carry the same weight as those located in the bottom. The gameplay does not change if the board is flipped 180 degrees. This however is not the case in the game of contract bridge. I am looking at ideas of how this can be resolved.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks']",
Can neural networks learn noise?,"
I'm interested in the following graphs. A neural network was trained to recognise digits from the MNIST dataset and then the labels were  randomly shuffled and the following behaviour was observed. How can this behaviour be explained?
What explains the apparent 'mirroring' of the graphs on the RHS, and the fact that training error on the RHS is approx. equal to validation error on the LHS?
It's evident from the graphs that the neural network is not learning noise - so what exactly is it learning?

","['neural-networks', 'machine-learning', 'overfitting', 'mnist']",
Object detection: when there's only 1 object in each image,"
Good day. I have a custom dataset for object detection, which has imbalance that each image has only one object annotation.
I trained the object detection model(Efficientdet-dx) on TensorFlow object detection API with this dataset. But the model predicts only one object in image, even though it has many trackable objects when the training has finished. It looks like the model learned in the wrong way that it should find only one object in a image.

Here's my question: In which way should I train the model, so the model find objects independently to the number of objects in the trainset?
It should be very helpful, if you help me.
What I tried:

Copy and Paste augmentation(CAP)

result: The dataset has no mask annotation unfortunately, so I used a deep learning model trained on open dataset for background subtraction, but the subtraction did not work well. As you can see, the edge of pasted objects is not clear. and some part of the objects are missing.





Mix another dataset like COCO, PASCAL VOC.

result: It wasn't that bad such like CAP. However, the mixed dataset becomes very big size and the unrelated labels also are in the predictable label list.



","['deep-learning', 'computer-vision', 'tensorflow', 'object-detection']",
How the vector-space isomorphism between $\mathbb{R}^{m \times n}$ and $\mathbb{R}^{mn}$ guarantees reshaping matrices to vectors?,"
Consider the following paragraph from section 5.4 Gradients fo Matrices of the chapter Vector Calculus from the textbook titled Mathematics for Machine Learning by Marc Peter Deisenroth et al.

Since matrices represent linear mappings, we can exploit the fact that
there is a vector-space isomorphism (linear, invertible mapping)
between the space $\mathbb{R}^{m \times n}$ of $m \times n$ matrices
and the space $\mathbb{R}^{mn}$ of mn vectors. Therefore, we can
re-shape our matrices into vectors of lengths $mn$ and $pq$,
respectively. The gradient using these $mn$ vectors results in a
Jacobian Matrices can be of size $mn \times pq$. .... In practical
applications, it is often desirable to re-shape the matrix into a
vector and continue working with this Jacobian matrix: The chain
rule... boils down to simple matrix multiplication, whereas in the
case of a Jacobian tensor, we will need to pay more attention to what
dimensions we need to sum out.

What I understood from the paragraph is: There is always a one-one mapping(?) between $\mathbb{R}^{m \times n}$ and $\mathbb{R}^{mn}$. So, we use this property to replace any element in $\mathbb{R}^{m \times n}$ (matrix) to an element in $\mathbb{R}^{mn}$.
I have doubt on how the property allows us to replace the matrix by vector without any discrepancies?
","['machine-learning', 'calculus', 'derivative', 'vector-space']",
What to predict in a limited transaction dataset?,"
I have been given a task with a real transaction dataset. The task is to predict something using either logistic regression or simple binary classification.
The columns are as follow:

Transaction ID
Quantity purchased
Product name
Coupon code
Transaction Date
City (where transaction was made)
Delivery fee (if any)
Total amount spent

I am having a rough time figuring out what to predict using regression or classification given only these columns.
i.e: Given a full row how much is the total spent... etc
In other words I need help deciding what will be the label of my dataset and what would be the reason behind choosing that label.
","['classification', 'datasets', 'prediction', 'regression']",
Are the authors of the VAE paper writing the PDFs as a function of the random variables?,"
Usually, I see the conventions:

discrete random variable is denoted as $X$,
the pmf is written as $P(X=x)$ or $p(X=x)$ or $p_{X}(x)$ or $p(x)$, where $x$ is an instance of $X$
a continuous random variable is denoted as $X$,
the pdf is denoted as $f_{X}(x)$ or $f(x)$, where $x$ is an instance of $X$; sometimes $p$ is used here too instead of $f$.

However, the VAE paper uses slightly different notation that I'm trying to understand

Let us consider some dataset $\mathbf{X}=\left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N}$ consisting of $N$ i.i.d. samples of some continuous or discrete variable $\mathrm{x}$. We assume that the data are generated by some random process, involving an unobserved continuous random variable $\mathbf{z}$. The process consists of two steps: (1) a value $\mathbf{z}^{(i)}$ is generated from some prior distribution $p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) ;(2)$ a value $\mathbf{x}^{(i)}$ is generated from some conditional distribution $p_{\boldsymbol{\theta}^{*}}(\mathbf{x} \mid \mathbf{z})$. We assume that the prior $p_{\boldsymbol{\theta}^{*}}(\mathbf{z})$ and likelihood $p_{\boldsymbol{\theta}^{*}}(\mathbf{x} \mid \mathbf{z})$ come from parametric families of distributions $p_{\boldsymbol{\theta}}(\mathbf{z})$ and $p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})$, and that their PDFs are differentiable almost everywhere w.r.t. both $\boldsymbol{\theta}$ and $\mathbf{z}$. Unfortunately, a lot of this process is hidden from our view: the true parameters $\theta^{*}$ as well as the values of the latent variables $\mathrm{z}^{(i)}$ are unknown to us.

So I am looking at these:

$p_{\boldsymbol{\theta}^{*}}(\mathbf{z})$
$p_{\boldsymbol{\theta}^{*}}(\mathbf{x} \mid \mathbf{z})$
dataset $\mathbf{X}=\left\{\mathbf{x}^{(i)}\right\}_{i=1}^{N}$

So I know the subscript for $\theta$ denotes those are the parameters for the pdf. It says ""discrete variable $\mathrm{x}$"", ""unobserved continuous random variable $\mathbf{z}$"", and  ""latent variables $\mathrm{z}^{(i)}$"".  In the top, where I wrote "" discrete random variable $X$"", seems like that's the equivalent of ""discrete variable $\mathrm{x}$"" in this paper.
So, it looks like they're writing the PDFs as a function of the random variables. Is my assumption correct? Because it is different than the typical conventions I see.
edit: looks like his other paper has a notation guide, in the appendix, though it seems like he's conflating both random vector and instances of vector in the notation?
https://arxiv.org/pdf/1906.02691.pdf
","['papers', 'variational-autoencoder', 'probability-distribution', 'notation']","When it comes to notation/terminology, often, people in machine learning are (a bit?) sloppy, which causes a lot of confusion, especially for newcomers to the field or people not very math-savvy. I was also confused about this notation at some point (see my last questions here, which are all about this confusing topic). See also this answer.In the VAE paper, $\mathbf{X}$ is a dataset, as the authors write.Your confusion also arises because the authors vaguely use the term ""probability distribution"", rather than pdf or pmf, to refer, for example, to $p_{\boldsymbol{\theta}^{*}}(\mathbf{z})$, which thus does not refer to a pdf or pmf. In fact, the authors also writetheir PDFs are differentiable almost everywhere w.r.t. both $\boldsymbol{\theta}$ and $\mathbf{z}$The $\mathbf{z}$ can refer toIf it's the first case, then $p_{\boldsymbol{\theta}^{*}}(\mathbf{z})$ is the composition of 2 functions (because a rv is also a function).If it's the second case, then $p_{\boldsymbol{\theta}^{*}}(\mathbf{z})$ is the evaluation of $p_{\boldsymbol{\theta}^{*}}$ at $\mathbf{z}$.I think the 2nd case is the most likely. In addition, people are being sloppy here and use the notation $p_{\boldsymbol{\theta}^{*}}(\mathbf{z})$ (rather than just $p_{\boldsymbol{\theta}^{*}}$) to emphasize $p_{\boldsymbol{\theta}^{*}}$ is a function of some input variable (not random variable!), which we denote with the letter $\mathbf{z}$ to remind ourselves that $\mathbf{z}$ is associated with a random variable denoted with the same letter (and maybe also in bold and lowercase).So, in this case, let's say we denote the random variable associated with $p_{\boldsymbol{\theta}^{*}}(\mathbf{z})$ with $\mathbf{z}$, then we could refer to this associated prior more explicitly as follows $p_\mathbf{z}(\mathbf{z})$ (but that would even be more confusing). It would have been a better idea to use $\mathbf{Z}$, but then we may use the upper case letters to denote matrices or sets (like the VAE paper), so we end up with this mess (which is one of the 2 mythical difficult problems well-known in Computer Science, i.e. naming things), which we need to learn to deal with or just ignore.Conclusion: when I look at $p_{\boldsymbol{\theta}^{*}}(\mathbf{z})$, which has been referred to as a probability distribution, I think there's also some associated random variable, which people, in that same context, will probably denote as $\mathbf{z}$ or $\mathbf{Z}$. There may also be some input variable (not a random variable), which we denote by $\mathbf{z}$ or $z$. If they are not mentioned, then I just ignore that. I never think that $p_{\boldsymbol{\theta}^{*}}(\mathbf{z})$ is the composition of 2 functions (even if that's the case), because that case was never useful in my readings."
Is there a mathematical formalism to deal with a missing reward signal?,"
Typically, a Reinforcement Learning learning problem is formalized as finding an optimal policy for a Markov Decision Process (MDP). In many real-life situations, however, an agent can only get partial information from the environment. For example, Partially Observable MDPs are used to model the case where the agent does not fully observe the state.
I was wondering whether there is any well-established formalism for the case where the agent does not fully observe the reward signal.
In particular, I am thinking about the case where for every state-action pair $(s, a)$ the agent receives the reward $R(s, a)$ with probability $1 - \varepsilon$ and does not receive anything with probability $\varepsilon$. Of course, in principle, this setting can be thought as a regular MDP with a stochastic reward, but here I would like the agent to behave optimally w.r.t. to $R$.
I would really appreciate if you could point some relevant literature to me!
","['reinforcement-learning', 'reference-request', 'markov-decision-process', 'reward-functions', 'pomdp']",
Reduction of state space of the game Connect Four to apply RL algorithms SARSA and Q-Learning,"
I would like to implement the reinforcement learning algorithms SARSA and Q-Learning for the board game Connect Four.
I am familiar with the algorithms and know about their limitations regarding large states spaces due to storing all of this information in the Q-Table. Connect Four has a large state space estimated around 7.1*1013, e.g. MIT Slide 7, which is why simply applying these algorithms won't work (though this thesis claims it did)
However, I have found this answer to a similar post that proposes a possible solution to simplify the state space.

For one thing, you could simplify the problem by separating the action space. If you consider the value of each column separately, based on the two columns next to it, you reduce N to 3 and the state space size to 106. Now, this is very manageable. You can create an array to represent this value function and update it using a simple RL algorithm, such as SARSA

Unfortunately, I don't understand the proposed simplification and would like to ask the following questions:

The action space is separated from the state space by considering each column separately. However, if my understanding of SARSA and QL is correct they use Q(S,A) to estimate the value function, therefore the state action pair is assigned a value.
How does one calculate the value of a column based on the two columns next to it?
Also what does next to it mean in this context? If the two adjacent columns of each column are used then we create five pairs (N=5) or are pairs created from the inside out (e.g. middle three columns, middle five, all seven)?
Is a state (of the entire board?) then mapped to the array containing the value function for each action/column?

Any references to other literature or simplifications would be much appreciated!
","['reinforcement-learning', 'q-learning', 'sarsa', 'connect-four']",
What is remembering in Hopfield network?,"
Hopfield is a simple and traditional network. We feed into the network some patterns (Learning/Training). There is no training in Hopfield as the weight calculation adds up all the strength between neurons. The network goes into remembering mode by feeding a new unseen pattern (partially corrupted), and then the input is deactivated. The network iterates until it reaches a global or local minimum.
My question is that it finally remembers anything. It means that it remembers one of those patterns (combinations).
For example, we have five neurons. It remembers one of those $2^5=32$ patterns. So, one can say OK, this is what I am looking for, but it is not. What mechanism is available in the Hopfield network to check whether the found pattern is identical or similar to the input pattern?
","['machine-learning', 'recurrent-neural-networks', 'hopfield-network']",
How do I use machine learning to create an optimization algorithm?,"
Let's say that I want to create an optimization algorithm, which is supposed to find an optimum value for a given objective function. Creating an optimization algorithm to explore through the search space can be quite challenging.
My question is: can machine learning be used to automatically create optimization algorithms? Is there any source to look at for this?
","['machine-learning', 'optimization', 'algorithm-request', 'optimizers']","Machine learning has been used to automatically learn new optimization/learning algorithms. This task is often known as meta-learning, i.e. you learn to learn, in this case, an optimization algorithm, but note that meta-learning does not just refer to learning optimization algorithms (see this blog post).The blog post Learning to Optimize with Reinforcement Learning (2017) is a good introduction to the topic and focuses on the approach proposed in this paper Learning to Optimize (2016), which uses reinforcement learning to solve this problem: more specifically, they learn a policy (in practice, represented as a neural network) that represents the learned optimization algorithm.There are other related approaches: for example, you may be interested in the paper Learning to learn by gradient descent by gradient descent (2016, NeurIPS)."
Is there a way to adapt Particle Swarm Optimization to an incremental/online learning setting?,"
As stated in the title, is there a way to adapt PSO to an online scenario where new data samples arrive continuously?
In more detail: suppose that I have a classifier with several parameters for which the optimal values are to be chosen automatically, instead of being predefined. I want to use PSO to select the parameters. I know this is doable in a static scenario, where the data set is fixed. However, if new data samples arrive over time (and in large amounts), is there a way to make PSO work on such dynamic data streams?
Also, I am open to other ways to implement self-adaptive parameters. PSO is a possible choice but if it's not possible I'd love to hear your suggestions about other approaches.
","['reference-request', 'optimization', 'hyperparameter-optimization', 'incremental-learning', 'online-learning']",
"Why is val accuracy 100% within 2 epochs and incorrectly predicting new images? (1,000 images per class when training)","
My CNN tensorflow model reports 100% validation accuracy within 2 epochs. But it incorrectly predicts on single new images. (It is multiclass problem. I have 3 classes). How to resolve this? Can you please help me understand these epoch results?
I have 1,000 images per class that are representative of my testing data. How can validation accuracy reach 1.00 in just the first epoch when I have a dataset of 3,000 images in total, equal amount per class? (I would expect this to start at around 33% percent -- 1/ 3 classes.)
I understand overfitting can be a problem. I've added a dropout layer to try to solve this potential problem. From this questionWhat to do if CNN cannot overfit a training set on adding dropout? I learned that a ""model is over-fitting if during training your training loss continues to decrease but (in the later epochs) your validation loss begins to increase. That means the model can not generalize well to images it has not previously encountered."" I don't believe my model is overfitting based on this description. (My model reports both high training and high validation accuracy. If my model was overfitting I'd expect high training accuracy and low validation accuracy.)
My model:
def model():
  model_input = tf.keras.layers.Input(shape=(h, w, 3)) 
  x = tf.keras.layers.Rescaling(rescale_factor)(model_input) 
  x = tf.keras.layers.Conv2D(16, 3, activation='relu',padding='same')(x)
  x = tf.keras.layers.Dropout(.5)(x)
  x = tf.keras.layers.MaxPooling2D()(x) 
  x = tf.keras.layers.Flatten()(x)
  x = tf.keras.layers.Dense(128, activation='relu')(x)
  outputs = tf.keras.layers.Dense(num_classes, activation = 'softmax')(x)

Epoch results:
Epoch 1/10
/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: ""`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?""
  return dispatch_target(*args, **kwargs)
27/27 [==============================] - 13s 124ms/step - loss: 1.0004 - accuracy: 0.5953 - val_loss: 0.5053 - val_accuracy: 0.8920
Epoch 2/10
27/27 [==============================] - 1s 46ms/step - loss: 0.1368 - accuracy: 0.9825 - val_loss: 0.0126 - val_accuracy: 1.0000
Epoch 3/10
27/27 [==============================] - 1s 42ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 5.9116e-04 - val_accuracy: 1.0000
Epoch 4/10
27/27 [==============================] - 1s 42ms/step - loss: 3.0633e-04 - accuracy: 1.0000 - val_loss: 3.5376e-04 - val_accuracy: 1.0000
Epoch 5/10
27/27 [==============================] - 1s 42ms/step - loss: 1.7445e-04 - accuracy: 1.0000 - val_loss: 2.2319e-04 - val_accuracy: 1.0000
Epoch 6/10
27/27 [==============================] - 1s 42ms/step - loss: 1.2910e-04 - accuracy: 1.0000 - val_loss: 1.8078e-04 - val_accuracy: 1.0000
Epoch 7/10
27/27 [==============================] - 1s 42ms/step - loss: 1.0425e-04 - accuracy: 1.0000 - val_loss: 1.4247e-04 - val_accuracy: 1.0000
Epoch 8/10
27/27 [==============================] - 1s 42ms/step - loss: 8.6284e-05 - accuracy: 1.0000 - val_loss: 1.2057e-04 - val_accuracy: 1.0000
Epoch 9/10
27/27 [==============================] - 1s 42ms/step - loss: 7.0085e-05 - accuracy: 1.0000 - val_loss: 9.3485e-05 - val_accuracy: 1.0000
Epoch 10/10
27/27 [==============================] - 1s 42ms/step - loss: 5.4979e-05 - accuracy: 1.0000 - val_loss: 8.5952e-05 - val_accuracy: 1.0000

Model.fit and model.compile:
model = model()

model = tf.keras.Model(model_input, outputs)
  
 model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])
  
hist = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=10
)

Code to predict new image:
def makePrediction(image):
  from IPython.display import display
  from PIL import Image
  from tensorflow.keras.preprocessing import image_dataset_from_directory 
  img = keras.preprocessing.image.load_img(
  image, target_size=(h, q)
  )
  img_array = keras.preprocessing.image.img_to_array(img)
  img_array = tf.expand_dims(img_array, 0) #Create a batch
 
  predicts = model.predict(img_array)
  p = class_names[np.argmax(predicts)]
  return p

Going to the ""data"" directory and using the folders to create a dataset. Each folder is a class label:
from keras.preprocessing import image
directory_data = ""data""
tf.keras.utils.image_dataset_from_directory(
    directory_testData, labels='inferred', label_mode='int',
    class_names=None, color_mode='rgb', batch_size=32, image_size=(256,
    256), shuffle=True, seed=123, validation_split=0.2, subset=""validation"",
    interpolation='bilinear', follow_links=False,
    crop_to_aspect_ratio=False
)
 
tf.keras.utils.image_dataset_from_directory(directory_testData, labels='inferred')

Creating dataset and splitting it:
Train_ds code: (Output: Found 1605 files belonging to 3 classes.
Using 1284 files for training.)
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  directory_data = ""data"",
  validation_split=0.2,
  subset=""training"",
  seed=123,
  image_size=(h, w),
  batch_size=batch_size)

Val_ds code: (Output: Found 1605 files belonging to 3 classes.
Using 321 files for validation.)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
directory_data = ""data"",
  validation_split=0.2,
  subset=""validation"",
  seed=123,
  image_size=(h, w),
  batch_size=batch_size)

","['machine-learning', 'deep-learning', 'tensorflow', 'keras']",
Why batch normalization before upsampling is giving worse results?,"
I am training a model to generate images.
The model contains 5+5 layers:
Conv2D -> Upsample -> Conv2D -> Upsample -> Conv2D -> Upsample -> Conv2D -> Upsample -> Conv2D -> Upsample

I am modifying it as
Conv2D -> BatchNorm -> Upsample -> Conv2D -> BatchNorm -> Upsample -> Conv2D -> BatchNorm -> Upsample -> Conv2D -> BatchNorm -> Upsample -> Conv2D -> BatchNorm -> Upsample

I am applying the batch normalization layers just before upsampling as shown above and hence I am not getting the results that are at least comparable to the results by the model without any batch normalization layer.
Is my placement of the batch normalization layer wrong? If yes, then why?
","['deep-learning', 'convolutional-neural-networks', 'generative-model', 'batch-normalization']",
What is the stride information of an image referring here?,"
In convolutional neural networks, the convolution and pooling operations have a parameter known as stride, which decides the amount of jump the kernel needs to do on the input image. You can get more information regarding stride from follows taken from here

Stride is the number of pixels shifts over the input matrix. When the
stride is 1 then we move the filters to 1 pixel at a time. When the
stride is 2 then we move the filters to 2 pixels at a time and so on.

But, I am not getting what does it mean by stride information of an image at the tensor level. Consider the following paragraph from the chapter named Real-world data representation using tensors from the textbook titled Deep Learning with PyTorch by Eli Stevens et al.

img = torch.from_numpy(img_arr)
out = img.permute(2, 0, 1)

We’ve seen this previously, but note that this operation does not make
a copy of the tensor data. Instead, out uses the same underlying
storage as img and only plays with the size and stride information
at the tensor level. This is convenient because the operation is very
cheap; but just as a heads-up: changing a pixel in img will lead to
a change in out.

It is mentioning about the stride information at the tensor level of an image. Do they mean the strides that are related to the CNN, pooling, etc., or are they referring to any other stride information?
","['deep-learning', 'convolutional-neural-networks', 'terminology', 'image-processing', 'stride']","Do they mean the strides that are related to the CNN, pooling, etc., or are they referring to any other stride information?The stride referred to by the quote ""only plays with the size and stride information at the tensor level"" is referring to internal storage of tensors. Luckily in most normal conversations about AI logic you do not care about this, it has nothing to do with neural network implementation, but is a lower-level optimisation (compared to some other ways of implementing tensors that would not use an internal stride).Most general-purpose computer chips do not use multi-part addressing for data, such as (x,y,z), but instead use a single memory location number. To construct a data structure to represent a 2-or-more dimensional tensor means combining more fundamental representations. There are a few different ways to do this. One might be an array of arrays, where the outer array contains pointers to memory addresses of inner arrays. However, a single array with some management numbers attached to it (typically at the beginning, or even as a separate data structure, so you can have multiple tensor views of the same data) has some advantages - some operations on the array are far more efficient. Reshaping and transposing are two operations that can be implemented far more efficiently when the storage is arranged in this way.So, along with the tensor cell data (the number inside each location), a typical tensor implementation will store information separately about how that data is structured. One useful piece of data is the stride, which is how many memory locations to add for each step in a particular direction in the n-indices view of the data. For example in a 2-indexed tensor with shape (6,8) where the data is stored in rows, then incrementing the column index will take a +1 step in memory location, whilst incrementing the row index will take a +8 step (the number of columns in a row). This is the row stride for that shape.Technically these two types of stride are conceptually very similar. The difference is the level within the code that they are implemented in. Usually for most work using a tensor library you only care about this ""internal"" stride happening because it gives you confidence that there is a low cost to a particular tensor operation, such as generating a different shaped view.Most importantly, unless you are working on your own tensor library, or digging into the internals of one using a low-level language (C), then you do not need to specify the stride values used. Instead you can take the quoted text as a declaration that a particular operation is implemented efficiently and you can use it in your own code with confidence that there is not a high CPU (or GPU) cost to using it."
Why do Transformers have a sequence limit at inference time?,"
As far as I understand, Transformer's time complexity increases quadratically with respect to the sequence length. As a result, during training to make training feasible, a maximum sequence limit is set, and to allow batching, all sequences smaller are padded.
However, after a Transformer is trained, and we want to run it on a single sequence at inference time, the computational costs are far less than training. Thus, it seems reasonable that I would want to run the transformer on a larger input sequence length during inference time. From a technical perspective, this should be feasible.
I keep reading online that a Transformer cannot be run on a sequence size larger than the one seen during training. Why is this? Is it because the network weights will be unfamiliar with sequences of this length? Or is it more fundamental?
","['machine-learning', 'natural-language-processing', 'transformer', 'architecture', 'sequence-modeling']","Transformer models have limited sequence length at inference time
because of positional embeddings. But there are workarounds.Self-attention in transformer does not distinguish the order of keys/values,
it works as if the sequence is a bag of words.So to expose the sequence order to the model, one typically adds an extra
""positional embedding"" vector to each input token embedding.
This extra positional information then allows the model to construct primitives like ""attention head that looks at previous token"".There are several variants of how to do this exactly.One way is to use an extra trainable parameter matrix [L,M]
where L is maximum input length and M is model dimension.
After training a model this way, it simply becomes impossible
to embed tokens with position > L. So we get a hard limit for which there is no workaround.Another way is to use a non-trainable matrix [L,M]
initialized with a specially crafted set of sinusoids
(original ""attention is all you need"" paper does this).
A model trained this way would not have a length limit,
because this matrix of sinusoids you can extend to arbitrary size.Yes another way is to use ""relative positional encoding"".
With this, for each pair of tokens you take relative position,
which would be in range [-L ... +L], and embed that instead.
Then you inject this vector into attention layer in the right way
(section 3.3 in https://arxiv.org/pdf/1901.02860.pdf).Now you still kind of have a limit of [-L, L].
But you can always ""clip"" relative position to this range,
and pretend that all pairs of tokens at distance >= L
have the same relative position (""very far from each other"").
And this allows to run inference with longer inputs."
Are derived or computed inputs bad for CNNs?,"
I am building a CNN and am wondering if inputting derived or computed inputs are generally bad for the effectiveness of CNNs? Or just NNs in general?
By derived or computed values I mean data that is not ""raw"" and instead is computed based on the raw data. For example, in a very simple form, using time-series data as the ""raw"" data and computing a 30 day SMA as a ""derived/computed"" value, and as another input.
Is this bad practice at boosting the effectiveness of the network? If it is not a bad practice, are there any tips on what kind of computed values someone should consider when adding new inputs?
The goal of my NN is for building predictions in time-series data.
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'data-preprocessing', 'feature-engineering']",
tfp.Distributions.Categorical.sample() is picking the same action everytime after certain iterations,"
I have written a code for an RL agent such that at each state the model calculates the probabilities of all possible actions and samples one action randomly to proceed further. To acheive this, I have written the following code
act_prob_dist = tfp.distributions.Categorical(probs=act_probs)
action = act_prob_dist.sample()

It is working fine in the initial stages of training. Once the model has learnt about the particular state really well and the probability of one particular action has increased significantly than the others, the sample() call is picking is the same action every time. For example, when the action probabilities of a particular state are
tf.Tensor(
[[0.05213022 0.06613996 0.4933109  0.02918373 0.04188393 0.04100212
  0.03228914 0.00716161 0.08877521 0.02158365 0.04645196 0.07092285
  0.00916469]], shape=(1, 13), dtype=float32)

The model is sampling an action randomly. After decent iterations of learning the action probabilities became
tf.Tensor(
[[1.12852089e-12 1.54888698e-06 6.40413802e-08 1.03480375e-11
  2.05246806e-08 2.17290430e-09 1.04494591e-09 5.20959872e-11
  9.99995708e-01 1.26053008e-08 6.85156265e-10 1.70332885e-06
  9.99039457e-07]], shape=(1, 13), dtype=float32)

The model started picking index with high probability every time(index 8 in this case). The documentation reads Generate samples of the specified shape. I'm assuming it implies the choosing happens randomly. Can someone please explain why same action is being chosen in my case?
PS: tf.version is returning tensorflow._api.v2.version
","['reinforcement-learning', 'deep-learning', 'tensorflow', 'deep-rl', 'tensorflow-probability']",
FrozenLake-v0 not training using REINFORCE,"
I am implementing a simple REINFORCE (policy gradient) algorithm for openAI's FrozenLake-v0 environment. However, it does not seem to learn anything at all.
I have used the same neural architecture for openAI's CartPole-v0, and trained it using REINFORCE (policy gradient), and it works perfectly. So, what I am doing incorrectly for the FrozenLake-v0 environment? I think this has to do with the nature of the environment, but I am unsure which aspects of training REINFORCE must be altered to accommodate the dynamics of FrozenLake-v0. It seems like a very simple environment to solve, given that it has only 16 states.
My code is as follows:
import gym
from gym.envs.registration import register
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt


# helper function for conversion of a state into an input to a neural network
def OH(x, n):
    '''
    :param x: state id
    :param n: n_states
    :return:  1-hot encoded numpy array of size [1,n]
    '''
    one_hot = np.zeros((n,))
    one_hot[x] = 1
    return one_hot



def running_mean(x, n):
    N=n
    kernel = np.ones(N)
    conv_len = x.shape[0]-N
    y = np.zeros(conv_len)
    for i in range(conv_len):
        y[i] = kernel @ x[i:i+N]
        y[i] /= N
    return y


# architecture of the Policy Network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, n_actions):
        super().__init__()
        self.n_actions = n_actions
        self.model = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, n_actions),
            nn.Softmax(dim=0)
        ).float()

    def forward(self, X):
        return self.model(X)


def train_reinforce_agent(env, episode_length = 100, max_episodes = 50000, gamma = 0.99, visualize_step = 50, learning_rate=0.003):

    # define the parametric model for the Policy: this is an instantiation of the PolicyNetwork class
    model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)
    # define the optimizer for updating the weights of the Policy Network
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)


    # hyperparameters of the reinforce agent
    EPISODE_LENGTH = episode_length
    MAX_EPISODES = max_episodes
    GAMMA = gamma
    VISUALIZE_STEP = max(1, visualize_step)
    score = []



    for episode in range(MAX_EPISODES):
        # reset the environment
        curr_state = env.reset()
        done = False
        transitions = []

        # rollout an entire episode from the Policy Network
        for t in range(EPISODE_LENGTH):
            act_prob = model(torch.from_numpy(curr_state).float())
            action = np.random.choice(np.array(list(range(env.action_space.n))), p=act_prob.data.numpy())
            prev_state = curr_state
            curr_state, _, done, info = env.step(action)
            transitions.append((prev_state, action, t+1))

            if done:
                break
        score.append(len(transitions))
        reward_batch = torch.Tensor([r for (s, a, r) in transitions]).flip(dims=(0,))


        # compute the return for every state-action pair from the rewards at every time-step
        batch_Gvals = []
        for i in range(len(transitions)):
            new_Gval = 0
            power = 0
            for j in range(i, len(transitions)):
                new_Gval = new_Gval + ((GAMMA ** power) * reward_batch[j]).numpy()
            power += 1
            batch_Gvals.append(new_Gval)

        # normalize the returns for the batch
        expected_returns_batch = torch.FloatTensor(batch_Gvals)
        expected_returns_batch /= expected_returns_batch.max()

        # batch the states, actions, prob after the episode
        state_batch = torch.Tensor([s for (s, a, r) in transitions])
        action_batch = torch.Tensor([a for (s, a, r) in transitions])
        pred_batch = model(state_batch)
        prob_batch = pred_batch.gather(dim=1, index=action_batch.long().view(-1, 1)).squeeze()


        # compute the loss for one episode
        loss = -torch.sum(torch.log(prob_batch) * expected_returns_batch)

        # back-propagate the loss
        optimizer.zero_grad()
        loss.backward()
        # update the parameters of the Policy Network
        optimizer.step()

        # print the status after every VISUALIZE_STEP episodes
        if episode % VISUALIZE_STEP == 0 and episode > 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(episode, np.mean(score[-VISUALIZE_STEP:-1])))


    # Training plot: Episodic reward over Training Episodes
    score = np.array(score)
    avg_score = running_mean(score, visualize_step)
    plt.figure(figsize=(15, 7))
    plt.ylabel(""Episode Duration"", fontsize=12)
    plt.xlabel(""Training Episodes"", fontsize=12)
    plt.plot(score, color='gray', linewidth=1)
    plt.plot(avg_score, color='blue', linewidth=3)
    plt.scatter(np.arange(score.shape[0]), score, color='green', linewidth=0.3)
    plt.show()

","['reinforcement-learning', 'deep-learning', 'deep-rl', 'policy-gradients', 'reinforce']",
Are there neural networks with (hard) constraints on the weights?,"
I don't know too much about Deep Learning, so my question might be silly. However, I was wondering whether there are NN architectures with some hard constraints on the weights of some layers. For example, let $(W^k_{ij})_{ij}$ be the weights of the (dense) $k$-th layer.
Are there architectures where it is imposed something like
$$
\sum_{i, j} (W^k_{ij})^2 = 1
$$
(namely the roll-out vector of weights is constrained to stay on a sphere) or $W^k_{ij}$ are equivalence classes $mod K$ for some number $K>0$?
Then, of course, one should probably think about proper activation functions for these cases, but it's probably not a big obstacle.
Putting constraints of these kinds will prevent the weights to grow indefinitely and maybe could prevent over-fitting?
","['neural-networks', 'deep-learning', 'deep-neural-networks', 'weights', 'weight-normalization']",
Are Genetic Algorithms suitable for a problem with a non-unique optimal solution?,"
I was wondering if a genetic algorithm is useful if the optimization problem has several optimal solutions.
My thought was that I should not use it since when combining two members of a population who have good fitness but are close to different optimal solutions, the child will get retarded.
Is this thinking wrong? If so, why?
","['optimization', 'genetic-algorithms']",
Rank of gradient-of-loss with respect to layer weights in an MLP,"
The paper: https://arxiv.org/abs/2110.11309, makes the following claim at the end of page 3:

The gradient of loss $L$ with respect to weights $W_l$ of an MLP is a rank-1 matrix for each of B batch elements $\nabla_{w_l}L = \sum_{i=1}^B \delta_{l+1}^i {u_l^i}^T$, where $\delta_{l+1}^i$ is the gradient of the loss for batch element $i$ with respect to the preactivations at layer $l + 1$, and ${u_l^i}^T$ are the inputs to layer $l$ for batch element i.

Suppose that we have an MLP with $k$ hidden layers (every hidden layer is followed by an activation function). Then the weight matrices will be $W_1, W_2, \dots, W_k$ (plus the biases, but they are irrelevant for now), and their sizes will be $(D_1, D), (D_2, D_1), \dots (D_k, D_{k-1})$ correspondingly, where $D$ is the number of input features.
Therefore, hidden layer $l$ has a weight matrix $W_l$ of size $(D_l, D_{l-1})$. Its gradient wrt the loss (for 1 batch element), $\frac{\partial L}{\partial W_l}$, will also be a matrix of size $(D_l, D_{l-1})$.
So if I understand correctly, the authors of the paper are claiming that $\frac{\partial L}{\partial W_l}$ is a rank-1 matrix? That is, every row (or column) can be expressed as a linear combination of 1 only row (or column)? If yes, why? How?
","['weights', 'multilayer-perceptrons', 'gradient']",
What is the time complexity of DDPG algorithm?,"
Suppose we have a DDPG algorithm.
The actor has N input nodes, two hidden layers with J nodes, and S output nodes.
The critic has N+S input nodes, two hidden layers with C nodes, and one output node.
How does the time complexity of this algorithm could be calculated??
","['neural-networks', 'reinforcement-learning', 'time-complexity']",
Why is there tanh(x)*sigmoid(x) in a LSTM cell?,"
CONTEXT
I was wondering why there are sigmoid and tanh activation functions in an LSTM cell.

My intuition was based on the flow of tanh(x)*sigmoid(x)

and the derivative of tanh(x)*sigmoid(x)

It seems to me that authors wanted to choose such a combination of functions, the derivative would make possible big changes around the 0, since we can use normalized data and weights. Another thing is that the output would go to 1 for positive values and go to 0 for negative values which is convenient.
On the other hand, it seems natural that we use sigmoid in forget gate, since we want to have a better focus on the important data. I just don't understand why there cannot only be a sigmoid function in the input gate.
OTHER SOURCES
What I found on the web is this article where the author claims:

To overcome the vanishing gradient problem, we need a method whose second derivative can sustain >for a long range before going to zero. Tanh is a good function that has all the above properties.

However, he doesn't explain why this is the case.
Also, I found the opposite statement here, where the author says that the second derivative of the activation function should go to zero, however, there is no proof for that claim.
QUESTION
Summing up:

Why cannot we put a signal with just a sigmoid on the input gate?
Why there are tanh(x)*sigmoid(x) signals in the input and output gate?

","['neural-networks', 'long-short-term-memory', 'activation-functions', 'sigmoid', 'tanh']","The tanh functions within the cell represent cell output or cell state. These are the values that either get passed on to other layers, or within the layer to the next time step. In theory, other activation functions could be used here according to taste, similar to other feed-forward or RNN networks. However, the -1 to 1 output range of tanh is useful, and I expect tanh has been experimentally validated as a good general case activation function here.The sigmoid functions are used as soft gates for manipulating the raw RNN values. Importantly for your analysis, there is no sigmoid that takes the same input as any tanh. Each of the green boxes in the cell diagram in your question has a separate learnable set of weights applied to the combined input+hidden_state vector.That means that your analysis of tanh(x)*sigmoid(x) is moot. The function is effectively tanh(x)*sigmoid(y) because inputs to each activation function can be radically different.The intuition is that the LSTM can learn relatively ""hard"" switches to classify when the sigmoid function should be 0 or 1 (depending on the gate function and input data). As the weights are independent on the gates and input value processing components, the gradients to the cell output and state components are not composed in a combined function, but simply multiplied by the current value of the relevant switch. A muliplying hard switch of 1 will allow the gradient to flow back directly from the output loss to the point at which the gate decision was made - depending on which gate was activated, this improved gradient signal will either be to the input processing weights or the hidden state procesing weights.It is also possible for the input and cell state processing to be mixed in various combinations, and the gradient is not guaranteed strong. However, in situations requiring strong memory-like signals (such as using punctuation characters when processing text), it is possible to observe LSTM learning those signals, effectively classifying inputs with high confidence (close to either 0 or 1), thus creating toggle switches, counters etc, within the cell state vector."
Why am I getting a very small number as CNN prediction?,"
I created a CNN using Tensorflow to identify pneumonia and sometimes it returns a very small number as a prediction. why is this happening?
I have attached the link for the dataset
Here I how I process and load the data.
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator( rescale = 1.0/255. )
val_datagen  = ImageDataGenerator( rescale = 1.0/255. )
test_datagen = ImageDataGenerator( rescale = 1.0/255. )

train_generator = train_datagen.flow_from_directory('/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/train/',
                                                    batch_size=20,
                                                    class_mode='binary',
                                                    target_size=(350, 350)) 

validation_generator =  val_datagen.flow_from_directory('/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/',
                                                         batch_size=20,
                                                         class_mode  = 'binary',
                                                         target_size = (350, 350))
test_generator = test_datagen.flow_from_directory('/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/test/',
                                                         batch_size=20,
                                                         class_mode  = 'binary',
                                                         target_size = (350, 350))

And here the Model, compile and fit functions
import tensorflow as tf

model = tf.keras.models.Sequential([
    # Note the input shape is the desired size of the image 150x150 with 3 bytes color
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(350, 350, 3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2), 
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), 
    tf.keras.layers.MaxPooling2D(2,2),
    # Flatten the results to feed into a DNN
    tf.keras.layers.Flatten(), 
    # 512 neuron hidden layer
    tf.keras.layers.Dense(1024, activation='relu'), 
    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')
    tf.keras.layers.Dense(1, activation='sigmoid')  
])

compile the model
from tensorflow.keras.optimizers import RMSprop

model.compile(optimizer=RMSprop(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics = ['accuracy'])

model fit
history = model.fit(train_generator,
                              validation_data=validation_generator,
                              steps_per_epoch=200,
                              epochs=2000,
                              validation_steps=200,
                              callbacks=[callbacks],
                              verbose=2)

The evaluation metrics as followings,
loss: 0.2351 - accuracy: 0.9847
The prediction shows a big negative number for the negative pneumonia, and for positive it shows more than .50.
I have two equations?

why I getting a very small number as 2.xxxx * 10e-20

why I can't get the following values as null?
val_acc  = history.history[ 'val_accuracy' ]
val_loss = history.history['val_loss' ]


I am a still student in machine learning and I really appreciate your effort to solve my question.
","['convolutional-neural-networks', 'tensorflow', 'image-recognition']",
How much labelling is required for NER with SpaCy?,"
I have transaction data and I would like to extract the merchant from the transaction description. I am new to this but I just came across Named Entity Recognition and SpaCy. I have hundreds of thousands of different merchants.
Some questions that I have:

How much labelling do I need to do given the number of merchants I need to extract?

How many different instances of the same merchant I need to label to get decent results?


","['machine-learning', 'deep-learning', 'natural-language-processing', 'named-entity-recognition', 'spacy']",
What does it mean to apply decomposition at inference-time in a machine translation system?,"
I'm reading this paper for sub-character decomposition for logographic languages and the authors mention decomposition at inference-time. They're using Transformer architecture.
More specifically, the authors write:

We propose a flexible inference-time sub-character decomposition procedure which targets unseen characters, and show that it aids adequacy and reduces misleading overtranslation in unseen character translation.

What do inference-time and inference-only decomposition mean in this context? My best guess would be that inference-time would be at some point during the decoding process, but I'm not 100% clear on whether that's the case and, if so, when exactly.
I'm going to keep digging and update if I find something helpful. In the meantime, if anyone needs more context just let me know.
","['natural-language-processing', 'terminology', 'transformer', 'machine-translation', 'inference']","I've found this article that seems to answer my question:
https://hazelcast.com/glossary/machine-learning-inference/From this, my understanding is that inference-time describes when a machine learning system is put into use following training; so basically at the time of task application.I think this would mean that the paper's authors are stating that the decomposition of sub-characters is occurring whenever the model is actively translating languages in a production environment."
Do we use validation and test sets for training a reinforcement learning agent?,"
I am pretty new to reinforcement learning and was working with some code for the PPO and DQN algorithms. After looking at the code, I noticed that the authors did not include any code to setup a validation or testing dataloader. In most other machine learning
training loops, we generally include a validation and testing dataset to assure that the model is not overfitting the training data. However, in reinforcement learning the data is all simulated from the same environment, so perhaps the overfitting issue is not such a big deal?
Anyhow, could someone please indicate whether it is standard practice to only use a training dataset or dataloader for reinforcement learning, and to ignore validation or testing datasets?
","['reinforcement-learning', 'training', 'dqn', 'proximal-policy-optimization']","No, we typically don't use a validation/test data set in Reinforcement Learning (RL). This is because of how we use the data in RL. The use of a data set is very different to the classic supervised/unsupervised paradigms. Some RL algorithms don't even have a data-set as such. For instance, the vanilla tabular Q-learning does not use a data-set -- it will see an experience tuple $(s, a, r, s')$ and make an update based on this, and discard it, until it is potentially see again during training.I have not looked at the code you have looked at for PPO and DQN but I would wager that the data loader they use is for a) in PPO when they are optimising the most recent trajectory, or b) use a data loader for the sampled experience from a replay buffer in DQN.Note that the replay buffer is technically a dataset, but it is not a traditional dataset as in the other paradigms. That is essentially becauseTo validate RL agents we typically assess how the trained agent performs on it's intended task."
Which algorithms are used to locate objects in a 3d space?,"
I can see mobile apps that can locate a 3D object on a surface with a mobile camera and you can turn around that object.
What is the name of the algorithm(s) that is used for that purpose? Or, is there AI in these algorithms? Are they use plane detection? After detection, which algorithm do they use to locate the objects?
It seems like a Computer Vision problem, but I do not know the title.

","['neural-networks', 'machine-learning', 'computer-vision', 'algorithm-request']",
Use soft-max post-training for a ReLU trained network?,"
For a project, I've trained multiple networks for multiclass classification all ending with a ReLU activation at the output.
Now the output logits are not probabilities.
Is it valid to get the probability of each class by applying a softmax function at the end, after training?
","['neural-networks', 'probability', 'softmax', 'multiclass-classification']",
How to build an expert system similar to ES-Builder Web,"
I made a simple expert system using ES-Builder. Please click the link to view it. ES-Builder is a web-based expert system shell. There is a tree-based knowledge representation. In ES builder, User Interfaces are also automatically designed. They generate a link as I have shared above and anyone can access it and can use it.
But when I try other ES shells such as JESS, CLIPS & PyKE, I only noticed that there we have to write facts and rules and the program is run on command line upon consulting the Expert System. There is no UI like in ES-Builder.
My question is, is there any way to build UI to the expert systems created by CLIPS/JESS? Or else should I create a web application using another framework like Spring, DotNet, and integrate it with the knowledge base created with CLIPS/JESS?
(I am a bit confused, because according to what I have learned: if we use an expert system shell then we need not program it using languages (such as Prolog). Because the User Interfaces and Inference Engine is already there. What we are remained to do is just to build the knowledge base. Similar to ES builder UI is auto built.)
Thank you very much for the support! If the question is confusing, I am happy to modify it in a more understandable manner.
",['expert-systems'],
Test accuracy decreases during my train process,"

I want to train a neural network model with the arcface loss function and try to combine it with domain adaption. But when the training process continues, I find the test accuracy first increases and then decreases, the model cannot reach convergence. I chose the office31 dataset, and the feature_extractor was resnet50.
I want to know if it is caused by my code, or by my loss function
The arcface function was set as
def Arc_pred(cosine, s=64.0, m=0.1):
    cosine = cosine / s
    thea = torch.acos(cosine)
    top = torch.exp(torch.cos(thea + m) * s)
    _top = torch.exp(torch.cos(thea) * s)
    bottom = torch.sum(torch.exp(cosine * s), dim=1).view(-1, 1)
    divide = (top / (bottom - _top + top)) + 1e-10
    return divide

and my total loss function was set as
total_loss = 0.1*target_entropy_loss + label_loss + arc_loss + discriminator_loss

In that, the target_entropy_loss tries to make the decision boundary cross the sparsest sample area，label_loss was the classification loss, discriminator_loss was a domain adaptation loss function.
I tried to set a learning rate schedule for my experiment, it seems it did not work. So, could it be caused by my loss function?
","['machine-learning', 'objective-functions', 'unsupervised-learning', 'performance', 'accuracy']",
General approaches in text encoding and labelling for NLP [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



What are the approaches of encoding text data? I would be glad to hear some summarization from experienced persons.
And are there any solutions accepting words outside the vocabulary and including them to the results (online machine learning)?
Data input
So my basic understanding is that if we want predict some value (linear regression) or say what is the probability of occuring some event (logistic regression) we have to gather some features as our input and encode them as number. But this is not necessarily true when working with continuous data like sentences.
The most naive aproach, which comes to my mind is just to assign some natural numbers to each word in the vocabulary. But this number does not contain any meaningful data about the word itself. On the other hand what seems to be important in NLP is just the order of the words. This is where I think about n-grams so we feed network with more than just one word. Or attention like in the Transformer.
Another idea, which cames to my mind is to vectorize the word using one of the Word Embedding technique. Here we have some context about the word so the input is not just a dumb number. But does it have any value when we want to predict the next word? Can Word Embedding be used in that way or it's purpose is completely different.
Last thing I was reading of was to encode characters rather than words but it feels pointless in such basic example as next word prediction. I would think about it more for sub-word tasks like inflection generating.
Labelling
Again based on my knowledge when we want to solve yes/no problem we're using sigmoid function. If we have more classes we can use one-hot encoding. But sometimes the output of the network might give us ambiguous meaning so we're using the softmax function so all output sum to 1.
How this looks in NLP area? When having a vocabulary consisting of 600k words do we really need 600k softmaxed outputs? I'm also thinking there about Word Embedding solutions where we can reduce the number of outputs to let's say 300 numbers and then find the closest word matching the output without using softmax.
","['neural-networks', 'natural-language-processing', 'data-preprocessing', 'word-embedding', 'data-labelling']",
Is it a bad practice to use cumulative rewards in reinforcement learning,"
I am using a DDPG agent for doing prediction on the position on an asset in a stock trading-like environment. I am using the cumulative reward as the reward for each timestep. Since it is trained over many years of data, the reward tends to become large. I have realized that after some training the agent becomes lazy, it just keeps the same position.
Is it a bad practice to use cumulative rewards as rewards? Would the daily revenue be a better reward for the agent?
","['reinforcement-learning', 'rewards', 'ddpg', 'reward-functions']",
Feature Extraction for printer classification,"
I need some advice. I am currently trying to do a printer classification with ML/DL.
What do I have?
11 colored-images with high resolution from 8 different inkjet-printers (in total 88 images)
I have 8 classes (printers)
All images are scanned with 2.400 dpi, so you are able to see the halftone of the images and the matrix dots
I know each printers are different in terms of size of matrix dots, dot pattern etc.
Based on that I need to do a feature extraction and train a ML model which can classify the correct printer. There is a previous work which has been done with Wavelet-Transformation for feature extraction and SVM for classification. The goal now is to find another approach of feature extraction and training.
My question here is, what do you think is the best solution?
My idea is:
Isolate the dots into binary color (black/white)
do an edge detection with opencv (using filters like sobel, canny etc.)
But I am not sure if this is a good approach. After reading a lot of papers on related work I found out many used Transfer Learning (e.g. VGG, Resnet) where features are learned in the training process.
So basically I have images and when you can zoom in you see for each printers the pattern are different. So instead of doing Wavelet-Transformation I need to do another approach.
In the litarature common feature extractor for this are Gray-levcel Co-Occurences, Wavelet-Transformation, Spartial filters which will be used in SVM or AdaBoost. Another approach is as said above with pre-trained CNN (transfer learning).
So, what do you think I should tackle next?

","['machine-learning', 'deep-learning', 'computer-vision', 'python', 'opencv']",
"What is the ""temperature"" in the GPT models?","
What does the temperature parameter mean when talking about the GPT models?
I know that a higher temperature value means more randomness, but I want to know how randomness is introduced.
Does temperature mean we add noise to the weights/activations or do we add randomness when choosing a token in the softmax layer?
","['machine-learning', 'terminology', 'gpt', 'language-model', 'gpt-3']","In sequence generating models, for vocabulary of size $N$ (number of words, parts of words, any other kind of token), one predicts the next token from distribution of the form:
$$
\mathrm{softmax} (x_i/T) \quad i = 1, \ldots N,
$$
Here $T$ is the temperature. The output of the softmax is the probability that the next token will be the $i$-th word in the vocabulary.The temperature determines how greedy the generative model is.If the temperature is low, the probabilities to sample other but the class with the highest log probability will be small, and the model will probably output the most correct text, but rather boring, with small variation.If the temperature is high, the model can output, with rather high probability, other words than those with the highest probability. The generated text will be more diverse, but there is a higher possibility of grammar mistakes and generation of nonsense.The difference between the low-temperature case (left) and the high-temperature case for the categorical distribution is illustrated in the picture above, where the heights of the bars correspond to probabilities.ExampleA good sample is provided in the Deep Learning with Python by François Chollet in chapter 12. An extract from the tutorial, refer to this notebook."
Where do the objective functions proposed in this paper by Carlini-Wagner attack come from?,"
I'm trying to understand the paper by Carlini and Wagner on deep neural networks adversarial attacks. On page 44, in Section V-A, it is explained how the loss function to the described problem was designed. One part of this loss function is called ""objective function"".
There are 7 such functions proposed on which experiments are made, but there is no information provided on where they came from and why those are chosen.
Is this some commonly known thing in the Deep Learning area? Do you recognize them and can tell me where they came from and why they were chosen?
\begin{align}
f_{1}\left(x^{\prime}\right) &= -\operatorname{loss}_{F, t}\left(x^{\prime}\right)+1 \\
f_{2}\left(x^{\prime}\right) &= \left(\max _{i \neq t}\left(F\left(x^{\prime}\right)_{i}\right)-F\left(x^{\prime}\right)_{t}\right)^{+} \\
f_{3}\left(x^{\prime}\right) &= \operatorname{softplus}\left(\max _{i \neq t}\left(F\left(x^{\prime}\right)_{i}\right)-F\left(x^{\prime}\right)_{t}\right)-\log (2) \\
f_{4}\left(x^{\prime}\right) &= \left(0.5-F\left(x^{\prime}\right)_{t}\right)^{+} \\
f_{5}\left(x^{\prime}\right) &= -\log \left(2 F\left(x^{\prime}\right)_{t}-2\right) \\
f_{6}\left(x^{\prime}\right) &= \left(\max _{i \neq t}\left(Z\left(x^{\prime}\right)_{i}\right)-Z\left(x^{\prime}\right)_{t}\right)^{+} \\
f_{7}\left(x^{\prime}\right)&= \operatorname{softplus}\left(\max _{i \neq t}\left(Z\left(x^{\prime}\right)_{i}\right)-Z\left(x^{\prime}\right)_{t}\right)-\log (2)
\end{align}
","['deep-learning', 'objective-functions', 'adversarial-ml']",
Does the Frechet Inception Distance (FID) consider color?,"
I was wondering if the Frechet inception distance
for two colored datasets would be the same than the FID calculated for the same datasets converted to grayscale.
I know that it depends on the feature extraction, which is the Inception network. And that is the question, I don't fully understand the role of color in CNNs. I thought that all color information is lost in the feature extraction, and in this case I grayscale and colored datasets would produce the same FID. But I am not sure about that.
","['convolutional-neural-networks', 'datasets', 'inception', 'fid-score']",
"Can Q-learning be used for my scenario, and how might I do so?","
I have already asked 2-3 general questions w.r.t Q learning and now I am asking a scenario specific one. I will try to be concise and understandable. I really really need help.
Scenario: I have a network with few nodes and links. On each link, there are some slots (#1 to #800). I generate traffic requests (come one by one) that want to go from one node to another and need certain slots to do so. So, my task is to allot the slots to each upcoming request and finally achieve a low rejection probability i.e. able to allot slots to as many requests as possible. The allotted slots are also freed up depending on when the arrived requests leave the system. I use the Poisson process to do, but this is not important here.
What I thought: There have been certain simple benchmarks to do this but I wanted to use Q learning so that in the long run the agent (a centralized controller) takes better decisions as to which slots to assign on the particular link i.e. which slots position (#1 to #800).
What I did: I decided to take the state space as the links (say 10 links in my case) and the action space as the #1 to #800 slots. I use binary notation 1 to say slot is occupied or 0 that is free.
Problem encountered: But it is long later I realized that my state space is infinitely big. For E.g. For request 1, I give two slots on Link1 & state is 1 1 0 0 0 .... up to 800 zeros. Another request comes (say 3 slots) and say Link1 state is now 1 1 1 1 1 0 0 0...up to 800 zeros. This is when I realized that the state space is unimaginably large as departures can also occur leading to freeing up and some 1s becoming 0s and so on.
What I am asking for: So, does anyone have any ideas on how can I still use Q learning in this case. The point is that someone already used deep Q on this. I was thinking I am approaching it in a different and simplified way of just using link state as state space that would enable me to have a small Q table. But it is later I realized that each link state will vary every time and lead to large state space thus putting me back to square one after investing a lot of time on this. So, please give any suggestions as I don't want to leave it altogether.
","['reinforcement-learning', 'deep-rl', 'q-learning', 'dqn']",
Alternatives to neural networks for function approximation in Q learning?,"
I want to know if there is anything other than neural networks (or Deep NNs) that I can effectively use to perform function approximation? I am asking this w.r.t to the use of approximators in Q learning with large state space.
","['neural-networks', 'q-learning', 'dqn', 'function-approximation']",
How to estimate conditional density using neural network?,"
Conditional Variational Autoencoders (CVAE) and Mixture Density Networks (MDN) are supposed to address this issue. However, these models provide the distribution parameters, e.g., mean and standard deviation, for each given sample, while I need a single underlying distribution that the given data is generated from.
To put it simply, I would like to find the parameters of a normal distribution that estimates $P(Y \mid X)$, given $X$ and $Y$. Let's imagine the given data, $X$, have an $(n, m)$ dimension, where $n$ and $m$ indicate the number of samples, and features, respectively.
Using CVAE/MDN I get the parameters with dimension $(n, d)$, where $d$ is the dimension of the parameters. But I am looking for a model to provide parameters with dimensions $(1, d)$.
","['neural-networks', 'generative-model', 'variational-autoencoder', 'density-estimation']",
"When to use the state value function $V(s)$ and when to use the state-action value function $Q(s, a)$?","
I saw the difference between value function $V(s)$ and $Q(s, a)$. But when do I use each one? When I coded in Matlab I only used $Q(s, a)$ directly (as I was thinking of a tabular approach). So, when is more beneficial than the other? I have a large state space.
","['reinforcement-learning', 'comparison', 'value-functions']","The core differences between using $V(s)$ or $Q(s,a)$ are:You either need a separate policy function $\pi(a|s)$ that it is the value function for, or you can derive a policy from it if you have access to the environment's distribution model $p(r,s'|s,a)$ - the probability of receiving reward $r$ and arriving in next state $s'$ given start $s,a$. The derived policy would be deterministic $\pi(s) = \text{argmax}_a \sum_{r,s'} p(r,s'|s,a)(r + \gamma V(s'))$In comparison, $Q(s,a)$ can be used to derive a policy without reference to any model: $\pi(s) = \text{argmax}_a Q(s,a)$. This makes the action value function Q necessary for model-free value-based control methods. Hence Monte-Carlo Control, SARSA, Q-Learning will all use action values.Space used by a state value table is $O(|\mathcal{S}|)$Space used by an action-value table is $O(|\mathcal{S} \times \mathcal{A}|)$These space concerns also apply when you are using approximation - the input domain you need to learn to approximate $Q(s,a)$ is larger than the one you need to learn $V(s)$. Although for large state spaces with lots of dimensions, the relative difference after approximation may be small enough that you don't really care.The smaller space makes $V(s)$ a good choice if you can use it. The benefits are smaller memory footprint, and often faster learning because there are fewer separate values to learn (not always faster, as you still need to explore all actions in all states to learn accurate values).ororIf none of these situations applies, then you will have to use action-value function $Q(s,a)$. You might also choose to use $Q(s,a)$ because it is easier to stick with it in whatever framework you are working in. Sometimes the benefits of using $V(s)$ instead are not large enough to be worth the effort."
Derive information for sub-scoring from one scoring model,"
I am currently working in Python with a random forest algorithm to perform a scoring. My output is binary.
The idea now is to derive sub-scores from the above model that give an opinion on different topics within my dataset.
Unfortunately, I don't have an output variable for these sub-scores in my dataset, so I have the idea of deriving the information from the feature importance of the larger model.
Any ideas on how to do this methodically?
As an example:
The goal is to determine if a person is creditworthy. The dataset contains a lot of information about the person's occupation, the area where he/she lives, past payment history etc. Now I want a score for overall creditworthiness and sub-scores for

Job features
Features on the housing situation
Features on the payment history/behaviour

","['machine-learning', 'ai-design', 'feature-extraction', 'random-forests', 'score-prediction']",
